id,title,created_at,reply_count,views,description,creator_link,creator_name,creator_alias,post_date,post_likes,replies,repliers_links,reply_dates,reply_likes
81879,Please redirect all xla questions to pytorch/xla github issues,2020-05-18T19:41:52.497Z,0,123,"<div class=""post"" itemprop=""articleBody""><NewLine><p>This category is to discuss xla/TPU related issues.</p><NewLine><p>Note that the recommended way to ask questions about pytorch TPU support (or XLA integration) is via  issues using provided template in <a href=""https://github.com/pytorch/xla"">https://github.com/pytorch/xla</a>.</p><NewLine></div>",https://discuss.pytorch.org/u/albanD,(Alban D),albanD,"July 20, 2020,  4:43pm",1 Like,,,,
96150,What does xmp.MpSerialExecutor() really do?,2020-09-13T21:58:38.695Z,0,26,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Good day, I’m kinda new to using pytorch xla. In the colab tutorial provided <code>serial executor</code> was used. I was just wondering in what other scenario can it be used</p><NewLine><pre><code class=""lang-auto""> # Using the serial executor avoids multiple processes to<NewLine>  # download the same data.<NewLine>  train_dataset, test_dataset = SERIAL_EXEC.run(get_dataset)<NewLine><NewLine>  train_sampler = torch.utils.data.distributed.DistributedSampler(<NewLine>    train_dataset,<NewLine>    num_replicas=xm.xrt_world_size(),<NewLine>    rank=xm.get_ordinal(),<NewLine>    shuffle=True)<NewLine>  train_loader = torch.utils.data.DataLoader(<NewLine>      train_dataset,<NewLine>      batch_size=FLAGS['batch_size'],<NewLine>      sampler=train_sampler,<NewLine>      num_workers=FLAGS['num_workers'],<NewLine>      drop_last=True)<NewLine>  test_loader = torch.utils.data.DataLoader(<NewLine>      test_dataset,<NewLine>      batch_size=FLAGS['batch_size'],<NewLine>      shuffle=False,<NewLine>      num_workers=FLAGS['num_workers'],<NewLine>      drop_last=True)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/random-ness,(random-ness),random-ness,"September 13, 2020,  9:58pm",,,,,
87811,Import error on using torch_XLA,2020-07-03T08:19:23.264Z,3,119,"<div class=""post"" itemprop=""articleBody""><NewLine><p>As I am working on TPU kernal,</p><NewLine><p>I have executed these lines<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/91cfa38021d6a0f3d2475d919c77978be98f598d"" href=""https://discuss.pytorch.org/uploads/default/original/3X/9/1/91cfa38021d6a0f3d2475d919c77978be98f598d.png"" title=""image""><img alt=""image"" data-base62-sha1=""kNU60X7a49Ft2sWfzpL06e7bbat"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/1/91cfa38021d6a0f3d2475d919c77978be98f598d_2_10x10.png"" height=""384"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/1/91cfa38021d6a0f3d2475d919c77978be98f598d_2_690x384.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/1/91cfa38021d6a0f3d2475d919c77978be98f598d_2_690x384.png, https://discuss.pytorch.org/uploads/default/optimized/3X/9/1/91cfa38021d6a0f3d2475d919c77978be98f598d_2_1035x576.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/9/1/91cfa38021d6a0f3d2475d919c77978be98f598d.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1228×684 95.9 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>and then got this error on running these imports<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/d06ab817ed0f8606f99a0412bd32598a2ded33d9"" href=""https://discuss.pytorch.org/uploads/default/original/3X/d/0/d06ab817ed0f8606f99a0412bd32598a2ded33d9.jpeg"" title=""wqwqw""><img alt=""wqwqw"" data-base62-sha1=""tJJXqu5V1h2W6S2Kk9A66Y1prcZ"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/0/d06ab817ed0f8606f99a0412bd32598a2ded33d9_2_10x10.png"" height=""259"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/0/d06ab817ed0f8606f99a0412bd32598a2ded33d9_2_690x259.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/0/d06ab817ed0f8606f99a0412bd32598a2ded33d9_2_690x259.jpeg, https://discuss.pytorch.org/uploads/default/optimized/3X/d/0/d06ab817ed0f8606f99a0412bd32598a2ded33d9_2_1035x388.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/d/0/d06ab817ed0f8606f99a0412bd32598a2ded33d9_2_1380x518.jpeg 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">wqwqw</span><span class=""informations"">1442×542 71.1 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p><a class=""mention-group"" href=""/groups/admins"">@admins</a> <a class=""mention"" href=""/u/smth"">@smth</a> <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> please help.</p><NewLine></div>",https://discuss.pytorch.org/u/IamSparky,(Soumo Chatterjee),IamSparky,"July 4, 2020,  7:36am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please don’t tag certain people, as this might discourage others to answer and might also tag users, who are not experts in the topic.</p><NewLine><p>That being said, I’ll move your topic to the XLA category to add visibility.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks ptrblck … can please suggest some tips and tricks to add visibility to my topics while posting? I would very greatful as I am new to pytorch forums.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve already moved the topic to the right category, so let’s wait until <a class=""mention"" href=""/u/ailzhang"">@ailzhang</a> and other XLA expert have some time to look into the topic. As it’s weekend, you might need to wait a bit. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ailzhang"">@ailzhang</a> or anyone from torch xla team if you can help please</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/iamsparky"">@IamSparky</a>, sorry for the late reply, we take all xla/tpu related questions/feature requests/bug reports as issues in pytorch/xla repo. Would you mind posting the issue in pytorch/xla github with a link to the colab notebook? I’m pretty sure this should work but it’s hard to locate without seeing the code.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/IamSparky; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/IamSparky; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ailzhang; <NewLine> ,"REPLY_DATE 1: July 4, 2020,  7:35am; <NewLine> REPLY_DATE 2: July 4, 2020,  7:39am; <NewLine> REPLY_DATE 3: July 4, 2020,  7:41am; <NewLine> REPLY_DATE 4: July 19, 2020,  8:59am; <NewLine> REPLY_DATE 5: July 20, 2020,  4:39pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
86480,Input tensor is not an XLA tensor: torch.FloatTensor,2020-06-23T00:17:42.884Z,0,108,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I tried implementing an autoencoder model using a custom dataset on tpu via PyTorch xla.<br/><NewLine>It was showing me error as <strong>Input tensor is not an XLA tensor: torch.FloatTensor</strong> so i changed all the variables to tpu device and I still can’t figure out why is it showing so.</p><NewLine><p>Below is a snippet of error and it seems that the same snippet is repeating for different tpu’s</p><NewLine><blockquote><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py”, line 119, in _start_fn<br/><NewLine>fn(gindex, *args)<br/><NewLine>File “”, line 94, in map_fn<br/><NewLine>train_losses,val_losses   = train(100,train_loader,test_loader,criterion,device)<br/><NewLine>File “”, line 21, in train<br/><NewLine>outputs = model(img_grey)<br/><NewLine>File “/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py”, line 550, in <strong>call</strong><br/><NewLine>result = self.forward(*input, **kwargs)<br/><NewLine>File “”, line 41, in forward<br/><NewLine>midlevel_features = self.midlevel_resnet(input)</p><NewLine></blockquote><NewLine><p>this also<br/><NewLine>Exception in device=TPU:5: torch_xla/csrc/aten_xla_bridge.cpp:69 : Check failed: xtensor</p><NewLine><p>Train Function</p><NewLine><pre><code class=""lang-auto"">def train(n_epochs,data_loader_train,data_loader_val,criterion,device):<NewLine>  best_losses=1e10 <NewLine>  model.train()  <NewLine>  train_losses=[]<NewLine>  val_losses=[]<NewLine><NewLine>  ## Trains<NewLine>  train_start = time.time()<NewLine>  for epoch in range(1, flags['num_epochs']+1):<NewLine> <NewLine>      data_loss = 0.0<NewLine>      total =0 <NewLine><NewLine>      time_start=time.time()<NewLine>      para_train_loader = pl.ParallelLoader(data_loader_train, [device]).per_device_loader(device)<NewLine>      for i,data in enumerate(para_train_loader):<NewLine><NewLine>          img_lab,img_original,img_grey,img_ab,target = data<NewLine>          img_lab,img_original,img_grey,img_ab,target= img_lab.to(device),img_original.to(device),img_grey.to(device),img_ab.to(device),target.to(device)<NewLine>        <NewLine>          outputs = model(img_grey)<NewLine>          loss = criterion(img_ab,outputs)<NewLine><NewLine>          optimizer.zero_grad()<NewLine>          loss.backward()<NewLine>          # optimizer.step()<NewLine>          xm.optimizer_step(optimizer)<NewLine><NewLine>          data_loss += loss.item()<NewLine>          total += 1         <NewLine>         <NewLine>          if i % 50 == 0:<NewLine>            print('Epoch: {} \tIteration: {} Training Loss: {:.6f} \tTime Taken :{:.3f}'.format(<NewLine>          epoch, <NewLine>          i,<NewLine>          data_loss/total,<NewLine>          time.time()-time_start<NewLine>          ))<NewLine> <NewLine><NewLine>      data_loss = data_loss/total<NewLine>      print('Process: {} \tEpoch: {} \tTraining Loss: {:.6f} \tTime Taken :{:.3f}'.format(<NewLine>          index,<NewLine>          epoch, <NewLine>          data_loss,<NewLine>          time.time() - time_start<NewLine>          ))<NewLine>      train_losses.append(data_loss) <NewLine>      losses = validation(data_loader_val, model, criterion,epoch,device)<NewLine>      val_losses.append(losses)<NewLine>          # Save checkpoint and replace old best model if current model is better<NewLine>      if losses &lt; best_losses:<NewLine>        best_losses = losses<NewLine>        print('=====Saving Best Model========')<NewLine>        torch.save(model.state_dict(), 'checkpoints/tpu_{}_resent_model-epoch-{}-losses-{:.3f}.pth'.format(index,epoch,losses)) <NewLine>  print(""Process"", index, ""finished training. Train time was:"", time.time() - time_start)    <NewLine>  return train_losses,val_losses<NewLine></code></pre><NewLine><p>map function</p><NewLine><pre><code class=""lang-auto"">def map_fn(index, flags):<NewLine>  # Sets a common random seed - both for initialization and ensuring graph is the same<NewLine>  torch.manual_seed(flags['seed'])<NewLine><NewLine>  # Acquires the (unique) Cloud TPU core corresponding to this process's index<NewLine>  device = xm.xla_device()  <NewLine><NewLine><NewLine>  image_size=224<NewLine>  transform=transforms.Compose([<NewLine>          transforms.Resize((image_size,image_size)),<NewLine>          transforms.RandomHorizontalFlip(),<NewLine>          transforms.ToTensor()<NewLine>          # transforms.Normalize([0.485, 0.456, 0.406],<NewLine>          #                      [0.229, 0.224, 0.225]<NewLine>                              #  )  # Imagenet standards<NewLine>      ])<NewLine><NewLine>  train_path=str('/content/gdrive/My Drive/Dataset_Grey_RGB/images_1/train')<NewLine>  test_path=str('/content/gdrive/My Drive/Dataset_Grey_RGB/images_1/val')<NewLine><NewLine><NewLine>  # normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],<NewLine>  #                                 std=[0.229, 0.224, 0.225])<NewLine>  # to_rgb = transforms.Lambda(lambda image: image.convert('RGB'))<NewLine>  # resize = transforms.Resize((224, 224))<NewLine>  # my_transform = transforms.Compose([resize, to_rgb, transforms.ToTensor(), normalize])<NewLine><NewLine>  # # Downloads train and test datasets<NewLine>  # # Note: master goes first and downloads the dataset only once (xm.rendezvous)<NewLine>  # #   all the other workers wait for the master to be done downloading.<NewLine><NewLine>  # if not xm.is_master_ordinal():<NewLine>  #   xm.rendezvous('download_only_once')<NewLine><NewLine>  train_dataset = GrayscaleImageFolder(root=train_path,transform=transform,)<NewLine><NewLine>  test_dataset = GrayscaleImageFolder(root=test_path,transform=transform)<NewLine>  <NewLine>  if xm.is_master_ordinal():<NewLine>    xm.rendezvous('init')<NewLine>    # xm.rendezvous('download_only_once')<NewLine>  <NewLine>  # Creates the (distributed) train sampler, which let this process only access<NewLine>  # its portion of the training dataset.<NewLine>  train_sampler = torch.utils.data.distributed.DistributedSampler(<NewLine>    train_dataset,<NewLine>    num_replicas=xm.xrt_world_size(),<NewLine>    rank=xm.get_ordinal(),<NewLine>    shuffle=True)<NewLine>  <NewLine>  test_sampler = torch.utils.data.distributed.DistributedSampler(<NewLine>    test_dataset,<NewLine>    num_replicas=xm.xrt_world_size(),<NewLine>    rank=xm.get_ordinal(),<NewLine>    shuffle=False)<NewLine>  <NewLine>  # Creates dataloaders, which load data in batches<NewLine>  # Note: test loader is not shuffled or sampled<NewLine>  train_loader = torch.utils.data.DataLoader(<NewLine>      train_dataset,<NewLine>      batch_size=flags['batch_size'],<NewLine>      sampler=train_sampler,<NewLine>      pin_memory=True,<NewLine>      num_workers=flags['num_workers'],<NewLine>      drop_last=True)<NewLine><NewLine>  test_loader = torch.utils.data.DataLoader(<NewLine>      test_dataset,<NewLine>      batch_size=flags['batch_size'],<NewLine>      sampler=test_sampler,<NewLine>      pin_memory=True,<NewLine>      num_workers=flags['num_workers'],<NewLine>      drop_last=True)<NewLine>  <NewLine>  ## Network, optimizer, and loss function creation|<NewLine><NewLine>  # Creates Autoencoer for 10 classes<NewLine>  # Note: each process has its own identical copy of the model<NewLine>  #  Even though each model is created independently, they're also<NewLine>  #  created in the same way.<NewLine>  # net = torchvision.models.alexnet(num_classes=10).to(device).train()<NewLine>  net = AutoEncoder().to(device)<NewLine>  criterion = torch.nn.MSELoss()<NewLine>  optimizer = torch.optim.RMSprop(net.parameters(), lr=0.001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)<NewLine><NewLine><NewLine>  train_losses,val_losses   = train(100,train_loader,test_loader,criterion,device)<NewLine>  plt.plot(train_losses)<NewLine>  plt.plot(val_losses)<NewLine>  plt.show()<NewLine></code></pre><NewLine><p>I am attaching code as well. Please let me know how can I rectify it or point me to the correct location where I can find its solution.</p><NewLine><p><a href=""https://github.com/rakshitsakhuja/AutoEncoder/blob/master/Gray_RGB_AutoEncoder_TPU.ipynb"" rel=""nofollow noopener"">Code</a></p><NewLine></div>",https://discuss.pytorch.org/u/rakshitsakhuja,(Rakshit Sakhuja),rakshitsakhuja,"June 23, 2020, 12:17am",,,,,
81839,Function tensor.cpu() takes a lot of time with pytorch xla multicore,2020-05-18T14:50:25.429Z,1,121,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using pytorch xla with TPU on google colab. As expected computations are indeed faster, however when I go back to cpu to save my tensors, the “action” of going back to the cpu takes a lot of time for big tensors. Here is the function I run in parallel:</p><NewLine><pre><code class=""lang-auto"">def _mp_fn(index, batch_size, model, typee, directory_join):<NewLine>    device = xm.xla_device()<NewLine>    xm.master_print(f""Here"")<NewLine>    if not xm.is_master_ordinal():<NewLine>        xm.rendezvous(""download_only_once"")<NewLine><NewLine>    loader = sherBERTEmbedding(f_df, typee)<NewLine><NewLine>    if xm.is_master_ordinal():<NewLine>        xm.rendezvous(""download_only_once"")<NewLine>    xm.master_print(f""There"")<NewLine><NewLine>    sampler = DistributedSampler(loader, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=False)<NewLine>    dataloader = DataLoader(<NewLine>        loader,<NewLine>        batch_size=batch_size,<NewLine>        sampler=sampler,<NewLine>        # drop_last=True,<NewLine>    )<NewLine><NewLine>    model = model.to(device)<NewLine>    model.eval()<NewLine><NewLine>    length = len(dataloader)<NewLine><NewLine>    para_loader = pl.ParallelLoader(dataloader, [device])<NewLine>    num_b = len(dataloader)<NewLine>    tensor_embeddings = torch.zeros((batch_size*num_b, 153, 768), dtype=torch.float).to(device)<NewLine>    tensor_ids = torch.zeros((batch_size*num_b, 153), dtype=torch.long).to(device)<NewLine>    tensor_index = torch.zeros(batch_size*num_b, dtype=torch.long).to(device)<NewLine><NewLine>    toremove = 0<NewLine>    prev_ind = 0<NewLine><NewLine>    with torch.no_grad():<NewLine>        for t, (x, x_pad_mask, index) in tqdm(<NewLine>            enumerate(para_loader.per_device_loader(device)), total=length<NewLine>        ):<NewLine>            x = x.to(device=device, dtype=torch.long)<NewLine>            x_pad_mask = x_pad_mask.to(device=device, dtype=torch.long)<NewLine>            result = get_embeddings_best_token(x, x_pad_mask, model_embed, index_best_tokens)<NewLine>            ids = get_ids_best_tokens(x, index_best_tokens)<NewLine><NewLine>            tensor_embeddings[prev_ind: prev_ind + result.size(0), :, :] = result<NewLine>            tensor_ids[prev_ind: prev_ind + ids.size(0), :] = ids<NewLine>            tensor_index[prev_ind: prev_ind +index.size(0)] = index.squeeze(1)<NewLine>            prev_ind += result.size(0)<NewLine><NewLine><NewLine>            if result.size(0) != batch_size:<NewLine>                toremove = (batch_size - result.size(0))<NewLine><NewLine>        if toremove == 0:<NewLine>            toremove = tensor_embeddings.size(0)+1<NewLine><NewLine><NewLine>        #saving<NewLine>        directory_embeddings = os.path.join(os.path.join(directory_join, typee), ""embeddings"")<NewLine>        directory_ids = os.path.join(os.path.join(directory_join, typee), ""ids"")<NewLine>        directory_index = os.path.join(os.path.join(directory_join, typee), ""index"")<NewLine><NewLine>        tosave = tensor_index[:-toremove]<NewLine>        ltosave = tosave.cpu()<NewLine>        torch.save(tosave, os.path.join(directory_index, str(xm.get_ordinal())+"".p""))<NewLine>        print(""Saving 1"")<NewLine><NewLine>        tosave = tensor_ids[:-toremove, :].squeeze()<NewLine>        ltosave = tosave.cpu()<NewLine>        torch.save(tosave, os.path.join(directory_ids, str(xm.get_ordinal())+"".p""))<NewLine>        print(""Saving 2"")<NewLine><NewLine>        tosave = tensor_embeddings[:-toremove, :, : ].view(-1,768).detach()<NewLine>        ltosave = tosave.cpu()<NewLine>        print(""Saving 3"")<NewLine>        torch.save(ltosave, os.path.join(directory_embeddings, str(xm.get_ordinal())+"".p""))<NewLine><NewLine><NewLine><NewLine><NewLine><NewLine>def get_embeddings_best_token(root, mask, model, index_best_tokens):<NewLine>    res = model((root, mask))<NewLine>    return res[:,index_best_tokens[0]:index_best_tokens[-1]+1, :]<NewLine><NewLine>def get_ids_best_tokens(x, index_best_tokens):<NewLine>    return x[:,index_best_tokens[0]:index_best_tokens[-1]+1]<NewLine></code></pre><NewLine><p>And I run in parallel with this:</p><NewLine><pre><code class=""lang-auto"">t1 = time.time()<NewLine>get_embeddings(8, model_embed, ""myType"", JOIN_DIR)<NewLine>t2 = time.time()<NewLine></code></pre><NewLine><p>I know that going back to cpu between computations on TPU is a bottleneck, however here I am just trying to save the tensors after the “computations”. I know that a special function exists to save tensors:  <code>torch_xla.core.xla_model.save(data, file_or_path, master_only=True, global_master=False)</code> . However, they explain that basically, it converts tensors from xla tensors to cpu tensors and then saves them. When I use this function, it takes around the same amount of time.</p><NewLine></div>",https://discuss.pytorch.org/u/Syndorik,(Alexandre Allani),Syndorik,"May 18, 2020,  7:42pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, just want to let you know that for all pytorch/xla (or pytorch on tpu) questions, please open an issue in <a href=""https://github.com/pytorch/xla"" rel=""nofollow noopener"">https://github.com/pytorch/xla</a>.<br/><NewLine>Thanks!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks! I will open an issue, and link it here if it appears to be a problem with my code!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ailzhang; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Syndorik; <NewLine> ,"REPLY_DATE 1: May 18, 2020,  8:51pm; <NewLine> REPLY_DATE 2: May 19, 2020, 12:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
81856,While using torch_xla.it shows the following error,2020-05-18T16:18:14.355Z,0,126,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code>518   torch_xla._XLAC._xla_step_marker(<NewLine>519       torch_xla._XLAC._xla_get_default_device(), [],<NewLine></code></pre><NewLine><p>– 520       wait=xu.getenv_as(‘XLA_SYNC_WAIT’, bool, False))<br/><NewLine>521   # Only emit metrics from the first local device index, to avoid emitting the<br/><NewLine>522   # same values from different threads.</p><NewLine><p>RuntimeError: Failed precondition: From /job:tpu_worker/replica:0/task:0:<br/><NewLine>The TPU system has not been initialized.<br/><NewLine>[[{{node XRTCompile}}]]</p><NewLine></div>",https://discuss.pytorch.org/u/Sunil_Sharma1,(Sunil Sharma),Sunil_Sharma1,"May 18, 2020,  7:42pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, for all pytorch/xla (or pytorch on tpu) questions, please open an issue in <a href=""https://github.com/pytorch/xla"" rel=""nofollow noopener"">https://github.com/pytorch/xla</a>.<br/><NewLine>Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ailzhang; <NewLine> ,"REPLY_DATE 1: May 18, 2020,  8:52pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
61771,Pytorch xla Exception: SIGSEGV,2019-11-21T18:28:16.649Z,0,471,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,</p><NewLine><p>I am currently using gcp in order to test the TPU on pytorch.<br/><NewLine>My code comes from <a href=""https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/mnist-training-xrt-1-15.ipynb"" rel=""nofollow noopener"">https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/mnist-training-xrt-1-15.ipynb</a> . I change or remove some part of codes like the TPU_ADDRESS.</p><NewLine><p>During the training, I got an exeption message :</p><NewLine><blockquote><NewLine><p>2019-11-21 18:22:14.125827: I    2364 tensorflow/compiler/xla/xla_client/mesh_service.cc:168] Waiting to connect to client mesh master (300 seconds) localhost:40359<br/><NewLine>2019-11-21 18:22:14.141768: I    2369 tensorflow/compiler/xla/xla_client/mesh_service.cc:168] Waiting to connect to client mesh master (300 seconds) localhost:40359<br/><NewLine>2019-11-21 18:22:14.137212: I    2365 tensorflow/compiler/xla/xla_client/mesh_service.cc:168] Waiting to connect to client mesh master (300 seconds) localhost:40359<br/><NewLine>2019-11-21 18:22:14.159541: I    2376 tensorflow/compiler/xla/xla_client/mesh_service.cc:168] Waiting to connect to client mesh master (300 seconds) localhost:40359<br/><NewLine>2019-11-21 18:22:14.170106: I    2380 tensorflow/compiler/xla/xla_client/mesh_service.cc:168] Waiting to connect to client mesh master (300 seconds) localhost:40359<br/><NewLine>2019-11-21 18:22:14.182444: I    2384 tensorflow/compiler/xla/xla_client/mesh_service.cc:168] Waiting to connect to client mesh master (300 seconds) localhost:40359<br/><NewLine>2019-11-21 18:22:14.191538: I    2392 tensorflow/compiler/xla/xla_client/mesh_service.cc:168] Waiting to connect to client mesh master (300 seconds) localhost:40359<br/><NewLine>Device :  xla:1<br/><NewLine>2019-11-21 18:22:22.612051: I    2364 tensorflow/compiler/xla/xla_client/computation_client.cc:195] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:40359<br/><NewLine>Device :  xla:0<br/><NewLine>2019-11-21 18:22:23.098120: I    2380 tensorflow/compiler/xla/xla_client/computation_client.cc:195] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:40359<br/><NewLine>Device :  xla:0<br/><NewLine>2019-11-21 18:22:23.538515: I    2384 tensorflow/compiler/xla/xla_client/computation_client.cc:195] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:40359<br/><NewLine>2019-11-21 18:22:23.689589: I    2376 tensorflow/compiler/xla/xla_client/computation_client.cc:195] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:40359<br/><NewLine>2019-11-21 18:22:23.776546: I    2392 tensorflow/compiler/xla/xla_client/computation_client.cc:195] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:40359<br/><NewLine>Device :  xla:0<br/><NewLine>Device :  xla:0<br/><NewLine>Device :  xla:0<br/><NewLine>2019-11-21 18:22:24.179791: I    2369 tensorflow/compiler/xla/xla_client/computation_client.cc:195] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:40359<br/><NewLine>2019-11-21 18:22:24.274278: I    2365 tensorflow/compiler/xla/xla_client/computation_client.cc:195] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:40359<br/><NewLine>Device :  xla:0<br/><NewLine>Device :  xla:0<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “test.py”, line 217, in <br/><NewLine>xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS[‘num_cores’],start_method=‘fork’)<br/><NewLine>File “/anaconda3/envs/torch-xla-nightly/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py”, line 173, in spawn<br/><NewLine>start_method=start_method)<br/><NewLine>File “/anaconda3/envs/torch-xla-nightly/lib/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 149, in start_processes<br/><NewLine>while not context.join():<br/><NewLine>File “/anaconda3/envs/torch-xla-nightly/lib/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 107, in join<br/><NewLine>(error_index, name)<br/><NewLine>Exception: process 5 terminated with signal SIGSEGV</p><NewLine></blockquote><NewLine><p>My code is this one :</p><NewLine><pre><code class=""lang-auto""> import collections<NewLine>from datetime import datetime, timedelta<NewLine>import os<NewLine>import requests<NewLine>import threading<NewLine><NewLine>_VersionConfig = collections.namedtuple('_VersionConfig', 'wheels,server')<NewLine>VERSION = ""xrt==1.15.0""  #@param [""xrt==1.15.0"", ""torch_xla==nightly""]<NewLine>CONFIG = {<NewLine>    'xrt==1.15.0': _VersionConfig('1.15', '1.15.0'),<NewLine>    'torch_xla==nightly': _VersionConfig('nightly', 'XRT-dev{}'.format(<NewLine>        (datetime.today() - timedelta(1)).strftime('%Y%m%d'))),<NewLine>}[VERSION]<NewLine>DIST_BUCKET = 'gs://tpu-pytorch/wheels'<NewLine>TORCH_WHEEL = 'torch-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)<NewLine>TORCH_XLA_WHEEL = 'torch_xla-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)<NewLine>TORCHVISION_WHEEL = 'torchvision-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)<NewLine><NewLine># Update TPU XRT version<NewLine>def update_server_xrt():<NewLine>  print('Updating server-side XRT to {} ...'.format(CONFIG.server))<NewLine>  url = 'http://{TPU_ADDRESS}:8475/requestversion/{XRT_VERSION}'.format(<NewLine>      TPU_ADDRESS=os.environ['TPU_IP_ADDRESS'].split(':')[0],<NewLine>      XRT_VERSION=CONFIG.server,<NewLine>  )<NewLine>  print('Done updating server-side XRT: {}'.format(requests.post(url)))<NewLine><NewLine>#update = threading.Thread(target=update_server_xrt)<NewLine>#update.start()<NewLine>#update.join()<NewLine><NewLine># Result Visualization Helper<NewLine>import math<NewLine>from matplotlib import pyplot as plt<NewLine><NewLine>M, N = 4, 6<NewLine>RESULT_IMG_PATH = '/tmp/test_result.png'<NewLine><NewLine>def plot_results(images, labels, preds):<NewLine>  images, labels, preds = images[:M*N], labels[:M*N], preds[:M*N]<NewLine>  inv_norm = transforms.Normalize((-0.1307/0.3081,), (1/0.3081,))<NewLine><NewLine>  num_images = images.shape[0]<NewLine>  fig, axes = plt.subplots(M, N, figsize=(11, 9))<NewLine>  fig.suptitle('Correct / Predicted Labels (Red text for incorrect ones)')<NewLine><NewLine>  for i, ax in enumerate(fig.axes):<NewLine>    ax.axis('off')<NewLine>    if i &gt;= num_images:<NewLine>      continue<NewLine>    img, label, prediction = images[i], labels[i], preds[i]<NewLine>    img = inv_norm(img)<NewLine>    img = img.squeeze() # [1,Y,X] -&gt; [Y,X]<NewLine>    label, prediction = label.item(), prediction.item()<NewLine>    if label == prediction:<NewLine>      ax.set_title(u'\u2713', color='blue', fontsize=22)<NewLine>    else:<NewLine>      ax.set_title(<NewLine>          'X {}/{}'.format(label, prediction), color='red')<NewLine>    ax.imshow(img)<NewLine>  plt.savefig(RESULT_IMG_PATH, transparent=True)<NewLine><NewLine><NewLine>  # Define Parameters<NewLine>FLAGS = {}<NewLine>FLAGS['datadir'] = ""/tmp/mnist""<NewLine>FLAGS['batch_size'] = 128<NewLine>FLAGS['num_workers'] = 4<NewLine>FLAGS['learning_rate'] = 0.01<NewLine>FLAGS['momentum'] = 0.5<NewLine>FLAGS['num_epochs'] = 10<NewLine>FLAGS['num_cores'] = 8<NewLine>FLAGS['log_steps'] = 20<NewLine>FLAGS['metrics_debug'] = False<NewLine><NewLine>import numpy as np<NewLine>import os<NewLine>import time<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine>import torch_xla<NewLine>import torch_xla.core.xla_model as xm<NewLine>import torch_xla.debug.metrics as met<NewLine>import torch_xla.distributed.parallel_loader as pl<NewLine>import torch_xla.distributed.xla_multiprocessing as xmp<NewLine>import torch_xla.utils.utils as xu<NewLine>from torchvision import datasets, transforms<NewLine><NewLine><NewLine>class MNIST(nn.Module):<NewLine><NewLine>  def __init__(self):<NewLine>    super(MNIST, self).__init__()<NewLine>    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)<NewLine>    self.bn1 = nn.BatchNorm2d(10)<NewLine>    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)<NewLine>    self.bn2 = nn.BatchNorm2d(20)<NewLine>    self.fc1 = nn.Linear(320, 50)<NewLine>    self.fc2 = nn.Linear(50, 10)<NewLine><NewLine>  def forward(self, x):<NewLine>    x = F.relu(F.max_pool2d(self.conv1(x), 2))<NewLine>    x = self.bn1(x)<NewLine>    x = F.relu(F.max_pool2d(self.conv2(x), 2))<NewLine>    x = self.bn2(x)<NewLine>    x = torch.flatten(x, 1)<NewLine>    x = F.relu(self.fc1(x))<NewLine>    x = self.fc2(x)<NewLine>    return F.log_softmax(x, dim=1)<NewLine><NewLine><NewLine>def train_mnist():<NewLine>  torch.manual_seed(1)<NewLine>  <NewLine>  # Get and shard dataset into dataloaders<NewLine>  norm = transforms.Normalize((0.1307,), (0.3081,))<NewLine>  train_dataset = datasets.MNIST(<NewLine>      os.path.join(FLAGS['datadir'], str(xm.get_ordinal())),<NewLine>      train=True,<NewLine>      download=True,<NewLine>      transform=transforms.Compose(<NewLine>          [transforms.ToTensor(), norm]))<NewLine>  test_dataset = datasets.MNIST(<NewLine>      os.path.join(FLAGS['datadir'], str(xm.get_ordinal())),<NewLine>      train=False,<NewLine>      download=True,<NewLine>      transform=transforms.Compose(<NewLine>          [transforms.ToTensor(), norm]))<NewLine>  train_sampler = torch.utils.data.distributed.DistributedSampler(<NewLine>    train_dataset,<NewLine>    num_replicas=xm.xrt_world_size(),<NewLine>    rank=xm.get_ordinal(),<NewLine>    shuffle=True)<NewLine>  train_loader = torch.utils.data.DataLoader(<NewLine>      train_dataset,<NewLine>      batch_size=FLAGS['batch_size'],<NewLine>      sampler=train_sampler,<NewLine>      num_workers=FLAGS['num_workers'],<NewLine>      drop_last=True)<NewLine>  test_loader = torch.utils.data.DataLoader(<NewLine>      test_dataset,<NewLine>      batch_size=FLAGS['batch_size'],<NewLine>      shuffle=False,<NewLine>      num_workers=FLAGS['num_workers'],<NewLine>      drop_last=True)<NewLine><NewLine>  # Scale learning rate to world size<NewLine>  lr = FLAGS['learning_rate'] * xm.xrt_world_size()<NewLine><NewLine>  # Get loss function, optimizer, and model<NewLine>  device = xm.xla_device()<NewLine>  print(""Device : "", device)<NewLine>  model = MNIST().to(device)<NewLine>  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=FLAGS['momentum'])<NewLine>  loss_fn = nn.NLLLoss()<NewLine><NewLine>  def train_loop_fn(loader):<NewLine>    tracker = xm.RateTracker()<NewLine>    model.train()<NewLine>    for x, (data, target) in enumerate(loader):<NewLine>      optimizer.zero_grad()<NewLine>      output = model(data)<NewLine>      loss = loss_fn(output, target)<NewLine>      loss.backward()<NewLine>      xm.optimizer_step(optimizer)<NewLine>      tracker.add(FLAGS['batch_size'])<NewLine>      #if x % FLAGS['log_steps'] == 0:<NewLine>       # print('[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(<NewLine>       #     xm.get_ordinal(), x, loss.item(), tracker.rate(),<NewLine>      #      tracker.global_rate(), time.asctime()), flush=True)<NewLine><NewLine>  def test_loop_fn(loader):<NewLine>    total_samples = 0<NewLine>    correct = 0<NewLine>    model.eval()<NewLine>    data, pred, target = None, None, None<NewLine>    for data, target in loader:<NewLine>      output = model(data)<NewLine>      pred = output.max(1, keepdim=True)[1]<NewLine>      correct += pred.eq(target.view_as(pred)).sum().item()<NewLine>      total_samples += data.size()[0]<NewLine><NewLine>    accuracy = 100.0 * correct / total_samples<NewLine>    #print('[xla:{}] Accuracy={:.2f}%'.format(<NewLine>       # xm.get_ordinal(), accuracy), flush=True)<NewLine>    return accuracy, data, pred, target<NewLine><NewLine>  # Train and eval loops<NewLine>  accuracy = 0.0<NewLine>  data, pred, target = None, None, None<NewLine>  for epoch in range(1, FLAGS['num_epochs'] + 1):<NewLine>    begin = time.time()<NewLine>    para_loader = pl.ParallelLoader(train_loader, [device])<NewLine>    train_loop_fn(para_loader.per_device_loader(device))<NewLine>    xm.master_print(""Finished training epoch {0} in {1} sec"".format(epoch, time.time()-begin))<NewLine><NewLine>    #para_loader = pl.ParallelLoader(test_loader, [device])<NewLine>    #accuracy, data, pred, target  = test_loop_fn(para_loader.per_device_loader(device))<NewLine>    #if FLAGS['metrics_debug']:<NewLine>     # xm.master_print(met.metrics_report(), flush=True)<NewLine><NewLine>  return accuracy, data, pred, target<NewLine><NewLine><NewLine># Start training processes<NewLine>def _mp_fn(rank, flags):<NewLine>  global FLAGS<NewLine>  FLAGS = flags<NewLine>  torch.set_default_tensor_type('torch.FloatTensor')<NewLine>  accuracy, data, pred, target = train_mnist()<NewLine>  #if rank == 0:<NewLine>    # Retrieve tensors that are on TPU core 0 and plot.<NewLine>    #plot_results(data.cpu(), pred.cpu(), target.cpu())<NewLine><NewLine>xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'],start_method='fork')<NewLine><NewLine>#train_mnist()<NewLine></code></pre><NewLine><p>Moreover if I use only train_mnist function instead of xmp.spawn, the training works but take around 13 seconds which is slower than in Colab (around 3 seconds) so I suspect than something is wrong.<br/><NewLine>The only difference I have with colab is that :</p><NewLine><ul><NewLine><li>update = threading.Thread(target=update_server_xrt) is commented</li><NewLine><li>put the good TPU_ADDRESS instead of Colab TPU IP</li><NewLine></ul><NewLine><p>I used the custom image with pytorch XLA already installed on GCP. Should I installed something else ? I think there should be no issue with the code as it works on colab,<br/><NewLine>I am using torch XLA nightly.</p><NewLine><p>I thank you for your advices <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/Shiro,(Shiro),Shiro,"May 18, 2020,  7:46pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, sorry for the late reply! Just want to let you know that for all pytorch/xla (or pytorch on tpu) questions, please open an issue in <a href=""https://github.com/pytorch/xla"" rel=""nofollow noopener"">https://github.com/pytorch/xla</a>.<br/><NewLine>Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ailzhang; <NewLine> ,"REPLY_DATE 1: May 18, 2020,  8:49pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
80567,Pytorch TPU printing 1 epoch performance 8 times,2020-05-10T12:12:23.688Z,0,255,"<div class=""post"" itemprop=""articleBody""><NewLine><p>is the code attached below bug free?</p><NewLine><pre><code class=""lang-auto"">def train_model():<NewLine>    global train_dataset, valid_dataset<NewLine>    <NewLine>    torch.manual_seed(42)<NewLine>    <NewLine>    train_sampler = torch.utils.data.distributed.DistributedSampler(<NewLine>        train_dataset,<NewLine>        num_replicas=xm.xrt_world_size(),<NewLine>        rank=xm.get_ordinal(),<NewLine>        shuffle=True)<NewLine>    <NewLine>    train_loader = torch.utils.data.DataLoader(<NewLine>        train_dataset,<NewLine>        batch_size=BATCH_SIZE,<NewLine>        sampler=train_sampler,<NewLine>        num_workers=0,<NewLine>        drop_last=True) # print(len(train_loader))<NewLine>    <NewLine>    '''valid_sampler = torch.utils.data.distributed.DistributedSampler(<NewLine>        train_dataset,<NewLine>        num_replicas=xm.xrt_world_size(),<NewLine>        rank=xm.get_ordinal(),<NewLine>        )'''<NewLine>        <NewLine>    valid_loader = torch.utils.data.DataLoader(<NewLine>        valid_dataset,<NewLine>        batch_size=BATCH_SIZE,<NewLine>        #sampler=valid_sampler,<NewLine>        shuffle=False,<NewLine>        num_workers=0,<NewLine>        drop_last=True)<NewLine>    <NewLine>    #xm.master_print(f""Train for {len(train_loader)} steps per epoch"")<NewLine>    LOGGER.debug(f""Train for {len(train_loader)} steps per epoch"")<NewLine>    # Scale learning rate to num cores<NewLine>    lr  = 0.0001 * xm.xrt_world_size()<NewLine><NewLine>    # Get loss function, optimizer, and model<NewLine>    device = xm.xla_device()<NewLine><NewLine>    #model = model()<NewLine>    '''<NewLine>    for param in model.base_model.parameters(): # freeze some layers<NewLine>        param.requires_grad = False'''<NewLine>    <NewLine>    <NewLine>    global model<NewLine>    <NewLine>    model = model.to(device)<NewLine><NewLine>    criterion = torch.nn.BCEWithLogitsLoss() #  MSELoss<NewLine>    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)<NewLine>    scheduler = OneCycleLR(optimizer, <NewLine>                           lr, <NewLine>                           div_factor=10.0, <NewLine>                           final_div_factor=50.0, <NewLine>                           epochs=NUM_EPOCH,<NewLine>                           steps_per_epoch=len(train_loader))<NewLine>    <NewLine>    <NewLine>    <NewLine>    def train_loop_fn(loader):<NewLine>        tracker = xm.RateTracker()<NewLine>        model.train()<NewLine>        <NewLine>        #xm.master_print('Epoch {}/{}'.format(epoch, num_epochs - 1))<NewLine>        LOGGER.debug('Epoch {}/{}'.format(epoch, num_epochs - 1))<NewLine>        #xm.master_print('-' * 10)<NewLine>        LOGGER.debug('-' * 10)<NewLine>        scheduler.step()<NewLine>        <NewLine>        running_loss = 0.0<NewLine>        tk0 = tqdm(loader, total=int(len(train_loader)))<NewLine>        counter = 0<NewLine>        for bi, d in enumerate(tk0):<NewLine>            inputs = d[""image""]<NewLine>            labels = d[""label""].view(-1, 1)<NewLine>            inputs = inputs.to(device, dtype=torch.float)<NewLine>            labels = labels.to(device, dtype=torch.float)<NewLine>            optimizer.zero_grad()<NewLine>            #with torch.set_grad_enabled(True):<NewLine>            outputs = model(inputs)<NewLine>            loss = criterion(outputs, labels)<NewLine>            #loss = criterion(outputs, torch.max(labels, 1)[1])<NewLine>            loss.backward()<NewLine>            xm.optimizer_step(optimizer)<NewLine>            running_loss += loss.item() * inputs.size(0)<NewLine>            #print(running_loss)<NewLine>            counter += 1<NewLine>            tk0.set_postfix(loss=(running_loss / (counter * BATCH_SIZE)))<NewLine>        epoch_loss = running_loss / len(train_loader)<NewLine>        #xm.master_print('Training Loss: {:.8f}'.format(epoch_loss))<NewLine>        LOGGER.debug('Training Loss: {:.8f}'.format(epoch_loss))<NewLine><NewLine>                <NewLine>    def test_loop_fn(loader):<NewLine>        tk0 = tqdm(loader, total=int(len(valid_loader)))<NewLine>        counter = 0<NewLine>        total_samples, correct = 0, 0<NewLine>        for bi, d in enumerate(tk0):<NewLine>            inputs = d[""image""]<NewLine>            labels = d[""label""].view(-1, 1)<NewLine>            inputs = inputs.to(device, dtype=torch.float)<NewLine>            labels = labels.to(device, dtype=torch.float)<NewLine>            optimizer.zero_grad()<NewLine>            <NewLine>            with torch.no_grad():<NewLine>                <NewLine>                output = model(inputs)<NewLine>                <NewLine>                pred = output.max(1, keepdim=True)[1]<NewLine>                correct += pred.eq(labels.view_as(pred)).sum().item()<NewLine>                total_samples += inputs.size()[0]<NewLine>        accuracy = 100.0 * correct / total_samples<NewLine>        #print('[xla:{}] Accuracy={:.4f}%'.format(xm.get_ordinal(), accuracy), flush=True)<NewLine>        model.train()<NewLine>        return accuracy<NewLine><NewLine>    # Train - valid  loop<NewLine>    accuracy = []<NewLine>    for epoch in range(1, num_epochs + 1):<NewLine>        start = time.time()<NewLine>        para_loader = pl.ParallelLoader(train_loader, [device])<NewLine>        train_loop_fn(para_loader.per_device_loader(device))<NewLine>        <NewLine>        para_loader = pl.ParallelLoader(valid_loader, [device])<NewLine>        accuracy.append(test_loop_fn(para_loader.per_device_loader(device)))<NewLine>        #xm.master_print(""Finished training epoch {}  Val-Acc {:.4f} in {:.4f} sec"".format(epoch, accuracy[-1],   time.time() - start))        <NewLine>        <NewLine>        LOGGER.debug(""Finished training epoch {}  Val-Acc {:.4f} in {:.4f} sec"".format(epoch, accuracy[-1],   time.time() - start))   <NewLine>        valauc = accuracy[-1]<NewLine>        if(epoch&gt;4):<NewLine>            xm.save(model.state_dict(), f""./epoch{epoch}valauc{valauc}.bin"")<NewLine>    return accuracy<NewLine><NewLine>def _mp_fn(rank, flags):<NewLine>    global acc_list<NewLine>    torch.set_default_tensor_type('torch.FloatTensor')<NewLine>    res = train_model()<NewLine><NewLine>FLAGS={}<NewLine>xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')<NewLine></code></pre><NewLine><p>1st epochs train log looks like this :</p><NewLine><p>2020-05-09 12:21:29,371 DEBUG Train for 445 steps per epoch<br/><NewLine>2020-05-09 12:21:29,710 DEBUG Train for 445 steps per epoch<br/><NewLine>2020-05-09 12:21:29,721 DEBUG Train for 445 steps per epoch<br/><NewLine>2020-05-09 12:21:29,911 DEBUG Train for 445 steps per epoch<br/><NewLine>2020-05-09 12:21:30,561 DEBUG Epoch 1/6<br/><NewLine>2020-05-09 12:21:30,564 DEBUG ----------<br/><NewLine>2020-05-09 12:21:31,065 DEBUG Epoch 1/6<br/><NewLine>2020-05-09 12:21:31,076 DEBUG ----------<br/><NewLine>2020-05-09 12:21:31,120 DEBUG Epoch 1/6<br/><NewLine>2020-05-09 12:21:31,130 DEBUG ----------<br/><NewLine>2020-05-09 12:21:31,390 DEBUG Epoch 1/6<br/><NewLine>2020-05-09 12:21:31,426 DEBUG ----------<br/><NewLine>2020-05-09 12:21:32,629 DEBUG Train for 445 steps per epoch<br/><NewLine>2020-05-09 12:21:33,573 DEBUG Train for 445 steps per epoch<br/><NewLine>2020-05-09 12:21:33,748 DEBUG Train for 445 steps per epoch<br/><NewLine>2020-05-09 12:21:33,883 DEBUG Train for 445 steps per epoch<br/><NewLine>2020-05-09 12:21:34,889 DEBUG Epoch 1/6<br/><NewLine>2020-05-09 12:21:34,914 DEBUG ----------<br/><NewLine>2020-05-09 12:21:35,573 DEBUG Epoch 1/6<br/><NewLine>2020-05-09 12:21:35,613 DEBUG ----------<br/><NewLine>2020-05-09 12:21:35,823 DEBUG Epoch 1/6<br/><NewLine>2020-05-09 12:21:35,845 DEBUG ----------<br/><NewLine>2020-05-09 12:21:36,128 DEBUG Epoch 1/6<br/><NewLine>2020-05-09 12:21:36,171 DEBUG ----------<br/><NewLine>2020-05-09 12:35:08,162 DEBUG Training Loss: 11.22450873<br/><NewLine>2020-05-09 12:35:08,172 DEBUG Training Loss: 11.19612112<br/><NewLine>2020-05-09 12:35:08,309 DEBUG Training Loss: 11.18398799<br/><NewLine>2020-05-09 12:35:08,352 DEBUG Training Loss: 11.16665337<br/><NewLine>2020-05-09 12:35:08,362 DEBUG Training Loss: 11.20103131<br/><NewLine>2020-05-09 12:35:08,357 DEBUG Training Loss: 11.19919075<br/><NewLine>2020-05-09 12:35:08,368 DEBUG Training Loss: 11.19310062<br/><NewLine>2020-05-09 12:35:08,386 DEBUG Training Loss: 11.21970569<br/><NewLine>2020-05-09 12:39:31,562 DEBUG Finished training epoch 1  Val-Acc 50.5348 in 1080.4523 sec</p><NewLine><p>the validation accuracy calculation is slow,somehow validation accuracy is using 1 core for calculation where in train phase it is using 8 cores,how do i solve this issue? i need to make the validation calculation fast, also in all epoch i see same validation accuracy, maybe i have bug in my code? another thing is,if i train this model for 8-10 epoch then kaggle kernel doesn’t finish commit,it gives error that’s not visible,so maybe somewhere in my code i am requesting more memory and getting OOM for that? also in my code if i try sampler=valid_sampler for valid_loader then i get error. please help me find bugs in my code,thank you a lot in advance</p><NewLine></div>",https://discuss.pytorch.org/u/mobassir94,(Mobassir),mobassir94,"May 18, 2020,  7:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>FYI this conversation has been moved to <a href=""https://github.com/pytorch/xla/issues/2054"" rel=""nofollow noopener"">https://github.com/pytorch/xla/issues/2054</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ailzhang; <NewLine> ,"REPLY_DATE 1: May 18, 2020,  8:47pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
