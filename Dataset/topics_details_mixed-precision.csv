id,title,created_at,reply_count,views,description,creator_link,creator_name,creator_alias,post_date,post_likes,replies,repliers_links,reply_dates,reply_likes
93965,About the mixed-precision category,2020-08-24T19:56:17.235Z,0,40,"<div class=""post"" itemprop=""articleBody""><NewLine><p>(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)</p><NewLine><p>Use the following paragraphs for a longer description, or to establish category guidelines or rules:</p><NewLine><ul><NewLine><li><NewLine><p>Why should people use this category? What is it for?</p><NewLine></li><NewLine><li><NewLine><p>How exactly is this different than the other categories we already have?</p><NewLine></li><NewLine><li><NewLine><p>What should topics in this category generally contain?</p><NewLine></li><NewLine><li><NewLine><p>Do we need this category? Can we merge with another category, or subcategory?</p><NewLine></li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/albanD,(Alban D),albanD,"August 24, 2020,  7:56pm",,,,,
97003,NAN loss after training several seconds,2020-09-21T12:58:51.152Z,8,58,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m running a code on graph convolutional networks. When i running a simple network, amp works well. But when i change to run a more complex one, its loss become NAN after training several seconds. I didn’t change any other files. How could i fix it?<br/><NewLine>Below are the details</p><NewLine><p>PyTorch: 1.6.0<br/><NewLine>torchvision: 0.7.0<br/><NewLine>cuda : 10.2<br/><NewLine>cudnn: 7.5<br/><NewLine>GPU: 2080ti<br/><NewLine>This is the <a href=""https://drive.google.com/file/d/1IBtlfMM0uGjd6AXAyWDnyr37B9wl6qZL/view?usp=sharing"" rel=""nofollow noopener"">model file</a>.</p><NewLine><blockquote><NewLine><p>Blockquote</p><NewLine></blockquote><NewLine><pre><code class=""lang-auto"">import math<NewLine>import numpy as np<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>from torch.autograd import Variable<NewLine>from torch.cuda.amp import autocast<NewLine><NewLine><NewLine>class GraphAttentionLayer_st(nn.Module):<NewLine><NewLine>    def __init__(self, in_features, out_features, A_type = 1, window_size=3, dilation=1):<NewLine>        super(GraphAttentionLayer_st, self).__init__()<NewLine>        self.in_features = in_features<NewLine>        self.out_features = out_features<NewLine>        self.A_type = A_type<NewLine>        self.window_size = window_size<NewLine><NewLine>        self.inter_channels = out_features//4<NewLine>        self.W1 = nn.Conv2d(in_features, self.inter_channels, kernel_size=1)   # [N, C, T, V] -&gt; [N, C_inter, T, V]<NewLine>        self.W2 = nn.Conv2d(in_features, self.inter_channels, kernel_size=1)   # [N, C, T, V] -&gt; [N, C_inter, T, V]<NewLine>        self.transform = nn.Conv2d(in_features, out_features, kernel_size=1)<NewLine>        self.out_conv = nn.Conv3d(out_features, out_features, kernel_size=(1, self.window_size, dilation))<NewLine>        self.out_bn = nn.BatchNorm2d(out_features)<NewLine><NewLine>    def forward(self, inp):<NewLine>        N, C, T, V = inp.size()<NewLine>        x1 = self.W1(inp).permute(0, 3, 1, 2).contiguous().view(N, V, self.inter_channels * T)<NewLine>        x2 = self.W2(inp).view(N, self.inter_channels*T, 25)   #  [N, C, T, V] -&gt; [N, C_inter*T, V]<NewLine>        attention = torch.matmul(x1, x2)/(self.inter_channels*T) # [N, V, V]<NewLine>        attention = F.softmax(attention, dim=-2) <NewLine>        x = self.transform(inp)       # [N, C_out, T, V]<NewLine>        out = torch.einsum('nctv,nuv-&gt;nctu',x, attention)<NewLine>        out = out.view(N, self.out_features, -1, self.window_size, V//self.window_size)<NewLine>        out = self.out_conv(out).squeeze()<NewLine>        out = self.out_bn(out)<NewLine>        return out<NewLine><NewLine>    def __repr__(self):<NewLine>        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -&gt; ' + str(self.out_features) + ')'<NewLine><NewLine><NewLine>class unit_tcn(nn.Module):<NewLine>    def __init__(self, in_channels, out_channels, kernel_size=9, stride=1):<NewLine>        super(unit_tcn, self).__init__()<NewLine>        pad = int((kernel_size - 1) / 2)<NewLine>        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(pad, 0),<NewLine>                              stride=(stride, 1))<NewLine><NewLine>        self.bn = nn.BatchNorm2d(out_channels)<NewLine>        self.relu = nn.ReLU()  # not used<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.bn(self.conv(x))<NewLine>        return x<NewLine><NewLine><NewLine>class unit_gcn(nn.Module):<NewLine>    def __init__(self, in_channels, out_channels, nheads=0):<NewLine>        super(unit_gcn, self).__init__()<NewLine>        self.nheads=nheads<NewLine>        if nheads&gt;0:<NewLine>            nhid=out_channels//nheads<NewLine>            self.attentions = nn.ModuleList([GraphAttentionLayer_st(in_channels, nhid, A_type=head, window_size=1, dilation=1) for head in range(nheads)])<NewLine>        self.bn = nn.BatchNorm2d(out_channels)<NewLine>        self.relu = nn.ReLU()<NewLine>    def forward(self, x):<NewLine>        N, C, T, V = x.size()<NewLine>        out=0<NewLine>        if self.nheads&gt;0:<NewLine>            out = torch.cat([att(x) for att in self.attentions], dim=1)<NewLine>        out = self.bn(out)<NewLine>        return self.relu(out)<NewLine><NewLine><NewLine>class TCN_GCN_unit(nn.Module):<NewLine>    def __init__(self, in_channels, out_channels, nheads=0, stride=1):<NewLine>        super(TCN_GCN_unit, self).__init__()<NewLine>        self.nheads = nheads<NewLine>        self.tcn1 = unit_tcn(out_channels, out_channels, stride=stride)<NewLine>        self.gcn1 = unit_gcn(in_channels, out_channels, nheads=nheads)<NewLine>        self.relu = nn.ReLU()<NewLine>    def forward(self, x):<NewLine><NewLine>        x = self.tcn1(self.gcn1(x))<NewLine>        return self.relu(x)<NewLine><NewLine><NewLine>class Model(nn.Module):<NewLine>    def __init__(self, num_class=60, num_point=25, num_person=2, graph=None, in_channels=3):<NewLine>        super(Model, self).__init__()<NewLine><NewLine>            <NewLine>        self.data_bn = nn.BatchNorm1d(num_person * in_channels * num_point)<NewLine>        nheads = 4<NewLine>        self.l1 = TCN_GCN_unit(3, 64, nheads=nheads)   <NewLine>        self.l2 = TCN_GCN_unit(64, 128, nheads=nheads,  stride=2)<NewLine>        self.l3 = TCN_GCN_unit(128, 256, nheads=nheads, stride=2)<NewLine>        self.fc = nn.Linear(256, num_class)<NewLine>    @autocast()<NewLine>    def forward(self, x):<NewLine>        N, C, T, V, M = x.size()<NewLine>        <NewLine>        x = x.permute(0, 4, 3, 1, 2).contiguous().view(N, M * V * C, T)   # (M V C) bn<NewLine>        x = self.data_bn(x)<NewLine>        x = x.view(N, M, V, C, T).permute(0, 1, 3, 4, 2).contiguous().view(N * M, C, T, V)<NewLine><NewLine>        x = self.l1(x)<NewLine>        x = self.l2(x)<NewLine>        x = self.l3(x)<NewLine>        <NewLine>        # N*M,C,T,V<NewLine>        c_new = x.size(1)<NewLine>        x = x.view(N, M, c_new, -1)<NewLine>        x = x.mean(3).mean(1)<NewLine>        return self.fc(x)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/hulianyuyy,(连宇 胡),hulianyuyy,"September 21, 2020, 12:58pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I also tried to run this file with APEX. But the loss became NaN soon. I observed the loss scalar factor became 1e-20. But it didn’t make effect to avoid the problem.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you check in the <code>forward</code> method which layer outputs the first Inf or NaN values?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I checked the data flow after you replied. I found it became NaN between the second and the third layer, i.e.between the <code>relu</code> function in <code>self.l2</code> and <code>unit_gcn</code> layer in <code>self.l3</code>. However, it confused me because there is no operation between them. The mode only assigned the output of <code>self.relu</code> in <code>self.l2</code> to <code>x</code>, and then <code>x</code> became NaN. It’s really confusing.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>In addition, i noticed that the PyTorch told me the gradients became NaN several iterations (about 40 with batch size 32) before inputs became NaN. I figure that the abnormal loss causes the weights to become NaN. So the main reason may be amp not working well.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>If the initial loss is too high and the <code>GradScaler</code> uses a scaling value, which is also high, the gradients might overflow, which is expected. The scaler will then skip this update and reduce the scaling factor.<br/><NewLine>However, since the updates are skipped, the forward pass should never return invalid values.</p><NewLine><p>If I understand it correctly, you are seeing the NaN values here:</p><NewLine><pre><code class=""lang-python"">class TCN_GCN_unit(nn.Module):<NewLine>    def __init__(self, in_channels, out_channels, nheads=0, stride=1):<NewLine>        super(TCN_GCN_unit, self).__init__()<NewLine>        self.nheads = nheads<NewLine>        self.tcn1 = unit_tcn(out_channels, out_channels, stride=stride)<NewLine>        self.gcn1 = unit_gcn(in_channels, out_channels, nheads=nheads)<NewLine>        self.relu = nn.ReLU()<NewLine>    def forward(self, x):<NewLine><NewLine>        x = self.tcn1(self.gcn1(x))<NewLine>        # x is valid<NewLine>        return self.relu(x) # returns NaNs?<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">class TCN_GCN_unit(nn.Module):<NewLine>    def __init__(self, in_channels, out_channels, nheads=0, stride=1):<NewLine>        super(TCN_GCN_unit, self).__init__()<NewLine>        self.nheads = nheads<NewLine>        self.tcn1 = unit_tcn(out_channels, out_channels, stride=stride)<NewLine>        self.gcn1 = unit_gcn(in_channels, out_channels, nheads=nheads)<NewLine>        self.relu = nn.ReLU()<NewLine>    def forward(self, x):<NewLine><NewLine>        x = self.tcn1(self.gcn1(x))<NewLine>        x = self.relu(x)<NewLine>        # I didn't observe NaN here.<NewLine>        return  x<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">x = self.l2(x)<NewLine># Actually i found NaN here. <NewLine>x = self.l3(x)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>i made a little modification in the code. I change the relu function as a single operation <code>x = self.relu(x)</code>, and then <code>return x</code>. In this way i can observe the input change accurately. But it behaves like above.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>So you don’t see the NaN inside the module (i.e. after the relu), but outside of it for the “same” tensor. Is my understanding correct?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, your understanding is correct</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hulianyuyy; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/hulianyuyy; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/hulianyuyy; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/hulianyuyy; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/hulianyuyy; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/hulianyuyy; <NewLine> ,"REPLY_DATE 1: September 23, 2020,  3:58am; <NewLine> REPLY_DATE 2: September 24, 2020,  8:33am; <NewLine> REPLY_DATE 3: September 25, 2020,  7:27am; <NewLine> REPLY_DATE 4: September 25, 2020,  8:23am; <NewLine> REPLY_DATE 5: September 25, 2020,  8:26am; <NewLine> REPLY_DATE 6: September 25, 2020,  9:25am; <NewLine> REPLY_DATE 7: September 25, 2020,  9:28am; <NewLine> REPLY_DATE 8: September 25, 2020,  4:58pm; <NewLine> REPLY_DATE 9: September 26, 2020,  1:48am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> 
97190,Disabling mixed precsion in my own layers,2020-09-23T02:43:04.302Z,0,27,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, after reading the docs about mixed precsion, <a href=""https://pytorch.org/docs/stable/notes/amp_examples.html"" rel=""nofollow noopener"">amp_example</a><br/><NewLine>I’m still confused with several problems.</p><NewLine><p>Let’s say if I have two networks, one is the standard resnet50 and another is a sparse conv layer.<br/><NewLine>input images are first passed through resnet50 and then sparse convs.</p><NewLine><p>If I only want to use half for resnet and keep float32 for the sparse conv layer (so I don’t have to modify the code)<br/><NewLine>I only need to warp the model within the autocast function and disable it before the sparse conv layers?<br/><NewLine>like,</p><NewLine><pre><code class=""lang-auto"">with autocast():<NewLine>      out = resnet50(x)<NewLine>      with autocast(enabled=False):<NewLine>             out = sparseconv(out.float())<NewLine></code></pre><NewLine><p>right?</p><NewLine><p>And from my knowledge, gradients are scaled during mixed precision,<br/><NewLine>If I have to write my own backward function for sparse conv layers (warped in autocast(disabled)),<br/><NewLine>Do I still need to consider the scale?</p><NewLine></div>",https://discuss.pytorch.org/u/tree33,,tree33,"September 23, 2020,  2:46am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Your <code>autocast</code> example looks correct.</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""97190"" data-username=""tree33""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tree33/40/10609_2.png"" width=""20""/> tree33:</div><NewLine><blockquote><NewLine><p>And from my knowledge, gradients are scaled during mixed precision</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, if you are using a <code>GradScaler</code> object.</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""97190"" data-username=""tree33""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tree33/40/10609_2.png"" width=""20""/> tree33:</div><NewLine><blockquote><NewLine><p>If I have to write my own backward function for sparse conv layers (warped in autocast(disabled)),<br/><NewLine>Do I still need to consider the scale?</p><NewLine></blockquote><NewLine></aside><NewLine><p>No, gradient scaling and auto casting are working together but are independent from each other.<br/><NewLine>You don’t need to explicitly implement gradient scaling for the custom layer.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 24, 2020,  7:55am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
97338,Loading saved Amp models into ensemble,2020-09-24T07:27:15.501Z,1,22,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have trained several models with amp in FP16 separately and saved all state dicts for both models and amp.</p><NewLine><p>Now as a continuation of that training, I would like to load those models into an ensemble, freeze all gradients and train a new model with a few final layers that learns based on the output from those models and several different inputs.</p><NewLine><p>My current code looks something like this</p><NewLine><pre><code class=""lang-auto"">modelA = ModelA()<NewLine>modelB = ModelB()<NewLine><NewLine>modelA.load_state_dict(checkpointA['model'])<NewLine>modelB.load_state_dict(checkpointB['model'])<NewLine><NewLine>for a in modelA.parameters():<NewLine>    a.requires_grad = False<NewLine>for b in modelB.parameters():<NewLine>    b.requires_grad = False<NewLine><NewLine>ensemble = EnsembleModel(modelA, modelB)<NewLine>optimizer = FusedAdam(filter(lambda p: p.requires_grad, ensemble .parameters()), lr=learning_rate)<NewLine><NewLine>ensemble , optimizer = amp.initialize(ensemble , optimizer, opt_level)<NewLine><NewLine>***perform training***<NewLine></code></pre><NewLine><p>Now to my questions:</p><NewLine><p>In the checkpoints for modelA and modelB I also saved the amp state dict. Should those be loaded in this case and how?<br/><NewLine>Does amp.initialize support models embedded inside an ensemble?<br/><NewLine>What if I want to unfreeze the lower models at a later point, do I need to reinitialize both the optimizer and amp in that case?</p><NewLine></div>",https://discuss.pytorch.org/u/Eiphodos,(David Hagerman Olzon),Eiphodos,"September 24, 2020,  7:43am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""97338"" data-username=""Eiphodos""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/e/dbc845/40.png"" width=""20""/> Eiphodos:</div><NewLine><blockquote><NewLine><p>In the checkpoints for modelA and modelB I also saved the amp state dict. Should those be loaded in this case and how?</p><NewLine></blockquote><NewLine></aside><NewLine><p>This wouldn’t be necessary, if you are not planning to finetune the pretrained models.</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""97338"" data-username=""Eiphodos""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/e/dbc845/40.png"" width=""20""/> Eiphodos:</div><NewLine><blockquote><NewLine><p>Does amp.initialize support models embedded inside an ensemble?</p><NewLine></blockquote><NewLine></aside><NewLine><p>We recommend to use the native amp implementation via <a href=""https://pytorch.org/docs/stable/amp.html""><code>torch.cuda.amp</code></a> instead of apex/amp. Since there is no <code>amp.initialize</code> method, it should just work, but let us know if you encounter any issues.</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""97338"" data-username=""Eiphodos""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/e/dbc845/40.png"" width=""20""/> Eiphodos:</div><NewLine><blockquote><NewLine><p>What if I want to unfreeze the lower models at a later point, do I need to reinitialize both the optimizer and amp in that case?</p><NewLine></blockquote><NewLine></aside><NewLine><p>The amp <code>state_dict</code> stores the loss scaler values etc. which is useful to continue the training with the same setup. If you initialize a new <code>scaler</code>, the first iterations might be skipped, if the scaling factor is too high, which wouldn’t necessarily be the case using the scaling factor from the last iteration in the pretraining.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see, thanks for the help!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Eiphodos; <NewLine> ,"REPLY_DATE 1: September 24, 2020,  7:53am; <NewLine> REPLY_DATE 2: September 24, 2020,  7:53am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
97224,Alternative to torch.inverse for 16 bit,2020-09-23T08:02:59.756Z,0,20,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><p>torch.inverse() doesn’t work with half precision.</p><NewLine><p>Is there an alternative way we can compute the inverse which could work? I known maybe it’s impossible with stability issues?</p><NewLine></div>",https://discuss.pytorch.org/u/Bruce_Muller,,Bruce_Muller,"September 23, 2020,  8:02am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not aware of any backends implementing half-precision inverse off the shelve. Part of that might be stability.</p><NewLine><p>Then a practical question would be what your use case is and what you hope to get from it. Is it for a certain problem shape? What is it that makes you prefer half over single precision? What do you need the inverse for?</p><NewLine><p>Anecdotally, way back when I studied numerical linear algebra and analysis at the university, they used to say that when you explicitly compute the inverse, you’re doing it wrong. Now that may not be the case here for you, but I must admit it seems very special-purpose to use the explicit inverse while needing low-precision.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: September 23, 2020,  8:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
97067,RuntimeError: Input type (torch.cuda.HalfTensor) and weight type (torch.cuda.FloatTensor) should be the same,2020-09-22T03:05:59.057Z,2,39,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I use torch.cuda.amp.autocast before model forward, But I get a error like topics.<br/><NewLine>The code is :</p><NewLine><pre><code class=""lang-auto"">class SuperConv2d(nn.Conv2d):<NewLine>    def __init__(self, in_channels, out_channels, kernel_size, stride=1,<NewLine>                 padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'):<NewLine>        super(SuperConv2d, self).__init__(in_channels, out_channels, kernel_size,<NewLine>                                          stride, padding, dilation, groups, bias, padding_mode)<NewLine><NewLine>    def forward(self, x, config):<NewLine>        in_nc = x.size(1)<NewLine>        out_nc = config['channel']<NewLine>        weight = self.weight[:out_nc, :in_nc]  # [oc, ic, H, W]<NewLine>        if self.bias is not None:<NewLine>            bias = self.bias[:out_nc]<NewLine>        else:<NewLine>            bias = None<NewLine>        return F.conv2d(x, weight, bias, self.stride, self.padding, self.dilation, self.groups)<NewLine></code></pre><NewLine><p>What can I change to use amp for this module?<br/><NewLine>Is it F.conv2d support amp?</p><NewLine><p>My environment is:</p><NewLine><blockquote><NewLine><p>gpu:  rtx2080ti<br/><NewLine>torch:  py3.7_cuda10.1.243_cudnn7.6.3_0<br/><NewLine>cuda: 10.1</p><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/Feywell,(Feywell),Feywell,"September 22, 2020,  3:05am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The code seems to work for me using your custom module:</p><NewLine><pre><code class=""lang-python"">conv = SuperConv2d(10, 10, 3, 1, 1).cuda()<NewLine>x = torch.randn(10, 10, 24, 24).cuda()<NewLine><NewLine>config = {'channel': 2}<NewLine><NewLine>with torch.cuda.amp.autocast():<NewLine>    out = conv(x, config)<NewLine><NewLine>print(out.dtype)<NewLine>&gt; torch.float16<NewLine></code></pre><NewLine><p>Could you check, what might be the difference in my code snippet?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Maybe, The difference is my input is float16, but it is float32 in your case.<br/><NewLine>Because, I use the module inside the network. For me, the input of this module is float16.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Doesn’t seem to be the issue, as the input is not also <code>float16</code>:</p><NewLine><pre><code class=""lang-python"">plain_conv = nn.Conv2d(10, 10, 3, 1, 1).cuda()<NewLine>conv = SuperConv2d(10, 10, 3, 1, 1).cuda()<NewLine>x = torch.randn(10, 10, 24, 24).cuda()<NewLine><NewLine>config = {'channel': 2}<NewLine><NewLine>with torch.cuda.amp.autocast():<NewLine>    out = plain_conv(x)<NewLine>    print(out.dtype)<NewLine>    out = conv(out, config)<NewLine><NewLine>print(out.dtype)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Feywell; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 22, 2020,  5:05am; <NewLine> REPLY_DATE 2: September 22, 2020,  6:13am; <NewLine> REPLY_DATE 3: September 22, 2020,  6:15am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
96564,Mixed precision and Spectral norm,2020-09-17T00:42:27.940Z,3,56,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I started playing around with new Amp interface. The thing is: I am training GANs and my models use spectral norm.</p><NewLine><p>I would like to know how things work with mixed precision when using spectral norm. Are my spectraly normalized weights eligible for fp16 precision? Do I need to do something extra to get things working (such as increasing spectral norm eps?)</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/dougsouza,(Douglas Souza),dougsouza,"September 17, 2020, 12:42am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>AMP will autocast operators as described in <a href=""https://pytorch.org/docs/stable/amp.html#ops-that-can-autocast-to-float16"">this list</a>. Since <code>spectral_norm</code> uses some of them (e.g. <code>matmul</code>) this operation would be performed in FP16. If you want to keep the calculation in FP32, you can disable <code>autocast</code> for this call.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>, thank for you answer. I have some questions about AMP, maybe eu can clarify for me.</p><NewLine><p>1 - About weight initilization: let’s say my initializer produces very small weights, will them flush to zero when run inside autocast?<br/><NewLine>2 - What about model inputs: if I am working with vector of very small floats as inputs to my model, what happens to them?</p><NewLine><p>Thanks</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>No, that shouldn’t be the case as the compute and accumulate <code>dtype</code> would still be performed in FP32 even if FP16 inputs are passed.</li><NewLine><li>If depends on the range of your values and if they are representable. Theoretically it would be possible, e.g. via:</li><NewLine></ol><NewLine><pre><code class=""lang-python"">x = torch.tensor(2**(-149)).cuda()<NewLine>print(x)<NewLine>&gt; tensor(1.4013e-45, device='cuda:0')<NewLine>print(x.half())<NewLine>&gt; tensor(0., device='cuda:0', dtype=torch.float16)<NewLine></code></pre><NewLine><p>but then I would doubt that the small FP32 values would contribute to your model in any way.<br/><NewLine>Note that a <code>GradScaler</code> should be used for mixed-precision training together with <code>autocast</code> to avoid gradient underflow.</p><NewLine><p>It would be interesting to know more about your use case, i.e. what kind of model “needs” values close to zero to perform properly.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>, there is no requirements to have inputs close to zero. I just wanted to know how AMP handled these cases.</p><NewLine><p>Thanks for your help!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dougsouza; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/dougsouza; <NewLine> ,"REPLY_DATE 1: September 18, 2020,  6:01am; <NewLine> REPLY_DATE 2: September 18, 2020,  2:03pm; <NewLine> REPLY_DATE 3: September 21, 2020,  2:13pm; <NewLine> REPLY_DATE 4: September 21, 2020,  2:13pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
96588,Does amp training can use torch.nn.DataParallel at the sametime？,2020-09-17T07:21:55.863Z,5,33,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I try to use amp with pytorch1.6 to speed up my training code.<br/><NewLine>But I have a problem ,when I use nn.DataParallel.</p><NewLine><p>I print some Intermediate variable.<br/><NewLine>I find the tensor is float16 in one gpu, but float32  in two gpus.<br/><NewLine>Is it support DataParallel model to use mixed-precision training?</p><NewLine><p>in one gpu:</p><NewLine><blockquote><NewLine><p>fake_image_orig:  torch.float16<br/><NewLine>gen loss:  torch.float32<br/><NewLine>discriminator_out dtype:  torch.float16<br/><NewLine>pred_fake:  torch.float16<br/><NewLine>amp discriminor<br/><NewLine>discriminator_out dtype:  torch.float16<br/><NewLine>self.get_zero_tensor(input) dtype:  torch.float16<br/><NewLine>input dtype:  torch.float16<br/><NewLine>self.get_zero_tensor(input) dtype:  torch.float16</p><NewLine></blockquote><NewLine><p>two gpus:</p><NewLine><blockquote><NewLine><p>discriminator_out dtype:  torch.float32<br/><NewLine>self.get_zero_tensor(input) dtype:  torch.float32<br/><NewLine>input dtype:  torch.float32<br/><NewLine>self.get_zero_tensor(input) dtype:  torch.float32<br/><NewLine>input dtype:  torch.float32<br/><NewLine>discriminator_out dtype:  torch.float32<br/><NewLine>self.get_zero_tensor(input) dtype:  torch.float32<br/><NewLine>input dtype:  torch.float32<br/><NewLine>self.get_zero_tensor(input) dtype:  torch.float32<br/><NewLine>input dtype:  torch.float32<br/><NewLine>fake_image_orig:  torch.float32<br/><NewLine>gen loss:  torch.float32<br/><NewLine>discriminator_out dtype:  torch.float32<br/><NewLine>pred_fake:  torch.float32<br/><NewLine>fake_image_orig:  torch.float32<br/><NewLine>gen loss:  torch.float32<br/><NewLine>discriminator_out dtype:  torch.float32<br/><NewLine>pred_fake:  torch.float32<br/><NewLine>fake_image_orig:  torch.float32<br/><NewLine>gen loss:  torch.float32<br/><NewLine>fake_image_orig:  torch.float32<br/><NewLine>gen loss:  torch.float32<br/><NewLine>discriminator_out dtype:  torch.float32</p><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/Feywell,(Feywell),Feywell,"September 17, 2020,  7:21am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, amp is usable in <code>nn.DataParallel</code> as described <a href=""https://pytorch.org/docs/stable/notes/amp_examples.html#dataparallel-in-a-single-process"">here</a>.<br/><NewLine>I guess you might have missed this note about the <code>@autocast()</code> decorator for the <code>forward</code> method, if you are using <code>nn.DataParallel</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>In cases where multi-GPU can be set by command line arguments, and the script thus has to work in single as well as multi-GPU scenarios, can we just put the autocast in forward for single GPUs as well?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>In face, I try to experiment  in another machine with pytorch-nightly.<br/><NewLine>I find it is work!<br/><NewLine>It is weird.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, that wouldn’t be necessary, but shouldn’t hurt either.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just to be clear: it is necessary in the DP cases, but not so in single GPU scenarios?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, that is correct.<br/><NewLine>The <code>forward</code> method has to be annotated for <code>nn.DataParallel</code> and DDP - multiple GPUs per process, not for single GPU or DDP - single GPU per process (recommended and fastest approach) use cases.<br/><NewLine>You should be able to do it anyway without any disadvantages, but let us know, if it’s breaking.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>It means I need use amo.autocast in eyery single forward for nn.DataParallel and DDP in multiGPUs</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/BramVanroy; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Feywell; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/BramVanroy; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Feywell; <NewLine> ,"REPLY_DATE 1: September 17, 2020,  9:17am; <NewLine> REPLY_DATE 2: September 17, 2020,  9:22am; <NewLine> REPLY_DATE 3: September 17, 2020,  9:25am; <NewLine> REPLY_DATE 4: September 17, 2020,  9:24am; <NewLine> REPLY_DATE 5: September 17, 2020,  9:34am; <NewLine> REPLY_DATE 6: September 17, 2020,  9:46am; <NewLine> REPLY_DATE 7: September 17, 2020, 10:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> 
95560,Fp16 training with feedforward network slower time and no memory reduction,2020-09-08T14:11:32.090Z,8,61,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I’m doing mixed-precision training (from the native amp in pytorch 1.6) on feedforward neural networks. Both the training time and memory consumed have increased as a result.</p><NewLine><p>The GPU is RTX 2080Ti. I tried to have all of the dimensions in multiples of 8 as well.</p><NewLine><p>The training time is less important to me, I mainly want to decrease the memory footprint as much as possible since I’m using large feedforward neural networks only.</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Najeeb_Nabwani,(Najeeb Nabwani),Najeeb_Nabwani,"September 8, 2020,  2:11pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well if ur memory consumption is so high that it irritates u then I suggest u downsample ur data with convolutional layers (if the problem is not a regression model problem) and try to update most of ur code variables inplace</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not really applicable in my situation. But I’m wondering more about why fp16 isn’t reducing my memory at all…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The good effects of half precision floating points may just be negligible in this case<br/><NewLine>U said u were using mixed precision right? Then just making it a single precision might give u what u want I guess</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Currently single precision is indeed faster. But I need fp16 primarily to reduce my memory footprint for when I want to run bigger networks.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>The fp16 should still decrease ur memory foot print even if it’s by a small factor.<br/><NewLine>It’s possible that the decrease is so small its negligible.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Unfortunately that’s exactly the problem, even on a model that takes 9GB, there is a memory <em>increase</em> when using fp16. I even tried pytorch lightning ‘precision=16’ (which uses native amp) and still no decrease. I’m guessing it’s related to the fact that I’m strictly using feedforward networks but I’m not entirely sure why.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>It might be the feed forward networks though I’m not entirely sure<br/><NewLine>Well what I’ll suggest is if u have the time try creating a conv network and in one Instance use single precision and in another use mix precision and check the difference in memory usage</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you seeing a speedup or slowdown?<br/><NewLine>I’m a bit confused, since you’ve mentioned both.</p><NewLine><p>Also, how are you measuring the memory usage?<br/><NewLine>Note that <code>nvidia-smi</code> shows the total allocated memory (CUDA context + cache + allocated memory), so you should use <code>torch.cuda.memory_allocated()</code> instead.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems that it doesn’t work well with feed forward. I don’t see a speedup nor reduced memory. However, when using other networks such as conv networks (as suggested) then there is indeed a significant speedup and memory reduction.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>torch.cuda.amp.autocast works in this way:</p><NewLine><ol><NewLine><li>cast the layer into fp16 if the corresponding operation is fp16-safe.<br/><NewLine><a href=""https://pytorch.org/docs/stable/amp.html#autocast-op-reference"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/amp.html#autocast-op-reference</a><br/><NewLine>For example, batch normalization will stay in fp32.</li><NewLine><li>run in the configured precision levels<br/><NewLine>If the layer input does not match the target precision, it will convert the type automatically.</li><NewLine></ol><NewLine><p>Using mixed-precision could sometimes be slow due to the type-casting operation.<br/><NewLine>At inference time, using pure half-precision is faster than using amp.</p><NewLine><p>But I’m not sure why your memory consumption is higher.<br/><NewLine>I always see less memory consumption with mixed-precision training on RTX 2080 Ti.<br/><NewLine>Maybe the above step 2 needs features in both fp32 and fp16 for precision switching layers.</p><NewLine><p>You may want to refer to this <a href=""https://arxiv.org/pdf/1710.03740.pdf"" rel=""nofollow noopener"">paper</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Henry_Chibueze; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Najeeb_Nabwani; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Henry_Chibueze; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Najeeb_Nabwani; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Henry_Chibueze; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Najeeb_Nabwani; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Henry_Chibueze; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Najeeb_Nabwani; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/seungjun; <NewLine> ,"REPLY_DATE 1: September 8, 2020,  2:23pm; <NewLine> REPLY_DATE 2: September 8, 2020,  2:42pm; <NewLine> REPLY_DATE 3: September 8, 2020,  2:49pm; <NewLine> REPLY_DATE 4: September 8, 2020,  2:53pm; <NewLine> REPLY_DATE 5: September 8, 2020,  3:06pm; <NewLine> REPLY_DATE 6: September 8, 2020,  3:09pm; <NewLine> REPLY_DATE 7: September 8, 2020,  3:28pm; <NewLine> REPLY_DATE 8: September 11, 2020,  6:26am; <NewLine> REPLY_DATE 9: September 15, 2020, 11:14am; <NewLine> REPLY_DATE 10: September 15, 2020, 11:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> 
95113,Convert_syncbn_model causes gradient overflow with apex mixed precision,2020-09-03T21:43:07.455Z,7,101,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I was trying to train my network using apex mixed precision. I’ve tried DenseNet and Resnet as backbones for a segmentation task using CityScapes. Unfortunately, when I try to synchronize the batch norm using <code>convert_syncbn_model</code>, the <code>scale_loss</code> ends up being zero after a few iterations because of gradient overflow. This does not happen if I remove the batch normalization.<br/><NewLine>The snipet of my code is the following:</p><NewLine><pre><code class=""lang-auto"">model.cuda(gpu)<NewLine>model = apex.parallel.convert_syncbn_model(model)<NewLine>optimizer = optim.Adam(model.parameters())<NewLine>model, optimizer = apex.amp.initialize(model, optimizer)<NewLine>#model = apex.parallel.convert_syncbn_model(model) #I also tried to put it here<NewLine>net = DDP(model, delay_allreduce=True)<NewLine></code></pre><NewLine><p>…</p><NewLine><pre><code class=""lang-auto"">loss = Cross_entropy(y_pred, y_gt)<NewLine>with apex.amp.scale_loss(loss, optimizer) as scaled_loss:<NewLine>                    scaled_loss.backward()<NewLine>optimizer.step()<NewLine>optimizer.zero_grad()<NewLine></code></pre><NewLine><p>System:<br/><NewLine>OS: Ubuntu 16.04 and 18.04<br/><NewLine>Pytorch: tried with 1.4, 1.5 and .1.6<br/><NewLine>apex: 0.1</p><NewLine><p>Did anyone experience the same?<br/><NewLine>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/hanzCV,(Hanz Cuevas Velásquez),hanzCV,"September 3, 2020,  9:43pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Mixed precision training utilities as well as synchronized batchnorm layers are now available in PyTorch directly, so you don’t need <code>apex</code> anymore.<br/><NewLine>We recommend to use these native implementations now. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/><br/><NewLine>Could you try them and see, if you encounter any issues?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve replaced the code with the following cuda.amp code:</p><NewLine><pre><code class=""lang-auto"">scaler = torch.cuda.amp.GradScaler()<NewLine><NewLine>for epoch in range(n_epochs):<NewLine>    for i, (X_batch, y_gt) in enumerate(batches):<NewLine>        X_batch = X_batch.cuda()<NewLine>        y_gt = y_gt.cuda()<NewLine>        optimizer.zero_grad()<NewLine>        with torch.cuda.amp.autocast():<NewLine>            y_pred = model(X_batch)<NewLine>            loss = Cross_entropy(y_pred, y_gt)<NewLine><NewLine>        scaler.scale(loss).backward()<NewLine>        scaler.step(optimizer)<NewLine>        scaler.update()<NewLine><NewLine></code></pre><NewLine><p>It is training stably so far but it is really slow. I’m using Densenet121 backbone and some convolutions to output 19 segmentation classes with a batch size of 2 with 2 GPUs GTX1080Ti. The input size is 512x512. When I don’t use cuda.amp the training speed per iteration is ~2 seconds involving forward and backward passes. However, if I use cuda.amp, the speed per iteration is ~6.6 seconds.</p><NewLine><p>System:<br/><NewLine>OS: 18.04<br/><NewLine>Pytorch: 1.6 (1.6.0+cu101)<br/><NewLine>Cuda: 10.1 (cannot get cuda 10.2 because I don’t have superuser access)<br/><NewLine>GPU: 2 GTX 1080Ti</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Your 1080Ti GPUs do not have TensorCores, so that you shouldn’t expect a speedup from the computations. However, how did you measure the performance? Did you synchronize the code properly using <code>torch.cuda.synchronize()</code> before starting and stopping the timers?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I measure the performance following the post bellow. Instead of <code>while</code> I have the forward, backward, optimizer step and zeroing the parameter gradients, then I print the time each iteration takes.</p><NewLine><p>I can confirm that the training did not face any problems yet, apart from the slow training <img alt="":confused:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=9"" title="":confused:""/>.<br/><NewLine>Do you think the cuda version might be causing this? Is there any verbose I can enable for debugging, or if it can print something like apex when there is gradient overflow and is adjusting the scale loss?</p><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""20137"" data-username=""ptrblck""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ptrblck/40/1823_2.png"" width=""20""/><a href=""https://discuss.pytorch.org/t/is-there-any-difference-between-x-to-cuda-vs-x-cuda-which-one-should-i-use/20137/5"">Is there any difference between x.to('cuda') vs x.cuda()? Which one should I use?</a><NewLine></div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">torch.cuda.synchronize()<NewLine>t1 = time.time()<NewLine>while i&lt; 500:<NewLine>    a += 1<NewLine>    a -= 1<NewLine>    i+=1<NewLine>torch.cuda.synchronize()<NewLine>t2 = time.time()<NewLine>print('cuda string', t2-t1)<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>Thanks for the help <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/>.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the information.<br/><NewLine>I assume the slowdown can be observed on a single device and would thus be unrelated to <code>SyncBatchnorm</code>.<br/><NewLine>Could you install the latest stable PyTorch release (or the nightly binaries) with CUDA10.2 and cudnn7.6.5.32?<br/><NewLine>Note that the binaries ship with their own CUDA and cudnn rumtimes. Your local installations won’t be used so you don’t need to update the local CUDA version.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks,<br/><NewLine>I have downloaded the latest PyTorch with CUDA 10.2 using (pip install torch torchvision). To reduce possible errors, I tested it in a machine with CUDA 10.2 as well. Unfortunately, the training time remains slow when using <code>torch.cuda.amp</code>.</p><NewLine><p>System:<br/><NewLine>Ubuntu 18.04<br/><NewLine>Cuda: 10.2<br/><NewLine>Pytorch: 1.6<br/><NewLine>GPU: 2 GTX 1080Ti</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the check.<br/><NewLine>Are you seeing the slowdown using the <code>torchvision.models.densenet121</code> or are you changing it somehow?<br/><NewLine>I would like to reproduce it with cudnn7 as well as cudnn8 and check, why amp is slowing down the model.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I modified the original PyTorch densenet121 to extract features at different scales. <a href=""https://drive.google.com/file/d/1ZCGB2_lytxRyel2cO1EjZdxmzMJZvtDm/view?usp=sharing"" rel=""nofollow noopener"">Here</a> is the modified densenet; it still can load the weights provided by PyTorch. I cannot send you our exact decoder, but I guess any SegNet-like architecture for pixel-wise segmentation should show the same behavior.</p><NewLine><p>To check if the error was caused by my code, I used this <a href=""https://github.com/nyoki-mtl/pytorch-segmentation"" rel=""nofollow noopener"">repo</a>, which is a deeplab v3 implementation with pretrained weights for cityscapes. I tested <code>torch.cuda.amp</code>, <code>O1</code> from <code>apex.amp</code>, and no mixed precision (the last one was tested with half of the size of the input image used by the mixed precision tests).</p><NewLine><p>I’ve run some epochs and found that <code>torch.cuda.amp</code> is twice as slow as <code>apex.amp</code>, and no mixed precision. In numbers, 1 iteration (forward and backward) using <code>torch.cuda.amp</code> takes ~2.50 seconds, the other two take around ~1.20 seconds.</p><NewLine><p>Configuration:<br/><NewLine>batch size: 2 per GPU<br/><NewLine>Image size: 512x512 (mixed precision), 256x512 (no mixed precision)<br/><NewLine>Batch sync: Yes</p><NewLine><p>Dataset: Cityscapes semantic segmentation<br/><NewLine>Code: <a href=""https://github.com/nyoki-mtl/pytorch-segmentation"" rel=""nofollow noopener"">https://github.com/nyoki-mtl/pytorch-segmentation</a><br/><NewLine>optimizer: adam<br/><NewLine>loss: cross entropy</p><NewLine><p>System:<br/><NewLine>Ubuntu 18.04<br/><NewLine>Cuda: 10.2<br/><NewLine>CudNN: 8.0.3<br/><NewLine>Pytorch: 1.6<br/><NewLine>GPU: 2 GTX 1080Ti</p><NewLine><p>If you need more details please let me know. Thanks for the help <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/>.</p><NewLine><hr/><NewLine><p>p.s.<br/><NewLine>Just to update more information about Apex. I’m still getting gradient overflow and <code>scale_loss</code> being reduce to zero when using <code>apex.parallel.convert_syncbn_model()</code> after few epochs.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hanzCV; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/hanzCV; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/hanzCV; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/hanzCV; <NewLine> ,"REPLY_DATE 1: September 5, 2020,  8:00am; <NewLine> REPLY_DATE 2: September 5, 2020,  8:22pm; <NewLine> REPLY_DATE 3: September 7, 2020,  2:53am; <NewLine> REPLY_DATE 4: September 8, 2020,  6:57am; <NewLine> REPLY_DATE 5: September 9, 2020,  1:09am; <NewLine> REPLY_DATE 6: September 9, 2020,  3:30pm; <NewLine> REPLY_DATE 7: September 10, 2020,  4:22am; <NewLine> REPLY_DATE 8: September 15, 2020,  9:53am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> 
95827,What is the correct way of computing a grad penalty using AMP?,2020-09-10T17:53:56.076Z,1,61,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m converting my distributed code to work with PyTorch’s AMP, but I have confusion on how I should compute my grad penalty as part of my WGAN-GP loss. I have referred to the <a href=""https://pytorch.org/docs/stable/notes/amp_examples.html#gradient-penalty"" rel=""nofollow noopener"">documentation</a> on this matter but I believe my use case is slightly different.</p><NewLine><p>Here’s my code on computing the grad penalty:</p><NewLine><pre><code class=""lang-auto"">out = discrim(sample)<NewLine><NewLine>gradients = torch.autograd.grad(inputs=sample, outputs=out,<NewLine>    grad_outputs=torch.ones(out.shape).to(sample.device),<NewLine>    create_graph=True, retain_graph=True, only_inputs=True)[0]<NewLine></code></pre><NewLine><p>You may assume that sample is a tensor yielded by my generator and out is a <code>N x 1</code> tensor. I’ll start off with saying that this code <em>works</em> without amp, but I’m just confused on how I’m suppose to use amp with this. This is what I’m currently trying:</p><NewLine><pre><code class=""lang-auto"">with autocast(False):<NewLine>    sample = scaler.scale(sample)<NewLine>    out = discrim(sample)<NewLine><NewLine>    gradients = torch.autograd.grad(inputs=sample, outputs=out,<NewLine>        grad_outputs=torch.ones(out.shape).to(sample.device),<NewLine>        create_graph=True, retain_graph=True, only_inputs=True)[0]<NewLine>    gradients = gradients / scaler.get_scale()<NewLine></code></pre><NewLine><p>Note that <code>sample</code> was computed under <code>autocast(True)</code>, but I’m not sure if this matters. This seems to work, but I think I want to compute <code>out</code> under the mixed precision context to be faster. This seems to be slow atm.</p><NewLine><p>This my second attempt at the problem:</p><NewLine><pre><code class=""lang-auto"">with autocast(True):<NewLine>    out = discrim(x=sample)<NewLine><NewLine>    with autocast(False):<NewLine>        sample = scaler.scale(sample)<NewLine>        out = scaler.scale(out)<NewLine><NewLine>        gradients = torch.autograd.grad(inputs=sample, outputs=out,<NewLine>            grad_outputs=torch.ones(out.shape).to(sample.device),<NewLine>            create_graph=True, retain_graph=True, only_inputs=True)[0]<NewLine><NewLine>        gradients = gradients / scaler.get_scale()<NewLine></code></pre><NewLine><p>But I get this error on autograd: <code>One of the differentiated Tensors appears to not have been used in the graph</code>. I’m not really sure on what’s the proper way of doing this. Any help is greatly appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/ayalaa2,(Alex Ayala),ayalaa2,"September 10, 2020,  6:15pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t believe your use case is different from what’s shown in <a href=""https://pytorch.org/docs/stable/notes/amp_examples.html#gradient-penalty"" rel=""nofollow noopener"">the docs</a>, aside from more explicit kwargs and <code>outputs</code> being a Tensor with (presumably) more than one element.  Perhaps, instead of</p><NewLine><blockquote><NewLine><p>To implement a gradient penalty <em>with</em> gradient scaling, the loss passed to <a href=""https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad"" rel=""nofollow noopener""> <code>torch.autograd.grad()</code> </a> should be scaled.</p><NewLine></blockquote><NewLine><p>the docs should say</p><NewLine><blockquote><NewLine><p>To implement a gradient penalty <em>with</em> gradient scaling, the <strong><code>outputs</code> Tensor(s)</strong> passed to <a href=""https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad"" rel=""nofollow noopener""> <code>torch.autograd.grad()</code> </a> should be scaled.</p><NewLine></blockquote><NewLine><p>Try</p><NewLine><pre><code class=""lang-auto"">with autocast():<NewLine>    out = discrim(sample)<NewLine><NewLine>gradients = torch.autograd.grad(outputs=scaler.scale(out), inputs=sample,<NewLine>    grad_outputs=torch.ones(out.shape, device=sample.device),<NewLine>    create_graph=True, retain_graph=True, only_inputs=True)[0]<NewLine><NewLine># proceed as shown in the docs<NewLine><NewLine># if gradients is a Tensor<NewLine>gradients = gradients / scaler.get_scale()<NewLine># if gradients is a tuple<NewLine>inv_scale = 1./scaler.get_scale()<NewLine>gradients = [p * inv_scale for p in gradients]<NewLine><NewLine>with autocast():<NewLine>    # compute penalty term from gradients<NewLine>    # add to loss if needed<NewLine><NewLine>scaler.scale(&lt;loss or penalty&gt;).backward()<NewLine></code></pre><NewLine><p>If that works I will update docs as described above.</p><NewLine><p>FYI in <code>grad</code>, <code>retain_graph</code> defaults to the value of <code>create_graph</code> and <code>only_inputs</code> defaults to <code>True</code> so these kwargs are not needed.  Also, I changed<br/><NewLine><code>torch.ones(out.shape).to(sample.device)</code> to <code>torch.ones(out.shape, device=sample.device)</code> which synthesizes <code>grad_outputs</code> directly on the device, avoids an expensive CPU-&gt;GPU copy, and is good practice regardless of Amp.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for the reply!</p><NewLine><p>I’ve implemented the suggestion you made and it all appears to be working. With your comment and cross-referencing with the docs, it makes much more sense on what’s happening now. Thank you also for the additional comments, I didn’t realize the <code>to(...)</code> had that problem and I made that change.</p><NewLine><p>I do agree that if the docs listed it as the <strong>output tensors</strong> that this would make more sense for newcomers in the future.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mcarilli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ayalaa2; <NewLine> ,"REPLY_DATE 1: September 15, 2020,  5:33am; <NewLine> REPLY_DATE 2: September 14, 2020,  8:40pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
95718,Do I need to save the state_dict oof GradScaler?,2020-09-09T22:41:21.198Z,0,42,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Do I need to save the state_dict of torch.cuda.amp.GradScaler and reload it to resume training? The docs say it dynamically estimates the scale factor each iteration, so I never saved it. So, will model.load_state_dict and optimizer.load_state_dict suffice?</p><NewLine></div>",https://discuss.pytorch.org/u/pytorch2,(Corey Levinson),pytorch2,"September 9, 2020, 10:41pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you want to restore the last scale factor (as well as the backoff and growth factor, if changed), then you should restore its <code>state_dict</code>.<br/><NewLine>Your training should also work without restoring the gradient scaler, but will most likely not reproduce the same results as a run without interruptions, as the new gradient scaler could skip iterations at different steps.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 12, 2020,  7:44am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
94035,Inference in ONNX mixed precision model,2020-08-25T10:39:56.177Z,0,97,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I trained frcnn model with automatic mixed precision and exported it to ONNX. I wonder however how would inference look like programmaticaly to leverage the speed up of mixed precision model, since pytorch uses <code>with autocast():</code>, and I can’t come with an idea how to put it in the inference engine, like onnxruntime.</p><NewLine><p>My specs:<br/><NewLine>torch==1.6.0+cu101<br/><NewLine>torchvision==0.7.0+cu101<br/><NewLine>onnx==1.7.0<br/><NewLine>onnxruntime-gpu==1.4.0</p><NewLine><p>Model exports just fine:</p><NewLine><pre><code class=""lang-auto"">torch.onnx.export(model, <NewLine>                  x, <NewLine>                  ""model_16.onnx"", <NewLine>                  verbose=True, do_constant_folding=True, opset_version=12,<NewLine>                  input_names=input_names, output_names=output_names)<NewLine></code></pre><NewLine><p>But I wonder how to leverage mixed precision speed up here:</p><NewLine><pre><code class=""lang-auto""><NewLine>import onnxruntime as ort<NewLine>ort_session = ort.InferenceSession('model_16.onnx')<NewLine>outputs = ort_session.run(None, {'input': x.numpy()})<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Westerby,(Westerby),Westerby,"August 25, 2020, 10:43am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not exactly sure how ONNX exports the model, but if tracing is used, the mixed-precision operations might have been already recorded. Do you see any FP16 operations, if you profile the ONNX model?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>thanks for the suggestions. I run the onnx runtime profiler on 16FP model from torch, but I’m not exactly sure how to look for execution of FP16 operations there. I’m attaching the log.<br/><NewLine><a class=""onebox"" href=""http://www.mediafire.com/file/rel0ze3y963nohv/onnxruntime_profile__2020-08-28_10-35-51.json/file"" rel=""nofollow noopener"" target=""_blank"">http://www.mediafire.com/file/rel0ze3y963nohv/onnxruntime_profile__2020-08-28_10-35-51.json/file</a><br/><NewLine>Here’s the code I used to run the profiler:</p><NewLine><pre><code class=""lang-auto"">import onnxruntime as ort<NewLine>options = ort.SessionOptions()<NewLine>options.enable_profiling = True<NewLine>ort_session = ort.InferenceSession('model_16.onnx', options)<NewLine>outputs = ort_session.run(None, {'input': images[0].cpu().numpy()})<NewLine>prof_file = ort_session.end_profiling()<NewLine></code></pre><NewLine><p>Anyway, if I do simple time measurement for inference, I don’t see much difference. To be honest I expected ONNX model to run faster.</p><NewLine><p>1.Pure torch 16FP model:</p><NewLine><pre><code class=""lang-auto"">from imutils.video import FPS<NewLine>fps = FPS().start()<NewLine>for i in range(100):<NewLine>    images = list(image.to('cuda:0') for image in x)<NewLine>    with autocast():<NewLine>        pred = model(images)<NewLine>    <NewLine>    fps.update()<NewLine><NewLine>fps.stop()<NewLine>print('Time taken: {:.2f}'.format(fps.elapsed()))<NewLine>print('~ FPS : {:.2f}'.format(fps.fps()))<NewLine><NewLine>Time taken: 2.19<NewLine>~ FPS : 45.57<NewLine></code></pre><NewLine><ol start=""2""><NewLine><li>Torch-&gt;ONNX 16FP model:</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">import onnxruntime as ort<NewLine>ort_session = ort.InferenceSession('model_16.onnx')<NewLine>fps = FPS().start()<NewLine><NewLine>for i in range(100):<NewLine>    outputs = ort_session.run(None, {'input': images[0].cpu().numpy()})<NewLine>    fps.update()<NewLine><NewLine>fps.stop()<NewLine>print('Time taken: {:.2f}'.format(fps.elapsed()))<NewLine>print('~ FPS : {:.2f}'.format(fps.fps()))<NewLine><NewLine>Time taken: 2.15<NewLine>~ FPS : 46.61<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Westerby; <NewLine> ,"REPLY_DATE 1: August 26, 2020, 10:08am; <NewLine> REPLY_DATE 2: August 28, 2020, 11:57am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
93923,Issue with automatic mixed precision,2020-08-24T13:20:32.029Z,2,88,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Without AMP</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch.cuda.amp import autocast, GradScaler<NewLine>from torchvision import models<NewLine><NewLine>model = models.mobilenet_v2(pretrained=True).cuda()<NewLine>loss_fnc = torch.nn.CrossEntropyLoss()<NewLine>optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)<NewLine><NewLine>X = torch.randn((32,3,300,300), dtype=torch.float32).cuda()<NewLine>y = torch.randint(0, 1000, (32,), dtype=torch.long).cuda()<NewLine><NewLine>model.train()<NewLine>for j in range(30):    <NewLine>    optimizer.zero_grad()<NewLine>    <NewLine>    y_hat = model(X)<NewLine>    loss = loss_fnc(y_hat, y)    <NewLine>    loss.backward()<NewLine>    optimizer.step()    <NewLine>    print (loss.item())<NewLine></code></pre><NewLine><p>Output:</p><NewLine><pre><code class=""lang-auto"">8.039933204650879<NewLine>5.690041542053223<NewLine>3.4787116050720215<NewLine>1.607206106185913<NewLine>0.6231755614280701<NewLine>0.23825135827064514<NewLine>0.08544095605611801<NewLine>0.04335329309105873<NewLine>0.016259444877505302<NewLine>0.01174827478826046<NewLine>0.0069425650872290134<NewLine>0.004459714516997337<NewLine>0.003734807949513197<NewLine>0.0024659112095832825<NewLine>0.0027059323620051146<NewLine>........................<NewLine></code></pre><NewLine><p>But with AMP it gives nan’s</p><NewLine><pre><code class=""lang-auto"">model = models.mobilenet_v2(pretrained=True).cuda()<NewLine>loss_fnc = torch.nn.CrossEntropyLoss()<NewLine>optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)<NewLine><NewLine>X = torch.randn((32,3,300,300), dtype=torch.float32).cuda()<NewLine>y = torch.randint(0, 1000, (32,), dtype=torch.long).cuda()<NewLine><NewLine>scaler = GradScaler()<NewLine>model.train()<NewLine>for j in range(30):    <NewLine>    optimizer.zero_grad()<NewLine>    <NewLine>    with autocast():<NewLine>        y_hat = model(X)<NewLine>        loss = loss_fnc(y_hat, y)<NewLine>    <NewLine>    scaler.scale(loss).backward()<NewLine>    scaler.step(optimizer)<NewLine>    scaler.update()<NewLine><NewLine>    print (loss.item())<NewLine></code></pre><NewLine><p>Output:</p><NewLine><pre><code class=""lang-auto"">8.393239974975586<NewLine>nan<NewLine>nan<NewLine>nan<NewLine>nan<NewLine>nan<NewLine>nan<NewLine>nan<NewLine>nan<NewLine>nan<NewLine>nan<NewLine>nan<NewLine>nan<NewLine>nan<NewLine>nan<NewLine>.......................<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/smujjiga,(Smujjiga),smujjiga,"August 24, 2020,  7:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the code snippet. I cannot reproduce the NaN outputs for the amp model and get:</p><NewLine><pre><code class=""lang-python"">7.946749687194824<NewLine>7.837947368621826<NewLine>8.126112937927246<NewLine>5.472558498382568<NewLine>3.0419869422912598<NewLine>1.3717389106750488<NewLine>0.4931238293647766<NewLine>0.21829186379909515<NewLine>0.09434405714273453<NewLine>0.0426550917327404<NewLine>0.021307891234755516<NewLine>0.013564658351242542<NewLine>0.010585908778011799<NewLine>0.00768495025113225<NewLine>0.005186968017369509<NewLine>0.0036425101570785046<NewLine>0.003632057225331664<NewLine>0.0027618384920060635<NewLine>0.0024199967738240957<NewLine>0.00201830524019897<NewLine>0.0015914703253656626<NewLine>0.0011888183653354645<NewLine>0.0011378041235730052<NewLine>0.000954880437348038<NewLine>0.0007914779707789421<NewLine>0.000882966909557581<NewLine>0.0007371000247076154<NewLine>0.0007495403406210244<NewLine>0.0006300751701928675<NewLine></code></pre><NewLine><p>Could you post the PyTorch, <code>torchvision</code>, CUDA, and cudnn versions you are using and how you’ve installed PyTorch?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> for looking into it. Below are the details</p><NewLine><p>PyTorch:  1.6.0<br/><NewLine>torchvision: 0.7.0<br/><NewLine>cuda : 10.0.130<br/><NewLine>cudnn: 7.5<br/><NewLine>GPU: Titan RTX</p><NewLine><p>Installed pytorch using <code>conda install pytorch torchvision cudatoolkit=10.1 -c pytorch</code></p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/9f02188ec06dc5fdfd6d7e2ab5c80faf23a4b754"" href=""https://discuss.pytorch.org/uploads/default/original/3X/9/f/9f02188ec06dc5fdfd6d7e2ab5c80faf23a4b754.png"" title=""image""><img alt=""image"" data-base62-sha1=""mGEpjwXSNPJymkE50J4gb1Zq5aQ"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/f/9f02188ec06dc5fdfd6d7e2ab5c80faf23a4b754_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/original/3X/9/f/9f02188ec06dc5fdfd6d7e2ab5c80faf23a4b754.png"" width=""622""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">740×594 17.8 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It works fine with CUDA 10.2.</p><NewLine><p>I will upgrade my env.</p><NewLine><p>Thanks</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/smujjiga; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/smujjiga; <NewLine> ,"REPLY_DATE 1: August 27, 2020,  6:52am; <NewLine> REPLY_DATE 2: August 28, 2020,  8:20am; <NewLine> REPLY_DATE 3: August 28, 2020,  9:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
94175,Optimizer.step() &ndash; ok; scaler.step(optimizer): No inf checks were recorded for this optimizer,2020-08-26T12:19:05.797Z,0,104,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am getting <code>AssertionError: No inf checks were recorded for this optimizer.</code> in <code>""/torch/cuda/amp/grad_scaler.py"", line 291</code> when mixed-precision is used in this weird example below. However, if no mixed-precision is used pytorch doesn’t complain (toggle <code>USE_HALF_PRECISION = True</code>).</p><NewLine><p>I am using PyTorch 1.6.0 (python 3.7, cuda 10.2.89, cudnn 7.6.5. – everything is in conda binaries). Here is the MWE.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch import nn<NewLine>from torch.cuda.amp.autocast_mode import autocast<NewLine>from torch.cuda.amp.grad_scaler import GradScaler<NewLine><NewLine>class Identity_with_weights(nn.Module):<NewLine>    '''For example a KNN algorithm which returns a closest entry from a database for x. Weights are needed<NewLine>    for a seamless inclusion of knn baseline to a set of baseline which do have some parameters. Otherwise<NewLine>    you would need to change the code (remove optimizer, backward pass etc) just for knn which is not neat.'''<NewLine>    def __init__(self):<NewLine>        super(Identity_with_weights, self).__init__()<NewLine>        self.__hidden__ = torch.nn.Linear(1, 1, bias=False)<NewLine><NewLine>    def forward(self, x):<NewLine>        # we need it to be able to call backward on the loss which uses x (outputs).<NewLine>        # Nothing will happen in these examples as it propagates to the input which is not used anywhere else<NewLine>        x.requires_grad = True<NewLine>        return x<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    # config<NewLine>    USE_HALF_PRECISION = True<NewLine>    device = torch.device('cuda:0')<NewLine><NewLine>    # define model<NewLine>    model = Identity_with_weights()<NewLine><NewLine>    # define training things<NewLine>    criterion = nn.L1Loss()<NewLine>    optimizer = torch.optim.Adam(model.parameters())<NewLine><NewLine>    # for amp<NewLine>    scaler = GradScaler()<NewLine><NewLine>    # targets are exactly the same as inputs, i.e. for reconstruction<NewLine>    inputs = torch.rand(8, 1)<NewLine>    targets = inputs.clone().detach()<NewLine><NewLine>    # send to device<NewLine>    model = model.to(device)<NewLine>    inputs = inputs.to(device)<NewLine>    targets = targets.to(device)<NewLine><NewLine>    # we don't need it for the sake of this example, but let's have it here anyway.<NewLine>    optimizer.zero_grad()<NewLine><NewLine>    # since outputs are going to be f16 and targets are f32, criterion will output non zero loss<NewLine>    if USE_HALF_PRECISION:<NewLine>        targets = targets.half()<NewLine>        inputs = inputs.half()<NewLine><NewLine>    # autocasting ops inside of the context manager<NewLine>    with autocast(USE_HALF_PRECISION):<NewLine>        outputs = model(inputs)<NewLine>        loss = criterion(outputs, targets)<NewLine><NewLine>    print(loss)<NewLine><NewLine>    # scaling loss if using half precision<NewLine>    if USE_HALF_PRECISION:<NewLine>        scaler.scale(loss).backward()<NewLine>        scaler.step(optimizer) ## ERROR HERE<NewLine>        scaler.update()<NewLine>    else:<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine></code></pre><NewLine><p>I think I am doing something wrong here. What does it complain about?</p><NewLine></div>",https://discuss.pytorch.org/u/Vladimir_Iashin,(Vladimir Iashin),Vladimir_Iashin,"August 26, 2020, 12:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Scaler (optimizer) looks for parameters used in the graph which is empty, hence, the error.</p><NewLine><p>This specific example was solved by replacing<br/><NewLine><code>x.requires_grad = True</code><br/><NewLine>with<br/><NewLine><code>x = x + 0 * self.__hidden__(x)</code></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vladimir_Iashin; <NewLine> ,"REPLY_DATE 1: August 27, 2020,  7:46pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
94184,Mixed precision in evaluation,2020-08-26T13:09:03.685Z,0,67,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I have large evaluation data set, which is the same size as the training data set and I’m performing the validation phase during training to be able to control the behavior of the training process.</p><NewLine><p>I’ve added automatic mixed precision in the training phase, but is it safe to wrap the validation step in the training process with amp.autocast() to speed up that forward propagation?<br/><NewLine>In general, is it safe/recommended to use mixed precision in model evaluation during the tuning process and if it is, what is the right way to implement?</p><NewLine><pre><code>for epoch in range(epochs):<NewLine>     # Training phase<NewLine>     train_loss, train_score = self.train_model(trainset)<NewLine><NewLine>     # Validation phase<NewLine>     valid_loss, valid_score = self.valid_model(validset)<NewLine><NewLine>@torch.no_grad()<NewLine>def valid_model(self, dataloader):<NewLine>     self.eval()    <NewLine><NewLine>     for batch in tqdm(dataloader):<NewLine>          # Evaluate with mixed precision<NewLine>          if self.setting.mixed_precision:<NewLine><NewLine>               # Runs the forward pass with autocasting, including loss and score calculation<NewLine>               with amp.autocast():<NewLine>                    loss, score = self.validation_step(batch)</code></pre><NewLine></div>",https://discuss.pytorch.org/u/doctore,,doctore,"August 26, 2020,  1:16pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, you can also use <code>autocast</code> during the validation step and wouldn’t need to apply the gradient scaling, since no gradient are calculated in this phase.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: August 27, 2020, 12:37pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
