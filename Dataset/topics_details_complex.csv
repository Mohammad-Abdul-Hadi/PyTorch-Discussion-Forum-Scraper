id,title,created_at,reply_count,views,description,creator_link,creator_name,creator_alias,post_date,post_likes,replies,repliers_links,reply_dates,reply_likes
70893,About the complex category,2020-02-24T16:41:23.896Z,0,81,"<div class=""post"" itemprop=""articleBody""><NewLine><p>(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)</p><NewLine><p>Use the following paragraphs for a longer description, or to establish category guidelines or rules:</p><NewLine><ul><NewLine><li><NewLine><p>Why should people use this category? What is it for?</p><NewLine></li><NewLine><li><NewLine><p>How exactly is this different than the other categories we already have?</p><NewLine></li><NewLine><li><NewLine><p>What should topics in this category generally contain?</p><NewLine></li><NewLine><li><NewLine><p>Do we need this category? Can we merge with another category, or subcategory?</p><NewLine></li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/albanD,(Alban D),albanD,"February 24, 2020,  4:41pm",,,,,
97420,"How to reshape [1, 1, 3, 50] to [1, 1, 3, 75]?",2020-09-24T21:57:13.301Z,1,46,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to do upsampling in UNet, but while concatenating the encoder matrix with the decoder matrix I am getting a different matrix, and here I don’t want to downsize the decoded matrix.</p><NewLine></div>",https://discuss.pytorch.org/u/ahmadkhan242,(Mohammad Ahmad),ahmadkhan242,"September 24, 2020,  9:57pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You cannot reshape a tensor of the shape <code>[1, 1, 3, 50]</code> into <code>[1, 1, 3, 75]</code> as the number of elements differ.<br/><NewLine>Depending on your use case you could either repeat a subset of the original tensor or use an interpolation method to increase the number of elements e.g. via <code>out = F.interpolate(torch.randn([1, 1, 3, 50]), (3, 75))</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, it worked.<br/><NewLine>Ya, I used the wrong word “reshape”.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ahmadkhan242; <NewLine> ,"REPLY_DATE 1: September 26, 2020,  8:12am; <NewLine> REPLY_DATE 2: September 26, 2020, 11:08pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
75053,Dataparallel model with custom functions,2020-04-01T22:31:05.324Z,7,608,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The dataparallel tutorial states that if we want to invoke custom functions we made in our model. We’d have to wrap our model into a subclass of data parallel where the subclass is supposed to look something like this.</p><NewLine><pre><code class=""lang-auto"">class MyDataParallel(nn.DataParallel):<NewLine>    def __getattr__(self, name):<NewLine>        return getattr(self.module, name)<NewLine></code></pre><NewLine><p>However when I do this, I get the following error</p><NewLine><blockquote><NewLine><pre><code class=""lang-auto""> File ""/.autofs/tools/spack/var/spack/environments/ganvoice/.spack-env/view/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 547, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/.autofs/tools/spack/var/spack/environments/ganvoice/.spack-env/view/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 142, in forward<NewLine>    for t in chain(self.module.parameters(), self.module.buffers()):<NewLine>  File ""acai.py"", line 27, in __getattr__<NewLine>    return getattr(self.module, name)<NewLine>  File ""acai.py"", line 27, in __getattr__<NewLine>    return getattr(self.module, name)<NewLine>  File ""acai.py"", line 27, in __getattr__<NewLine>    return getattr(self.module, name)<NewLine>  [Previous line repeated 327 more times]<NewLine></code></pre><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/sidwa0,(sidwa),sidwa0,"April 1, 2020, 10:31pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>DataParallel is inherited from nn.Module, you cannot overwrite <strong>setattr</strong> and <strong>getattr</strong><br/><NewLine>as they are used to make the whole machinery to work</p><NewLine><p>In general nn.Module is a complex class. Don’t try to modify it.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m training autoencoder models. So it will have 2 networks within a class and the only way to avoid having to subclass DataParallel would be to include loss functions within the model which I don’t like at all. Since I may change the loss functions later and they are independent of the model.</p><NewLine><p>What are your suggestions in such a case.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you explain what are you trying to do?<br/><NewLine>If you want to acess submodules you can just call  instance.module.<strong>whatver</strong> so you can still acess to the objects<br/><NewLine>This feels</p><NewLine><pre><code class=""lang-auto"">return getattr(self.module, name)<NewLine></code></pre><NewLine><p>like a by-pass not to have to call instance.module.<strong>whatever</strong> but directly instance.<strong>whatever</strong><br/><NewLine>I understand it’s more confortable but problematic.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to do what is suggested by the PyTorch tutorial. I have multiple functions for forward pass which don’t have the name “forward”. I need to invoke all the forward pass functions to train my model. This is not possible if I make it DataParallel. The PyTorch suggestion didn’t work for me and I was wondering if there are any alternatives. which would allow multi-gpu usage while keeping the code single GPU compatible.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can properly call them by using self during the forward pass.<br/><NewLine>What you cannot do is to call them externally since that way you are not splitting the inputs among the gpus available.</p><NewLine><p>You aren’t either supposed to call model.forward(inputs) but model(inputs) as interally it execute <code>model.__call__()</code> and forward is run in <code>__call__</code></p><NewLine><p>So in short your scheme should be</p><NewLine><pre><code class=""lang-auto"">class Model()<NewLine>  def init<NewLine>  def function1<NewLine>  def function 2<NewLine>  def forward(inputs);<NewLine>      x=self.function1(inputs)<NewLine>      x=self.function2(x)<NewLine><NewLine><NewLine>model = DataParallel(Model(args,kwargs)).cuda()<NewLine>output= model(input)<NewLine></code></pre><NewLine><p>That way whatever you code will work but if you do</p><NewLine><pre><code class=""lang-auto"">model = DataParallel(Model(args,kwargs)).cuda()<NewLine>a=model.module.function1(input)<NewLine>b=model.module.function2(input)<NewLine>or even <NewLine>out=model.forward(input)<NewLine>it won't work because you are skipping to run the code inside<NewLine>model.__call__()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I already know all the information you are giving me. You are still not addressing the PyTorch workaround for the situation. The workaround is not working for me,(the code for which I have shared in the original post). In case you don’t know which PyTorch tutorial I was referring to,have a look at <a href=""https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html"" rel=""nofollow noopener"">this</a>.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, it’s not like I’m not addressing it but I don’t recomend it.<br/><NewLine>The main issue  here is that module is a nn.Module. Then it’s hidden in _modules private dict. The way they designed to gather nn.Modules is through <strong>getattr</strong>. As you are overwritting <strong>getattr</strong> it gets into a infinity recursion.</p><NewLine><p>I really really doubt that workaround won’t lead to further bugs. You can fix it doing the following. Basically I am invoking the parent <strong>getattr</strong> method (the way they originally created to gather nn.Modules) in case you try to get the object ‘module’. Otherwise it directly goes to the children’s objects.</p><NewLine><pre><code class=""lang-auto"">class MyDataParallel(nn.DataParallel):<NewLine>    def __getattr__(self, name):<NewLine>        if name == 'module':<NewLine>            return super().__getattr__('module')<NewLine>        else:<NewLine>            return getattr(self.module, name)<NewLine></code></pre><NewLine><p>You can also do:</p><NewLine><pre><code class=""lang-auto"">class MyDataParallel(nn.DataParallel):<NewLine>    def __getattr__(self, name):<NewLine>        if name == 'module':<NewLine>            return self._modules['module']<NewLine>        else:<NewLine>            return getattr(self.module, name)<NewLine></code></pre><NewLine><p>But anyway if you want a safer code I would do the following.</p><NewLine><pre><code class=""lang-auto"">class MyDataParallel(nn.DataParallel):<NewLine>    def __init__(self, my_methods, *args, **kwargs):<NewLine>        super().__init__(*args, **kwargs)<NewLine>        self._mymethods = my_methods<NewLine><NewLine>    def __getattr__(self, name):<NewLine>        if name in self._mymethods:<NewLine>            return getattr(self.module, name)<NewLine><NewLine>        else:<NewLine>            return super().__getattr__(name)<NewLine></code></pre><NewLine><p>You just need to pass a list of the methods you defined. This way you ensure that if it’s one of your methods it comes from the proper object. The same error you found for a nn.Module will raise if you store tensors in the DataParallel object (as those objects are also hidden in _buffers and _parameters and properly gathered through <strong>getattr</strong>)</p><NewLine><p>Btw <a class=""mention"" href=""/u/alband"">@albanD</a> could you have a look at it to fix the tutorial?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m having trouble using <code>nn.DataParallel</code> with a custom model. I followed the guidance in <a href=""https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"" rel=""nofollow noopener"">this tutorial</a>, but when I run the model, it is not actually using both GPUs, and my first GPU fills up. I’m not sure how to incorporate the guidance from this forum into my code. Here is my model and what I’m using to make it parallel, taken from the tutorial:</p><NewLine><pre><code class=""lang-auto""># specify NN<NewLine>class SegNet(nn.Module):<NewLine>    def __init__(self, params):<NewLine>        super(SegNet, self).__init__()<NewLine><NewLine>        C_in, H_in, W_in = params[""input_shape""]<NewLine>        init_f = params[""initial_filters""]<NewLine>        num_outputs = params[""num_outputs""]<NewLine><NewLine>        self.conv1 = nn.Conv2d(C_in, init_f, kernel_size=3, stride=1, padding=1)<NewLine>        self.conv2 = nn.Conv2d(init_f, 2 * init_f, kernel_size=3, stride=1, padding=1)<NewLine>        self.conv3 = nn.Conv2d(2 * init_f, 4 * init_f, kernel_size=3, padding=1)<NewLine>        self.conv4 = nn.Conv2d(4 * init_f, 8 * init_f, kernel_size=3, padding=1)<NewLine>        self.conv5 = nn.Conv2d(8 * init_f, 16 * init_f, kernel_size=3, padding=1)<NewLine><NewLine>        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)<NewLine><NewLine>        self.conv_up1 = nn.Conv2d(16 * init_f, 8 * init_f, kernel_size=3, padding=1)<NewLine>        self.conv_up2 = nn.Conv2d(8 * init_f, 4 * init_f, kernel_size=3, padding=1)<NewLine>        self.conv_up3 = nn.Conv2d(4 * init_f, 2 * init_f, kernel_size=3, padding=1)<NewLine>        self.conv_up4 = nn.Conv2d(2 * init_f, init_f, kernel_size=3, padding=1)<NewLine><NewLine>        self.conv_out = nn.Conv2d(init_f, num_outputs, kernel_size=3, padding=1)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = F.relu(self.conv1(x))<NewLine>        x = F.max_pool2d(x, 2, 2)<NewLine><NewLine>        x = F.relu(self.conv2(x))<NewLine>        x = F.max_pool2d(x, 2, 2)<NewLine><NewLine>        x = F.relu(self.conv3(x))<NewLine>        x = F.max_pool2d(x, 2, 2)<NewLine><NewLine>        x = F.relu(self.conv4(x))<NewLine>        x = F.max_pool2d(x, 2, 2)<NewLine><NewLine>        x = F.relu(self.conv5(x))<NewLine><NewLine>        x = self.upsample(x)<NewLine>        x = F.relu(self.conv_up1(x))<NewLine><NewLine>        x = self.upsample(x)<NewLine>        x = F.relu(self.conv_up2(x))<NewLine><NewLine>        x = self.upsample(x)<NewLine>        x = F.relu(self.conv_up3(x))<NewLine><NewLine>        x = self.upsample(x)<NewLine>        x = F.relu(self.conv_up4(x))<NewLine><NewLine>        output = self.conv_out(x)<NewLine><NewLine>        return output<NewLine><NewLine><NewLine># specify model parameters<NewLine>params_model = {<NewLine>    ""input_shape"": (3, h, w),<NewLine>    ""initial_filters"": 16,<NewLine>    ""num_outputs"": 1,<NewLine>}<NewLine><NewLine>model = SegNet(params_model)<NewLine><NewLine># tell pytorch to use both gpus<NewLine>if torch.cuda.device_count() &gt; 1:<NewLine>  print(""Let's use"", torch.cuda.device_count(), ""GPUs!"")<NewLine>  model = nn.DataParallel(model)<NewLine><NewLine>model.to(device)<NewLine><NewLine></code></pre><NewLine><p>But then when I try training with my dataset, I get this error because it is only using one GPU. When I use a similar strategy with one of the torchvision models, it runs perfectly.</p><NewLine><blockquote><NewLine><p>RuntimeError: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 11.00 GiB total capacity; 9.90 GiB already allocated; 23.01 MiB free; 9.98 GiB reserved in total by PyTorch)</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>With this solution too, it’s not possible to call custom (non-“forward”) methods while ensuring data-parallelism (the whole batch ends up getting passed in the custom function call). One solution for this can be to call the custom functions from inside forward by doing a simple if-else where you just figure out which function needs to be called. This can be done by passing the function name to forward and calling the necessary function by comparing against a set of hardcoded names. One other thing, each of the tensors in the call to the forward should have batchsize in the 0th dimension (this is sometimes not the case when using RNNs – this FAQ(<a href=""https://pytorch.org/docs/stable/notes/faq.html#pack-rnn-unpack-with-data-parallelism"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/notes/faq.html#pack-rnn-unpack-with-data-parallelism</a>) also talks about it)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/JuanFMontesinos; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/sidwa0; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/JuanFMontesinos; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/sidwa0; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/JuanFMontesinos; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/sidwa0; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/JuanFMontesinos; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mikey_t; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/nik1996; <NewLine> ,"REPLY_DATE 1: April 2, 2020,  2:34am; <NewLine> REPLY_DATE 2: April 2, 2020,  4:40pm; <NewLine> REPLY_DATE 3: April 2, 2020,  6:01pm; <NewLine> REPLY_DATE 4: April 4, 2020,  7:45pm; <NewLine> REPLY_DATE 5: April 4, 2020, 10:54pm; <NewLine> REPLY_DATE 6: April 7, 2020,  3:39am; <NewLine> REPLY_DATE 7: April 7, 2020,  6:40pm; <NewLine> REPLY_DATE 8: September 1, 2020,  4:32pm; <NewLine> REPLY_DATE 9: September 11, 2020,  6:27am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 2 Likes; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> 
95192,nn.Linear as forward function complex neural network,2020-09-04T13:34:22.787Z,1,53,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey guys!</p><NewLine><p>I am running into a problem when using the complex library. I am trying to write a little training program in which I am using torch.nn.Linear as a forward function. The input data I am working with, however, can be complex.<br/><NewLine>Therefore in torch.nn.Linear I end up trying to do matmul with complex valued tensors, which is not allowed (under the release 1.6.0). The reason that I end up in a situation in which I want to perform complex valued tensors is that my cost function is complex which leads to complex gradients and hence a possibly complex weights.<br/><NewLine>Can someone tell me if there is perhaps another forward function I could use with which I would not run into these problems? Or can someone give me some insight in another way in which I could write little neural network with a complex cost function?</p><NewLine></div>",https://discuss.pytorch.org/u/Schalky,(~),Schalky,"September 4, 2020,  1:34pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you don’t mind me asking, what does it mean to have a complex cost function (i.e. how do you “minimize”)?<br/><NewLine>One obvious way could be to feed the real and imaginary into the network separately, it would seem PyTorch still needs a bit before supporting complex for everything. Matmul should work with the preview nightly but not Linear, so you could define your own linear if you want.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>So I am minimising over the difference in probality of making a certain measurement (in the learnt and optimal case). This probability is determined by both the imaginary and real part of the amplitude. And the values that I am optimising over influence the final state of the system in a way determined by complex functions. So there is no way (to my knowledge) to rewrite the system in such a way that I can get rid of these complex values here without loosing a lot of information.</p><NewLine><p>I will have a look at the preview nightly and perhaps define my own linear.</p><NewLine><p>Thanks for your help,<br/><NewLine>Merel</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Schalky; <NewLine> ,"REPLY_DATE 1: September 4, 2020,  2:39pm; <NewLine> REPLY_DATE 2: September 7, 2020,  8:51am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
95196,"1j * cos( theta), error?",2020-09-04T13:51:31.529Z,1,48,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey guys!</p><NewLine><p>So I am trying to create the following rather simple function:</p><NewLine><pre><code class=""lang-auto""># Make sure to use pytorch version 1.6.0 as I am using type torch.cfloat<NewLine>from torch import cos, tensor<NewLine>def func():<NewLine>	return tensor([[1.0, - 1j*cos(tensor(1.0))],[1.0,0.0]], dtype=torch.cfloat)<NewLine>print(func())<NewLine></code></pre><NewLine><p>I get an error stating that: “RuntimeError: value cannot be converted to type double without overflow: (0,-0.848705)”.<br/><NewLine>I have played around with different options for a while and it seems like this error is induced by the fact that I am doing 1j*cos(tensor(1.0)). Can someone explain what is going wrong when I multiply this cosine of tensor(1.0) by this complex amplitude?<br/><NewLine>I</p><NewLine></div>",https://discuss.pytorch.org/u/Schalky,(~),Schalky,"September 4, 2020,  1:51pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>With a slightly newer PyTorch (self-compiled from the dev branch, you could also use a nightly):</p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; torch.__version__<NewLine>'1.7.0a0+98ad5ff'<NewLine><NewLine>&gt;&gt;&gt; torch.tensor([[1.0, - 1j*torch.cos(torch.tensor(1.0))],[1.0,0.0]], dtype=torch.cfloat)<NewLine>tensor([[1.+0.0000j, 0.-0.5403j],<NewLine>        [1.+0.0000j, 0.+0.0000j]])<NewLine></code></pre><NewLine><p>so maybe something got fixed (complex support is rather new and there probably are rough edges).</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the tip! I will try this with the newer version of torch.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Schalky; <NewLine> ,"REPLY_DATE 1: September 4, 2020,  2:33pm; <NewLine> REPLY_DATE 2: September 7, 2020,  8:43am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
95233,How to convert numpy.ndarray with data type np.complex128 to a torch tensor?,2020-09-04T23:29:31.719Z,0,38,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a variable named <strong>feature_data</strong> is of type numpy.ndarray, with every element in it being a complex number of form x + yi. How do I convert this to Torch tensor?</p><NewLine><p>When I use the following syntax: <strong>torch.from_numpy(feature_data)</strong>, it gives me an error saying<br/><NewLine><strong>can’t convert np.ndarray of type numpy.complex128. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool.</strong></p><NewLine><p>I want my torch tensor to have complex numbers in it after conversion from numpy.ndarray.</p><NewLine></div>",https://discuss.pytorch.org/u/Gautam_Phadke,(Gautam Phadke),Gautam_Phadke,"September 4, 2020, 11:30pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Try this:  torch.Tensor(np.array([ ]), dtype=torch.cfloat)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Henry_Chibueze; <NewLine> ,"REPLY_DATE 1: September 5, 2020, 10:26am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
94808,Alfred workflow to quickly jump to PyTorch API docs,2020-09-01T10:37:11.078Z,0,36,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’ve created an <a href=""https://www.alfredapp.com/"" rel=""nofollow noopener"">Alfred</a> workflow to quickly jump to the official PyTorch API docs<br/><NewLine><a href=""https://github.com/lsgrep/mldocs"" rel=""nofollow noopener"">https://github.com/lsgrep/mldocs</a><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/b8088e1af8c3345385e77d5b24e889898b01bc97"" href=""https://discuss.pytorch.org/uploads/default/original/3X/b/8/b8088e1af8c3345385e77d5b24e889898b01bc97.jpeg"" title=""CleanShot 2020-09-01 at 17.55.30@2x""><img alt=""CleanShot 2020-09-01 at 17.55.30@2x"" data-base62-sha1=""qg2boroOk82xKuNOtr8x3P8bWzJ"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/8/b8088e1af8c3345385e77d5b24e889898b01bc97_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/8/b8088e1af8c3345385e77d5b24e889898b01bc97_2_538x500.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/8/b8088e1af8c3345385e77d5b24e889898b01bc97_2_538x500.jpeg, https://discuss.pytorch.org/uploads/default/optimized/3X/b/8/b8088e1af8c3345385e77d5b24e889898b01bc97_2_807x750.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/b/8/b8088e1af8c3345385e77d5b24e889898b01bc97_2_1076x1000.jpeg 2x"" width=""538""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">CleanShot 2020-09-01 at 17.55.30@2x</span><span class=""informations"">1364×1266 335 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/lsgrep,(Yusup),lsgrep,"September 1, 2020, 10:37am",,,,,
91865,Complex matrix multiplication,2020-08-06T16:02:45.328Z,0,209,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Given the documentation stating that</p><NewLine><blockquote><NewLine><p>Operations on complex tensors (e.g., <a href=""https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv"" rel=""nofollow noopener""> <code>torch.mv()</code> </a>, <a href=""https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"" rel=""nofollow noopener""> <code>torch.matmul()</code> </a>) are likely to be faster and more memory efficient than operations on float tensors mimicking them.</p><NewLine></blockquote><NewLine><p>I would expect matmul to be implemented for complex tensors, however when I try to execute the following:</p><NewLine><pre><code class=""lang-auto"">a = torch.tensor([[1.4 + 3j, 2 + 5j], [1.4 + 3j, 2 + 5j]], dtype=torch.cfloat)<NewLine>a @ a<NewLine></code></pre><NewLine><p>I get <code>RuntimeError: _th_addmm_out not supported on CPUType for ComplexFloat</code>.</p><NewLine><p>This also happens when using <code>torch.matmul</code> or <code>torch.mm</code> instead of the short operator.</p><NewLine><p>Am I doing something wrong?</p><NewLine><p><strong>Edit</strong>: The error occurs on both CPU and GPU.</p><NewLine></div>",https://discuss.pytorch.org/u/weidler,,weidler,"August 6, 2020,  4:33pm",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><strong>Update</strong>: matrix vector multiplication seems to work if explicitly done on a vector (<code>torch.mv()</code>) but cannot be deduced from e.g. multiplying shapes (2, 2) x (2, 1).</p><NewLine><p>Would be really nice to know if this is expected as in “not implemented” or if this can be done differently.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/weidler; <NewLine> ,"REPLY_DATE 1: August 7, 2020,  7:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
80746,Making Infinity Addition Idempotent,2020-05-11T19:42:28.154Z,1,90,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">r1 = torch.randn(5)<NewLine>r2 = torch.tensor([-float('inf') for k in range(5)])<NewLine>dp = 0.<NewLine>for i in range(r1.shape[0]):<NewLine>    dp+=(r1[i]*r2[i])<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">dp<NewLine></code></pre><NewLine><p>Output</p><NewLine><pre><code class=""lang-auto"">tensor(nan)<NewLine></code></pre><NewLine><p>So it seems adding -inf multiple times gives a nan. This creates a problem while implementing masked attention.</p><NewLine><p>Is there a way in which adding -inf multiple times still gives me -inf ? I am not looking for a hack involving some conditional expression. Perhaps, I am looking for a solution which makes -inf idempotent with respect to addition.</p><NewLine></div>",https://discuss.pytorch.org/u/circa,,circa,"May 11, 2020,  7:42pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Circa!</p><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""1"" data-topic=""80746"" data-username=""circa""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/circa/40/22337_2.png"" width=""20""/> circa:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">r1 = torch.randn(5)<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>It is likely that not all of the elements of <code>r1</code> have the same sign.<br/><NewLine>Try replacing your original first line with:</p><NewLine><pre><code class=""lang-python"">r1 = torch.abs (torch.randn(5))<NewLine></code></pre><NewLine><aside class=""quote no-group""><NewLine><blockquote><NewLine><pre><code>dp+=(r1[i]*r2[i])<NewLine></code></pre><NewLine><pre><code class=""lang-auto""></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>Let’s say that <code>r1[0]</code> is positive.  Then after the first iteration, <code>dp</code> will be<br/><NewLine>equal to <code>-inf</code>.  If <code>r1[1]</code> happens to be negative, in the second iteration<br/><NewLine>you will be calculating <code>(-inf) + inf = nan</code>, as it should be.  (And once<br/><NewLine><code>dp</code> becomes <code>nan</code>, it stays <code>nan</code>.)</p><NewLine><aside class=""quote no-group""><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">dp<NewLine></code></pre><NewLine><p>Output</p><NewLine><pre><code class=""lang-auto"">tensor(nan)<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>Note, if you run your test many times (with different random values<br/><NewLine>for <code>r1</code>, you will occasionally get <code>-inf</code> and <code>inf</code> for the result, instead<br/><NewLine>of <code>nan</code>.</p><NewLine><aside class=""quote no-group""><NewLine><blockquote><NewLine><p>So it seems adding -inf multiple times gives a nan.</p><NewLine></blockquote><NewLine></aside><NewLine><p>The short answer is that you are not adding <code>-inf</code> multiple times.<br/><NewLine>You are usually adding <code>-inf</code> to <code>inf</code> somewhere in your loop.</p><NewLine><p>Best.</p><NewLine><p>K. Frank</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ahh…I see.<br/><NewLine>That solved it.<br/><NewLine>Thanks a lot.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/KFrank; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/circa; <NewLine> ,"REPLY_DATE 1: May 11, 2020,  8:44pm; <NewLine> REPLY_DATE 2: May 12, 2020,  6:40am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
79407,PyTorch RNN LSTM,2020-05-02T13:31:55.812Z,0,81,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have some basic questions about Pytorch that I have not see answered and for me its very much so needed in order for me to get started. First and foremost I want to use the University of Maryland Global Terrorism Database (GTD) part of the START website which has over 190,000 observations, and approximately 135 dimensions (features). I plan on using one hot encoding for the variables, deleting some features out, and using feature engineering for others. That being said, I have some questions:</p><NewLine><p>With the dataset - do I need to delete out the header columns? Or can I leave those in there?</p><NewLine><p>Once I delete out what I don’t want - lets say I am left with 130 features, including latitude, logitude values, as well as total attacks for numbers, - my understanding is that whatever I input it will output feature wise correct? So I will get 130 predicted values for the new features - what does this mean with regards to my input?</p><NewLine><p>How should I set it up?</p><NewLine></div>",https://discuss.pytorch.org/u/QuirkyDataScientist1,(Aaron),QuirkyDataScientist1,"May 2, 2020,  1:31pm",,,,,
77944,Adding package to support non-GPU accelerators,2020-04-22T23:35:19.647Z,0,89,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I want to add a package that allows tensors and modules to be sent to a non-GPU accelerator similar to <code>pytorch/xla</code> by creating a new device type</p><NewLine><p>I believe this device would need its own allocator but I am not sure of what memory management primitives are required for this to work.</p><NewLine><p>Appreciate any help <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/almeetb,,almeetb,"April 22, 2020, 11:35pm",,,,,
77707,Risk Model to populate a map,2020-04-21T13:22:43.768Z,0,80,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Good Morning,</p><NewLine><pre><code> I want to create a risk model using pytorch in which I have data from the Global Terrorism Database (GTD) and I want to create a deep learning model that will output based on the inputs the highest risk areas (lat, lon coordinates to populate a map) but I want to be able to filter it based on the attributes. <NewLine><NewLine> How would I create such a model? Code to get me started?<NewLine></code></pre><NewLine><p>~QuirkyDataScientist1</p><NewLine></div>",https://discuss.pytorch.org/u/QuirkyDataScientist1,(Aaron),QuirkyDataScientist1,"April 21, 2020,  1:22pm",,,,,
72096,How to load models on multiple gpus and forward() it?,2020-03-05T05:22:47.720Z,2,94,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I load my 2 model on gpu1 and gpu2. current_device is set on gpu1</p><NewLine><p>then I can forward model on gpu1 but cannot model on gpu2 with this error</p><NewLine><pre><code class=""lang-auto"">RuntimeError: all tensors must be on devices[0]<NewLine></code></pre><NewLine><p>After I change the current device to gpu1 with this line, the same error occurs</p><NewLine><pre><code class=""lang-auto"">torch.cuda.set_device(1)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/juhyung,(손주형),juhyung,"March 5, 2020,  5:22am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>did you follow PyTorch <a href=""https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html#sphx-glr-beginner-blitz-data-parallel-tutorial-py"" rel=""nofollow noopener"">Data Parallelism tutorial</a>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mmisiur"">@mmisiur</a> yes, but I think my use case is a little bit different.<br/><NewLine>I want to load some models to multiple gpus respectively and run each model on its gpu.<br/><NewLine>Thanks for reminding it.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok, right. So it’s hard to say what is wrong without your code. But if I understand what you want to do (load one model on one gpu, second model on second gpu, and pass some input through them) I think the proper way to do this, and one that works for me is:</p><NewLine><pre><code class=""lang-auto""># imports<NewLine>import torch<NewLine><NewLine># define models<NewLine>m0 = torch.nn.Linear(10,5)<NewLine>m1 = torch.nn.Linear(10,5)<NewLine><NewLine># define devices<NewLine>d0 = torch.device(""cuda:0"")<NewLine>d1 = torch.device(""cuda:1"")<NewLine><NewLine># define tensors<NewLine>t0 = torch.rand(10)<NewLine>t1 = torch.rand(10)<NewLine><NewLine># move to devices<NewLine>t0 = t0.to(d0)<NewLine>t1 = t1.to(d1)<NewLine>m0 = m0.to(d0)<NewLine>m1 = m1.to(d1)<NewLine><NewLine># forward pass<NewLine>out0 = m0(t0)<NewLine>out1 = m1(t1)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh I just noticed that I didn’t move tensors…<br/><NewLine>Thanks!!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mmisiur; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/juhyung; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mmisiur; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/juhyung; <NewLine> ,"REPLY_DATE 1: March 5, 2020,  8:11am; <NewLine> REPLY_DATE 2: March 5, 2020,  8:38am; <NewLine> REPLY_DATE 3: March 5, 2020,  9:06am; <NewLine> REPLY_DATE 4: March 5, 2020,  9:06am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
72093,How model is loaded on gpu?,2020-03-05T04:09:22.817Z,0,88,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m testing loading same model in gpu like this.</p><NewLine><pre><code class=""lang-auto"">import model<NewLine>import numpy as np<NewLine><NewLine>tt = np.zeros((512, 512, 3), dtype=np.uint8)<NewLine>ms = {}<NewLine>while True:<NewLine>	for i in range(300):<NewLine>		if not ms.get(i):<NewLine>			ms.update({i:model.Model()})<NewLine>			for k,v in ms.items():<NewLine>				v.infer(tt)<NewLine>			print(i, 'loaded')<NewLine>			print(len(ms), 'length')<NewLine></code></pre><NewLine><p>When I load just 1 model, it occupies 1G of GPU mem.<br/><NewLine>But when i load 160 model like above it occupies just 16G of gpu mem. why usage of gpu mem is not increasing linearly?</p><NewLine></div>",https://discuss.pytorch.org/u/juhyung,(손주형),juhyung,"March 5, 2020,  4:09am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The first CUDA call will initialize the CUDA context, which will use some memory on your device (depending on the CUDA version, used GPU etc.).<br/><NewLine>You could check the memory usage in PyTorch using <code>print(torch.cuda.memory_allocated())</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: March 5, 2020,  4:18am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
70890,Torch Tensor from `numpy` complex not working,2020-02-24T16:20:59.649Z,1,340,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How much of the complex API is actually supported on Torch?</p><NewLine><p><strong>1.4.0 Documentation</strong></p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; torch.real(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))<NewLine>tensor([ -1,  -2,  3])<NewLine></code></pre><NewLine><p><strong>Running on Colab, Torch version: 1.4.0</strong></p><NewLine><pre><code class=""lang-auto"">RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-65-6ab8515aa959&gt; in &lt;module&gt;()<NewLine>----&gt; 1 torch.real(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))<NewLine><NewLine>RuntimeError: Could not infer dtype of complex<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/eduardo4jesus,(Eduardo Reis),eduardo4jesus,"February 24, 2020,  4:41pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/eduardo4jesus"">@eduardo4jesus</a> this should work on the latest master build. I fixed it last week: <a href=""https://github.com/pytorch/pytorch/pull/33361"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/33361</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh I see. I actually though about it, but then I checked that the documentation version was right. So, I got confused.</p><NewLine><p>Any chance of having built in complex multiplication coming soon?</p><NewLine><p>Thanks a lot.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>yeah we are working on adding the matrix multiplication. should be out soon</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/anjali411; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/eduardo4jesus; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/anjali411; <NewLine> ,"REPLY_DATE 1: February 24, 2020,  4:46pm; <NewLine> REPLY_DATE 2: February 24, 2020, 11:31pm; <NewLine> REPLY_DATE 3: February 25, 2020,  4:24am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
