id,title,created_at,reply_count,views,description,creator_link,creator_name,creator_alias,post_date,post_likes,replies,repliers_links,reply_dates,reply_likes
23,About the reinforcement-learning category,2017-01-14T16:59:32.627Z,2,1587,"<div class=""post"" itemprop=""articleBody""><NewLine><p>A section to discuss RL implementations, research, problems</p><NewLine></div>",https://discuss.pytorch.org/u/smth,,smth,"January 14, 2017,  5:03pm",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The <code>reinforce.py</code> and <code>actor-critic.py</code> examples do not really converge for a <code>running_reward &gt; 200</code> for me. Did anyone get it to work? I found that the running reward reached 199 quite often and then the rewards start to decrease. Does anyone have a similar experience?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote"" data-full=""true"" data-post=""2"" data-topic=""23""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""//discuss.pytorch.org/letter_avatar_proxy/v2/letter/l/87869e/40.png"" width=""20""/> lakehanne:</div><NewLine><blockquote><NewLine><p>The <a href=""http://reinforce.py"" rel=""nofollow noopener"">reinforce.py</a> and <a href=""http://actor-critic.py"" rel=""nofollow noopener"">actor-critic.py</a> examples do not really converge for a running_reward &gt; 200 for me. Did anyone get it to work? I found that the running reward reached 199 quite often and then the rewards start to decrease. Does anyone have a similar experience?</p><NewLine></blockquote><NewLine></aside><NewLine><p>I had the same problem, it only reached at 199 at my env, then back and force…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I got it work the first few times I ran it, but later without any changes the same situation happened as you mentioned. Very weird. Thought it’s using the same random seed thought out.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Dear all,</p><NewLine><p>I’ve been trying to replicate DeepMind’s original results (<a href=""http://arxiv.org/pdf/1312.5602v1.pdf"" rel=""nofollow noopener"">http://arxiv.org/pdf/1312.5602v1.pdf</a>) for the particular case of Atari Pong, but I am not succeeding…</p><NewLine><p>One interesting thing that I am observing is that, after a few training iterations (one match is enough), my Q-network starts outputting zeros regardless of the input state! Initially, I thought there was a bug in my code, but now I think that it somehow makes sense. In Pong, the obtained reward is almost always zero (except in the frames where we score or concede a goal) and the Bellman equation is:</p><NewLine><p>Q(s,a) = reward + GAMMA * max_a’ (Q(s’,a’))</p><NewLine><p>so, every time we get a zero reward, the Bellman equation is easily satisfied if Q(s,a) = max_a’ (Q(s’,a’)) = 0. That’s why I think my Q-network is basically learning to output zeros regardless of the input… Any hints on how I can overcome this issue?</p><NewLine><p>I am following the exact same methodology as in DeepMind’s paper, including the network architecture and the preprocessing.</p><NewLine><p>Btw, I am not sure if this is the right place to ask this question, anyway I would be very grateful if any of you could help… <img alt="":stuck_out_tongue:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/stuck_out_tongue.png?v=5"" title="":stuck_out_tongue:""/></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/smth"">@smth</a> ,</p><NewLine><p>I really fell in love with pytorch framework.<br/><NewLine>Besides other frameworks, I feel , i am doing things just from scratch. Great for research.<br/><NewLine>Lots of wishes. Please tell courses like deep learning specialization, andrew ng to add pytorch.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,</p><NewLine><p>I am having some doubts about the <a href=""https://github.com/pytorch/examples/blob/59caa16a29476f290d2e4d38162a89372e5ccb15/reinforcement_learning/reinforce.py#L62"" rel=""nofollow noopener"">Pytorch example REINFORCE implementation</a>. My detailed doubts can be found in <a href=""https://discuss.pytorch.org/t/is-pytorch-reinforce-implementation-correct/85609"">this post</a>.</p><NewLine><p>If any of you could help to clarify this uncertainty, it would be much appreciated.</p><NewLine><p>Thank you.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/lakehanne; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/elynn; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/howard50b; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/dpernes; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mathematics; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/cruzas; <NewLine> ,"REPLY_DATE 1: June 10, 2017, 12:47pm; <NewLine> REPLY_DATE 2: June 19, 2017,  9:51am; <NewLine> REPLY_DATE 3: August 17, 2017,  4:27am; <NewLine> REPLY_DATE 4: November 27, 2017,  6:33pm; <NewLine> REPLY_DATE 5: April 22, 2020,  6:10am; <NewLine> REPLY_DATE 6: June 16, 2020,  8:41am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
97574,How to decide the batch dimension in batch training with GCN?,2020-09-26T20:40:51.865Z,0,23,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Here is my GCN definition:</p><NewLine><pre><code class=""lang-auto"">import math<NewLine><NewLine>import torch<NewLine>import numpy as np<NewLine>from torch.nn.parameter import Parameter<NewLine>from torch.nn.modules.module import Module<NewLine><NewLine><NewLine>class GraphConvolution(Module):<NewLine>    """"""<NewLine>    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907<NewLine>    """"""<NewLine><NewLine>    def __init__(self, in_features, out_features, bias=True):<NewLine>        super(GraphConvolution, self).__init__()<NewLine>        self.in_features = in_features<NewLine>        self.out_features = out_features<NewLine>        # self.weight = Parameter(torch.FloatTensor(in_features, out_features))<NewLine>        self.weight = Parameter(torch.FloatTensor(out_features, in_features))<NewLine>        if bias:<NewLine>            # self.bias = Parameter(torch.FloatTensor(out_features))<NewLine>            self.bias = Parameter(torch.FloatTensor(out_features, 1))<NewLine>        else:<NewLine>            self.register_parameter('bias', None)<NewLine>        self.reset_parameters()<NewLine><NewLine>    def reset_parameters(self):<NewLine>        stdv = 1. / math.sqrt(self.out_features)<NewLine>        self.weight.data.uniform_(-stdv, stdv)<NewLine>        if self.bias is not None:<NewLine>            self.bias.data.uniform_(-stdv, stdv)<NewLine><NewLine>    def forward(self, input, adj):<NewLine>        #support = torch.mm(input, self.weight)<NewLine>        #output = torch.spmm(adj, support)<NewLine>        <NewLine>        support = torch.mm(self.weight, input)<NewLine>        output = torch.mm(support, adj.T)<NewLine>        if self.bias is not None:<NewLine>            return output + self.bias<NewLine>        else:<NewLine>            return output<NewLine><NewLine>    def __repr__(self):<NewLine>        return self.__class__.__name__ + ' (' \<NewLine>               + str(self.in_features) + ' -&gt; ' \<NewLine>               + str(self.out_features) + ')'<NewLine></code></pre><NewLine><p>my buffer definition:</p><NewLine><pre><code class=""lang-auto"">class ReplayBuffer():<NewLine>    def __init__(self, max_size, input_shape):<NewLine>        self.mem_size = max_size<NewLine>        self.mem_cntr = 0<NewLine>        self.state_memory = np.zeros((self.mem_size, input_shape))<NewLine><NewLine>    def store_transition(self, state):<NewLine>        index = self.mem_cntr % self.mem_size<NewLine>        self.state_memory[index] = state<NewLine>        self.mem_cntr += 1<NewLine><NewLine>    def sample_buffer(self, batch_size):<NewLine>        max_mem = min(self.mem_cntr, self.mem_size)<NewLine>        batch = np.random.choice(max_mem, batch_size)<NewLine>        states = self.state_memory[batch]<NewLine>        return states<NewLine></code></pre><NewLine><p>NN definition:</p><NewLine><pre><code class=""lang-auto"">class NN(Module):<NewLine>    def __init__(self, in_dim, out_dim):<NewLine>        super(NN, self).__init__()<NewLine>        <NewLine>        # [out * in] * [in * N] * [N * N] ----&gt; [out * N]<NewLine>        self.gc1 = GraphConvolution(in_dim, out_dim)<NewLine>        <NewLine>    def forward(self, state, adj):<NewLine>        x = self.gc1(state, adj)<NewLine>        return x<NewLine></code></pre><NewLine><p>and my test  part:</p><NewLine><pre><code class=""lang-auto"">net = NN(1, 9)<NewLine>adj = torch.tensor(np.eye(9), dtype=torch.float32)<NewLine><NewLine>buffer = ReplayBuffer(max_size=100000, input_shape=9)<NewLine>for i in range(9):<NewLine>    state = torch.tensor(np.random.rand(9), dtype=torch.float32).unsqueeze(0)<NewLine>    buffer.store_transition(state)<NewLine><NewLine>batch_size = 3<NewLine>state_batch = torch.tensor(buffer.sample_buffer(batch_size), dtype=torch.float32)<NewLine><NewLine>batch_output = net(state_batch, adj)<NewLine></code></pre><NewLine><p>and the Error is:</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-54-5941a970f106&gt; in &lt;module&gt;<NewLine>----&gt; 1 batch_output = net(state_batch, adj)<NewLine><NewLine>~/anaconda3/envs/Circuits/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)<NewLine>    548             result = self._slow_forward(*input, **kwargs)<NewLine>    549         else:<NewLine>--&gt; 550             result = self.forward(*input, **kwargs)<NewLine>    551         for hook in self._forward_hooks.values():<NewLine>    552             hook_result = hook(self, input, result)<NewLine><NewLine>&lt;ipython-input-20-39181b909740&gt; in forward(self, state, adj)<NewLine>      7 <NewLine>      8     def forward(self, state, adj):<NewLine>----&gt; 9         x = self.gc1(state, adj)<NewLine>     10         return x<NewLine><NewLine>~/anaconda3/envs/Circuits/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)<NewLine>    548             result = self._slow_forward(*input, **kwargs)<NewLine>    549         else:<NewLine>--&gt; 550             result = self.forward(*input, **kwargs)<NewLine>    551         for hook in self._forward_hooks.values():<NewLine>    552             hook_result = hook(self, input, result)<NewLine><NewLine>&lt;ipython-input-12-5d6f496f5bd3&gt; in forward(self, input, adj)<NewLine>     35         #output = torch.spmm(adj, support)<NewLine>     36 <NewLine>---&gt; 37         support = torch.mm(self.weight, input)<NewLine>     38         output = torch.mm(support, adj.T)<NewLine>     39         if self.bias is not None:<NewLine><NewLine>RuntimeError: size mismatch, m1: [9 x 1], m2: [3 x 9] at ../aten/src/TH/generic/THTensorMath.cpp:41<NewLine></code></pre><NewLine><p>How can I implement batch training, or more specificly, how should I define the dimension of the data stored in the buffer?</p><NewLine></div>",https://discuss.pytorch.org/u/Jingyang_Zhang,(Jingyang Zhang),Jingyang_Zhang,"September 26, 2020,  8:43pm",,,,,
45778,Best practice to share CUDA tensors across multiprocess,2019-05-21T08:10:58.897Z,0,306,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m trying to build multiprocess dataloader in my local machine, for my RL implementation(ACER).<br/><NewLine>The reason why I don’t use pytorch dataloader and DataParallel is that I need to get data from remote server. So there are two types of process for my system.</p><NewLine><p>Learner: It’s the main process. It gets the data from receivers, feed the data to DataParallel for traning; its role is consumer</p><NewLine><p>Receiver: It receives the data from network connection, load data into GPU memory, then share the reference for data to Learner. There should be more than one receivers.</p><NewLine><p>So I want to code as below,</p><NewLine><pre><code class=""lang-python""># Receiver<NewLine>def receiver(data_q):<NewLine>    # do something<NewLine>    while True:<NewLine>        data = sock.recv()<NewLine>        data = data.to('cuda')<NewLine>        data_q.put(data)<NewLine> <NewLine># Trainer<NewLine>def learner(data_q)<NewLine>    # do something<NewLine>    while True:<NewLine>        data = data_q.get()<NewLine>        model.train_with(data)<NewLine><NewLine>if __name__ == '__main__':<NewLine>    data_q = multiprocessing.Queue(maxsize=10)<NewLine>    procs = [<NewLine>        Process(target=receiver, args=(data_q)) for _ in range(10)<NewLine>    ]<NewLine>    procs.append(Process(target=learner, args=(data_q)))<NewLine>    for p in procs: p.start()<NewLine>    for p in procs: p.join()<NewLine></code></pre><NewLine><p>but I have some questions.</p><NewLine><ol><NewLine><li><NewLine><p>Can I use above code with python multiprocessing queue?</p><NewLine></li><NewLine><li><NewLine><p>How can I share the CUDA tensors? It seems share_memory_ function is no-op for them. Is it enough to just put CUDA tensors to pytorch mp queue?</p><NewLine></li><NewLine><li><NewLine><p>In <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/notes/multiprocessing.html</a>, it says I should keep the CUDA tensors to be not out of scope. Does it mean I should keep the reference for CUDA tensor in receiver(producer) process until processing in learner(consumer) process is done? Or is it enough to keep receiver(producer) process alive even though there’s no reference for CUDA tensor anymore in receiver process?</p><NewLine></li><NewLine><li><NewLine><p>In <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/notes/multiprocessing.html</a>, it says that it’s better to reuse the buffer passed through the queue. Does it mean share the tensor between receiver(producer) and learner(consumer) at first, then don’t release it from memory but just refill new data to that tensor with inplace operation? If that’s the case, what is the best practice to let receiver know data processing is completed at learner, so that receiver can fill the (already shared) tensor with new data? Should I make flag lock(or event, queue, etc.) for each receiver processes?</p><NewLine></li><NewLine><li><NewLine><p>Or is it better to share the model to all the receivers so that can call the train function within its process (with lock, to make sure only one process does the training at the same time)?</p><NewLine></li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/wwiiiii,(Jeong TaeYeong),wwiiiii,"May 21, 2019,  8:23am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you figure out the answers to these questions? I’m trying to understand the same issues - especially <span class=""hashtag"">#2</span>. How to share CUDA tensors in multiprocessing?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/lkc; <NewLine> ,"REPLY_DATE 1: September 25, 2020, 12:18am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
14403,DQN with LSTMCell,2018-03-05T17:18:19.221Z,7,1784,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello there I doing a project were we regulate temperature to a reference temperature.</p><NewLine><p>I have current a DQN where i am trying to implement a LSTM layer so i know whether the temperature is going up or down. I have tried this here(the full code can have been uploaded&lt;DRL.py&gt;):</p><NewLine><pre><code class=""lang-auto""><NewLine># AI for temperature regulator for pump<NewLine><NewLine># Importing the libraries<NewLine><NewLine>import numpy as np<NewLine>import random # random samples from different batches (experience replay)<NewLine>import os # For loading and saving brain<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim # for using stochastic gradient descent<NewLine>import torch.autograd as autograd # Conversion from tensor (advanced arrays) to avoid all that contains a gradient<NewLine># We want to put the tensor into a varaible taht will also contain a<NewLine># gradient and to this we need:<NewLine>from torch.autograd import Variable<NewLine># to convert this tensor into a variable containing the tensor and the gradient<NewLine><NewLine># Initializing and setting the variance of a tensor of weights<NewLine>def normalized_columns_initializer(weights, std=1.0):<NewLine>    out = torch.randn(weights.size())<NewLine>    out *= std / torch.sqrt(out.pow(2).sum(1).expand_as(out)) # thanks to this initialization, we have var(out) = std^2<NewLine>    return out<NewLine><NewLine># Initializing the weights of the neural network in an optimal way for the learning<NewLine>def weights_init(m):<NewLine>    weight_shape = list(m.weight.data.size()) #?? list containing the shape of the weights in the object ""m""<NewLine>    fan_in = weight_shape[1] # dim1<NewLine>    fan_out = weight_shape[0] # dim0<NewLine>    w_bound = np.sqrt(6. / (fan_in + fan_out)) # weight bound<NewLine>    m.weight.data.uniform_(-w_bound, w_bound) # generating some random weights of order inversely proportional to the size of the tensor of weights<NewLine>    m.bias.data.fill_(0) # initializing all the bias with zeros<NewLine><NewLine># Creating the architecture of the Neural Network<NewLine>class Network(nn.Module): #inherinting from nn.Module<NewLine>    <NewLine>    #Self - refers to the object that will be created from this class<NewLine>    #     - self here to specify that we're referring to the object<NewLine>    def __init__(self, input_size, nb_action): #[self,input neuroner, output neuroner]<NewLine>        super(Network, self).__init__() #inorder to use modules in torch.nn<NewLine>        # Input and output neurons<NewLine>        self.lstm = nn.LSTMCell(input_size, 30) # making an LSTM (Long Short Term Memory) to learn the temporal properties of the input<NewLine>        self.fcL = nn.Linear(30, nb_action) # full connection of the<NewLine>        print('hej12')<NewLine>        self.apply(weights_init) # initilizing the weights of the model with random weights<NewLine>        self.fcL.weight.data = normalized_columns_initializer(self.fcL.weight.data, 0.01) # setting the standard deviation of the fcL tensor of weights to 0.01<NewLine>        self.fcL.bias.data.fill_(0) # initializing the actor bias with zeros<NewLine>        self.lstm.bias_ih.data.fill_(0) # initializing the lstm bias with zeros<NewLine>        self.lstm.bias_hh.data.fill_(0) # initializing the lstm bias with zeros<NewLine>        self.train() # setting the module in ""train"" mode to activate the dropouts and batchnorms<NewLine><NewLine>		<NewLine>		<NewLine>    # For function that will activate neurons and perform forward propagation<NewLine>    def forward(self, state):<NewLine>        inputs, (hx, cx) = state # getting separately the input images to the tuple (hidden states, cell states)<NewLine>        x = F.relu(self.lstm(inputs)) # forward propagating the signal from the input images to the 1st convolutional layer<NewLine>        hx, cx = self.lstm(x, (hx, cx)) # the LSTM takes as input x and the old hidden &amp; cell states and ouputs the new hidden &amp; cell states<NewLine>        x = hx # getting the useful output, which are the hidden states (principle of the LSTM)<NewLine>        return self.fcL(x), (hx, cx) # returning the output of the actor (Q(S,A)), and the new hidden &amp; cell states ((hx, cx))<NewLine></code></pre><NewLine><p>I get the trace back from the function def weights_init<br/><NewLine>from<br/><NewLine>weight_shape = list(m.weight.data.size())<br/><NewLine>that<br/><NewLine>AttributeError: ‘LSTMCell’ object has no attribute ‘weight’</p><NewLine><p>I have tried to get inspiration from this A3C code:</p><NewLine><pre><code class=""lang-auto""># AI for Breakout<NewLine><NewLine># Importing the librairies<NewLine>import numpy as np<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine><NewLine># Initializing and setting the variance of a tensor of weights<NewLine>def normalized_columns_initializer(weights, std=1.0):<NewLine>    out = torch.randn(weights.size())<NewLine>    out *= std / torch.sqrt(out.pow(2).sum(1).expand_as(out)) # thanks to this initialization, we have var(out) = std^2<NewLine>    return out<NewLine><NewLine># Initializing the weights of the neural network in an optimal way for the learning<NewLine>def weights_init(m):<NewLine>    classname = m.__class__.__name__ # python trick that will look for the type of connection in the object ""m"" (convolution or full connection)<NewLine>    if classname.find('Conv') != -1: # if the connection is a convolution<NewLine>        weight_shape = list(m.weight.data.size()) # list containing the shape of the weights in the object ""m""<NewLine>        fan_in = np.prod(weight_shape[1:4]) # dim1 * dim2 * dim3<NewLine>        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0] # dim0 * dim2 * dim3<NewLine>        w_bound = np.sqrt(6. / (fan_in + fan_out)) # weight bound<NewLine>        m.weight.data.uniform_(-w_bound, w_bound) # generating some random weights of order inversely proportional to the size of the tensor of weights<NewLine>        m.bias.data.fill_(0) # initializing all the bias with zeros<NewLine>    elif classname.find('Linear') != -1: # if the connection is a full connection<NewLine>        weight_shape = list(m.weight.data.size()) # list containing the shape of the weights in the object ""m""<NewLine>        fan_in = weight_shape[1] # dim1<NewLine>        fan_out = weight_shape[0] # dim0<NewLine>        w_bound = np.sqrt(6. / (fan_in + fan_out)) # weight bound<NewLine>        m.weight.data.uniform_(-w_bound, w_bound) # generating some random weights of order inversely proportional to the size of the tensor of weights<NewLine>        m.bias.data.fill_(0) # initializing all the bias with zeros<NewLine><NewLine># Making the A3C brain<NewLine><NewLine>class ActorCritic(torch.nn.Module):<NewLine><NewLine>    def __init__(self, num_inputs, action_space):<NewLine>        super(ActorCritic, self).__init__()<NewLine>        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1) # first convolution<NewLine>        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1) # second convolution<NewLine>        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1) # third convolution<NewLine>        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1) # fourth convolution<NewLine>        self.lstm = nn.LSTMCell(32 * 3 * 3, 256) # making an LSTM (Long Short Term Memory) to learn the temporal properties of the input - we obtain a big encoded vector S of size 256 that encodes an event of the game<NewLine>        num_outputs = action_space.n # getting the number of possible actions<NewLine>        self.critic_linear = nn.Linear(256, 1) # full connection of the critic: output = V(S)<NewLine>        self.actor_linear = nn.Linear(256, num_outputs) # full connection of the actor: output = Q(S,A)<NewLine>        self.apply(weights_init) # initilizing the weights of the model with random weights<NewLine>        self.actor_linear.weight.data = normalized_columns_initializer(self.actor_linear.weight.data, 0.01) # setting the standard deviation of the actor tensor of weights to 0.01<NewLine>        self.actor_linear.bias.data.fill_(0) # initializing the actor bias with zeros<NewLine>        self.critic_linear.weight.data = normalized_columns_initializer(self.critic_linear.weight.data, 1.0) # setting the standard deviation of the critic tensor of weights to 1<NewLine>        self.critic_linear.bias.data.fill_(0) # initializing the critic bias with zeros<NewLine>        self.lstm.bias_ih.data.fill_(0) # initializing the lstm bias with zeros<NewLine>        self.lstm.bias_hh.data.fill_(0) # initializing the lstm bias with zeros<NewLine>        self.train() # setting the module in ""train"" mode to activate the dropouts and batchnorms<NewLine><NewLine>    def forward(self, inputs):<NewLine>        inputs, (hx, cx) = inputs # getting separately the input images to the tuple (hidden states, cell states)<NewLine>        x = F.elu(self.conv1(inputs)) # forward propagating the signal from the input images to the 1st convolutional layer<NewLine>        x = F.elu(self.conv2(x)) # forward propagating the signal from the 1st convolutional layer to the 2nd convolutional layer<NewLine>        x = F.elu(self.conv3(x)) # forward propagating the signal from the 2nd convolutional layer to the 3rd convolutional layer<NewLine>        x = F.elu(self.conv4(x)) # forward propagating the signal from the 3rd convolutional layer to the 4th convolutional layer<NewLine>        x = x.view(-1, 32 * 3 * 3) # flattening the last convolutional layer into this 1D vector x<NewLine>        hx, cx = self.lstm(x, (hx, cx)) # the LSTM takes as input x and the old hidden &amp; cell states and ouputs the new hidden &amp; cell states<NewLine>        x = hx # getting the useful output, which are the hidden states (principle of the LSTM)<NewLine>        return self.critic_linear(x), self.actor_linear(x), (hx, cx) # returning the output of the critic (V(S)), the output of the actor (Q(S,A)), and the new hidden &amp; cell states ((hx, cx))<NewLine><NewLine></code></pre><NewLine><p>i Hope someone can help me because i dont understand why i get this error!</p><NewLine><p>Best regards Søren Koch</p><NewLine></div>",https://discuss.pytorch.org/u/qLience,(Søren Koch),qLience,"March 5, 2018,  5:18pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The Initialization in the A3C code is only for Linear and Conv layers (notice the <code>if classname.find</code> conditions).</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for giving the time!</p><NewLine><p>but I also have a linear?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The fcL layer is linear, you should add the <code>if classname.find('Linear') != -1:</code> to the function.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>thank you!</p><NewLine><p>Now i ran into another problem… would you mind taking a look now you have been so awesome on this on?</p><NewLine><p>Now it get:<br/><NewLine>RuntimeError: The expanded size of the tensor (30) must match the existing size (20) at non-singleton dimension 1. at c:\anaconda2\conda-bld\pytorch_1513133520683\work\torch\lib\th\generic/THTensor.c:309</p><NewLine><p>From the &lt; def normalized_columns_initializer(weights, std=1.0) &gt; function<br/><NewLine>in line:</p><NewLine><pre><code class=""lang-auto"">out *= std / torch.sqrt(out.pow(2).sum(1).expand_as(out)) # thanks to this initialization, we have var(out) = std^2<NewLine></code></pre><NewLine><p>Best regards Søren Koch</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>You have two options:</p><NewLine><ol><NewLine><li>Run the code with PyTorch 0.1.12</li><NewLine><li>Change <code>out *= std / torch.sqrt(out.pow(2).sum(1).expand_as(out))</code> to<br/><NewLine><code>out *= std / torch.sqrt(out.pow(2).sum(1,keepdim=True).expand_as(out))</code><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Wow okay i would never have got that one! Thanks Shani!</p><NewLine><p>Jesus i feel like a noob asking one more question but i have a last problem… I sure your awesomeness can easily handle this…</p><NewLine><p>Now i get:<br/><NewLine>ValueError: not enough values to unpack (expected 2, got 1)</p><NewLine><p>From the  function<br/><NewLine>in line:</p><NewLine><pre><code class=""lang-auto"">inputs, (hx, cx) = state <NewLine></code></pre><NewLine><p>Best regards Søren Koch</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>No problem.<br/><NewLine>What do you send as a state? Your input should have 2 items, a state (in Atari it’s an image) and a tuple of the LSTM hidden and cell states (hx,cx) from the previous run.<br/><NewLine>Example from A3C:</p><NewLine><pre><code class=""lang-auto"">        if done: # Initialization<NewLine>            cx = Variable(torch.zeros(1, 256))<NewLine>            hx = Variable(torch.zeros(1, 256))<NewLine>        else: # The hx,cx from the previous iteration<NewLine>            cx = Variable(cx.data)<NewLine>            hx = Variable(hx.data)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">value, logit, (hx, cx) = model(<NewLine>                (Variable(state.unsqueeze(0)), (hx, cx)))  # notice what the model receives as inputs.<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I only sent a state because i do not understand how or where i implement the hx and cx (Just for the record i am not using CNN im just feeding temperatures into my model). The state i am feeding can be seen in function  and .</p><NewLine><p>Here is the whole code</p><NewLine><pre><code class=""lang-auto""># AI for temperature regulator for pump<NewLine><NewLine># Importing the libraries<NewLine><NewLine>import numpy as np<NewLine>import random # random samples from different batches (experience replay)<NewLine>import os # For loading and saving brain<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim # for using stochastic gradient descent<NewLine>import torch.autograd as autograd # Conversion from tensor (advanced arrays) to avoid all that contains a gradient<NewLine># We want to put the tensor into a varaible taht will also contain a<NewLine># gradient and to this we need:<NewLine>from torch.autograd import Variable<NewLine># to convert this tensor into a variable containing the tensor and the gradient<NewLine><NewLine># Initializing and setting the variance of a tensor of weights<NewLine>def normalized_columns_initializer(weights, std=1.0):<NewLine>    out = torch.randn(weights.size())<NewLine>    out *= std / torch.sqrt(out.pow(2).sum(1,keepdim=True).expand_as(out)) # thanks to this initialization, we have var(out) = std^2<NewLine>    return out<NewLine><NewLine># Initializing the weights of the neural network in an optimal way for the learning<NewLine>def weights_init(m):<NewLine>    classname = m.__class__.__name__ # python trick that will look for the type of connection in the object ""m"" (convolution or full connection)<NewLine>    if classname.find('Linear') != -1:<NewLine>        weight_shape = list(m.weight.data.size()) #?? list containing the shape of the weights in the object ""m""<NewLine>        fan_in = weight_shape[1] # dim1<NewLine>        fan_out = weight_shape[0] # dim0<NewLine>        w_bound = np.sqrt(6. / (fan_in + fan_out)) # weight bound<NewLine>        m.weight.data.uniform_(-w_bound, w_bound) # generating some random weights of order inversely proportional to the size of the tensor of weights<NewLine>        m.bias.data.fill_(0) # initializing all the bias with zeros<NewLine><NewLine># Creating the architecture of the Neural Network<NewLine>class Network(nn.Module): #inherinting from nn.Module<NewLine>    <NewLine>    #Self - refers to the object that will be created from this class<NewLine>    #     - self here to specify that we're referring to the object<NewLine>    def __init__(self, input_size, nb_action): #[self,input neuroner, output neuroner]<NewLine>        super(Network, self).__init__() #inorder to use modules in torch.nn<NewLine>        # Input and output neurons<NewLine>        self.lstm = nn.LSTMCell(input_size, 30) # making an LSTM (Long Short Term Memory) to learn the temporal properties of the input<NewLine>        self.fcL = nn.Linear(30, nb_action) # full connection of the<NewLine>        self.apply(weights_init) # initilizing the weights of the model with random weights<NewLine>        self.fcL.weight.data = normalized_columns_initializer(self.fcL.weight.data, 0.01) # setting the standard deviation of the fcL tensor of weights to 0.01<NewLine>        self.fcL.bias.data.fill_(0) # initializing the actor bias with zeros<NewLine>        self.lstm.bias_ih.data.fill_(0) # initializing the lstm bias with zeros<NewLine>        self.lstm.bias_hh.data.fill_(0) # initializing the lstm bias with zeros<NewLine>        self.train() # setting the module in ""train"" mode to activate the dropouts and batchnorms<NewLine><NewLine>		<NewLine>		<NewLine>    # For function that will activate neurons and perform forward propagation<NewLine>    def forward(self, state):<NewLine>        inputs, (hx, cx) = state # getting separately the input images to the tuple (hidden states, cell states)<NewLine>        x = F.relu(self.lstm(inputs)) # forward propagating the signal from the input images to the 1st convolutional layer<NewLine>        hx, cx = self.lstm(x, (hx, cx)) # the LSTM takes as input x and the old hidden &amp; cell states and ouputs the new hidden &amp; cell states<NewLine>        x = hx # getting the useful output, which are the hidden states (principle of the LSTM)<NewLine>        return self.fcL(x), (hx, cx) # returning the output of the actor (Q(S,A)), and the new hidden &amp; cell states ((hx, cx))<NewLine><NewLine><NewLine># Implementing Experience Replay<NewLine># We know that RL is based on MDP<NewLine># So going from one state(s_t) to the next state(s_t+1)<NewLine># We gonna put 100 transition between state into what we call the memory<NewLine># So we can use the distribution of experience to make a decision<NewLine>class ReplayMemory(object):<NewLine>    <NewLine>    def __init__(self, capacity):<NewLine>        self.capacity = capacity #100 transitions<NewLine>        self.memory = [] #memory to save transitions<NewLine>    <NewLine>    # pushing transitions into memory with append<NewLine>    #event=transition<NewLine>    def push(self, event):<NewLine>        self.memory.append(event)<NewLine>        if len(self.memory) &gt; self.capacity: #memory only contain 100 events<NewLine>            del self.memory[0] #delete first transition from memory if there is more that 100<NewLine>    <NewLine>    # taking random sample<NewLine>    def sample(self, batch_size):<NewLine>        #Creating variable that will contain the samples of memory<NewLine>        #zip =reshape function if list = ((1,2,3),(4,5,6)) zip(*list)= (1,4),(2,5),(3,6)<NewLine>        #                (state,action,reward),(state,action,reward)  <NewLine>        samples = zip(*random.sample(self.memory, batch_size))<NewLine>        #This is to be able to differentiate with respect to a tensor<NewLine>        #and this will then contain the tensor and gradient<NewLine>        #so for state,action and reward we will store the seperately into some<NewLine>        #bytes which each one will get a gradient<NewLine>        #so that eventually we'll be able to differentiate each one of them<NewLine>        return map(lambda x: Variable(torch.cat(x, 0)), samples)<NewLine><NewLine># Implementing Deep Q Learning<NewLine><NewLine>class Dqn():<NewLine>    <NewLine>    def __init__(self, input_size, nb_action, gamma, lrate, T):<NewLine>        self.gamma = gamma #self.gamma gets assigned to input argument<NewLine>        self.T = T<NewLine>        # Sliding window of the evolving mean of the last 100 events/transitions<NewLine>        self.reward_window = []<NewLine>        #Creating network with network class<NewLine>        self.model = Network(input_size, nb_action)<NewLine>        #creating memory with memory class<NewLine>        #We gonna take 100000 samples into memory and then we will sample from this memory to <NewLine>        #to get a snakk number of random transitions<NewLine>        self.memory = ReplayMemory(100000)<NewLine>        #creating optimizer (stochastic gradient descent)<NewLine>        self.optimizer = optim.Adam(self.model.parameters(), lr = lrate) #learning rate<NewLine>        #input vector which is batch of input observations<NewLine>        #by unsqeeze we create a fake dimension to this is<NewLine>        #what the network expect for its inputs<NewLine>        #have to be the first dimension of the last_state<NewLine>        self.last_state = torch.Tensor(input_size).unsqueeze(0)<NewLine>        #Inilizing<NewLine>        self.last_action = 0<NewLine>        self.last_reward = 0<NewLine>    <NewLine>    def select_action(self, state):<NewLine>        #Q value depends on state<NewLine>        #Temperature parameter T will be a positive number and the closer<NewLine>        #it is to ze the less sure the NN will when taking an action<NewLine>        #forexample<NewLine>        #softmax((1,2,3))={0.04,0.11,0.85} ==&gt; softmax((1,2,3)*3)={0,0.02,0.98} <NewLine>        #to deactivate brain then set T=0, thereby it is full random<NewLine>        probs = F.softmax((self.model(Variable(state, volatile = True))*self.T),dim=1) # T=100<NewLine>        #create a random draw from the probability distribution created from softmax<NewLine>        action = probs.multinomial()<NewLine>        return action.data[0,0]<NewLine>    <NewLine>    # See section 5.3 in AI handbook<NewLine>    def learn(self, batch_state, batch_next_state, batch_reward, batch_action):<NewLine>        outputs = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)<NewLine>        #next input for target see page 7 in attached AI handbook<NewLine>        next_outputs = self.model(batch_next_state).detach().max(1)[0]<NewLine>        target = self.gamma*next_outputs + batch_reward<NewLine>        #Using hubble loss inorder to obtain loss<NewLine>        td_loss = F.smooth_l1_loss(outputs, target)<NewLine>        #using  lass loss/error to perform stochastic gradient descent and update weights <NewLine>        self.optimizer.zero_grad() #reintialize the optimizer at each iteration of the loop<NewLine>        #This line of code that backward propagates the error into the NN<NewLine>        #td_loss.backward(retain_variables = True) #userwarning<NewLine>        td_loss.backward(retain_graph = True)<NewLine>		#And this line of code uses the optimizer to update the weights<NewLine>        self.optimizer.step()<NewLine>    <NewLine>    def update(self, reward, new_signal):<NewLine>        #Updated one transition and we have dated the last element of the transition<NewLine>        #which is the new state<NewLine>        new_state = torch.Tensor(new_signal).float().unsqueeze(0)<NewLine>        self.memory.push((self.last_state, new_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward])))<NewLine>        #After ending in a state its time to play a action<NewLine>        action = self.select_action(new_state)<NewLine>        if len(self.memory.memory) &gt; 100:<NewLine>            batch_state, batch_next_state, batch_action, batch_reward = self.memory.sample(100)<NewLine>            self.learn(batch_state, batch_next_state, batch_reward, batch_action)<NewLine>        self.last_action = action<NewLine>        self.last_state = new_state<NewLine>        self.last_reward = reward<NewLine>        self.reward_window.append(reward)<NewLine>        if len(self.reward_window) &gt; 1000:<NewLine>            del self.reward_window[0]<NewLine>        return action<NewLine>    <NewLine>    def score(self):<NewLine>        return sum(self.reward_window)/(len(self.reward_window)+1.)<NewLine>    <NewLine>    def save(self):<NewLine>        torch.save({'state_dict': self.model.state_dict(),<NewLine>                    'optimizer' : self.optimizer.state_dict(),<NewLine>                   }, 'last_brain.pth')<NewLine>    <NewLine>    def load(self):<NewLine>        if os.path.isfile('last_brain.pth'):<NewLine>            print(""=&gt; loading checkpoint... "")<NewLine>            checkpoint = torch.load('last_brain.pth')<NewLine>            self.model.load_state_dict(checkpoint['state_dict'])<NewLine>            self.optimizer.load_state_dict(checkpoint['optimizer'])<NewLine>            print(""done !"")<NewLine>        else:<NewLine>            print(""no checkpoint found..."")<NewLine></code></pre><NewLine><p>Thanks for you for you quick responses!</p><NewLine><p>Best regards Søren Koch</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>Did you figure out how to proceed?<br/><NewLine>My question is how is the (hx, cx) tuple will be updated when we optimise with a sampled batch from the buffer. Do we have to also store (hx,cx) along with state - action - new state - reward transitions?<br/><NewLine>I am asking this question to extend to DRQN from DQN implementation.<br/><NewLine>Thanks!</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey Everyone !!<br/><NewLine>Can Anyone Please Explain me the meaning and source of this formulae<br/><NewLine>out *= std/torch.sqrt(out.pow(2).sum(1).expand_as(out))??<br/><NewLine>Best Regards,</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Shani_Gamrian; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/qLience; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Shani_Gamrian; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/qLience; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Shani_Gamrian; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/qLience; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Shani_Gamrian; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/qLience; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/explorr; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/adarsh_pandey; <NewLine> ,"REPLY_DATE 1: March 5, 2018,  5:41pm; <NewLine> REPLY_DATE 2: March 5, 2018,  5:45pm; <NewLine> REPLY_DATE 3: March 5, 2018,  5:54pm; <NewLine> REPLY_DATE 4: March 5, 2018,  6:06pm; <NewLine> REPLY_DATE 5: March 5, 2018,  6:32pm; <NewLine> REPLY_DATE 6: March 5, 2018,  6:57pm; <NewLine> REPLY_DATE 7: March 5, 2018,  7:11pm; <NewLine> REPLY_DATE 8: March 5, 2018,  9:51pm; <NewLine> REPLY_DATE 9: August 13, 2018,  5:10pm; <NewLine> REPLY_DATE 10: September 23, 2020,  3:36pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> 
97168,Generalisation of Deep RL,2020-09-22T20:51:32.423Z,0,15,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I am interested in evaluating deep reinforcement models. How can we evaluate a model trained in a Reinforcement learning manner ? can we quantify the DRL generalization ?<br/><NewLine>I mean if we have a model trained to solve some tasks can we just fix its weights and launch the test to  new samples from the same task</p><NewLine></div>",https://discuss.pytorch.org/u/ar795,(ar795),ar795,"September 22, 2020,  8:51pm",,,,,
97155,Reproducibility in multiagent environments,2020-09-22T17:21:15.535Z,0,17,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I need to create a reproducable code for PPO algorithm but in a multiagent environment. When I applied the same algorithm in a single agent setting the reproducibility was succesfull.</p><NewLine><p>I think the problems stands in the initialization of the actor and critic networks.</p><NewLine><p>I  create the networks using the code snippet bellow, similar code snippet for the actor networks as well.</p><NewLine><pre><code class=""lang-auto"">            for _ in range(len(self.env.agents)):<NewLine>                torch.manual_seed(self.seed)<NewLine>                critic = Critic(sum(obs_size), self.hidden_size).to(self.device)<NewLine>                self.critics.append(critic)<NewLine></code></pre><NewLine><p>Also, the environment is initialized using a seed as well, as shown bellow:</p><NewLine><pre><code class=""lang-auto"">def init_env(gym_id: str, seed: int):<NewLine>    env = make_env(gym_id, discrete_action=True)<NewLine>    env.seed(seed)<NewLine>    np.random.seed(seed)<NewLine>    return env, seed<NewLine></code></pre><NewLine><p>Additionally, the buffer from which the agents update their policy is also set to a specific seed, as shown bellow:</p><NewLine><pre><code class=""lang-auto"">ids = np.arange(self.trajectory_size)<NewLine>        for agent, _ in enumerate(self.env.agents):<NewLine>            for epoch in range(self.epochs):<NewLine>                np.random.seed(self.seed)<NewLine>                np.random.shuffle(ids)<NewLine></code></pre><NewLine><p>Is there anything else that is sensitive to randomness (and has to be set to a seed) in the PPO algorithm that I cannot see?</p><NewLine></div>",https://discuss.pytorch.org/u/Kimonili,,Kimonili,"September 22, 2020,  5:21pm",,,,,
96902,Deep q network not learning and step not stepping towards target,2020-09-20T07:17:02.771Z,0,24,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am trying to create a simple deep q network for rl with conv2d layers. I can’t figure out what I am doing wrong, and the only thing I can see that doesn’t seem right is when I get the model predicition for a state after the optimizer step it doesn’t seem to get closer to the target.</p><NewLine><p>I am using pixels from pong in openai gym with single channel 90x90 images, a batch size of 32 and replay memory.</p><NewLine><p>As an example if I try with a batch size of one, and try running self(states) again right after the optimizer step the output is as follows:</p><NewLine><pre><code class=""lang-auto"">current_q_values -&gt; -0.16351485  0.29163417  0.11192469 -0.08969332  0.11081569  0.37215832<NewLine>q_target -&gt;         -0.16351485  0.5336551   0.11192469 -0.08969332  0.11081569  0.37215832<NewLine>self(states) -&gt;     -0.8427617   0.6415581   0.44988257 -0.43897176  0.8693738   0.40007943<NewLine></code></pre><NewLine><p>Does this look as what would be expected for a single step?</p><NewLine><p>The network with loss and optimizer:</p><NewLine><pre><code>    self.in_layer = Conv2d(channels, 32, 8)<NewLine>    self.hidden_conv_1 = Conv2d(32, 64, 4)<NewLine>    self.hidden_conv_2 = Conv2d(64, 128, 3)<NewLine>    self.hidden_fc1 = Linear(128 * 78 * 78, 64)<NewLine>    self.hidden_fc2 = Linear(64, 32)<NewLine>    self.output = Linear(32, action_space)<NewLine><NewLine>    self.loss = torch.nn.MSELoss()<NewLine>    self.optimizer = torch.optim.Adam(<NewLine>        self.parameters(), lr=learning_rate) # lr is 0.001<NewLine><NewLine>def forward(self, state):<NewLine>    in_out = fn.relu(self.in_layer(state))<NewLine>    in_out = fn.relu(self.hidden_conv_1(in_out))<NewLine>    in_out = fn.relu(self.hidden_conv_2(in_out))<NewLine>    in_out = in_out.view(-1, 128 * 78 * 78)<NewLine>    in_out = fn.relu(self.hidden_fc1(in_out))<NewLine>    in_out = fn.relu(self.hidden_fc2(in_out))<NewLine>    return self.output(in_out)<NewLine></code></pre><NewLine><p>Then the learning block:</p><NewLine><pre><code>        self.optimizer.zero_grad()<NewLine><NewLine>        sample = self.sample(self.batch_size)<NewLine>        states = torch.stack([i[0] for i in sample])<NewLine>        actions = torch.tensor([i[1] for i in sample], device=device)<NewLine>        rewards = torch.tensor([i[2] for i in sample], dtype=torch.float32, device=device)<NewLine>        next_states = torch.stack([i[3] for i in sample])<NewLine>        dones = torch.tensor([i[4] for i in sample], dtype=torch.uint8, device=device)<NewLine><NewLine>        current_q_vals = self(states)<NewLine>        next_q_vals = self(next_states)<NewLine>        q_target = current_q_vals.clone()<NewLine>        q_target[torch.arange(states.size()[0]), actions] = rewards + (self.gamma * next_q_vals.max(dim=1)[0]) * (~dones).float()<NewLine><NewLine>        loss = fn.smooth_l1_loss(current_q_vals, q_target)<NewLine>        loss.backward()<NewLine><NewLine>        self.optimizer.step()<NewLine></code></pre><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Ryan_Mccauley,(Ryan Mccauley),Ryan_Mccauley,"September 20, 2020,  7:17am",,,,,
96883,How to reuse buffers passed through a Queue,2020-09-20T01:56:44.034Z,0,22,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I hope you are doing well!</p><NewLine><p>Following the <a href=""https://pytorch.org/docs/master/notes/multiprocessing.html#reuse-buffers-passed-through-a-queue"" rel=""nofollow noopener"">multiprocessing Best Practices page</a>, a best practice is stated to</p><NewLine><blockquote><NewLine><p>…make it send the buffers back</p><NewLine></blockquote><NewLine><p>There have been other forum posts about clarifying this point (see <a href=""https://discuss.pytorch.org/t/examples-for-asynchronous-rl-impala-ape-x-with-actors-sending-observations-not-gradients-to-a-learners-replay-buffer/38306"">this one</a> or <a href=""https://discuss.pytorch.org/t/torch-multiprocessing-how-to-reuse-buffers-passed-through-a-queue/16778/10"">this one</a>) but those did not get any conclusive answers to the question.</p><NewLine><p>User phizaz suggested this near the bottom of the second post:</p><NewLine><pre><code class=""lang-auto"">def run(Qr, Qs):<NewLine>    while True:<NewLine>        b = Qr.get()<NewLine>        torch.randn(..., out=b) # reusing the buffer tensor without re-allocating<NewLine>        Qs.put(b)<NewLine><NewLine>def main():<NewLine>    ...<NewLine>    for i in range(buffer_size): # allocate buffers<NewLine>        b = torch.empty(...)<NewLine>        Qs.put(b)<NewLine>    ... start processes ...<NewLine>    while True:<NewLine>        b = Qr.get() # main's Qr = run's Qs<NewLine>        ... use b ...<NewLine>        Qs.put(b) # main's Qs = run's Qr<NewLine></code></pre><NewLine><p>Though he mentioned</p><NewLine><blockquote><NewLine><p>Caveat: I see the cost of an additional queue to be huge. It might not be worth the benefit of not reallocating additional shared memory.</p><NewLine></blockquote><NewLine><p>Which I think he is probably correct about. So what exactly was the best practices page alluding to?</p><NewLine><p>Very interested in a response. Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/Axel-Jacobsen,(Axel Jacobsen),Axel-Jacobsen,"September 20, 2020,  1:56am",,,,,
96679,"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.DoubleTensor [256, 3]],",2020-09-18T01:36:07.448Z,1,28,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am implementing PPO RL algorithm in pytorch and getting above error.However I am unable to find why this error is comming.</p><NewLine><p>Below is my code.</p><NewLine><p><span class=""hashtag"">#main</span> loop for generating data into replaybuffer.</p><NewLine><pre><code class=""lang-auto"">while count&lt;max_timesteps-1:<NewLine>        episode_length += 1<NewLine>        if done:<NewLine>            cx = Variable(torch.zeros(params.lstm_layers, 1, params.lstm_size))<NewLine>            hx = Variable(torch.zeros(params.lstm_layers, 1, params.lstm_size))<NewLine>        else:<NewLine>            cx = Variable(cx.data)<NewLine>            hx = Variable(hx.data)<NewLine><NewLine>        values = []<NewLine>        log_probs = []<NewLine>        rewards = []<NewLine>        entropies = []<NewLine>        adv = []<NewLine>        st = []<NewLine>        rew = []<NewLine>        act = []<NewLine>        while count&lt;max_timesteps-1:<NewLine>            St = (Variable(state.unsqueeze(0)))<NewLine>            st.append(St)<NewLine>            value, action_values = model(St)<NewLine>            prob = F.softmax(action_values - max(action_values), dim = -1)<NewLine>            log_prob = torch.log(prob).reshape(-1,)<NewLine>            entropy = -(log_prob * prob).sum(1, keepdim=True)<NewLine>            entropies.append(entropy)<NewLine>            m = categorical.Categorical(prob)<NewLine>            action = m.sample().reshape(-1,)<NewLine>            log_prob_a = log_prob.gather(0, Variable(action))<NewLine>            act.append(action)<NewLine>            state, reward, done = env.step(action)<NewLine>            reward = max(min(reward, 1), -1)<NewLine>            <NewLine>            count +=1<NewLine>            <NewLine>            if done:<NewLine>                episode_length = 0<NewLine>                state = env.reset()<NewLine>                <NewLine>            <NewLine>            values.append(value)<NewLine>            log_probs.append(log_prob_a)<NewLine>            rewards.append(reward)<NewLine>            print(""rank "",rank,"" action:"",action, ""reward "",reward)<NewLine><NewLine>            if done:<NewLine>                break<NewLine><NewLine>        R = torch.zeros(1, 1)<NewLine>        if not done:<NewLine>            St = Variable(state.unsqueeze(0))<NewLine>            value, _ = model(Variable(St))<NewLine>            R = value.data<NewLine>        values.append(Variable(R))<NewLine>        R = Variable(R)<NewLine>        gae = torch.zeros(1, 1)<NewLine>        for i in reversed(range(len(rewards))):<NewLine>            R = params.gamma * R + rewards[i]<NewLine>            rew.insert(0,R)<NewLine>            # advantage = R - values[i]<NewLine>            TD = rewards[i] + params.gamma * values[i + 1].data - values[i].data<NewLine>            gae = gae * params.gamma * params.tau + TD<NewLine>            adv.insert(0,gae)<NewLine><NewLine>        for i in reversed(range(len(rewards))):<NewLine>            transition = [st[i], adv[i], rew[i], act[i], log_probs[i], values[i]]<NewLine>            r.add(transition)<NewLine></code></pre><NewLine><p><span class=""hashtag"">#ActorCritic</span> Class</p><NewLine><pre><code class=""lang-auto"">class ActorCritic(torch.nn.Module):<NewLine><NewLine>    def __init__(self, params):<NewLine>        super(ActorCritic, self).__init__()<NewLine><NewLine>        self.num_inputs = params.num_inputs<NewLine>        self.action_space = params.action_dim<NewLine>        self.hidden_size = params.hidden_size<NewLine>        num_inputs = params.num_inputs<NewLine>        self.lstm = nn.LSTM(num_inputs, 8,num_layers = params.lstm_layers)<NewLine>        self.fc1 = nn.Linear(8, 256)<NewLine>        self.fc1.apply(init_weights)<NewLine>        self.fc2 = nn.Linear(256, 256)<NewLine>        self.fc2.apply(init_weights)<NewLine>        # self.fc3 = nn.Linear(256, 256)<NewLine>        # self.fc3.apply(init_weights)<NewLine>        self.critic_linear = nn.Linear(256, 1)<NewLine>        self.critic_linear.apply(init_weights)<NewLine>        self.actor_linear = nn.Linear(256, self.action_space)<NewLine>        self.actor_linear.apply(init_weights)<NewLine>        self.train()<NewLine><NewLine>    def forward(self, inputs):<NewLine>        f1 = F.elu(self.fc1(inputs))<NewLine>        f2 = F.elu(self.fc2(f1))<NewLine>        # x = torch.tanh(self.fc3(x))<NewLine>        critic = self.critic_linear(f2)<NewLine>        actor = self.actor_linear(f2)<NewLine>        return  critic, actor<NewLine>    <NewLine>    <NewLine>    def get_state(self,inputs):<NewLine>        inputs,(hx,cx) = inputs<NewLine>        st, (hx,cx) = self.lstm(inputs,(hx,cx))<NewLine>        return st, (hx,cx)<NewLine></code></pre><NewLine><p><span class=""hashtag"">#code</span> for training part</p><NewLine><pre><code class=""lang-auto"">ind = np.random.randint(0, len(r.storage), size=int(0.2*len(r.storage)))<NewLine>                    for i in ind:<NewLine>                        state, adv, reward, action, old_log_prob, value = r.storage[i]<NewLine>                        V, act_val = model(Variable(state))<NewLine>                        prob = F.softmax(act_val - max(act_val), dim = -1)<NewLine>                        log_prob = torch.log(prob).reshape(-1,)<NewLine>                        entropy = -(log_prob * prob).sum(1, keepdim=True)<NewLine>                        action_log_prob = log_prob.gather(0, Variable(action))<NewLine>                        ratio = torch.exp(action_log_prob - old_log_prob)<NewLine>                        surr1 = ratio * adv<NewLine>                        surr2 = torch.clamp(ratio, 1.0 - 0.2,1.0 + 0.2) * adv<NewLine>                        actor_loss = -torch.min(surr1, surr2).mean()<NewLine>                        value_loss = 0.5 * (reward - V).pow(2).mean()<NewLine>                        entr_loss = 0.01 * entropy.mean()<NewLine>                        <NewLine>                        optimizer.zero_grad()<NewLine>                        actor_loss.mean().backward(retain_graph = True)<NewLine>                        (0.5 * value_loss).mean().backward(retain_graph = True)<NewLine>                        entr_loss.mean().backward()<NewLine><NewLine>                        torch.nn.utils.clip_grad_norm_(model.parameters(), 40)<NewLine>                        optimizer.step()<NewLine></code></pre><NewLine><p>Full traceback:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.DoubleTensor [256, 3]], which is output 0 of TBackward, is at version 3; expected version 2 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!<NewLine><NewLine><NewLine>[W ..\torch\csrc\autograd\python_anomaly_mode.cpp:60] Warning: Error detected in AddmmBackward. Traceback of forward call that caused the error:<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\runpy.py"", line 193, in _run_module_as_main<NewLine>    ""__main__"", mod_spec)<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\runpy.py"", line 85, in _run_code<NewLine>    exec(code, run_globals)<NewLine>  File ""C:\Users\granthjain\AppData\Roaming\Python\Python37\site-packages\spyder_kernels\console\__main__.py"", line 23, in &lt;module&gt;<NewLine>    start.main()<NewLine>  File ""C:\Users\granthjain\AppData\Roaming\Python\Python37\site-packages\spyder_kernels\console\start.py"", line 332, in main<NewLine>    kernel.start()<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\ipykernel\kernelapp.py"", line 612, in start<NewLine>    self.io_loop.start()<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\tornado\platform\asyncio.py"", line 149, in start<NewLine>    self.asyncio_loop.run_forever()<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\asyncio\base_events.py"", line 541, in run_forever<NewLine>    self._run_once()<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\asyncio\base_events.py"", line 1786, in _run_once<NewLine>    handle._run()<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\asyncio\events.py"", line 88, in _run<NewLine>    self._context.run(self._callback, *self._args)<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\tornado\ioloop.py"", line 690, in &lt;lambda&gt;<NewLine>    lambda f: self._run_callback(functools.partial(callback, future))<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\tornado\ioloop.py"", line 743, in _run_callback<NewLine>    ret = callback()<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\tornado\gen.py"", line 787, in inner<NewLine>    self.run()<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\tornado\gen.py"", line 748, in run<NewLine>    yielded = self.gen.send(value)<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\ipykernel\kernelbase.py"", line 365, in process_one<NewLine>    yield gen.maybe_future(dispatch(*args))<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\tornado\gen.py"", line 209, in wrapper<NewLine>    yielded = next(result)<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\ipykernel\kernelbase.py"", line 268, in dispatch_shell<NewLine>    yield gen.maybe_future(handler(stream, idents, msg))<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\tornado\gen.py"", line 209, in wrapper<NewLine>    yielded = next(result)<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\ipykernel\kernelbase.py"", line 545, in execute_request<NewLine>    user_expressions, allow_stdin,<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\tornado\gen.py"", line 209, in wrapper<NewLine>    yielded = next(result)<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\ipykernel\ipkernel.py"", line 306, in do_execute<NewLine>    res = shell.run_cell(code, store_history=store_history, silent=silent)<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\ipykernel\zmqshell.py"", line 536, in run_cell<NewLine>    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\IPython\core\interactiveshell.py"", line 2877, in run_cell<NewLine>    raw_cell, store_history, silent, shell_futures)<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\IPython\core\interactiveshell.py"", line 2922, in _run_cell<NewLine>    return runner(coro)<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\IPython\core\async_helpers.py"", line 68, in _pseudo_sync_runner<NewLine>    coro.send(None)<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\IPython\core\interactiveshell.py"", line 3146, in run_cell_async<NewLine>    interactivity=interactivity, compiler=compiler, result=result)<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\IPython\core\interactiveshell.py"", line 3337, in run_ast_nodes<NewLine>    if (await self.run_code(code, result,  async_=asy)):<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\IPython\core\interactiveshell.py"", line 3417, in run_code<NewLine>    exec(code_obj, self.user_global_ns, self.user_ns)<NewLine>  File ""&lt;ipython-input-1-a2ecfc8e1e15&gt;"", line 1, in &lt;module&gt;<NewLine>    runfile('C:/Users/granthjain/Desktop/startup_code/get_data/ppo_try.py', wdir='C:/Users/granthjain/Desktop/startup_code/get_data')<NewLine>  File ""C:\Users\granthjain\AppData\Roaming\Python\Python37\site-packages\spyder_kernels\customize\spydercustomize.py"", line 541, in runfile<NewLine>    post_mortem=post_mortem)<NewLine>  File ""C:\Users\granthjain\AppData\Roaming\Python\Python37\site-packages\spyder_kernels\customize\spydercustomize.py"", line 440, in exec_code<NewLine>    exec(compiled, ns_globals, ns_locals)<NewLine>  File ""C:\Users\granthjain\Desktop\get_data\ppo_try.py"", line 376, in &lt;module&gt;<NewLine>    train(0, params, model, optimizer, ticker, sc, r)<NewLine>  File ""C:\Users\granthjain\Desktop\get_data\ppo_try.py"", line 128, in train<NewLine>    value, action_values = model(St)<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\torch\nn\modules\module.py"", line 722, in _call_impl<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""C:\Users\granthjain\Desktop\get_data\ppo_try.py"", line 226, in forward<NewLine>    actor = self.actor_linear(f2)<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\torch\nn\modules\module.py"", line 722, in _call_impl<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\torch\nn\modules\linear.py"", line 91, in forward<NewLine>    return F.linear(input, self.weight, self.bias)<NewLine>  File ""C:\Users\granthjain\anaconda\envs\env_full\lib\site-packages\torch\nn\functional.py"", line 1674, in linear<NewLine>    ret = torch.addmm(bias, input, weight.t())<NewLine> (function print_stack)<NewLine></code></pre><NewLine><p>The traceback is giving the DoubleTensor[256,3], actor_linear fully connected layer is is of size 256,3.</p><NewLine><p>I am unable to figure it out what is wrong with the code.</p><NewLine></div>",https://discuss.pytorch.org/u/granth_jain,(granth jain),granth_jain,"September 18, 2020,  1:36am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Try to add <code>.clone()</code> operations to tensors to isolate the offending line of code.<br/><NewLine>I cannot find anything suspicious by skimming through your code.</p><NewLine><p>PS: <code>Variable</code>s are deprecated since PyTorch <code>0.4</code>, so you can use tensors now <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I tried to clone everything and removed Variable. I am still getting this error. I have encountered this error earlier also but there I was able to identify a variable that was updated inplace.</p><NewLine><p>But I am unable to get which variable is getting updated inplace here.</p><NewLine><p>Please help</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/granth_jain; <NewLine> ,"REPLY_DATE 1: September 19, 2020,  7:19am; <NewLine> REPLY_DATE 2: September 19, 2020, 11:55am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
94195,Centralized learning-decentralized execution clarification (engineering perspective on PPO algo),2020-08-26T15:14:50.272Z,11,121,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,</p><NewLine><p>I can understand the theoritical concept of the centralized learning-decentralized execution approach, but I am quite confused about the coding-engineering changes to be done in the update of the networks in the PPO algo.</p><NewLine><p>I think that the actor network (I have seperate networks) will use each agent’s actor loss to update the network, but how the critcs are updated? Should I calculate the cummulative critic loss (from all the agents) and backpropagate it in every single critic network?</p><NewLine></div>",https://discuss.pytorch.org/u/Kimonili,,Kimonili,"August 26, 2020,  3:14pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is it in a multi-agent scenario? Do you want to create a multi-agent PPO framework similar to MADDPG?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, that is correct. I found this <a href=""https://www.mdpi.com/1424-8220/20/16/4546/html"" rel=""nofollow noopener"">paper</a> that created a multiagent version of the PPO algorithm with a centralized approach. The problem is that they have not published the code yet.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well, then its very simple, critic <code>V(x)_i</code> of agent i will collect the current observed state of agent i and the so called “partial state” of other agents j, then predict a value. So the code structure of the training part would be parallelly sample an action from all agent policies <code>P(x)_m, m \in {0, n}</code> where n is the number of agents, then you can exchange the state information between all agents, then parallelly use critics to evaluate values, then parallely update critics, and finally parallely update actors using values produced by critics.</p><NewLine><p>The simplest (with medium performance) way to implement this is using a threadpool, eg: the one from <code>joblib</code> or <code>multiprocessing</code> module, if your model is small. A more complex way would be using <code>torch.jit.fork_</code>, which is even better than using the threadpool because it completely avoids the GIL problem (about 50% faster than threadpool). <code>ProcessPool</code> is only recommended if your model is on aa GPU and very large (like using a resnet or something of similar complexity and parameter size, there are many caveats in using a ProcessPool and I don’t recommend it.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>So in general, there will be N=number_of_agents critic and actor networks. The centralized learning appears in the fact that each critic network recieves state information from the other agents and then updates its weights?</p><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""94195"" data-username=""iffiX""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/iffix/40/24443_2.png"" width=""20""/> iffiX:</div><NewLine><blockquote><NewLine><p>sample an action from all agent policies <code>P(x)_m, m \in {0, n}</code> where n is the number of agents</p><NewLine></blockquote><NewLine></aside><NewLine><p>So here all the agents make an action simultaneously.</p><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""94195"" data-username=""iffiX""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/iffix/40/24443_2.png"" width=""20""/> iffiX:</div><NewLine><blockquote><NewLine><p>then you can exchange the state information between all agents</p><NewLine></blockquote><NewLine></aside><NewLine><p>Given the action each agent made in the previous step, they transision to a new state. The state of each agent will be shared to all the agents. So basically, every agent will be aware of the position of every agent.</p><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""94195"" data-username=""iffiX""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/iffix/40/24443_2.png"" width=""20""/> iffiX:</div><NewLine><blockquote><NewLine><p>then parallelly use critics to evaluate values</p><NewLine></blockquote><NewLine></aside><NewLine><p>Here the critics are evaluating its agent’s actions but being aware of the new dynamics of the environments? (after getting informed of the relative position of every agent)</p><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""94195"" data-username=""iffiX""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/iffix/40/24443_2.png"" width=""20""/> iffiX:</div><NewLine><blockquote><NewLine><p>then parallely update critics, and finally parallely update actors using values produced by critics.</p><NewLine></blockquote><NewLine></aside><NewLine><p>This step is related to the previous one so the question remains the same.</p><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""94195"" data-username=""iffiX""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/iffix/40/24443_2.png"" width=""20""/> iffiX:</div><NewLine><blockquote><NewLine><p>The simplest (with medium performance) way to implement this is using a threadpool, eg: the one from <code>joblib</code> or <code>multiprocessing</code> module, if your model is small. A more complex way would be using <code>torch.jit.fork_</code> , which is even better than using the threadpool because it completely avoids the GIL problem (about 50% faster than threadpool). <code>ProcessPool</code> is only recommended if your model is on aa GPU and very large (like using a resnet or something of similar complexity and parameter size, there are many caveats in using a ProcessPool and I don’t recommend it.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I believe then that the best choice is <code>torch.jit.fork_</code> , which I understood that can run in CPU as well.</p><NewLine><p>I am really sorry for the questions, I just want to be sure I understand this correctly! <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/></p><NewLine><p>Thank you so much!</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""94195"" data-username=""Kimonili""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/k/5fc32e/40.png"" width=""20""/> Kimonili:</div><NewLine><blockquote><NewLine><p>So in general, there will be N=number_of_agents critic and actor networks. The centralized learning appears in the fact that each critic network recieves state information from the other agents and then updates its weights?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes</p><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""94195"" data-username=""Kimonili""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/k/5fc32e/40.png"" width=""20""/> Kimonili:</div><NewLine><blockquote><NewLine><p>So here all the agents make an action simultaneously.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes</p><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""94195"" data-username=""Kimonili""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/k/5fc32e/40.png"" width=""20""/> Kimonili:</div><NewLine><blockquote><NewLine><p>Given the action each agent made in the previous step, they transision to a new state. The state of each agent will be shared to all the agents. So basically, every agent will be aware of the position of every agent.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes</p><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""94195"" data-username=""Kimonili""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/k/5fc32e/40.png"" width=""20""/> Kimonili:</div><NewLine><blockquote><NewLine><p>Here the critics are evaluating its agent’s actions but being aware of the new dynamics of the environments? (after getting informed of the relative position of every agent)</p><NewLine></blockquote><NewLine></aside><NewLine><p>Well, PPO critic do not evaluate actions of agents, just state of the current agent, and partial states from other agents, since the critic represents value function <code>V(s)</code> and not Q value function <code>Q(s, a)</code>.</p><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""94195"" data-username=""Kimonili""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/k/5fc32e/40.png"" width=""20""/> Kimonili:</div><NewLine><blockquote><NewLine><p>Here the critics are evaluating its agent’s actions but being aware of the new dynamics of the environments? (after getting informed of the relative position of every agent)</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes</p><NewLine><p>A reference to my MADDPG implementation, includes three variants of parallel mechanism mentioned above, if you would like to take a look:<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/iffiX/machin/blob/master/machin/frame/algorithms/maddpg.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/iffiX/machin/blob/master/machin/frame/algorithms/maddpg.py"" rel=""nofollow noopener"" target=""_blank"">iffiX/machin/blob/master/machin/frame/algorithms/maddpg.py</a></h4><NewLine><pre><code class=""lang-py"">import copy<NewLine>import inspect<NewLine>import itertools<NewLine>from random import choice, randint<NewLine>from .utils import determine_device<NewLine>from machin.utils.visualize import make_dot<NewLine>from machin.utils.logging import default_logger<NewLine>from machin.model.nets.base import static_module_wrapper<NewLine>from machin.parallel.pool import P2PPool, ThreadPool<NewLine># pylint: disable=wildcard-import, unused-wildcard-import<NewLine>from .ddpg import *<NewLine><NewLine><NewLine>class SHMBuffer(Buffer):<NewLine>    @staticmethod<NewLine>    def make_tensor_from_batch(batch, device, concatenate):<NewLine>        # this function is used in post processing, and we will<NewLine>        # move all cpu tensors to shared memory.<NewLine>        if concatenate and len(batch) != 0:<NewLine>            item = batch[0]<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/iffiX/machin/blob/master/machin/frame/algorithms/maddpg.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/iffix"">@iffiX</a> Thank you very much! I will take a look right away! You were very informative!</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/iffix"">@iffiX</a>,</p><NewLine><p>I used the multiprocessing module in order to parallelize the experiment in the multiagent particle environment. I saw that you used the same env as well. What happens in my code is that when I apply the step function (the one from the environment’s module) I get this error:</p><NewLine><pre><code class=""lang-auto"">_pickle.PicklingError: Can't pickle &lt;class '.Scenario'&gt;: import of module '' failed<NewLine></code></pre><NewLine><p>Apparently this Scenario class can’t be pickled. Are you aware of a wrapper that avoids this in some way?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I would recommend you send a command to create the environment inside the subprocess directly, instead of pickling it, there are many problems related to pickling gym-based environments and are way too painful to deal with.</p><NewLine><p>I made a wrapper for parallelizing openai gym execution in subprocesses at:<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/iffiX/machin/blob/master/machin/env/wrappers/openai_gym.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/iffiX/machin/blob/master/machin/env/wrappers/openai_gym.py"" rel=""nofollow noopener"" target=""_blank"">iffiX/machin/blob/master/machin/env/wrappers/openai_gym.py</a></h4><NewLine><pre><code class=""lang-py"">from itertools import repeat<NewLine>from typing import Tuple, Callable<NewLine>from threading import Lock<NewLine>from multiprocessing import get_context<NewLine>import gym<NewLine>import numpy as np<NewLine><NewLine>from machin.parallel.exception import ExceptionWithTraceback<NewLine>from machin.parallel.queue import SimpleQueue<NewLine>from machin.parallel.pickle import dumps, loads<NewLine><NewLine>from .base import *<NewLine>from ..utils.openai_gym import disable_view_window<NewLine><NewLine><NewLine>class GymTerminationError(Exception):<NewLine>    def __init__(self):<NewLine>        super(GymTerminationError, self).__init__(<NewLine>            ""One or several environments have terminated, ""<NewLine>            ""reset before continuing.""<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/iffiX/machin/blob/master/machin/env/wrappers/openai_gym.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>It might will also work for the particle env, but I have not tested it.<NewLine>        </div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <a class=""mention"" href=""/u/iffix"">@iffiX</a>, again I am gratefull for your feedbacks. Just one last question, how is the state of each agent shared to every critic network of all agents?</p><NewLine><p>What I do not understand is this: If we had decentralized learning, each critic network of every <code>agent i</code> would recieve as input just one vector, the state vector of the <code>agent i</code>. Now that the states are shared, how should I add the state vectors of the other agents into the <code>agent's i</code> critic network?</p><NewLine><p>Should it be a list of tensors, where each element of the list is the state tensor of each agent in the environment?</p><NewLine><p>Finally, I was thinking that the state of <code>agent i</code> must greater affect the policy of the <code>agent i</code> compared to the effect of the state of the <code>agent j</code> or <code>agent z</code> to the policy of <code>agent i</code>. If this is true, the network’s weights will figure this out and adjust the weights in order to make more significant the state vector of <code>agent i</code>?</p><NewLine><p>Thank you in advance!</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>In the centralized training, decentralized acting scenario, yes, critics will observe state vectors of other agents, and it is usually done by collecting states of all agents and concatenate them into a fixed dimension state tensor and feed into the critic network. You can of course permute the order of states if your agents are homogenous, this <strong>may</strong> increase network robustness.</p><NewLine><p>It is really unknown whether your network will take <code>agent i</code> more seriously than other agents, however, you may explicitly apply the attention mechanism, which is usually used in NLP areas, to sort of peeking into the thought of your critic network.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Really interesting conversation.</p><NewLine><p>Another topic is whether its better to have one critic network per agent or one critic network for each cluster of homogenous agents, as in the centralized training scenario the input vector will be the same for every critic. Some claim that this might have better performance.<br/><NewLine>However, I suppose that the attention mechanism scenario is can only be applied for multiple critic networks.</p><NewLine><blockquote><NewLine><p>You can of course permute the order of states if your agents are homogenous, this  <strong>may</strong>  increase network robustness.</p><NewLine></blockquote><NewLine><p>What I did is concatenating the states and feed them to every critic network with the same order. I could instead try to always feed the critic of <code>agent i</code>, with the state of <code>agent i</code> being in the start of the concatenated states.</p><NewLine><p>Thank you for your detailed answer <a class=""mention"" href=""/u/iffix"">@iffiX</a> !</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have tried:</p><NewLine><ol><NewLine><li>permute order randomly</li><NewLine><li>make <code>agent i</code> first</li><NewLine><li>order states from 1 to n<br/><NewLine>In my undergraduate thesis, there is no significant difference.</li><NewLine></ol><NewLine><p>A visualization of attention weight, using method 3:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/bf72b5667277a4672a49f280e99db1f5176d07cc"" href=""https://discuss.pytorch.org/uploads/default/original/3X/b/f/bf72b5667277a4672a49f280e99db1f5176d07cc.png"" title=""image""><img alt=""image"" data-base62-sha1=""rjCXrXnnQuo5SgIU5G9BxZnLUZe"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/f/bf72b5667277a4672a49f280e99db1f5176d07cc_2_10x10.png"" height=""294"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/f/bf72b5667277a4672a49f280e99db1f5176d07cc_2_690x294.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/f/bf72b5667277a4672a49f280e99db1f5176d07cc_2_690x294.png, https://discuss.pytorch.org/uploads/default/optimized/3X/b/f/bf72b5667277a4672a49f280e99db1f5176d07cc_2_1035x441.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/b/f/bf72b5667277a4672a49f280e99db1f5176d07cc.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1063×454 33.8 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>For single/multiple critics in the homogenous scenario:<br/><NewLine>I have tried:</p><NewLine><ol><NewLine><li>one central critic</li><NewLine><li>one critic for every agent</li><NewLine><li>one critic for every agent, but average their parameters every few episodes. ( by mean() their state dict and broadcast new mean parameters to all critics)<br/><NewLine>1 performs best and 3 performs the worst, I assume that averaging has damaged the information collected by critic.</li><NewLine></ol><NewLine><p>You may try these methods yourself as I have only test my solution with one multi-agent environment modified from the bipedalwalker-v2, contact me if you would like to take a look at the modified implementation:</p><NewLine><p><img alt=""image"" data-base62-sha1=""b93lMdoiUTg53949WCcYLPeGdJL"" height=""400"" src=""https://discuss.pytorch.org/uploads/default/original/3X/4/e/4e1dec25d12aa7d4de026d591289d398d67baf39.png"" width=""600""/></p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/iffix"">@iffiX</a> impressive experimentation, congratulations!!</p><NewLine><aside class=""quote no-group"" data-post=""13"" data-topic=""94195"" data-username=""iffiX""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/iffix/40/24443_2.png"" width=""20""/> iffiX:</div><NewLine><blockquote><NewLine><p>You may try these methods yourself as I have only test my solution with one multi-agent environment modified from the bipedalwalker-v2, contact me if you would like to take a look at the modified implementation:</p><NewLine></blockquote><NewLine></aside><NewLine><p>I will, thank you very much for the informaiton! These are very interesting topics.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Kimonili; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Kimonili; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Kimonili; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Kimonili; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Kimonili; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/Kimonili; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/Kimonili; <NewLine> ,"REPLY_DATE 1: August 28, 2020,  2:30am; <NewLine> REPLY_DATE 2: September 1, 2020,  1:47pm; <NewLine> REPLY_DATE 3: September 1, 2020,  2:01pm; <NewLine> REPLY_DATE 4: September 1, 2020,  2:35pm; <NewLine> REPLY_DATE 5: September 1, 2020,  2:40pm; <NewLine> REPLY_DATE 6: September 1, 2020,  2:44pm; <NewLine> REPLY_DATE 7: September 9, 2020,  3:13pm; <NewLine> REPLY_DATE 8: September 10, 2020,  5:24am; <NewLine> REPLY_DATE 9: September 12, 2020, 12:57pm; <NewLine> REPLY_DATE 10: September 18, 2020,  6:48pm; <NewLine> REPLY_DATE 11: September 18, 2020,  8:08pm; <NewLine> REPLY_DATE 12: September 19, 2020,  5:51am; <NewLine> REPLY_DATE 13: September 19, 2020, 11:15am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> 
96811,How many hidden layers for a3c with 8 inputs ranging from -2000 to 2000 and 3 discrete actions,2020-09-19T06:11:02.709Z,0,16,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to build an A3C with 8 inputs which are normalized and then discretized as int b/w -2000 to 2000.I need to have the past info as well therefore I am using LSTM after first layer.</p><NewLine><p>However I did not have any major success in training the model even after 1Lakh episodes.<br/><NewLine>May I have some help in what should be the ideal number of hidden layer and size of hidden layer for such a problem.</p><NewLine><p>class ActorCritic(torch.nn.Module):</p><NewLine><pre><code>def __init__(self, params):<NewLine>    super(ActorCritic, self).__init__()<NewLine><NewLine>    self.num_inputs = params.num_inputs<NewLine>    self.action_space = params.action_dim<NewLine>    self.hidden_size = params.hidden_size<NewLine>    num_inputs = params.num_inputs<NewLine>    self.lstm = nn.LSTM(num_inputs, 8,num_layers = params.lstm_layers)<NewLine>    self.fc1 = nn.Linear(8, 8)<NewLine>    self.fc1.apply(init_weights)<NewLine>    self.fc2 = nn.Linear(8, 8)<NewLine>    self.fc2.apply(init_weights)<NewLine>    # self.fc3 = nn.Linear(8, 8)<NewLine>    # self.fc3.apply(init_weights)<NewLine>    # self.fc4 = nn.Linear(8, 8)<NewLine>    # self.fc4.apply(init_weights)<NewLine>    self.critic_linear = nn.Linear(8, 1)<NewLine>    self.critic_linear.apply(init_weights)<NewLine>    self.actor_linear = nn.Linear(8, self.action_space)<NewLine>    self.actor_linear.apply(init_weights)<NewLine>    self.train()<NewLine><NewLine>def forward(self, inputs):<NewLine>    inputs, (hx, cx) = inputs<NewLine>    inputs = inputs.reshape(1,1,-1)<NewLine>    output, (hx, cx) = self.lstm(inputs, (hx, cx))<NewLine>    x = torch.tanh(self.fc1(output))<NewLine>    x = torch.tanh(self.fc2(x))<NewLine>    # x = torch.tanh(self.fc3(x))<NewLine>    # x = torch.tanh(self.fc3(x))<NewLine>    return self.critic_linear(x), self.actor_linear(x), (hx, cx)<NewLine><NewLine>def save(self, filename, directory):<NewLine>    torch.save(self.state_dict(), '%s/%s_actor.pth' % (directory, filename))<NewLine><NewLine>def load(self, filename, directory):<NewLine>        self.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))<NewLine></code></pre><NewLine><p>I have around 2 million rows of data with me. and 1 episode takes around 10 rows on average. The input is raw data therefore no convolutions needed.</p><NewLine><p>Any expert advice on this would be of great help.</p><NewLine><p>Thanks,<br/><NewLine>Granth</p><NewLine></div>",https://discuss.pytorch.org/u/granth_jain,(granth jain),granth_jain,"September 19, 2020,  6:30am",,,,,
96548,Why didn&rsquo;t loss.backward() update the network&rsquo;s grads?,2020-09-16T19:59:12.493Z,3,53,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, recently I used the <em><strong>DDPG agent</strong></em> based on PyTorch to build a project.</p><NewLine><p>When I was trying to read a batch of combinations of (state, action, reward, next_state) from the buffer and to update the weights of the 4 networks, it seemed loss.backward() function didn’t update the grads at all! There is just no data shown on the tensorboard page.</p><NewLine><p>Here is the code for “update based on the buffer” part:</p><NewLine><pre><code class=""lang-auto"">def learn(self, adj):<NewLine>    if self.buffer.mem_cntr &lt; self.batch_size:<NewLine>        return<NewLine>    state_vectors, action_vectors, rewards, next_state_vectors = \<NewLine>                               self.buffer.sample_buffer(self.batch_size)<NewLine><NewLine>    adj = T.tensor(adj, dtype = T.float32).to(self.actor.device)<NewLine>    for i in range(self.batch_size):<NewLine>        state_vector = T.tensor(state_vectors[i], dtype = T.float32).unsqueeze(0).to(self.actor.device)<NewLine>        next_state_vector = T.tensor(next_state_vectors[i], dtype = T.float32).unsqueeze(0).to(self.actor.device)<NewLine>        action_vector = T.tensor(action_vectors[i], dtype = T.float32).unsqueeze(0).to(self.actor.device)<NewLine>        reward = T.tensor(rewards[i], dtype = T.float32).to(self.actor.device)<NewLine><NewLine>        next_target_action_vector = self.target_actor.forward(next_state_vector, adj)<NewLine>        next_target_q = self.target_critic(next_state_vector, next_target_action_vector, adj)<NewLine>        q = self.critic.forward(state_vector, action_vector, adj)<NewLine><NewLine>        target_q = reward + self.gamma * next_target_q<NewLine>    <NewLine>        self.critic.optimizer.zero_grad()<NewLine>        critic_loss = F.mse_loss(q, target_q)<NewLine>        critic_loss.backward()<NewLine>        self.critic.optimizer.step()<NewLine>            <NewLine>        self.actor.optimizer.zero_grad()<NewLine>        actor_loss = -self.critic.forward(state_vector, <NewLine>                                              self.actor.forward(state_vector, adj), <NewLine>                                              adj) / self.batch_size<NewLine>        actor_loss.backward()<NewLine>        self.actor.optimizer.step()<NewLine><NewLine>    self.soft_update()<NewLine></code></pre><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/0502cb48b929ffb8175613ecddd7d036d60f0a88"" href=""https://discuss.pytorch.org/uploads/default/original/3X/0/5/0502cb48b929ffb8175613ecddd7d036d60f0a88.png"" title=""Screenshot from 2020-09-16 16-04-20""><img alt=""Screenshot from 2020-09-16 16-04-20"" data-base62-sha1=""IknagjsSAyBN6d2uA5rIVGjUtW"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/5/0502cb48b929ffb8175613ecddd7d036d60f0a88_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/5/0502cb48b929ffb8175613ecddd7d036d60f0a88_2_461x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/5/0502cb48b929ffb8175613ecddd7d036d60f0a88_2_461x500.png, https://discuss.pytorch.org/uploads/default/original/3X/0/5/0502cb48b929ffb8175613ecddd7d036d60f0a88.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/0/5/0502cb48b929ffb8175613ecddd7d036d60f0a88.png 2x"" width=""461""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screenshot from 2020-09-16 16-04-20</span><span class=""informations"">685×742 123 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine>How to modify the codes to make the actor-network work correctly?</p><NewLine></div>",https://discuss.pytorch.org/u/Jingyang_Zhang,(Jingyang Zhang),Jingyang_Zhang,"September 16, 2020,  8:05pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s not how batch samples works. You need to sample a batch of samples simultaneously and update the actor/critic network by these samples. The loss of the actor is the mean of critic loss on these samples, not just decided by the batch_size. Hope I made my point, good luck!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Liu, thank you for your reply! I know your point but for some reason, I have to replace some layers in the original actor and critic network, the networks consist of GCN(Graph Convolution Network) layers and linear layer, instead of mere Linear layers now. For GCN layers, I have two inputs: state_vector, which is a 1XN tensor, and an adjacency matrix, which is an NXN tensor. Inside the layer, the following computation is performed:</p><NewLine><pre><code class=""lang-auto"">[weight matrix]     X      [input]        X         [adjacency matrix]<NewLine>(out_dim X in_dim)  X    (in_dim X N)     X               (N X N)<NewLine></code></pre><NewLine><p>and the output is a (out_dim X N) tensor, then this output is passed to the next GCN layer<br/><NewLine>As you can see, the first input(state_vector) is an 1 X N tensor, but the batch tensor is a (batch_size X N) tensor, which makes it impossible to pass it to the GCN layer, this is the reason I have to use a for loop to extract a single tuple of (state, action, reward, next_state) to make sure the dimensions match.<br/><NewLine>If I don’t update the weights in each loop, does that count as “simultaneously”?</p><NewLine><p>To be honest, I am not sure if I have written the correct code to perform loss.backward() and optimizer.step()</p><NewLine><p>Can I recode in this way?</p><NewLine><pre><code class=""lang-auto"">self.critic.optimizer.zero_grad()<NewLine>self.actor.optimizer_zero_grad()<NewLine><NewLine># for loop to extract tuples and perform loss.backward()<NewLine><NewLine>self.critic.optimizer.step()<NewLine>self.actor.optimizer.step()<NewLine></code></pre><NewLine><p><img alt="":frowning:"" class=""emoji only-emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, you are welcome. You don’t need to worry about the dimension, because PyTorch will consider the first dimension of the input to the network as <code>BATCH</code> dimension (as long as your network is inherited from <code>nn.Module</code>). For example, the input size of your network is <code>1xD</code>, and you feed <code>BxD</code> arrays to your network, that will definitely work well, PyTorch will consider <code>B</code> as the <code>BATCH</code> dimension. And you don’t need to recode, the original code is ok. Just fix the batch update part. Hope that helps, Good luck!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Liu,</p><NewLine><p>I did as you say, but it did show dimension mismatch:</p><NewLine><pre><code class=""lang-auto""><NewLine>  File ""/home/yubi/practice/GCN_RL_Circuit_Project/circuit_rl_experiment/stage5_gcn_ddpg/main_circuit_gcn_ddpg.py"", line 85, in &lt;module&gt;<NewLine>    agent.learn(normalized_adj)<NewLine><NewLine>  File ""/home/yubi/practice/GCN_RL_Circuit_Project/circuit_rl_experiment/stage5_gcn_ddpg/gcn_ddpg_agent.py"", line 221, in learn<NewLine>    target_actions = self.target_actor.forward(states_, adj)<NewLine><NewLine>  File ""/home/yubi/practice/GCN_RL_Circuit_Project/circuit_rl_experiment/stage5_gcn_ddpg/gcn_ddpg_agent.py"", line 61, in forward<NewLine>    x = self.gc1(x, adj)<NewLine><NewLine>  File ""/home/yubi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine><NewLine>  File ""/home/yubi/practice/GCN_RL_Circuit_Project/circuit_rl_experiment/stage5_gcn_ddpg/gcn_layers.py"", line 37, in forward<NewLine>    support = torch.mm(self.weight, input)<NewLine><NewLine>RuntimeError: size mismatch, m1: [18 x 1], m2: [32 x 9] at /opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/THC/generic/THCTensorMathBlas.cu:290<NewLine></code></pre><NewLine><p>and this is my GCN layer code:</p><NewLine><pre><code class=""lang-auto"">import math<NewLine><NewLine>import torch<NewLine><NewLine>from torch.nn.parameter import Parameter<NewLine>from torch.nn.modules.module import Module<NewLine><NewLine><NewLine>class GraphConvolution(Module):<NewLine>    """"""<NewLine>    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907<NewLine>    """"""<NewLine><NewLine>    def __init__(self, in_features, out_features, bias=True):<NewLine>        super(GraphConvolution, self).__init__()<NewLine>        self.in_features = in_features<NewLine>        self.out_features = out_features<NewLine>        # self.weight = Parameter(torch.FloatTensor(in_features, out_features))<NewLine>        self.weight = Parameter(torch.FloatTensor(out_features, in_features))<NewLine>        if bias:<NewLine>            # self.bias = Parameter(torch.FloatTensor(out_features))<NewLine>            self.bias = Parameter(torch.FloatTensor(out_features, 1))<NewLine>        else:<NewLine>            self.register_parameter('bias', None)<NewLine>        self.reset_parameters()<NewLine><NewLine>    def reset_parameters(self):<NewLine>        stdv = 1. / math.sqrt(self.out_features)<NewLine>        self.weight.data.uniform_(-stdv, stdv)<NewLine>        if self.bias is not None:<NewLine>            self.bias.data.uniform_(-stdv, stdv)<NewLine><NewLine>    def forward(self, input, adj):<NewLine>        #support = torch.mm(input, self.weight)<NewLine>        #output = torch.spmm(adj, support)<NewLine>        <NewLine>        support = torch.mm(self.weight, input)<NewLine>        output = torch.mm(support, torch.transpose(adj, 0, 1))<NewLine>        if self.bias is not None:<NewLine>            return output + self.bias<NewLine>        else:<NewLine>            return output<NewLine><NewLine>    def __repr__(self):<NewLine>        return self.__class__.__name__ + ' (' \<NewLine>               + str(self.in_features) + ' -&gt; ' \<NewLine>               + str(self.out_features) + ')'<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/liuruiqi1107; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Jingyang_Zhang; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/liuruiqi1107; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Jingyang_Zhang; <NewLine> ,"REPLY_DATE 1: September 16, 2020,  8:53pm; <NewLine> REPLY_DATE 2: September 16, 2020,  9:11pm; <NewLine> REPLY_DATE 3: September 16, 2020,  9:38pm; <NewLine> REPLY_DATE 4: September 16, 2020, 11:11pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
96165,Initializing policy - reinforcment learning,2020-09-14T01:32:55.257Z,0,25,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to build a model for a policy and have a general question regarding the initial values of the policy.<br/><NewLine>It looks to me that at from the first forward step the softmax will not return equal probability.<br/><NewLine>Questions:</p><NewLine><ol><NewLine><li>how can I force the policy to be equal probability for all the actions at the first step?</li><NewLine><li>what would be a good method to increase exploration when training the policy?</li><NewLine><li>what are some of the ways one can apply constraints on the actions? for example if a specific state makes one of the actions impossible, beside setting it in the rewards in a way of a penalty (restraining the actions)</li><NewLine></ol><NewLine><p>example policy:</p><NewLine><pre><code class=""lang-auto"">class Policy(nn.Module):<NewLine>    def __init__(self, state_size, action_size, fc1_size=64):<NewLine>        super().__init__()<NewLine>        self.fc1 = nn.Linear(state_size, fc1_size)<NewLine>        self.fc2 = nn.Linear(fc1_size, action_size)<NewLine><NewLine>    def forward(self, state):<NewLine>        x = F.relu(self.fc1(state))<NewLine>        x = self.fc2(x)<NewLine>        x = F.softmax(x, dim=1)<NewLine>        return x<NewLine><NewLine>    def act(self, state):<NewLine>        state = state.unsqueeze(0)<NewLine>        probs = self.forward(state).cpu()<NewLine>        p = Categorical(probs)<NewLine>        action = p.sample()<NewLine>        log_prob = p.log_prob(action)<NewLine>        return action.item(), log_prob<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/youdar,,youdar,"September 14, 2020,  5:32am",,,,,
94598,A3c in pytorch.Does normal a3c with multinomial/categorical sampling works well in continuous STATE with discrete action as well?,2020-08-30T16:09:55.577Z,3,73,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to develop A3C reinforcement learning in pytorch.</p><NewLine><p>But I am getting same action being taken on multinomial as well as categorical sampling of actions.</p><NewLine><p>Below is my code…I need help in understanding whether simple A3C works well in continuous state space as well.</p><NewLine><p>Thanks,<br/><NewLine>Granth</p><NewLine><pre><code>    while count&lt;max_timesteps-1:<NewLine>        episode_length += 1<NewLine>        if done:<NewLine>            cx = Variable(torch.zeros(1, params.state_dim))<NewLine>            hx = Variable(torch.zeros(1, params.state_dim))<NewLine>        else:<NewLine>            cx = Variable(cx.data)<NewLine>            hx = Variable(hx.data)<NewLine>        values = []<NewLine>        log_probs = []<NewLine>        rewards = []<NewLine>        entropies = []<NewLine>        while count&lt;max_timesteps-1:<NewLine>            value, action_values, (hx, cx) = model((Variable(state.unsqueeze(0)), (hx, cx)))<NewLine>            prob = F.softmax(action_values,dim = -1)<NewLine>            log_prob = F.log_softmax(action_values, dim=-1)<NewLine>            entropy = -(log_prob * prob).sum(1, keepdim=True)<NewLine>            entropies.append(entropy)<NewLine>            cdist = categorical.Categorical(prob)<NewLine>            action = cdist.sample()<NewLine>            log_prob = log_prob[0, Variable(action)].data<NewLine>            state, reward, done = env.step(action)<NewLine>            done = (done or count == max_timesteps-2)<NewLine>            reward = max(min(reward, 1), -1)<NewLine>            <NewLine>            <NewLine>            count +=1<NewLine>            <NewLine>            if done:<NewLine>                episode_length = 0<NewLine>                state = env.reset()<NewLine>                <NewLine>            <NewLine>            values.append(value)<NewLine>            log_probs.append(log_prob)<NewLine>            rewards.append(reward)<NewLine>            print(ticker,"" action:"",action, ""reward "",reward)<NewLine><NewLine>            if done:<NewLine>                break<NewLine>            <NewLine>        R = torch.zeros(1, 1)<NewLine>        if not done:<NewLine>            value, _, _ = model((Variable(state.unsqueeze(0)), (hx, cx)))<NewLine>            R = value.data<NewLine>        values.append(Variable(R))<NewLine>        policy_loss = 0<NewLine>        value_loss = 0<NewLine>        R = Variable(R)<NewLine>        gae = torch.zeros(1, 1)<NewLine>        for i in reversed(range(len(rewards))):<NewLine>            R = params.gamma * R + rewards[i]<NewLine>            advantage = R - values[i]<NewLine>            value_loss = value_loss + 0.5 * advantage.pow(2)<NewLine>            TD = rewards[i] + params.gamma * values[i + 1].data - values[i].data<NewLine>            gae = gae * params.gamma * params.tau + TD<NewLine>            policy_loss = policy_loss - log_probs[i] * Variable(gae) - 0.01 * entropies[i]<NewLine><NewLine>        optimizer.zero_grad()<NewLine>        (policy_loss + 0.5 * value_loss).backward()<NewLine>        torch.nn.utils.clip_grad_norm_(model.parameters(), 40)<NewLine>        optimizer.step()</code></pre><NewLine></div>",https://discuss.pytorch.org/u/granth_jain,(granth jain),granth_jain,"August 30, 2020,  4:12pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>A3C works in continuous space, but its unstable (even more than A2C), and produce poorer results than PPO / IMPALA(distributed)</p><NewLine><p>For correct implementation, consider reference:<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/iffiX/machin/blob/b1a8b0ce99be9e4d47e132c1325e1aaedb87e0a4/machin/frame/algorithms/a2c.py#L312"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/iffiX/machin/blob/b1a8b0ce99be9e4d47e132c1325e1aaedb87e0a4/machin/frame/algorithms/a2c.py#L312"" rel=""nofollow noopener"" target=""_blank"">iffiX/machin/blob/b1a8b0ce99be9e4d47e132c1325e1aaedb87e0a4/machin/frame/algorithms/a2c.py#L312</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""302"" style=""counter-reset: li-counter 301 ;""><NewLine><li>will be cleared after update is finished.</li><NewLine><li><NewLine></li><li>Args:</li><NewLine><li>    update_value: Whether update the Q network.</li><NewLine><li>    update_policy: Whether update the actor network.</li><NewLine><li>    concatenate_samples: Whether concatenate the samples.</li><NewLine><li><NewLine></li><li>Returns:</li><NewLine><li>    mean value of estimated policy value, value loss</li><NewLine><li>""""""</li><NewLine><li class=""selected"">sum_act_loss = 0</li><NewLine><li>sum_value_loss = 0</li><NewLine><li>self.actor.train()</li><NewLine><li>self.critic.train()</li><NewLine><li>for _ in range(self.actor_update_times):</li><NewLine><li>    # sample a batch</li><NewLine><li>    batch_size, (state, action, advantage) = \</li><NewLine><li>        self.replay_buffer.sample_batch(self.batch_size,</li><NewLine><li>                                        sample_method=""random_unique"",</li><NewLine><li>                                        concatenate=concatenate_samples,</li><NewLine><li>                                        sample_attrs=[</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/iffiX/machin/blob/b1a8b0ce99be9e4d47e132c1325e1aaedb87e0a4/machin/frame/algorithms/a3c.py#L131"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/iffiX/machin/blob/b1a8b0ce99be9e4d47e132c1325e1aaedb87e0a4/machin/frame/algorithms/a3c.py#L131"" rel=""nofollow noopener"" target=""_blank"">iffiX/machin/blob/b1a8b0ce99be9e4d47e132c1325e1aaedb87e0a4/machin/frame/algorithms/a3c.py#L131</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""121"" style=""counter-reset: li-counter 120 ;""><NewLine><li>    if self.is_syncing:</li><NewLine><li>        self.critic_grad_server.pull(self.critic)</li><NewLine><li>    return super(A3C, self)._criticize(state)</li><NewLine><li><NewLine></li><li>def update(self,</li><NewLine><li>           update_value=True,</li><NewLine><li>           update_policy=True,</li><NewLine><li>           concatenate_samples=True,</li><NewLine><li>           **__):</li><NewLine><li>    # DOC INHERITED</li><NewLine><li class=""selected"">    org_sync = self.is_syncing</li><NewLine><li>    self.is_syncing = False</li><NewLine><li>    super(A3C, self).update(update_value, update_policy,</li><NewLine><li>                            concatenate_samples)</li><NewLine><li>    self.is_syncing = org_sync</li><NewLine><li>    self.actor_grad_server.push(self.actor)</li><NewLine><li>    self.critic_grad_server.push(self.critic)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Thanks for the answer…could you please help to let know if there is a version of ppo with lstm.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you just need ppo + lstm with 1-step back brop, there are. But it would be more tricky if you would like to have the BPTT (back-propagate through time feature)</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am unable to figure it out how PPO is to be implemented with LSTM.</p><NewLine><p>Also,given the main objective of PPO is to have an update near to the current policy…is it same to clip the gradient in A3C to a small value and then expect the results to be same as PPO??</p><NewLine><p>may I get help in how to implement this and if there a correct implementation of ppo+lstm can you please help to share.</p><NewLine><p>Thanks</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/granth_jain; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/granth_jain; <NewLine> ,"REPLY_DATE 1: August 31, 2020,  4:18am; <NewLine> REPLY_DATE 2: August 31, 2020,  3:54pm; <NewLine> REPLY_DATE 3: September 1, 2020,  1:48am; <NewLine> REPLY_DATE 4: September 13, 2020,  6:11pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
95949,"After loading my saved model, it doesnt behave the same",2020-09-11T16:24:07.958Z,4,50,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am constructing my model using this architecture (DQN):</p><NewLine><pre><code class=""lang-auto"">class Network(nn.Module):<NewLine>    def __init__(self, in_dim: int, out_dim: int):<NewLine>        super(Network, self).__init__()<NewLine><NewLine>        <NewLine>        self.layers = nn.Sequential(<NewLine>            nn.Linear(in_dim, 256),<NewLine>            nn.ReLU(),<NewLine>            nn.Linear(256, 128),<NewLine>            nn.ReLU(),<NewLine>            nn.Linear(128, 64),<NewLine>            nn.ReLU(),<NewLine>            nn.Linear(64, out_dim),<NewLine>            nn.ReLU())<NewLine>        <NewLine>    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:<NewLine>        return self.layers(x)<NewLine></code></pre><NewLine><p>and after testing my trained model I had around 95% accuracy.<br/><NewLine>However, when I saved the model and tried to load it afterward the model won’t<br/><NewLine>act as it should and it gives around 50% accuracy as if it was just choosing randomly.<br/><NewLine>I tried all the methods I found on the internet but it was all in vain.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/42e14366c0e3f93c33c401ecb382b3a64ba96182"" href=""https://discuss.pytorch.org/uploads/default/original/3X/4/2/42e14366c0e3f93c33c401ecb382b3a64ba96182.png"" title=""image""><img alt=""image"" data-base62-sha1=""9xE8wdWAIYcshgyHHuRVFtCwZ4m"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/2/42e14366c0e3f93c33c401ecb382b3a64ba96182_2_10x10.png"" height=""435"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/2/42e14366c0e3f93c33c401ecb382b3a64ba96182_2_690x435.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/2/42e14366c0e3f93c33c401ecb382b3a64ba96182_2_690x435.png, https://discuss.pytorch.org/uploads/default/original/3X/4/2/42e14366c0e3f93c33c401ecb382b3a64ba96182.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/4/2/42e14366c0e3f93c33c401ecb382b3a64ba96182.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">874×551 138 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>But none of them seems to be working… what am I doing wrong please?</p><NewLine></div>",https://discuss.pytorch.org/u/omar_bouhamed,(omar bouhamed),omar_bouhamed,"September 11, 2020,  6:43pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What kind of data are u testing the saved model on?<br/><NewLine>If it is a separate data(evaluation data) other than the training data then it’s bound to happen<br/><NewLine>This just means that ur In-sample accuracy is higher than your out-sample accuracy in which case ur model has over-fitted on training data (it memorized the data rather than learn to generalize)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><strong>Thank you for the reply.</strong><br/><NewLine>Am testing my model on data from the same data set i trained the model on (split 80/20), the model is giving 95% accuracy on the test samples. However, when i save the model and load it, it won’t give the same accuracy even on the same test samples.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>To see if the problem comes from the model loading, can you try to look if the weights are the same before being saved and after loading a model.<br/><NewLine>For example looking at your last layer :<br/><NewLine><code>agent.dqn.layers[-2].weight</code><br/><NewLine>See if the matrixes are the same.<br/><NewLine>If yes, then it comes from the way you are evaluating your model and we will need to see what you do there.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>thank you Artilind.<br/><NewLine>I checked the weight of both models and they are the same.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/eab60ba17c2584d919a8c508ee7ecf1283401757"" href=""https://discuss.pytorch.org/uploads/default/original/3X/e/a/eab60ba17c2584d919a8c508ee7ecf1283401757.png"" title=""image""><img alt=""image"" data-base62-sha1=""xulL29PTjIccBkkg84uhkjL1Hvx"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/a/eab60ba17c2584d919a8c508ee7ecf1283401757_2_10x10.png"" height=""453"" src=""https://discuss.pytorch.org/uploads/default/original/3X/e/a/eab60ba17c2584d919a8c508ee7ecf1283401757.png"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1393×916 34.8 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine>however when I use the same code to test both models on the same 1000 sample<br/><NewLine>the original one score 924/1000 correct answer<br/><NewLine>as the loaded model  just 500/1000</p><NewLine><pre><code class=""lang-auto"">agent.dqn.layers[-2].weight<NewLine>agent1.dqn.layers[-2].weight<NewLine><NewLine><NewLine>Parameter containing:<NewLine>tensor([[-2.7366e-01,  1.9941e-01, -7.6141e-01,  1.4397e-01, -1.5718e-01,<NewLine>         -3.5489e-01, -4.0804e-01, -1.0615e-02, -1.5499e-02,  2.7892e-02,<NewLine>         -2.9361e-01,  3.0443e-01, -1.7186e-01, -2.7947e-01, -2.0401e-01,<NewLine>          2.3119e-02,  5.1811e-02, -2.5943e-01, -3.1975e-02, -2.3041e-02,<NewLine>         -2.8438e-02, -2.2843e-01,  4.4978e-02,  2.2255e-02,  4.0609e-02,<NewLine>          2.5109e-01,  1.1982e-01,  1.8197e-02,  7.7862e-01, -5.5143e-01,<NewLine>         -6.0077e-02, -7.6426e-01, -3.8195e-01, -7.4313e-01, -1.1739e-01,<NewLine>          5.3178e-02,  1.6637e-01, -9.6227e-01,  1.4221e-02, -1.8411e-01,<NewLine>         -1.0946e-01, -3.8608e-01,  5.6576e-02,  5.7527e-02, -9.9113e-02,<NewLine>         -2.0707e-01, -1.5844e-01, -5.2088e-03, -3.3738e-01,  3.4554e-02,<NewLine>          1.0015e-02,  1.5227e-02, -6.7776e-01, -3.7915e-02, -7.8485e-02,<NewLine>          4.5949e-02, -2.5784e-02, -3.3047e-01,  6.4348e-02, -1.5789e-01,<NewLine>         -1.2264e-02, -1.7147e-01,  7.6571e-02, -3.0366e-02],<NewLine>        [-2.1560e-01, -3.1336e-01, -2.1208e-02,  2.4466e-02, -1.1601e-01,<NewLine>          7.1008e-02, -8.4347e-02, -2.1182e-02, -4.9379e-01, -4.9651e-01,<NewLine>          6.4734e-03, -4.3814e-02, -4.6654e-01,  6.1255e-03, -1.4655e-02,<NewLine>         -1.8655e-01, -9.6411e-01,  1.1566e-02,  4.6149e-02, -1.1270e-01,<NewLine>         -3.3037e-02, -4.3250e-02, -2.2556e-01, -3.3957e-01,  4.7250e-03,<NewLine>         -4.1394e-01, -6.5579e-01, -4.6811e-02, -8.6857e-02, -5.4740e-01,<NewLine>         -9.8245e-01,  2.1111e-01,  1.5543e-01, -3.3294e-02,  2.6496e-01,<NewLine>         -4.8543e-01, -3.9470e-01, -4.6092e-02, -2.2453e-01, -4.8163e-01,<NewLine>         -1.9894e-01,  4.2989e-02,  3.3548e-02,  3.9954e-01, -3.8888e-02,<NewLine>         -4.7950e-01, -5.6876e-01, -4.5290e-01, -4.2524e-02, -5.1943e-02,<NewLine>         -4.1490e-01, -1.6104e-02,  6.7158e-04, -4.7883e-01, -1.5516e-01,<NewLine>         -4.5682e-02, -6.8973e-02,  6.2119e-02, -4.4249e-01, -8.8247e-01,<NewLine>         -5.6976e-02, -1.2550e-01, -5.3172e-01, -5.2279e-02]], device='cuda:0')<NewLine><NewLine>Parameter containing:<NewLine>tensor([[-2.7366e-01,  1.9941e-01, -7.6141e-01,  1.4397e-01, -1.5718e-01,<NewLine>         -3.5489e-01, -4.0804e-01, -1.0615e-02, -1.5499e-02,  2.7892e-02,<NewLine>         -2.9361e-01,  3.0443e-01, -1.7186e-01, -2.7947e-01, -2.0401e-01,<NewLine>          2.3119e-02,  5.1811e-02, -2.5943e-01, -3.1975e-02, -2.3041e-02,<NewLine>         -2.8438e-02, -2.2843e-01,  4.4978e-02,  2.2255e-02,  4.0609e-02,<NewLine>          2.5109e-01,  1.1982e-01,  1.8197e-02,  7.7862e-01, -5.5143e-01,<NewLine>         -6.0077e-02, -7.6426e-01, -3.8195e-01, -7.4313e-01, -1.1739e-01,<NewLine>          5.3178e-02,  1.6637e-01, -9.6227e-01,  1.4221e-02, -1.8411e-01,<NewLine>         -1.0946e-01, -3.8608e-01,  5.6576e-02,  5.7527e-02, -9.9113e-02,<NewLine>         -2.0707e-01, -1.5844e-01, -5.2088e-03, -3.3738e-01,  3.4554e-02,<NewLine>          1.0015e-02,  1.5227e-02, -6.7776e-01, -3.7915e-02, -7.8485e-02,<NewLine>          4.5949e-02, -2.5784e-02, -3.3047e-01,  6.4348e-02, -1.5789e-01,<NewLine>         -1.2264e-02, -1.7147e-01,  7.6571e-02, -3.0366e-02],<NewLine>        [-2.1560e-01, -3.1336e-01, -2.1208e-02,  2.4466e-02, -1.1601e-01,<NewLine>          7.1008e-02, -8.4347e-02, -2.1182e-02, -4.9379e-01, -4.9651e-01,<NewLine>          6.4734e-03, -4.3814e-02, -4.6654e-01,  6.1255e-03, -1.4655e-02,<NewLine>         -1.8655e-01, -9.6411e-01,  1.1566e-02,  4.6149e-02, -1.1270e-01,<NewLine>         -3.3037e-02, -4.3250e-02, -2.2556e-01, -3.3957e-01,  4.7250e-03,<NewLine>         -4.1394e-01, -6.5579e-01, -4.6811e-02, -8.6857e-02, -5.4740e-01,<NewLine>         -9.8245e-01,  2.1111e-01,  1.5543e-01, -3.3294e-02,  2.6496e-01,<NewLine>         -4.8543e-01, -3.9470e-01, -4.6092e-02, -2.2453e-01, -4.8163e-01,<NewLine>         -1.9894e-01,  4.2989e-02,  3.3548e-02,  3.9954e-01, -3.8888e-02,<NewLine>         -4.7950e-01, -5.6876e-01, -4.5290e-01, -4.2524e-02, -5.1943e-02,<NewLine>         -4.1490e-01, -1.6104e-02,  6.7158e-04, -4.7883e-01, -1.5516e-01,<NewLine>         -4.5682e-02, -6.8973e-02,  6.2119e-02, -4.4249e-01, -8.8247e-01,<NewLine>         -5.6976e-02, -1.2550e-01, -5.3172e-01, -5.2279e-02]], device='cuda:0')<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think it’s from ur evaluation code then.<br/><NewLine>Try saving and loading the model the normal way instead of as a pickle file and check if u still have same issue</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Henry_Chibueze; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/omar_bouhamed; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Artlind; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/omar_bouhamed; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Henry_Chibueze; <NewLine> ,"REPLY_DATE 1: September 11, 2020,  5:29pm; <NewLine> REPLY_DATE 2: September 11, 2020,  5:33pm; <NewLine> REPLY_DATE 3: September 11, 2020,  7:56pm; <NewLine> REPLY_DATE 4: September 11, 2020,  8:24pm; <NewLine> REPLY_DATE 5: September 12, 2020,  6:52am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
94895,DQN always gives same output regardless of input,2020-09-02T04:44:57.788Z,0,66,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am solving combinational optimization problem with DQN.<br/><NewLine>My goal is to reach the optimal state</p><NewLine><p>I just revised a little from the following pytorch DQN tutorials</p><NewLine><p><a class=""onebox"" href=""https://tutorials.pytorch.kr/intermediate/reinforcement_q_learning.html"" rel=""nofollow noopener"" target=""_blank"">https://tutorials.pytorch.kr/intermediate/reinforcement_q_learning.html</a></p><NewLine><pre><code class=""lang-auto""><NewLine>class DQN(nn.Module):<NewLine><NewLine>    def __init__(self, h, w, outputs):<NewLine>        super(DQN, self).__init__()<NewLine>        self.channel = 7<NewLine>        self.conv1 = nn.Conv2d(self.channel, 16, kernel_size=3, stride=2)<NewLine>        self.bn1 = nn.BatchNorm2d(16)<NewLine>        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)<NewLine>        self.bn2 = nn.BatchNorm2d(32)<NewLine>        self.conv3 = nn.Conv2d(32, 32, kernel_size=1, stride=2)<NewLine>        self.bn3 = nn.BatchNorm2d(32)<NewLine><NewLine><NewLine>        def conv2d_size_out(size, kernel_size=5, stride=2):<NewLine>            return (size - (kernel_size - 1) - 1) // stride + 1<NewLine><NewLine>        convw = 2<NewLine>        convh = 2<NewLine>        linear_input_size = convw * convh * 32<NewLine>        self.head = nn.Linear(linear_input_size, outputs)<NewLine><NewLine><NewLine>    def forward(self, x):<NewLine>        x = F.relu(self.bn1(self.conv1(x)))<NewLine>        x = F.relu(self.bn2(self.conv2(x)))<NewLine>        x = F.relu(self.bn3(self.conv3(x)))<NewLine>        return self.head(x.view(x.size(0), -1))<NewLine><NewLine></code></pre><NewLine><pre><code class=""lang-auto"">def select_action(state):<NewLine>    global steps_done<NewLine>    sample = random.random()<NewLine>    #sample = 1.0<NewLine>    eps_threshold = EPS_END + (EPS_START - EPS_END) * \<NewLine>        math.exp(-1. * steps_done / EPS_DECAY)<NewLine>    steps_done += 1<NewLine>    if sample &gt; eps_threshold:<NewLine>        with torch.no_grad():<NewLine>            # t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.<NewLine>            # 최대 결과의 두번째 열은 최대 요소의 주소값이므로,<NewLine>            # 기대 보상이 더 큰 행동을 선택할 수 있습니다.<NewLine>            return policy_net(state).max(1)[1].view(1, 1)<NewLine>    else:<NewLine>        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)<NewLine><NewLine><NewLine>def optimize_model():<NewLine>    if len(memory) &lt; BATCH_SIZE:<NewLine>        return<NewLine>    transitions = memory.sample(BATCH_SIZE)<NewLine><NewLine>    batch = Transition(*zip(*transitions))<NewLine><NewLine>    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,<NewLine>                                          batch.next_state)), device=device, dtype=torch.bool)<NewLine><NewLine>    non_final_next_states = torch.cat([s for s in batch.next_state<NewLine>                                                if s is not None])<NewLine>    state_batch = torch.cat(batch.state)<NewLine>    action_batch = torch.cat(batch.action)<NewLine>    reward_batch = torch.cat(batch.reward)<NewLine><NewLine><NewLine>    state_action_values = policy_net(state_batch).gather(1, action_batch)  # double DQN<NewLine>    <NewLine>    next_state_values = torch.zeros(BATCH_SIZE, device=device, dtype=torch.double)<NewLine>    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()<NewLine><NewLine><NewLine><NewLine>    expected_state_action_values = (next_state_values * GAMMA) + reward_batch<NewLine><NewLine><NewLine><NewLine>    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))<NewLine><NewLine>    optimizer.zero_grad()<NewLine>    loss.backward()<NewLine>    for param in policy_net.parameters():<NewLine>        param.grad.data.clamp_(-1, 1)<NewLine>    optimizer.step()<NewLine><NewLine></code></pre><NewLine><p>After some learning, all output gives same value regardless of input<br/><NewLine>So, I changed the reward to 0 for every action, but the situation was same</p><NewLine><p>Please give me an advice</p><NewLine></div>",https://discuss.pytorch.org/u/Jaelee,,Jaelee,"September 2, 2020,  4:46am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you try to reduce the learning rate ?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Artlind; <NewLine> ,"REPLY_DATE 1: September 11, 2020,  7:38pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
95654,Softmax giving nans and negative values as output,2020-09-09T11:19:20.122Z,1,56,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am using softmax at the end of my model.</p><NewLine><p>However after some training softmax is giving negative probability.In some situations I have encountered nans as probability as well.</p><NewLine><p>one solution i found on searching is to use normalized softmax…however I can not find any pytorch imlpementaion for this.</p><NewLine><p>Can someone please help to let know if there is a normalized softmax available or how to achieve this so that forward and backward propagations are smooth.</p><NewLine><p>Please note that I am already using torch.nn.utils.clip_grad_norm_(model.parameters(), 40) to avoid exploding gradients</p><NewLine></div>",https://discuss.pytorch.org/u/granth_jain,(granth jain),granth_jain,"September 9, 2020, 11:21am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The focus of pytorch is not on data processing. All related issues must be resolved with the help of third-party toolkits. You can use the StandardScaler of <code>scikit-learn</code>. Of course, you can also learn about <code>skorch</code> (usually you don’t need this package, just <code>scikit-learn</code>).</p><NewLine><p><a class=""onebox"" href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standardscaler#sklearn.preprocessing.StandardScaler"" rel=""nofollow noopener"" target=""_blank"">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standardscaler#sklearn.preprocessing.StandardScaler</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi.</p><NewLine><p>Thanks, I am trying to take a sigmoid before taking softmax…also can you please help to let me know what is the ideal value for clip in torch.nn.utils.clip_grad_norm_ while training an A3C</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, I am not familiar with the a3c algorithm you mentioned, and I cannot provide you with more help.But I found a newbie guide, hope it helps you.<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/beginner/transformer_tutorial.html</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>softmax is used normalized (max value subtracted), and negative softmax outputs are mathematically impossible. You should recheck what are you looking at. And maybe try LayerNorm(affine=true) or x.clamp_(-10.0,10.0) before softmax.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hhaoao; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/granth_jain; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/hhaoao; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/googlebot; <NewLine> ,"REPLY_DATE 1: September 9, 2020,  1:16pm; <NewLine> REPLY_DATE 2: September 9, 2020,  1:18pm; <NewLine> REPLY_DATE 3: September 9, 2020,  1:25pm; <NewLine> REPLY_DATE 4: September 9, 2020,  2:18pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
95578,"Dqn numpy only solve Pendulum, copying from pytorch",2020-09-08T16:24:38.606Z,0,44,"<div class=""post"" itemprop=""articleBody""><NewLine><p>dqn numpy only solve Pendulum<br/><NewLine></p><aside class=""onebox allowlistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""32""/><NewLine><a href=""https://github.com/Isawyou/dqn-numpy-only-not-working"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""420"" src=""https://avatars3.githubusercontent.com/u/59315334?s=400&amp;v=4"" width=""420""/><NewLine><h3><a href=""https://github.com/Isawyou/dqn-numpy-only-not-working"" rel=""nofollow noopener"" target=""_blank"">Isawyou/dqn-numpy-only-not-working</a></h3><NewLine><p>the second file is pytorch implementation, solves Pendulum-v0 in one minute, I copied every piece of code, still not working, is there something wrong ? contact me if there is a bug to solve Pendul...</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>",https://discuss.pytorch.org/u/okay_All_Right,(okay All Right),okay_All_Right,"September 8, 2020,  5:17pm",,,,,
59755, TypeError: &lsquo;NoneType&rsquo; object is not iterable,2019-11-01T08:32:32.904Z,2,1388,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to implement a Deep Q network (DQN) using a graph convolution network (GCN) using the dynamic graph library (DGL). The base code is taken from this repository (<a href=""https://github.com/louisv123/COLGE/blob/master/agent.py"" rel=""nofollow noopener"">https://github.com/louisv123/COLGE/blob/master/agent.py</a>). However after I calculate the loss between the policy network and target network and run loss.backward() I get  <code>TypeError: 'NoneType' object is not iterable</code> . I have printed the loss value and it is not None.</p><NewLine><p>I ran the original code from the repository and it is running perfectly. I have also implemented the GCN code in DGL and it seems to run. I have also visualized the graph using the torchviz. But I am unable to find why it is giving an error.</p><NewLine><p>The code snippet is given below:</p><NewLine><pre><code class=""lang-auto"">current_q_values= self.model(last_observation_tens, self.G)<NewLine>next_q_values=current_q_values.clone()<NewLine>current_q_values[range(self.minibatch_length),action_tens,:] = target<NewLine>L=self.criterion(current_q_values,next_q_values)<NewLine>print('loss:',L.item())<NewLine>self.optimizer.zero_grad()<NewLine>L.backward(retain_graph=True)<NewLine>self.optimizer.step()<NewLine></code></pre><NewLine><p>loss: 1461729.125</p><NewLine><pre><code class=""lang-auto"">  ---------------------------------------------------------------------------<NewLine>TypeError                                 Traceback (most recent call last)<NewLine>&lt;ipython-input-17-cd5e862dd609&gt; in &lt;module&gt;()<NewLine>     62 <NewLine>     63 if __name__ == ""__main__"":<NewLine>---&gt; 64     main()<NewLine><NewLine>7 frames<NewLine>&lt;ipython-input-17-cd5e862dd609&gt; in main()<NewLine>     55         print(""Running a single instance simulation..."")<NewLine>     56         my_runner = Runner(env_class, agent_class, args.verbose)<NewLine>---&gt; 57         final_reward = my_runner.loop(graph_dic,args.ngames,args.epoch, args.niter)<NewLine>     58         print(""Obtained a final reward of {}"".format(final_reward))<NewLine>     59         agent_class.save_model()<NewLine><NewLine>&lt;ipython-input-14-45cfc883a37b&gt; in loop(self, graphs, games, nbr_epoch, max_iter)<NewLine>     45                         # if self.verbose:<NewLine>     46                         #   print(""Simulation step {}:"".format(i))<NewLine>---&gt; 47                         (obs, act, rew, done) = self.step()<NewLine>     48                         action_list.append(act)<NewLine>     49 <NewLine><NewLine>&lt;ipython-input-14-45cfc883a37b&gt; in step(self)<NewLine>     16         #reward = torch.tensor([reward], device=device)<NewLine>     17 <NewLine>---&gt; 18         self.agent.reward(observation, action, reward,done)<NewLine>     19 <NewLine>     20         return (observation, action, reward, done)<NewLine><NewLine>&lt;ipython-input-16-76d612e8663c&gt; in reward(self, observation, action, reward, done)<NewLine>    129               print('loss:',L.item())<NewLine>    130               self.optimizer.zero_grad()<NewLine>--&gt; 131               L.backward(retain_graph=True)<NewLine>    132               self.optimizer.step()<NewLine>    133 <NewLine><NewLine>/usr/local/lib/python3.6/dist-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)<NewLine>    148                 products. Defaults to ``False``.<NewLine>    149        <NewLine>--&gt; 150         torch.autograd.backward(self, gradient, retain_graph, create_graph)<NewLine>    151 <NewLine>    152     def register_hook(self, hook):<NewLine><NewLine>/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)<NewLine>     97     Variable._execution_engine.run_backward(<NewLine>     98         tensors, grad_tensors, retain_graph, create_graph,<NewLine>---&gt; 99         allow_unreachable=True)  # allow_unreachable flag<NewLine>    100 <NewLine>    101 <NewLine><NewLine>/usr/local/lib/python3.6/dist-packages/torch/autograd/function.py in apply(self, *args)<NewLine>     75 <NewLine>     76     def apply(self, *args):<NewLine>---&gt; 77         return self._forward_cls.backward(self, *args)<NewLine>     78 <NewLine>     79 <NewLine><NewLine>/usr/local/lib/python3.6/dist-packages/dgl/backend/pytorch/tensor.py in backward(ctx, grad_out)<NewLine>    394     def backward(ctx, grad_out):<NewLine>    395         reducer, graph, target, in_map, out_map, in_data_nd, out_data_nd, degs \<NewLine>--&gt; 396             = ctx.backward_cache<NewLine>    397         ctx.backward_cache = None<NewLine>    398         grad_in = None<NewLine><NewLine>TypeError: 'NoneType' object is not iterable```<NewLine><NewLine>Kindly help.</code></pre><NewLine></div>",https://discuss.pytorch.org/u/abhinav_choudhury,(abhinav choudhury),abhinav_choudhury,"November 1, 2019,  8:10pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Do you get any more information from the stack trace?<br/><NewLine>Can you run with anomaly mode enabled <a href=""https://pytorch.org/docs/stable/autograd.html#anomaly-detection"" rel=""nofollow noopener"">doc</a> to see if you get a better stack trace?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for the reply.<br/><NewLine>I ran the code with anomaly mode enabled</p><NewLine><pre><code class=""lang-auto"">with autograd.detect_anomaly():<NewLine>              (last_observation_tens, action_tens, reward_tens, observation_tens)=self.get_sample()<NewLine>              target = reward_tens + self.gamma *torch.max(self.model_target(observation_tens, self.G) + observation_tens * (-1e5), dim=1)[0]<NewLine>              current_q_values= self.model(last_observation_tens, self.G)<NewLine>              next_q_values=current_q_values.clone()<NewLine>              current_q_values[range(self.minibatch_length),action_tens,:] = target<NewLine>              L=self.criterion(current_q_values,next_q_values)<NewLine>              print('loss:',L.item())<NewLine>              self.optimizer.zero_grad()<NewLine>              L.backward(retain_graph=True)<NewLine>              self.optimizer.step()'''<NewLine></code></pre><NewLine><p>and got the following output<br/><NewLine><code>/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:40: UserWarning: No forward pass information available. Enable detect anomaly during forward pass for more information.</code><br/><NewLine>How do I enable detect anomaly during forward pass?<br/><NewLine>I also updated the question with the full stack trace.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You seem to have it already available during the forward…<br/><NewLine>Can you provide a small code sample (30 lines) that reproduces this issue?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>The error is self-explanatory. You are trying to subscript an object which you think is a list or dict, but actually is None. This means that you tried to do:</p><NewLine><p><code>None[something]</code></p><NewLine><p>This  error means that you attempted to index an object that doesn’t have that functionality. You might have noticed that the method sort() that only modify the list have no return value printed – they return the default None. ‘NoneType’ object is not <a href=""http://net-informations.com/python/err/nonetype.htm"" rel=""nofollow noopener"">subscriptable</a> is the one thrown by python when you use the square bracket notation object[key] where an object doesn’t define the <strong>getitem</strong> method . This is a design principle for all mutable data structures in Python.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://360digitmg.com/python-typeerror-nonetype-object-is-not-subsriptable"" rel=""nofollow noopener"">TypeError: ‘NoneType’ object is not subscriptable</a><br/><NewLine>The error is self-explanatory. You are trying to subscript an object which is a None actually…</p><NewLine><p>Example 1</p><NewLine><p>list1=[5,1,2,6]        # Create a simple list<br/><NewLine>order=list1.sort()  # sort the elements in created list and store into another variable.<br/><NewLine>order[0]               # Trying to access the first element after sorting</p><NewLine><p>TypeError     Traceback (most recent call last)</p><NewLine><p>in ()<br/><NewLine>list1=[5,1,2,6]<br/><NewLine>order=list1.sort()<br/><NewLine>----&gt; order[0]</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/abhinav_choudhury; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/reekjohns; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Digi_TMG; <NewLine> ,"REPLY_DATE 1: November 1, 2019,  3:17pm; <NewLine> REPLY_DATE 2: November 1, 2019,  8:16pm; <NewLine> REPLY_DATE 3: November 1, 2019,  9:12pm; <NewLine> REPLY_DATE 4: July 27, 2020,  9:06am; <NewLine> REPLY_DATE 5: September 8, 2020, 10:41am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
95157,Action values goes to nan for all the actions while training A3C,2020-09-04T08:56:14.863Z,0,50,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using pytorch to make an A3C with 4 processes as in below code.</p><NewLine><p>But to my surprise while training the A3C action values goes to nan for all actions. Initially it was not the case that action values were gone to nan.</p><NewLine><p>But after an overnight training it goes to nan. Can someone please help to let me know what issue is there.</p><NewLine><pre><code class=""lang-auto"">class SharedAdam(torch.optim.Adam):<NewLine>    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,<NewLine>                 weight_decay=0):<NewLine>        super(SharedAdam, self).__init__(params, lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)<NewLine>        # State initialization<NewLine>        for group in self.param_groups:<NewLine>            for p in group['params']:<NewLine>                state = self.state[p]<NewLine>                state['step'] = 0<NewLine>                state['exp_avg'] = torch.zeros_like(p.data)<NewLine>                state['exp_avg_sq'] = torch.zeros_like(p.data)<NewLine>    <NewLine>                # share in memory<NewLine>                state['exp_avg'].share_memory_()<NewLine>                state['exp_avg_sq'].share_memory_()<NewLine><NewLine><NewLine><NewLine>class ActorCritic(torch.nn.Module):<NewLine><NewLine>    def __init__(self, num_inputs, action_space):<NewLine>        super(ActorCritic, self).__init__()<NewLine><NewLine>        self.num_inputs = num_inputs<NewLine>        self.action_space = action_space<NewLine>        self.lstm = nn.LSTMCell(num_inputs, num_inputs)<NewLine>        num_outputs = action_space<NewLine>        self.fc1 = nn.Linear(num_inputs, 256)<NewLine>        self.fc1.apply(init_weights)<NewLine>        self.fc2 = nn.Linear(256, 256)<NewLine>        self.fc2.apply(init_weights)<NewLine>        self.critic_linear = nn.Linear(256, 1)<NewLine>        self.critic_linear.apply(init_weights)<NewLine>        self.actor_linear = nn.Linear(256, num_outputs)<NewLine>        self.actor_linear.apply(init_weights)<NewLine>        self.lstm.bias_ih.data.fill_(0)<NewLine>        self.lstm.bias_hh.data.fill_(0)<NewLine>        self.sig1 = nn.Sigmoid()<NewLine>        self.train()<NewLine><NewLine>    def forward(self, inputs):<NewLine>        inputs, (hx, cx) = inputs<NewLine>        hx, cx = self.lstm(inputs, (hx, cx))<NewLine>        x = self.sig1(self.fc1(hx))<NewLine>        x = torch.tanh(self.fc2(x))<NewLine>        return self.critic_linear(x), self.actor_linear(x), (hx, cx)<NewLine>    <NewLine>    def save(self, filename, directory):<NewLine>        torch.save(self.state_dict(), '%s/%s_actor.pth' % (directory, filename))<NewLine><NewLine>    def load(self, filename, directory):<NewLine>            self.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))<NewLine></code></pre><NewLine><p>below is code for training</p><NewLine><pre><code class=""lang-auto"">def train(rank,params, model, optimizer,data):<NewLine>    try:<NewLine>        data = data.dropna()<NewLine><NewLine>        count = 0<NewLine><NewLine>        data = torch.DoubleTensor(np.asarray(data))<NewLine><NewLine>        env = ENV(params.state_dim, params.action_dim, data)<NewLine>        print(""env created\n"")<NewLine>        # init training variables<NewLine>        max_timesteps = data.shape[0] - 1<NewLine>        state = env.reset()<NewLine>        done = True<NewLine>        episode_length = 0<NewLine>        count = 0<NewLine>        while count&lt;max_timesteps-1:<NewLine>            episode_length += 1<NewLine>            if done:<NewLine>                cx = Variable(torch.zeros(1, params.state_dim))<NewLine>                hx = Variable(torch.zeros(1, params.state_dim))<NewLine>            else:<NewLine>                cx = Variable(cx.data)<NewLine>                hx = Variable(hx.data)<NewLine><NewLine>            values = []<NewLine>            log_probs = []<NewLine>            rewards = []<NewLine>            entropies = []<NewLine>            while count&lt;max_timesteps-1:<NewLine>                value, action_values, (hx, cx) = model((Variable(state.unsqueeze(0)), (hx, cx)))<NewLine>                prob = F.softmax(action_values,dim = -1)<NewLine>                log_prob = F.log_softmax(action_values, dim=-1).reshape(-1,)<NewLine>                entropy = -(log_prob * prob).sum(1, keepdim=True)<NewLine>                entropies.append(entropy)<NewLine>                <NewLine>                action = sample(prob)<NewLine>                <NewLine>                <NewLine>                log_prob = log_prob.gather(0, Variable(action))<NewLine>         <NewLine>                state, reward, done = env.step(action)<NewLine>                done = (done or count == max_timesteps-2)<NewLine>                reward = max(min(reward, 1), -1)<NewLine>                <NewLine>                count +=1<NewLine>                <NewLine>                if done:<NewLine>                    episode_length = 0<NewLine>                    state = env.reset()<NewLine>                    <NewLine>                <NewLine>                values.append(value)<NewLine>                log_probs.append(log_prob)<NewLine>                rewards.append(reward)<NewLine>                print(ticker, ""rank "",rank,"" action:"",action, ""reward "",reward)<NewLine><NewLine>                if done:<NewLine>                    break<NewLine>                <NewLine>            R = torch.zeros(1, 1)<NewLine>            if not done:<NewLine>                value, _, _ = model((Variable(state.unsqueeze(0)), (hx, cx)))<NewLine>                R = value.data<NewLine>            values.append(Variable(R))<NewLine>            policy_loss = 0<NewLine>            value_loss = 0<NewLine>            R = Variable(R)<NewLine>            gae = torch.zeros(1, 1)<NewLine>            for i in reversed(range(len(rewards))):<NewLine>                R = params.gamma * R + rewards[i]<NewLine>                advantage = R - values[i]<NewLine>                value_loss = value_loss + 0.5 * advantage.pow(2)<NewLine>                TD = rewards[i] + params.gamma * values[i + 1].data - values[i].data<NewLine>                gae = gae * params.gamma * params.tau + TD<NewLine>                policy_loss = policy_loss - log_probs[i] * Variable(gae) - 0.01 * entropies[i]<NewLine><NewLine>            optimizer.zero_grad()<NewLine>            (policy_loss + 0.5 * value_loss).backward()<NewLine>            torch.nn.utils.clip_grad_norm_(model.parameters(), 40)<NewLine>            optimizer.step()<NewLine>            <NewLine>    except:<NewLine>        traceback.print_exc()<NewLine></code></pre><NewLine><p>below is code for sampling an action</p><NewLine><pre><code class=""lang-auto"">def sample(logits):<NewLine>    noise = torch.rand(logits.shape)<NewLine>    return torch.argmax(logits - torch.log(-torch.log(noise)), 1)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/granth_jain,(granth jain),granth_jain,"September 4, 2020,  8:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Your <code>sample</code> function might return invalid outputs.<br/><NewLine><code>torch.rand</code> samples from <code>[0, 1)</code> and if you sample a zero in it, <code>torch.log</code> would return <code>-Inf</code>.<br/><NewLine>Try to add a small <code>eps</code> value to this operation or check other operations, which could potentially output an invalid value.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 8, 2020,  4:43pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
94971,Best sampling function for A3C prob,2020-09-02T19:46:23.408Z,2,66,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am confused in which is the best probability distribution sampling functions is best for training an A3C reinforcement learning model.</p><NewLine><p>May I get some advise of experience holders.</p><NewLine><p>Thanks,<br/><NewLine>Granth</p><NewLine></div>",https://discuss.pytorch.org/u/granth_jain,(granth jain),granth_jain,"September 2, 2020,  7:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I guess you are asking for sampling functions in the contiguous domain and not discrete domain?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying for discrete domain.</p><NewLine><p>Actually I have tried multinomial and categorical …but both gets stuck if they want to avoid negative rewads and try an action which does nothing.</p><NewLine><p>Can you please helps to let me know any sampling function that tries the low probability action as well.</p><NewLine><p>Also is it ok to sample a random action while training A3C.</p><NewLine><p>Thanks,<br/><NewLine>Granth</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The problem is that your network has converged and keep on outputing “invalid” actions, setting their probability to be high and “valid” ones to be low. You should check your implementation rather than blaming the distribution itself.</p><NewLine><p>A2C, A3C… these policy based methods relies on sampling from a distribution to calculate the needed log probability, it is not only “ok” but also “must”.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/granth_jain; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: September 3, 2020,  1:58am; <NewLine> REPLY_DATE 2: September 3, 2020,  6:02pm; <NewLine> REPLY_DATE 3: September 5, 2020,  6:30am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
94742,Updating multiple networks simultaneously on CPU,2020-08-31T22:51:41.611Z,2,99,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Currently working on moving an implementation of a couple popular reinforcement learning algorithms from TensorFlow to PyTorch, and the PyTorch code is noticeably slower (up to 50%). We believe it’s because some algorithms, such as Soft-Actor Critic, have multiple semi-independent neural networks (e.g. Policy, Value, Q Function) that during the update step need to be evaluated, their losses computed, and backpropagation performed on them. In TensorFlow these could all be computed with a single <code>session.run</code> call, and they would be parallelized across multiple cores. However, in PyTorch we are limited to evaluating and updating each network sequentially.</p><NewLine><p>Note that our networks are quite small and we don’t expect much benefit in running on GPU. However, the asynchronous execution in CUDA seems to alleviate a lot of the performance gap between TensorFlow, but at the end of the day we still need to support CPU.</p><NewLine><p>Was wondering what the “right” way to do this for PyTorch on CPU. I’ve played around with using Python threading to evaluate the networks (PyTorch does release the GIL, right?) as well as changing the <code>num_threads</code> and <code>KMP_BLOCKTIME</code> settings, with some success, but am still at the 50% performance gap. Guidance would be much appreciated, thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/ervteng,,ervteng,"August 31, 2020, 10:51pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Show your implementation.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here is the pseudo-code of the model update (soft actor-critic):</p><NewLine><pre><code class=""lang-auto""># Sample observations, dones, rewards from experience replay buffer<NewLine>observations, next_observations, dones, rewards = sample_batch()<NewLine><NewLine># Evaluate current policy on sampled observations<NewLine>(<NewLine>    sampled_actions,<NewLine>    log_probs,<NewLine>    entropies,<NewLine>    sampled_values,<NewLine>) = policy.sample_actions(observations)<NewLine><NewLine># Evaluate Q networks on observations and actions<NewLine>q1p_out, q2p_out = value_network(observations, sampled_actions)<NewLine>q1_out, q2_out = value_network(observations, actions)<NewLine><NewLine># Evaluate target network on next observations<NewLine>with torch.no_grad():<NewLine>    target_values = target_network(next_observations)<NewLine><NewLine># Evaluate losses<NewLine>q1_loss, q2_loss = sac_q_loss(q1_out, q2_out, target_values, dones, rewards)<NewLine>value_loss = sac_value_loss(log_probs, sampled_values, q1p_out, q2p_out)<NewLine>policy_loss = sac_policy_loss(log_probs, q1p_out)<NewLine>entropy_loss = sac_entropy_loss(log_probs)<NewLine><NewLine>total_value_loss = q1_loss + q2_loss + value_loss<NewLine><NewLine># Backprop and weights update<NewLine>policy_optimizer.zero_grad()<NewLine>policy_loss.backward()<NewLine>policy_optimizer.step()<NewLine><NewLine>value_optimizer.zero_grad()<NewLine>total_value_loss.backward()<NewLine>value_optimizer.step()<NewLine><NewLine>entropy_optimizer.zero_grad()<NewLine>entropy_loss.backward()<NewLine>entropy_optimizer.step()<NewLine></code></pre><NewLine><p>I’m looking for a way to parallelize the <code>value_network</code>, <code>sample_actions</code>, and <code>target_network</code> evaluations, as well as the three <code>backward()</code> calls, as these components consume the most amount of time. I’ve tried running each in different Python threads, but probably a combination of the GIL and the context-switching overhead seems to negate any possible speedup.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If your model is small, you can consider about using <code>torch.jit._fork</code> and <code>torch.jit._wait</code>, they can work around the GIL problem, but also requires to convert your models to <code>JitScriptModule</code>.</p><NewLine><p>While pytorch does release GIL, it could still affect model performance if your model consists of many small linear layers like 100(in)-&gt;100(out).</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>The <code>jit._fork</code> seems promising, thanks! It wasn’t clear to me from the documentation, but does the fork and wait have to be called within a <code>JitScriptModule</code> or can it be called <em>on</em> functions from <code>JitScriptModule</code>s in regular Python code?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Compile a model to <code>JitScriptModule</code> like this:</p><NewLine><pre><code class=""lang-auto"">jit_actor = t.jit.script(acc)<NewLine></code></pre><NewLine><p>and call it like this:</p><NewLine><pre><code class=""lang-auto"">future = [t.jit._fork(model, *args_list) for model, args_list in zip(models, args_lists)]<NewLine></code></pre><NewLine><p>Then asynchronously wait for results:</p><NewLine><pre><code class=""lang-auto"">result = [t.jit._wait(fut) for fut in future]<NewLine></code></pre><NewLine><p>These APIs work outside of your JIT module.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ervteng; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ervteng; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: September 1, 2020,  2:01am; <NewLine> REPLY_DATE 2: September 2, 2020, 12:54am; <NewLine> REPLY_DATE 3: September 2, 2020,  2:15am; <NewLine> REPLY_DATE 4: September 2, 2020,  5:59pm; <NewLine> REPLY_DATE 5: September 3, 2020,  1:57am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
94372,Conv module in forward can&rsquo;t alloc,2020-08-28T01:58:04.578Z,17,172,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, here is the problem description:<br/><NewLine>I’m training an IMPALA agent on a GPU with allocated memory 113000MB, and every time I train over 1.1M frames I got this error:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/15b84947e88687e777d984ad28ac710f1ac9f83d"" href=""https://discuss.pytorch.org/uploads/default/original/3X/1/5/15b84947e88687e777d984ad28ac710f1ac9f83d.png"" title=""Screen Shot 2020-08-28 at 11.49.58 am""><img alt=""Screen Shot 2020-08-28 at 11.49.58 am"" data-base62-sha1=""368RdwpapXnBRKpt0JzfxbKXfzv"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/1/5/15b84947e88687e777d984ad28ac710f1ac9f83d_2_10x10.png"" height=""228"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/1/5/15b84947e88687e777d984ad28ac710f1ac9f83d_2_690x228.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/1/5/15b84947e88687e777d984ad28ac710f1ac9f83d_2_690x228.png, https://discuss.pytorch.org/uploads/default/optimized/3X/1/5/15b84947e88687e777d984ad28ac710f1ac9f83d_2_1035x342.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/1/5/15b84947e88687e777d984ad28ac710f1ac9f83d_2_1380x456.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screen Shot 2020-08-28 at 11.49.58 am</span><span class=""informations"">1754×580 101 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine>It didn’t show something like “Cuda out of memory error”, but just a RUNTIME Error: Can’t alloc. The feature_extraction model contains a six-layer Conv2d net.<br/><NewLine>I wonder if my model is too big for scalable training? What can I do to reduce the memory usage?</p><NewLine></div>",https://discuss.pytorch.org/u/Shizhe_Cai,(Shizhe Cai),Shizhe_Cai,"August 28, 2020,  1:58am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is a CPU memory bad allocation problem, the screensho is not enough to determine the problem root.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/iffix"">@iffiX</a> Thanks for the problem identification! I’m new to here and only can upload one image at a time <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/> . Can you give me some hints to to determine the problem root? I currently have 40 actors to run and fill the experience buffer</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is showing all of your code applicable?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Unfortunately, the code is too large, however, I can show you the code of creating actors and batch_and_learn in each thread:<br/><NewLine>here is the actor initialization:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/edbe9925cd0427de7ec5c9374433cd448d6a94d2"" href=""https://discuss.pytorch.org/uploads/default/original/3X/e/d/edbe9925cd0427de7ec5c9374433cd448d6a94d2.png"" title=""Screen Shot 2020-08-28 at 12.53.37 pm""><img alt=""Screen Shot 2020-08-28 at 12.53.37 pm"" data-base62-sha1=""xVbvWUWZKdoiywSNPlvKaoD9BaG"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/d/edbe9925cd0427de7ec5c9374433cd448d6a94d2_2_10x10.png"" height=""375"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/d/edbe9925cd0427de7ec5c9374433cd448d6a94d2_2_428x375.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/d/edbe9925cd0427de7ec5c9374433cd448d6a94d2_2_428x375.png, https://discuss.pytorch.org/uploads/default/optimized/3X/e/d/edbe9925cd0427de7ec5c9374433cd448d6a94d2_2_642x562.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/e/d/edbe9925cd0427de7ec5c9374433cd448d6a94d2_2_856x750.png 2x"" width=""428""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screen Shot 2020-08-28 at 12.53.37 pm</span><span class=""informations"">1086×950 75.6 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>And here is the batch and learn in each thread<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/96b2231a6195e3f610846a0dfc6f7d0c07463463"" href=""https://discuss.pytorch.org/uploads/default/original/3X/9/6/96b2231a6195e3f610846a0dfc6f7d0c07463463.png"" title=""Screen Shot 2020-08-28 at 12.55.08 pm""><img alt=""Screen Shot 2020-08-28 at 12.55.08 pm"" data-base62-sha1=""lv7hd3mrWsnl7CBSo5ZbZRzZ8bN"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/6/96b2231a6195e3f610846a0dfc6f7d0c07463463_2_10x10.png"" height=""375"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/6/96b2231a6195e3f610846a0dfc6f7d0c07463463_2_422x375.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/6/96b2231a6195e3f610846a0dfc6f7d0c07463463_2_422x375.png, https://discuss.pytorch.org/uploads/default/optimized/3X/9/6/96b2231a6195e3f610846a0dfc6f7d0c07463463_2_633x562.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/9/6/96b2231a6195e3f610846a0dfc6f7d0c07463463_2_844x750.png 2x"" width=""422""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screen Shot 2020-08-28 at 12.55.08 pm</span><span class=""informations"">1510×1340 243 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine>(I still can only upload one image at a time <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/> )</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>There doesn’t seem to be anything wrong in the showed part. I think you may try to decrease the the local /global replay buffer size in your implementation, /or skip learning &amp; rolling out by appending radomly generated tensor data into your buffer and see when they will overflow, and track down the distributed bug.</p><NewLine><p>If you would like to have a correct implementation of IMPALA as reference, I have written a tested one before.<br/><NewLine>Doc here:<br/><NewLine><a class=""onebox"" href=""https://machin.readthedocs.io/en/latest/tutorials/unleash_distributed_power.html#impala"" rel=""nofollow noopener"" target=""_blank"">https://machin.readthedocs.io/en/latest/tutorials/unleash_distributed_power.html#impala</a><br/><NewLine>and source code here:<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/iffiX/machin/blob/master/machin/frame/algorithms/impala.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/iffiX/machin/blob/master/machin/frame/algorithms/impala.py"" rel=""nofollow noopener"" target=""_blank"">iffiX/machin/blob/master/machin/frame/algorithms/impala.py</a></h4><NewLine><pre><code class=""lang-py"">from typing import Union, Dict, List, Tuple, Callable, Any<NewLine>import numpy as np<NewLine>import torch as t<NewLine>import torch.nn as nn<NewLine><NewLine>from machin.frame.buffers.buffer_d import Transition, DistributedBuffer<NewLine>from machin.model.nets.base import NeuralNetworkModule<NewLine>from .base import TorchFramework<NewLine>from .utils import safe_call<NewLine><NewLine>from machin.parallel.server import PushPullModelServer<NewLine>from machin.parallel.distributed import RpcGroup<NewLine><NewLine><NewLine>def _make_tensor_from_batch(batch: List[Any], device, concatenate):<NewLine>    """"""<NewLine>    Used to convert compact every attribute of every step of a whole episode<NewLine>    into a single tensor.<NewLine><NewLine>    Args:<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/iffiX/machin/blob/master/machin/frame/algorithms/impala.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/shizhe_cai"">@Shizhe_Cai</a> you could copy paste your code between triple backward ticks instead of uploading a screenshot, Something like this -</p><NewLine><pre><code class=""lang-auto"">your entire code here, this would make it more readable...<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/iffix"">@iffiX</a> Thank you, I will try to reduce the buffer size at first. <a class=""mention"" href=""/u/a_d"">@a_d</a> My implementation is actually based on the TorchBeast implementation<br/><NewLine><a href=""https://github.com/facebookresearch/torchbeast/blob/master/torchbeast/monobeast.py"" rel=""nofollow noopener"">https://github.com/facebookresearch/torchbeast/blob/master/torchbeast/monobeast.py</a><br/><NewLine>, all I’ve modified is that I added some new models to train.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry to bother you but I still didn’t solve the problem. I tried to half the buffer size but it still cause alloc error when reaching 1.1M frames.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>The allocation error could be a leak at anywhere, I suppose that you should remove training &amp; sampling completely, and just stuff random data into your system and test its stability.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi iffi,<br/><NewLine>I think I’ve located the problem, but I don’t know how to fix it. As you know the IMPLALA model has actors and learners. I’m using the torchbeast implementation called monobeast: that run actors on a number of processes on CPU, and do the learning part on GPU. There are some drawbacks of this pattern, which also indicated in the torchbeast paper: the model require a large amount of shared memory between processes, and do the model eval and environment.step on cpu too. And their are some copies of tensors that is unnecessary. So as the frame number goes large, more and more memories were eaten by some unused tensors. That’s why no matter what buffer size and number of actors i use, the model always got an error when training for about 1.1M frames, there is no room for the actor model to do the forward operation.</p><NewLine><p>I wonder how I can free up those memory/ I’ve tried del output, and del loss. the limit increase to 1.3M frames but that’s it. here is the part of fill in the buffer, followed by the part of withdrawing the buffer.<br/><NewLine>`def act(i: int, free_queue: mp.SimpleQueue, full_queue: mp.SimpleQueue,<br/><NewLine>model: torch.nn.Module, buffers: Buffers,<br/><NewLine>episode_state_count_dict: dict, train_state_count_dict: dict,<br/><NewLine>initial_agent_state_buffers, flags):</p><NewLine><pre><code>try:<NewLine>    log.info('Actor %i started.', i)<NewLine>    timings = prof.Timings()<NewLine><NewLine>    gym_env = create_env(flags)<NewLine><NewLine>    if flags.num_input_frames &gt; 1:<NewLine>        gym_env = FrameStack(gym_env, flags.num_input_frames)<NewLine><NewLine>    if 'procgen' in flags.env:<NewLine>        env = ProcGenEnvironment(gym_env, flags.start_level, flags.num_levels, flags.distribution_mode)<NewLine>    else:<NewLine>        seed = i ^ int.from_bytes(os.urandom(4), byteorder='little')<NewLine>        gym_env.seed(seed)<NewLine>        env = Environment(gym_env, fix_seed=flags.fix_seed, env_seed=flags.env_seed)<NewLine><NewLine>    env_output = env.initial()<NewLine>    agent_state = model.initial_state(batch_size=1)<NewLine>    agent_output, unused_state = model(env_output, agent_state)<NewLine><NewLine>    while True:<NewLine>        index = free_queue.get()<NewLine>        if index is None:<NewLine>            break<NewLine><NewLine>        # Write old rollout end.<NewLine>        for key in env_output:<NewLine>            buffers[key][index][0, ...] = env_output[key]<NewLine>        for key in agent_output:<NewLine>            buffers[key][index][0, ...] = agent_output[key]<NewLine>        for i, tensor in enumerate(agent_state):<NewLine>            initial_agent_state_buffers[index][i][...] = tensor<NewLine><NewLine><NewLine>        # Update the episodic state counts<NewLine>        episode_state_key = tuple(env_output['frame'].view(-1).tolist())<NewLine>        if episode_state_key in episode_state_count_dict:<NewLine>            episode_state_count_dict[episode_state_key] += 1<NewLine>        else:<NewLine>            episode_state_count_dict.update({episode_state_key: 1})<NewLine>        buffers['episode_state_count'][index][0, ...] = \<NewLine>            torch.tensor(1 / np.sqrt(episode_state_count_dict.get(episode_state_key)))<NewLine>        <NewLine>        # Reset the episode state counts when the episode is over<NewLine>        if env_output['done'][0][0]:<NewLine>            for episode_state_key in episode_state_count_dict:<NewLine>                episode_state_count_dict = dict()<NewLine><NewLine>        # Update the training state counts<NewLine>        train_state_key = tuple(env_output['frame'].view(-1).tolist())<NewLine>        if train_state_key in train_state_count_dict:<NewLine>            train_state_count_dict[train_state_key] += 1<NewLine>        else:<NewLine>            train_state_count_dict.update({train_state_key: 1})<NewLine>        buffers['train_state_count'][index][0, ...] = \<NewLine>            torch.tensor(1 / np.sqrt(train_state_count_dict.get(train_state_key)))<NewLine><NewLine>        # delete output<NewLine>        del agent_output<NewLine><NewLine>        # Do new rollout<NewLine>        for t in range(flags.unroll_length):<NewLine>            timings.reset()<NewLine><NewLine>            with torch.no_grad():<NewLine>                agent_output, agent_state = model(env_output, agent_state)<NewLine><NewLine>            timings.time('model')<NewLine><NewLine>            env_output = env.step(agent_output['action'])<NewLine><NewLine>            timings.time('step')<NewLine><NewLine>            for key in env_output:<NewLine>                buffers[key][index][t + 1, ...] = env_output[key]<NewLine><NewLine>            for key in agent_output:<NewLine>                buffers[key][index][t + 1, ...] = agent_output[key]<NewLine>            <NewLine>            # Update the episodic state counts<NewLine>            episode_state_key = tuple(env_output['frame'].view(-1).tolist())<NewLine>            if episode_state_key in episode_state_count_dict:<NewLine>               episode_state_count_dict[episode_state_key] += 1<NewLine>            else:<NewLine>                episode_state_count_dict.update({episode_state_key: 1})<NewLine>            buffers['episode_state_count'][index][t + 1, ...] = \<NewLine>                torch.tensor(1 / np.sqrt(episode_state_count_dict.get(episode_state_key)))<NewLine><NewLine>            # Reset the episode state counts when the episode is over<NewLine>            if env_output['done'][0][0]:<NewLine>                episode_state_count_dict = dict()<NewLine><NewLine>            # Update the training state counts<NewLine>            train_state_key = tuple(env_output['frame'].view(-1).tolist())<NewLine>            if train_state_key in train_state_count_dict:<NewLine>                train_state_count_dict[train_state_key] += 1<NewLine>            else:<NewLine>                train_state_count_dict.update({train_state_key: 1})<NewLine>            buffers['train_state_count'][index][t + 1, ...] = \<NewLine>                torch.tensor(1 / np.sqrt(train_state_count_dict.get(train_state_key)))<NewLine><NewLine>            timings.time('write')<NewLine>        full_queue.put(index)<NewLine><NewLine>    if i == 0:<NewLine>        log.info('Actor %i: %s', i, timings.summary())<NewLine><NewLine>except KeyboardInterrupt:<NewLine>    pass  <NewLine>except Exception as e:<NewLine>    logging.error('Exception in worker process %i', i)<NewLine>    traceback.print_exc()<NewLine>    print()<NewLine>    raise e`<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">def get_batch(free_queue: mp.SimpleQueue,<NewLine>              full_queue: mp.SimpleQueue,<NewLine>              buffers: Buffers,<NewLine>              initial_agent_state_buffers,<NewLine>              flags,<NewLine>              timings,<NewLine>              lock=threading.Lock()):<NewLine>    with lock:<NewLine>        timings.time('lock')<NewLine>        indices = [full_queue.get() for _ in range(flags.batch_size)]<NewLine>        timings.time('dequeue')<NewLine>    batch = {<NewLine>        key: torch.stack([buffers[key][m] for m in indices], dim=1)<NewLine>        for key in buffers<NewLine>    }<NewLine>    initial_agent_state = (<NewLine>        torch.cat(ts, dim=1)<NewLine>        for ts in zip(*[initial_agent_state_buffers[m] for m in indices])<NewLine>    )<NewLine>    timings.time('batch')<NewLine>    for m in indices:<NewLine>        free_queue.put(m)<NewLine>    timings.time('enqueue')<NewLine>    batch = {<NewLine>        k: t.to(device=flags.device, non_blocking=True)<NewLine>        for k, t in batch.items()<NewLine>    }<NewLine>    initial_agent_state = tuple(t.to(device=flags.device, non_blocking=True)<NewLine>                                for t in initial_agent_state)<NewLine>    timings.time('device')<NewLine>    return batch, initial_agent_state<NewLine></code></pre><NewLine><p>Is there a way I can free those used buffer? Cause to my understanding, memory usage consistently increasing does not make sense right?</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Pytorch is not quite clear about this part, in fact, in this previously asked question (by me):<br/><NewLine><aside class=""quote"" data-post=""5"" data-topic=""94380""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/alband/40/215_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/pytorch-cpu-memory-usage/94380/5"">Pytorch cpu memory usage</a><NewLine></div><NewLine><blockquote><NewLine>    Hi, <NewLine>The allocator I mention here is not from pytorch but from your libc (we have no control over it). These are classic CPU allocators. Famous alternatives include jemalloc or tmalloc. But I haven’t tested them myself.<NewLine>  </blockquote><NewLine></aside><NewLine><br/><NewLine>In come cases (my implementation 2), even if the tensors are dereferenced, their memory are still not freed immediately after dereferencing, for more deep down implementation information, maybe you could consider asking the pytorch development member <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> and <a class=""mention"" href=""/u/alband"">@albanD</a>.</p><NewLine><p>I have not tried the jemalloc solution yet, but it is easy enough to install it by simply making it like:</p><NewLine><pre><code class=""lang-auto"">wget https://github.com/jemalloc/jemalloc/releases/download/5.0.1/jemalloc-5.0.1.tar.bz2<NewLine>tar -jxvf jemalloc-5.0.1.tar.bz2<NewLine>cd jemalloc-5.0.1<NewLine>sudo apt-get install autogen autoconf<NewLine><NewLine>./autogen.sh<NewLine>make -j2<NewLine>sudo make install<NewLine>sudo ldconfig<NewLine>cd ../<NewLine>rm -rf jemalloc-5.0.1 jemalloc-5.0.1.tar.bz2<NewLine></code></pre><NewLine><p>then export <code>LD_PRELOAD</code> environment variable before running your program:</p><NewLine><pre><code class=""lang-auto"">LD_PRELOAD=/home/mingfeim/packages/jemalloc-5.2.0/lib/libjemalloc.so ./your_script.sh<NewLine></code></pre><NewLine><p>Give it a try?</p><NewLine><p>Reference:<br/><NewLine><aside class=""onebox allowlistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://common.cnblogs.com/favicon.ico?v=20200522"" width=""32""/><NewLine><a href=""https://www.cnblogs.com/evenleee/p/11957076.html"" rel=""nofollow noopener"" target=""_blank"">cnblogs.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail"" height=""20"" src="""" width=""20""/><NewLine><h3><a href=""https://www.cnblogs.com/evenleee/p/11957076.html"" rel=""nofollow noopener"" target=""_blank"">Linux安装jemalloc笔记 - evenleo - 博客园</a></h3><NewLine><p>前言 最近研究一个工具库需要用 jemalloc 做内存分配器，但在 ubuntu 下安装过程中遇到很多问题，故记下安装过程的笔记，避免以后遇到在这上面浪费时间。 安装过程 环境：VMware Ubu</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine><aside class=""onebox allowlistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://static.zhihu.com/static/favicon.ico"" width=""32""/><NewLine><a href=""https://zhuanlan.zhihu.com/p/79989669"" rel=""nofollow noopener"" target=""_blank"">知乎专栏</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail"" height=""20"" src="""" width=""20""/><NewLine><h3><a href=""https://zhuanlan.zhihu.com/p/79989669"" rel=""nofollow noopener"" target=""_blank"">PyTorch在CPU上的一些Performance BKM</a></h3><NewLine><p>这里简单介绍一下用PyTorch在CPU上的一些性能相关的BKM。内容以inference为主，毕竟CPU上主要的场景还是inference；另外这里CPU都指的是Intel Xeon.gist里面写了英文版的，内容和这里的基本相当： General guideli…</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Really appreciate your suggestion. Actually I’m using the HPC in my University and submit the slurm script to get the job run on a node. So I don’t know if I’m able to using jemalloc in this case. But again thank you for the idea!</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see, then it would be very difficult regarding subitting a precompiled library or compile remotely on nodes, I guess you will need to tweak the framework code instead. Or you could change an implementation.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> and <a class=""mention"" href=""/u/alband"">@albanD</a> It would be awesome if you can share some ideas about this.<br/><NewLine>One more update: I’m using the procgen environment, for the Coinrun, I can only train 1.1M frames. But I trained the same model on the Maze environment, it reached 5M frames without error</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>And could you please have a look at another problem I posted, its about the behaviour of Multinomial with input inf and nan and &lt;0 value in difference torch version.<a href=""https://discuss.pytorch.org/t/torch-1-6-0-runtimeerror-probability-tensor-contains-either-inf-nan-or-element-0-but-good-with-torch-1-1-0/94570"">Torch 1.6.0 RuntimeError: probability tensor contains either <code>inf</code>, <code>nan</code> or element &lt; 0, But good with Torch 1.1.0</a></p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am not sure this is the same. The issue you have should never lead to OOM.<br/><NewLine>The memory is still available for this process and can be re-used. It is more an issue that it makes the memory not available for other processes and confuses reporting tools.</p><NewLine><p>I am really not sure where this errors can come from though. I am pretty sure this is not in our own code. So maybe cuda-related stuff?<br/><NewLine>Do you actually run out of RAM while the program runs?</p><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m using the HPC system, and I even crash two nodes because of the OOM. So I’m pretty sure that the memory out of usage.</p><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>If the whole node crashes, then the stack trace is most likely not very representative: it just points to whatever it was executing when the node ran out of memory.<br/><NewLine>I think you should try and reproduce this on your local machine, with a smaller dataset/model and see if you can reproduce the memory increase.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Shizhe_Cai; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Shizhe_Cai; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Shizhe_Cai; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/a_d; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Shizhe_Cai; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Shizhe_Cai; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/Shizhe_Cai; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/Shizhe_Cai; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/Shizhe_Cai; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/Shizhe_Cai; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/Shizhe_Cai; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: August 28, 2020,  2:38am; <NewLine> REPLY_DATE 2: August 28, 2020,  2:45am; <NewLine> REPLY_DATE 3: August 28, 2020,  2:45am; <NewLine> REPLY_DATE 4: August 28, 2020,  2:55am; <NewLine> REPLY_DATE 5: August 28, 2020,  2:56am; <NewLine> REPLY_DATE 6: August 28, 2020,  3:06am; <NewLine> REPLY_DATE 7: August 28, 2020,  3:08am; <NewLine> REPLY_DATE 8: August 28, 2020,  3:28am; <NewLine> REPLY_DATE 9: August 29, 2020,  3:13am; <NewLine> REPLY_DATE 10: August 29, 2020,  3:15am; <NewLine> REPLY_DATE 11: August 30, 2020,  9:44am; <NewLine> REPLY_DATE 12: August 30, 2020,  1:44pm; <NewLine> REPLY_DATE 13: August 30, 2020,  2:19pm; <NewLine> REPLY_DATE 14: August 30, 2020,  2:22pm; <NewLine> REPLY_DATE 15: August 30, 2020,  2:23pm; <NewLine> REPLY_DATE 16: August 30, 2020,  2:26pm; <NewLine> REPLY_DATE 17: August 31, 2020,  3:38pm; <NewLine> REPLY_DATE 18: September 1, 2020,  2:32am; <NewLine> REPLY_DATE 19: September 1, 2020,  1:59pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> REPLY 18 LIKES: ; <NewLine> REPLY 19 LIKES: ; <NewLine> 
52822,Does ModuleList behaves differently from Sequence,2019-08-07T21:31:08.413Z,6,230,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there,</p><NewLine><p>I was trying to implement a A2C model to train one of the OpenGym project.</p><NewLine><p>I created two models that are identical to me in terms of structure and forward logic. The main difference between the two models is that one is created using ModulelList with sequence wrapping inside while the other one is using Sequence. However, only the Sequence implementation is learning.</p><NewLine><h3>Model One</h3><NewLine><pre><code class=""lang-auto"">torch.manual_seed(999)<NewLine>base_h_num = 0<NewLine>actor_h_num = 1 <NewLine>critic_h_num = 1<NewLine>act_size = 4<NewLine>input_dim = 33<NewLine><NewLine>class A2C_model(nn.Module):<NewLine>    def __init__(self, input_dim, act_size):<NewLine>        super().__init__()<NewLine>        self.input_dim = input_dim<NewLine>        self.act_size = act_size<NewLine>        self.base = self.create_base(self.input_dim)<NewLine>        self.mu = self.create_actor()<NewLine>        self.val = self.create_critic()<NewLine>        self.std = nn.Parameter(torch.ones(1, act_size))<NewLine>  <NewLine>    def create_base(self, input_dim):<NewLine>        module_list = nn.ModuleList()<NewLine>        layer = nn.Sequential()<NewLine>        fc = nn.Linear(input_dim, 128)<NewLine>        layer.add_module(f""fc_layer_1"", fc)<NewLine>        layer.add_module(f""RELU_layer_1"", nn.ReLU())<NewLine>        module_list.append(layer)<NewLine>        self.add_hidden_layer(module_list, base_h_num,128, 128)<NewLine>        return module_list<NewLine><NewLine>    def create_actor(self):<NewLine>        module_list = nn.ModuleList()<NewLine>        layer = nn.Sequential()<NewLine>        self.add_hidden_layer(module_list, actor_h_num, 128, 128)<NewLine>        module_list.append(nn.Sequential(nn.Linear(128, self.act_size)))<NewLine>        return module_list<NewLine>    <NewLine>    def create_critic(self):<NewLine>        module_list = nn.ModuleList()<NewLine>        layer = nn.Sequential()<NewLine>        self.add_hidden_layer(module_list, critic_h_num, 128, 128)<NewLine>        module_list.append(nn.Sequential(nn.Linear(128, 1)))<NewLine>        return module_list<NewLine>    <NewLine>    <NewLine>    def add_hidden_layer(self, module_list, num_hidden_layer,<NewLine>                         input_dim, output_dim):<NewLine>        if num_hidden_layer == 0:<NewLine>            return<NewLine>        for i in range(1, num_hidden_layer+1):<NewLine>            layer = nn.Sequential()<NewLine>            fc = nn.Linear(input_dim, output_dim)<NewLine>            layer.add_module(f""fc_layer_{i}"", fc)<NewLine>            layer.add_module(f""RELU_layer_{i}"", nn.ReLU())<NewLine>            module_list.append(layer)<NewLine><NewLine>    def forward(self, x):<NewLine>        for b in self.base:<NewLine>            x = b(x)<NewLine>        mu = x<NewLine>        for m in self.mu:<NewLine>            mu = m(mu)<NewLine>        dist = torch.distributions.Normal(mu, self.std)<NewLine>        actions = dist.sample()     <NewLine>        log_prob = dist.log_prob(actions)<NewLine>        for v in self.val:<NewLine>            x = v(x)<NewLine>        return torch.clamp(actions, -1, 1), log_prob, x<NewLine></code></pre><NewLine><h3>Model 2</h3><NewLine><pre><code class=""lang-auto"">class A2C_model(nn.Module):<NewLine>    def __init__(self, input_dim , act_size):<NewLine>        super(ActorCriticNetwork, self).__init__()<NewLine>        self.fc1 = nn.Linear(input_dim , 128)<NewLine>        self.actor_fc = nn.Linear(128, 128)<NewLine>        self.actor_out = nn.Linear(128, act_size)<NewLine>        self.std = nn.Parameter(torch.ones(1, act_size))<NewLine>        self.critic_fc = nn.Linear(128, 128)<NewLine>        self.critic_out = nn.Linear(128, 1)<NewLine>        <NewLine>    def forward(self, state):<NewLine>        x = F.relu(self.fc1(state))<NewLine>        mean = self.actor_out(F.relu(self.actor_fc(x)))<NewLine>        dist = torch.distributions.Normal(mean, self.std)<NewLine>        action = dist.sample()<NewLine>        log_prob = dist.log_prob(action)<NewLine>        value = self.critic_out(F.relu(self.critic_fc(x)))<NewLine>        return torch.clamp(action, -1, 1), log_prob, value<NewLine></code></pre><NewLine><p>I created the ModuleList version is to play around with number of hidden layers and it will be easier to make changes to hidden layers. However, it performs very badly as compared to the Sequence implementation.</p><NewLine><p>I have tried different seeds but the ModuleList version has no luck. This really makes me wonder what I have done wrong. I hope someone can help me what is root cause of this descrepancy so I won’t make the same mistake again. Cheers!</p><NewLine><p>I have been trying to figure this thing out for days really appreciate if someone can help me!</p><NewLine><p>Cheers!</p><NewLine></div>",https://discuss.pytorch.org/u/Pengbo_Ma,(Jayden),Pengbo_Ma,"August 9, 2019,  1:57pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s a bit strange, as both models yield the same output and get the same gradients, if you initialize them with the same values and set the seed before calling the <code>forward</code> method:</p><NewLine><pre><code class=""lang-python"">model1 = A2C_model(input_dim, act_size)<NewLine>model2 = A2C_model2(input_dim, act_size)<NewLine><NewLine>with torch.no_grad():<NewLine>    for param1, param2 in zip(model1.parameters(), model2.parameters()):<NewLine>        param1.copy_(param2)<NewLine><NewLine><NewLine>x = torch.randn(1, input_dim)<NewLine>torch.manual_seed(2809)<NewLine>output1 = model1(x)<NewLine>torch.manual_seed(2809)<NewLine>output2 = model2(x)<NewLine>print(output1)<NewLine>print(output2)<NewLine><NewLine>output1[2].backward()<NewLine>output2[2].backward()<NewLine><NewLine>for param1, param2 in zip(model1.parameters(), model2.parameters()):<NewLine>    if param1.grad is None and param2.grad is None:<NewLine>        print('both none')<NewLine>    else:<NewLine>        print((param1.grad == param2.grad).all())<NewLine></code></pre><NewLine><p>Could you use my code snippet to copy the parameters of your “good” model to the bad one and train the bad model for a bit just to see, if it achieves a similar accuracy then?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am also expiencing a similar issue and I am really confused about it. I implemented the exact same two models. One with ModuleList() and/or as individual single layers and another one with Sequential(). In my case also just Sequential() is learning properly. It seems like the ModuleList() model is learning the general dataset mean instead of reacting on the input. So it seems like somehow the gradients are not flowing properly back until the input. It happens for a task where two twin networks are trained that influence each other. I run it over multiple runs everytime only the Sequential() model is able to find a proper solution.</p><NewLine><p>This really feels like a super strange bug.</p><NewLine><p>I am usin Pytorch 1.2.0 with Cuda 10.0</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you track down the problem in your case? Or is it really a bug with pytorch/cuda?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""52822"" data-username=""marcel1991""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/a8b319/40.png"" width=""20""/> marcel1991:</div><NewLine><blockquote><NewLine><p>So it seems like somehow the gradients are not flowing properly back until the input.</p><NewLine></blockquote><NewLine></aside><NewLine><p>You could check it by printing all <code>.grad</code> attributes of the model parameters after the <code>backward</code> call.<br/><NewLine>If some of them are <code>None</code>, your computation graph was (accidentally) detached at some point.</p><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""52822"" data-username=""marcel1991""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/a8b319/40.png"" width=""20""/> marcel1991:</div><NewLine><blockquote><NewLine><p>I am usin Pytorch 1.2.0 with Cuda 10.0</p><NewLine></blockquote><NewLine></aside><NewLine><p>Could you update to the latest stable version and post a code snippet to reproduce this issue?</p><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""52822"" data-username=""marcel1991""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/a8b319/40.png"" width=""20""/> marcel1991:</div><NewLine><blockquote><NewLine><p>Or is it really a bug with pytorch/cuda?</p><NewLine></blockquote><NewLine></aside><NewLine><p>My code snippet yields the same output and gradients for both models, so I assume it’s not a bug in PyTorch/CUDA but in the user code.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I will try to check it when I have time if the gradients are flowwing back properly. Unfortunately I am not able to update Cuda 10.0 that easily and Pytorch 1.2. is the latest version for Cuda 10.0. I already updated Pytorch from 1.1 to 1.2 but same behaviour in both versions.</p><NewLine><p>This behaviour just happens with the same model that is forwarded twice with two different inputs and where both outputs are somehow influencing each other.</p><NewLine><p>E.g. something like this:</p><NewLine><pre><code class=""lang-auto"">output1 = model.forward(input1)<NewLine>output2 = model.forward(input2)<NewLine><NewLine>loss = (output1 - output2).pow(2).sum()<NewLine></code></pre><NewLine><p>But where the optimal output somehow can only be achieved by considering the input and not just by setting the output all to zero, like in this example. I try to think of a simple reproducable example because I cannot post my code here and also not reduce my code which is a quite complex.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""52822"" data-username=""marcel1991""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/a8b319/40.png"" width=""20""/> marcel1991:</div><NewLine><blockquote><NewLine><p>Unfortunately I am not able to update Cuda 10.0 that easily and Pytorch 1.2. is the latest version for Cuda 10.0.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Note that the binaries ship with their own CUDA, cudnn, etc. runtime libraries, so your system installed CUDA version will not be used.<br/><NewLine>If you want to use your system CUDA, you would have to rebuild PyTorch from source.</p><NewLine><p>Your code snippet could yield different <code>output</code> tensors e.g. if dropout or batchnorm layers were used, so you could try to call <code>model.eval()</code> before comparing the output.</p><NewLine><p>PS: don’t use <code>model.forward(input)</code>, as this will not use potentially registered hooks and could yield unwanted behavior. Call the model directly via <code>model(input)</code>. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah okay interesting, thank you <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/> . But the Nvidia driver must be compatible with the Pytorch cuda version i guess? I will try to give it a go later with the newest stable version.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, you would have to install an appropriate NVIDIA driver for the CUDA version you are using (via the binaries or locally on your system).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/marcel1991; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/marcel1991; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/marcel1991; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/marcel1991; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: August 10, 2019, 12:24am; <NewLine> REPLY_DATE 2: August 28, 2020,  4:08pm; <NewLine> REPLY_DATE 3: August 28, 2020,  4:18pm; <NewLine> REPLY_DATE 4: August 28, 2020, 11:58pm; <NewLine> REPLY_DATE 5: August 30, 2020,  4:10pm; <NewLine> REPLY_DATE 6: August 30, 2020, 11:24pm; <NewLine> REPLY_DATE 7: August 31, 2020,  8:43am; <NewLine> REPLY_DATE 8: August 31, 2020,  4:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
94717,Ppo+lstm working code,2020-08-31T16:06:51.556Z,0,47,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am looking for ppo + lstm implementation.<br/><NewLine>Can someone please help to let me know of available working code in pytorch for ppo + lstm.</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/granth_jain,(granth jain),granth_jain,"August 31, 2020,  4:06pm",,,,,
94710,PPO agent is not learning anything,2020-08-31T14:45:54.096Z,0,45,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I implemented this from ppo clip pseudocode from openai and code was also inspired by one tutorial but my agent is not learning anything.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch import nn<NewLine>from torch import optim<NewLine>import torch.nn.functional as F<NewLine>import gym<NewLine><NewLine><NewLine>class Policy(nn.Module):<NewLine>    def __init__(self, input_dim, n_actions, pol_type):<NewLine>        super(Policy, self).__init__()<NewLine>        self.pol_type = pol_type<NewLine>        self.fc1 = nn.Linear(input_dim, 256)<NewLine>        self.fc2 = nn.Linear(256, 256)<NewLine>        self.fc3 = nn.Linear(256, 256)<NewLine>        self.ac = nn.Linear(256, n_actions)<NewLine>        self.cri = nn.Linear(256, 1)<NewLine>        self.optimizer = optim.Adam(self.parameters())<NewLine>        self.cuda()<NewLine><NewLine>    def forward(self, x):<NewLine>        x = torch.tensor(x, dtype=torch.float32).cuda()<NewLine>        x = F.relu(self.fc1(x))<NewLine>        x = F.relu(self.fc2(x))<NewLine>        x = F.relu(self.fc3(x))<NewLine>        if self.pol_type == ""actor"":<NewLine>            x = F.softmax(self.ac(x).detach(), dim=-1)<NewLine>        elif self.pol_type == ""critic"":<NewLine>            x = self.cri(x)<NewLine>        return x<NewLine><NewLine><NewLine>class PPO:<NewLine>    def __init__(self, input_dim, n_actions):<NewLine>        self.gamma = 0.99<NewLine>        self.epsilon = 0.2<NewLine>        self.actor = Policy(input_dim, n_actions, ""actor"")<NewLine>        self.critic = Policy(input_dim, n_actions, ""critic"")<NewLine>        self.log_prob = None<NewLine>        self.prev_log_prob = None<NewLine><NewLine>    def select_action(self, obs):<NewLine>        probs = self.actor(obs)<NewLine>        dis = torch.distributions.Categorical(probs)<NewLine>        action = dis.sample()<NewLine>        self.log_prob = dis.log_prob(action)<NewLine>        return action.item()<NewLine><NewLine>    def train(self, state, reward, next_state, done):<NewLine>        if self.prev_log_prob is not None:<NewLine>            reward = torch.tensor(reward, dtype=torch.float32).cuda()<NewLine>            critic = self.critic(state)<NewLine>            next_critic = self.critic(next_state)<NewLine>            advantage = reward + self.gamma * next_critic * (1 - int(done)) - critic<NewLine>            now_vs_prev = (self.log_prob - self.prev_log_prob).exp() * advantage<NewLine>            g = torch.clamp(now_vs_prev, (1 - self.epsilon), (1 + self.epsilon)) * advantage<NewLine>            actor_loss = -torch.min(now_vs_prev, g)<NewLine>            critic_loss = torch.mean(advantage ** 2)<NewLine>            loss = (actor_loss + critic_loss)<NewLine>            self.actor.optimizer.zero_grad()<NewLine>            self.critic.optimizer.zero_grad()<NewLine>            loss.backward()<NewLine>            self.critic.optimizer.step()<NewLine>            self.actor.optimizer.step()<NewLine>        self.prev_log_prob = self.log_prob<NewLine><NewLine><NewLine>num_episodes = 400<NewLine>env = gym.make(""MountainCar-v0"")<NewLine>n_actions = env.action_space.n<NewLine>input_dim = env.observation_space.shape[0]<NewLine>agent = PPO(input_dim, n_actions)<NewLine>for episode in range(num_episodes):<NewLine>    done = False<NewLine>    score = 0<NewLine>    state = env.reset()<NewLine>    while not done:<NewLine>        if episode &gt; 370:<NewLine>            env.render()<NewLine>        action = agent.select_action(state)<NewLine>        next_state, reward, done, _ = env.step(action)<NewLine>        agent.train(state, reward, next_state, done)<NewLine>        score += reward<NewLine>    print(episode, score)<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Jaredeco,(Jano Redecha),Jaredeco,"August 31, 2020,  2:45pm",,,,,
90082,"New RL library, for PyTorch!",2020-07-22T03:36:10.516Z,0,430,"<div class=""post"" itemprop=""articleBody""><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/2b8e78246fe737117d5ea114f7d5d9c518d00bb8"" href=""https://discuss.pytorch.org/uploads/default/original/3X/2/b/2b8e78246fe737117d5ea114f7d5d9c518d00bb8.png"" title=""icon_title""><img alt=""icon_title"" data-base62-sha1=""6djLL4m1J9Uelfc91S2ZCmjSaEw"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/b/2b8e78246fe737117d5ea114f7d5d9c518d00bb8_2_10x10.png"" height=""94"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/b/2b8e78246fe737117d5ea114f7d5d9c518d00bb8_2_344x94.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/b/2b8e78246fe737117d5ea114f7d5d9c518d00bb8_2_344x94.png, https://discuss.pytorch.org/uploads/default/optimized/3X/2/b/2b8e78246fe737117d5ea114f7d5d9c518d00bb8_2_516x141.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/2/b/2b8e78246fe737117d5ea114f7d5d9c518d00bb8_2_688x188.png 2x"" width=""344""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">icon_title</span><span class=""informations"">1279×352 19.1 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>Greetings everyone! I am happy to announce that my RL library, <strong>Machin</strong>, <strong>designed for PyTorch</strong>, is close to its first public debut after several months of hard development!</p><NewLine><p>Machin is designed with the elegant torch style in mind, while aiming to cover most of the functions provided by <a href=""https://github.com/ray-project/ray"" rel=""nofollow noopener"">Ray</a>. It is hosted at <a href=""https://github.com/iffiX/machin"" rel=""nofollow noopener"">https://github.com/iffiX/machin</a></p><NewLine><p>Currently, Machin is at this development stage:</p><NewLine><pre><code class=""lang-auto"">alpha-1: laying down a framework for all code<NewLine>alpha-2: finish testing all modules                       <NewLine>alpha-3: clean and update old examples               <NewLine>alpha-3.5: add architecture docs, tutorial docs, etc.<NewLine>beta-1: public tests, collect response and perfect design    &lt; Machin is at here<NewLine>first official release!<NewLine></code></pre><NewLine><h2>Algorithms</h2><NewLine><p>Machin is able to support a variety of RL algorithms, including:</p><NewLine><h4>Single agent algorithms:</h4><NewLine><ul><NewLine><li><a href=""https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf"" rel=""nofollow noopener"">Deep Q-Network (DQN)</a></li><NewLine><li><a href=""https://arxiv.org/pdf/1509.06461.pdf"" rel=""nofollow noopener"">Double DQN</a></li><NewLine><li><a href=""https://arxiv.org/abs/1511.06581"" rel=""nofollow noopener"">Dueling DQN</a></li><NewLine><li><a href=""https://arxiv.org/abs/1710.02298"" rel=""nofollow noopener"">RAINBOW</a></li><NewLine><li><a href=""https://arxiv.org/pdf/1509.02971.pdf"" rel=""nofollow noopener"">Deep Deterministic policy Gradient (DDPG)</a></li><NewLine><li><a href=""https://arxiv.org/pdf/1802.09477.pdf"" rel=""nofollow noopener"">Twin Delayed DDPG (TD3)</a></li><NewLine><li><a href=""https://hal.archives-ouvertes.fr/hal-00187279/document"" rel=""nofollow noopener"">Hystereric DDPG (Modified from Hys-DQN)</a></li><NewLine><li><a href=""https://openai.com/blog/baselines-acktr-a2c/"" rel=""nofollow noopener"">Advantage Actor-Critic (A2C)</a></li><NewLine><li><a href=""https://arxiv.org/pdf/1707.06347.pdf"" rel=""nofollow noopener"">Proximal Policy Optimization (PPO)</a></li><NewLine><li><a href=""https://arxiv.org/pdf/1812.05905.pdf"" rel=""nofollow noopener"">Soft Actor Critic (SAC)</a></li><NewLine></ul><NewLine><h4>Multi-agent algorithms:</h4><NewLine><ul><NewLine><li><a href=""https://arxiv.org/pdf/1706.02275.pdf"" rel=""nofollow noopener"">Multi-agent DDPG (MADDPG)</a></li><NewLine></ul><NewLine><h4>Massively parallel algorithms:</h4><NewLine><ul><NewLine><li><a href=""https://arxiv.org/abs/1602.01783"" rel=""nofollow noopener"">Asynchronous A2C (A3C)</a></li><NewLine><li><a href=""https://arxiv.org/pdf/1803.00933"" rel=""nofollow noopener"">APEX-DQN, APEX-DDPG</a></li><NewLine><li><a href=""https://arxiv.org/pdf/1802.01561"" rel=""nofollow noopener"">IMPALA</a></li><NewLine><li><a href=""https://arxiv.org/pdf/1803.07055.pdf"" rel=""nofollow noopener"">Augmented random search (ARS, non-gradient)</a></li><NewLine></ul><NewLine><h4>Enhancements:</h4><NewLine><ul><NewLine><li><a href=""https://arxiv.org/pdf/1511.05952.pdf"" rel=""nofollow noopener"">Prioritized Experience Replay (PER)</a></li><NewLine><li><a href=""https://arxiv.org/pdf/1506.02438.pdf"" rel=""nofollow noopener"">Generalized Advantage Estimation (GAE)</a></li><NewLine><li><a href=""https://arxiv.org/pdf/1507.06527.pdf"" rel=""nofollow noopener"">Recurrent networks in DQN, etc.</a></li><NewLine></ul><NewLine><h4>Algorithms to be supported:</h4><NewLine><ul><NewLine><li><a href=""https://arxiv.org/abs/1606.03476"" rel=""nofollow noopener"">Generative Adversarial Imitation Learning (GAIL)</a></li><NewLine><li>Evolution Strategies</li><NewLine><li>Linear Optimization Methods</li><NewLine><li><a href=""https://arxiv.org/abs/1803.11485"" rel=""nofollow noopener"">QMIX (multi agent)</a></li><NewLine><li>Model-based methods</li><NewLine></ul><NewLine><h2>Parallel</h2><NewLine><p>Machin is capable of:</p><NewLine><ol><NewLine><li>Distributed &amp; asynchronus training: provide implementations for parameter servers and gradient reduction servers.</li><NewLine><li>Advance RPC: Role based launching, service registraction, resource sharing. constructed on the basis of torch RPC.</li><NewLine><li>Process level Parallelization, based on enhanced Processes, Threads, Pools, Events.</li><NewLine><li>Model level Parallelization, Splitting(Not yet implemented) and Assignning model shards to devices, by heuristic.</li><NewLine></ol><NewLine><h2>Utilities</h2><NewLine><ol><NewLine><li>Model checking toolset: check for nan values, abnormal gradients etc in your model, based on hooks.</li><NewLine><li>Logging videos, images, produced during your training</li><NewLine><li>Model serialization, auto loading and management toolset. No longer worry about unmatched devices!</li><NewLine><li>Many more.</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/iffiX,(Iffi),iffiX,"August 15, 2020,  2:50am",9 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I wonder if there is any specific need for Model based methods? I am quite interested in this area, and wants to hear your suggestions on:</p><NewLine><ol><NewLine><li>What model-based algorithms are STOA and worth supporting.</li><NewLine><li>Is there a good overview paper for this ?</li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: August 31, 2020,  4:13am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
94570,"Torch 1.6.0 RuntimeError: probability tensor contains either `inf`, `nan` or element &lt; 0, But good with Torch 1.1.0",2020-08-30T10:14:33.352Z,4,98,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m runing this piece of code:</p><NewLine><pre><code class=""lang-auto"">    def forward(self, inputs, core_state=()):<NewLine>        x = inputs[""frame""]<NewLine>        # time x batch x 64 x 64 x 3<NewLine>        T, B, *_ = x.shape<NewLine><NewLine>        # merge time and batch<NewLine>        # [T*B x 64 x 64 x 3]<NewLine>        x = torch.flatten(x, 0, 1)<NewLine><NewLine>        x = x.float()<NewLine><NewLine>        # [T*B x 3 x 64 x 64]<NewLine>        x = x.transpose(1, 3)<NewLine><NewLine>        # x = checkpoint_sequential(self.feat_extract, 2, x)<NewLine>        x = self.feat_extract(x)<NewLine>        x = x.view(T*B, -1)<NewLine><NewLine>        # core_input = checkpoint_sequential(self.fc, 2, x)<NewLine>        core_input = self.fc(x)<NewLine><NewLine>        core_output = core_input<NewLine>        core_state = tuple()<NewLine><NewLine>        policy_logits = self.policy(core_output)<NewLine>        baseline = self.baseline(core_output)<NewLine>        if self.training:<NewLine>            action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1)<NewLine>        else:<NewLine>            action = torch.argmax(policy_logits, dim=1)<NewLine><NewLine>        policy_logits = policy_logits.view(T, B, self.num_actions)<NewLine>        baseline = baseline.view(T, B)<NewLine>        action = action.view(T, B)<NewLine><NewLine>        return dict(policy_logits=policy_logits, baseline=baseline,<NewLine>                    action=action), core_state<NewLine></code></pre><NewLine><p>And got this error:</p><NewLine><blockquote><NewLine><p>Traceback (most recent call last):<br/><NewLine>File ""/usr/local/easybuild-2019/easybuild/software/compiler/gcccore/8.3.0/python/3.7.4/lib/python3$<br/><NewLine>self.run()<br/><NewLine>File ""/usr/local/easybuild-2019/easybuild/software/compiler/gcccore/8.3.0/python/3.7.4/lib/python3$<br/><NewLine>self._target(*self._args, **self._kwargs)<br/><NewLine>File “/data/gpfs/projects/punim1126/CL-RIDE-master-project/src/utils.py”, line 335, in act<br/><NewLine>raise e<br/><NewLine>File “/data/gpfs/projects/punim1126/CL-RIDE-master-project/src/utils.py”, line 287, in act<br/><NewLine>agent_output, agent_state = model(env_output, agent_state)<br/><NewLine>File “/home/shizhec/.local/lib/python3.7/site-packages/torch/nn/modules/module.py”, line 722, in _$<br/><NewLine>result = self.forward(*input, **kwargs)<br/><NewLine>File “/data/gpfs/projects/punim1126/CL-RIDE-master-project/src/models.py”, line 675, in forward<br/><NewLine>action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1)<br/><NewLine>RuntimeError: probability tensor contains either <code>inf</code>, <code>nan</code> or element &lt; 0</p><NewLine></blockquote><NewLine><p>I’m using torch 1.6.0, however, when I use torch 1.1.0, I don’t get any error anymore, and I could train the model correctly. Anyone know why this is happening?</p><NewLine></div>",https://discuss.pytorch.org/u/Shizhe_Cai,(Shizhe Cai),Shizhe_Cai,"August 30, 2020, 10:15am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Output of <code>policy_logits</code>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p><span class=""hashtag"">#one</span> output before error:<br/><NewLine>tensor([[  4.0024, -22.5107,   8.6548, -26.3529, 199.5710, -23.9216, -40.0046,<br/><NewLine>3.1537, -19.8343, -39.2633,  34.4721,  -7.2311, -56.9415,  -5.0400,<br/><NewLine>15.7165]])<br/><NewLine><span class=""hashtag"">#where</span> the error occur:<br/><NewLine>tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>for torch==1.1.0:<br/><NewLine>I never got those nan tensors, all process works fine. This is strange.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>You code seems OK, something might have changed between these two versions, so you can either:</p><NewLine><ol><NewLine><li>Modify you framework, then track and find the origin of the first NaN value</li><NewLine><li>Switch back to 1.1.0, for now.</li><NewLine></ol><NewLine><p>I would recommend you use the second solution, according to your complex situation presented in the previous question.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>NaN could really happen at anywhere, mainly:</p><NewLine><ol><NewLine><li>division by 0</li><NewLine><li>something involving the annoying log/exp calculation, like log probability, I have just located a Nan problem myself this morning. Eg:</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">   a=Normal(1, 1e-23)<NewLine>   a.log_prob(a.sample()) -&gt; NaN because sigma is too small.<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Shizhe_Cai; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Shizhe_Cai; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: August 30, 2020,  2:29pm; <NewLine> REPLY_DATE 2: August 30, 2020,  2:58pm; <NewLine> REPLY_DATE 3: August 30, 2020,  3:10pm; <NewLine> REPLY_DATE 4: August 31, 2020,  2:03am; <NewLine> REPLY_DATE 5: August 31, 2020,  2:05am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
94608,DQN algorithm in pytorch not converging,2020-08-30T21:47:19.007Z,0,46,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am new to deep reinforcement learning and have implemented the algorithm on my own but the value is not converging could anyone take a look and tell me what is wrong with my algorithm and can i do to make it better Here is the code:</p><NewLine><pre><code class=""lang-auto"">import gym<NewLine>import torch<NewLine>import numpy as np<NewLine>import torch <NewLine>import random<NewLine>from collections import deque<NewLine>from itertools import count<NewLine><NewLine>class ReplayBuffer:<NewLine>    def __init__(self):<NewLine>        self.buffer=deque(maxlen=50000)<NewLine>    def push(self,state,action,reward,next_state,done):<NewLine>        if(len(self.buffer)&lt;=1000):<NewLine>            self.buffer.append((state,action,reward,next_state,done))<NewLine>    def sample(self, batch_size: int, continuous: bool = True):<NewLine>        if batch_size &gt; len(self.buffer):<NewLine>            batch_size = len(self.buffer)<NewLine>        if continuous:<NewLine>            rand = random.randint(0, len(self.buffer) - batch_size)<NewLine>            return [self.buffer[i] for i in range(rand, rand + batch_size)]<NewLine>        else:<NewLine>            indexes = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)<NewLine>            return [self.buffer[i] for i in indexes]<NewLine>class NNetwork(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.l1=torch.nn.Linear(4,128)<NewLine>        self.l2=torch.nn.Linear(128,128)<NewLine>        self.l3=torch.nn.Linear(128,2)<NewLine>        <NewLine>        self.optimizer=torch.optim.Adam(params=self.parameters(),lr=0.001)<NewLine>        self.criterion=torch.nn.MSELoss()<NewLine>    def forward(self,x):<NewLine>        al1=torch.nn.ReLU()(self.l1(x))<NewLine>        al2=torch.nn.ReLU()(self.l2(al1))<NewLine>        al3=self.l3(al2)<NewLine>        return al3<NewLine>class Agent():<NewLine>    def __init__(self):<NewLine>        <NewLine>        self.env=gym.make('CartPole-v0')<NewLine>        self.mem=ReplayBuffer()<NewLine>        self.q_local=NNetwork()<NewLine>        self.q_target=NNetwork()<NewLine>        self.q_target.load_state_dict(self.q_local.state_dict())<NewLine>        self.epsilon=1.0<NewLine>        self.e_decay=0.0995<NewLine>        self.e_min=0.1<NewLine>        self.update=4<NewLine>        self.score=0<NewLine>        self.gamma=0.99<NewLine><NewLine>    def predict(self,state):<NewLine>        if (np.random.randn()&lt;self.epsilon):<NewLine>            return random.randint(0,1)<NewLine>        else:<NewLine>            index=self.q_local.forward(torch.Tensor(state).unsqueeze(0))<NewLine>            return torch.argmax(index,dim=1).item()<NewLine>    <NewLine>    def step(self):<NewLine>        state=self.env.reset()<NewLine>        done=False<NewLine>        i=0<NewLine>        while not done:<NewLine>            action=self.predict(state)<NewLine>            n_state,reward,done,_=self.env.step(action)<NewLine>            self.mem.push(state,action,reward,n_state,done)<NewLine>            self.score+=reward<NewLine>            self.learn()<NewLine>            state=n_state<NewLine>            i+=1<NewLine>            if(i%10==0):<NewLine>                if(self.epsilon&gt;self.e_min):<NewLine>                    self.epsilon=self.epsilon-self.e_decay<NewLine>                else:<NewLine>                    self.epsilon=self.e_min<NewLine>                self.q_target.load_state_dict(self.q_local.state_dict())<NewLine>          <NewLine>        print(self.score)<NewLine>        self.score=0<NewLine>    def learn(self):<NewLine>        if(len(self.mem.buffer)%32==0):<NewLine>            return<NewLine>        batch =self.mem.sample(32)<NewLine>        state,action,reward,n_state,done= zip(*batch)<NewLine>        state=torch.Tensor(state)<NewLine>        action=torch.Tensor(action).unsqueeze(1)<NewLine>        n_state=torch.Tensor(n_state)<NewLine>        reward=torch.Tensor(reward).unsqueeze(1)<NewLine>        done=torch.Tensor(done).unsqueeze(1)<NewLine><NewLine>        self.q_local.optimizer.zero_grad()<NewLine>        <NewLine>        q_N=self.q_local.forward(state).gather(1,action.long())<NewLine>        q_t=self.q_target.forward(n_state)<NewLine>        y=reward+(1-done)*self.gamma*torch.max(q_t,dim=1,keepdim=True)[0]<NewLine>        <NewLine>        loss=self.q_local.criterion(q_N,y)<NewLine>        loss.backward()<NewLine>        self.q_local.optimizer.step()<NewLine>agent=Agent()<NewLine>for t in count():<NewLine>    print(""EP "",t)<NewLine>    agent.step()<NewLine></code></pre><NewLine><p>Well I am being ale to produce few scores but it is not converging</p><NewLine></div>",https://discuss.pytorch.org/u/Debjit_Chowdhury,(Debjit Chowdhury),Debjit_Chowdhury,"August 30, 2020,  9:47pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I think the reason why your algorithm is not converging is that you are calculating your gradient both when you calculate <code>q_N</code> and <code>q_t</code>, and that will cause gradient incorrectness. You can use <code>q_t.detach()</code> to remove <code>q_t</code> from the calculate chain. Hope it works!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/liuruiqi1107; <NewLine> ,"REPLY_DATE 1: August 30, 2020, 10:30pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
94282,LSTMCell Bias question,2020-08-27T10:07:10.502Z,1,63,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I read the documentation for LSTM cell:([<a href=""https://pytorch.org/docs/master/generated/torch.nn.LSTMCell.html"" rel=""nofollow noopener"">https://pytorch.org/docs/master/generated/torch.nn.LSTMCell.html</a>]</p><NewLine><p>Noted that there are two biases required, namely:</p><NewLine><ul><NewLine><li><NewLine><strong>~LSTMCell.bias_ih</strong>  – the learnable input-hidden bias, of shape (4*hidden_size)</li><NewLine><li><NewLine><strong>~LSTMCell.bias_hh</strong>  – the learnable hidden-hidden bias, of shape (4*hidden_size)</li><NewLine></ul><NewLine><p>Assuming that batch size is 1,  and the size of input(x) and hidden(h) state is 128 and 64, then:</p><NewLine><p>i(t) = [1, 128]<br/><NewLine>h(t) = [1, 64]</p><NewLine><p>w(i) = [64, 128+64]<br/><NewLine>b(i) = [64]</p><NewLine><p>I feel uncertain why we need two biases, to fit each set of activation (i.e. sigmoid and tanh) functions.  As the shape of outputs must be equal to the hidden state. So one bias is enough for each set. why we need two biases for each set?</p><NewLine><p>Also, for LSTM model, we have five activation functions, namely:</p><NewLine><ol><NewLine><li>forget gate (sigmoid)</li><NewLine><li>input gate (sigmoid)</li><NewLine><li>input gate (tanh)</li><NewLine><li>output gate (sigmoid)</li><NewLine><li>output gate (tanh)</li><NewLine></ol><NewLine><p>The documentation mentions 4*, instead of 5*.  Is it related to <span class=""hashtag"">#1</span> to <span class=""hashtag"">#4</span> above as <span class=""hashtag"">#5</span> is not required for bias due to solely c(t).</p><NewLine><p>Thanks for your patience for reading.</p><NewLine></div>",https://discuss.pytorch.org/u/Alan_Kong,(Alan Kong),Alan_Kong,"August 27, 2020, 10:23am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I guess the probable reason is:</p><NewLine><p>output of activation function = activation function (i.e. sigmoid/tanh) (matrix addition (w(i) * i(t) + b(i), w(h) * h(t-1) + b(h)))</p><NewLine><p>which is different from:</p><NewLine><p>output of activation function = activation function (i.e. sigmoid/tanh) (w(i + t-1) * matrix concatenation (i(t), h(t-1)) + b(h))) [as referred by the formulas from <a href=""https://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""nofollow noopener"">colah’s blog</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s for consistency reasons (see <a href=""https://discuss.pytorch.org/t/why-rnn-needs-two-biases/557/4"">this</a>).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Alan_Kong; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mariosasko; <NewLine> ,"REPLY_DATE 1: August 28, 2020,  8:00am; <NewLine> REPLY_DATE 2: August 30, 2020,  7:39am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
94219,Policy Gradient: Model is not learning,2020-08-26T20:51:17.081Z,4,84,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I am implementing a simple Policy Gradient algorithm but it seems that the model is not learning anything. Its actions are still almost random even after about 2000 episodes in cartpole-v1. I have stepped through the code to make sure I am storing the action log probability and episode rewards correctly, and it seems that is not the problem. Is there something that I’m missing?</p><NewLine><pre><code>import torch<NewLine>import torch.autograd as autograd        <NewLine>from torch import Tensor                  <NewLine>import torch.nn as nn                     <NewLine>import torch.nn.functional as F           <NewLine>import torch.optim as optim               <NewLine>from torch.distributions import categorical<NewLine>import gym<NewLine>import numpy as np<NewLine>from torch.utils.tensorboard import SummaryWriter<NewLine><NewLine>class PGN(nn.Module):<NewLine><NewLine>  def __init__(self, obs_size, n_actions):<NewLine>    super(PGN, self).__init__()<NewLine>    self.fc1= nn.Linear(obs_size, 256)<NewLine>    self.fc2= nn.Linear(256,256)<NewLine>    self.fc3= nn.Linear(256, n_actions)<NewLine><NewLine>  def forward(self, obs):<NewLine>    x= F.relu(self.fc1(obs))<NewLine>    x= F.relu(self.fc2(x))<NewLine>    x=self.fc3(x)<NewLine>    return x<NewLine><NewLine>def train():<NewLine>  #hyperparameters<NewLine>  lr=0.001<NewLine>  GAMMA =0.99<NewLine><NewLine>  #make environment<NewLine>  env = gym.make('CartPole-v1')  <NewLine>  obs=env.reset()<NewLine>  done = False<NewLine>                                                <NewLine>  net= PGN(len(obs),env.action_space.n)<NewLine>  optimizer=optim.Adam(net.parameters(),lr=lr)<NewLine><NewLine>  action_list=[]<NewLine>  reward_list=[]<NewLine><NewLine>  count = 0                    #count number of episodes<NewLine>  num_episodes =2000           #max number of episodes<NewLine>  max_r=0                      #for visualization purposes<NewLine><NewLine>  while (True):<NewLine><NewLine>    output = net(torch.from_numpy(obs).float())<NewLine>    action_prob = torch.distributions.Categorical(F.softmax(output))<NewLine>    action = action_prob.sample()<NewLine><NewLine>    obs, reward, done, info = env.step(action.item()) <NewLine><NewLine>    #env.render()<NewLine><NewLine>    max_r += reward<NewLine><NewLine>    action_list.append(action_prob.log_prob(action))<NewLine>    reward_list.append(reward)<NewLine><NewLine>    if done:<NewLine>      print(max_r)                #print episode reward at end of episode<NewLine><NewLine>      max_r =0 <NewLine><NewLine>      obs=env.reset()<NewLine>      total_reward=0<NewLine>      reward_list_calc=[]<NewLine><NewLine>      for i in reversed(reward_list):                       #discount rewards. to do this in linear time, we traverse backward<NewLine>        total_reward *=GAMMA <NewLine>        total_reward += i<NewLine>        reward_list_calc.append(total_reward) <NewLine><NewLine>      reward_list = list(reversed(reward_list_calc))<NewLine>      reward_list = np.asarray(reward_list)<NewLine><NewLine>      mean = np.mean(reward_list)<NewLine>      std = np.std(reward_list) if np.std(reward_list) &gt; 0 else 1<NewLine>      reward_list = (reward_list-mean)/std                        #apparently z-scores are used to normalize data. we want to reduce variance here<NewLine><NewLine>      action_list = torch.tensor(action_list, requires_grad = True)<NewLine>      reward_list = torch.tensor(reward_list, requires_grad = True)       <NewLine><NewLine>      loss = -reward_list*action_list<NewLine>      loss = torch.sum(loss)<NewLine><NewLine>      optimizer.zero_grad()<NewLine>      loss.backward()<NewLine>      optimizer.step()<NewLine><NewLine>      reward_list=[]<NewLine>      action_list=[]<NewLine><NewLine>      count+=1     #number of episode<NewLine><NewLine>      if(count==num_episodes):<NewLine>        print(""done"")<NewLine>        break<NewLine><NewLine>      <NewLine>train()</code></pre><NewLine></div>",https://discuss.pytorch.org/u/Yemi,(Olayemi Yesufu),Yemi,"August 26, 2020,  8:55pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I finally got it to work, but i’m not exactly sure why. Can someone explain?<br/><NewLine>I changed the way I calculated Loss. I also noticed that when I converted the action_list to a torch tensor, the model won’t learn.</p><NewLine><p>From:</p><NewLine><pre><code>  action_list = torch.tensor(action_list, requires_grad = True)<NewLine>  reward_list = torch.tensor(reward_list, requires_grad = True)       <NewLine><NewLine>  loss = -reward_list*action_list<NewLine>  loss = torch.sum(loss)<NewLine></code></pre><NewLine><p>To:</p><NewLine><pre><code>  reward_list = torch.tensor(reward_list)  <NewLine>  loss = 0<NewLine>  for r, logprob in zip(reward_list, action_list):<NewLine>      loss += -r * logprob</code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>probably because <code>requires_grad</code> has changed the gradient flow through action log probability.<br/><NewLine>BTW:</p><NewLine><pre><code class=""lang-auto"">reward_list = torch.tensor(reward_list, requires_grad = True)    <NewLine></code></pre><NewLine><p>There is no need to add <code>requires_grad</code> here</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the response. I made sure to study more about PyTorch’s autograd, but I still have a question. Why exactly is having the gradient flow through the action log probability a bad thing in this case?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, I mean you need gradient flow through the action log probability path, but <code>requires_grad</code> “probably” truncates it, normally I would not perform any other modifications to these tensors in the grad path except adding/multiplying them.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you! This cleared things up for me.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Yemi; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Yemi; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Yemi; <NewLine> ,"REPLY_DATE 1: August 28, 2020, 12:48am; <NewLine> REPLY_DATE 2: August 29, 2020, 12:23am; <NewLine> REPLY_DATE 3: August 29, 2020, 12:23am; <NewLine> REPLY_DATE 4: August 29, 2020,  1:41am; <NewLine> REPLY_DATE 5: August 29, 2020,  3:26am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
94355,Learning time increase over-time,2020-08-27T19:44:50.398Z,0,73,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am training several networks at the same time. With the passing of episodes the training time takes longer. From the 10th to the 90th episode the total training time of all networks goes from 6.42 seconds to 13.7 seconds. What may be the reason for this increase? Thank you in advance</p><NewLine><pre><code class=""lang-auto""># dqn class<NewLine>    def updateSARSD(self, state, action, reward, next_state, done):<NewLine>        self.remember(state, action, reward, next_state, int(done) )<NewLine>        # Optimize the target network<NewLine>        self.learn()<NewLine><NewLine>    def learn(self):<NewLine><NewLine>        if self.memory.mem_cntr &lt; self.batch_size * 2:<NewLine>            return<NewLine><NewLine>        self.q_eval.optimizer.zero_grad()<NewLine>        self.replace_target_network()<NewLine><NewLine>        states, actions, rewards, next_states, dones = self.sample_memory()<NewLine><NewLine>        indices = np.arange(self.batch_size)<NewLine>        q_pred = self.q_eval.forward(states)[indices,actions]<NewLine>        q_next = self.q_next.forward(next_states).max(dim=1)[0]<NewLine>        q_next[dones] = 0.0<NewLine>        q_target = rewards +  self.gamma * q_next<NewLine><NewLine>        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)<NewLine>        loss.backward()<NewLine>        self.q_eval.optimizer.step()<NewLine><NewLine>        self.learn_step_counter += 1<NewLine>        self.decrement_epsilon()<NewLine><NewLine>        return loss<NewLine><NewLine>    def replace_target_network(self):<NewLine><NewLine>        if self.learn_step_counter % self.replace_target_cnt == 0:<NewLine>            self.q_next.load_state_dict(self.q_eval.state_dict())<NewLine><NewLine>    def remember(self, *args):<NewLine>        self.memory.push(*args)<NewLine><NewLine>    def sample_memory(self):<NewLine><NewLine>        state, action, reward, new_state, done = self.memory.sample(self.batch_size)<NewLine><NewLine>        states = torch.tensor(state, dtype=torch.float, device=self.q_eval.device)<NewLine>        rewards = torch.tensor(reward, dtype=torch.float, device=self.q_eval.device)<NewLine>        dones = torch.tensor(done)<NewLine>        actions = torch.tensor(action)<NewLine>        next_states = torch.tensor(new_state, dtype=torch.float, device=self.q_eval.device)<NewLine><NewLine>        return states, actions, rewards, next_states, dones<NewLine><NewLine># memory_buffer class<NewLine><NewLine>    def push(self, state, action, reward, new_state, done):<NewLine><NewLine>        index = self.mem_cntr % self.mem_size<NewLine>        self.state_memory[index] = state<NewLine>        self.action_memory[index] = action<NewLine>        self.reward_memory[index] = reward<NewLine>        self.new_state_memory[index] = new_state<NewLine>        self.terminal_memory[index] = done<NewLine>        self.mem_cntr += 1<NewLine><NewLine>    def sample(self, batch_size):<NewLine><NewLine>        max_mem = min(self.mem_cntr, self.mem_size)<NewLine>        batch = np.random.choice(max_mem, batch_size, replace=False) # replace = False to not repeat any memory<NewLine><NewLine>        states = self.state_memory[batch]<NewLine>        actions = self.action_memory[batch]<NewLine>        rewards = self.reward_memory[batch]<NewLine>        next_states = self.new_state_memory[batch]<NewLine>        dones = self.terminal_memory[batch]<NewLine><NewLine>        return states, actions, rewards, next_states, dones<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/David_Silva,(David Silva),David_Silva,"August 27, 2020,  7:45pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you detached the “action” tensor before you storing it with <code>remember</code> called by <code>updateSARSD</code>?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: August 28, 2020,  2:34am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
93252,Actor network loss in Proximal Policy Optimization,2020-08-18T16:11:32.145Z,0,42,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,</p><NewLine><p>I have implemented a PPO algorithm where the actor and the critic are completely different networks. During training there are some times where the actor loss is getting negative. Loss is considered “optimal” when it is equal to 0, meaning that the network could not do better than that. How negative actor loss can be explained logically and if its not weird, should my goal be to get the most negative loss possible and not just 0?</p><NewLine><p>As for the critic loss, the minimum loss is always &gt;= 0.</p><NewLine><p>Thank you in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/Kimonili,,Kimonili,"August 18, 2020,  4:11pm",,,,,
91702,Deep Deterministic Policy Gradient implementation,2020-08-05T09:41:47.244Z,5,208,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I want to use DDPG in my project so I set out to first get a working example. I’ve found this nice implementation in Keras (<a href=""https://keras.io/examples/rl/ddpg_pendulum/"" rel=""nofollow noopener"">https://keras.io/examples/rl/ddpg_pendulum/</a>) which works well and decided to translate it to PyTorch 1:1. Unfortunately the model doesn’t learn. Since I have no friends to ask, could someone please have a look on my code and try to spot where I’ve made a mistake?</p><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""16""/><NewLine><a href=""https://github.com/lubiluk/ddpg"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars0.githubusercontent.com/u/385792?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/lubiluk/ddpg"" rel=""nofollow noopener"" target=""_blank"">lubiluk/ddpg</a></h3><NewLine><p>Contribute to lubiluk/ddpg development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>",https://discuss.pytorch.org/u/lubiluk,(Paweł Gajewski),lubiluk,"August 5, 2020,  9:41am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you have an idea which part of the code might be wrong or where you were unsure, how to implement a specific use case in PyTorch?</p><NewLine><p>If not, you could try to check the model implementation first e.g. by using a constant input (tensor of all ones should work). Of course you would need to load the parameters from one model to the other and I don’t know, how hard this would be.<br/><NewLine>Once the models return the same output, you could debug the code step by step.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Find 1 Problem: where is terminal?:</p><NewLine><pre><code class=""lang-auto"">reward + discount * ~terminal * next_value<NewLine></code></pre><NewLine><p>Other parts seems to be ok.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> I was most unsure about how to define a network that has parallel input processing with result concatenation.  See Critic class and it’s forward method.</p><NewLine><p><a class=""mention"" href=""/u/iffix"">@iffiX</a> it’s an interesting point. I think you are referring to the definition of ‘y’. But in the original Keras code there is no terminal as well in that line.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>The <code>Critic</code> implementation looks correct to me.<br/><NewLine>Note that I’m not deeply familiar with Keras and assume that the <code>Concatenate</code> layer concatenates the input tensors in the “feature dimension”.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I was pointed out that I should model.eval() and model.train(), every time I want to call my models. It’s because I use batch normalization and it operates differently depending on train flag. Now the model learns correctly.<br/><NewLine>Is it a good idea to always wrap model calls with eval/train?</p><NewLine><pre><code class=""lang-auto"">model.eval()<NewLine>r = model(input)<NewLine>model.train()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh, that point was neglected, “eval” and “train” are not needed if you are not using something special like<br/><NewLine><code>BatchNorm</code><br/><NewLine>See <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=eval#torch.nn.Module.eval"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=eval#torch.nn.Module.eval</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""91702"" data-username=""lubiluk""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/lubiluk/40/27506_2.png"" width=""20""/> lubiluk:</div><NewLine><blockquote><NewLine><p>Is it a good idea to always wrap model calls with eval/train?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, I would recommend to always call <code>model.train()</code> before the training and <code>model.eval()</code> before the evaluation or testing of the model. Even if your current model does not use any batchnorm or dropout layers, the model itself or custom layers might use the <code>self.training</code> attribute to change their behavior.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Somehow this doesn’t seem right:</p><NewLine><pre><code class=""lang-auto"">        critic_optimizer.zero_grad()<NewLine><NewLine>        target_actions = target_actor(next_state_batch)<NewLine>        y = reward_batch + gamma * target_critic([next_state_batch, target_actions])<NewLine>        critic_model.eval()<NewLine>        critic_value = critic_model([state_batch, action_batch])<NewLine>        critic_model.train()<NewLine>        critic_loss = torch.mean(torch.square(y - critic_value))<NewLine><NewLine>        critic_loss.backward()<NewLine>        critic_optimizer.step()<NewLine></code></pre><NewLine><p>Didn’t I just exclude batch norm parameters from training effectively getting rid of normalization? I mean this is part of the training routine so it seems logical to have the model in train() mode. I have read the original Batch Norm paper and it seems to confirm that.<br/><NewLine>Now I’m really confused.<br/><NewLine>Just for testing I entirely commented out batch normalization and the model is training well without it.</p><NewLine><p>So the conclusion is, there is something wrong with my batch normalization and I still don’t know what…</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>I missed that the previous post was related to the model implementation and not a general question.<br/><NewLine>By calling <code>critic_model.eval()</code>, you would use the running stats instead of the batch stats to normalize the data. The affine parameters of the batchnorm layers would still be trained.<br/><NewLine>I’m not familiar with the model, but why did you add this line of code and what is the Keras model doing?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve added this line because I tried all combinations of eval and train, and learning works only when i put eval everywhere. But that probably means that batch norm is not used at all.<br/><NewLine>I also tried to remove batchnorm layers altogether and it also enables learning.<br/><NewLine>Keras model probably also has a slight bug as it always keeps batchnorm layer in evaluation mode. But surprisingly, when I put it in training mode then learning abilities are not affected.<br/><NewLine>I am thoroughly confused right now and I will probably go carefully through the implementations to debug what’s going on. Just like you suggested in your first post. I think I need to debug my autograd chain. Is there a way to see it as a graph?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Try use this:</p><NewLine><pre><code class=""lang-auto"">from torchviz import make_dot<NewLine><NewLine>def visualize_graph(final_tensor, visualize_dir="""", exit_after_vis=True):<NewLine>    """"""<NewLine>    Visualize a pytorch flow graph<NewLine><NewLine>    Args:<NewLine>        final_tensor: The last output tensor of the flow graph<NewLine>        visualize_dir: Directory to place the visualized files<NewLine>        exit_after_vis: Whether to exit the whole program<NewLine>            after visualization.<NewLine>    """"""<NewLine>    g = make_dot(final_tensor)<NewLine>    g.render(directory=visualize_dir, view=False, quiet=True)<NewLine>    if exit_after_vis:<NewLine>        exit(0)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your help. I’ve spend some more time with my models and what I’ve found out is that nn.BatchNorm1d (PyTorch) works differently than layers.BatchNormalization (Keras). There are some discussions about that on the internet but not conclusive. I can’t work out how to make them behave the same.</p><NewLine><p>In the end I’ve replaced nn.BatchNorm1d with nn.LayerNorm and the agent learns flawlessly.</p><NewLine><p>I would really like to know what’s wrong with nn.BatchNorm1d in PyTorch. Until I find out I’ve got to avoid it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/lubiluk; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/lubiluk; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/lubiluk; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/lubiluk; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/lubiluk; <NewLine> ,"REPLY_DATE 1: August 8, 2020,  8:50am; <NewLine> REPLY_DATE 2: August 8, 2020,  2:26pm; <NewLine> REPLY_DATE 3: August 9, 2020,  7:11am; <NewLine> REPLY_DATE 4: August 10, 2020,  4:05am; <NewLine> REPLY_DATE 5: August 10, 2020,  9:16am; <NewLine> REPLY_DATE 6: August 10, 2020,  3:49pm; <NewLine> REPLY_DATE 7: August 11, 2020,  8:44am; <NewLine> REPLY_DATE 8: August 11, 2020,  7:34pm; <NewLine> REPLY_DATE 9: August 13, 2020,  8:52am; <NewLine> REPLY_DATE 10: August 14, 2020,  9:44am; <NewLine> REPLY_DATE 11: August 15, 2020,  2:49am; <NewLine> REPLY_DATE 12: August 18, 2020,  9:32am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> 
92999,"Backward error, although there are two different networks for actor and critic. (PPO implementation)",2020-08-16T13:46:06.281Z,10,97,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have seen the topics discussed about this error,</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.<NewLine></code></pre><NewLine><p>but I think in my case is different. I have implemented a PPO algorithm where the actor and the critic are two completely different networks, and so I backward the actor loss for the actor network, and the critic loss for the critic network, as shown below:</p><NewLine><pre><code class=""lang-auto""># critic loss backward implementation<NewLine>self.critic_optimizer.zero_grad()<NewLine>critic_loss.backward()<NewLine>nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)<NewLine>self.critic_optimizer.step()<NewLine><NewLine># actor loss backward implementation<NewLine>self.actor_optimizer.zero_grad()<NewLine>actor_loss.backward() ######### ERROR ARISES HERE<NewLine>nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)<NewLine>self.actor_optimizer.step()<NewLine></code></pre><NewLine><p>The thing is that although these networks are different, the error occurs when I call the backward of the actor loss (the second backward call in my code), which is not related to the backward of the critic loss. I there a way for pytorch to understand that the actor’s backward is not related with the critic’s backward?</p><NewLine></div>",https://discuss.pytorch.org/u/Kimonili,,Kimonili,"August 16, 2020,  1:55pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please show your whole implementation</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This error could occur for multiple reasons. One time I observed this when my input tensor had  <code>requires_grad=True</code>, so one thing to check is the input tensor.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>In this code block I specify the two networks:</p><NewLine><pre><code class=""lang-auto"">class Actor(nn.Module):<NewLine>    def __init__(self, obs_size, action_size, hidden_size,<NewLine>                 activation=nn.Tanh()):<NewLine>        super(Actor, self).__init__()<NewLine><NewLine>        self.action = nn.Sequential(<NewLine>            layer_init(nn.Linear(obs_size, hidden_size)),<NewLine>            activation,<NewLine>            layer_init(nn.Linear(hidden_size, hidden_size)),<NewLine>            activation,<NewLine>            layer_init(nn.Linear(hidden_size, action_size), std=0.01),<NewLine>        )<NewLine><NewLine>    def forward(self):<NewLine>        raise NotImplementedError<NewLine><NewLine>    def get_action(self, state, action = None):<NewLine>        logits = self.action(state)<NewLine>        probs = Categorical(logits=logits)<NewLine>        if action is None:<NewLine>            action = probs.sample()<NewLine>        return action, probs.log_prob(action), probs.entropy()<NewLine><NewLine><NewLine>class Critic(nn.Module):<NewLine>    def __init__(self, obs_size: int, hidden_size, activation=nn.Tanh()):<NewLine>        """"""Initialize.""""""<NewLine>        super(Critic, self).__init__()<NewLine><NewLine>        self.value = nn.Sequential(<NewLine>            layer_init(nn.Linear(obs_size, hidden_size)),<NewLine>            activation,<NewLine>            layer_init(nn.Linear(hidden_size, hidden_size)),<NewLine>            activation,<NewLine>            layer_init(nn.Linear(hidden_size, 1), std=1.)<NewLine>        )<NewLine><NewLine>    def forward(self):<NewLine>        raise NotImplementedError<NewLine><NewLine>    def get_value(self, state):<NewLine>        return self.value(state)<NewLine><NewLine></code></pre><NewLine><p>the networks call:</p><NewLine><pre><code class=""lang-auto"">self.actor = Actor(self.obs_size, self.action_size, self.hidden_size).to(self.device)<NewLine>self.critic = Critic(self.obs_size, self.hidden_size).to(self.device)<NewLine></code></pre><NewLine><p>optimizers:</p><NewLine><pre><code class=""lang-auto"">self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.actor_lr)<NewLine>self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.critic_lr)<NewLine></code></pre><NewLine><p>update of the networks function:</p><NewLine><pre><code class=""lang-auto"">    def update_model(self, next_state: np.ndarray):<NewLine>     <NewLine>        last_value = self.critic.get_value(next_state.to(self.device)).reshape(1, -1)<NewLine><NewLine>        returns, advantages = compute_gae(last_value, self.rewards, self.masks, self.values,<NewLine>                                          self.gamma, self.lam, self.device)<NewLine><NewLine>        # squeeze<NewLine>        states_traj = self.states.squeeze()<NewLine>        log_probs_traj = self.log_probs.squeeze()<NewLine>        actions_traj = self.actions.squeeze()<NewLine>        advantages_traj = advantages.squeeze()<NewLine>        returns_traj = returns.squeeze()<NewLine>        values_traj = self.values.squeeze()<NewLine><NewLine>        <NewLine>        ids = np.arange(self.trajectory_size)<NewLine>        for epoch in range(self.epochs):<NewLine>            np.random.shuffle(ids)<NewLine>            for start in range(0, self.trajectory_size, self.mini_batch_size):<NewLine>                end = start + self.mini_batch_size<NewLine>                minibatch_ind = ids[start:end]<NewLine>                advantages_minib = advantages_traj[minibatch_ind]<NewLine>                if self.normalize_adv:<NewLine>                    advantages_minib = (advantages_minib - advantages_minib.mean()) / (advantages_minib.std() + 1e-8)<NewLine><NewLine>                _, newlogproba, entropy = self.actor.get_action(states_traj[minibatch_ind],<NewLine>                                                                actions_traj.long()[minibatch_ind])<NewLine>                ratio = (newlogproba - log_probs_traj[minibatch_ind]).exp()<NewLine><NewLine>                # actor loss<NewLine>                surr_loss = -advantages_minib * ratio<NewLine>                clipped_surr_loss = -advantages_minib * torch.clamp(ratio, 1 - self.epsilon,<NewLine>                                                                    1 + self.epsilon)<NewLine>                actor_loss_max = torch.max(surr_loss, clipped_surr_loss).mean()<NewLine>                entropy_loss = entropy.mean()<NewLine>                actor_loss = actor_loss_max - self.entropy_weight * entropy_loss<NewLine><NewLine>                # critic_loss<NewLine>                new_values = self.critic.get_value(states_traj[minibatch_ind]).view(-1)<NewLine>                if self.clipped_value_loss:<NewLine>                    critic_loss_unclipped = (new_values - returns_traj[minibatch_ind]) ** 2<NewLine>                    value_clipped = values_traj[minibatch_ind] + torch.clamp(new_values -<NewLine>                                                                             values_traj[minibatch_ind],<NewLine>                                                                             - self.epsilon, self.epsilon)<NewLine>                    critic_loss_clipped = (value_clipped - returns_traj[minibatch_ind]) ** 2<NewLine>                    critic_loss_max = torch.max(critic_loss_clipped, critic_loss_unclipped)<NewLine>                    critic_loss = 0.5 * critic_loss_max.mean() * self.critic_weight<NewLine>                else:<NewLine>                    critic_loss = 0.5 * (new_values - returns_traj[minibatch_ind] ** 2).mean() * self.critic_weight<NewLine><NewLine>                loss = actor_loss + critic_loss<NewLine><NewLine>                # critic backward implementation<NewLine>                self.critic_optimizer.zero_grad()<NewLine>                critic_loss.backward()<NewLine>                nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)<NewLine>                self.critic_optimizer.step()<NewLine><NewLine>                # actor backward implementation<NewLine>                self.actor_optimizer.zero_grad()<NewLine>                actor_loss.backward()<NewLine>                nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)<NewLine>                self.actor_optimizer.step()<NewLine><NewLine>        return actor_loss, critic_loss<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>You mean the input of the forward pass of a network?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can add this line:</p><NewLine><pre><code class=""lang-auto"">advantages_minib = advantages_minib.detach()<NewLine></code></pre><NewLine><p>after:</p><NewLine><pre><code class=""lang-auto"">if self.normalize_adv:<NewLine>    advantages_minib = (advantages_minib - advantages_minib.mean()) / (advantages_minib.std() + 1e-8)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""92999"" data-username=""iffiX""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/iffix/40/24443_2.png"" width=""20""/> iffiX:</div><NewLine><blockquote><NewLine><p><code>advantages_minib = advantages_minib.detach()</code></p><NewLine></blockquote><NewLine></aside><NewLine><p>It worked for the first mini batch, as both backwards were made succesfully but when I backwarded with the second mini batch, I got the same error for the critic backward function. So I added</p><NewLine><pre><code class=""lang-auto"">critic_loss.backward(retain_graph=True)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">actor_loss.backward(retain_graph=True)<NewLine></code></pre><NewLine><p>But got this error after the second pass:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [64, 1]], which is output 0 of TBackward, is at version 4; expected version 3 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, you should not add “retain_graph=True” because in PPO actor does not need gradient from critic.<br/><NewLine>you probably should also detach:</p><NewLine><pre><code class=""lang-auto"">returns_traj[minibatch_ind]<NewLine></code></pre><NewLine><p>basically you actor gets gradients through entropy and your critic gets gradients from value loss. Anything other than these two tensors should also be detached.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>No, you should not add “retain_graph=True” because in PPO actor does not need gradient from critic.</p><NewLine></blockquote><NewLine><p>So whenever we call  <code>retain_graph=True</code>  it applies to the next backward call, no matter to which network we applied it initially?</p><NewLine><p>Thank you so much for the help! It runs perfectly right now!</p><NewLine><p>I would really appreciate it if you have time to explain to me why <code>returns_traj[minibatch_ind].detach()</code> and <code>advantages_minib = advantages_minib.detach()</code> played so important role in this error.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>retain_graph</code> basically keeps the forward(or backward) path after the backward call, however, torch will use buffers to store temporary data in some forward calls, and these data are utilized in their respective backward pass, their version will be stored so that any update on these buffers will be noticed by torch and an error will be thrown to notify users of this. This mechanism also applies to parameters. Parameters and Buffers are basically tensors with special wraps.</p><NewLine><p>when you call “step”, the optimizer will perform parameter updates, thus their version number will be increased, therefore you got this error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [64, 1]], which is output 0 of TBackward, is at version 4; expected version 3 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).<NewLine></code></pre><NewLine><p>“retain_graph” is mainly used to perform multiple backward pass on networks with multiple outputs (i.e.: multi-head output) before a final call to <code>step()</code>.</p><NewLine><p>Detach stops gradient from flowing through that path, so you will not get those errors.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Okay now I see whats the point of <code>retain_graph</code>. Thank you for the explanation.</p><NewLine><p>You also said:</p><NewLine><blockquote><NewLine><p>basically you actor gets gradients through entropy and your critic gets gradients from value loss.</p><NewLine></blockquote><NewLine><p>This means that the actor network is being updated through actor loss, which consists of the surrogate loss (clipped or unclipped) discounted by the entropy. We thus update the weights of the actor based on backpropagatiion of the entropy gradeints in order to change the policy distribution.</p><NewLine><p>As for the critic, we detached the returns. And so the gradients of the predicted values from the critic network are going to backpropagate and update the weights.</p><NewLine><p>Are the above statements true?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>correct.</p><NewLine><p>And actor will also receive gradients from entropy if you are adding entropy loss to actor loss.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>You know even if you have a good understanding of an algorithm, its code execution and details like these, are difficult to be recognized by only reading the paper and pseudocode.</p><NewLine><p>Again, thank you very much! <a class=""mention"" href=""/u/iffix"">@iffiX</a></p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p><img alt="":smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smile.png?v=9"" title="":smile:""/> Glad I could help</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vmirly1; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Kimonili; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Kimonili; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Kimonili; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Kimonili; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Kimonili; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/Kimonili; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: August 16, 2020,  1:54pm; <NewLine> REPLY_DATE 2: August 16, 2020,  1:57pm; <NewLine> REPLY_DATE 3: August 16, 2020,  2:03pm; <NewLine> REPLY_DATE 4: August 16, 2020,  2:05pm; <NewLine> REPLY_DATE 5: August 16, 2020,  3:00pm; <NewLine> REPLY_DATE 6: August 16, 2020,  2:47pm; <NewLine> REPLY_DATE 7: August 16, 2020,  2:51pm; <NewLine> REPLY_DATE 8: August 16, 2020,  3:02pm; <NewLine> REPLY_DATE 9: August 16, 2020,  3:09pm; <NewLine> REPLY_DATE 10: August 16, 2020,  3:26pm; <NewLine> REPLY_DATE 11: August 16, 2020,  3:28pm; <NewLine> REPLY_DATE 12: August 16, 2020,  3:31pm; <NewLine> REPLY_DATE 13: August 16, 2020,  3:32pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> 
92690,Custom loss function dependent on the samples,2020-08-13T13:28:54.825Z,1,74,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am working with A2C and A3C systems. I have an environment where at each step, I can do several action, each action at his own advantage. I am trying to design a custom loss function where I can vary the advantage (delta) for each datapoint, but I stumble upon some issue when I tried to do that:</p><NewLine><pre><code class=""lang-auto"">import numpy as np<NewLine><NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine><NewLine><NewLine>class Net(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Net, self).__init__()<NewLine>        self.fc = nn.Linear(10, 4)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.fc(x)<NewLine>        x = nn.Softmax(dim=1)(x)<NewLine>        return x<NewLine><NewLine><NewLine>def custom_loss(delta):<NewLine>    def loss(y_pred, y_true):<NewLine>        y_pred_clamped = torch.clamp(y_pred, 1e-8, 1 - 1e-8)<NewLine>        log_likelihood = y_true * torch.log(y_pred_clamped)<NewLine>        return torch.sum(-log_likelihood * delta)<NewLine>    return loss<NewLine><NewLine><NewLine>batch_size = 32<NewLine>n_sample = 10000<NewLine>network = Net()<NewLine>optimizer = optim.Adam(network.parameters(), lr=0.01)<NewLine><NewLine>x = torch.ones((1,10))<NewLine>print(network(x), '\n')<NewLine><NewLine>target = [0.2, 0.4, 0.3, 0.1]<NewLine><NewLine>for i in range(int(n_sample/batch_size)):<NewLine>    optimizer.zero_grad()<NewLine>    <NewLine>    delta = torch.ones((batch_size))  # Each sample of the dataset has its own delta, but by simplicity I set them all at 1<NewLine>    inputs = torch.ones((batch_size,10))<NewLine>    targets = torch.FloatTensor(batch_size*[target])<NewLine>    outputs = network(inputs)<NewLine>    <NewLine>    loss = custom_loss(delta)(outputs, targets)<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine><NewLine>x = torch.ones((1,10), dtype=torch.float)<NewLine>print(network(x))<NewLine></code></pre><NewLine><p>This returns:</p><NewLine><blockquote><NewLine><p>RuntimeError: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 1</p><NewLine></blockquote><NewLine><p>I understand why it doesn’t work, and I understand that it is a weird question. But it would be very convenient for me if something like this could work. I am guessing that the loss function is computed like it was one line at the time (even if I assume it’s parallelized). Maybe it is possible to vectorise the loss part ?</p><NewLine><p>Any help would be appreciated!<br/><NewLine>Thomas,</p><NewLine></div>",https://discuss.pytorch.org/u/thomas,,thomas,"August 13, 2020,  9:31pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>When does the error appear? Is it when you do <code>-log_likelihood * delta</code> ?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>yes!</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-3-c142911ed308&gt; in &lt;module&gt;<NewLine>     43     outputs = network(inputs)<NewLine>     44 <NewLine>---&gt; 45     loss = custom_loss(delta)(outputs, targets)<NewLine>     46     loss.backward()<NewLine>     47     optimizer.step()<NewLine><NewLine>&lt;ipython-input-3-c142911ed308&gt; in loss(y_pred, y_true)<NewLine>     21         y_pred_clamped = torch.clamp(y_pred, 1e-8, 1 - 1e-8)<NewLine>     22         log_likelihood = y_true * torch.log(y_pred_clamped)<NewLine>---&gt; 23         return torch.sum(-log_likelihood * delta)<NewLine>     24     return loss<NewLine>     25 <NewLine><NewLine>RuntimeError: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 1<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found the solution <a href=""https://discuss.pytorch.org/t/how-can-i-compute-3d-tensor-2d-tensor-multiplication/639"">here</a>. I thought for some reason that the loss was computed differently because of the error message. Turns out it is pretty easy to do it by simply changing the loss function to :</p><NewLine><pre><code class=""lang-auto"">def custom_loss(delta):<NewLine>    def loss(y_pred, y_true):<NewLine>        y_pred_clamped = torch.clamp(y_pred, 1e-8, 1 - 1e-8)<NewLine>        log_likelihood = y_true * torch.log(y_pred_clamped)<NewLine>        return torch.sum(-log_likelihood * delta.unsqueeze(-1).expand_as(log_likelihood))<NewLine>    return loss<NewLine></code></pre><NewLine><p>Thanks alban for your time and making me realize my mistake <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/thomas; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/thomas; <NewLine> ,"REPLY_DATE 1: August 13, 2020,  3:28pm; <NewLine> REPLY_DATE 2: August 13, 2020,  8:55pm; <NewLine> REPLY_DATE 3: August 13, 2020,  9:09pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
92592,"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [1024, 2]], which is output 0 of TBackward, is at version 2; expected version 1 instead",2020-08-12T16:48:47.922Z,0,110,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi. I’m trying to implement CNN-LSTM. On second loop of learning I’m getting error. What is the cause?  I don’t have ‘+=’ operations(I think), but squeezing and unsqueezing. But even if I had inplace operations why does it happens?  Was twiking some, but still haven’t found the correct solution.</p><NewLine><pre><code class=""lang-auto"">class A2C(nn.Module):<NewLine>    def __init__(self, input_size, num_actions, args, convolution=True):<NewLine>        super(A2C, self).__init__()<NewLine>        self.num_actions = num_actions<NewLine>        self.convolution = convolution<NewLine>        self.sequence_lenght = args.sequence_lenght<NewLine>        self.batch_size = args.batch_size<NewLine>        self.lstm_hidden_size = args.lstm_hidden_size<NewLine><NewLine>        if convolution:<NewLine>            convolution_layers = [nn.Conv2d(input_size[0], 512, kernel_size=8, stride=4), nn.ReLU(),<NewLine>                                  nn.Conv2d(512, 256, kernel_size=4, stride=2), nn.ReLU(),<NewLine>                                  nn.Conv2d(256, 128, kernel_size=3, stride=2), nn.ReLU(),<NewLine>                                  nn.Conv2d(128, 64, kernel_size=3, stride=1), nn.ReLU()]<NewLine>            self.conv = nn.Sequential(*convolution_layers)<NewLine><NewLine>            self.input_size = self.get_conv_out(input_size)<NewLine><NewLine>        if convolution == False:<NewLine>            self.input_size = input_size[0]<NewLine><NewLine>        self.lstm = nn.LSTM(self.input_size, self.lstm_hidden_size, num_layers = 1, batch_first=True)<NewLine><NewLine>        if args.hidden_layers_num == 1:<NewLine>            layers_a = [nn.Linear(self.lstm_hidden_size, args.hidden_1), nn.ReLU(), nn.Linear(args.hidden_1, num_actions)]<NewLine>            layers_c = [nn.Linear(self.lstm_hidden_size, args.hidden_1), nn.ReLU(), nn.Linear(args.hidden_1, 1)]<NewLine>        if args.hidden_layers_num != 1:<NewLine>            layers_a = [nn.Linear(self.lstm_hidden_size, args.hidden_1), nn.ReLU(), nn.Linear(args.hidden_1, args.hidden_2), nn.ReLU(), nn.Linear(args.hidden_2, num_actions)]<NewLine>            layers_c = [nn.Linear(self.lstm_hidden_size, args.hidden_1), nn.ReLU(), nn.Linear(args.hidden_1, args.hidden_2), nn.ReLU(), nn.Linear(args.hidden_2, 1)]<NewLine><NewLine>        self.Actor = nn.Sequential(*layers_a)<NewLine>        self.Critic = nn.Sequential(*layers_c)<NewLine><NewLine>    def get_conv_out(self, shape):<NewLine>        o = self.conv(torch.zeros(1, *shape))<NewLine>        return int(np.prod(o.size()))<NewLine><NewLine>    def forward(self, obs, h_x, c_x):<NewLine>        obs = torch.FloatTensor([obs]).to(device)<NewLine><NewLine>        if self.convolution:<NewLine>            obs = self.conv(obs).view(1, -1).unsqueeze(0)<NewLine><NewLine>        obs, (h_x, c_x) = self.lstm(obs, (h_x, c_x))<NewLine><NewLine>        logits = self.Actor(obs).squeeze(0) # **&lt;&lt;&lt;&lt; ========ERROR**<NewLine>        values = self.Critic(obs).squeeze(0)<NewLine><NewLine>        action_probs = F.softmax(logits, dim=1).cpu().detach().numpy()[0]<NewLine>        action = np.random.choice(self.num_actions, p=action_probs)<NewLine>        return action, logits, values, h_x, c_x<NewLine><NewLine>    def init_hidden(self):<NewLine>        return torch.zeros(1, 1, self.lstm_hidden_size), torch.zeros(1, 1, self.lstm_hidden_size)<NewLine></code></pre><NewLine><p>This is my sample generator. Env is continuous and I’m taking n steps as sequence and saving last detached hidden state as start of another loop. (could be a problem if using multiple environments or detach in every env is enough to separate them?)</p><NewLine><pre><code class=""lang-auto"">def play(self, net, device):<NewLine>        <NewLine>        if self.first == True:<NewLine>            self.state = self.env.reset()<NewLine>            self.first = False<NewLine><NewLine>        done = False<NewLine><NewLine>        if self.h_x == None:<NewLine>            self.h_x, self.c_x = net.init_hidden()<NewLine><NewLine>        values = []<NewLine>        logits_ = []<NewLine>        actions = []<NewLine>        rewards = []<NewLine>        total_reward = 0.0<NewLine>        _idx = 0<NewLine>        while True:<NewLine>            action, logits, value, self.h_x, self.c_x = net(self.state, self.h_x.to('cuda'), self.c_x.to('cuda')) # **&lt;&lt;&lt;&lt; ========ERROR**<NewLine>            next_state, reward, done, _ = self.env.step(action)<NewLine>            if _idx == 0:<NewLine>                reward-=2*(self.env.trade_fees) * self.env.leverage * 10_000<NewLine>            _idx = _idx+1<NewLine>            values.append(value)<NewLine>            logits_.append(logits)<NewLine>            actions.append(action)<NewLine><NewLine>            if done and self.if_trading_env == False:<NewLine>                reward = -1 # &lt;---<NewLine>            rewards.append(reward)<NewLine>            total_reward+=reward<NewLine><NewLine>            self.state = next_state<NewLine><NewLine>            if len(actions) &gt;= args.sequence_lenght:<NewLine>                self.h_x = self.h_x.detach()<NewLine>                self.c_x = self.c_x.detach()<NewLine><NewLine>                return values, logits_, actions, discounted_rewards(rewards, self.gamma), total_reward<NewLine></code></pre><NewLine><p>My training loop.</p><NewLine><pre><code class=""lang-auto""> idx = 0<NewLine>    while True:<NewLine>        <NewLine>        batch_counter = 0<NewLine>        batch_values = []<NewLine>        batch_logits = []<NewLine>        batch_actions =[]<NewLine>        batch_vals_ref = []<NewLine>        while True:<NewLine>            for env in enviroments:<NewLine>                values, _logits, actions, vals_ref, total_reward = env.play(net, device) #**&lt;&lt;&lt;&lt; ========ERROR**<NewLine><NewLine>                batch_values.append(values)<NewLine>                batch_logits.append(_logits)<NewLine>                batch_actions.append(actions)<NewLine>                batch_vals_ref.append(vals_ref)<NewLine>                episodes_rewrds.append(total_reward)<NewLine>                batch_counter += 1<NewLine>                if batch_counter &gt;= args.batch_size:<NewLine>                    break<NewLine>            if batch_counter &gt;= args.batch_size:<NewLine>                break<NewLine><NewLine>        for i in range(len(batch_values)):<NewLine><NewLine>            torch.cuda.empty_cache()<NewLine><NewLine>            values_v = torch.stack(batch_values[i]).to(device)<NewLine>            logits_v = torch.stack(batch_logits[i]).squeeze(1).to(device)<NewLine>            actions_t = torch.LongTensor(batch_actions[i]).to(device)<NewLine>            vals_ref_v = torch.FloatTensor(batch_vals_ref[i]).to(device)<NewLine><NewLine>            net.zero_grad()<NewLine><NewLine>            value_loss = args.zeta * F.mse_loss(values_v.squeeze(-1).squeeze(-1), vals_ref_v) <NewLine>            advantage = vals_ref_v - values_v.detach()<NewLine>            log_probs = F.log_softmax(logits_v, dim=1)<NewLine>            log_prob_action = advantage * log_probs[range(len(actions_t)), actions_t]<NewLine>            policy_loss = - log_prob_action.mean()<NewLine>            actions_probs = F.softmax(logits_v, dim=1)<NewLine>            entropy_loss = - args.entropy_beta * (actions_probs * log_probs).sum(dim=1).mean()<NewLine>            total_policy_loss = policy_loss + entropy_loss<NewLine><NewLine>            total_policy_loss.backward(retain_graph=True) # **&lt;&lt;&lt;&lt; ========ERROR**<NewLine>            value_loss.backward()<NewLine>            nn_utils.clip_grad_norm_(net.parameters(), args.clip_grad)<NewLine>            optimizer.step()<NewLine><NewLine>        idx +=1<NewLine>        print(idx, round(np.mean(episodes_rewrds), 2))<NewLine>        torch.save(net.state_dict(), NET_PARAMS_PATH)<NewLine><NewLine>        if np.mean(episodes_rewrds) &gt; 1_000_000:<NewLine>            break<NewLine></code></pre><NewLine><p>And this is my error.</p><NewLine><pre><code class=""lang-auto"">Warning: Error detected in MmBackward. Traceback of forward call that caused the error:<NewLine>  File ""E:\Market Data Collection\crypto_gym\A2C_LSTM_multi_1.0.py"", line 320, in &lt;module&gt;<NewLine>    values, _logits, actions, vals_ref, total_reward = env.play(net, device)<NewLine>  File ""E:\Market Data Collection\crypto_gym\A2C_LSTM_multi_1.0.py"", line 257, in play<NewLine>    action, logits, value, self.h_x, self.c_x = net(self.state, self.h_x.to('cuda'), self.c_x.to('cuda'))<NewLine>  File ""C:\Python38\lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""E:\Market Data Collection\crypto_gym\A2C_LSTM_multi_1.0.py"", line 202, in forward<NewLine>    logits = self.Actor(obs).squeeze(0)<NewLine>  File ""C:\Python38\lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""C:\Python38\lib\site-packages\torch\nn\modules\container.py"", line 100, in forward<NewLine>    input = module(input)<NewLine>  File ""C:\Python38\lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""C:\Python38\lib\site-packages\torch\nn\modules\linear.py"", line 87, in forward<NewLine>    return F.linear(input, self.weight, self.bias)<NewLine>  File ""C:\Python38\lib\site-packages\torch\nn\functional.py"", line 1612, in linear<NewLine>    output = input.matmul(weight.t())<NewLine> (print_stack at ..\torch\csrc\autograd\python_anomaly_mode.cpp:60)<NewLine>Traceback (most recent call last):<NewLine>  File ""E:\Market Data Collection\crypto_gym\A2C_LSTM_multi_1.0.py"", line 355, in &lt;module&gt;<NewLine>    total_policy_loss.backward(retain_graph=True)<NewLine>  File ""C:\Python38\lib\site-packages\torch\tensor.py"", line 198, in backward<NewLine>    torch.autograd.backward(self, gradient, retain_graph, create_graph)<NewLine>  File ""C:\Python38\lib\site-packages\torch\autograd\__init__.py"", line 98, in backward<NewLine>    Variable._execution_engine.run_backward(<NewLine>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [1024, 2]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!<NewLine></code></pre><NewLine><p>Also have a question. Can I shuffle samples from multiple batches for training when using LSTM in my code? Hidden states are already preserved in samples when I was adding them on environment iteration.</p><NewLine></div>",https://discuss.pytorch.org/u/Full,(Paul),Full,"August 12, 2020,  4:48pm",,,,,
87630,"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3, 1]], which is output 0 of TanhBackward, is at version 1; expected version 0 instead",2020-07-01T18:11:26.156Z,4,287,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey,</p><NewLine><p>I’m getting this error but i don’t understand why it has a problem with ‘action_value = torch.tanh(self.action_values(x))’ when I use relu or hardtanh there is no problem.</p><NewLine><pre><code class=""lang-auto"">class NAF(nn.Module):<NewLine>    def __init__(self, state_size, action_size,layer_size, n_step, seed):<NewLine>        super(NAF, self).__init__()<NewLine>        self.seed = torch.manual_seed(seed)<NewLine>        self.input_shape = state_size<NewLine>        self.action_size = action_size<NewLine>                <NewLine>        self.head_1 = nn.Linear(self.input_shape, layer_size)<NewLine>        self.ff_1 = nn.Linear(layer_size, layer_size)<NewLine>        self.action_values = nn.Linear(layer_size, action_size)<NewLine>        self.value = nn.Linear(layer_size, 1)<NewLine>        self.matrix_entries = nn.Linear(layer_size, int(self.action_size*(self.action_size+1)/2))<NewLine>        <NewLine><NewLine>    <NewLine>    def forward(self, input_):<NewLine>        """"""<NewLine>        <NewLine>        """"""<NewLine><NewLine>        x = torch.relu(self.head_1(input_))<NewLine>        x = torch.relu(self.ff_1(x))<NewLine>        action_value = torch.tanh(self.action_values(x))<NewLine>        entries = torch.tanh(self.matrix_entries(x))<NewLine>        V = self.value(x)<NewLine>        <NewLine><NewLine>        # create lower-triangular matrix<NewLine>        L = torch.zeros((input_.shape[0], self.action_size, self.action_size)).to(device)<NewLine><NewLine>        # get lower triagular indices<NewLine>        tril_indices = torch.tril_indices(row=self.action_size, col=self.action_size, offset=0)  <NewLine>        <NewLine>        # fill matrix with entries<NewLine>        L[:, tril_indices[0], tril_indices[1]] = entries#.clone().detach()<NewLine>        L.diagonal(dim1=1,dim2=2).exp_()<NewLine><NewLine>        # calculate state-dependent, positive-definite square matrix<NewLine>        P = L*L.transpose(1, 2)<NewLine><NewLine>        # calculate action:<NewLine>        dist = Normal(action_value, 1) #torch.inverse(P)<NewLine>        action = dist.sample()<NewLine>        action = torch.clamp(action, min=-1, max=1).unsqueeze(-1)<NewLine>        action_value.unsqueeze_(-1)<NewLine><NewLine>        # calculate Advantage:<NewLine>        A = (-0.5 * (action - action_value).transpose(1, 2) * P * (action - action_value)).squeeze(2)<NewLine>        <NewLine>        Q = A + V<NewLine><NewLine>        return action, Q, V<NewLine>    <NewLine></code></pre><NewLine><p>the problem only appears when trying to backprop</p><NewLine><pre><code class=""lang-auto"">    def learn(self, experiences):<NewLine>        """"""Update value parameters using given batch of experience tuples.<NewLine>        Params<NewLine>        ======<NewLine>            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples <NewLine>        """"""<NewLine>        self.optimizer.zero_grad()<NewLine>        states, actions, rewards, next_states, dones = experiences<NewLine><NewLine>        # get the Value for the next state from target model<NewLine>        with torch.no_grad():<NewLine>            _, _, V_ = self.qnetwork_target(next_states)<NewLine><NewLine>        # Compute Q targets for current states <NewLine>        V_targets = rewards + (self.GAMMA * V_ * (1 - dones))<NewLine>        <NewLine>        # Get expected Q values from local model<NewLine>        _, Q, _ = self.qnetwork_local(states)<NewLine><NewLine>        # Compute loss<NewLine>        loss = F.mse_loss(Q, V_targets)<NewLine>        <NewLine>        # Minimize the loss<NewLine>        loss.backward()<NewLine>        #clip_grad_norm_(self.qnetwork_local.parameters(),1)<NewLine>        self.optimizer.step()<NewLine><NewLine>        # ------------------- update target network ------------------- #<NewLine>        self.soft_update(self.qnetwork_local, self.qnetwork_target)<NewLine>        return loss.detach().cpu().numpy()           <NewLine></code></pre><NewLine><p>hope you guys can help</p><NewLine></div>",https://discuss.pytorch.org/u/Se_di,(Se di),Se_di,"July 1, 2020,  6:12pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The issue is created by the inplace <code>unsqueeze_</code> call on <code>action_value</code>, but is raised in the <code>tanh</code>.<br/><NewLine>If you use <code>action_value = action_value.unsqueeze(-1)</code> instead, your code should work.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>indeed now it works, thank you!!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, I also encountered with the similar error. Checked in the same way you posted but still didn’t figure out what’s actually going wrong. Below are the code</p><NewLine><pre><code class=""lang-auto"">def run_facial_transfer(args):<NewLine>    g_all = nn.Sequential(OrderedDict([<NewLine>        ('g_mapping', G_mapping()),<NewLine>        #('truncation', Truncation(avg_latent)),<NewLine>        ('g_synthesis', G_synthesis(resolution=1024))    <NewLine>        ]))<NewLine><NewLine>    g_all.load_state_dict(torch.load(args.weight_file, map_location=device))<NewLine>    g_all.eval()<NewLine>    g_all.to(device)<NewLine>    g_mapping,g_synthesis=g_all[0],g_all[1]<NewLine><NewLine><NewLine>    img_0=image_reader(args.src_im1, 1024) #(1,3,1024,1024) -1~1<NewLine>    img_0=img_0.to(device)<NewLine><NewLine>    img_1=image_reader(args.src_im2, 1024)<NewLine>    img_1=img_1.to(device) #(1,3,1024,1024)<NewLine><NewLine><NewLine>    MSE_Loss=nn.MSELoss(reduction=""mean"")<NewLine>    upsample2d=torch.nn.Upsample(scale_factor=0.5, mode='bilinear')<NewLine><NewLine>    img_p0=img_0.clone() #resize for perceptual net<NewLine>    img_p0=upsample2d(img_p0)<NewLine>    img_p0=upsample2d(img_p0) #(1,3,256,256)<NewLine><NewLine>    img_p1=img_1.clone()<NewLine>    img_p1=upsample2d(img_p1)<NewLine>    img_p1=upsample2d(img_p1) #(1,3,256,256)<NewLine><NewLine><NewLine>    perceptual_net=VGG16_for_Perceptual(n_layers=[2,4,14,21]).to(device) #conv1_1,conv1_2,conv2_2,conv3_3<NewLine>    dlatent_a=torch.zeros((1,18,512),requires_grad=True,device=device) #appearace latent s1<NewLine>    dlatent_e=torch.zeros((1,18,512),requires_grad=True,device=device) # expression latent s2<NewLine>    optimizer=optim.Adam({dlatent_a,dlatent_e},lr=0.01,betas=(0.9,0.999),eps=1e-8)<NewLine><NewLine>    alpha=torch.zeros((1,18,512)).to(device)<NewLine>    alpha[:,3:5,:]=1<NewLine><NewLine>    print(""Start"")<NewLine>    loss_list=[]<NewLine>    for i in range(args.iteration):<NewLine>        optimizer.zero_grad()<NewLine>        synth_img_a=g_synthesis(dlatent_a)<NewLine>        synth_img_a= (synth_img_a + 1.0) / 2.0<NewLine><NewLine>        synth_img_e=g_synthesis(dlatent_e)<NewLine>        synth_img_e= (synth_img_e + 1.0) / 2.0<NewLine><NewLine>        loss_1=caluclate_contentloss(synth_img_a,perceptual_net,img_p1,MSE_Loss,upsample2d)<NewLine>        print(loss_1.shape)<NewLine>        loss_1.backward()<NewLine>        optimizer.step()<NewLine><NewLine>        loss_2=caluclate_styleloss(synth_img_e,img_p0,perceptual_net,upsample2d)<NewLine>        loss_2.backward()<NewLine>        optimizer.step()<NewLine><NewLine>        loss_1=loss_1.detach().cpu().numpy()<NewLine>        loss_2=loss_2.detach().cpu().numpy()<NewLine><NewLine><NewLine>        dlatent1=dlatent_a*alpha+dlatent_e*(1-alpha)<NewLine>        dlatent2=dlatent_a*(1-alpha)+dlatent_e*alpha<NewLine><NewLine>        synth_img1=g_synthesis(dlatent1)<NewLine>        synth_img1= (synth_img1 + 1.0) / 2.0<NewLine><NewLine>        synth_img2=g_synthesis(dlatent2)<NewLine>        synth_img2= (synth_img2 + 1.0) / 2.0<NewLine><NewLine>        if i%10==0:<NewLine>            print(""iter{}:   loss0 --{},  loss1 --{}"".format(i,loss_1,loss_2))<NewLine>            save_image(synth_img_a.clamp(0,1),""save_image/exchange/a/{}_a.png"".format(i))<NewLine>            save_image(synth_img_e.clamp(0,1),""save_image/exchange/e/{}_e.png"".format(i))<NewLine>            save_image(synth_img1.clamp(0,1),""save_image/exchange/result1/{}_exchange1.png"".format(i))<NewLine>            save_image(synth_img2.clamp(0,1),""save_image/exchange/result2/{}_exchange2.png"".format(i))<NewLine><NewLine><NewLine><NewLine>            # np.save(""latent_W/exchange1.npy"",dlatent1.detach().cpu().numpy())<NewLine>            # np.save(""latent_W/exchange2.npy"",dlatent2.detach().cpu().numpy())<NewLine>        <NewLine><NewLine>def caluclate_contentloss(synth_img,perceptual_net,img_p,MSE_Loss,upsample2d): #W_l<NewLine><NewLine>     real_0,real_1,real_2,real_3=perceptual_net(img_p)<NewLine>     synth_p=upsample2d(synth_img) #(1,3,256,256)<NewLine>     synth_p=upsample2d(synth_p)<NewLine>     synth_0,synth_1,synth_2,synth_3=perceptual_net(synth_p)<NewLine><NewLine>     perceptual_loss=0<NewLine><NewLine><NewLine>     perceptual_loss=MSE_Loss(synth_0,real_0)+perceptual_loss<NewLine>     perceptual_loss=MSE_Loss(synth_1,real_1)+perceptual_loss<NewLine><NewLine>     perceptual_loss=MSE_Loss(synth_2,real_2)+perceptual_loss<NewLine>     perceptual_loss=MSE_Loss(synth_3,real_3)+perceptual_loss<NewLine><NewLine><NewLine><NewLine>     return perceptual_loss<NewLine><NewLine><NewLine><NewLine>class StyleLoss(nn.Module):<NewLine>     def __init__(self, target_feature):<NewLine>          super(StyleLoss, self).__init__()<NewLine>          self.target = self.gram_matrix(target_feature).detach()<NewLine>     def forward(self, input):<NewLine>          G = self.gram_matrix(input)<NewLine>          self.loss = F.mse_loss(G, self.target)<NewLine>          return self.loss<NewLine>     def gram_matrix(self,input):<NewLine>          a, b, c, d = input.size()  <NewLine>          features = input.view(a * b, c * d)  <NewLine><NewLine>          G = torch.mm(features, features.t())  <NewLine>          return G.div(a * b * c * d)<NewLine><NewLine><NewLine>def caluclate_styleloss(synth_img,img_p,perceptual_net,upsample2d):<NewLine><NewLine>     synth_p=upsample2d(synth_img) #(1,3,256,256)<NewLine>     synth_p=upsample2d(synth_p)<NewLine><NewLine>     _,_,_,style_real=perceptual_net(img_p) #conv3_3<NewLine>     _,_,_,style_synth=perceptual_net(synth_p)<NewLine><NewLine>     style_loss=StyleLoss(style_real)<NewLine><NewLine>     loss=style_loss(style_synth)<NewLine><NewLine>     return loss<NewLine></code></pre><NewLine><p>run_facial_transfer(args) then get this,<br/><NewLine><strong>one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [1, 512]], which is output 0 of SelectBackward, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).</strong></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t see any inplace operations in the posted code, so I guess the error might be raised by an inplace op in the models.<br/><NewLine>Did you enable anomaly detection and rerun the code?<br/><NewLine>If so, could you post the complete stack trace here?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi. I’m trying to implement CNN-LSTM and also have same problem. I don’t have ‘+=’ operations(I think), but squeezing and unsqueezing. Was twiking some, but still haven’t found the correct solution. I marked where the error occurs in the code with # <strong>&lt;&lt;&lt;&lt; ========ERROR</strong> .</p><NewLine><pre><code class=""lang-auto"">class A2C(nn.Module):<NewLine>    def __init__(self, input_size, num_actions, args, convolution=True):<NewLine>        super(A2C, self).__init__()<NewLine>        self.num_actions = num_actions<NewLine>        self.convolution = convolution<NewLine>        self.sequence_lenght = args.sequence_lenght<NewLine>        self.batch_size = args.batch_size<NewLine>        self.lstm_hidden_size = args.lstm_hidden_size<NewLine><NewLine>        if convolution:<NewLine>            convolution_layers = [nn.Conv2d(input_size[0], 512, kernel_size=8, stride=4), nn.ReLU(),<NewLine>                                  nn.Conv2d(512, 256, kernel_size=4, stride=2), nn.ReLU(),<NewLine>                                  nn.Conv2d(256, 128, kernel_size=3, stride=2), nn.ReLU(),<NewLine>                                  nn.Conv2d(128, 64, kernel_size=3, stride=1), nn.ReLU()]<NewLine>            self.conv = nn.Sequential(*convolution_layers)<NewLine><NewLine>            self.input_size = self.get_conv_out(input_size)<NewLine><NewLine>        if convolution == False:<NewLine>            self.input_size = input_size[0]<NewLine><NewLine>        self.lstm = nn.LSTM(self.input_size, self.lstm_hidden_size, num_layers = 1, batch_first=True)<NewLine><NewLine>        if args.hidden_layers_num == 1:<NewLine>            layers_a = [nn.Linear(self.lstm_hidden_size, args.hidden_1), nn.ReLU(), nn.Linear(args.hidden_1, num_actions)]<NewLine>            layers_c = [nn.Linear(self.lstm_hidden_size, args.hidden_1), nn.ReLU(), nn.Linear(args.hidden_1, 1)]<NewLine>        if args.hidden_layers_num != 1:<NewLine>            layers_a = [nn.Linear(self.lstm_hidden_size, args.hidden_1), nn.ReLU(), nn.Linear(args.hidden_1, args.hidden_2), nn.ReLU(), nn.Linear(args.hidden_2, num_actions)]<NewLine>            layers_c = [nn.Linear(self.lstm_hidden_size, args.hidden_1), nn.ReLU(), nn.Linear(args.hidden_1, args.hidden_2), nn.ReLU(), nn.Linear(args.hidden_2, 1)]<NewLine><NewLine>        self.Actor = nn.Sequential(*layers_a)<NewLine>        self.Critic = nn.Sequential(*layers_c)<NewLine><NewLine>    def get_conv_out(self, shape):<NewLine>        o = self.conv(torch.zeros(1, *shape))<NewLine>        return int(np.prod(o.size()))<NewLine><NewLine>    def forward(self, obs, h_x, c_x):<NewLine>        obs = torch.FloatTensor([obs]).to(device)<NewLine><NewLine>        if self.convolution:<NewLine>            batch_size = obs.size()[0]<NewLine>            obs = self.conv(obs).view(1, -1).unsqueeze(0)<NewLine><NewLine>        obs, (h_x, c_x) = self.lstm(obs, (h_x, c_x))<NewLine><NewLine>        logits = self.Actor(obs).squeeze(0) # **&lt;&lt;&lt;&lt; ========ERROR**<NewLine>        values = self.Critic(obs).squeeze(0)<NewLine><NewLine>        action_probs = F.softmax(logits, dim=1).cpu().detach().numpy()[0]<NewLine>        action = np.random.choice(self.num_actions, p=action_probs)<NewLine>        return action, logits, values, h_x, c_x<NewLine><NewLine>    def init_hidden(self):<NewLine>        return torch.zeros(1, 1, self.lstm_hidden_size), torch.zeros(1, 1, self.lstm_hidden_size)<NewLine></code></pre><NewLine><p>This is my sample generator. Env is continuous  and I’m taking n steps as sequence and saving last detached hidden state as start of another loop. (could be a problem if using multiple environments or detach in every env is enough to separate them?)</p><NewLine><pre><code class=""lang-auto"">def play(self, net, device):<NewLine>        <NewLine>        if self.first == True:<NewLine>            self.state = self.env.reset()<NewLine>            self.first = False<NewLine><NewLine>        done = False<NewLine><NewLine>        if self.h_x == None:<NewLine>            self.h_x, self.c_x = net.init_hidden()<NewLine><NewLine>        values = []<NewLine>        logits_ = []<NewLine>        actions = []<NewLine>        rewards = []<NewLine>        total_reward = 0.0<NewLine>        _idx = 0<NewLine>        while True:<NewLine>            action, logits, value, self.h_x, self.c_x = net(self.state, self.h_x.to('cuda'), self.c_x.to('cuda')) # **&lt;&lt;&lt;&lt; ========ERROR**<NewLine>            next_state, reward, done, _ = self.env.step(action)<NewLine>            if _idx == 0:<NewLine>                reward-=2*(self.env.trade_fees) * self.env.leverage * 10_000<NewLine>            _idx = _idx+1<NewLine>            values.append(value)<NewLine>            logits_.append(logits)<NewLine>            actions.append(action)<NewLine><NewLine>            if done and self.if_trading_env == False:<NewLine>                reward = -1 # &lt;---<NewLine>            rewards.append(reward)<NewLine>            total_reward+=reward<NewLine><NewLine>            self.state = next_state<NewLine><NewLine>            if len(actions) &gt;= args.sequence_lenght:<NewLine>                self.h_x = self.h_x.detach()<NewLine>                self.c_x = self.c_x.detach()<NewLine><NewLine>                return values, logits_, actions, discounted_rewards(rewards, self.gamma), total_reward<NewLine></code></pre><NewLine><p>My training loop.</p><NewLine><pre><code class=""lang-auto"">    idx = 0<NewLine>    while True:<NewLine>        <NewLine>        batch_counter = 0<NewLine>        batch_values = []<NewLine>        batch_logits = []<NewLine>        batch_actions =[]<NewLine>        batch_vals_ref = []<NewLine>        while True:<NewLine>            for env in enviroments:<NewLine>                values, _logits, actions, vals_ref, total_reward = env.play(net, device) #**&lt;&lt;&lt;&lt; ========ERROR**<NewLine><NewLine>                batch_values.append(values)<NewLine>                batch_logits.append(_logits)<NewLine>                batch_actions.append(actions)<NewLine>                batch_vals_ref.append(vals_ref)<NewLine>                episodes_rewrds.append(total_reward)<NewLine>                batch_counter += 1<NewLine>                if batch_counter &gt;= args.batch_size:<NewLine>                    break<NewLine>            if batch_counter &gt;= args.batch_size:<NewLine>                break<NewLine><NewLine>        for i in range(len(batch_values)):<NewLine><NewLine>            torch.cuda.empty_cache()<NewLine><NewLine>            values_v = torch.stack(batch_values[i]).to(device)<NewLine>            logits_v = torch.stack(batch_logits[i]).squeeze(1).to(device)<NewLine>            actions_t = torch.LongTensor(batch_actions[i]).to(device)<NewLine>            vals_ref_v = torch.FloatTensor(batch_vals_ref[i]).to(device)<NewLine><NewLine>            net.zero_grad()<NewLine><NewLine>            value_loss = args.zeta * F.mse_loss(values_v.squeeze(-1).squeeze(-1), vals_ref_v) <NewLine>            advantage = vals_ref_v - values_v.detach()<NewLine>            log_probs = F.log_softmax(logits_v, dim=1)<NewLine>            log_prob_action = advantage * log_probs[range(len(actions_t)), actions_t]<NewLine>            policy_loss = - log_prob_action.mean()<NewLine>            actions_probs = F.softmax(logits_v, dim=1)<NewLine>            entropy_loss = - args.entropy_beta * (actions_probs * log_probs).sum(dim=1).mean()<NewLine>            total_policy_loss = policy_loss + entropy_loss<NewLine><NewLine>            total_policy_loss.backward(retain_graph=True) # **&lt;&lt;&lt;&lt; ========ERROR**<NewLine>            value_loss.backward()<NewLine>            nn_utils.clip_grad_norm_(net.parameters(), args.clip_grad)<NewLine>            optimizer.step()<NewLine><NewLine>        idx +=1<NewLine>        print(idx, round(np.mean(episodes_rewrds), 2))<NewLine>        torch.save(net.state_dict(), NET_PARAMS_PATH)<NewLine><NewLine>        if np.mean(episodes_rewrds) &gt; 1_000_000:<NewLine>            break<NewLine></code></pre><NewLine><p>And this is my error.</p><NewLine><pre><code class=""lang-auto"">Warning: Error detected in MmBackward. Traceback of forward call that caused the error:<NewLine>  File ""E:\Market Data Collection\crypto_gym\A2C_LSTM_multi_1.0.py"", line 320, in &lt;module&gt;<NewLine>    values, _logits, actions, vals_ref, total_reward = env.play(net, device)<NewLine>  File ""E:\Market Data Collection\crypto_gym\A2C_LSTM_multi_1.0.py"", line 257, in play<NewLine>    action, logits, value, self.h_x, self.c_x = net(self.state, self.h_x.to('cuda'), self.c_x.to('cuda'))<NewLine>  File ""C:\Python38\lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""E:\Market Data Collection\crypto_gym\A2C_LSTM_multi_1.0.py"", line 202, in forward<NewLine>    logits = self.Actor(obs).squeeze(0)<NewLine>  File ""C:\Python38\lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""C:\Python38\lib\site-packages\torch\nn\modules\container.py"", line 100, in forward<NewLine>    input = module(input)<NewLine>  File ""C:\Python38\lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""C:\Python38\lib\site-packages\torch\nn\modules\linear.py"", line 87, in forward<NewLine>    return F.linear(input, self.weight, self.bias)<NewLine>  File ""C:\Python38\lib\site-packages\torch\nn\functional.py"", line 1612, in linear<NewLine>    output = input.matmul(weight.t())<NewLine> (print_stack at ..\torch\csrc\autograd\python_anomaly_mode.cpp:60)<NewLine>Traceback (most recent call last):<NewLine>  File ""E:\Market Data Collection\crypto_gym\A2C_LSTM_multi_1.0.py"", line 355, in &lt;module&gt;<NewLine>    total_policy_loss.backward(retain_graph=True)<NewLine>  File ""C:\Python38\lib\site-packages\torch\tensor.py"", line 198, in backward<NewLine>    torch.autograd.backward(self, gradient, retain_graph, create_graph)<NewLine>  File ""C:\Python38\lib\site-packages\torch\autograd\__init__.py"", line 98, in backward<NewLine>    Variable._execution_engine.run_backward(<NewLine>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [1024, 2]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!<NewLine></code></pre><NewLine><p>Also have a question. Can I shuffle samples from multiple batches for training when using LSTM in my code. Hidden states are already preserved in samples when I was adding them on environment iteration?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Se_di; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ged_nigel; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Full; <NewLine> ,"REPLY_DATE 1: July 2, 2020,  2:43pm; <NewLine> REPLY_DATE 2: July 2, 2020,  2:43pm; <NewLine> REPLY_DATE 3: August 1, 2020,  6:46pm; <NewLine> REPLY_DATE 4: August 2, 2020,  2:27am; <NewLine> REPLY_DATE 5: August 9, 2020,  9:11pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
91799,addcdiv_() takes 2 positional arguments but 3 were given,2020-08-06T04:41:32.330Z,0,76,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am implementing SharedAdam optimizer in pytorch and calling super(…).step functions inheriting torch.optim.Adam class.</p><NewLine><pre><code>Implementing the class as below:- <NewLine><NewLine>class SharedAdam(optim.Adam): # object that inherits from optim.Adam<NewLine><NewLine>    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):<NewLine>        super(SharedAdam, self).__init__(params, lr, betas, eps, weight_decay) # inheriting from the tools of optim.Adam<NewLine>        for group in self.param_groups: # self.param_groups contains all the attributes of the optimizer, including the parameters to optimize (the weights of the network) contained in self.param_groups['params']<NewLine>            for p in group['params']: # for each tensor p of weights to optimize<NewLine>                state = self.state[p] # at the beginning, self.state is an empty dictionary so state = {} and self.state = {p:{}} = {p: state}<NewLine>                state['step'] = torch.zeros(1) # counting the steps: state = {'step' : tensor([0])}<NewLine>                state['exp_avg'] = p.data.new().resize_as_(p.data).zero_() # the update of the adam optimizer is based on an exponential moving average of the gradient (moment 1)<NewLine>                state['exp_avg_sq'] = p.data.new().resize_as_(p.data).zero_() # the update of the adam optimizer is also based on an exponential moving average of the squared of the gradient (moment 2)<NewLine><NewLine>    # Sharing the memory<NewLine>    def share_memory(self):<NewLine>        for group in self.param_groups:<NewLine>            for p in group['params']:<NewLine>                state = self.state[p]<NewLine>                state['step'].share_memory_() # tensor.share_memory_() acts a little bit like tensor.cuda()<NewLine>                state['exp_avg'].share_memory_() # tensor.share_memory_() acts a little bit like tensor.cuda()<NewLine>                state['exp_avg_sq'].share_memory_() # tensor.share_memory_() acts a little bit like tensor.cuda()<NewLine><NewLine><NewLine>    def step(self):<NewLine>        super(SharedAdam,self).step()<NewLine></code></pre><NewLine><p>However I am getting below error.<br/><NewLine>Can anyone please help to let know what is the issue here…</p><NewLine><pre><code>File ""..torch\optim\adam.py"", line 106, in step<NewLine>    p.data.addcdiv_(-step_size, exp_avg, denom)<NewLine>TypeError: addcdiv_() takes 2 positional arguments but 3 were given<NewLine></code></pre><NewLine><p>Can anyone please help to let me know what should be the solution here.</p><NewLine></div>",https://discuss.pytorch.org/u/granth_jain,(granth jain),granth_jain,"August 6, 2020,  4:41am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Based on the error message and stack trace, it seems you might be mixing PyTorch versions.<br/><NewLine>Did you copy the <code>adam.py</code> file manually or is the error pointing to your local PyTorch installation?<br/><NewLine><code>adam.py</code> doesn’t contain this particular line of code anymore, as the <code>.data</code> attribute is deprecated.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Thanks for the reply, updating pytorch has fix for this issue.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/granth_jain; <NewLine> ,"REPLY_DATE 1: August 8, 2020, 10:01am; <NewLine> REPLY_DATE 2: August 8, 2020, 11:24am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
4460,Training gets slow down by each batch slowly,2017-06-30T01:22:45.587Z,20,12919,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there,</p><NewLine><p>I have a pre-trained model, and I added an actor-critic method into the model and trained only on the rl-related parameter (I fixed the parameters from pre-trained model). However, I noticed that the training speed gets slow down slowly at each batch and memory usage on GPU also increases. For example, the first batch only takes 10s and the 10k^th batch takes 40s to train.</p><NewLine><p>I am sure that all the pre-trained model’s parameters have been changed into mode “autograd=false”. There are only four parameters that are changing in the current program. I also noticed that if I changed the gradient clip threshlod, it would mitigate this phenomenon but the training will eventually get very slow still. For example, if I do not use any gradient clipping, the 1st batch takes 10s and 100th batch taks 400s to train. And if I set gradient clipping to 5, the 100th batch will only takes 12s (comparing to 1st batch only takes 10s).</p><NewLine><p>FYI, I am using SGD with learning rate equal to 0.0001.</p><NewLine><p>Is there anyone who knows what is going wrong with my code? I have been working on fixing this problem for two week…</p><NewLine><p>Many Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/cosmmb,,cosmmb,"June 30, 2017,  1:22am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>This is most likely due to your training loop holding on to some things it shouldn’t.<br/><NewLine>You should make sure to wrap your input into a Variable at every iteration. Also makes sure that you are not storing some temporary computations in an ever growing list without deleting them.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply! Your suggestions are really helpful. I deleted some variables that I generated during training for each batch. Currently, the memory usage would not increase but the training speed still gets slower batch-batch. I double checked the calculation of loss and I did not find anything that is accumulated from the previous batch. Do you know why it is still getting slower?</p><NewLine><p>I also tried another test. For example, the average training speed for epoch 1 is 10s. After I trained this model for a few hours, the average training speed for epoch 10 was slow down to 40s. So I just stopped the training and loaded the learned parameters from epoch 10, and restart the training again from epoch 10. I though if there is anything related to accumulated memory which slows down the training, the restart training will help. However, after I restarted the training from epoch 10, the speed got even slower, now it increased to 50s per epoch.</p><NewLine><p>Thanks!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>If you are using custom network/loss function, it is also possible that the computation gets more expensive as you get closer to the optimal solution?</p><NewLine><p>To track this down, you could get timings for different parts separately: data loading, network forward, loss computation, backward pass and parameter update. Hopefully just one will increase and you will be able to see better what is going on.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Problem confirmed. As for generating training data on-the-fly, the speed is very fast at beginning but significantly slow down after a few iterations (3000). At least 2-3 times slower.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have observed a similar slowdown in training with pytorch running under R using the reticulate package.<br/><NewLine>System: Linux pixel 4.4.0-66-generic <span class=""hashtag"">#87-Ubuntu</span> SMP Fri Mar 3 15:29:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux<br/><NewLine>Ubuntu 16.04.2 LTS<br/><NewLine>R version 3.4.2 (2017-09-28) with reticulate_1.2<br/><NewLine>Python 3.6.3 with pytorch version ‘0.2.0_3’</p><NewLine><p>Net architecture:</p><NewLine><p>Sequential (<br/><NewLine>(Linear-1): Linear (277 -&gt; 8)<br/><NewLine>(PReLU-1): PReLU (1)<br/><NewLine>(Linear-2): Linear (8 -&gt; 6)<br/><NewLine>(PReLU-2): PReLU (1)<br/><NewLine>(Linear-3): Linear (6 -&gt; 4)<br/><NewLine>(PReLU-3): PReLU (1)<br/><NewLine>(Linear-Last): Linear (4 -&gt; 1)<br/><NewLine>)</p><NewLine><p>Loss function: BCEWithLogitsLoss()<br/><NewLine>The run was CPU only (no GPU).  Although the system had multiple Intel Xeon E5-2640 v4 cores @ 2.40GHz, this run used only 1.<br/><NewLine>The net was trained with SGD, batch size 32.  Each batch contained a random selection of training records. There was a steady drop in number of batches processed per second over the course of 20000 batches, such that the last batches were about 4 to 1 slower than the first. Although memory requirements did increase over the course of the run, the system had a lot more memory than was needed, so the slowdown could not be attributed to paging.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Turns out I had declared the Variable tensors holding a batch of features and labels outside the loop over the 20000 batches, then filled them up for each batch.  Moving the declarations of those tensors inside the loop (which I thought would be less efficient) solved my slowdown problem.  Now the final batches take no more time than the initial ones.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I had the same problem with you, and solved it by your solution. Do you know why moving the declaration inside the loop can solve it ?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>It is because, since you’re working with <code>Variable</code>s, the history is saved for every operations you’re performing. And when you call <code>backward()</code>, the whole history is scanned.<br/><NewLine>So if you have a shared element in your training loop, the history just grows up and so the scanning takes more and more time.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>What is the right way of handling this now that <code>Tensor</code> also tracks history?</p><NewLine><p>I migrated to PyTorch 0.4 (e.g., removed some code wrapping tensors into variables), and now the training loop is getting progressily slower. I’m not sure where this problem is coming from. Is there a way of drawing the computational graphs that are currently being tracked by Pytorch? These issues seem hard to debug.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>If a shared tensor is not requires_grad, is its histroy still scanned?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>No if a tensor does not requires_grad, it’s history is not built when using it. Note that you cannot change this attribute after the forward pass to change how the backward behaves on an already created computational graph. It has to be set to False while you create the graph.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m experiencing the same issue with pytorch 0.4.1<br/><NewLine>I implemented adversarial training, with the cleverhans wrapper and at each batch the training time is increasing.<br/><NewLine>How can I track the problem down to find a solution?</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <img alt="":sweat_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/sweat_smile.png?v=6"" title="":sweat_smile:""/> Why does the the speed slow down when generating data on-the-fly(reading every batch from the hard disk while training)? Does that continue forever or does the speed stay the same after a number of iterations?</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>I observed the same problem. The solution in my case was replacing  <code>itertools.cycle()</code> on DataLoader by a standard <code>iter()</code> with handling StopIteration exception. You can also check if dev/shm increases during training.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have the same issue:</p><NewLine><p>0%|          | 0/66 [00:00&lt;?, ?it/s]<br/><NewLine>2%|▏         | 1/66 [05:53&lt;6:23:05, 353.62s/it]<br/><NewLine>3%|▎         | 2/66 [06:11&lt;4:29:46, 252.91s/it]<br/><NewLine>5%|▍         | 3/66 [06:28&lt;3:11:06, 182.02s/it]<br/><NewLine>6%|▌         | 4/66 [06:41&lt;2:15:39, 131.29s/it]<br/><NewLine>8%|▊         | 5/66 [06:43&lt;1:34:15, 92.71s/it]<br/><NewLine>9%|▉         | 6/66 [06:46&lt;1:05:41, 65.70s/it]<br/><NewLine>11%|█         | 7/66 [06:49&lt;46:00, 46.79s/it]<br/><NewLine>12%|█▏        | 8/66 [06:51&lt;32:26, 33.56s/it]<br/><NewLine>14%|█▎        | 9/66 [06:54&lt;23:04, 24.30s/it]<br/><NewLine>15%|█▌        | 10/66 [06:57&lt;16:37, 17.81s/it]<br/><NewLine>17%|█▋        | 11/66 [06:59&lt;12:09, 13.27s/it]<br/><NewLine>18%|█▊        | 12/66 [07:02&lt;09:04, 10.09s/it]<br/><NewLine>20%|█▉        | 13/66 [07:05&lt;06:56,  7.86s/it]<br/><NewLine>21%|██        | 14/66 [07:07&lt;05:27,  6.30s/it]</p><NewLine><p>Cannot understand this behavior… sometimes it takes 5 minutes for a mini batch or just a couple of seconds.</p><NewLine><ul><NewLine><li>my first epoch took me just 5 minutes.</li><NewLine></ul><NewLine><p>94%|█████████▍| 62/66 [05:06&lt;00:15, 3.96s/it]<br/><NewLine>95%|█████████▌| 63/66 [05:09&lt;00:10, 3.56s/it]<br/><NewLine>97%|█████████▋| 64/66 [05:11&lt;00:06, 3.29s/it]<br/><NewLine>98%|█████████▊| 65/66 [05:14&lt;00:03, 3.11s/it]</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>It turned out the batch size matters. So, my advice is to select a smaller batch size, also play around with the number of workers.</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, Could you please inform on how to clear the temporary computations ?</p><NewLine><p>thanks,</p><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p>You should not save from one iteration to the other a Tensor that has requires_grad=True. If you want to save it for later inspection (or accumulating the loss), you should <code>.detach()</code> it before. So that pytorch knows you won’t try and backpropagate through it.</p><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>The answer comes from here - <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/why-the-training-slow-down-with-time-if-training-continuously-and-gpu-utilization-begins-to-jitter-dramatically/11444/4?u=sizhky"">Why the training slow down with time if training continuously? And Gpu utilization begins to jitter dramatically?</a></p><NewLine><p>I used <code>torch.cuda.empty_cache()</code> at end of every loop</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/cosmmb; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ywu36; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/dslate; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/dslate; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/fengziyue; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/negrinho; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/acgtyrant; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/mciccone; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/Bassel; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/marcinplata; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/unnir; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/unnir; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/satheesh; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/sizhky; <NewLine> ,"REPLY_DATE 1: June 30, 2017,  9:00am; <NewLine> REPLY_DATE 2: June 30, 2017,  5:50pm; <NewLine> REPLY_DATE 3: July 3, 2017,  8:18am; <NewLine> REPLY_DATE 4: September 9, 2017,  1:14am; <NewLine> REPLY_DATE 5: November 1, 2017,  2:36pm; <NewLine> REPLY_DATE 6: November 6, 2017, 10:28pm; <NewLine> REPLY_DATE 7: February 3, 2018,  4:20am; <NewLine> REPLY_DATE 8: February 5, 2018, 10:32am; <NewLine> REPLY_DATE 9: May 11, 2018,  8:00pm; <NewLine> REPLY_DATE 10: July 5, 2018,  2:19am; <NewLine> REPLY_DATE 11: July 5, 2018, 11:24am; <NewLine> REPLY_DATE 12: October 13, 2018,  6:53am; <NewLine> REPLY_DATE 13: January 3, 2019,  9:51pm; <NewLine> REPLY_DATE 14: February 21, 2019,  9:09am; <NewLine> REPLY_DATE 15: February 27, 2019, 11:06am; <NewLine> REPLY_DATE 16: February 27, 2019,  1:08pm; <NewLine> REPLY_DATE 17: March 5, 2019,  5:58pm; <NewLine> REPLY_DATE 18: March 5, 2019,  6:34pm; <NewLine> REPLY_DATE 19: April 26, 2019,  7:39pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 3 Likes; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 4 Likes; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 3 Likes; <NewLine> REPLY 9 LIKES: 2 Likes; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: 2 Likes; <NewLine> REPLY 12 LIKES: 1 Like; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> REPLY 18 LIKES: ; <NewLine> REPLY 19 LIKES: ; <NewLine> 
90457,"RuntimeError: size mismatch, m1: [471 x 1024], m2: [471 x 1024] at /opt/conda/conda-bld/pytorch_1579040055865/work/aten/src/THC/generic/THCTensorMathBlas.cu:290",2020-07-24T20:11:02.840Z,1,78,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Am extending a <a href=""https://github.com/muvavarirwa/chessy"" rel=""nofollow noopener"">personal/hobby project</a> – to support MADDPG. Full disclosure – not the most elegant code here…</p><NewLine><p>Getting size mismatch error – though dimensions should work as they are identical per error.</p><NewLine><p><strong>Model</strong> is defined under <strong>model.py</strong>. Also summarized here:</p><NewLine><p>CriticNetwork(<br/><NewLine>(fc1): Linear(in_features=471, out_features=1024, bias=True)<br/><NewLine>(fc2): Linear(in_features=1024, out_features=10240, bias=True)<br/><NewLine>(fc3): Linear(in_features=10240, out_features=1024, bias=True)<br/><NewLine>(fc4): Linear(in_features=1024, out_features=1, bias=True)<br/><NewLine>)<br/><NewLine>ActorNetwork(<br/><NewLine>(fc1): Linear(in_features=384, out_features=1024, bias=True)<br/><NewLine>(fc2): Linear(in_features=1024, out_features=10240, bias=True)<br/><NewLine>(fc3): Linear(in_features=10240, out_features=1024, bias=True)<br/><NewLine>(fc4): Linear(in_features=1024, out_features=87, bias=True)<br/><NewLine>)</p><NewLine><p>Whereas the ARGS are defined in <strong>args.py</strong></p><NewLine><h1>===========================================================================</h1><NewLine><p>select a game: [‘chessy’ or ‘checkers’]: chessy<br/><NewLine>select number of teams:  [0, 1 or 2] 2<br/><NewLine>Choose a name for this team [e.g. ‘blue_team’ or ‘green_team’]: Agent<br/><NewLine>Choose a color for team_2 [e.g. ‘blue’: green<br/><NewLine>Please enter team’s skill_level [1 = Novice, 10 = expert]: 10<br/><NewLine>Please enter team’s strategy [0 = ‘cooperative’, 1 = ‘competitive’]: 1<br/><NewLine>Choose a name for this team [e.g. ‘blue_team’ or ‘green_team’]: Environment<br/><NewLine>Choose a color for team_2 [e.g. ‘blue’: red<br/><NewLine>Please enter team’s skill_level [1 = Novice, 10 = expert]: 1<br/><NewLine>Please enter team’s strategy [0 = ‘cooperative’, 1 = ‘competitive’]: 1<br/><NewLine>How many trials would you like to run? [1 - 1,000,000] 10000<br/><NewLine>Do you want to see the board positions in realtime? [ ‘Yes’ or ‘No’ ]no<br/><NewLine>==============================RUN_TRIALS==================================</p><NewLine><p>/home/ubuntu/chessy/model.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.<br/><NewLine>return F.softmax(self.fc4(x))</p><NewLine><hr/><NewLine><p>RuntimeError                              Traceback (most recent call last)<br/><NewLine> in <br/><NewLine>7     global HISTORY_FILE<br/><NewLine>8     user_input = userInput()<br/><NewLine>----&gt; 9     run_trials(user_input)<br/><NewLine>10<br/><NewLine>11 <span class=""hashtag"">#main</span>()</p><NewLine><p> in run_trials(user_input)<br/><NewLine>29         env = Game(user_input.game, args, 8, sides, user_input.display_board_positions)<br/><NewLine>30<br/><NewLine>—&gt; 31         run_trial(user_input,env, mCritic)<br/><NewLine>32<br/><NewLine>33         cycle += 1</p><NewLine><p> in run_trial(user_input, env, mCritic)<br/><NewLine>26         next_state         = np.array(list(map(int, env.state))).astype(np.float32)<br/><NewLine>27<br/><NewLine>—&gt; 28         agent_p0.step(state, action, int(reward), next_state, done, mCritic)<br/><NewLine>29<br/><NewLine>30         time_step += 1</p><NewLine><p>~/chessy/agent.py in step(self, state, action, reward, next_state, done, mCritic)<br/><NewLine>59         if len(self.memory) &gt; self.args[‘BATCH_SIZE’]:<br/><NewLine>60             experiences = self.memory.sample()<br/><NewLine>—&gt; 61             self.train(experiences, mCritic)<br/><NewLine>62<br/><NewLine>63</p><NewLine><p>~/chessy/agent.py in train(self, experiences, mCritic)<br/><NewLine>99             # Get predicted next-state actions and Q values from target models<br/><NewLine>100             actions_next   = self.actor_target(next_states)<br/><NewLine>–&gt; 101             Q_targets_next = mCritic.target(next_states, actions_next)<br/><NewLine>102<br/><NewLine>103             # Compute Q targets for current states (y_i)</p><NewLine><p>~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in <strong>call</strong>(self, *input, **kwargs)<br/><NewLine>530             result = self._slow_forward(*input, **kwargs)<br/><NewLine>531         else:<br/><NewLine>–&gt; 532             result = self.forward(*input, **kwargs)<br/><NewLine>533         for hook in self._forward_hooks.values():<br/><NewLine>534             hook_result = hook(self, input, result)</p><NewLine><p>~/chessy/model.py in forward(self, state, action)<br/><NewLine>70         state  = torch.transpose(state, 0,1)<br/><NewLine>71         action = torch.transpose(action,0,1)<br/><NewLine>—&gt; 72         x = F.relu(self.fc1(torch.cat((state, action))))<br/><NewLine>73         x = F.relu(self.fc2(x))<br/><NewLine>74         x = F.relu(self.fc3(x))</p><NewLine><p>~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in <strong>call</strong>(self, *input, **kwargs)<br/><NewLine>530             result = self._slow_forward(*input, **kwargs)<br/><NewLine>531         else:<br/><NewLine>–&gt; 532             result = self.forward(*input, **kwargs)<br/><NewLine>533         for hook in self._forward_hooks.values():<br/><NewLine>534             hook_result = hook(self, input, result)</p><NewLine><p>~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py in forward(self, input)<br/><NewLine>85<br/><NewLine>86     def forward(self, input):<br/><NewLine>—&gt; 87         return F.linear(input, self.weight, self.bias)<br/><NewLine>88<br/><NewLine>89     def extra_repr(self):</p><NewLine><p>~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py in linear(input, weight, bias)<br/><NewLine>1368     if input.dim() == 2 and bias is not None:<br/><NewLine>1369         # fused op is marginally faster<br/><NewLine>-&gt; 1370         ret = torch.addmm(bias, input, weight.t())<br/><NewLine>1371     else:<br/><NewLine>1372         <span class=""hashtag"">#print</span>(“INPUT:\n{}\n”.format(input))</p><NewLine><p>RuntimeError: size mismatch, m1: [471 x 1024], m2: [471 x 1024] at /opt/conda/conda-bld/pytorch_1579040055865/work/aten/src/THC/generic/THCTensorMathBlas.cu:290</p><NewLine><p>action = “2,2,-1”</p><NewLine><p>action.split("","")</p><NewLine><p>x = torch.Tensor(np.random.choice((0,1),87)).to(device)</p><NewLine><p>x.shape</p><NewLine><p>y = torch.Tenso</p><NewLine></div>",https://discuss.pytorch.org/u/Ranga_Muvavarirwa,(Ranga),Ranga_Muvavarirwa,"July 24, 2020,  8:12pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you check the shape of <code>torch.cat((state, action))</code> and make sure it has the desired shape of <code>[batch_size, 471]</code>?</p><NewLine><p>PS: you can post code snippets by wrapping them into three backticks ```, which makes debugging easier. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a><br/><NewLine>Turned out that I was improperly transposing tensor. Issue resolved in checked-in code on github.</p><NewLine><p>Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Ranga_Muvavarirwa; <NewLine> ,"REPLY_DATE 1: July 26, 2020,  9:00am; <NewLine> REPLY_DATE 2: July 27, 2020,  1:19pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
90483,Reward not increasing in a bi-pedal system,2020-07-25T02:52:05.098Z,5,96,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello all.<br/><NewLine>This question in regard to OpenAI gym’s Bipedal Walker using policy gradient. I am sorry if this question is not related to the purpose of these forums, but I was not able to find a openAI gym forum per se.</p><NewLine><p>I am trying to build a bipedal system using the REINFORCE algorithm. Now the problem I am facing is that the model never updates, i.e. the system has the same action at episode 0 and even at episode 20000. The reward stays constant throughout. This leads me to believe that the policyNet is not being updated, though I cannot find my mistake…<br/><NewLine>Can someone please tell me what is going wrong here?<br/><NewLine>TIA -<br/><NewLine>here is the code -</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import numpy<NewLine>from torch.autograd import Variable<NewLine>import gym<NewLine><NewLine>def train(model, name):<NewLine>    model.train()<NewLine>    env = gym.make(name)<NewLine>    env.reset()<NewLine>    num_episodes = 20000<NewLine>    max_steps = 10000<NewLine>    optim = torch.optim.Adam(model.parameters(), lr=1)<NewLine>    numsteps = []<NewLine>    rewards = []<NewLine>    avg_numsteps = []<NewLine><NewLine>    min_reward = -1000<NewLine><NewLine>    for episode in range(num_episodes):<NewLine>        state = env.reset()<NewLine>        probs = []<NewLine>        rewards = []<NewLine><NewLine>        for steps in range(max_steps):<NewLine>            action, log_prob = model.action(state)<NewLine>            env.render()<NewLine>            state, reward, finished, _ = env.step(action.squeeze(0).detach().numpy())<NewLine>            env.render()<NewLine>            probs.append(log_prob)<NewLine>            rewards.append(reward)<NewLine>            if finished:<NewLine>                break<NewLine><NewLine>        if finished:<NewLine>            Rewards = []<NewLine>            for i in range(len(rewards)):<NewLine>                G = 0<NewLine>                p = 0<NewLine>                for reward in rewards[i:]:<NewLine>                    G = G + 0.9 * p * reward<NewLine>                    p = p + 1<NewLine>                Rewards.append(G)<NewLine><NewLine>            Rewards = torch.tensor(Rewards)<NewLine><NewLine>            discounted_reward = (Rewards - Rewards.mean()) / (Rewards.std() + 1e-9)<NewLine><NewLine>            gradients = []<NewLine>            for log_prob, G in zip(log_prob, discounted_reward):<NewLine>                gradients.append(-log_prob * G)<NewLine><NewLine>            optim.zero_grad()<NewLine>            policy_gradient = Variable(torch.stack(gradients).sum(), requires_grad=True)<NewLine>            policy_gradient.backward()<NewLine>            optim.step()<NewLine>            numsteps.append(steps)<NewLine>            avg_numsteps.append(numpy.mean(numsteps[-10:]))<NewLine>            rewards.append(numpy.sum(rewards))<NewLine><NewLine>            print(""episode: {}, total reward: {}, average_reward: {}, length: {}\n"".format(episode,<NewLine>                                                                                           numpy.sum(rewards),<NewLine>                                                                                           numpy.round(numpy.mean(<NewLine>                                                                                               rewards[-10:]),<NewLine>                                                                                                       decimals=3),<NewLine>                                                                                           steps))<NewLine><NewLine>        if numpy.sum(rewards) &gt; min_reward:<NewLine>            torch.save(model.state_dict(), '/home/atharva/policyNet.pth')<NewLine>            min_reward = numpy.sum(rewards)<NewLine><NewLine><NewLine>def test(model, name):<NewLine>    env = gym.make(name)<NewLine>    model.eval()<NewLine>    state = env.reset()<NewLine>    with torch.no_grad():<NewLine>        while True:<NewLine>            action, log_prob = model(state)<NewLine>            state, reward, finished, _ = env.step(action.squeeze(0).numpy())<NewLine>            env.render()<NewLine>            if finished:<NewLine>                break<NewLine><NewLine><NewLine>class PolicyNet(nn.Module):<NewLine>    def __init__(self, inputs, actions, hidden_size):<NewLine>        super(PolicyNet, self).__init__()<NewLine>        self.num_actions = actions<NewLine>        self.layer1 = nn.Linear(inputs, hidden_size)<NewLine>        self.layer2 = nn.Linear(hidden_size, hidden_size)<NewLine>        self.layer3 = nn.Linear(hidden_size, 2*hidden_size)<NewLine>        self.layer4 = nn.Linear(2*hidden_size, hidden_size)<NewLine>        self.layer5 = nn.Linear(hidden_size, actions)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.layer1(x)<NewLine>        x = nn.functional.relu(x)<NewLine>        x = self.layer2(x)<NewLine>        x = nn.functional.relu(x)<NewLine>        x = self.layer3(x)<NewLine>        x = nn.functional.relu(x)<NewLine>        x = self.layer4(x)<NewLine>        x = nn.functional.relu(x)<NewLine>        x = self.layer5(x)<NewLine>        return x<NewLine><NewLine>    def action(self, state):<NewLine>        state = torch.from_numpy(state).float().unsqueeze(0)<NewLine>        actions = self.forward(Variable(state))<NewLine>        #prob = numpy.random.choice(self.num_actions, p=numpy.squeeze(actions.detach().numpy()))<NewLine>        log_prob = torch.log(actions.squeeze(0))<NewLine>        return actions, log_prob<NewLine></code></pre><NewLine><p>And the model is being called like -</p><NewLine><pre><code class=""lang-auto"">from REINFORCE import PolicyNet, train<NewLine><NewLine>model = PolicyNet(24, 4, 256)<NewLine>train(model, 'BipedalWalker-v3')<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/a_d,,a_d,"July 25, 2020,  2:52am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">        actions = self.forward(Variable(state))<NewLine>        #prob = numpy.random.choice(self.num_actions, p=numpy.squeeze(actions.detach().numpy()))<NewLine>        log_prob = torch.log(actions.squeeze(0))<NewLine></code></pre><NewLine><p>maybe change <code>self.forward</code> to <code>self.__call__</code></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for replying <a class=""mention"" href=""/u/iffix"">@iffiX</a><br/><NewLine>I will try that, but on a side note, I have passed model.parameter() to the optimizer that technically should update the parameters right irrespective of self.forward() or self.<strong>call</strong> … am I right ?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>But to my knowledge, calling <code>forward</code> direcly will not invoke the autograd backend, since torch have overloaded the <code>__call__</code> function of <code>nn.Modules</code></p><NewLine><p>Since you are using the old <code>Variable</code> api, I am not sure whether this will have any impact on your code.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh I did not know that… will try it out.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I did try it out… The loss, as well as the rewards, remain constant… and even the action taken is same…<br/><NewLine>any other way this could be fixed?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Why are you calculating log probability of a <strong>contiguous action</strong>, this way? Direcly <code>log</code> ?? <img alt="":joy:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/joy.png?v=9"" title="":joy:""/></p><NewLine><pre><code class=""lang-auto"">    def action(self, state):<NewLine>        state = torch.from_numpy(state).float().unsqueeze(0)<NewLine>        actions = self.__call__(state)<NewLine>        log_prob = torch.log(actions.squeeze(0))<NewLine>        return actions, log_prob<NewLine></code></pre><NewLine><p>I think there are probably many more problems in your code, and I suggest you read others implementations before continuing.</p><NewLine><p>Hint: inorder to let a policy gradient method (REINFORCE, A2C, PPO…) produce a contiguous action and optimize its log probability, you need reparameterization.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot for your help <a class=""mention"" href=""/u/iffix"">@iffiX</a>,  will start reading…</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/a_d; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/a_d; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/a_d; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/a_d; <NewLine> ,"REPLY_DATE 1: July 25, 2020,  2:27pm; <NewLine> REPLY_DATE 2: July 25, 2020,  4:05pm; <NewLine> REPLY_DATE 3: July 25, 2020,  4:07pm; <NewLine> REPLY_DATE 4: July 25, 2020,  4:09pm; <NewLine> REPLY_DATE 5: July 25, 2020,  4:29pm; <NewLine> REPLY_DATE 6: July 25, 2020,  4:46pm; <NewLine> REPLY_DATE 7: July 26, 2020,  2:07am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
90305,DQN Agent is not learning anything,2020-07-23T16:20:47.868Z,1,96,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My DQN agent in gym FrozenLake environment is not learning I tried everything but I can not make it work. Please if someone can help me, Thank you.</p><NewLine><pre><code class=""lang-auto"">import gym<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import numpy as np<NewLine>import collections<NewLine>import random<NewLine>from torch import optim<NewLine>from tqdm import tqdm<NewLine><NewLine><NewLine>class DQN(nn.Module):<NewLine>    def __init__(self, n_actions, n_inputs, lr=0.01):<NewLine>        super(DQN, self).__init__()<NewLine>        self.fc1 = nn.Linear(n_inputs, 64)<NewLine>        self.fc2 = nn.Linear(64, 128)<NewLine>        self.fc3 = nn.Linear(128, n_actions)<NewLine><NewLine>        self.optimizer = optim.Adam(self.parameters(), lr=lr)<NewLine>        self.device = 'cuda:0'<NewLine>        self.loss = nn.MSELoss()<NewLine>        self.to(self.device)<NewLine><NewLine>    def forward(self, state):<NewLine>        x = F.relu(self.fc1(state))<NewLine>        x = F.relu(self.fc2(x))<NewLine>        actions = self.fc3(x)<NewLine>        return actions<NewLine><NewLine><NewLine>class Agent:<NewLine>    def __init__(self, gamma=0.9, epsilon=0.9):<NewLine>        self.env = gym.make('FrozenLake-v0')<NewLine>        self.n_actions = self.env.action_space.n<NewLine>        self.gamma = gamma<NewLine>        self.epsilon = epsilon<NewLine>        self.policy_net = DQN(self.n_actions, 1)<NewLine>        self.target_net = DQN(self.n_actions, 1)<NewLine>        self.target_net.load_state_dict(self.policy_net.state_dict())<NewLine>        self.replay_memory = collections.deque(maxlen=10000)<NewLine>        self.min_replay_memory_size = 100<NewLine>        self.batch_size = 64<NewLine>        self.target_update_counter = 0<NewLine>        self.reward_list = []<NewLine><NewLine>    def update_replay_memory(self, obs):<NewLine>        self.replay_memory.append(obs)<NewLine><NewLine>    def get_q(self, state):<NewLine>        return self.policy_net.forward(torch.tensor([state], dtype=torch.float32).cuda())<NewLine><NewLine>    def choose_action(self, q):<NewLine>        if np.random.random() &gt; self.epsilon:<NewLine>            action = torch.argmax(q).item()<NewLine>        else:<NewLine>            action = self.env.action_space.sample()<NewLine>        return action<NewLine><NewLine>    def train(self):<NewLine>        if len(self.replay_memory) &lt; self.min_replay_memory_size:<NewLine>            return<NewLine>        batch = random.sample(self.replay_memory, self.batch_size)<NewLine>        current_states = torch.tensor([transition[0] for transition in batch], dtype=torch.float32).cuda()<NewLine>        current_states = current_states.unsqueeze(1)<NewLine>        self.policy_net.eval()<NewLine>        self.policy_net.zero_grad()<NewLine>        current_qs_list = self.policy_net.forward(<NewLine>            current_states).detach().cpu().numpy()<NewLine>        new_current_states = torch.tensor([transition[3] for transition in batch], dtype=torch.float32).cuda()<NewLine>        new_current_states = new_current_states.unsqueeze(1)<NewLine>        self.target_net.eval()<NewLine>        self.target_net.zero_grad()<NewLine>        future_qs_list = self.target_net.forward(<NewLine>            new_current_states).detach().cpu().numpy()<NewLine>        X = []<NewLine>        y = []<NewLine>        for index, (current_state, action, reward, new_current_state, done) in enumerate(batch):<NewLine>            done = False<NewLine>            if not done:<NewLine>                max_future_q = np.max(future_qs_list[index])<NewLine>                new_q = reward + self.gamma * max_future_q<NewLine>            else:<NewLine>                new_q = reward<NewLine>            current_qs = current_qs_list[index]<NewLine>            current_qs[action] = new_q<NewLine>            X.append(current_state)<NewLine>            y.append(current_qs)<NewLine>        self.policy_net.train()<NewLine>        self.policy_net.zero_grad()<NewLine>        preds = self.policy_net.forward(torch.tensor(X, dtype=torch.float32).unsqueeze(1).cuda())<NewLine>        loss = self.policy_net.loss(preds, torch.tensor(y).cuda())<NewLine>        loss.backward()<NewLine>        self.policy_net.optimizer.step()<NewLine>        if self.target_update_counter &gt; 5:<NewLine>            self.target_net.load_state_dict(self.policy_net.state_dict())<NewLine>            self.target_update_counter = 0<NewLine>            self.epsilon -= 0.02<NewLine><NewLine>    def step(self):<NewLine>        done = False<NewLine>        state = self.env.reset()<NewLine>        reward_all = 0<NewLine>        while not done:<NewLine>            q = self.get_q(state)<NewLine>            action = self.choose_action(q)<NewLine>            next_state, reward, done, _ = self.env.step(action)<NewLine>            reward_all += reward<NewLine>            self.update_replay_memory([state, action, reward, next_state, done])<NewLine>            self.train()<NewLine>            if done:<NewLine>                agent.target_update_counter += 1<NewLine>            state = next_state<NewLine>        self.reward_list.append(reward_all)<NewLine><NewLine><NewLine>agent = Agent()<NewLine>episodes = 1000<NewLine>for episode in tqdm(range(episodes)):<NewLine>    agent.step()<NewLine><NewLine>print(sum(agent.reward_list) / episodes)<NewLine><NewLine>agent.env.close()<NewLine><NewLine><NewLine>"""""" average reward is 0.0043""""""<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Jaredeco,(Jano Redecha),Jaredeco,"July 23, 2020,  4:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Some evident erros, there might be more:</p><NewLine><ol><NewLine><li><NewLine><p>What’s this? n-step learning?</p><NewLine><pre><code class=""lang-auto"">  for index, (current_state, action, reward, new_current_state, done) in enumerate(batch):<NewLine>         done = False<NewLine>         if not done:<NewLine>             max_future_q = np.max(future_qs_list[index])<NewLine>             new_q = reward + self.gamma * max_future_q<NewLine>         else:<NewLine>             new_q = reward<NewLine></code></pre><NewLine></li><NewLine><li><NewLine><p>No direct call of Forward when you want to calculate gradients:</p><NewLine><pre><code class=""lang-auto"">preds = self.policy_net.forward(...)<NewLine></code></pre><NewLine><p>You should do:</p><NewLine><pre><code class=""lang-auto"">preds = self.policy_net(...)<NewLine></code></pre><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried removing .forward() but it did not change anything, still very bad results.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Jaredeco; <NewLine> ,"REPLY_DATE 1: July 24, 2020,  1:24am; <NewLine> REPLY_DATE 2: July 24, 2020,  5:58am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
89904,Learning to run a power network in a sustainable world - NeurIPS 2020 competition,2020-07-20T13:55:23.748Z,0,81,"<div class=""post"" itemprop=""articleBody""><NewLine><p>“Learning to run a power network in a sustainable world” (<a href=""https://l2rpn.chalearn.org/"" rel=""nofollow noopener"">https://l2rpn.chalearn.org/</a><a href=""http://lnkd.in/duXYUCz"" rel=""nofollow noopener""> <strong>)</strong> </a>) is listed among 2020 official NeurIPS competitions!</p><NewLine><p>We will be thrilled if you come join us now (<a href=""https://competitions.codalab.org/competitions/24902"" rel=""nofollow noopener"">https://competitions.codalab.org/competitions/24902</a><a href=""http://lnkd.in/dmbKVkD"" rel=""nofollow noopener""> <strong>)</strong> </a>) to unleash the power of AI for these critical real-world power grid infrastructures, especially in times of crisis. What would it be either today and tomorrow without reliable electricity ?<br/><NewLine>We are pleased to answer any related questions here or on Discord Channel: <a href=""https://www.google.com/url?q=https%3A%2F%2Fdiscord.gg%2FcYsYrPT&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGqXjpx8cJuSUnOC1TlJDr5KrPUsw"" rel=""nofollow noopener"">https://discord.gg/cYsYrPT</a></p><NewLine></div>",https://discuss.pytorch.org/u/marota,(Antoine Marot),marota,"July 20, 2020,  1:55pm",,,,,
29803,Optimized MultivariateNormal with diagonal covariance matrix,2018-11-16T21:12:08.801Z,1,1358,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The current <code>MultivariateNormal</code> implementation is adding a significant overhead to my code when I use large batch sizes and after looking into the source, it seems the main cause is in one helper function used in the <code>log_prob</code> method, which runs an explicit for loop for a batch operation:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/eef083e4774c52161148883c53ce5a1f081e64ca/torch/distributions/multivariate_normal.py#L55"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/eef083e4774c52161148883c53ce5a1f081e64ca/torch/distributions/multivariate_normal.py#L55"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/eef083e4774c52161148883c53ce5a1f081e64ca/torch/distributions/multivariate_normal.py#L55</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""45"" style=""counter-reset: li-counter 44 ;""><NewLine><li>return flat_bmat_inv.reshape(bmat.shape)</li><NewLine><li><NewLine></li><NewLine><li><NewLine></li><NewLine><li>def _batch_trtrs_lower(bb, bA):</li><NewLine><li>""""""</li><NewLine><li>Applies `torch.trtrs` for batches of matrices. `bb` and `bA` should have</li><NewLine><li>the same batch shape.</li><NewLine><li>""""""</li><NewLine><li>flat_b = bb.reshape((-1,) + bb.shape[-2:])</li><NewLine><li>flat_A = bA.reshape((-1,) + bA.shape[-2:])</li><NewLine><li class=""selected"">flat_X = torch.stack([torch.trtrs(b, A, upper=False)[0] for b, A in zip(flat_b, flat_A)])</li><NewLine><li>return flat_X.reshape(bb.shape)</li><NewLine><li><NewLine></li><NewLine><li><NewLine></li><NewLine><li>def _batch_mahalanobis(bL, bx):</li><NewLine><li>r""""""</li><NewLine><li>Computes the squared Mahalanobis distance :math:`\mathbf{x}^\top\mathbf{M}^{-1}\mathbf{x}`</li><NewLine><li>for a factored :math:`\mathbf{M} = \mathbf{L}\mathbf{L}^\top`.</li><NewLine><li><NewLine></li><NewLine><li>Accepts batches for both bL and bx. They are not necessarily assumed to have the same batch</li><NewLine><li>shape, but `bL` one should be able to broadcasted to `bx` one.</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>The other issue is that I just need a diagonal covariance for my current model, but the <code>MultivariateNormal</code> object is very general and runs some unnecessary computations that could be optimized for a diagonal covariance, like <code>torch.trtrs</code>. Would it make sense to have a <code>MultivariateNormal</code> implementation with some optimizations for strictly diagonal covariances? I’ve noticed there’s a new <code>LowRankMultivariateNormal</code> in the master branch that hasn’t made it into the stable release yet. I believe that implementation might be more suitable, the constructor takes a <code>cov_diag</code> explicitly, but it also takes a <code>cov_factor</code>, which might run some unnecessary computations for a strictly diagonal covariance as well:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/eef083e4774c52161148883c53ce5a1f081e64ca/torch/distributions/lowrank_multivariate_normal.py#L91"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/eef083e4774c52161148883c53ce5a1f081e64ca/torch/distributions/lowrank_multivariate_normal.py#L91"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/eef083e4774c52161148883c53ce5a1f081e64ca/torch/distributions/lowrank_multivariate_normal.py#L91</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""81"" style=""counter-reset: li-counter 80 ;""><NewLine><li>    Thanks to these formulas, we just need to compute the determinant and inverse of</li><NewLine><li>    the small size ""capacitance"" matrix::</li><NewLine><li>        capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor</li><NewLine><li>""""""</li><NewLine><li>arg_constraints = {""loc"": constraints.real,</li><NewLine><li>                   ""cov_factor"": constraints.real,</li><NewLine><li>                   ""cov_diag"": constraints.positive}</li><NewLine><li>support = constraints.real</li><NewLine><li>has_rsample = True</li><NewLine><li><NewLine></li><NewLine><li class=""selected"">def __init__(self, loc, cov_factor, cov_diag, validate_args=None):</li><NewLine><li>    if loc.dim() &lt; 1:</li><NewLine><li>        raise ValueError(""loc must be at least one-dimensional."")</li><NewLine><li>    event_shape = loc.shape[-1:]</li><NewLine><li>    if cov_factor.dim() &lt; 2:</li><NewLine><li>        raise ValueError(""cov_factor must be at least two-dimensional, ""</li><NewLine><li>                         ""with optional leading batch dimensions"")</li><NewLine><li>    if cov_factor.shape[-2:-1] != event_shape:</li><NewLine><li>        raise ValueError(""cov_factor must be a batch of matrices with shape {} x m""</li><NewLine><li>                         .format(event_shape[0]))</li><NewLine><li>    if cov_diag.shape[-1:] != event_shape:</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>What’s the recommended approach to create an efficient multivariate normal distribution with a strictly diagonal covariance matrix?</p><NewLine></div>",https://discuss.pytorch.org/u/juniorrojas,(Junior Rojas),juniorrojas,"November 16, 2018,  9:26pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can just use <code>torch.distributions.Normal</code> in that case</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>n = 2<NewLine>d = 5<NewLine>diagonal = torch.rand(d) + 1.<NewLine>mu = torch.rand(n, d)<NewLine>p1 = torch.distributions.Normal(mu, diagonal.reshape(1, d))<NewLine>p2 = torch.distributions.MultivariateNormal(mu, scale_tril=torch.diag(diagonal).reshape(1, d, d))<NewLine>x = torch.rand((n,d))<NewLine>print(p1.log_prob(x).sum(dim=1) - p2.log_prob(x))<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The above answer is the way to go, but it can result in confusing issues if you’re new to <code>torch.distributions</code>.</p><NewLine><p>In order to get the correct Kullback-Leibler divergence (and the correct shape of <code>.log_prob</code>), we need to wrap the <code>Normal</code> in the <code>Independent</code> class which reinterprets some number of batch dimensions as event dimensions. This is not done by the default <code>Normal</code> which simply assumes all dimensions to be batch-dimensions which is generally not the behaviour you want when you’re effectively defining a multivariate normal with diagonal covariance. See below and <a href=""https://pytorch.org/docs/stable/distributions.html#independent"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/distributions.html#independent</a></p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; loc = torch.zeros(3)<NewLine>&gt;&gt;&gt; scale = torch.ones(3)<NewLine>&gt;&gt;&gt; mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))<NewLine>&gt;&gt;&gt; [mvn.batch_shape, mvn.event_shape]<NewLine>[torch.Size(()), torch.Size((3,))]<NewLine>&gt;&gt;&gt; normal = Normal(loc, scale)<NewLine>&gt;&gt;&gt; [normal.batch_shape, normal.event_shape]<NewLine>[torch.Size((3,)), torch.Size(())]<NewLine>&gt;&gt;&gt; diagn = Independent(normal, 1)<NewLine>&gt;&gt;&gt; [diagn.batch_shape, diagn.event_shape]<NewLine>[torch.Size(()), torch.Size((3,))]<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please use JakobHavtorn’s approach, not mine, that’s the most appropriate way to go as of now.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/talesa; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/JakobHavtorn; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/talesa; <NewLine> ,"REPLY_DATE 1: July 13, 2020, 10:14am; <NewLine> REPLY_DATE 2: January 18, 2020,  3:22pm; <NewLine> REPLY_DATE 3: July 13, 2020, 10:17am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
73554,Found bug in pytorch reinforcement tutorials,2020-03-17T19:11:43.681Z,6,251,"<div class=""post"" itemprop=""articleBody""><NewLine><p>hello, i found bug in pytorch reinforcement tutorial, which is on pytorch.org.i run colab version and it gave me error on this code:<br/><NewLine>‘’’<br/><NewLine>env.reset()<br/><NewLine>plt.figure()<br/><NewLine>plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),</p><NewLine><pre><code class=""lang-auto"">       interpolation='none')<NewLine></code></pre><NewLine><p>plt.title(‘Example extracted screen’)<br/><NewLine>plt.show()<br/><NewLine>‘’’<br/><NewLine>and error says:<br/><NewLine>‘’’<br/><NewLine>Cannot connect to “None”<br/><NewLine>‘’’<br/><NewLine>i haven’t read code yet.so what is solution, is that a bug? and how to fix it.To see whole code, you can go to <a href=""https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html</a>, this is official website.</p><NewLine></div>",https://discuss.pytorch.org/u/dato_nefaridze,(dato nefaridze),dato_nefaridze,"March 17, 2020,  7:11pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Which <code>matplotlib</code> version are you using?<br/><NewLine>Apparently the <code>interpolation</code> argument is not recognized.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>i know, like i wrote in question i run the code on google colab.for more info go to the <a href=""http://pytorch.org"" rel=""nofollow noopener"">pytorch.org</a> website, where the tutorials are.thanks</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>please help me, i just started RL, and that bug discouraged me :D.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>The error message is still pointing to <code>matplotlib</code>, so which version are you using?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>i am using 3.2.0 version.official tutorial uses that version.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the information.<br/><NewLine>In that case I doubt the error is thrown from the posted lines of code but rather from the gym interaction.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks, but it is pytorch’s tutorial, i haven’t written any line of code.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I can reproduce the issue on Colab, while the tutorial runs on my local machine.<br/><NewLine>Could you please open an issue <a href=""https://github.com/pytorch/tutorials/issues"">here</a>?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>I was facing the same problem so I opened the issue by following your link:<br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/tutorials/issues/915"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/tutorials</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/tutorials/issues/915"" rel=""nofollow noopener"" target=""_blank"">Error when run reinforcement_q_learning.ipynb</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-03-30"" data-format=""ll"" data-time=""07:38:41"" data-timezone=""UTC"">07:38AM - 30 Mar 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/tv76"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""tv76"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars0.githubusercontent.com/u/46939018?v=4"" width=""20""/><NewLine>          tv76<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">Periodically I try to run the example (my computer: Windows 10, browser: Google Chrome):<NewLine>pytorch.org -&gt; Tutorials -&gt; Reinforcement Learning -&gt; Reinforcement...</p><NewLine></div><NewLine><div class=""labels""><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><h3><NewLine><strong><a href=""https://github.com/abyaadrafid"" rel=""nofollow noopener"">abyaadrafid</a></strong> answered on it there on May 30:</h3><NewLine><p>!apt install xvfb -y<br/><NewLine>!pip install pyvirtualdisplay<br/><NewLine>!pip install piglet</p><NewLine><p>from pyvirtualdisplay import Display<br/><NewLine>display = Display(visible=0, size=(1400, 900))<br/><NewLine>display.start()</p><NewLine><p>using pyvirtualdisplay fixed it for me.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dato_nefaridze; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/dato_nefaridze; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/dato_nefaridze; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/dato_nefaridze; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/tv76; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/tv76; <NewLine> ,"REPLY_DATE 1: March 18, 2020,  4:11am; <NewLine> REPLY_DATE 2: March 18, 2020,  5:15am; <NewLine> REPLY_DATE 3: March 18, 2020,  4:27pm; <NewLine> REPLY_DATE 4: March 19, 2020,  2:51am; <NewLine> REPLY_DATE 5: March 19, 2020,  5:44am; <NewLine> REPLY_DATE 6: March 19, 2020,  9:45am; <NewLine> REPLY_DATE 7: March 19, 2020,  9:54am; <NewLine> REPLY_DATE 8: March 23, 2020,  5:52am; <NewLine> REPLY_DATE 9: March 30, 2020,  7:54am; <NewLine> REPLY_DATE 10: July 10, 2020,  3:29pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> REPLY 10 LIKES: ; <NewLine> 
88259,Multidmensional Actions,2020-07-07T15:14:08.092Z,2,92,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello there,<br/><NewLine>Please,How can we apply Reinforce or any Policy gradient algorithm when the actions space is multidimensional, let’s say that for each state the action is a vector a= [a_1,a_2,a_3] where a_i are discrete ?<br/><NewLine>In this case the output of  the policy network must model a joint probability distribution<br/><NewLine>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/ar795,(ar795),ar795,"July 7, 2020,  3:14pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You need the reparameterization trick:</p><NewLine><p>if each dimension of your action vector is of the same meaning (same number and meaning of choices), consider about <code>torch.multinomial</code></p><NewLine><p>if not and each dimension are independent from each other, then create multiple outputs in your network and direct them to multiple categorical distribution, and draw each dimension of your sample from these distributions.</p><NewLine><p>If they are correlated, then you need to build your own model of distribution.</p><NewLine><p>Another option is to map all combinations: a_1 x a_2 x a_3 to a single categorical distribution.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply ,<br/><NewLine>Yes,  the action vector components are all of the same meaning :<br/><NewLine>Let’s say that the output of my policy network is a 2D tensor of shape (m,n) (sum of each row equals one)<br/><NewLine>and I want to generate according to this probability matrix an action vector of size m a = [a_1,…,a_m]<br/><NewLine>In this case the multinomial distributions is the suitable one ?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>yes，you should use the multinomial distribution.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ar795; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: July 7, 2020,  5:19pm; <NewLine> REPLY_DATE 2: July 7, 2020,  6:15pm; <NewLine> REPLY_DATE 3: July 8, 2020,  2:08am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
88246,Policy iteration agent not improving,2020-07-07T14:02:56.331Z,0,56,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey everyone,</p><NewLine><p>I am trying to implement an approach similar to this one: <a href=""https://arxiv.org/abs/1912.01603"" rel=""nofollow noopener"">https://arxiv.org/abs/1912.01603</a></p><NewLine><p>In the policy generation part, the agent tries to generate a policy by the returns of the environment model and the estimated values of a value network, which in turn computes the value of states based on the policy. However, my agent does not seem to be improving and its loss increases steadily because the value network assigns lower and lower values to the states. It seems like the agent converges to always output an action of 1.</p><NewLine><p>Here is the important part of my code:</p><NewLine><pre><code class=""lang-auto"">def agent_training(epochs):<NewLine><NewLine>    for p in gen.parameters():<NewLine><NewLine>      p.requires_grad_(False)<NewLine><NewLine>    <NewLine><NewLine>    for epoch in range(epochs):<NewLine><NewLine>        #load samples from training data<NewLine><NewLine>        sample= data_agent.__next__().to(device)<NewLine><NewLine>        try:<NewLine><NewLine>            #check that it is the correct shape<NewLine><NewLine>            sample = sample.view(agent_batches,d_state+d_action+d_state+3)<NewLine><NewLine>        except:<NewLine><NewLine>            continue<NewLine><NewLine>        sample = sample[:,:d_state+1]        <NewLine><NewLine>        states = compute_transitions(sample)<NewLine><NewLine>        value_estimates = compute_value_estimates(states)<NewLine><NewLine>        train_action_model(value_estimates)<NewLine><NewLine>        <NewLine><NewLine>        states = states[:,:,:d_state+1]<NewLine><NewLine>        train_value_model(states.detach(),value_estimates.detach())<NewLine><NewLine>        <NewLine><NewLine>         #plot losses<NewLine><NewLine>        if epoch%20 == 0:<NewLine><NewLine>            print(value_estimates[0,0],values(states)[0,0])<NewLine><NewLine>            fig, ax1 = plt.subplots()<NewLine><NewLine>            ax2 = ax1.twinx()<NewLine><NewLine>            ax1.plot(range(len(v_losses)), v_losses, 'g', label = 'Value Loss')<NewLine><NewLine>            ax2.plot(range(len(a_losses)), a_losses,'r', label=""Agent Loss"")<NewLine><NewLine>            ax1.set_xlabel(""Epoch"")<NewLine><NewLine>            ax1.set_ylabel(""Value Loss"")<NewLine><NewLine>            ax2.set_ylabel(""Agent Loss"")<NewLine><NewLine>            plt.legend()<NewLine><NewLine>            plt.show()<NewLine><NewLine>        del states<NewLine><NewLine>        del value_estimates<NewLine><NewLine>            <NewLine><NewLine>def compute_transitions(states):<NewLine><NewLine>  state_tensor = torch.empty(agent_batches,t,d_state+2).to(device)<NewLine><NewLine>  s = states<NewLine><NewLine>  for i in range(t):<NewLine><NewLine>      n_batch = noise(agent_batches).to(device)<NewLine><NewLine>      action = agent(s)<NewLine><NewLine>      g_out = gen(n_batch,s,action)  <NewLine><NewLine>      s = g_out[:,:d_state+1].to(device)<NewLine><NewLine>      state_tensor[:,i,:] = g_out[:,:d_state+2] <NewLine><NewLine>  return state_tensor`<NewLine><NewLine>`def compute_value_estimates(states):<NewLine><NewLine>  value_tensor = torch.empty(agent_batches,t,1).to(device)<NewLine><NewLine>  for i in range(t):<NewLine><NewLine>    value_tensor[:,i,:] = value_estimates(states,i)<NewLine><NewLine>  return value_tensor`<NewLine><NewLine>def train_action_model(value_estimates):<NewLine><NewLine>  a_optimizer.zero_grad()<NewLine><NewLine>  #compute loss<NewLine><NewLine>  loss = torch.mean(-torch.sum(value_estimates.squeeze(),dim=1))<NewLine><NewLine>  a_losses.append(loss)<NewLine><NewLine>  loss.backward()<NewLine><NewLine>  torch.nn.utils.clip_grad_norm_(agent.parameters(),100)<NewLine><NewLine>  a_optimizer.step()     `<NewLine><NewLine>`def train_value_model(states,value_estimates):<NewLine><NewLine>  v_optimizer.zero_grad()<NewLine><NewLine>  output = values(states)<NewLine><NewLine>  loss_fn = torch.nn.MSELoss()<NewLine><NewLine>  loss = loss_fn(value_estimates.squeeze(),output.squeeze())<NewLine><NewLine>  v_losses.append(loss)<NewLine><NewLine>  loss.backward()<NewLine><NewLine>  torch.nn.utils.clip_grad_norm_(values.parameters(),100)<NewLine><NewLine>  v_optimizer.step()`<NewLine><NewLine>`def value_estimates(sample,steps_in_future):<NewLine><NewLine>  v = torch.zeros(agent_batches,1).to(device)<NewLine><NewLine>  for i in range(1,t):<NewLine><NewLine>    v += ((1-lambda_v)*(lambda_v**(i-1))*value_estimates_k(sample,i,steps_in_future))<NewLine><NewLine>  v += lambda_v**(t-1)*value_estimates_k(sample,t,t)<NewLine><NewLine>  return v`<NewLine><NewLine>`def value_estimates_k(sample,k,steps_in_future):<NewLine><NewLine>  v = torch.zeros(agent_batches,1).to(device)<NewLine><NewLine>  h = min(k,t-steps_in_future)<NewLine><NewLine>  for j in range(h):<NewLine><NewLine>      reward = (sample[:,j,d_state+1]*(0.99**j))<NewLine><NewLine>      v += reward.unsqueeze(dim=1)<NewLine><NewLine>  estimate = values(sample[:,h,:d_state+1])<NewLine><NewLine>  v = v+ (0.99**h)*estimate<NewLine><NewLine>  return v<NewLine><NewLine></code></pre><NewLine><p>I hope anyone can help my figure out why my code is not working.</p><NewLine><p><img alt=""index"" data-base62-sha1=""sGs5Pt5kv8zBvgSzev1oVDmW6Wz"" height=""262"" src=""https://discuss.pytorch.org/uploads/default/original/3X/c/9/c9096460730206b88753c39378fd169bdbaec237.png"" width=""429""/></p><NewLine></div>",https://discuss.pytorch.org/u/Durotan97,(Timo Bertram),Durotan97,"July 7, 2020,  3:36pm",,,,,
87606,Render Issue with Official Reinforcement Learning Tutorial,2020-07-01T15:13:25.893Z,0,97,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,<br/><NewLine>I’m having some trouble running the official reinforcement learning <a href=""https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#training"" rel=""nofollow noopener"">tutorial</a> in the available <a href=""https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/2b3f06b04b5e96e4772746c20fcb4dcc/reinforcement_q_learning.ipynb"" rel=""nofollow noopener"">colab notebook</a>.  I haven’t done anything beyond try to run the cells but I keep getting an error from (I believe) gym’s render function. I don’t know if colab won’t run the render function for some reason or if I am just doing something wrong, but some clarity would be great!<br/><NewLine>The code in the cell is:</p><NewLine><pre><code class=""lang-auto"">resize = T.Compose([T.ToPILImage(),<NewLine>                    T.Resize(40, interpolation=Image.CUBIC),<NewLine>                    T.ToTensor()])<NewLine><NewLine><NewLine>def get_cart_location(screen_width):<NewLine>    world_width = env.x_threshold * 2<NewLine>    scale = screen_width / world_width<NewLine>    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART<NewLine><NewLine>def get_screen():<NewLine>    # Returned screen requested by gym is 400x600x3, but is sometimes larger<NewLine>    # such as 800x1200x3. Transpose it into torch order (CHW).<NewLine>    screen = env.render(mode='rgb_array').transpose((2, 0, 1))<NewLine>    # Cart is in the lower half, so strip off the top and bottom of the screen<NewLine>    _, screen_height, screen_width = screen.shape<NewLine>    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]<NewLine>    view_width = int(screen_width * 0.6)<NewLine>    cart_location = get_cart_location(screen_width)<NewLine>    if cart_location &lt; view_width // 2:<NewLine>        slice_range = slice(view_width)<NewLine>    elif cart_location &gt; (screen_width - view_width // 2):<NewLine>        slice_range = slice(-view_width, None)<NewLine>    else:<NewLine>        slice_range = slice(cart_location - view_width // 2,<NewLine>                            cart_location + view_width // 2)<NewLine>    # Strip off the edges, so that we have a square image centered on a cart<NewLine>    screen = screen[:, :, slice_range]<NewLine>    # Convert to float, rescale, convert to torch tensor<NewLine>    # (this doesn't require a copy)<NewLine>    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255<NewLine>    screen = torch.from_numpy(screen)<NewLine>    # Resize, and add a batch dimension (BCHW)<NewLine>    return resize(screen).unsqueeze(0).to(device)<NewLine><NewLine><NewLine>env.reset()<NewLine>plt.figure()<NewLine>plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),<NewLine>           interpolation='none')<NewLine>plt.title('Example extracted screen')<NewLine>plt.show()<NewLine></code></pre><NewLine><p>and the stack trace that I get is:</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>NameError                                 Traceback (most recent call last)<NewLine>&lt;ipython-input-10-6a36d3e0eceb&gt; in &lt;module&gt;()<NewLine>     38 env.reset()<NewLine>     39 plt.figure()<NewLine>---&gt; 40 plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),<NewLine>     41            interpolation='none')<NewLine>     42 plt.title('Example extracted screen')<NewLine><NewLine>&lt;ipython-input-10-6a36d3e0eceb&gt; in get_screen()<NewLine>     12     # Returned screen requested by gym is 400x600x3, but is sometimes larger<NewLine>     13     # such as 800x1200x3. Transpose it into torch order (CHW).<NewLine>---&gt; 14     screen = env.render(mode='rgb_array').transpose((2, 0, 1))<NewLine>     15     print(screen)<NewLine>     16     # Cart is in the lower half, so strip off the top and bottom of the screen<NewLine><NewLine>/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/cartpole.py in render(self, mode)<NewLine>    172 <NewLine>    173         if self.viewer is None:<NewLine>--&gt; 174             from gym.envs.classic_control import rendering<NewLine>    175             self.viewer = rendering.Viewer(screen_width, screen_height)<NewLine>    176             l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2<NewLine><NewLine>/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/rendering.py in &lt;module&gt;()<NewLine>     23 <NewLine>     24 try:<NewLine>---&gt; 25     from pyglet.gl import *<NewLine>     26 except ImportError as e:<NewLine>     27     raise ImportError('''<NewLine><NewLine>/usr/local/lib/python3.6/dist-packages/pyglet/gl/__init__.py in &lt;module&gt;()<NewLine>    233 elif compat_platform == 'darwin':<NewLine>    234     from .cocoa import CocoaConfig as Config<NewLine>--&gt; 235 del base  # noqa: F821<NewLine>    236 <NewLine>    237 <NewLine><NewLine>NameError: name 'base' is not defined<NewLine>&lt;Figure size 432x288 with 0 Axes&gt;<NewLine></code></pre><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/BaruchG,(Baruch),BaruchG,"July 1, 2020,  3:13pm",,,,,
86933,An error will occur if the batch size is set to 1 or more,2020-06-25T17:48:47.795Z,0,146,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m doing reinforcement learning<br/><NewLine>Batch size cannot be greater than 1<br/><NewLine>I’m trying a batch size of 32</p><NewLine><p>I can’t find anything strange when I look at Ross<br/><NewLine>I’m in trouble</p><NewLine><blockquote><NewLine><p>tensor([[1.3092e-03, 2.1749e-03, 2.6177e-06, 2.2517e-03, 2.1730e-03, 4.7358e-05,<br/><NewLine>1.3836e-03, 8.5324e-04, 1.0955e-03, 1.9888e-03, 1.3575e-03, 1.6514e-05,<br/><NewLine>2.5816e-04, 1.8731e-04, 1.9047e-03, 5.3802e-04, 6.7010e-04, 8.9404e-04,<br/><NewLine>4.5198e-04, 1.1572e-04, 3.0101e-04, 1.6938e-03, 4.7421e-04, 6.1655e-04,<br/><NewLine>3.7556e-03, 1.9652e-04, 3.1410e-04, 6.3015e-05, 3.7714e-04, 2.9124e-08,<br/><NewLine>6.6303e-04, 2.7347e-05]], device=‘cuda:0’, grad_fn=)</p><NewLine></blockquote><NewLine><hr/><NewLine><p>RuntimeError                              Traceback (most recent call last)<br/><NewLine> in <br/><NewLine>427             trin.Double_R2D2_IQN_pioritized_Nstep_NAF_replay(batch_size, gamma,step=episode,<br/><NewLine>428                                                              state_size=state_,action_size=acthon,<br/><NewLine>–&gt; 429                                                              multireward_steps=multireward_steps)<br/><NewLine>430<br/><NewLine>431         if done or t==max_number_of_steps + 1:</p><NewLine><p> in Double_R2D2_IQN_pioritized_Nstep_NAF_replay(self, batch_size, gamma, step, state_size, action_size, multireward_steps)<br/><NewLine>263             loss=loss*weights<br/><NewLine>264             print(loss)<br/><NewLine>–&gt; 265         loss.backward() # ここを変更<br/><NewLine>266         optimizer.step()<br/><NewLine>267         self.Rs=[0 for _ in range(multireward_steps)]</p><NewLine><p>~\Anaconda3\envs\pyflan\lib\site-packages\torch\tensor.py in backward(self, gradient, retain_graph, create_graph)<br/><NewLine>196                 products. Defaults to <code>False</code>.<br/><NewLine>197         “”""<br/><NewLine>–&gt; 198         torch.autograd.backward(self, gradient, retain_graph, create_graph)<br/><NewLine>199<br/><NewLine>200     def register_hook(self, hook):</p><NewLine><p>~\Anaconda3\envs\pyflan\lib\site-packages\torch\autograd_<em>init</em>_.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)<br/><NewLine>92         grad_tensors = list(grad_tensors)<br/><NewLine>93<br/><NewLine>—&gt; 94     grad_tensors = _make_grads(tensors, grad_tensors)<br/><NewLine>95     if retain_graph is None:<br/><NewLine>96         retain_graph = create_graph</p><NewLine><p>~\Anaconda3\envs\pyflan\lib\site-packages\torch\autograd_<em>init</em>_.py in _make_grads(outputs, grads)<br/><NewLine>33             if out.requires_grad:<br/><NewLine>34                 if out.numel() != 1:<br/><NewLine>—&gt; 35                     raise RuntimeError(“grad can be implicitly created only for scalar outputs”)<br/><NewLine>36                 new_grads.append(torch.ones_like(out, memory_format=torch.preserve_format))<br/><NewLine>37             else:</p><NewLine><p>RuntimeError: grad can be implicitly created only for scalar outputs</p><NewLine><p>The code is</p><NewLine><pre><code>    targets= torch.zeros(batch_size,device='cuda:0')<NewLine>    trin_x=list(range(batch_size))<NewLine>    weights = torch.ones(batch_size,device='cuda:0')<NewLine>    idx=np.random.choice(np.arange(len(memory_TDerror.buffer)), size=batch_size,<NewLine>                         replace=False,p = memory_TDerror.buffer/np.sum(memory_TDerror.buffer))<NewLine>    <NewLine>    for i in idx:<NewLine>        inputs=(torch.cat([memory.buffer[i-multireward_steps][0]]),<NewLine>                           torch.cat([memory.buffer[i-multireward_steps][1]]))<NewLine>    for _,i in enumerate(idx):<NewLine>        with torch.no_grad():<NewLine>            targets[_]=memory.buffer[i][2].to(""cuda:0"")+(gamma ** multireward_steps)*targetQN.forward(memory.buffer[i][0],""net_v"")<NewLine>        <NewLine>        priority = rank_sum(memory_TDerror.buffer[i], self.alpha)<NewLine>        weights[_]=(len(memory.buffer) * priority) ** (-self.beta)<NewLine>    weights = weights / max(weights)   <NewLine>    #リプレイとおなじ<NewLine>    optimizer.zero_grad()<NewLine>    output = mainQN.forward(inputs,""net_q"")<NewLine>    if self.IQN==True:<NewLine>        self.loss_IQN(target,output,weights)<NewLine>    else:<NewLine>        loss = huber(output-targets)<NewLine>        loss=loss*weights<NewLine>        print(loss)<NewLine>    loss.backward() # ここを変更<NewLine>    optimizer.step()<NewLine>    self.Rs=[0 for _ in range(multireward_steps)]</code></pre><NewLine></div>",https://discuss.pytorch.org/u/skerlet_flandorle,(flandorle),skerlet_flandorle,"June 25, 2020,  5:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Based on the error message it seems that <code>loss</code> is not a scalar value but a tensor with multiple values.<br/><NewLine>You could either reduce it to a scalar (e.g. with <code>torch.mean</code>, if that would fir your use case) or you could pass the gradients to <code>backward</code> e.g. via <code>loss.backward(torch.ones_like(loss))</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: June 26, 2020,  9:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
52136,`BatchNorm1d()` with batchsize=1,2019-07-31T04:41:55.840Z,5,792,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Q1: How does <code>BatchNorm1d()</code> judge the current <code>forward()</code> is training or inference? Is there some parameters can be observed and setted manually?<br/><NewLine>Q2: Specifically speaking, I’m trying to implement a reinforcement learning task. While in the training process, the transition tuple &lt;state, action, state_next, reward&gt; has to be generated one by one calling <code>forward()</code>, the state is input and the reward and the action are outputs, and then is add to the experience reply memory for the subsequent minibatch optimization, which means in the process of generating the transition tuple, the batch size of the input of <code>forward()</code> is 1.  Hence, the <code>BatchNorm1d()</code> can not work because the standard deviation is 0. How to deal with the problem effectively? My existing simple idea is to randomly generate some transition tuple not using <code>forward()</code> for initialization. Is there some effectively and professional methods?<br/><NewLine>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/hcl,,hcl,"July 31, 2019,  4:41am",3 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li><NewLine><p>The internal <code>.training</code> attribute determines the behavior of some layers, e.g. batch norm layers.<br/><NewLine>If you call <code>model.train()</code> or <code>model.eval()</code> (or <code>model.bn_layer.train()</code>), this internal flag will be switched.</p><NewLine></li><NewLine><li><NewLine><p>If you are using a single sample as your batch, you might consider using other normalization lazers, e.g. <code>InstanceNorm</code>.</p><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there,</p><NewLine><p>I run on multiple GPUs with random varying batch sizes.<br/><NewLine>I found that sometimes the last batch of the epoch is of size 1 and than BatchNorm1D throws an error and stops my run.</p><NewLine><p>I solved locally with try&lt;-&gt;except but it seems like a bug to me…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>To remove the last, potentially smaller, batch from an epoch, you could specify <code>drop_last = True</code> in your <code>DataLoader</code>.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>How can I use InstanceNorm layer with samples (batch size = 1)?</p><NewLine><p>When I’m trying to do something like this:</p><NewLine><pre><code class=""lang-auto"">x = torch.randn([1, 512])<NewLine>m = nn.InstanceNorm1d(512)<NewLine>m(x)<NewLine></code></pre><NewLine><p>I’m getting error like this:</p><NewLine><pre><code class=""lang-auto"">InstanceNorm1d returns 0-filled tensor to 2D tensor.This is because InstanceNorm1d reshapes inputs to(1, N * C, ...) from (N, C,...) and this makesvariances 0.<NewLine></code></pre><NewLine><p>Did I misunderstand something?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>nn.InstanceNorm1d</code> will calculate the statistics for each sample in the batch separately.<br/><NewLine>While this might be an advantage over batchnorm layers for small batch sizes (unsure if <code>InstanceNorm</code> would perform better than e.g. <code>GroupNorm</code>), the statistics would still need more than a single scalar in the temporal dimension.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/nono; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/kontrabas; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: August 2, 2019,  4:34pm; <NewLine> REPLY_DATE 2: March 4, 2020,  9:19am; <NewLine> REPLY_DATE 3: March 4, 2020,  1:09pm; <NewLine> REPLY_DATE 4: June 24, 2020, 10:37am; <NewLine> REPLY_DATE 5: June 25, 2020,  3:41am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
79981,A question on detach() in DQN loss,2020-05-06T11:56:50.498Z,1,150,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>In Deep-Q-Networks we compute the loss as:</p><NewLine><pre><code class=""lang-auto"">loss = Q(s,a) - (r + gamma * maxQ'(s', a'))<NewLine></code></pre><NewLine><p>So in order to calculate values of Q(s,a) and maxQ’(s’,a’), we need to do to a forward pass on the model two times. If I understood correctly this would create two different computation graphs for each forward pass.</p><NewLine><p>So my question is whether I “have to” detach() the resulting value of maxQ’(s’,a’) before doing the backward pass. Does it lead to errors if I don’t, and why?</p><NewLine></div>",https://discuss.pytorch.org/u/Maziar,(Maziar),Maziar,"May 6, 2020, 11:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can write your code in this way to avoid the unwanted grad calculation:</p><NewLine><pre><code class=""lang-auto"">Q(s, a) = your_model(s, a)<NewLine>with torch.no_grad():<NewLine>    maxQ'(s',a') = your_model(s', a')<NewLine></code></pre><NewLine><p>Then the forward calculation of maxQ will not accumulate grads on parameters of your model.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks Liu.</p><NewLine><p>However, my question is mostly about if I have to do that, and whether it would produce errors if I don’t, and why. (in a situation where we have x and y, both resulting from forward passes and both have gradients and now we want to backprop loss(x, y))</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Any help/pointers/references on this topic is still appreciated.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>You are welcome, yes you have to do that either by using ‘with torch.no_grad()’ or ‘.detach()’, otherwise your gradient is not accumulated correctly with respect to the loss.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/liuruiqi1107; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Maziar; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Maziar; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/liuruiqi1107; <NewLine> ,"REPLY_DATE 1: May 6, 2020, 12:50pm; <NewLine> REPLY_DATE 2: May 6, 2020,  1:32pm; <NewLine> REPLY_DATE 3: May 7, 2020,  1:59pm; <NewLine> REPLY_DATE 4: June 17, 2020,  6:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
84887,Activation function and distribution for action space between 0 and 1?,2020-06-10T10:49:34.280Z,2,136,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am building my first REINFORCE (policy gradient) model with a continuous action space between 0 and 1. Right now, I use the following code for this:</p><NewLine><pre><code class=""lang-auto"">        self.loc_layer = nn.Sequential(<NewLine>            nn.Linear(size, n_params),<NewLine>            nn.Sigmoid()<NewLine>        )<NewLine>        self.scale_layer = nn.Sequential(<NewLine>            nn.Linear(size, n_params),<NewLine>            nn.Sigmoid()<NewLine>        )<NewLine><NewLine>    	# ...<NewLine><NewLine>        loc = self.loc_layer(x)<NewLine>        if self.training:<NewLine>            scale = self.scale_layer(x)<NewLine>            dist = Normal(loc=loc, scale=scale)<NewLine>            params = dist.sample()<NewLine>            log_prob = dist.log_prob(params).sum(dim=1)<NewLine>            params = params.clamp(min=0, max=1)<NewLine>        else:<NewLine>            params = loc<NewLine>            log_prob = None<NewLine></code></pre><NewLine><p>However, this does not seem right to me. For example, I do not like that I have to clamp to ensure that the actions (<code>params</code>) are between 0 and 1. Therefore, I consider using a Beta or Logit-Normal distribution instead. In other examples, I have seen that Softplus is often used as the activation function for the scale argument. But this does not seem right here because I want to limit the action space.</p><NewLine><p>Can anyone with some experience in this area give me a recommendation to which activation function and distribution to use in this case?</p><NewLine></div>",https://discuss.pytorch.org/u/joel,(Joel Richard),joel,"June 10, 2020, 10:49am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Three solutions:</p><NewLine><ol><NewLine><li><NewLine><p>use a normal distribution, use tanh as <code>mu</code> activation (to keep the center in range, prevent shifting too much) and then clamp, but you should do clamping only on the action sent to the environment, and not actions stored in buffers. In this way, you are not changing the pdf of your action, but changing the reward distribution.</p><NewLine></li><NewLine><li><NewLine><p>use a normal distribution, use tanh to clamp your output right after sampling, but in order to calculate the log probability of your sample, you need to do this step, which is used in the <a href=""https://arxiv.org/pdf/1801.01290.pdf"" rel=""nofollow noopener"">Soft Actor Critic algorithm</a>.<br/><NewLine><img alt=""image"" data-base62-sha1=""r0WQoG5NPGkCIm1d1dJuCuJjzLD"" height=""51"" src=""https://discuss.pytorch.org/uploads/default/original/3X/b/d/bd5629b56fefadc5c22e2b775f6f69e9a7acda0d.png"" width=""332""/><br/><NewLine>It should be relatively easy to deduce this equation</p><NewLine></li><NewLine><li><NewLine><p>use a different distribution, but normally, the normal distribution performs the best in all algorithms, I have not tried Beta or Logit-Normal distribution and don’t know their performance. This method is theoretically possible, but result is not garanteed.</p><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot for your answer! Do you think it’s better to use a transformed tanh (i.e. <code>(tanh + 1)/2</code>) instead of sigmoid for actions in the range of 0 to 1? If so, because of the stronger gradient or is there another reason?</p><NewLine><p>I have switched to softplus for the activation function of the scale parameter (since this seems be commonly used) and it seems to work. Is this what you would have recommended as well?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>tanh and sigmoid does not quite affect the performance.</p><NewLine><p>Anything that will output a positive value can be used as your sigma (scale) activation, softplus is commonly used because its smooth, close to linear (y=x) when x &gt;&gt; 0 and works on the negative region.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/joel; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: June 17, 2020, 12:02pm; <NewLine> REPLY_DATE 2: June 17, 2020, 12:33pm; <NewLine> REPLY_DATE 3: June 17, 2020, 12:37pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
83782,NN training for function estimation,2020-06-02T00:30:06.877Z,3,116,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello guys,</p><NewLine><p>I’m a pretty new learner in ML/AI. This domain is so amazing but no so easy. In order to learn and understand I’m trying to do my own exercise to learn how Pytorch is working.<br/><NewLine>To do so I created a test where I want the model to predict the function f(x) = 2x. Pretty simple I guess. But my first attempt to write a python script give me not so bad results.</p><NewLine><p>Here is the script :</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine>"" Data for training""<NewLine>x_train = torch.FloatTensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])<NewLine>y_train = torch.FloatTensor([2, 4, 6, 8, 10, 12, 14, 16, 18, 20])<NewLine><NewLine>"" Data for evaluation of model""<NewLine>x_test = torch.FloatTensor([2, 4, 6, 8, 10, 12, 14, 16, 18, 20])<NewLine><NewLine>model = torch.nn.Sequential(torch.nn.Linear(10, 3),torch.nn.ReLU(),torch.nn.Linear(3, 10),)<NewLine><NewLine>loss_fn = torch.nn.MSELoss(reduction='sum')<NewLine><NewLine>learning_rate = 1e-4<NewLine>for t in range(48500):<NewLine>    y_pred = model(x_train)<NewLine>    loss = loss_fn(y_pred, y_train)<NewLine>    if t % 1000 == 999:<NewLine>        print(t, loss.item())<NewLine>    <NewLine>    model.zero_grad()<NewLine>    loss.backward()<NewLine>    with torch.no_grad():<NewLine>        for param in model.parameters():<NewLine>            param -= learning_rate * param.grad<NewLine>            <NewLine>y_pred_to_validate = model(x_test)<NewLine>print(y_pred_to_validate.detach().numpy())<NewLine>type or paste code here<NewLine></code></pre><NewLine><p>And the result displayed is not so bad :</p><NewLine><pre><code class=""lang-auto"">[ 4.138885  8.290704 12.023239 16.034138 19.773151 24.239693 28.20026<NewLine> 31.853294 35.714657 39.374302]<NewLine></code></pre><NewLine><p>I would like to have your thoughts and your advises are welcome. To be honest the model I created I take it from another example found on the net but I don’t really understand if it’s well suited for this case or not. It’s the same for the number of iteration, I put 48500 because it gave me a good result and I know that overfitting is not really recommended.</p><NewLine><p>I like criticism, it’s the best way to improve myself. Thank you for your time and long life to PyTorch.</p><NewLine><p>David.</p><NewLine></div>",https://discuss.pytorch.org/u/TheScalper,(The Scalper),TheScalper,"June 2, 2020, 12:44am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do not hand craft the gradient descent part, use <code>torch.optim</code><br/><NewLine>And your model is strange, use one single <code>torch.nn.Linear(10, 10)</code> is more than sufficient for such a simple function.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply. You’re right about the gradient descent. I will try to use the optim to see the difference.</p><NewLine><p>I agree also with you about the model. Related to the training test I choose yes we can use a simpler model but from what I read the linear/relu/linear is a kind of all purpose model. Is it correct?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, and in your case, your result looks ok. How to choose your model depends on your application and is totally up to you.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello guys,</p><NewLine><p>I have 1 question about the part of the for loop code. We are iterating several time to adjust the parameters to fit the model. Using the ML words, can we say it is reinforcement or not? If not what is the difference between reinforcement and this adjustment loop?</p><NewLine><p>Thanks for your help.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I personally think the defining feature of reinforcement learning is the bellman equation, bellman equation with discount &lt; 1 gaurantees that you will have a converging result.</p><NewLine><p>You code does not show that you are using this methodology, therefore I don’t think it is reinforcement learning (personally).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/TheScalper; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/TheScalper; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: June 4, 2020,  4:47pm; <NewLine> REPLY_DATE 2: June 7, 2020, 11:33am; <NewLine> REPLY_DATE 3: June 7, 2020, 12:07pm; <NewLine> REPLY_DATE 4: June 17, 2020, 11:50am; <NewLine> REPLY_DATE 5: June 17, 2020, 12:07pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
85609,Is Pytorch REINFORCE implementation correct?,2020-06-16T08:31:44.110Z,0,112,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,</p><NewLine><p>Perhaps I am very much misunderstanding some of the semantics of <code>loss.backward()</code> and <code>optimizer.step()</code>. In the <a href=""https://github.com/pytorch/examples/blob/59caa16a29476f290d2e4d38162a89372e5ccb15/reinforcement_learning/reinforce.py#L62"" rel=""nofollow noopener"">Pytorch example implementation of the REINFORCE algorithm</a>, we have the following excerpt from the <code>finish_episode()</code> function</p><NewLine><pre><code class=""lang-auto"">for log_prob, R in zip(policy.saved_log_probs, returns):<NewLine>    policy_loss.append(-log_prob * R)<NewLine>optimizer.zero_grad()<NewLine>policy_loss = torch.cat(policy_loss).sum()<NewLine>policy_loss.backward()<NewLine>optimizer.step()<NewLine></code></pre><NewLine><p>In the REINFORCE algorithm described in the Reinforcement Learning book by Richard S. Sutton and Andrew G. Barto, the reinforce update for the parameter vector, <code>\theta</code>, is done by</p><NewLine><pre><code class=""lang-auto"">\theta_{t+1} = \theta_t + \alpha G_t (\nabla\pi(A_t|S_t,\theta_t) / \pi(A_t|S_t,\theta_t)),<NewLine></code></pre><NewLine><p>i.e., the parameter vector is updated in <em>every</em> step through gradient ascent.</p><NewLine><p>Maybe I am wrong, but <code>policy_loss.backward()</code> appears to compute the gradient of all the arguments in the loss with respect to a <em>single</em> parameter vector <code>\theta</code>, and then <code>optimizer.step()</code> essentially adds these gradients assuming that <code>\theta_t</code> is the same for all t values, which does not seem to be equivalent to the theoretical implementation.</p><NewLine><p>Is there something that I am missing or not seeing clearly here?</p><NewLine><p>Thank you for your help.</p><NewLine></div>",https://discuss.pytorch.org/u/cruzas,(Samuel),cruzas,"June 16, 2020,  8:46am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually in real applications, whether to update your actor&amp;critic per step or update them after collecting several episodes does not quite affect the performance of your algorithm.</p><NewLine><p>However, it could be very inefficient to compute backward pass per step, as you cannot fill up your cpu ALU units or CUDA cores on your gpu, the computation price would be way too expensive. Therefore, batching is an economical choice., although it does not conform to the “theoretical model” of the REINFORCE algorithm.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: June 17, 2020,  1:14pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
85514,Unnormalize rewards of final network,2020-06-15T12:13:47.354Z,0,94,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey there,</p><NewLine><p>I trained a NN using PPO. My network gives me the action I should do for a given state and the estimated value for that state and action. I trained the network with normalized rewards:</p><NewLine><pre><code class=""lang-auto"">        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)<NewLine></code></pre><NewLine><p>Questions:</p><NewLine><ol><NewLine><li><NewLine><p>In practice (when using the NN) I just get the normalized estimated value - is there any way to get the true estimated value? - I do not have rewards.mean() etc. thus I cannot just calculate it as expalined <a href=""https://discuss.pytorch.org/t/unnormalization/78463"">here</a>.</p><NewLine></li><NewLine><li><NewLine><p>What is the estimated value of my value function ? Does it simply rate the actual state and action or does it evaluate the given state and gives me a hint on the final score?</p><NewLine></li><NewLine><li><NewLine><p>What I actually want is a network that predicts me the final score for a given state. Can I use the value function to achieve this? Or should I use something different -&gt; train a seperate NN?</p><NewLine></li><NewLine></ol><NewLine><p>My code is at:<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/CesMak/mcts_cardgame/blob/master/modules/ppo_witches_v4.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/CesMak/mcts_cardgame/blob/master/modules/ppo_witches_v4.py"" rel=""nofollow noopener"" target=""_blank"">CesMak/mcts_cardgame/blob/master/modules/ppo_witches_v4.py</a></h4><NewLine><pre><code class=""lang-py"">import torch<NewLine>import torch.nn as nn<NewLine>from torch.distributions import Categorical<NewLine>import gym<NewLine>import gym_witches_multiv2<NewLine>import datetime<NewLine><NewLine># For exporting the model:<NewLine>import torch.onnx<NewLine>import onnx<NewLine>import onnxruntime<NewLine><NewLine>import numpy as np<NewLine>import os<NewLine>import random<NewLine><NewLine>from copy import deepcopy # used for baches and memory<NewLine><NewLine># use ray for remote / parallel playing games speed up of 40%<NewLine>import ray #pip install ray[rllib]<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/CesMak/mcts_cardgame/blob/master/modules/ppo_witches_v4.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>A snippet of my network is:</p><NewLine><pre><code class=""lang-auto""><NewLine>#Actor Model:<NewLine>class ActorModel(nn.Module):<NewLine>    def __init__(self, state_dim, action_dim, n_latent_var):<NewLine>        super(ActorModel, self).__init__()<NewLine>        self.a_dim   = action_dim<NewLine><NewLine>        self.ac      = nn.Linear(state_dim, n_latent_var)<NewLine>        self.ac_prelu= nn.PReLU()<NewLine>        self.ac1      = nn.Linear(n_latent_var, n_latent_var)<NewLine>        self.ac1_prelu= nn.PReLU()<NewLine><NewLine>        # Actor layers:<NewLine>        self.a1      = nn.Linear(n_latent_var+action_dim, action_dim)<NewLine><NewLine>        # Critic layers:<NewLine>        self.c1      = nn.Linear(n_latent_var, n_latent_var)<NewLine>        self.c1_prelu= nn.PReLU()<NewLine>        self.c2      = nn.Linear(n_latent_var, 1)<NewLine><NewLine>    def forward(self, input):<NewLine>        # For 4 players each 15 cards on hand:<NewLine>        # input=on_table(60)+ on_hand(60)+ played(60)+ play_options(60)+ add_states(15)<NewLine>        # add_states = color free (4)+ would win (1) = 5  for each player<NewLine>        #input.shape  = 15*4*4=240+3*5 (add_states) = 255<NewLine><NewLine>        #Actor and Critic:<NewLine>        ac = self.ac(input)<NewLine>        ac = self.ac_prelu(ac)<NewLine>        ac = self.ac1(ac)<NewLine>        ac = self.ac1_prelu(ac)<NewLine><NewLine>        # Get Actor Result:<NewLine>        if len(input.shape)==1:<NewLine>            options = input[self.a_dim*3:self.a_dim*4]<NewLine>            actor_out =torch.cat( [ac, options], 0)<NewLine>        else:<NewLine>            options = input[:, self.a_dim*3:self.a_dim*4]<NewLine>            actor_out   = torch.cat( [ac, options], 1)<NewLine>        actor_out = self.a1(actor_out)<NewLine>        actor_out = actor_out.softmax(dim=-1)<NewLine><NewLine>        # Get Critic Result:<NewLine>        critic = self.c1(ac)<NewLine>        critic = self.c1_prelu(critic)<NewLine>        critic = self.c2(critic)<NewLine><NewLine>        return actor_out, critic<NewLine><NewLine>class ActorCritic(nn.Module):<NewLine>    def __init__(self, state_dim, action_dim, n_latent_var):<NewLine>        super(ActorCritic, self).__init__()<NewLine>        self.a_dim   = action_dim<NewLine><NewLine>        # actor critic<NewLine>        self.actor_critic = ActorModel(state_dim, action_dim, n_latent_var)<NewLine><NewLine>    def act(self, state, memory):<NewLine>        if type(state) is np.ndarray:<NewLine>            state = torch.from_numpy(state).float()<NewLine>        action_probs, _ = self.actor_critic(state)<NewLine>        # here make a filter for only possible actions!<NewLine>        #action_probs = action_probs *state[self.a_dim*3:self.a_dim*4]<NewLine>        dist = Categorical(action_probs)<NewLine>        action = dist.sample()# -&gt; gets the lowest non 0 value?!<NewLine><NewLine>        if memory is not None:<NewLine>            #necessary to convet all to numpy otherwise deepcopy not possible!<NewLine>            memory.states.append(state.data.numpy())<NewLine>            memory.actions.append(int(action.data.numpy()))<NewLine>            memory.logprobs.append(float(dist.log_prob(action).data.numpy()))<NewLine><NewLine>        return action.item()<NewLine><NewLine>    def evaluate(self, state, action):<NewLine>        action_probs, state_value = self.actor_critic(state)<NewLine>        dist = Categorical(action_probs)<NewLine><NewLine>        action_logprobs = dist.log_prob(action)<NewLine>        dist_entropy    = dist.entropy()<NewLine>        return action_logprobs, torch.squeeze(state_value), dist_entropy<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/CesMak,(Markus Lamprecht),CesMak,"June 15, 2020, 12:13pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Since you are just using the normalized target value calculated using the MCTS method below:</p><NewLine><pre><code class=""lang-auto"">def monteCarloRewards(self, memory):<NewLine>        # Monte Carlo estimate of state rewards:<NewLine>        # see: https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511<NewLine>        rewards = []<NewLine>        discounted_reward = 0<NewLine>        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):<NewLine>            if is_terminal:<NewLine>                discounted_reward = 0<NewLine>            discounted_reward = reward + (self.gamma * discounted_reward)<NewLine>            rewards.append(discounted_reward)<NewLine>        rewards.reverse()<NewLine>        # Normalizing the rewards:<NewLine>        rewards = torch.tensor(rewards)<NewLine>        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)<NewLine>        return rewards<NewLine></code></pre><NewLine><p>and not using the generalized advantage function (GAE):</p><NewLine><p>Your critic will only work if your network generates two output from one state input, i.e. your actor and critic stems from the same root, and you are doing this correctly. Otherwise critic will not affect the training.<br/><NewLine>(P.S I have accidentally seperated the actor and the critic before, and it seems that actor will work fine just with enough MCTS samples, even without the critic.)</p><NewLine><p>So, answers for your questions:<br/><NewLine>Q1 and Q2. Critic here just serves as a supplementary gradient source, so if you would like your critic to just optimize on (unormalized) MCTS score, it will work fine, but will probably be unstable.<br/><NewLine>Q3. Of course you can, you can even remove the critic completely (see the PS section above) and just let your actor optimize on the <code>log_prob * target_value</code>, but it would likely be unstable.</p><NewLine><p>So in general, if you would like to get a “True” estimation of your target value, I would suggest you train a different network, and not touch your actor-critic setup here.</p><NewLine><p>If you are using the GAE function, then in your implementation, you may seperate your critic and actor, your critic must directly optmize on the target value and the output of your critic will be used by GAE, and your actor will optimize on the normalized “advantage” value given by the GAE function, in this case, I would suggest you use the critic to directly give a future value prediction.</p><NewLine><p>Am I clear? @ me if you still have any questions.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: June 15, 2020,  5:22pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
85445,Policy gradient training loss goes to zero,2020-06-14T18:37:48.929Z,0,104,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I am training my network with the policy gradient approach. My rewards are either -1 or 1, and I calculate the loss and update the parameters just like in <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""nofollow noopener"">here</a>. However, the loss increases and goes to zero and the average reward in each epoch quickly converges to -1. I attached the loss and reward plots. My optimizer is SGD, and I tried ADAM and also played around with the learning rate, but no success. Does anyone have any ideas?</p><NewLine><p>Thanks.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/7d2dca24839cc0c40a4aa7b6027041bdc456b52f"" href=""https://discuss.pytorch.org/uploads/default/original/3X/7/d/7d2dca24839cc0c40a4aa7b6027041bdc456b52f.png"" title=""image""><img alt=""image"" data-base62-sha1=""hRnMPAqehcRaGrwdRZkpy5FuhRl"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/7/d/7d2dca24839cc0c40a4aa7b6027041bdc456b52f_2_10x10.png"" height=""316"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/7/d/7d2dca24839cc0c40a4aa7b6027041bdc456b52f_2_690x316.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/7/d/7d2dca24839cc0c40a4aa7b6027041bdc456b52f_2_690x316.png, https://discuss.pytorch.org/uploads/default/original/3X/7/d/7d2dca24839cc0c40a4aa7b6027041bdc456b52f.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/7/d/7d2dca24839cc0c40a4aa7b6027041bdc456b52f.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">829×380 24.6 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>Edit: at the output I have a softmax layer with 11 outputs (actions) and it seems that after training, the model “learns” to output 0 for all actions but one. Then, the log probability of that action will be 0 and that is why the loss is zero. This is very bizarre to me because I expect the loss to decrease and not increase.</p><NewLine></div>",https://discuss.pytorch.org/u/sepehr78,,sepehr78,"June 14, 2020,  6:57pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you please share your implementation?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: June 15, 2020,  2:00am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
84884,Errors using transforms,2020-06-10T10:07:28.163Z,0,78,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am performing transforms an images for a dog classifier and I am getting an error. Here is the code and the error I am getting:</p><NewLine><p>import os<br/><NewLine>from torchvision import datasets<br/><NewLine>from torchvision import datasets<br/><NewLine>from PIL import ImageFile</p><NewLine><h3>TODO: Write data loaders for training, validation, and test sets</h3><NewLine><p>ImageFile.LOAD_TRUNCATED_IMAGES = True</p><NewLine><h1>number of subprocesses to use for data loading</h1><NewLine><p>num_workers = 0</p><NewLine><h1>batch_sizes</h1><NewLine><p>batch_size = 20</p><NewLine><h2>Specify appropriate transforms, and</h2><NewLine><p>transforms ={</p><NewLine><pre><code>'train' : transforms.Compose([transforms.Resize(256),<NewLine>                              transforms.RandomResizedCrop(224),<NewLine>                              transforms.RandomHorizontalFlip(),<NewLine>                              transforms.RandomRotation(10),<NewLine>                              transforms.RandomVerticalFlip(),<NewLine>                              transforms.ToTensor(),<NewLine>                              transforms.Normalize(mean=[0.485, 0.456, 0.406],<NewLine>                                                  std=[0.229,0.224,0.225])]),<NewLine>                              <NewLine>'valid' : transforms.Compose([transforms.Resize(256),<NewLine>                              transforms.RandomResizedCrop(224),<NewLine>                              transforms.RandomHorizontalFlip(),<NewLine>                              transforms.RandomRotation(10),<NewLine>                              transforms.RandomVerticalFlip(),<NewLine>                              transforms.ToTensor(),<NewLine>                              transforms.Normalize(mean=[0.485, 0.456, 0.406],<NewLine>                                                  std=[0.229,0.224,0.225])]),<NewLine>                              <NewLine>'test' : transforms.Compose([transforms.Resize(256),<NewLine>                              transforms.RandomResizedCrop(224),<NewLine>                              transforms.RandomHorizontalFlip(),<NewLine>                              transforms.RandomRotation(10),<NewLine>                              transforms.RandomVerticalFlip(),<NewLine>                              transforms.ToTensor(),<NewLine>                              transforms.Normalize(mean=[0.485, 0.456, 0.406],<NewLine>                                                  std=[0.229,0.224,0.225])])<NewLine></code></pre><NewLine><p>}</p><NewLine><p>Image_datasets = {x: datasets.ImageFolder(os.path.join(‘dogImages’,y),transforms[xy])<br/><NewLine>for x in[‘train’, ‘valid’,‘test’]}</p><NewLine><p>data_loaders = {x: torch.utils.data.DataLoader(Image_datasets, batch_size=batch_size, shuffle=True,<br/><NewLine>num_workers=num_workers)<br/><NewLine>for x in[‘train’, ‘valid’,‘test’]}</p><NewLine><p>Error:</p><NewLine><pre><code>                             ---------------------------------------------------------------------------<NewLine></code></pre><NewLine><p>AttributeError                            Traceback (most recent call last)<br/><NewLine> in <br/><NewLine>15 transforms ={<br/><NewLine>16<br/><NewLine>—&gt; 17     ‘train’ : transforms.Compose([transforms.Resize(256),<br/><NewLine>18                                   transforms.RandomResizedCrop(224),<br/><NewLine>19                                   transforms.RandomHorizontalFlip(),</p><NewLine><p>AttributeError: ‘dict’ object has no attribute ‘Compose’</p><NewLine></div>",https://discuss.pytorch.org/u/Brian_Asimba,(Brian Asimba),Brian_Asimba,"June 10, 2020, 10:07am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems you are redefining the <code>transforms</code> variable with your <code>dict</code>.<br/><NewLine>Change the name of the <code>dict</code> to <code>transform_dict</code> or something else and rerun the code.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: June 11, 2020,  6:46am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
40411,Issues with concatenating tensors for policy history,2019-03-20T13:45:27.349Z,0,433,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m following an older tutorial on policy gradient RL for cartpole <a href=""https://medium.com/@ts1829/policy-gradient-reinforcement-learning-in-pytorch-df1383ea0baf"" rel=""nofollow noopener"">seen here</a>, and I’m getting the following error for the select action step</p><NewLine><pre><code class=""lang-auto"">class Policy(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Policy, self).__init__()<NewLine>        self.state_space = env.observation_space.shape[0]<NewLine>        self.action_space = env.action_space.n<NewLine>        <NewLine>        self.l1 = nn.Linear(self.state_space, 128, bias=False)<NewLine>        self.l2 = nn.Linear(128, self.action_space, bias=False)<NewLine>        <NewLine>        self.gamma = gamma<NewLine>        <NewLine>        # Episode policy and reward history <NewLine>        self.policy_history = Variable(torch.Tensor()) <NewLine>        self.reward_episode = []<NewLine>        # Overall reward and loss history<NewLine>        self.reward_history = []<NewLine>        self.loss_history = []<NewLine>def forward(self, x):    <NewLine>        model = torch.nn.Sequential(<NewLine>            self.l1,<NewLine>            nn.Dropout(p=0.6),<NewLine>            nn.ReLU(),<NewLine>            self.l2,<NewLine>            nn.Softmax(dim=-1)<NewLine>        )<NewLine>        return model(x)<NewLine>policy = Policy()<NewLine>optimizer = optim.Adam(policy.parameters(), lr=learning_rate)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">def select_action(state):<NewLine>    #Select an action (0 or 1) by running policy model and choosing based on the probabilities in state<NewLine>    state = torch.from_numpy(state).type(torch.FloatTensor)<NewLine>    state = policy(Variable(state))<NewLine>    c = Categorical(state) # turns the <NewLine>    action = c.sample()<NewLine>    <NewLine>    # Add log probability of our chosen action to our history    <NewLine>    if policy.policy_history.dim() != 0:<NewLine>        policy.policy_history = torch.cat([policy.policy_history, c.log_prob(action)])<NewLine>    else:<NewLine>        policy.policy_history = (c.log_prob(action))<NewLine>    return action<NewLine></code></pre><NewLine><p>I get the following error when I begin to train the network</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-21-c1f1eef9c02f&gt; in &lt;module&gt;()<NewLine>      1 episodes = 1000<NewLine>----&gt; 2 main(episodes)<NewLine><NewLine>&lt;ipython-input-20-2b725c0b3414&gt; in main(episodes)<NewLine>      7 <NewLine>      8         for time in range(1000):<NewLine>----&gt; 9             action = select_action(state)<NewLine>     10             # Step through environment using chosen action<NewLine>     11             state, reward, done, _ = env.step(action.data[0])<NewLine><NewLine>&lt;ipython-input-18-f02a0e7d07a5&gt; in select_action(state)<NewLine>      8     # Add log probability of our chosen action to our history<NewLine>      9     if policy.policy_history.dim() != 0:<NewLine>---&gt; 10         policy.policy_history = torch.cat([policy.policy_history, c.log_prob(action)])<NewLine>     11     else:<NewLine>     12         policy.policy_history = (c.log_prob(action))<NewLine><NewLine>RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated<NewLine></code></pre><NewLine><p>How can I fix this error?</p><NewLine></div>",https://discuss.pytorch.org/u/maxmatical,(Maxmatical),maxmatical,"March 20, 2019,  1:45pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey there. I am facing the same issue. Were you able to solve it? <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Yara; <NewLine> ,"REPLY_DATE 1: June 10, 2020,  2:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
84008,Check flow of gradients concerning hidden states in a recurrent policy,2020-06-03T07:49:42.619Z,3,195,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everybody!</p><NewLine><p>I’m currently implementing a recurrent PPO policy and as of now it works for gym-minigrid-hallway and CartPole (masked velocity). However, I’ve got quiet some open questions left that leave me in doubts about my implementation.</p><NewLine><p>In comparison to a none-recurrent PPO implementation, I just added the hidden states of a GRU layer to the agents’ experience tuples. Concerning the sampling of mini batches, I make sure that sequences are maintained for each agent. That are all the changes I’ve made.</p><NewLine><p>One odd thing is that one PPO cycle/update does not take longer to compute while using the recurrent policy. I expected it to be notably more expensive. This raises the question to me whether the sequence of hidden states are back-propagated correctly. How could I check/verify that the gradients flow into the whole sequence of experience tuples, while not flowing into a new episode?</p><NewLine><p>For reference, these are all the <a href=""https://github.com/MarcoMeter/neroRL/issues/1"" rel=""nofollow noopener"">open questions and a link to my implementation</a>.</p><NewLine></div>",https://discuss.pytorch.org/u/leckofunny,(Marco Pleines),leckofunny,"June 3, 2020,  7:49am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Try to visualize your back-propagation graph and examine each node using:</p><NewLine><pre><code class=""lang-auto"">from torchviz import make_dot<NewLine><NewLine>graph = make_dot(final_tensor)<NewLine>graph.view()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/iffix"">@iffiX</a><br/><NewLine>I called make_dot right after <a href=""https://github.com/MarcoMeter/neroRL/blob/master/neroRL/trainers/PPO/trainer.py#L297"" rel=""nofollow noopener"">loss.backward()</a>:</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/9bb9c0b37e4a44d840b714365c9abacfa7ae96f0"" href=""https://discuss.pytorch.org/uploads/default/original/3X/9/b/9bb9c0b37e4a44d840b714365c9abacfa7ae96f0.png"" title=""Digraph.gv""><img alt=""Digraph.gv"" data-base62-sha1=""mdBYQdJNE21BF2T7n92pmNjVqdq"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/b/9bb9c0b37e4a44d840b714365c9abacfa7ae96f0_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/b/9bb9c0b37e4a44d840b714365c9abacfa7ae96f0_2_244x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/b/9bb9c0b37e4a44d840b714365c9abacfa7ae96f0_2_244x500.png, https://discuss.pytorch.org/uploads/default/optimized/3X/9/b/9bb9c0b37e4a44d840b714365c9abacfa7ae96f0_2_366x750.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/9/b/9bb9c0b37e4a44d840b714365c9abacfa7ae96f0_2_488x1000.png 2x"" width=""244""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Digraph.gv</span><span class=""informations"">1164×2376 130 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>I’m struggling now to identify the information about how many hidden_states are back-propagated.<br/><NewLine>Should multiple gru nodes be apparent if sequences of hidden states were back-propagated?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Your hidden states are not back propagated, the below part is your gru layer, check your code carefully.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/8a78e23343d281228d9e0f7281d29f4e9a5cd290"" href=""https://discuss.pytorch.org/uploads/default/original/3X/8/a/8a78e23343d281228d9e0f7281d29f4e9a5cd290.png"" title=""image""><img alt=""image"" data-base62-sha1=""jKYSJAVx2KMtekx8Zg0c9HylXlS"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/a/8a78e23343d281228d9e0f7281d29f4e9a5cd290_2_10x10.png"" height=""176"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/a/8a78e23343d281228d9e0f7281d29f4e9a5cd290_2_690x176.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/a/8a78e23343d281228d9e0f7281d29f4e9a5cd290_2_690x176.png, https://discuss.pytorch.org/uploads/default/original/3X/8/a/8a78e23343d281228d9e0f7281d29f4e9a5cd290.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/8/a/8a78e23343d281228d9e0f7281d29f4e9a5cd290.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">824×211 27.3 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>A litle bit more details:<br/><NewLine><img alt=""image"" data-base62-sha1=""jZdzCUJgNRLMpvMCUTIjw64kVGC"" height=""165"" src=""https://discuss.pytorch.org/uploads/default/original/3X/8/c/8c14e0836594f4bfa63a56ffccd936a3e5d45196.png"" width=""363""/><br/><NewLine>is represented by this:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/5642d44904d86405e47f31bc4f60edd47f7abb27"" href=""https://discuss.pytorch.org/uploads/default/original/3X/5/6/5642d44904d86405e47f31bc4f60edd47f7abb27.png"" title=""image""><img alt=""image"" data-base62-sha1=""cj6f5NkNBlJmR6mXOWlFNrkqhNR"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/5/6/5642d44904d86405e47f31bc4f60edd47f7abb27_2_10x10.png"" height=""326"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/5/6/5642d44904d86405e47f31bc4f60edd47f7abb27_2_690x326.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/5/6/5642d44904d86405e47f31bc4f60edd47f7abb27_2_690x326.png, https://discuss.pytorch.org/uploads/default/original/3X/5/6/5642d44904d86405e47f31bc4f60edd47f7abb27.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/5/6/5642d44904d86405e47f31bc4f60edd47f7abb27.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1024×485 64.9 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just made a backprop graph from this <a href=""https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/master/a2c_ppo_acktr/algo/ppo.py#L80"" rel=""nofollow noopener"">repo</a> where I suspect a working backpropagation through past hidden states.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/02812636a06a4910460e5849f382333fa5892d76"" href=""https://discuss.pytorch.org/uploads/default/original/3X/0/2/02812636a06a4910460e5849f382333fa5892d76.png"" title=""Digraph.gv""><img alt=""Digraph.gv"" data-base62-sha1=""m9EFzULyQxq6rQFCsiIngcCt2S"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/2/02812636a06a4910460e5849f382333fa5892d76_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/2/02812636a06a4910460e5849f382333fa5892d76_2_239x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/2/02812636a06a4910460e5849f382333fa5892d76_2_239x500.png, https://discuss.pytorch.org/uploads/default/optimized/3X/0/2/02812636a06a4910460e5849f382333fa5892d76_2_358x750.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/0/2/02812636a06a4910460e5849f382333fa5892d76_2_478x1000.png 2x"" width=""239""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Digraph.gv</span><span class=""informations"">988×2059 101 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>I’m wondering now what it should look like if past hidden states are successfully back-propagated.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just recalled that hidden states are also stored as the <code>current_state</code> and <code>next_state</code> entries, and they are detached, so a single GRU layer is possible, since in this scenario, gradients will not flow through those detached hidden states. It really depends on your model implementation.</p><NewLine><p>I also noticed that you have serveral different modules in the bottom PPO part, compared to that repo, may be you should also check those modules? It is also possible that you may have wrongly implemented PPO, there are many possibilities.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/bc62f1b4681fc16b09d317d0e78b9f8cbd6f695c"" href=""https://discuss.pytorch.org/uploads/default/original/3X/b/c/bc62f1b4681fc16b09d317d0e78b9f8cbd6f695c.png"" title=""image""><img alt=""image"" data-base62-sha1=""qSxKA6npNx6EzJI1SBB7Me7n8Is"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/c/bc62f1b4681fc16b09d317d0e78b9f8cbd6f695c_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/original/3X/b/c/bc62f1b4681fc16b09d317d0e78b9f8cbd6f695c.png"" width=""286""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">519×906 17.8 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>So if past hidden states are back-propagated, I should see more GRU nodes, right?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, but it depends on your model. if in each time step, you are just feeding the last hidden state <code>h(t-1)</code>, the observed state <code>x(t)</code> into your GRU layer (this is the basic GRU / LSTM/ RNN model), then there should be only one GRU layer in your flow graph.</p><NewLine><p>And if you are feeding more history states (such as 4), then you will see more GRU layers.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Marco,</p><NewLine><p>I think you shouldn’t be storing the hidden states of the GRU in the experience buffer. Or if you do, you shouldn’t be feeding those as constants since autograd needs to know that gradients have to backpropagate through time. Every time you do an update you should feed all the observations in the sequence, and let the network recompute its hidden states.</p><NewLine><p>There is also the question of what should be the initial hidden state of that sequence. In DRQN people normally use a vector of zeros, since the hidden states of the GRU can be very different when the experience was collected than when the experience is actually used for updating the network. However, for PPO you can actually use the hidden state that was computed while interacting with the environment.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/miguel_suau"">@Miguel_Suau</a><br/><NewLine>I agree as hidden states from the sampling phase can already be considered deprecated.</p><NewLine><p>I’m still looking into the issue of observing the flow of gradients into the hidden states.</p><NewLine><p>I just rendered a graph based on this code <a href=""https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python/"" rel=""nofollow noopener"">https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python/</a> .<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/4b92ed7b1cbef40f4284a867563d3c36c0531092"" href=""https://discuss.pytorch.org/uploads/default/original/3X/4/b/4b92ed7b1cbef40f4284a867563d3c36c0531092.png"" title=""Digraph.gv""><img alt=""Digraph.gv"" data-base62-sha1=""aMyBhyS2ovuxhVKrGnxMVO3sbl0"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/b/4b92ed7b1cbef40f4284a867563d3c36c0531092_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/b/4b92ed7b1cbef40f4284a867563d3c36c0531092_2_167x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/b/4b92ed7b1cbef40f4284a867563d3c36c0531092_2_167x500.png, https://discuss.pytorch.org/uploads/default/optimized/3X/4/b/4b92ed7b1cbef40f4284a867563d3c36c0531092_2_250x750.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/4/b/4b92ed7b1cbef40f4284a867563d3c36c0531092_2_334x1000.png 2x"" width=""167""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Digraph.gv</span><span class=""informations"">2099×6249 800 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>The sequence length is defined as 12 and it looks like that some structure of this graph is repeated 12 times. But this behavior is not apparent for this repo: ikostrikov/pytorch-a2c-ppo-acktr-gail (see one of the graphs above). In that repo, the hidden states are recomputed during the mini batch updates.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/leckofunny; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/leckofunny; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/leckofunny; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Miguel_Suau; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/leckofunny; <NewLine> ,"REPLY_DATE 1: June 4, 2020,  4:43pm; <NewLine> REPLY_DATE 2: June 5, 2020, 10:06am; <NewLine> REPLY_DATE 3: June 5, 2020, 10:51am; <NewLine> REPLY_DATE 4: June 5, 2020, 11:20am; <NewLine> REPLY_DATE 5: June 5, 2020, 11:30am; <NewLine> REPLY_DATE 6: June 5, 2020, 12:11pm; <NewLine> REPLY_DATE 7: June 5, 2020, 12:42pm; <NewLine> REPLY_DATE 8: June 8, 2020,  4:20pm; <NewLine> REPLY_DATE 9: June 9, 2020, 10:13am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> REPLY 9 LIKES: ; <NewLine> 
84388,Theano library function equivalent in PyTorch,2020-06-06T09:18:08.304Z,5,156,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>If you see (<a href=""http://deeplearning.net/software/theano/library/compile/function.html"" rel=""nofollow noopener"">http://deeplearning.net/software/theano/library/compile/function.html</a>) Theano can create functions. I absolutely sure that PyTorch also can have something like that but how we can equivalent the Update parameter of Theano function in PyTorch? (I saw this <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/porting-code-from-thano-lasagne-to-pytorch/36388"">Porting code from Thano/Lasagne to PyTorch</a> but they didn’t talk about the update)</p><NewLine><p>Let’ s have an example:</p><NewLine><pre><code class=""lang-auto"">def rmsprop_updates(grads, params, stepsize, rho=0.9, epsilon=1e-9):<NewLine><NewLine>    updates = []<NewLine><NewLine>    for param, grad in zip(params, grads):<NewLine>        accum = theano.shared(np.zeros(param.get_value(borrow=True).shape, dtype=param.dtype))<NewLine>        accum_new = rho * accum + (1 - rho) * grad ** 2<NewLine>        updates.append((accum, accum_new))<NewLine>        updates.append((param, param + (stepsize * grad / T.sqrt(accum_new + epsilon))))<NewLine>        # lasagne has '-' after param<NewLine>    return updates<NewLine><NewLine>updates = rmsprop_updates( grads, params, self.lr_rate, self.rms_rho, self.rms_eps)<NewLine><NewLine>N = states.shape[0]<NewLine>loss = T.log(prob_act[T.arange(N), actions]).dot(values) / N<NewLine><NewLine>self._train_fn = theano.function([states, actions, values], loss, updates=updates)<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/ahmadreza9,(Ahmadreza),ahmadreza9,"June 6, 2020,  9:23am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Instead of compiling a function from a string, the programming paradigm used by pytorch is using modules as building blocks to create a function, there is only one important thing in pytorch: <strong>tensor</strong>, tensor is both the container for data, the interface for low-level device handling, and the gradient flow component. Therefore, instead of defining a graph, you construct a graph <strong>dynamically</strong> using normal calculus operators like <code>+, -, *, /</code></p><NewLine><p>e.g.:</p><NewLine><pre><code class=""lang-auto"">f = theano.function([x], 2*x)<NewLine></code></pre><NewLine><p>is equivuivalent to:</p><NewLine><pre><code class=""lang-auto""># suppose you have some tensor ``x``<NewLine># or create it<NewLine>x = torch.zeros([100,100], device=""cuda:0"", dtype=torch.int)<NewLine>def f(input):<NewLine>    return input * 2<NewLine>print(f(x))<NewLine></code></pre><NewLine><p>Moreover, since pytorch is dynamic, sometimes users may require a just-in-time compilation utility to remove the tensor construction cost in python, you can do that by using <code>torch.jit</code></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I know paradigm and your simple example which is explained in the first hyperlink of my comment. I want to create custom optimizer like this (<a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/custom-optimizer-in-pytorch/22397"">Custom Optimizer in PyTorch</a>) so I should update weights like this:</p><NewLine><pre><code class=""lang-auto"">weight_update = smth_with_good_dimensions<NewLine>param.data.sub_(weight_update * learning_rate)<NewLine></code></pre><NewLine><p>Now, how can we have a function in PyTorch like theano.function for my rms_prop update?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>As <a class=""mention"" href=""/u/iffix"">@iffiX</a> mentioned, we do not compile graph in PyTorch, it will be constructed during data flow in forward pass. So, for an optimizer, you just define a class that accepts parameters of a <code>model = nn.Module</code> and implements <code>step</code> function for it. Literally, based on your code, you need to only remove last line <code>self._train_fn = ...</code> as there is no compile stage in PyTorch.<br/><NewLine>Here is the SGD implementation:<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html#SGD"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html#SGD</a></p><NewLine><p>Bests</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>From the documentation of theano, I guess you are askinig for manual control over gradient updates?<br/><NewLine>then:<br/><NewLine>For simple modules, use <code>register_backward_hook</code><br/><NewLine>Or control gradients on each of your input directly using: <code>register_hook</code></p><NewLine><p>Optimizers are implemented in <code>torch.optim</code>, including RMSProp, Adam, SGD, etc.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>And indeed, if you want a completely new optimizer rather than control gradients by hand, you should inherit from <code>torch.optim.Optimizer</code> and implement the <code>step</code> method. (remember to wrap it with <span class=""mention"">@torch.no_grad</span>())</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/iffix"">@iffiX</a>. This comment is the most relevant answer to my question. I will test  <code>register_backward_hook</code> function to manually control the gradient of my network(grads).</p><NewLine><pre><code class=""lang-auto"">N = states.shape[0]<NewLine><NewLine>loss = T.log(prob_act[T.arange(N), actions]).dot(values) / N  # call it ""loss""<NewLine><NewLine>grads = T.grad(loss, params)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ahmadreza9; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Nikronic; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ahmadreza9; <NewLine> ,"REPLY_DATE 1: June 7, 2020,  5:11am; <NewLine> REPLY_DATE 2: June 8, 2020,  9:18am; <NewLine> REPLY_DATE 3: June 7, 2020, 10:11am; <NewLine> REPLY_DATE 4: June 7, 2020, 10:15am; <NewLine> REPLY_DATE 5: June 7, 2020, 10:15am; <NewLine> REPLY_DATE 6: June 8, 2020,  9:20am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
84617,Help with computing gradients wrt output itself,2020-06-08T08:33:57.416Z,0,75,"<div class=""post"" itemprop=""articleBody""><NewLine><h1>Obtaining the gradients with respect to the layer outputs</h1><NewLine><p>The formula for a NN output is typically y = activation(w.x) and when we do automatic differentiation we usually differentiate with respects to the weight vector, as in back-propagation. However, how do I differentiate with respect to y itself?</p><NewLine><p>To provide some background, I am passing y, the output of my actor network into my critic network and I would like to minimize the output of my critic network. So I would differentiate the predicted q-value with respect to the input of the critic / output of the actor. However, when I do backwards(), it computes the gradients of the weights in the output layer of the actor network. How can I get the gradient of the output of the actor network itself.</p><NewLine><p>Currently, the very ghetto way that I have been able to achieve this is to take the weight vector of the output layer of the actor network, add the computed gradient to it, then manually compute the output of the actor network (by matrix multiplication). Pass the output to the critic and verify that the predicted q-val has indeed decreased.</p><NewLine><p>Is there a better way to do this?</p><NewLine></div>",https://discuss.pytorch.org/u/kangtinglee,(Marcus Lee),kangtinglee,"June 8, 2020,  8:33am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Okay I figured out how to do this. And it was amazingly simple too… but I’ll leave this here anyway for anyone else in future who might stumble on the same problem because I had a really hard time googling for a solution.</p><NewLine><pre><code class=""lang-auto"">act = actor(state)<NewLine>act.retain_grad()<NewLine>q = critic(act)<NewLine>loss = -q<NewLine>loss.backward()<NewLine>new_act = act + act.grad * 0.01<NewLine>q = critic(new_act)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/kangtinglee; <NewLine> ,"REPLY_DATE 1: June 8, 2020,  9:02am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
73207,Reinforce for a multiplayer game,2020-03-14T11:25:12.459Z,1,191,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey there,</p><NewLine><p>I want to use Policy Gradients (see <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""nofollow noopener"">REINFORCE</a> and <a href=""https://pytorch.org/docs/stable/distributions.html"" rel=""nofollow noopener"">probability distributions</a>) to train a very simple 4-player card game. I have a lot of questions on this algorithm (part of the code is below, full code is available <a href=""https://github.com/CesMak/mcts_cardgame/tree/master/modules"" rel=""nofollow noopener"">reinforcement_learning.py</a>.</p><NewLine><p><strong>Questions</strong></p><NewLine><ol><NewLine><li>How to design the network?</li><NewLine></ol><NewLine><ul><NewLine><li>I have only little experience on this and would like to hear your suggestions</li><NewLine><li>How many hidden layers, conv2d or linear etc?</li><NewLine></ul><NewLine><ol start=""2""><NewLine><li>How to calculate the discounted rewards?</li><NewLine></ol><NewLine><ul><NewLine><li>I am currently using not discounted rewards</li><NewLine><li>Why should I use discounted rewards? (I have 2 discounted reward functions in the code below which of them would you use?)</li><NewLine></ul><NewLine><ol start=""3""><NewLine><li>The algorithm learns very slowly (or nothing at all?)</li><NewLine></ol><NewLine><ul><NewLine><li>after 10000 games the result is: [-61199.0, -52777.0, -58842.0, -60025.0]), Player 2 is the reinforcement player.</li><NewLine><li>How to measure if the algorithm learns something? -&gt; monitor losses.mean() ?</li><NewLine><li>How long should I train?</li><NewLine><li>What improvements can I do to learn faster? (Including more Reinforcement players ?, sharing policys?)</li><NewLine></ul><NewLine><ol start=""4""><NewLine><li>Batches</li><NewLine></ol><NewLine><ul><NewLine><li>I do not use any batches as you can see in the code</li><NewLine><li>In other examples I saw that many are using batches - why?</li><NewLine></ul><NewLine><p><strong>Rules of the game explained:</strong></p><NewLine><p>[8] 0 Laura	RANDOM	Card 9 of R	Hand Index 3<br/><NewLine>[8] 1 Alfons	REINFO	Card 13 of R	Hand Index 5<br/><NewLine>[8] 2 Frank	RANDOM	Card 6 of R	Hand Index 4<br/><NewLine>[8] 3 Lea	RANDOM	Card 5 of R	Hand Index 4<br/><NewLine>Update rewards:  [0, -4, 0, 0]</p><NewLine><ul><NewLine><li>Round 8, Player Index 0 = Laura plays a Random possible card</li><NewLine><li>Alfons Player Index 1 plays as Reinforcement Player he wins this round (highest card)</li><NewLine><li>In this round he earns 4 minus points</li><NewLine></ul><NewLine><p><strong>General Rules</strong></p><NewLine><ul><NewLine><li>All red cards give -1 point</li><NewLine><li>60 Cards in total (15 rounds has one game)</li><NewLine></ul><NewLine><p><strong>State Vector</strong> 180x1 binary vector consists of</p><NewLine><ul><NewLine><li>60x1 binary vector of played cards</li><NewLine><li>60x1 binary vector of cards in hand of the ai player</li><NewLine><li>60x1 binary vector of cards currently played</li><NewLine></ul><NewLine><p><strong>Code</strong></p><NewLine><pre><code class=""lang-auto""># tested with python 3.7.5<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine>import torch.nn.functional as F<NewLine>from torch.distributions import Categorical<NewLine>import numpy as np<NewLine><NewLine>import stdout<NewLine><NewLine>from gameClasses import card, deck, player, game<NewLine><NewLine>class PolicyGradientLoss(nn.Module):<NewLine>    def forward(self, log_action_probabilities, discounted_rewards):<NewLine>        # log_action_probabilities -&gt; (B, timesteps, 1)<NewLine>        # discounted_rewards -&gt; (B, timesteps, 1)<NewLine>        losses = -discounted_rewards * log_action_probabilities # -&gt; (B, timesteps, 1)<NewLine>        loss = losses.mean()<NewLine>        print(""Mean Loss  :"" ,round(loss.item(), 5), ""Shape losses:"", losses.shape)<NewLine>        return loss<NewLine><NewLine>### TODO GAME HERE<NewLine>class TestReinforce:<NewLine>    def __init__(self, parent=None):<NewLine>        #self.playingPolicy = PlayingPolicy()<NewLine>        self.witchesPolicy  = WitchesPolicy()<NewLine>        self.options = {}<NewLine>        self.options_file_path =  ""../data/reinforce_options.json""<NewLine>        with open(self.options_file_path) as json_file:<NewLine>            self.options = json.load(json_file)<NewLine>        self.my_game     = game(self.options)<NewLine><NewLine>    def notifyTrick(self, value):<NewLine>        # der schlimmste wert ist -17 (g10, g5, r1, r2)<NewLine>        # ausser wenn noch mal2 hinzukommt?! dann ist es wohl 21?!<NewLine>        #value +=21<NewLine>        normalizedReward = value / 21 # 21 zuvor sonst 26<NewLine>        if abs(normalizedReward)&gt;1:<NewLine>            stdout.enable()<NewLine>            print(normalizedReward)<NewLine>            print(eeee)<NewLine>        #self.playingPolicy.feedback(normalizedReward)<NewLine>        self.witchesPolicy.feedback(normalizedReward)<NewLine><NewLine>    def selectAction(self):<NewLine>        '''<NewLine>        the returned action is a hand card index no absolut index!<NewLine>        '''<NewLine>        current_player = self.my_game.active_player<NewLine>        if ""RANDOM"" in self.my_game.ai_player[current_player]:<NewLine>            action = self.my_game.getRandomOption_()<NewLine>        elif ""REINFO""  in self.my_game.ai_player[current_player]:<NewLine>            # get state of active player<NewLine>            active_player, state, options = self.my_game.getState()<NewLine>            #print(""Options"", options)<NewLine>            #print(""State: [Ontable, hand, played]\n"", state)<NewLine><NewLine>            #torch_tensor = self.playingPolicy(torch.tensor(state).float()   , torch.tensor(options))<NewLine>            torch_tensor = self.witchesPolicy(torch.tensor(state).float()   , torch.tensor(options))<NewLine>            # absolut action index:<NewLine>            action_idx   = int(torch_tensor[:, 0])<NewLine>            log_action_probability = torch_tensor[:, 1]<NewLine>            card   = self.my_game.players[current_player].getIndexOfCard(action_idx)<NewLine>            action = self.my_game.players[current_player].specificIndexHand(card)<NewLine>        return action<NewLine><NewLine>    def play(self):<NewLine>        total_points = [0, 0, 0, 0]<NewLine>        for j in range(0, 200):<NewLine>            i=0<NewLine>            nuGames = 100<NewLine>            while i&lt;nuGames:<NewLine>                action = self.selectAction()<NewLine>                current_player = self.my_game.active_player<NewLine>                card   = self.my_game.players[current_player].hand[action]<NewLine>                print(""[{}] {} {}\t{}\tCard {}\tHand Index {}"".format(self.my_game.current_round, current_player, self.my_game.names_player[current_player], self.my_game.ai_player[current_player], card, action))<NewLine>                rewards, round_finished = self.my_game.step_idx(action, auto_shift=False)<NewLine>                if round_finished:<NewLine>                    # player idx of Reinforce<NewLine>                    self.notifyTrick(rewards[1])<NewLine>                    print(""Update rewards: "", rewards, ""\n"")<NewLine>                    if len(self.my_game.players[current_player].hand) == 0: # game finished<NewLine>                        print(""update policy at end of one game!"")<NewLine>                        #self.playingPolicy.updatePolicy()<NewLine>                        self.witchesPolicy.updatePolicy()<NewLine>                        stdout.enable()<NewLine>                        if i == nuGames-1:<NewLine>                            print(""game finished with:::"", self.my_game.total_rewards, ""\n"")<NewLine>                        stdout.disable()<NewLine>                        self.my_game.reset_game()<NewLine>                        i+=1<NewLine>            if j&gt;100:<NewLine>                for i in range(len(total_points)):<NewLine>                    total_points[i] += self.my_game.total_rewards[i]<NewLine>            self.my_game.total_rewards = [0, 0, 0, 0]<NewLine>        stdout.enable()<NewLine>        print(total_points)<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    trainer = TestReinforce()<NewLine>    trainer.play()<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/CesMak,(Markus Lamprecht),CesMak,"March 14, 2020, 11:25am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li><NewLine><p>most of the time for most gym environments three Linear layers is enough, maybe 64 ~ 500 neurons per level and I would suggest you use a pyramid like structure. Conv2d is only necessary for visual inputs.</p><NewLine></li><NewLine><li><NewLine><p>Must use discount &lt; 1, otherwise there is no garantee on convergence, because the convergence of magic like RL algorithms relies on a simple math principle, you must have learned it in your freshman math analysis class: <a href=""https://en.wikipedia.org/wiki/Convergent_series"" rel=""nofollow noopener"">converging series</a> or alittle bit more advanced <a href=""https://en.wikipedia.org/wiki/Compactification_(mathematics)"" rel=""nofollow noopener"">compaction</a></p><NewLine></li><NewLine><li><NewLine><p>Because the naive REINFORCE algorithm is bad, try use DQN, RAINBOW, DDPG,TD3, A2C, A3C, PPO, TRPO, ACKTR or whatever you like. Follow the train result reference <a href=""https://github.com/openai/gym/wiki/Leaderboard"" rel=""nofollow noopener"">openai gym train reference</a>, normally you need to let the agent interact with the environment for 100K or even 1M steps, for extremely complex real life scenes, you need stacks of servers and massively distributed algorithms like IMPALA. There are <strong>many many many many</strong> methods to learn faster, but I would recommend you to start with PPO. But PPO is not a solution once and for all, it cannot solve some scenes like the hardcore bipedalwalker from openai gym.</p><NewLine></li><NewLine><li><NewLine><p>You will know that it is learning, by looking at its behavior changing from a total noise, to a fool, to an expert, and a fool again. Reward and loss might be good indicators, but be careful of your reward design, as your networks will exploit it and take lazy counter measures! Thats’ called reward shaping.</p><NewLine></li><NewLine><li><NewLine><p>Please use batches, because it can stablelize training, <strong>because</strong> pytorch will average the gradients computed from each sample from the batch, You see more, you can judge better, right? normally 100~256 samples per batch, but some studies says monsterous batchsize could be helpful like 20000+</p><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for this extensive very good answer! This is what I was looking for.</p><NewLine><p>I successfully got PPO for a multiplayer card game working. I also let the network learn correct moves and it has learned 16.95 of 17 after 8h of training (on single i7 cpu no gpu used). After 8h training is slowed down dramastically and is not proceeding much. Any ideas on what I can do next? Any suggestion for an algo I should try next?</p><NewLine><p>This is my repro:<br/><NewLine>Most recent version will be (soon) here:<br/><NewLine></p><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""16""/><NewLine><a href=""https://github.com/CesMak/mcts_cardgame/tree/master/modules"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars3.githubusercontent.com/u/20240718?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/CesMak/mcts_cardgame/tree/master/modules"" rel=""nofollow noopener"" target=""_blank"">CesMak/mcts_cardgame</a></h3><NewLine><p>Reinforcement Learning (PPO) applied to a multiplayer simple card game (Witches) - CesMak/mcts_cardgame</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have read your code, and from my personal perspective, you may adjust your hyper-parameters, network structures etc. Training results depends on your algorithm type, your test scenario, and how you define the rewards.</p><NewLine><p>PS: layer initialization method is also possible to change, since pytorch use xavier to initilize weights, may be consider about using orthorgonal methods.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/CesMak; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: June 4, 2020,  6:19pm; <NewLine> REPLY_DATE 2: June 7, 2020,  8:53am; <NewLine> REPLY_DATE 3: June 7, 2020, 10:28am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
79356,Am I training my model the right way?,2020-05-02T05:57:42.318Z,2,163,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The first code snippet is my implementation which I (think?) understood from pytorch. I wanted to implement the Deep Q learning algorithm without using frame like the one in the docs.<br/><NewLine>This is what I did</p><NewLine><pre><code class=""lang-auto"">non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)<NewLine><NewLine># Stacking converts to shape [BATCH_SIZE, 8]<NewLine> non_final_next_states = torch.stack([s for s in batch.next_state if s is not None])<NewLine><NewLine>state_batch = torch.stack(batch.state)<NewLine>reward_batch = torch.stack(batch.reward)<NewLine>      <NewLine>action_batch = torch.cat(batch.action)<NewLine>      <NewLine> state_action_values = self.DQN(state_batch).gather(1, action_batch)<NewLine><NewLine>next_state_values = torch.zeros(self.BUFFER_SIZE)<NewLine>next_state_values[non_final_mask] = self.DQN(non_final_next_states).max(1)[0]<NewLine>       <NewLine>expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch.reshape(-1)<NewLine><NewLine>loss = self.DQN.loss(state_action_values, expected_state_action_values.unsqueeze(1))<NewLine>        <NewLine>self.DQN.optimizer.zero_grad()<NewLine>loss.backward()<NewLine>self.DQN.optimizer.step()<NewLine></code></pre><NewLine><p>In the above code my average score does not go above -50. (Just a small note I stored the above transitions in the form of torch tensors)</p><NewLine><p>However with this implementation</p><NewLine><pre><code class=""lang-auto"">self.DQN.optimizer.zero_grad()<NewLine># Then proceeded to extract states, action and rewards<NewLine>state_batch = torch.tensor(batch.state, dtype=torch.float32)<NewLine>action_batch = torch.tensor(batch.action, dtype=torch.int64)<NewLine>reward_batch = torch.tensor(batch.reward, dtype=torch.float32)<NewLine><NewLine>non_terminal_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)<NewLine>non_terminal_state = torch.tensor([s for s in batch.next_state if s is not None], dtype=torch.float32)<NewLine><NewLine>state_action_values = self.DQN(state_batch)<NewLine>target_state_action_values = state_action_values.clone()<NewLine><NewLine>next_state_action_values = torch.zeros(self.BUFFER_SIZE)<NewLine>next_state_action_values[non_terminal_mask] = self.DQN(non_terminal_state).max(1)[0]<NewLine><NewLine>batch_index = torch.arange(0, self.BUFFER_SIZE, dtype=torch.int64)<NewLine>target_state_action_values[batch_index, action_batch] = reward_batch + next_state_action_values * self.GAMMA<NewLine><NewLine>loss = self.DQN.loss(target_state_action_values, state_action_values)<NewLine>loss.backward()<NewLine>self.DQN.optimizer.step()<NewLine></code></pre><NewLine><p>With the above code my score does not seem to go above 30. (Just a small note that above I stored the transitions as np arrays (directly from gym)).</p><NewLine><p>I am concerned that there is something wrong with my model creation and learning pipelining. Any help would be appreciated.<br/><NewLine>Below is my model class.</p><NewLine><pre><code class=""lang-auto"">class DQN(nn.Module):<NewLine><NewLine>    def __init__(self, input_size, hidden_size, output_size):<NewLine>        super(DQN, self).__init__()<NewLine>        self.input_layer = nn.Linear(input_size, hidden_size)<NewLine>        self.hidden1 = nn.Linear(hidden_size, hidden_size)<NewLine>        self.hidden2 = nn.Linear(hidden_size, hidden_size)<NewLine>        self.output_layer = nn.Linear(hidden_size, output_size)<NewLine>        self.loss = nn.MSELoss()<NewLine>        self.optimizer = optim.Adam(self.parameters())<NewLine><NewLine>    def forward(self, state):<NewLine>        x = F.relu(self.input_layer(state))<NewLine>        x = F.relu(self.hidden1(x))<NewLine>        x = F.relu(self.hidden2(x))<NewLine>        actions = self.output_layer(x)<NewLine><NewLine>        return actions<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Kevlyn_Kadamala,(Kevlyn Kadamala),Kevlyn_Kadamala,"May 2, 2020,  5:57am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please check your code <strong>very carefully</strong> as most of the time you may have wrongly processed your data, mixing state &amp; next_state, or missing something important.</p><NewLine><p>Besides there are many DQN variations, including a single q net, Fixed target DQN, double DQN, and dueling DQN, even RAINBOW. So, only you can fix the problem yourself as it’s really hard trying to understand how you store and processes state, next_state, action, reward, terminal in your <strong>undocumented code snippet</strong>. Their shape and data structure are not clear.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I apologize for the nature of my post. With hindsight I have realized that it wasn’t well documented and it doesn’t really portray what I am trying to achieve. Since the time of posting I have decided to learn more about reward modelling and agent construction and hence haven’t move forward in anyway.<br/><NewLine>Thank you for your inputs, I’ll certainly keep it in mind for my future posts <img alt="":smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smile.png?v=9"" title="":smile:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It is ok, don’t be shy about it, if you could provide some shape hints like this:</p><NewLine><pre><code class=""lang-auto"">keys = self.linear_keys(input)  # shape: (N, T, key_size)<NewLine>query = self.linear_query(input)  # shape: (N, T, key_size)<NewLine>values = self.linear_values(input)  # shape: (N, T, value_size)<NewLine></code></pre><NewLine><p>It will make your code much easier to read and check. <img alt="":smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smile.png?v=9"" title="":smile:""/>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Kevlyn_Kadamala; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: June 4, 2020,  5:16pm; <NewLine> REPLY_DATE 2: June 5, 2020,  4:45am; <NewLine> REPLY_DATE 3: June 5, 2020,  4:48am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
65378,Is model is multiprocessing really running in parallel?,2019-12-30T13:53:10.968Z,0,225,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello all,</p><NewLine><p>I am trying to implement multiprocessing environments for parallel rollouts.</p><NewLine><p>I call <code>model.share_memory()</code> and then use torch.multiprocessing to start several processes.</p><NewLine><p>In each process, I use<code> model(input)</code> for feed-forward computation.</p><NewLine><p>My question is:</p><NewLine><p>Are the feed-forward in all the processes running in parallel (like parallel GPU computation)? Or actually the model is only used by one process at one time and all the processes share the model via some locks?</p><NewLine></div>",https://discuss.pytorch.org/u/xuehy,(Guessit),xuehy,"December 30, 2019,  1:53pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well, if you are using GPU, then you are still doing computations on the same GPU, but on CPU, you are…running some kind of multiprocessing parallel.</p><NewLine><p>Please read multiprocessing tutorials first.</p><NewLine><p>This question belongs to <code>distributed</code> and not <code>RL</code></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: June 4, 2020,  6:23pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
74548,Actor Critic with Multivariate Normal - network weights fail to update,2020-03-27T16:52:47.783Z,0,159,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m quite new to pytorch (and ML/RL at all, for that matter). I’m trying to adapt an Actor Critic algorithm I’ve copied from one of Machine Learning with Phil’s YouTube videos to my own environment. I’ve gotten his version working with gym’s cartpole. My environment has a continuous action space, and I want to learn the mean of a 2D gaussian policy (later, I may also learn the covariance matrix, but for now I’m just leaving it fixed). The policy means should be bounded on the intervals (0, 2 pi) and (0, pi/2), so I put a sigmoid activation function on the output layer and multiply them by 2 pi and pi/2, respectively.  So this is how my agent chooses an action (I commented the original cartpole version for comparison):</p><NewLine><pre><code class=""lang-auto""> def choose_action(self, observation):<NewLine>        # probabilities = F.softmax(self.actor.forward(observation))<NewLine>        # action_probs = T.distributions.Categorical(probabilities)<NewLine>        policy_thetas = torch.sigmoid(self.actor.forward(observation))<NewLine>        mu, S = get_params_from_policy_thetas(policy_thetas.detach.numpy())<NewLine>        action_probs = mn.MultivariateNormal(torch.FloatTensor(mu),<NewLine>                                             torch.FloatTensor(S))<NewLine>        action = action_probs.sample()<NewLine>        self.log_probs = action_probs.log_prob(action)<NewLine>        <NewLine>        return action.item()<NewLine></code></pre><NewLine><p>And then my agent learns according to</p><NewLine><pre><code class=""lang-auto"">def learn(self, state, reward, new_state, done):<NewLine>        self.actor.optimizer.zero_grad()<NewLine>        self.critic.optimizer.zero_grad()<NewLine>        <NewLine>        critic_value = self.critic.forward(state)<NewLine>        critic_value_ = self.critic.forward(new_state)<NewLine>        <NewLine>        delta = ((reward+self.gamma*critic_value_*(1-int(done)))-critic_value)<NewLine>        <NewLine>        actor_loss = -self.log_probs * delta<NewLine>        critic_loss = delta**2<NewLine>        <NewLine>        (actor_loss + critic_loss).backward()<NewLine>        <NewLine>        self.actor.optimizer.step()<NewLine>        self.critic.optimizer.step()<NewLine></code></pre><NewLine><p>My code will run just fine for any number of episodes, but neither of the network weights ever seem to be updated. I thought this might be because actor_loss.grad and critic_loss.grad are both None, but this is also the case for the (properly working) cartpole. Then I thought maybe the DCG was broken somewhere, and most likely in the choose_action() function, because of some sub-optimal way I’m getting the parameters of the policy from the network output?</p><NewLine><p>I’ve spent a long while searching around the forums but I’m just lost in approaching debugging this. Any help would be much appreciated.</p><NewLine><p>Full code below (without my environment, it will work if you uncomment the lines in choose_action and comment out my multivariate normal stuff):</p><NewLine><pre><code class=""lang-auto"">import torch as T<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine>import numpy as np<NewLine>import gym<NewLine>from torch.distributions import multivariate_normal as mn<NewLine><NewLine>def get_params_from_policy_thetas(policy_thetas):<NewLine>    s11 = 10**-2<NewLine>    s22 = 10**-2<NewLine>    mu1 = (policy_thetas[0])*2*np.pi<NewLine>    mu2 = (policy_thetas[1])*np.pi/2<NewLine>    S = np.diag([s11,s22])<NewLine>    mu = np.asarray([mu1,mu2])<NewLine>    return(mu,S)<NewLine><NewLine>class GenericNetwork(nn.Module):<NewLine>    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions):<NewLine>        super(GenericNetwork, self).__init__()<NewLine>        self.input_dims = input_dims<NewLine>        self.fc1_dims = fc1_dims<NewLine>        self.fc2_dims = fc2_dims<NewLine>        self.n_actions = n_actions<NewLine>        self.lr = lr<NewLine>        <NewLine>        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)<NewLine>        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)<NewLine>        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)<NewLine>        <NewLine>        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)<NewLine>        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu:0')<NewLine>        self.to(self.device)<NewLine>        <NewLine>    def forward(self, observation):<NewLine>        state = T.Tensor(observation).to(self.device)<NewLine>        x = F.relu(self.fc1(state))<NewLine>        x = F.relu(self.fc2(x))<NewLine>        x = self.fc3(x)<NewLine>        <NewLine>        return x<NewLine><NewLine>class Agent(object):<NewLine>    def __init__(self, alpha, beta, input_dims, gamma = 0.99, <NewLine>                 l1_size = 256, l2_size=256, n_actions=2):<NewLine>        self.gamma = gamma<NewLine>        self.log_probs = None<NewLine>        self.actor = GenericNetwork(alpha,input_dims,l1_size,l2_size,n_actions)<NewLine>        self.critic = GenericNetwork(beta,input_dims,l1_size,l2_size,n_actions=1)<NewLine>        <NewLine>    def choose_action(self, observation):<NewLine>        # probabilities = F.softmax(self.actor.forward(observation))<NewLine>        # action_probs = T.distributions.Categorical(probabilities)<NewLine>        policy_thetas = torch.sigmoid(self.actor.forward(observation))<NewLine>        mu, S = get_params_from_policy_thetas(policy_thetas.detach.numpy())<NewLine>        action_probs = mn.MultivariateNormal(torch.FloatTensor(mu),<NewLine>                                             torch.FloatTensor(S))<NewLine>        action = action_probs.sample()<NewLine>        self.log_probs = action_probs.log_prob(action)<NewLine>        <NewLine>        return action.item()<NewLine>    <NewLine>    def learn(self, state, reward, new_state, done):<NewLine>        self.actor.optimizer.zero_grad()<NewLine>        self.critic.optimizer.zero_grad()<NewLine>        <NewLine>        critic_value = self.critic.forward(state)<NewLine>        critic_value_ = self.critic.forward(new_state)<NewLine>        <NewLine>        delta = ((reward+self.gamma*critic_value_*(1-int(done)))-critic_value)<NewLine>        <NewLine>        actor_loss = -self.log_probs * delta<NewLine>        critic_loss = delta**2<NewLine>        <NewLine>        (actor_loss + critic_loss).backward()<NewLine>        <NewLine>        self.actor.optimizer.step()<NewLine>        self.critic.optimizer.step()<NewLine><NewLine>if __name__ == '__main__':<NewLine>    agent = Agent(alpha = 0.00001, beta = 0.0005, input_dims = [4], <NewLine>                  gamma = 0.99, l1_size = 32, l2_size = 32, n_actions = 2)<NewLine>    env = gym.make('CartPole-v1')<NewLine>    score_history = []<NewLine>    n_episodes = 2500<NewLine>    for i in range(n_episodes):<NewLine>        done = False<NewLine>        score = 0<NewLine>        observation = env.reset()<NewLine>        while not done:<NewLine>            action = agent.choose_action(observation)<NewLine>            observation_,reward,done,info = env.step(action)<NewLine>            score+=reward<NewLine>            agent.learn(observation, reward, observation_, done)<NewLine>            observation = observation_<NewLine>        print('episode ', i, 'score %.3f' % score)<NewLine>        score_history.append(score)        <NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/stepmm,,stepmm,"March 27, 2020,  4:52pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>In order to use A-C series algorithms to train your actor network, you must make sure log probabilities are differentiable, right? Then the <code>action_probs.log_prob(action)</code> must be differentiable, therefore you <strong>cannot put</strong> detached <code>torch.FloatTensor(mu), torch.FloatTensor(S)</code> into the distribution since gradient will not flow through your <code>torch-&gt;numpy-&gt;torch</code> conversion. Instead, you must directly give the output of your actor to the distribution.</p><NewLine><p>An example of an agent used in the 4-dim continuous gym bipedal walker environment:</p><NewLine><pre><code class=""lang-auto"">class Actor(nn.Module):<NewLine>    # naive actor for env.walker<NewLine>    def __init__(self, state_dim, action_dim, max_action):<NewLine>        super(Actor, self).__init__()<NewLine><NewLine>        self.fc1 = nn.Linear(state_dim, 128)<NewLine>        self.fc2 = nn.Linear(128, 128)<NewLine>        self.fc_mu = nn.Linear(128, action_dim)<NewLine>        self.fc_sigma = nn.Linear(128, action_dim)<NewLine>        self.max_action = max_action<NewLine><NewLine>    def forward(self, state, action=None):<NewLine>        a = t.relu(self.fc1(state))<NewLine>        a = t.relu(self.fc2(a))<NewLine><NewLine>        mu = self.max_action * t.tanh(self.fc_mu(a))<NewLine><NewLine>        # we assume that each dimension of your action is not correlated<NewLine>        # therefore the covariance matrix is a positive definite diagonal matrix<NewLine><NewLine>        # static, preset standard error<NewLine>        # diag = t.full(mu.shape, 0.5, device=mu.device)<NewLine><NewLine>        # dynamic, trainable standard error<NewLine>        diag = softplus(self.fc_sigma(a))<NewLine>        cov = t.diag_embed(diag)<NewLine>        a_dist = MultivariateNormal(mu, cov)<NewLine>        action = action if action is not None else a_dist.sample()<NewLine>        action_log_prob = a_dist.log_prob(action)<NewLine>        entropy = a_dist.entropy()<NewLine>        return action.detach(), action_log_prob.unsqueeze(1), entropy.mean()<NewLine></code></pre><NewLine><p>I hope this answer is not too late though.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: June 4, 2020,  5:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
72661,Need Help Super Mario Bros PPO Implementation,2020-03-10T03:25:24.641Z,0,220,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I need help in implementing Super Mario Bros agentwith PPO. I tried to run this code using CPU in Google Colab. However, I received this error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-7-7383d21bcea4&gt; in &lt;module&gt;()<NewLine>    187 <NewLine>    188             #action = random_choice_prob_index(dist)<NewLine>--&gt; 189             action=dist.sample()#.view(-1, 1)<NewLine>    190 <NewLine>    191             next_state, reward, done, _ = env.step(action)<NewLine><NewLine>/usr/local/lib/python3.6/dist-packages/torch/distributions/categorical.py in sample(self, sample_shape)<NewLine>    105         probs = self.probs.expand(param_shape)<NewLine>    106         probs_2d = probs.reshape(-1, self._num_events)<NewLine>--&gt; 107         sample_2d = torch.multinomial(probs_2d, 1, True)<NewLine>    108         return sample_2d.reshape(sample_shape)<NewLine>    109 <NewLine><NewLine>RuntimeError: invalid multinomial distribution (encountering probability entry &lt; 0)<NewLine></code></pre><NewLine><p>I’m not sure what’s my error is. Can someone who has experience in Reinforcement Learning help me with my code?</p><NewLine><p>This is my agent code:</p><NewLine><pre><code class=""lang-auto"">  # Prepare environments<NewLine>    envs = [make_env() for i in range(NUM_ENVS)]<NewLine>    envs = SubprocVecEnv(envs)<NewLine>    env = gym_super_mario_bros.make('SuperMarioBros-v0')<NewLine>    env = JoypadSpace(env, SIMPLE_MOVEMENT)<NewLine>    state = envs.reset()<NewLine>    print(state.shape)<NewLine>    #Size 240<NewLine>    state_size=envs.observation_space.shape[0]<NewLine>    #Number of actions 7 (according to NES Controller)<NewLine>    action_size=envs.action_space.n<NewLine><NewLine><NewLine>    model = BaseActorCriticNetwork(state_size,action_size).to(device)<NewLine>    print(model)<NewLine>    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)<NewLine><NewLine>    frame_idx  = 0<NewLine>    train_epoch = 0<NewLine>    best_reward = None<NewLine><NewLine>    early_stop = False<NewLine>    while not early_stop:<NewLine><NewLine>        log_probs = []<NewLine>        values    = []<NewLine>        states    = []<NewLine>        actions   = []<NewLine>        rewards   = []<NewLine>        masks     = []<NewLine><NewLine>        for _ in range(PPO_STEPS):<NewLine>            state = torch.FloatTensor(state).to(device)<NewLine>            state = state.float()<NewLine>            #Change format to NCHW<NewLine>            state = state.permute(0,3,2,1)<NewLine>            #feed the state into the model<NewLine>            dist, value = model(state)<NewLine>            #Change the output to distribution<NewLine>            dist=Categorical(dist)<NewLine><NewLine>            #action = random_choice_prob_index(dist)<NewLine>            action=dist.sample()#.view(-1, 1)<NewLine>        <NewLine>            next_state, reward, done, _ = env.step(action)<NewLine>            log_prob = dist.log_prob(action)<NewLine>            <NewLine>            log_probs.append(log_prob)<NewLine></code></pre><NewLine><p>This is my model code(Implemented using ActorCritic):</p><NewLine><pre><code class=""lang-auto"">class BaseActorCriticNetwork(nn.Module):<NewLine>    def __init__(self, input_size, output_size, use_noisy_net=False):<NewLine>        super(BaseActorCriticNetwork, self).__init__()<NewLine>        if use_noisy_net:<NewLine>            linear = NoisyLinear<NewLine>        else:<NewLine>            linear = nn.Linear<NewLine><NewLine>        self.feature = nn.Sequential(<NewLine>            linear(input_size, 128),<NewLine>            nn.ReLU(),<NewLine>            linear(128, 128),<NewLine>            nn.ReLU()<NewLine>        )<NewLine>        self.actor = linear(128, output_size)<NewLine>        self.critic = linear(128, 1)<NewLine>        #stabilize the weights<NewLine>        for p in self.modules():<NewLine>            if isinstance(p, nn.Conv2d):<NewLine>                init.kaiming_uniform_(p.weight)<NewLine>                p.bias.data.zero_()<NewLine><NewLine>            if isinstance(p, nn.Linear):<NewLine>                init.kaiming_uniform_(p.weight, a=1.0)<NewLine>                p.bias.data.zero_()<NewLine><NewLine>    def forward(self, state):<NewLine>        x = self.feature(state)<NewLine>        policy = self.actor(x)<NewLine>        value = self.critic(x)<NewLine>        return policy, value<NewLine></code></pre><NewLine><p>Thanks,<br/><NewLine>Aqil</p><NewLine></div>",https://discuss.pytorch.org/u/fahimaqil,(Muhamad Fahim Aqil Bin Muhamad Sahlan),fahimaqil,"March 10, 2020,  3:25am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Where is your ppo training function? You haven’t even shown the surrograte loss clipping.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: June 4, 2020,  5:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
72857,Model update with &ldquo;share_memory&rdquo; need lock protection,2020-03-11T10:24:31.992Z,0,394,"<div class=""post"" itemprop=""articleBody""><NewLine><p>TL;DR.<br/><NewLine>Maybe a lock is needed for <code>optimizer.step()</code> while using multiprocessing.</p><NewLine><hr/><NewLine><p>The example provided in [MULTIPROCESSING BEST PRACTICES] (<a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/notes/multiprocessing.html</a>) is to update parameters without lock protection.</p><NewLine><pre><code class=""lang-auto"">import torch.multiprocessing as mp<NewLine>from model import MyModel<NewLine><NewLine>def train(model):<NewLine>    # Construct data_loader, optimizer, etc.<NewLine>    for data, labels in data_loader:<NewLine>        optimizer.zero_grad()<NewLine>        loss_fn(model(data), labels).backward()<NewLine>        optimizer.step()  # This will update the shared parameters<NewLine><NewLine>if __name__ == '__main__':<NewLine>    num_processes = 4<NewLine>    model = MyModel()<NewLine>    # NOTE: this is required for the ``fork`` method to work<NewLine>    model.share_memory()<NewLine>    processes = []<NewLine>    for rank in range(num_processes):<NewLine>        p = mp.Process(target=train, args=(model,))<NewLine>        p.start()<NewLine>        processes.append(p)<NewLine>    for p in processes:<NewLine>        p.join()<NewLine></code></pre><NewLine><p>And I checked some A3C code from github, and they do it in the similar way.</p><NewLine><p>To double check whether it’s safe to do that, I created a test case:</p><NewLine><ul><NewLine><li>The global network is init with 0</li><NewLine><li>one worker will add it 1 for 1000 times</li><NewLine><li>another worker will less it 1 for 1000 times</li><NewLine><li>if it is safe, the final network’s weight shall be 0</li><NewLine><li>repeat 100 times to check whether it is safe</li><NewLine></ul><NewLine><p>Test results:</p><NewLine><ul><NewLine><li>With lock protection, it’s safe</li><NewLine><li>Without lock protection (comment the lock, and unindent the <code>optimizer.step()</code>), from time to time, the weight can not come back to 0, which means: not safe</li><NewLine></ul><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.multiprocessing as mp<NewLine>import torch.optim as optim<NewLine>from torch.autograd import Variable<NewLine>import os<NewLine><NewLine><NewLine>class Net(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Net, self).__init__()<NewLine>        self.fc1 = nn.Linear(1, 1)<NewLine>        self.fc1.weight.data.fill_(0.0)<NewLine>        self.fc1.bias.data.fill_(0.0)<NewLine><NewLine>    def forward(self):<NewLine>        return 0<NewLine><NewLine>def worker1(net, grad, lock):<NewLine>    optimizer = optim.SGD(net.parameters(), lr=1)<NewLine>    for _ in range(1000):<NewLine>        optimizer.zero_grad()<NewLine>        for param in net.parameters():<NewLine>            param._grad = Variable(torch.ones_like(param))<NewLine>        with lock:<NewLine>            optimizer.step()<NewLine>        # print(f'in process {os.getpid()}, the grad of weight is: {net.fc1.weight.grad.item()}, weight is {net.fc1.weight.item()}')<NewLine><NewLine>def worker2(net, grad, lock):<NewLine>    optimizer = optim.SGD(net.parameters(), lr=1)<NewLine>    for _ in range(1000):<NewLine>        optimizer.zero_grad()<NewLine>        for param in net.parameters():<NewLine>            param._grad = -Variable(torch.ones_like(param))<NewLine>        with lock:<NewLine>            optimizer.step()<NewLine>        # print(f'in process {os.getpid()}, the grad of weight is: {net.fc1.weight.grad.item()}, weight is {net.fc1.weight.item()}')<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    n = Net()<NewLine>    n.share_memory()<NewLine><NewLine>    lock = mp.Lock()<NewLine><NewLine>    for idx in range(100):<NewLine>        # print(idx)<NewLine>        # print(f'in main {os.getpid()}, weight is {n.fc1.weight.item()}')<NewLine><NewLine>        p1 = mp.Process(target = worker1, args=(n, 1.0, lock))<NewLine>        p2 = mp.Process(target = worker2, args=(n, 1.0, lock))<NewLine><NewLine>        p1.start()<NewLine>        p2.start()<NewLine><NewLine>        p1.join()<NewLine>        p2.join()<NewLine><NewLine>        # print(f'in main {os.getpid()}, weight is {n.fc1.weight.item()}')<NewLine><NewLine>        if n.fc1.weight.item() != 0:<NewLine>            print(f'{idx} in main {os.getpid()}, weight is {n.fc1.weight.item()}')<NewLine>            break<NewLine>    <NewLine>    print('finished')<NewLine></code></pre><NewLine><p>My pytorch version is 1.4.0+cpu</p><NewLine><p>However, as I find it works without lock in the official “Hogwild” example and other examples, maybe the safe issue is not that important…</p><NewLine></div>",https://discuss.pytorch.org/u/S_Q,(S Q),S_Q,"March 11, 2020, 10:24am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>thread safety is not important, the hogwild essay basically spent pages trying to explain that the update is sparse and you can update in <strong>whatever</strong> way(sequence). It’s magic, so forget about the safety. <img alt="":rofl:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/rofl.png?v=9"" title="":rofl:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: June 4, 2020,  5:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
74534,I have doubt in deep q learning,2020-03-27T13:36:46.579Z,0,133,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a doubt DQN say for nth state i am getting</p><NewLine><pre><code class=""lang-auto"">qnthvalues = [1,2,3]<NewLine></code></pre><NewLine><p>So here max q value is selected which 2 pos or the 3 rd value and i am doing the action 3 and getting qn+1thvalue and now should i apply bellman eq for that action or the 3rd value of qn+1th value and leave other value the same for target value</p><NewLine><pre><code class=""lang-auto"">qn+1value = [2,3,4]<NewLine>targetq_values = [2,3,bellmaneq(4)]<NewLine>               Or<NewLine>targetq_values = bellmaneq(qn+1values)<NewLine>#for all q values of that state<NewLine></code></pre><NewLine><p>(So for all q values we will be applying or will be applying for the action q value alone.</p><NewLine></div>",https://discuss.pytorch.org/u/gokul_adethya,(gokul adethya),gokul_adethya,"March 27, 2020,  1:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>target_value = reward (from env) + discount * (1 - terminal) * next_value</p><NewLine><p>you can acquire next_value by using the q net itself(vanilla DQN), a target network(fixed target), or select an action using the online network and use this action to find the q value(double DQN)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: June 4, 2020,  5:22pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
76661,DQN: Error for loss.backward() at terminal state with fixed target,2020-04-14T01:27:41.743Z,0,93,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am currently trying DQN for Tic Tac Toe. I am stuck with the following error:</p><NewLine><blockquote><NewLine><p>element 0 of tensors does not require grad and does not have a grad_fn</p><NewLine></blockquote><NewLine><p>I think the error results whenever the game is played to the end and reaches a terminal state, as the target is then manually set to some value and now the gradient cannot be computed during the backward propagation. How can I fix this?</p><NewLine><p>Here is my code for the updating of the NN:</p><NewLine><pre><code class=""lang-auto"">def update_NN(state, next_state, action, player, discount, lr, loss_all):<NewLine>    pred = torch.tensor(net(torch.tensor(state).float().view(-1, 9)).squeeze().detach().numpy()[action])<NewLine>    reward = 0<NewLine>    winner, game_status = check_result(next_state)<NewLine>    if game_status == 'Done' and winner == player:<NewLine>        reward = 100<NewLine>    if game_status == 'Done' and winner != player:<NewLine>        reward = -1<NewLine>    if game_status == 'Draw':<NewLine>        reward = 10<NewLine><NewLine>    if next_state.count(0) == 0:<NewLine>        target = torch.tensor(reward).float()<NewLine>    else:<NewLine>        target = torch.tensor(reward).float() + discount * torch.max(net(torch.tensor(next_state).float()))<NewLine>    # Evaluate loss<NewLine>    loss = loss_fn(pred, target)<NewLine>    print(loss)<NewLine>    loss_all.append(loss)<NewLine>    optimizer.zero_grad()<NewLine>    # Backward pass<NewLine>    loss.backward()<NewLine>    # Update<NewLine>    optimizer.step()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/spadel,,spadel,"April 14, 2020,  1:27am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""76661"" data-username=""spadel""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/8797f3/40.png"" width=""20""/> spadel:</div><NewLine><blockquote><NewLine><p><code>net(torch.tensor(state).float().view(-1, 9)).squeeze().detach()</code></p><NewLine></blockquote><NewLine></aside><NewLine><p>You have detached the output of your network.<br/><NewLine>Besides, DQN is usually not implemented in this way, the network would accept a state s and output q values for all actions.</p><NewLine><p>Maybe read this <a href=""https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noopener"">tutorial</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: June 4, 2020,  5:05pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
80418,Digit recognition manually write all methods,2020-05-09T08:40:56.937Z,0,78,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all, I’m trying to replicate the same neural network as <a href=""https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627"" rel=""nofollow noopener"">here</a> by manually writing all the required functions alongside with their derivatives for backpropagation (Linear, ReLU and CrossEntropyLoss). The forward propagation seems to be ok, as if I use the weights and biases of linear1, linear2 and linear3 and pass them through my forward method, I obtain the same loss for a batch of images as in the network created with nn.Sequential, which uses PyTorch defined layers. Anyway, when I call the backward method written by me, it looks like the matrices don’t match the required sizes to be able to multiply them and obtain the weights / biases gradients, and I tried multiple ways to transpose some of those matrices to match the sizes, but in the end I cannot match all of them. Could anyone advice me how should I modify the backward for weights / biases gradients to be correctly computed? <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/></p><NewLine><p>Sorry if it isn’t in the right category!</p><NewLine><pre><code class=""lang-auto"">import numpy as np<NewLine>import torch<NewLine>import torchvision<NewLine>import matplotlib.pyplot as plt<NewLine>from torchvision import datasets, transforms<NewLine>from torch import nn, optim<NewLine><NewLine>transform = transforms.Compose([transforms.ToTensor(),<NewLine>                                transforms.Normalize((.5,), (.5,)),<NewLine>                                ])<NewLine><NewLine>trainset = datasets.MNIST('PATH_TO_STORE_TRAINSET', download = True,<NewLine>                          train = True, transform = transform)<NewLine><NewLine>valset = datasets.MNIST('PATH_TO_STORE_TESTSET', download = True,<NewLine>                        train = False, transform = transform)<NewLine><NewLine>trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, <NewLine>                                          shuffle = True)<NewLine><NewLine>valloader = torch.utils.data.DataLoader(valset, batch_size = 64, <NewLine>                                          shuffle = True)<NewLine><NewLine>images, labels = next(iter(trainloader))<NewLine>images = images.view(images.shape[0], -1)<NewLine><NewLine>linear1 = nn.Linear(input_size, hidden_sizes[0])<NewLine>linear2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])<NewLine>linear3 = nn.Linear(hidden_sizes[1], output_size)<NewLine>relu = nn.ReLU()<NewLine>cel = nn.CrossEntropyLoss()<NewLine><NewLine>l1 = linear1(images)<NewLine>r1 = relu(l1)<NewLine>l2 = linear2(r1)<NewLine>r2 = relu(l2)<NewLine>l3 = linear3(r2)<NewLine>l = cel(l3, labels)<NewLine>l.backward()<NewLine><NewLine>class NN(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(NN, self).__init__()<NewLine>        # parameters<NewLine>        # self.w1 = torch.randn(128, 784)<NewLine>        # self.w2 = torch.randn(64, 128)<NewLine>        # self.w3 = torch.randn(10, 64)<NewLine>        # self.b1 = torch.rand(128)<NewLine>        # self.b2 = torch.randn(64)<NewLine>        # self.b3 = torch.randn(10)<NewLine>        self.w1 = linear1.weight<NewLine>        self.w2 = linear2.weight<NewLine>        self.w3 = linear3.weight<NewLine>        self.b1 = linear1.bias<NewLine>        self.b2 = linear2.bias<NewLine>        self.b3 = linear3.bias<NewLine>        self.lr = 0.003<NewLine>        self.momentum = 0.9<NewLine>    <NewLine>    def softMax(self, input):<NewLine>        e_x = torch.exp(input)<NewLine>        result = torch.zeros(e_x.shape)<NewLine>        for dim in range(e_x.shape[0]):<NewLine>            result[dim] = e_x[dim] / torch.sum(e_x[dim])<NewLine>            <NewLine>        return result<NewLine>    <NewLine>    def crossEntropy(self, input, target):<NewLine>        m = target.shape[0]<NewLine>        p = self.softMax(input)<NewLine>        log_likelihood = -torch.log(p[range(m), target])<NewLine>        loss = torch.sum(log_likelihood) / m<NewLine>        return loss<NewLine>    <NewLine>    def crossEntropyDeriv(self, input, target):   <NewLine>        m = target.shape[0]<NewLine>        grad = self.softMax(input)<NewLine>        grad[range(m), target] -= 1<NewLine>        grad /= m<NewLine>        return grad<NewLine>        <NewLine>    def relu(self, input):<NewLine>        result = torch.max(input, torch.zeros(input.shape))<NewLine>        return result<NewLine><NewLine>    def reluDerivative(self, input):<NewLine>        result = torch.ones(input.shape)<NewLine>        result[input &lt; 0] -= 1<NewLine>        return result<NewLine>    <NewLine>    def linear(self, input, w, b):<NewLine>        return input.matmul(w.t()) + b<NewLine><NewLine>    def linearDerivativewrtW(self, input, w, b):<NewLine>        return input<NewLine>    <NewLine>    def linearDerivativewrtB(self, input, w, b):<NewLine>        return torch.ones(b.shape)<NewLine>    <NewLine>    def linearDerivative(self, input, w, b):<NewLine>        return w<NewLine>        <NewLine>    def forward(self, images):<NewLine>        self.images = images<NewLine>        self.linear1 = self.linear(self.images, self.w1, self.b1)<NewLine>        self.relu1 = self.relu(self.linear1)<NewLine>        self.linear2 = self.linear(self.relu1, self.w2, self.b2)<NewLine>        self.relu2 = self.relu(self.linear2)<NewLine>        self.linear3 = self.linear(self.relu2, self.w3, self.b3)<NewLine>        return self.linear3<NewLine>    <NewLine>    def backward(self, output, target):<NewLine>        deltaL_Linear3 = self.crossEntropyDeriv(output, target)<NewLine>       <NewLine>        deltaL_w3 = deltaL_Linear3.matmul(self.linearDerivativewrtW(self.relu2, self.w3, self.b3))<NewLine>        deltaL_b3 = deltaL_Linear3.matmul(self.linearDerivativewrtB(self.relu2, self.w3, self.b3))<NewLine>        deltaL_relu2 = deltaL_Linear3.matmul(self.linearDerivative(self.relu2, self.w3, self.b3))<NewLine>        <NewLine>        deltaL_Linear2 = deltaL_relu2.matmul(self.reluDerivative(self.linear2))<NewLine>        <NewLine>        deltaL_w2 = deltaL_Linear2.matmul(self.linearDerivativewrtW(self.relu1, self.w2, self.b2))<NewLine>        deltaL_b2 = deltaL_Linear2.matmul(self.linearDerivativewrtB(self.relu1, self.w2, self.b2))<NewLine>        deltaL_relu1 = deltaL_Linear2.matmul(self.linearDerivative(self.relu1, self.w2, self.b2))<NewLine>        <NewLine>        deltaL_Linear1 = deltaL_relu1.matmul(self.reluDerivative(self.linear1))<NewLine>       <NewLine>        deltaL_w1 = deltaL_Linear1.matmul(self.linearDerivativewrtW(self.images, self.w1, self.b1))<NewLine>        deltaL_b1 = deltaL_Linear1.matmul(self.linearDerivativewrtB(self.images, self.w1, self.b1))<NewLine>        <NewLine>        self.w1 = self.w1 - self.lr * deltaL_w1<NewLine>        self.b1 = self.b1 - self.lr * deltaL_b1<NewLine>        <NewLine>        self.w2 = self.w2 - self.lr * deltaL_w2<NewLine>        self.b2 = self.b2 - self.lr * deltaL_b2<NewLine>        <NewLine>        self.w3 = self.w3 - self.lr * deltaL_w3<NewLine>        self.b3 = self.b3 - self.lr * deltaL_b3<NewLine>        <NewLine>Network = NN()<NewLine>output = Network.forward(images)<NewLine>Network.backward(output, target)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">runfile('C:/Users/calser/Downloads/DigitRecognition/Training.py', wdir='C:/Users/calser/Downloads/DigitRecognition')<NewLine>Traceback (most recent call last):<NewLine><NewLine>  File ""C:\Users\calser\Downloads\DigitRecognition\Training.py"", line 152, in &lt;module&gt;<NewLine>    Network.backward(output, target)<NewLine><NewLine>  File ""C:\Users\calser\Downloads\DigitRecognition\Training.py"", line 126, in backward<NewLine>    deltaL_w3 = deltaL_Linear3.matmul(self.linearDerivativewrtW(self.relu2, self.w3, self.b3))<NewLine><NewLine>RuntimeError: size mismatch, m1: [64 x 10], m2: [64 x 64] at C:\w\1\s\tmp_conda_3.7_100118\conda\conda-bld\pytorch_1579082551706\work\aten\src\TH/generic/THTensorMath.cpp:136<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Calin_Serban,(Calin Serban),Calin_Serban,"May 9, 2020,  8:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please do not implement those functions by yourself, they are already implemented in pytorch, search the documentation for their respective functions.</p><NewLine><p>In order to match up the sizes, I would recommend you debug your program and take caution of <code>tensor.shape</code> and the <a href=""https://pytorch.org/docs/stable/notes/broadcasting.html"" rel=""nofollow noopener"">broadcasting rule</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: June 4, 2020,  4:58pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
81721,How to avoid gradient vanish in pathwise derivative policy gradient,2020-05-18T02:05:13.103Z,0,94,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to train a pathwise derivative policy. But the output of my NN to be nan after about 5000 trainings. I guess because of the gradients’ vanish. How to solve my problem?</p><NewLine><p>My NN is following:</p><NewLine><pre><code class=""lang-auto"">class ACTOR_QVALUE(nn.Module):<NewLine>    def __init__(self, input_size, hidden_size, action_size):<NewLine>        super(ACTOR_QVALUE, self).__init__()<NewLine>        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=2,<NewLine>                            bias=True, batch_first=True, dropout=0, bidirectional=False)<NewLine>        self.hidden_mu1 = nn.Linear(hidden_size + action_size, 64)<NewLine>        self.hidden_mu2 = nn.Linear(64, 32)<NewLine>        self.hidden_mu3 = nn.Linear(32, 16)<NewLine>        self.hidden_mu4 = nn.Linear(16, action_size)<NewLine><NewLine>        self.hidden_layer1 = nn.Linear(64 + action_size, 32)<NewLine>        self.hidden_layer2 = nn.Linear(32, 8)<NewLine>        self.hidden_layer3 = nn.Linear(8, 1)<NewLine><NewLine>    def forward(self, env_state, action_state, action=None, type='actor'):<NewLine>        lstm_out, (h_n, c_n) = self.lstm(env_state)<NewLine>        cat_layer = torch.cat((lstm_out[:, -1, :], action_state), 1)<NewLine>        mu = F.relu(self.hidden_mu1(cat_layer))<NewLine>        if type == 'actor':<NewLine>            mu = F.leaky_relu(self.hidden_mu2(mu))<NewLine>            mu = F.leaky_relu(self.hidden_mu3(mu))<NewLine>            mu = torch.softmax(self.hidden_mu4(mu), dim=1)<NewLine>            return mu<NewLine>        else:<NewLine>            cat_layer = torch.cat((mu, action), 1)<NewLine>            q = F.leaky_relu(self.hidden_layer1(cat_layer))<NewLine>            q = F.leaky_relu(self.hidden_layer2(q))<NewLine>            q = self.hidden_layer3(q)<NewLine>            return q<NewLine></code></pre><NewLine><p>where env_state(shape: 250*120) and action_state(shape: 1*8) is the state infomations,  action means action whith shape 1*8. The NN output action if type=‘actor’, else output the q_value.</p><NewLine></div>",https://discuss.pytorch.org/u/cmcai0104,(C.M. Cai),cmcai0104,"May 18, 2020,  2:06am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Nan is not caused by gradient vanishing, more oftenly it is caused by updating a part of your model continuously using extremely large gradients, like something divided by zero. then Nan will ocurr in all parameters.</p><NewLine><p>Try clipping your gradient using <code>torch.nn.utils.clip_grad_norm_</code>or debug your model gradients using <code>module.register_backward_hook</code></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: June 4, 2020,  4:53pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
83466,Gym atari installation,2020-05-30T13:25:43.059Z,1,216,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I want to run the <a href=""https://github.com/ikostrikov/pytorch-a3c"" rel=""nofollow noopener"">pytorch-a3c example</a> with env = PongDeterministic-v4. I’ve had gym, gym[atari], atari-py installed by pip3. And the following codes:</p><NewLine><pre><code class=""lang-auto"">[who@localhost pytorch-a3c]$ python3<NewLine>Python 3.7.7 (default, Mar 13 2020, 21:39:43) <NewLine>[GCC 9.2.1 20190827 (Red Hat 9.2.1-1)] on linux<NewLine>Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.<NewLine>&gt;&gt;&gt; from gym import envs<NewLine>&gt;&gt;&gt; env_names = [spec.id for spec in envs.registry.all()]<NewLine>&gt;&gt;&gt; for name in sorted(env_names):<NewLine>...     print(name)<NewLine>... <NewLine><NewLine></code></pre><NewLine><p>has had the env listed:</p><NewLine><pre><code class=""lang-auto"">Pong-ram-v0<NewLine>Pong-ram-v4<NewLine>Pong-ramDeterministic-v0<NewLine>Pong-ramDeterministic-v4<NewLine>Pong-ramNoFrameskip-v0<NewLine>Pong-ramNoFrameskip-v4<NewLine>Pong-v0<NewLine>Pong-v4<NewLine>PongDeterministic-v0<NewLine>PongDeterministic-v4<NewLine>PongNoFrameskip-v0<NewLine>PongNoFrameskip-v4<NewLine><NewLine></code></pre><NewLine><p>I have run the project as its Usage recommended as:</p><NewLine><pre><code class=""lang-auto"">python3 main.py --env-name ""PongDeterministic-v4"" --num-processes 16<NewLine></code></pre><NewLine><p>and encountered following exception:</p><NewLine><pre><code class=""lang-auto"">Process Process-1:<NewLine>Traceback (most recent call last):<NewLine>  File ""/usr/lib64/python3.7/multiprocessing/process.py"", line 297, in _bootstrap<NewLine>    self.run()<NewLine>  File ""/usr/lib64/python3.7/multiprocessing/process.py"", line 99, in run<NewLine>    self._target(*self._args, **self._kwargs)<NewLine>  File ""/home/zf/workspaces/workspace_python/pytorch-a3c/test.py"", line 21, in test<NewLine>    state = env.reset()<NewLine>  File ""/home/zf/.local/lib/python3.7/site-packages/gym/core.py"", line 257, in reset<NewLine>    observation = self.env.reset(**kwargs)<NewLine>  File ""/home/zf/.local/lib/python3.7/site-packages/gym/core.py"", line 258, in reset<NewLine>    return self.observation(observation)<NewLine>  File ""/home/zf/.local/lib/python3.7/site-packages/gym/core.py"", line 265, in observation<NewLine>    raise NotImplementedError<NewLine>NotImplementedError<NewLine><NewLine></code></pre><NewLine><p>And the command for env = Tennis-ram-v0 also failed with the same exception.</p><NewLine><p>I am not experienced in python environment. Would you please hint the possible solutions?</p><NewLine></div>",https://discuss.pytorch.org/u/fulltopic,,fulltopic,"May 30, 2020,  1:25pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This error seems to be <code>gym</code> related and while I’m not experienced with this library, I guess the method tries to display something?<br/><NewLine>If that’s the case, could you check if some visualization/rendering libs and other dependencies are installed, such as <code>pyglet</code>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Seems like a gym version/custom wrapper problem defined in the library.</p><NewLine><p>It’s calling the abstract class method that will raise this error <a href=""https://github.com/openai/gym/blob/3bd5ef71c2ca3766a26c3dacf87a33a9390ce1e6/gym/core.py#L255"" rel=""nofollow noopener"">here</a>, so the error is expected.</p><NewLine><p>The question is why is the method called? Can you show the code where the environment gets initialized, and where it calls <code>reset()</code>, i.e. snippet from your <code>test.py</code>.</p><NewLine><p>U can also try this example of creating and calling the gym env methods that works:</p><NewLine><pre><code class=""lang-python"">import gym<NewLine><NewLine>env = gym.make('PongDeterministic-v4')<NewLine>env.reset()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Seemed the issue has been solved by <a href=""https://github.com/ikostrikov/pytorch-a3c/issues/66"" rel=""nofollow noopener"">pytorch-a3c issue#66</a>. The project still failed to work while the problem is beyond this question.</p><NewLine><p>Sorry for not have been searching for solution efficiently and thank you very much for you help</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kengz; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/fulltopic; <NewLine> ,"REPLY_DATE 1: May 31, 2020,  7:42am; <NewLine> REPLY_DATE 2: June 1, 2020,  5:08pm; <NewLine> REPLY_DATE 3: June 9, 2020,  2:12pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
57701,Why we use Categorical,2019-10-08T14:52:30.096Z,1,191,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Please explain why we use Categorical? I can’t figure it out. What do we get at the categorical output?</p><NewLine><pre><code class=""lang-auto"">def predict(state):<NewLine>    # Select an action (0 or 1) by running policy model<NewLine>    # and choosing based on the probabilities in state<NewLine>    state = torch.from_numpy(state).type(torch.FloatTensor)<NewLine>    action_probs = policy(state)<NewLine>    distribution = Categorical(action_probs)<NewLine>    action = distribution.sample2()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/slavavs,(slavavs),slavavs,"October 8, 2019,  2:52pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">distribution = Categorical(action_probs)<NewLine>    action = distribution.sample2()<NewLine></code></pre><NewLine><p>As I have understod it Categorical returns the propabillety of every possible action. Then one possible action gets sampled from this distribution (You can’t just do argmax(action_probs) because in some sence then there would be no exploration also the intension is to increase the propabillety of good actions and decrease the propabillety of bad actions that is why whe have to sample from the distripution instead of chosing the highest value)</p><NewLine><p>I am fairly new to reinforcement learning and python so pleas correct me if I am wrong.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Why we use Categorical (or any other distribution) is a direct result of the policy formulation, where action is defined to be sampled from a distribution, <code>a ~ π(s)</code>, with probability of action = <code>π(a|s)</code>. It’s an implementation of the algorithm. You may change the distribution (e.g. to Gaussian for continuous actions), but you may not change it to an argmax because that is fundamentally a different algorithm/sampling mechanism (argmax is not a distribution).</p><NewLine><p>Policy-based methods in RL like ActorCritic are different from Q-learning that uses a greedy-selection of action based on it’s Q-estimation (hence the argmax in Q-learning).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Tim2; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kengz; <NewLine> ,"REPLY_DATE 1: April 14, 2020,  2:35pm; <NewLine> REPLY_DATE 2: June 1, 2020,  5:15pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
83549,Model not training properly on GPU,2020-05-31T08:36:49.098Z,1,172,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi guys,</p><NewLine><p>so I have been trying to get into reinforcement learning and have taken to a Udemy course, which has worked perfectly fine, until it came to an implementation of a deep Q learning algorithm.<br/><NewLine>The issue I am having is that my model works fine on the CPU (only issue it being significantly slower), with an expected learning curve. The agent seems to learn how to play pong after 220 games and comes to an average score of 16 (over the last 100 games) after 500 games.</p><NewLine><p>If I try to run it on my GPU (GTX980M) though to improve performance, the learning curve seems to fall off a cliff at some point, seemingly reaching a maximum of -18 points on average and then dropping back to -21. The learning curve for the runs on the GPU looks as follows:<br/><NewLine><img alt=""PongNoFrameskip-v4_DQN_pyTorch_50000Mem_300Games_cu_results"" data-base62-sha1=""9dQc04SRPDPwaDIqNKgnfr0sh7z"" height=""480"" src=""https://discuss.pytorch.org/uploads/default/original/3X/4/0/40a41fe9a6acc587c7af1eba2a5f2d877879ad19.png"" width=""640""/></p><NewLine><p>I have since reversed to simply cloning the repository from the course ( <a href=""https://github.com/philtabor/Deep-Q-Learning-Paper-To-Code/tree/master/DQN"" rel=""nofollow noopener"">https://github.com/philtabor/Deep-Q-Learning-Paper-To-Code/tree/master/DQN</a> ), but the issue persists and I have not yet found out why. I have reinstalled PyTorch and Cuda recently without success (Python 3.6.8, Torch 1.5.0, Cuda 10.1)</p><NewLine><p>Any ideas as to why I am getting such an odd behaviour?</p><NewLine><p>Any input would be greatly appreciated.</p><NewLine><p>Thanks in advance,<br/><NewLine>Alex</p><NewLine></div>",https://discuss.pytorch.org/u/Ausizio,(Alex),Ausizio,"May 31, 2020,  8:36am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>How reproducible is this behavior? I.e. for 10 runs with different seeds, how many times does the CPU model converge and the GPU model diverge?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>On the same system I have been able to reproduce the behavior 9 out of 10 times. I have not been able to reproduce it on a different system and hardware though.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Were you using the same setup on the different systems, i.e. same OS, PyTorch version, CUDA, cudnn, etc. or what were the differences?<br/><NewLine>Did you see any hardware issues in the first system before and could you run a stress test on the GPU?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>The second system was running the same version of Python, but other than that I am not sure.<br/><NewLine>I also had the same issue on the first system with different versions of PyTorch and Cuda earlier, but I do not remember which exact versions I used, as I have since reinstalled and updated them.<br/><NewLine>I did not find any hardware issues while running the stress test.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Ausizio; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Ausizio; <NewLine> ,"REPLY_DATE 1: May 31, 2020, 10:51am; <NewLine> REPLY_DATE 2: May 31, 2020, 11:04am; <NewLine> REPLY_DATE 3: June 1, 2020,  7:24am; <NewLine> REPLY_DATE 4: June 1, 2020, 10:37am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
79941,Discussion on combining sub-state into one state for training rl model,2020-05-06T06:09:55.640Z,0,67,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>Would like to ask the community, if anyone has suggestion on representing a single state from multiple sub-states for rl training.</p><NewLine><p>For example, I have a substate that is represented by a 2d tensor (width by height). But then i have another substate represented as a vector of <em>n</em> inputs.</p><NewLine><p>With these two sub-states, I would like to combine them to represent 1 input state to my neural architecture.</p><NewLine><p>I’m not sure if it would be valid to perhaps convert the 2d tensor substate into a 1d tensor and append the other vector input to it?</p><NewLine><p>Appreciate any suggestions or advice, tku!</p><NewLine></div>",https://discuss.pytorch.org/u/nlpdl,,nlpdl,"May 6, 2020,  3:47pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Take an image and a vector as example, you cannot combine both tensors directly; instead you may treat them as different modes, and have a multi-head network talking them as inputs:</p><NewLine><ul><NewLine><li>im_head: have a conv module with the image as input</li><NewLine><li>vec_head: have a plain mlp module with the vector as input</li><NewLine></ul><NewLine><p>Now you can explore a few ways of combining them:</p><NewLine><ol><NewLine><li>flatten the im_head output, then concat it with the vec_head output, or</li><NewLine><li>flatten the im_head output, add a linear so that it comes out the same shape as vec_head, then do element-wise addition</li><NewLine><li>more generally, taking from 2, you can do FiLM: <a href=""https://distill.pub/2018/feature-wise-transformations/"" rel=""nofollow noopener"">https://distill.pub/2018/feature-wise-transformations/</a><NewLine></li><NewLine></ol><NewLine><p>Once combined, you can further pass through more MLPs then to whatever output architecture you need.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/kengz; <NewLine> ,"REPLY_DATE 1: May 30, 2020, 10:48pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
82780,Tensor creation slow on cpu (from replay buffer),2020-05-25T07:10:07.918Z,0,103,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I’m implementing Deep Q-learning and my code is slow due to the creation of Tensors from the replay buffer. Here’s how it goes:</p><NewLine><p>I maintain a  <code>deque</code>  with a size of 10’000 and sample a batch from it everytime I want to do a backward pass. The following line is really slow:</p><NewLine><p><code>curr_graphs = torch.Tensor(list(state(*zip(*xp_samples.curr_state)).graph))</code></p><NewLine><p>That I decomposed to see what is really taking time</p><NewLine><p><code>zipped = zip(*xp_samples.curr_state)</code><br/><NewLine><code>new_s = state(*zipped)</code><br/><NewLine><code>listt = list(new_s.graph)</code><br/><NewLine><code>curr_graphs = torch.Tensor(listt)</code></p><NewLine><p>To notice that the last line, i.e the tensor creation, is what is taking all the computation. What is happening is that xp_samples and curr_state are named tuples. In this snippet I unpack and then zip and then unpack again to group the data by name from curr_state.</p><NewLine><p>In my opinion it has to assemble data from memory using a lot of pointers to create the Tensor and thus is losing time moving things around. What would be the fastest way to create a tensor from sampled data from a buffer that I maintain? Should I allocate the size of the content of the deque so that it is continous in memory? I feel like it won’t speed up the process.</p><NewLine><p>Here are the details of the <code>deque</code> if that’s relevant:</p><NewLine><p><code>class ReplayBuffer:</code><br/><NewLine><code>    def __init__(self, maxlen):</code><br/><NewLine><code>        self.buffer = deque(maxlen=maxlen)</code><br/><NewLine><code> </code><br/><NewLine><code>    def add(self, new_xp):</code><br/><NewLine><code>        self.buffer.append(new_xp)</code><br/><NewLine><code> </code><br/><NewLine><code>    def sample(self, batch_size):</code><br/><NewLine><code>        xps = random.choices(self.buffer, k=batch_size)</code><br/><NewLine><code>        return xp(*zip(*xps))</code></p><NewLine></div>",https://discuss.pytorch.org/u/floepfl,(Floepfl),floepfl,"May 25, 2020,  7:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sample indices directly, use numpy arrays, or store as tensors directly. Please see this similar question and its answer: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/how-to-make-the-replay-buffer-more-efficient/80986/2"">How to make the replay buffer more efficient？</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/kengz; <NewLine> ,"REPLY_DATE 1: May 30, 2020, 10:38pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
62305,TUTORIAL DQN example: NoSuchDisplayException in Colab,2019-11-27T13:13:19.757Z,0,394,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I try to run “REINFORCEMENT LEARNING (DQN) TUTORIAL” in Colab and get a NoSuchDisplayException: Cannot connect to “None”.</p><NewLine><blockquote><NewLine><h2>env.reset()<br/><NewLine>plt.figure()<br/><NewLine>plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),<br/><NewLine>interpolation=‘none’)<br/><NewLine>plt.title(‘Example extracted screen’)<br/><NewLine>plt.show()</h2><NewLine><p>NoSuchDisplayException                    Traceback (most recent call last)<br/><NewLine> in ()<br/><NewLine>39 env.reset()<br/><NewLine>40 plt.figure()<br/><NewLine>—&gt; 41 plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),<br/><NewLine>42            interpolation=‘none’)<br/><NewLine>43 plt.title(‘Example extracted screen’)</p><NewLine><p>10 frames<br/><NewLine>/usr/local/lib/python3.6/dist-packages/pyglet/canvas/xlib.py in <strong>init</strong>(self, name, x_screen)<br/><NewLine>84         self._display = xlib.XOpenDisplay(name)<br/><NewLine>85         if not self._display:<br/><NewLine>—&gt; 86             raise NoSuchDisplayException(‘Cannot connect to “%s”’ % name)<br/><NewLine>87<br/><NewLine>88         screen_count = xlib.XScreenCount(self._display)</p><NewLine><p>NoSuchDisplayException: Cannot connect to “None”</p><NewLine></blockquote><NewLine><p>I thought that the tutorials is adapted to the Colab, because there is even a link to launch them. What is an easy way to avoid this error? Or is it necessary to deal with pyvirtualdisplay?</p><NewLine></div>",https://discuss.pytorch.org/u/droogg,,droogg,"November 27, 2019,  1:13pm",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Still facing the issue. Did you find any workaround?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Add a line above your notebook to plot inline:<br/><NewLine><code>%matplotlib inline</code></p><NewLine><p>source: <a href=""https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.00-Introduction-To-Matplotlib.ipynb#scrollTo=0YPlWWlcucAx"" rel=""nofollow noopener"">https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.00-Introduction-To-Matplotlib.ipynb#scrollTo=0YPlWWlcucAx</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/abyaadrafid; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kengz; <NewLine> ,"REPLY_DATE 1: May 28, 2020,  3:46pm; <NewLine> REPLY_DATE 2: May 30, 2020, 10:36pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
80986,How to make the replay buffer more efficient？,2020-05-13T07:48:02.515Z,0,190,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have implement a DQN model via pytorch, but I find that it runs slow during training, and the main reason is that sampling from replay buffer takes up almost all the time.</p><NewLine><p>Here are my implementation of replay buffer.</p><NewLine><pre><code class=""lang-auto"">class DQNBuffer:<NewLine>    def __init__(self, maxlen=100000, device=None):<NewLine>        self.mem = deque(maxlen=maxlen)<NewLine>        self.maxlen = maxlen<NewLine>        self.device = device<NewLine><NewLine>    def store(self, s, a, r, s_, a_, d):<NewLine>        self.mem.append([s, a, r, s_, a_, d])<NewLine><NewLine>    def sample(self, batch_size):<NewLine>        bat = random.sample(self.mem, batch_size)<NewLine>        batch = list(zip(*bat))<NewLine>        data = []<NewLine>        for i in range(len(batch)):<NewLine>            data.append(T.as_tensor(batch[i], dtype=T.float32, device=self.device))<NewLine>        return data<NewLine><NewLine>    def __len__(self):<NewLine>        return len(self.mem)<NewLine></code></pre><NewLine><p>I compute the consumption of time when training one batch of samples. Here is the code:</p><NewLine><pre><code class=""lang-auto"">import timeit<NewLine>def train_batch(agent: DQNAgent, batch_size, gamma):<NewLine>    agent.gstep += 1<NewLine>    agent.optimizer.zero_grad()<NewLine>    print(""=================================="")<NewLine><NewLine>    A = timeit.default_timer()<NewLine>    s, a, r, s_, a_, d = agent.buf.sample(batch_size)<NewLine>    print(timeit.default_timer() - A)<NewLine><NewLine>    B = timeit.default_timer()<NewLine>    qhat = agent.tnet(s_, a_).reshape(-1, ).detach()<NewLine>    target = r + (1.0-d)*gamma*qhat<NewLine>    loss = F.mse_loss(agent.qnet(s, a).reshape(-1, ), target)<NewLine>    loss.backward()<NewLine>    agent.optimizer.step()<NewLine>    print(timeit.default_timer() - B)<NewLine><NewLine>    print(""=================================="")<NewLine>    return float(loss)<NewLine></code></pre><NewLine><p>some of the output ( 4 batches ):</p><NewLine><pre><code class=""lang-auto"">==================================<NewLine>0.11982669999999995<NewLine>0.011224900000000204<NewLine>==================================<NewLine>==================================<NewLine>0.12090409999999974<NewLine>0.011249399999999632<NewLine>==================================<NewLine>==================================<NewLine>0.12087029999999999<NewLine>0.01114000000000015<NewLine>==================================<NewLine>==================================<NewLine>0.11867099999999997<NewLine>0.011347199999999447<NewLine>==================================<NewLine></code></pre><NewLine><p><code>agent.buf.sample(batch_size)</code> takes over 90% of the time – it’s too slow.<br/><NewLine>How to optimize the <code>sample</code> operation?</p><NewLine></div>",https://discuss.pytorch.org/u/legend94rz,,legend94rz,"May 13, 2020,  7:48am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>use index to pick samples as opposed to directly sampling on the list:</li><NewLine><li>save on tensor creation, do it by batch directly, and if your data is numpy use torch.from_numpy:</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">    def sample(self, batch_size):<NewLine>        batch_idxs = np.random.randint(len(self), size=batch_size)<NewLine>        batches = list(zip(*self.mem[batch_idxs]))<NewLine>        return [torch.from_numpy(batch) for batch in batches]<NewLine></code></pre><NewLine><ol start=""3""><NewLine><li>If you want to save on the Tensor creation cost, store your data directly as tensors, then sample directly without <code>torch.from_numpy(batch)</code>.</li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/kengz; <NewLine> ,"REPLY_DATE 1: May 30, 2020, 10:22pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
46751,R2D2 in PyTorch,2019-05-31T15:31:33.117Z,0,359,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, does anyone have an R2D2 example for PyTorch? Preferrably pong or some basic env? Would love to see a small efficient implementation</p><NewLine></div>",https://discuss.pytorch.org/u/Muppet,(Muppet),Muppet,"May 31, 2019,  3:31pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you check <a href=""https://github.com/astooke/rlpyt"" rel=""nofollow noopener"">rlpyt</a>? You can check <a href=""https://github.com/jinbeizame007/pytorch-r2d2-DPG"" rel=""nofollow noopener"">this</a> too. If you know another clean implementation, I’m interested to know too.<br/><NewLine>Thanks.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Isaac_Kargar; <NewLine> ,"REPLY_DATE 1: May 24, 2020,  5:05am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
81418,BatchNorm in RNN Step,2020-05-15T17:43:39.492Z,1,99,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I had pre-trained a RNN with BatchNormalization with collected data. In training, the input was in sizes of {batchSize, seqLen, others} so that the num_features of BatchNorm layer is seqLen (a fixed number). Then I want to transfer the RNN into A3C and train it online.  In this case, the input is {1, 1, others} in real-time action generation, and the num_features of BatchNorm should be 1. But the BatchNorm(num_features = seqLen) could still be used in offline network updating.</p><NewLine><p>How to deal with the case? Could I just ignore the BatchNorm layer in real-time forward? Do I have to train the net in step-by-step way?</p><NewLine><p>Thank you very much!</p><NewLine></div>",https://discuss.pytorch.org/u/fulltopic,,fulltopic,"May 15, 2020,  5:43pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you are dealing with a different number of features for the batchnorm layer, you could add a condition into your model, which would then pick the right batchnorm layer for the current input.</p><NewLine><p>I’m not familiar with your use case, but maybe it would also make sense to slice and copy the batchnorm layer parameters and buffers from the <code>seqLen</code> use case to the single feature use case?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>. “slice and copy batchnorm layer parameters and buffers” works.</p><NewLine><p>Is there any official/widely-used way to do “slice and copy batchnorm layer” instead of manipulating “weights/bias” tensor values directly?</p><NewLine><p>Thank you very much!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could use something like:</p><NewLine><pre><code class=""lang-python"">small_bn = nn.BatchNorm2d(1)<NewLine>with torch.no_grad():<NewLine>    small_bn.weight = nn.Parameter(large_bn.weight[0:1]) # or take the mean, slice at another pos?<NewLine>    # same for bias, running_mean and running_var<NewLine></code></pre><NewLine><p>I don’t know, if that would make sense at all for your use case, but you might consider it. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/fulltopic; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: May 23, 2020, 11:53pm; <NewLine> REPLY_DATE 2: May 21, 2020,  3:06pm; <NewLine> REPLY_DATE 3: May 22, 2020,  6:49am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
80524,Will weights initialized code be executed again during training after being initialized?,2020-05-10T03:50:12.989Z,0,57,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In the Actor Critic framework, suppose I have an actor like this(fan_in_initializer() is just a self-defined method),<br/><NewLine>When I create an actor instance, line 10, 13 and 16 will be executed once, then if I train the model, will these lines of code be executed again?</p><NewLine><pre><code class=""lang-auto"">class Actor(nn.Module):<NewLine><NewLine>    def __init__(self, s_dim, a_dim):<NewLine>        super(Actor, self).__init__()<NewLine><NewLine>        self.s_dim = s_dim<NewLine>        self.a_dim = a_dim<NewLine><NewLine>        self.fc1 = nn.Linear(s_dim, 512)<NewLine>        self.fc1.weight.data = fan_in_initializer(self.fc1.weight.data.size()) # line 10<NewLine><NewLine>        self.fc2 = nn.Linear(512, 256)<NewLine>        self.fc2.weight.data = fan_in_initializer(self.fc2.weight.data.size()) # line 13<NewLine><NewLine>        self.fc3 = nn.Linear(256, a_dim)<NewLine>        self.fc3.weight.data.uniform_(-EPS,EPS) # line 16<NewLine><NewLine>    def forward(self, s):<NewLine>        x = F.relu(self.fc1(s))<NewLine>        x = F.relu(self.fc2(x))<NewLine>        a = F.tanh(self.fc3(x))<NewLine>        return a<NewLine></code></pre><NewLine><p>Thanks in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/Curimeowcat,(Curimeow Cat),Curimeowcat,"May 10, 2020,  3:53am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, the <code>__init__</code> method will only be called, when you create an instance of the model via:</p><NewLine><pre><code class=""lang-python"">model = Actor(1, 1)<NewLine></code></pre><NewLine><p>During training the <code>forward</code> method will be called.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: May 10, 2020,  7:47am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
18186,DDPG gradient with respect to action,2018-05-16T07:04:01.215Z,7,2238,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am trying to implement DDPG with pytorch, I am sure most of my implementation is right, but the policies don’t converge so I am not too certain about the gradient part. The paper shows that the gradient of Q(s,a) is with respect to action a. I am defining the loss to be Q(s,a), but how do I know that the gradient will be taken derivative with respect to a? Since the loss = Q(s,a), I am setting loss.requires_grad = True, and loss.volatile = False.</p><NewLine></div>",https://discuss.pytorch.org/u/DrerD,,DrerD,"May 16, 2018, 10:51am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Q should never be the loss function. DDPG is a case of Deep Actor-Critic algorithm, so you have two gradients: one for the actor (the parameters leading to the action (<code>mu</code>)) and one for the critic (that estimates the value of a state-action (<code>Q</code>) – this is our case – , or sometimes the value of a state (<code>V</code>) ).</p><NewLine><p>In DDPG, the critic loss is the temporal difference (as in classique deep Q learning):<br/><NewLine><code>critic_loss = (R - gamma*Q(t+1) - Q(t))**2</code><br/><NewLine>Then the critic’s gradient is obtained by a simple backward of this loss.</p><NewLine><p>For the actor gradient, things are more complex: it’s an estimation of the policy gradient, given by:<br/><NewLine><code>actor_grad = Q_grad * mu_grad</code><br/><NewLine>Where <code>mu</code> is the output of the network, estimating the optimal mean of the action’s Gaussian distribution.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>sorry, I should say I am trying to find the actor gradient. I am referencing <a href=""https://github.com/ghliu/pytorch-ddpg/blob/master/ddpg.py"" rel=""nofollow noopener"">this implementation</a> and he does<br/><NewLine>policy_loss = -self.critic([to_tensor(state_batch), self.actor(to_tensor(state_batch))].mean()<br/><NewLine>which is just simply defining the loss = -mean(critic(state,actor(state)))<br/><NewLine>My belief is that the autograd will find the gradient with respect to action, which is actor_grad = Q_grad * mu_grad given the chain rule property.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok I see, it makes sens to directly derivates <code>Q(S, pi(S))</code> wrt <code>pi</code> 's parameters.</p><NewLine><p>In the paper, the gradient is wrt a, because they decompose the derivative (hence the <code>mu_grad</code> in my equation above). If you directly take <code>Q</code> as a loss, you must derivate it wrt policy’s parameters.</p><NewLine><p>So, just doing</p><NewLine><pre><code class=""lang-auto"">pi_loss = -Q(state, pi(state))<NewLine>pi_loss.backward()<NewLine>pi_optimizer.step()<NewLine></code></pre><NewLine><p>should be ok.</p><NewLine><p>In the paper, look at equation 6: it’s a mater of computing the first line (Q directly with derivative wrt pi) or the second line (decomposition, with a derivative wrt a)</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>so back to my question, how is the gradient wrt action specified?  Q(s,a) has both state and action as variable, and the gradient can be taken wrt to state if it’s not specified. This is what’s confusing me.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>As I said, you don’t want to derivate wrt action in your case, but wrt the parameters of your policy. The states must be detached from the graph, and the code in my post above should do what you want.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh I see, so is there anyway to actually see that the gradient is actually with respect to policy parameters?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>When you are using an optimizer on some parameters, only these parameter will be affected by the gradient step. So, if you did something like</p><NewLine><pre><code class=""lang-python"">policy_optim = nn.Optimizer(policy.parameters(), lr)<NewLine></code></pre><NewLine><p>above in your code, and then</p><NewLine><pre><code class=""lang-python"">policy_loss = -Q(s, policy(s))<NewLine>policy_loss.backward()<NewLine>policy_optim.step()<NewLine></code></pre><NewLine><p>… then, only the parameter of the policy will be affected, with the gradient of your loss wrt these parameters. It’s that simple!</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/alexis-jacq"">@alexis-jacq</a> I am coming a bit late in the discussion and actually I would like to know why do you need to detach the state from the graph when you take directly Q as a loss?<br/><NewLine>DDPG is a bit complicated to understand regarding the gradient update of the actor.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>You don’t need to detach the state. My last point was : taking Q as a loss, only the parameters given to the optimizer will be optimized.</p><NewLine><p>The gradient update in DDPG is not that complicated. In fact, it’s even simpler than stochastic policy gradients:</p><NewLine><p>Since Q approximates your returns, you want to maximize E_a~mu(s) [Q(a,s)] = E_s [Q(mu(s), s)]<br/><NewLine>So, you take the derivative of this thing : E_s [mu’(s) * Q’(mu(s), s)] and do a gradient descent on the parameters of mu.</p><NewLine><p>With an automatic derivation tool like pytorch, you don’t even care about going that far, you just take -Q(mu(s), s) as a loss to maximize (wrt mu’s parameters).</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/alexis-jacq"">@alexis-jacq</a> Thank you for your answer, it is clear now.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/alexis-jacq"">@alexis-jacq</a> I was also confused with this actor gradient problem and not any more with your replies. Thanks a lot!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/DrerD; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/DrerD; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/DrerD; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/fabrice; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/fabrice; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/Rance; <NewLine> ,"REPLY_DATE 1: May 16, 2018, 10:50am; <NewLine> REPLY_DATE 2: May 16, 2018,  5:18pm; <NewLine> REPLY_DATE 3: May 16, 2018,  7:43pm; <NewLine> REPLY_DATE 4: May 17, 2018,  1:20am; <NewLine> REPLY_DATE 5: May 17, 2018, 10:20am; <NewLine> REPLY_DATE 6: May 17, 2018, 10:07pm; <NewLine> REPLY_DATE 7: May 18, 2018,  8:03pm; <NewLine> REPLY_DATE 8: December 9, 2018,  9:18pm; <NewLine> REPLY_DATE 9: December 14, 2018, 11:17pm; <NewLine> REPLY_DATE 10: December 17, 2018,  5:23pm; <NewLine> REPLY_DATE 11: May 8, 2020,  7:33am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> 
80048,ValueError: max() arg is an empty sequence,2020-05-06T17:58:53.139Z,1,165,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,</p><NewLine><p>I was trying to do the tutorial on reinforcement learning but this error shows up when trying to plot the initial configuration of the PoleCart. After a bit research I think that the problem is with the backend of matplotlib. The tutorial says to set up matplotlib in this way:</p><NewLine><h1>set up matplotlib</h1><NewLine><p>is_ipython = ‘inline’ in matplotlib.get_backend()<br/><NewLine>if is_ipython:<br/><NewLine>from IPython import display</p><NewLine><p>but the string that comes out of matplotlib.get_backend() for me is:</p><NewLine><p>module://backend_interagg</p><NewLine><p>Am I doing something wrong?</p><NewLine></div>",https://discuss.pytorch.org/u/Michele_Guerra,(Michele Guerra),Michele_Guerra,"May 6, 2020,  5:58pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you using IPython (Jupyter notebooks or Google Colab)? If yes, one solution is setting <code>%matplotlib inline</code> which is a magic function. If you can share a link to the tutorial, I can check that out myself.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve been using PyCharm, in a normal setting, so I’m guessing no IPython, also because I’ve tried to write the magic function and it gives me a syntax error.</p><NewLine><p><a class=""onebox"" href=""https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html</a></p><NewLine><p>This is the link to the tutorial <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you tried this?</p><NewLine><p><a href=""https://stackoverflow.com/questions/49048520/how-to-prevent-pycharm-from-overriding-default-backend-as-set-in-matplotlib"" rel=""nofollow noopener"">How to prevent PyCharm from overriding default backend as set in matplotlib?</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh I forgot to mention that I was connecting remotely to another computer and that was giving me problems! Now everything has worked out, since it’s just a tutorial I will run it locally. But now I will also try to run it remotely, even though it seems trivial: <a href=""https://stackoverflow.com/questions/41892039/how-to-enable-x11-forwarding-in-pycharm-ssh-session"" rel=""nofollow noopener"">https://stackoverflow.com/questions/41892039/how-to-enable-x11-forwarding-in-pycharm-ssh-session</a></p><NewLine><p>Thanks a lot for your help!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/russellizadi; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Michele_Guerra; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/russellizadi; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Michele_Guerra; <NewLine> ,"REPLY_DATE 1: May 6, 2020,  6:07pm; <NewLine> REPLY_DATE 2: May 6, 2020,  7:15pm; <NewLine> REPLY_DATE 3: May 6, 2020,  7:36pm; <NewLine> REPLY_DATE 4: May 7, 2020,  8:34pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
80006,Question on training LSTM to play Snake,2020-05-06T13:21:10.672Z,1,96,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I follow the DQN tutorial and trained a CNN to play a game, now I want to switch to LSTM but got a problem.</p><NewLine><p>I found the tutorial for LSTM, and it recommends using (time sequence, Batch size, features). In my code, my input to LSTM is (4,32,100), where 4 means 4 consecutive frames, 32 is the batch size, 100 is a vector representing the current state.</p><NewLine><p>Then I add a nn.Linear layer after LSTM, and the input size of linear layer is 4<em>32</em>lstm_output_size, and here comes the problem. In trainig, the batch size is 32-&gt;(4,32,100), but in testing, the batch size is1 -&gt; (4,1,100), which will cause an error.</p><NewLine><p>I tried to train the LSTM with batch size 1, but it will take significantly longer time, is there any way that can let me train the LSTM with batch=32 and do inference with batch=1?</p><NewLine></div>",https://discuss.pytorch.org/u/waw_waw,(waw waw),waw_waw,"May 6, 2020,  1:21pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The number seems to be formatted a bit wrong, but I understand that the input features to the linear layer are defined as <code>seq_len * batch_size * lstm_output_size</code>?<br/><NewLine>You shouldn’t use the batch size as the number of input features, as this will limit your use case to only this particular size, and assumes that the samples in the batch are somehow related to each other.</p><NewLine><p>Instead, <code>permute</code> the LSTM output so that the batch size is at dim0 and reset the <code>in_features</code> to e.g. <code>seq_len * lstm_output_size</code>.<br/><NewLine>This will allow you to use arbitrary batch sizes.</p><NewLine><p>Alternatively, you could also use <code>batch_first=True</code> when you are creating an instance of <code>LSTM</code>, which will accept and return the batch size in dim0.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: May 7, 2020,  5:10am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
80154,Custom loss is not updating the weights,2020-05-07T10:40:26.184Z,2,62,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I do have list of history of states.<br/><NewLine>Then by numpy i calculate <strong>discounted_rewards</strong><br/><NewLine>Then i multiply output of model with discounted_rewards using <strong>torch.mm</strong><br/><NewLine>then</p><NewLine><pre><code class=""lang-auto"">  print(self.global_model.state_dict())<NewLine>    print(""total_loss"",total_loss)<NewLine>    total_loss.backward()<NewLine>    self.opt.step()<NewLine>    print(self.global_model.state_dict())<NewLine></code></pre><NewLine><p>it’s output is</p><NewLine><blockquote><NewLine><p>(‘dense1.weight’, tensor([[ 0.3997, -0.1907,  0.1120,  0.3016],<br/><NewLine>[ 0.1156,  0.0646,  0.1802,  0.3558],<br/><NewLine>[ 0.0321,  0.2537,  0.0879,  0.2441],<br/><NewLine>[-0.2952, -0.0886, -0.3235,  0.3006]])), (‘dense1.bias’, tensor([ 0.1927,  0.3048, -0.3551, -0.0302])), ('dense2.weig</p><NewLine><p>total_loss.backward() tensor(2.5806, dtype=torch.float64, grad_fn=)</p><NewLine><p>(‘dense1.weight’, tensor([[ 0.3997, -0.1907,  0.1120,  0.3016],<br/><NewLine>[ 0.1156,  0.0646,  0.1802,  0.3558],<br/><NewLine>[ 0.0321,  0.2537,  0.0879,  0.2441],<br/><NewLine>[-0.2952, -0.0886, -0.3235,  0.3006]])), (‘dense1.bias’, tensor([ 0.192</p><NewLine></blockquote><NewLine><p>and</p><NewLine><pre><code class=""lang-auto"">self.opt = torch.optim.SGD(self.global_model.parameters(),lr = 0.01)<NewLine></code></pre><NewLine><p>So it is not updating the weights. what i am missing ?</p><NewLine><p>Model is</p><NewLine><pre><code class=""lang-auto""><NewLine>        self.dense1 = torch.nn.Linear(4,4) <NewLine>        self.dense2 = torch.nn.Linear(4,4) <NewLine>        self.dense3 = torch.nn.Linear(4,4) <NewLine>        self.dense4 = torch.nn.Linear(4,4) <NewLine>        self.probs = torch.nn.Linear(4,2) <NewLine>        self.values = torch.nn.Linear(4, 1)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Rajan_Lagah,(Rajan Lagah),Rajan_Lagah,"May 7, 2020, 10:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you share the loss function code and the part of the program where you have called the criterion?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey i have notice that<br/><NewLine>If i do</p><NewLine><pre><code class=""lang-auto"">            print(self.local_model.state_dict())<NewLine>            print(""total_loss.backward()"",total_loss)<NewLine>            total_loss.backward()<NewLine>            opt_2 = torch.optim.SGD(self.local_model.parameters(),lr = 0.01)<NewLine>            opt_2.step()<NewLine>            self.opt.step()<NewLine>            print(self.local_model.state_dict())<NewLine></code></pre><NewLine><p>Then it show weight change.<br/><NewLine>But i want to update weights using loss of global_model.<br/><NewLine>I am using local_model to predict the result<br/><NewLine>then i want to apply grad to global model<br/><NewLine>A3C stuff<br/><NewLine>Do you know how to ?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The predictions and weights will be same all the time if you don’t backpropogate local_model and the loss function will always return a constant value because since the gradients of local_model are not being updated,it will give same prediction every time and labels are also constant so if you are able to interpret this then you will notice that loss will come out constant all the time so while you backpropogate through global_model, the gradients will be same.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/cskarthik7; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Rajan_Lagah; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/cskarthik7; <NewLine> ,"REPLY_DATE 1: May 7, 2020, 11:55am; <NewLine> REPLY_DATE 2: May 7, 2020, 11:58am; <NewLine> REPLY_DATE 3: May 7, 2020, 12:58pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
77711,Replacing Q lookup table with neural network,2020-04-21T14:10:29.435Z,0,156,"<div class=""post"" itemprop=""articleBody""><NewLine><p>From reading tutorial <a href=""https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e"" rel=""nofollow noopener"">https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e</a> the following code successfully trains an RL algorithm to make decisions in the ‘Taxi-v3’ OpenAi gym environment.</p><NewLine><pre><code class=""lang-auto"">import gym<NewLine>import numpy as np<NewLine>import time<NewLine><NewLine>""""""<NewLine>SARSA on policy learning python implementation.<NewLine>This is a python implementation of the SARSA algorithm in the Sutton and Barto's book on<NewLine>RL. It's called SARSA because - (state, action, reward, state, action). The only difference<NewLine>between SARSA and Qlearning is that SARSA takes the next action based on the current policy<NewLine>while qlearning takes the action with maximum utility of next state.<NewLine>Using the simplest gym environment for brevity: https://gym.openai.com/envs/FrozenLake-v0/<NewLine>""""""<NewLine><NewLine>def init_q(s, a, type=""ones""):<NewLine>    """"""<NewLine>    @param s the number of states<NewLine>    @param a the number of actions<NewLine>    @param type random, ones or zeros for the initialization<NewLine>    """"""<NewLine>    if type == ""ones"":<NewLine>        return np.ones((s, a))<NewLine>    elif type == ""random"":<NewLine>        return np.random.random((s, a))<NewLine>    elif type == ""zeros"":<NewLine>        return np.zeros((s, a))<NewLine><NewLine><NewLine>def epsilon_greedy(Q, epsilon, n_actions, s, train=False):<NewLine>    """"""<NewLine>    @param Q Q values state x action -&gt; value<NewLine>    @param epsilon for exploration<NewLine>    @param s number of states<NewLine>    @param train if true then no random actions selected<NewLine>    """"""<NewLine>    if train or np.random.rand() &lt; epsilon:<NewLine>        action = np.argmax(Q[s, :])<NewLine>    else:<NewLine>        action = np.random.randint(0, n_actions)<NewLine>    return action<NewLine><NewLine>def sarsa(alpha, gamma, epsilon, episodes, max_steps, n_tests, render = True, test=False):<NewLine>    """"""<NewLine>    @param alpha learning rate<NewLine>    @param gamma decay factor<NewLine>    @param epsilon for exploration<NewLine>    @param max_steps for max step in each episode<NewLine>    @param n_tests number of test episodes<NewLine>    """"""<NewLine>    env = gym.make('Taxi-v3')<NewLine>    n_states, n_actions = env.observation_space.n, env.action_space.n<NewLine>    print('n_states' ,n_states)<NewLine>    Q = init_q(n_states, n_actions, type=""ones"")<NewLine>    print('Q shape:' , Q.shape)<NewLine>    <NewLine>    timestep_reward = []<NewLine>    for episode in range(episodes):<NewLine>        print(f""Episode: {episode}"")<NewLine>        total_reward = 0<NewLine>        s = env.reset()<NewLine>        print('s:' , s)<NewLine>        a = epsilon_greedy(Q, epsilon, n_actions, s)<NewLine>        t = 0<NewLine>        done = False<NewLine>        while t &lt; max_steps:<NewLine>            if render:<NewLine>                env.render()<NewLine>            t += 1<NewLine>            s_, reward, done, info = env.step(a)<NewLine>            print('state is' , s)<NewLine>            total_reward += reward<NewLine>            a_ = epsilon_greedy(Q, epsilon, n_actions, s_)<NewLine>            if done:<NewLine>                Q[s, a] += alpha * ( reward  - Q[s, a] )<NewLine>            else:<NewLine>                Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_] ) - Q[s, a] )<NewLine>            s, a = s_, a_<NewLine>            if done:<NewLine>                if render:<NewLine>                    print(f""This episode took {t} timesteps and reward {total_reward}"")<NewLine>                timestep_reward.append(total_reward)<NewLine>                break<NewLine>#             print('Updated Q values:' , Q)<NewLine>    if render:<NewLine>        print(f""Here are the Q values:\n{Q}\nTesting now:"")<NewLine>    if test:<NewLine>        test_agent(Q, env, n_tests, n_actions)<NewLine>    return timestep_reward<NewLine><NewLine>def test_agent(Q, env, n_tests, n_actions, delay=0.1):<NewLine>    for test in range(n_tests):<NewLine>        print(f""Test #{test}"")<NewLine>        s = env.reset()<NewLine>        done = False<NewLine>        epsilon = 0<NewLine>        total_reward = 0<NewLine>        while True:<NewLine>            time.sleep(delay)<NewLine>            env.render()<NewLine>            a = epsilon_greedy(Q, epsilon, n_actions, s, train=True)<NewLine>            print(f""Chose action {a} for state {s}"")<NewLine>            s, reward, done, info = env.step(a)<NewLine>            total_reward += reward<NewLine>            if done:  <NewLine>                print(f""Episode reward: {total_reward}"")<NewLine>                time.sleep(1)<NewLine>                break<NewLine><NewLine><NewLine>if __name__ ==""__main__"":<NewLine>    alpha = 0.4<NewLine>    gamma = 0.999<NewLine>    epsilon = 0.9<NewLine>    episodes = 200<NewLine>    max_steps = 20<NewLine>    n_tests = 20<NewLine>    timestep_reward = sarsa(alpha, gamma, epsilon, episodes, max_steps, n_tests)<NewLine>    print(timestep_reward)<NewLine></code></pre><NewLine><p>I’m attempting to modify this code to use a Deep Q Learning instead of Q Learning.<br/><NewLine>From reading <a href=""https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf"" rel=""nofollow noopener"">https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf</a><br/><NewLine>it mentions using a separate target network which I think is translates to swapping the Q lookup table to<br/><NewLine>a neural network function approximator . To try to keep things simple as possible I’m not planning to<br/><NewLine>utilize replay memory for this initial solution.</p><NewLine><p>From paper ‘Combining Q-Learning with Artiﬁcial Neural Networks in an Adaptive Light Seeking Robot’<br/><NewLine>(src:m <a href=""https://pdfs.semanticscholar.org/79db/40a28420ebd2d108a1401db195dd37e9aefd.pdf"" rel=""nofollow noopener"">https://pdfs.semanticscholar.org/79db/40a28420ebd2d108a1401db195dd37e9aefd.pdf</a>) it states when replacing Q table lookup with neural network:<br/><NewLine>“Obtain Q(x, a) for each action by substituting the state and action pairs into the neural net, keeping<br/><NewLine>track of those values.” &amp; “GenerateQtarget(x,a)  according  to  equation1 and use Q target to train the net<br/><NewLine>as shown in fig 8 below.”</p><NewLine><p>Does this mean swapping</p><NewLine><blockquote><NewLine><p>Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_] ) - Q[s, a] )</p><NewLine></blockquote><NewLine><p>with a neural network where the network inputs are state (s) and the output values are a (action) ?</p><NewLine></div>",https://discuss.pytorch.org/u/Adrian.1,,Adrian.1,"April 21, 2020,  2:10pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Adrian,</p><NewLine><aside class=""quote no-group quote-modified"" data-post=""1"" data-topic=""77711"" data-username=""Adrian.1""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/adrian.1/40/8180_2.png"" width=""20""/> Adrian.1:</div><NewLine><blockquote><NewLine><p>Does this mean swapping</p><NewLine><blockquote><NewLine><p>Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_] ) - Q[s, a] )</p><NewLine></blockquote><NewLine><p>with a neural network where the network inputs are state (s) and the output values are a (action) ?</p><NewLine></blockquote><NewLine></aside><NewLine><p>I think it’s a typo but you are missing a max for Q[s_, a_] values, since you need to find state-action pair with the maximum value for all actions.</p><NewLine><p>The neural network works as a function approximator here, so instead of looking up a table you can use the network to find Q values for all actions in that state. When you predict those values (by inputing state to network), i.e:<br/><NewLine><code>a_vector_of_Q_values_for_all_actions_in_s__ = model(s_)</code><br/><NewLine>you can find the max of them to find <code>Y = reward + gamma * maxQ(s_, a_)</code></p><NewLine><p>Now if <code>X = model(s)</code> then backpropagating <code>loss(X[a],Y)</code> constitutes the learning part of the algorithm which replaces the Q-value update in tabular Q-learning.</p><NewLine><p>I also found this post very useful to understand the transition from tabular Q-learning to DQN: <a href=""https://towardsdatascience.com/why-going-from-implementing-q-learning-to-deep-q-learning-can-be-difficult-36e7ea1648af"" rel=""nofollow noopener"">https://towardsdatascience.com/why-going-from-implementing-q-learning-to-deep-q-learning-can-be-difficult-36e7ea1648af</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Maziar; <NewLine> ,"REPLY_DATE 1: May 6, 2020,  2:27pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
79324,Seq-to-Seq Encoder Decoder Models with Reinforcement Learning - CUDA memory consumption debugging,2020-05-01T23:01:04.620Z,3,89,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Problem summary: I have implemented a seq-to-seq encoder decoder model that uses Reinforcement Learning for model training. I have encountered following error while training (exactly when attention is being computed i.e.<br/><NewLine>intermediate = vector.matmul(self._w_matrix).unsqueeze(1) + matrix.matmul(self._u_matrix)):</p><NewLine><p>‘’‘RuntimeError: CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 11.17 GiB total capacity; 9.86 GiB already allocated; 310.81 MiB free; 10.58 GiB reserved in total by PyTorch)’’’</p><NewLine><p>Model Details:</p><NewLine><ol><NewLine><li>The encoder model encodes an input sequence using Pytorch’s LSTM cell (hidden_size=512).</li><NewLine><li>The decoder model’s LSTM cell is initialised using encoder model’s output. And at every time-step(t), decoder model’s input is a concatenation of (a) previous time step’s prediction (t-1) and (b) attended input produced by attending to encoder outputs.</li><NewLine><li>Training objective: We beam sample top-k predictions from the decoder model and generate the reward for each decoding. I am back-propagating loss = log probabilities of top-k decodings * their respective rewards (REINFORCE trick).</li><NewLine></ol><NewLine><p>The attention mechanism to compute weights is the following:<br/><NewLine>Attention is computed between a vector, x (in our case h_t) and a matrix, y (outputs of encoder) using an additive attention function. The function has two matrices <code>W</code>, <code>U</code> and a vector <code>V</code>. The similarity between the vector <code>x</code> and the matrix <code>y</code> is computed as <code>V tanh(Wx + Uy)</code>.</p><NewLine><p>Parameters: I am sampling 64 top decodings, batch-size: 16, hidden size of 512 for every LSTM cell (encoder and decoder both), max sequence length of 50 when decoding using beam search.</p><NewLine><p>I know this might be confusing without exact code. Let me know if any part of the code needs to be shared for more clarity. I want to debug what is consuming most of the memory because the same error appears on GPU with larger memory of 32GBs (tried on k80 and v100).</p><NewLine></div>",https://discuss.pytorch.org/u/jabhinav,(Abhinav Jain),jabhinav,"May 1, 2020, 11:23pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""79324"" data-username=""jabhinav""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/jabhinav/40/23628_2.png"" width=""20""/> jabhinav:</div><NewLine><blockquote><NewLine><p>I want to debug what is consuming most of the memory because the same error appears on GPU with larger memory of 32GBs (tried on k80 and v100).</p><NewLine></blockquote><NewLine></aside><NewLine><p>I would recommend to add <code>print(torch.cuda.memory_allocated())</code> statements in your code and check each operation or layer sequentially.<br/><NewLine>This would give you the overview where most of the memory is used.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, will debug and report if I am not able to resolve it.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> I tried debugging by checking each operation sequentially. Figured out that decoder is essentially the bottleneck because using encoder outputs, I am decoding new predictions at each time-step using previous step prediction and attended inputs (Using LSTM cell). Saving new predictions at each time step in a list is also saving its computation graph which is essentially increasing the memory consumed.</p><NewLine><p>What I want to understand is for the following memory consumption during sequence decoding:</p><NewLine><blockquote><NewLine><p><em><strong>TRAINING ITERATION: 0</strong></em><br/><NewLine><code>Decoding start</code><br/><NewLine><code>allocated: 58M, max allocated: 82M, cached: 102M, max cached: 102M</code><br/><NewLine><code>-- Before Beam Sampling</code><br/><NewLine><code>allocated: 58M, max allocated: 82M, cached: 102M, max cached: 102M</code><br/><NewLine>– <strong>TimeStep_0</strong> –<br/><NewLine><code>-- _prepare_output_projections | Start</code><br/><NewLine><code>allocated: 58M, max allocated: 82M, cached: 102M, max cached: 102M</code><br/><NewLine><code>-- _prepare_output_projections | New Decoder state computed</code><br/><NewLine><code>allocated: 60M, max allocated: 82M, cached: 102M, max cached: 102M</code><br/><NewLine>– <strong>TimeStep_1</strong> –<br/><NewLine><code>--  _prepare_output_projections | Start</code><br/><NewLine><code>allocated: 94M, max allocated: 119M, cached: 156M, max cached: 156M</code><br/><NewLine><code>--  _prepare_output_projections | New Decoder state computed</code><br/><NewLine><code>allocated: 162M, max allocated: 207M, cached: 264M, max cached: 264M</code><br/><NewLine>– <strong>TimeStep_2</strong> –<br/><NewLine><code>--  _prepare_output_projections | Start</code><br/><NewLine><code>allocated: 231M, max allocated: 256M, cached: 266M, max cached: 266M</code><br/><NewLine><code>--  _prepare_output_projections | New Decoder state computed</code><br/><NewLine><code>allocated: 299M, max allocated: 344M, cached: 374M, max cached: 374M</code><br/><NewLine>…<br/><NewLine>…<br/><NewLine>– <strong>TimeStep_19</strong> –<br/><NewLine><code>-- 1) _prepare_output_projections | Start</code><br/><NewLine><code>allocated: 2395M, max allocated: 2420M, cached: 2442M, max cached: 2442M</code><br/><NewLine><code>-- 4) _prepare_output_projections | New Decoder state computed</code><br/><NewLine><code>allocated: 2464M, max allocated: 2508M, cached: 2548M, max cached: 2548M</code><br/><NewLine><code>-- After Beam Sampling</code><br/><NewLine><code>allocated: 2529M, max allocated: 2530M, cached: 2550M, max cached: 2550M</code></p><NewLine></blockquote><NewLine><blockquote><NewLine><p><em><strong>TRAINING ITERATION: 1</strong></em><br/><NewLine><code>Decoding start</code><br/><NewLine><code>allocated: 214M, max allocated: 2600M, cached: 2664M, max cached: 2664M</code><br/><NewLine><code>-- Before Beam Sampling</code><br/><NewLine><code>allocated: 214M, max allocated: 2600M, cached: 2664M, max cached: 2664M</code><br/><NewLine>– <strong>TimeStep_0</strong> –<br/><NewLine><code>-- _prepare_output_projections | Start</code><br/><NewLine><code>allocated: 214M, max allocated: 2600M, cached: 2664M, max cached: 2664M</code><br/><NewLine><code>-- _prepare_output_projections | New Decoder state computed</code><br/><NewLine><code>allocated: 218M, max allocated: 2600M, cached: 2664M, max cached: 2664M</code><br/><NewLine>– <strong>TimeStep_1</strong> –<br/><NewLine><code>--  _prepare_output_projections | Start</code><br/><NewLine><code>allocated: 307M, max allocated: 2600M, cached: 2786M, max cached: 2786M</code><br/><NewLine><code>--  _prepare_output_projections | New Decoder state computed</code><br/><NewLine><code>allocated: 429M, max allocated: 2600M, cached: 2850M, max cached: 2850M</code><br/><NewLine>– <strong>TimeStep_2</strong> –<br/><NewLine><code>--  _prepare_output_projections | Start</code><br/><NewLine><code>allocated: 605M, max allocated: 2600M, cached: 2974M, max cached: 2974M</code><br/><NewLine><code>--  _prepare_output_projections | New Decoder state computed</code><br/><NewLine><code>allocated: 727M, max allocated: 2600M, cached: 3036M, max cached: 3036M</code><br/><NewLine>…<br/><NewLine>…<br/><NewLine>– <strong>TimeStep_33</strong> –<br/><NewLine><code>-- 1) _prepare_output_projections | Start</code><br/><NewLine><code>allocated: 9839M, max allocated: 9899M, cached: 9958M, max cached: 9958M</code><br/><NewLine><code>-- 4) _prepare_output_projections | New Decoder state computed</code><br/><NewLine><code>allocated: 9962M, max allocated: 10006M, cached: 10074M, max cached: 10074M</code><br/><NewLine><code>-- After Beam Sampling</code><br/><NewLine><code>allocated: 10136M, max allocated: 10137M, cached: 10196M, max cached: 10196M</code></p><NewLine></blockquote><NewLine><blockquote><NewLine><p>_prepare_output_projections is the function that computes new decoder state (h_t, c_t) using previous step prediction and attended input. The function also returns new predictions by projecting h_t, output of LSTM cell into the vocabulary space.</p><NewLine></blockquote><NewLine><p>At each time-step, I am predicting for batch_size x beam_size sequences. That is, if <code>batch_size = 8</code> and <code>beam_size = 100</code>, I am making 8x100 = 800 predictions at each time step. Above stats are shown for <code>batch_size = 2</code> and <code>beam_size = 32</code>. <code>Max_time_steps_allowed = 50</code></p><NewLine><p>I can see that with increase in number of predictions, allocated memory increases. Is there something wrong, or is this is the expected behaviour?  I want to experiment with <code>batch_size =16</code> for stable training. Should I go with gradient accumulation? Please, advise.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you need to call <code>backward</code> at some point and want Autograd to use all computation graphs?<br/><NewLine>I’m not familiar with your use case, but for RNNs you could detach the last state(s) so that the backward pass only calculates the gradients for the current step.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am calling backward on computed reward which is calculated in the following fashion:<br/><NewLine>For each training sample in the batch, I will have to first decode <code>n</code> complete sequences (<code>n = beam_size</code>), evaluate them based on a metric to calculate reward(or loss) for back-propagation.</p><NewLine><blockquote><NewLine><p>For example, for each sample in the batch, decode n sequences so that output of decoder is of size: (batch_size, n, max_seq_len). The metric can evaluate sequences only after they are complete (i.e. only after end_token is sampled). Reward is computed for a total of batch_size*n sequences.</p><NewLine></blockquote><NewLine><blockquote><NewLine><p>If using <code>batch_size = 8</code>, and <code>beam size = 32</code>. I am sampling <code>8*32</code> sequences from the decoder, calculating their (1) log probability and (2) reward, using Reinforce Trick to calculate loss which is <code>reward*log_probability</code> before calling backward. Here is the loss -</p><NewLine></blockquote><NewLine><p><img alt=""image"" data-base62-sha1=""foHdU35PIMaSRnlPFCC4cnaO3AS"" height=""74"" src=""https://discuss.pytorch.org/uploads/default/original/3X/6/b/6beb4da2c8b6c9d8b39e5f0baa086657b7681b2a.png"" width=""668""/></p><NewLine><p>The loss is the expectation of reward for decoded sequences, r(.) is the reward for complete sequences, <code>y_t</code> are hidden states sample from the decoder and <code>pi</code> is decoder model.</p><NewLine><p>In this training regime, I have no GT to match predictions with at each time-step. I can only back-propagate after I have sampled complete sequence. Thus saving predictions for all sequences for all time-steps is saving their computation graphs which I think is incrementally increasing GPU memory consumption. Right?</p><NewLine><p>Solutions tried: I have tried gradient accumulation where I am splitting batch_size into sub-batches, calling loss.backward for each of them before calling optimiser.step().</p><NewLine><p>Question: Is there anything that can be done to minimise such huge memory consumption. Let me know if you need specific code excerpt to debug.</p><NewLine><p>Paper for reference <a href=""http://openreview.net/pdf?id=H1Xw62kRZ"" rel=""nofollow noopener"">https://openreview.net/pdf?id=H1Xw62kRZ</a></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the update.</p><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""79324"" data-username=""jabhinav""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/jabhinav/40/23628_2.png"" width=""20""/> jabhinav:</div><NewLine><blockquote><NewLine><p>Solutions tried: I have tried gradient accumulation where I am splitting batch_size into sub-batches, calling loss.backward for each of them before calling optimiser.step().</p><NewLine></blockquote><NewLine></aside><NewLine><p>If your model is not depending on the batch size, e.g. via batchnorm layers, this should be a valid approach to trade compute for memory.<br/><NewLine>Did you still run out of memory using this approach?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jabhinav; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jabhinav; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/jabhinav; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: May 2, 2020,  6:09am; <NewLine> REPLY_DATE 2: May 2, 2020,  9:39am; <NewLine> REPLY_DATE 3: May 4, 2020,  9:37am; <NewLine> REPLY_DATE 4: May 4, 2020,  7:51pm; <NewLine> REPLY_DATE 5: May 5, 2020,  9:08am; <NewLine> REPLY_DATE 6: May 6, 2020, 12:25am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
79391,Gradient values are None,2020-05-02T11:36:42.231Z,2,118,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">lenobs = 100800<NewLine>class ActorCritic(nn.Module):<NewLine>    def __init__(self, ran):<NewLine>        super(ActorCritic, self).__init__()<NewLine>        torch.random.manual_seed(ran)<NewLine>        self.l1 = nn.Linear(lenobs,25)<NewLine>        self.l2 = nn.Linear(25,50)<NewLine>        self.actor_lin1 = nn.Linear(50,6)<NewLine>        self.l3 = nn.Linear(50,25)<NewLine>        self.critic_lin1 = nn.Linear(25,1)<NewLine><NewLine>    def forward(self,x):<NewLine>        x = F.normalize(x,dim=0)<NewLine>        y = F.relu(self.l1(x))<NewLine>        y = F.normalize(y,dim=0)<NewLine>        y = F.relu(self.l2(y))<NewLine>        y = F.normalize(y,dim=0)<NewLine>        actor = F.log_softmax(self.actor_lin1(y),dim=0)<NewLine>        c = F.relu(self.l3(y.detach()))<NewLine>        critic = F.hardtanh(self.critic_lin1(c))<NewLine>        return actor, critic<NewLine><NewLine><NewLine>def doTrain(model, ran):  <NewLine>    <NewLine>    env = gym.make('Pong-v0')<NewLine>    mi = model(ran)<NewLine>    optimizer = optim.Adam(lr=1e-4,params=mi.parameters())<NewLine>    <NewLine>    values, rewards, logprobs = [],[],[]<NewLine>    observation = env.reset()<NewLine>    done = False<NewLine>    N = 0<NewLine>    while done == False and N&lt;10:<NewLine>        N+=1<NewLine>        pobservation = torch.from_numpy(observation)<NewLine>        flattened_pobservation = pobservation.view(-1).float()<NewLine>        policy, value = mi(flattened_pobservation)<NewLine>        values.append(value.item())<NewLine>        sampler = Categorical(policy)<NewLine>        action = sampler.sample()<NewLine>        logprobs.append(policy[action.item()].item())<NewLine>        observation, reward, done, log = env.step(action.item())<NewLine>        if done:<NewLine>            rewards.append(1.0)<NewLine><NewLine>        else:<NewLine>            rewards.append(reward)<NewLine><NewLine>    torch_values = torch.Tensor(values).view(-1)<NewLine>    torch_rewards = torch.Tensor(rewards)<NewLine>    torch_logprobs = torch.Tensor(logprobs)#.flip(0)<NewLine>    <NewLine>    returns = []<NewLine>    gamma = 0.90<NewLine>    clc = 0.1<NewLine>    ret = torch.Tensor([0])<NewLine>    for r in torch_rewards:<NewLine>        ret = r + gamma*ret<NewLine>        returns.append(ret)<NewLine>    returns = torch.tensor(returns, requires_grad = True).view(-1)<NewLine>    returns = F.normalize(returns,dim=0)<NewLine>    actor_loss = -1*torch_logprobs * (returns - torch_values.detach())<NewLine>    critic_loss = torch.pow(torch_values - returns,2)<NewLine>    loss = actor_loss.sum() + clc*critic_loss.sum()<NewLine>    optimizer.zero_grad()<NewLine>    loss.backward()<NewLine>    gradients = []<NewLine>    <NewLine>    <NewLine>    for i in mi.parameters():<NewLine>        try:<NewLine>            gradients.append(i.grad)<NewLine>        except:<NewLine>            gradients.append('No Grad')<NewLine>    <NewLine><NewLine>    <NewLine>    optimizer.step()<NewLine>    return gradients<NewLine><NewLine><NewLine><NewLine>updatedParams = []<NewLine>results = []<NewLine><NewLine><NewLine>with concurrent.futures.ProcessPoolExecutor() as executor:<NewLine>    for i in range(5):<NewLine>        results.append(executor.submit(doTrain, ActorCritic, int((torch.randn(1)**2)*200)))<NewLine>    <NewLine><NewLine>    for f in concurrent.futures.as_completed(results):<NewLine>        updatedParams.append(f.result())<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">updatedParams<NewLine></code></pre><NewLine><pre><code class=""lang-auto""><NewLine>[[None, None, None, None, None, None, None, None, None, None],<NewLine> [None, None, None, None, None, None, None, None, None, None],<NewLine> [None, None, None, None, None, None, None, None, None, None],<NewLine> [None, None, None, None, None, None, None, None, None, None],<NewLine> [None, None, None, None, None, None, None, None, None, None]]<NewLine></code></pre><NewLine><p>I am trying to implement A2C for Pong.<br/><NewLine>I can’t figure why I am getting <strong>None</strong> for gradient values. Have I broken the computation graph<br/><NewLine>somewhere ?</p><NewLine></div>",https://discuss.pytorch.org/u/circa,,circa,"May 2, 2020, 12:06pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""79391"" data-username=""circa""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/circa/40/22337_2.png"" width=""20""/> circa:</div><NewLine><blockquote><NewLine><p>Have I broken the computation graph somewhere ?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, it seems this line of code detaches <code>y</code> from the computation graph:</p><NewLine><pre><code class=""lang-python"">c = F.relu(self.l3(y.detach()))<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I removed <code>.detach()</code><br/><NewLine>But the output remains same.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looking further into the code, it seems you are calling <code>detach()</code> multiple times, e.g. also in <code>torch_values.detach()</code>, are calling <code>item()</code> in <code>value.item()</code>, and are recreating tensors, which will also break the computation graph in:</p><NewLine><pre><code class=""lang-python"">torch_values = torch.Tensor(values).view(-1)<NewLine>torch_rewards = torch.Tensor(rewards)<NewLine>torch_logprobs = torch.Tensor(logprobs)<NewLine>returns = torch.tensor(returns, requires_grad = True).view(-1)<NewLine></code></pre><NewLine><p>All these operations will create new tensors without a history and Autograd won’t be able to calculate the gradients for all preceding operations.<br/><NewLine>Instead of recreating the tensors you should just use them in all further operations.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/circa; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: May 3, 2020,  3:10am; <NewLine> REPLY_DATE 2: May 3, 2020, 11:09am; <NewLine> REPLY_DATE 3: May 4, 2020, 12:12am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
79163,BrokenPipeError: [Errno 32] Broken pipe,2020-04-30T19:02:42.013Z,3,156,"<div class=""post"" itemprop=""articleBody""><NewLine><p>BrokenPipeError: [Errno 32] Broken pipe<br/><NewLine>threads issue, unable to set threads<br/><NewLine>data loader problem<br/><NewLine>RuntimeError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 11.00 GiB total capacity; 8.25 GiB already allocated; 70.74 MiB free; 8.31 GiB reserved in total by PyTorch)</p><NewLine></div>",https://discuss.pytorch.org/u/chillum1718,(truppy),chillum1718,"April 30, 2020,  7:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You are running out of memory on your GPU.<br/><NewLine>Try to decrease the batch size or alternatively try to trade compute for memory via <code>torch.utils.checkpoint</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/7f763b49664bbb2fb77a71e0e6cc72015b0c8535"" href=""https://discuss.pytorch.org/uploads/default/original/3X/7/f/7f763b49664bbb2fb77a71e0e6cc72015b0c8535.png"" title=""Capture1""><img alt=""Capture1"" data-base62-sha1=""ibzWP1vOoW31MpZ6WhQfUn2vtNr"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/7/f/7f763b49664bbb2fb77a71e0e6cc72015b0c8535_2_10x10.png"" height=""296"" src=""https://discuss.pytorch.org/uploads/default/original/3X/7/f/7f763b49664bbb2fb77a71e0e6cc72015b0c8535.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Capture1</span><span class=""informations"">787×338 16 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine>when i define No. of workers 2 it gives me an error [broken pipe error]<br/><NewLine>iam using intel 9920x<br/><NewLine>RTX 2080 TI 11GB<br/><NewLine>windows 10 pro<br/><NewLine>i have also uploaded the screenshot</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have a look at the <a href=""https://pytorch.org/docs/stable/notes/windows.html#usage-multiprocessing"">Windows FAQ</a> for multiprocessing. Maybe you are missing the if-clause protection.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/chillum1718; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: May 1, 2020,  1:48am; <NewLine> REPLY_DATE 2: May 1, 2020,  7:52am; <NewLine> REPLY_DATE 3: May 2, 2020,  3:48am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
12022,Creating a Clipped Loss Function,2018-01-08T17:57:44.071Z,2,2708,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!</p><NewLine><p>According to the  <a href=""https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf"" rel=""nofollow noopener"">DeepMind DQL Paper</a>, the error term is clipped between -1 and 1. I am using clamp for that, but using it doesn’t allow me to use a default loss function like MSELoss.</p><NewLine><p>How can I proceed to do this?</p><NewLine><p>Code may be found in <a href=""https://github.com/diegoalejogm/deep-q-learning/blob/master/Deep%20Q%20Learning.ipynb"" rel=""nofollow noopener"">this repo</a>.</p><NewLine></div>",https://discuss.pytorch.org/u/diegoalejogm,(Diego Gomez),diegoalejogm,"January 8, 2018,  5:57pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t understand. Why can’t you use error fns with clamp?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t know how to do it correctly. If I try to use default errors, they usually take two arguments (target, prediction), but then I am unable to clamp on the loss for each pair. On the other hand, if I use clamp(target - prediction, min, max), I end up with only one tensor, and then I can’t use default errors.</p><NewLine><p>Does it make sense?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh I see. You can try the <code>reduce=False</code> kwarg on loss functions so they give you a tensor. Then you can do clamp and reduction yourself <img alt="":slight_smile:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=5"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/simonw"">@SimonW</a>, thanks for your help! I’ve just updated the optimizer:</p><NewLine><pre><code>loss_func = torch.nn.MSELoss(size_average=False, reduce=False)<NewLine></code></pre><NewLine><p>And also coded the backward pass accordingly:</p><NewLine><pre><code># Run backward pass<NewLine>error = loss_func(q_phi, y)<NewLine>error = torch.clamp(error, min=-1, max=1)**2<NewLine>error = error.sum() <NewLine>error.backward()<NewLine></code></pre><NewLine><p>And it seems like no errors appear, which implies that the ‘backward’ operation is running correctly!</p><NewLine><p>Will test it out in the Atari Environment and let you know how it goes. The code is in <a href=""https://github.com/diegoalejogm/deep-q-learning/blob/master/Deep%20Q%20Learning.ipynb"" rel=""nofollow noopener"">here</a> in case anyone wants to check it out meanwhile.</p><NewLine><p>Thanks again!</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>With code snippet below</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch.autograd import Variable<NewLine>w = Variable(torch.Tensor([1.]), requires_grad=True)<NewLine>l = w**2<NewLine>l_clip = l.clamp(max=0.8)<NewLine>torch.autograd.grad(l, w, retain_graph=True) # prints (tensor([ 2.]),)<NewLine>torch.autograd.grad(l_clip, w, retain_graph=True) # prints (tensor([ 0.]),)<NewLine></code></pre><NewLine><p>It seems that clamp operation stops gradient flow if the value is clipped. Thus I think your code above might not work as you intended.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/hailin_chen"">@Hailin_Chen</a>  Could you propose a solution for this issue?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SimonW; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/diegoalejogm; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/SimonW; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/diegoalejogm; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Hailin_Chen; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/diegoalejogm; <NewLine> ,"REPLY_DATE 1: January 8, 2018, 11:11pm; <NewLine> REPLY_DATE 2: January 9, 2018,  5:21pm; <NewLine> REPLY_DATE 3: January 9, 2018, 10:28pm; <NewLine> REPLY_DATE 4: January 10, 2018,  4:07am; <NewLine> REPLY_DATE 5: August 10, 2018,  5:16am; <NewLine> REPLY_DATE 6: April 25, 2020,  3:56pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> 
77889,RBM and Auto-encoder comparison,2020-04-22T15:47:22.844Z,0,51,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi.</p><NewLine><p>im trying to implement a recommender system using RBM and deep auto-encoder architectures separately.RBM try to predict ratings for not rated movies(1 like/0 no),and AE try to predict the exact rating of the movie(from 1 to 5 stars).<br/><NewLine>i used the RMSE and MAE metrics for comparison.however i noticed that they are note necessarily used for the same thing,RBM use binary inputs to predict features accuracy (%),and AE use inputs to predict the user rating for a specific movie . so we can not compare them using RMSE and MAE ?(in my case i get MAE of RBM 0.24 (75%accuracy) and with AE0 i get 0.77 (if user gives rate 4 stars we systeme can give between 3 and 4) not exactly the same job .what do u say?</p><NewLine></div>",https://discuss.pytorch.org/u/rostom007,,rostom007,"April 22, 2020,  3:47pm",,,,,
73616,How to ensure dimensions much if states batch has different dimension from actions,2020-03-18T09:51:35.380Z,1,267,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to train a DQN to do optimal energy scheduling. Each state comes as a vector of 4 variables (represented by floats) saved in the replay memory as a state tensor, each action is an integer saved in the memory as a tensor too. I extract the batch of experiences as:</p><NewLine><pre><code class=""lang-auto"">def extract_tensors(experiences):<NewLine>    # Convert batch of Experiences to Experience of batches<NewLine>    batch = Experience(*zip(*experiences))<NewLine><NewLine>    t1 = torch.cat(batch.state)<NewLine>    t2 = torch.cat(batch.action)<NewLine>    t3 = torch.stack(batch.reward)<NewLine>    t4 = torch.cat(batch.next_state)<NewLine><NewLine>    return (t1,t2,t3,t4)<NewLine></code></pre><NewLine><p>I then unpacked them for purposes of updating the prediction and the target networks as:</p><NewLine><pre><code class=""lang-auto"">experiences = memory.sample(batch_size)<NewLine>states, actions, rewards, next_states = extract_tensors(experiences)<NewLine></code></pre><NewLine><p>My Qvalues class for update looks like this:</p><NewLine><pre><code class=""lang-auto"">class QValues():<NewLine><NewLine>   device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")<NewLine><NewLine>   @staticmethod<NewLine>   def get_current(policy_net, states, actions):<NewLine>       return policy_net(states).gather(dim=1, index=actions)<NewLine><NewLine>   @staticmethod        <NewLine>   def get_next(target_net, next_states):                <NewLine>       final_state_locations = next_states.flatten(start_dim=1) \<NewLine>           .max(dim=1)[0].eq(0).type(torch.bool)<NewLine>       non_final_state_locations = (final_state_locations == False)<NewLine>       non_final_states = next_states[non_final_state_locations]<NewLine>       batch_size = next_states.shape[0]<NewLine>       values = torch.zeros(batch_size).to(QValues.device)<NewLine>       values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()<NewLine><NewLine>       return values<NewLine></code></pre><NewLine><p>When I try running the training loop, I get the error below:</p><NewLine><pre><code class=""lang-auto"">&lt;ipython-input-8-4a79494b54ca&gt; in &lt;module&gt;<NewLine>    214 <NewLine>    215 <NewLine>--&gt; 216                         current_q_values = QValues.get_current(policy_net, states, actions)<NewLine>    217                         next_q_values = QValues.get_next(target_net, next_states)<NewLine>    218                         target_q_values = (next_q_values * gamma) + rewards<NewLine><NewLine>&lt;ipython-input-8-4a79494b54ca&gt; in get_current(policy_net, states, actions)<NewLine>    160     @staticmethod<NewLine>    161     def get_current(policy_net, states, actions):<NewLine>--&gt; 162         return policy_net(states).gather(dim=1, index=actions)<NewLine>    163 <NewLine>    164     @staticmethod<NewLine><NewLine>RuntimeError: invalid argument 4: Index tensor must have same dimensions as input tensor at c:<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/E_OArwa,,E_OArwa,"March 18, 2020,  9:51am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Got the somewhat the same error.Although in my case im getting a size mismatch when the network attempts to calculate the batched states. Did you manage a work around?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>After doing abit of research i found a quick workaround by adding “actions = actions.squeeze()” in the get_current method as follows. Worked for me. Hope it helps!:</p><NewLine><p><img alt=""solution"" data-base62-sha1=""sPv3aPXZTcnh3eP40OdvjcXNAR6"" height=""181"" src=""https://discuss.pytorch.org/uploads/default/original/3X/c/a/ca0f378a5e370357287a5b49d28c90fbe4aca354.png"" width=""638""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, thank you for the reply. I have tried that but it is still returning the error.</p><NewLine><pre><code class=""lang-auto"">&lt;ipython-input-4-fbc914f54837&gt; in &lt;module&gt;<NewLine>    215 <NewLine>    216 <NewLine>--&gt; 217                         current_q_values = QValues.get_current(policy_net, states, actions)<NewLine>    218                         next_q_values = QValues.get_next(target_net, next_states)<NewLine>    219                         target_q_values = (next_q_values * gamma) + rewards<NewLine><NewLine>&lt;ipython-input-4-fbc914f54837&gt; in get_current(policy_net, states, actions)<NewLine>    161     def get_current(policy_net, states, actions):<NewLine>    162         actions = actions.squeeze()<NewLine>--&gt; 163         return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))<NewLine>    164 <NewLine>    165     @staticmethod<NewLine><NewLine>RuntimeError: Invalid index in gather at c:\a\w\1\s\tmp_conda_3.7_070024\conda\conda-bld\pytorch-cpu_1544079887239\work\aten\src\th\generic/THTensorEvenMoreMath.cpp:457```</code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>What is the shape of your states? Depending on the dimensions you used for your network, the shape of the states batch should be torch.Size([batch_size, number of colour channels, width, height]) and each state should be torch.Size([dimension, number of colour channels, width, height])   (assuming you are using images for input) . Where dimension should match the dimensions set when creating the network. Dimension should be 1 to match with the “get current” method</p><NewLine><p><em><strong><strong><strong><strong>UPDATE</strong></strong></strong></strong></em>*******************<br/><NewLine>Yes. You could use “state = state.squeeze().unsqueeze(dim=0)” to change the dimension of your state that will agree with the “get current” method. This should be done before pushing the state to memory</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>My states just tensors of 4 floats. My environment gives states as a list say [4, 20, 4, 32], so I convert them to tensors before I forward them.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Kikumu; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Kikumu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/E_OArwa; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Kikumu; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/E_OArwa; <NewLine> ,"REPLY_DATE 1: April 20, 2020,  1:02pm; <NewLine> REPLY_DATE 2: April 20, 2020,  6:47pm; <NewLine> REPLY_DATE 3: April 21, 2020,  9:55am; <NewLine> REPLY_DATE 4: April 22, 2020,  6:29am; <NewLine> REPLY_DATE 5: April 21, 2020,  1:31pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
77450,Comparison between deep Auto-Encoder (stacked) and RBM training?,2020-04-19T17:05:03.291Z,0,55,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi.</p><NewLine><p>im trying to implement a recommander system using RBM and deep auto-encoder architectures separately with pytorch. After the training i got a better performance result in RBM over AE.however,i have seen many papers and studies which said that the AE is always better than RBM .i used the same dataset for both architectures. and im trying to do a comparison between these two methods of deep learning.</p><NewLine><p>My question is why did i got a different results than excpected? should i change something or it is ok ?</p><NewLine></div>",https://discuss.pytorch.org/u/rostom007,,rostom007,"April 19, 2020,  5:05pm",,,,,
76793,How do I implement TD(lambda) with approximation,2020-04-14T23:14:18.385Z,0,119,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone.<br/><NewLine>How do I implement backward view algorithm of TD(lambda) of function approximate.<br/><NewLine>what is the loss function? or we have to update weight manually by the formula?<br/><NewLine>are there some elegant method to update grad?</p><NewLine></div>",https://discuss.pytorch.org/u/yannick,,yannick,"April 14, 2020, 11:14pm",,,,,
76282,MultiPlayer weight sharing of exact same network,2020-04-11T06:41:04.573Z,0,77,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey there,</p><NewLine><p>I implemented a four player card game using pytorch and reinforcement learning (PPO). To train the agents I make four exact copys and let them play against each other. I now would like to share after a certain update time the weights between this same networks.</p><NewLine><p>I found this procedure:</p><NewLine><ol><NewLine><li><NewLine><p>Make all your modules</p><NewLine></li><NewLine><li><NewLine><p>Make all your clones</p><NewLine></li><NewLine><li><NewLine><p>Add all the modules and clones to a single nn.Container</p><NewLine></li><NewLine><li><NewLine><p>Call :getParameters on the nn.Container to get params and grads. This will preserve any sharing of parameters between modules inside the nn.Container.</p><NewLine></li><NewLine><li><NewLine><p>Now using the modules and clones as normal will play nice with optim because all of the params and grads reference the same storage as the tensors from :getParameters.</p><NewLine></li><NewLine></ol><NewLine><p>I tried to implement it as follows:  (ppo are the models)</p><NewLine><pre><code class=""lang-auto"">            container = nn.Container()<NewLine>            for i in range(4):<NewLine>                container:add(ppo[i])<NewLine>            params = container.parameters()<NewLine></code></pre><NewLine><p><strong>How to apply the parameters now back to each model?</strong><br/><NewLine><strong>Is the above method the correct approach?</strong></p><NewLine><p>Further Snippets:</p><NewLine><pre><code class=""lang-auto"">class PPO:<NewLine>    def __init__(self, state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip, lr_decay=1000000):<NewLine>        self.lr = lr<NewLine>        self.betas = betas<NewLine>        self.gamma = gamma<NewLine>        self.eps_clip = eps_clip<NewLine>        self.K_epochs = K_epochs<NewLine><NewLine>        self.policy = ActorCritic(state_dim, action_dim, n_latent_var)<NewLine>        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas, eps=1e-5) # no eps before!<NewLine>        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var)<NewLine>        self.policy_old.load_state_dict(self.policy.state_dict())<NewLine>        #TO decay learning rate during training:<NewLine>        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=lr_decay, gamma=0.9)<NewLine>        self.MseLoss = nn.MSELoss()<NewLine><NewLine><NewLine>class ActorMod(nn.Module):<NewLine>    def __init__(self, state_dim, action_dim, n_latent_var):<NewLine>        super(ActorMod, self).__init__()<NewLine>        self.l1      = nn.Linear(state_dim, n_latent_var)<NewLine>        self.l1_tanh = nn.PReLU()<NewLine>        self.l2      = nn.Linear(n_latent_var, n_latent_var)<NewLine>        self.l2_tanh = nn.PReLU()<NewLine>        self.l3      = nn.Linear(n_latent_var+60, action_dim)<NewLine><NewLine>    def forward(self, input):<NewLine>        x = self.l1(input)<NewLine>        x = self.l1_tanh(x)<NewLine>        x = self.l2(x)<NewLine>        out1 = self.l2_tanh(x) # 64x1<NewLine>        if len(input.shape)==1:<NewLine>            out2 = input[180:240]   # 60x1 this are the available options of the active player!<NewLine>            output =torch.cat( [out1, out2], 0)<NewLine>        else:<NewLine>            out2 = input[:, 180:240]<NewLine>            output =torch.cat( [out1, out2], 1) #how to do that?<NewLine>        x = self.l3(output)<NewLine>        return x.softmax(dim=-1)<NewLine><NewLine><NewLine>class ActorCritic(nn.Module):<NewLine>    def __init__(self, state_dim, action_dim, n_latent_var):<NewLine>        super(ActorCritic, self).__init__()<NewLine><NewLine>        # actor<NewLine>        #TODO see question: https://discuss.pytorch.org/t/pytorch-multiple-inputs-in-sequential/74040<NewLine>        self.action_layer = ActorMod(state_dim, action_dim, n_latent_var)<NewLine><NewLine>        # critic<NewLine>        self.value_layer = nn.Sequential(<NewLine>                nn.Linear(state_dim, n_latent_var),<NewLine>                nn.PReLU(),#prelu<NewLine>                nn.Linear(n_latent_var, n_latent_var),<NewLine>                nn.PReLU(),<NewLine>                nn.Linear(n_latent_var, 1)<NewLine>                )<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/CesMak,(Markus Lamprecht),CesMak,"April 11, 2020,  6:41am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok here is what I now tried:</p><NewLine><p>I have exact 4 same models which are playing against each other and trained using PPO.</p><NewLine><p>I now tried to calculate the mean of the parameters and update the models like this:</p><NewLine><pre><code class=""lang-auto"">            for [dict1, dict2, dict3, dict4] in zip(ppo[0].policy.state_dict().items(), ppo[1].policy.state_dict().items(), ppo[2].policy.state_dict().items(), ppo[3].policy.state_dict().items()):<NewLine>                val1, val2, val3, val4 = dict1[1], dict2[1], dict3[1], dict4[1]<NewLine>                final_dict[dict1[0]] = (val1+val2+val3+val4)/4<NewLine>            for i in range(4):<NewLine>                ppo[i].policy.load_state_dict(final_dict)<NewLine></code></pre><NewLine><p>However this is not working correctly… (it does not learn faster…)</p><NewLine><p>Any ideas?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/CesMak; <NewLine> ,"REPLY_DATE 1: April 14, 2020,  2:51pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
75406,Can I train a asynchronous actor critic model without keeping a copy of the model?,2020-04-05T06:32:04.801Z,0,64,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m editing code from <a href=""https://github.com/facebookresearch/torchbeast"" rel=""nofollow noopener"">torchbeast</a> to fit my use case. I want to use a discriminator for calculating the reward at a certain state.</p><NewLine><p>I noticed that torchbeast uses a copy of the model for inference called actor_model and a the original model for learning.</p><NewLine><p>This is part of polybeast.py:</p><NewLine><pre><code class=""lang-auto"">def learn(<NewLine>    ...<NewLine>    for tensors in learner_queue:<NewLine>        ...<NewLine>        optimizer.zero_grad()<NewLine>        total_loss.backward()<NewLine>        nn.utils.clip_grad_norm_(model.parameters(), flags.grad_norm_clipping)<NewLine>        optimizer.step()<NewLine>        scheduler.step()<NewLine><NewLine>        actor_model.load_state_dict(model.state_dict())<NewLine>        ...<NewLine></code></pre><NewLine><p>I tried the same thing with the discriminator but I ran out of memory and had to use minibatches. I thought of using locks so only one thread is using the discriminator. Is there any other solution to this problem? Is it impossible?</p><NewLine></div>",https://discuss.pytorch.org/u/urw7rs,,urw7rs,"April 5, 2020,  6:33am",,,,,
16077,CNN and Actor Critic,2018-04-07T06:54:23.460Z,1,637,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>When using convolutional neural networks with reinforcement learning, it is best to have the critic and the policy share the convolutional layers or rather create two separate entities of it ?</p><NewLine><p>Thanks !</p><NewLine></div>",https://discuss.pytorch.org/u/Mehdi,,Mehdi,"April 7, 2018,  6:54am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>my thinking is that it’s probably best to share conv layers in this case.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>In the general sense of Actor-Critic family of algorithms, there is no need to share the network parameters. You could have total separate two networks. That would cost you more memory and compute and most likely take longer.</p><NewLine><p>Sharing the parameters improves the training time however it makes it harder to train because the parameters need more careful tuning. This is because the norm of gradients flowing back from the actor gradients and the critic gradients are at completely different scales which is hard to calibrate.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/CesMak; <NewLine> ,"REPLY_DATE 1: April 7, 2018,  3:30pm; <NewLine> REPLY_DATE 2: April 4, 2020,  8:20am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
44567,Using libtorch to implement DQN,2019-05-07T03:32:25.143Z,0,258,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I need to implement DQN algorithm using libtorch. But it seems that libtorch didn’t provide<br/><NewLine>the method of copying the parameters from the training network to the target network, which is necessary for DQN.</p><NewLine><p>The corresponding Python code is like this:</p><NewLine><pre><code class=""lang-auto""> def update_target_q_network(self):<NewLine>        self.target_model.load_state_dict(self.model.state_dict())<NewLine></code></pre><NewLine><p>Are there any practical solutions to this question?</p><NewLine></div>",https://discuss.pytorch.org/u/stephenxudong,(Stephen),stephenxudong,"May 7, 2019,  3:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>try this:</p><NewLine><pre><code class=""lang-auto"">for (size_t i = 0; i &lt; target_model.parameters().size(); i++)<NewLine>{<NewLine>        target_model.parameters()[i].copy_(model-&gt;parameters()[i]);<NewLine>}<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/EmmiOcean; <NewLine> ,"REPLY_DATE 1: March 31, 2020,  5:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
25358,Question about how the -m.log_prob() function in torch.distributions.bernoulli works?,2018-09-17T23:32:23.259Z,1,606,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to figure out what the -m.log_prob() function is actually doing when implementing Policy gradients.</p><NewLine><p>I have stepped through the m.log_prob() function in the debugger <strong>many</strong> times - and fail to see how it is working. My understanding is that m.log_prob() calls</p><NewLine><p>m.log_prob() &gt;&gt; which calls logits() &gt;&gt; when then calls cross_entropy_with_logits()</p><NewLine><p>this is all fine, except that I cannot recreate the value being created in the logits function.</p><NewLine><p>I get that logits() is just the log odds, so log( p /(1 −  p ) but when inputting the same value for p (generated by the net on line 1 below , probs = p) in into the equation for logits by hand, fail to recreate the same value produced by PyTorch’s logit function!</p><NewLine><p>The reason for all of this being that I think this function is at the core of what is going on - and because Pytorch is so efficient-obscures “the magic” of what is actually going on under the hood in Vanilla Policy Gradients (aka REINFORCE).</p><NewLine><p>specifically, I am referring to what is going on inside the <strong>-m.log_prob()</strong> function here:</p><NewLine><ol><NewLine><li>probs = policy_network(state)</li><NewLine><li>Note that this is equivalent to what used to be called multinomial</li><NewLine><li>m = Categorical(probs)</li><NewLine><li>action = m.sample()</li><NewLine><li>next_state, reward = env.step(action)</li><NewLine><li>loss = <strong>-m.log_prob(action)</strong> * reward</li><NewLine><li>loss.backward()</li><NewLine></ol><NewLine><p>this example is straight out of the pytorch documentation here: <a href=""https://pytorch.org/docs/stable/distributions.html?highlight=reinforce"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/distributions.html?highlight=reinforce</a></p><NewLine></div>",https://discuss.pytorch.org/u/paulsteven,(Paul),paulsteven,"September 18, 2018, 11:14pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just checking in here - does it not just call .gather on the logits?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/penguinshin; <NewLine> ,"REPLY_DATE 1: November 6, 2018,  2:44am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
74686,Error when run reinforcement_q_learning.ipynb,2020-03-29T09:53:12.720Z,0,103,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Periodically I try to run the example (my computer: Windows 10, browser: Google Chrome):</p><NewLine><p><a href=""http://pytorch.org"" rel=""nofollow noopener"">pytorch.org</a> -&gt; Tutorials -&gt; Reinforcement Learning -&gt; Reinforcement Learning (DQN) Tutorial -&gt; Run in Google Colab -&gt; menu Runtime -&gt; menu item Run All.</p><NewLine><h2>But I always get such error:</h2><NewLine><p>NoSuchDisplayException                    Traceback (most recent call last)<br/><NewLine> in ()<br/><NewLine>39 env.reset()<br/><NewLine>40 plt.figure()<br/><NewLine>—&gt; 41 plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),<br/><NewLine>42            interpolation=‘none’)<br/><NewLine>43 plt.title(‘Example extracted screen’)</p><NewLine><p>9 frames<br/><NewLine>/usr/local/lib/python3.6/dist-packages/pyglet/canvas/xlib.py in <strong>init</strong>(self, name, x_screen)<br/><NewLine>121         self._display = xlib.XOpenDisplay(name)<br/><NewLine>122         if not self._display:<br/><NewLine>–&gt; 123             raise NoSuchDisplayException(‘Cannot connect to “%s”’ % name)<br/><NewLine>124<br/><NewLine>125         screen_count = xlib.XScreenCount(self._display)</p><NewLine><p>NoSuchDisplayException: Cannot connect to “None”</p><NewLine></div>",https://discuss.pytorch.org/u/tv76,(Anatoly),tv76,"March 29, 2020, 10:49am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems to be a known issue (<a href=""https://discuss.pytorch.org/t/found-bug-in-pytorch-reinforcement-tutorials/73554/9"">related post</a>).<br/><NewLine>I can’t find an issue, so I assume the other user didn’t create one.<br/><NewLine>Would you mind creating the issue for this tutorial?<br/><NewLine>If not, please let me know, and I’ll do it.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh, sure:<br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/tutorials/issues/915"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/tutorials</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/tutorials/issues/915"" rel=""nofollow noopener"" target=""_blank"">Error when run reinforcement_q_learning.ipynb</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-03-30"" data-format=""ll"" data-time=""07:38:41"" data-timezone=""UTC"">07:38AM - 30 Mar 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/tv76"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""tv76"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars0.githubusercontent.com/u/46939018?v=4"" width=""20""/><NewLine>          tv76<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">Periodically I try to run the example (my computer: Windows 10, browser: Google Chrome):<NewLine>pytorch.org -&gt; Tutorials -&gt; Reinforcement Learning -&gt; Reinforcement...</p><NewLine></div><NewLine><div class=""labels""><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tv76; <NewLine> ,"REPLY_DATE 1: March 29, 2020, 11:39pm; <NewLine> REPLY_DATE 2: March 30, 2020,  7:46am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
13263,Help for DQN example,2018-02-04T14:45:56.807Z,0,1312,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello folks.</p><NewLine><p>I create an dqn implement according the tutorial <strong><em>reinforcement_q_learning</em></strong>, with the following changes.</p><NewLine><ol><NewLine><li>Use gym observation as state</li><NewLine><li>Use an MLP instead of the DQN class in the tutorial</li><NewLine></ol><NewLine><p>The model diverged if <em>loss = F.smooth_l1_loss</em>{ loss_fn = nn.SmoothL1Loss()} ,<br/><NewLine>If loss_fn = nn.MSELoss(), the model seems to work (much slower than the tutorial)</p><NewLine><p>What did I do wrong? Any ideas?</p><NewLine><pre><code class=""lang-auto"">import math<NewLine>import random<NewLine>import numpy as np<NewLine>from collections import namedtuple<NewLine>import matplotlib.pyplot as plt<NewLine>from itertools import count<NewLine>from time import sleep<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine>import torch.nn.functional as F<NewLine>from torch.autograd import Variable<NewLine>import gym<NewLine><NewLine>use_cuda = False<NewLine>FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor<NewLine>LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor<NewLine>ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor<NewLine>Tensor = FloatTensor<NewLine><NewLine>env = gym.make('CartPole-v0')<NewLine> <NewLine>Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))<NewLine><NewLine>class ReplayMemory(object):<NewLine><NewLine>    def __init__(self, capacity):<NewLine>        self.capacity = capacity<NewLine>        self.memory = []<NewLine>        self.position = 0<NewLine><NewLine>    def push(self, *args):<NewLine>        if len(self.memory) &lt; self.capacity:<NewLine>            self.memory.append(None)<NewLine>        self.memory[self.position] = Transition(*args)<NewLine>        self.position = (self.position + 1) % self.capacity<NewLine><NewLine>    def sample(self, batch_size):<NewLine>        return random.sample(self.memory, batch_size)<NewLine><NewLine>    def __len__(self):<NewLine>        return len(self.memory)<NewLine><NewLine>BATCH_SIZE = 128<NewLine>GAMMA = 0.999<NewLine>EPS_START = 0.9<NewLine>EPS_END = 0.05<NewLine>EPS_DECAY = 200<NewLine><NewLine>model = nn.Sequential(<NewLine>          nn.Linear(4,1024), <NewLine>          nn.ReLU(), <NewLine>          nn.Linear(1024,1024), <NewLine>          nn.ReLU(),<NewLine>          nn.Linear(1024,2)<NewLine>        )<NewLine><NewLine>optimizer = optim.Adam(model.parameters(),  lr=1e-4)<NewLine>loss_fn = nn.MSELoss()<NewLine># loss_fn = nn.SmoothL1Loss()<NewLine>memory = ReplayMemory(10000)<NewLine><NewLine>steps_done = 0<NewLine><NewLine>def select_action(state):<NewLine>    global steps_done<NewLine>    sample = random.random()<NewLine>    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)<NewLine>    steps_done += 1<NewLine>    if sample &gt; eps_threshold:<NewLine>        output = model(Variable(state.view(-1, 4), volatile=True))<NewLine>        output = output.data<NewLine>        _, index = output.max(1)<NewLine>        print(f'State By Model {output}')<NewLine>        print(f'State By Model {index[0]}')<NewLine>        return index.view(1, 1)<NewLine>    else:<NewLine>        index = random.randrange(2)<NewLine>        # print(f'State By Random {index}')<NewLine>        return LongTensor([[index]])<NewLine><NewLine><NewLine>episode_durations = []<NewLine><NewLine>def plot_durations():<NewLine>    plt.figure(2)<NewLine>    plt.clf()<NewLine>    durations_t = torch.FloatTensor(episode_durations)<NewLine>    plt.title('Training...')<NewLine>    plt.xlabel('Episode')<NewLine>    plt.ylabel('Duration')<NewLine>    plt.plot(durations_t.numpy())<NewLine>    # Take 100 episode averages and plot them too<NewLine>    if len(durations_t) &gt;= 100:<NewLine>        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)<NewLine>        means = torch.cat((torch.zeros(99), means))<NewLine>        plt.plot(means.numpy())<NewLine><NewLine>    plt.pause(0.1)  # pause a bit so that plots are updated<NewLine><NewLine>last_sync = 0<NewLine><NewLine>def optimize_model():<NewLine>    global last_sync<NewLine>    if len(memory) &lt; BATCH_SIZE:<NewLine>        return<NewLine>    transitions = memory.sample(BATCH_SIZE)<NewLine>    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for<NewLine>    # detailed explanation).<NewLine>    batch = Transition(*zip(*transitions))<NewLine><NewLine>    state_batch = [s.view(1,-1) for s in batch.state]<NewLine>    state_batch = torch.cat(state_batch, 0)<NewLine>    state_batch = Variable(state_batch)<NewLine><NewLine>    action_batch = torch.cat(batch.action)<NewLine>    action_batch = Variable(action_batch) <NewLine>    <NewLine>    reward_batch = torch.cat(batch.reward)<NewLine>    reward_batch = Variable(reward_batch)<NewLine><NewLine>    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the<NewLine>    # columns of actions taken<NewLine>    model_outputs = model(state_batch)<NewLine>    state_action_values = model_outputs.gather(1, action_batch)<NewLine><NewLine><NewLine>    non_final_mask = map(lambda s: s is not None, batch.next_state)<NewLine>    non_final_mask = tuple(non_final_mask)<NewLine>    # Compute a mask of non-final states and concatenate the batch elements<NewLine>    non_final_mask = ByteTensor(non_final_mask)<NewLine><NewLine>    # We don't want to backprop through the expected action values and volatile<NewLine>    # will save us on temporarily changing the model parameters'<NewLine>    # requires_grad to False!<NewLine>    non_final_next_states = [s.view(1, -1) for s in batch.next_state if s is not None]<NewLine>    non_final_next_states = torch.cat(non_final_next_states, 0)<NewLine>    non_final_next_states = Variable(non_final_next_states, volatile=True)<NewLine>    # Compute V(s_{t+1}) for all next states.<NewLine>    next_state_values = Variable(torch.zeros(BATCH_SIZE).type(Tensor))<NewLine>    next_model_outputs = model(non_final_next_states)<NewLine>    max_next_state_values, _ = next_model_outputs.max(1)<NewLine>    next_state_values[non_final_mask] = max_next_state_values<NewLine>    # Now, we don't want to mess up the loss with a volatile flag, so let's<NewLine>    # clear it. After this, we'll just end up with a Variable that has<NewLine>    # requires_grad=False<NewLine>    next_state_values.volatile = False<NewLine>    # Compute the expected Q values<NewLine>    expected_state_action_values = (next_state_values * GAMMA) + reward_batch<NewLine><NewLine>    # Compute Huber loss<NewLine>    loss = loss_fn(state_action_values, expected_state_action_values)<NewLine><NewLine>    print(f'Loss={loss.data[0]}');<NewLine>    # Optimize the model<NewLine>    optimizer.zero_grad()<NewLine>    loss.backward()<NewLine>    # for param in model.parameters(): param.grad.data.clamp_(-1, 1)<NewLine>    optimizer.step()<NewLine><NewLine>num_episodes = 1000<NewLine>for i_episode in range(num_episodes):<NewLine>    # Initialize the environment and state<NewLine>    observation = env.reset()<NewLine>    state = FloatTensor(observation)  <NewLine>    for t in count():<NewLine>        env.render()<NewLine>#        print(f'State:{state.tolist()}')<NewLine>        action = select_action(state)<NewLine>        step_action = action[0, 0]<NewLine>        observation, reward, done, info = env.step(step_action)<NewLine>#        print(f'observation={observation}')<NewLine>#        print(f'reward={reward}')<NewLine>#        print(f'done={done}') <NewLine>        reward = Tensor([reward])<NewLine>        <NewLine>        # Observe new state<NewLine>        next_state = FloatTensor(observation) if not done else None<NewLine><NewLine>        # Store the transition in memory<NewLine>        assert state is not None<NewLine>        memory.push(state, action, next_state, reward) <NewLine><NewLine>        # if next_state is not None : print(f'NextState:{next_state.tolist()}')<NewLine><NewLine>        # Move to the next state<NewLine>        state = next_state<NewLine>        <NewLine>        # Perform one step of the optimization (on the target network)<NewLine>        optimize_model()<NewLine>        if done:<NewLine>            episode_durations.append(t + 1)<NewLine>            plot_durations()<NewLine>            break<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/zzfnohell,(John Z),zzfnohell,"February 4, 2018,  2:45pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>First thing I see is that your hidden layers are too big for the problem. That might be one of the reason you do not converge. Try with one 64 hidden neurons layer and see if it improves anything. It does work for me. Maybe also have smaller batch size and a slower decay (you might need more than 1000 iterations with DQN).</p><NewLine><p>Good luck !</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I took your code and modified it a bit, but I think the thing that improved the most the duration of the episodes was changing the optimizer from Adam to RMSprop:</p><NewLine><p>optimizer = optim.RMSprop(model.parameters())<br/><NewLine>“loss_fn = nn.SmoothL1Loss()”</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/d4c7d04ef9158787e8be774d5a05a682c9e072aa"" href=""https://discuss.pytorch.org/uploads/default/original/3X/d/4/d4c7d04ef9158787e8be774d5a05a682c9e072aa.png"" title=""20200329_2225_RMSpropOptimizer_SmothL1Loss_1000iterations""><img alt=""20200329_2225_RMSpropOptimizer_SmothL1Loss_1000iterations"" data-base62-sha1=""umlk0aiybXdjWFqJ5wHArfT28dA"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/4/d4c7d04ef9158787e8be774d5a05a682c9e072aa_2_10x10.png"" height=""195"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/4/d4c7d04ef9158787e8be774d5a05a682c9e072aa_2_690x195.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/4/d4c7d04ef9158787e8be774d5a05a682c9e072aa_2_690x195.png, https://discuss.pytorch.org/uploads/default/optimized/3X/d/4/d4c7d04ef9158787e8be774d5a05a682c9e072aa_2_1035x292.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/d/4/d4c7d04ef9158787e8be774d5a05a682c9e072aa_2_1380x390.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">20200329_2225_RMSpropOptimizer_SmothL1Loss_1000iterations</span><span class=""informations"">1698×480 70.6 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>and with Adam optimizer (“loss_fn = nn.SmoothL1Loss()” ):</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/b9b81e4d411c4254222cd31b8f8c085be41102d6"" href=""https://discuss.pytorch.org/uploads/default/original/3X/b/9/b9b81e4d411c4254222cd31b8f8c085be41102d6.png"" title=""20200329_2007_AdamOptimizer_SmothL1Loss_1000iterations""><img alt=""20200329_2007_AdamOptimizer_SmothL1Loss_1000iterations"" data-base62-sha1=""quWNPRhTX7K22WcXXrT1UvyOycS"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/9/b9b81e4d411c4254222cd31b8f8c085be41102d6_2_10x10.png"" height=""198"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/9/b9b81e4d411c4254222cd31b8f8c085be41102d6_2_690x198.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/9/b9b81e4d411c4254222cd31b8f8c085be41102d6_2_690x198.png, https://discuss.pytorch.org/uploads/default/optimized/3X/b/9/b9b81e4d411c4254222cd31b8f8c085be41102d6_2_1035x297.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/b/9/b9b81e4d411c4254222cd31b8f8c085be41102d6_2_1380x396.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">20200329_2007_AdamOptimizer_SmothL1Loss_1000iterations</span><span class=""informations"">1670×480 60.6 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>The link to my code on GitHub:<br/><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicon.ico"" width=""16""/><NewLine><a href=""https://github.com/DanielF29/Simple-CartPole-NN-using-PyTorch-and-OpenAI-Gym"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars0.githubusercontent.com/u/36831342?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/DanielF29/Simple-CartPole-NN-using-PyTorch-and-OpenAI-Gym"" rel=""nofollow noopener"" target=""_blank"">DanielF29/Simple-CartPole-NN-using-PyTorch-and-OpenAI-Gym</a></h3><NewLine><p>Simple CartPole controled by a NN using PyTorch and OpenAI Gym - DanielF29/Simple-CartPole-NN-using-PyTorch-and-OpenAI-Gym</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Mehdi; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Delfox29; <NewLine> ,"REPLY_DATE 1: February 6, 2018,  4:42pm; <NewLine> REPLY_DATE 2: March 30, 2020,  2:26am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
73809,Understanding log_prob for Normal distribution in pytorch,2020-03-19T22:17:25.524Z,0,1539,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m currently trying to solve Pendulum-v0 from the openAi gym environment which has a continuous action space. As a result, I need to use a Normal Distribution to sample my actions. What I don’t understand is the dimension of the log_prob when using it :</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch.distributions import Normal <NewLine><NewLine>means = torch.tensor([[0.0538],<NewLine>        [0.0651]])<NewLine>stds = torch.tensor([[0.7865],<NewLine>        [0.7792]])<NewLine><NewLine>dist = Normal(means, stds)<NewLine>a = torch.tensor([1.2,3.4])<NewLine>d = dist.log_prob(a)<NewLine>print(d.size())<NewLine></code></pre><NewLine><p>I was expecting a tensor of size 2 (one log_prob for each actions) but it output a tensor of size(2,2).</p><NewLine><p>However, when using a Categorical distribution for discrete environment the log_prob has the expected size:</p><NewLine><pre><code class=""lang-auto"">logits = torch.tensor([[-0.0657, -0.0949],<NewLine>        [-0.0586, -0.1007]])<NewLine><NewLine>dist = Categorical(logits = logits)<NewLine>a = torch.tensor([1, 1])<NewLine>print(dist.log_prob(a).size())<NewLine></code></pre><NewLine><p>give me a tensor a size(2).</p><NewLine><p>Why is the log_prob for Normal distribution of a different size ?</p><NewLine></div>",https://discuss.pytorch.org/u/sabeaussan,(sabeaussan),sabeaussan,"March 19, 2020, 10:17pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello sabeaussan!</p><NewLine><aside class=""quote no-group quote-modified"" data-full=""true"" data-post=""1"" data-topic=""73809"" data-username=""sabeaussan""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/dec6dc/40.png"" width=""20""/> sabeaussan:</div><NewLine><blockquote><NewLine><p>What I don’t understand is the dimension of the log_prob when using it:</p><NewLine><pre><code class=""lang-auto"">means = torch.tensor([[0.0538],<NewLine>        [0.0651]])<NewLine>stds = torch.tensor([[0.7865],<NewLine>        [0.7792]])<NewLine><NewLine>dist = Normal(means, stds)<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>You have created <code>dist</code> as a “batch” of two gaussian distributions.<br/><NewLine>(The first has <code>mean = 0.0538</code> and <code>std = 0.7865</code>, and the second<br/><NewLine>has <code>mean = 0.0651</code> and <code>std = 0.7792</code>.)</p><NewLine><aside class=""quote no-group""><NewLine><blockquote><NewLine><p>I was expecting a tensor of size 2 (one log_prob for each actions) but it output a tensor of size(2,2).</p><NewLine></blockquote><NewLine></aside><NewLine><p>You get two <code>log_prob</code>s (one for each gaussian in your “batch”) for<br/><NewLine>each of the two elements of the tensor <code>a</code>, for a total of four <code>log_prob</code>s.</p><NewLine><p>This is illustrated by some additions I made to the code you posted:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>from torch.distributions import Normal<NewLine>torch.__version__<NewLine><NewLine>torch.manual_seed (2020)<NewLine><NewLine>means = torch.FloatTensor([[0.0538],<NewLine>        [0.0651]])<NewLine>stds = torch.FloatTensor([[0.7865],<NewLine>        [0.7792]])<NewLine><NewLine>dist = Normal(means, stds)<NewLine>a = torch.FloatTensor([1.2,3.4])<NewLine>d = dist.log_prob(a)<NewLine>print(d.size())<NewLine><NewLine>disB = Normal (torch.FloatTensor ([0.0]), torch.FloatTensor ([1.0]))<NewLine><NewLine>dist.log_prob (a)<NewLine>disB.log_prob (a)<NewLine><NewLine>dist.sample()<NewLine>disB.sample()<NewLine>dist.sample_n (3)<NewLine>disB.sample_n (3)<NewLine><NewLine>b = torch.FloatTensor([5.6])<NewLine>dist.log_prob (b)<NewLine>disB.log_prob (b)<NewLine></code></pre><NewLine><p>And here are the results:</p><NewLine><pre><code class=""lang-nohighlight"">&gt;&gt;&gt; import torch<NewLine>&gt;&gt;&gt; from torch.distributions import Normal<NewLine>&gt;&gt;&gt; torch.__version__<NewLine>'0.3.0b0+591e73e'<NewLine>&gt;&gt;&gt;<NewLine>&gt;&gt;&gt; torch.manual_seed (2020)<NewLine>&lt;torch._C.Generator object at 0x000002A2CFE06630&gt;<NewLine>&gt;&gt;&gt;<NewLine>&gt;&gt;&gt; means = torch.FloatTensor([[0.0538],<NewLine>...         [0.0651]])<NewLine>&gt;&gt;&gt; stds = torch.FloatTensor([[0.7865],<NewLine>...         [0.7792]])<NewLine>&gt;&gt;&gt;<NewLine>&gt;&gt;&gt; dist = Normal(means, stds)<NewLine>&gt;&gt;&gt; a = torch.FloatTensor([1.2,3.4])<NewLine>&gt;&gt;&gt; d = dist.log_prob(a)<NewLine>&gt;&gt;&gt; print(d.size())<NewLine>torch.Size([2, 2])<NewLine>&gt;&gt;&gt;<NewLine>&gt;&gt;&gt; disB = Normal (torch.FloatTensor ([0.0]), torch.FloatTensor ([1.0]))<NewLine>&gt;&gt;&gt;<NewLine>&gt;&gt;&gt; dist.log_prob (a)<NewLine><NewLine>-1.7407 -9.7294<NewLine>-1.7301 -9.8282<NewLine>[torch.FloatTensor of size 2x2]<NewLine><NewLine>&gt;&gt;&gt; disB.log_prob (a)<NewLine><NewLine>-1.6389<NewLine>-6.6989<NewLine>[torch.FloatTensor of size 2]<NewLine><NewLine>&gt;&gt;&gt;<NewLine>&gt;&gt;&gt; dist.sample()<NewLine><NewLine> 1.0269<NewLine>-0.6832<NewLine>[torch.FloatTensor of size 2x1]<NewLine><NewLine>&gt;&gt;&gt; disB.sample()<NewLine><NewLine> 1.5415<NewLine>[torch.FloatTensor of size 1]<NewLine><NewLine>&gt;&gt;&gt; dist.sample_n (3)<NewLine><NewLine>(0 ,.,.) =<NewLine> -0.2670<NewLine>  0.7512<NewLine><NewLine>(1 ,.,.) =<NewLine>  0.0954<NewLine>  0.1236<NewLine><NewLine>(2 ,.,.) =<NewLine>  0.4295<NewLine> -0.4615<NewLine>[torch.FloatTensor of size 3x2x1]<NewLine><NewLine>&gt;&gt;&gt; disB.sample_n (3)<NewLine><NewLine>-2.1489<NewLine>-1.1463<NewLine>-0.2720<NewLine>[torch.FloatTensor of size 3x1]<NewLine><NewLine>&gt;&gt;&gt;<NewLine>&gt;&gt;&gt; b = torch.FloatTensor([5.6])<NewLine>&gt;&gt;&gt; dist.log_prob (b)<NewLine><NewLine>-25.5424<NewLine>-25.8980<NewLine>[torch.FloatTensor of size 2x1]<NewLine><NewLine>&gt;&gt;&gt; disB.log_prob (b)<NewLine><NewLine>-16.5989<NewLine>[torch.FloatTensor of size 1]<NewLine></code></pre><NewLine><p>Best.</p><NewLine><p>K. Frank</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/KFrank; <NewLine> ,"REPLY_DATE 1: March 20, 2020,  7:03pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
27386,Categorical(probs).sample() generates RuntimeError: invalid argument 2: invalid multinomial distribution (encountering probability entry &lt; 0),2018-10-16T15:48:59.518Z,10,4139,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I’m working on an adaptation of the <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py"" rel=""nofollow noopener"">pytorch actor_critic_py</a> for an RRBot example within an OpenAI ROS Kinetic Gazebo 7 environment.</p><NewLine><pre><code class=""lang-python"">  def select_action(self, state):<NewLine>    state = torch.from_numpy(state).float()<NewLine>    probs, state_value = self.model(state)<NewLine>    m = Categorical(probs)<NewLine>    action = m.sample()<NewLine>    self.model.saved_actions.append(self.saved_action(m.log_prob(action), state_value))<NewLine>    return action.item()<NewLine></code></pre><NewLine><p>At some point, either during initialization or when the RRBot swing up task is approximately in this state during simulation:</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/5e252e48252d691098dcff57c77f81956d528497"" href=""https://discuss.pytorch.org/uploads/default/original/2X/5/5e252e48252d691098dcff57c77f81956d528497.png"" title=""rrbot-example.png""><img alt=""rrbot-example"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/5e252e48252d691098dcff57c77f81956d528497_2_10x10.png"" height=""408"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/5e252e48252d691098dcff57c77f81956d528497_2_690x408.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/5e252e48252d691098dcff57c77f81956d528497_2_690x408.png, https://discuss.pytorch.org/uploads/default/optimized/2X/5/5e252e48252d691098dcff57c77f81956d528497_2_1035x612.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/5/5e252e48252d691098dcff57c77f81956d528497.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">rrbot-example.png</span><span class=""informations"">1280×757 55.7 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>I consistently get the following run-time error:</p><NewLine><pre><code class=""lang-bash"">[WARN] [1539704325.305267, 1074.472000]: PUBLISHING REWARD...<NewLine>[WARN] [1539704325.305413, 1074.472000]: PUBLISHING REWARD...DONE=0.0,EP=13<NewLine>Traceback (most recent call last):<NewLine>  File ""/project/ros-kinetic-deep-rl/catkin_ws/src/rrbot_openai_ros_tutorial/src/rrbot_v0_start_training_actor_critic.py"", line 204, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""/project/ros-kinetic-deep-rl/catkin_ws/src/rrbot_openai_ros_tutorial/src/rrbot_v0_start_training_actor_critic.py"", line 125, in main<NewLine>    action = agent.select_action(state)<NewLine>  File ""/project/ros-kinetic-deep-rl/catkin_ws/src/rrbot_openai_ros_tutorial/src/rrbot_v0_start_training_actor_critic.py"", line 64, in select_action<NewLine>    action = m.sample()<NewLine>  File ""/usr/local/lib/python2.7/dist-packages/torch/distributions/categorical.py"", line 110, in sample<NewLine>    sample_2d = torch.multinomial(probs_2d, 1, True)<NewLine>RuntimeError: invalid argument 2: invalid multinomial distribution (encountering probability entry &lt; 0) at /pytorch/aten/src/TH/generic/THTensorRandom.cpp:297<NewLine>[DEBUG] [1539704325.306117, 1074.472000]: END Reseting RobotGazeboEnvironment<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/edowson,(Elvis Dowson),edowson,"October 19, 2018,  5:52am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> Any thoughts on what might be causing this? I tried changing the learning rate, but it still crashes with the above message.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s a strange issue, as the <code>probs</code> passed to <code>Categorical</code> are created using <code>F.softmax(action_scores, dim=-1)</code>. Could you check if the dimension is set properly for your <code>action_scores</code>? <code>F.softmax</code> should not return negative values.<br/><NewLine>Also, could you add a print statement of <code>action_states</code> and <code>probs</code> just for the sake of debugging?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I had a similar prob for some reinforcement learning prob. the reason with me was: the softmax had turned into a vector of lovely NaNs. then categorical fails with above error.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Were the values passed to softmax already NaNs or did the softmax op created them?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>So could you solve this prob?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did anyone solve this problem?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you see the same initial error message or do you encouter NaN values?<br/><NewLine>As the error message states, negative probabilities are not supported.</p><NewLine><p>CC <a class=""mention"" href=""/u/cuiguangwu"">@cuiguangwu</a></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">def getAction2(self, state):<NewLine>        state = torch.FloatTensor(state) <NewLine>        logits, _ = self.model2.forward(state)<NewLine>        dist = F.softmax(logits, dim = -1)<NewLine>        self.check2.append(dist)<NewLine>        probs = Categorical(dist)<NewLine>        return probs.sample()<NewLine></code></pre><NewLine><p>i have collected the output of the softmax at the point where the error is raised in the code. It has generated Nan values.<br/><NewLine>The error:</p><NewLine><pre><code class=""lang-auto""><NewLine>  File ""&lt;ipython-input-12-986f834e9152&gt;"", line 1, in &lt;module&gt;<NewLine>    runfile('C:/Users/Prudhvinath.DESKTOP-09Q8801/sciebo/Thesis/JSSP/TwoAgents/20JobsTwoAgents.py', wdir='C:/Users/Prudhvinath.DESKTOP-09Q8801/sciebo/Thesis/JSSP/TwoAgents')<NewLine><NewLine>  File ""C:\Users\Prudhvinath.DESKTOP-09Q8801\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 827, in runfile<NewLine>    execfile(filename, namespace)<NewLine><NewLine>  File ""C:\Users\Prudhvinath.DESKTOP-09Q8801\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile<NewLine>    exec(compile(f.read(), filename, 'exec'), namespace)<NewLine><NewLine>  File ""C:/Users/Prudhvinath.DESKTOP-09Q8801/sciebo/Thesis/JSSP/TwoAgents/20JobsTwoAgents.py"", line 406, in &lt;module&gt;<NewLine>    generateEpisode(x)<NewLine><NewLine>  File ""C:/Users/Prudhvinath.DESKTOP-09Q8801/sciebo/Thesis/JSSP/TwoAgents/20JobsTwoAgents.py"", line 337, in generateEpisode<NewLine>    action = agent.getAction2(state)<NewLine><NewLine>  File ""C:/Users/Prudhvinath.DESKTOP-09Q8801/sciebo/Thesis/JSSP/TwoAgents/20JobsTwoAgents.py"", line 179, in getAction2<NewLine>    return probs.sample()<NewLine><NewLine>  File ""C:\Users\Prudhvinath.DESKTOP-09Q8801\Anaconda3\lib\site-packages\torch\distributions\categorical.py"", line 107, in sample<NewLine>    sample_2d = torch.multinomial(probs_2d, 1, True)<NewLine><NewLine>RuntimeError: invalid multinomial distribution (encountering probability entry &lt; 0)<NewLine></code></pre><NewLine><p>checking for the output of the softmax and I have already made an agent instance.</p><NewLine><pre><code class=""lang-auto"">In [13]: agent.check2<NewLine>Out[13]: [tensor([nan, nan], grad_fn=&lt;SoftmaxBackward&gt;)]<NewLine></code></pre><NewLine><p>so the softmax is creating the Nans.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>The nans are not generated in by softmax. The model itself is generating the nan because of the exploding gradients due to the learning rate.<br/><NewLine>I tried to check my network weights. It has generated <code> nan</code></p><NewLine><pre><code class=""lang-auto"">In [29]: for param in agent.model1.parameters():<NewLine>    print(param.data)<NewLine>    <NewLine>tensor([[    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,<NewLine>             nan],<NewLine>        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,<NewLine>             nan],<NewLine>        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,<NewLine>             nan],<NewLine>        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,<NewLine>             nan],<NewLine>        [-0.7366, -0.1832, -0.1841,  0.3141,  0.0334, -0.0575, -0.0015,  0.0069,<NewLine>         -0.1040],<NewLine>        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,<NewLine>             nan],<NewLine>        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,<NewLine>             nan],<NewLine>        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,<NewLine>             nan],<NewLine>        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,<NewLine>             nan],<NewLine>        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,<NewLine>             nan]])<NewLine>tensor([   nan,    nan,    nan,    nan, 0.1520,    nan,    nan,    nan,    nan,<NewLine>           nan])<NewLine>tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],<NewLine>        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])<NewLine>tensor([nan, nan])<NewLine>tensor([[ 4.5539e+00, -4.2161e+00,  2.1670e-01, -2.9069e+00, -4.2264e+00,<NewLine>         -4.5930e+00, -3.9898e+00,  4.3225e+00,  4.2810e+00],<NewLine>        [ 4.4970e+00, -4.4960e+00,  3.8166e+00, -2.9493e+00, -4.6124e+00,<NewLine>         -4.6803e+00, -3.9606e+00,  4.0320e+00,  4.2299e+00],<NewLine>        [-3.0815e-01, -8.5769e-02,  3.1897e-01,  3.0792e-01, -7.2797e-02,<NewLine>          1.2266e-02,  2.3653e-01,  2.4632e-01,  3.8586e-02],<NewLine>        [-8.0415e-02,  9.2836e-02,  3.7588e-01,  3.3804e-01,  1.5777e-02,<NewLine>          7.4958e-02, -6.0354e-02,  8.1592e-02, -3.8448e-01],<NewLine>        [-1.4028e-01,  1.0577e-01,  3.7370e-01,  2.5323e-01,  1.0640e-01,<NewLine>         -1.5946e-01,  1.5165e-01, -1.5983e-04,  1.1991e-01],<NewLine>        [ 3.5704e-02,  8.9192e-02, -4.2397e-02, -1.8162e-01,  2.7302e-02,<NewLine>         -8.2681e-02,  2.4023e-01,  1.8748e-01, -3.6148e-01],<NewLine>        [ 4.4627e+00, -4.1383e+00,  1.0344e+00, -2.9011e+00, -4.5565e+00,<NewLine>         -4.4978e+00, -3.5755e+00,  4.2879e+00,  4.3042e+00],<NewLine>        [ 4.0489e+00, -3.4591e+00,  1.2957e+00, -2.6597e+00, -3.5383e+00,<NewLine>         -3.4929e+00, -3.4063e+00,  3.7202e+00,  4.0741e+00],<NewLine>        [-5.6180e-02,  3.9072e-02, -5.6076e-02,  3.0225e-01, -9.5747e-02,<NewLine>          1.5115e-01,  1.0766e-02,  2.7571e-01, -3.0291e-01],<NewLine>        [ 4.2704e+00, -4.1625e+00,  9.1064e-01, -3.1105e+00, -4.2028e+00,<NewLine>         -4.3451e+00, -3.8066e+00,  4.3735e+00,  4.4310e+00]])<NewLine>tensor([ 4.3880,  3.9937, -0.0354, -0.0842, -0.1234, -0.2485,  4.0131,  4.0434,<NewLine>        -0.3865,  4.1969])<NewLine>tensor([[ 4.3842,  4.5874, -0.0854, -0.1604, -0.2546, -0.0998,  4.6371,  4.2951,<NewLine>         -0.0997,  4.7081]])<NewLine>tensor([3.3452])<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>The problem here I think is the exploding or vanishing gradient problem. I am trying to do gradient clipping to get the graident calculated without exploding.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""11"" data-topic=""27386"" data-username=""reddymap""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/reddymap/40/18931_2.png"" width=""20""/> reddymap:</div><NewLine><blockquote><NewLine><p>gradient clipping</p><NewLine></blockquote><NewLine></aside><NewLine><p>Hey did you have success? I have the same problem.</p><NewLine><p>How to clip the gradients with pytorch?</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could use <a href=""https://pytorch.org/docs/stable/nn.html?highlight=clip_grad#torch.nn.utils.clip_grad_norm_""><code>torch.nn.utils.clip_grad_norm_</code></a> or <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.utils.clip_grad_value_""><code>torch.nn.utils.clip_grad_value_</code></a>.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Great thanks <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>I currently use it like this:</p><NewLine><pre><code class=""lang-auto"">        self.optimizer.zero_grad()<NewLine>        loss = self.criterion(log_action_probabilities, rewards)<NewLine>        loss.backward()<NewLine>        # clipping to prevent nans:<NewLine>        # see https://discuss.pytorch.org/t/proper-way-to-do-gradient-clipping/191/6<NewLine>        torch.nn.utils.clip_grad_norm_(self.parameters(), 5)<NewLine>        self.optimizer.step()<NewLine>        self.log_action_probabilities.clear()<NewLine>        self.rewards.clear()<NewLine></code></pre><NewLine><p>With the clipping I try to get rid of this error:</p><NewLine><pre><code class=""lang-auto""><NewLine>    action_idx = distribution.sample()      #<NewLine>  File ""/home/markus/Documents/06_Software_Projects/mcts/mcts_env/lib/python3.6/site-packages/torch/distributions/categorical.py"", line 107, in sample<NewLine>    sample_2d = torch.multinomial(probs_2d, 1, True)<NewLine>RuntimeError: invalid multinomial distribution (encountering probability entry &lt; 0)<NewLine></code></pre><NewLine><p>Is this the right case / way to use the clipping?</p><NewLine><p>Does anyone know other methods of handling the above error such as: Using a different network (did not help for me), using other optimizers, other loss functions?</p><NewLine><p>My current problem is that without a lr of lower than 0.1 my algorithm seems to learn nothing. But with this high lr I get the above error. Clipping in the way above does not help:</p><NewLine><pre><code class=""lang-auto"">    def forward(self, state: torch.tensor, legalCards: torch.tensor):<NewLine>        state = state.resize_(180)<NewLine>        probs = self.network(torch.FloatTensor(state))<NewLine>        probs = probs * legalCards<NewLine>        distribution = Categorical(probs)<NewLine>        print(probs)<NewLine>        print(distribution)            #<NewLine>        action_idx = distribution.sample()      #<NewLine>        log_action_probability = distribution.log_prob(action_idx)<NewLine>        self.log_action_probabilities.append(log_action_probability)<NewLine></code></pre><NewLine><p>Will produce this output after some time:</p><NewLine><pre><code class=""lang-auto"">tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,<NewLine>        0.0000e+00, 0.0000e+00, 0.0000e+00, 5.1009e-16, 0.0000e+00, 0.0000e+00,<NewLine>        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,<NewLine>        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,<NewLine>        0.0000e+00, 0.0000e+00, 5.0108e-22, 0.0000e+00, 0.0000e+00, 0.0000e+00,<NewLine>        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,<NewLine>        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,<NewLine>        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,<NewLine>        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,<NewLine>        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],<NewLine>       grad_fn=&lt;MulBackward0&gt;)<NewLine>Categorical(probs: torch.Size([60]))<NewLine><NewLine><NewLine><NewLine>tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,<NewLine>        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,<NewLine>        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],<NewLine>       grad_fn=&lt;MulBackward0&gt;)<NewLine>Categorical(probs: torch.Size([60]))<NewLine><NewLine></code></pre><NewLine><p>Maybe the problem is that <strong>probs</strong> goes to zero…</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not really familiar with RL, but could you clip <code>probs_2d</code> to valid probabilities, i.e. avoid negative values?</p><NewLine><p>The usage of grad clipping looks alright.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hm sry I am quite new what exactly do you mean by clip probs_2d ??? How would I do that?</p><NewLine><p>like this?:</p><NewLine><pre><code class=""lang-auto""><NewLine>        probs = self.network(torch.FloatTensor(state))<NewLine>        probs = probs * legalCards<NewLine>        probs =   torch.nn.utils.clip_grad_norm_(probs , 5)<NewLine>        distribution = Categorical(probs)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>I meant something like <code>torch.clamp(probs_2d, 0, 1)</code>, which would propagate the gradient for values inside the interval. But as I said, I’m not sure if that’s a valid approach for your method.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/edowson; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/alextheoldgreyhorse; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/cuiguangwu; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/reddymap; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/reddymap; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/reddymap; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/reddymap; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/CesMak; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/CesMak; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/CesMak; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: October 19, 2018,  6:20am; <NewLine> REPLY_DATE 2: October 19, 2018,  1:21pm; <NewLine> REPLY_DATE 3: June 15, 2019, 11:41pm; <NewLine> REPLY_DATE 4: June 16, 2019, 12:22am; <NewLine> REPLY_DATE 5: January 1, 2020,  4:56pm; <NewLine> REPLY_DATE 6: January 2, 2020, 10:12am; <NewLine> REPLY_DATE 7: January 3, 2020,  7:10am; <NewLine> REPLY_DATE 8: January 3, 2020, 11:40am; <NewLine> REPLY_DATE 9: January 3, 2020,  3:27pm; <NewLine> REPLY_DATE 10: January 6, 2020,  3:18pm; <NewLine> REPLY_DATE 11: March 18, 2020,  8:33pm; <NewLine> REPLY_DATE 12: March 19, 2020,  3:21am; <NewLine> REPLY_DATE 13: March 19, 2020,  7:17am; <NewLine> REPLY_DATE 14: March 19, 2020,  8:17am; <NewLine> REPLY_DATE 15: March 19, 2020, 11:36am; <NewLine> REPLY_DATE 16: March 19, 2020,  6:56pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> 
73434,CUDA multiprocessing in jupyter notebook is possible?,2020-03-16T19:01:27.098Z,0,261,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to run CUDA multiprocessing in jupyter notebook?</p><NewLine><p>Currently I am implementing A3C in notebook.</p><NewLine><p>For running multiprocessing with CUDA, <code>mp.set_start_method('spawn')</code> is required.(mp is torch.multiprocessing)</p><NewLine><p>and <code>mp.set_start_method('spawn')</code> is required <code>__name__ == ""__main__""</code>.</p><NewLine><p>I have known <code>__name__ == ""__main__""</code> is used in .py not in .ipynb</p><NewLine><p>Is there any way instead of using  <code>__name__ == ""__main__""</code>?</p><NewLine></div>",https://discuss.pytorch.org/u/HyoungsungKim,(Hyoungsung Kim),HyoungsungKim,"March 16, 2020,  7:02pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you sure the <code>__name__ == ""__main__""</code> is required and there’s no easy alternative to use here ? Maybe posting the actual could would help since there might be an easy way to modify it as to not require using <code>set_start_method</code> (E.g. by using DataParallel instead of mp)</p><NewLine><p>That being said, there is a way to run code in a notebook as if it were ran under main:</p><NewLine><pre><code class=""lang-auto"">exec(compile('your code here', '__main__.py', 'exec'))<NewLine></code></pre><NewLine><p>Though I’m not sure if this will actually produce the expected behavior here and I don’t really have a multiple GPU machine handy right now to test.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/George3d6; <NewLine> ,"REPLY_DATE 1: March 16, 2020,  9:49pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
72418,Torch.onnx._export fails,2020-03-07T20:02:39.516Z,1,343,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey there,</p><NewLine><p>I have this model:</p><NewLine><pre><code class=""lang-auto"">class PlayingPolicy(nn.Module):<NewLine><NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine><NewLine>        # Parameters<NewLine>        self.gamma = 0.9 # reward discount factor<NewLine><NewLine>        # Network<NewLine>        in_channels = 4 # four players<NewLine>        out_channels = 8<NewLine><NewLine>        self.conv418 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 4), stride=1, dilation=(1, 8))<NewLine>        self.conv881 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 8), stride=8, dilation=(1, 1))<NewLine>        self.classify = nn.Linear(104, len(belot.cards))# len(belot.cards)=32 #  KARO, HERC, PIK, TREF, dalje<NewLine><NewLine>        # Optimizer<NewLine>        self.optimizer = optim.SGD(self.parameters(), lr=1e-2, momentum=0.9)<NewLine>        # ...<NewLine><NewLine>    def forward(self, state: np.ndarray, bidder, trump, legalCards):<NewLine>        # datatype: &lt;class 'numpy.ndarray'&gt;, &lt;enum 'Suit'&gt;, &lt;class 'int'&gt;, &lt;class 'list'&gt;<NewLine>        # ....<NewLine></code></pre><NewLine><p>and when I try to</p><NewLine><pre><code class=""lang-auto"">            input_nn = (playingState, bidderIndex, trumpIndex, [])<NewLine>            torch.onnx._export(policy, input_nn, path,  export_params=True)<NewLine></code></pre><NewLine><p>I get this error:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""train.py"", line 64, in &lt;module&gt;<NewLine>    train_player()<NewLine>  File ""train.py"", line 38, in train_player<NewLine>    game.saveNetworks()<NewLine>  File ""/home/markus/Documents/06_Software_Projects/belot/game/play.py"", line 402, in saveNetworks<NewLine>    player.saveNetwork()<NewLine>  File ""/home/markus/Documents/06_Software_Projects/belot/players/PlayerRL/player.py"", line 183, in saveNetwork<NewLine>    torch.onnx._export(policy, input_nn, path,  export_params=True)<NewLine>  File ""/home/markus/Documents/06_Software_Projects/belot/belot_env/lib/python3.6/site-packages/torch/onnx/__init__.py"", line 26, in _export<NewLine>    result = utils._export(*args, **kwargs)<NewLine>  File ""/home/markus/Documents/06_Software_Projects/belot/belot_env/lib/python3.6/site-packages/torch/onnx/utils.py"", line 416, in _export<NewLine>    fixed_batch_size=fixed_batch_size)<NewLine>  File ""/home/markus/Documents/06_Software_Projects/belot/belot_env/lib/python3.6/site-packages/torch/onnx/utils.py"", line 279, in _model_to_graph<NewLine>    graph, torch_out = _trace_and_get_graph_from_model(model, args, training)<NewLine>  File ""/home/markus/Documents/06_Software_Projects/belot/belot_env/lib/python3.6/site-packages/torch/onnx/utils.py"", line 236, in _trace_and_get_graph_from_model<NewLine>    trace_graph, torch_out, inputs_states = torch.jit._get_trace_graph(model, args, _force_outplace=True, _return_inputs_states=True)<NewLine>  File ""/home/markus/Documents/06_Software_Projects/belot/belot_env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 277, in _get_trace_graph<NewLine>    outs = ONNXTracedModule(f, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)<NewLine>  File ""/home/markus/Documents/06_Software_Projects/belot/belot_env/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/markus/Documents/06_Software_Projects/belot/belot_env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 332, in forward<NewLine>    in_vars, in_desc = _flatten(args)<NewLine>RuntimeError: Only tuples, lists and Variables supported as JIT inputs/outputs. Dictionaries and strings are also accepted but their usage is not recommended. But got unsupported type numpy.ndarray<NewLine></code></pre><NewLine><p>any ideas how to solve this one?</p><NewLine><p>I alreay checked <a href=""https://github.com/lanpa/tensorboardX/issues/109"" rel=""nofollow noopener"">here</a> but it did not help me…</p><NewLine><p>PS: My Model<br/><NewLine>PlayingPolicy(<br/><NewLine>(conv418): Conv2d(4, 8, kernel_size=(3, 4), stride=(1, 1), dilation=(1, 8))<br/><NewLine>(conv881): Conv2d(4, 8, kernel_size=(3, 8), stride=(8, 8))<br/><NewLine>(classify): Linear(in_features=104, out_features=32, bias=True)<br/><NewLine>(criterion): PolicyGradientLoss()<br/><NewLine>)</p><NewLine></div>",https://discuss.pytorch.org/u/CesMak,(Markus Lamprecht),CesMak,"March 7, 2020,  8:06pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>state</code> is passed as a numpy array, which is not supported.<br/><NewLine>Try to pass it as a tensor and check, if it’s working.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks,</p><NewLine><p>that was the solution:</p><NewLine><p><strong>ALL INPUTS and OUTPUTS (of forward) have to be Tensors</strong></p><NewLine><pre><code class=""lang-auto"">input_nn = (torch.tensor(playingState, dtype=torch.float32), torch.tensor(bidderIndex, dtype=torch.int32), torch.tensor(trumpIndex), mask)<NewLine>            torch.onnx._export(policy, input_nn, path,  export_params=True)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">        forward(....)<NewLine>        #.....<NewLine>        returned_tensor = torch.zeros(1, 2)<NewLine>        returned_tensor[:, 0] = action_idx.item()<NewLine>        returned_tensor[:, 1] = log_action_probability<NewLine>        return returned_tensor<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/CesMak; <NewLine> ,"REPLY_DATE 1: March 8, 2020, 10:47am; <NewLine> REPLY_DATE 2: March 8, 2020, 10:48am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
50879,How to use DataLoader for ReplayBuffer,2019-07-17T23:27:39.056Z,2,725,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In RL, the data is not static but keeps growing due to new samples explored by the agent.</p><NewLine><p>I would like to use <code>DataLoader</code> for preparing/loading data from a replay buffer more efficiently. However, it seems that the concept of <code>DataLoader</code> is not well designed for non-stationary data.</p><NewLine><p>So, what would be the best way to extract/load/transform data from a large replay buffer efficiently?</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/zer0ne,,zer0ne,"July 17, 2019, 11:28pm",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Bringing this thread up.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you come up with a good solution? I am running into the same problem… The dataloader is usable if you set the number of workers to 0, but that defeats the actual usefulness of a dataloader.</p><NewLine><p>At the moment I am just sampling from large tensors, so I do not have a good solution. I am thinking of using Ray and starting the batch collection process that way.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://pytorch.org/docs/stable/data.html#iterable-style-datasets""><code>IterableDataset</code></a> was introduced recently, which could be a good candidate for your use case.<br/><NewLine>I’m not deeply familiar with RL, but based on the descriptions in this topic, it seems the map-style datasets have some limitations in RL setups.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your suggestion. As a rough overview of what we would need for RL is to sample from a growing dataset. For every batch, newly added samples in the dataset should be considered as well.</p><NewLine><p>The problem I had with the regular map-style approach was that setting the number of workers to &gt; 0 spawns new processes which seem to receive a deep copy of the sampler.  The sampler that they received assumes the dataset size to be constant, which does not work for RL. I thought of somehow fixing this by writing my own sampler, but I do not know if this is going to work. Concretely, I get the error:</p><NewLine><pre><code class=""lang-auto"">    return iter(torch.randint(high=n, size=(self.num_samples,), dtype=torch.int64).tolist())<NewLine>RuntimeError: invalid argument 2: max must be greater than min, but got: min = 0, max = 0 at /pytorch/aten/src/TH/generic/THTensorRandom.cpp:43<NewLine></code></pre><NewLine><p>So the problem is that self.num_samples stays at 0. Is there a way to let the sampler update its self.num_samples? Or could a custom sampler maybe access the <strong>len</strong> method of the current dataset?</p><NewLine><p>I also looked into the IterableDataset and do not yet quite see how it would be used for RL. It seems to me that the dataset that will be passed to the worker subprocesses will not update their length, but I haven’t tried it yet. I am thinking that maybe I could modify the <strong>iter</strong> method to return the actual length of the current dataset? But then the dataset would only return the indices and obviously I want it to return the states, actions etc and collate them…</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Edit: Do not use this! I messed up. Because I did not specify the Dataset to be an IterableDataset, the <strong>iter</strong> method was not used. Instead it simply iterated over indices in the dataset instead of sampling random indices.<br/><NewLine>I still cannot find a way to make it efficiently run with a dataloader. Multiprocessing works, but is 8x slower with 2 workers and 16x slower with 8 workers.</p><NewLine><p><a class=""mention"" href=""/u/zer0ne"">@zer0ne</a> <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> I finally managed to get it working. One can use the standard Dataset (not the IterableDataset!) by implementing the methods:</p><NewLine><pre><code class=""lang-auto"">def __getitem__(self, key):<NewLine>    pass<NewLine>    # return single transition: state, action, reward, next_state, done<NewLine></code></pre><NewLine><p>and</p><NewLine><pre><code class=""lang-auto"">    def __iter__(self):<NewLine>        while True:<NewLine>            idx = random.randint(0, len(self) - 1)<NewLine>            yield self[idx]<NewLine></code></pre><NewLine><p>Then we need to instantiate this class and create a dataloader based off it. The dataloader can then be used in this way:</p><NewLine><pre><code class=""lang-auto"">    def sample(self):<NewLine>        try:<NewLine>            out = next(self.iter)<NewLine>        except StopIteration:<NewLine>            self.iter = iter(self.dataloader)<NewLine>            out = next(self.iter)<NewLine>        return out<NewLine></code></pre><NewLine><p>This will work for any number of workers!<br/><NewLine>Of course a custom <strong>collate_fn</strong>  needs to be passed into the Dataloader creation.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zer0ne; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/NotNANtoN; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/NotNANtoN; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/NotNANtoN; <NewLine> ,"REPLY_DATE 1: July 27, 2019,  7:46pm; <NewLine> REPLY_DATE 2: January 25, 2020,  3:13pm; <NewLine> REPLY_DATE 3: January 25, 2020, 10:56pm; <NewLine> REPLY_DATE 4: January 27, 2020,  2:48pm; <NewLine> REPLY_DATE 5: February 24, 2020,  4:44pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
70631,RAM Issues during training,2020-02-21T21:41:10.852Z,2,120,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m experiencing severe memory leaks in my DDPG algorithm found <a href=""https://github.com/kangtinglee/CarRacing"" rel=""nofollow noopener"">here</a>.</p><NewLine><p>Even after the first couple episodes, my RAM usage goes up by almost a whole gb.</p><NewLine><p>I’ve tried shutting off different parts of my code such as putting no_grad() wherever possible, limiting my replay buffer to only 500 entries which will definitely not reach the memory usages I am seeing, and just not training the agent at all and only doing inference.</p><NewLine><p>I’ve also tried using other versions of torch such as 1.3 but the result is the same.</p><NewLine><p>Please help, any direction is appreciated. I understand that this is not a debugging session, but I appreciate any help at all <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/kangtinglee,(Marcus Lee),kangtinglee,"February 21, 2020,  9:41pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>A quick glance at the code does not reveal anything.<br/><NewLine>Given how your replay buffer is implemented, it is expected to grow for a bit until it is full.</p><NewLine><p>Could you be clearer about what you tried and the effect?<br/><NewLine>Did you mean that when you don’t train, you don’t see this behavior?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the very swift reply.</p><NewLine><p>What I’m trying to say is that my replay bufffer has no chance to accumulate to a size that it will even become a problem before my computer runs out of memory. How I determined that my replay buffer is not the problem is by limiting its capacity to only a few hundred episodes, since I’m only storing (96,96) 2D arrays in it, a few hundred of those can’t have caused the memory problem.</p><NewLine><p>When I don’t train, I still continue to see this behaviour. By not training I mean that I comment out all parts of the code that attempts to do any form of backprop or loss estimation, so the only time the network is being invoked is when I do the forward pass.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><ul><NewLine><li>And if you replace the forward pass by a just returning a random output?</li><NewLine><li>And if you replace all the calls to the gym simulator to just return a random value?</li><NewLine></ul><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello alban,</p><NewLine><p>Thanks for helping me with my problem! The problem went away with a clean install of PyTorch and my cuda/cudnn dependencies!</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Awesome ! Thats a simple enough solution !</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kangtinglee; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/kangtinglee; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: February 21, 2020,  9:47pm; <NewLine> REPLY_DATE 2: February 21, 2020, 10:00pm; <NewLine> REPLY_DATE 3: February 21, 2020, 10:09pm; <NewLine> REPLY_DATE 4: February 22, 2020,  1:27pm; <NewLine> REPLY_DATE 5: February 23, 2020,  6:37pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> 
70588,"Q-values increase, also with negative TD",2020-02-21T11:59:38.157Z,0,68,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I set up a simple DQL task, but it wasn’t learning, so I decreased the number of steps and samples to 1 to track down the error.</p><NewLine><p>The TD values are calculated correctly, but no matter if they are positive or negative, after the update, the corresponding Q-value increased. I use :</p><NewLine><p>optimiser = torch.optim.Adam(model.parameters(), lr=1e-3, eps=1e-3, amsgrad=True)<br/><NewLine>gamma = 0.9<br/><NewLine>for episode in range(episodes):<br/><NewLine>TD = []<br/><NewLine>…<br/><NewLine>for step in range(run_length):<br/><NewLine>…<br/><NewLine>TDstep = reward + gamma*torch.max(Qnext.detach(), dim =1).values - Q[range(batch_size),action]<br/><NewLine>TD.append(TDstep)</p><NewLine><pre><code>TD = torch.stack(TD).sum()<NewLine>TD.backward()<NewLine><NewLine>optimiser.step()<NewLine>optimiser.zero_grad()<NewLine></code></pre><NewLine><p>With example output:</p><NewLine><p>action taken: 2<br/><NewLine>Q-values: tensor([-0.0832, -0.5799,  0.6861, -0.1725, -0.0538])<br/><NewLine>reward: tensor(-0.0050)<br/><NewLine>max Q(s+1): tensor([0.6873])<br/><NewLine>TD: tensor([-0.0726], grad_fn=)<br/><NewLine>action taken: 4<br/><NewLine>Q-values: tensor([-0.0823, -0.5819,  0.6942, -0.1716, -0.0544])<br/><NewLine>reward: tensor(-0.1900)<br/><NewLine>max Q(s+1): tensor([0.6797])<br/><NewLine>TD: tensor([0.4761], grad_fn=)<br/><NewLine>action taken: 2<br/><NewLine>Q-values: tensor([-0.0809, -0.5839,  0.7002, -0.1697, -0.0506])<br/><NewLine>reward: tensor(-0.0050)<br/><NewLine>max Q(s+1): tensor([0.6873])<br/><NewLine>TD: tensor([-0.0867], grad_fn=)<br/><NewLine>action taken: 1<br/><NewLine>Q-values: tensor([-0.0796, -0.5859,  0.7069, -0.1680, -0.0477])</p><NewLine><p>As you can see, though the TD for action 2 in the first step is negative, the Q-value for action 2 increased after the update. The TD for action 4 is positive and the Q-value for action 4 also increases after the corresponding update.</p><NewLine><p>I checked the updated weights and only the right weights are affected. So I’m clueless.</p><NewLine></div>",https://discuss.pytorch.org/u/HappyLemon,,HappyLemon,"February 21, 2020,  2:59pm",,,,,
69932,DQN network running but agent is not improving,2020-02-16T10:52:53.071Z,0,471,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m new to machine learning and Programming in general. I’m trying to get a DQN to beat the OpenAI gym Mountain car-v0 game. the code runs without any errors but does not seem to improve at the game at all. I ran 50,000 episodes and the average score over past 100 episodes remained unchanged at -200. This is the code. If anyone is willing to go through it and let me know what I’ve done wrong I would greatly appreciate it.</p><NewLine><pre><code class=""lang-auto"">import numpy as np<NewLine>import random<NewLine>import os<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine><NewLine>import gym<NewLine><NewLine>sample_size = 25<NewLine><NewLine># Creating the architecture of the Neural Network<NewLine><NewLine>class Network(nn.Module):<NewLine>    <NewLine>    def __init__(self, input_size, nb_action):<NewLine>        super(Network, self).__init__()<NewLine>        self.input_size = input_size<NewLine>        self.nb_action = nb_action<NewLine>        self.fc1 = nn.Linear(input_size, 30)<NewLine>        self.fc2 = nn.Linear(30, nb_action)<NewLine>    <NewLine>    def forward(self, state):<NewLine>        x = F.relu(self.fc1(state))<NewLine>        q_values = self.fc2(x)<NewLine>        return q_values<NewLine><NewLine># Implementing Experience Replay<NewLine><NewLine>class ReplayMemory(object):<NewLine>    <NewLine>    def __init__(self, capacity):<NewLine>        self.capacity = capacity<NewLine>        self.memory = []<NewLine>    <NewLine>    def push(self, event):<NewLine>        self.memory.append(event)<NewLine>        if len(self.memory) &gt; self.capacity:<NewLine>            del self.memory[0]<NewLine>    <NewLine>    def sample(self, batch_size):<NewLine>        samples = zip(*random.sample(self.memory, batch_size))<NewLine>        with torch.no_grad():<NewLine>            return map(lambda x: torch.cat(x, 0), samples)<NewLine><NewLine># Implementing Deep Q Learning<NewLine><NewLine>class Dqn():<NewLine>    <NewLine>    def __init__(self, input_size, nb_action, gamma):<NewLine>        self.gamma = gamma<NewLine><NewLine>        self.model = Network(input_size, nb_action)<NewLine>        self.memory = ReplayMemory(100000)<NewLine>        self.optimizer = optim.Adam(self.model.parameters(), lr = 0.001)<NewLine>        self.last_state = torch.Tensor(input_size).unsqueeze(0)<NewLine>        self.last_action = 0<NewLine>        self.last_reward = 0 <NewLine>    <NewLine>    def select_action(self, state):<NewLine>        state_transformed = torch.Tensor(state).float().unsqueeze(0)<NewLine>        probs = F.softmax(self.model(state_transformed)*100, dim= 1) # T=100<NewLine>        action = probs.multinomial(1)<NewLine>        self.last_action = action.data[0,0]<NewLine>        self.last_state = state_transformed<NewLine>        return action.data[0,0]<NewLine>    <NewLine>    def learn(self, batch_state, batch_next_state, batch_reward, batch_action):<NewLine>        outputs = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)<NewLine>        next_outputs = self.model(batch_next_state).detach().max(1)[0]  <NewLine>        target = self.gamma*next_outputs + batch_reward<NewLine>        td_loss = F.smooth_l1_loss(outputs, target)<NewLine>        self.optimizer.zero_grad()<NewLine>        td_loss.backward(retain_graph = True)<NewLine>        self.optimizer.step()<NewLine>    <NewLine>    def update(self, reward, new_signal):<NewLine>        new_state = torch.Tensor(new_signal).float().unsqueeze(0)<NewLine>        self.memory.push((self.last_state, new_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([reward]))) <NewLine>        if len(self.memory.memory) &gt; sample_size:<NewLine>            batch_state, batch_next_state, batch_action, batch_reward = self.memory.sample(sample_size)<NewLine>            self.learn(batch_state, batch_next_state, batch_reward, batch_action)<NewLine>    <NewLine>        self.reward_window.append(reward)<NewLine>        if len(self.reward_window) &gt; 1000:<NewLine>            del self.reward_window[0]<NewLine>        return action<NewLine>    <NewLine>    def score(self):<NewLine>        return sum(self.reward_window)/(len(self.reward_window)+1.)<NewLine>    <NewLine>    def save(self):<NewLine>        torch.save({'state_dict': self.model.state_dict(),<NewLine>                    'optimizer' : self.optimizer.state_dict(),<NewLine>                   }, 'Brain_save_1.pth')<NewLine>    <NewLine>    def load(self):<NewLine>        '''<NewLine>        loads brain<NewLine>        '''<NewLine>        if os.path.isfile('Brain_save_1.pth'):<NewLine>            print(""=&gt; loading checkpoint... "")<NewLine>            checkpoint = torch.load('Brain_save_1.pth')<NewLine>            self.model.load_state_dict(checkpoint['state_dict'])<NewLine>            self.optimizer.load_state_dict(checkpoint['optimizer'])<NewLine>            print(""done !"")<NewLine>        else:<NewLine>            print(""no checkpoint found..."")<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    env = gym.make('MountainCar-v0')<NewLine>    EPISODES = 25000<NewLine>    show_every = 500<NewLine>    save_check = 24998<NewLine>    scores =[]<NewLine>    env.reset()<NewLine><NewLine><NewLine>    brain = Dqn(2, env.action_space.n, 0.95)<NewLine>    brain.load()<NewLine><NewLine>    for episode in range(EPISODES):<NewLine>        score = 0<NewLine>        done = False<NewLine>        obs = env.reset()<NewLine>        reward = 0<NewLine><NewLine>        if (episode == save_check):<NewLine>          brain.save()<NewLine>          print('File saved')<NewLine><NewLine>        while not done:<NewLine>            action = brain.select_action(obs).item()<NewLine>            obs_,reward,done,info = env.step(action)<NewLine>            brain.update(reward,obs_)<NewLine>            obs = obs_<NewLine>            score += reward <NewLine>        scores.append(score)<NewLine><NewLine>        if episode % show_every == 0:<NewLine>            avg_score = np.mean(scores[-100:])<NewLine>            print('episode ', episode, 'score %.1f avg score %.1f' %(score, avg_score))<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Hari_Anesh,(Hari Anesh),Hari_Anesh,"February 16, 2020, 10:52am",1 Like,,,,
68881,CPU Memory Leak,2020-02-06T16:18:26.968Z,6,640,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I’m trying to experiment different configuration with the A3C code posted on GitHub under the following link:<br/><NewLine><a href=""https://github.com/MorvanZhou/pytorch-A3C"" rel=""nofollow noopener"">https://github.com/MorvanZhou/pytorch-A3C</a></p><NewLine><p>All my tested are concerned with the script “discrete_A3C.py”<br/><NewLine><a href=""https://github.com/MorvanZhou/pytorch-A3C/blob/master/discrete_A3C.py"" rel=""nofollow noopener"">https://github.com/MorvanZhou/pytorch-A3C/blob/master/discrete_A3C.py</a></p><NewLine><p>Python Version: 3.6.9<br/><NewLine>Torch Version: 1.4.0</p><NewLine><p>If I keep everything as in the original code the memory usage is consistent and does not increase over time. Which can be shown by plotting the memory usage graph using the command:</p><NewLine><blockquote><NewLine><p>mprof run --multiprocess discrete_A3C.py</p><NewLine></blockquote><NewLine><p>However, I tried to change the NN architecture to be as follows:</p><NewLine><pre><code class=""lang-auto"">    def __init__(self, s_dim, a_dim):<NewLine>        super(Net, self).__init__()<NewLine>        self.s_dim = s_dim<NewLine>        self.a_dim = a_dim<NewLine><NewLine>        self.fc1 = nn.Linear(s_dim, 128)<NewLine>        self.fc2 = nn.Linear(128, 128)<NewLine><NewLine>        self.pi1 = nn.Linear(128, 64)<NewLine>        self.pi2 = nn.Linear(64, a_dim)  # Q values for each action are output<NewLine><NewLine>        self.v1 = nn.Linear(128, 128)<NewLine>        self.v2 = nn.Linear(128, 1)<NewLine>        set_init([self.pi1, self.pi2, self.fc1, self.fc2, self.v1, self.v2])<NewLine>        self.distribution = torch.distributions.Categorical<NewLine><NewLine>    def forward(self, x):<NewLine>        x = F.relu6(self.fc1(x))<NewLine>        x = F.relu6(self.fc2(x))<NewLine><NewLine>        pi1 = F.relu6(self.pi1(x))<NewLine>        logits = self.pi2(pi1)<NewLine><NewLine>        v1 = F.relu6(self.v1(x))<NewLine>        values = self.v2(v1)<NewLine>        return logits, values<NewLine></code></pre><NewLine><p>re-running the same command as before however it can be noticed a linear increase of memory usage overtime.</p><NewLine><p>Please check the image below where the figure on the left is from running the original code, and the figure on the right is from the modified architecture.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/e8357caa8eb7fef156416362dd30544a3e6605e6"" href=""https://discuss.pytorch.org/uploads/default/original/3X/e/8/e8357caa8eb7fef156416362dd30544a3e6605e6.jpeg"" title=""image""><img alt=""image"" data-base62-sha1=""x8dmQnwavdSMYbFGrhZewpfRH4q"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/8/e8357caa8eb7fef156416362dd30544a3e6605e6_2_10x10.png"" height=""184"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/8/e8357caa8eb7fef156416362dd30544a3e6605e6_2_690x184.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/8/e8357caa8eb7fef156416362dd30544a3e6605e6_2_690x184.jpeg, https://discuss.pytorch.org/uploads/default/optimized/3X/e/8/e8357caa8eb7fef156416362dd30544a3e6605e6_2_1035x276.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/e/8/e8357caa8eb7fef156416362dd30544a3e6605e6_2_1380x368.jpeg 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">3706×991 292 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>I tried to debug this myself during the past week but can not come up with any clues, if anyone can provide any insights would be of valuable help.</p><NewLine></div>",https://discuss.pytorch.org/u/mchawaV,,mchawaV,"February 6, 2020,  4:18pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>This looks very similar to <a href=""https://github.com/pytorch/pytorch/issues/32284"">https://github.com/pytorch/pytorch/issues/32284</a>.<br/><NewLine>Does it actually run out of memory (like your process gets killed by the OS)? Or does it stabilizes when it gets to full memory?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes , it does get out of memory eventually.</p><NewLine><p>I read the link, it seems similar indeed however in my case I’m not using any MaxPool2D and also the leak happens due to architecture change without using new functions.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Interesting.<br/><NewLine>Could you give both architectures side by side?<br/><NewLine>also does running <code>import gc; gc.collect()</code> helps?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried to use gc.collect() but it does not help at all.</p><NewLine><p>Below are the two architectures side-by-side and the only modification to the whole code, the left one is the original one where the problem does not exist, the one on the right is the modified one which causes the issue:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/900805321dd7fddff0fedcd84722716cac8a43d8"" href=""https://discuss.pytorch.org/uploads/default/original/3X/9/0/900805321dd7fddff0fedcd84722716cac8a43d8.jpeg"" title=""image (1)""><img alt=""image (1)"" data-base62-sha1=""ky9We5CXvmWOqGOopA7cwSQOefe"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/0/900805321dd7fddff0fedcd84722716cac8a43d8_2_10x10.png"" height=""222"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/0/900805321dd7fddff0fedcd84722716cac8a43d8_2_690x222.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/0/900805321dd7fddff0fedcd84722716cac8a43d8_2_690x222.jpeg, https://discuss.pytorch.org/uploads/default/original/3X/9/0/900805321dd7fddff0fedcd84722716cac8a43d8.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/9/0/900805321dd7fddff0fedcd84722716cac8a43d8.jpeg 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image (1)</span><span class=""informations"">1014×327 84 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>What is <code>F.relu6</code> here?</p><NewLine><p>Otherwise this should not change anything.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>F.relu6 is torch.nn.functional.relu6</p><NewLine><p>documentation can be found here:<br/><NewLine><a href=""https://pytorch.org/docs/stable/nn.functional.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/nn.functional.html</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ha, I didn’t knew this was a thing <img alt="":open_mouth:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/open_mouth.png?v=9"" title="":open_mouth:""/></p><NewLine><p>Trying locally with these two nets do not lead to any issue.<br/><NewLine>Do you have a small (40/50 lines) code sample I can run locally that will reproduce this?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you using CentOS ? , I did a quick test on CentOS and it seems no issue there. But the issue happens on Ubuntu 18.04</p><NewLine><p>You can find the whole code on the github repo in the description , just change the 2 functions I wrote above and launch the script discrete_A3C.py .</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes i checked on centos.</p><NewLine><blockquote><NewLine><p>You can find the whole code on the github repo</p><NewLine></blockquote><NewLine><p>Unfortunately I cannot install gym and run the multiprocess training here locally (due to some restrictions on the installs on the machine I use) <img alt="":confused:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=9"" title="":confused:""/></p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok , I will double check if the issue can be reproduced on CentOS and get back.<br/><NewLine>Also I found some interesting results after some tinkering that needs confirmation.</p><NewLine><p>Will be back tomorrow. Thanks mate.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>The issue exists on CentOSv8.</p><NewLine><p>Can you retry to do the test at your side with the following packages installed ?</p><NewLine><blockquote><NewLine><p>cloudpickle==1.2.2<br/><NewLine>cycler==0.10.0<br/><NewLine>future==0.18.2<br/><NewLine>gym==0.15.4<br/><NewLine>joblib==0.14.1<br/><NewLine>kiwisolver==1.1.0<br/><NewLine>matplotlib==3.1.2<br/><NewLine>numpy==1.18.1<br/><NewLine>opencv-python==4.1.2.30<br/><NewLine>pandas==1.0.0<br/><NewLine>Pillow==7.0.0<br/><NewLine>pkg-resources==0.0.0<br/><NewLine>pyglet==1.3.2<br/><NewLine>pyparsing==2.4.6<br/><NewLine>python-dateutil==2.8.1<br/><NewLine>pytz==2019.3<br/><NewLine>scikit-learn==0.22.1<br/><NewLine>scipy==1.4.1<br/><NewLine>six==1.14.0<br/><NewLine>sklearn==0.0<br/><NewLine>torch==1.4.0<br/><NewLine>torchvision==0.5.0<br/><NewLine>xlrd==1.2.0</p><NewLine></blockquote><NewLine><p>I doubt that maybe it’s something in my environment that is causing this issue.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mchawaV; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mchawaV; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mchawaV; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mchawaV; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mchawaV; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/mchawaV; <NewLine> ,"REPLY_DATE 1: February 6, 2020,  4:32pm; <NewLine> REPLY_DATE 2: February 6, 2020,  4:37pm; <NewLine> REPLY_DATE 3: February 6, 2020,  5:48pm; <NewLine> REPLY_DATE 4: February 6, 2020,  6:04pm; <NewLine> REPLY_DATE 5: February 6, 2020,  7:07pm; <NewLine> REPLY_DATE 6: February 6, 2020, 10:18pm; <NewLine> REPLY_DATE 7: February 6, 2020, 10:25pm; <NewLine> REPLY_DATE 8: February 6, 2020, 10:38pm; <NewLine> REPLY_DATE 9: February 6, 2020, 11:08pm; <NewLine> REPLY_DATE 10: February 6, 2020, 11:14pm; <NewLine> REPLY_DATE 11: February 8, 2020, 12:43pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> REPLY 11 LIKES: ; <NewLine> 
4123,DQN example from PyTorch diverged!,2017-06-19T03:27:21.778Z,14,8540,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello folks.</p><NewLine><p>I just implemented my DQN by following the example from PyTorch. I found nothing weird about it, but it diverged. I run the original code again and it also diverged.</p><NewLine><p>The behaviors are like this. It often reaches a high average (around 200, 300) within 100 episodes. Then it starts to perform worse and worse, and stops around an average around 20, just like some random behaviors. I tried a lot of changes, the original version was surprisingly the best one, as described.</p><NewLine><p>Any ideas?</p><NewLine></div>",https://discuss.pytorch.org/u/Eddie_Li,(Eddie Li),Eddie_Li,"June 19, 2017,  3:27am",4 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Another concern is more theoretical. Since I’m training it on CartPole, in which sequential or temporal information matters, would DQN be able to learn the best strategy, if enough training given?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I just submit a PR of DQN in PyTorch:<br/><NewLine><aside class=""onebox githubpullrequest""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/examples/pull/172"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/examples</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Pull Request""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 12 16"" width=""60""><path d=""M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/examples/pull/172"" rel=""nofollow noopener"" target=""_blank"">DQN</a><NewLine></h4><NewLine><div class=""branches""><NewLine><code>pytorch:master</code> ← <code>stegben:dqn</code><NewLine></div><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2017-06-17"" data-format=""ll"" data-time=""16:40:13"" data-timezone=""UTC"">04:40PM - 17 Jun 17 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/stegben"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""stegben"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars3.githubusercontent.com/u/6868283?v=4"" width=""20""/><NewLine>          stegben<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""2 commits changed 2 files with 180 additions and 0 deletions""><NewLine><a href=""https://github.com/pytorch/examples/pull/172/files"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+180</span><NewLine><span class=""removed"">-0</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>I’ve encounter that problem before. Maybe you use the same Q network when updating? The Q network for finding the greatest action should be fixed.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks man.</p><NewLine><p>I tried after I saw your reply. The nature DQN paper didn’t mention how frequently they updated their target network. But I tried every 2, 10, and 100 iterations. None of them solved my divergence problem. But they made my averages stay at 200 for a very long time. I guess it’s working, but not enough.</p><NewLine><p>BTW I’m using exactly the same implementation given by PyTorch tutorials, which takes screens as inputs.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>How do you keep the average over 200 for a period? In my experiments, it cannot keep. There is only bursts into high durations and the following duration will fall to very low values. I also tried the the update frequency but it does not work. Can you show me your duration-episode curve?</p><NewLine><p>I am also using the difference image as the state.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey xuehy. Sorry since it was not very successful so I didn’t keep a lot of records. What I was doing was just adjusting hyperparameters. What I do remember is, memory capacity matters a lot.</p><NewLine><p>So far I haven’t found a solution to make DQN converge. The potential reasons in my mind are 1) NN models are prone to divergence in theory; 2) simple image recognition model doesn’t learn CartPole very well.</p><NewLine><p>I don’t know how far did you get since then, but I would suggest both include a LSTM cell, and try your model on other games.  I haven’t done them yet though.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found that the usage of smooth l1 loss (Huber) always led to divergence on the cart pole environment (somebody else also had that problem I’ll add the link later)</p><NewLine><p>It worked for me when using l2 (mse) loss</p><NewLine><p>Further investigation showed that the loss explodedn with the Huber loss, (I ll need to check what happened to the gradient)</p><NewLine><p>I found this very strange considering that the Huber loss is specifically designed to be stable</p><NewLine><p>If anyone has any idea why this happens please let me know</p><NewLine><p>Tldr if you usw smoothl1 loss, just try mse</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for pointing out! But I still can’t make it converge. In my case it just diverges very slowly (in case that my hyperparameters are problematic, I use the original example).</p><NewLine><p>Can you let us know what hyperparameters and other tricks, like fixed Q target, you are using?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey,<br/><NewLine>You can take a look at the original implementation (beware, it’s in LUA): <a href=""https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner"" rel=""nofollow noopener"">https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner</a><br/><NewLine>There are several tricks they use:</p><NewLine><ol><NewLine><li>They play using the static network, the dynamic one it trained, every 10K steps (hyper-parameter called target_q) the static is replaced by the dynamic.</li><NewLine><li>Reward clipping - reward is clipped to +- 1</li><NewLine><li>They clip the delta (loss) to +- 1 also.</li><NewLine><li>Probably some more tricks I forgot / missed.</li><NewLine></ol><NewLine><p>Besides that, once the basic DQN is working, I would suggest to add in Double Q Learning. It is pretty straight-forward to implement and gives steady convergence and better results.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/eddie_li"">@Eddie_Li</a> I’ll look for the code and put it on github (it’s a mess though). I was not able to find the blog post stating that the Huber loss leads to divergence on the cart pole example <img alt="":confused:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/confused.png?v=5"" title="":confused:""/></p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here is the github link <a href=""https://github.com/jaidmin/pytorch-q-learning"" rel=""nofollow noopener"">https://github.com/jaidmin/pytorch-q-learning</a></p><NewLine><p>I just pushed my q-learning folder to github, the cartpole example is in “pytorch-q-learning-simple.ipynb”, that<br/><NewLine>it converges for me after ~ a couple hundred / maybe 1-2 thousend episodes.<br/><NewLine>As you can see I use fixed q-targets and a replay memory</p><NewLine><p>I’ll create a gist with a simple minimal example later</p><NewLine><p>If you find any errors in the code or have any questions please let me know</p><NewLine><p>johannes</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just changed the state from the image difference to the original state of the game environment. After tuning the learning rate, everything is fine now. It seems the image input is difficult to train.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Definitely, it’s way harder to train than training your model with the observations, which in that particular case of <code>CartPole-v0</code> return the underlying physical model of the game. However, it is also way cooler, as it demonstrates the power of this algorithm <img alt="":slight_smile:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=5"" title="":slight_smile:""/></p><NewLine><p>However, I’d like to mention that the original Deep Q Learning algorithm described by <a href=""https://www.google.de/search?q=deep+q+learning+atari&amp;oq=deep+q+learning+atari&amp;aqs=chrome..69i57.4568j0j7&amp;sourceid=chrome&amp;ie=UTF-8"" rel=""nofollow noopener"">Mnih et al.</a> didn’t use the difference of previous and current frame as a state representation, but rather a stack of the 4 last seen and processed frames (Resulting in a 1x4x84x84 input tensor as they were training on gray scale images). Also, they leveraged a technique called frame skipping:</p><NewLine><p>The agent selects an action on every kth frame (I think it was 4th in the paper) and  this action is applied to the environment for k-1 frames. This helps the agent to see more frames in the same time, as computing an action requires significantly more computing power than stepping the environment.</p><NewLine><p>Also, the paper mentioned priorly deployed a second ‘target network’, which would be updated every 10.000 frames, which additionally stabilizes the algorithm.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>xuehy, I am facing the same problem on CartPole… the agent learns the most optimal behavior after 2K episodes, but then wanders off into less optimal regions. Can you please provide your learning rate, discount factor, exploration epsilon that worked for you? Thanks!</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well. Learning doesn’t take such long time. I only train it until the 300th episode. The result is stable after around 100th episode. The discount factor is 0.9999. The learning rate is 0.01 with adam. the exploration epsilon starts at 0.9 and ends at 0.05 and the eps_decay is 200.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m afraid people like me who trained on pixels can’t make it work because the example is wrong on its preprocessing. See denizs’ post.</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Double DQN might help. DQN is known to have a very high variance and there is no guarantee of convergence. Double DQN yields a much lower variance and a better policy from my experiments on CartPole. While I used state signals from Gym directly, this might also work for the images.</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! I think the preprocessing makes sense, but not very well, as the difference of the two frames capture the following information:</p><NewLine><ol><NewLine><li>the current position and the next position</li><NewLine><li>which position is current, and which position is next</li><NewLine></ol><NewLine><p>However, it does not capture the absolute position very well, because it uses a region of interest, which will keep fixed when the cartpole is in the middle. I think this might be where divergence comes.</p><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p>I wanted to add my two cents.</p><NewLine><p>One of the problems is related to what is known as the “vanishing gradient” problem in supervised learning. Bottom line is, your agent learns a good policy and then stops exploring the areas that are sub-optimal. At one point, all your agent knows about is the best states, and actions because there is nothing other than the samples coming from a near-optimal policy in your replay buffer. Then, all updates to your network come from the same couple of near-optimal states, actions.</p><NewLine><p>So, due to the vanishing gradient problem, your agent forgets how to get to the best, straight-up pole, position. It knows how to stay there, but not how to get there. You have no more samples of that in your replay buffer, guess what. As soon as the initial state is minimally different, chaos…</p><NewLine><p>BTW, this happens in virtually any DQN/Cart Pole example I’ve tested, even the ones using the continuous variables as opposed to images. Yes, this includes OpenAI Baselines! Just change the code so it keeps training indefinitely and you’ll find the same divergence issues.</p><NewLine><p>The way I got it to perform better is to increase the Replay Buffer size to 100,000 or 1,000,000 (you may need a solid implementation - see OpenAI Baselines: <a href=""https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py"" rel=""nofollow noopener"">https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py</a>), and increase the Batch Size to ~64 or ~128. I think reducing the learning rate would help as well, but also slow down learning, of course. Though, I suppose this will only postpone the issues, but at least I ran 1,000 episodes of perfect performance which works for me.</p><NewLine><p>Finally, I found it interesting to review the basics described by Sutton in his book: <a href=""http://incompleteideas.net/book/the-book-2nd.html"" rel=""nofollow noopener"">http://incompleteideas.net/book/the-book-2nd.html</a></p><NewLine><p>From the book, take a look at Example 3.4: Pole-Balancing.</p><NewLine><blockquote><NewLine><p>Example 3.4: Pole-Balancing The objective in this task is to apply forces to a cart moving along<br/><NewLine>a track so as to keep a pole hinged to the cart from falling over: A failure is said to occur if the pole<br/><NewLine>falls past a given angle from vertical or if the cart runs off the track. The pole is reset to vertical<br/><NewLine>after each failure. This task could be treated as episodic, where the natural episodes are the repeated<br/><NewLine>attempts to balance the pole. The reward in this case could be +1 for every time step on which failure<br/><NewLine>did not occur, so that the return at each time would be the number of steps until failure. In this case,<br/><NewLine>successful balancing forever would mean a return of infinity. Alternatively, we could treat pole-balancing<br/><NewLine>as a continuing task, using discounting. In this case the reward would be −1 on each failure and zero<br/><NewLine>at all other times. The return at each time would then be related to −γ<br/><NewLine>K, where K is the number of<br/><NewLine>time steps before failure. In either case, the return is maximized by keeping the pole balanced for as<br/><NewLine>long as possible.</p><NewLine></blockquote><NewLine><p>As the Cart-Pole example is setup as an episodic task in OpenAI Gym; +1 every time step on which failure did not occur, gamma should then be set to 1.</p><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>And if you want to explore what it the list of tricks that make the cartpole balancing faster and longer with Pytorch, from simple DQN to Double Duelling DQN, I recommend this tutorial:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/criteo-research/paiss_deeprl/blob/master/exercises_pytorch.ipynb"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/criteo-research/paiss_deeprl/blob/master/exercises_pytorch.ipynb"" target=""_blank"">criteo-research/paiss_deeprl/blob/master/exercises_pytorch.ipynb</a></h4><NewLine><pre><code class=""lang-ipynb"">{<NewLine> ""cells"": [<NewLine>  {<NewLine>   ""cell_type"": ""markdown"",<NewLine>   ""metadata"": {},<NewLine>   ""source"": [<NewLine>    ""# PAISS Practical Deep-RL by Criteo Research (Pytorch version)""<NewLine>   ]<NewLine>  },<NewLine>  {<NewLine>   ""cell_type"": ""code"",<NewLine>   ""execution_count"": null,<NewLine>   ""metadata"": {<NewLine>    ""collapsed"": true<NewLine>   },<NewLine>   ""outputs"": [],<NewLine>   ""source"": [<NewLine>    ""%pylab inline\n"",<NewLine>    ""\n"",<NewLine>    ""from utils import RLEnvironment, RLDebugger\n"",<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/criteo-research/paiss_deeprl/blob/master/exercises_pytorch.ipynb"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Eddie_Li; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/stegben; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Eddie_Li; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/xuehy; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Eddie_Li; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/j.laute; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Eddie_Li; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/tessler; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/j.laute; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/j.laute; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/xuehy; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/denizs; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/vv_ss; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/xuehy; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/Eddie_Li; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/mufeili; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/AlbertM; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/mimoralea; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> ,"REPLY_DATE 1: June 19, 2017,  3:29am; <NewLine> REPLY_DATE 2: June 19, 2017, 10:15am; <NewLine> REPLY_DATE 3: June 19, 2017,  7:34pm; <NewLine> REPLY_DATE 4: July 1, 2017,  1:10pm; <NewLine> REPLY_DATE 5: July 10, 2017,  4:00am; <NewLine> REPLY_DATE 6: July 10, 2017,  6:28am; <NewLine> REPLY_DATE 7: July 10, 2017,  5:57pm; <NewLine> REPLY_DATE 8: July 11, 2017,  6:57am; <NewLine> REPLY_DATE 9: July 11, 2017,  8:00am; <NewLine> REPLY_DATE 10: July 11, 2017,  8:55am; <NewLine> REPLY_DATE 11: July 11, 2017,  9:24am; <NewLine> REPLY_DATE 12: July 12, 2017, 10:28am; <NewLine> REPLY_DATE 13: July 20, 2017,  1:21pm; <NewLine> REPLY_DATE 14: July 21, 2017,  9:08am; <NewLine> REPLY_DATE 15: July 21, 2017,  5:21pm; <NewLine> REPLY_DATE 16: July 22, 2017,  3:44pm; <NewLine> REPLY_DATE 17: November 16, 2017,  3:50am; <NewLine> REPLY_DATE 18: July 17, 2018,  9:26pm; <NewLine> REPLY_DATE 19: July 19, 2018,  7:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: 4 Likes; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 2 Likes; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: 1 Like; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: 2 Likes; <NewLine> REPLY 17 LIKES: ; <NewLine> REPLY 18 LIKES: 1 Like; <NewLine> REPLY 19 LIKES: ; <NewLine> 
68503,Observation and action space as spaces.Dict,2020-02-03T11:36:37.944Z,0,122,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Can anyone give me good code example, where the observation space or the action space of an environment for a neural network has been a spaces.Dict?</p><NewLine><p>I’m trying to create my own environment. My observation space consists of three different inputs and my action space contains two different actions with their own action space.<br/><NewLine>For example, my current action space looks like this:</p><NewLine><p>‘’’<br/><NewLine>self.action_mutation = spaces.Discrete(4)<br/><NewLine>self.action_node = spaces.Discrete(100)</p><NewLine><p>self.action_space = spaces.Dict({‘mutation’: self.action_mutation, ‘node’: self.action_node})<br/><NewLine>‘’’</p><NewLine><p>But I’m not sure how to work with such spaces when I want to build my neural network.</p><NewLine></div>",https://discuss.pytorch.org/u/Gandalf,,Gandalf,"February 3, 2020, 11:36am",,,,,
68243,RuntimeError: _th_normal_ not supported on CPUType for Long,2020-01-31T13:35:37.838Z,0,231,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am getting the following error. It’s hard for me to post my whole code because I’m working with a reinforcement learning library. I’m trying to use their A2C algorithm, they sample actions from a Gaussian distribution when given a state. Their code expects <code>torch.float32</code>.  I have checked and my inputs are indeed <code>torch.float32</code> not Long. I’m not sure what to do. I know the algorithm runs as I have run it before, but I get this error when I try to use it in an environment with discrete actions.</p><NewLine><p>Here is the library’s draw_action function:</p><NewLine><pre><code class=""lang-auto"">def draw_action_t(self, state):<NewLine>        print('draw_action_t',state.dtype)<NewLine>        return self.distribution_t(state).sample().detach()<NewLine></code></pre><NewLine><p>and</p><NewLine><pre><code class=""lang-auto"">def distribution_t(self, state):<NewLine>        mu, sigma = self.get_mean_and_covariance(state)<NewLine>        return torch.distributions.MultivariateNormal(loc=mu, covariance_matrix=sigma)<NewLine></code></pre><NewLine><p>Final error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: _th_normal_ not supported on CPUType for Long<NewLine></code></pre><NewLine><p>Help is much appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/lionely,,lionely,"January 31, 2020,  1:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Bump, please would appreciate some help!! I’m not sure what is causing this problem</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/lionely; <NewLine> ,"REPLY_DATE 1: February 3, 2020,  4:44am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
68428,Reinforcement learning with Transformer for NLP,2020-02-02T17:17:18.069Z,0,173,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone.</p><NewLine><p>My question is related to implementing reinforcement learning [Gradient Policy] with a Transformer sequence to sequence model.</p><NewLine><p>To detail, I am using a Transformer seq2seq model for abstractive summarization. In order to do this, I need to sample a token from the Transformer during inference [no golden target] and to get the probability of that sample.</p><NewLine><p>The way I am trying to do this is:</p><NewLine><pre><code class=""lang-auto"">def get_distribution(model, batch):<NewLine><NewLine>    src, (shift_tgt, lbl_tgt), segs, clss, mask_src, mask_tgt, mask_cls = batch<NewLine><NewLine>    # the mock tgt are just torch.zeros tensors that have the same shape as the tgt<NewLine>    mock_tgt = get_mock_tgt(shift_tgt)<NewLine>    mock_return = get_mock_tgt(shift_tgt)<NewLine><NewLine>    max_length = shift_tgt.shape[1]<NewLine>    <NewLine>    log_probs = []<NewLine>    <NewLine>    for i in range(0, max_length-1):<NewLine>        prediction = model(src, mock_tgt, segs, clss, mask_src, mask_tgt, mask_cls)<NewLine>        prediction = F.softmax(prediction, dim=2)<NewLine>        <NewLine>        multi_dist = Categorical(prediction[:, i])<NewLine>        x_t = multi_dist.sample()<NewLine>        mock_tgt[:, i+1] = x_t<NewLine>        mock_return[:, i] = x_t<NewLine>        log_prob = multi_dist.log_prob(x_t)<NewLine>        log_probs.append(log_prob)<NewLine>        <NewLine>    return mock_return, log_probs<NewLine></code></pre><NewLine><p>However, I doubt this is the correct way. More exactly, the Transformer outputs an entire tensor at a time, instead of just one token. In order to get the Distribution of just one token then I use <strong>multi_dist = Categorical(prediction[:, i])</strong> and store the token given by the sample function for the next inference step.</p><NewLine><p>How am I supposed to do this the right way? I have tried searching for it everywhere but cannot seem to find an answer for this.</p><NewLine><p>Help <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/></p><NewLine></div>",https://discuss.pytorch.org/u/Andrei_Ungureanu,(Andrei Ungureanu),Andrei_Ungureanu,"February 2, 2020,  5:17pm",,,,,
68226,How to move multiple joints with PyTorch,2020-01-31T10:21:42.812Z,0,78,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey Guys.</p><NewLine><p>I control the arm on the simulator by reinforcement learning.<br/><NewLine>The arm has three joints, and I want to move the each joints -1 °, 0 °, or 1 ° each step.<br/><NewLine>What kind of neural network can?</p><NewLine><pre><code class=""lang-auto"">class Net(nn.Module):<NewLine><NewLine>    def __init__(self, n_in, n_mid, n_out):<NewLine>        super(Net, self).__init__()<NewLine>        self.fc1 = nn.Linear(n_in, n_mid) <NewLine>        self.fc2 = nn.Linear(n_mid, n_mid)<NewLine>        self.fc3 = nn.Linear(n_mid, n_out)<NewLine><NewLine>    def forward(self, x):<NewLine>        h1 = F.relu(self.fc1(x))<NewLine>        h2 = F.relu(self.fc2(h1))<NewLine>        output = self.fc3(h2)<NewLine>        return output<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/DLBOXER,,DLBOXER,"January 31, 2020, 10:21am",,,,,
67917,I don&rsquo;t find the error,2020-01-28T15:38:41.772Z,2,323,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">    def update(self, log_prob1, log_prob2,value, reward):<NewLine>       <NewLine><NewLine>        advantage = reward - value<NewLine><NewLine>        policy_loss = ((-log_prob1 * advantage) + (-log_prob2 * advantage)).mean()<NewLine>        value_loss = F.smooth_l1_loss(value, reward)<NewLine>        loss = policy_loss + value_loss<NewLine><NewLine>        self.optimizer.zero_grad()<NewLine>        loss.backward()<NewLine>        self.optimizer.step()<NewLine></code></pre><NewLine><p>In loss.backward i have a runtime error:<br/><NewLine>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [16, 1]], which is output 0 of SqrtBackward, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).</p><NewLine><p>Theese variables are torch tensor of 16x1, where is the inplace operation?</p><NewLine></div>",https://discuss.pytorch.org/u/Giuseppe,(Giuseppe Puglisi),Giuseppe,"January 29, 2020, 10:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I can’t see any inplace operations in your code snippet.<br/><NewLine>Could you post dummy tensor shapes, so that we could reproduce this issue?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">                    rewards = []<NewLine>                    for a1, a2 in zip(action1, action2):<NewLine><NewLine><NewLine>                        rewards.append(loader[mode].dataset.get_reward(state.item(), a1[0], a1[1], a2[0], a2[1]))<NewLine><NewLine>                    rewards = torch.Tensor(rewards).view(-1,1).cuda()#BS x 1<NewLine>                    log_prob1=log_prob1.view(-1,1)<NewLine>                    log_prob2 = log_prob2.view(-1, 1)<NewLine>                    state_value = state_value.view(-1,1)<NewLine><NewLine>                    if mode == 'train':<NewLine>                        loss_to_plot, policy_to_plot, reward_to_plot, value_to_plot = estimator.update(log_prob1,<NewLine>                                                                                                        log_prob2,<NewLine>                                                                                                        state_value,<NewLine>                                                                                                        rewards)<NewLine><NewLine><NewLine><NewLine>This is the code that call the method above.<NewLine>All tensors are 16x1, the only thing different is in structure: miss ""grad_fn=&lt;ViewBackward&gt; "" in tensor reward. I can't see any inplace operation too.<NewLine>I print these tensors:<NewLine><NewLine><NewLine><NewLine></code></pre><NewLine><p>tensor([[-2.2734],<br/><NewLine>[-2.3429],<br/><NewLine>[-2.3219],<br/><NewLine>[-1.6636],<br/><NewLine>[-3.4040],<br/><NewLine>[-3.1213],<br/><NewLine>[-4.9740],<br/><NewLine>[-2.1452],<br/><NewLine>[-2.5107],<br/><NewLine>[-4.0225],<br/><NewLine>[-3.3761],<br/><NewLine>[-1.9465],<br/><NewLine>[-3.5561],<br/><NewLine>[-1.5919],<br/><NewLine>[-1.8437],<br/><NewLine>[-2.8795]], device=‘cuda:0’, grad_fn=ViewBackward)<br/><NewLine>tensor([[-1.7228],<br/><NewLine>[-2.8122],<br/><NewLine>[-2.3623],<br/><NewLine>[-2.4175],<br/><NewLine>[-1.9202],<br/><NewLine>[-1.6540],<br/><NewLine>[-3.5124],<br/><NewLine>[-1.7577],<br/><NewLine>[-5.4544],<br/><NewLine>[-1.6919],<br/><NewLine>[-3.0612],<br/><NewLine>[-3.8174],<br/><NewLine>[-2.7434],<br/><NewLine>[-4.5231],<br/><NewLine>[-3.1026],<br/><NewLine>[-2.8684]], device=‘cuda:0’, grad_fn=ViewBackward)<br/><NewLine>tensor([[-0.0289],<br/><NewLine>[ 0.0604],<br/><NewLine>[ 0.0796],<br/><NewLine>[-0.0133],<br/><NewLine>[-0.1071],<br/><NewLine>[-0.0423],<br/><NewLine>[-0.0217],<br/><NewLine>[ 0.1402],<br/><NewLine>[-0.0927],<br/><NewLine>[-0.0896],<br/><NewLine>[ 0.0655],<br/><NewLine>[ 0.0261],<br/><NewLine>[ 0.0024],<br/><NewLine>[ 0.0145],<br/><NewLine>[-0.0375],<br/><NewLine>[ 0.0536]], device=‘cuda:0’, grad_fn=ViewBackward)<br/><NewLine>tensor([[1.0000],<br/><NewLine>[0.5000],<br/><NewLine>[0.0000],<br/><NewLine>[0.5000],<br/><NewLine>[1.0000],<br/><NewLine>[0.0000],<br/><NewLine>[0.0000],<br/><NewLine>[0.0000],<br/><NewLine>[1.0000],<br/><NewLine>[1.0000],<br/><NewLine>[0.0000],<br/><NewLine>[0.5000],<br/><NewLine>[0.5000],<br/><NewLine>[0.5000],<br/><NewLine>[0.5000],<br/><NewLine>[0.0000]], device=‘cuda:0’)</p><NewLine><pre><code class=""lang-auto""><NewLine><NewLine>I print respectively in the same order when i pass them</code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Giuseppe; <NewLine> ,"REPLY_DATE 1: January 28, 2020,  7:55pm; <NewLine> REPLY_DATE 2: January 29, 2020,  7:38am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
19660,How to implement TD(λ),2018-06-13T18:37:43.401Z,1,992,"<div class=""post"" itemprop=""articleBody""><NewLine><p>i have a vanilla NN that looks like this</p><NewLine><pre><code class=""lang-auto"">self.model = torch.nn.Sequential(<NewLine>		 	torch.nn.Linear(self.INPUT_SIZE, self.HIDDEN_SIZE),<NewLine>		 	torch.nn.Sigmoid(),<NewLine>		 	torch.nn.Linear(self.HIDDEN_SIZE, self.OUTPUT_SIZE),<NewLine>		 	torch.nn.Sigmoid())<NewLine></code></pre><NewLine><p>i want to implement TD(λ) between steps.  λ is a constant btw 0, 1 that sets lifespan of a gradient trace.<br/><NewLine>psuedo-code for this is</p><NewLine><pre><code class=""lang-auto"">loss.backward()<NewLine>model.gradients = model.gradients + λ * model.previous_gradients<NewLine>optimizer.step()<NewLine>model.previous_gradients = model.gradients<NewLine></code></pre><NewLine><p>i think i might be able to accomplish this using gradient hooks, but i’m unsure how to do that.  i can’t shake the feeling that it might be easier than that.  SGD’s momentum is mathematically similar, but i can’t tell if it’s identical.</p><NewLine></div>",https://discuss.pytorch.org/u/Tyler_Walker,(Tyler Walker),Tyler_Walker,"June 13, 2018,  6:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>As far as I understand, using gradient hook should only allow you to modify the new gradient. But you want to modify the previous one, so I don’t see a simpler way than:</p><NewLine><pre><code class=""lang-auto""># multiply previous gradient by lambda:<NewLine>for p in model.parameters():<NewLine>    p.grad *= lambda<NewLine>loss.backward() # add the new gradient<NewLine>optimizer.step()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>oh, that’s interesting!  then never call optimizer.zero_grad()<br/><NewLine>thanks.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am pretty confused on how to implement TD(lambda), can you tell me what is your loss function, i thougth that there were no losses for which you get the TD Lambda gradient ?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Tyler_Walker; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Dhawgupta; <NewLine> ,"REPLY_DATE 1: June 14, 2018,  9:47pm; <NewLine> REPLY_DATE 2: June 14, 2018, 10:11pm; <NewLine> REPLY_DATE 3: January 16, 2020,  9:37pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
66620,Can I set the batch_size of lstm model to be None like tf in pytorch,2020-01-14T10:04:28.068Z,0,87,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to create a nn model by pytorch to implement reinforcement.</p><NewLine><p>I need to use this model to interact with the environment, so I want to set the batch_size=1.</p><NewLine><p>But the model cannot perform well, so I want to train the model firstly by supervised learning in order to enhance the performance of my model, so I want to set the batch_size&gt;1(like 64) to accelerate the training process.</p><NewLine></div>",https://discuss.pytorch.org/u/cmcai0104,(C.M. Cai),cmcai0104,"January 14, 2020, 10:04am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>In pytorch, the batch size is not a property of the model.<br/><NewLine>You can give an input of any batch size as input and it will give you the corresponding output with the same batch size.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: January 14, 2020,  3:51pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
65705,How to Vectorize/Parallelize Reinforcement Learning Environments?,2020-01-03T19:59:11.740Z,1,242,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I feel like this is such an obvious problem, but I can’t find any clear answers. I have a Python class that conforms to OpenAI’s environment API, but it’s written in a way that it receives one input action per step and returns one reward per step. How do I parallelize this environment? I haven’t been able to find any clear answer online. A few people suggested baselines or stable_baselines, but these don’t appear to work with PyTorch and they’re currently broken by the switch to TensorFlow 2.0.</p><NewLine><p>There are some other RL libraries (e.g. <a href=""https://github.com/zuoxingdong/lagom"" rel=""nofollow noopener"">https://github.com/zuoxingdong/lagom</a>, <a href=""https://github.com/astooke/rlpyt"" rel=""nofollow noopener"">https://github.com/astooke/rlpyt</a>), but they don’t appear to have professional support, so I’m concerned that if I use one, it’ll quickly become unusable.</p><NewLine></div>",https://discuss.pytorch.org/u/RylanSchaeffer,(Rylan Schaeffer),RylanSchaeffer,"January 4, 2020,  3:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not an RL expert, but <a href=""https://github.com/catalyst-team/catalyst"">Catalyst</a> is in the PyTorch ecosystem, so it might get some future support.<br/><NewLine>Have you had a look at this library already and would it fit your needs?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I decided to simply vectorize my environments and that appears to have given me the speed boost I need. I used the code provided here: <a href=""https://stackoverflow.com/a/59599319/4570472"" rel=""nofollow noopener"">https://stackoverflow.com/a/59599319/4570472</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/RylanSchaeffer; <NewLine> ,"REPLY_DATE 1: January 5, 2020,  6:42am; <NewLine> REPLY_DATE 2: January 10, 2020,  4:40pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
66265,AttributeError: &lsquo;numpy.ndarray&rsquo; object has no attribute &lsquo;dim&rsquo; from torch/nn/functional.py,2020-01-10T07:57:33.341Z,4,2005,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m working on a DDPG implementation and getting <code>AttributeError: 'numpy.ndarray' object has no attribute 'dim'</code> from my Actor class.<br/><NewLine>Based on answer here <a>https://discuss.pytorch.org/t/attributeerror-numpy-ndarray-object-has-no-attribute-dim/16026/2</a>, I tried using <code>Variable()</code>, however, the args are Tensors already and I get the <code>TypeError: expected np.ndarray (got Tensor)</code> error.</p><NewLine><p><strong>Code</strong></p><NewLine><pre><code class=""lang-auto"">    def forward(self, state):<NewLine>        """"""<NewLine>        Build the policy/actor network that maps states to actions.<NewLine>        Using Tanh activation as per DDPG paper<NewLine>        """"""<NewLine>        # print(type(state)) # &lt;class 'torch.Tensor'&gt;<NewLine>#         state = Variable(T.from_numpy(state))<NewLine>        x = self.inp(state)<NewLine>        l1_states = F.relu(x)<NewLine>        l2_states = F.relu(self.h1(l1_states))<NewLine>        l3_actions = T.tanh(self.out(l2_states))<NewLine><NewLine>        return l3_actions<NewLine></code></pre><NewLine><p><strong>Error:</strong></p><NewLine><pre><code class=""lang-auto"">&lt;ipython-input-11-d3087ef19a20&gt; in forward(self, state)<NewLine>     71         # print(type(state)) # &lt;class 'torch.Tensor'&gt;<NewLine>     72 #         state = Variable(T.from_numpy(state))<NewLine>---&gt; 73         x = self.inp(state)<NewLine>     74         l1_states = F.relu(x)<NewLine>     75         l2_states = F.relu(self.h1(l1_states))<NewLine><NewLine>~/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)<NewLine>    539             result = self._slow_forward(*input, **kwargs)<NewLine>    540         else:<NewLine>--&gt; 541             result = self.forward(*input, **kwargs)<NewLine>    542         for hook in self._forward_hooks.values():<NewLine>    543             hook_result = hook(self, input, result)<NewLine><NewLine>~/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/modules/linear.py in forward(self, input)<NewLine>     85 <NewLine>     86     def forward(self, input):<NewLine>---&gt; 87         return F.linear(input, self.weight, self.bias)<NewLine>     88 <NewLine>     89     def extra_repr(self):<NewLine><NewLine>~/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/functional.py in linear(input, weight, bias)<NewLine>   1366         - Output: :math:`(N, *, out\_features)`<NewLine>   1367     """"""<NewLine>-&gt; 1368     if input.dim() == 2 and bias is not None:<NewLine>   1369         # fused op is marginally faster<NewLine>   1370         ret = torch.addmm(bias, input, weight.t())<NewLine><NewLine>AttributeError: 'numpy.ndarray' object has no attribute 'dim'<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/jintolonappan,(Jinto Lonappan),jintolonappan,"January 10, 2020,  7:57am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Based on the error message it looks like <code>state</code> is a numpy array.<br/><NewLine>Could you check the type again, please, and make sure it’s a tensor?</p><NewLine><p>Don’t use <code>Variables</code>, as they are deprecated since PyTorch <code>0.4.0</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group quote-modified"" data-post=""1"" data-topic=""66265""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/jintolonappan/40/19517_2.png"" width=""20""/> jintolonappan:</div><NewLine><blockquote><NewLine><p>print(type(state)) # &lt;class ‘torch.Tensor’&gt;</p><NewLine></blockquote><NewLine></aside><NewLine><p><code>&lt;class 'torch.Tensor'&gt;</code> was the output of <code>print(type(state)) </code></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is this error raised directly after the print statement and did you check all <code>state</code> inputs?<br/><NewLine>Could some of them be numpy arrays somehow?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the suggestion to check <em>all</em> <code>state</code> inputs. Looks like a bug in my code when I sample from ReplayBuffer. <img alt="":exploding_head:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/exploding_head.png?v=9"" title="":exploding_head:""/><br/><NewLine>I was able to move forward now <img alt="":+1:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/+1.png?v=9"" title="":+1:""/></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Great to hear it’s working! <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jintolonappan; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jintolonappan; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: January 10, 2020,  8:18am; <NewLine> REPLY_DATE 2: January 10, 2020,  8:19am; <NewLine> REPLY_DATE 3: January 10, 2020,  8:24am; <NewLine> REPLY_DATE 4: January 10, 2020,  8:48am; <NewLine> REPLY_DATE 5: January 10, 2020,  8:49am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
50102,Synchronization for sharing/updating shared model state dict across multi-process,2019-07-09T09:59:35.231Z,1,218,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, while I’m doing a RL project, I need some way to share model state dict across multi-processes.</p><NewLine><p>I found the <a href=""https://github.com/dgriff777/rl_a3c_pytorch"" rel=""nofollow noopener"">A3C implementation</a> which has this feature, but I have some questions about it.</p><NewLine><ol><NewLine><li><NewLine><p>In the above repo, the author defined model instance on CPU shared memory, and then all the sub processes also share that. In each subprocess, it has its own model instance on GPU, and it loads the state from globally shared CPU model instance without any lock.<br/><NewLine>So my question is, Is it guranteed that it’s safe to access parameters of the model on CPU shared memory simultaneously in multiple processes without any lock or synchronization?<br/><NewLine>I’m pretty sure it’s fine since model parameters are treated as read-only variable here, but I just want to double check it.</p><NewLine></li><NewLine><li><NewLine><p>The next question is my main interest. <a href=""https://github.com/dgriff777/rl_a3c_pytorch/blob/master/train.py#L104"" rel=""nofollow noopener"">At here</a>, the author simply copies the gradient of model in sub-process to shared model without any lock mechanism, and also optimization is done without synchronization. How come is it possible? Why isn’t everything messed up when multiple processes overwrite the gradient of shared model without any lock mechanism?</p><NewLine></li><NewLine></ol><NewLine><p>If I missed out some points, please let me know <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/></p><NewLine></div>",https://discuss.pytorch.org/u/wwiiiii,(Jeong TaeYeong),wwiiiii,"July 9, 2019,  9:59am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Okay, I found there’s already well documented answer at <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/notes/multiprocessing.html</a>.<br/><NewLine>And also with hogwild!-spirit, it’s fine to overwrite gradient from other process.</p><NewLine><p>So the only thing I wonder now is that how the <a href=""https://github.com/dgriff777/rl_a3c_pytorch/blob/master/shared_optim.py#L184"" rel=""nofollow noopener"">updating shared model parameter</a> could be done without any lock mechanism. Are the <code>add_</code> or <code>addcdiv_</code> atomic operation?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have the same question, and I think it’s not atomic operatino.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wwiiiii; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Yanzzzzz; <NewLine> ,"REPLY_DATE 1: July 10, 2019,  7:56am; <NewLine> REPLY_DATE 2: January 10, 2020,  7:43am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
65424,Is it professional when dealing with the softmax layer in mobile,2019-12-31T02:15:00.295Z,18,268,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to do the task of softmax layer in pytorch, I know Tensorflow can do it</p><NewLine></div>",https://discuss.pytorch.org/u/MarcSteven,(Marc Steven),MarcSteven,"December 31, 2019,  2:15am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you explain your use case and your concerns a bit?<br/><NewLine>Which task would you like to use with a softmax layer?<br/><NewLine>PyTorch has a softmax implementation, but I’m not sure I understand your issue completely.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I mean How to apply softmax layer in PyTorch. <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can apply it as any other layer:</p><NewLine><pre><code class=""lang-python"">softmax = nn.Softmax(dim=1)<NewLine>x = torch.randn(2, 10)<NewLine>output = softmax(x)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>As the final layer? Right? <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>It depends on your use case and e.g. the loss function you are using.<br/><NewLine><code>nn.CrossEntropyLoss</code> expects raw logits for example, while <code>nn.NLLLoss</code> expects log probabilities (<code>F.log_softmax</code> applied to the last layer).</p><NewLine><p>Without some more information it is hard to answer your general questions.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/fdf5094b430ebac03acf8f8c22d3376f5b081826"" href=""https://discuss.pytorch.org/uploads/default/original/3X/f/d/fdf5094b430ebac03acf8f8c22d3376f5b081826.png"" title=""09""><img alt=""09"" data-base62-sha1=""AeBMQD5evY0LILHZcLCUnRvnqPc"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/d/fdf5094b430ebac03acf8f8c22d3376f5b081826_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/d/fdf5094b430ebac03acf8f8c22d3376f5b081826_2_388x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/d/fdf5094b430ebac03acf8f8c22d3376f5b081826_2_388x500.png, https://discuss.pytorch.org/uploads/default/optimized/3X/f/d/fdf5094b430ebac03acf8f8c22d3376f5b081826_2_582x750.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/f/d/fdf5094b430ebac03acf8f8c22d3376f5b081826_2_776x1000.png 2x"" width=""388""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">09</span><span class=""informations"">954×1228 48 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div> have softmaxlayer<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/37e81cb31ff870efb3047da2b2b0370efbdae595"" href=""https://discuss.pytorch.org/uploads/default/original/3X/3/7/37e81cb31ff870efb3047da2b2b0370efbdae595.png"" title=""50""><img alt=""50"" data-base62-sha1=""7YzyDVgynnZSWELibiffxAfm3Jz"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/3/7/37e81cb31ff870efb3047da2b2b0370efbdae595_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/3/7/37e81cb31ff870efb3047da2b2b0370efbdae595_2_525x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/3/7/37e81cb31ff870efb3047da2b2b0370efbdae595_2_525x500.png, https://discuss.pytorch.org/uploads/default/optimized/3X/3/7/37e81cb31ff870efb3047da2b2b0370efbdae595_2_787x750.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/3/7/37e81cb31ff870efb3047da2b2b0370efbdae595_2_1050x1000.png 2x"" width=""525""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">50</span><span class=""informations"">1470×1398 142 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div> Without softmax layer, in mobile I need to do the task about the softmax layer. Now I wanna accomplished the task about the softmax layer in model, I searched the doc and then cannot find the documents to tell me how to do it. By the way I have  a doubt about convert input type parameters, how to convert the multiArray to the image, I did a project about image classifier ,Thank for your replies! <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>How did you define this model?<br/><NewLine>Did you export a PyTorch model to ONNX or did you take another path?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah I convert the Model of PyTorch to onnx and then convert it to CoreML model. Based on the demo apple offered the model deal with the softmax layer and then only show the result. But the model only needs to deal with the multiArray to accomplish softmax layer work.<br/><NewLine>I mean any links to deal with the case.</p><NewLine><ul><NewLine><li>1,Change the input and output type - change input to image and then convert output to dictionary({string:float}</li><NewLine></ul><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you post a link to the demo, please?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t have a demo,I received one model from PyTorch. and then it doesn’t do softmax layer task. Now I want to do it in Pytorch model.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""9"" data-topic=""65424""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/a6a055/40.png"" width=""20""/> MarcSteven:</div><NewLine><blockquote><NewLine><p>Based on the demo apple offered the model deal with the softmax layer and then only show the result.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I was referring to this demo you mentioned.<br/><NewLine>Based on the graph you’ve posted I assume it’s a CoreML graph, as these operations do not seem to come from PyTorch directly.</p><NewLine><p>Could you explain your use case completely, i.e. what kind of model or code are you using now and what are you trying to achieve?</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>I used mlmodel from PyTorch- convert PyTorch to onnx and then convert it to mlmodel.<br/><NewLine>I don’t want to do the task of softmax layer in the field of mobile, so I hope let Machine learning engineer to change the code to do the softmax layer .<br/><NewLine>For instance how to change the input and output type.I did image classifier .</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you post a (minimal) code snippet to reproduce your workflow or link to a repository?<br/><NewLine>I’m currently unsure how to help without seeing what you actually did.</p><NewLine><p>If you cannot publish the model, just create a dummy model with a similar architecture.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry，perhaps my description is fuzzy.<br/><NewLine>For me I only want to solve this issue.<br/><NewLine>The result of precondition is multiArray, when I received the results, I need to deal with it on the mobile. For me I don’t want to do the task, so I know it’s best practices to do in the model.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> I mean don’t do the softmax layer in the mobile, is it best practices to in the pytorch?</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>It depends on your actual use case and the applied loss function.<br/><NewLine>For a multi-class classification problem, you wouldn’t use a softmax layer as your activation output, but pass the raw logits into <code>nn.CrossEntropyLoss</code> (or use <code>F.log_softmax</code> and <code>nn.NLLLoss</code> alternatively).<br/><NewLine>That being said, I’m still unsure, how the graphs were created and if any conversion library added these softmax layers or where these layers are generally coming from.</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>The project is a image classifier -emotion.<br/><NewLine>And then I want to convert the result to dict , but unfortunately the forward() function cannot return dict, how to do it</p><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can return <code>dicts</code> in your <code>forward</code> method:</p><NewLine><pre><code class=""lang-python"">class MyModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(MyModel, self).__init__()<NewLine>        self.fc1 = nn.Linear(1, 1)<NewLine>        self.fc2 = nn.Linear(1, 1)<NewLine>        <NewLine>    def forward(self, x):<NewLine>        x1 = self.fc1(x)<NewLine>        x2 = self.fc2(x)<NewLine>        return {'x1': x1, 'x2': x2}<NewLine><NewLine>model = MyModel()<NewLine>out_dict = model(torch.randn(1, 1))<NewLine>print(out_dict)<NewLine>&gt; {'x1': tensor([[0.5168]], grad_fn=&lt;AddmmBackward&gt;), 'x2': tensor([[0.2288]], grad_fn=&lt;AddmmBackward&gt;)}<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah absolutely it works.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/MarcSteven; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/MarcSteven; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/MarcSteven; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/MarcSteven; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/MarcSteven; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/MarcSteven; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/MarcSteven; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/MarcSteven; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/MarcSteven; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/MarcSteven; <NewLine> ,"REPLY_DATE 1: December 31, 2019,  3:26am; <NewLine> REPLY_DATE 2: December 31, 2019,  3:28am; <NewLine> REPLY_DATE 3: December 31, 2019,  3:30am; <NewLine> REPLY_DATE 4: December 31, 2019,  5:30am; <NewLine> REPLY_DATE 5: December 31, 2019,  6:10am; <NewLine> REPLY_DATE 6: December 31, 2019,  6:44am; <NewLine> REPLY_DATE 7: December 31, 2019,  9:10am; <NewLine> REPLY_DATE 8: December 31, 2019,  9:17am; <NewLine> REPLY_DATE 9: December 31, 2019,  9:19am; <NewLine> REPLY_DATE 10: December 31, 2019,  9:24am; <NewLine> REPLY_DATE 11: December 31, 2019,  9:32am; <NewLine> REPLY_DATE 12: December 31, 2019,  9:39am; <NewLine> REPLY_DATE 13: December 31, 2019,  9:43am; <NewLine> REPLY_DATE 14: December 31, 2019,  9:52am; <NewLine> REPLY_DATE 15: January 2, 2020, 11:43am; <NewLine> REPLY_DATE 16: January 3, 2020,  6:18am; <NewLine> REPLY_DATE 17: January 7, 2020,  2:50am; <NewLine> REPLY_DATE 18: January 7, 2020,  5:47am; <NewLine> REPLY_DATE 19: January 7, 2020,  5:45am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> REPLY 18 LIKES: ; <NewLine> REPLY 19 LIKES: ; <NewLine> 
64542,RuntimeError: invalid multinomial distribution (encountering probability entry &lt; 0),2019-12-19T14:19:44.460Z,1,992,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><p>I am working on an environment in Reinforcement Learning based which is based on Graph network. I am trying to implement the Actor-Critic network. Here I have two agents which are following the policy. I encounter this error (code is shown below with error) most of the time when I run the model for a certain amount of episodes with specific learning rates. Very few times the program executed successfully. But lot of times it throws an error stating ‘invalid multinomial distribution’. I am using Categorical Distribution and I dont think I have any values with probabilities less than zero.<br/><NewLine>I am pasting the line where I am getting the error.<br/><NewLine>I have gone through the answer (<a href=""https://discuss.pytorch.org/t/categorical-probs-sample-generates-runtimeerror-invalid-argument-2-invalid-multinomial-distribution-encountering-probability-entry-0/27386"">https://discuss.pytorch.org/t/categorical-probs-sample-generates-runtimeerror-invalid-argument-2-invalid-multinomial-distribution-encountering-probability-entry-0/27386</a>)  which was given for the same kind of error, but I could not figure out any solution.</p><NewLine><pre><code class=""lang-auto"">def getAction1(self, state):<NewLine>        state = torch.FloatTensor(state)<NewLine>        logits, _ = self.model1.forward(state)<NewLine>        dist = F.softmax(logits, dim = -1)<NewLine>        probs = Categorical(dist)<NewLine>        return probs.sample()<NewLine></code></pre><NewLine><p>The error</p><NewLine><pre><code class=""lang-auto""><NewLine>  File ""C:/Users/Prudhvinath.DESKTOP-09Q8801/sciebo/Thesis/JSSP/TwoAgents/20JobsTwoAgents.py"", line 164, in getAction1<NewLine>    return probs.sample()<NewLine><NewLine>  File ""C:\Users\Prudhvinath.DESKTOP-09Q8801\Anaconda3\lib\site-packages\torch\distributions\categorical.py"", line 107, in sample<NewLine>    sample_2d = torch.multinomial(probs_2d, 1, True)<NewLine><NewLine>RuntimeError: invalid multinomial distribution (encountering probability entry &lt; 0)<NewLine></code></pre><NewLine><p>The <code>Line164</code> redirects to this line in the main code <code>return probs.sample()</code></p><NewLine><p>Can you please help me here?</p><NewLine><p>thanks in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/reddymap,(Prudhvinath Reddymalla),reddymap,"December 20, 2019, 10:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello Prudvinath!</p><NewLine><aside class=""quote no-group quote-modified"" data-full=""true"" data-post=""1"" data-topic=""64542""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/r/8e8cbc/40.png"" width=""20""/> reddymap:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">def getAction2(self, state):<NewLine>        state = torch.FloatTensor(state)<NewLine>        logits, _ = self.model2.forward(state)<NewLine>        dist = F.softmax(logits, dim = -1)<NewLine>        probs = Categorical(dist)<NewLine>        return probs.sample()<NewLine></code></pre><NewLine><p>The error</p><NewLine><pre><code class=""lang-auto"">  File ""C:/Users/Prudhvinath.DESKTOP-09Q8801/sciebo/Thesis/JSSP/TwoAgents/20JobsTwoAgents.py"", line 164, in getAction1<NewLine>    return probs.sample()<NewLine><NewLine>RuntimeError: invalid multinomial distribution (encountering probability entry &lt; 0)<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>Your error message says that the error occurs “<code>in getAction1</code>,”<br/><NewLine>but you’ve posted code for <code>def getAction2(self, state)</code>.</p><NewLine><p>Could you double-check whether the code you’ve posted is<br/><NewLine>relevant to your error?</p><NewLine><p>(Also, when your error gives a line number, “<code>line 164</code>,” it would<br/><NewLine>be helpful if you could flag the line in question when you post your<br/><NewLine>code so we can be sure of what we’re looking at.)</p><NewLine><p>Best.</p><NewLine><p>K. Frank</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Frank,</p><NewLine><p>Thank you so much for the response.  I have just now checked my question. Since there are two <code> getAction</code> i.e <code>getAction1</code>and <code>getAction2</code>  in my code and both being similar, I have posted one.</p><NewLine><p>Now I have updated the question.</p><NewLine><p>Thanks and regards<br/><NewLine>Prudhvi</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/KFrank; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/reddymap; <NewLine> ,"REPLY_DATE 1: December 19, 2019,  4:12pm; <NewLine> REPLY_DATE 2: December 20, 2019, 10:28am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
64525,Ignore or punish illegal moves?,2019-12-19T11:56:20.965Z,0,215,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello.</p><NewLine><p>I’m training a bot to play a specific game. Some actions are considered illegal at particular states. I’m currently ignoring the illegal moves and selecting the second (or third if the second is illegal as well) highest q-value from outputted q-values for each action.</p><NewLine><p>I was being paranoid about the efficiency of this method and decided to take a look at what other people do. I came across this article: <a href=""https://medium.com/@carsten.friedrich/part-4-neural-network-q-learning-a-tic-tac-toe-player-that-learns-kind-of-2090ca4798d"" rel=""nofollow noopener"">https://medium.com/@carsten.friedrich/part-4-neural-network-q-learning-a-tic-tac-toe-player-that-learns-kind-of-2090ca4798d</a></p><NewLine><p>He is doing the same thing and justifies this method by saying ""We will be ignoring the fact that for a particular board state some positions would already be taken and no longer be an option. The player will deal with this when choosing a move and ignore illegal moves no matter what their Q values are. That is, we do not try to teach the Neural Network what moves are legal or not. Again, general advice you find is that this is the better approach ""</p><NewLine><p>Relevant code of mine looks like following for now:</p><NewLine><pre><code>#Exploit<NewLine>	   else:<NewLine>		   with torch.no_grad():<NewLine>			   tensor_from_net = policy_net(state).to(self.device)<NewLine>			   while (True):<NewLine>				   max_index = tensor_from_net.argmax()<NewLine>				#If illegal move is given as output by the model, punish that action and make it select an action again.<NewLine>				if max_index.item() not in available_actions:<NewLine>					tensor_from_net[max_index] = torch.tensor(-100)<NewLine>				else:<NewLine>					break<NewLine>			return max_index.unsqueeze_(0)<NewLine></code></pre><NewLine><p>That “punishment” in the while loop is temporary though, it is not affecting the net.</p><NewLine><ol><NewLine><li>Which approach is better? Ignore the illegal moves or punish the illegal moves?</li><NewLine><li>If ignoring the illegal moves is better, is my approach okay? What can be done to enhance it? And if you think punishing the illegal moves is better, I’d be appreciated if you give me a hint on how to implement it.</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/SUMMERSON,(Kubilay Yazoğlu),SUMMERSON,"December 19, 2019, 11:58am",,,,,
64369,Advice on implementing input and output data scaling,2019-12-17T21:56:42.699Z,0,824,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve searched for a while and I can’t find any examples or conclusive guidance on how to implement input or output scaling.</p><NewLine><p>Situation:<br/><NewLine>I am training an RNN on sequence input data to predict outputs (many-to-many).  Both the inputs and outputs are continuous-valued so I should scale them (to zero mean and unit variance).</p><NewLine><p>Obviously there is <a href=""https://discuss.pytorch.org/t/pytorch-tensor-scaling/38576"">no built-in function to do scaling in Pytorch</a>.  I thought <code>transforms.Normalize</code> might be suitable but every time I try to use it on my data it says <code>TypeError: tensor is not a torch image.</code> Is it only designed for images?</p><NewLine><p>I can easily <a href=""https://discuss.pytorch.org/t/pytorch-tensor-scaling/38576/2"">implement scaling by hand</a> or <a href=""https://discuss.pytorch.org/t/normalization-of-input-data-to-qnetwork/14800/2"">as a custom transformer as others have</a> so this is not the main issue.  The bigger question is where to put the code.</p><NewLine><ol><NewLine><li><NewLine><p>One way is to add this to my custom dataset.  I could initialize the scaler mean and variance when the dataset is initialized and then <a href=""https://discuss.pytorch.org/t/how-to-normalize-a-tensor-to-0-mean-and-1-variance/18766/2"">add the transformation to the dataset’s <code>__getitem__</code> method</a>  Since I’m also doing output scaling then I will have to ‘hang on’ to the transform method so I can do the inverse operation to the predictions before comparing them with the targets.</p><NewLine></li><NewLine><li><NewLine><p>Another option is to add it to the data loader.</p><NewLine></li><NewLine><li><NewLine><p>Some people think it should be <a href=""https://discuss.pytorch.org/t/input-normalization-inside-the-model/56260"">added to the estimator model</a> (RNN in this case).</p><NewLine></li><NewLine><li><NewLine><p>Keep it out of all objects and manually add it to the high-level code running the training and prediction tasks.</p><NewLine></li><NewLine></ol><NewLine><p>Please advise.</p><NewLine></div>",https://discuss.pytorch.org/u/billtubbs,(Bill Tubbs),billtubbs,"December 17, 2019, 10:59pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>In the absence of any responses to my question I developed a custom transform (based on <a href=""https://discuss.pytorch.org/t/pytorch-tensor-scaling/38576/2"">this example</a>) and pass two instances of it (one as <code>input transform</code>, one as <code>output_transform</code>) to the custom dataset at intialisation (option 1).  Then I will use the <code>output_transform.inverse_transform()</code> method to “decode” the predictions later when using the estimator.</p><NewLine><p>Here is the code if anyone is interested:</p><NewLine><pre><code class=""lang-auto"">class StandardScaler():<NewLine>    """"""Standardize data by removing the mean and scaling to<NewLine>    unit variance.  This object can be used as a transform<NewLine>    in PyTorch data loaders.<NewLine><NewLine>    Args:<NewLine>        mean (FloatTensor): The mean value for each feature in the data.<NewLine>        scale (FloatTensor): Per-feature relative scaling.<NewLine>    """"""<NewLine><NewLine>    def __init__(self, mean=None, scale=None):<NewLine>        if mean is not None:<NewLine>            mean = torch.FloatTensor(mean)<NewLine>        if scale is not None:<NewLine>            scale = torch.FloatTensor(scale)<NewLine>        self.mean_ = mean<NewLine>        self.scale_ = scale<NewLine><NewLine>    def fit(self, sample):<NewLine>        """"""Set the mean and scale values based on the sample data.<NewLine>        """"""<NewLine>        self.mean_ = sample.mean(0, keepdim=True)<NewLine>        self.scale_ = sample.std(0, unbiased=False, keepdim=True)<NewLine>        return self<NewLine><NewLine>    def __call__(self, sample):<NewLine>        return (sample - self.mean_)/self.scale_<NewLine><NewLine>    def inverse_transform(self, sample):<NewLine>        """"""Scale the data back to the original representation<NewLine>        """"""<NewLine>        return sample * self.scale_ + self.mean_<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/billtubbs; <NewLine> ,"REPLY_DATE 1: December 18, 2019,  5:11pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
30464,DQN - exploding loss problem,2018-11-25T08:11:31.861Z,0,407,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to solve an image localization problem similar to the paper below. In short, I am trying to train an agent with dqn to control a bounding box to localize an object. I pretrained a resnet on image patches that contain and do not contain the object of interest. Then I add Linear layers to the resnet to form the DQN.<br/><NewLine>The DQN has several actions like translation and scaling.</p><NewLine><p>The problem I am facing right now is an exploding loss problem. The loss keeps on increasing as I train it. With an Adam optimizer, I have tried learning rate ranging from 1e-3 to 1e-12 with batch size 50, 100 and 200. I also tried techniques like double dqn and prioritized experience replay. However, the exploding loss problem still cannot be alleviated. Therefore, I am writing to seek advice and suggestions on any possible reasons for this happening.</p><NewLine><p><a class=""onebox"" href=""http://slazebni.cs.illinois.edu/publications/iccv15_active.pdf"" rel=""nofollow noopener"" target=""_blank"">http://slazebni.cs.illinois.edu/publications/iccv15_active.pdf</a></p><NewLine><p>Thank you very much</p><NewLine></div>",https://discuss.pytorch.org/u/soulless,,soulless,"November 25, 2018,  6:08pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What are the activations on the outputs of your linear layers? Are you able to show the pytorch model code?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/timesler; <NewLine> ,"REPLY_DATE 1: December 1, 2019,  6:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
62315,Size Mismatch when passing a state batch to network,2019-11-27T14:21:15.849Z,0,383,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all. Since I’m a beginner in ML, this question or the design overall may sound silly, sorry about that. I’m open to any suggestions.</p><NewLine><p>I have a simple network with three linear layers one of which is output layer.</p><NewLine><pre><code>    self.fc1 = nn.Linear(in_features=2, out_features=12)<NewLine>	self.fc2 = nn.Linear(in_features=12, out_features=16)<NewLine>	self.out = nn.Linear(in_features=16, out_features=4)<NewLine></code></pre><NewLine><p>My states are consisting of two values, coordinate x and why.  That’s why input layer has two features.</p><NewLine><p>In main.py I’m sampling and extracting memories in ReplayMemory class and pass them to get_current function:</p><NewLine><pre><code>        experiences = memory.sample(batch_size)<NewLine>		states, actions, rewards, next_states = qvalues.extract_tensors(experiences)<NewLine><NewLine>		current_q_values = qvalues.QValues.get_current(policy_net, states, actions)<NewLine></code></pre><NewLine><p>Since a single state is consisting of two values, length of the states tensor is batchsize x 2 while length of the actions is batchsize. (Maybe that’s the problem?)</p><NewLine><p>When I pass “states” to my network in get_current function to obtain predicted q-values for the state, I get this error:</p><NewLine><p><strong>size mismatch, m1: [1x16], m2: [2x12]</strong></p><NewLine><p>It looks like it is trying to grab the states tensor as if it is a single state tensor. I don’t want that. In the tutorials that I follow, they pass the states tensor which is a stack of multiple states, and there is no problem. What am I doing wrong? <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>This is how I store an experience:</p><NewLine><pre><code class=""lang-auto"">memory.push(dqn.Experience(state, action, next_state, reward))<NewLine></code></pre><NewLine><p>This is my extract tensors function:</p><NewLine><pre><code class=""lang-auto"">def extract_tensors(experiences):<NewLine>    # Convert batch of Experiences to Experience of batches<NewLine>    batch = dqn.Experience(*zip(*experiences))<NewLine><NewLine>    state_batch = torch.cat(tuple(d[0] for d in experiences))<NewLine>    action_batch = torch.cat(tuple(d[1] for d in experiences))<NewLine>    reward_batch = torch.cat(tuple(d[2] for d in experiences))<NewLine>    nextState_batch = torch.cat(tuple(d[3] for d in experiences))<NewLine><NewLine>    print(action_batch)<NewLine><NewLine>    return (state_batch,action_batch,reward_batch,nextState_batch)<NewLine></code></pre><NewLine><p>Tutorial that I follow is this project’s tutorial.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/nevenp/dqn_flappy_bird/blob/master/dqn.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/nevenp/dqn_flappy_bird/blob/master/dqn.py"" rel=""nofollow noopener"" target=""_blank"">nevenp/dqn_flappy_bird/blob/master/dqn.py</a></h4><NewLine><pre><code class=""lang-py"">import os<NewLine>import random<NewLine>import sys<NewLine>import time<NewLine><NewLine>import cv2<NewLine>import numpy as np<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine><NewLine>from game.flappy_bird import GameState<NewLine><NewLine><NewLine>class NeuralNetwork(nn.Module):<NewLine><NewLine>    def __init__(self):<NewLine>        super(NeuralNetwork, self).__init__()<NewLine><NewLine>        self.number_of_actions = 2<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/nevenp/dqn_flappy_bird/blob/master/dqn.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Look between 148th and 169th lines. And especially 169th line where it passes the states batch to the network.</p><NewLine></div>",https://discuss.pytorch.org/u/SUMMERSON,(Kubilay Yazoğlu),SUMMERSON,"November 27, 2019,  2:50pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>SOLVED. It turned out that I didn’t know how to properly create 2d tensor. 2D Tensor must be like this:</p><NewLine><p>states = torch.tensor([[1, 1], [2,2]], dtype=torch.float)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SUMMERSON; <NewLine> ,"REPLY_DATE 1: November 28, 2019,  6:34pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
62119,Policy Gradient for NLP,2019-11-25T14:39:56.575Z,0,219,"<div class=""post"" itemprop=""articleBody""><NewLine><p>As a beginner in RL, I am totally at a loss on how to implement a policy gradient for NLP tasks (such as NMT).</p><NewLine><p>More specifically, I am trying to fine-tune a pre-trained Seq2Seq model via a policy gradient that gets rewards dependent on BLEU scores.</p><NewLine><p>However, I didn’t manage to find any resources (at least not fit for my experience?) on how to do this?</p><NewLine><p>From what I understand I need to create a loss based on the vocabulary distributions of each time step (of the decoder) and the reward received by a prediction weighted down by a base line. However I am confused how would this look since I am only generating a reward for the complete translation. Also what is the baseline supposed to be (a reward from a non-RL trained model ? ) ?</p><NewLine><p><code>loss = - torch.sum(torch.log( policy(state) * (reward - baseline)))</code><br/><NewLine><code>loss.backward()</code></p><NewLine><p>I know this is probably incredibly simple for some of you, but it goes way over my head <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/> Would appreciate some help for a noob.</p><NewLine></div>",https://discuss.pytorch.org/u/andrei_97,(Andrei),andrei_97,"November 25, 2019,  2:39pm",,,,,
61952,LSTM for RL with batching,2019-11-23T12:23:46.176Z,0,273,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,</p><NewLine><p>I hope this subject was not already created.</p><NewLine><p>I’m currently implementing IMPALA (<a href=""https://arxiv.org/abs/1802.01561"" rel=""nofollow noopener"">https://arxiv.org/abs/1802.01561</a>), a deep reinforcement learning method that aims to be distributed without suffering from the same bottlenecks as A3C (no GPU) or GA3C (instability). It basically collects trajectories and applies the v-trace algo to perform importance sampling.<br/><NewLine>It also uses an LSTM, and that’s where i’m struggling! I chose to have same length trajectories, but with a mask that tells when the LSTMs states should be reset, and that’s the implementation I decided to use :</p><NewLine><pre><code class=""lang-python"">x_out = []<NewLine><NewLine>for i in range(seq_len):<NewLine>    result, lstm_hxs = self.model.lstm(x[i], lstm_hxs)<NewLine>    lstm_hxs = [(done_mask[:, :, i]*state) for state in lstm_hxs]<NewLine>    x_out.append(result)<NewLine><NewLine>x = torch.stack(tensors=x_results, dim=0)<NewLine></code></pre><NewLine><p>I though of other methods, such as having different lengths episodes (the LSTM state is reset to zero once an episode is finished) and do padding, but I don’t know which approach would be the best…</p><NewLine><p>Thanks a lot !</p><NewLine></div>",https://discuss.pytorch.org/u/Sheepsody,(Victor Vialard),Sheepsody,"November 23, 2019, 12:23pm",,,,,
61504,Multi Term Loss for Policy Gradient Algorithm,2019-11-19T16:03:09.506Z,0,94,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve been attempting to implement a somewhat exotic Actor-Critic policy gradient algorithm (here’s the paper if you’re interested <a href=""https://arxiv.org/pdf/1802.05313.pdf"" rel=""nofollow noopener"">RL from Imperfect Deomonstrations</a>). The algorithm is similar to DQN, in that we have a network predicting Q values and use a target network to stabilize the results, but instead of updating with mse loss we calculate V from Q and do an update based on the following update equation:<br/><NewLine><img alt=""image"" data-base62-sha1=""gq5qnuoAloBdz2PAh4eGOY8cHcw"" height=""62"" src=""https://discuss.pytorch.org/uploads/default/original/3X/7/3/7315b61b60a8f1b1e8d7ce8d6a9df71a31f4e6d4.png"" width=""648""/><br/><NewLine>This part is tripping me up. How would I go about writing the loss function for this? I can calculate values for the V and Q terms, but I’m not sure how to actually handle something in this form.</p><NewLine></div>",https://discuss.pytorch.org/u/wykle,(CK),wykle,"November 19, 2019,  4:03pm",,,,,
60053,Why is PyTorch Maximizing the Loss?,2019-11-05T11:31:57.619Z,1,819,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a simple training loop that looks like this.</p><NewLine><pre><code class=""lang-auto"">optim = torch.optim.Adam(verify_net.parameters())<NewLine># The training is designed to continue untill timeout or goal<NewLine>while True:<NewLine>        optim.zero_grad()<NewLine>        # Forward pass<NewLine>        outputs = verify_net(inputs)<NewLine><NewLine>        # Compute how much bigger every label is be <NewLine>        # from the true label in the worst case<NewLine>        losses = torch.stack(<NewLine>            [upper_bound(outputs[i] - outputs[true_label]) \<NewLine>                for i in range(len(outputs)) if i != true_label])<NewLine><NewLine>        if (losses &lt; 0).all():<NewLine>            return True<NewLine><NewLine>        loss = torch.sum(losses)<NewLine>        print(loss)<NewLine>        loss.backward()<NewLine>        optim.step()<NewLine></code></pre><NewLine><p>The optimization goal is to make <code>outputs[true_label]</code> greater then all other outputs in a <em>worst case</em> that is computed by the <code>upper_bound</code> function</p><NewLine><p><code>upper_bound = lambda x: x[0] + torch.sum(torch.abs(x[1:]))</code></p><NewLine><p>If after any step all upper bounds <code>losses[i] &lt; 0</code> then the loop may break and we are happy. But till then the optimizer is to do its task. The output I get by printing the loss after each step is this</p><NewLine><pre><code class=""lang-auto"">tensor(632.8606, grad_fn=&lt;SumBackward0&gt;)<NewLine>tensor(668.1876, grad_fn=&lt;SumBackward0&gt;)<NewLine>tensor(698.2267, grad_fn=&lt;SumBackward0&gt;)<NewLine>tensor(733.7394, grad_fn=&lt;SumBackward0&gt;)<NewLine>tensor(764.9390, grad_fn=&lt;SumBackward0&gt;)<NewLine>tensor(799.8583, grad_fn=&lt;SumBackward0&gt;)<NewLine>tensor(834.7762, grad_fn=&lt;SumBackward0&gt;)<NewLine>tensor(861.3510, grad_fn=&lt;SumBackward0&gt;)<NewLine>tensor(895.9908, grad_fn=&lt;SumBackward0&gt;)<NewLine>tensor(930.6293, grad_fn=&lt;SumBackward0&gt;)<NewLine>tensor(965.2657, grad_fn=&lt;SumBackward0&gt;)<NewLine>tensor(1000.8009, grad_fn=&lt;SumBackward0&gt;)<NewLine></code></pre><NewLine><p>It seems like <code>Adam</code> is doing a fantastic job at maximizing the loss however this really isn’t the behavior I’d expect or want. My first attempt was to change <code>loss</code> into <code>-loss</code> but I had no luck because <code>Adam</code> starting to minimize the now negated loss <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/></p><NewLine><p>Any suggestions would be much appreciated. Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/Tom_Ginsberg,(Tom Ginsberg),Tom_Ginsberg,"November 5, 2019, 11:35am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><p>Have you tried reducing the learning rate? Or switch to a simple SGD?<br/><NewLine>High learning rates can cause the network to diverge sometimes.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Even with <code> lr=.000001</code> I get the same behaviour for both <code>Adam</code> and <code>SGD</code></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you try and simplify the problem to see if you still see the same behavior?</p><NewLine><ul><NewLine><li>Change the model to a very basic thing (like one single Linear layer)</li><NewLine><li>Change the loss to something simpler? (remove the absolute values)</li><NewLine></ul><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Tom_Ginsberg; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: November 5, 2019,  3:17pm; <NewLine> REPLY_DATE 2: November 5, 2019,  3:37pm; <NewLine> REPLY_DATE 3: November 5, 2019,  3:48pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
59776,Can someone debug my implementation of Policy Gradients (REINFORCE) for playing Atari breakout?,2019-11-01T13:07:14.616Z,0,302,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m learning about RL, and I’m struggling to get the simple REINFORCE algorithm working for playing Atari Breakout.</p><NewLine><p>Self-contained code here: <a href=""https://colab.research.google.com/drive/1x1vM8l1kXT3SuOeOR1E_a9I943_t5GcK"" rel=""nofollow noopener"">https://colab.research.google.com/drive/1x1vM8l1kXT3SuOeOR1E_a9I943_t5GcK</a></p><NewLine><p>My problem is that the algorithm overfits one action, meaning that one action is played nearly every step (probabilistic due to the sampling operation when choosing an action). Therefore it doesn’t learn to play properly.</p><NewLine><p>I must have an implementation error somewhere because as I understand it actions which lead to negative reward should be relatively discouraged, but they’re not.</p><NewLine><p>Is my implementation of the loss correct? My understanding is that you only backprop through the neuron that codes the action you actually selected, but the gradient gets distributed to all other neurons by the softmax?</p><NewLine><p>If someone can figure out what I did wrong I’d be very grateful. It would also be great to know how you went about figuring out what was wrong.</p><NewLine></div>",https://discuss.pytorch.org/u/deadbeef,(Harry),deadbeef,"November 1, 2019,  1:07pm",,,,,
10419,Why no eval() and train() mode switch in the DQN tutorial?,2017-11-26T10:19:38.037Z,1,491,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, it really confuses me that why the <a href=""http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noopener"">DQN tutorial</a> has no model.eval() and model.train(), since there are several BatchNorm layers in the model.</p><NewLine><p>As I understand, model.eval() should be put before calling select_action(). And in function optimize_model(), model.train() should be put before calling the first model.forward() while model.eval() should be put before calling the second one. Is it correct?</p><NewLine></div>",https://discuss.pytorch.org/u/Finspire13,(Finspire13),Finspire13,"November 26, 2017, 10:20am",3 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am also confused and I was going to post a similar question.</p><NewLine><p>My guess is that you should run everything in train mode or only the action selection should be with model.eval(). I think training might become unstable if you use results from one model in eval and another one in train mode.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pedroveloso13; <NewLine> ,"REPLY_DATE 1: October 23, 2019,  3:55pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
56581,Extracting reduced dimension data from autoencoder in pytorch,2019-09-23T00:28:47.941Z,3,971,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!</p><NewLine><p>I have defined my autoencoder in pytorch as following:</p><NewLine><pre><code class=""lang-auto"">        self.encoder = nn.Sequential(<NewLine>            nn.Conv2d(input_shape[0], 32, kernel_size=1, stride=1),<NewLine>            nn.ReLU(),<NewLine>            nn.Conv2d(32, 64, kernel_size=1, stride=1),<NewLine>            nn.ReLU(),<NewLine>            nn.Conv2d(64, 64, kernel_size=1, stride=1),<NewLine>            nn.ReLU()<NewLine>        )<NewLine><NewLine>        self.decoder = nn.Sequential(<NewLine>            nn.Conv2d(64, 64, kernel_size=1, stride=1),<NewLine>            nn.ReLU(),<NewLine>            nn.Conv2d(64, 32, kernel_size=1, stride=1),<NewLine>            nn.ReLU(),<NewLine>            nn.Conv2d(32, input_shape[0], kernel_size=1, stride=1),<NewLine>            nn.ReLU(),<NewLine>            nn.Sigmoid()<NewLine>        )<NewLine></code></pre><NewLine><p>Everything works fine, I can train it.</p><NewLine><p>But, what I need is to get a reduced dimension encoding which requires creating a new linear layer of the dimension N much lower than the image dimension so that I can extract the activations.</p><NewLine><p>If anybody can help me with fitting a linear layer in the decoder part I would appreciate (i know how to Flatten() the data, but I guess I need to “unflatten” it again to interface with the Conv2d layer again)</p><NewLine><p>Thank you in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/guitarman,,guitarman,"September 23, 2019, 12:28am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>After doing some research on pytorch I have gotten the following</p><NewLine><pre><code class=""lang-auto"">self.encoder = nn.Sequential(<NewLine>    nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),<NewLine>    nn.ReLU(),<NewLine>    nn.Conv2d(32, 64, kernel_size=4, stride=2),<NewLine>    nn.ReLU(),<NewLine>    nn.Conv2d(64, 8, kernel_size=3, stride=1),<NewLine>    nn.ReLU(),<NewLine>    nn.MaxPool2d(7, stride=1)<NewLine>)<NewLine><NewLine>self.decoder = nn.Sequential(<NewLine>    nn.ConvTranspose2d(8, 64, kernel_size=3, stride=1),<NewLine>    nn.ReLU(),<NewLine>    nn.Conv2d(64, 32, kernel_size=4, stride=2),<NewLine>    nn.ReLU(),<NewLine>    nn.Conv2d(32, input_shape[0], kernel_size=8, stride=4),<NewLine>    nn.ReLU(),<NewLine>    nn.Sigmoid()<NewLine>)<NewLine></code></pre><NewLine><p>This gives me a 8-dimensional bottleneck at the output of the encoder which works fine torch.Size([1, 8, 1, 1]) . Just the way I wanted to in the “bottleneck” layer.</p><NewLine><p>What I cannot do is train the autoencoder with:</p><NewLine><pre><code class=""lang-auto"">    def forward(self, x):<NewLine>        x = self.encoder(x)<NewLine>        x = self.decoder(x)<NewLine>        return x<NewLine></code></pre><NewLine><p>The decoder gives me an error:</p><NewLine><p><code>Calculated padded input size per channel: (3 x 3). Kernel size: (4 x 4). Kernel size can't be greater than actual input size</code></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The output of your first <code>nn.ConvTranspose2d</code> layer will be <code>[batch_size, 64, 3, 3]</code>, which is too small for the next one, which uses a kernel size of 4.<br/><NewLine>You could use a kernel size of 4 for the first <code>nn.ConvTranpose2d</code> layer.<br/><NewLine>This, however, will yield an error in the last one, since you are using <code>nn.Conv2d</code> layers afterwards in your decoder. Is that a typo or do you really want to lower the spatial size again?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for the fast reply.</p><NewLine><p>I have managed to get my autoencoder “right” <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/> , I have a bottleneck layer of 2x2x2 which gives me 8 clusters like this:</p><NewLine><pre><code class=""lang-auto"">        self.encoder = nn.Sequential(<NewLine>            nn.Conv2d(1, 32, kernel_size=8, stride=4),<NewLine>            nn.ReLU(),<NewLine>            nn.Conv2d(32, 64, kernel_size=4, stride=2),<NewLine>            nn.ReLU(),<NewLine>            nn.Conv2d(64, 2, kernel_size=3, stride=1),<NewLine>            nn.ReLU(),<NewLine>            nn.MaxPool2d(6, stride=1)<NewLine>        )<NewLine><NewLine>        self.decoder = nn.Sequential(<NewLine>            nn.ConvTranspose2d(2, 64, kernel_size=3, stride=1),<NewLine>            nn.ReLU(),<NewLine>            nn.ConvTranspose2d(64, 32, kernel_size=8, stride=4),<NewLine>            nn.ReLU(),<NewLine>            nn.ConvTranspose2d(32, 1, kernel_size=8, stride=4),<NewLine>            nn.ReLU(),<NewLine>            nn.Sigmoid()<NewLine>        )<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.encoder(x)<NewLine>        x = self.decoder(x)<NewLine>        return x<NewLine></code></pre><NewLine><p>and my training is simple, the loss is how much the prediction differs from the original image:</p><NewLine><pre><code class=""lang-auto""><NewLine>    distance = nn.MSELoss()<NewLine><NewLine>    loss = distance(states,output)<NewLine></code></pre><NewLine><p>but now the problem is that the network is not learning (the loss is constant at 10k which is bad)</p><NewLine><p>even with a high learning rate:</p><NewLine><p><code>optimizer_encoder = optim.SGD(model2.parameters(), lr=0.005)</code></p><NewLine><p>I have no idea why <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have removed</p><NewLine><pre><code>       nn.ReLU(),<NewLine>       nn.Sigmoid()<NewLine></code></pre><NewLine><p>on the encoder end and now the loss is in rather normal range and decreasing.</p><NewLine><p>If anyone has the same problem or is creating an unsupervised autoencoder <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/guitarman; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/guitarman; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/guitarman; <NewLine> ,"REPLY_DATE 1: September 23, 2019,  6:16pm; <NewLine> REPLY_DATE 2: September 24, 2019,  1:58am; <NewLine> REPLY_DATE 3: September 24, 2019,  1:22am; <NewLine> REPLY_DATE 4: September 24, 2019,  1:58am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
14800,Normalization of input data to Qnetwork,2018-03-12T10:43:38.475Z,3,3767,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello there,</p><NewLine><p>I am well known with that a “normal” neural network should use normalized input data so one variable does not have a bigger influence on the weights in the NN than others.</p><NewLine><p>But what if you have a Qnetwork where your training data and test data can differ a lot and can change over time in a continous problem?</p><NewLine><p>My idea was to just run a normal run without normalization of input data and then see the variance and mean from the input datas of the run and then use the variance and mean to  normalize my input data of my next run.<br/><NewLine>But what is the standard to do in this case?</p><NewLine><p>Best regards Søren Koch</p><NewLine></div>",https://discuss.pytorch.org/u/qLience,(Søren Koch),qLience,"March 12, 2018, 10:43am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There is no standard as far as I know. What I usualy do is this :<br/><NewLine>( this is from <a href=""https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm"">https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm</a> )</p><NewLine><pre><code class=""lang-python"">class Normalizer():<NewLine>    def __init__(self, num_inputs):<NewLine>        self.n = torch.zeros(num_inputs)<NewLine>        self.mean = torch.zeros(num_inputs)<NewLine>        self.mean_diff = torch.zeros(num_inputs)<NewLine>        self.var = torch.zeros(num_inputs)<NewLine><NewLine>    def observe(self, x):<NewLine>        self.n += 1.<NewLine>        last_mean = self.mean.clone()<NewLine>        self.mean += (x-self.mean)/self.n<NewLine>        self.mean_diff += (x-last_mean)*(x-self.mean)<NewLine>        self.var = torch.clamp(self.mean_diff/self.n, min=1e-2)<NewLine><NewLine>    def normalize(self, inputs):<NewLine>        obs_std = torch.sqrt(self.var)<NewLine>        return (inputs - self.mean)/obs_std<NewLine></code></pre><NewLine><p>Then each time I get a new state, I just do:</p><NewLine><pre><code class=""lang-python"">normalizer.observe(new_state)<NewLine>new_state = normalizer.normalize(new_state)<NewLine>''' <NewLine>new_state must be a simple tensor, <NewLine>if it's a variable, use new_state.data<NewLine>'''<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks! btw. what is you data obs data type ? it is just because i get a error due to i am using a list</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah I see, here there is a weird thing since the input of <code>observe</code> is a variable while the input of <code>normalize</code> is a tensor. Let me correct it so everything must be simple tensor (input and output).</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do we agree that this kind of “online” normalization is not injective ? In the sense that two distinct inputs that are observed and normalized at different time may be mapped to the same output value.</p><NewLine><p>Furthermore, this mapping / filtering / normalization is not guaranteed to be monotonic (especially in the beginning when very few data have been observed).</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>A bit of a late reply, but:<br/><NewLine>this kind of normalization made me struggle for quite some time. Using it, my DQN applied on cartpole could not learn properly. So you definitely need to be very careful.<br/><NewLine>I will from now on either not normalize at all or freeze the update of the normalization parameters after some initial steps.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/qLience; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/SunHaozhe; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/NotNANtoN; <NewLine> ,"REPLY_DATE 1: December 17, 2019,  9:55pm; <NewLine> REPLY_DATE 2: April 16, 2018,  8:42am; <NewLine> REPLY_DATE 3: April 17, 2018,  8:18am; <NewLine> REPLY_DATE 4: July 12, 2019, 11:52am; <NewLine> REPLY_DATE 5: September 3, 2019,  7:21am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
54213,CNN not training,2019-08-24T16:49:40.951Z,0,128,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am completely new to CNN’s, and I do not quite know how to design or use them efficiently. That being said, I am attempting to build a CNN that learns to play Pac-man with reinforcement learning. I have trained it for about 3 hours and have seen little to no improvement. My observation space is 3 channels * 15 * 19, and there are 5 actions. Here is my code, I am open to any and all suggestions. Thanks for all your help.</p><NewLine><pre><code>from minipacman import MiniPacman as pac<NewLine>from torch import nn<NewLine>import torch<NewLine>import random<NewLine>import torch.optim as optimal<NewLine>from torch.autograd import Variable<NewLine>import matplotlib.pyplot as plt<NewLine>import numpy as np<NewLine>import keyboard<NewLine><NewLine><NewLine>loss_fn = nn.MSELoss()<NewLine>epsilon = 1<NewLine>env = pac(""regular"", 1000)<NewLine>time = 0<NewLine>action = random.randint(0, 4)<NewLine>q = np.zeros(3)<NewLine>alpha = 0.01<NewLine>gamma = 0.9<NewLine>tick = 0<NewLine>decay = 0.9999<NewLine><NewLine><NewLine>class Value_Approximator (nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Value_Approximator, self).__init__()<NewLine>        # Convolution 1<NewLine>        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=2)<NewLine>        self.relu1 = nn.ReLU()<NewLine><NewLine>        # Max pool 1<NewLine>        self.maxpool1 = nn.MaxPool2d(kernel_size=2)<NewLine><NewLine>        # Convolution 2<NewLine>        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)<NewLine>        self.relu2 = nn.ReLU()<NewLine><NewLine>        # Max pool 2<NewLine>        self.maxpool2 = nn.MaxPool2d(kernel_size=2)<NewLine><NewLine>        # Fully connected 1 (readout)<NewLine>        self.fc1 = nn.Linear(384, 5)<NewLine><NewLine>    def forward(self, x):<NewLine>        # Convolution 1<NewLine>        out = self.cnn1(x)<NewLine>        out = self.relu1(out)<NewLine><NewLine>        # Max pool 1<NewLine>        out = self.maxpool1(out)<NewLine><NewLine>        # Convolution 2<NewLine>        out = self.cnn2(out)<NewLine>        out = self.relu2(out)<NewLine><NewLine>        # Max pool 2<NewLine>        out = self.maxpool2(out)<NewLine><NewLine>        # Resize<NewLine>        # Original size: (100, 32, 7, 7)<NewLine>        # out.size(0): 100<NewLine>        # New out size: (100, 32*7*7)<NewLine>        out = out.view(out.size(0), -1)<NewLine><NewLine>        # Linear function (readout)<NewLine>        out = self.fc1(out)<NewLine><NewLine>        return out<NewLine><NewLine>approx = Value_Approximator()<NewLine>optimizer = optimal.SGD(approx.parameters(), lr=alpha)<NewLine><NewLine><NewLine>while time &lt; 50000:<NewLine>    print(""Time: ""+str(time))<NewLine>    print(""Epsilon: ""+str(epsilon))<NewLine>    print()<NewLine>    time += 1<NewLine>    state = env.reset()<NewLine>    tick = 0<NewLine><NewLine>    epsilon *= decay<NewLine><NewLine>    if epsilon &lt; 0.1:<NewLine>        epsilon = 0.1<NewLine><NewLine>    while True:<NewLine>        tick += 1<NewLine>        state = np.expand_dims(state, 1)<NewLine>        state = state.reshape(1, 3, 15, 19)<NewLine>        q = approx.forward(torch.from_numpy(state))[0]<NewLine><NewLine>        if random.uniform(0, 1) &lt; epsilon:<NewLine>            action = env.action_space.sample()<NewLine>        else:<NewLine>            _, action = torch.max(q, -1)<NewLine>            action = action.item()<NewLine>        new_state, reward, terminal, _ = env.step(action)<NewLine>        show_state = new_state<NewLine>        new_state = np.expand_dims(new_state, 1)<NewLine>        new_state = state.reshape(1, 3, 15, 19)<NewLine><NewLine>        q_new = approx.forward(torch.from_numpy(new_state).type(torch.FloatTensor))[0]  # "" find Q (s', a') ""<NewLine>        #  find optimal action Q value for next step<NewLine>        _, new_max = torch.max(q_new, -1)<NewLine>        new_max = new_max.item()<NewLine><NewLine>        q_target = q.clone()<NewLine>        q_target = Variable(q_target.data)<NewLine><NewLine>        #  update target value function according to TD<NewLine>        q_target[action] = reward + torch.mul(new_max, gamma)  # "" reward + gamma*(max(Q(s', a')) ""<NewLine><NewLine>        loss = loss_fn(q, q_target)  # "" reward + gamma*(max(Q(s', a')) - Q(s, a)) ""<NewLine>        # Update original policy according to Q_target ( supervised learning )<NewLine>        approx.zero_grad()<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine><NewLine>        #  Q and Q_target should converge<NewLine>        if time % 100 == 0:<NewLine>            state = torch.FloatTensor(show_state).permute(1, 2, 0).cpu().numpy()<NewLine><NewLine>            plt.subplot(131)<NewLine>            plt.title(""Imagined"")<NewLine>            plt.imshow(state)<NewLine>            plt.subplot(132)<NewLine>            plt.title(""Actual"")<NewLine>            plt.imshow(state)<NewLine>            plt.show(block=False)<NewLine>            plt.pause(0.000001)<NewLine><NewLine>        if keyboard.is_pressed('1'):<NewLine>            torch.save(approx.state_dict(), 'trained-10000.mdl')<NewLine>        if keyboard.is_pressed('9'):<NewLine>            torch.save(approx.state_dict(), 'trained-10000.mdl')<NewLine><NewLine>        if terminal or tick &gt; 100:<NewLine>            plt.close()<NewLine>            break<NewLine><NewLine>        state = new_state<NewLine><NewLine><NewLine>torch.save(approx.state_dict(), 'trained-10000.mdl')</code></pre><NewLine></div>",https://discuss.pytorch.org/u/Preston_Willis,(Preston Willis),Preston_Willis,"August 24, 2019,  5:28pm",,,,,
54064,Can Policy Gradient run in parallel with pytorch?,2019-08-22T14:51:36.878Z,0,134,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In policy gradient, we need to run several batches to collect the trajectories and update the network. Can this process run in parallel in pytorch? I tried it before but it seemed that Pytorch cannot transmit gradient between threads. Any advice? Many thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/zikun_liu,(zikun liu),zikun_liu,"August 22, 2019,  2:51pm",,,,,
53659,TD(lambda) backward view,2019-08-18T12:46:49.824Z,0,352,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi ,<br/><NewLine>i am trying to apply backward view algorithm but i can’t able to update my  weight manually.<br/><NewLine>here is the formula for updating weight:</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/62b879f0ea33ef937e1ab6c46b686222f3606f32"" href=""https://discuss.pytorch.org/uploads/default/original/2X/6/62b879f0ea33ef937e1ab6c46b686222f3606f32.png"" title=""Capture.PNG""><img alt=""Capture"" data-base62-sha1=""e5k1Uu1EnXLD63eYi01bFqnMPsK"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/6/62b879f0ea33ef937e1ab6c46b686222f3606f32_2_10x10.png"" height=""166"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/6/62b879f0ea33ef937e1ab6c46b686222f3606f32_2_690x166.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/6/62b879f0ea33ef937e1ab6c46b686222f3606f32_2_690x166.png, https://discuss.pytorch.org/uploads/default/original/2X/6/62b879f0ea33ef937e1ab6c46b686222f3606f32.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/6/62b879f0ea33ef937e1ab6c46b686222f3606f32.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Capture.PNG</span><span class=""informations"">889×215 18.3 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>I try to update weight like this:</p><NewLine><pre><code class=""lang-auto"">def learn(self, batch_state, batch_next_state, batch_reward, batch_action):<NewLine>        self.optimizer.zero_grad()<NewLine>        outputs = self.dqn(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)<NewLine>        next_outputs = self.dqn(batch_next_state).detach().max(1)[0]<NewLine>        target = self.gamma*next_outputs + batch_reward<NewLine>        loss = F.mse_loss(outputs, target)<NewLine>        loss.backward()<NewLine>        #self.optimizer.step()<NewLine>        Weight =[]<NewLine>        with torch.no_grad():<NewLine>            for p in self.dqn.parameters():<NewLine>                Weight.append(p)<NewLine>                Weight = self.backward_view(outputs, target, batch_reward, p, loss, p.grad)<NewLine>                p.copy_(Weight)<NewLine>                <NewLine>    def backward_view(self, outputs, target, batch_reward, p, loss, grad):<NewLine>        Weight = []<NewLine>        alpha, lambd = 0.0001, 0.5<NewLine>        for bs in range(32):<NewLine>            i = 0<NewLine>            for parameter in p:<NewLine>                self.E[i] = torch.Tensor(lambd*self.gamma*self.E[i]) + grad[bs]<NewLine>                parameter -= alpha * self.E[i]*(target[bs] - outputs[bs]) <NewLine>                Weight.append(parameter)<NewLine>                i += 1<NewLine>        return Weight<NewLine><NewLine>but got following error:<NewLine>TypeError: new(): data must be a sequence (got float)<NewLine><NewLine></code></pre><NewLine><p>can anyone tell me how to update weight according to this formula using pytorch, please?</p><NewLine></div>",https://discuss.pytorch.org/u/Swakshar_Deb,(Swakshar Deb),Swakshar_Deb,"August 18, 2019,  2:25pm",,,,,
53517,Entropy loss decrease sharply when training a drl agent,2019-08-16T04:51:48.822Z,0,184,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I am training a drl agent with discrete action space using ppo-based algorithm, the training process is not stable and when training to 1800 episode around, the entropy loss deceases sharply and the agent got a really bad average reward. I have used nn.LayerNorm and state, reward is constrained to around (-1,1),  Does anyone has some good suggestions?<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/55f9e37641b91888097025f71a026d997bb8baab"" href=""https://discuss.pytorch.org/uploads/default/original/2X/5/55f9e37641b91888097025f71a026d997bb8baab.png"" title=""newplot(1).png""><img alt=""newplot(1)"" data-base62-sha1=""cgzY3ByK6C6nXD8I3kqxAlOP0y7"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/55f9e37641b91888097025f71a026d997bb8baab_2_10x10.png"" height=""443"" src=""https://discuss.pytorch.org/uploads/default/original/2X/5/55f9e37641b91888097025f71a026d997bb8baab.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">newplot(1).png</span><span class=""informations"">700×450 9.25 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/a7f5a7000c0cefccb32313edf94a2a05d6dc0670"" href=""https://discuss.pytorch.org/uploads/default/original/2X/a/a7f5a7000c0cefccb32313edf94a2a05d6dc0670.png"" title=""newplot(2).png""><img alt=""newplot(2)"" data-base62-sha1=""nXQ2O4QRGLCM2vzfc0MbxT9EccE"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/a/a7f5a7000c0cefccb32313edf94a2a05d6dc0670_2_10x10.png"" height=""443"" src=""https://discuss.pytorch.org/uploads/default/original/2X/a/a7f5a7000c0cefccb32313edf94a2a05d6dc0670.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">newplot(2).png</span><span class=""informations"">700×450 15 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/5034087c9c559cafd7c19029d804532341c1f27d"" href=""https://discuss.pytorch.org/uploads/default/original/2X/5/5034087c9c559cafd7c19029d804532341c1f27d.png"" title=""newplot(3).png""><img alt=""newplot(3)"" data-base62-sha1=""brvG2NOEKAhvaoP35KhJf0pvDkN"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/5034087c9c559cafd7c19029d804532341c1f27d_2_10x10.png"" height=""443"" src=""https://discuss.pytorch.org/uploads/default/original/2X/5/5034087c9c559cafd7c19029d804532341c1f27d.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">newplot(3).png</span><span class=""informations"">700×450 14.8 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/08e9611461ee5aa5c6d013d39403331f626f08b3"" href=""https://discuss.pytorch.org/uploads/default/original/2X/0/08e9611461ee5aa5c6d013d39403331f626f08b3.png"" title=""newplot(4).png""><img alt=""newplot(4)"" data-base62-sha1=""1gPPBRvqf6QzcjC8XcpbyGmKHqb"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/0/08e9611461ee5aa5c6d013d39403331f626f08b3_2_10x10.png"" height=""443"" src=""https://discuss.pytorch.org/uploads/default/original/2X/0/08e9611461ee5aa5c6d013d39403331f626f08b3.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">newplot(4).png</span><span class=""informations"">700×450 16.7 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/007,,007,"August 16, 2019,  4:51am",,,,,
53485,How to deal with the limited action space in Reinforcement Learning?,2019-08-15T20:04:56.404Z,0,135,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m new in reinforcement learning and I’m recently using A3C for continuous control.</p><NewLine><p>My action i.e the output of the actor network is a vector with size(64*1) and these 64 values are used as the probabilities, so all of them should be in the scale of (0,1).  I use the normal distribution with the decayed variance to choose the real action and calculate the log-probabilities, but if I do so, the action will be out of range.</p><NewLine><p>I tried to clip the action between 0 and 1, but that would destroy the normal distribution and lead to some bad results. Have you ever had the same problem? Any advice? Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/zikun_liu,(zikun liu),zikun_liu,"August 15, 2019,  8:26pm",,,,,
52659,How to choose distributions for multi-dimensional output?,2019-08-06T10:13:22.447Z,0,134,"<div class=""post"" itemprop=""articleBody""><NewLine><p>As for my reinforcement learning network, the dimension of “action” is 2.<br/><NewLine>I have to chose a distribution for it, then sample from the distribution to choose action</p><NewLine><p>Can I just  choose to generate two seperate distributions for it, or have to choose other distributions, such as GMM or multivariat normal?</p><NewLine></div>",https://discuss.pytorch.org/u/Xiaodong_MEI,(Xiaodong MEI),Xiaodong_MEI,"August 6, 2019, 10:13am",,,,,
51872,Different workers act exactly the same,2019-07-29T03:25:38.932Z,0,132,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I implement DPPO algorithm in pytorch, I found that different workers act exactly the same, and I don’t know why. In my game environment, every worker faces the same initial state, and I use torch.distributions.sample function to sample actions in worker. In ‘main’ function, I use</p><NewLine><p>np.random.seed(params.seed)<br/><NewLine>torch.manual_seed(params.seed)<br/><NewLine>torch.backends.cudnn.deterministic = True<br/><NewLine>torch.backends.cudnn.benchmark = False</p><NewLine><p>to guarantee the reproducibility of the algorithm. So I think it might have some influence on the variation between workers. And I wonder how to solve that problem. Help me!</p><NewLine></div>",https://discuss.pytorch.org/u/007,,007,"July 29, 2019,  3:25am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The main、model、distributions function is as follows:</p><NewLine><ol><NewLine><li>main: <a href=""https://paste.ubuntu.com/p/JFDgrrkWPF/"" rel=""nofollow noopener"">https://paste.ubuntu.com/p/JFDgrrkWPF/</a><NewLine></li><NewLine><li>model: <a href=""https://paste.ubuntu.com/p/NgBdXPgdnb/"" rel=""nofollow noopener"">https://paste.ubuntu.com/p/NgBdXPgdnb/</a><NewLine></li><NewLine><li>distribution: <a href=""https://paste.ubuntu.com/p/3Y9fMc6HzF/"" rel=""nofollow noopener"">https://paste.ubuntu.com/p/3Y9fMc6HzF/</a><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Problem solved! Thanks to the code :<a href=""https://github.com/ikostrikov/pytorch-a3c/blob/master/train.py"" rel=""nofollow noopener"">https://github.com/ikostrikov/pytorch-a3c/blob/master/train.py</a><br/><NewLine>I added     torch.manual_seed(params.seed + rank) in each of the worker to guarantee the variation between different workers and the reproduction of my algorithm.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/007; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/007; <NewLine> ,"REPLY_DATE 1: July 29, 2019,  3:31am; <NewLine> REPLY_DATE 2: July 29, 2019,  8:42am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
51390,Implementing a q-learning agent in a turn-based game,2019-07-23T13:21:47.005Z,1,229,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>This year I got introduced to neural networks and machine learning in Uni, and now I want to create a bot for a game this summer. The game is called Awale, and it is fairly simple. All you have to do is choose one out of six cups every turn, and your opponent does the same. Under some circumstances you win some points, and the player with the most points in the end wins the game.</p><NewLine><p>So I wanted to make a Deep Q-learning agent for this. I have built something like this before, for an assignment, but I thought the assignment was a bit oversimplified and I wanted a bit more of a challenge. I have built the architecture for the game, and i am now almost ready to train my agents. The only thing I am doubting, and I hope you can help me with this, is the following.</p><NewLine><p>Every turn in the game, I have to do a forward pass three times (once for me, once for the opponent, and once again for me), while I only want to calculate the loss for the first forward pass. I understand that a call to <code>with torch.nograd()</code> i can do forward passes without autograd doing calculations in the background, so I had the idea to surround the second and third forward pass in the  <code>nograd()</code>, but is this the best way to go here?</p><NewLine><p>I hope you can help me understand,</p><NewLine><p>Stijn</p><NewLine><p>Edited for clarity</p><NewLine></div>",https://discuss.pytorch.org/u/AuguB,(Augustijn de Boer),AuguB,"July 23, 2019,  1:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Augustijn,</p><NewLine><p>the idea sounds really interesting and it would be great, if you could keep us updated about your progress here. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""51390""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/augub/40/14343_2.png"" width=""20""/> AuguB:</div><NewLine><blockquote><NewLine><p>I understand that a call to <code>with torch.nograd()</code> i can do forward passes without autograd doing calculations in the background, so I had the idea to surround the second and third forward pass in the <code>nograd()</code> , but is this the best way to go here?</p><NewLine></blockquote><NewLine></aside><NewLine><p>I think that’s the best approach to make sure only the output of the first forward pass has a valid <code>grad_fn</code> to calculate the gradients.<br/><NewLine>Let us know, if you encounter any problems with this approach.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, thanks for your reply.</p><NewLine><p>I am still trying to get this absolutely right, so let me rephrase my question, jus to be absolutely sure that i am doing this right.</p><NewLine><p>Model M takes an input s_t and gives an output Q_t. Q_t will be the output with which we want to calculate the loss.<br/><NewLine>Now we want to create the target vector, and since this is a Q-learning approach, we have to do some more forward passes.</p><NewLine><p>Using Q_t, we can compute a_t, and with a_t, we can compute r_t and s_{t+1} (these are all game mechanics).<br/><NewLine>We can feed s_{t+1} to the network again, and acquire Q_{t+1}. This will be used to acquire a_{t+1}, and that will in turn be used to acquire r_{t+1} and s_{t+2}. Now we are almost there. We feed s_{t+2} to the network, and acquire Q_{t+2}. Now we run a_t through the network again to get a ‘copy’ of Q_t (call it Q_t’) and replace Q_t’[a_t] with r_t+theta*max(Q_{t+2}) (theta is a factor that discounts future rewards). We then compute the loss with loss_function(Q_t, Q_t’).</p><NewLine><p>The tricky thing here is that both the output and the target vectors that are fed into the loss function are output of the same network. I only want the backward pass to run through the part of the graph that is resposible for producing Q_t.</p><NewLine><p>Am i doing this right by wrapping everything except the first forward pass (where M takes input s_t and generates output Q_t) with torch.nograd()?</p><NewLine><p>If this is the case then i can start training.</p><NewLine><p>Kind regards,</p><NewLine><p>Stijn</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Stijn,</p><NewLine><p>I’m not really familiar with RL and I’m not sure to see all possible pitfalls in this approach.<br/><NewLine>A “safe” approach could be to detach the target before calculating the loss.<br/><NewLine>This would make sure to only backpropagate using the first input parameter to the criterion.<br/><NewLine>Would that work for your approach?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/AuguB; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 23, 2019, 10:23pm; <NewLine> REPLY_DATE 2: July 25, 2019,  5:22pm; <NewLine> REPLY_DATE 3: July 28, 2019,  8:08pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
5111,Synchronous updates for DPPO,2017-07-22T11:07:05.639Z,5,2921,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to implement Deepmind’s Distributed Proximal Policy Optimization (<a href=""https://arxiv.org/abs/1707.02286"">https://arxiv.org/abs/1707.02286</a>)</p><NewLine><p>But I am not really confident with multiprocessing, and I don’t see how to realise the synchronous updates. The idea is to have a chief process that collects the gradients sent by the training workers. When the chief receives enough gradients (more than N gradients) it sums them and does the optimizer’s update. The workers that send the gradients have to wait for the chief’s update before continuing their runs. (the paper explains it in a much nicer way in the supplemental section.)</p><NewLine><p>Also, unfortunately, all the hyperparameters are not provided by Google. I have to tune it by myself.</p><NewLine><p>My code is there:<br/><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""32""/><NewLine><a href=""https://github.com/alexis-jacq/Pytorch-DPPO"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""400"" src=""https://avatars1.githubusercontent.com/u/9195965?s=400&amp;v=4"" width=""400""/><NewLine><h3><a href=""https://github.com/alexis-jacq/Pytorch-DPPO"" target=""_blank"">alexis-jacq/Pytorch-DPPO</a></h3><NewLine><p>Pytorch implementation of Distributed Proximal Policy Optimization: https://arxiv.org/abs/1707.02286 - alexis-jacq/Pytorch-DPPO</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>So far, I am trying to solve a simple pendulum environment (and it’s not even converging).<br/><NewLine>Don’t hesitate to contribute, I need help!</p><NewLine></div>",https://discuss.pytorch.org/u/alexis-jacq,(Alexis David Jacq),alexis-jacq,"July 22, 2017, 11:07am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>ELF (rlpytorch) provides source code for synchronous updates:<br/><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""32""/><NewLine><a href=""https://github.com/facebookresearch/ELF/tree/master/rlpytorch"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""400"" src=""https://avatars1.githubusercontent.com/u/16943930?s=400&amp;v=4"" width=""400""/><NewLine><h3><a href=""https://github.com/facebookresearch/ELF/tree/master/rlpytorch"" rel=""nofollow noopener"" target=""_blank"">facebookresearch/ELF</a></h3><NewLine><p>An End-To-End, Lightweight and Flexible Platform for Game Research - facebookresearch/ELF</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually, nvm. ELF is still asynchronous, but it uses batching &amp; bucketing so that it’s less asynchronous than normal A3C.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>ELF updates look synchronous to me. When used in this context it mean using the same clock so the processes are actually done sequentialially.</p><NewLine><p>You could use that as reference but instead of when they update parameters from queue one by one just change that to how you want the gradients accumulated then updated.</p><NewLine><p>You will want to use a queque to do this synchronous</p><NewLine><p>Checkout the code on this part for refs <a href=""https://github.com/facebookresearch/ELF/blob/master/rlpytorch/utils.py"" rel=""nofollow noopener"">https://github.com/facebookresearch/ELF/blob/master/rlpytorch/utils.py</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/dgriff"">@dgriff</a> Have you gotten ELF working with gym?<br/><NewLine>I’m currently trying to modify ELF source code to work with gym.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not sure what you mean ELF suppose to be alternative to gym.</p><NewLine><p>Or do you mean their actor critic example they provide which looks to be the same model as tensorpack example that does gym which has same author so probably is the case. So I would assume same performance as well which is a quite a good implementation. But have no plans on changing this example to run in gym sorry.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think I solved my synchronous problem. I added 3 function to my shared models:</p><NewLine><pre><code class=""lang-python"">    def cum_grads(self):<NewLine>        for name, p in self.named_parameters():<NewLine>            if p.grad is not None:<NewLine>                val = self.__getattr__(name+'_grad')<NewLine>                val += p.grad.data<NewLine>                self.__setattr__(name+'_grad', val)<NewLine><NewLine>    def reset_grads(self):<NewLine>        self.zero_grad()<NewLine>        for name, p in self.named_parameters():<NewLine>            if p.grad is not None:<NewLine>                val = self.__getattr__(name+'_grad')<NewLine>                val = p.grad.data<NewLine>                self.__setattr__(name+'_grad', val)<NewLine><NewLine>    def synchronize(self):<NewLine>        for name, p in self.named_parameters():<NewLine>            val = self.__getattr__(name+'_grad')<NewLine>            p._grad = Variable(val)<NewLine></code></pre><NewLine><p>When a worker have computed a new loss, I use cum_grads:</p><NewLine><pre><code class=""lang-auto"">ensure_shared_grads(policy, shared_p)<NewLine>shared_p.cum_grads()<NewLine>counter.increment()<NewLine></code></pre><NewLine><p>And the chief just loops as follow:</p><NewLine><pre><code class=""lang-python"">def chief(rank, params, traffic_light, counter, shared_p, shared_v, optimizer_p, optimizer_v):<NewLine>    while True:<NewLine>        time.sleep(1)<NewLine>        # workers will wait after last loss computation<NewLine>        if counter.get() &gt; params.update_treshold:<NewLine>            shared_p.synchronize()<NewLine>            shared_v.synchronize()<NewLine>            optimizer_p.step()<NewLine>            optimizer_v.step()<NewLine>            counter.reset()<NewLine>            shared_p.reset_grads()<NewLine>            shared_v.reset_grads()<NewLine>            traffic_light.switch() # workers start new loss computation<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s looks to be somewhat still asynchronous. One quick test you can do to see if synchronous is that you should no longer have to use a special shared optimizer and should now work with just the regular pytorch optimizers</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Good point! I don’t longer need the shared optimizer. It still works with a simple one.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>In fact, my implementation only solves the invertpendulum environment. I am searching for any help, but the paper is not clearly describing the pseudocode (a lot of sub-steps and hyperparameters are not detailed either in DPPO paper and PPO_clip paper). Besides, the only implementation I found is the official <a href=""https://github.com/openai/baselines"">TF implementation</a> by John Schulman, which I almost rigorously translated into Pytorch, without success.</p><NewLine><p>Unfortunately for me, I can’t even try the TF implementation, since python3 version of mujoco seems incompatible with my machine <img alt="":confused:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/confused.png?v=5"" title="":confused:""/>. It could have at least provided me with some helpful expectation, like what looks like a “working” learning curve.</p><NewLine><p>If any one has some useful documentation or an implementation that works (any language/library) of any close algorithm (TRPO, A2C), please tell me</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Little confused on somethings I see in <a href=""http://train.py"" rel=""nofollow noopener"">train.py</a></p><NewLine><p>What’s the whole updating old model right before you call backward and step about?</p><NewLine><p>Also are you asking for help on converting model to solve other environments or that model is not able to solve other environments?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>What’s the whole updating old model right before you call backward and step about?</p><NewLine></blockquote><NewLine><p>If I do it after the backward step or anywhere else, the old and new models will have same weights… I can also update the old model once, it is unclear in the paper if this update must be done once or at every steps.</p><NewLine><blockquote><NewLine><p>Also are you asking for help on converting model to solve other environments or that model is not able to solve other environments?</p><NewLine></blockquote><NewLine><p>According to <a href=""https://arxiv.org/abs/1707.06347"">the ppo paper</a>, the set of hyperparameters I use in <code>ppo.py</code> should work in every environment with no modification at all. But only if I change both the batch size and the number of steps to 1000, it solves the InvertPendulum (quickly) and the InvertDoublePendulum (much slower than in the paper). With batch size = 64 and number of steps = 2048 as suggested by the paper, It solves nothing.</p><NewLine><p>So I am quite certain there is something wrong somewhere in the training loop, but I can’t figure out what is wrong.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://github.com/ikostrikov/pytorch-a2c-ppo-acktr:"">https://github.com/ikostrikov/pytorch-a2c-ppo-acktr:</a> an implementation that seems to work properly!<br/><NewLine>Now I can go back to this project and check what was wrong with my code.</p><NewLine><p>Thanks and Congrats <a class=""mention"" href=""/u/ilya_kostrikov"">@Ilya_Kostrikov</a>!</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Finally, I found what was wrong, thanks to Oleg Klimov (<a href=""https://github.com/openai/baselines/issues/87#issuecomment-363235729"">https://github.com/openai/baselines/issues/87#issuecomment-363235729</a>)!</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, David. This is a really clear and concise implementation of DPPO! Only a question, how do you guarantee the variation in actions between different workers. In my game environment, every worker faces the same initial state, and I use torch.distributions.sample function to sample actions in worker. Unfortunately, I found that the continuous actions sampled by different workers are exactly the same! I use</p><NewLine><p>np.random.seed(params.seed)<br/><NewLine>torch.manual_seed(params.seed)<br/><NewLine>torch.backends.cudnn.deterministic = True<br/><NewLine>torch.backends.cudnn.benchmark = False</p><NewLine><p>to guarantee the reproducibility of the algorithm. So I think it might have some influence on the variation between workers. And I wonder how you solve it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ethancaballero; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ethancaballero; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/dgriff; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ethancaballero; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/dgriff; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/dgriff; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/dgriff; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/007; <NewLine> ,"REPLY_DATE 1: July 24, 2017, 12:01am; <NewLine> REPLY_DATE 2: July 24, 2017,  1:14am; <NewLine> REPLY_DATE 3: July 24, 2017,  5:55am; <NewLine> REPLY_DATE 4: July 24, 2017,  7:48am; <NewLine> REPLY_DATE 5: July 24, 2017,  1:06pm; <NewLine> REPLY_DATE 6: July 24, 2017,  5:38pm; <NewLine> REPLY_DATE 7: July 26, 2017, 11:19am; <NewLine> REPLY_DATE 8: July 26, 2017, 12:14pm; <NewLine> REPLY_DATE 9: August 5, 2017,  4:16pm; <NewLine> REPLY_DATE 10: August 5, 2017,  8:17pm; <NewLine> REPLY_DATE 11: August 7, 2017,  7:25am; <NewLine> REPLY_DATE 12: October 14, 2017,  8:52am; <NewLine> REPLY_DATE 13: February 27, 2018,  4:36pm; <NewLine> REPLY_DATE 14: July 28, 2019,  1:56pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> 
37518,Inverting Gradients - Gradient of critic network output wrt action,2019-02-18T09:57:21.017Z,2,428,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello there,<br/><NewLine>I want to implement the inverting gradients as shown in the paper "" Deep Reinforcement Learning in Parametrized Action Space"". But firstly I need to have gradient of the critic network’s output ( Q(s,a)  ) with respect to the action ( dQ(s,a) / da ) which is (6) equation in the paper.</p><NewLine><p>I can write the loss as,<br/><NewLine>loss = value_net(state, policy_net(state))</p><NewLine><p>But how can I differentiate this network with only respect to the action instead state ?</p><NewLine></div>",https://discuss.pytorch.org/u/frknayk,(furkan),frknayk,"February 18, 2019,  9:57am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/frknayk"">@frknayk</a> I am trying to implement this paper too and having trouble implement its gradient inversion. Have you figured it out?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have found tensorflow implementation<br/><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicon.ico"" width=""16""/><NewLine><a href=""https://github.com/stevenpjg/ddpg-aigym"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars2.githubusercontent.com/u/15150767?s=400&amp;amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/stevenpjg/ddpg-aigym"" rel=""nofollow noopener"" target=""_blank"">stevenpjg/ddpg-aigym</a></h3><NewLine><p>Continuous control with deep reinforcement learning - Deep Deterministic Policy Gradient (DDPG) algorithm implemented in OpenAI Gym environments - stevenpjg/ddpg-aigym</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>But later I decided to use loss = value_net(state,policy_net(state)) for some reasons</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/frknayk"">@frknayk</a> Were you able to implement Inverting Gradients? If yes can you please share the snippet?</p><NewLine><p>Thanks</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/soulless; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/frknayk; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Rahul_Patel; <NewLine> ,"REPLY_DATE 1: April 3, 2019,  7:15pm; <NewLine> REPLY_DATE 2: April 16, 2019, 12:36pm; <NewLine> REPLY_DATE 3: July 27, 2019,  4:55am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
51537,DCGAN beginner curiosities: HWC size? Batch size?,2019-07-24T17:54:55.956Z,0,108,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am new to PyTorch and working with GANs. I looked around online already but I had a couple of basic beginner curiosities if anyone could help.</p><NewLine><p>Not sure I understand three dimensional jpegs. HWC size? Is that similar to jpeg + frame size? I noticed a transformer resizes training images. Does the transformer crop images that aren’t square in a certain way?</p><NewLine><p>Also any advice on batch size minimum to train? I created my own dataset but it’s generating static for fake images. I don’t think I have enough input. Is there a good amount of images to think about in terms of minimum training data? And anywhere I could learn more about GAN inputs?</p><NewLine><p>Any ideas are welcomed!<br/><NewLine>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/peteaj,(Peter Jacobson),peteaj,"July 24, 2019,  5:54pm",,,,,
16130,The difference between actor-critic example and A2C?,2018-04-08T20:37:36.988Z,1,2047,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In PyTorch example repo there is an example code of <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py"" rel=""nofollow noopener"">actor-critic</a>. I am quite curious that what is the difference between this code and <a href=""https://github.com/openai/baselines/tree/master/baselines/a2c"" rel=""nofollow noopener"">A2C</a> ? Because it also learns a baseline for advantage estimate which sounds like what A2C is doing ?</p><NewLine></div>",https://discuss.pytorch.org/u/zuoxingdong,(Xingdong Zuo),zuoxingdong,"April 8, 2018,  8:41pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">for (log_prob, value), r in zip(saved_actions, rewards):<NewLine>        reward = r - value.data[0]<NewLine>        policy_losses.append(-log_prob * reward)<NewLine>        value_losses.append(F.smooth_l1_loss(value, Variable(torch.Tensor([r]))))<NewLine></code></pre><NewLine><p>So in the current code, if you see how the loss is calculated, you’d see that the critic is taught to learn the reward of a particular state, action pair, given by r, or the action-value function rather than the advantage function. In A2C, the critic would learn the advantage function.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>actor-critic is a kind of method.Advantage actor-critic(A2C) is a good implement of this method.<br/><NewLine>I don’t think what you have said is all right. Actually the <strong>reward</strong> in the code you stick is the advantage function.In the latest code, the <strong>reward</strong> has been changed to <strong>advantage</strong>.And the advantage function can be represented as <img alt=""QGRVDKGOS~%7D896%24DL6J0YKP"" data-base62-sha1=""pdMe3vITdXkXpqGQ2iYZws2bqi0"" height=""25"" src=""https://discuss.pytorch.org/uploads/default/original/2X/b/b0c50e8079cf25dfe13466cf2f029308c9ec618c.png"" width=""158""/>，which is equal to the code : <strong>advantage = r - value.data[0]</strong></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/keerthanpg; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/huiqing; <NewLine> ,"REPLY_DATE 1: April 9, 2018,  3:11am; <NewLine> REPLY_DATE 2: July 23, 2019,  4:25am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> 
50925,Is this possible to give gradient clipping in a specific layer?,2019-07-18T09:17:07.062Z,0,272,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have been trying to implement <a href=""https://arxiv.org/pdf/1507.06527.pdf"" rel=""nofollow noopener"">DRQN</a>. In the last page, I can see “LSTM’s gradients were clipped to a value of ten to ensure learning stability” in  Appednix C : Experimental Details. Is this possible? Only thing I know is to give clipvalue as a parameter of optimizer.</p><NewLine></div>",https://discuss.pytorch.org/u/Uk_Jo,(Uk Jo),Uk_Jo,"July 18, 2019,  9:17am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, <a href=""https://pytorch.org/docs/stable/nn.html#clip-grad-norm"" rel=""nofollow noopener"">torch.nn.utils.clip_grad_norm_</a> will clip the gradients of the passed parameters.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 20, 2019, 12:14am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
50976,How to create weights-shared network for auxiliary tasks,2019-07-18T14:47:56.297Z,0,430,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, guys. Recently, i have been engaged in rewriting the tensorflow implement of <a href=""https://github.com/miyosuda/unreal"" rel=""nofollow noopener"">miyosuda/unreal</a> of paper <a href=""https://arxiv.org/pdf/1611.05397.pdf"" rel=""nofollow noopener"">Reinforcement learning with unsupervised auxiliary tasks.</a> using pytorch. In thise paper, it describes three auxiliary tasks which used the shared weights of Conv and LSTM layers created by base A3C.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" href=""https://raw.githubusercontent.com/miyosuda/unreal/master/doc/network0.png"" rel=""nofollow noopener"" title=""network0.png""><img alt=""network"" height=""500"" src=""https://raw.githubusercontent.com/miyosuda/unreal/master/doc/network0.png"" width=""385""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">network0.png</span><span class=""informations"">837×1086</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div>. In my implement, I just add a flag parameters in module’s <code>forward</code> method in order to decide which task to be the output. Here is my implement:</p><NewLine><pre><code class=""lang-python"">from __future__ import division<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>from torch.autograd import Variable<NewLine>from utils import norm_col_init, weights_init, normalized_columns_initializer<NewLine>from torch.nn.init import uniform_<NewLine>import numpy as np<NewLine><NewLine><NewLine>class UNREAL(torch.nn.Module):<NewLine><NewLine>    def __init__(self, in_channels,<NewLine>                 action_size,<NewLine>                 enable_pixel_control=True,<NewLine>                 enable_value_replay=True,<NewLine>                 enable_reward_prediction=True):<NewLine>        super(UNREAL, self).__init__()<NewLine><NewLine>        self._action_size = action_size<NewLine>        self._enable_pixel_control = enable_pixel_control<NewLine>        self._enable_value_replay = enable_value_replay<NewLine>        self._enable_reward_prediction = enable_reward_prediction<NewLine>        # A3C base<NewLine>        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=8, stride=4)  # RGB -&gt; 16<NewLine>        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2)  # 16-&gt;32<NewLine>        # FC<NewLine>        self.linear_fc = nn.Linear(in_features=2592, out_features=256)<NewLine>        # self.lstm = nn.LSTMCell(256 + self._action_size + 1, 256)  # conv + action_size + reward(1)<NewLine>        self.lstm = nn.LSTM(256 + self._action_size + 1, 256, 1, batch_first=True)  # input, hidden, layer=1<NewLine>        # Actor-Critic<NewLine>        self.critic_linear = nn.Linear(256, 1)<NewLine>        self.actor_linear = nn.Linear(256, self._action_size)<NewLine>        # softmax<NewLine>        self.softmax = nn.Softmax(1)<NewLine>        # <NewLine>        self._conv_init(self.conv1)<NewLine>        self._conv_init(self.conv2)<NewLine>        self._fc_init(self.linear_fc)<NewLine><NewLine>        # aux task<NewLine>        if self._enable_pixel_control:<NewLine>            self._create_pixel_control()<NewLine>            self._fc_init(self.pc_linear)<NewLine>            self._conv_init(self.pc_deconv_a)<NewLine>            self._conv_init(self.pc_deconv_v)<NewLine>        if self._enable_value_replay:<NewLine>            pass  # value replay<NewLine>        if self._enable_reward_prediction:<NewLine>            self._create_reward_prediction()<NewLine>            self._fc_init(self.rp_linear)<NewLine><NewLine><NewLine><NewLine><NewLine>    def _conv_init(self, conv: nn.Conv2d):<NewLine>        d = 1.0 / np.sqrt(conv.in_channels * conv.kernel_size[0] * conv.kernel_size[1])<NewLine>        uniform_(conv.weight.data, a=-d, b=d)<NewLine>        uniform_(conv.bias.data, a=-d, b=d)<NewLine><NewLine>    def _fc_init(self, linear: nn.Linear):<NewLine>        d = 1.0 / np.sqrt(linear.in_features)<NewLine>        uniform_(linear.weight.data, a=-d, b=d)<NewLine>        uniform_(linear.bias.data, a=-d, b=d)<NewLine><NewLine>    def _create_pixel_control(self):<NewLine><NewLine>        self.pc_linear = nn.Linear(256, 9 * 9 * 32)<NewLine>        self.pc_deconv_v = nn.ConvTranspose2d(in_channels=32, out_channels=1, kernel_size=4, stride=2)<NewLine>        self.pc_deconv_a = nn.ConvTranspose2d(in_channels=32, out_channels=self._action_size, stride=2, kernel_size=4)<NewLine><NewLine>    def _create_reward_prediction(self):<NewLine>        self.rp_linear = nn.Linear(9 * 9 * 32 * 3, 3)<NewLine><NewLine>    def forward(self, task_type, states, hx=None, cx=None, last_action_rewards=None):<NewLine><NewLine>        x = F.relu(self.conv1(states))<NewLine>        x = F.relu(self.conv2(x))<NewLine>        # rp <NewLine>        if task_type == 'rp':<NewLine>            x = x.view(1, 9 * 9 * 32 * 3)  # state: [batch, h, w, 3]<NewLine>            x = self.rp_linear(x)<NewLine>            return x  # logits<NewLine>        x = x.view(-1, 2592)<NewLine><NewLine>        #  unroll<NewLine>        x = F.relu(self.linear_fc(x))  # unroll_step, 256<NewLine>        #  last_action, last_reward<NewLine>        x = torch.cat([x, last_action_rewards], dim=1)<NewLine>        # LSTM flatten<NewLine>        x = x.view(-1, 1, 256 + self._action_size + 1)  # (unroll_step, 1, 256 + action_size + 1),<NewLine><NewLine>        x, (hx, cx) = self.lstm(x, (hx, cx))  # (batch, seq, dim)<NewLine>        x = x.squeeze(dim=1) <NewLine>        if task_type == 'a3c':<NewLine>            return self.critic_linear(x), self.actor_linear(x), hx, cx  # crtic: [batch,1], actor: [batch,action_size], hx, cx<NewLine>        elif task_type == 'pc':<NewLine>            x = F.relu(self.pc_linear(x))<NewLine>            x = torch.reshape(x, [-1, 32, 9, 9])  # NCHW<NewLine>            pc_deconv_v = F.relu(self.pc_deconv_v(x))<NewLine>            pc_deconv_a = F.relu(self.pc_deconv_a(x))<NewLine>            pc_deconv_a_mean = torch.mean(pc_deconv_a, dim=1, keepdim=True)  <NewLine>            # pc_q<NewLine>            pc_q = pc_deconv_v + pc_deconv_a - pc_deconv_a_mean<NewLine>            # max q<NewLine>            pc_q_max = torch.max(pc_q, dim=1, keepdim=False)[0] <NewLine>            return pc_q, pc_q_max, hx, cx<NewLine>        elif task_type == 'vr':<NewLine>            return self.critic_linear(x)  # a3c <NewLine><NewLine></code></pre><NewLine><h1>Environment</h1><NewLine><ul><NewLine><li>pytorch 1.1</li><NewLine><li>python 3.6.5</li><NewLine><li>enable pytorch built-in multiprocess, 8 agents</li><NewLine><li>shared model is hold in GPU, local network in memory</li><NewLine></ul><NewLine><p>But when i run the code, i find it is harder for the network to convergence. May be I made mistakes? Thanks <img alt="":smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smile.png?v=9"" title="":smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/ddayzzz,,ddayzzz,"July 18, 2019,  2:56pm",,,,,
50067,Proper way to generate gradient of log_prob(random_variable) where random variable is not sampled from the distribution,2019-07-09T01:38:25.082Z,0,138,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>Grad of network parameters are all zero even though the loss value is non-zero. I don’t want to sample from the distribution and train (not implementing vanilla REINFORCE)</p><NewLine><pre><code class=""lang-auto"">probs = policy(state)<NewLine>m = Catagorical(probs)<NewLine>log_prob = m.log_prob(action) # **action** generated from another source(not a neural network)<NewLine>loss =  - log_prob * R<NewLine>loss.backward()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/anirudhs96,(Anirudh Suresh),anirudhs96,"July 9, 2019,  1:38am",,,,,
49632,GPU and its Memory usage is very low,2019-07-03T16:18:56.692Z,0,346,"<div class=""post"" itemprop=""articleBody""><NewLine><p>GPU usage is very less(1% to 10%, sometimes 30%) and GPU memory usage is also very less(less than 10%).<br/><NewLine>I am using transformer model with reinforcement learning to solve graph related problems. So my data is basically, list of nodes with its features.</p><NewLine><p>I feel that resources are not being used efficiently, kindly help me resolve this.</p><NewLine><p>Here is how my DataLoader look like:<br/><NewLine>training_dataloader = DataLoader(training_dataset, batch_size=opts.batch_size, num_workers=32, pin_memory=True)<br/><NewLine>my data is basically, random based generation like below,</p><NewLine><p>def generate_instance(size, prize_type):<br/><NewLine># Details see paper<br/><NewLine>MAX_LENGTHS = {<br/><NewLine>20: 2.,<br/><NewLine>50: 3.,<br/><NewLine>100: 4.<br/><NewLine>}</p><NewLine><pre><code>loc = torch.FloatTensor(size, 2).uniform_(0, 1)<NewLine>depot = torch.FloatTensor(2).uniform_(0, 1)<NewLine># Methods taken from Fischetti et al. 1998<NewLine>if prize_type == 'const':<NewLine>    prize = torch.ones(size)<NewLine>elif prize_type == 'unif':<NewLine>    prize = (1 + torch.randint(0, 100, size=(size, ))) / 100.<NewLine>else:  # Based on distance to depot<NewLine>    assert prize_type == 'dist'<NewLine>    prize_ = (depot[None, :] - loc).norm(p=2, dim=-1)<NewLine>    prize = (1 + (prize_ / prize_.max(dim=-1, keepdim=True)[0] * 99).int()).float() / 100.<NewLine><NewLine>return {<NewLine>    'loc': loc,<NewLine>    # Uniform 1 - 9, scaled by capacities<NewLine>    'prize': prize,<NewLine>    'depot': depot,<NewLine>    'max_length': torch.tensor(MAX_LENGTHS[size])<NewLine>}<NewLine></code></pre><NewLine><p>Thanks!<br/><NewLine>Anil</p><NewLine></div>",https://discuss.pytorch.org/u/aswamy,(Anil),aswamy,"July 3, 2019,  4:18pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What is your batch size? You could try increasing it and see what your GPU usage is after that.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Batch size is 512 and I think it’s large enough. But I’ll try increasing</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Prerna_Dhareshwar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/aswamy; <NewLine> ,"REPLY_DATE 1: July 3, 2019,  4:49pm; <NewLine> REPLY_DATE 2: July 3, 2019,  5:02pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
4003,What&rsquo;s the right way of implementing policy gradient?,2017-06-14T04:20:37.367Z,10,15098,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone!<br/><NewLine>I’m trying to perform this gradient update directly, without computing loss.<br/><NewLine><img height=""39"" src=""https://discuss.pytorch.org/uploads/default/original/1X/9d03fd1985406773f4ae4e494d4555f356f3080f.png"" width=""382""/><br/><NewLine>But I simply haven’t seen any ways I can achieve this. Looks like first I need some function to compute the gradient of policy, and then somehow feed it to the backward function.</p><NewLine></div>",https://discuss.pytorch.org/u/Eddie_Li,(Eddie Li),Eddie_Li,"June 14, 2017,  4:20am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think you should look at this topic explaining the tensor.reinforce method:<br/><NewLine><aside class=""quote quote-modified"" data-post=""1"" data-topic=""1294""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/yunjey/40/1386_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/what-is-action-reinforce-r-doing-actually/1294"">What is action.reinforce(r) doing actually?</a> <a class=""badge-wrapper bullet"" href=""/c/reinforcement-learning""><span class=""badge-category-bg"" style=""background-color: #AB9364;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""A section to discuss RL implementations, research, problems"">reinforcement-learning</span></a><NewLine></div><NewLine><blockquote><NewLine>    Hi, <NewLine>I am studying RL with reinforcement/reinforce.py in pytorch/examples. I have some questions about it. <NewLine><NewLine><NewLine>What does <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py#L68"" rel=""nofollow noopener"">action.reinforce®</a> internally do ? <NewLine><NewLine><NewLine>Below is REINFORCE update rule where v_t is a return. We need to do gradient “ascent” as below but if we use <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py#L71"" rel=""nofollow noopener"">optimizer.step</a>, it is gradient “descent”. Is [action.reinforce®]((<a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""nofollow noopener"">https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py</a>) multiplying log probability by -r? Then, it makes sense. <NewLine>[image] <NewLine><NewLine><NewLine>In <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py#L70"" rel=""nofollow noopener"">autograd.ba…</a><NewLine></blockquote><NewLine></aside><NewLine><br/><NewLine>If the action is the result of a sampling, calling <code>action.reinforce(r)</code> acts as a policy gradient.<br/><NewLine>You can find a code example of implementation here:<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""nofollow noopener"" target=""_blank"">pytorch/examples/blob/master/reinforcement_learning/reinforce.py</a></h4><NewLine><pre><code class=""lang-py"">import argparse<NewLine>import gym<NewLine>import numpy as np<NewLine>from itertools import count<NewLine><NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine>from torch.distributions import Categorical<NewLine><NewLine><NewLine>parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')<NewLine>parser.add_argument('--gamma', type=float, default=0.99, metavar='G',<NewLine>                    help='discount factor (default: 0.99)')<NewLine>parser.add_argument('--seed', type=int, default=543, metavar='N',<NewLine>                    help='random seed (default: 543)')<NewLine>parser.add_argument('--render', action='store_true',<NewLine>                    help='render the environment')<NewLine>parser.add_argument('--log-interval', type=int, default=10, metavar='N',<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Or you can just have<br/><NewLine>loss = samples_logprob * (reward - baseline)</p><NewLine><p>(logprob is the output of the network, and reward and baseline are leaf variables)</p><NewLine><p>If you do backward, the gradient backpropogated is equivalent to the policy gradient.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi everyone, I am new to pytorch, my question is a litter different from <a class=""mention"" href=""/u/eddie_li"">@Eddie_Li</a>,<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/7d4f429dc060210bd980e721a2bbdb275ec2fe04"" href=""https://discuss.pytorch.org/uploads/default/original/2X/7/7d4f429dc060210bd980e721a2bbdb275ec2fe04.png"" title=""屏幕快照 2017-08-09 下午12.06.07.png""><img alt=""07"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/7/7d4f429dc060210bd980e721a2bbdb275ec2fe04_2_10x10.png"" height=""119"" src=""https://discuss.pytorch.org/uploads/default/original/2X/7/7d4f429dc060210bd980e721a2bbdb275ec2fe04.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">屏幕快照 2017-08-09 下午12.06.07.png</span><span class=""informations"">740×128 8.89 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine>is there any way to change grad after backward?<br/><NewLine>I mean loss = right side of the red line<br/><NewLine>After backward, can I do pai(theta) * loss.grad, and then optimizer.step() ?<br/><NewLine>Is that way work?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>the right side of your red line looks rather the gradient of your loss?</p><NewLine><p>Your loss would be something like</p><NewLine><pre><code class=""lang-auto"">loss = - torch.sum(torch.log( policy(state) * (reward - baseline)))<NewLine></code></pre><NewLine><p>then you compute the gradient of this loss with respect to all the parameters/variables that requires a gradient in your code by calling:</p><NewLine><pre><code class=""lang-auto"">loss.backward()<NewLine></code></pre><NewLine><p>and if you created, before the training loop, an optimizer associated to your policy like this:</p><NewLine><pre><code class=""lang-auto"">optim_policy = optim.MyOptimizer( policy.parameters(), lr = wathever)<NewLine></code></pre><NewLine><p>you can update your optimizer simply like this, after each backward call of your loss:</p><NewLine><pre><code class=""lang-auto"">optim_policy.step()<NewLine></code></pre><NewLine><p>the parameters of your policy (theta) will be updated with the gradient of your loss in order to minimize your loss.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your help, but my question is a little different<br/><NewLine>In my formula A* is a set of all possible episodes<br/><NewLine>when I update parameters , I want something like<br/><NewLine>policy * grad of loss<br/><NewLine>or in other words<br/><NewLine>(p1<em>loss(p1) + …+pN</em>loss(pN)) , where p1+…+pN = 1<br/><NewLine>your solution is<br/><NewLine>(loss(p1)+…+loss(pN)) / N<br/><NewLine>I want to change grad between loss.backward() and  optim_policy.step()<br/><NewLine>I tried loss.grad() but it is None  <img alt="":frowning:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/frowning.png?v=5"" title="":frowning:""/></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Then, I am not sure to understand your question… You want to use the policy as weights for the gradient update ? that way, you should re-write your loss:</p><NewLine><pre><code class=""lang-auto"">weighted_loss = p1*loss(p1) + .. + pn*loss(pn)<NewLine>weighted_loss.backward()<NewLine>optimizer.step()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Emm, here is the full formula<br/><NewLine>policy is the weight of loss.grad, not the weight of loss itself.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/0d9f7dad16012975eaac5907212d848d3f12160c"" href=""https://discuss.pytorch.org/uploads/default/original/2X/0/0d9f7dad16012975eaac5907212d848d3f12160c.png"" title=""屏幕快照 2017-08-09 下午8.03.42.png""><img alt=""42"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/0/0d9f7dad16012975eaac5907212d848d3f12160c_2_10x10.png"" height=""161"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/0/0d9f7dad16012975eaac5907212d848d3f12160c_2_690x161.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/0/0d9f7dad16012975eaac5907212d848d3f12160c_2_690x161.png, https://discuss.pytorch.org/uploads/default/optimized/2X/0/0d9f7dad16012975eaac5907212d848d3f12160c_2_1035x241.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/0/0d9f7dad16012975eaac5907212d848d3f12160c.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">屏幕快照 2017-08-09 下午8.03.42.png</span><span class=""informations"">1348×316 27.9 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>policy is the weight of loss.grad, not the weight of loss itself.</p><NewLine></blockquote><NewLine><p>taken as a scalar quantity (that’s what I mean by weight) it’s just the same: <code>grad(w*x) = w*grad(x)</code><br/><NewLine>you just have to make sure you are not using it as a variable of the tree (using pi.detach() should do it)</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot, it looks like detach() is what I want:blush:</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am doing a similar example to this and I am having trouble defining the “baseline” variable. I tried</p><NewLine><pre><code class=""lang-auto"">base_line=Variable(Variable(torch.ones(2), requires_grad=True))<NewLine>base_line=base_line[0]<NewLine></code></pre><NewLine><p>and get the error “RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn”. I only have one baseline value, could you suggest a way to define this varieble?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Generally, the baseline is an approximation of the expected reward, that does not depend on the policy parameters (so it does not affect the direction of the gradient).</p><NewLine><p>This approximation can be the output of another network that takes the state as input and returns a value, and you minimize the distance between the observed rewards and the predicted values. Then, over a rollout, you predict the values of all states with this model, and remove the values from the rewards:</p><NewLine><pre><code class=""lang-python""># policy = Policy() # the network that computes the prob of your actions<NewLine># value = Value() # the network that predicts the rewards<NewLine>loss = - torch.sum( torch.log(policy(states)) * (rewards - values(states) )<NewLine></code></pre><NewLine><p>That’s the idea of the actor critic for policy gradient. You have a full example here: <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py"">https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py</a></p><NewLine><p>If your rollout is long enough, the sum of rewards approximates Q and the sum of values approximates V, so the term (rewards - values) approximate the advantage (Q-V) which is a non-biased estimator of the temporal difference (hence the name advantage-actor-critic).</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just a quick question. The log probabilities are negative. If the rewards are positive, then the product with high rewards will make negative log prob values even lower (and loss even bigger)? Isn’t it opposite to what we need? Or the rewards should be negative? Did I miss anything?</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>the <code>log_prob*advantage</code> quantity is the objective to maximize. If a reward is negative, then having a large negative log_probs gives a large objective. At the opposite, if a reward is positive, then having small negative log_probs (close to 0) maximizes the objective.</p><NewLine><p>With Pytorch, we need a loss to minimize, so we take <code>loss=-objective</code>.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""4003""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ruotianluo/40/383_2.png"" width=""20""/> ruotianluo:</div><NewLine><blockquote><NewLine><p>backward, the gradient backpropogated</p><NewLine></blockquote><NewLine></aside><NewLine><p>Hello, I implemented the loss like you said, and it works.<br/><NewLine>However, why doesn’t it have the opposite sign? Because the given gradient is actually for gradient ascent.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ruotianluo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/11118; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/11118; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/11118; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/11118; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/JMPF; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/ymeng; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/mansur; <NewLine> ,"REPLY_DATE 1: June 14, 2017,  8:22am; <NewLine> REPLY_DATE 2: June 15, 2017,  5:51am; <NewLine> REPLY_DATE 3: August 9, 2017,  4:16am; <NewLine> REPLY_DATE 4: August 9, 2017,  9:04am; <NewLine> REPLY_DATE 5: August 9, 2017, 10:18am; <NewLine> REPLY_DATE 6: August 9, 2017, 11:57am; <NewLine> REPLY_DATE 7: August 9, 2017, 12:11pm; <NewLine> REPLY_DATE 8: August 9, 2017,  7:45pm; <NewLine> REPLY_DATE 9: August 10, 2017,  6:00am; <NewLine> REPLY_DATE 10: July 30, 2018,  1:37pm; <NewLine> REPLY_DATE 11: July 30, 2018,  8:26pm; <NewLine> REPLY_DATE 12: December 18, 2018, 10:54pm; <NewLine> REPLY_DATE 13: December 19, 2018, 11:00am; <NewLine> REPLY_DATE 14: July 2, 2019,  3:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> 
49382,"Policy gradients, reinforce with baselines loss function",2019-07-01T08:53:02.705Z,0,809,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am having trouble with the loss function corresponding to the REINFORCE with Baseline algorithm as described in Sutton and Barto book:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/9e17eb15999dd87a9845c7f36ef8727970f3c950"" href=""https://discuss.pytorch.org/uploads/default/original/2X/9/9e17eb15999dd87a9845c7f36ef8727970f3c950.png"" title=""image.png""><img alt=""image"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/9/9e17eb15999dd87a9845c7f36ef8727970f3c950_2_10x10.png"" height=""348"" src=""https://discuss.pytorch.org/uploads/default/original/2X/9/9e17eb15999dd87a9845c7f36ef8727970f3c950.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">907×458 24.4 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine>The last line is the update for the policy net.<br/><NewLine>Let gamma=1 for simplicity…<br/><NewLine>Now I want to construct loss function for the policy net output, so that I could backpropagate through it after playing one episode.<br/><NewLine>I am guessing it should be P_loss = - sum_over_episode(log_p*delta). The minus comes from the fact that in pytorch we minimize a loss function, but the update rule in the book tells us how to do gradient ascent for maximizing the objective.<br/><NewLine>I implemented training at each episode as follows:</p><NewLine><pre><code class=""lang-auto""># network<NewLine>class MLP(nn.Module):<NewLine>    def __init__(self, input_dim, output_dim, hidden=256):<NewLine>        super().__init__()<NewLine>        self.fc1 = nn.Linear(input_dim, hidden)<NewLine>        self.fc2 = nn.Linear(hidden, hidden)<NewLine>        self.fc3 = nn.Linear(hidden, output_dim)<NewLine>    def forward(self, x):<NewLine>        x = F.relu(self.fc1(x))<NewLine>        x = F.relu(self.fc2(x))<NewLine>        q = F.relu(self.fc3(x))<NewLine>        q = F.log_softmax(q, dim=-1)<NewLine>        return q<NewLine><NewLine>def policy_loss(returns, baselines, log_p_taken):<NewLine>  loss = -torch.sum((returns - baselines)*log_p_taken)<NewLine>  return loss<NewLine></code></pre><NewLine><p>when training:</p><NewLine><pre><code class=""lang-auto"">for ep in range(N_ep):<NewLine>  ...<NewLine>  self.optimizer_p.zero_grad()<NewLine>  log_P = P_net(ep_states)  # P_net is an instance of MLP net declared above<NewLine>  log_P_taken = torch.gather(log_P, 1, ep_taken_actions.view(-1, 1)).squeeze()<NewLine>  P_loss = policy_loss(ep_returns, ep_baselines, log_P_taken)<NewLine>  P_loss.backward()<NewLine>  self.optimizer_p.step()<NewLine>  ...<NewLine><NewLine></code></pre><NewLine><p>However, training seems to never converge with that loss, but trainin with P_loss_negative = - P_loss gives good results. Training with P_loss_negative passes cartpole_v0 and almost passes cartpole_v1, while training with P_loss always diverges.<br/><NewLine>Please help me find what is wrong. Thank you!!</p><NewLine></div>",https://discuss.pytorch.org/u/mansur,,mansur,"July 1, 2019,  9:01am",1 Like,,,,
48237,Doubt about creating a special neural network,2019-06-18T09:19:25.242Z,1,204,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello guys I am new to pytorch and I am trying to create the following neural network:</p><NewLine><p><img alt=""net"" height=""134"" src=""https://discuss.pytorch.org/uploads/default/original/2X/1/1a9f6c3f06afe9d938928629b6a6100484bfffd2.png"" width=""424""/></p><NewLine><p><a class=""onebox"" href=""http://urban-computing.com/pdf/UbiComp2019JiZheng.pdf"" rel=""nofollow noopener"" target=""_blank"">http://urban-computing.com/pdf/UbiComp2019JiZheng.pdf</a></p><NewLine><p>The trick is that I have 5 copies of them and in the end I am joining them with a softmax function.</p><NewLine><p>Am I doing this correctly? I do not have much experience with strange neural networks and how autograd works in them.</p><NewLine><p>The input is ([batch_size, neural network number, input_state])</p><NewLine><p>def net(torch.nn.Module):<br/><NewLine>def <strong>init</strong>(self, bases, state_space, action_space, n_hidden = 20):<br/><NewLine>super(policy_estimator,self).<strong>init</strong>()</p><NewLine><pre><code>    self.base11 = torch.nn.Linear(state_space,n_hidden)<NewLine>    self.base12 = torch.nn.Linear(n_hidden, n_hidden)<NewLine>    self.base13 = torch.nn.Linear(n_hidden, 1)<NewLine><NewLine>    self.base21 = torch.nn.Linear(state_space,n_hidden)<NewLine>    self.base22 = torch.nn.Linear(n_hidden, n_hidden)<NewLine>    self.base23 = torch.nn.Linear(n_hidden, 1)<NewLine><NewLine>    self.base31 = torch.nn.Linear(state_space,n_hidden)<NewLine>    self.base32 = torch.nn.Linear(n_hidden, n_hidden)<NewLine>    self.base33 = torch.nn.Linear(n_hidden, 1)<NewLine><NewLine>    self.base41 = torch.nn.Linear(state_space,n_hidden)<NewLine>    self.base42 = torch.nn.Linear(n_hidden, n_hidden)<NewLine>    self.base43 = torch.nn.Linear(n_hidden, 1)<NewLine><NewLine>    self.base51 = torch.nn.Linear(state_space,n_hidden)<NewLine>    self.base52 = torch.nn.Linear(n_hidden, n_hidden)<NewLine>    self.base53 = torch.nn.Linear(n_hidden, 1)<NewLine><NewLine>    # Activation functions<NewLine>    self.activation_tanh = torch.nn.Tanh()<NewLine>    self.activation_softmax = torch.nn.Softmax(dim=1)<NewLine><NewLine>def forward(self, x):<NewLine><NewLine>    out1  = self.activation_tanh(self.base11(x[:,0,:]))<NewLine>    out1 = self.activation_tanh(self.base12(out1))<NewLine>    out1 = self.base13(out1)<NewLine><NewLine>    out2 = self.activation_tanh(self.base21(x[:, 1, :]))<NewLine>    out2 = self.activation_tanh(self.base22(out2))<NewLine>    out2 = self.base23(out2)<NewLine><NewLine>    out3 = self.activation_tanh(self.base31(x[:, 2, :]))<NewLine>    out3 = self.activation_tanh(self.base32(out3))<NewLine>    out3 = self.base33(out3)<NewLine><NewLine>    out4 = self.activation_tanh(self.base41(x[:, 3, :]))<NewLine>    out4 = self.activation_tanh(self.base42(out4))<NewLine>    out4 = self.base43(out4)<NewLine><NewLine>    out5 = self.activation_tanh(self.base51(x[:, 4, :]))<NewLine>    out5 = self.activation_tanh(self.base52(out5))<NewLine>    out5 = self.base53(out5)<NewLine><NewLine>    score_torch=torch.stack([out1.squeeze(dim=0),out2.squeeze(dim=0),out3.squeeze(dim=0), out4.squeeze(dim=0), out5.squeeze(dim=0)], dim=1)<NewLine>    action_probs = self.activation_softmax(score_torch)<NewLine><NewLine>    return action_probs<NewLine></code></pre><NewLine><p>Thanks so much!</p><NewLine></div>",https://discuss.pytorch.org/u/deepfailure,(Tom),deepfailure,"June 18, 2019,  9:19am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could yopu deeper explain it? Looks like a normal fully connected in the graph.</p><NewLine><blockquote><NewLine><p>The trick is that I have 5 copies of them and in the end I am joining them with a softmax function.</p><NewLine></blockquote><NewLine><p>Its not very explanatory.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes my bad. The idea of the paper is to give a score to a base.</p><NewLine><p>Each base has its own neural network that gives a scalar in the end, y_i. Then all of the station scores are joined to output a normalized probability with a softmax.</p><NewLine><p>So if I have 5 bases, I will have 5 neural networks, 5 scores and then I will normalize with a softmax.</p><NewLine><p>In the code I merge the different scores with a torch.stack. I am not sure if pytorch will detect this structure correctly in order to do the gradients.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeh it does, gradients are element-wise, thus it keeps perfect tracking no matter you separate a tensor, stack it or perform any other operation.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/JuanFMontesinos; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/deepfailure; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/JuanFMontesinos; <NewLine> ,"REPLY_DATE 1: June 18, 2019,  3:02pm; <NewLine> REPLY_DATE 2: June 18, 2019,  3:38pm; <NewLine> REPLY_DATE 3: June 18, 2019,  3:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
47615,Output of actor is (almost)same for different states,2019-06-11T12:17:50.503Z,0,159,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have been trying to implement Actor Critic with a convolutional neural network. There are two different images as the state for the reinforcement learning agent. The output(actions for Actor) of the CNN is the (almost)same for different inputs after (random)initialisation. Due to this even after training the agent never learns anything useful.</p><NewLine><p>State definitions(2 inputs):</p><NewLine><p>Input 1: [1,1,90,128] image with maximum value for the pixel being 45.</p><NewLine><p>Input 2: [1,1,45,80] image with maximum value for the pixel being 45.</p><NewLine><p>Expected output by the actor: [x,y]: a 2-dimensional vector according to the state. Here x is expected in the range [0,160] and y is expected in the range [0,112]</p><NewLine><p>Different types of modifications tried with the input:</p><NewLine><p>1: Feed both the images as it is.</p><NewLine><p>2: Feed both images with normalisation as <code>(img/45)</code> so that pixel values are from [0,1]</p><NewLine><p>3:Feed both images with normalisation as <code>2*((img/45)-0.5)</code> so that pixel values are from [-1,1]</p><NewLine><p>4: Feed both images with normalisation as <code>(img-mean)/std</code></p><NewLine><p>Result: The output of the CNN remains almost same.</p><NewLine><p>The code for the definition of actor has been given below.</p><NewLine><pre><code>import numpy as np<NewLine>import pandas as pd<NewLine>from tqdm import tqdm<NewLine>import time<NewLine>import cv2<NewLine>import matplotlib.pyplot as plt<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>from torch.autograd import Variable<NewLine>import torch.nn.functional as F<NewLine><NewLine><NewLine>device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")<NewLine><NewLine>class Actor(nn.Module):<NewLine>    def __init__(self, action_dim, max_action):<NewLine>        super(Actor,self).__init__()<NewLine>        # state image [1,1,90,128]<NewLine>        self.conv11 = nn.Conv2d(1,16,5)<NewLine>        self.conv11_bn = nn.BatchNorm2d(16)<NewLine>        self.conv12 = nn.Conv2d(16,16,5)<NewLine>        self.conv12_bn = nn.BatchNorm2d(16)<NewLine>        self.fc11 = nn.Linear(19*29*16,500)<NewLine>        # dim image [1,1,45,80]<NewLine>        self.conv21 = nn.Conv2d(1,16,5) <NewLine>        self.conv21_bn = nn.BatchNorm2d(16)<NewLine>        self.conv22 = nn.Conv2d(16,16,5)<NewLine>        self.conv2_bn = nn.BatchNorm2d(16)<NewLine>        self.fc21 = nn.Linear(8*17*16,250)<NewLine>        # common pool<NewLine>        self.pool  = nn.MaxPool2d(2,2)<NewLine>        # after concatenation<NewLine>        self.fc2 = nn.Linear(750,100)<NewLine>        self.fc3 = nn.Linear(100,10)<NewLine>        self.fc4 = nn.Linear(10,action_dim)<NewLine>        self.max_action = max_action<NewLine>        <NewLine>    def forward(self,x,y):<NewLine>        # state image<NewLine>        x = self.conv11_bn(self.pool(F.relu(self.conv11(x))))<NewLine>        x = self.conv11_bn(self.pool(F.relu(self.conv12(x))))<NewLine>        x = x.view(-1,19*29*16)<NewLine>        x = F.relu(self.fc11(x))<NewLine>        # state dim<NewLine>        y = self.conv11_bn(self.pool(F.relu(self.conv21(y))))<NewLine>        y = self.conv11_bn(self.pool(F.relu(self.conv22(y))))<NewLine>        y = y.view(-1,8*17*16)<NewLine>        y = F.relu(self.fc21(y))<NewLine>        # concatenate<NewLine>        z = torch.cat((x,y),dim=1)<NewLine>        z = F.relu(self.fc2(z))<NewLine>        z = F.relu(self.fc3(z))<NewLine>        z = self.max_action*torch.tanh(self.fc4(z))<NewLine>        return z<NewLine><NewLine># to read different sample states for testing<NewLine>obs = []<NewLine>for i in range(200):<NewLine>    obs.append(np.load('eval_episodes/obs_'+str(i)+'.npy',allow_pickle=True))<NewLine><NewLine>obs = np.array(obs)<NewLine><NewLine>def tensor_from_numpy(state):<NewLine>    # to add dimensions to tensor to make it [batch_size,channels,height,width] <NewLine>    state_img = state<NewLine>    state_img = torch.from_numpy(state_img).float()<NewLine>    state_img = state_img[np.newaxis, :]<NewLine>    state_img = state_img[np.newaxis, :].to(device)<NewLine>    return state_img<NewLine><NewLine><NewLine>actor = Actor(2,torch.FloatTensor([160,112]))<NewLine>for i in range(20):<NewLine>    a = tensor_from_numpy(obs[i][0])<NewLine>    b = tensor_from_numpy(obs[i][2])    <NewLine>    print(actor(a,b))<NewLine></code></pre><NewLine><p>Output of the above code:</p><NewLine><pre><code>tensor([[28.8616,  3.0934]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[27.4125,  3.2864]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[28.2210,  2.6859]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[27.6312,  3.9528]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[25.9290,  4.2942]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[26.9652,  4.5730]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[27.1342,  2.9612]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[27.6494,  4.2218]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[27.3122,  1.9945]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[29.6915,  1.9938]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[28.2001,  2.5967]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[26.8502,  4.4917]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[28.6489,  3.2022]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[28.1455,  2.7610]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[27.2369,  3.4243]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[25.9513,  5.3057]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[28.1400,  3.3242]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[28.2049,  2.6622]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[26.7446,  2.5966]], grad_fn=&lt;MulBackward0&gt;)<NewLine>tensor([[25.3867,  5.0346]], grad_fn=&lt;MulBackward0&gt;)<NewLine></code></pre><NewLine><p>The states(<code>.npy</code>) files can be found <a href=""https://drive.google.com/file/d/1Grd57e0hmJzQgLDowhvW6xLaozZygrUW/view?usp=sharing"" rel=""nofollow noopener"">here</a></p><NewLine><p>For different states the actions should vary from [0-160,0-112] but here the outputs just vary a little bit.</p><NewLine><p>Note: the input images are initially sparse(lots of zeros in the image)</p><NewLine><p>Is there a problem with the state pixel values or the network definition?</p><NewLine></div>",https://discuss.pytorch.org/u/nsidn98,(Siddharth Nayak),nsidn98,"June 12, 2019,  6:33pm",,,,,
47407,Help with GAN with rectagular data,2019-06-08T09:15:44.347Z,1,187,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all, i’m working on time series data applications of GANs, the dataset is usually non-square (e.g. Time 100 days x 10 features, 1 channel).</p><NewLine><p>Lots of codes (e.g. in github) are designed for square images. If you modify the code and once you change input/output sizes, kernel sizes, strides and padding; tbh i cannot the sizes right different layers of the of the CNN (in particular for generator net).</p><NewLine><p>any help here? tricks? thanks</p><NewLine></div>",https://discuss.pytorch.org/u/rodrigodupleich,(rod_dup),rodrigodupleich,"June 8, 2019,  9:18am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/><br/><NewLine>I’m doing actually a similar thing for my bachelorthesis. My GAN has to work with CNNs and an additional context vector, which I haven’t implemented yet.<br/><NewLine>I tried to build a GAN with STGCN. That are CNNs for spatio temporal data. They use non square kernelsizes for the convolution.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks. this a good reference Taxaria:</p><NewLine><aside class=""onebox pdf""><NewLine><header class=""source""><NewLine><a href=""https://arxiv.org/pdf/1603.07285.pdf"" rel=""nofollow noopener"" target=""_blank"">arxiv.org</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://arxiv.org/pdf/1603.07285.pdf"" rel=""nofollow noopener"" target=""_blank""><span class=""pdf-onebox-logo""></span></a><NewLine><h3><a href=""https://arxiv.org/pdf/1603.07285.pdf"" rel=""nofollow noopener"" target=""_blank"">1603.07285.pdf</a></h3><NewLine><p class=""filesize"">892.34 KB</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Taxaria; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/rodrigodupleich; <NewLine> ,"REPLY_DATE 1: June 8, 2019, 11:43am; <NewLine> REPLY_DATE 2: June 10, 2019,  8:39pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
47255,DQN eps_decay represents what?,2019-06-06T15:03:20.123Z,0,230,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I dont fully understand the eps_decay parameter in the DQN example. It is explained the eps_start represents the chance to select a random action at the beginning of the training and eps_ends is the chance to select a random action at the end of the training. With these two variables, one would assume the decay rate from the eps_start to the eps_end chance would depend on the number of episodes the training takes. So, from 0.9 chance to 0.1 is a difference of 0.8 and you can divide this by the number of episodes that will happen, and you get the decay rate per episode. So, my question is how does eps_decay come into this? I can understand if you want it to fully decay not at the end of the training but let’s say at half of the episodes, you could double the decay rate. However, the given example parameter is for example 200 in the cartpool environment. What does this 200 represent?</p><NewLine></div>",https://discuss.pytorch.org/u/Milky,(Milky ),Milky,"June 6, 2019,  3:03pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>TLDR: it’s just a fancy way to decay the epsilon</p><NewLine><p>As you say, if you want the parameter to decay halfway you can change this via this parameter. Have a look at this <a href=""https://www.desmos.com/calculator/cdepi0lme1"" rel=""nofollow noopener"">link</a> where you can change the value to see how the decay changes. If eps_decay = 100, then the eps will be 0.363 at time step 100.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/403539c4ba2dd87c2f01f9fbfa08f555263c2dda"" href=""https://discuss.pytorch.org/uploads/default/original/2X/4/403539c4ba2dd87c2f01f9fbfa08f555263c2dda.png"" title=""Screen Shot 2019-06-09 at 18.27.42.png""><img alt=""42"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/4/403539c4ba2dd87c2f01f9fbfa08f555263c2dda_2_10x10.png"" height=""158"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/4/403539c4ba2dd87c2f01f9fbfa08f555263c2dda_2_690x158.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/4/403539c4ba2dd87c2f01f9fbfa08f555263c2dda_2_690x158.png, https://discuss.pytorch.org/uploads/default/optimized/2X/4/403539c4ba2dd87c2f01f9fbfa08f555263c2dda_2_1035x237.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/4/403539c4ba2dd87c2f01f9fbfa08f555263c2dda_2_1380x316.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screen Shot 2019-06-09 at 18.27.42.png</span><span class=""informations"">2232×514 49.2 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Oli; <NewLine> ,"REPLY_DATE 1: June 9, 2019,  4:36pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
46655,Cuda out of memory DQN training,2019-05-30T14:39:50.786Z,2,446,"<div class=""post"" itemprop=""articleBody""><NewLine><p>i am training my own DQN agent for a game i wrote in python, and the training loop works fine, but after about 150 episodes, cuda runs out of memory at optimize model function:</p><NewLine><p>File “trainingAgent.py”, line 99, in <br/><NewLine>optimize_model()<br/><NewLine>File “trainingAgent.py”, line 67, in optimize_model<br/><NewLine>next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()<br/><NewLine>File “C:…\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py”, line 489, in <strong>call</strong><br/><NewLine>result = self.forward(*input, **kwargs)<br/><NewLine>File “C:…\DQN.py”, line 28, in forward<br/><NewLine>x = F.relu(self.bn3(self.conv3(x)))<br/><NewLine>File “C:…\Python\Python37\lib\site-packages\torch\nn\functional.py”, line 862, in relu<br/><NewLine>result = torch.relu(input)<br/><NewLine>RuntimeError: CUDA out of memory. Tried to allocate 18.13 MiB (GPU 0; 8.00 GiB total capacity; 4.81 GiB already allocated; 17.54 MiB free; 1.43 GiB cached)</p><NewLine><p>So yeah I have 8GB gpu memory, it is a gaming gpu. I’m not sure if this is normal but I guess not. Is it because my input screen matrix is too large? (300x300) cause i dont subtract the last screen from the current scren like in the example DQN algorithm (though i dont know if this lowers the input size?), but just use the current screen as the input for the network. Or is it some kind of memory leak? what could i do to solve the issue? I would like to be able to run at least 2000 episodes, and possibly 10,000 episodes if needed</p><NewLine></div>",https://discuss.pytorch.org/u/Milky,(Milky ),Milky,"May 30, 2019,  6:42pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think gradient accumulation is the source of this problem. Seeing that the training worked for some epochs and you got the error after that.</p><NewLine><p>Refer to this <a href=""https://pytorch.org/docs/stable/notes/faq.html"" rel=""nofollow noopener"">pytorch documentation</a> that can help if this is the case.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks for the answer, I am looking but I do not accumulate anything in my training loop, besides the frame count of the current game</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>My guess at the moment is the replayMemory memory, which at every frame pushes 2 states of my game and since each of my states are 300x300, the memory blows up</p><NewLine><p>memory.push(state, action, next_state, reward)</p><NewLine><p>edit: indeed, I commented this out, along with the optimizer function. Just commenting out the optimizer, i still ran out of memory, but once I commented out the memory push, my GPU memory was free for 1000 games. I guess I shall try to train using the difference of current and last screen like in the cartpool training</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, <a class=""mention"" href=""/u/milky"">@Milky</a>, I recently wrote a lib called <code>pytorch_memlab</code>, probably you can use the <code>MemReporter</code> to inspect which kind of tensors are eating up your GPU memory.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Kushaj; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Milky; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Milky; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Stone; <NewLine> ,"REPLY_DATE 1: May 30, 2019,  6:53pm; <NewLine> REPLY_DATE 2: May 30, 2019,  7:23pm; <NewLine> REPLY_DATE 3: May 30, 2019,  7:42pm; <NewLine> REPLY_DATE 4: May 30, 2019,  8:14pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
46347,Why does multi layer perceprons outperform RNN in CartPole?,2019-05-27T18:00:03.694Z,3,220,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Recently I compared two models for a DQN on CartPole-v0 environment. One of them is a multilayer perceptron with 3 layer and the other is an RNN built up from an LSTM and 1 fully connected layer. I have an experience replay buffer of size 200000 and the training doesn’t start until it is filled up. Although MLP has solved the problem under a reasonable amount of training steps (this means achieve a mean reward of 195 for the last 100 episodes), the RNN model could not converge as quickly and its maximum mean reward did not even reached 195 too!</p><NewLine><p>I have already tried to increase batch size, add more neurons to the LSTM’S hidden state, increase the RNN’S sequence length and making the fully connected layer more complex - but every attempt failed as I saw enormous fluctuations in mean reward so the model harly converged at all. May these are the sings of early overfitting?</p><NewLine><pre><code class=""lang-auto"">class DQN(nn.Module):<NewLine>    def __init__(self, n_input, output_size, n_hidden, n_layers, dropout=0.3):<NewLine>        super(DQN, self).__init__()<NewLine><NewLine>        self.n_layers = n_layers<NewLine>        self.n_hidden = n_hidden<NewLine><NewLine>        self.lstm = nn.LSTM(input_size=n_input,<NewLine>            hidden_size=n_hidden,<NewLine>            num_layers=n_layers,<NewLine>            dropout=dropout,<NewLine>            batch_first=True)<NewLine><NewLine>        self.dropout= nn.Dropout(dropout)<NewLine><NewLine>        self.fully_connected = nn.Linear(n_hidden, output_size)<NewLine><NewLine>    def forward(self, x, hidden_parameters):<NewLine>        batch_size = x.size(0)<NewLine><NewLine>        output, hidden_state = self.lstm(x.float(), hidden_parameters)<NewLine><NewLine>        seq_length = output.shape[1]<NewLine><NewLine>        output1 = output.contiguous().view(-1, self.n_hidden)<NewLine>        output2 = self.dropout(output1)<NewLine>        output3 = self.fully_connected(output2)<NewLine><NewLine>        new = output3.view(batch_size, seq_length, -1)<NewLine>        new = new[:, -1]<NewLine><NewLine>        return new.float(), hidden_state<NewLine><NewLine>    def init_hidden(self, batch_size, device):<NewLine>        weight = next(self.parameters()).data<NewLine><NewLine>        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),<NewLine>            weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))<NewLine><NewLine>        return hidden<NewLine></code></pre><NewLine><p>Contrarly to what I expected the simpler model gave much better result that the other; even though RNN’s supposed to be better in processing time series data.</p><NewLine><p>Can anybody tell me what’s the reason for this?</p><NewLine><p>Also, I have to state that I applied no feature engineering and both DQN’s worked with raw data. May using normalized features could RNN outperform the MLP? (I mean feeding both model with normalized data)</p><NewLine><p>Is there anything you can recommend me to improve training efficiency on RNN’s to achieve the best results?</p><NewLine></div>",https://discuss.pytorch.org/u/Balazs_Koncz,(Balázs Koncz),Balazs_Koncz,"May 27, 2019,  6:00pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>CartPole is actually a very simple problem. Just by using a simple random search you can get more than 200 reward, take a look at <a href=""http://kvfrans.com/simple-algoritms-for-solving-cartpole/"" rel=""nofollow noopener"">this</a>.</p><NewLine><p>RNNs should outperform MLP’s when the temporal/sequential information is really relevant. In the case of CarPole, you actually don’t care that much about what happened 2 iterations before, the actual state is much more relevant for you to decide what to do than the previous ones. This is not the case for a lot of applications where RNNs shine, like NLP, where, for instance, the last word of a sentence generally isn’t enough for you to interpret the whole sentence, and more, the order of the words matters a lot in language.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>What do you feed as input of the neural network (x)?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/leviviana"">@LeviViana</a><br/><NewLine>Thank you very much for your response and your provided resource!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Its the last N observation from the environment, and they are raw values.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well, this explains why the mlp outperforms the lstm. Conceptually, to solve cartpole, all you need are the instant positions, speeds and accelerations. If you feed them directly (or even if you feed 3 consecutive positions, which allows to compute these informations), you don’t capture any additionnal useful information using a lstm.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Maybe using relative data e.g.: the differences betwen each consecutive speeds, accelerations etc… would improve the performance?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am quite sure position, speed and acceleration are sufficient to describe the full dynamics of the system (a representation of the mass would be learnt at some point). Why would you want to consider higher moments?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think there was a misunderstooding, I just want the model to learn faster, and I meant to use the deltas of speed/position etc… instead of absolute values of speed/position …</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/LeviViana; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Yann; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Balazs_Koncz; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Balazs_Koncz; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Yann; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Balazs_Koncz; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Yann; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Balazs_Koncz; <NewLine> ,"REPLY_DATE 1: May 28, 2019, 11:46am; <NewLine> REPLY_DATE 2: May 27, 2019,  8:08pm; <NewLine> REPLY_DATE 3: May 28, 2019, 11:47am; <NewLine> REPLY_DATE 4: May 28, 2019, 11:48am; <NewLine> REPLY_DATE 5: May 28, 2019,  1:17pm; <NewLine> REPLY_DATE 6: May 28, 2019,  3:05pm; <NewLine> REPLY_DATE 7: May 28, 2019,  3:48pm; <NewLine> REPLY_DATE 8: May 29, 2019,  2:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
46324,Make multiple AIs vote the decision to take,2019-05-27T13:43:38.709Z,0,145,"<div class=""post"" itemprop=""articleBody""><NewLine><p>hello everyone !</p><NewLine><p>I have trained various  neural networks, often with the same parameters and training environnement, but I often ended up with imperfect AIs which were always slightly different from their peers. Especially when the environnment contained some randomness.</p><NewLine><p>I just had the idea of making different neural networks with the same training, let’s say a dozen, and to make them vote the decision to take.</p><NewLine><p>My hope being that the different AIs, who are right around 90% of the time, would not be wrong all at the same time.</p><NewLine><p>I wanted to know if anyone ever tried anything similar ? is the idea good or just stupid (and if the latter, why ?)</p><NewLine><p>It would take long to train those, so if anyone tried before and can tell me about the result, that would be nice (and save me much time ! ^^)</p><NewLine><p>Have a nice day !</p><NewLine></div>",https://discuss.pytorch.org/u/heitaum,(Mathieu Hernandez),heitaum,"May 27, 2019,  1:43pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s already invented for machine learning <a href=""https://en.wikipedia.org/wiki/AdaBoost"" rel=""nofollow noopener"">https://en.wikipedia.org/wiki/AdaBoost</a>. I haven’t seen works on deep learning but it should be similar.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/JuanFMontesinos; <NewLine> ,"REPLY_DATE 1: May 27, 2019,  2:57pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
46044,A2C model converging to a low score,2019-05-23T15:21:07.134Z,0,371,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to implement the A2C algorithm. But, my model is converging to a low score(10) per episode.<br/><NewLine>I am implementing from the following link <a href=""https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f"" rel=""nofollow noopener"">https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f</a></p><NewLine><pre><code class=""lang-auto"">import numpy as np<NewLine>import matplotlib.pyplot as plt<NewLine>import gym<NewLine>import sys<NewLine><NewLine>import torch<NewLine>from torch import nn<NewLine>from torch import optim<NewLine>import torch.nn.functional as F<NewLine><NewLine>env = gym.make('CartPole-v0')<NewLine>state = env.reset()<NewLine>total_rewards = []<NewLine><NewLine>num_episodes=1000<NewLine>batch_size=32<NewLine>gamma=0.98<NewLine>epsilon = 1<NewLine>epsilon_min = 0.005<NewLine>epsilon_decay = 0.998<NewLine><NewLine>class ACNetwork(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(ACNetwork,self).__init__()<NewLine>        self.n_inputs = env.observation_space.shape[0]<NewLine>        self.n_outputs = env.action_space.n<NewLine>        <NewLine>        self.l1 = nn.Linear(self.n_inputs, 32)<NewLine>        self.l2 = nn.Linear(32,64)<NewLine>        self.l3 = nn.Linear(64,1)<NewLine>        self.l4 = nn.Linear(64,self.n_outputs)<NewLine>        <NewLine>    def forward(self,x):<NewLine>        x = self.l1(x)<NewLine>        x = F.relu(x)<NewLine>        x = self.l2(x)<NewLine>        x = F.relu(x)<NewLine>        values = self.l3(x)<NewLine>        probs = F.softmax(self.l4(x),dim=-1)<NewLine>        return values,probs<NewLine>    <NewLine>agent = ACNetwork()<NewLine>optimizer = optim.Adam(agent.parameters(),lr=0.01)<NewLine>done = False<NewLine><NewLine>def select_action(state):<NewLine>    _,action_probs = agent(torch.from_numpy(state).type(torch.FloatTensor))<NewLine>    action_probs = action_probs.detach().numpy()<NewLine>    return np.argmax(action_probs)<NewLine><NewLine><NewLine>for ep in range(num_episodes):<NewLine>    state = env.reset()<NewLine>    ep_reward = 0<NewLine>    states = []<NewLine>    next_states = []<NewLine>    actions = []<NewLine>    rewards = []<NewLine>    dones = []<NewLine>    done = False<NewLine>    while done == False:<NewLine>        action = select_action(state)<NewLine>        next_state, reward, done, _ = env.step(action)<NewLine>        <NewLine>        if done:<NewLine>            reward = -1<NewLine>        else:<NewLine>            reward = reward <NewLine>            ep_reward += 1<NewLine>        <NewLine>        states.append(state)<NewLine>        next_states.append(next_state)<NewLine>        rewards.append(reward)<NewLine>        actions.append(action)<NewLine>        state = next_state<NewLine>        dones.append(done)<NewLine>        <NewLine>        if done:<NewLine>            print(f""EPISODE : {ep}   REWARD : {ep_reward}"")<NewLine>            # CONVERTING TO TENSOR<NewLine>            states = torch.FloatTensor(states)<NewLine>            next_states = torch.FloatTensor(next_states)<NewLine>            actions = torch.LongTensor(actions)<NewLine>            rewards = torch.FloatTensor(rewards)<NewLine>            dones = torch.FloatTensor(dones)<NewLine>            # PREDICTING VALUES AND PROBABILTIES<NewLine>            optimizer.zero_grad()<NewLine>            values,probs = agent(states)<NewLine>            values = torch.squeeze(values)<NewLine>            next_values,_ = agent(next_states)<NewLine>            next_values = torch.squeeze(next_values)<NewLine>            # LOSS CALCULAION<NewLine>            adv = rewards + gamma*next_values - values<NewLine>            log_probs = torch.log(probs[[np.arange(len(actions)), actions]])<NewLine>            policy_loss = -(log_probs*adv)<NewLine>            policy_loss = torch.mean(policy_loss)<NewLine>            value_loss = torch.pow(adv,2)<NewLine>            value_loss = torch.mean(value_loss)<NewLine>            loss = value_loss + policy_loss<NewLine>            loss.backward()<NewLine>            optimizer.step()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Gauranga_Das,(Gauranga Das),Gauranga_Das,"May 23, 2019,  3:21pm",,,,,
45341,DQN with bitmap as input,2019-05-15T19:27:33.146Z,0,149,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to implement the DQN algorithm from the example, however, the environment that I use has as states bitmaps. When I put my state in the training loop, I get error in:<br/><NewLine>return policy_net(state).max(1)[1].view(1, 1), for choosing the next action that isn’t exploration (the random action). The error is:<br/><NewLine>RuntimeError: Expected 4-dimensional input for 4-dimensional weight [16, 3, 5, 5], but got 2-dimensional input of size [300, 300] instead.<br/><NewLine>My input is indeed [300,300], but it expects 4-dimensional weight [16.3.5.5]. I believe this is because of the get_screen function from the cart game using the BCHW format tensor, after transposing and unsqueezing the rgb_array from the screen. Since I don’t have rgb values but a bitmap I can’t have the BCHW format tensor. So I wanted to ask, how to best go about implementing a DQN algorithm. Is there perhaps another “skeleton” algorithm that I can use?</p><NewLine></div>",https://discuss.pytorch.org/u/Milky,(Milky ),Milky,"May 15, 2019,  7:27pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It looks like you would like to pass a grayscale image into the model.<br/><NewLine>Based on the error message it also sounds like the first layer is an <code>nn.Conv2d</code> layer with <code>in_channels=3</code> and <code>out_channels=16</code>.<br/><NewLine>If that’s the case, you should unsqueeze dim0 and dim0 for your input (conv layers expect an input of <code>[batch_size, channels, height, width]</code>), and change the number of input channels of the first conv layer to 1.</p><NewLine><pre><code class=""lang-python"">input = input[None, None, :, :]<NewLine></code></pre><NewLine><p>Let me know, if that helps.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: May 15, 2019, 10:07pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
45313,Joblib with pytorch to parallelize sampling process,2019-05-15T12:32:52.419Z,0,424,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I tried to use Pytorch(v=0.4.1) together with Joblib in order to parallelize the sampling process in a RL task. Setting the seed for Pytorch, Numpy, and python wasn’t enough to get reproducible experiments. In other words, Pytorch yielded me different predictions for different runs of the same algorithm in the same task with the same seeds. I am wondering if there is any known issue between Pytorch and Joblib. With Multiprocessing the results are just fine.</p><NewLine></div>",https://discuss.pytorch.org/u/gcnn,(gcnn),gcnn,"May 15, 2019, 12:32pm",,,,,
44115,How to load data efficiently in online learning,2019-05-01T22:11:20.326Z,0,239,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m working in the reinforcement learning. In RL, we need to grow the replay buffer and draw samples in an online manner.</p><NewLine><p>I can’t figure out a way to use the <code>DataLoader</code> and <code>Sampler</code> concepts in PyTorch. They seem to only support batch training.</p><NewLine></div>",https://discuss.pytorch.org/u/fabian,(Fabian Han),fabian,"May 1, 2019, 10:11pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You must make your own replay buffer by hand (because the quality of RL sometimes strongly depends on the quality of the buffer, you can’t impose a generic method). However, the most standard approach (not the most efficient) is to save the last 1e6 examples of transitions and to uniformly sample from them. You have one simple example here:<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pranz24/pytorch-soft-actor-critic/blob/master/replay_memory.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pranz24/pytorch-soft-actor-critic/blob/master/replay_memory.py"" rel=""nofollow noopener"" target=""_blank"">pranz24/pytorch-soft-actor-critic/blob/master/replay_memory.py</a></h4><NewLine><pre><code class=""lang-py"">import random<NewLine>import numpy as np<NewLine><NewLine>class ReplayMemory:<NewLine>    def __init__(self, capacity):<NewLine>        self.capacity = capacity<NewLine>        self.buffer = []<NewLine>        self.position = 0<NewLine><NewLine>    def push(self, state, action, reward, next_state, done):<NewLine>        if len(self.buffer) &lt; self.capacity:<NewLine>            self.buffer.append(None)<NewLine>        self.buffer[self.position] = (state, action, reward, next_state, done)<NewLine>        self.position = (self.position + 1) % self.capacity<NewLine><NewLine>    def sample(self, batch_size):<NewLine>        batch = random.sample(self.buffer, batch_size)<NewLine>        state, action, reward, next_state, done = map(np.stack, zip(*batch))<NewLine>        return state, action, reward, next_state, done<NewLine><NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pranz24/pytorch-soft-actor-critic/blob/master/replay_memory.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> ,"REPLY_DATE 1: May 4, 2019,  9:51am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
42438,PyTorch 0.2.0 reinforce and PyTorch 1.0.1 alternative are giving different results,2019-04-12T23:01:32.613Z,0,166,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to train a controller using policy gradient reinforcement learning approach which iteratively update the policy based on sampled estimates of the reward. There are two codes one with PyTorch 0.2.0 using action.reinforce(reward) and the other with PyTorch 1.0.1 using the following modification.</p><NewLine><p>The gist of the code with both PyTorch versions are provided below. TheY are giving me quite different results. PyTorch 1.0.1 result is converging within 10 epochs but to a worse local minima than PyTorch 0.2.0 result. The final PyTorch 0.2.0 result is better but has slightly higher variance over N rollouts than PyTorch 1.0.1 version. PyTorch 0.2.0 code is also taking around 5 times longer to run.</p><NewLine><p>I am wondering if I am not doing the conversion between PyTorch versions properly or someone else has also noticed such differences in results and this can be a concern. Thank you.</p><NewLine><pre><code class=""lang-auto"">RuntimeError: reinforce() was removed.<NewLine>Use torch.distributions instead.<NewLine>See https://pytorch.org/docs/master/distributions.html<NewLine><NewLine>Instead of:<NewLine><NewLine>probs = policy_network(state)<NewLine>action = probs.multinomial()<NewLine>next_state, reward = env.step(action)<NewLine>action.reinforce(reward)<NewLine>action.backward()<NewLine><NewLine>Use:<NewLine><NewLine>probs = policy_network(state)<NewLine># NOTE: categorical is equivalent to what used to be called multinomial<NewLine>m = torch.distributions.Categorical(probs)<NewLine>action = m.sample()<NewLine>next_state, reward = env.step(action)<NewLine>loss = -m.log_prob(action) * reward<NewLine>loss.backward()<NewLine></code></pre><NewLine><p>The gist of the PyTorch 0.2.0 code is:</p><NewLine><pre><code class=""lang-auto""># predict actions for N rollouts, where each action is a 51 dimensional vector<NewLine>sumR = 0<NewLine>for e in epochs:<NewLine>    actions, rewards = [], []<NewLine>    for _ in range(N):<NewLine>        probs = controller(input)<NewLine>        action = probs.multinomial()    # each action is a 51 dimensional vector<NewLine>        <NewLine>        # calculate reward<NewLine>        reward = calculateReward(action)<NewLine>        <NewLine>        actions.append(action)<NewLine>        rewards.append(reward)<NewLine>    <NewLine>    avgR = mean(rewards)<NewLine>    b = sumR/ (e + 1)<NewLine>    sumR += avgR<NewLine>    <NewLine>    # update controller<NewLine>    for action in actions:<NewLine>        actions.reinforce(avgR - b)<NewLine>        optimizer.zero_grad()<NewLine>        autograd.backward(action, [None for _ in action])<NewLine>        optimizer.step()<NewLine></code></pre><NewLine><p>The gist of the PyTorch 1.0.1 code is:</p><NewLine><pre><code class=""lang-auto""># predict actions for N rollouts, where each action is a 51 dimensional vector<NewLine>sumR = 0<NewLine>for e in epochs:<NewLine>    logProbs, actions, reward = [], [], []<NewLine>    for _ in range(N):<NewLine>        probs = controller(input)<NewLine>        m = torch.distributions.Categorical(probs)<NewLine>        action = m.sample()    # each action is a 51 dimensional vector<NewLine>        <NewLine>        # calculate reward<NewLine>        reward = calculateReward(action)<NewLine>        <NewLine>        logProbs.append(m.log_prob(action))    # each logProb is a 51 dimensional vector<NewLine>        actions.append(action)<NewLine>        rewards.append(reward)<NewLine>    <NewLine>    avgR = mean(rewards)<NewLine>    b = sumR/ (e + 1)<NewLine>    sumR += avgR<NewLine>    <NewLine>    # update controller<NewLine>    for action, logProb in zip(actions, logProbs):<NewLine>        loss = - logProb.sum() * (avgR - b)<NewLine>        optimizer.zero_grad()<NewLine>        loss.backward(retain_graph=True)<NewLine>        optimizer.step()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Lopa,(Monami Banerjee),Lopa,"April 12, 2019, 11:08pm",1 Like,,,,
12845,Actor Critic fails unexplicably,2018-01-25T16:30:38.762Z,5,1648,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi !</p><NewLine><p>I implemented an actor critic algorithm, very much inspired from PyTorch’s one. It is doing awesome in CartPole, for instance, getting over 190 in a few hundred iterations. Delighted from this, I prepared for using it in my very own environment in which a robot has to touch a point in space.<br/><NewLine>In the simplest case:</p><NewLine><ul><NewLine><li>One single and fixed target</li><NewLine><li>Oriented reward (reward = 1/distance from effector to target)</li><NewLine></ul><NewLine><p>It is not even capable to get over 50%. Furthermore, the performance drops, falling from 20% to 2~3%. It is something that puzzles me. The environment is exactly the same I used for REINFORCE with baseline in which I’m able to get around 100% of success.</p><NewLine><p><img alt=""ACCatcher"" height=""480"" src=""https://discuss.pytorch.org/uploads/default/original/2X/7/7b4db2819f574d27968c33a92218d3c139eea189.png"" width=""640""/></p><NewLine><p>Would anyone have an idea about where I could find the flaw ? I must admit, I do not understand. I really thought that given REINFORCE with baseline performances, it would be a piece of cake for AC.</p><NewLine><p>Thanks !</p><NewLine><p><strong>EDIT 29/01/18</strong></p><NewLine><p>Among the various parameters that can cause this implementation to fail, I’m considering that the fact that the critic is not using experience replay -&gt; hence, introducing a lot of variance when bootstrapping.<br/><NewLine>But if it is so, how come it works so well on Cartpole ? Is it because cartpole is so simple that the actor can find it’s way without listening to the critic until the critic finally gets it right ?<br/><NewLine>I’m going to add experience replay and share my results.</p><NewLine><p><strong>END 29/01/18</strong></p><NewLine><p>PS: Link to my github with the files: <a href=""https://github.com/Mehd6384/Robot-World---RL"" rel=""nofollow noopener"">https://github.com/Mehd6384/Robot-World---RL</a></p><NewLine></div>",https://discuss.pytorch.org/u/Mehdi,,Mehdi,"January 29, 2018,  7:39am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If I understand, an action is a set of joints, then the robot does a trial and receives a reward depending on the distance from the target.</p><NewLine><p>In that case, you are not doing sequential learning. For example, in Cartpole, you have to learn the sequence of actions ‘left left left right right left’. A new action depends on the previous state and action.</p><NewLine><p>Here, when you know the target, you can directly compute a set of angles, wherever was the arm’s initial position. (or maybe the target is moving, so the new state depends on the last position of the target)</p><NewLine><p>After, this does not explain why it fails, as actor critics are solving non-sequential tasks. But it is quite sub-optimal.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey ! Thanks for your answer, but I’m not sure I understand want you mean. Are you saying that the reward function is not optimal and it should be based upon the joints angles that would actually reach the target?</p><NewLine><p>Here’s a picture to clarify the setting:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/f32d4deec6f97d94efd0c995e904205fd0373b9d"" href=""https://discuss.pytorch.org/uploads/default/original/2X/f/f32d4deec6f97d94efd0c995e904205fd0373b9d.png"" title=""RobotSetup.png""><img alt=""RobotSetup"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/f/f32d4deec6f97d94efd0c995e904205fd0373b9d_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/original/2X/f/f32d4deec6f97d94efd0c995e904205fd0373b9d.png"" width=""497""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">RobotSetup.png</span><span class=""informations"">694×698 5.98 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>Thanks !</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>it’s not clear what your robot environment’s action space.</p><NewLine><p>With cartpole the action space is discrete action selecting either left and right. (ie. your policy selects either 0 or 1 for left or right action).</p><NewLine><p>What is the action space for your robotic example? I see in the rep the action space is 2*n, where n is number of joints.</p><NewLine><p>Is your policy suppose to output real values (for example degrees of rotation) for each joint? Or is it just suppose to pick 1 joint from the 2*n joints available?</p><NewLine><p>If you’re using the same cartpole implementation for policy, then it will only work for the second case I described above.</p><NewLine><p>If your policy is suppose to output real joint angle values, then you will need a continuous policy distribution (i.e pytorch.distributions.normal) for each k joint angles.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, thanks for your answer.</p><NewLine><p>The agent outputs an action, here an integer between 0 and 2*n.<br/><NewLine>The action indicates the joint number (action/2) and the direction:<br/><NewLine>left if action%2 == 0 else, right.</p><NewLine><p>The robot then rotates the concerned joint by alpha*direction were alpha is arbitrary and can be modified by user.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>My point is that your target new position does not depend on its previous position and neither on your previous action. If it is the case, you are not trying to solve a MDP, but something different, and actor critics may work but is not the optimal solution.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello Alexis,</p><NewLine><p>The target is actually fixed. And the robot plays the whole episode. It starts in a configuration and gradually moves according to action. It is also a completely deterministic environment and a MDP.</p><NewLine><p>Here’s a video: <a href=""https://youtu.be/ygFx3-ql8TU"" rel=""nofollow noopener"">https://youtu.be/ygFx3-ql8TU</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am glad it worked for you, I am trying to check out your repo but the web page is not being found… Where I can find it?</p><NewLine><p>Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Mehdi; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/G-Wang; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Mehdi; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Mehdi; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Maria_Arroyo_Bernal; <NewLine> ,"REPLY_DATE 1: January 29, 2018, 10:03am; <NewLine> REPLY_DATE 2: January 29, 2018, 10:43am; <NewLine> REPLY_DATE 3: January 30, 2018,  8:12pm; <NewLine> REPLY_DATE 4: January 30, 2018, 10:11pm; <NewLine> REPLY_DATE 5: January 31, 2018,  1:45pm; <NewLine> REPLY_DATE 6: January 31, 2018,  2:41pm; <NewLine> REPLY_DATE 7: April 12, 2019,  9:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
41875,Fine-tuning specific Model,2019-04-07T12:19:57.054Z,0,150,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I’m new to the subject. I don’t quite understand how to delete the last Fully Connected Layer from<br/><NewLine>. (I want to Fine-tune )</p><NewLine><p>Please Delte it thx</p><NewLine></div>",https://discuss.pytorch.org/u/Mark0502,(Mark),Mark0502,"April 11, 2019, 12:23pm",,,,,
42081,BatchNorm1d ValueError: expected 2D or 3D input (got 1D input),2019-04-09T14:48:30.466Z,0,2662,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all!</p><NewLine><p>I’m trying to implement batch normalizing to my Neural Network, but I always get the same error. I looked through internet and couldn’t find a satisfying answer for my problem.</p><NewLine><p>Here’s My code:<br/><NewLine>import …</p><NewLine><p>HIDDEN_LAYER = 32</p><NewLine><p>class DeepQNetwork(nn.Module):</p><NewLine><pre><code>def __init__(self, no_inputs, no_outputs):<NewLine>    super(DeepQNetwork,self).__init__()<NewLine><NewLine>    self.lin1 = nn.Linear(no_inputs,HIDDEN_LAYER)<NewLine>    self.lin2 = nn.Linear(HIDDEN_LAYER,HIDDEN_LAYER)<NewLine>    self.lin3 = nn.Linear(HIDDEN_LAYER,no_outputs)<NewLine><NewLine>    self.bn = nn.BatchNorm1d(num_features=HIDDEN_LAYER)<NewLine><NewLine>def forward(self, x):<NewLine>    print('input = {}'.format(x))<NewLine>    output = Variable(x)<NewLine>    output = self.lin1(self.bn(output))<NewLine>    output = F.relu(output)<NewLine>    output = self.lin2(self.bn(output))<NewLine>    output = F.relu(output)<NewLine>    output = self.lin3(output)<NewLine><NewLine>    return F.relu(output)<NewLine></code></pre><NewLine><p>output of the code:</p><NewLine><p>input = tensor([ 0.0011,  0.0148,  0.0056, -0.0481], device=‘cuda:0’)</p><NewLine><p>Traceback (most recent call last):</p><NewLine><p>File “/gymcartpole/venv/DeepQ.py”, line 28, in forward<br/><NewLine>output = self.lin1(self.bn(output))<br/><NewLine>File “/gymcartpole/venv/lib/python3.6/site-packages/torch/nn/modules/module.py”, line 489, in <strong>call</strong><br/><NewLine>result = self.forward(*input, **kwargs)<br/><NewLine>File “/gymcartpole/venv/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py”, line 60, in forward<br/><NewLine>self._check_input_dim(input)<br/><NewLine>File “/gymcartpole/venv/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py”, line 169, in _check_input_dim<br/><NewLine>.format(input.dim()))<br/><NewLine>ValueError: expected 2D or 3D input (got 1D input)</p><NewLine><p>please help me <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/oktopus97,,oktopus97,"April 9, 2019,  2:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>nn.BatchNorm1d</code> expects an input of the shape <code>[batch_size, channels]</code> or <code>[batch_size, channels, length]</code>.<br/><NewLine>Currently you are just passing a tensor with a single dimension to the layer.<br/><NewLine>If your data has 4 features, you should add the batch dimension using:</p><NewLine><pre><code class=""lang-python"">input = input.unsqueeze(0)<NewLine></code></pre><NewLine><p>before passing it to your model.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: April 10, 2019,  8:44pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> 
41084,"Concatenating observations that include image, pose and sensor readings",2019-03-28T07:01:38.815Z,9,533,"<div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> What would the correct way be to concatenate observations (image (84x84x1), pose(x,y,z,r,p,y), sonar(range)), first using numpy, and then converting it to a torch tensor?</p><NewLine><p>I have to process first using numpy, so that there are no PyTorch depedencies added to the OpenAI Gym get_obs() method. And then convert the observations to a Tensor once I get it from the Gym environment.</p><NewLine></div>",https://discuss.pytorch.org/u/edowson,(Elvis Dowson),edowson,"March 28, 2019,  7:01am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You probably shouldn’t concatenate them directly, but put each one through an appropriate feature extractor (CNN for the image, and MLP for the others) before concatenating the outputs of the feature extractors.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/omegastick"">@Omegastick</a> The example that I’m working on is a reinforcement learning example, using DQN. The observations from the environment are the visual camera input and the drone’s sensor reading. Since this is an end-to-end learning example, the inputs are the observations, and the outputs are the actions, with rewards guiding the agent’s learning process.</p><NewLine><p>It is true, that for another example, I would use a CNN to do some sort of feature extract, like the location of a 3D waypoint marker. I could then concatenate the location of the way-point marker observation to the list I mentioned earlier. But for now, I am working on a simpler version of the problem, that is the next step after the standard DQN reinforcement example of only using visual inputs as the observations.</p><NewLine><p>At the moment, I am trying to do something like this:</p><NewLine><pre><code class=""lang-auto""># concatenate observations<NewLine>image = camera.processed_image<NewLine>print(""image.shape: {}"".format(image.shape))<NewLine><NewLine>x, y, z, r, p, y = 1.1, 1.2, 1.3, 2.1, 2.2, 2.3<NewLine>pose = [x, y, z, r, p, y]<NewLine>sensor_range = [3.1]<NewLine><NewLine>obs = np.concatenate((image, pose, sensor_range), axis=0)<NewLine></code></pre><NewLine><p>But I am having a problem with arranging the array dimentions correctly:</p><NewLine><pre><code class=""lang-auto"">image.shape: (180, 320)<NewLine>    obs = np.concatenate((image, pose, sensor_range), axis=0)<NewLine>ValueError: all the input arrays must have same number of dimensions<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, I’ve used the exact same technique in reinforcement learning (using PPO, rather than DQN, but the concepts still apply).</p><NewLine><p>Concatenating a 2D image and a 1D vector isn’t going to work unless you flatten the image. But you don’t want to do that, because then you lose all the benefits of CNNs.</p><NewLine><p>Here’s some code to show what I mean (I haven’t ran it, no guarantees it wont throw errors):</p><NewLine><pre><code class=""lang-python"">image_extractor = nn.Sequential(<NewLine>    nn.Conv2d(1, 32, 8, stride=4),<NewLine>    nn.ReLU(),<NewLine>    nn.Conv2d(32, 64, 4, stride=2),<NewLine>    nn.ReLU(),<NewLine>    nn.Conv2d(64, 32, 3, stride=1),<NewLine>    nn.ReLU(),<NewLine>    Flatten(),<NewLine>    nn.Linear(32 * 7 * 7, 64),  # Assuming 80x80x1 image<NewLine>    nn.ReLU()<NewLine>)<NewLine><NewLine>linear_extractor = nn.Sequential(<NewLine>    nn.Linear(7, 64),<NewLine>    nn.ReLU(),<NewLine>    nn.Linear(64, 64),<NewLine>    nn.ReLU()<NewLine>)<NewLine><NewLine>q_network = nn.Sequential(<NewLine>    nn.Linear(128, 64),<NewLine>    nn.ReLU(),<NewLine>    nn.Linear(128, 4)  # Assuming 4 actions<NewLine>)<NewLine><NewLine>image_obs = torch.Tensor(camera.processed_image)<NewLine>linear_obs = torch.from_numpy(np.concatenate(pose, sensor_range), axis=0))<NewLine><NewLine>image_features = image_extractor(image_obs)<NewLine>linear_features = linear_extractor(linear_obs)<NewLine>features = torch.cat([image_features, linear_features])<NewLine><NewLine>q_values = q_network(features)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/omegastick"">@Omegastick</a> Do you have a link to a paper that does what you have suggested?</p><NewLine><p>It shouldn’t be the case that you’re using a CNN for the image and an MLP for the pose and sensor values, just to make the inputs compatible to an RL algorithm’s function approximator.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/edowson"">@edowson</a> I think I miscommunicated. The CNN and MLP aren’t making the inputs compatible for the function approximator, the are a <em>part of</em> the function approximator. Putting each separate input through an embedding is pretty standard practice (see <a href=""https://d4mucfpksywv.cloudfront.net/research-covers/openai-five/network-architecture.pdf"" rel=""nofollow noopener"">OpenAI 5</a>).</p><NewLine><p>If you have a 2 dimensional image, and you don’t want to flatten it (which is very understandable, computation quickly becomes infeasible if you do) then your only remaining option (outside of a few, very experimental techniques) is to use a 2D CNN. However, 2D CNNs simply aren’t compatible with 1D inputs, so they need be converted to compatible shapes (usually using a CNN to project the 2D image onto a 1D embedding) before they can be concatenated and used together.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/omegastick"">@Omegastick</a> Thanks, the OpenAI 5 architecture diagram is just the sort of thing that I need for my experiments, albeit with a fewer number of observations. Is there a reference pytorch implementation that you know of for OpenAI 5, so that I can learn from that?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>On a related note, this post by <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> talks about concatenating a layer output with additional input data. I need to think and read a bit more, for what I need to do though.</p><NewLine><p><img alt=""image"" height=""251"" src=""https://discuss.pytorch.org/uploads/default/original/2X/3/391bc74b8cd5babfec4010f685cb73f2dd36a222.png"" width=""450""/></p><NewLine><aside class=""quote quote-modified"" data-post=""2"" data-topic=""20462""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/ptrblck/40/1823_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/concatenate-layer-output-with-additional-input-data/20462/2"">Concatenate layer output with additional input data</a> <a class=""badge-wrapper bullet"" href=""/c/vision""><span class=""badge-category-bg"" style=""background-color: #AB9364;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""Topics related to either pytorch/vision or vision research related topics"">vision</span></a><NewLine></div><NewLine><blockquote><NewLine>    Here is a small example for your use case: <NewLine>class MyModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(MyModel, self).__init__()<NewLine>        self.cnn = models.inception_v3(pretrained=False, aux_logits=False)<NewLine>        self.cnn.fc = nn.Linear(<NewLine>            self.cnn.fc.in_features, 20)<NewLine>        <NewLine>        self.fc1 = nn.Linear(20 + 10, 60)<NewLine>        self.fc2 = nn.Linear(60, 5)<NewLine>        <NewLine>    def forward(self, image, data):<NewLine>        x1 = self.cnn(image)<NewLine>        x2 = data<NewLine>        <NewLine>        x = torch.cat((x1, x2…<NewLine>  </blockquote><NewLine></aside><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>While this approach might generally work, I had some trouble concatenating the outputs of a pre-trained CNN and a Fully-Connected model in the past maybe due to different output value stats. It seemed the whole model just ignored the FC part and just used the CNN outputs.<br/><NewLine>After carefully rescaling the outputs it was working, so you might also want to consider this.<br/><NewLine>Let me know, how it works out. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=7"" title="":wink:""/></p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> If we set aside running the observations through a NN, for the moment, what sort of data-structure should I use to treat the image, pose and sonar senor reading as a single sample, using numpy?</p><NewLine><p>The word ‘Tensor’ comes to mind, since at one data point, you have multiple quantities being represented: image (84x84x1), pose (6x1) + sensor reading (1x1).</p><NewLine><p>All but the last dimension of these three quantities match.</p><NewLine><p>How can I create a ‘Tensor’ using numpy, as a first step? This is so that I treat observations as tensors within the OpenAI Gym environment.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>As <a class=""mention"" href=""/u/omegastick"">@Omegastick</a> explained, you would need to flatten all data so that you could concatenate the inputs in the feature dimension.<br/><NewLine>The structure of the image tensor will get lost, if you are using linear layers. On the other hand while <code>nn.Conv1d</code> might capture some of the image structure, the “temporal” dimension between the different data samples might cause other issues.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Omegastick; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/edowson; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Omegastick; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/edowson; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Omegastick; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/edowson; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/edowson; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/edowson; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: March 28, 2019,  7:24am; <NewLine> REPLY_DATE 2: March 28, 2019,  7:40am; <NewLine> REPLY_DATE 3: March 28, 2019,  7:49am; <NewLine> REPLY_DATE 4: March 28, 2019,  9:06am; <NewLine> REPLY_DATE 5: March 28, 2019, 10:10am; <NewLine> REPLY_DATE 6: March 28, 2019, 10:26am; <NewLine> REPLY_DATE 7: March 28, 2019, 10:54am; <NewLine> REPLY_DATE 8: March 28, 2019, 12:08pm; <NewLine> REPLY_DATE 9: March 28, 2019,  5:59pm; <NewLine> REPLY_DATE 10: March 28, 2019,  7:12pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> 
40401,Loss function for Reinforcement Learning,2019-03-20T13:07:17.691Z,0,447,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey,</p><NewLine><p>still being new to PyTorch, I am still a bit uncertain about ways of using inbuilt loss functions correctly. On PyTorch’s official <a href=""https://pytorch.org/docs/stable/nn.html#mseloss"" rel=""nofollow noopener"">website</a> on loss functions, examples are provided where both so called inputs and target values are provided to a loss function. I assume, that the input tensor models the output of a network, such that loss functions compute the loss as a function of the difference between the target and the input values.</p><NewLine><p>In my concrete case, I am working on an implementation of Reinforcement Learning (DQN). So, If I am not mistaken and assuming that the above is correct, applying a single (simplified) update to my network in each iteration of an update-function’s for-loop should look somewhat like that:</p><NewLine><pre><code class=""lang-auto"">def updateNetwork():<NewLine>        <NewLine>        mini_batch = self.constructMiniBatch()<NewLine><NewLine>        for sample in mini_batch:<NewLine>            # compute q_values (predictions)<NewLine>            predicted_q_values = self.q_net(sample.current_state, sample.current_state_actions)<NewLine>            <NewLine>            # compute estimated max target value of next state<NewLine>            discounted_future_rewards = discount_factor * self.target_net(sample.next_state, sample.next_state_actions)<NewLine>            # argmax<NewLine>            max_future_reward, _ = t.squeeze(discounted_future_rewards).max(0)<NewLine><NewLine>            # sum things up <NewLine>            max_future_reward += sample.observed_reward<NewLine><NewLine>            # first: target values = predictions; then only set particular q_value to actual target value (max_future_reward) for which we have an observed reward<NewLine>            target_values = predicted_q_values.clone()<NewLine>            target_values[0, action_chosen_in_the_past] = max_future_reward<NewLine><NewLine>            # apply update - This should make (only) the predicted q_value for which we observed a concrete reward in the past more similar target value <NewLine>            self.q_net.optimizer.zero_grad()<NewLine>            loss = self.q_net.loss(predicted_q_values, target_values)<NewLine>            loss.backward()<NewLine>            self.q_net.optimizer.step()<NewLine>    <NewLine></code></pre><NewLine><p>If this is correct so far, I was wondering how to translate this cumbersome variant of sequential batch training in a more efficient form of batch training, avoiding the outer for-loop. I assume, this would lead to better performance due to better parallelization on the GPU.<br/><NewLine>Could I simply pass a batch of inputs through the network, get a batch of q-values returned, and then pass both the batch of predicted q_values and the batch of target_values to the loss function? Is that how easy it works?</p><NewLine><p>By the way, I am aware that there is an official <a href=""https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noopener"">tutorial</a> on DQNs on the PyTorch website, but I am still a bit uncertain about it.</p><NewLine><p>Thanks a lot in advance!</p><NewLine><p>Daniel</p><NewLine></div>",https://discuss.pytorch.org/u/Bick95,(Dan),Bick95,"March 20, 2019,  1:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""40401""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v3/letter/b/3d9bf3/40.png"" width=""20""/> Bick95:</div><NewLine><blockquote><NewLine><p>PyTorch’s official <a href=""https://pytorch.org/docs/stable/nn.html#mseloss"" rel=""nofollow noopener"">website</a> on loss functions, examples are provided where both so called inputs and target values are provided to a loss function. I assume, that the input tensor models the output of a network, such that loss functions compute the loss as a function of the difference between the target and the input values.</p><NewLine></blockquote><NewLine></aside><NewLine><p>That 's correct. I agree that the naming your “prediction” input is somewhat misleading. I personally use <code>prediction = model(data) output = loss(prediction, target) output.backward()</code><br/><NewLine>to train my models.</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""40401""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v3/letter/b/3d9bf3/40.png"" width=""20""/> Bick95:</div><NewLine><blockquote><NewLine><p>Could I simply pass a batch of inputs through the network, get a batch of q-values returned, and then pass both the batch of predicted q_values and the batch of target_values to the loss function? Is that how easy it works?</p><NewLine></blockquote><NewLine></aside><NewLine><p>I’m currently working on PPO stuff and I don’t claim to be an expert on Q learning but I think the problem is that the exploration is fundamentally serial because you look for the most promising action to take for a given state and a given approximation of the Q-Value so where you end up next is dependent on that. What you could do though is running multiple processes in which you use your Q-network to fill a joint buffer. Once the buffer is full you can use batches to update your Q-Network.</p><NewLine><p>I hope this helps <img alt="":grimacing:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/grimacing.png?v=7"" title="":grimacing:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Fasc; <NewLine> ,"REPLY_DATE 1: March 26, 2019,  2:07pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
20685,DQN is not learning,2018-07-04T06:55:59.382Z,1,702,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have written code for Deep Q learning and tried on the bunch of environment but on each of them, my algorithms failed to perform correctly. I printed the q values for few states and I found all of the action has learned almost same values. Not sure why this is happening.</p><NewLine><pre><code class=""lang-auto"">mem = Memory(memsize=MEMORY_SIZE)<NewLine>main_model = Network(image_input_size=INPUT_IMAGE_DIM,out_size=OUTPUT_SIZE).float().cuda()<NewLine>target_model = Network(image_input_size=INPUT_IMAGE_DIM,out_size=OUTPUT_SIZE).float().cuda()<NewLine><NewLine>target_model.load_state_dict(main_model.state_dict())<NewLine>criterion = nn.SmoothL1Loss()<NewLine>optimizer = torch.optim.Adam(main_model.parameters())<NewLine><NewLine># filling memory with transitions<NewLine>prev_state = env.reset()<NewLine>processed_prev_state = preprocess_image(prev_state)<NewLine><NewLine>for i in range(0,MAX_STEPS):<NewLine>    action = np.random.randint(0,4)<NewLine>    next_state,reward,done = env.step(action)<NewLine>    processed_next_state = preprocess_image(next_state)<NewLine>    <NewLine>    mem.add_sample((processed_prev_state,action,reward,processed_next_state))<NewLine>    <NewLine>    if done == True:<NewLine>        print('BRO GAME OVER !')<NewLine>    prev_state = next_state<NewLine>    processed_prev_state = processed_next_state<NewLine><NewLine>print('Populated %d Samples'%(len(mem.memory)))<NewLine><NewLine># Algorithm Starts<NewLine>total_steps = 0<NewLine>epsilon = INITIAL_EPSILON<NewLine>loss_stat = []<NewLine>total_reward_stat = []<NewLine><NewLine>for episode in range(0,TOTAL_EPISODES):<NewLine>    prev_state = env.reset()<NewLine>    processed_prev_state = preprocess_image(prev_state)<NewLine>    step_count = 0<NewLine>    total_reward = 0<NewLine>    <NewLine>    while step_count &lt; MAX_STEPS:<NewLine>        <NewLine>        step_count +=1<NewLine>        total_steps +=1<NewLine>        <NewLine>        if np.random.rand() &lt;= epsilon:<NewLine>            action = np.random.randint(0,4)<NewLine>        else:<NewLine>            with torch.no_grad():<NewLine>                torch_x = torch.from_numpy(processed_prev_state).float().cuda()<NewLine>                <NewLine>                model_out = main_model.forward(torch_x,bsize=1)<NewLine>                action = int(torch.argmax(model_out.view(OUTPUT_SIZE),dim=0))<NewLine>        <NewLine>        <NewLine>        next_state, reward, game_over = env.step(action)<NewLine>        processed_next_state = preprocess_image(next_state)<NewLine>        total_reward += reward<NewLine>        <NewLine>        mem.add_sample((processed_prev_state,action,reward,processed_next_state))<NewLine>        <NewLine>        if game_over == True:<NewLine>            print('BRO GAME OVER')<NewLine>        <NewLine>        prev_state = next_state<NewLine>        processed_prev_state = processed_next_state<NewLine>        <NewLine>        if (total_steps % FREEZE_INTERVAL) == 0:<NewLine>            target_model.load_state_dict(main_model.state_dict())<NewLine>        <NewLine>        batch = mem.get_batch(size=BATCH_SIZE)<NewLine>        current_states = []<NewLine>        next_states = []<NewLine>        acts = []<NewLine>        rewards = []<NewLine>        <NewLine>        for element in batch:<NewLine>            current_states.append(element[0])<NewLine>            acts.append(element[1])<NewLine>            rewards.append(element[2])<NewLine>            next_states.append(element[3])<NewLine>        current_states = np.array(current_states)<NewLine>        next_states = np.array(next_states)<NewLine>        rewards = np.array(rewards)<NewLine>        <NewLine>        Q_next = target_model.forward(torch.from_numpy(next_states).float().cuda(),bsize=BATCH_SIZE)<NewLine>        Q_s = main_model.forward(torch.from_numpy(current_states).float().cuda(),bsize=BATCH_SIZE)<NewLine>        target_out = (torch.from_numpy(rewards).float().cuda() + (GAMMA * torch.max(Q_next.detach(),dim=1)[0]))<NewLine>        target_values = torch.zeros(Q_s.size()).cuda()<NewLine>        target_values.copy_(Q_s.detach())<NewLine>        <NewLine>        for i in range(0,BATCH_SIZE):<NewLine>            target_values[i][acts[i]] = target_out[i].detach()<NewLine>        <NewLine>        loss = criterion(Q_s,target_values)<NewLine>        <NewLine>        # saving performance stat<NewLine>        loss_stat.append(loss.item())<NewLine>        <NewLine>        # make previous grad zero<NewLine>        optimizer.zero_grad()<NewLine>        <NewLine>        # backpropogate<NewLine>        loss.backward()<NewLine>        <NewLine>        # update params<NewLine>        optimizer.step()<NewLine>    <NewLine>    # save performance stat<NewLine>    total_reward_stat.append(total_reward/MAX_STEPS)<NewLine>    <NewLine>    if epsilon &gt; FINAL_EPSILON:<NewLine>        epsilon -= (INITIAL_EPSILON - FINAL_EPSILON)/TOTAL_EPISODES<NewLine>    <NewLine>    if (episode + 1)% PERFORMANCE_SAVE_INTERVAL == 0:<NewLine>        perf = {}<NewLine>        perf['loss'] = loss_stat<NewLine>        perf['total_reward'] = total_reward_stat<NewLine>        save_obj(name='MDP_ENV',obj=perf)<NewLine>    <NewLine>    #print('Completed episode : ',episode+1,' Epsilon : ',epsilon,' Reward : ',total_reward/MAX_STEPS)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Mayank_Kumar_Pal,(Mayank Kumar Pal),Mayank_Kumar_Pal,"July 4, 2018,  6:55am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do the values of the weights change after each <code>.step</code>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes. I tried to print the sum of the weights of the fc2 layer after each <code>.step</code> and every time I got the different sum.</p><NewLine><pre><code class=""lang-auto""># update params<NewLine>        optimizer.step()<NewLine><NewLine>print(torch.sum(main_model.fc1.weight.data).item())<NewLine></code></pre><NewLine><p>gives</p><NewLine><p>14.266899108886719<br/><NewLine>16.328222274780273<br/><NewLine>16.12299346923828<br/><NewLine>15.318760871887207<br/><NewLine>14.778365135192871<br/><NewLine>14.173827171325684<br/><NewLine>13.708841323852539<br/><NewLine>13.355201721191406<br/><NewLine>12.91303825378418<br/><NewLine>12.379934310913086<br/><NewLine>11.765584945678711<br/><NewLine>11.129434585571289<br/><NewLine>…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Mayank, did you get any clue about what happened? I almost got the almost same q-value for any states. Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Shani_Gamrian; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Mayank_Kumar_Pal; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Guanjie_Huang; <NewLine> ,"REPLY_DATE 1: July 4, 2018,  1:59pm; <NewLine> REPLY_DATE 2: July 4, 2018,  2:17pm; <NewLine> REPLY_DATE 3: March 26, 2019, 12:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
40782,RuntimeError: copy_if failed to synchronize: device-side assert triggered,2019-03-24T21:18:40.962Z,5,3003,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m getting the following errors with my code. It is an adapted version of the PyTorch DQN example.</p><NewLine><pre><code class=""lang-auto"">/pytorch/aten/src/THC/THCTensorScatterGather.cu:97: void THCudaTensor_gatherKernel(TensorInfo&lt;Real, IndexType&gt;, TensorInfo&lt;Real, IndexType&gt;, TensorInfo&lt;long, IndexType&gt;, int, IndexType) [with IndexType = unsigned int, Real = float, Dims = 2]: block: [0,0,0], thread: [62,0,0] Assertion `indexValue &gt;= 0 &amp;&amp; indexValue &lt; src.sizes[dim]` failed.<NewLine>Traceback (most recent call last):<NewLine>  File ""/project/ros-kinetic-alphapilot/catkin_ws/src/alphapilot_openai_ros/ardrone_race_track/src/ardrone_v1_ddqn.py"", line 548, in &lt;module&gt;<NewLine>    optimize_model()<NewLine>  File ""/project/ros-kinetic-alphapilot/catkin_ws/src/alphapilot_openai_ros/ardrone_race_track/src/ardrone_v1_ddqn.py"", line 451, in optimize_model<NewLine>    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()<NewLine>RuntimeError: copy_if failed to synchronize: device-side assert triggered<NewLine></code></pre><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_ddqn.py#L451"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_ddqn.py#L451"" rel=""nofollow noopener"" target=""_blank"">edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_ddqn.py#L451</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""441"" style=""counter-reset: li-counter 440 ;""><NewLine><li># columns of actions taken. These are the actions which would've been taken</li><NewLine><li># for each batch state according to policy_net</li><NewLine><li>state_action_values = policy_net(state_batch).gather(1, action_batch)</li><NewLine><li><NewLine></li><NewLine><li># Compute V(s_{t+1}) for all next states.</li><NewLine><li># Expected values of actions for non_final_next_states are computed based</li><NewLine><li># on the ""older"" target_net; selecting their best reward with max(1)[0].</li><NewLine><li># This is merged based on the mask, such that we'll have either the expected</li><NewLine><li># state value or 0 in case the state was final.</li><NewLine><li>next_state_values = torch.zeros(BATCH_SIZE, device=device)</li><NewLine><li class=""selected"">next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()</li><NewLine><li># Compute the expected Q values</li><NewLine><li>expected_state_action_values = (next_state_values * GAMMA) + reward_batch.float()</li><NewLine><li><NewLine></li><NewLine><li># Compute Huber loss</li><NewLine><li>loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))</li><NewLine><li><NewLine></li><NewLine><li># Optimize the model</li><NewLine><li>optimizer.zero_grad()</li><NewLine><li>loss.backward()</li><NewLine><li>for param in policy_net.parameters():</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>The hyperparameters are as follows:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/config/ardrone_v1_race_track_ddqn_params.yaml#L14-L29"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/config/ardrone_v1_race_track_ddqn_params.yaml#L14-L29"" rel=""nofollow noopener"" target=""_blank"">edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/config/ardrone_v1_race_track_ddqn_params.yaml#L14-L29</a></h4><NewLine><pre class=""onebox""><code class=""lang-yaml""><ol class=""start lines"" start=""14"" style=""counter-reset: li-counter 13 ;""><NewLine><li># screen parameters</li><NewLine><li>screen_height: 180</li><NewLine><li>screen_width: 320</li><NewLine><li><NewLine></li><NewLine><li>show_image: False</li><NewLine><li><NewLine></li><NewLine><li># ddqn parameters</li><NewLine><li>gamma: 0.999</li><NewLine><li><NewLine></li><NewLine><li>epsilon_start: 0.9</li><NewLine><li>epsilon_end: 0.01</li><NewLine><li>epsilon_decay: 500</li><NewLine><li><NewLine></li><NewLine><li>batch_size: 128</li><NewLine><li>replay_memory_size: 1024</li><NewLine><li>target_network_update_interval: 10</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>",https://discuss.pytorch.org/u/edowson,(Elvis Dowson),edowson,"March 24, 2019,  9:18pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I ran with <code>device=cpu</code> to debug the error, and the error is asserted at line 443:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_ddqn.py#L440-L443"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_ddqn.py#L440-L443"" rel=""nofollow noopener"" target=""_blank"">edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_ddqn.py#L440-L443</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""440"" style=""counter-reset: li-counter 439 ;""><NewLine><li># Compute Q(s_t, a) - the model computes Q(s_t), then we select the</li><NewLine><li># columns of actions taken. These are the actions which would've been taken</li><NewLine><li># for each batch state according to policy_net</li><NewLine><li>state_action_values = policy_net(state_batch).gather(1, action_batch)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><pre><code class=""lang-auto"">  File ""/project/ros-kinetic-alphapilot/catkin_ws/src/alphapilot_openai_ros/ardrone_race_track/src/ardrone_v1_ddqn.py"", line 443, in optimize_model<NewLine>    state_action_values = policy_net(state_batch).gather(1, action_batch)<NewLine>RuntimeError: Invalid index in gather at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:457<NewLine></code></pre><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> would you happen to know how I can fix this?</p><NewLine><p>The default cartpole example had 2 actions.</p><NewLine><pre><code class=""lang-auto"">    else:<NewLine>        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)<NewLine></code></pre><NewLine><p>I noticed that it crashes, when I update it for my task environment, which has 7 actions.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_ddqn.py#L379-L380"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_ddqn.py#L379-L380"" rel=""nofollow noopener"" target=""_blank"">edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_ddqn.py#L379-L380</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""379"" style=""counter-reset: li-counter 378 ;""><NewLine><li>else:</li><NewLine><li>    return torch.tensor([[random.randrange(N_ACTIONS)]], device=device, dtype=torch.long)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p><code>N_ACTIONS=7</code>, in this case. If i set it to 2, there is no crash.</p><NewLine><p>Why would this be an issue? I can’t seem to locate any other part of the code that hard-codes the total number of actions.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you print the shape of <code>policy_net(state_batch)</code> and the min and max values of <code>action_batch</code>?<br/><NewLine>Some indices are apparently out of bounds for the <code>gather</code> operation.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> Here is the output.</p><NewLine><pre><code class=""lang-auto"">print(""policy_net(state_batch).shape: {}"".format(policy_net(state_batch).shape))<NewLine>print(""state_batch.shape: {}"".format(state_batch.shape))<NewLine>print(""action_batch: shape= {}, max= {}, min= {}"".format(action_batch.shape, action_batch.max(), action_batch.min()))<NewLine>state_action_values = policy_net(state_batch).gather(1, action_batch)<NewLine></code></pre><NewLine><p>output:</p><NewLine><pre><code class=""lang-bash"">policy_net(state_batch).shape: torch.Size([128, 2])<NewLine>state_batch.shape: torch.Size([128, 3, 180, 320])<NewLine>action_batch: shape= torch.Size([128, 1]), max= 6, min= 0<NewLine></code></pre><NewLine><p>I have a total of 7 actions. Action values 0 to 6 are mapped for the following drone movements: <code>FORWARDS, BACKWARDS, STRAFE_LEFT, ;STRAFE_RIGHT, UP, DOWN, STOP</code>.</p><NewLine><p>There are a total of 8 observations. <code>x, y, z ,r, p, y, sonar_value, collision</code></p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_goto_task_env.py#L39-L40"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_goto_task_env.py#L39-L40"" rel=""nofollow noopener"" target=""_blank"">edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_goto_task_env.py#L39-L40</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""39"" style=""counter-reset: li-counter 38 ;""><NewLine><li>number_actions = rospy.get_param('/drone/n_actions')</li><NewLine><li>self.action_space = spaces.Discrete(number_actions)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_goto_task_env.py#L158-L180"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_goto_task_env.py#L158-L180"" rel=""nofollow noopener"" target=""_blank"">edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_goto_task_env.py#L158-L180</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""158"" style=""counter-reset: li-counter 157 ;""><NewLine><li>if action == 0: #FORWARDS</li><NewLine><li>    linear_speed_vector.x = self.linear_forward_speed</li><NewLine><li>    self.last_action = ""FORWARDS""</li><NewLine><li>elif action == 1: #BACKWARDS</li><NewLine><li>    linear_speed_vector.x = -1*self.linear_forward_speed</li><NewLine><li>    self.last_action = ""BACKWARDS""</li><NewLine><li>elif action == 2: #STRAFE_LEFT</li><NewLine><li>    linear_speed_vector.y = self.linear_forward_speed</li><NewLine><li>    self.last_action = ""STRAFE_LEFT""</li><NewLine><li>elif action == 3: #STRAFE_RIGHT</li><NewLine><li>    linear_speed_vector.y = -1*self.linear_forward_speed</li><NewLine><li>    self.last_action = ""STRAFE_RIGHT""</li><NewLine><li>elif action == 4: #UP</li><NewLine><li>    linear_speed_vector.z = self.linear_forward_speed</li><NewLine><li>    self.last_action = ""UP""</li><NewLine><li>elif action == 5: #DOWN</li><NewLine><li>    linear_speed_vector.z = -1*self.linear_forward_speed</li><NewLine><li>    self.last_action = ""DOWN""</li><NewLine><li>elif action == 6: #STOP</li><NewLine><li>    linear_speed_vector.x = 0.0</li><NewLine></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_goto_task_env.py#L158-L180"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>I’m running it on the cpu, to debug, and it gives the following error:</p><NewLine><pre><code class=""lang-bash"">  File ""/project/ros-kinetic-alphapilot/catkin_ws/src/alphapilot_openai_ros/ardrone_race_track/src/ardrone_v1_ddqn.py"", line 446, in optimize_model<NewLine>    state_action_values = policy_net(state_batch).gather(1, action_batch)<NewLine>RuntimeError: Invalid index in gather at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:457<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> After viewing the shape of the <code>policy_net(state_batch)</code>, it would appear that the number of outputs were hard-coded to 2.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_ddqn.py#L289"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_ddqn.py#L289"" rel=""nofollow noopener"" target=""_blank"">edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_ddqn.py#L289</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""279"" style=""counter-reset: li-counter 278 ;""><NewLine><li>    self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)</li><NewLine><li>    self.bn3 = nn.BatchNorm2d(32)</li><NewLine><li><NewLine></li><NewLine><li>    # Number of Linear input connections depends on output of conv2d layers</li><NewLine><li>    # and therefore the input image size, so compute it.</li><NewLine><li>    def conv2d_size_out(size, kernel_size = 5, stride = 2):</li><NewLine><li>        return (size - (kernel_size - 1) - 1) // stride  + 1</li><NewLine><li>    convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))</li><NewLine><li>    convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))</li><NewLine><li>    linear_input_size = convw * convh * 32</li><NewLine><li class=""selected"">    self.head = nn.Linear(linear_input_size, 2) # 448 or 512</li><NewLine><li><NewLine></li><NewLine><li># Called with either one element to determine next action, or a batch</li><NewLine><li># during optimization. Returns tensor([[left0exp,right0exp]...]).</li><NewLine><li>def forward(self, x):</li><NewLine><li>    x = F.relu(self.bn1(self.conv1(x)))</li><NewLine><li>    x = F.relu(self.bn2(self.conv2(x)))</li><NewLine><li>    x = F.relu(self.bn3(self.conv3(x)))</li><NewLine><li>    return self.head(x.view(x.size(0), -1))</li><NewLine><li><NewLine></li><NewLine><li><NewLine></li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> I’ve submitted a pull request with updates to the <code>reinforcement_q_learning.py</code> tutorial. I’ve made the DQN network accept the number of outputs and updated the example to obtain the number of actions from the gym environment action space. This will help avoid similar issues for others who my try the DQN example with different gym environments.</p><NewLine><aside class=""onebox githubpullrequest""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/tutorials/pull/451"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/tutorials</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://github.com/edowson"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""edowson"" class=""thumbnail onebox-avatar"" height=""90"" src=""https://avatars2.githubusercontent.com/u/10365427?v=4"" width=""90""/><NewLine></a><NewLine><h4><NewLine><a href=""https://github.com/pytorch/tutorials/pull/451"" rel=""nofollow noopener"" target=""_blank"">Fixes for PyTorch reinforcement learning DQN tutorial.</a><NewLine></h4><NewLine><div class=""date""><NewLine>  by <a href=""https://github.com/edowson"" rel=""nofollow noopener"" target=""_blank"">edowson</a><NewLine>  on <a href=""https://github.com/pytorch/tutorials/pull/451"" rel=""nofollow noopener"" target=""_blank"">06:12PM - 25 Mar 19 UTC</a><NewLine></div><NewLine><div class=""github-commit-stats""><NewLine><strong>2 commits</strong><NewLine>  changed <strong>2 files</strong><NewLine>  with <strong>13 additions</strong><NewLine>  and <strong>6 deletions</strong>.<NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah, it looks like the hard-coded number of outputs creates this issue. Thanks for the PR and the fix! <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=7"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/edowson; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/edowson; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/edowson; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/edowson; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: March 25, 2019,  6:04am; <NewLine> REPLY_DATE 2: March 25, 2019, 12:43pm; <NewLine> REPLY_DATE 3: March 25, 2019,  1:52pm; <NewLine> REPLY_DATE 4: March 25, 2019,  6:21pm; <NewLine> REPLY_DATE 5: March 25, 2019,  6:20pm; <NewLine> REPLY_DATE 6: March 26, 2019, 12:41am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> 
40799,Issue with handling invalid moves in reinforcement learning,2019-03-25T05:57:35.300Z,0,275,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am new to pyTorch and so far I love it compared to Tensorflow.</p><NewLine><p>I run into an issue during a reinforcement learning exercise.<br/><NewLine>What I am trying to do is to use valid actions when computing the softmax.<br/><NewLine>For example, let’s say I have this logit from the model for 3 actions: (0.3, 0.4, 0.1)<br/><NewLine>but I don’t want to use the first item so I set it to 0: (0, 0.4, 0.1) and take the softmax of only (0.4, 0.1):<br/><NewLine>F.softmax(torch.tensor([ 0.4, 0.1]), dim=-1) = tensor([0.5744, 0.4256])<br/><NewLine>So I would eventually like to get to [0, 0.5744, 0.4256] and feed this to the system like below:</p><NewLine><p>I had many version of filterValidActions() but it always throws this error:<br/><NewLine><code>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation</code></p><NewLine><p>I know that I do in-place operation on action <code>probs</code> but I could not figure out how I can take softmax on valid actions and also retain the grad on it.<br/><NewLine>every time I have to make an assignment like this:<br/><NewLine><code>probs[:,:,2] = 0</code> when  action is 0 and this creates the issue.</p><NewLine><pre><code class=""lang-auto"">def select_my_action(state, model_hidden, prev_action):<NewLine>    state = torch.FloatTensor(state) <NewLine>    state = state.view(1, 1, state.size(0)).to(device)<NewLine>    probs, state_value, model_hidden = model(state, model_hidden)<NewLine><NewLine>    probs = filterValidActions(probs, prev_action)<NewLine>    <NewLine>    m = Categorical(probs)<NewLine>    action = m.sample()<NewLine>    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))<NewLine>    return action.item(), model_hidden<NewLine><NewLine>def filterValidActions(probs, action):<NewLine>    newProbs = probs.clone()<NewLine>    if (action == 0): # 0 -&gt; [0,1]        <NewLine>        new2Probs = F.softmax(newProbs[:,:,0:2], dim=-1)<NewLine>        newProbs[:,:,0:2] = new2Probs<NewLine>        newProbs[:,:,2] = 0<NewLine>    return newProbs<NewLine></code></pre><NewLine><p>I looked at this link but I still could not figure out my issue… <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=7"" title="":frowning:""/><br/><NewLine><aside class=""quote quote-modified"" data-post=""1"" data-topic=""836""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v3/letter/y/f04885/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/encounter-the-runtimeerror-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation/836"">Encounter the RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation</a><NewLine></div><NewLine><blockquote><NewLine>    I am going to define my layer. How ever, I encounter the RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation while running backward(). <NewLine>I found that if i commented the second for loop ‘for j in range(self.number_person):’ or make ‘u_i[:,j,:] = (1 - self.lumbda)*u_i[:,j,:]’, the backward() was fine. <NewLine>I wonder where is inplace operation and why it does not work? ‘p_rnn_feature’ and ‘u_sum’ has been compute before. <NewLine>BTW, this code is run on p…<NewLine>  </blockquote><NewLine></aside><NewLine></p><NewLine><p>Do you have any solution to this issue?<br/><NewLine>Thanks a lot</p><NewLine></div>",https://discuss.pytorch.org/u/musti76,,musti76,"March 25, 2019,  6:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""40799""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v3/letter/m/94ad74/40.png"" width=""20""/> musti76:</div><NewLine><blockquote><NewLine><p>new2Probs = F.softmax(newProbs[:,:,0:2], dim=-1) newProbs[:,:,0:2] = new2Probs newProbs[:,:,2] = 0</p><NewLine></blockquote><NewLine></aside><NewLine><p>You can change these three lines as below and try if it works:</p><NewLine><pre><code class=""lang-auto"">only2probs = F.softmax(newProbs[:,:,0:2], dim=-1)<NewLine>zeros = torch.zeros(only2probs.shape[0], only2probs.shape[1], 1)<NewLine>newProbs = torch.cat((only2probs, zeros), dim=-1)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""40799""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/innovarul/40/5282_2.png"" width=""20""/> InnovArul:</div><NewLine><blockquote><NewLine><p>only2probs = F.softmax(newProbs[:,:,0:2], dim=-1) zeros = torch.zeros(only2probs.shape[0], only2probs.shape[1], 1) newProbs = torch.cat((only2probs, zeros), dim=-1)</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thank You so much Arul. It did work!<br/><NewLine>It does retain the grad as well.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/InnovArul; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/musti76; <NewLine> ,"REPLY_DATE 1: March 25, 2019,  7:34pm; <NewLine> REPLY_DATE 2: March 25, 2019,  8:15pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
40581,Error: void THCudaTensor_gatherKernel() failed,2019-03-22T04:47:37.525Z,0,627,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve adapted the <a href=""https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noopener"">PyTorch DQN tutorial</a>  to take inputs from a ROS camera topic, and use that as the observation to create an ARDrone DDQN example, using the Gazebo 7 simulator, but I’m getting a crash.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/b79c00275ae33b70274e92766435e417fe733b66"" href=""https://discuss.pytorch.org/uploads/default/original/2X/b/b79c00275ae33b70274e92766435e417fe733b66.png"" title=""ardrone-race-track-01.png""><img alt=""ardrone-race-track-01"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/b/b79c00275ae33b70274e92766435e417fe733b66_2_10x10.png"" height=""291"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/b/b79c00275ae33b70274e92766435e417fe733b66_2_517x291.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/b/b79c00275ae33b70274e92766435e417fe733b66_2_517x291.png, https://discuss.pytorch.org/uploads/default/optimized/2X/b/b79c00275ae33b70274e92766435e417fe733b66_2_775x436.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/b/b79c00275ae33b70274e92766435e417fe733b66_2_1034x582.png 2x"" width=""517""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">ardrone-race-track-01.png</span><span class=""informations"">1280×720 63.2 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>I’ve debugged the image, to ensure that the rescaled image is working properly before feeding it to the model:</p><NewLine><p><img alt=""processed_image"" height=""90"" src=""https://discuss.pytorch.org/uploads/default/original/2X/0/0f553897e269c7cfed597420e4bbb6cc5cdf54f7.png"" width=""160""/></p><NewLine><p>I’ve set the following hyperparameters:</p><NewLine><pre><code class=""lang-auto"">  batch_size: 128<NewLine>  target_network_update_interval: 2<NewLine></code></pre><NewLine><p>It throws up the following error, at line 192:</p><NewLine><p><a class=""onebox"" href=""https://github.com/edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_start_training_ddqn.py#L192"" rel=""nofollow noopener"" target=""_blank"">https://github.com/edowson/alphapilot_openai_ros/blob/master/ardrone_race_track/src/ardrone_v1_start_training_ddqn.py#L192</a></p><NewLine><pre><code class=""lang-auto""><NewLine>  File ""/project/ros-kinetic-alphapilot/catkin_ws/src/alphapilot_openai_ros/ardrone_race_track/src/ardrone_v1_start_training_ddqn.py"", line 423, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""/project/ros-kinetic-alphapilot/catkin_ws/src/alphapilot_openai_ros/ardrone_race_track/src/ardrone_v1_start_training_ddqn.py"", line 398, in main<NewLine>    agent.train()<NewLine>  File ""/project/ros-kinetic-alphapilot/catkin_ws/src/alphapilot_openai_ros/ardrone_race_track/src/ardrone_v1_start_training_ddqn.py"", line 237, in train<NewLine>    self.optimize_model()<NewLine>  File ""/project/ros-kinetic-alphapilot/catkin_ws/src/alphapilot_openai_ros/ardrone_race_track/src/ardrone_v1_start_training_ddqn.py"", line 192, in optimize_model<NewLine>    next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()<NewLine>  File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 491, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/project/ros-kinetic-alphapilot/catkin_ws/src/alphapilot_openai_ros/ardrone_race_track/src/model/dqn/dqn.py"", line 103, in forward<NewLine>    x = F.relu(self.bn3(self.conv3(x)))<NewLine>  File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 491, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/conv.py"", line 339, in forward<NewLine>    self.padding, self.dilation, self.groups)<NewLine>RuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR<NewLine>[INFO] [1553228217.202863, 5023.352000]: Shutting down node: ardrone_v1_goto_ddqn<NewLine>/pytorch/aten/src/THC/THCTensorScatterGather.cu:97: void THCudaTensor_gatherKernel(TensorInfo&lt;Real, IndexType&gt;, TensorInfo&lt;Real, IndexType&gt;, TensorInfo&lt;long, IndexType&gt;, int, IndexType) [with IndexType = unsigned int, Real = float, Dims = 2]: block: [0,0,0], thread: [32,0,0] Assertion `indexValue &gt;= 0 &amp;&amp; indexValue &lt; src.sizes[dim]` failed.<NewLine>/pytorch/aten/src/THC/THCTensorScatterGather.cu:97: void THCudaTensor_gatherKernel(TensorInfo&lt;Real, IndexType&gt;, TensorInfo&lt;Real, IndexType&gt;, TensorInfo&lt;long, IndexType&gt;, int, IndexType) [with IndexType = unsigned int, Real = float, Dims = 2]: block: [0,0,0], thread: [33,0,0] Assertion `indexValue &gt;= 0 &amp;&amp; indexValue &lt; src.sizes[dim]` failed.<NewLine><NewLine>&lt;snip&gt;<NewLine><NewLine>/pytorch/aten/src/THC/THCTensorScatterGather.cu:97: void THCudaTensor_gatherKernel(TensorInfo&lt;Real, IndexType&gt;, TensorInfo&lt;Real, IndexType&gt;, TensorInfo&lt;long, IndexType&gt;, int, IndexType) [with IndexType = unsigned int, Real = float, Dims = 2]: block: [0,0,0], thread: [125,0,0] Assertion `indexValue &gt;= 0 &amp;&amp; indexValue &lt; src.sizes[dim]` failed.<NewLine>/pytorch/aten/src/THC/THCTensorScatterGather.cu:97: void THCudaTensor_gatherKernel(TensorInfo&lt;Real, IndexType&gt;, TensorInfo&lt;Real, IndexType&gt;, TensorInfo&lt;long, IndexType&gt;, int, IndexType) [with IndexType = unsigned int, Real = float, Dims = 2]: block: [0,0,0], thread: [127,0,0] Assertion `indexValue &gt;= 0 &amp;&amp; indexValue &lt; src.sizes[dim]` failed.<NewLine><NewLine>Process finished with exit code 1<NewLine></code></pre><NewLine><p>I using python-2.7 on Ubuntu-16.04. Here are the versions of the libraries.</p><NewLine><pre><code class=""lang-auto"">PYTORCH_VERSION='nightly'<NewLine>CUDA_VERSION='9.0'<NewLine>CUDNN_VERSION='7.3.1.20'<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/edowson,(Elvis Dowson),edowson,"March 22, 2019,  8:33am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I also changed the Torch version from nightly to release 1.0.1 and cuDNN to 7.4.2.24, but I still get the same error.</p><NewLine><pre><code class=""lang-bash"">CUDA_VERSION='9.0'<NewLine>CUDNN_VERSION='7.4.2.24'<NewLine>PYTORCH_VERSION='1.0.1'<NewLine>TORCHVISION_VERSION='0.2.2'<NewLine></code></pre><NewLine><p>I also reduce the batch size to 1, and get the same error. The GPU is a Titan-V with 12GB HBM2 memory, and total GPU utilization is around 27%, so it isn’t an issue with running an older GPU.</p><NewLine><pre><code class=""lang-auto"">  File ""/project/ros-kinetic-alphapilot/catkin_ws/src/alphapilot_openai_ros/ardrone_race_track/src/ardrone_v1_start_training_ddqn.py"", line 192, in optimize_model<NewLine>    next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()<NewLine>  File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 489, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/project/ros-kinetic-alphapilot/catkin_ws/src/alphapilot_openai_ros/ardrone_race_track/src/model/dqn/dqn.py"", line 102, in forward<NewLine>    x = F.relu(self.bn2(self.conv2(x)))<NewLine>  File ""/usr/local/lib/python2.7/dist-packages/torch/nn/functional.py"", line 862, in relu<NewLine>    result = torch.relu(input)<NewLine>RuntimeError: CUDA error: device-side assert triggered<NewLine>/pytorch/aten/src/THC/THCTensorScatterGather.cu:97: void THCudaTensor_gatherKernel(TensorInfo&lt;Real, IndexType&gt;, TensorInfo&lt;Real, IndexType&gt;, TensorInfo&lt;long, IndexType&gt;, int, IndexType) [with IndexType = unsigned int, Real = float, Dims = 2]: block: [0,0,0], thread: [0,0,0] Assertion `indexValue &gt;= 0 &amp;&amp; indexValue &lt; src.sizes[dim]` failed.<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/edowson; <NewLine> ,"REPLY_DATE 1: March 22, 2019,  9:10am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
40116,Trying to understand why output of nn.Linear (for output layer) isn&rsquo;t retaining,2019-03-18T01:56:25.116Z,0,277,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Here is some code for reference, but I’m trying to understand why the output of the final linear layer gives None for the grad. The final layer of weights will give me a gradient, but I’d consider the result of the final layer would be considered a leaf node. I’m also trying to understand exactly what is happening when in the second to last line of my forward function, it seems like the linear layer is creating a new tensor, but it seems like the gradients are not being retained. However, even when I call .retain_graph_() to the final computation in forward() it still returns None for the gradient. It does work when I make a copy of the tensor and set retain_graph to True (don’t quite understand why I can’t alter the original tensor in the desired way). I’d really appreciate some help with this!</p><NewLine><pre><code class=""lang-auto"">if torch.cuda.is_available():<NewLine>    device = torch.device('cuda')<NewLine>else:<NewLine>    device = torch.device('cpu')<NewLine>type = torch.float<NewLine>args = {'device': device, 'dtype': type}<NewLine><NewLine>class SingleRNN(nn.Module):<NewLine>    def __init__(self, n_inputs, n_outputsize):<NewLine>        super(SingleRNN, self).__init__()<NewLine><NewLine>        numHU = 2<NewLine>        bias = False<NewLine><NewLine>        self.Wxh = nn.Linear(<NewLine>            in_features=n_inputs, out_features=numHU , bias=bias)<NewLine><NewLine>        self.Whh = nn.Linear(<NewLine>            in_features=numHU + self.Wxh.out_features, out_features=numHU , bias=bias)<NewLine><NewLine>        self.Why = nn.Linear(<NewLine>            in_features=numHU , out_features=n_outputsize, bias=bias)<NewLine><NewLine>    def forward(self, X0, H0):<NewLine>        self.L0_X= self.Wxh(X0)<NewLine>        self.L1_H = self.Whh(torch.cat((self.L0_X, H0), dim=1))<NewLine>        self.L2_Y = self.Why(self.L1_H) # so what is happening here, self.L1_H is a tensor that's being passed to the forward function of a Linear layer (does this by default initialize the output to not retain the gradient even though it is a leaf node), the same behavior occurs when i set this line to: self.Why(self.L1_H).retain_graph_(True). However, this works self.L2_Y = tensor(self.Why(self.L1_H), retain_graph=True) and gives the desired gradient, why does altering the existing tensor output not work, why do I need to make a new copy? <NewLine>        return self.L2_Y, self.L1_H<NewLine><NewLine><NewLine>def main():<NewLine>    torch.manual_seed(999)<NewLine>    cudnn.benchmark = True<NewLine><NewLine>    N_INPUT = 4<NewLine>    N_OUTPUTSIZE = 1<NewLine><NewLine>    X0 = torch.tensor([[0, 1, 2, 0]],<NewLine>                      **args)  # t=0 =&gt; 4 X 4<NewLine><NewLine>    H0 = torch.tensor([[0, 0,]],<NewLine>                      **args)  # initialize hidden state to 0's<NewLine><NewLine>    Ytarg = torch.tensor([[1]], **args)<NewLine><NewLine>    model = SingleRNN(N_INPUT, N_OUTPUTSIZE).cuda()<NewLine><NewLine>    criterion = nn.MSELoss()<NewLine>    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)<NewLine><NewLine>    for i in range(1):<NewLine>        parameters = model.parameters()<NewLine><NewLine>        Y_out, H0 = model.forward(X0, H0)<NewLine><NewLine>        loss = criterion.forward(input=Y_out, target=Ytarg)<NewLine>        loss.backward(retain_graph=True)<NewLine>        optimizer.step()<NewLine>        print (model.L2_Y.grad) #-&gt;prints None <NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    main()<NewLine><NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Ceyer_Wakilpoor,(Ceyer Wakilpoor),Ceyer_Wakilpoor,"March 18, 2019,  2:00am",,,,,
39275,Multiple cpu producers with few gpus not utilize 100% of the gpus,2019-03-08T08:25:29.298Z,0,141,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I tried to implement board game self-play data generation in parallel using multiple cpus to do self-paly concurrently. For parent process, i created 4 NN model for 30cpus (1 model for 10 cpus and 1 model to train) each model is in different gpus.(the model is implemented as 20 blocks resnet-like architecture with batchnorm) Pseudo code as follows</p><NewLine><pre><code class=""lang-auto"">nnet = NN(gpu_num=0)<NewLine>nnet1 = NN(gpu_num=1)<NewLine>nnet2 = NN(gpu_num=2)<NewLine>nnet3 = NN(gpu_num=3)<NewLine><NewLine>for i in range(num_iteration):<NewLine>    nnet1.load_state_dict(nnet.state_dict())<NewLine>    nnet2.load_state_dict(nnet.state_dict())<NewLine>    nnet3.load_state_dict(nnet.state_dict())<NewLine>    samples = parallel_self_play()<NewLine>    nnet.train(samples)<NewLine></code></pre><NewLine><p>parallel_self_play() is implemented as follows</p><NewLine><pre><code class=""lang-auto"">pool = mp.Pool(processes=num_cpu) #30<NewLine>for i in range(self.args.numEps):<NewLine>    results = []<NewLine>    if i % 3 == 0:<NewLine>        net = self.nnet1<NewLine>    elif i % 3 == 1:<NewLine>        net = self.nnet2<NewLine>    else:<NewLine>        net = self.nnet3<NewLine><NewLine>     results.append(pool.apply_async(AsyncSelfPlay, args=(net))<NewLine>     # get results from results array then return it<NewLine>     return results <NewLine></code></pre><NewLine><p>My code work perfectly fine with almost 100% gpu utilization throughout the first self-play (less than 10 minutes for an iteration) but after the first iteration (training) when i loaded new weights into nnet1-3 gpu utilization never reach 80% again (~30min - 1hour per iteration). I notice a few things while mess around with me code</p><NewLine><ol><NewLine><li><NewLine><p>This model includes batchnorm layers, when switch model to train() mode -&gt; train -&gt; switch back to eval() causes the self-play (use forward pass from model) to not use gpu at all.</p><NewLine></li><NewLine><li><NewLine><p>If it doesn’t switch from eval() -&gt; train() (train using eval mode) this causes gpu utilization to be lower  (30-50%) but not entirely gone.</p><NewLine></li><NewLine><li><NewLine><p>If the models that are not the main one doesn’t load the weights from the main one, self-play still utilize 100% gpu so my guess is that something happened during training process and change some states in the model.</p><NewLine></li><NewLine></ol><NewLine><p>This also happen when use only 8 cpus - 1gpu architecture and train model on the fly (no intermediate one).</p><NewLine><p>Can someone guide me where to fix my code or how i should train my model?</p><NewLine></div>",https://discuss.pytorch.org/u/51616,(51616),51616,"March 8, 2019,  8:25am",,,,,
35992,Doesn&rsquo;t official REINFORCE example work?,2019-01-30T14:51:35.431Z,2,1186,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was trying to re-implement the Cliff Walking game in Sutton’s book. A simple way is to transfer the official REINFORCE example to this game. Therefore, I directly apply the algorithm given in <a href=""https://github.com/pytorch/examples/edit/master/reinforcement_learning/reinforce.py"" rel=""nofollow noopener"">https://github.com/pytorch/examples/edit/master/reinforcement_learning/reinforce.py</a>. However, it seems that the model doesn’t learn well in this simple setting. I was just wondering where goes wrong?</p><NewLine><pre><code class=""lang-auto"">import argparse<NewLine>import numpy as np<NewLine>from itertools import count<NewLine><NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine>from torch.distributions import Categorical<NewLine><NewLine><NewLine>class CliffWalking(object):<NewLine>    def __init__(self, step_limit=None):<NewLine>        self.shape = (4, 12)<NewLine><NewLine>        # always start from the left-dow corner<NewLine>        # self.pos = np.asarray([self.shape[0] - 1, 0])<NewLine>        self.pos = np.asarray([2, 11])<NewLine><NewLine>        # build a<NewLine>        self.cliff = np.zeros(self.shape, dtype=np.bool)<NewLine>        self.cliff[-1, 1:-1] = 1<NewLine><NewLine>        self.actions = {<NewLine>            'U': 0,<NewLine>            'D': 1,<NewLine>            'L': 2,<NewLine>            'R': 3<NewLine>        }<NewLine><NewLine>        self.action2shift = {<NewLine>            0: [-1, 0],<NewLine>            1: [1, 0],<NewLine>            2: [0, -1],<NewLine>            3: [0, 1]<NewLine>        }<NewLine><NewLine>        self.transmit_tensor = self._build_transmit_tensor_()<NewLine><NewLine>        self.num_actions = len(self.actions)<NewLine>        self.state_dim = 2<NewLine><NewLine>    def _build_transmit_tensor_(self):<NewLine>        trans_matrix = [[[] for _ in range(self.shape[1])] for __ in range(self.shape[0])]<NewLine>        for i in range(self.shape[0]):<NewLine>            for j in range(self.shape[1]):<NewLine>                for a in range(len(self.actions)):<NewLine>                    trans_matrix[i][j].append(self._cal_new_position_((i, j), a))<NewLine><NewLine>        return trans_matrix<NewLine><NewLine>    def _cal_new_position_(self, old_pos, action):<NewLine>        old_pos = np.asarray(old_pos)<NewLine>        new_pos = old_pos + self.action2shift[action]<NewLine>        new_pos[0] = max(new_pos[0], 0)<NewLine>        new_pos[0] = min(new_pos[0], self.shape[0] - 1)<NewLine>        new_pos[1] = max(new_pos[1], 0)<NewLine>        new_pos[1] = min(new_pos[1], self.shape[1] - 1)<NewLine><NewLine>        reward = -1.<NewLine>        terminate = False<NewLine><NewLine>        if self.cliff[old_pos[0]][old_pos[1]]:<NewLine>            reward = -100.<NewLine>            new_pos[0] = self.shape[0] - 1<NewLine>            new_pos[1] = 0<NewLine>            terminate = True<NewLine><NewLine>        if old_pos[0] == self.shape[0] - 1 and old_pos[1] == self.shape[1] - 1:<NewLine>            terminate = True<NewLine>            new_pos[0] = self.shape[0] - 1<NewLine>            new_pos[1] = self.shape[1] - 1<NewLine><NewLine>        return new_pos, reward, terminate<NewLine><NewLine>    def take_action(self, action):<NewLine>        if isinstance(action, str):<NewLine>            new_pos, reward, terminate = self.transmit_tensor[self.pos[0]][self.pos[1]][self.actions[action]]<NewLine>        else:<NewLine>            new_pos, reward, terminate = self.transmit_tensor[self.pos[0]][self.pos[1]][action]<NewLine>        self.pos[0] = new_pos[0]<NewLine>        self.pos[1] = new_pos[1]<NewLine>        self.steps += 1<NewLine>        return new_pos, reward, terminate<NewLine><NewLine>    def show_pos(self):<NewLine>        env = np.zeros(self.shape)<NewLine>        env[self.pos[0]][self.pos[1]] = 1<NewLine>        print(env)<NewLine><NewLine>    def reset(self):<NewLine>        # self.pos = np.asarray([self.shape[0] - 1, 0])<NewLine>        self.pos = np.asarray([2, 10])<NewLine>        self.steps = 0<NewLine>        return self.pos<NewLine><NewLine><NewLine>parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')<NewLine>parser.add_argument('--gamma', type=float, default=0.99, metavar='G',<NewLine>                    help='discount factor (default: 0.99)')<NewLine>parser.add_argument('--seed', type=int, default=543, metavar='N',<NewLine>                    help='random seed (default: 543)')<NewLine>parser.add_argument('--render', action='store_true',<NewLine>                    help='render the environment')<NewLine>parser.add_argument('--log-interval', type=int, default=10, metavar='N',<NewLine>                    help='interval between training status logs (default: 10)')<NewLine>args = parser.parse_args()<NewLine><NewLine><NewLine>env = CliffWalking()<NewLine>torch.manual_seed(args.seed)<NewLine><NewLine><NewLine>class Policy(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Policy, self).__init__()<NewLine>        self.affine1 = nn.Linear(2, 48)<NewLine>        self.affine2 = nn.Linear(48, 4)<NewLine><NewLine>        self.saved_log_probs = []<NewLine>        self.rewards = []<NewLine><NewLine>    def forward(self, x):<NewLine>        x = F.relu(self.affine1(x))<NewLine>        action_scores = self.affine2(x)<NewLine>        return F.softmax(action_scores, dim=1)<NewLine><NewLine><NewLine>policy = Policy()<NewLine>optimizer = optim.Adam(policy.parameters(), lr=1e-2)<NewLine>eps = np.finfo(np.float32).eps.item()<NewLine><NewLine><NewLine>def select_action(state):<NewLine>    state = torch.from_numpy(state).float().unsqueeze(0)<NewLine>    probs = policy(state)<NewLine>    m = Categorical(probs)<NewLine>    action = m.sample()<NewLine>    policy.saved_log_probs.append(m.log_prob(action))<NewLine>    return action.item()<NewLine><NewLine><NewLine>def finish_episode():<NewLine>    R = 0<NewLine>    policy_loss = []<NewLine>    rewards = []<NewLine>    for r in policy.rewards[::-1]:<NewLine>        R = r + args.gamma * R<NewLine>        rewards.insert(0, R)<NewLine>    rewards = torch.tensor(rewards)<NewLine>    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)<NewLine>    for log_prob, reward in zip(policy.saved_log_probs, rewards):<NewLine>        policy_loss.append(-log_prob * reward)<NewLine>    optimizer.zero_grad()<NewLine>    policy_loss = torch.cat(policy_loss).sum()<NewLine>    policy_loss.backward()<NewLine>    optimizer.step()<NewLine>    del policy.rewards[:]<NewLine>    del policy.saved_log_probs[:]<NewLine><NewLine><NewLine>def main():<NewLine>    for i_episode in count(1):<NewLine>        state = env.reset()<NewLine>        done = False<NewLine>        while not done:  # Don't infinite loop while learning<NewLine>            action = select_action(state)<NewLine>            state, reward, done = env.take_action(action)<NewLine><NewLine>            policy.rewards.append(reward)<NewLine><NewLine>        finish_episode()<NewLine><NewLine>        if i_episode % 10 == 0 and (not i_episode == 0):<NewLine>            scores = []<NewLine>            for i in range(100):<NewLine>                terminate = False<NewLine>                state = env.reset()<NewLine>                score = 0<NewLine>                while not terminate:<NewLine>                    action = select_action(state)<NewLine>                    next_state, reward, terminate = env.take_action(action)<NewLine>                    score += reward<NewLine>                scores.append(score)<NewLine>            print('[test score]episode %d: %.2f' % (i_episode, np.mean(np.asarray(scores))))<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    main()<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/ShawnGuo,,ShawnGuo,"January 30, 2019,  2:51pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s an interesting problem. Of course, the algorithm REINFORCE  as implemented in the official example does work on the cliff game. But your code will never learn anything for two reasons:</p><NewLine><ol><NewLine><li><NewLine><p>Your implementation of the cliff is not 100% exactly the game from the book (and quite a complex implementation for a simple grid-world).</p><NewLine></li><NewLine><li><NewLine><p>The model (not the algorithm) you are using is not adapted to the type of inputs.</p><NewLine></li><NewLine></ol><NewLine><p>First, in the cliff game, the only terminal state is the goal. If you reach the cliff, you have a transition that leads you back to the starting point with reward -100. This detail is important, because if you break episodes at cliffs, you may never explore enough and quickly learn to stay around the start point. Here is a simple way to implement the true cliff game as a gym environment:</p><NewLine><pre><code class=""lang-python"">class CliffWalking(object):<NewLine>    def __init__(self):<NewLine>        self.R = -np.ones((4, 12))<NewLine>        self.R[-1, -1] = 0<NewLine>        self.R[-1, 1:-1] = -100<NewLine>        print(self.R)<NewLine><NewLine>    def reset(self):<NewLine>        self.x = 3<NewLine>        self.y = 0<NewLine>        obs = self.x + 4*self.y<NewLine>        return obs<NewLine><NewLine>    def step(self, action):<NewLine>        if action==0 and self.x&lt;3:<NewLine>            self.x += 1<NewLine>        if action==1 and self.x&gt;0:<NewLine>            self.x -= 1<NewLine>        if action==2 and self.y&lt;11:<NewLine>            self.y += 1<NewLine>        if action==3 and self.y&gt;0:<NewLine>            self.y -= 1<NewLine><NewLine>        # get reward:<NewLine>        reward = self.R[self.x, self.y]<NewLine><NewLine>        done = False<NewLine>        # if goal, done= True:<NewLine>        if self.x==3 and self.y==11:<NewLine>            done = True<NewLine><NewLine>        # reset if goal or cliff:<NewLine>        if self.x==3:<NewLine>            self.y = 0<NewLine><NewLine>        obs = self.x + 4*self.y<NewLine>        info = (action, self.x, self.y, reward)<NewLine>        return obs, reward, done, info<NewLine></code></pre><NewLine><p>Now, the inputs (states) are a position in the grid (in your code, it’s x-y coordinate, in my it’s simply x + 4y, the coordinate in a flat version of the grid). For a neural network, such integer inputs are impossible to handle. A neural network expects floating values, for ex between -1 and 1 or between 0 and 1. One solution would be to use an embedding: for ex,  take a vector of length 12*4 with zero everywhere but a one at your position in the grid. But that’s very complicated for nothing and very hard to learn because of the number of parameters. A much simpler solution is to use a tabular model instead of a neural network:</p><NewLine><pre><code class=""lang-python"">class Policy(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Policy, self).__init__()<NewLine>        self.Q = nn.Parameter(torch.zeros(1, 4*12, 4))<NewLine>        self.saved_log_probs = []<NewLine>        self.rewards = []<NewLine><NewLine>    def forward(self, x):<NewLine>        action_scores = self.Q[:,x,:]<NewLine>        return F.softmax(action_scores, dim=1)<NewLine></code></pre><NewLine><p>Then, you can chose long-enough episodes (1000 steps) so you ensure that your algorithm will reach the goal by chance at least once:</p><NewLine><pre><code class=""lang-python"">def main():<NewLine>    for i_episode in range(500):<NewLine>        state = env.reset()<NewLine>        sum_reward = 0<NewLine>        for t in range(1000):<NewLine>            action = select_action(state, i_episode)<NewLine>            state, reward, done, info = env.step(action)<NewLine>            sum_reward += reward<NewLine>            if args.render:<NewLine>                env.render()<NewLine>            policy.rewards.append(reward)<NewLine>            if done:<NewLine>                break<NewLine><NewLine>        running_reward = sum_reward<NewLine>        finish_episode()<NewLine>        if i_episode % args.log_interval == 0:<NewLine>            print('Episode {}\tSum rewards: {:.2f}'.format(<NewLine>                i_episode, running_reward))<NewLine></code></pre><NewLine><p>Now it should work quite well, and using good learning rate (0.5 with Adam descent) it learns in ~ 100 episodes a near-optimal solution using 16 action-steps (the opimal is 13, but is “unsafe”, as described in the book).</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""35992""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/alexis-jacq/40/180_2.png"" width=""20""/> alexis-jacq:</div><NewLine><blockquote><NewLine><p>action_scores</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thanks so much! I’d like to explain why I change the game rule. It is because that one episode may take too long to terminate. As I want to avoid too long episode, I added another another rule. But, you’re right, the original version doesn’t terminate an episode after walking down the cliff.</p><NewLine><p>Your suggestion on one-hot representation of state is really really helpful! Thanks so much for this, I ignored this detail.</p><NewLine><p>If you don’t mind, I would like to discuss the code you gave. I implement the tabular-based TD method in my repo <a href=""https://github.com/Air-Fighter/ReinforcementLearningBookExamples/blob/master/6CliffWalk_Ch6.py"" rel=""nofollow noopener"">here</a>. Both of Q-learning and Sarsa work fine. The only reason I use NN is want to implement simple policy gradient methods first and then move on to deep RL.</p><NewLine><p>Thanks so much for your help!!!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just knew that “native replacement of linear model with neural networks would suffer from high correlation between consecutive experiences”. Although I didn’t fully understand this problem, it seems that it is what caused my problem.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ShawnGuo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ShawnGuo; <NewLine> ,"REPLY_DATE 1: January 31, 2019,  2:33pm; <NewLine> REPLY_DATE 2: February 1, 2019, 11:37am; <NewLine> REPLY_DATE 3: March 3, 2019,  1:13pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
38306,"Examples for asynchronous RL (IMPALA, Ape-X) with actors sending observations (not gradients) to a learner&rsquo;s replay buffer",2019-02-26T15:22:11.057Z,0,736,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Some of the most recently successful distributed/parallel approaches in RL, like IMPALA and Ape-X, start a pool of multiple actors that collect experience/observations and share/send these to a centralized learner (or distributed learners). Optimized Tensorflow implementations are available<br/><NewLine>(IMPALA from DeepMind and Ape-X from Uber research; can provide links if needed, but that’s TF, so maybe not relevant enough).</p><NewLine><p>However, I can’t seem to find a set of examples that would suggest how to implement this reasonably efficiently in pytorch. Specifically, one common setting is having a <em>single</em> machine with multiple CPU cores (e.g. 32-64) and a few GPUs (1-8). For this case, it seems that python’s multiprocessing libraries and torch.multiprocessing could be useful. However, I can’t find in-depth documentation or examples. Python docs provide minimal information, pytorch docs mention ‘best practices’ but do not detail how to implement them. For example, “buffer reuse” is suggested for torch.multiprocessing.Queue, but it is not clear exactly how to do this (<a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/torch-multiprocessing-how-to-reuse-buffers-passed-through-a-queue/16778"">Torch.multiprocessing: how to ""reuse buffers passed through a Queue""?</a>). Moreover, for most inter-process communication pickling is involved, but it is not clear whether it is efficient. Is there any way to avoid pickling, or maybe it’s ok as-is? An alternative is something like shared memory arrays in python (multiprocessing.Array), but most documentation warns against using these.</p><NewLine><p>Torch advertises torch.distributed in favor of torch.multiprocesssing. But most examples show to how to gather/share gradient information, or pre-load large static dataset, and stress inter-process communication in gather/map-reduce scenarios, focusing on multiple machines.</p><NewLine><p>It seems the above RL setup on a single (large) machine needs something slightly more custom. Are there open-source (pytorch) code examples for the following use case: actors gather observations using CPU-bound computation, but also use forward passes on (large) NNs to compute actions; these large NNs are updated (asynchronously) by the learner (e.g. using one GPU).</p><NewLine><p>I have a basic implementation of this (for a single machine), but it could be much improved with a proper pytorch advice. I use model.share_memory() for cuda tensors and pass the references for these to actors; I maintain a CPU-based replay buffer that is populated by a CPU worker interfacing with the queue.</p><NewLine><p>Here is a question with some of the similar concerns as I have, I but can’t see the resulting implementation, so can’t use this as an actual example: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/multiprocessing-cuda-memory/18951/10"">Multiprocessing CUDA memory</a></p><NewLine></div>",https://discuss.pytorch.org/u/cntctrk,,cntctrk,"March 3, 2019, 12:05pm",3 Likes,,,,
38407,How can I implement an environment run purely on GPU?,2019-02-27T14:02:28.295Z,10,297,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was wondering about how can I implement an environment purely on GPU. Say, if all the variables in the environment class are torch.Tensor, well they stay on GPU during run-time?</p><NewLine><p>Take the following environment for example:</p><NewLine><pre><code class=""lang-auto"">class ENV_GPU(object):<NewLine>    def __init__(self, a=3):<NewLine>        self.num = torch.zeros((a,), dtype=torch.int8)<NewLine>    def step(action):<NewLine>        self.num[action] += 1<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/ShawnGuo,,ShawnGuo,"February 27, 2019,  2:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-python"">ENV_GPU().to(torch.device('cuda')<NewLine></code></pre><NewLine><p>It will move your ENV_GPU object to GPU.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your answering. However, as there is no such method called “to” in ENV_GPU, an AttributeError raised.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>.to()</code> is implemented in <code>nn.Module</code>.<br/><NewLine>If you derive your class from <code>nn.Module</code> and define <code>step</code> as <code>forward</code> it should work:</p><NewLine><pre><code class=""lang-python"">class ENV_GPU(nn.Module):<NewLine>    def __init__(self, a=3):<NewLine>        super(ENV_GPU, self).__init__()<NewLine>        num = torch.zeros((a,), dtype=torch.int8)<NewLine>        self.register_buffer('num', num)        <NewLine>        <NewLine>    def forward(self, action):<NewLine>        self.num[action] += 1<NewLine><NewLine>model = ENV_GPU()<NewLine>model.to('cuda')<NewLine>model(0)<NewLine>print(model.num)<NewLine>&gt; tensor([1, 0, 0], device='cuda:0', dtype=torch.int8)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your answering. May I ask what is ""self.register_buffer(‘num’, num) "" for?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>The attributes of a module will be moved to the device, if they are registered as buffers (i.e. they don’t need gradients) or as an <code>nn.Parameter</code> (i.e. they should be updated).<br/><NewLine>Since you defined <code>num</code> as <code>torch.int8</code>, an <code>nn.Parameter</code> won’t work, as only floating point tensors can require gradients.<br/><NewLine>If you just register <code>num</code> as <code>self.num = torch.zeros((a,), dtype=torch.int8)</code> it won’t be moved to the device.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>OK, I see. Thanks very much for your help.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>By the way, what if I need to calculate some intermediate results during every step? How can I make sure that there is no date moved between CPU and GPU? Take the following for example,</p><NewLine><pre><code class=""lang-auto"">    def forward(self, action):<NewLine>        self.knapsack_num += self.action2shift[action]<NewLine>        self.knapsack_num = torch.clamp(self.knapsack_num, max=self.knapsack_max)<NewLine>        reward = torch.zeros((1,), dtype=torch.float)<NewLine>        if action == self.num_food_types:<NewLine>            if torch.equal(self.expected_num, self.knapsack_num + self.warehouse_num):<NewLine>                reward = 100 * torch.ones((1,), dtype=torch.float)<NewLine>            else:<NewLine>                reward = -100 * torch.ones((1,), dtype=torch.float)<NewLine>            return self.knapsack_num, reward, True<NewLine>        else:<NewLine>            return self.knapsack_num, reward, False<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you would like to create new tensors inside <code>forward</code>, you should pass the device using the device of an already registered tensor:</p><NewLine><pre><code class=""lang-python"">...<NewLine>reward = torch.zeros((1,), dtype=torch.float, device=self.num.device)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Wow, I see, this is really convenient. Is the “buffers” like an “no update required” “parameters” in a nn.Module?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah, basically just tensors registered with the module, so that they will be moved to the host or device and saved in the <code>state_dict</code> in case you would like to serialize your model.<br/><NewLine>The running estimates of <code>nn.BatchNorm</code> layers are a good example. While they don’t need gradients to be updated, they should still be moved with the layer and saved to disc.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Cool! Thanks very much for your help.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Kushaj; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ShawnGuo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ShawnGuo; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ShawnGuo; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ShawnGuo; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/ShawnGuo; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/ShawnGuo; <NewLine> ,"REPLY_DATE 1: February 27, 2019,  2:13pm; <NewLine> REPLY_DATE 2: March 2, 2019,  1:20pm; <NewLine> REPLY_DATE 3: March 2, 2019,  1:55pm; <NewLine> REPLY_DATE 4: March 2, 2019,  1:48pm; <NewLine> REPLY_DATE 5: March 2, 2019,  1:52pm; <NewLine> REPLY_DATE 6: March 2, 2019,  1:54pm; <NewLine> REPLY_DATE 7: March 2, 2019,  1:58pm; <NewLine> REPLY_DATE 8: March 2, 2019,  2:01pm; <NewLine> REPLY_DATE 9: March 2, 2019,  2:08pm; <NewLine> REPLY_DATE 10: March 2, 2019,  2:10pm; <NewLine> REPLY_DATE 11: March 2, 2019,  2:13pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 2 Likes; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> REPLY 11 LIKES: ; <NewLine> 
20494,What is the justification for normalizing each episode&rsquo;s reward targets in the policy gradient examples?,2018-06-29T20:40:27.346Z,0,674,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I’m confused about <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py#L75"" rel=""nofollow noopener"">this line</a> in the actor_critic.py example. It also appears in Andrej Karpathy’s Pong w/Pixels code. Is there any good justification to normalize the reward targets on a per episode basis? My understanding is that normalizing reward values should be done over batches or over all previous episodic rewards encountered during training, in order to keep the RL task stationary. I haven’t been able to find anything regarding this per episode normalization in the literature.</p><NewLine></div>",https://discuss.pytorch.org/u/kphng,,kphng,"June 29, 2018,  8:40pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have the same question.</p><NewLine><p>Intuitively, if we normalize over individual episodes, then episodes with a high overall reward will be normalized into the same range as episodes with very poor rewards.</p><NewLine><p>For example, consider we take actions at states that give us a near constant -50 reward for all time steps, and another other where we receive +50. Then, all actions in both episodes will be reinforced to the same degree after we z-score them. This will still properly reinforce on a per action basis within episodes, but effectively reduce all episodes to having the same amount of positive and negative reinforcement. I can only see this per action normalization working in a true MDP environment.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/gmaggiol; <NewLine> ,"REPLY_DATE 1: March 2, 2019,  5:57am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
33944,Diagnosing slow backward pass with RL gradient over minibatch,2019-01-06T14:48:38.845Z,1,449,"<div class=""post"" itemprop=""articleBody""><NewLine><p>For an RL task, I want to do parameter updates over minibatches of e.g. 100 or 500 episodes. All tensor operations in my code are unbatched (i.e. batch dimension, if any, is 1). This is because the RL controller is actually composed of multiple separate models, making it hard to program the forward pass as one chain of tensor operations.</p><NewLine><p>In pseudocode, per minibatch, I do:</p><NewLine><pre><code class=""lang-auto"">loss_terms = []<NewLine><NewLine>for i in range(100):<NewLine>     result_state = play_episode()<NewLine>     loss = mse_loss(result_state, target_state)<NewLine>     loss_terms.append(reward)<NewLine><NewLine>mean_loss = torch.mean(torch.stack(loss_terms))<NewLine>optimizer.zero_grad()<NewLine>mean_loss.backward()<NewLine>optimizer.step()<NewLine></code></pre><NewLine><p>The backwards pass is very slow, and in other topics I read that stack is a slow operation - but indexing is allegedly slow too, so I’m not sure pre-allocating a minibatch-sized tensor and index-filling it would help much. Is the computational graph simply too large and should I evaluate the gradient per episode instead? What’s the best practice on this?</p><NewLine><p>Here’s the profiler output for 3 iterations of minibatch size 100. Everything is on CPU. (Also - cProfile worked fine, but autograd profiler completely flooded my RAM, to the extend that profiling with minibatch size 500 crashed. Does that make sense?)</p><NewLine><pre><code class=""lang-auto"">--------------------------------------------------------------------------------<NewLine>  cProfile output<NewLine>--------------------------------------------------------------------------------<NewLine>         23976946 function calls (22476653 primitive calls) in 80.041 seconds<NewLine><NewLine>   Ordered by: internal time<NewLine>   List reduced from 2332 to 15 due to restriction &lt;15&gt;<NewLine><NewLine>   ncalls  tottime  percall  cumtime  percall filename:lineno(function)<NewLine>        6   26.195    4.366   26.195    4.366 {method 'run_backward' of 'torch._C._EngineBase' objects}<NewLine>   744975   12.643    0.000   12.643    0.000 {method 'matmul' of 'torch._C._TensorBase' objects}<NewLine>   750393    3.830    0.000   19.936    0.000 \torch\nn\functional.py:1010(linear)<NewLine>1541421/48762    3.787    0.000   45.183    0.001 \torch\nn\modules\module.py:471(__call__)<NewLine>   568890    3.180    0.000    3.180    0.000 {built-in method torch._C._nn.threshold}<NewLine>   750393    3.140    0.000    3.140    0.000 {method 't' of 'torch._C._TensorBase' objects}<NewLine>   455112    2.636    0.000    8.157    0.000 \ibp-pytorch\utilities.py:5(tensor_from)<NewLine>   455112    1.974    0.000    1.974    0.000 {built-in method cat}<NewLine>   744975    1.821    0.000   22.271    0.000 \torch\nn\modules\linear.py:54(forward)<NewLine>    43344    1.732    0.000   36.132    0.001 \ibp-pytorch\imaginator.py:71(&lt;listcomp&gt;)<NewLine>   176085    1.541    0.000   31.485    0.000 \torch\nn\modules\container.py:89(forward)<NewLine>      301    1.340    0.004   27.767    0.092 \ibp-pytorch\imagination_based_planner.py:153(new_episode)<NewLine>    43344    1.056    0.000   43.402    0.001 \ibp-pytorch\imaginator.py:70(forward)<NewLine>   308826    1.047    0.000    1.047    0.000 {built-in method tensor}<NewLine>   558054    0.968    0.000    0.968    0.000 {method 'float' of 'torch._C._TensorBase' objects}<NewLine><NewLine><NewLine>--------------------------------------------------------------------------------<NewLine>  autograd profiler output (CPU mode)<NewLine>--------------------------------------------------------------------------------<NewLine>        top 15 events sorted by cpu_time_total<NewLine><NewLine>---------------------  ---------------  ---------------  ---------------  ---------------  ---------------<NewLine>Name                          CPU time        CUDA time            Calls        CPU total       CUDA total<NewLine>---------------------  ---------------  ---------------  ---------------  ---------------  ---------------<NewLine>matmul                     10614.158us          0.000us                1      10614.158us          0.000us<NewLine>mm                         10587.491us          0.000us                1      10587.491us          0.000us<NewLine>_mm                        10585.851us          0.000us                1      10585.851us          0.000us<NewLine>add_                        9396.106us          0.000us                1       9396.106us          0.000us<NewLine>matmul                      7469.952us          0.000us                1       7469.952us          0.000us<NewLine>mm                          7444.106us          0.000us                1       7444.106us          0.000us<NewLine>_mm                         7442.054us          0.000us                1       7442.054us          0.000us<NewLine>MmBackward                  7168.823us          0.000us                1       7168.823us          0.000us<NewLine>matmul                      7065.029us          0.000us                1       7065.029us          0.000us<NewLine>stack                       7030.157us          0.000us                1       7030.157us          0.000us<NewLine>threshold                   6931.695us          0.000us                1       6931.695us          0.000us<NewLine>threshold_forward           6924.311us          0.000us                1       6924.311us          0.000us<NewLine>stack                       6900.516us          0.000us                1       6900.516us          0.000us<NewLine>stack                       6820.106us          0.000us                1       6820.106us          0.000us<NewLine>mm                          6572.720us          0.000us                1       6572.720us          0.000us```</code></pre><NewLine></div>",https://discuss.pytorch.org/u/Driesssens,(Jasper Driessens),Driesssens,"January 6, 2019,  2:49pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>why don’t you directly sum the losses instead of stacking them into a tensor ?</li><NewLine></ol><NewLine><pre><code class=""lang-python"">loss_sum = 0<NewLine><NewLine>for i in range(100):<NewLine>     result_state = play_episode()<NewLine>     loss = mse_loss(result_state, target_state)<NewLine>     loss_sum += reward<NewLine><NewLine>optimizer.zero_grad()<NewLine>loss_sum.backward()<NewLine>optimizer.step()<NewLine></code></pre><NewLine><ol start=""2""><NewLine><li>we need more information about your code (maybe somewhere your computation graph could be alleviated for the backward pass)</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/driesssens"">@Driesssens</a> did you finally find the solution to your problem? I kind of have the same issue…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not really. I tried some other things like directly summing the losses (and keeping track of the amount of summed items to be able to take the average at the end) but none was clearly faster than the others. In the end it turned out that much smaller batches (20-50) sped up learning for my problem significantly, alleviating the problem.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/grishabhg; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Driesssens; <NewLine> ,"REPLY_DATE 1: January 6, 2019,  5:43pm; <NewLine> REPLY_DATE 2: February 21, 2019,  8:28pm; <NewLine> REPLY_DATE 3: February 26, 2019,  8:54am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
38223,Unrolling nn.LSTM,2019-02-25T19:44:44.868Z,5,795,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am currently implementing a DRQN network which works correctly, however I want to unroll the LSTM network for a specified amount of steps, how do I do this in pytorch? Could someone provide some insight?</p><NewLine><p>I have followed this tutorial, but there is no mention of unrolling:<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html</a></p><NewLine><blockquote><NewLine><pre><code>class DRQNBody(nn.Module):<NewLine>def __init__(self, in_channels=4):<NewLine>    super(DRQNBody, self).__init__()<NewLine>    self.feature_dim = 512<NewLine>    self.rnn_input_dim = 7*7*64<NewLine>    self.batch_size = -1<NewLine>    self.conv1 = layer_init(nn.Conv2d(in_channels, 32, kernel_size=8, stride=4))<NewLine>    self.conv2 = layer_init(nn.Conv2d(32, 64, kernel_size=4, stride=2))<NewLine>    self.conv3 = layer_init(nn.Conv2d(64, 64, kernel_size=3, stride=1))<NewLine>    self.lstm = nn.LSTM(self.rnn_input_dim, self.feature_dim , num_layers = 4)<NewLine>    self.hidden = self.init_hidden()<NewLine><NewLine>def forward(self, x):<NewLine>    y = F.relu(self.conv1(x))<NewLine>    y = F.relu(self.conv2(y))<NewLine>    y = F.relu(self.conv3(y))<NewLine>    y = y.view(y.size(0), -1) # flattening<NewLine>    y = y.view(1, self.batch_size, self.rnn_input_dim)   # Adding dimention <NewLine>    output, self.hidden = self.lstm(y, self.hidden)<NewLine>    y = output<NewLine>    y = torch.squeeze(y,1)<NewLine>    return y</code></pre><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/Marjak,,Marjak,"February 25, 2019,  8:57pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Use a for loop</p><NewLine><pre><code class=""lang-auto"">for xt in 10:<NewLine>            output, (hn,cn) = self.lstm(xt[:,None,:], (hn,cn))<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks for the reply, this solves my problem although I dont understand the meaning behind the indexing in xt[:,None,:]</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Was having fun, it would depend on how you loaded the data.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah okay, now I am unrolling in the forward pass now, how do I do it in the backward pass? And are both needed for DRQN?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>PyTorch handles the backprop for you. Your computation graph is build dynamically and as a result in the graph the LSTM is repeated 10 times and you would automatically backprop through all the 10 layers. Sorry man, I don’t have much knowledge about reinforcement learning.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I get the feeling this doesnt unroll it alone, I think I  need to somehow add the output as an input too?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Kushaj; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Marjak; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Kushaj; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Marjak; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Kushaj; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Marjak; <NewLine> ,"REPLY_DATE 1: February 25, 2019,  9:05pm; <NewLine> REPLY_DATE 2: February 25, 2019,  8:55pm; <NewLine> REPLY_DATE 3: February 25, 2019,  8:57pm; <NewLine> REPLY_DATE 4: February 25, 2019,  9:01pm; <NewLine> REPLY_DATE 5: February 25, 2019,  9:04pm; <NewLine> REPLY_DATE 6: February 26, 2019, 12:21am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> 
37482,Reinforcement_q_learning.py,2019-02-18T02:52:03.386Z,3,255,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I can run the Jupyter notebook version of this without any issues but the <code>reinforcement_learing.py</code> version gives me an error:</p><NewLine><p>Found GPU0 NVS 4200M which is of cuda capability 2.1.<br/><NewLine>PyTorch no longer supports this GPU because it is too old.</p><NewLine><p>warnings.warn(old_gpu_warn % (d, name, major, capability[1]))<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “/media/nobu/Ubuntu Backup/Comp Sci/_DeepLearning/pytorch/reinforcement_q_learning.py”, line 460, in <br/><NewLine>state = current_screen - last_screen<br/><NewLine>RuntimeError: CUDA error: no kernel image is available for execution on the device</p><NewLine><p>Process finished with exit code 1</p><NewLine></div>",https://discuss.pytorch.org/u/Nobutaka_Kim,(Nobutaka Kim),Nobutaka_Kim,"February 18, 2019,  2:52am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Your GPU is quite old by now and the PyTorch binaries do not ship CUDA with the necessary compute capability. If you really want to use this GPU, you could try to build from source as described <a href=""https://github.com/pytorch/pytorch#from-source"" rel=""nofollow noopener"">here</a>. However, as far as I can tell your GPU should only have ~1GB of memory besides a probably low performance.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>So how do I just edit the code so that it uses CPU?</p><NewLine><p>Thanks,</p><NewLine><p>Nobu</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>At the beginning of the code just set <code>device = 'cpu'</code>.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks bro!  I’m sorry for the lazy nature of my question but you pytorch guys seem so much cooler than the tensorflow ones:)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Nobutaka_Kim; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Nobutaka_Kim; <NewLine> ,"REPLY_DATE 1: February 18, 2019, 11:52pm; <NewLine> REPLY_DATE 2: February 19, 2019,  1:34am; <NewLine> REPLY_DATE 3: February 19, 2019,  6:07am; <NewLine> REPLY_DATE 4: February 19, 2019,  8:00am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
37562,Multiprocessing with cuda model,2019-02-18T16:14:27.262Z,0,375,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am trying to test out my own Alpha Zero implementation (I know hardware is gonna limit me hard) and for the self play part I would like to use all of my CPU cores to accelerate the logic. I am moving the whole model to CPU before entering the multiprocessing, however I still get CUDA errors about shared memory and I am quite unsure how to adress this.</p><NewLine><p>To control the device usage, I am setting a global torch device, that controls what is supposed to be used (automatically sends all relevant tensors.to(GLOBAL_DEVICE)).</p><NewLine><p>My code reads:</p><NewLine><p>Inside the NNetWrapper class:</p><NewLine><pre><code class=""lang-auto"">class NNetWrapper:<NewLine>    def __init__(self, nnet, game_dim, action_dim):<NewLine>        self.nnet = nnet<NewLine>        self.board_x, self.board_y = game_dim, game_dim<NewLine>        self.action_size = action_dim<NewLine><NewLine>    def to_device(self):<NewLine>        if GLOBAL_DEVICE.type == 'cpu':<NewLine>            self.nnet.cpu()<NewLine>        else:<NewLine>            self.nnet.cuda()<NewLine>        return<NewLine>....<NewLine></code></pre><NewLine><p>And in the coach class, that does the self play:</p><NewLine><pre><code class=""lang-auto"">                curr_device = deepcopy(GLOBAL_DEVICE)<NewLine>                GLOBAL_DEVICE = torch.device('cpu')<NewLine>                self.nnet.to_device()<NewLine><NewLine>                if multiprocess:<NewLine>                    pbar = tqdm(total=self.num_episodes)<NewLine>                    pbar.set_description('Creating self-play training turns')<NewLine>                    with ProcessPoolExecutor(max_workers=cpu_count()) as executor:<NewLine>                        game_states = []<NewLine>                        for _ in range(self.num_episodes):<NewLine>                            self.game.reset()<NewLine>                            game_states.append(deepcopy(self.game.state))<NewLine>                        futures = list(<NewLine>                            (executor.submit(self.exec_ep,<NewLine>                                             mcts=MCTS(deepcopy(self.nnet), num_mcts_sims=self.mcts_sims),<NewLine>                                             reset_game=True,<NewLine>                                             state=game_states[i])<NewLine>                             for i in range(self.num_episodes)))<NewLine>                        for _ in as_completed(futures):<NewLine>                            pbar.update(1)<NewLine></code></pre><NewLine><p>However I get the following errors:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""C:\ProgramData\Miniconda3\lib\multiprocessing\queues.py"", line 234, in _feed<NewLine>    obj = _ForkingPickler.dumps(obj)<NewLine>  File ""C:\ProgramData\Miniconda3\lib\multiprocessing\reduction.py"", line 51, in dumps<NewLine>    cls(buf, protocol).dump(obj)<NewLine>  File ""C:\ProgramData\Miniconda3\lib\site-packages\torch\multiprocessing\reductions.py"", line 213, in reduce_tensor<NewLine>    (device, handle, storage_size_bytes, storage_offset_bytes) = storage._share_cuda_()<NewLine>RuntimeError: cuda runtime error (71) : operation not supported at c:\a\w\1\s\tmp_conda_3.6_173528\conda\conda-bld\pytorch_1549561085620\work\torch\csrc\generic\StorageSharing.cpp:232<NewLine></code></pre><NewLine><p>Why am I seeing CUDA errors when I send the model previously to CPU? Am I not doing enough for this? On a computer without CUDA this code runs just fine.</p><NewLine><p>PS: I know I am supposed to use the torch.multiprocessing package, however I am rather new to multiprocessing and cant quite figure out an exact way to replicate the PoolProcessExecutor pipe. I’d be happy about tips to that end as well.</p><NewLine><p>Thanks in advance,<br/><NewLine>Michael</p><NewLine></div>",https://discuss.pytorch.org/u/Mikele,(Michael),Mikele,"February 18, 2019,  4:16pm",,,,,
14199,Copying part of the weights,2018-03-01T12:12:01.687Z,2,5766,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to copy a part of the weight from one network to another.<br/><NewLine>Using something like polyak averaging</p><NewLine><p>Example:</p><NewLine><p>weights_new = k*weights_old + (1-k)*weights_new</p><NewLine><p>This is required to implement DDPG.</p><NewLine><p>How can I do this?</p><NewLine></div>",https://discuss.pytorch.org/u/Navneet_M_Kumar,(Navneet M Kumar),Navneet_M_Kumar,"March 1, 2018, 12:12pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Something like this should do</p><NewLine><pre><code class=""lang-auto""># per layer and per weight param<NewLine>other_model.layer.weight.data = k * model.layer.weight.data + (1-k) * other_model.layer.weight.data</code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>your solution is missing a for loop, no? How do you actually do this with a for loop?</p><NewLine><p>Error message:</p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; net<NewLine>Sequential(<NewLine>  (0): Linear(in_features=2, out_features=2)<NewLine>  (1): Linear(in_features=2, out_features=2)<NewLine>)<NewLine>&gt;&gt;&gt; net.layer<NewLine>Traceback (most recent call last):<NewLine>  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;<NewLine>  File ""/Users/brandomiranda/miniconda3/envs/pytorch_overparam/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 366, in __getattr__<NewLine>    type(self).__name__, name))<NewLine>AttributeError: 'Sequential' object has no attribute 'layer'<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>real solution:</p><NewLine><aside class=""onebox stackexchange""><NewLine><header class=""source""><NewLine><a href=""https://stackoverflow.com/questions/48560227/take-the-average-of-the-weights-of-two-networks-in-pytorch"" rel=""nofollow noopener"" target=""_blank"">stackoverflow.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://stackoverflow.com/users/3990607/patapouf-ai"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""patapouf_ai"" class=""thumbnail onebox-avatar"" height=""60"" src=""https://i.stack.imgur.com/si8ll.png?s=128&amp;g=1"" width=""60""/><NewLine></a><NewLine><h4><NewLine><a href=""https://stackoverflow.com/questions/48560227/take-the-average-of-the-weights-of-two-networks-in-pytorch"" rel=""nofollow noopener"" target=""_blank"">Take the average of the weights of two networks in PyTorch</a><NewLine></h4><NewLine><div class=""tags""><NewLine><strong>python, neural-network, deep-learning, weight</strong><NewLine></div><NewLine><div class=""date""><NewLine>  asked by<NewLine>  <NewLine>  <a href=""https://stackoverflow.com/users/3990607/patapouf-ai"" rel=""nofollow noopener"" target=""_blank""><NewLine>    patapouf_ai<NewLine>  </a><NewLine>  on <a href=""https://stackoverflow.com/questions/48560227/take-the-average-of-the-weights-of-two-networks-in-pytorch"" rel=""nofollow noopener"" target=""_blank"">10:15AM - 01 Feb 18</a><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><pre><code class=""lang-auto"">beta = 0.5 #The interpolation parameter    <NewLine>params1 = model1.named_parameters()<NewLine>params2 = model2.named_parameters()<NewLine><NewLine>dict_params2 = dict(params2)<NewLine><NewLine>for name1, param1 in params1:<NewLine>    if name1 in dict_params2:<NewLine>        dict_params2[name1].data.copy_(beta*param1.data + (1-beta)*dict_params2[name1].data)<NewLine><NewLine>model.load_state_dict(dict_params2)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>named_paramters() doesn’t works well with my code. I got some “missing keys” problem when “load_state_dict”.<br/><NewLine>state_dict() is the solution.</p><NewLine><p>beta = 0.5 <span class=""hashtag"">#The</span> interpolation parameter<br/><NewLine>params1 = model1.state_dict()<br/><NewLine>params2 = model2.state_dict()</p><NewLine><p>dict_params2 = dict(params2)</p><NewLine><p>for name1, param1 in params1:<br/><NewLine>if name1 in dict_params2:<br/><NewLine>dict_params2[name1].data.copy_(beta*param1.data + (1-beta)*dict_params2[name1].data)</p><NewLine><p>model.load_state_dict(dict_params2)</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Tried to apply this to load pretrained weights from resnet18 and it failed on loading batchnorm.running_mean</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jpeg729; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Brando_Miranda; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Brando_Miranda; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Lin_Li; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/wolterlw; <NewLine> ,"REPLY_DATE 1: March 1, 2018,  1:24pm; <NewLine> REPLY_DATE 2: March 31, 2018,  3:14am; <NewLine> REPLY_DATE 3: March 31, 2018,  3:16am; <NewLine> REPLY_DATE 4: October 8, 2018, 10:44am; <NewLine> REPLY_DATE 5: February 17, 2019,  3:20am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 3 Likes; <NewLine> REPLY 4 LIKES: 2 Likes; <NewLine> REPLY 5 LIKES: ; <NewLine> 
36910,Backprob policy,2019-02-11T15:44:30.559Z,0,123,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey Guys,<br/><NewLine>i have a dqn network. I want to backward my policy for a given input trough my network to get the state back.<br/><NewLine>I want to use this to get the most mayor indices in my input vector for a policy if i change the policy a bit.</p><NewLine></div>",https://discuss.pytorch.org/u/drudolph,(Daniel),drudolph,"February 11, 2019,  3:44pm",,,,,
36856,Manager.queue get() error,2019-02-11T01:27:34.985Z,0,163,"<div class=""post"" itemprop=""articleBody""><NewLine><p>RuntimeError: invalid device pointer: 0x204aa4200 at /opt/conda/conda-bld/pytorch_1544173631724/work/aten/src/THC/THCCachingAllocator.cpp:301</p><NewLine><p>When I use torch.multiprocessing.manager.queue to share some data generated from different processes to a background thread, then I get the error when the background thread try to use queue.get() data from the shared queue. The shared data is Cuda tensors. Any suggestions? Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Han_Zheng,(Han Zheng),Han_Zheng,"February 11, 2019,  1:27am",,,,,
36723,Can you help me adapt the actor-critic example for multi-gpu?,2019-02-08T19:19:28.030Z,1,264,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m struggling to adapt the actor-critic example for a single machine multi-gpu setup.</p><NewLine><p>Here’s a <a href=""https://gist.github.com/JohnAllen/d6117dc781cc61b35d179a28d04f6102#file-actor-critic-multi-gpu-py"" rel=""nofollow noopener"">gist</a> of what I’m working with (or all of the code that seems relevant; note that that won’t compile due to private reward and action classes).</p><NewLine><p>Current failure is a size mismatch on line 55: <code>RuntimeError: size mismatch, m1: [1 x 43], m2: [128 x 256]</code> which tells me it’s splitting something into 3 parts (128/3 = ~ 43).</p><NewLine><p>Any feedback would be great as I’m new to splitting things up on multi-GPUs and new to Pytorch in general.</p><NewLine></div>",https://discuss.pytorch.org/u/JohnAllen,(John Allen),JohnAllen,"February 8, 2019,  7:25pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you using 3 GPUs for <code>nn.DataParallel</code>?<br/><NewLine>Could you print the shape of <code>x</code> in your <code>forward</code> method at these lines of code:</p><NewLine><pre><code class=""lang-python"">elif self.num_cuda_devices &gt; 1:<NewLine>    print(x.size())<NewLine>    x = self.input_layer(x.float()).cuda()  <NewLine>    x = torch.sigmoid(x)<NewLine>    print(x.size())<NewLine>    x = self.hidden_1(x).cuda()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yup, 3 GPUs which are recognized by nvidia-smi and pytorch.</p><NewLine><p>When I add:</p><NewLine><pre><code class=""lang-auto"">device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")<NewLine>if torch.cuda.device_count() &gt; 1:<NewLine>    print(""data parallel"")<NewLine>    model = torch.nn.DataParallel(model,  device_ids=[0, 1, 2])<NewLine>    model.cuda()<NewLine>model.to(device)<NewLine></code></pre><NewLine><p>to my code but not the part in my model declaration (<code>self.hidden_1 = nn.DataParallel(self.hidden_1)</code>) my code runs fine but does not use all 3 GPUs, just one.</p><NewLine><p>The sizes on those lines are:</p><NewLine><p>torch.Size([65])<br/><NewLine>torch.Size([128])</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the debugging!<br/><NewLine>Since some methods are missing in your gist, you would have to help me out with some more debugging. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=6"" title="":wink:""/></p><NewLine><p>It looks like the batch dimension is missing for <code>x</code>, but I’m not sure why and how your model seems to be working even without <code>nn.DataParallel</code>.<br/><NewLine>Usually your input should have the shape <code>[batch_size, nb_features]</code> to be a valid input for a linear layer.<br/><NewLine>If you are using a single sample, it should be <code>[1, 65]</code> for your model.</p><NewLine><p>Since <code>nn.DataParallel</code> splits your data in <code>dim0</code>, you should provide a batch size being a multiple of the number of GPUs. In your setup you should provide a batch size of <code>3, 6, 9, ...</code> so that the data can be split among each GPU.</p><NewLine><p>Let me know, if you need some more help in debugging.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah, I think what’s going on is in RL, we look at the state at one timestep (size: [65]).  The batch size could be greater than 1, but we still pass a single time_step of state to the model. So the model doesn’t look at [32, 65], but [65].  But <code>nn.parallel</code> expects more shape[0] to be greater than 1, like you say it splits in <code>dim0</code>.</p><NewLine><p>Most examples I see have a <code>Variable</code> declared with something like <code>[batch_size, num_features]</code> but mine doesn’t so I’ll have to figure out whether or not I want to keep passing a single example (I suppose if I want multiple GPUs I do have to change it).</p><NewLine><p>Thanks for the feedback</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/JohnAllen; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/JohnAllen; <NewLine> ,"REPLY_DATE 1: February 9, 2019, 12:59pm; <NewLine> REPLY_DATE 2: February 9, 2019,  5:02pm; <NewLine> REPLY_DATE 3: February 9, 2019,  5:09pm; <NewLine> REPLY_DATE 4: February 9, 2019,  6:21pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
36360,Best practices for exploration/exploitation in Reinforcement Learning,2019-02-04T14:51:46.720Z,0,223,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My question follows my examination of the code in the PyTorch DQN tutorial, but then refers to Reinforcement Learning in general: what are the best practices for optimal exploration/exploitation in reinforcement learning?</p><NewLine><p>In the DQN tutorial, the steps_done variable is a global variable, and the EPS_DECAY = 200.  This means that:<br/><NewLine>after 128 steps, the epsilon threshold = 0.500<br/><NewLine>after 889 steps, the epsilon threshold = 0.0600<br/><NewLine>after 1500 steps, the epsilon threshold = 0.05047</p><NewLine><p>This might work for the CartPole problem featured in the tutorial  – where the early episodes might be very short and the task fairly simple – but what about on more complex problems in which far more exploration is required?  For example, if we had a problem with 40,000 episodes, each of which had 10,000 timesteps, how would we set up the epsilon greedy exploration policy?  Is there some rule of thumb that’s used in RL work?</p><NewLine><p>Thank you in advance for any help.</p><NewLine></div>",https://discuss.pytorch.org/u/hconroy,,hconroy,"February 4, 2019,  2:51pm",,,,,
35735,Expected 4-dimensional weight for 4-dimensional input,2019-01-27T22:35:31.932Z,2,8558,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to pass a state with 5 channels and 30 data points (time steps) through a 1d conv net with a dense network.  I think my problem is in reshaping my data because I keep getting expected or less then expected number of inputs as errors.  Anyone have any ideas what could be my problem? <a href=""https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf"" rel=""nofollow noopener"">Here is a blog on the kind of data I am working with</a> except I am using it in a reinforcement learning environment.</p><NewLine><p><strong>Here is dummy data with 5 channels</strong></p><NewLine><pre><code class=""lang-auto"">a = np.zeros(30) + 0<NewLine>b = np.zeros(30) + 1<NewLine>c = np.zeros(30) + 2<NewLine>d = np.zeros(30) + 3<NewLine>e = np.zeros(30) + 4<NewLine><NewLine>encoded_array = np.column_stack((a, b, c, d, e))<NewLine><NewLine>state = np.transpose(encoded_array)<NewLine><NewLine>state = state.reshape(1, 5, 30)<NewLine></code></pre><NewLine><p><strong>It has a shape of (1, 5, 30) and looks like this…</strong></p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/c344e84666b44c0477218c8cf7dc87a80e303d00"" href=""https://discuss.pytorch.org/uploads/default/original/2X/c/c344e84666b44c0477218c8cf7dc87a80e303d00.png"" title=""jupyter21.png""><img alt=""jupyter21"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/c/c344e84666b44c0477218c8cf7dc87a80e303d00_2_10x10.png"" height=""332"" src=""https://discuss.pytorch.org/uploads/default/original/2X/c/c344e84666b44c0477218c8cf7dc87a80e303d00.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">jupyter21.png</span><span class=""informations"">699×337 14.1 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p><strong>Here is my netowrk</strong></p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import numpy as np<NewLine><NewLine><NewLine>class QNetwork(nn.Module):<NewLine>    def __init__(self, state_size, action_size, seed, conv1=64, conv2=64, fc1_units=128, fc2_units=8):<NewLine>        super(QNetwork, self).__init__()<NewLine>        self.seed = torch.manual_seed(seed)<NewLine><NewLine>        self.conv1 = nn.Conv1d(in_channels=5, out_channels=conv1, kernel_size=2)<NewLine>        self.conv2 = nn.Conv1d(in_channels=conv1, out_channels=conv2, kernel_size=2)<NewLine>        self.mp = nn.MaxPool1d(2)<NewLine><NewLine>        self.fc1 = nn.Linear(1792, fc1_units)<NewLine>        self.fc2 = nn.Linear(fc1_units, fc2_units)<NewLine>        self.fc3 = nn.Linear(fc2_units, action_size)<NewLine><NewLine>    def forward(self, x):<NewLine><NewLine>        #x = torch.randn(1, 5, 30)<NewLine><NewLine>        in_size = x.size(0)<NewLine><NewLine>        x = F.relu(self.conv1(x))<NewLine>        x = F.relu(self.conv2(x))<NewLine><NewLine>        x = x.view(in_size, -1)<NewLine><NewLine>        x = F.relu(self.fc1(x))<NewLine>        x = F.relu(self.fc2(x))<NewLine><NewLine>        return self.fc3(x)<NewLine></code></pre><NewLine><p><strong>Errors I am getting</strong><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/0a18b4867bb31c16864bf023d26045cc5ca2fd79"" href=""https://discuss.pytorch.org/uploads/default/original/2X/0/0a18b4867bb31c16864bf023d26045cc5ca2fd79.png"" title=""errors.png""><img alt=""errors"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/0/0a18b4867bb31c16864bf023d26045cc5ca2fd79_2_10x10.png"" height=""267"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/0/0a18b4867bb31c16864bf023d26045cc5ca2fd79_2_690x267.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/0/0a18b4867bb31c16864bf023d26045cc5ca2fd79_2_690x267.png, https://discuss.pytorch.org/uploads/default/optimized/2X/0/0a18b4867bb31c16864bf023d26045cc5ca2fd79_2_1035x400.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/0/0a18b4867bb31c16864bf023d26045cc5ca2fd79.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">errors.png</span><span class=""informations"">1065×413 57.3 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/Bastulli,(Joe Bastulli),Bastulli,"January 28, 2019,  4:23pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think I found out why I am having issues. Somewhere along the line my tensor gets reshaped into something other then defined.</p><NewLine><p><img alt=""tensor_resize"" height=""338"" src=""https://discuss.pytorch.org/uploads/default/original/2X/a/a25ba168f53de1f7b5df00bb0f193481bfca7afd.png"" width=""582""/></p><NewLine><p>Once the size tensor size is reshaped somehow to (160, 30) it throws invalid shape. Getting closer to solving this</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>You are reshaping the tensor with the <code>x = x.view(in_size, -1)</code> line.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, I just realized this and now flatten my tensors with <code>x = x.view(x.size(0), -1)</code>  This way, it just gets the size of the tensor after the convs</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Bastulli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Alex_Hurt; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Bastulli; <NewLine> ,"REPLY_DATE 1: January 28, 2019,  4:23pm; <NewLine> REPLY_DATE 2: February 1, 2019, 12:01am; <NewLine> REPLY_DATE 3: February 1, 2019, 12:01am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
35248,Can I backpropagate different distributions at once using Policy Gradient?,2019-01-21T19:02:01.873Z,0,222,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, my issue is that I use a set of different batches to compute one (multinomial) distribution and one sampling per each batch, in a single epoch. How can I backpropagate those different distributions? I’m thinking on just creating a single Multinomial containing all the distribution vectors as a matrix and also get all the sampling vectors at once (as a matrix, again). Does it have sense?</p><NewLine><p>Thank you in advanced.</p><NewLine></div>",https://discuss.pytorch.org/u/Ignasi_Mas,(Ignasi Mas),Ignasi_Mas,"January 21, 2019,  7:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to understand: you want to learn a policy composed by N (supposedly) independent multinomial distributions.</p><NewLine><p>So, something like  <code>\pi(a_i = k | s) ~ p_i^k (1-p_i)^k</code> up to a normalization for i=1…N</p><NewLine><p>And you want to learn it as a single distribution: <code>\pi(A=[k1..kn] | s)</code> right?</p><NewLine><p>In that case, assuming all distributions are independent, you can sum the log_prob of all distributions and back propagate this sum (times a reward). If all parameters are given to the optimizer, it should work.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> ,"REPLY_DATE 1: January 31, 2019,  6:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
35804,Parallel online policy gradient on Module level with torch.multiprocessing,2019-01-28T16:42:06.240Z,0,259,"<div class=""post"" itemprop=""articleBody""><NewLine><p>What would be the right way to do the following, probably using torch.multiprocessing:</p><NewLine><p>Have one torch.nn.Module. Have it train in parallel on multiple CPU cores. Each thread or process computes policy gradients online on a couple of episodes in some RL environment. Gradients are summed or averaged and the optimizer update step is done synchronously.</p><NewLine><p>I could copy the module object to subprocesses and manually collect and combine the gradients, but I feel like there should be a clean and simple solution. I’m looking for something like in the <a href=""https://pytorch.org/tutorials/intermediate/dist_tuto.html"" rel=""nofollow noopener"">torch.distributed tutorial</a>, just on a single machine and without the use of a distributed communication framework.</p><NewLine></div>",https://discuss.pytorch.org/u/Driesssens,(Jasper Driessens),Driesssens,"January 28, 2019,  4:42pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Does <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html#hogwild"" rel=""nofollow noopener"">this example</a> using <code>model.share_memory()</code> helps?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> ,"REPLY_DATE 1: January 31, 2019,  3:00pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
35539,Bad inference performance on some CPUs,2019-01-24T21:32:33.697Z,0,698,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I measured some CPU prediction performance and I got a huge difference in prediction times that I don’t really understand.</p><NewLine><p>I am using this Residual Network with 12 hidden layers for prediction:<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/sirmammingtonham/alphastone/blob/master/alphabot/NNet.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/sirmammingtonham/alphastone/blob/master/alphabot/NNet.py"" rel=""nofollow noopener"" target=""_blank"">sirmammingtonham/alphastone/blob/master/alphabot/NNet.py</a></h4><NewLine><pre><code class=""lang-py"">import os<NewLine>import time<NewLine>import numpy as np<NewLine>import sys<NewLine>sys.path.append('../../')<NewLine>from utils import Bar, AverageMeter, dotdict<NewLine><NewLine>import torch<NewLine>import torch.optim as optim<NewLine>from torch.autograd import Variable<NewLine><NewLine>from alphanet import DQN as nnet<NewLine><NewLine>args = dotdict({<NewLine>    'lr': 0.001,<NewLine>    'dropout': 0.3,<NewLine>    'epochs': 10,<NewLine>    'batch_size': 64,<NewLine>    'cuda': True,<NewLine>    'num_channels': 512,<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/sirmammingtonham/alphastone/blob/master/alphabot/NNet.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/sirmammingtonham/alphastone/blob/master/alphabot/alphanet.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/sirmammingtonham/alphastone/blob/master/alphabot/alphanet.py"" rel=""nofollow noopener"" target=""_blank"">sirmammingtonham/alphastone/blob/master/alphabot/alphanet.py</a></h4><NewLine><pre><code class=""lang-py"">'''<NewLine>neural net output as follows:<NewLine>21x16 2d tensor = a<NewLine>    a[0-9,] encode for playing card in hand in position <NewLine>    a[10-16,] encode for attacking with minion at position<NewLine>    a[17,] encode for hero power<NewLine>    a[18,] encode for hero attack<NewLine>    a[19,] encode for end turn<NewLine>    a[20,] encode for card index when given choice<NewLine>    a[,0-15] encode for targeting available target at index (2 for heroes, 14 for board)<NewLine>'''<NewLine>import sys<NewLine>sys.path.append('..')<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>'''<NewLine>def get_emb(ni,nf):<NewLine>    e = nn.Embedding(ni, nf)<NewLine>    e.weight.data.uniform_(-0.01,0.01)<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/sirmammingtonham/alphastone/blob/master/alphabot/alphanet.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>With Pytorch 1.0 (precompiled, no builds from source) a single prediction takes on average<br/><NewLine>0.022s (no VM Windows 10) or 0.1s (Ubuntu 18.04 VM) on an Intel Core i7-4770K @ 4.2 Ghz and<br/><NewLine>0.038s (no VM Windows Server 2016 Datacenter) on an Intel Xeon X5680 @ 3.33Ghz but<br/><NewLine>6.85s on an Opteron 6136 (Ubuntu 18.04 on a VM).<br/><NewLine>I also got nearly that slow values on an Xeon X5355 (Ubuntu 18.04 on a VM).</p><NewLine><p>No I am trying to figure out what’s the reason for THAT bad performance in comparison to the Intel i7.<br/><NewLine>Is it because SSE4.1 or 4.2 is not supported?</p><NewLine></div>",https://discuss.pytorch.org/u/Dookie,(Dookie),Dookie,"January 25, 2019, 11:47pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>PyTorch uses MKL-DNN (<a href=""https://github.com/intel/mkl-dnn"" rel=""nofollow noopener"">https://github.com/intel/mkl-dnn</a>) for CPU convolutions. It’s <a href=""https://github.com/intel/mkl-dnn#system-requirements"" rel=""nofollow noopener"">optimized</a> for Haswell and newer architectures (circa 2013+). I’ve never tried it on much older processors.</p><NewLine><p>Two suggestions:</p><NewLine><ol><NewLine><li>Run your program under <code>perf top</code> or <code>perf record</code> to see where the time is spent</li><NewLine><li>Try adjusting <code>OMP_NUM_THREADS</code>. Try setting it to <code>1</code> or the number of unused cores (and values in between). Sometimes oversubscription (too many threads) can be a problem.</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/colesbury"">@colesbury</a>, thanks for your quick tips. They were very helpful. It makes sense to me hat some of those CPUs were too old for optimizations. And OMP_NUM_THREADS=1 allowed me to improve my programs performance significantly. Cheers!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/colesbury; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Dookie; <NewLine> ,"REPLY_DATE 1: January 24, 2019,  9:50pm; <NewLine> REPLY_DATE 2: January 25, 2019, 11:21pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
35448,Why is memory being allocated on GPU?,2019-01-24T02:57:59.913Z,1,243,"<div class=""post"" itemprop=""articleBody""><NewLine><p>pytorch 1.0<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/4a366bbb363ddf56a7418daad1c639ef0a470136"" href=""https://discuss.pytorch.org/uploads/default/original/2X/4/4a366bbb363ddf56a7418daad1c639ef0a470136.png"" title=""Screenshot from 2019-01-23 21-53-51.png""><img alt=""Screenshot%20from%202019-01-23%2021-53-51"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/4/4a366bbb363ddf56a7418daad1c639ef0a470136_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/4/4a366bbb363ddf56a7418daad1c639ef0a470136_2_495x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/4/4a366bbb363ddf56a7418daad1c639ef0a470136_2_495x500.png, https://discuss.pytorch.org/uploads/default/original/2X/4/4a366bbb363ddf56a7418daad1c639ef0a470136.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/4/4a366bbb363ddf56a7418daad1c639ef0a470136.png 2x"" width=""495""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screenshot from 2019-01-23 21-53-51.png</span><span class=""informations"">727×733 124 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine>relevant source code: <a href=""https://github.com/heronsystems/adeptRL/blob/master/adept/environments/managers/subproc_env_manager.py#L148-L193"" rel=""nofollow noopener"">https://github.com/heronsystems/adeptRL/blob/master/adept/environments/managers/subproc_env_manager.py#L148-L193</a></p><NewLine><p>even if I set os.environ[“CUDA_VISIBLE_DEVICES”] = “”, torch.cuda.is_available() returns False, but memory is still being allocated on GPU? maybe something to do with shared memory?</p><NewLine></div>",https://discuss.pytorch.org/u/Joe_Tatusko,(Joe Tatusko),Joe_Tatusko,"January 24, 2019,  2:57am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>hmm, maybe sc2 is not actually headless</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Joe_Tatusko; <NewLine> ,"REPLY_DATE 1: January 24, 2019,  4:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
33170,Multi agent deep reinforcement learning to an environment with discrete action space,2018-12-27T11:32:55.843Z,4,735,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I have been doing the udacity deep-reinforcement-learning nanodegree and I came out with a doubt. Do you know or have heard about any cutting edge deep reinforcement-learning algorithm which can be successfully applied for discrete action-spaces in multi-agent settings?</p><NewLine><p>I have been researching and I have found <a href=""https://arxiv.org/pdf/1706.02275.pdf"" rel=""nofollow noopener"">MADDPG</a> and <a href=""https://arxiv.org/pdf/1804.09817.pdf"" rel=""nofollow noopener"">Soft Q-learning</a> algorithms as the top ones in the state-of-the-art. I implemented the first one over an Unity environment and works well! However, they are mainly focused on environments with continuous action space. Although they can be applied to discrete action-space (e.g. MADDPG with gumbel softmax) it seems it is not what they are intended for (I have tried with MADDPG (w/ Gumbel softmax) achieving disastrous results…). In their corresponding papers they don’t give a lot of details of how to use them in these settings.</p><NewLine><p>Can somebody help me with this?</p><NewLine></div>",https://discuss.pytorch.org/u/ivallesp,(Iván Vallés),ivallesp,"December 27, 2018, 11:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""33170""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v2/letter/i/e56c9b/40.png"" width=""20""/> ivallesp:</div><NewLine><blockquote><NewLine><p>deep reinforcement-learning algorithm which can be successfully applied for discrete action-spaces in multi-agent settings</p><NewLine></blockquote><NewLine></aside><NewLine><p>there’s quite a bit if you do a regular google search. here’s a <a href=""https://www.google.com/search?ei=xr4kXOTfL6m2ggfjk4fICw&amp;q=deep+reinforcement-learning+algorithm+which+can+be+successfully+applied+for+discrete+action-spaces+in+multi-agent+settings&amp;oq=deep+reinforcement-learning+algorithm+which+can+be+successfully+applied+for+discrete+action-spaces+in+multi-agent+settings&amp;gs_l=psy-ab.3..0i71l8.176976.176976..177590...0.0..0.0.0.......0....1j2..gws-wiz.mXVbeICx6pQ"" rel=""nofollow noopener"">link</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Concerning the soft-Q learning approach, the adaptation to discret worlds looks simple:</p><NewLine><p>in the critic update, use<br/><NewLine>Q(a,s) = r(a,s) + sum_s’ ( T(s’|a,s) * V(s’) )<br/><NewLine>V(s) = log( sum_a exp( Q(a,s) / alpha )</p><NewLine><p>and directly compute the new policy<br/><NewLine>pi(a|s) = softmax( Q / alpha ) (a,s)<br/><NewLine>directly for all agents.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>Q learning was originally developed for markov decision processes with discrete action spaces.A fine example:<br/><NewLine><a href=""https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>But this is not multi-agent…</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>same here, this is not multi agent</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>The paper you mention about multi-agent soft-Q learning is a centralized approach, where each agent are sharing a common critic, with a joint policy (one network giving as output one action per agent). My answer focused on that case.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Beatrice_Paige; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pszabo; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ivallesp; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ivallesp; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> ,"REPLY_DATE 1: December 27, 2018, 12:05pm; <NewLine> REPLY_DATE 2: January 5, 2019,  5:52pm; <NewLine> REPLY_DATE 3: January 7, 2019, 12:13pm; <NewLine> REPLY_DATE 4: January 22, 2019, 12:57pm; <NewLine> REPLY_DATE 5: January 22, 2019, 12:58pm; <NewLine> REPLY_DATE 6: January 22, 2019,  5:18pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
35077,Torch.multiprocessing possible alternative to barrier,2019-01-19T09:48:11.184Z,0,226,"<div class=""post"" itemprop=""articleBody""><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/4a4fb33428e72c3cff8cda4aea731c6b980d0ff7"" href=""https://discuss.pytorch.org/uploads/default/original/2X/4/4a4fb33428e72c3cff8cda4aea731c6b980d0ff7.png"" title=""image.png""><img alt=""image"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/4/4a4fb33428e72c3cff8cda4aea731c6b980d0ff7_2_10x10.png"" height=""215"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/4/4a4fb33428e72c3cff8cda4aea731c6b980d0ff7_2_689x215.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/4/4a4fb33428e72c3cff8cda4aea731c6b980d0ff7_2_689x215.png, https://discuss.pytorch.org/uploads/default/optimized/2X/4/4a4fb33428e72c3cff8cda4aea731c6b980d0ff7_2_1033x322.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/4/4a4fb33428e72c3cff8cda4aea731c6b980d0ff7.png 2x"" width=""689""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">1181×369 67.7 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><pre><code class=""lang-auto"">import torch.multiprocessing as mp<NewLine>import torch<NewLine>import numpy as np<NewLine>import time<NewLine><NewLine>def read(q,b):<NewLine>    for i in range(1000):<NewLine>        start = time.time()<NewLine>        for j in range(100):<NewLine>            temp = q.get()<NewLine>            b.wait()<NewLine>        end = time.time()<NewLine>        print('read', str(end-start))<NewLine>    <NewLine>def write(q,b):<NewLine>    local = torch.ones(3000,3000)<NewLine>    # local = np.ones((3000,3000))<NewLine>    # local = [[1 for _ in range(3000)] for _ in range(3000)]<NewLine>    for i in range(1000):<NewLine>        start = time.time()<NewLine>        for j in range(100):<NewLine>            q.put(local)<NewLine>            b.wait()<NewLine>        end = time.time()<NewLine>        print('write', str(end-start))<NewLine><NewLine>def mp_handler(q, b):<NewLine>    p = mp.Process(target=read, args=(q,b))<NewLine>    p2 = mp.Process(target=write, args=(q,b))<NewLine><NewLine>    p.start()<NewLine>    p2.start()<NewLine>    p.join()<NewLine>    p2.join()<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    start = time.time()<NewLine>    q = mp.Queue()<NewLine>    bar = mp.Barrier(2)<NewLine><NewLine>    mp_handler(q, bar)<NewLine><NewLine>    end = time.time()<NewLine>    print('Finished', str(end - start))<NewLine></code></pre><NewLine><p>When I comment out those two b.wait() lines I get the error above. But the write process runs really fast before it crashes. Is there an alternative to barrier to speed things up? Am I using barrier properly in this case?</p><NewLine></div>",https://discuss.pytorch.org/u/DanielTellier,(Daniel Tellier),DanielTellier,"January 19, 2019,  9:49am",,,,,
16285,Unreasonable performances of a simple linear policy,2018-04-11T23:21:07.034Z,2,491,"<div class=""post"" itemprop=""articleBody""><NewLine><p>There is no pytorch here. I just wanted to share the fact that 150 lines of code with numpy and a simple linear policy with a basic SGD can reach such performance in MuJoCo environments:</p><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://assets-cdn.github.com/favicon.ico"" width=""16""/><NewLine><a href=""https://github.com/alexis-jacq/numpy_ARS"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars1.githubusercontent.com/u/9195965?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/alexis-jacq/numpy_ARS"" target=""_blank"">alexis-jacq/numpy_ARS</a></h3><NewLine><p>numpy_ARS - A 150-lines python code for Augmented Random Search (https://arxiv.org/abs/1803.07055) with numpy.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>The algorithm is from Ben Recht’s team (<a href=""http://www.argmin.net/2018/03/20/mujocoloco/"">http://www.argmin.net/2018/03/20/mujocoloco/</a>)</p><NewLine><p>My next step is to implement a special pytorch optimizer (or a module?) to make these 150 numpy lines into 50 pytorch lines.</p><NewLine></div>",https://discuss.pytorch.org/u/alexis-jacq,(Alexis David Jacq),alexis-jacq,"April 11, 2018, 11:21pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>That would be great. I am looking forward for your PyTorch implementation of ARS. I found one here:<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/LAIRLAB/ARS-experiments/blob/8c832b8c5e996621469436464716234679457cbf/ars/mnist_ars.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/LAIRLAB/ARS-experiments/blob/8c832b8c5e996621469436464716234679457cbf/ars/mnist_ars.py"" rel=""nofollow noopener"" target=""_blank"">LAIRLAB/ARS-experiments/blob/8c832b8c5e996621469436464716234679457cbf/ars/mnist_ars.py</a></h4><NewLine><pre><code class=""lang-py"">'''<NewLine>Augmented random search for MNIST<NewLine>Author: Anirudh Vemula<NewLine>'''<NewLine>from __future__ import print_function<NewLine>import argparse<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine>from torchvision import datasets, transforms<NewLine>from torch.autograd import Variable<NewLine>from envs.mnist.mnist import MNIST<NewLine>from utils.ars import *<NewLine>import numpy as np<NewLine>import random<NewLine>import ipdb<NewLine><NewLine>parser = argparse.ArgumentParser(description='PyTorch MNIST Example')<NewLine># Experiment parameters<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/LAIRLAB/ARS-experiments/blob/8c832b8c5e996621469436464716234679457cbf/ars/mnist_ars.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I did this : <a href=""https://github.com/alexis-jacq/Pytorch_Policy_Search_Optimizer"" rel=""nofollow noopener"">https://github.com/alexis-jacq/Pytorch_Policy_Search_Optimizer</a></p><NewLine><p>So it’s possible to explore ARS performance using other kind of policies than linear using Pytorch tools.<br/><NewLine>But I did this before version 0.4, it’s probably a bit old-fashion now.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much (also for your fast response), this is great. However, it would be even greater if it can also include a simple classification example (rather than RL) where I guess the augmented random search can still be used (at least that is the case in the above example). Because it would be much easier for me to grasp it on such simple classification setting than RL, in which I am newbie; and give it a short in my existing problems. Cheers.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Kamer_Ali_Yuksel; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Kamer_Ali_Yuksel; <NewLine> ,"REPLY_DATE 1: January 14, 2019, 10:45am; <NewLine> REPLY_DATE 2: January 17, 2019,  9:58am; <NewLine> REPLY_DATE 3: January 17, 2019, 12:59pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
34677,Understanding Enropy,2019-01-15T03:13:57.704Z,0,433,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m a student working on a PPO implementation and trying to understand how entropy is inferred in the following examples.  I have a basic understanding of what entropy is after watching Aurelian Geron’s video.  (Sorry.  I’m a newb and not allowed to overuse links)</p><NewLine><p>In <a href=""https://github.com/ShangtongZhang/DeepRL"" rel=""nofollow noopener"">shangtongzhang’s</a> implementation of PPO,</p><NewLine><pre><code class=""lang-auto"">def __init__(self,<NewLine>                 state_dim,<NewLine>                 action_dim,<NewLine>                 phi_body=None,<NewLine>                 actor_body=None,<NewLine>                 critic_body=None):<NewLine>        super(GaussianActorCriticNet, self).__init__()<NewLine>        self.network = ActorCriticNet(state_dim, action_dim, phi_body, actor_body, critic_body)<NewLine>        self.std = nn.Parameter(torch.zeros(action_dim))<NewLine>        self.to(Config.DEVICE)<NewLine><NewLine>    def forward(self, obs, action=None):<NewLine>        obs = tensor(obs)<NewLine>        phi = self.network.phi_body(obs)<NewLine>        phi_a = self.network.actor_body(phi)<NewLine>        phi_v = self.network.critic_body(phi)<NewLine>        mean = F.tanh(self.network.fc_action(phi_a))<NewLine>        v = self.network.fc_critic(phi_v)<NewLine>        dist = torch.distributions.Normal(mean, F.softplus(self.std))<NewLine>        if action is None:<NewLine>            action = dist.sample()<NewLine>        log_prob = dist.log_prob(action).sum(-1).unsqueeze(-1)<NewLine>        entropy = dist.entropy().sum(-1).unsqueeze(-1)<NewLine></code></pre><NewLine><p>The distribution is driven by the line <code>self.std = nn.Parameter(torch.zeros(action_dim))</code> where the softplus function is applied to this to get a distribution.  Isn’t this just a bunch of zeros that never change?</p><NewLine><p>This suggests that entropy is a fixed property or basically specified by the creator of the network.  If so, I’m struggling to see the point of adding an entropy term to the advantage.  I’ve seen implementations that use a parameter in place of the zeros.  This makes more sense to me if entropy is basically a specified and static parameter.</p><NewLine><p>In <a href=""https://github.com/ostamand/continuous-control"" rel=""nofollow noopener"">this</a> implementation:</p><NewLine><pre><code class=""lang-auto""><NewLine>        self.fc_actor_mean = nn.Linear(256, self.action_dim)<NewLine>        self.fc_actor_std = nn.Linear(256, self.action_dim)<NewLine>        self.fc_critic = nn.Linear(256, 1)<NewLine><NewLine>        self.std = nn.Parameter(torch.zeros(1, action_dim))<NewLine><NewLine>    def forward(self, x, action=None):<NewLine>        x = F.relu(self.fc1(x))<NewLine>        x = F.relu(self.fc2(x))<NewLine><NewLine>        # Actor<NewLine>        mean = torch.tanh(self.fc_actor_mean(x))<NewLine>        std = F.softplus(self.fc_actor_std(x))<NewLine>        dist = torch.distributions.Normal(mean, std)<NewLine>        if action is None:<NewLine>            action = dist.sample()<NewLine>        log_prob = dist.log_prob(action)<NewLine><NewLine>        # Critic<NewLine>        # State value V(s)<NewLine>        v = self.fc_critic(x)<NewLine><NewLine></code></pre><NewLine><p>the distribution is taken as one of the heads of the network rather than a bunch of zeros.  This is conceptually more intuitive in the sense that the distribution and therefore entropy is dynamic and a property of the network and its parameters.  Is this a valid way of doing this and if so, how is the distribution of the outputs of this head conditioned to be relevant?</p><NewLine></div>",https://discuss.pytorch.org/u/shogan50,"(Scott Hogan, P E )",shogan50,"January 15, 2019,  3:13am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""34677""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v2/letter/s/ee59a6/40.png"" width=""20""/> shogan50:</div><NewLine><blockquote><NewLine><p>The distribution is driven by the line <code>self.std = nn.Parameter(torch.zeros(action_dim))</code> where the softplus function is applied to this to get a distribution. Isn’t this just a bunch of zeros that never change?</p><NewLine></blockquote><NewLine></aside><NewLine><p>In this code, <code>self.std</code> should be named <code>self.logit_std</code>. That’s why it is given to be eaten by a softplus thing.<br/><NewLine>Only the output of F.softplus(std) is the standard deviation.</p><NewLine><p>The fact it’s a parameter (<code>nn.Parameter</code>) says to pytorch that it must be learned (so this value is not fixed and will adapt in order to maximize the reward, minus a lagrangian penalty for entropy).</p><NewLine><p>So the entropy (that as you said, closely depends on this variable, but also on the means) will be learned, and using such a trick is quite efficient (rather than the second code where the standard deviation depends on the input). The standard deviation is, if you want, the tradeoff between exploration and exploitation: the smaller the std, the less exploration.</p><NewLine><p>And the entropy penalty in the loss is used to regulate the learning rate : the main role is to avoid fast convergence to local minima with deterministic solutions.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you sincerely!  I have many hours invested in trying to grok this one detail.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/shogan50; <NewLine> ,"REPLY_DATE 1: January 16, 2019, 12:00am; <NewLine> REPLY_DATE 2: January 16, 2019,  1:21am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
34648,Torch/RL newbie: Trying to do PPO,2019-01-14T18:31:04.229Z,0,657,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to implement PPO Actor-Critic Style based on the paper by Schulman et al (<a href=""https://arxiv.org/pdf/1707.06347.pdf"" rel=""nofollow noopener"">paper</a>).</p><NewLine><p>Long story short: My code doesn’t learn and I do not know why. I suspect my hyperparameters are poorly chosen</p><NewLine><p><a href=""https://github.com/Mufabo/Learning-Reinforcement-Learning/blob/master/PPO/barebonePPO.py"" rel=""nofollow noopener"">github link to my code</a></p><NewLine><pre><code class=""lang-auto""># Imports<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine>from torch.distributions import Normal<NewLine>import numpy as np<NewLine>import gym<NewLine><NewLine># Hyperparameters<NewLine>env = ""Pendulum-v0""<NewLine>env = gym.make(env)<NewLine>numInputs = env.observation_space.shape[0]<NewLine>numOutputs = env.action_space.shape[0]<NewLine><NewLine>HSzIn = 10 * numInputs<NewLine>HSzOut = 10 * numOutputs<NewLine>GAMMA = 0.95 # discount factor<NewLine>LAMBDA = 0.95 # Lambda for GAE<NewLine>SampleSz = 2**10 # Number of generated samples per Iteration<NewLine>BatchSz = 2**5 # Size of Minibatch for update<NewLine>numIterations = 10000 # Number of iterations<NewLine>LearningRate = 1e-4<NewLine>numUpdates = 2**5<NewLine>Epsilon = 0.2<NewLine>CRITIC_DISCOUNT = 0.5<NewLine>ENTROPY_BETA = 0.001<NewLine><NewLine>def test(env,net):<NewLine>    state = env.reset()<NewLine>    done = False<NewLine>    rew = 0<NewLine>    while not done:<NewLine>            state = torch.FloatTensor(np.transpose(state))<NewLine>            dist, value = net(state)<NewLine>            action = dist.sample()<NewLine>            nextState, reward, done , _ = env.step(action.detach().numpy())<NewLine>            rew += reward<NewLine>    print(rew)<NewLine><NewLine><NewLine>class ActorCritic(nn.Module):<NewLine>    def __init__(self, numInputs, numOutputs, hiddenSz=HSzIn, hiddenSzOut=HSzOut, std=0.0):<NewLine>        super(ActorCritic, self).__init__()<NewLine><NewLine>        self.critic = nn.Sequential(<NewLine>            nn.Linear(numInputs, hiddenSz),<NewLine>            nn.Tanh(),<NewLine>            nn.Linear(hiddenSz, hiddenSzOut),<NewLine>            nn.Tanh(),<NewLine>            nn.Linear(hiddenSzOut,1)<NewLine>            )<NewLine><NewLine>        self.actor = nn.Sequential(<NewLine>            nn.Linear(numInputs, hiddenSz),<NewLine>            nn.Tanh(),<NewLine>            nn.Linear(hiddenSz, hiddenSzOut),<NewLine>            nn.Tanh(),<NewLine>            nn.Linear(hiddenSzOut, numOutputs),<NewLine>                )<NewLine><NewLine>        self.logStd = nn.Parameter(torch.ones(1,numOutputs) * std) # how to update variance ? <NewLine><NewLine>    def forward(self,x):<NewLine>        value = self.critic(x)<NewLine>        mu = self.actor(x)<NewLine>        std = self.logStd.exp()<NewLine>        dist = Normal(mu,std)<NewLine>        return dist, value<NewLine><NewLine># Initialize Net and optimizer<NewLine>net = ActorCritic(numInputs,numOutputs)<NewLine>optimizer = optim.Adam(net.parameters(), lr=LearningRate)<NewLine><NewLine># generate samples and data necessary for the update<NewLine>for ite in range(numIterations):<NewLine>    logProbs  = torch.zeros(1, SampleSz)<NewLine>    values    = torch.zeros(1, SampleSz)<NewLine>    states    = torch.zeros(SampleSz,numInputs)<NewLine>    actions   = torch.zeros(SampleSz, numOutputs)<NewLine>    rewards   = torch.zeros(1, SampleSz)<NewLine>    dones     = torch.zeros(1, SampleSz)<NewLine><NewLine>    state = env.reset()<NewLine>    for step in range(SampleSz):<NewLine>        state = torch.Tensor(np.transpose(state)) # transpose to avoid size-mismatch type-error<NewLine>        dist, value = net(state)<NewLine>        action = dist.sample()<NewLine>        nextState, reward, done , _ = env.step(action.detach().numpy())<NewLine>        <NewLine>        logProbs[0][step] = dist.log_prob(action)<NewLine>        values[0][step] = value<NewLine>        rewards[0][step] = torch.Tensor(reward)# reward is one-element array of type float32<NewLine>        dones[0][step] = 1 - done<NewLine>        states[:][step] = state<NewLine>        actions[:][step] = action<NewLine>        <NewLine>        if done:<NewLine>            state = env.reset()<NewLine>        else:<NewLine>            state = nextState<NewLine><NewLine>    # GAE<NewLine>    _,finalValue = net(torch.Tensor(np.transpose(nextState))) # add V(s') for final s <NewLine><NewLine>    allValues = torch.cat((values,finalValue),1)<NewLine>    advan_t = 0<NewLine>    reward = 0<NewLine>    advantages = torch.zeros(1, SampleSz)<NewLine>    sumDiscRew = torch.zeros(1, SampleSz)<NewLine>    for i in reversed(range(SampleSz)):<NewLine>        if dones[0][i]:<NewLine>            reward = 0<NewLine>        # delta is the TD residual<NewLine>        delta = rewards[0][i] + GAMMA*allValues[0][i+1]*dones[0][i] - allValues[0][i]<NewLine>        advan_t = delta + GAMMA*LAMBDA*advan_t*dones[0][i] # implicit reset of advan_t when new traj starts<NewLine>        advantages[0][i] = advan_t<NewLine>        sumDiscRew[0][i] = rewards[0][i] + GAMMA*reward<NewLine><NewLine>        <NewLine>    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)<NewLine><NewLine>    <NewLine>    # Update Network<NewLine>    for _ in range(numUpdates):<NewLine>        randIndices = np.random.randint(0, SampleSz, BatchSz)<NewLine>        # Compute loss functions<NewLine>        surr1 = torch.zeros(1, BatchSz)<NewLine>        surr2 = torch.zeros(1, BatchSz)<NewLine>        valueDiffs = torch.zeros(1, BatchSz)<NewLine>        entropies = torch.zeros(1, BatchSz)<NewLine>        ind = 0<NewLine>        for i in randIndices:<NewLine>            dist, value = net(states[i])<NewLine>            entropy = dist.entropy()<NewLine><NewLine>            entropies[0][ind] = entropy<NewLine>            <NewLine>            new_log_prob = dist.log_prob(actions[i])<NewLine>            ratio = (new_log_prob - logProbs[0][i]).exp()<NewLine>            surr1[0][ind] = ratio * advantages[0][i]<NewLine>            surr2[0][ind] = torch.clamp(ratio, 1.0 - Epsilon, 1.0 + Epsilon) * advantages[0][i]<NewLine><NewLine>            valueDiffs[0][ind] = sumDiscRew[0][i] - value<NewLine><NewLine>            ind += 1<NewLine>            <NewLine>        actor_loss  = - torch.min(surr1, surr2).mean()<NewLine><NewLine>        # MSE between discounted sum of discounted future rewards and predictions<NewLine>        critic_loss = valueDiffs.pow(2).mean()<NewLine><NewLine>        loss = CRITIC_DISCOUNT * critic_loss + actor_loss  - ENTROPY_BETA * entropies.mean()<NewLine><NewLine>        optimizer.zero_grad()<NewLine>        loss.backward(retain_graph=True)<NewLine>        optimizer.step()<NewLine>        <NewLine>    <NewLine>test(env,net)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Mufabo,(Mufabo),Mufabo,"January 14, 2019,  6:31pm",,,,,
24705,Several questions regarding my implementation of PPO on Pytorch,2018-09-07T16:34:20.961Z,1,1987,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! First time posting here!<br/><NewLine>I’ve been learning RL this summer and this week I’ve tried to make a PPO implementation on Pytorch with the help of some repositories from github with similiar algorithms.<br/><NewLine>The code runs OpenAI’s Lunar Lander but I have several errors that I have not been able to fix, the biggest one being that the algorithm quickly converges to doing the same action regardless of the state. The other major problem I’ve found is that even though I’m using backwards() only once, I get an error asking me to set retain_graph to True.<br/><NewLine>Because of that, I see no improvement of the rewards obtained over 1000 steps, I don’t know if the algorithm needs more steps to be able to see an improvement.</p><NewLine><p>I’m really sorry if this kind of problems have no place in this forums, I just didn’t know where to post this.<br/><NewLine>Also I’m sorry for the messy code, it’s my first time doing this kind of algorithms, and I’m fairly new with pytorch and machine learning in general.</p><NewLine><p>Tanks a lot in advance!!</p><NewLine><p>Code:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import numpy as np<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import matplotlib.pyplot as plt<NewLine>from torch.distributions import Categorical<NewLine>import gym<NewLine><NewLine>class actorCritic(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(actorCritic, self).__init__()<NewLine>        self.fc = nn.Sequential(<NewLine>        nn.Linear(8, 16),<NewLine>        nn.Linear(16, 32), <NewLine>        nn.Linear(32, 64), <NewLine>        nn.ReLU(inplace=True)<NewLine>        )<NewLine>        <NewLine>        self.pi = nn.Linear(64, 4)<NewLine>        self.value = nn.Linear(64, 1)<NewLine>    def forward(self, x):<NewLine>        x = self.fc(x)<NewLine>        pi_1 = self.pi(x)<NewLine>        pi_out = F.softmax(pi_1, dim=-1)<NewLine>        value_out = self.value(x)<NewLine>        return pi_out, value_out<NewLine><NewLine>def GAE(rewards, values, masks):<NewLine>    gamma = 0.99<NewLine>    lamb = 0.95<NewLine>    advan_t = 0<NewLine>    sizes = rewards.size()<NewLine><NewLine>    advantages = torch.zeros(1, sizes[1])<NewLine>    for t in reversed(range(sizes[1])):<NewLine>        delta = rewards[0][t] + gamma*values[0][t+1]*masks[0][t] - values[0][t]<NewLine>        advan_t = delta + gamma*lamb*advan_t*mask[0][t]<NewLine>        advantages[0][t] = advan_t<NewLine>    <NewLine>    real_values = values[:,:sizes[1]] + advantages<NewLine><NewLine>    return advantages, real_values<NewLine>def plot_rewards(rewards):<NewLine>    plt.figure(2)<NewLine>    plt.clf() <NewLine>    plt.plot(rewards)<NewLine>    plt.pause(0.001) <NewLine>    plt.savefig('TruePPO 500 steps.png')    <NewLine>    <NewLine>def interact(times, states):<NewLine><NewLine>    rewards = torch.zeros(1, times)<NewLine><NewLine>    actions = torch.zeros(1, times)<NewLine>    mask = torch.ones(1, times)<NewLine>    for steps in range(times):<NewLine>        action_probs, _ = network(states[steps])<NewLine>        m = Categorical(action_probs)<NewLine>        action = int(m.sample())<NewLine><NewLine>        obs, reward, done, _ = env.step(action)<NewLine><NewLine>        if done:<NewLine>            obs = env.reset()<NewLine>            mask[0][steps] = 0<NewLine><NewLine>        states[steps+1] = torch.from_numpy(obs).float()<NewLine>        rewards[0][steps] = reward<NewLine>        actions[0][steps] = action<NewLine> <NewLine>    return states, rewards, actions, mask<NewLine><NewLine>    <NewLine>#Parameters<NewLine><NewLine>total_steps = 1000<NewLine>batch_size = 10<NewLine>env = gym.make('LunarLander-v2')<NewLine>network = actorCritic()<NewLine>old_network = actorCritic()<NewLine>optimizer = torch.optim.Adam(network.parameters(), lr = 0.001)<NewLine>states = torch.zeros(batch_size+1, 8)<NewLine>steps = 0<NewLine>obs_ = env.reset()<NewLine>obs = torch.from_numpy(obs_).float()<NewLine>states[0] = obs<NewLine>reward_means = []<NewLine>nn_paramD = network.state_dict()<NewLine>old_network.load_state_dict(nn_paramD)<NewLine><NewLine><NewLine>while steps &lt; total_steps:<NewLine>    print (steps)<NewLine>    states, rewards, actions, mask = interact(batch_size, states)<NewLine>    #calculate values, GAE, normalize advantages, randomize, calculate loss, backprop,<NewLine>    _, values = network(states)<NewLine>    values = values.view(-1, batch_size+1)<NewLine><NewLine>    advantages, v_targ = GAE(rewards, values, mask)<NewLine><NewLine>    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-5)<NewLine><NewLine>    optimizer.zero_grad()<NewLine>    for n in range(rewards.size()[1]):<NewLine><NewLine>        probabilities, _ = network(states[n])<NewLine>        print (probabilities)<NewLine>        m = Categorical(probabilities)<NewLine>        action_prob = m.probs[int(actions[0][n])]<NewLine><NewLine>        entropia = m.entropy()<NewLine>        <NewLine>        old_probabilities, _ = old_network(states[n])<NewLine>        m_old = Categorical(old_probabilities)<NewLine>        old_action_prob = m.probs[actions[0][n].int()]<NewLine>        old_action_prob.detach()<NewLine>        <NewLine>        ratio = action_prob / old_action_prob<NewLine>        <NewLine>        surr1 = ratio*advantages[0][n]<NewLine>        surr2 = torch.clamp(ratio, min = (1.-0.2), max = (1.+0.2))<NewLine>        <NewLine>        policy_loss = -torch.min(surr1, surr2)<NewLine><NewLine>        value_loss = 0.5*(values[0][n]-v_targ[0][n])**2<NewLine><NewLine>        entropy_loss = -entropia<NewLine><NewLine>        total_loss = policy_loss + value_loss + 0.01*entropy_loss<NewLine><NewLine>        total_loss.backward(retain_graph = True)<NewLine><NewLine>        optimizer.step()<NewLine><NewLine>    reward_means.append(rewards.numpy().mean())<NewLine>    old_network.load_state_dict(nn_paramD)<NewLine>    nn_paramD = network.state_dict()<NewLine>    steps += 1<NewLine>    plot_rewards(reward_means)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/kyoto,,kyoto,"September 7, 2018,  4:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There is a lot of detail there, or in any RL implementation for that matter. Taking a quick look, there are a number of quirks and ? in your impl. Without digging deep into slicing and off by one level detail, a few thoughts:</p><NewLine><ol><NewLine><li><NewLine><p>It’s a great learning experience to start from scratch, but there is a lot to get right, and just a small mistake to get wrong and nothing will work. I’d spend a little more time looking at other (working) reference impl to guide you.</p><NewLine></li><NewLine><li><NewLine><p>You’re calling zero grad once for your whole inner loop. That is likely why you get the retain_graph warning. Doing one zero and multiple backward, and one step is a way of accumulating your gradients before an update. What you’re doing is all kinds of crazy <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/> You should generally do the following within the loop:</p><NewLine></li><NewLine></ol><NewLine><pre><code class=""lang-auto"">optimizer.zero_grad()<NewLine>loss.backward()<NewLine># most RL algorithms also do a nn.utils.clip_grad_norm_(model.parameters(), self.max_grad_norm)<NewLine>optimizer.step()<NewLine></code></pre><NewLine><ol start=""3""><NewLine><li><NewLine><p>For PPO specifically, your notion of ‘batches’ and samples in general is not typical. Usually you build up more trajectories from your simulator and then sample larger batches from them multiple times (PPO epochs).</p><NewLine></li><NewLine><li><NewLine><p>From outer loop to outer loop the states (observations) aren’t being rolled over. You leave next state in states[steps+1], but you don’t bring that last observation into states[0] for the start of the next. Unless I’m missing something you’re losing all continuity in your observations from one outer loop to another, given that the environment episodes don’t necessarily line up with your rollout sampling.</p><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much for the reply Ross! You helped me a ton and I’ll definitely look into some other implementations first. Thanks!!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/rwightman; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kyoto; <NewLine> ,"REPLY_DATE 1: September 19, 2018,  4:22am; <NewLine> REPLY_DATE 2: January 11, 2019,  3:21pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
34244,Updatation of Parameters without using optimizer.step(),2019-01-09T10:55:25.807Z,12,1489,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to implement Actor-Critic Algorithm with eligibility trace. As mentioned in algorithm I need to initialize trace vector with  same number of network parameters to zero and then update manually. And at the end I need to update both the network Actor as well Critic network parameters manually without using optimizer.step() function. Is it possible?</p><NewLine></div>",https://discuss.pytorch.org/u/parekh_vivek,(Parekh Vivek),parekh_vivek,"January 9, 2019, 10:05pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Yes of course.<br/><NewLine>You can just create all these Tensors and then instead of doing <code>opt.step()</code>, do:</p><NewLine><pre><code class=""lang-auto"">with torch.no_grad():<NewLine>     params.copy_(new_params)<NewLine></code></pre><NewLine><p>The <code>torch.no_grad()</code> is important to make sure that this is not recorded in the computational graph !</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi , Thanks for reply … can you elobarate more in detail with small example</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Instead of doing the classical:</p><NewLine><pre><code class=""lang-auto"">pred = model(inp)<NewLine>loss = critetion(pred, ground_truth)<NewLine>optimizer.zero_grad()<NewLine>loss.backward()<NewLine>optimizer.step()<NewLine></code></pre><NewLine><p>You can compute the updates by hand and then set them into the weights.<br/><NewLine>This is basically what SGD does where the update_function return <code>p + lr*p.grad</code>.</p><NewLine><pre><code class=""lang-auto"">pred = model(inp)<NewLine>loss = your_loss(pred)<NewLine>model.zero_grad()<NewLine>loss.backward()<NewLine>with torch.no_grad():<NewLine>  for p in model.parameters():<NewLine>    new_val = update_function(p, p.grad, loss, other_params)<NewLine>    p.copy_(new_val)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""34244""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/alband/40/215_1.png"" width=""20""/> albanD:</div><NewLine><blockquote><NewLine><p>torch.no_grad():</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thanks<br/><NewLine>Can you tell  me here have you define update_function(p,p.grad,loss,other_params) manually or its already there in pytorch documentation as I couldnot find and hwo to do with Adam same thing if i want to do?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>You have to implement it yourself so that it computes the new value that you want. In your case I guess it should compute what is in the paper you cite.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""34244""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/alband/40/215_1.png"" width=""20""/> albanD:</div><NewLine><blockquote><NewLine><p>You can compute the updates by hand and then set them into the weights.</p><NewLine></blockquote><NewLine></aside><NewLine><p>ok thanks.Stil I have confusion how can I define like and cross verify whether its true or not? if you can help in that</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry I don’t understand that question “how can I define like and cross verify whether its true or not?” could you explain please?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Like in my case I define update_function and then update the parameters so whether its true updated or not how can I check. Like f=x**2 ,I know gradient is 2x I can verify manually like that.</p><NewLine><p>In my case above there are two neural network and I have t update both the neural network parametr manually with one function so implemenatation wise I am clue less how can I achieve wrt to above algorithm</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>What do you mean by “its true updated” do you mean the correct gradients for your function? Well they most certainly are not (in the sense that your fonction don’t have proper gradients) otherwise you could just use the autograd to compute them, you would not have to compute them by hand <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/></p><NewLine><p>I am not familiar with the algorithm you shared above but if you have two models you can loop through model1 parameters then model2 parameters. But that depends on what the algorithm is doing, you’ll need to look into that.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>I can’t for your problem as I don’t know it.<br/><NewLine>If you were doing SGD-like updates, it would be:</p><NewLine><pre><code class=""lang-auto"">def update_function(param, grad, loss, learning_rate):<NewLine>  return param - learning_rate * grad<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks no problem …for this much help</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/parekh_vivek; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/parekh_vivek; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/parekh_vivek; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/parekh_vivek; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/parekh_vivek; <NewLine> ,"REPLY_DATE 1: January 9, 2019, 11:50am; <NewLine> REPLY_DATE 2: January 9, 2019, 12:08pm; <NewLine> REPLY_DATE 3: January 9, 2019,  1:25pm; <NewLine> REPLY_DATE 4: January 9, 2019,  3:37pm; <NewLine> REPLY_DATE 5: January 9, 2019,  3:38pm; <NewLine> REPLY_DATE 6: January 9, 2019,  3:41pm; <NewLine> REPLY_DATE 7: January 9, 2019,  3:42pm; <NewLine> REPLY_DATE 8: January 9, 2019,  3:46pm; <NewLine> REPLY_DATE 9: January 9, 2019,  3:55pm; <NewLine> REPLY_DATE 10: January 9, 2019,  4:29pm; <NewLine> REPLY_DATE 11: January 9, 2019,  4:51pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> REPLY 11 LIKES: ; <NewLine> 
33968,Memory leak during backprop in Reinforcement Learning tutorial?,2019-01-06T23:08:21.126Z,0,335,"<div class=""post"" itemprop=""articleBody""><NewLine><p>There seems to be a memory leak in the official Reinforcement Learning tutorial for pytorch 1.0.0.<br/><NewLine><a href=""https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html</a><br/><NewLine>I run the original python source code reinforcement_q_learning.py (cpu implementation), but increased num_episodes from 50 to 5000 to make the increasing RAM consumption visible.</p><NewLine><p>(I had a custom reinforcement learning model, where I had a similar leak, and after eliminating other possible error sources I checked the ‘original’ code and there it is.)<br/><NewLine>It might be a major bug, could you check it?</p><NewLine></div>",https://discuss.pytorch.org/u/pszabo,(P Sz),pszabo,"January 6, 2019, 11:08pm",,,,,
33387,"Pytorch categorical distribution, probably a bug?",2018-12-29T22:18:48.655Z,1,675,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,</p><NewLine><p>I tried to use <code>torch.distributions.Categorical</code> and do not understand, why these two methods of calculating loss and gradient do not deliver identical results, only losses are equal:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>inp = torch.tensor( [[ 2 / 7, 4. / 7, 1 / 7 ]], requires_grad = True )<NewLine>for a in range( 3 ):<NewLine>    action = torch.tensor( [a] )<NewLine>    m = torch.distributions.Categorical( probs=inp )<NewLine>    loss = -m.log_prob( action )<NewLine>    loss.backward()<NewLine>    print( loss, inp.grad )<NewLine>    inp.grad.zero_()<NewLine><NewLine>for a in range( 3 ):<NewLine>    action = torch.tensor( [a] )<NewLine>    loss = torch.nn.NLLLoss()( inp.log(), action )<NewLine>    loss.backward()<NewLine>    print( loss, inp.grad )<NewLine>    inp.grad.zero_()<NewLine></code></pre><NewLine><p>Output:<br/><NewLine><code><br/><NewLine>tensor([1.2528], grad_fn=) tensor([[-2.5000,  1.0000,  1.0000]])<br/><NewLine>tensor([0.5596], grad_fn=) tensor([[ 1.0000, -0.7500,  1.0000]])<br/><NewLine>tensor([1.9459], grad_fn=) tensor([[ 1.0000,  1.0000, -6.0000]])<br/><NewLine>tensor(1.2528, grad_fn=) tensor([[-3.5000,  0.0000,  0.0000]])<br/><NewLine>tensor(0.5596, grad_fn=) tensor([[ 0.0000, -1.7500,  0.0000]])<br/><NewLine>tensor(1.9459, grad_fn=) tensor([[ 0.0000,  0.0000, -7.0000]])<br/><NewLine></code></p><NewLine><p>I would prefer to be a fool, or is this a bug? <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/></p><NewLine><p>Best regards,<br/><NewLine>Roman</p><NewLine></div>",https://discuss.pytorch.org/u/romasffm,(Roman Gerasimov),romasffm,"December 29, 2018, 10:49pm",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Great observation!<br/><NewLine>It’s a bit more subtle than a bug. While <code>Categorical.log_prob</code> returns a gradient with sum zero would leave you in the realm of probability measures when infinitesimally moving in that direction, <code>NLLLoss</code>'s backward has a constant offset that will happily catapult you outside the admissible input space.<br/><NewLine>In math lingo, <code>Categorical.log_prob</code> computes the gradient on the (not quite but locally if no probs are zero) manifold of probability vectors, which lies in the tangent space of that “manifold” while <code>NLLLoss</code> computes the gradient on the larger parameter space of the formula given in the documentation. The difference is orthogonal to the tangent space of probabilities.<br/><NewLine>The effect will go away if you force the input to <code>NLLLoss</code> to lie in the log-of-probability-measures manifold, i.e. change the line to</p><NewLine><pre><code class=""lang-python"">    loss = torch.nn.NLLLoss()( inp.log().log_softmax(1), action )<NewLine></code></pre><NewLine><p>Then the <code>log_softmax</code> does not change your input (it is a projection on log probability measures, so the identity on them) but its backward will do the projection on the tangent space (if you liked the math: because the normal component is in the kernel of the Jacobian) and you’ll get gradients for <code>inp</code> that  might better match your expectation.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks Thomas, great explanation!</p><NewLine><p>So I am a fool, a bit of at least <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/></p><NewLine><p>As I woke up after some hours of sleep and analyzing the source code I also had a suspicion,<br/><NewLine>where my error was, but You made me see it clear.</p><NewLine><p>Allow me to notice, that the sum of the first gradients is also not equal to zero.<br/><NewLine>The shift of 1 between both groups can be explained with <code>logsoftmax()</code>,<br/><NewLine>but what about nonzero sums?</p><NewLine><p>Grads are orthogonal to <code>[ 2 / 7, 4. / 7, 1 / 7 ]</code>. And that is the point.<br/><NewLine>Prob manifold is a simplex, so the grads have to have zero sum of components,<br/><NewLine>to let me stay in the manifold.</p><NewLine><p>What do you mean?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Good catch! The difference must be constant (to be defined after de-meaning aka projecting onto the tangent space). Now, if PyTorch doesn’t do this by default, I wonder if training would be better if it did. Some of those reports on the forum… You can do this via register_hook on the input, I’ll certainly try that when I have the time.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/romasffm; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: December 30, 2018,  2:58pm; <NewLine> REPLY_DATE 2: December 30, 2018,  3:10pm; <NewLine> REPLY_DATE 3: December 30, 2018,  7:22pm; <NewLine> ",REPLY 1 LIKES: 3 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
33199,How to implement a Continuous Control of a quadruped robot with Deep Reinforcement Learning in Pytorch and Pybullet?,2018-12-27T17:33:10.299Z,0,667,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have designed this robot in URDF format and its environment in pybullet. Each leg has a minimum and maximum value of movement.</p><NewLine><p>What reinforcement algorithm will be best to create a walking policy in a simple environment in which a positive reward will be given if it walks in the positive X-axis direction?</p><NewLine><p>The expected output from the policy is an array in the range of (-1, 1) for each joint. The input of the policy is the position of each joint, the center of mass of the body, the difference in height between the floor and the body to see if it has fallen and the movement in the x-axis.</p><NewLine><p>Limitations</p><NewLine><p>left_front_joint      =&gt; lower=""-0.4"" upper=“2.5” id=0</p><NewLine><p>left_front_leg_joint  =&gt; lower=""-0.6"" upper=“0.7” id=2</p><NewLine><p>right_front_joint     =&gt; lower=""-2.5"" upper=“0.4” id=3</p><NewLine><p>right_front_leg_joint =&gt; lower=""-0.6"" upper=“0.7” id=5</p><NewLine><p>left_back_joint       =&gt; lower=""-2.5"" upper=“0.4” id=6</p><NewLine><p>left_back_leg_joint   =&gt; lower=""-0.6"" upper=“0.7” id=8</p><NewLine><p>right_back_joint      =&gt; lower=""-0.4"" upper=“2.5” id=9</p><NewLine><p>right_back_leg_joint  =&gt; lower=""-0.6"" upper=“0.7” id=11</p><NewLine><p>The code above is just a test of the environment with a manual set of movements hardcoded in the robot just to test how it could walk later. The environment is set to real time, but I assume it needs to be in a frame by frame lapse during the policy training.</p><NewLine><p>A video of it can be seen in:</p><NewLine><div class=""lazyYT"" data-height=""270"" data-parameters=""feature=oembed&amp;wmode=opaque"" data-width=""480"" data-youtube-id=""j9sysG-EIkQ"" data-youtube-title=""Walking Spider Simulation Using Deep Reinforcement Learning using Pybullet and...""></div><NewLine><pre><code>import pybullet as p<NewLine>import time<NewLine>import pybullet_data<NewLine><NewLine>def moveLeg( robot=None, id=0, position=0, force=1.5  ):<NewLine>    if(robot is None):<NewLine>        return;<NewLine>    p.setJointMotorControl2(<NewLine>        robot,<NewLine>        id,<NewLine>        p.POSITION_CONTROL,<NewLine>        targetPosition=position,<NewLine>        force=force,<NewLine>        #maxVelocity=5<NewLine>    )<NewLine><NewLine>pixelWidth = 1000<NewLine>pixelHeight = 1000<NewLine>camTargetPos = [0,0,0]<NewLine>camDistance = 0.5<NewLine>pitch = -10.0<NewLine>roll=0<NewLine>upAxisIndex = 2<NewLine>yaw = 0<NewLine><NewLine>physicsClient = p.connect(p.GUI)#or p.DIRECT for non-graphical version<NewLine>p.setAdditionalSearchPath(pybullet_data.getDataPath()) #optionally<NewLine>p.setGravity(0,0,-10)<NewLine>viewMatrix = p.computeViewMatrixFromYawPitchRoll(camTargetPos, camDistance, yaw, pitch, roll, upAxisIndex)<NewLine>planeId = p.loadURDF(""plane.urdf"")<NewLine>cubeStartPos = [0,0,0.05]<NewLine>cubeStartOrientation = p.getQuaternionFromEuler([0,0,0])<NewLine>#boxId = p.loadURDF(""r2d2.urdf"",cubeStartPos, cubeStartOrientation)<NewLine>boxId = p.loadURDF(""src/spider.xml"",cubeStartPos, cubeStartOrientation)<NewLine># boxId = p.loadURDF(""spider_simple.urdf"",cubeStartPos, cubeStartOrientation)<NewLine><NewLine><NewLine><NewLine>toggle = 1<NewLine><NewLine><NewLine><NewLine>p.setRealTimeSimulation(1)<NewLine><NewLine>for i in range (10000):<NewLine>    #p.stepSimulation()<NewLine>    <NewLine><NewLine>    moveLeg( robot=boxId, id=0,  position= toggle * -2 ) #LEFT_FRONT<NewLine>    moveLeg( robot=boxId, id=2,  position= toggle * -2 ) #LEFT_FRONT<NewLine><NewLine>    moveLeg( robot=boxId, id=3,  position= toggle * -2 ) #RIGHT_FRONT<NewLine>    moveLeg( robot=boxId, id=5,  position= toggle *  2 ) #RIGHT_FRONT<NewLine><NewLine>    moveLeg( robot=boxId, id=6,  position= toggle *  2 ) #LEFT_BACK<NewLine>    moveLeg( robot=boxId, id=8,  position= toggle * -2 ) #LEFT_BACK<NewLine><NewLine>    moveLeg( robot=boxId, id=9,  position= toggle *  2 ) #RIGHT_BACK<NewLine>    moveLeg( robot=boxId, id=11, position= toggle *  2 ) #RIGHT_BACK<NewLine>    #time.sleep(1./140.)g<NewLine>    #time.sleep(0.01)<NewLine>    time.sleep(1)<NewLine>    <NewLine>    toggle = toggle * -1<NewLine><NewLine>    #viewMatrix        = p.computeViewMatrixFromYawPitchRoll(camTargetPos, camDistance, yaw, pitch, roll, upAxisIndex)<NewLine>    #projectionMatrix  = [1.0825318098068237, 0.0, 0.0, 0.0, 0.0, 1.732050895690918, 0.0, 0.0, 0.0, 0.0, -1.0002000331878662, -1.0, 0.0, 0.0, -0.020002000033855438, 0.0]<NewLine>    #img_arr = p.getCameraImage(pixelWidth, pixelHeight, viewMatrix=viewMatrix, projectionMatrix=projectionMatrix, shadow=1,lightDirection=[1,1,1])<NewLine><NewLine>cubePos, cubeOrn = p.getBasePositionAndOrientation(boxId)<NewLine>print(cubePos,cubeOrn)<NewLine>p.disconnect()<NewLine></code></pre><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" href=""https://github.com/rubencg195/WalkingSpider_OpenAI_PyBullet_ROS/raw/master/images/PyBullet.png"" rel=""nofollow noopener"" title=""PyBullet.png""><img alt=""robot"" height=""499"" src=""https://github.com/rubencg195/WalkingSpider_OpenAI_PyBullet_ROS/raw/master/images/PyBullet.png"" width=""635""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">PyBullet.png</span><span class=""informations"">1026×807</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" href=""https://github.com/rubencg195/WalkingSpider_OpenAI_PyBullet_ROS/raw/master/images/URDF.png"" rel=""nofollow noopener"" title=""URDF.png""><img alt=""robot"" height=""477"" src=""https://github.com/rubencg195/WalkingSpider_OpenAI_PyBullet_ROS/raw/master/images/URDF.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">URDF.png</span><span class=""informations"">1399×968</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/rubencg195,(Ruben Chevez),rubencg195,"December 27, 2018,  5:33pm",1 Like,,,,
118,Asynchronous parameters updating?,2017-01-22T07:27:29.311Z,6,3952,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The background is A3C algorithm, where many worker threads share a common network <code>parameters</code> and share a common <code>rmsprop</code> states, with each thread holding its own <code>gradParameters</code>. Periodically, each worker thread updates the common <code>parameters</code> using the common <code>rmsprop</code> states with its own <code>gradParameters</code> in a lock-free, asynchronous way.</p><NewLine><p>Previously in Torch 7, it’s rather easy to do this with <code>threads</code> and <code>optim</code> library:</p><NewLine><pre><code class=""lang-auto"">-- in main thread: shared parameters<NewLine>params, _ = sharedNet:getParameters()<NewLine><NewLine>-- in worker thread: its own gradParameters<NewLine>tNet = sharedNet:clone()<NewLine>_, gradParams = tNet:getParameters()<NewLine><NewLine>-- in worker thread: stuff<NewLine><NewLine>-- in worker thread: updating shared parameters with its own gradParameters<NewLine>function feval() return nil, gradParams end<NewLine>optim.rmsprop(feval, params, sharedStates)<NewLine></code></pre><NewLine><p>But I don’t see an obvious way to do the same thing with pytorch, because now the <code>parameters</code> and <code>gradParameters</code> are tied together under the <code>nn.Parameter</code> class… Any suggestion for it? (I find the <code>mnist_hogwild.py</code> example but the updating details are different from what I described above.)</p><NewLine><p>Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/pengsun,(Pengsun),pengsun,"January 22, 2017,  7:27am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have already implemented A3C in pytorch, and it works just fine.  When you get a copy with everything shared in the subprocess just do this to break the gradient sharing, and use the optimizer as you’d normaly do:</p><NewLine><pre><code class=""lang-python"">for param in model.parameters():<NewLine>    param.grad.data = param.grad.data.clone()<NewLine></code></pre><NewLine><p>This is also covered in <a href=""http://pytorch.org/docs/notes/multiprocessing.html#asynchronous-multiprocess-training-e-g-hogwild"">the notes</a>, that I encourage you to read.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/apaszke"">@apaszke</a> thanks for the code!</p><NewLine><p>As you mentioned your private impl of A3C, I have one more simple question: according to the paper, A3C only synchronizes local network <code>parameters</code> periodically with the shared network <code>parameters</code> (in contrast to Asynchronous N-step Q where the network <code>parameters</code> is always synchronized).  To do this, did you write code like:</p><NewLine><pre><code class=""lang-python""># after every several steps (e.g., 5 or 20)<NewLine>for t_param, shared_param in zip(t_model.parameters(), shared_model.parameters()):<NewLine>    t_param.data.copy_(shared_param.data)<NewLine></code></pre><NewLine><p>Or did you find it not critical to the accuracy in your implementation? (Previously, I strictly follow the paper and I can reproduce the scoring curve for breakout as in the paper’s figure)</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, exactly. I used to have such code, but then I started distributing only <code>state_dicts</code> and now it looks more like this:</p><NewLine><pre><code class=""lang-auto"">t_model.load_state_dict(shared_state_dict)<NewLine></code></pre><NewLine><p>As said before, this doesn’t make the <code>t_model</code> parameters shared, but only copies the content of shared_state_dict.</p><NewLine><p>But your solution is valid too. Of course you have to apply updates to <code>shared_model</code> as well.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Great, thanks so much <a class=""mention"" href=""/u/apaszke"">@apaszke</a>! <code>load_state_dict</code> looks much better!</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/apaszke"">@apaszke</a>  adding A3C implementation to examples repo will be of great help. RIght now, I couldn’t find any examples using state_dicts to share parameters</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>You don’t need to speciffically use state_dicts, sharing models is fine too. Did you look in <a href=""http://pytorch.org/docs/notes/multiprocessing.html#asynchronous-multiprocess-training-e-g-hogwild"">the notes</a>? We’ll probably add A3C to examples sooner or later, but I can’t promise when.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I implemented A3C following the hogwild example:<br/><NewLine></p><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""32""/><NewLine><a href=""https://github.com/ikostrikov/pytorch-a3c"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""400"" src=""https://avatars1.githubusercontent.com/u/1212494?s=400&amp;v=4"" width=""400""/><NewLine><h3><a href=""https://github.com/ikostrikov/pytorch-a3c"" rel=""nofollow noopener"" target=""_blank"">ikostrikov/pytorch-a3c</a></h3><NewLine><p>PyTorch implementation of Asynchronous Advantage Actor Critic (A3C) from ""Asynchronous Methods for Deep Reinforcement Learning"". - ikostrikov/pytorch-a3c</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>It converges on PongDeterministic-v3 in 10 minutes with 16 threads. However, it work poorly on PongDeterministic-v3.</p><NewLine><p>Could you please take a look? I wonder whether I got asynchronous updates in PyTorch right.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think I’d recommend reusing the same grad tensors in multiple Variables, but apart form that I can’t see anything wrong at a glance.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>The problem was in the architecture that I used initially, seems to work on Breakout now.</p><NewLine><p>Reusing the same grad tensor might cause some problems?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Don’t think there’s anything that could go wrong at the moment, but I can’t guarantee that will change. An official version is that you never should have two <code>Variable</code>s having the same data, and <code>var.grad</code> is a Variable.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi IIya, <a class=""mention"" href=""/u/ilya_kostrikov"">@Ilya_Kostrikov</a>  I’m experimenting with your wonderful A3C implementation - I’m porting over a custom environment (basically blackbox optimization) I’ve got working in TensorFlow. I keep running int this error though,</p><NewLine><p>AttributeError: ‘float’ object has no attribute ‘backward’</p><NewLine><p>Any guesses what’s caused it? Apart from my custom environment, the only difference to your code is I’ve remove the conv layers, and used a 2 layer LSTM - which is the model I’m using in TensorFlow.</p><NewLine><p>As a performance improvement have you tried concatenating the previous action, reward, and a timestep counter onto the end of the state as 3 scalars - I noticed a significant improvement in my TF implementation when I do this.</p><NewLine><p><a class=""mention"" href=""/u/apaszke"">@apaszke</a> - would it be possible to have a look at your A3C implementation - I’ve spent nearly a whole day trying to debug mine?</p><NewLine><p>Thanks a lot for your help <img alt="":slight_smile:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=5"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ajaytalati"">@AjayTalati</a> my implementation is part of a larger project and is a bit specific to it, sorry. The problem you’re facing is that you think you’re working with a Variable, while you actually have a float object. If you upload the code somewhere and let me run it I’ll tell you where that is.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, <a class=""mention"" href=""/u/apaszke"">@apaszke</a>, thanks for replying. Indeed, I got confused which parts of the A3C algorithm should be Variables? There’s another open source PyTorch A3C implementation, by</p><NewLine><p><a href=""https://github.com/rarilurelo/pytorch_a3c"" rel=""nofollow noopener"">rarilurelo/pytorch_a3c</a></p><NewLine><p>Which I’m working through, which seems closer to my TF implementation. Hopefully I’ll figure it out soon.</p><NewLine><p>At the moment my environment code, (I guess like yours), is stuck in the middle of a larger project, and is not stand alone, yet. This is actually the reason why I’m moving away from TF.</p><NewLine><p>Hopefully, I’ll be able to wrap my environment into something like an OpenAI env, and be able to open source it on Github within a few days, and get back to you. It’s a nice example of graph optimisation/travelling salesman, so it should be of interest to quite a few people.</p><NewLine><p>All the best,</p><NewLine><p>Aj</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ajaytalati"">@AjayTalati</a> sounds good! Let me know once it’s public!</p><NewLine><p>About Variables, you only need to use the for parts that will require differentiation, and it’s best to not use them for everything else. If you have any questions about any specific cases, I’ll be happy to answer.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>you mean</p><NewLine><blockquote><NewLine><p>for (shared, local) in zip(shared_net.parameters(), local_net.parameters()):<br/><NewLine>shared_param.grad.data = local_param.grad.data.clone()</p><NewLine></blockquote><NewLine><p>right?</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ilya_kostrikov"">@Ilya_Kostrikov</a> - Quick question about your implementation:<br/><NewLine>Sorry in case it seems obvious to you, but I admittedly don’t have in-depth knowledge about (parallel) optimization algorithms. In your implementation, you moved the init of <code>step</code>, <code>square_avg</code> etc from <code>step</code> to <code>__init__</code>.</p><NewLine><p>Is there another reason for this apart form the obvious, that we can’t call <code>share_memory</code> before calling <code>step</code> for the first time?</p><NewLine><p>Cheers,<br/><NewLine>Deniz</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/apaszke"">@apaszke</a></p><NewLine><p>Sr to bring this up again, but no one answers my question about this.<br/><NewLine>When following the example code of Hogwild, I found out that the model’s params can be partially changed when I’m doing the forwarding on it.<br/><NewLine>Do you use any kinds of semaphore or mutex when optim.step or forwarding the model?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pengsun; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/pengsun; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Anurup_Arvind; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Ilya_Kostrikov; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Ilya_Kostrikov; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/AjayTalati; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/AjayTalati; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/onlytailei; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/denizs; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/Hung_Nguyen; <NewLine> ,"REPLY_DATE 1: January 22, 2017, 10:01am; <NewLine> REPLY_DATE 2: January 22, 2017, 11:30am; <NewLine> REPLY_DATE 3: January 22, 2017, 11:33am; <NewLine> REPLY_DATE 4: January 22, 2017, 11:38am; <NewLine> REPLY_DATE 5: January 23, 2017, 10:20am; <NewLine> REPLY_DATE 6: January 23, 2017,  3:24pm; <NewLine> REPLY_DATE 7: February 13, 2017,  5:42pm; <NewLine> REPLY_DATE 8: February 13, 2017,  6:18pm; <NewLine> REPLY_DATE 9: February 13, 2017,  7:13pm; <NewLine> REPLY_DATE 10: February 13, 2017, 10:57pm; <NewLine> REPLY_DATE 11: February 17, 2017,  6:12pm; <NewLine> REPLY_DATE 12: February 17, 2017,  6:18pm; <NewLine> REPLY_DATE 13: February 19, 2017,  8:07am; <NewLine> REPLY_DATE 14: February 19, 2017, 12:58pm; <NewLine> REPLY_DATE 15: March 16, 2017,  5:40pm; <NewLine> REPLY_DATE 16: July 7, 2017, 11:47am; <NewLine> REPLY_DATE 17: December 21, 2018, 11:52pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: 1 Like; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> 
31339,Understanding backward in reinforce,2018-12-05T11:25:58.109Z,0,302,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi guys,<br/><NewLine>I’m confused about backward in reinforce.<br/><NewLine>This is my implementation:</p><NewLine><pre><code class=""lang-auto""> for sample, rew in zip(self.policy_history, reward):<NewLine>     loss = torch.sum(torch.mul(-torch.log(sample.clamp(min=1e-6)), rew), -1)<NewLine>     policy_loss.append(loss)<NewLine><NewLine>policy_loss = torch.stack(policy_loss).sum() / batch_size<NewLine>policy_loss.backward()<NewLine></code></pre><NewLine><p>But is reinforce supposed to be optimized with gradient ascent while my optmizer apply gradient descent?<br/><NewLine>I’m worried about if optimizer.step() is trying to bring the policy_loss to zero and making my rewards less and less.<br/><NewLine>And what changes in my code if I use negative rewards, like penalty?</p><NewLine></div>",https://discuss.pytorch.org/u/rs9000,(Rs),rs9000,"December 5, 2018, 11:42am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Check this:<br/><NewLine><a href=""https://www.quora.com/What-is-the-difference-between-backpropagation-and-reinforcement-learning-in-training-artificial-neural-networks-Are-the-two-techniques-completely-different-or-related"" rel=""nofollow noopener"">What is the difference between backpropagation and reinforcement learning, in training artificial neural networks? Are the two techniques completely different or related?</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, I understand differences between RF and BB, but my question is how is possible that minimize a positive policy_loss means maximize rewards?<br/><NewLine>My policy loss is sum(-logprob*rew) so it’s a positive quantity.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/savchynt; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/rs9000; <NewLine> ,"REPLY_DATE 1: December 5, 2018, 12:02pm; <NewLine> REPLY_DATE 2: December 5, 2018,  1:05pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
31023,Backpropagation Through Time On LSTM for Reinforcement Learning,2018-12-01T11:07:41.418Z,0,214,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!</p><NewLine><p>I am trying to apply a LSTM architecture into a Deep Q-learning model, to be able to perceive effects of the actions through time. I think that my code have some conceptual error, because the agent isn’t learning with time.</p><NewLine><p>On this toy example, the agent is a car that have to ride in a road that have the format of a sin wave and have to stay in the middle of the road. It can take action : go up, go down, or stand still. It can get positive reward, if it stays near the middle, -1 reward if it goes out of the middle and -3 if it hits the wall (if it hits the wall his positionwill be the same of the wall, so he cant get out of the road). The agent if feed with his position and the current time on the loop.<br/><NewLine>There is a link to the code below:</p><NewLine><p><a href=""https://github.com/GoBraz/Reinforcement-Learning/blob/master/DQN_SinWave_test.py"" rel=""nofollow noopener"">DQN - Car on sin wave road</a></p><NewLine><p>To create the agent that interacts with the environment, I developed the code below:</p><NewLine><p><a href=""https://github.com/GoBraz/Reinforcement-Learning/blob/master/ai_LSTM.py"" rel=""nofollow noopener"">LSTM Agent</a></p><NewLine><p>I’m a little bit confused with how I perform backpropagation on the actions stored on the experience replay. <a href=""https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc"" rel=""nofollow noopener"">This</a> awesome post from Arthur Juliani on Medium said that I have to store episodes intead of timesteps to perform backprop on the experience replay with a LSTM architecture. I’ve tried to implement this idea in the code below (that is inside the second link on this post):</p><NewLine><pre><code class=""lang-auto""> def learn(self):<NewLine>        # Defining the counter<NewLine>        i = 0<NewLine>        # Getting an episode from the memory<NewLine>        episode = self.memory.sample_episode()<NewLine>        # Defining the graph lenght for a case where len(self.memory.memory) != 0<NewLine>        graph_end = len(episode) - 1<NewLine>        # Loop to perform backprop through time on the episode selected from the memory<NewLine>        while i &lt; graph_end :<NewLine>            state, next_state, action, reward = episode[i]<NewLine>            # Calculate the Q-value for the action taken on the episodes of memory sampled<NewLine>            outputs = self.model(state).gather(1, action.unsqueeze(1)).squeeze(1)<NewLine>            # Calculate the Q-value of all possible actions, given the next state and select the bigger to act on a optimal policy Q*<NewLine>            next_outputs = self.model(next_state).detach().max(1)[0]<NewLine>            # Calculate the value of the discounted reward and then the actual Q-value of the action taken<NewLine>            target = self.gamma*next_outputs + reward<NewLine>            # Calculate the loss<NewLine>            td_loss = F.smooth_l1_loss(outputs, target) <NewLine>            self.optimizer.zero_grad()<NewLine>            td_loss.backward(retain_graph = True)<NewLine>            # Aply the optimization step for each value, actualizing the values of the patamethers of the NN<NewLine>            self.optimizer.step()<NewLine>            i += 1<NewLine>        self.model.hx1.detach_()<NewLine>        self.model.hx2.detach_()<NewLine>        self.model.cx1.detach_()<NewLine>        self.model.cx2.detach_()<NewLine></code></pre><NewLine><p>I`m doing something wrong on this algorithm of TBPTT? There is something about DQN algorithm that I´m not considering for this implementation?</p><NewLine></div>",https://discuss.pytorch.org/u/GoBraz,(Luiz),GoBraz,"December 1, 2018, 11:07am",,,,,
30291,Where does the learning actually happen in the Reinforcement Learning tutorial?,2018-11-22T17:43:00.967Z,0,170,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I will attempt to answer the question posed in the topic.</p><NewLine><p>Looking at the tutorial code, I think this part is crucial:</p><NewLine><pre><code class=""lang-auto""># Compute V(s_{t+1}) for all next states.<NewLine>    next_state_values = torch.zeros(BATCH_SIZE, device=device)<NewLine>    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()<NewLine></code></pre><NewLine><p>It seems that every next_state_value where the non_final_mask is 0 is set to 0.0.  So when the following line is executed:</p><NewLine><pre><code class=""lang-auto""># Compute the expected Q values<NewLine>    expected_state_action_values = (next_state_values * GAMMA) + reward_batch<NewLine></code></pre><NewLine><p>the expected_state_action_values of those non_final_mask = 0 states are lower than the expected_state_action_values of the rest – even after the reward has been factored in.  The gradient from the subsequent loss will shift the weights slightly away from the states whose following state will be terminal.</p><NewLine><p>Is this how the reinforcement learning works here?  Please let me know if I’m understanding the process correctly.</p><NewLine><p>Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/hconroy,,hconroy,"November 22, 2018,  5:43pm",,,,,
29813,Ensure Batch Losses Have Low Entropy or Stdev in an Epoch,2018-11-17T00:38:27.521Z,0,177,"<div class=""post"" itemprop=""articleBody""><NewLine><p>It is a common practice in RL to normalize discounted rewards when doing policy gradients. Similarily, I want the model to perform similarily across all of the batches. Hence, I would like to normalize losses in a similar way by substracting their mean and dividing to their standard deviation before summing losses. (Another approach could be multiplying the sum of the losses with their entropy, do you have any idea which one would be preferrable?). I would welcome if someone can enlighten me about how I can implement this in Pytorch. I already know that I can do zero_grad and optimizer.step() for each epoch rather than each batch. Would it work if I write the batch losses into a Tensor and then do their normalization before summing them? Would it work if I use loss.backwards on the summed tensor before optimizer.step() at the end of an epoch? Thank you very much.</p><NewLine></div>",https://discuss.pytorch.org/u/Kamer_Ali_Yuksel,(Kamer Ali Yüksel),Kamer_Ali_Yuksel,"November 17, 2018, 12:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried the approach but this time it didn’t fit the memory. I guess that PyTorch accumulates all batch information when I do this way and that doesn’t have the possibility of fitting into memory. Does anybody have a better idea to accomplish this? Thanks.</p><NewLine><pre><code class=""lang-auto"">    scheduler.step()<NewLine>    optimizer.zero_grad()<NewLine>    losses = torch.zeros(len(train_loader),).float().cuda()<NewLine>    for i, (batch_features, _, _, batch_targets) in enumerate(train_loader):<NewLine><NewLine>        ...<NewLine>        outputs = model(features).float().cuda()<NewLine><NewLine>        batch_loss = custom_loss(outputs, batch_targets)<NewLine><NewLine>        losses[i] = batch_loss<NewLine>    entro = losses / losses.sum()<NewLine>    entro = (entro * torch.log(entro)).mean()<NewLine>    loss = losses.mean() * entro * -1.0<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Kamer_Ali_Yuksel; <NewLine> ,"REPLY_DATE 1: November 17, 2018,  1:31am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
29629,MultivariateNormal constructor with GPU tensors takes seconds to execute for large batch sizes,2018-11-15T02:03:50.543Z,0,243,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve noticed that the <code>MultivariateNormal</code> constructor can take seconds to execute for large batch sizes if the arguments are GPU tensors. For the following code snippet, just running the constructor takes 2.5 seconds on a Titan Xp and only 15 milliseconds on the CPU. If a policy needs to create a distribution every time we run the forward pass it can become a significant overhead. What kind of processing is going on in the constructor that might be affecting the execution time?</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import time<NewLine><NewLine>cuda = True # change this to False to see CPU time<NewLine><NewLine>if cuda:<NewLine>   device = torch.device(""cuda"")<NewLine>else:<NewLine>   device = torch.device(""cpu"")<NewLine><NewLine>batch_size = 1000<NewLine>event_size = 10<NewLine><NewLine>mean = torch.randn(batch_size, event_size, dtype=torch.float32)<NewLine>mean = mean.to(device)<NewLine><NewLine>covariance = torch.eye(event_size, dtype=torch.float32)<NewLine>covariance = covariance.unsqueeze(0).expand(batch_size, -1, -1)<NewLine>covariance = covariance.to(device)<NewLine><NewLine>if cuda:<NewLine>    torch.cuda.synchronize()<NewLine>t0 = time.time()<NewLine>torch.distributions.MultivariateNormal(mean, covariance)<NewLine>if cuda:<NewLine>    torch.cuda.synchronize()<NewLine>t1 = time.time()<NewLine>dt = t1 - t0<NewLine><NewLine>print(""MultivariateNormal constructor took {} seconds"".format(dt))<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/juniorrojas,(Junior Rojas),juniorrojas,"November 15, 2018,  2:03am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>For anybody else with a similar issue, creating the distribution by explicitly specifying <code>scale_tril</code> is much more efficient. The GPU version is still a bit slower than the CPU version, but not by much. The documentation says that if a covariance matrix is passed to create the distribution, the corresponding triangular matrices are computed with a Cholesky decomposition. My guess is that the internal Cholesky decomposition on GPU tensors is much slower than on CPU tensors for some reason.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/juniorrojas; <NewLine> ,"REPLY_DATE 1: November 15, 2018,  8:55pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
28853,Ideas for helping policy gradient converge,2018-11-05T23:54:08.003Z,0,167,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,<br/><NewLine>I’m working on a 2-state RL learning problem where the states are independent of any actions taken (and thus all rollouts can be done in batch).</p><NewLine><p>loss = -(log(A_p) *R).sum()</p><NewLine><p>I am able to come up with a simple supervised solution for the same problem, using a threshold, so I feel like it should be possible to get there with a stochastic policy gradient. Does anyone have ideas as to why the above might be the case, and potential tricks that might help convergence?</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/penguinshin,(Penguinshin),penguinshin,"November 14, 2018,  2:08am",,,,,
29190,How to convert softmax output to target suitable for MSELoss?,2018-11-09T15:12:01.274Z,0,282,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!<br/><NewLine>I am doing reinforcement learning. Started to write a technical specification but I am stuck with converting softmax output to target suitable for MSELoss(). So, softmax will give me a probability but I want to feed it to MSELoss in shape of [batch_size, *].<br/><NewLine>How can I do it?</p><NewLine><p><strong>EDIT:</strong><br/><NewLine>Found an answer: <a href=""https://discuss.pytorch.org/t/how-should-i-implement-cross-entropy-loss-with-continuous-target-outputs/10720/19"">How should I implement cross-entropy loss with continuous target outputs?</a> Sorry, but if you would like to add I will be happy to listen.</p><NewLine></div>",https://discuss.pytorch.org/u/andreiliphd,(Andrei Li),andreiliphd,"November 9, 2018,  3:19pm",,,,,
29149,Question on loss used in Vanilla REINFORCE implementation,2018-11-09T05:20:13.366Z,0,195,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was looking at the definition of the loss for REINFORCE from <a href=""https://rllab.readthedocs.io/en/latest/user/implement_algo_basic.html"" rel=""nofollow noopener"">https://rllab.readthedocs.io/en/latest/user/implement_algo_basic.html</a> which had it defined as:<br/><NewLine><img alt=""loss"" height=""29"" src=""https://discuss.pytorch.org/uploads/default/original/2X/2/2ee2244deb056a918e715af1a0bef0bf6b7fd8e0.png"" width=""289""/><br/><NewLine>I was wondering why in the example implementation (<a href=""https://github.com/pytorch/examples/blob/81f47e8ea49c74494d2aa8dc1c9c4ddc6c0eca73/reinforcement_learning/reinforce.py#L71"" rel=""nofollow noopener"">https://github.com/pytorch/examples/blob/81f47e8ea49c74494d2aa8dc1c9c4ddc6c0eca73/reinforcement_learning/reinforce.py#L71</a>) there is no division by the number of steps (T)?</p><NewLine></div>",https://discuss.pytorch.org/u/Oliver_Limoyo,(Oliver Limoyo),Oliver_Limoyo,"November 9, 2018,  5:24am",,,,,
29078,Can I backprop during one of output tensor detached or attached based on one boolean variable?,2018-11-08T11:01:48.454Z,0,144,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I am building a multi agent reinforcement learning using ddpg where each agent has parameterized action spaces. If some action is chosen, the other low level action parameters become useless.  In case of that, I don’t want policy network to be trained, so that I thought if the output value is detached, It won’t be trained. But DDPG is using sample batch. Is it possible to customize backward process by myself?</p><NewLine></div>",https://discuss.pytorch.org/u/Uk_Jo,(Uk Jo),Uk_Jo,"November 8, 2018, 11:01am",,,,,
28695,"In the official Q-Learning example, what does the env.unwrapped do exactly?",2018-11-03T19:45:45.059Z,0,3150,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I know this might be a gym question rather than a pytorch one, but the open ai forum is just somehow not available at the moment. Anyway, I hope that someone could kindly help me out with this. Thank you so much in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/deepbluesome,,deepbluesome,"November 3, 2018,  7:45pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can go through the code here:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/openai/gym/blob/master/gym/core.py"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/openai/gym/blob/master/gym/core.py"" target=""_blank"">openai/gym/blob/master/gym/core.py</a></h4><NewLine><pre><code class=""lang-py"">from gym import logger<NewLine><NewLine>import gym<NewLine>from gym import error<NewLine>from gym.utils import closer<NewLine><NewLine>env_closer = closer.Closer()<NewLine><NewLine># Env-related abstractions<NewLine><NewLine>class Env(object):<NewLine>    """"""The main OpenAI Gym class. It encapsulates an environment with<NewLine>    arbitrary behind-the-scenes dynamics. An environment can be<NewLine>    partially or fully observed.<NewLine><NewLine>    The main API methods that users of this class need to know are:<NewLine><NewLine>        step<NewLine>        reset<NewLine>        render<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/openai/gym/blob/master/gym/core.py"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>As far as I know,  there is a core super class called <code>gym.Env</code> and there are other sub classes of this to implement different environments (CartPoleEnv, MountainCarEnv etc). This <code>unwrapped</code> property is used to get the underlying <code>gym.Env</code> object from other environments.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The <code>unwrapped</code> just removes all the wrappers the environment instance has. In OpenAI Gym, you can specify wrapper around the environments in a hierarchical manner. For example, you could use a Monitor wrapper like this:</p><NewLine><pre><code class=""lang-auto"">    mdir = tempfile.mkdtemp()<NewLine>    env = gym.make(env_name)<NewLine>    env = wrappers.Monitor(env, mdir, force=True, mode=monitor_mode)<NewLine>    env.seed(seed)<NewLine></code></pre><NewLine><p>This wrapper allows you to monitor training and it attaches videos at the end of training that you could access like this:</p><NewLine><pre><code class=""lang-auto"">for video_path, meta_path in env.videos:<NewLine>    print(video_path, meta_path)<NewLine></code></pre><NewLine><p>But you could add other “layers” or “wrappers” to do different things. For instance, to limit the number of max time steps per episode, to stack a number of most recent observations/states/images (in the case of ATARI) of make palatalized instances of the environment (for A3C agents, for instance).</p><NewLine><p>The <code>unwrapped</code> call just removes all these layers and returns the raw/core environment.</p><NewLine><p>I think in the official example all the developer is trying to do is to remove the 200 time step limit the cart pole example defaults to. Check out these links for more information:</p><NewLine><p><a href=""https://github.com/openai/gym/blob/master/gym/envs/__init__.py#L53-L58"" rel=""nofollow noopener"">https://github.com/openai/gym/blob/master/gym/envs/<strong>init</strong>.py#L53-L58</a></p><NewLine><p><a href=""https://github.com/openai/gym/blob/master/gym/envs/registration.py#L49-L54"" rel=""nofollow noopener"">https://github.com/openai/gym/blob/master/gym/envs/registration.py#L49-L54</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/InnovArul; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mimoralea; <NewLine> ,"REPLY_DATE 1: November 3, 2018, 11:19pm; <NewLine> REPLY_DATE 2: November 7, 2018, 12:39am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
28753,Constant memory leak,2018-11-05T00:08:11.135Z,7,400,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Guys,</p><NewLine><p>I am experiencing memory leak in my PongDeterministicV4 PPO experiments. I can recreate the constant memory increasing issue with the half-way saved model easily, but I could not be able to locate the issue. With brand new start, the memory start to increase significantly after 100 episodes. I tried pympler tool and it looked to me that the memory used by other objects are ok.</p><NewLine><p>If you run the training with half-way saved model, you will be able to get 10GB memory usage within 15 episodes.</p><NewLine><p>The repository is here - <a href=""https://github.com/weicheng113/PongPPO"" rel=""nofollow noopener"">https://github.com/weicheng113/PongPPO</a><br/><NewLine>I will paste the screenshots next. I spent a week trying to locate the issue, but have not got any luck yet. Thanks in advance.</p><NewLine><p>Cheng</p><NewLine></div>",https://discuss.pytorch.org/u/cheng_wei,(Cheng Wei),cheng_wei,"November 5, 2018, 12:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/55be2dc881ce0473730540db6dcc38f33650a3c6"" href=""https://discuss.pytorch.org/uploads/default/original/2X/5/55be2dc881ce0473730540db6dcc38f33650a3c6.jpeg"" title=""leak1.jpg""><img alt=""leak1"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/55be2dc881ce0473730540db6dcc38f33650a3c6_2_10x10.png"" height=""463"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/55be2dc881ce0473730540db6dcc38f33650a3c6_2_690x463.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/55be2dc881ce0473730540db6dcc38f33650a3c6_2_690x463.jpeg, https://discuss.pytorch.org/uploads/default/optimized/2X/5/55be2dc881ce0473730540db6dcc38f33650a3c6_2_1035x694.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/5/55be2dc881ce0473730540db6dcc38f33650a3c6_2_1380x926.jpeg 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">leak1.jpg</span><span class=""informations"">2488×1672 607 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/5eb8e0d5fafc3134dfdf82ccc6e649fc4475c3ec"" href=""https://discuss.pytorch.org/uploads/default/original/2X/5/5eb8e0d5fafc3134dfdf82ccc6e649fc4475c3ec.jpeg"" title=""tensors.jpg""><img alt=""tensors"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/5eb8e0d5fafc3134dfdf82ccc6e649fc4475c3ec_2_10x10.png"" height=""437"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/5eb8e0d5fafc3134dfdf82ccc6e649fc4475c3ec_2_690x437.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/5eb8e0d5fafc3134dfdf82ccc6e649fc4475c3ec_2_690x437.jpeg, https://discuss.pytorch.org/uploads/default/optimized/2X/5/5eb8e0d5fafc3134dfdf82ccc6e649fc4475c3ec_2_1035x655.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/5/5eb8e0d5fafc3134dfdf82ccc6e649fc4475c3ec_2_1380x874.jpeg 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">tensors.jpg</span><span class=""informations"">1896×1202 177 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/6840878a12fadb8a9a1fc0df542d13c454a0dbae"" href=""https://discuss.pytorch.org/uploads/default/original/2X/6/6840878a12fadb8a9a1fc0df542d13c454a0dbae.jpeg"" title=""file structure.jpg""><img alt=""file%20structure"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/6/6840878a12fadb8a9a1fc0df542d13c454a0dbae_2_10x10.png"" height=""328"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/6/6840878a12fadb8a9a1fc0df542d13c454a0dbae_2_690x328.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/6/6840878a12fadb8a9a1fc0df542d13c454a0dbae_2_690x328.jpeg, https://discuss.pytorch.org/uploads/default/optimized/2X/6/6840878a12fadb8a9a1fc0df542d13c454a0dbae_2_1035x492.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/6/6840878a12fadb8a9a1fc0df542d13c454a0dbae_2_1380x656.jpeg 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">file structure.jpg</span><span class=""informations"">2242×1066 175 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/4194896ebf20cef2058be1a1858f4e2e4b0b5270"" href=""https://discuss.pytorch.org/uploads/default/original/2X/4/4194896ebf20cef2058be1a1858f4e2e4b0b5270.jpeg"" title=""memory consumption.jpg""><img alt=""memory%20consumption"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/4/4194896ebf20cef2058be1a1858f4e2e4b0b5270_2_10x10.png"" height=""499"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/4/4194896ebf20cef2058be1a1858f4e2e4b0b5270_2_497x499.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/4/4194896ebf20cef2058be1a1858f4e2e4b0b5270_2_497x499.jpeg, https://discuss.pytorch.org/uploads/default/optimized/2X/4/4194896ebf20cef2058be1a1858f4e2e4b0b5270_2_745x748.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/4/4194896ebf20cef2058be1a1858f4e2e4b0b5270_2_994x998.jpeg 2x"" width=""497""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">memory consumption.jpg</span><span class=""informations"">2286×2296 412 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just tried to train freshly and count different object increment. I can see the number of tensors increased significantly over time. The most use of tensors is in ppo_agent.py - <a href=""https://github.com/weicheng113/PongPPO/blob/master/ppo_agent.py"" rel=""nofollow noopener"">https://github.com/weicheng113/PongPPO/blob/master/ppo_agent.py</a>. It is a simple file, only containing a few lines of code. I could not see any suspicious mis-use of tensors.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/5f166874a6cfdee33ac6702e5e7150d8ecfad981"" href=""https://discuss.pytorch.org/uploads/default/original/2X/5/5f166874a6cfdee33ac6702e5e7150d8ecfad981.jpeg"" title=""image.jpg""><img alt=""image"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/5f166874a6cfdee33ac6702e5e7150d8ecfad981_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/5f166874a6cfdee33ac6702e5e7150d8ecfad981_2_506x500.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/5f166874a6cfdee33ac6702e5e7150d8ecfad981_2_506x500.jpeg, https://discuss.pytorch.org/uploads/default/optimized/2X/5/5f166874a6cfdee33ac6702e5e7150d8ecfad981_2_759x750.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/5/5f166874a6cfdee33ac6702e5e7150d8ecfad981_2_1012x1000.jpeg 2x"" width=""506""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.jpg</span><span class=""informations"">1326×1308 302 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/16ce53a26cfa5a61de381ee381958588e529ef1f"" href=""https://discuss.pytorch.org/uploads/default/original/2X/1/16ce53a26cfa5a61de381ee381958588e529ef1f.jpeg"" title=""image.jpg""><img alt=""image"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/1/16ce53a26cfa5a61de381ee381958588e529ef1f_2_10x10.png"" height=""499"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/1/16ce53a26cfa5a61de381ee381958588e529ef1f_2_512x499.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/1/16ce53a26cfa5a61de381ee381958588e529ef1f_2_512x499.jpeg, https://discuss.pytorch.org/uploads/default/optimized/2X/1/16ce53a26cfa5a61de381ee381958588e529ef1f_2_768x748.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/1/16ce53a26cfa5a61de381ee381958588e529ef1f_2_1024x998.jpeg 2x"" width=""512""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.jpg</span><span class=""informations"">1650×1610 369 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/0fa9d16bb11d9d3a4428f35dffbccaf3e5727392"" href=""https://discuss.pytorch.org/uploads/default/original/2X/0/0fa9d16bb11d9d3a4428f35dffbccaf3e5727392.jpeg"" title=""image.jpg""><img alt=""image"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/0/0fa9d16bb11d9d3a4428f35dffbccaf3e5727392_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/0/0fa9d16bb11d9d3a4428f35dffbccaf3e5727392_2_435x500.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/0/0fa9d16bb11d9d3a4428f35dffbccaf3e5727392_2_435x500.jpeg, https://discuss.pytorch.org/uploads/default/optimized/2X/0/0fa9d16bb11d9d3a4428f35dffbccaf3e5727392_2_652x750.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/0/0fa9d16bb11d9d3a4428f35dffbccaf3e5727392_2_870x1000.jpeg 2x"" width=""435""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.jpg</span><span class=""informations"">1322×1518 356 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not familiar with the code and I’m not sure, which method is called how often, but in the file you’ve linked there is a function which adds <code>tensors</code> to lists: <a href=""https://github.com/weicheng113/PongPPO/blob/7be735649db7425676244e81e1ce874ddf021c19/ppo_agent.py#L171"">line of code</a>.</p><NewLine><p>Could you check somehow, if and when the <code>clear</code> function is called?<br/><NewLine>If the tensors are added to the lists, your memory will grow continuously.</p><NewLine><p>It’s usually a good idea to post code directly, as this makes the search easier in the forum in case someone else has a similar issue. You can post code using three backticks ` <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=6"" title="":wink:""/></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>. I think I got it resolved. After several weeks battle, finally got it resolved.<img alt="":v:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/v.png?v=6"" title="":v:""/></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Good to hear!<br/><NewLine>What was the issue? Could you post a short description so that others won’t run into the same problems?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>, It was my mistake. I cut episode by t_max=1000. After some time, as the agent is getting more intelligent, they all don’t get done after 1000 steps. see the code comment on ‘if np.any(dones): // no one has dones after some time.’. So it will not enter learning step after time, and self.parallel_trajectory keeps accumulating.</p><NewLine><pre><code class=""lang-auto"">def step(self, i_episode, states, actions, action_probs, rewards, next_states, dones):<NewLine>        self.parallel_trajectory.add(<NewLine>            parallel_states=states,<NewLine>            parallel_actions=actions,<NewLine>            parallel_action_probs=action_probs,<NewLine>            parallel_rewards=rewards,<NewLine>            parallel_next_states=next_states,<NewLine>            parallel_dones=dones)<NewLine><NewLine>        if np.any(dones): **// no one has dones after some time.**<NewLine>            states, actions, action_probs, rewards, next_states, dones = self.parallel_trajectory.numpy()<NewLine>            returns = self.parallel_trajectory.discounted_returns(self.discount)<NewLine>            states_tensor, actions_tensor, action_probs_tensor, returns_tensor, next_states_tensor = self.to_tensor(<NewLine>                states=states,<NewLine>                actions=actions,<NewLine>                action_probs=action_probs,<NewLine>                returns=returns,<NewLine>                next_states=next_states)<NewLine>            self.learn(<NewLine>                states=states_tensor,<NewLine>                actions=actions_tensor,<NewLine>                action_probs=action_probs_tensor,<NewLine>                returns=returns_tensor,<NewLine>                next_states=next_states_tensor)<NewLine>            del self.parallel_trajectory<NewLine>self.parallel_trajectory = ParallelTrajectory(n=self.num_parallels)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/cheng_wei; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/cheng_wei; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/cheng_wei; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/cheng_wei; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/cheng_wei; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/cheng_wei; <NewLine> ,"REPLY_DATE 1: November 5, 2018, 12:10am; <NewLine> REPLY_DATE 2: November 5, 2018,  1:51am; <NewLine> REPLY_DATE 3: November 5, 2018,  1:37am; <NewLine> REPLY_DATE 4: November 5, 2018,  1:38am; <NewLine> REPLY_DATE 5: November 5, 2018, 10:40am; <NewLine> REPLY_DATE 6: November 5, 2018, 10:45am; <NewLine> REPLY_DATE 7: November 5, 2018, 10:47am; <NewLine> REPLY_DATE 8: November 5, 2018, 10:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
28065,Do we need to use off-policy methods for policy shaping?,2018-10-26T09:08:16.160Z,0,173,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Let’s say that there is human teacher that wants to manually modify the policy of the agent (policy shaping) to speed up the learning of the agent. Do I have to use off-policy methods or I can get away with on-policy? Why?</p><NewLine></div>",https://discuss.pytorch.org/u/stefa91,,stefa91,"October 26, 2018,  9:08am",,,,,
27613,Learning rate as a matrix,2018-10-19T16:11:44.693Z,0,215,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Greetings to all,<br/><NewLine>I have a question regarding modifying learning rates for each weight in a module.<br/><NewLine>Specifically, having the update rule:<br/><NewLine>W = W + lr * grad(W),<br/><NewLine>how can I make “lr” into a matrix of same dimensions as W, and perform a element-wise multiplication with the gradient? Can I do that to one of the Optimizer implementations in PyTorch?</p><NewLine><p>I don’t know if this question makes sense.</p><NewLine></div>",https://discuss.pytorch.org/u/calavera,(Djordje),calavera,"October 19, 2018,  4:12pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am afraid the provided optimzers won’t allow that.<br/><NewLine>I think the simplest is to reimplement whichever optimizer you want to use based on the original implementation like the one for sgd <a href=""https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py"">here</a>.<br/><NewLine>For sgd  it should be fairly simple, but I am not sure how adam is supposed to behave in that case.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: October 19, 2018,  4:19pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
27499,Simple policy gradient application - wrong learning,2018-10-18T06:01:31.136Z,0,203,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I’m trying to learn how to implement a simple RL approach in a toy example. The goal is to be able to generate a sequence of tokens (actions) = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. A state is the sequence of tokens generates so far, with 0 as padding. E.g., [2, 8, 3, 0, 0, 0, 0, 0, 0, 0] after the first 3 steps. I assume I can only get a reward for a complete sequence.</p><NewLine><p>The code is the following:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import numpy as np<NewLine>from torch.distributions import Categorical<NewLine><NewLine>SEQ_LEN = 10<NewLine>EPOCHS = 10000<NewLine>HIDDEN_SIZE = 50<NewLine>ROLLOUT_NUM = 10<NewLine>N_TOKENS = 11<NewLine>LR = 1e-3<NewLine>EPS = np.finfo(np.float32).eps.item()<NewLine><NewLine>model = nn.Sequential(nn.Linear(SEQ_LEN, HIDDEN_SIZE), nn.Linear(HIDDEN_SIZE, N_TOKENS), nn.LogSoftmax(-1))<NewLine>optimizer = torch.optim.Adam(model.parameters(), lr=LR)<NewLine><NewLine># Comute reward: +1 for each token in the correct position. [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] -&gt; RMAX<NewLine>def get_rw(beh):<NewLine>    r = 0.<NewLine>    for i in range(SEQ_LEN):<NewLine>        if beh[i] == i + 1:<NewLine>            r += 1<NewLine>        else:<NewLine>            break<NewLine>    return torch.tensor(r)<NewLine><NewLine>for epoch in range(EPOCHS):<NewLine>    # Initial state is [0, ..., 0]<NewLine>    rb = [0.] * SEQ_LEN<NewLine>    loss = 0.<NewLine>    rewards = []<NewLine>    l_probs = []<NewLine>    for c in range(SEQ_LEN):<NewLine>        # Process the state<NewLine>        out = model(torch.tensor(rb))<NewLine>        # Sample a token<NewLine>        cd = Categorical(logits=out)<NewLine>        r = cd.sample()<NewLine>        l_probs.append(cd.log_prob(r))<NewLine>        # Update the state representation<NewLine>        rb[c] = r.item()<NewLine>        with torch.no_grad():<NewLine>            # Perform rollouts from the current state<NewLine>            rew = 0.<NewLine>            for _ in range(ROLLOUT_NUM):<NewLine>                i_rb = list(rb)<NewLine>                # Produce tokens to complete the sequence<NewLine>                for u in range(c + 1, SEQ_LEN):<NewLine>                    i_out = model(torch.tensor(i_rb))<NewLine>                    i_cd = Categorical(logits=i_out)<NewLine>                    i_rb[u] = i_cd.sample().item()<NewLine>                # Reward for a complete rollout<NewLine>                rew += get_rw(i_rb)<NewLine>        rewards.append(rew / ROLLOUT_NUM)<NewLine>    rewards = torch.tensor(rewards)<NewLine>    rewards = (rewards - rewards.mean()) / (rewards.std() + EPS)<NewLine>    # Minimize the negative of average reward<NewLine>    loss = -(torch.stack(l_probs) * rewards).sum()<NewLine>    optimizer.zero_grad()<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine><NewLine>    # Produce a sequence to check<NewLine>    with torch.no_grad():<NewLine>        bh = [0.] * SEQ_LEN<NewLine>        for i in range(SEQ_LEN):<NewLine>            out = model(torch.tensor(bh))<NewLine>            cd = Categorical(logits=out)<NewLine>            r = cd.sample()<NewLine>            bh[i] = r.item()<NewLine>        print(bh)<NewLine></code></pre><NewLine><p>The problem is that with time the sequence produced will be constant, e.g., [6, 6, 6, 6, 6, 6, 6, 6, 6, 6].</p><NewLine><p>I tried different learning rates, network size etc… but the behavior doesn’t change. I guess I’m just missing some basic knowledge about the technique I’m trying to apply. Any help will be gratly appreciated.</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/SaricVr,,SaricVr,"October 18, 2018,  8:43am",,,,,
27409,Caffe2 runs already-trained SegNet?,2018-10-16T23:51:06.498Z,0,224,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have been training SegNet (<a href=""http://mi.eng.cam.ac.uk/projects/segnet/"" rel=""nofollow noopener"">http://mi.eng.cam.ac.uk/projects/segnet/</a>) on various models for literally weeks.  It’s been working fine so far but I now have the need for speed.  But I can’t afford to retrain for weeks.</p><NewLine><p>Will Caffe2 accept models and weights that were trained with SegNet?</p><NewLine></div>",https://discuss.pytorch.org/u/Kevin_Johnsrude,(Kevin Johnsrude),Kevin_Johnsrude,"October 16, 2018, 11:51pm",,,,,
26614,RuntimeError: invalid argument 4: Index tensor must have same dimensions as input tensor at,2018-10-05T20:11:44.324Z,1,4113,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Iam new to this forum and to pytorch.<br/><NewLine>Could someone help me with that error<br/><NewLine>RuntimeError: invalid argument 4: Index tensor must have same dimensions as input tensor at</p><NewLine><p>def learn(self, batch, gamma):<br/><NewLine>“”""Update value parameters using given batch of experience tuples.</p><NewLine><pre><code>    Params<NewLine>    ======<NewLine>        experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples <NewLine>        gamma (float): discount factor<NewLine>    """"""<NewLine>    <NewLine>    #states, actions, rewards, next_states, dones = experiences<NewLine>    states = np.array([each[0][0] for each in batch], ndmin=3)<NewLine>    actions = np.array([each[0][1] for each in batch])<NewLine>    rewards = np.array([each[0][2] for each in batch]) <NewLine>    next_states = np.array([each[0][3] for each in batch], ndmin=3)<NewLine><NewLine>    dones = np.array([each[0][4] for each in batch])<NewLine>    #dones  = np.array([map(lambda x: 1 if x else 0, dones)],dtype=np.float16)<NewLine>    dones = dones.astype(np.int16)<NewLine>    states = torch.from_numpy(states).float().cuda()<NewLine>    next_states = torch.from_numpy(next_states).float().cuda()<NewLine>    rewards = torch.from_numpy(rewards).float().cuda()<NewLine>    actions = torch.from_numpy(actions).long().cuda()<NewLine>    dones = torch.from_numpy(dones).float().cuda()<NewLine>    #next_states = torch.from_numpy(next_states).float().unsqueeze(0).cuda()<NewLine>    #print(states.shape)<NewLine>    states = states.view(32,8)<NewLine>    #print(states.shape)<NewLine>    next_states = next_states.view(32,8)<NewLine>    ## TODO: compute and minimize the loss<NewLine>    ""*** YOUR CODE HERE ***""<NewLine>    Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)<NewLine>    print(rewards.shape)<NewLine>    print(Q_targets_next)<NewLine>    Q_targets =  rewards + (gamma * Q_targets_next * (1-dones))<NewLine>    <NewLine>    Q_expected = self.qnetwork_local(states).gather(1, actions)<NewLine>    <NewLine>    loss = F.mse_loss(Q_expected, Q_targets)<NewLine>    self.optimizer.zero_grad()<NewLine>    loss.backward()<NewLine>    self.optimizer.step()<NewLine><NewLine>    # ------------------- update target network ------------------- #<NewLine>    self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau) <NewLine></code></pre><NewLine><p>I don’t know if this gather is correct here found the code online and it worked<br/><NewLine>now I try to modify to my problem</p><NewLine><p>Thank you very much for your help<br/><NewLine>If you need more information let me know<br/><NewLine>Best Regards<br/><NewLine>Chris</p><NewLine></div>",https://discuss.pytorch.org/u/Chris_Freiburg,(Chris ),Chris_Freiburg,"October 5, 2018,  8:11pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Which line throws this error?<br/><NewLine>Could you post the shapes and if necessary the values of all used tensors in this operation?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>oh yes I forgot to post the trace-back</p><NewLine><h2>/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:67: RuntimeWarning: divide by zero encountered in double_scalars</h2><NewLine><p>RuntimeError                              Traceback (most recent call last)<br/><NewLine> in ()<br/><NewLine>----&gt; 1 scores = dqn()<br/><NewLine>2<br/><NewLine>3 # plot the scores<br/><NewLine>4 fig = plt.figure()<br/><NewLine>5 ax = fig.add_subplot(111)</p><NewLine><p> in dqn(n_episodes, max_t, eps_start, eps_end, eps_decay)<br/><NewLine>23             score += reward<br/><NewLine>24             <span class=""hashtag"">#reward</span> = np.tanh(reward)<br/><NewLine>—&gt; 25             agent.step(state, action, reward, next_state, done)<br/><NewLine>26             state = next_state<br/><NewLine>27             if done:</p><NewLine><p> in step(self, state, action, reward, next_state, done)<br/><NewLine>57             if np.count_nonzero(self.memory.tree.tree) &gt; self.batch_size:<br/><NewLine>58                 tree_idx, batch, ISWeights_mb = self.memory.sample(BATCH_SIZE)<br/><NewLine>—&gt; 59                 self.learn(batch, GAMMA)<br/><NewLine>60<br/><NewLine>61     def act(self, state, eps=0.):</p><NewLine><p> in learn(self, batch, gamma)<br/><NewLine>114         Q_targets =  rewards + (gamma * Q_targets_next * (1-dones))<br/><NewLine>115<br/><NewLine>–&gt; 116         Q_expected = self.qnetwork_local(states).gather(1, actions)<br/><NewLine>117<br/><NewLine>118         loss = F.mse_loss(Q_expected, Q_targets)</p><NewLine><p>RuntimeError: invalid argument 4: Index tensor must have same dimensions as input tensor at /pytorch/aten/src/THC/generic/THCTensorScatterGather.cu:16</p><NewLine><p>Thank you for your help</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the stack trace. Could you print out the shapes of <code>self.qnetwork_local(states)</code> and <code>actions</code>? If might be <code>actions</code> has to be unsqueezed, but I would need to know your shapes.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>for actions : torch.Size([32])<br/><NewLine>for the network: torch.Size([32, 4])</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><hr/><NewLine><p>RuntimeError                              Traceback (most recent call last)<br/><NewLine> in ()<br/><NewLine>----&gt; 1 scores = dqn()<br/><NewLine>2<br/><NewLine>3 # plot the scores<br/><NewLine>4 fig = plt.figure()<br/><NewLine>5 ax = fig.add_subplot(111)</p><NewLine><p> in dqn(n_episodes, max_t, eps_start, eps_end, eps_decay)<br/><NewLine>23             score += reward<br/><NewLine>24             <span class=""hashtag"">#reward</span> = np.tanh(reward)<br/><NewLine>—&gt; 25             agent.step(state, action, reward, next_state, done)<br/><NewLine>26             state = next_state<br/><NewLine>27             if done:</p><NewLine><p> in step(self, state, action, reward, next_state, done)<br/><NewLine>57             if np.count_nonzero(self.memory.tree.tree) &gt; self.batch_size:<br/><NewLine>58                 tree_idx, batch, ISWeights_mb = self.memory.sample(BATCH_SIZE)<br/><NewLine>—&gt; 59                 self.learn(batch, GAMMA)<br/><NewLine>60<br/><NewLine>61     def act(self, state, eps=0.):</p><NewLine><p> in learn(self, batch, gamma)<br/><NewLine>120         Q_expected = self.qnetwork_local(states).gather(1, actions.unsqueeze(1))<br/><NewLine>121<br/><NewLine>–&gt; 122         loss = F.mse_loss(Q_expected, Q_targets)<br/><NewLine>123         self.optimizer.zero_grad()<br/><NewLine>124         loss.backward()</p><NewLine><p>/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py in mse_loss(input, target, size_average, reduce)<br/><NewLine>1567     “”""<br/><NewLine>1568     return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss,<br/><NewLine>-&gt; 1569                            input, target, size_average, reduce)<br/><NewLine>1570<br/><NewLine>1571</p><NewLine><p>/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py in _pointwise_loss(lambd, lambd_optimized, input, target, size_average, reduce)<br/><NewLine>1535         return torch.mean(d) if size_average else torch.sum(d)<br/><NewLine>1536     else:<br/><NewLine>-&gt; 1537         return lambd_optimized(input, target, size_average, reduce)<br/><NewLine>1538<br/><NewLine>1539</p><NewLine><p>RuntimeError: input and target shapes do not match: input [32 x 1], target [32 x 32] at /pytorch/aten/src/THCUNN/generic/MSECriterion.cu:15</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried and it worked but now a have a new problem<br/><NewLine>Is there any tutorial where they explain this stuff with unsqueeze etc , because I have no idea what I’m doing there OK I see they need to have the right shape but still</p><NewLine><p>Thanks for your help<br/><NewLine>Maybe you also have an idea for this one</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I finally managed, after a lot of reshaping the tensors it worked thanks for your help</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Chris_Freiburg; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Chris_Freiburg; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Chris_Freiburg; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Chris_Freiburg; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Chris_Freiburg; <NewLine> ,"REPLY_DATE 1: October 6, 2018,  4:10am; <NewLine> REPLY_DATE 2: October 6, 2018,  6:31am; <NewLine> REPLY_DATE 3: October 6, 2018,  4:31pm; <NewLine> REPLY_DATE 4: October 6, 2018,  5:09pm; <NewLine> REPLY_DATE 5: October 6, 2018,  5:30pm; <NewLine> REPLY_DATE 6: October 6, 2018,  5:32pm; <NewLine> REPLY_DATE 7: October 7, 2018,  7:07am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
26568,Dqn - memory leak (RAM keeps increasing),2018-10-05T04:52:19.760Z,0,433,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am implementing dqn with images as states…and for some reasons the RAM usage keeps increasing despite a fixed replay memory size, and after I delete the variables. Can anyone tell me what’s wrong? Thanks a lot!</p><NewLine><pre><code class=""lang-auto"">    def learn(self):<NewLine>        self.iter_counter += 1<NewLine>        if(len(self.memory) &lt; self.batch_size):<NewLine>            return<NewLine>        #Random transition batch is taken from experience replay memory<NewLine>        transitions = self.memory.sample(self.batch_size)<NewLine>        batch_state = []<NewLine>        batch_action = []<NewLine>        batch_reward = []<NewLine>        batch_state_next_state = []<NewLine>        batch_done = []<NewLine>        for t in transitions:<NewLine>            bs, ba, br, bsns, bd = t<NewLine>            batch_state.append(transform_img_for_model(bs))<NewLine>            batch_action.append(ba)<NewLine>            batch_reward.append(br)<NewLine>            batch_state_next_state.append(transform_img_for_model(bsns))<NewLine>            batch_done.append(bd)<NewLine><NewLine>        batch_state = Variable(torch.stack(batch_state).cuda(async=True))<NewLine>        batch_action = torch.FloatTensor(batch_action).unsqueeze_(0)<NewLine>        batch_action = batch_action.view(batch_action.size(1), -1)<NewLine>        batch_action = Variable(batch_action.cuda(async=True))<NewLine>        batch_reward = torch.FloatTensor(batch_reward).unsqueeze_(0)<NewLine>        batch_reward = batch_reward.view(batch_reward.size(1), -1)<NewLine>        batch_reward = Variable(batch_reward.cuda(async=True))<NewLine>        batch_next_state = Variable(torch.stack(batch_state_next_state).cuda(async=True))<NewLine><NewLine>        # current Q values are estimated by NN for all actions<NewLine>        current_q_values = self.evaluate_net(batch_state).gather(1, batch_action.long())<NewLine>        # expected Q values are estimated from actions which gives maximum Q value<NewLine>        max_next_q_values = self.target_net(batch_next_state).detach().max(1)[0]<NewLine>        max_next_q_values = max_next_q_values.unsqueeze_(0)<NewLine>        max_next_q_values = max_next_q_values.view(max_next_q_values.size(1), -1)<NewLine>        expected_q_values = batch_reward + (self.gamma * max_next_q_values)<NewLine><NewLine>        # loss is measured from error between current and newly expected Q values<NewLine>        loss = self.loss_function(current_q_values, expected_q_values)<NewLine><NewLine>        # backpropagation of loss to NN<NewLine>        self.optimizer.zero_grad()<NewLine>        loss.backward()<NewLine>        self.optimizer.step()<NewLine><NewLine>        #free variables<NewLine>        del batch_state, batch_action, batch_reward, batch_next_state, loss, transitions, current_q_values, max_next_q_values, expected_q_values<NewLine>        #for obj in gc.get_objects():<NewLine>        #    if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):<NewLine>        #        print(type(obj), obj.size())        <NewLine>        if(self.iter_counter % self.iter_update_target == 0):<NewLine>            self.target_net.load_state_dict(self.evaluate_net.state_dict())<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/soulless,,soulless,"October 5, 2018,  4:52am",1 Like,,,,
26462,Optimizer zero_grad() / step() only works outside of loop?,2018-10-03T11:41:48.559Z,0,195,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Here is a piece of code from my implementation of CartPole-v1.</p><NewLine><p>It works, but only when optimizer.zero_grad() and step() are on the outside of the loop, otherwize no learning. I don’t quite understand this behavior. I have seen them working inside the loop in the official tutorial, though that is not RL.</p><NewLine><blockquote><NewLine><pre><code>    self.optimizer.zero_grad()<NewLine>    for i in range(len(recorder.state_tape)):<NewLine>        state = recorder.state_tape[i]<NewLine>        action = recorder.action_tape[i]<NewLine>        reward = recorder.reward_tape[i]<NewLine><NewLine>        probs = model(state)<NewLine>        dist = Bernoulli(probs)<NewLine>        loss = -dist.log_prob(action) * reward  # use original action<NewLine>        loss.backward()<NewLine><NewLine>    self.optimizer.step()<NewLine></code></pre><NewLine></blockquote><NewLine><p>The whole program is here,</p><NewLine><p><a href=""https://gist.github.com/mtian2018/5dc5e69dda5666c4655676bac4dad996"" rel=""nofollow noopener"">https://gist.github.com/mtian2018/5dc5e69dda5666c4655676bac4dad996</a></p><NewLine></div>",https://discuss.pytorch.org/u/mtian2018,(Mtian2018),mtian2018,"October 3, 2018, 11:41am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>The difference between putting it inside or outside the loop is the difference between a batch of size 1 or <code>len(recorder.state_tape)</code>. So it is possible that in your case, this larger batch size is needed for the training to be stable and the model to learn?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: October 3, 2018,  4:22pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
26463,Categorical vs Bernoulli in solving CartPole,2018-10-03T11:51:12.965Z,0,230,"<div class=""post"" itemprop=""articleBody""><NewLine><p>This is related to the question I asked a short while ago, I thought it’s to much in one question.</p><NewLine><p>some code piece, the whole script is at the end. It works with a Bernoulli distribution. But when I tried with Categorical, changing network outputs to two and others, it doesn’t learn. Bernoulli still works, perhaps even better when I use the second item of the changed network as probability. It’s confusing to me.</p><NewLine><blockquote><NewLine><p>…<br/><NewLine>model = nn.Sequential(<br/><NewLine>nn.Linear(env.observation_space.shape[0], 24),<br/><NewLine>nn.ReLU(),<br/><NewLine>nn.Linear(24, 36),<br/><NewLine>nn.ReLU(),<br/><NewLine>nn.Linear(36, 1),<br/><NewLine>nn.Sigmoid(),<br/><NewLine>)<br/><NewLine>…<br/><NewLine>for step in range(1000):<br/><NewLine>state = torch.from_numpy(next_state).float()</p><NewLine><pre><code>        probs = model(state)<NewLine>        dist = Bernoulli(probs)<NewLine>        action = dist.sample()<NewLine><NewLine>        next_state, reward, done, __ = env.step(int(action.item()))<NewLine>        recorder.record(state, action, reward)<NewLine><NewLine>        if done:<NewLine>            break<NewLine></code></pre><NewLine></blockquote><NewLine><p>The whole script is here:<br/><NewLine><a href=""https://gist.github.com/mtian2018/5dc5e69dda5666c4655676bac4dad996"" rel=""nofollow noopener"">https://gist.github.com/mtian2018/5dc5e69dda5666c4655676bac4dad996</a></p><NewLine></div>",https://discuss.pytorch.org/u/mtian2018,(Mtian2018),mtian2018,"October 3, 2018, 11:51am",,,,,
25808,How to implement simple LSTM in reinforcement task (&lsquo;CartPole-v0&rsquo;),2018-09-24T05:41:34.186Z,0,898,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve realised I don’t understand LSTMs in Pytorch quite as well as I thought, so I’m adapting the CartPole demo from Soumith Chintala to give myself a simple challenge of switching the main Linear layer with an LSTM.</p><NewLine><p>The example here fails on the first pass with:<br/><NewLine><strong>RuntimeError: Input batch size 1 doesn’t match hidden[0] batch size 128</strong><br/><NewLine>on<br/><NewLine><strong>x, self.hidden = self.lstm(x, self.hidden)</strong></p><NewLine><p>Now if I change this line to<br/><NewLine><strong>x, _ = self.lstm(x, self.hidden)</strong></p><NewLine><p>… it converges and completes the task in a reasonable 950 Episodes.</p><NewLine><p>However, because it’s not feeding the Hidden states back if I do this (forgive my improvised terminology), it’s presumably not really taking advantage of the capabilities of the LSTM? I can’t quite sus the error either, as the self.hidden I’m feeding it is 1,128, and the self.hidden it’s outputting seems to be 1,128.</p><NewLine><p>I notice more advanced implementations of A3C models to play Atari games tend to use the hidden.values – up until now, I’ve really not used this aspect, generally using them x = self.lstm(x) … Which presumably is fine so long as all your work’s being done within batches?</p><NewLine><p>Thanks so much for any help. I’d also be interested in whether an LSTM (rather than LSTMCell) might be utilised for this problem? But I’m having separate problems getting the data the right shape.</p><NewLine><pre><code class=""lang-auto"">import argparse<NewLine>import gym<NewLine>import numpy as np<NewLine>from itertools import count<NewLine>from collections import namedtuple<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine>from torch.distributions import Categorical<NewLine><NewLine><NewLine>GAMMA = 0.99<NewLine>env = gym.make('CartPole-v0')<NewLine>env.seed(1)<NewLine>torch.manual_seed(1)<NewLine><NewLine>SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])<NewLine><NewLine><NewLine>class Policy(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Policy, self).__init__()<NewLine>        self.lstm = nn.LSTMCell(3, 128)<NewLine>        self.action_head = nn.Linear(128, 2)<NewLine>        self.value_head = nn.Linear(128, 1)<NewLine>        self.saved_actions = []<NewLine>        self.rewards = []<NewLine>        self.hidden = None<NewLine><NewLine><NewLine>    def forward(self, x):<NewLine>        x = x.unsqueeze(0)<NewLine>        x, self.hidden = self.lstm(x, self.hidden)<NewLine>        x = x.squeeze(0)<NewLine>        action_scores = self.action_head(x)<NewLine>        state_values = self.value_head(x)<NewLine>        return F.softmax(action_scores, dim=-1), state_values<NewLine><NewLine><NewLine>model = Policy()<NewLine>optimizer = optim.Adam(model.parameters(), lr=0.001)<NewLine>eps = np.finfo(np.float32).eps.item()<NewLine><NewLine><NewLine>def select_action(state):<NewLine>    state = torch.from_numpy(state).float()<NewLine>    probs, state_value = model(state.narrow(0,1,3))<NewLine>    m = Categorical(probs)<NewLine>    action = m.sample()<NewLine>    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))<NewLine>    return action.item()<NewLine><NewLine><NewLine>def finish_episode():<NewLine>    R = 0<NewLine>    saved_actions = model.saved_actions<NewLine>    policy_losses = []<NewLine>    value_losses = []<NewLine>    rewards = []<NewLine>    for r in model.rewards[::-1]:<NewLine>        R = r + GAMMA * R<NewLine>        rewards.insert(0, R)<NewLine>    rewards = torch.tensor(rewards)<NewLine>    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)<NewLine>    for (log_prob, value), r in zip(saved_actions, rewards):<NewLine>        reward = r - value.item()<NewLine>        policy_losses.append(-log_prob * reward)<NewLine>        value_losses.append(F.smooth_l1_loss(value, torch.tensor([r])))<NewLine>    optimizer.zero_grad()<NewLine>    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine>    del model.rewards[:]<NewLine>    del model.saved_actions[:]<NewLine><NewLine><NewLine>def main():<NewLine>    running_reward = 10<NewLine>    for i_episode in count(1):<NewLine>        state = env.reset()<NewLine>        for t in range(10000):<NewLine>            action = select_action(state)<NewLine>            state, reward, done, _ = env.step(action)<NewLine>            model.rewards.append(reward)<NewLine>            if done:<NewLine>                break<NewLine><NewLine>        running_reward = running_reward * 0.99 + t * 0.01<NewLine>        finish_episode()<NewLine>        if i_episode % 10 == 0:<NewLine>            print('Episode {}\tLast length: {:5d}\tAverage length: {:.2f}'.format(<NewLine>                i_episode, t, running_reward))<NewLine>        if running_reward &gt; env.spec.reward_threshold:<NewLine>            print(""Solved! Running reward is now {} and ""<NewLine>                  ""the last episode runs to {} time steps!"".format(running_reward, t))<NewLine>            for t in range(100000):<NewLine>                action = select_action(state)<NewLine>                state, reward, done, _ = env.step(action)<NewLine>                env.render()<NewLine>                model.rewards.append(reward)<NewLine>            # if done:<NewLine>            #     break<NewLine>            break<NewLine><NewLine>if __name__ == '__main__':<NewLine>    main()<NewLine>    env.env.close()<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Swift2046,(Benski),Swift2046,"September 24, 2018,  4:00pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Okay, I think I got it. This is the only way I learn.</p><NewLine><p>So I reset the Hidden states every time it’s done, and otherwise .detach() them. And it takes two or three times longer to train, and does a pretty horrible job – but presumably LSTMs aren’t very suited to this task.</p><NewLine><p>EDIT: one more update – completes the task around 1680 episodes on v1, a somewhat human-looking model here, with a natural wobble and some fairly dynamic rescues … unlike the machine-precision of usual solutions.</p><NewLine><pre><code class=""lang-auto"">import argparse<NewLine>import gym<NewLine>import numpy as np<NewLine>from itertools import count<NewLine>from collections import namedtuple<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine>from torch.distributions import Categorical<NewLine>from torch.autograd import Variable<NewLine><NewLine><NewLine>GAMMA = 0.99<NewLine>HIDDEN_SIZE = 64<NewLine>env = gym.make('CartPole-v1')<NewLine>env.seed(1)<NewLine>torch.manual_seed(1)<NewLine><NewLine>SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])<NewLine><NewLine><NewLine>class Policy(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Policy, self).__init__()<NewLine>        self.lstm = nn.LSTMCell(4, HIDDEN_SIZE)<NewLine>        # self.affine = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)<NewLine>        self.action_head = nn.Linear(HIDDEN_SIZE, 2)<NewLine>        self.value_head = nn.Linear(HIDDEN_SIZE, 1)<NewLine>        self.saved_actions = []<NewLine>        self.rewards = []<NewLine>        self.reset()<NewLine>        <NewLine>    def reset(self):<NewLine>        self.hidden = Variable(torch.zeros(1, HIDDEN_SIZE)), Variable(torch.zeros(1, HIDDEN_SIZE))<NewLine><NewLine>    def detach_weights(self):<NewLine>        self.hidden = self.hidden[0].detach(), self.hidden[1].detach()<NewLine><NewLine>    def forward(self, x):<NewLine>        x = x.unsqueeze(0)<NewLine>        self.hidden = self.lstm(x, self.hidden)<NewLine>        x = self.hidden[0]<NewLine>        x = x.squeeze(0)<NewLine>        # x = self.affine(x)<NewLine>        action_scores = self.action_head(x)<NewLine>        state_values = self.value_head(x)<NewLine>        return F.softmax(action_scores, dim=-1), state_values<NewLine><NewLine><NewLine>model = Policy()<NewLine>optimizer = optim.Adam(model.parameters(), lr=0.001)<NewLine>eps = np.finfo(np.float32).eps.item()<NewLine><NewLine><NewLine>def select_action(state):<NewLine>    state = torch.from_numpy(state).float()<NewLine>    probs, state_value = model(state)<NewLine>    m = Categorical(probs)<NewLine>    action = m.sample()<NewLine>    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))<NewLine>    return action.item()<NewLine><NewLine><NewLine>def finish_episode():<NewLine>    R = 0<NewLine>    saved_actions = model.saved_actions<NewLine>    policy_losses = []<NewLine>    value_losses = []<NewLine>    rewards = []<NewLine>    for r in model.rewards[::-1]:<NewLine>        R = r + GAMMA * R<NewLine>        rewards.insert(0, R)<NewLine>    rewards = torch.tensor(rewards)<NewLine>    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)<NewLine>    for (log_prob, value), r in zip(saved_actions, rewards):<NewLine>        reward = r - value.item()<NewLine>        policy_losses.append(-log_prob * reward)<NewLine>        value_losses.append(F.smooth_l1_loss(value, torch.tensor([r])))<NewLine>    optimizer.zero_grad()<NewLine>    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine>    del model.rewards[:]<NewLine>    del model.saved_actions[:]<NewLine><NewLine><NewLine>def main():<NewLine>    running_reward = 10<NewLine>    for i_episode in count(1):<NewLine>        state = env.reset()<NewLine>        for t in range(10000):<NewLine>            action = select_action(state)<NewLine>            state, reward, done, _ = env.step(action)<NewLine>            model.rewards.append(reward)<NewLine>            if done:<NewLine>                model.reset()<NewLine>                break<NewLine>            else:<NewLine>                model.detach_weights()<NewLine><NewLine>        running_reward = running_reward * 0.99 + t * 0.01<NewLine>        finish_episode()<NewLine>        if i_episode % 10 == 0:<NewLine>            print('Episode {}\tLast length: {:5d}\tAverage length: {:.2f}'.format(<NewLine>                i_episode, t, running_reward))<NewLine>        if running_reward &gt; env.spec.reward_threshold:<NewLine>            model.reset()<NewLine>            print(""Solved! Running reward is now {} and ""<NewLine>                  ""the last episode runs to {} time steps!"".format(running_reward, t))<NewLine>            for t in range(100000):<NewLine>                action = select_action(state)<NewLine>                state, reward, done, _ = env.step(action)<NewLine>                env.render()<NewLine>                model.rewards.append(reward)<NewLine>                # if done:<NewLine>                #     break<NewLine>            break<NewLine><NewLine>if __name__ == '__main__':<NewLine>    main()<NewLine>    env.env.close()<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Swift2046; <NewLine> ,"REPLY_DATE 1: September 25, 2018,  6:27am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> 
25827,[Solved] Pytorch 0.3.0 Adam Error: &lsquo;function&rsquo; object has no attribute &lsquo;parameters&rsquo;,2018-09-24T11:03:23.971Z,3,3583,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I started using pytorch recently, I’m forced to use version 0.3.0, I created a network that works on CPU, now I would like to try on GPU, I read in the documentation that I should use "" <code>model.to (device)</code> "", device for example cuda: 0, but this gave me an error "" <code>MyModel: object has no attribute to</code> "", so I tried like this:</p><NewLine><pre><code class=""lang-auto"">model = MyNet(N_CHANNEL, H_STATE, N_LAYERS, BIDIRECTIONAL, N_CATEGORIES).cuda<NewLine>optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)<NewLine></code></pre><NewLine><p>But I can not fix this error:<br/><NewLine><code>AttributeError: 'function' object has no attribute 'parameters'</code></p><NewLine><p>Can someone help me?<br/><NewLine>Thank you</p><NewLine></div>",https://discuss.pytorch.org/u/lucagto,(Luca Vasapolli),lucagto,"September 24, 2018,  2:19pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>.cuda()</code> is a function. Therefore you have to use parenthesis.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you so much, a simple error that was making me crazy.<br/><NewLine>Now the network starts on a TITAN PASCAL GPU, suggestions on the num_workers to be set in the Dataloader?<br/><NewLine>Thank you.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Good to hear!<br/><NewLine><code>num_workers</code> depends on your CPU and probably your i/o throughput. Start with the number of cores and try different values around it. The <a href=""https://github.com/pytorch/examples/blob/753d086ba76365612c3e8acc59f575f9ff11b15f/imagenet/main.py#L209"">Imagenet example</a> uses code to time the data loading time which might be useful for you.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok, thank you so much.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/lucagto; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/lucagto; <NewLine> ,"REPLY_DATE 1: September 24, 2018, 11:19am; <NewLine> REPLY_DATE 2: September 24, 2018,  1:43pm; <NewLine> REPLY_DATE 3: September 24, 2018,  1:58pm; <NewLine> REPLY_DATE 4: September 24, 2018,  2:18pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
25718,Vanilla REINFORCE for continuous distributions,2018-09-22T07:36:42.720Z,2,1077,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m writing a baseline for my model based on REINFORCE. That means I expect it not to work very well, which it makes it difficult check whether my implementation is correct. I tried this simple script to check that I’ve understood how to do REINFORCE in Pytorch.</p><NewLine><p>It trains an MLP to produce 4 simple curves (identity, square, cube and sin) on a 1D input. The output consists of 4 values (the means) and 4 variances, together making 4 1D Gaussians. I sample an output vector from this result, and apply REINFORCE to get a loss.</p><NewLine><p>My question is simply, is this the standard way to apply reinforce for Normal distributions, and to distribute the loss over the batch? It seems to work for this simple example, but I need to make sure that I’m not crippling my baseline by misunderstanding something.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch import nn<NewLine>from torch.autograd import Variable<NewLine>import torch.nn.functional as F<NewLine><NewLine>batch = 64<NewLine>iterations = 50000<NewLine><NewLine># Two layer MLP, producing means and sigmas for the output<NewLine>h = 128<NewLine>model = nn.Sequential(<NewLine>    nn.Linear(1, h), nn.Sigmoid(),<NewLine>    nn.Linear(h, 8)<NewLine>)<NewLine><NewLine>opt = torch.optim.Adam(model.parameters(), lr = 0.0005)<NewLine><NewLine>for i in range(iterations):<NewLine><NewLine>    x = torch.randn(batch, 1)<NewLine>    y = torch.cat([x, x ** 2, x ** 3, torch.sin(x)], dim=1)<NewLine><NewLine>    x, y = Variable(x), Variable(y)<NewLine><NewLine>    res = model(x)<NewLine>    means, sigs = res[:, :4], torch.exp(res[:, 4:])<NewLine><NewLine>    dists = torch.distributions.Normal(means, sigs)<NewLine>    samples = dists.sample()<NewLine><NewLine>    # REINFORCE<NewLine>    mloss = F.mse_loss(samples, y, reduce=False)<NewLine>    loss = - dists.log_prob(samples) * - mloss<NewLine>    loss = loss.mean()<NewLine><NewLine>    opt.zero_grad()<NewLine>    loss.backward()<NewLine>    opt.step()<NewLine><NewLine>    if i % 1000 == 0:<NewLine>        print('{: 6} grad'.format(i), list(model.parameters())[0].grad.mean())<NewLine>        print('      ', 'loss', F.mse_loss(samples.data, y.data, reduce=False).mean(dim=0))<NewLine>        print('      ', 'sigs', sigs.mean(dim=0))<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/pbloem,(Peter Bloem),pbloem,"September 23, 2018,  2:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I cannot recall what the REINFORCE is. Could you refer that method?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s the basic method behind policy gradient reinforcement learning. It’s referenced in the docs here:<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/docs/stable/distributions.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/stable/distributions.html</a><br/><NewLine>This blogpost provides a more elaborate explanation:<br/><NewLine><a class=""onebox"" href=""http://karpathy.github.io/2016/05/31/rl/"" rel=""nofollow noopener"" target=""_blank"">http://karpathy.github.io/2016/05/31/rl/</a></p><NewLine><p>It’s normally used in explicit reinforcement learning settings, when your network produces a distribution over possible actions and the environment provides you with a reward for the action. This is also how it is referenced in the docs. However, it can be used for any model that uses some non-differentiable sampling step. The principle is generalized in <a href=""https://arxiv.org/pdf/1506.05254"" rel=""nofollow noopener"">this paper</a>.</p><NewLine><p>My example is a little artificial: the model produces a distribution on the outputs and then samples from that distribution, computes a loss on the sample, and then estimates a gradient on that sample using REINFORCE.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thx for referring. Could you define your problem with normal RL ways. I cannot figure out which are the state, action, reward in your example.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you were to map this to an RL setting, then an “action” consists of choosing a real-valued 4D vector. The model produces a distribution over these actions in the form of 4 independent 1D Normal distributions, from which a single action is sampled.</p><NewLine><p>The “reward” is just the negative of the loss, which is the MSE between the target (y) and the sampled action (samples).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/LinjX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pbloem; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/LinjX; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/pbloem; <NewLine> ,"REPLY_DATE 1: September 22, 2018,  8:47am; <NewLine> REPLY_DATE 2: September 22, 2018,  8:59am; <NewLine> REPLY_DATE 3: September 23, 2018,  3:45am; <NewLine> REPLY_DATE 4: September 23, 2018,  4:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
25195,Question regarding sampling of Transition pairs in DQN tutorial,2018-09-14T15:12:19.061Z,0,271,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Continuing the discussion from <a href=""https://discuss.pytorch.org/t/reinforcement-learning-dqn-tutorial-bugs/15722/4"">Reinforcement Learning (DQN) tutorial bugs</a>:</p><NewLine><p>What will happen if in case all sampled states have next state as None</p><NewLine><pre><code>if len(memory) &lt; BATCH_SIZE:<NewLine>        return<NewLine>transitions = memory.sample(BATCH_SIZE)<NewLine>batch = Transition(*zip(*transitions))<NewLine><NewLine># Compute a mask of non-final states and concatenate the batch elements<NewLine>non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,<NewLine>                                      batch.next_state)), device=device, dtype=torch.uint8)<NewLine>non_final_next_states = torch.cat([s for s in batch.next_state<NewLine>                                            if s is not None])<NewLine>state_batch = torch.cat(batch.state)<NewLine>action_batch = torch.cat(batch.action)<NewLine>reward_batch = torch.cat(batch.reward)<NewLine></code></pre><NewLine><p>Is this is a bug?</p><NewLine></div>",https://discuss.pytorch.org/u/pjavia,(Perikumar Javia),pjavia,"September 14, 2018,  3:12pm",,,,,
24343,Simple question about loss.backward(),2018-09-02T03:55:00.610Z,0,277,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to create a version of A3C reinforcement learning in caffe and have a question on how ‘loss.backward()’ works.</p><NewLine><p>In caffe (for which I am familiar with), the loss value is calculated, placed in a variable such as a ‘double’ or ‘float’ and then used to calculate the gradient diffs within each layer’s backward function all of which are then applied by the optimizer used which later applies the diffs to the various learnable blobs based on the learning rate, decay etc.</p><NewLine><p>How does this work in pytorch?  For example, I see in numerous A3C examples how the loss is calculated, but what occurs when the call to ‘loss.backward()’ is made?</p><NewLine><p>Does the ‘loss.backward()’ function apply the loss ‘value’ to each layer within the model defined much in the same way that caffe does or is there an extra bit of magic that I’m missing?</p><NewLine><p>Any comments are appreciated.</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/ZoroDerVonCodier,,ZoroDerVonCodier,"September 2, 2018,  3:55am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Basically, the <code>backward</code> call evaluates the graph built in the forward pass and computes the gradients using the chain rule.<br/><NewLine>You can find more information <a href=""https://pytorch.org/docs/stable/notes/autograd.html"">here</a>.</p><NewLine><p>Note that in PyTorch the gradients will be accumulated, so usually you want to zero them again before calling the next backward pass.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 2, 2018,  1:04pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
16838,VAE- Gumbel Softmax,2018-04-23T14:35:23.539Z,0,3473,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I implemented a gumbel-softmax based variational autoencoder following the tensorflow implementation here (<a href=""https://github.com/ericjang/gumbel-softmax/blob/master/Categorical%20VAE.ipynb"" rel=""nofollow noopener"">https://github.com/ericjang/gumbel-softmax/blob/master/Categorical%20VAE.ipynb</a>). The code appears to work, however the convergence is much slower than with TensorFlow, using the same optimizer (Adam) and learning rate. For instance TensorFlow has already converged after 5000 iterations, whereas my implementation converges much more slowly. The initial value of the loss is almost identical across both implementations, suggesting that my implementation is broadly correct. I also explicitly calculate the binary cross entropy for the decoder to verify that the Bernoulli implementation in the distributions library is correct. Code is attached below, along with the change of loss after 1000 iterations.</p><NewLine><p>I disabled learning rate adjustments and temperature annealing in TF and my implementation to keep things simple. Same annealing temperature was used in both implementations. I also verified that both mnist data used in both implementations is un-normalized floating point data between 0-1.</p><NewLine><p>Welcome any thoughts/suggestions!</p><NewLine><p>Code:</p><NewLine><pre><code class=""lang-auto""><NewLine>from __future__ import print_function<NewLine>import argparse<NewLine>import torch<NewLine>import torch.utils.data<NewLine>from torch import nn, optim<NewLine>from torch.autograd import Variable<NewLine>from torch.nn import functional as F<NewLine>from torchvision import datasets, transforms<NewLine>from torchvision.utils import save_image<NewLine>import numpy as np<NewLine>import matplotlib.pyplot as plt<NewLine><NewLine>parser = argparse.ArgumentParser(description='VAE MNIST Example')<NewLine>parser.add_argument('--batch-size', type=int, default=100, metavar='N',<NewLine>                    help='input batch size for training (default: 128)')<NewLine>parser.add_argument('--epochs', type=int, default=10, metavar='N',<NewLine>                    help='number of epochs to train (default: 10)')<NewLine>parser.add_argument('--no-cuda', action='store_true', default=False,<NewLine>                    help='enables CUDA training')<NewLine>parser.add_argument('--seed', type=int, default=1, metavar='S',<NewLine>                    help='random seed (default: 1)')<NewLine>parser.add_argument('--log-interval', type=int, default=10, metavar='N',<NewLine>                    help='how many batches to wait before logging training status')<NewLine>args = parser.parse_args()<NewLine>args.cuda = not args.no_cuda and torch.cuda.is_available()<NewLine><NewLine>torch.manual_seed(args.seed)<NewLine>if args.cuda:<NewLine>    torch.cuda.manual_seed(args.seed)<NewLine><NewLine>kwargs = {'num_workers': 0, 'pin_memory': True} if args.cuda else {}<NewLine>train_loader = torch.utils.data.DataLoader(<NewLine>    datasets.MNIST('../data', train=True, download=True,<NewLine>                   transform=transforms.ToTensor()),<NewLine>    batch_size=args.batch_size, shuffle=True, **kwargs)<NewLine>test_loader = torch.utils.data.DataLoader(<NewLine>    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),<NewLine>    batch_size=args.batch_size, shuffle=True, **kwargs)<NewLine><NewLine>K=10 # number of classes<NewLine>N=30 # number of categorical distributions<NewLine>tau0 = 1.0<NewLine>tau = tau0<NewLine>tau = Variable(torch.tensor(tau))<NewLine>def sample_gumbel(shape, eps=1e-20):<NewLine>    U = torch.Tensor(shape).uniform_(0,1).cuda()<NewLine>    return -(torch.log(-torch.log(U + eps) + eps))<NewLine><NewLine>def gumbel_softmax_sample(logits, temperature):<NewLine>    y = logits + sample_gumbel(logits.size())<NewLine>    return F.softmax(y / temperature, dim=-1)<NewLine><NewLine>def gumbel_softmax(logits, temperature, hard=False):<NewLine>    """"""<NewLine>    input: [*, n_class]<NewLine>    return: [*, n_class] an one-hot vector<NewLine>    """"""<NewLine>    y = gumbel_softmax_sample(logits, temperature)<NewLine>    if hard:<NewLine>        shape = y.size()<NewLine>        _, ind = y.max(dim=-1)<NewLine>        y_hard = torch.zeros_like(y).view(-1, shape[-1])<NewLine>        y_hard.scatter_(1, ind.view(-1, 1), 1)<NewLine>        y_hard = y_hard.view(*shape)<NewLine>        y = (y_hard - y).detach() + y<NewLine>    return y<NewLine><NewLine>class VAE(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(VAE, self).__init__()<NewLine><NewLine>        self.fce1 = nn.Linear(784, 512)<NewLine>        self.fce2 = nn.Linear(512, 256)<NewLine>        self.fce3 = nn.Linear(256, K*N)<NewLine>        self.fcd1 = nn.Linear(K*N, 256)<NewLine>        self.fcd2 = nn.Linear(256, 512)<NewLine>        self.fcd3 = nn.Linear(512, 784)<NewLine><NewLine>        self.relu = nn.ReLU()<NewLine>        self.sigmoid = nn.Sigmoid()<NewLine>        self.softmax = nn.Softmax(1)<NewLine><NewLine>    def encode(self, x):<NewLine>        he1 = self.relu(self.fce1(x))<NewLine>        he2 = self.relu(self.fce2(he1))<NewLine>        he3 = self.fce3(he2)<NewLine>        logits_y = he3.view(-1, K)<NewLine>        qy = self.softmax(logits_y)<NewLine>        log_qy = torch.log(qy + 1e-20)<NewLine>        return logits_y, log_qy, qy<NewLine><NewLine>    def reparameterize(self, mu, logvar):<NewLine>        if self.training:<NewLine>            std = logvar.mul(0.5).exp_()<NewLine>            eps = Variable(std.data.new(std.size()).normal_())<NewLine>            return eps.mul(std).add_(mu)<NewLine>        else:<NewLine>            return mu<NewLine><NewLine>    def decode(self, z):<NewLine>        # sample and reshape back (shape=(batch_size,N,K))<NewLine>        # set hard=True for ST Gumbel-Softmax<NewLine>        ge = gumbel_softmax(z, tau, hard=False).view(-1, N, K)<NewLine>        hd1 = self.relu(self.fcd1(ge.view(-1, N*K)))<NewLine>        hd2 = self.relu(self.fcd2(hd1))<NewLine>        hd3 = self.fcd3(hd2)<NewLine>        return hd3<NewLine><NewLine>    def forward(self, x):<NewLine>        logits_y, log_qy, qy = self.encode(x.view(-1, 784))<NewLine>        return self.decode(logits_y),log_qy, qy<NewLine><NewLine><NewLine>model = VAE()<NewLine>if args.cuda:<NewLine>    model.cuda()<NewLine>optimizer = optim.Adam(model.parameters(), lr=1e-3)<NewLine><NewLine><NewLine># Reconstruction + KL divergence losses summed over all elements and batch<NewLine>def loss_function(recon_x, log_qy, qy, data):<NewLine>    sigmoid = nn.Sigmoid()<NewLine>    kl_tmp = (qy * (log_qy - torch.log(torch.tensor(1.0 / K)))).view(-1, N, K)<NewLine>    KL = torch.sum(torch.sum(kl_tmp, 2),1)<NewLine>    shape = data.size()<NewLine>    #elbo = torch.sum(recon_x.log_prob(data.view(shape[0], shape[1] * shape[2] * shape[3])), 1) - KL<NewLine>    data_ = data.view(shape[0], shape[1] * shape[2] * shape[3])<NewLine>    # calculate binary cross entropy using explicit calculation rather than using pytorch distribution API<NewLine>    bce = torch.sum(data_ * torch.log(sigmoid(recon_x)) + (1 - data_) * torch.log(1 - sigmoid(recon_x)), 1)<NewLine>    elbo = bce - KL<NewLine>    return torch.mean(-elbo), torch.mean(bce), torch.mean(KL)<NewLine><NewLine>ANNEAL_RATE=0.00003<NewLine>MIN_TEMP=0.5<NewLine><NewLine>def train(epoch):<NewLine>    model.train()<NewLine>    train_loss = 0<NewLine><NewLine>    for batch_idx, (data, _) in enumerate(train_loader):<NewLine>        data = Variable(data)<NewLine>        if args.cuda:<NewLine>            data = data.cuda()<NewLine>        optimizer.zero_grad()<NewLine>        px, log_qy, qy = model(data)<NewLine>        recon_x = torch.distributions.bernoulli.Bernoulli(logits=px)<NewLine>        #loss = loss_function(recon_x, log_qy, qy, data)<NewLine>        loss, bce, KL = loss_function(px, log_qy, qy, data)<NewLine>        loss.backward()<NewLine>        train_loss += loss.data[0]<NewLine>        optimizer.step()<NewLine>        #if batch_idx % 1000 == 1:<NewLine>        #    tau = Variable(torch.tensor(np.maximum(tau0 * np.exp(-ANNEAL_RATE * batch_idx), MIN_TEMP)))<NewLine><NewLine>        if batch_idx % args.log_interval == 0:<NewLine>            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f} \tBCE: {:.6f} \tKL: {:.6f}'.format(<NewLine>                epoch, batch_idx * len(data), len(train_loader.dataset),<NewLine>                       100. * batch_idx / len(train_loader),<NewLine>                       loss.data[0], bce.data[0], KL.data[0]))<NewLine><NewLine>    print('====&gt; Epoch: {} Average loss: {:.4f}'.format(<NewLine>        epoch, train_loss / len(train_loader.dataset)))<NewLine><NewLine>    M = 100 * N<NewLine>    np_y = np.zeros((M, K))<NewLine>    np_y[range(M), np.random.choice(K, M)] = 1<NewLine>    np_y = np.reshape(np_y, [100, N, K])<NewLine><NewLine>    px = model.decode(Variable(torch.tensor(np_y).cuda()))<NewLine>    recon_x = torch.nn.Sigmoid()(px).detach().cpu().numpy()<NewLine>    #recon_x = torch.distributions.Bernoulli(logits=px).sample()<NewLine>    np_x = recon_x.reshape((10, 10, 28, 28))<NewLine>    # split into 10 (1,10,28,28) images, concat along columns -&gt; 1,10,28,280<NewLine>    np_x = np.concatenate(np.split(np_x, 10, axis=0), axis=3)<NewLine>    # split into 10 (1,1,28,280) images, concat along rows -&gt; 1,1,280,280<NewLine>    np_x = np.concatenate(np.split(np_x, 10, axis=1), axis=2)<NewLine>    x_img = np.squeeze(np_x)<NewLine>    plt.imshow(x_img, cmap=plt.cm.gray, interpolation='none')<NewLine>    plt.show()<NewLine><NewLine>args.epochs = 1<NewLine>for epoch in range(1, args.epochs + 1):<NewLine>    train(epoch)<NewLine><NewLine></code></pre><NewLine><p>Convergence (My Code):<br/><NewLine>|Train Epoch: 1 [0/60000 (0%)]|Loss: 542.632080 |BCE: -542.598450 |KL: 0.033671|<br/><NewLine>|Train Epoch: 1 [1000/60000 (2%)]|Loss: 273.803497 |BCE: -264.880951 |KL: 8.922541|<br/><NewLine>|Train Epoch: 1 [2000/60000 (3%)]|Loss: 213.316895 |BCE: -213.277115 |KL: 0.039777|<br/><NewLine>|Train Epoch: 1 [3000/60000 (5%)]|Loss: 215.961517 |BCE: -215.944489 |KL: 0.017020|<br/><NewLine>|Train Epoch: 1 [4000/60000 (7%)]|Loss: 208.288528 |BCE: -208.240768 |KL: 0.047799|<br/><NewLine>|Train Epoch: 1 [5000/60000 (8%)]|Loss: 203.074173 |BCE: -203.020462 |KL: 0.053740|</p><NewLine><p>Convergence (TensorFlow)<br/><NewLine>Step 1, ELBO: -544.939, KL: 0.540, BCE: -544.399<br/><NewLine>Step 1001, ELBO: -136.944, KL: 11.626, BCE: -125.318<br/><NewLine>Step 2001, ELBO: -121.850, KL: 13.688, BCE: -108.161<br/><NewLine>Step 3001, ELBO: -108.513, KL: 14.693, BCE: -93.820<br/><NewLine>Step 4001, ELBO: -110.518, KL: 15.602, BCE: -94.916</p><NewLine></div>",https://discuss.pytorch.org/u/ankur6ue,(Ankur Mohan),ankur6ue,"August 31, 2018, 12:08am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you paste reformatted code? It is a headache for me to  re-arrange your code.<br/><NewLine>Have a look at <a href=""https://github.com/YongfeiYan/Gumbel_Softmax_VAE"" rel=""nofollow noopener"">this implementation</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Yongfei_Yan; <NewLine> ,"REPLY_DATE 1: August 30, 2018,  4:54am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
23935,Best pytorch RL GitHub on image pixels,2018-08-26T23:37:54.945Z,0,779,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Unfortunately, most of the best GitHub repos I’ve found for RL have been with tensorflow. The ones for pytorch that I’ve seen are mediocre at best (in terms of speed, accuracy and code readibility).</p><NewLine><p>What’s the best repo you’ve seen (best in terms of perform using RL algorithms that consume images)? Added bonus if: organized code, prioritized experience replay, and I also like: actor critic, ddpg, dqn agents.</p><NewLine></div>",https://discuss.pytorch.org/u/Ranahanocka,(Rana Hanocka),Ranahanocka,"August 26, 2018, 11:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The best I know so far: <a href=""https://github.com/ikostrikov/pytorch-a2c-ppo-acktr"">https://github.com/ikostrikov/pytorch-a2c-ppo-acktr</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Feel free to check out my code: <a href=""https://github.com/MillionIntegrals/vel"" rel=""nofollow noopener"">https://github.com/MillionIntegrals/vel</a><br/><NewLine>I’ve tried to make it fast, accurate and readable <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=6"" title="":wink:""/></p><NewLine><p>If you have any questions, feel free to ask, open an issue etc.<br/><NewLine>Not all the methods are implemented yet, but I’m aiming for feature parity with openai baselines, if I have some time I’ll finish implementing ACER this weekend.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jrx; <NewLine> ,"REPLY_DATE 1: August 27, 2018,  2:18am; <NewLine> REPLY_DATE 2: August 29, 2018,  1:13pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
21825,Replay buffer with policy gradient,2018-07-27T09:37:06.457Z,0,882,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’d like to ask a simple question regarding a training based on policy gradient and experience replay.</p><NewLine><p>When I am collecting experiences by interacting with an environment with a policy approximated with neural networks, should I turn off history tracking with no_grad or should I leave it and let it be tracked?</p><NewLine><p>I think we need to track only when we replay the buffer containing experience trajectories but I am not so sure.</p><NewLine><p>I’ve looked up a few code snippets and all of them didn’t turn off history tracking when they collect experience trajectories to store in the replay buffer. Then all the accumulated operations will be back propagated along with arbitrary many sampled  trajectories to play out as many times as we wish. To me only sampled trajectories are to be used for training but not the ones used to collect for replay.</p><NewLine><p>Hope you got to what I asking with my description. I am new to PyTorch and feel free to correct me if get anything wrong.</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/MirKwon,(Mir Kwon),MirKwon,"July 27, 2018,  9:37am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Not sure I can speak for which repos you are referring to, but if you look at the official DQN tutorial:</p><NewLine><p><a class=""onebox"" href=""https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html</a></p><NewLine><p>you can see that select action (what is used during the collection of experience tuples), is using <code>with torch.no_grad()</code></p><NewLine><pre><code class=""lang-auto"">def select_action(state):<NewLine>    global steps_done<NewLine>    sample = random.random()<NewLine>    eps_threshold = EPS_END + (EPS_START - EPS_END) * \<NewLine>        math.exp(-1. * steps_done / EPS_DECAY)<NewLine>    steps_done += 1<NewLine>    if sample &gt; eps_threshold:<NewLine>        with torch.no_grad():<NewLine>            return policy_net(state).max(1)[1].view(1, 1)<NewLine>    else:<NewLine>        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Ranahanocka; <NewLine> ,"REPLY_DATE 1: August 29, 2018,  4:19am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> 
17955,DQN official tutorial,2018-05-11T21:22:50.637Z,0,676,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Why do we need non_final_mask? Why is the mask a tuple instead of a list?<br/><NewLine><a href=""http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html</a></p><NewLine><p>What’s the difference between non_final_mask and non_final_next_states?<br/><NewLine>size of non_final_mask  torch.Size([20])<br/><NewLine>size of non_final_next_states  torch.Size([20, 3, 40, 80])</p><NewLine><pre><code class=""lang-auto"">def optimize_model():<NewLine>    if len(memory) &lt; BATCH_SIZE:<NewLine>        return<NewLine>    transitions = memory.sample(BATCH_SIZE)<NewLine>    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for<NewLine>    # detailed explanation).<NewLine>    batch = Transition(*zip(*transitions))<NewLine>  # Compute a mask of non-final states and concatenate the batch elements<NewLine>    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,<NewLine>                                          batch.next_state)), device=device, dtype=torch.uint8)<NewLine>    non_final_next_states = torch.cat([s for s in batch.next_state<NewLine>                                                if s is not None])<NewLine>    state_batch = torch.cat(batch.state)<NewLine>    action_batch = torch.cat(batch.action)<NewLine>    reward_batch = torch.cat(batch.reward)<NewLine><NewLine>    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the<NewLine>    # columns of actions taken<NewLine>    state_action_values = policy_net(state_batch).gather(1, action_batch)<NewLine><NewLine>    # Compute V(s_{t+1}) for all next states.<NewLine>    next_state_values = torch.zeros(BATCH_SIZE, device=device)<NewLine>    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()<NewLine>    # Compute the expected Q values<NewLine>    expected_state_action_values = (next_state_values * GAMMA) + reward_batch<NewLine><NewLine>    # Compute Huber loss<NewLine>    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))<NewLine><NewLine>    # Optimize the model<NewLine>    optimizer.zero_grad()<NewLine>    loss.backward()<NewLine>    for param in policy_net.parameters():<NewLine>        param.grad.data.clamp_(-1, 1)<NewLine>    optimizer.step()<NewLine>``</code></pre><NewLine></div>",https://discuss.pytorch.org/u/optimal_minimal,,optimal_minimal,"May 11, 2018,  9:29pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>non-final mask are for states (s) whose next state (s’) are <em>not</em> going into final state.<br/><NewLine>since V(s’) = 0, the next_state_values remain zeros from the initialization</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""17955""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/optimal_minimal/40/4712_1.png"" width=""20""/> optimal_minimal:</div><NewLine><blockquote><NewLine><p>next_state_values = torch.zeros(BATCH_SIZE, device=device)</p><NewLine></blockquote><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Ranahanocka; <NewLine> ,"REPLY_DATE 1: August 29, 2018,  3:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
24033,ERROR: wc-&gt;status == IBV_WC_SUCCESS. 12 vs 0. Memory region send for slot 0: transport retry counter exceeded,2018-08-28T08:38:58.491Z,0,221,"<div class=""post"" itemprop=""articleBody""><NewLine><p>pytorch : 0.4.1<br/><NewLine>python : 3.6<br/><NewLine>conda env<br/><NewLine>I install from source, and use gloo mode, 4 GPUs in one node, but get error, but use 1 GPU is ok, and nccl mode with 4 gpus is ok too.<br/><NewLine>error:</p><NewLine><pre><code class=""lang-auto"">terminate called after throwing an instance of 'gloo::EnforceNotMet'<NewLine>  what():  [enforce fail at /home/zhoushengkai/code/pytorch_v0.4.1_for_whl/third_party/gloo/gloo/transport/ibverbs/pair.cc:462] wc-&gt;status == IBV_WC_SUCCESS. 12 vs 0. Memory region send for slot 0: transport retry counter exceeded<NewLine>terminate called after throwing an instance of 'gloo::EnforceNotMet'<NewLine>  what():  [enforce fail at /home/zhoushengkai/code/pytorch_v0.4.1_for_whl/third_party/gloo/gloo/transport/ibverbs/pair.cc:462] wc-&gt;status == IBV_WC_SUCCESS. 12 vs 0. Memory region send for slot 0: transport retry counter exceeded<NewLine>terminate called after throwing an instance of 'gloo::EnforceNotMet'<NewLine>  what():  [enforce fail at /home/zhoushengkai/code/pytorch_v0.4.1_for_whl/third_party/gloo/gloo/transport/ibverbs/pair.cc:462] wc-&gt;status == IBV_WC_SUCCESS. 12 vs 0. Memory region send for slot 0: transport retry counter exceeded<NewLine>terminate called after throwing an instance of 'gloo::EnforceNotMet'<NewLine>  what():  [enforce fail at /home/zhoushengkai/code/pytorch_v0.4.1_for_whl/third_party/gloo/gloo/transport/ibverbs/pair.cc:462] wc-&gt;status == IBV_WC_SUCCESS. 12 vs 0. Memory region send for slot 0: transport retry counter exceeded<NewLine>done<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/zsk423200,,zsk423200,"August 28, 2018,  8:38am",,,,,
23920,Get partial derivative in pytorch,2018-08-26T12:25:34.513Z,0,183,"<div class=""post"" itemprop=""articleBody""><NewLine><p><code>coords[i]</code> is a list containing 3 elements <code>x,y,z</code> and I want to get the derivative of <code>G[i]</code> w.r.t. each of <code>x,y,z</code> partially i.e. $\frac{\partial (G[i])}{\partial x_{i}}$, in some sort of a functional form like <code>f(x)</code> so that I can pass a scalar <code>x</code> to <code>f()</code>.</p><NewLine><p>This is one of the functions I am using as one of my inputs to a Neural Network and I want to find the partial derivative of my <code>NN</code> w.r.t to <code>x</code>. Hence, I am trying to find  $ \frac{\partial (NN)}{\partial (G1[i])} . \frac{\partial (G1[i])}{\partial (x_{i})} $</p><NewLine><pre><code>import pytorch <NewLine>def sym1(coords):<NewLine>	global avg<NewLine>	global eeta<NewLine>	global Rs<NewLine>	global e<NewLine>	R_avg=Rc<NewLine>	G1=[]<NewLine>	for i,m in enumerate(coords):<NewLine>		G1.append(0)<NewLine>		Ri=np.array(coords[i])<NewLine>		for j in range(i,len(coords)):<NewLine>			if(i!=j):<NewLine>				Rj=np.array(coords[j])<NewLine>				Rij=Ri-Rj<NewLine>				Rij_norm=np.linalg.norm(Rij)<NewLine>				sum1=e**(-eeta*((Rij_norm-Rs)**2))<NewLine>				sum2=cutoff(Rij_norm)<NewLine>				summation=sum1*sum2<NewLine>				G1[i]=G1[i]+summation<NewLine><NewLine>	return G1</code></pre><NewLine></div>",https://discuss.pytorch.org/u/fireballpoint1,(Mayank Modi),fireballpoint1,"August 26, 2018, 12:25pm",,,,,
23460,Pytorch DQN tutorial - where is autograd?,2018-08-19T21:03:08.328Z,10,773,"<div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""onebox"" href=""https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html</a></p><NewLine><p>while the comments in the tutorial specify that autograd is used, it is never explicitly declared (that I can see). In supervised learning, the inputs are usually set as input_data = Variable(input_data) and then out = net.forward(data). However, here, Variable is never used. I do see that the loss tensor contains a gradient - but I am not sure where this came from.</p><NewLine><p>Another observation, if I set<br/><NewLine>state_action_values = Variable(state_action_values,requires_grad=True)<br/><NewLine>then the code will not run - throwing an error on:<br/><NewLine>for param in policy_net.parameters():<br/><NewLine>param.grad.data.clamp_(-1, 1)</p><NewLine><p>saying that ‘NoneType’ has no attribute data (where as clearly before adding the Variable code it did…)</p><NewLine><p>Any ideas? Why is Variable not necessary here?</p><NewLine></div>",https://discuss.pytorch.org/u/Ranahanocka,(Rana Hanocka),Ranahanocka,"August 19, 2018,  9:03pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Which PyTorch version are you using?<br/><NewLine>In <code>0.4.0</code> <code>Variables</code> and <code>tensors</code> were merged, so that you don’t have to wrap your tensors anymore.<br/><NewLine>If you are still used to <code>Variables</code>, the <a href=""https://pytorch.org/2018/04/22/0_4_0-migration-guide.html"">migration guide</a> might help.<br/><NewLine>Also, the current release is <code>0.4.1</code>. Make sure to update to this version. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=5"" title="":wink:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Wow, thanks for the reply <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>, I didn’t realize that change. I’m going to check into this. I think I’m still on 0.3.1 since my code wasn’t forward compatible last time I tried updating <img alt="":rofl:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/rofl.png?v=5"" title="":rofl:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can find the install instructions on the <a href=""https://pytorch.org/"">website</a>.<br/><NewLine>If you encounter any problems updating PyTorch or your code, just let us know and we can try to help you out.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""23460""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ptrblck/40/1823_1.png"" width=""20""/> ptrblck:</div><NewLine><blockquote><NewLine><p><code>Variables</code> , the <a href=""https://pytorch.org/2018/04/22/0_4_0-migration-guide.html"" rel=""nofollow noopener"">migration guide </a> might help.</p><NewLine></blockquote><NewLine></aside><NewLine><p>So this migration guide says tensor history will only be tracked if requires_grad=True , but this is not in the DQN tutorial. What am I missing?</p><NewLine><p>Also, I checked my version on this machine I actually compiled from source, so it looks like I have the newest release.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I haven’t explored the tutorial in detail, but from what I know <code>state_action_values</code> are the output of the model, and should already require gradients.<br/><NewLine>Could you check it with <code>state_action_values.requires_grad</code>?</p><NewLine><p>Also, if you re-wrap a <code>Tensor</code>, it will lose it’s associated computation graph and you are thus detaching it.<br/><NewLine>That’s the reason, why <code>.grad</code> is empty in the example you’ve posted.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes,<code>state_action_values</code> are indeed the output of the <code>policy_net</code>, which has the input <code>state_batch</code>. I checked and <code>state_action_values.requires_grad=True</code> even though it was never explicitly written in code. (I guess this is by default when passing tensors through models, unless with torch.no_grad() - right?)</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, as the model will have some parameters requiring gradients, this property will be passed on:</p><NewLine><pre><code class=""lang-python"">a = torch.randn(1)<NewLine>b = torch.randn(1)<NewLine>c = torch.randn(1, requires_grad=True)<NewLine><NewLine>d = a * b<NewLine>d.requires_grad<NewLine><NewLine>e = a * c<NewLine>e.requires_grad<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, your example is clear. I see, so because the module <code>Parameters</code> inside the network are automatically set to requires_grad, then everything that goes through it gets a grad.</p><NewLine><p>I just modified the DQN to load in a pre-trained network. It looks like it was trained versions ago (and used Variable). Now - I run into the same problem as before</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""23460""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ranahanocka/40/6235_1.png"" width=""20""/> Ranahanocka:</div><NewLine><blockquote><NewLine><p>or param in policy_net.parameters():<br/><NewLine>param.grad.data.clamp_(-1, 1)</p><NewLine><p>saying that ‘NoneType’ has no attribute data</p><NewLine></blockquote><NewLine></aside><NewLine><p>I guess this is because of the fact that the stored model dictionary had a Variable in it right (their module defined Variable within the network itself)? This means that I can’t use older pre-trained networks on torch 0.4+?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>OK, I performed network surgery, redefined the network and only loaded the state_dict from the modules that existed (removing the method that had a variable function). Still no dice. I added requires_grad=True to the tensors, but still there are parameters in the network without a gradient. Not sure what is going on, but it feels like there might be something wrong. (note that everything is OK when I am training from scratch, I can load models back in, etc).</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>As far as I understand, the problem occurs if you want to use the now model code with an old <code>state_dict</code>?<br/><NewLine>Could you create a small executable code snippet so that I can have a look at it?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yep, that did the trick <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=5"" title="":slight_smile:""/><br/><NewLine>Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Ranahanocka; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Ranahanocka; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Ranahanocka; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Ranahanocka; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Ranahanocka; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/Ranahanocka; <NewLine> ,"REPLY_DATE 1: August 19, 2018,  9:50pm; <NewLine> REPLY_DATE 2: August 19, 2018, 10:04pm; <NewLine> REPLY_DATE 3: August 19, 2018, 10:06pm; <NewLine> REPLY_DATE 4: August 19, 2018, 11:09pm; <NewLine> REPLY_DATE 5: August 21, 2018,  2:30am; <NewLine> REPLY_DATE 6: August 19, 2018, 11:49pm; <NewLine> REPLY_DATE 7: August 20, 2018,  8:43am; <NewLine> REPLY_DATE 8: August 20, 2018,  3:28pm; <NewLine> REPLY_DATE 9: August 20, 2018,  6:04pm; <NewLine> REPLY_DATE 10: August 20, 2018,  7:16pm; <NewLine> REPLY_DATE 11: August 21, 2018,  2:31am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> REPLY 11 LIKES: ; <NewLine> 
23324,[Solved] Implementation of A2C doesn&rsquo;t learn,2018-08-17T07:37:10.492Z,0,495,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey everyone<br/><NewLine>I’m new in RL field so I’m re-implementing all the classical algs. I did DQN and REINFORCE but I got some troubles with A2C. I coded it from scratch and it didn’t learn  (it’s even doing worse than random). I checked my code with the examples from github and I can’t spot the difference. I’ve been through it many times and I still don’t get it.<br/><NewLine>Here is my <a href=""https://github.com/Ricocotam/DeepRL/blob/master/src/actor_critic.py"" rel=""nofollow noopener"">code</a> and the <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py"" rel=""nofollow noopener"">example</a>.</p><NewLine><p>I have another question. Why does the example normalize ( minus mean, divided by std) the discounted rewards ?</p><NewLine><p>Thanks for the help</p><NewLine><p>Edit  : After one hour of investigation into any value I could analyse : a misspelled variable. Gg</p><NewLine></div>",https://discuss.pytorch.org/u/Ricocotam,(Adrien Pouyet),Ricocotam,"August 17, 2018,  1:05pm",,,,,
17685,Out of Memory Issues,2018-05-07T18:14:55.737Z,0,1060,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am running a neural network ( a variant of resnet) to optimize a bunch of images. In my outer loop, I am simply doing a forward and optimizing the weights of the network. In parallel, I am running a separate forward during and between every weight update. What I have found so far is that after a couple of episodes of calling forward in the inner loop, the inner loop breaks due to “out of memory” issues.  I am on pytorch 0.3.1. Is there a way to free gpu memory during training?</p><NewLine><p>I have the network constructed like so:</p><NewLine><pre><code class=""lang-python"">import hashlib<NewLine>import torch, time<NewLine>import random, math<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>from collections import namedtuple<NewLine>from torch.autograd import Variable<NewLine><NewLine>use_cuda = torch.cuda.is_available()<NewLine>Tensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor<NewLine><NewLine>def conv3x3(in_planes, out_planes, stride=1):<NewLine>    """"""3x3 convolution with padding""""""<NewLine>    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride,<NewLine>                     padding=1, bias=False)<NewLine><NewLine>class BasicBlock(nn.Module):<NewLine>    expansion = 1<NewLine><NewLine>    def __init__(self, inplanes, planes, stride=1, downsample=None):<NewLine>        super(BasicBlock, self).__init__()<NewLine>        self.conv1 = conv3x3(inplanes, planes, stride)<NewLine>        self.bn1 = nn.BatchNorm3d(planes)<NewLine>        self.relu = nn.ReLU(inplace=True)<NewLine>        self.conv2 = conv3x3(planes, planes)<NewLine>        self.bn2 = nn.BatchNorm3d(planes)<NewLine>        self.downsample = downsample<NewLine>        self.stride = stride<NewLine><NewLine>    def forward(self, x):<NewLine>        residual = x<NewLine><NewLine>        out = self.conv1(x)<NewLine>        out = self.bn1(out)<NewLine>        out = self.relu(out)<NewLine><NewLine>        out = self.conv2(out)<NewLine>        out = self.bn2(out)<NewLine><NewLine>        if self.downsample is not None:<NewLine>            residual = self.downsample(x)<NewLine><NewLine>        out += residual<NewLine>        out = self.relu(out)<NewLine><NewLine>        return out<NewLine><NewLine>class ResNet(nn.Module):<NewLine><NewLine>    def __init__(self, block, layers, num_classes=36,  name=None, device_id=None):<NewLine>        """"""<NewLine>        Name helps in controlling the hash value of this class<NewLine>        """"""<NewLine>        self.name = name<NewLine>        self.inplanes = 64<NewLine>        self.value = None<NewLine>        self.prior_prob = None<NewLine>        self.num_classes = num_classes<NewLine>        super(ResNet, self).__init__()<NewLine>        self.conv1 = nn.Conv3d(36, 64, kernel_size=7, stride=2, padding=3,<NewLine>                               bias=False)<NewLine>        self.bn1 = nn.BatchNorm3d(64)<NewLine>        self.relu = nn.ReLU(inplace=True)<NewLine>        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)<NewLine>        self.layer1 = self._make_layer(block, 64, layers[0])<NewLine>        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)<NewLine>        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)<NewLine>        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)<NewLine><NewLine>        self.loss_value_term = []   # (z - v)^2<NewLine>        self.loss_param_term = []   # pi^T log(p)<NewLine>        self.loss_log_prob_term = []  # c||\theta||^2<NewLine>        self.device_id = device_id<NewLine><NewLine><NewLine>        # this for logit probs head for angle probabilities<NewLine>        self.probhead = self._make_layer(block, num_classes, layers[4], stride=1)<NewLine><NewLine>        for m in self.modules():<NewLine>            with torch.cuda.device(self.device_id):<NewLine>                m = m.cuda()<NewLine><NewLine>    def _make_layer(self, block, planes, blocks, stride=1):<NewLine>        downsample = None<NewLine>        if stride != 1 or self.inplanes != planes * block.expansion:<NewLine>            downsample = nn.Sequential(<NewLine>                nn.Conv3d(self.inplanes, planes * block.expansion,<NewLine>                          kernel_size=1, stride=stride, bias=False),<NewLine>                nn.BatchNorm3d(planes * block.expansion),<NewLine>            )<NewLine><NewLine>        layers = []<NewLine>        layers.append(block(self.inplanes, planes, stride, downsample))<NewLine>        self.inplanes = planes * block.expansion<NewLine>        for i in range(1, blocks):<NewLine>            layers.append(block(self.inplanes, planes))<NewLine><NewLine>        return nn.Sequential(*layers)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.conv1(x)<NewLine>        x = self.bn1(x)<NewLine>        x = self.relu(x)<NewLine>        x = self.maxpool(x)<NewLine><NewLine>        x = self.layer1(x)<NewLine>        x = self.layer2(x)<NewLine>        x = self.layer3(x)<NewLine>        x = self.layer4(x)<NewLine><NewLine>        # define probability distribution over state-action pairs<NewLine>        px = self.probhead(x)<NewLine>        px = px.view(px.size(0), -1)<NewLine>        s1, s2 = px.size()<NewLine>        linear_layer = nn.Linear(s1*s2, self.num_classes)<NewLine>        linear_layer = linear_layer.cuda(self.device_id) if use_cuda else linear_layer<NewLine>        probs = linear_layer(px)<NewLine>        probs = probs.cuda(self.device_id) if use_cuda else probs<NewLine>        probs = F.softmax(probs, dim=1)<NewLine><NewLine>        valuehead = nn.Sequential(<NewLine>                            nn.Linear(s1 * s2, 512),<NewLine>                            nn.ReLU(inplace=True),<NewLine>                            nn.Linear(512, 256),<NewLine>                            nn.ReLU(inplace=True),<NewLine>                            nn.Linear(256, 1),<NewLine>                            )<NewLine>        valuehead = valuehead.cuda(self.device_id) if use_cuda else valuehead<NewLine>        value = F.tanh(valuehead(px))<NewLine><NewLine><NewLine>        probs_tune_head = nn.Sequential(<NewLine>                            nn.Linear(s1 * s2, 512),<NewLine>                            nn.ReLU(inplace=True),<NewLine>                            nn.Linear(512, 256),<NewLine>                            nn.ReLU(inplace=True),<NewLine>                            nn.Linear(256, 3),<NewLine>                            )<NewLine>        probs_tune_head = probs_tune_head.cuda(self.device_id) if use_cuda else probs_tune_head<NewLine>        probs_tune = F.tanh(probs_tune_head(px))<NewLine><NewLine>        del valuehead, probs_tune_head, linear_layer<NewLine><NewLine>        self.value       = value<NewLine>        self.prior_prob = probs<NewLine>        self.probs_tune  = probs_tune<NewLine><NewLine>        return probs, value, probs_tune<NewLine><NewLine>    def __hash__(self):<NewLine>        return int(hashlib.md5(self.name.encode('utf-8')).hexdigest(),16)<NewLine><NewLine>    def __eq__(self,other):<NewLine>        if hash(self)==hash(other):<NewLine>            return True<NewLine>        return False<NewLine><NewLine></code></pre><NewLine><p>And it runs like so:</p><NewLine><pre><code class=""lang-python"">player = ResNet(BasicBlock, [3, 4, 6, 3, 1], num_classes=5, name='player', device_id=11) # use 10 deg resolution<NewLine><NewLine>for episode in range(500):<NewLine>			# obtain input planes to be fed to the neural network<NewLine>			probs, value, value_deg = player(Variable(torch.randn([1, 36, 122, 64, 64])).cuda(player.device_id))<NewLine>			# turn player's strategy from a pure strategy to a mixed strategy<NewLine>			# construct root node for tree search<NewLine>			state.device_id = player_gpu<NewLine>			root_node   = Node(parent=None, state_obj=state, prior_prob=probs, value=value,<NewLine>					   value_deg=value_deg, device_id=player.device_id)<NewLine>			prev_net.append(self.player)<NewLine><NewLine>			if len(prev_net) &gt; 2: prev_net.pop(0)<NewLine><NewLine>			# player pool is an instance of python's multiprocessing<NewLine>			best_node = player_pool.apply_async(mcts.run_tree_search, (root_node, prev_net[-2]))<NewLine>			best_node = best_node.get()<NewLine>			#best_node = mcts.run_tree_search(root_node, player)   # in this inner node, I am continually calling the forward method of player<NewLine></code></pre><NewLine><p>The inner loop runs the network like so</p><NewLine><pre><code class=""lang-auto""><NewLine>			planes =Variable(torch.randn([1, 36, 122, 64, 64]))   <NewLine>			prior_prob, value, value_deg = self.player(planes)  # player is an instance of the network<NewLine></code></pre><NewLine><p>After a couple of iterations, I get a <code>cuda runtime error</code>:</p><NewLine><pre><code class=""lang-bash"">THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1523242347739/work/torch/lib/THC/generic/THCStorage.cu line=58 error=2 : out of memory<NewLine>Traceback (most recent call last):<NewLine>  File ""varian_main.py"", line 409, in &lt;module&gt;<NewLine>    neural_fsp.train_nets(mask.rsplit(sep=""."")[0])<NewLine>  File ""varian_main.py"", line 311, in train_nets<NewLine>    best_node_oppo = mcts.run_tree_search(root_node_oppo, self.player_oppo)<NewLine>  File ""/mnt/md0/lex/RadOncol/beam_optim/scripts/monte_carlo/mcts.py"", line 123, in run_tree_search<NewLine>    new_node      = self.tree_policy(root_node)<NewLine>  File ""/mnt/md0/lex/RadOncol/beam_optim/scripts/monte_carlo/mcts.py"", line 141, in tree_policy<NewLine>    return self.expand(node)<NewLine>  File ""/mnt/md0/lex/RadOncol/beam_optim/scripts/monte_carlo/mcts.py"", line 160, in expand<NewLine>    prior_prob, value, value_deg = self.player(planes)<NewLine>  File ""/home/lekan/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 357, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/mnt/md0/lex/RadOncol/beam_optim/scripts/models/varian_model.py"", line 109, in forward<NewLine>    x = self.conv1(x)<NewLine>  File ""/home/lekan/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 357, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/lekan/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/modules/conv.py"", line 388, in forward<NewLine>    self.padding, self.dilation, self.groups)<NewLine>  File ""/home/lekan/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/functional.py"", line 126, in conv3d<NewLine>    return f(input, weight, bias)<NewLine>RuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1523242347739/work/torch/lib/THC/generic/THCStorage.cu:58<NewLine></code></pre><NewLine><p>I can’t seem to find what I am doing wrong. Do we have to manually free cuda memory everytime we repeatedly call forward?</p><NewLine><p>I am on <code>0.3.1</code> version</p><NewLine></div>",https://discuss.pytorch.org/u/lakehanne,,lakehanne,"May 7, 2018,  6:16pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>When I implemented the ResNet architecture, I too bumped into the same problem of out of memory execution. I realized that my GPU was getting populated since I was having a batch size of 64. So I reduced my batch size to 4 and it worked fine. This is just one such scenario.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just wanted to point out an obvious (possibly!) anomaly in your code:</p><NewLine><p>You are creating new layers (probs_tune_head, valuehead) in the <code>forward()</code> method of ResNet.<br/><NewLine>If you want to optimize the parameters, create the layers in <code>__init__()</code> method and use the created variables in <code>forward()</code> function.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/kelam_goutam; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/InnovArul; <NewLine> ,"REPLY_DATE 1: August 8, 2018,  7:55am; <NewLine> REPLY_DATE 2: August 8, 2018,  8:17am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
10082,Pretrained loaded but the performance worse at beginning,2017-11-17T19:53:04.718Z,0,1667,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying reinforcement learning using pytorch,<br/><NewLine>and when I load a pretrained model as initial,<br/><NewLine>the performance is worse at begin, and it become better very fast</p><NewLine><p>this is a plot the shows the performance drop when resume.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/bddded521700606316d856d2b230d0caf50425bc"" href=""https://discuss.pytorch.org/uploads/default/original/2X/b/bddded521700606316d856d2b230d0caf50425bc.png"" title=""image.png""><img alt=""image"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/b/bddded521700606316d856d2b230d0caf50425bc_2_10x10.png"" height=""492"" src=""https://discuss.pytorch.org/uploads/default/original/2X/b/bddded521700606316d856d2b230d0caf50425bc.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">740×528 11.1 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>I save my model like <a href=""https://discuss.pytorch.org/t/saving-and-loading-a-model-in-pytorch/2610/3"">this post</a> suggest:<br/><NewLine>and I didn’t delete any layer of my model</p><NewLine><pre><code class=""lang-auto""> save_checkpoint({<NewLine>            'epoch': epoch + 1,<NewLine>            'state_dict': model.state_dict(),<NewLine>            'optimizer' : optimizer.state_dict(),<NewLine>      })<NewLine><NewLine>def save_checkpoint(state, filename='checkpoint.pth.tar'):<NewLine>    torch.save(state, filename)<NewLine></code></pre><NewLine><p>and resume model like this:</p><NewLine><pre><code class=""lang-auto"">def load_pretrained(model, pretrained_dict):<NewLine>    model_dict = model.state_dict()<NewLine>    # filter out unmatch dict <NewLine>    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}<NewLine>    model_dict.update(pretrained_dict)<NewLine>    model.load_state_dict(pretrained_dict)<NewLine><NewLine>if checkpoint_file is not None:<NewLine>    print('loading checkpoint_file {}'.format(checkpoint_file))<NewLine>    cp = torch.load(checkpoint_file)<NewLine>    load_pretrained(optimizer, cp['optimizer'])<NewLine>    load_pretrained(state_dict, cp['state_dict'])<NewLine>    trained_episodes = cp['epoch']<NewLine></code></pre><NewLine><p>Is that normal that need some time to let the performance recover?</p><NewLine></div>",https://discuss.pytorch.org/u/SSARCandy,"(Hsu,Shu Hsuan)",SSARCandy,"November 18, 2017, 12:37am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This looks weird to me… I’m not sure what happened.</p><NewLine><p>What do you know away in <code>pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}</code>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok, I figured out why.</p><NewLine><p>I’m training DQN, and there have two network (<code>q</code> and <code>target_q</code>) when training.<br/><NewLine>in the training process, only update one of the net (<code>q</code>), and the other net (<code>target_q</code>) will periodically copy parameters from the <code>q</code> net.</p><NewLine><p>The performance problem is because I only save the <code>q</code> net parameters, after I save the both net, the issue is solved.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey,</p><NewLine><p>I have trained model for robotic arm in Gazebo simulation. When I am loading that model, I can see all<br/><NewLine>trained weights in place but new loaded model does not produce same successful runs.What could be the reason.</p><NewLine><p>DRQN Network is similar to<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/dusty-nv/jetson-reinforcement/blob/master/python/DQN.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/dusty-nv/jetson-reinforcement/blob/master/python/DQN.py"" rel=""nofollow noopener"" target=""_blank"">dusty-nv/jetson-reinforcement/blob/master/python/DQN.py</a></h4><NewLine><pre><code class=""lang-py""># -*- coding: utf-8 -*-<NewLine><NewLine>import argparse<NewLine>import math<NewLine>import random<NewLine>import numpy as np<NewLine>from collections import namedtuple<NewLine>from itertools import count<NewLine>from copy import deepcopy<NewLine><NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine>import torch.nn.functional as F<NewLine>from torch.autograd import Variable<NewLine><NewLine>import sys<NewLine><NewLine><NewLine># if gpu is to be used<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/dusty-nv/jetson-reinforcement/blob/master/python/DQN.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>Am I missing something?</p><NewLine><p>Thanks in Advance</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SimonW; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/SSARCandy; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Ashwini_Magar; <NewLine> ,"REPLY_DATE 1: November 18, 2017, 12:25am; <NewLine> REPLY_DATE 2: November 18, 2017,  7:47pm; <NewLine> REPLY_DATE 3: August 7, 2018, 11:45am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
22580,How to choose RoCE use tcpip or rdma,2018-08-07T08:56:39.216Z,0,287,"<div class=""post"" itemprop=""articleBody""><NewLine><p>how to choose IB or RoCE use which protoc:tcpip or rdma, what is the param when start pytorch</p><NewLine></div>",https://discuss.pytorch.org/u/zsk423200,,zsk423200,"August 7, 2018,  8:56am",,,,,
16048,DQN saved model doesn&rsquo;t play correct,2018-04-06T11:10:14.896Z,0,710,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have the following dqn model:</p><NewLine><pre><code class=""lang-auto"">class DQN(nn.Module):<NewLine><NewLine>    def __init__(self, input_shape, n_actions):<NewLine>        super(DQN, self).__init__()<NewLine><NewLine>        self.conv = nn.Sequential(<NewLine>            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),<NewLine>            nn.ReLU(),<NewLine>            nn.Conv2d(32, 64, kernel_size=4, stride=2),<NewLine>            nn.ReLU(),<NewLine>            nn.Conv2d(64, 64, kernel_size=3, stride=1),<NewLine>            nn.ReLU()<NewLine>        )<NewLine><NewLine>        conv_out_size = self._get_conv_out(input_shape)<NewLine>        self.fc = nn.Sequential(<NewLine>            nn.Linear(conv_out_size, 512),<NewLine>            nn.ReLU(),<NewLine>            nn.Linear(512, n_actions)<NewLine>        )<NewLine><NewLine>    def _get_conv_out(self, shape):<NewLine>        o = self.conv(Variable(torch.zeros(1, *shape)))<NewLine>        return int(np.prod(o.size()))<NewLine><NewLine>    def forward(self, x):<NewLine>        fx = x.float() / 256.0<NewLine>        conv_out = self.conv(fx).view(fx.size()[0], -1)<NewLine>        return self.fc(conv_out)<NewLine></code></pre><NewLine><p>I trained it for Pong and got more than 18.0 mean reward for the last 100 games.<br/><NewLine>It was saved:</p><NewLine><pre><code class=""lang-auto"">torch.save(net.state_dict(), 'pong_model.pt')<NewLine></code></pre><NewLine><p>But when I try to load it and play some games I get wrong results.</p><NewLine><pre><code class=""lang-auto"">net = models.DQN(env.observation_space.shape, env.action_space.n)<NewLine>net.load_state_dict(torch.load('pong_model.pt', map_location=lambda storage, loc: storage))<NewLine>for i in range(20):<NewLine>    state = env.reset()<NewLine>    while True:<NewLine>        env.render()<NewLine>        action = agent(state, net)<NewLine>        next_state, _, done, _ = env.step(action)<NewLine>        if done:<NewLine>            break<NewLine>        state = next_state<NewLine></code></pre><NewLine><p>It can’t win even once. What could be wrong?</p><NewLine></div>",https://discuss.pytorch.org/u/Victor,,Victor,"April 6, 2018, 11:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you post the code for ‘agent()’?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">class EpsilonGreedyAgent(object):<NewLine><NewLine>    def __init__(self, actions_num):<NewLine>        self.actions_num = actions_num<NewLine>    <NewLine>    def _get_state_value(self, state, model):<NewLine>        state = np.expand_dims(state, 0)<NewLine>        state = Variable(torch.from_numpy(state), volatile=False)<NewLine>        if use_cuda:<NewLine>            state = state.cuda()<NewLine>        return model(state).data.cpu().numpy()<NewLine><NewLine>    def __call__(self, state, model, eps):<NewLine>        state_value = self._get_state_value(state, model)<NewLine>        sample = random.random()<NewLine><NewLine>        if sample &gt; eps:<NewLine>            action = np.argmax(state_value)<NewLine>        else:<NewLine>            action = random.randrange(self.actions_num)<NewLine>        return action<NewLine></code></pre><NewLine><p>I found my mistake. I didn’t use all wrappers for environment when I was testing model.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi ,</p><NewLine><p>I have trained model for robotic arm  in Gazebo simulation. But when I am loading that model, I can see all<br/><NewLine>trained weights in place and model is not winning single time. How should I wrap Gazebo environment. Any suggestions.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/keerthanpg; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Victor; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Ashwini_Magar; <NewLine> ,"REPLY_DATE 1: April 9, 2018,  3:26am; <NewLine> REPLY_DATE 2: April 9, 2018,  6:22am; <NewLine> REPLY_DATE 3: July 30, 2018,  1:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
22001,Computing loss to maximize reward,2018-07-30T12:13:49.277Z,0,995,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to get a policy (with a neural network - I am using tanh activation functions, but any would work) by maximizing the reward I obtain from my environment. I am kind of new to reinforcement-learning and particularly to Pytorch, I would like to create an actor-only strategy that maximizes my rewards (which I get from a simulated environment) over a continuous state environment (hence using a parametrized policy).</p><NewLine><p>I have the following network structure:</p><NewLine><pre><code class=""lang-auto"">class PolicyNetwork(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(PolicyNetwork, self).__init__()<NewLine>        self.affine1 = nn.Linear(3, 6)<NewLine>        self.affine2 = nn.Linear(6, 6)<NewLine>        self.affine3 = nn.Linear(6, 1)<NewLine><NewLine>        self.rewards = []<NewLine><NewLine>    def forward(self, x):<NewLine>        action = F.tanh(self.affine1(x)) # maybe change to linear<NewLine>        action = F.tanh(self.affine2(action))<NewLine>        action = F.tanh(self.affine2(action))<NewLine>        return self.affine3(action)<NewLine><NewLine>policy = PolicyNetwork()<NewLine></code></pre><NewLine><p>My environment computes scalar rewards (numpy float64) at every iteration, and I do the following:</p><NewLine><pre><code class=""lang-auto"">policy.rewards.append(reward)<NewLine></code></pre><NewLine><p>Then, at the end of each episode I do:</p><NewLine><pre><code class=""lang-auto"">finish_episode(gamma,base_line)<NewLine></code></pre><NewLine><p>with</p><NewLine><pre><code class=""lang-auto"">gamma=0.99<NewLine>criterion = nn.MSELoss()<NewLine>base_line=1.0 # this is unreachable given my environment - I know maximum is 0.6<NewLine><NewLine>def finish_episode(gamma,base_line):<NewLine>    R = 0<NewLine>    rewards = []<NewLine>    for r in policy.rewards[::-1]:<NewLine>        R = r + gamma * R<NewLine>        rewards.insert(0, R)<NewLine>    rewards = torch.tensor(rewards)<NewLine>    loss=0<NewLine>    for reward in rewards:<NewLine>        loss += criterion(reward, base_line)<NewLine>    optimizer.zero_grad()<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine>    del policy.rewards[:]<NewLine></code></pre><NewLine><p>and I get the following error: AttributeError: ‘float’ object has no attribute ‘requires_grad’</p><NewLine><p>I have also tried:</p><NewLine><pre><code class=""lang-auto"">base_line=Variable(Variable(torch.ones(2), requires_grad=True))<NewLine>base_line=base_line[0]<NewLine></code></pre><NewLine><p>and get the error “RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn”</p><NewLine><p>I guess there is a simple way o do this, but have just not found it. Any suggestions would be greatly appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/JMPF,(JMPF),JMPF,"July 30, 2018, 12:13pm",,,,,
21831,Can we interpolate frames with pytorch?,2018-07-27T11:39:10.281Z,2,763,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Folks,<br/><NewLine>I was trying to test an algorithm I found in a github page that interpolates frames. I was testing the algorithm on the datasets middlebury provided, the link: <a href=""http://vision.middlebury.edu/flow/data/"" rel=""nofollow noopener"">http://vision.middlebury.edu/flow/data/</a> .(I use the coloured ones with only two frames. The error also occured in the Yosemite image file) The algorithm worked fine on every image except the last one. It raises the error:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last): <NewLine>File ""run.py"", line 257, in  <NewLine>tensorInputFirst = torch.FloatTensor(numpy.rollaxis(numpy.asarray(PIL.Image.open(arguments_strFirst))[:,:,::-1], 2, 0).astype(numpy.float32) / 255.0) <NewLine>IndexError: too many indices for array<NewLine>I know what the error means, an index thats not in the list, but I don't know its connection with frame interpolation and why does it happen. Can somebody help.<NewLine></code></pre><NewLine><p>The code I found on github:</p><NewLine><pre><code class=""lang-auto"">#!/usr/bin/env python2.7<NewLine><NewLine>import sys<NewLine>import getopt<NewLine>import math<NewLine>import numpy<NewLine>import torch<NewLine>import torch.utils.serialization<NewLine>import PIL<NewLine>import PIL.Image<NewLine><NewLine>from moviepy.video.io.ffmpeg_reader import FFMPEG_VideoReader<NewLine>from moviepy.video.io.ffmpeg_writer import FFMPEG_VideoWriter<NewLine><NewLine>from SeparableConvolution import SeparableConvolution # the custom SeparableConvolution layer<NewLine><NewLine>torch.cuda.device(1) <NewLine><NewLine>torch.backends.cudnn.enabled = True <NewLine><NewLine>##########################################################<NewLine><NewLine>arguments_strModel = 'lf'<NewLine>arguments_strFirst = './images/first.png'<NewLine>arguments_strSecond = './images/second.png'<NewLine>arguments_strOut = './result.png'<NewLine>arguments_strVideo = ''<NewLine>arguments_strVideoOut = ''<NewLine><NewLine>for strOption, strArgument in getopt.getopt(sys.argv[1:], '', [ strParameter[2:] + '=' for strParameter in sys.argv[1::2] ])[0]:<NewLine>    if strOption == '--model':<NewLine>        arguments_strModel = strArgument # which model to use, l1 or lf, please see our paper for more details<NewLine><NewLine>    elif strOption == '--first':<NewLine>        arguments_strFirst = strArgument # path to the first frame<NewLine><NewLine>    elif strOption == '--second':<NewLine>        arguments_strSecond = strArgument # path to the second frame<NewLine><NewLine>    elif strOption == '--out':<NewLine>        arguments_strOut = strArgument # path to where the output should be stored<NewLine><NewLine>    elif strOption == '--video':<NewLine>        arguments_strVideo = strArgument # path to the video<NewLine><NewLine>    elif strOption == '--video-out':<NewLine>        arguments_strVideoOut = strArgument # path to the video<NewLine><NewLine><NewLine>##########################################################<NewLine><NewLine>class Network(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Network, self).__init__()<NewLine><NewLine>        def Basic(intInput, intOutput):<NewLine>            return torch.nn.Sequential(<NewLine>                torch.nn.Conv2d(in_channels=intInput, out_channels=intOutput, kernel_size=3, stride=1, padding=1),<NewLine>                torch.nn.ReLU(inplace=False),<NewLine>                torch.nn.Conv2d(in_channels=intOutput, out_channels=intOutput, kernel_size=3, stride=1, padding=1),<NewLine>                torch.nn.ReLU(inplace=False),<NewLine>                torch.nn.Conv2d(in_channels=intOutput, out_channels=intOutput, kernel_size=3, stride=1, padding=1),<NewLine>                torch.nn.ReLU(inplace=False)<NewLine>            )<NewLine><NewLine><NewLine>        def Subnet():<NewLine>            return torch.nn.Sequential(<NewLine>                torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),<NewLine>                torch.nn.ReLU(inplace=False),<NewLine>                torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),<NewLine>                torch.nn.ReLU(inplace=False),<NewLine>                torch.nn.Conv2d(in_channels=64, out_channels=51, kernel_size=3, stride=1, padding=1),<NewLine>                torch.nn.ReLU(inplace=False),<NewLine>                torch.nn.Upsample(scale_factor=2, mode='bilinear'),<NewLine>                torch.nn.Conv2d(in_channels=51, out_channels=51, kernel_size=3, stride=1, padding=1)<NewLine>            )<NewLine><NewLine>        self.moduleConv1 = Basic(6, 32)<NewLine>        self.modulePool1 = torch.nn.AvgPool2d(kernel_size=2, stride=2)<NewLine><NewLine>        self.moduleConv2 = Basic(32, 64)<NewLine>        self.modulePool2 = torch.nn.AvgPool2d(kernel_size=2, stride=2)<NewLine><NewLine>        self.moduleConv3 = Basic(64, 128)<NewLine>        self.modulePool3 = torch.nn.AvgPool2d(kernel_size=2, stride=2)<NewLine><NewLine>        self.moduleConv4 = Basic(128, 256)<NewLine>        self.modulePool4 = torch.nn.AvgPool2d(kernel_size=2, stride=2)<NewLine><NewLine>        self.moduleConv5 = Basic(256, 512)<NewLine>        self.modulePool5 = torch.nn.AvgPool2d(kernel_size=2, stride=2)<NewLine><NewLine>        self.moduleDeconv5 = Basic(512, 512)<NewLine>        self.moduleUpsample5 = torch.nn.Sequential(<NewLine>            torch.nn.Upsample(scale_factor=2, mode='bilinear'),<NewLine>            torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),<NewLine>            torch.nn.ReLU(inplace=False)<NewLine>        )<NewLine><NewLine>        self.moduleDeconv4 = Basic(512, 256)<NewLine>        self.moduleUpsample4 = torch.nn.Sequential(<NewLine>            torch.nn.Upsample(scale_factor=2, mode='bilinear'),<NewLine>            torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),<NewLine>            torch.nn.ReLU(inplace=False)<NewLine>        )<NewLine><NewLine>        self.moduleDeconv3 = Basic(256, 128)<NewLine>        self.moduleUpsample3 = torch.nn.Sequential(<NewLine>            torch.nn.Upsample(scale_factor=2, mode='bilinear'),<NewLine>            torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),<NewLine>            torch.nn.ReLU(inplace=False)<NewLine>        )<NewLine><NewLine>        self.moduleDeconv2 = Basic(128, 64)<NewLine>        self.moduleUpsample2 = torch.nn.Sequential(<NewLine>            torch.nn.Upsample(scale_factor=2, mode='bilinear'),<NewLine>            torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),<NewLine>            torch.nn.ReLU(inplace=False)<NewLine>        )<NewLine><NewLine>        self.moduleVertical1 = Subnet()<NewLine>        self.moduleVertical2 = Subnet()<NewLine>        self.moduleHorizontal1 = Subnet()<NewLine>        self.moduleHorizontal2 = Subnet()<NewLine><NewLine>        self.modulePad = torch.nn.ReplicationPad2d([ int(math.floor(51 / 2.0)), int(math.floor(51 / 2.0)), int(math.floor(51 / 2.0)), int(math.floor(51 / 2.0)) ])<NewLine><NewLine>        self.load_state_dict(torch.load('./network-' + arguments_strModel + '.pytorch'))<NewLine><NewLine>    def forward(self, variableInput1, variableInput2):<NewLine>        variableJoin = torch.cat([variableInput1, variableInput2], 1)<NewLine><NewLine>        variableConv1 = self.moduleConv1(variableJoin)<NewLine>        variablePool1 = self.modulePool1(variableConv1)<NewLine><NewLine>        variableConv2 = self.moduleConv2(variablePool1)<NewLine>        variablePool2 = self.modulePool2(variableConv2)<NewLine><NewLine>        variableConv3 = self.moduleConv3(variablePool2)<NewLine>        variablePool3 = self.modulePool3(variableConv3)<NewLine><NewLine>        variableConv4 = self.moduleConv4(variablePool3)<NewLine>        variablePool4 = self.modulePool4(variableConv4)<NewLine><NewLine>        variableConv5 = self.moduleConv5(variablePool4)<NewLine>        variablePool5 = self.modulePool5(variableConv5)<NewLine><NewLine>        variableDeconv5 = self.moduleDeconv5(variablePool5)<NewLine>        variableUpsample5 = self.moduleUpsample5(variableDeconv5)<NewLine><NewLine>        variableDeconv4 = self.moduleDeconv4(variableUpsample5 + variableConv5)<NewLine>        variableUpsample4 = self.moduleUpsample4(variableDeconv4)<NewLine><NewLine>        variableDeconv3 = self.moduleDeconv3(variableUpsample4 + variableConv4)<NewLine>        variableUpsample3 = self.moduleUpsample3(variableDeconv3)<NewLine><NewLine>        variableDeconv2 = self.moduleDeconv2(variableUpsample3 + variableConv3)<NewLine>        variableUpsample2 = self.moduleUpsample2(variableDeconv2)<NewLine><NewLine>        variableCombine = variableUpsample2 + variableConv2<NewLine><NewLine>        variableDot1 = SeparableConvolution()(self.modulePad(variableInput1), self.moduleVertical1(variableCombine), self.moduleHorizontal1(variableCombine))<NewLine>        variableDot2 = SeparableConvolution()(self.modulePad(variableInput2), self.moduleVertical2(variableCombine), self.moduleHorizontal2(variableCombine))<NewLine><NewLine>        return variableDot1 + variableDot2<NewLine><NewLine><NewLine>moduleNetwork = Network().cuda()<NewLine><NewLine>##########################################################<NewLine><NewLine>def process(tensorInputFirst, tensorInputSecond, tensorOutput):<NewLine>    assert(tensorInputFirst.size(1) == tensorInputSecond.size(1))<NewLine>    assert(tensorInputFirst.size(2) == tensorInputSecond.size(2))<NewLine><NewLine>    intWidth = tensorInputFirst.size(2)<NewLine>    intHeight = tensorInputFirst.size(1)<NewLine><NewLine>    assert(intWidth &lt;= 1280) # while our approach works with larger images, we do not recommend it unless you are aware of the implications<NewLine>    assert(intHeight &lt;= 720) # while our approach works with larger images, we do not recommend it unless you are aware of the implications<NewLine><NewLine>    intPaddingLeft = int(math.floor(51 / 2.0))<NewLine>    intPaddingTop = int(math.floor(51 / 2.0))<NewLine>    intPaddingRight = int(math.floor(51 / 2.0))<NewLine>    intPaddingBottom = int(math.floor(51 / 2.0))<NewLine>    modulePaddingInput = torch.nn.Module()<NewLine>    modulePaddingOutput = torch.nn.Module()<NewLine><NewLine>    if True:<NewLine>        intPaddingWidth = intPaddingLeft + intWidth + intPaddingRight<NewLine>        intPaddingHeight = intPaddingTop + intHeight + intPaddingBottom<NewLine><NewLine>        if intPaddingWidth != ((intPaddingWidth &gt;&gt; 7) &lt;&lt; 7):<NewLine>            intPaddingWidth = (((intPaddingWidth &gt;&gt; 7) + 1) &lt;&lt; 7) # more than necessary<NewLine><NewLine>        if intPaddingHeight != ((intPaddingHeight &gt;&gt; 7) &lt;&lt; 7):<NewLine>            intPaddingHeight = (((intPaddingHeight &gt;&gt; 7) + 1) &lt;&lt; 7) # more than necessary<NewLine><NewLine>        intPaddingWidth = intPaddingWidth - (intPaddingLeft + intWidth + intPaddingRight)<NewLine>        intPaddingHeight = intPaddingHeight - (intPaddingTop + intHeight + intPaddingBottom)<NewLine><NewLine>        modulePaddingInput = torch.nn.ReplicationPad2d([intPaddingLeft, intPaddingRight + intPaddingWidth, intPaddingTop, intPaddingBottom + intPaddingHeight])<NewLine>        modulePaddingOutput = torch.nn.ReplicationPad2d([0 - intPaddingLeft, 0 - intPaddingRight - intPaddingWidth, 0 - intPaddingTop, 0 - intPaddingBottom - intPaddingHeight])<NewLine><NewLine>    if True:<NewLine>        tensorInputFirst = tensorInputFirst.cuda()<NewLine>        tensorInputSecond = tensorInputSecond.cuda()<NewLine><NewLine>        modulePaddingInput = modulePaddingInput.cuda()<NewLine>        modulePaddingOutput = modulePaddingOutput.cuda()<NewLine><NewLine>    if True:<NewLine>        variablePaddingFirst = modulePaddingInput(torch.autograd.Variable(data=tensorInputFirst.view(1, 3, intHeight, intWidth), volatile=True))<NewLine>        variablePaddingSecond = modulePaddingInput(torch.autograd.Variable(data=tensorInputSecond.view(1, 3, intHeight, intWidth), volatile=True))<NewLine>        variablePaddingOutput = modulePaddingOutput(moduleNetwork(variablePaddingFirst, variablePaddingSecond))<NewLine><NewLine>        tensorOutput.resize_(3, intHeight, intWidth).copy_(variablePaddingOutput.data[0])<NewLine><NewLine>    if True:<NewLine>        tensorInputFirst.cpu()<NewLine>        tensorInputSecond.cpu()<NewLine>        tensorOutput.cpu()<NewLine><NewLine><NewLine>tensorOutput = torch.FloatTensor()<NewLine><NewLine>if arguments_strVideo and arguments_strVideoOut:<NewLine>    reader = FFMPEG_VideoReader(arguments_strVideo, False)<NewLine>    writer = FFMPEG_VideoWriter(arguments_strVideoOut, reader.size, reader.fps*2)<NewLine>    reader.initialize()<NewLine>    nextFrame = reader.read_frame()<NewLine>    for x in range(0, reader.nframes):<NewLine>        firstFrame = nextFrame<NewLine>        nextFrame = reader.read_frame()<NewLine>        tensorInputFirst = torch.FloatTensor(numpy.rollaxis(firstFrame[:,:,::-1], 2, 0) / 255.0)<NewLine>        tensorInputSecond = torch.FloatTensor(numpy.rollaxis(nextFrame[:,:,::-1], 2, 0) / 255.0)<NewLine>        process(tensorInputFirst, tensorInputSecond, tensorOutput)<NewLine>        writer.write_frame(firstFrame)<NewLine>        writer.write_frame((numpy.rollaxis(tensorOutput.clamp(0.0, 1.0).numpy(), 0, 3)[:,:,::-1] * 255.0).astype(numpy.uint8))<NewLine>    writer.write_frame(nextFrame)<NewLine>    writer.close()<NewLine>else:<NewLine>    tensorInputFirst = torch.FloatTensor(numpy.rollaxis(numpy.asarray(PIL.Image.open(arguments_strFirst))[:,:,::-1], 2, 0).astype(numpy.float32) / 255.0)<NewLine>    tensorInputSecond = torch.FloatTensor(numpy.rollaxis(numpy.asarray(PIL.Image.open(arguments_strSecond))[:,:,::-1], 2, 0).astype(numpy.float32) / 255.0) <NewLine>    process(tensorInputFirst, tensorInputSecond, tensorOutput)<NewLine>    PIL.Image.fromarray((numpy.rollaxis(tensorOutput.clamp(0.0, 1.0).numpy(), 0, 3)[:,:,::-1] * 255.0).astype(numpy.uint8)).save(arguments_strOut)<NewLine></code></pre><NewLine><p>UPDATE</p><NewLine><p>I found the reason of the error. It is because the image is grayscale while the other images I tested was coloured as coloured ones are 24 bit depth while grayscales are 8 bit depth. How can I modify the code for it to work on grayscale images.</p><NewLine><p>Thanks &amp; Regards<br/><NewLine>Tejaswini<br/><NewLine><a href=""https://mindmajix.com/python-training"" rel=""nofollow noopener"">Python Developer</a></p><NewLine></div>",https://discuss.pytorch.org/u/TejaswiniUL,,TejaswiniUL,"July 27, 2018, 11:39am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you try to convert your image to RGB by using:</p><NewLine><pre><code class=""lang-python"">PIL.Image.open(arguments_strFirst)).convert('RGB')[:,:,::-1]<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> i will do it.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>In case anyone is curious, this is the source repository: <a href=""https://github.com/sniklaus/pytorch-sepconv"" rel=""nofollow noopener"">https://github.com/sniklaus/pytorch-sepconv</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/TejaswiniUL; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/sniklaus; <NewLine> ,"REPLY_DATE 1: July 27, 2018, 11:46am; <NewLine> REPLY_DATE 2: July 27, 2018, 12:03pm; <NewLine> REPLY_DATE 3: July 27, 2018,  6:18pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
21046,Type Error (NoneType),2018-07-12T03:53:29.308Z,0,393,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m implementing a deep Q learning algorithm and get the following error when running my code from the command line:</p><NewLine><pre><code class=""lang-auto"">TypeError: mul() received an invalid combination of arguments - got (NoneType), but expected one of:<NewLine> * (Tensor other)<NewLine>      didn't match because some of the arguments have invalid types: (NoneType)<NewLine> * (float other)<NewLine>      didn't match because some of the arguments have invalid types: (NoneType)<NewLine></code></pre><NewLine><p>The line in question is:<br/><NewLine><code>expected_values = next_state_vals * gamma + rewards_t</code></p><NewLine><p><code>gamma</code> is a scalar value, and printing <code>next_state_vals.type()</code> and <code>rewards_t.type()</code> show they are both <code>torch.FloatTensor</code>. Printing both tensors confirms this as well.</p><NewLine><p>For some additional context, the preceding lines are as follows with <code>rewards</code> and <code>next_states</code> both as numpy arrays.</p><NewLine><pre><code class=""lang-auto"">rewards_t = torch.FloatTensor(rewards).to(device)<NewLine>next_states = torch.FloatTensor(next_states).to(device)<NewLine>next_state_vals = target_network(next_states).max(1)[0]<NewLine>next_state_vals = next_state_vals.detach()<NewLine>expected_values = next_state_vals * gamma + rewards_t<NewLine></code></pre><NewLine><p>Any help would be greatly appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/cdh,(Christian),cdh,"July 12, 2018,  4:26am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>For some reason my <code>gamma</code> value was getting reset to <code>None</code> and I just never noticed it.</p><NewLine><p>Lesson: don’t code well past your bedtime…</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/cdh; <NewLine> ,"REPLY_DATE 1: July 12, 2018,  4:26am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
20976,Should action log-probability computed after or before constraining the action?,2018-07-10T15:42:53.224Z,0,379,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Suppose we implement a Gaussian policy, and we would like to constraint the sampled action with upper/lower bound (e.g. <code>[-2, 2]</code>, we do it by <code>2*torch.tanh(action)</code>). Then it raises up a question, where should we compute the log-probability, after or before the action constraint ?</p><NewLine></div>",https://discuss.pytorch.org/u/Xingdong_Zuo,(Xingdong Zuo),Xingdong_Zuo,"July 10, 2018,  3:42pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you do the clipping after the sampling, the distribution is no longer gaussian (with a peak at the closest constraint to the mean).</p><NewLine><p>If you sample and then you apply tanh after sampling, then the distribution is even less gaussian, and is too much distorted to estimate the gradient of the return.</p><NewLine><p>I would suggest the first solution, but you have cleaner tricks (when you sample outside the limit, you go back from the mean, like in Nokia’s snake game, the resulting distribution is closer to a gaussian).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> ,"REPLY_DATE 1: July 10, 2018,  9:09pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
6749,Actor Critic Loss explodes,2017-08-26T10:00:10.980Z,3,2659,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello I have some problems with implementing the Actor Critic Policy Gradient Algorithm,</p><NewLine><p>When I implement REINFORCE like this, everything is okay:</p><NewLine><pre><code class=""lang-python"">class Estimator(nn.Module):<NewLine>    def __init__(self, num_actions):<NewLine>        super().__init__()<NewLine>        self.num_actions = num_actions<NewLine>        self.dense_1 = nn.Linear(4, 32)<NewLine>        self.out = nn.Linear(32, num_actions)<NewLine><NewLine>    def forward(self,x):<NewLine>        x =self.dense_1(x)<NewLine>        x = F.softmax(self.out(x))<NewLine>        return x<NewLine>    <NewLine>env = gym.make(""CartPole-v0"")<NewLine>estimator = Estimator(2)<NewLine>estimator.cuda()<NewLine>opt = optim.Adam(estimator.parameters())<NewLine>loss = []<NewLine>running_reward = 0<NewLine>for i in range(100000): # number episodes<NewLine>    episode = []<NewLine>    chosen_actions = []<NewLine>    rewards = []<NewLine>    done = False<NewLine>    state = env.reset()<NewLine>                  <NewLine>    while not done:<NewLine>        probs = estimator(Variable(torch.unsqueeze(torch.from_numpy(state),0).float().cuda())) # calculate the probs of choosing actions<NewLine>        action = probs.multinomial()<NewLine>        chosen_actions.append(action)<NewLine>        next_state, reward, done, _ = env.step(action.data[0,0])<NewLine>        rewards.append(reward)<NewLine>        state = next_state<NewLine>       <NewLine>    <NewLine>    R = 0<NewLine>    for r in rewards[::-1]:<NewLine>        R = r +  R<NewLine>        rewards.insert(0, R)<NewLine>        <NewLine>    for action, r in zip(chosen_actions, rewards):<NewLine>        action.reinforce(r)<NewLine>        <NewLine>    opt.zero_grad()<NewLine>    autograd.backward(chosen_actions, [None for _ in chosen_actions])<NewLine>    opt.step()<NewLine>    running_reward = running_reward * 0.99 + len(chosen_actions) * 0.01<NewLine>    if (i+1) % 10 == 0:<NewLine>        print(""Episode: {} Running Reward: {}"".format(i+1,round(running_reward,2)))<NewLine></code></pre><NewLine><p>But if I try to implement the actor critic with the using the <a href=""https://youtu.be/KHZVXao4qXs?t=1h13m35s"" rel=""nofollow noopener"">Generalized Advantage Estimator</a> the Algorithm fails.</p><NewLine><p>Two things are happening:</p><NewLine><ol><NewLine><li>The policy is learning to ALWAYS choose one off the actions (the probability is approaching 1)</li><NewLine><li>the loss (and the output) of the state value estimator explode.</li><NewLine></ol><NewLine><p>I checked the implementation of the policy gradient by pretraining a state value estimator using TD and then plugging it into the code. That works just fine, so I suspect I have made some error implementing the state value updates.</p><NewLine><p>Here is the code:</p><NewLine><p>(I tried a lot of different hyperparamenters like learning rate of both optimizers, the number of state value estimator updates per timestep and also starting the policy gradient algorithm after a short amount of time where the state value predictions are already learning, unfortunatly nothing has worked…)</p><NewLine><pre><code class=""lang-python"">class Estimator(nn.Module):<NewLine>    def __init__(self, num_actions):<NewLine>        super().__init__()<NewLine>        self.num_actions = num_actions<NewLine>        self.dense_1 = nn.Linear(4, 32)<NewLine>        self.out = nn.Linear(32, num_actions)<NewLine><NewLine>    def forward(self,x):<NewLine>        x =self.dense_1(x)<NewLine>        x = F.softmax(self.out(x))<NewLine>        return x<NewLine><NewLine>class V_Estimator(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.dense_1 = nn.Linear(4, 32)<NewLine>        self.out = nn.Linear(32, 1)<NewLine><NewLine>    def forward(self,x):<NewLine>        x = F.relu(self.dense_1(x))<NewLine>        x = self.out(x)<NewLine>        return x<NewLine>    <NewLine>    <NewLine>estimator = Estimator(2)<NewLine>estimator.cuda()<NewLine>v_estimator = V_Estimator()<NewLine>v_estimator.cuda()<NewLine>opt = optim.Adam(estimator.parameters(), lr=0.0001)<NewLine>v_opt = optim.Adam(v_estimator.parameters(), lr=0.0001)<NewLine>env = gym.make(""CartPole-v0"")<NewLine>mse = nn.MSELoss()<NewLine>buffer = ReplayBuffer(100000)<NewLine>running_reward = 0<NewLine>for i in range(10000): # number episodes<NewLine>    episode_len = 0<NewLine>    done = False<NewLine>    state = env.reset()<NewLine>    <NewLine><NewLine>    while not done:<NewLine>        episode_len += 1<NewLine>        state_python = state<NewLine>        state = Variable(torch.unsqueeze(torch.from_numpy(state),0).float().cuda())<NewLine>        probs = estimator(state) <NewLine>        #print(probs.data.cpu().numpy()) # one of the action probabilites just approaches 1<NewLine>        action = probs.multinomial()<NewLine><NewLine>        action_python = action.data[0,0]<NewLine>        v_estimate_curr = v_estimator(state)<NewLine>        #v_estimate_curr = v_estimate(state)<NewLine>        #print(v_estimate_curr)<NewLine>        next_state, reward, done, _ = env.step(action_python)<NewLine>        v_estimate_next = v_estimator(Variable(torch.unsqueeze(torch.from_numpy(next_state),0).float().cuda()))<NewLine>        #v_estimate_next = v_estimate(Variable(torch.unsqueeze(torch.from_numpy(next_state),0).float().cuda()))<NewLine>        #print(v_estimate_next)<NewLine>        <NewLine>        td_error = reward + v_estimate_next - v_estimate_curr<NewLine>        <NewLine>        <NewLine>        buffer.add(state_python, action_python, reward, done, next_state)<NewLine>        state = next_state<NewLine>        <NewLine>        #refit v-estimator<NewLine>        average_state_value_loss = 0<NewLine>        state_value_updates = 30<NewLine>        for j in range(state_value_updates):<NewLine>            s_batch, a_batch, r_batch, d_batch, s2_batch  = buffer.sample_batch(128)<NewLine>            #print(""s_batch shape: {}"".format(s_batch.shape))<NewLine>            targ = v_estimator(Variable(torch.from_numpy(s2_batch)).float().cuda())<NewLine>            #print(""targ shape: {}"".format(targ.data.cpu().numpy().shape))<NewLine>            torch_rew_batch = Variable(torch.unsqueeze(torch.from_numpy(r_batch).float().cuda(),-1))<NewLine>            #print(""torch_rew_batch shape: {}"".format(torch_rew_batch.data.cpu().numpy().shape))<NewLine>            targ = targ + torch_rew_batch<NewLine>            targ = targ.detach()<NewLine>            targ.requires_grad = False<NewLine>            #print(""targ shape: {}"".format(targ.data.cpu().numpy().shape))<NewLine>            out = v_estimator(Variable(torch.from_numpy(s_batch)).float().cuda())<NewLine>            #print(""out shape: {}"".format(out.data.cpu().numpy().shape))<NewLine>            v_loss = mse(out, targ)<NewLine>            average_state_value_loss += v_loss.data[0] / state_value_updates<NewLine>            <NewLine>            v_opt.zero_grad()<NewLine>            v_loss.backward()<NewLine>            v_opt.step()<NewLine><NewLine>        # update policy gradient<NewLine>        #if i &gt; 100: # starting after 100 episodes to give the state value nn some time to learn<NewLine>        opt.zero_grad()<NewLine>        action.reinforce(td_error.data)<NewLine>        action.backward()<NewLine>        opt.step()<NewLine>    running_reward = running_reward * 0.9 + episode_len * 0.1<NewLine>    print(""current episode: "" + str(i)+ "" - running reward: "" + str(round(running_reward,2)) + "" - average state value estimator loss: {}"".format(average_state_value_loss))<NewLine><NewLine></code></pre><NewLine><p>I looked at the implementation in the pytorch examples repo but they do things a little differently (like sharing policy parameters)</p><NewLine><p>If anybody has any idea on how to fix the error I would greatly appreciate it</p><NewLine><p>Johannes</p><NewLine><p>PS: for reproducebility execute this first, then both code samples run:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>from torch.autograd import Variable<NewLine>import gym<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch<NewLine>from torch.autograd import Variable<NewLine>from torch import autograd<NewLine>from torch import optim<NewLine><NewLine>from collections import deque<NewLine>import random<NewLine>import numpy as np<NewLine><NewLine>class ReplayBuffer(object):<NewLine><NewLine>    def __init__(self, buffer_size):<NewLine>        self.buffer_size = buffer_size<NewLine>        self.count = 0<NewLine>        self.buffer = deque()<NewLine><NewLine>    def add(self, s, a, r, d, s2):<NewLine>        experience = (s, a, r, d, s2)<NewLine>        if self.count &lt; self.buffer_size:<NewLine>            self.buffer.append(experience)<NewLine>            self.count += 1<NewLine>        else:<NewLine>            self.buffer.popleft()<NewLine>            self.buffer.append(experience)<NewLine><NewLine>    def size(self):<NewLine>        return self.count<NewLine><NewLine>    def sample_batch(self, batch_size):<NewLine>        '''<NewLine>        batch_size specifies the number of experiences to add<NewLine>        to the batch. If the replay buffer has less than batch_size<NewLine>        elements, simply return all of the elements within the buffer.<NewLine>        '''<NewLine><NewLine>        if self.count &lt; batch_size:<NewLine>            batch = random.sample(self.buffer, self.count)<NewLine>        else:<NewLine>            batch = random.sample(self.buffer, batch_size)<NewLine><NewLine>        s_batch = np.array([np.array(_[0]) for _ in batch])<NewLine>        a_batch = np.array([_[1] for _ in batch])<NewLine>        r_batch = np.array([_[2] for _ in batch])<NewLine>        d_batch = np.array([_[3] for _ in batch])<NewLine>        s2_batch = np.array([np.array(_[4]) for _ in batch])<NewLine><NewLine>        return s_batch, a_batch, r_batch, d_batch, s2_batch<NewLine><NewLine>    def clear(self):<NewLine>        self.buffer.clear()<NewLine>        self.count = 0<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/j.laute,(Johannes Laute),j.laute,"August 26, 2017, 10:05am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Curious if you managed to figure out what the reason was? I think I’m suffering from something similar…</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes I think I solved it. I will dig up the code later today and post it.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/j.laute"">@j.laute</a> was wondering if there were any updates on this. Thanks!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>EDIT: just saw that the code sample uses mse loss, in that case im sorry no idea how I solved it, quite some time ago. Will post when I’m back but that is over a month away. Would recommend to search for some working pytorch implementation online and compare</p><NewLine><p>Sorry, completely forgot to post the code. Unfortunately I am on summer break right now and have no access to the code.</p><NewLine><p>The solution was in my case to use the mse loss instead of the (smooth) l1 loss (which was really unintuitive as the smooth l1 loss is explicitly recommend to prevent problems from the mse loss)</p><NewLine><p>Never got round to actually find out why the l1 loss didn’t work for me, surely I made some implementation mistake.</p><NewLine><p>Cheers, Johannes</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/nicochan; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/j.laute; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/rayyuan; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/j.laute; <NewLine> ,"REPLY_DATE 1: January 26, 2018, 11:22am; <NewLine> REPLY_DATE 2: January 26, 2018, 12:33pm; <NewLine> REPLY_DATE 3: June 22, 2018,  7:37pm; <NewLine> REPLY_DATE 4: July 2, 2018,  6:08am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
19729,Tool for policy search,2018-06-14T23:38:52.992Z,0,302,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I did this a month ago and I didn’t push the repo as I thought it was not that useful.</p><NewLine><p>But if someone wants to explore deeper architectures than a simple linear model and run ars-based policy search (<a href=""https://arxiv.org/abs/1803.07055"">https://arxiv.org/abs/1803.07055</a>), or even try Open-AI evolution strategies (<a href=""https://arxiv.org/abs/1703.03864"">https://arxiv.org/abs/1703.03864</a>), this “optimizer” may save a bunch of lines.</p><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://assets-cdn.github.com/favicon.ico"" width=""32""/><NewLine><a href=""https://github.com/alexis-jacq/Pytorch_Policy_Search_Optimizer"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""400"" src=""https://avatars1.githubusercontent.com/u/9195965?s=400&amp;v=4"" width=""400""/><NewLine><h3><a href=""https://github.com/alexis-jacq/Pytorch_Policy_Search_Optimizer"" target=""_blank"">alexis-jacq/Pytorch_Policy_Search_Optimizer</a></h3><NewLine><p>Pytorch_Policy_Search_Optimizer - A tool for optimizing RL policy modules based on random search</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>",https://discuss.pytorch.org/u/alexis-jacq,(Alexis David Jacq),alexis-jacq,"June 14, 2018, 11:39pm",,,,,
19131,CPU memory leak (rnnFusedPointwise.py),2018-06-03T19:37:35.841Z,0,613,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>This problem is slowing down my research, any help appreciated! I’m on pytorch 0.4.</p><NewLine><p>I’m copy’ing an LSTM state to a new graph location where I want it’s data to be copied into indices of a torch.nn.Parameter(). I was detaching at first, but now I’m doing: cell_state.data.cpu(). I have a shared torch.nn.Module() living on the cpu that has two Parameter()'s. I’m copying into to shared module’s Parmeter()'s like so:</p><NewLine><pre><code class=""lang-python"">shared_module.shared_param[idx, :].copy_(cell_state.data.cpu())<NewLine></code></pre><NewLine><p>Is there anything wrong with this? Something in rnnFusedPointwise is growing by 588 bytes every rollout/backprop:</p><NewLine><pre><code class=""lang-auto"">================================== Begin Trace:                                                                                                                                                             <NewLine>/home/joe/Documents/tools/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/_functions/thnn/rnnFusedPointwise.py:74: size=44.2 KiB (+588 B), count=1576 (+21), average=29 B                       <NewLine>.../rlresearch.pytorch/src/utils.py:116: size=784 B (-72 B), count=5 (-1), average=157 B                                                                                                    <NewLine>.../pytorch/lib/python3.6/site-packages/torch/_thnn/utils.py:108: size=7968 B (-70 B), count=93 (-1), average=86 B                                                     <NewLine>.../pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py:607: size=13.3 KiB (+0 B), count=167 (+0), average=81 B                                             <NewLine>.../pytorch/lib/python3.6/site-packages/torch/nn/_functions/rnn.py:30: size=11.4 KiB (+0 B), count=66 (+0), average=176 B                                              <NewLine>.../pytorch/lib/python3.6/site-packages/torch/_thnn/utils.py:26: size=9248 B (+0 B), count=1 (+0), average=9248 B                                                      <NewLine>.../pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py:704: size=8160 B (+0 B), count=15 (+0), average=544 B                                               <NewLine>.../rlresearch.pytorch/src/models/base.py:67: size=7416 B (+0 B), count=21 (+0), average=353 B                                                                                              <NewLine>.../pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py:764: size=4536 B (+0 B), count=9 (+0), average=504 B                                                <NewLine>.../pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py:610: size=4318 B (+0 B), count=58 (+0), average=74 B                                                <NewLine>================================== Begin Trace:                                                                                                                                                             <NewLine>.../pytorch/lib/python3.6/site-packages/torch/nn/_functions/thnn/rnnFusedPointwise.py:74: size=44.7 KiB (+588 B), count=1597 (+21), average=29 B                       <NewLine>.../rlresearch.pytorch/src/utils.py:116: size=856 B (+72 B), count=6 (+1), average=143 B                                                                                                    <NewLine>.../pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py:607: size=13.3 KiB (+0 B), count=167 (+0), average=81 B                                             <NewLine>.../pytorch/lib/python3.6/site-packages/torch/nn/_functions/rnn.py:30: size=11.4 KiB (+0 B), count=66 (+0), average=176 B                                              <NewLine>.../pytorch/lib/python3.6/site-packages/torch/_thnn/utils.py:26: size=9248 B (+0 B), count=1 (+0), average=9248 B                                                      <NewLine>.../pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py:704: size=8160 B (+0 B), count=15 (+0), average=544 B                                               <NewLine>.../pytorch/lib/python3.6/site-packages/torch/_thnn/utils.py:108: size=7968 B (+0 B), count=93 (+0), average=86 B                                                      <NewLine>.../rlresearch.pytorch/src/models/base.py:67: size=7416 B (+0 B), count=21 (+0), average=353 B                                                                                              <NewLine>.../pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py:764: size=4536 B (+0 B), count=9 (+0), average=504 B                                                <NewLine>.../pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py:610: size=4318 B (+0 B), count=58 (+0), average=74 B<NewLine></code></pre><NewLine><p><strong>Background Info:</strong></p><NewLine><p>I’m trying to add a long-term memory to my reinforcement learning agent. Without the long-term memory, everything works with no leaks. The long-term memory is a Differentiable Neural Dictionary (DND). I simplified the implementation a lot and used fixed size parameter tensors instead of unbounded lists as used in the paper. The implementation is <a href=""https://github.com/jtatusko/rlresearch.pytorch/blob/dnd/src/models/base.py#L12-L46"" rel=""nofollow noopener"">fairly straightforward</a>.</p><NewLine><p>I have multiple GPU worker processes that have a shared model and a shared DND long-term memory that live on the CPU. The long-term memory stores the agent’s LSTM cell-state as the keys and cat(action, value, next cell state) as value. Every inference step, the agent’s cell state is queried for similar cell states and values are returned as a similarity-weighted vector.</p><NewLine><p>Life of a worker (<a href=""https://github.com/jtatusko/rlresearch.pytorch/blob/dnd/src/workers/recalling.py"" rel=""nofollow noopener"">https://github.com/jtatusko/rlresearch.pytorch/blob/dnd/src/workers/recalling.py</a>):</p><NewLine><ol><NewLine><li>L54-L55 Each worker updates it’s GPU model and long-term memory from a snapshot of the CPU model and long-term memory.</li><NewLine><li>L60-L71 Each worker performs n steps (20 by default) with the environment and saves all relevant information for backprop in a rollout cache.</li><NewLine><li>L73-L77 All memories are .data.cpu()'ed before being added to the long-term memory write cache.</li><NewLine><li>L88 At the end of the rollout, the rollout_cache is processed to get losses and losses are backprop’ed.</li><NewLine><li>L109 Model gradients are sync’ed to the CPU model</li><NewLine><li>L110 DND gradients are sync’ed to the CPU DND</li><NewLine><li>L111 Optimizer (CPU) steps</li><NewLine><li>L114-L117 Long-term memory write cache is processed and written to the CPU’s DND. Everything in the cache has already been .data.cpu()'ed from Step 4. Worker’s can’t write into the same index because the index is shared across the workers and sync’ed with a torch.multiprocessing.Lock().</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/Joe_Tatusko,(Joe Tatusko),Joe_Tatusko,"June 3, 2018,  7:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>snapshot’s over 10 passes:<br/><NewLine>rnnFusedPointwise.py:74: size=401 KiB (+5880 B), count=14629 (+210), average=28 B</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/d8935a7b34de9779ca0c5bcfff973643a6de9f5d"" href=""https://discuss.pytorch.org/uploads/default/original/2X/d/d8935a7b34de9779ca0c5bcfff973643a6de9f5d.jpg"" title=""Architecture.jpg""><img alt=""Architecture"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/d/d8935a7b34de9779ca0c5bcfff973643a6de9f5d_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/d/d8935a7b34de9779ca0c5bcfff973643a6de9f5d_2_375x500.jpg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/d/d8935a7b34de9779ca0c5bcfff973643a6de9f5d_2_375x500.jpg, https://discuss.pytorch.org/uploads/default/optimized/2X/d/d8935a7b34de9779ca0c5bcfff973643a6de9f5d_2_562x750.jpg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/d/d8935a7b34de9779ca0c5bcfff973643a6de9f5d_2_750x1000.jpg 2x"" width=""375""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Architecture.jpg</span><span class=""informations"">3024×4032 1.74 MB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine>red = forward flow<br/><NewLine>green = backprop flow</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Solved by detach_()'ing DND parameters after each optimization step</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Joe_Tatusko; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Joe_Tatusko; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Joe_Tatusko; <NewLine> ,"REPLY_DATE 1: June 4, 2018,  2:51pm; <NewLine> REPLY_DATE 2: June 4, 2018,  6:47pm; <NewLine> REPLY_DATE 3: June 4, 2018,  8:31pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
14481,How to implement action sampling for differing allowed actions,2018-03-06T18:17:31.091Z,1,1139,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to implement a basic Policy Gradient training setup. I saw some examples for games that had always the same possible actions in each state. However I am wondering how one would implement the action sampling from the policy when in some states not all actions are allowed.</p><NewLine><p>Right now I do the action sampling like this:</p><NewLine><pre><code class=""lang-auto"">action = torch.multinomial(policy, 1)<NewLine>log_probs = torch.log(torch.gather(policy, -1, action))<NewLine></code></pre><NewLine><p>For the limited action set I thought of something like this</p><NewLine><pre><code class=""lang-auto"">limited_policy = policy[valids]<NewLine>action = torch.multinomial(limited_policy, 1)<NewLine>log_probs = torch.log(torch.gather(limited_policy, -1, action))<NewLine></code></pre><NewLine><p>Here the problem is that the shape of the limited policy changes through the valid indexing and therefore not all rows have the same length anymore and then I cannot use it as a batch anymore.</p><NewLine><p>So what is the right way of handling the limited action space that it also handles the gradients properly?</p><NewLine><p><strong>EDIT</strong></p><NewLine><p>I now set the probability actions manually to zero for not allowed actions like this:</p><NewLine><pre><code class=""lang-auto"">policy[1 - valids] = 0<NewLine>action = torch.multinomial(policy, 1)<NewLine>log_probs = torch.log(torch.gather(policy, -1, action))<NewLine></code></pre><NewLine><p>But I would prefer if there is a solution that somehow consideres this in the sampling function. So that the network can actually “learn” that these actions are not allowed or less wanted in some states.</p><NewLine></div>",https://discuss.pytorch.org/u/marcel1991,,marcel1991,"March 6, 2018,  7:26pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There is no need to learn which action is or is not allowed. By setting the forbidden actions probabilities to zero, your agent will only explore the allowed ones and learn what is the best action out of the allowed set.</p><NewLine><p>However, if you want to reduce the log_prob of the forbidden actions in your gradient step, you can add a gradient direction that simply reduce the log_probabilities for the forbidden actions. in other words, instead of:</p><NewLine><pre><code class=""lang-python"">theta = theta - alpha * grad_logprobs * returns # (REINFORCE iteration)<NewLine></code></pre><NewLine><p>you do:</p><NewLine><pre><code class=""lang-python"">theta = theta - alpha * (grad_logprobs * returns + grad_logprobs * forbidden) <NewLine># where forbidden = 0 if a state-action is allowed<NewLine></code></pre><NewLine><p>or again:</p><NewLine><pre><code class=""lang-python"">theta = theta - alpha * grad_logprobs * (returns+forbidden)<NewLine></code></pre><NewLine><p>But I think it would be longer to train, as you will have to explore a bigger space.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hm okay thank you. Why is a forbidden task considered as a “higher reward”? Shouldn’t it be the negative or inverse like</p><NewLine><pre><code class=""lang-auto"">grad_logprobs *  (returns - forbidden)<NewLine></code></pre><NewLine><p>Wouldn’t that emphasize to do the forbidden task more often?<br/><NewLine>Or am I understanding something completely wrong here?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, it should be negative, but the learning rate positive. I was too fast on this. The correct iteration is:</p><NewLine><pre><code class=""lang-auto"">theta = theta + alpha * grad_logprobs * (returns - forbidden)<NewLine></code></pre><NewLine><p>That way, you optimize the return, but still minimize the log_prob of forbidden actions.</p><NewLine><p>By the way, you can do this and still force your agent to only explorate allowed actions, so you don’t lose time with exploration.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Okay, thank you very much for the good explanation.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Okay I have a problem with the implementation of this now.</p><NewLine><p>I do clone the action probabilities to avoid an inplace operation like this:</p><NewLine><pre><code class=""lang-auto"">pActions, values = self.forward(observations)<NewLine>legalActions = pActions.clone()<NewLine>legalActions[1 - valids] = 0<NewLine>actions = torch.multinomial(legalActions, 1).long()<NewLine>logProbs = torch.log(torch.gather(pActions, -1, actions))<NewLine></code></pre><NewLine><p>However now I get an cuda runtime error that seems to come from the multinomial operation. However the traceback says the error is triggered by the line where I calculate the policy loss with (negLogProbs * advt).mean() after which I call lossPolicy.backward():</p><NewLine><pre><code class=""lang-auto"">/home/marcel/anaconda3/envs/pytorch/pytorch/aten/src/THC/THCTensorRandom.cuh:182: void sampleMultinomialOnce(long *, long, int, T *, T *, int, int) [with T = float, AccT = float]: block: [0,0,0], thread: [2,0,0] Assertion `THCNumerics&lt;T&gt;::ge(val, zero)` failed.<NewLine>THCudaCheck FAIL file=/home/marcel/anaconda3/envs/pytorch/pytorch/aten/src/THC/generated/../THCReduceAll.cuh line=339 error=59 : device-side assert triggered<NewLine>Traceback (most recent call last):<NewLine> ....<NewLine><NewLine>  File ""/home/marcel/Projects/test/train.py"", line 40, in train<NewLine>    lossPolicy = (negLogProbs * advt).mean()<NewLine>RuntimeError: cuda runtime error (59) : device-side assert triggered at /home/marcel/anaconda3/envs/pytorch/pytorch/aten/src/THC/generated/../THCReduceAll.cuh:339<NewLine><NewLine></code></pre><NewLine><p>Is there anything wrong with the cloning of the legal actions? Does this backpropagate correctly?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello. I’m facing a similar problem.<br/><NewLine>Have you found a solution?<br/><NewLine>Thanks.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>The cuda runtime error is often misleading and can be the cause of different errors.</p><NewLine><p>What you can do is let your network run on the cpu for debugging. This gives usually a better error behaviour and you can find the line which actually lead to the error.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/marcel1991; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/marcel1991; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/marcel1991; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Ricardo_Gama; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/marcel1991; <NewLine> ,"REPLY_DATE 1: March 8, 2018, 11:53am; <NewLine> REPLY_DATE 2: March 8, 2018, 11:54am; <NewLine> REPLY_DATE 3: March 9, 2018,  9:22am; <NewLine> REPLY_DATE 4: March 9, 2018,  9:36am; <NewLine> REPLY_DATE 5: March 10, 2018, 12:11pm; <NewLine> REPLY_DATE 6: May 27, 2018,  8:34am; <NewLine> REPLY_DATE 7: May 28, 2018,  7:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 2 Likes; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> 
18677,Call pytorch script from Java?,2018-05-25T14:25:58.284Z,0,437,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone!</p><NewLine><p>Is there a way to “call” a pytorch script from a java script to perform a specific task? Specifically I have an RL agent written in java, who I want him to sometimes perform based on a trained NN I created in pytorch.</p><NewLine><p>Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/intelG,,intelG,"May 25, 2018,  2:25pm",,,,,
8849,Gym: Pendulum-v0 not solvable by vanilla policy gradient ? increase max torques?,2017-10-19T09:02:10.746Z,2,1875,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The original max torques is +/- 2 with max speed +/- 8, according to some solutions, it needs to swing several times to balance upward. I guess it is not solvable by <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""nofollow noopener"">vanilla policy gradient</a> with 1 layer MLP with 50 neurons. What might be good values for max torques and max speed such that the pendulum needs to swing only once or twice to balance upward ?</p><NewLine></div>",https://discuss.pytorch.org/u/zuoxingdong,(Xingdong Zuo),zuoxingdong,"October 19, 2017,  9:02am",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Out of curiosity, were you able to learn Pendulum-v0 with policy gradient?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I changed the max/min torque to +8/-8 and still unable to solve it with REINFORCE or REINFORCE with a baseline. Maybe I need to tune it more.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Indeed, REINFORCE is not that great in order to learn features through linear layers. Adding a prediction of values increases the speed to learn relevant features in the hidden layer. That’s why actor-critic is much more stable, and still work with 30 neurons.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/florin; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/JACKHAHA363; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> ,"REPLY_DATE 1: April 13, 2018,  5:56pm; <NewLine> REPLY_DATE 2: May 13, 2018,  6:19pm; <NewLine> REPLY_DATE 3: May 13, 2018,  8:22pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
16786,Error ion categorical multi sample,2018-04-22T03:57:59.339Z,0,370,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to sample from a variable so I can apply the reinforce algorithm on a toy problem.</p><NewLine><p>What I found was that sampling more than one scalar from a probability distribution yields spurious errors when the log probability is being computed.</p><NewLine><p>Here is an example:</p><NewLine><pre><code class=""lang-python""><NewLine>x = Variable(torch.Tensor([[0.1, 0.2, 0.1, 0.25, 0.25, 0.1]]), requires_grad=True)<NewLine>print(x.size())<NewLine>m = Categorical(x)<NewLine>action = m.sample_n(5)<NewLine>print('action: ', action.size())<NewLine># next_state, reward = env.step(action)<NewLine>loss = -m.log_prob(action.unsqueeze(0)) #* reward<NewLine>print('loss: ', loss)<NewLine>loss.backward()<NewLine></code></pre><NewLine><p>We get</p><NewLine><pre><code class=""lang-python"">torch.Size([1, 6])<NewLine>action:  torch.Size([5, 1])<NewLine>---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-115-d59bfdfba3ea&gt; in &lt;module&gt;()<NewLine>      6 print('action: ', action.size())<NewLine>      7 # next_state, reward = env.step(action)<NewLine>----&gt; 8 loss = -m.log_prob(action.unsqueeze(0)) #* reward<NewLine>      9 print('loss: ', loss)<NewLine>     10 loss.backward()<NewLine><NewLine>~/anaconda3/envs/py35/lib/python3.5/site-packages/torch/distributions.py in log_prob(self, value)<NewLine>    151             return p.gather(-1, value).log()<NewLine>    152 <NewLine>--&gt; 153         return p.gather(-1, value.unsqueeze(-1)).squeeze(-1).log()<NewLine>    154 <NewLine>    155 <NewLine><NewLine>RuntimeError: invalid argument 4: Index tensor must have same dimensions as input tensor at /opt/conda/conda-bld/pytorch_1512383260527/work/torch/lib/TH/generic/THTensorMath.c:503<NewLine></code></pre><NewLine><p>I looked into the source code for Categorical and nothing seems out of place.</p><NewLine><p>Solving the reinforce problem by meself yields no errors:</p><NewLine><pre><code class=""lang-python""><NewLine>m = Categorical(x)<NewLine>action = m.sample_n(5)<NewLine>p = -x/x.sum(-1, keepdim=True)<NewLine>reward = 1<NewLine>loss = p.log()* reward<NewLine>print('loss ', loss.size()) <NewLine>loss.mean().backward()<NewLine></code></pre><NewLine><p>Output:</p><NewLine><pre><code class=""lang-python""><NewLine>action:  torch.Size([5, 1])<NewLine>loss  torch.Size([1, 6])<NewLine></code></pre><NewLine><p>Could someone please help?</p><NewLine></div>",https://discuss.pytorch.org/u/lakehanne,,lakehanne,"April 22, 2018,  4:01am",,,,,
13434,&lsquo;Normal&rsquo; object has no attribute &lsquo;rsample&rsquo;,2018-02-08T05:29:02.496Z,0,934,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Trying to implement the pathwise derivative for a stochastic policy as mentioned <a href=""http://pytorch.org/docs/master/distributions.html#"" rel=""nofollow noopener"">here</a>. From the documentation:</p><NewLine><blockquote><NewLine><p>Another way to implement these stochastic/policy gradients would be to use the reparameterization trick from rsample() method, where the parameterized random variable can be defined as a parameterized deterministic function of a parameter-free random variable. The reparameterized sample is required to be differentiable. The code for implementing the pathwise estimation would be as follows:</p><NewLine></blockquote><NewLine><pre><code class=""lang-auto"">params = policy_network(state)<NewLine>m = Normal(*params)<NewLine># any distribution with .has_rsample == True could work based on the application<NewLine>action = m.rsample()<NewLine>next_state, reward = env.step(action)  # Assume that reward is differentiable<NewLine>loss = -reward<NewLine>loss.backward()<NewLine></code></pre><NewLine><p>Where I’m assuming that the params are the mean actions from a normal distribution over each action (some clarification on this would be good). However, when I implement this, I get the error given in the title. My action selection function is:</p><NewLine><pre><code class=""lang-auto"">def select_action(state):<NewLine>    state = torch.from_numpy(state).float().unsqueeze(0)<NewLine>    mu, state_value = model(Variable(state))<NewLine>    m = torch.distributions.Normal(mu, env.action_space.shape[0])<NewLine>    action = m.rsample()<NewLine>    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))<NewLine>    return action.data[0]<NewLine></code></pre><NewLine><p>Did I make a mistake here? Is there a working example of the pathwise derivative available for learning?</p><NewLine><p>Cheers</p><NewLine></div>",https://discuss.pytorch.org/u/seanny1986,(Sean Morrison),seanny1986,"February 8, 2018,  5:37am",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Maybe you were using PyTorch 0.3? <code>.rsample()</code> is only available in PyTorch 0.4</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/fritzo; <NewLine> ,"REPLY_DATE 1: April 21, 2018,  2:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
16084,"Forecast of Power generation plant, with LSTM?",2018-04-07T11:36:35.753Z,2,440,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everybody,</p><NewLine><p>I have been involved with pytorch only recently, so I ask for some leniency with me.<br/><NewLine>I do not know if I answer correctly to my current project or choose the right approaches. I hope you can help me.</p><NewLine><p>I have the data of a power generation plant as time series and would like to predict how some values will behave. To be precise I have 6 input parameters and 4 output parameters, the 4 output parameters I would like to predict some time steps into the future.<br/><NewLine>I tried to use LSTM to forecast my exit savings. Unfortunately I did not succeed here.</p><NewLine><p>Can you give a few tips or possibly provide prommier approaches.</p><NewLine><p>Best regards</p><NewLine></div>",https://discuss.pytorch.org/u/jensen_m,,jensen_m,"April 7, 2018, 11:36am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’d probably base my first stab on the <a href=""https://github.com/pytorch/examples/tree/master/time_sequence_prediction"" rel=""nofollow noopener"">time series example</a>.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your quick response.</p><NewLine><p>I have spent some time in the last few days, unfortunately I have to say I still do not quite understand how I can use it with 4 input parameters and 6 output parameters.</p><NewLine><p>Can you help me?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you don’t have a feedback feom output to next input, typically, you’d have a Linear (4, hidden_size) an RNN/GRU/LSTM with hidden_size input/output of sort and a Linear(hidden_size, 6).</p><NewLine><p>Depending on the nature of the data, there are customerary models and/or preprocessing. For example Graves’ classic handwriting model uses offsets instead of coordinates to make the prediction targets more stationary and uses a Mixture Density Network to model the outputs.<br/><NewLine><a href=""https://github.com/t-vi/pytorch-tvmisc/blob/master/misc/graves_handwriting_generation.ipynb"" rel=""nofollow noopener"">I have implemented the handwriting generation in PyTorch here</a>, but I’d highly recommend reading the original paper.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jensen_m; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: April 7, 2018,  1:03pm; <NewLine> REPLY_DATE 2: April 13, 2018, 10:57am; <NewLine> REPLY_DATE 3: April 13, 2018,  3:34pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
15329,Episodic Policy Gradient in Pytorch,2018-03-22T21:26:55.436Z,0,633,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to translate <a href=""https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5"" rel=""nofollow noopener"">Andrej Karpathy’s pong example</a> to Pytorch. My problem comes down to the loss. I cannot backpropagate at each timestep because I need to wait until the end of an episode to be able to compute de discounted returns for each step. I need to be able to compute the gradient with respect to my model’s parameters for the loss function of every step.</p><NewLine><p><strong>Here’s what I do :</strong></p><NewLine><p>My model outputs a sigmoid, so it basically chooses between two actions. I sample an action from my policy network <code>prob_a = model(x)</code> and the log-likelihood for that step is defined as <code>LL = torch.log(y - a)</code> where I set <code>y = 1</code> if the action taken was <code>a = 1</code> and <code>y = 0</code> if the action taken was <code>a = 0</code>. I accumulate those log-likelihoods (which are autograd.Variable) for every step in <code>LL_list</code>. When the episode ends, I also compute the discounted return for every step in <code>G_list</code>. I then try to compute the loss over the whole episode in this way :</p><NewLine><pre><code>LLs = torch.stack(LL_list)<NewLine>Gs= Variable(torch.FloatTensor(G_list))<NewLine><NewLine>loss = torch.mean(LLs * Gs)<NewLine>loss.backward()<NewLine></code></pre><NewLine><p>Episodes last for about a thousand steps (so that is the size of LLs and Gs).</p><NewLine><p><strong>What happens :</strong></p><NewLine><p>When I run that, the execution gets stuck forever in <code>loss.backward()</code>.</p><NewLine><p><strong>Question :</strong></p><NewLine><p>I feel like I might be missing something quite important here (maybe Pytorch actually builds an immense graph and takes forever to backpropagate through it). Does anyone have an idea about what might be going on?</p><NewLine><p>I guess in a more general setting, my question would be : How to compute the loss that involves Variables obtained through several propagations through the model ?</p><NewLine></div>",https://discuss.pytorch.org/u/julienroy13,(Julien Roy),julienroy13,"March 22, 2018,  9:43pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found a workaround. Apparently torch.stack() on my list of Variables made backpropagation ridiculously long. By computing my loss by iterating over my list of Variables instead of converting it in a single tensor seem to have resolved the issue. However, I stil don’t understand why torch.stack() would cause such a slow down.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>For me I use torch.cat to concatenate a list of torch tensors/variables (i.e a list of log prob or actions).</p><NewLine><p>I’ve been able to do batch policy gradient this way very fast (faster than iterating through the list and summing up the loss).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/julienroy13; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/G-Wang; <NewLine> ,"REPLY_DATE 1: March 23, 2018,  3:07am; <NewLine> REPLY_DATE 2: April 11, 2018,  7:27pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
13764,Network always predicts a single move,2018-02-18T17:36:22.381Z,3,421,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a game with a square board in which the player moves around. They cannot move onto the last square that they were previously on, or out of bounds, but they can move around anywhere inside the square.</p><NewLine><p>The game state is represented as a matrix. We mark the edge of the play area with a magic number and the player with a magic number, as well as the player’s previous space. There are a few other magic numbers for items in the play area that are not used (yet). The player starts at a random location.</p><NewLine><p>We scale all of these between 0 and 1, then flatten it as input to the agent. The reward is 1 for making a proper step, and -1 for stepping out of boards or on it’s previous location (the game resets in this case). There are four output moves corresponding to each direction. This agent uses vanilla policy gradients based on the reinforce.py example in the PyTorch repository, but we’ve expanded the network size to be 2 hidden layers with 128 nodes.</p><NewLine><p>In nearly all cases, the agent seems to very quickly converge on moving in only a single or two directions until it hits out of bounds. I think this is a local minima, since by going only in a corner direction or single direction it never accidentally moves onto its previous space. But this is clearly not optimal since it does not learn to avoid out of bounds.</p><NewLine><p>I’ve tried using convolutional layers, various network sizes, various learning parameters, verifying the input data, using an actor-critic model, encouraging exploration by randomly forcing a move with certain probability, different rewards, slightly different game mechanics, and a range of other things but I must be missing something! Everything I’ve tried results in the (believed) local minimum, with the exception of forcing a random move which also results in an exploding gradient and crash as it gets close to the local minimum (log probability of the forced move becomes huge as the probability approaches zero). Any advice for debugging this?</p><NewLine></div>",https://discuss.pytorch.org/u/chrisfosterelli1,(Chris Foster),chrisfosterelli1,"February 18, 2018,  5:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What are you using for the state of your environment? It’s possible you have already accounted for this, but the agent needs some way to know where it was in the previous state of the environment. Otherwise, the only way it will be able to ensure it does not accidentally return to its previous location is to always go a single direction. If you have not accounted for this, try stacking the current observation with the previous observation for your state. Or a less computationally expensive state would be to subtract some portion of the previous observation from the current observation.</p><NewLine><p>If you have a sufficient state and the problem continues to persist, it’s probably a bug in your code, but there is the chance that your hyperparameters need tuning. For something as simple as your game, good rules of thumb are using .99 for your reward discount factor, and .001-.0001 for your learning rate (this will depend on if you are averaging or summing the gradient terms). Also, a good piece of advice (from John Schulman) is to try increasing the amount of data you use per update. You probably won’t need anything more than 20,000 frames per update(?), but honestly this is one of those black magic things that is problem and implementation dependent.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the response Satchel! We create a 2D matrix representing the square board, bordered by values of 0.8, and represent the agent with 0.1, and the agent’s previous location with 0.2. All other locations are 0. There are some other values I’d like to include in the grid eventually, but right now I’m trying to get a basic sample working. The 2D matrix is flattened prior to input.</p><NewLine><p>I’ve tried learning rates ranging from 0.01 and 0.0001, and currently use 0.99 for the discount factor. I can try experimenting with other values.</p><NewLine><p>However, I think your suggestion of amount of data per update is interesting. Since the agent dies a lot at first, most of the episodes are only 1-4 moves in length. Currently, the update is done at the end of every episode. It sounds like this might be way too frequent, so I’ll try increasing this.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Nice, you’re state sounds great.</p><NewLine><p>Yeah, try accumulating like 100-200 of those episodes’ gradients and average them for the update (the normalizing factor can be included in your learning rate). It’d be great if you let me know what you end up doing, and how it goes!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey Satchel,</p><NewLine><p>Sorry for the long delay! I wanted to be sure to follow up on this. We implemented gradient averaging for the update, as well as a couple smaller tweaks like using an entropy-based loss component, and it did the trick! The agent seemed to move out of its local minima and start exploring actual strategies that led to a much longer episode length.</p><NewLine><p>Thank you for your help!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/grantsrb; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/chrisfosterelli1; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/grantsrb; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/chrisfosterelli1; <NewLine> ,"REPLY_DATE 1: February 18, 2018,  9:45pm; <NewLine> REPLY_DATE 2: February 18, 2018, 10:25pm; <NewLine> REPLY_DATE 3: February 18, 2018, 11:05pm; <NewLine> REPLY_DATE 4: March 24, 2018,  4:30am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
14911,RuntimeError - size mismatch when using qnetwork with eligibility trace,2018-03-14T13:57:00.283Z,1,768,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello there,</p><NewLine><p>I have tried to use some of existing code that and get the same error as stated in the title and can also be seen below. I get my state from my environment which is simulink because we are doing a pump project for the biggest circular pump manufacture in the world.</p><NewLine><p>output:</p><NewLine><pre><code class=""lang-auto"">action output &lt;class 'numpy.ndarray'&gt;<NewLine>hej<NewLine>eli &lt;class 'torch.autograd.variable.Variable'&gt; value Variable containing:<NewLine> 18.9000<NewLine> 19.0000<NewLine>[torch.FloatTensor of size 2]<NewLine><NewLine>Traceback (most recent call last):<NewLine>  File ""ai_eligibility.py"", line 139, in &lt;module&gt;<NewLine>    inputs, targets = eligibility_trace(batch)<NewLine>  File ""ai_eligibility.py"", line 104, in eligibility_trace<NewLine>    output = qnetwork(input)<NewLine>  File ""C:\Users\koch\Anaconda3\envs\py36_pytorch_kivy\lib\site-packages\torch\nn\modules\module.py"", line 325, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""ai_eligibility.py"", line 47, in forward<NewLine>    x = F.relu(self.fc1(state))<NewLine>  File ""C:\Users\koch\Anaconda3\envs\py36_pytorch_kivy\lib\site-packages\torch\nn\modules\module.py"", line 325, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""C:\Users\koch\Anaconda3\envs\py36_pytorch_kivy\lib\site-packages\torch\nn\modules\linear.py"", line 55, in forward<NewLine>    return F.linear(input, self.weight, self.bias)<NewLine>  File ""C:\Users\koch\Anaconda3\envs\py36_pytorch_kivy\lib\site-packages\torch\nn\functional.py"", line 837, in linear<NewLine>    output = input.matmul(weight.t())<NewLine>  File ""C:\Users\koch\Anaconda3\envs\py36_pytorch_kivy\lib\site-packages\torch\autograd\variable.py"", line 386, in matmul<NewLine>    return torch.matmul(self, other)<NewLine>  File ""C:\Users\koch\Anaconda3\envs\py36_pytorch_kivy\lib\site-packages\torch\functional.py"", line 168, in matmul<NewLine>    return torch.mm(tensor1.unsqueeze(0), tensor2).squeeze_(0)<NewLine>RuntimeError: size mismatch, m1: [1 x 2], m2: [1 x 30] at c:\anaconda2\conda-bld\pytorch_1513133520683\work\torch\lib\th\generic/THTensorMath.c:1416<NewLine></code></pre><NewLine><p>This is my ai code:</p><NewLine><pre><code class=""lang-auto""># AI for pump<NewLine><NewLine><NewLine><NewLine># Importing the libraries<NewLine>import numpy as np<NewLine>import random<NewLine>import torch<NewLine>import math<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine>from torch.autograd import Variable<NewLine><NewLine><NewLine># Importing the other Python files<NewLine>import experience_replay_4_eligibility<NewLine>import env<NewLine><NewLine><NewLine># Part 1 - Building the AI<NewLine><NewLine># Making the brain<NewLine><NewLine># Creating the architecture of the Neural Network<NewLine>class Qnetwork(nn.Module): #inherinting from nn.Module<NewLine>    <NewLine>    def __init__(self, input_size, nb_action):<NewLine>        super(Qnetwork, self).__init__()<NewLine>        # Input and output neurons<NewLine>        self.input_size = input_size<NewLine>        self.nb_action = nb_action<NewLine>        self.fc1 = nn.Linear(input_size, 30)<NewLine>        self.fc2 = nn.Linear(30, nb_action) # 30 neurons in hidden layer<NewLine>    <NewLine>    # For function that will activate neurons and perform forward propagation<NewLine>    def forward(self, state):<NewLine>        x = F.relu(self.fc1(state))<NewLine>        q_values = self.fc2(x)<NewLine>        return q_values<NewLine><NewLine># Making the body<NewLine><NewLine>class SoftmaxBody(nn.Module):<NewLine>    <NewLine>    def __init__(self, T):<NewLine>        super(SoftmaxBody, self).__init__()<NewLine>        self.T = T<NewLine><NewLine>    def forward(self, outputs):<NewLine>        probs = F.softmax(outputs * self.T)   <NewLine>        actions = probs.multinomial()<NewLine>        return actions<NewLine><NewLine># Making the AI<NewLine><NewLine>class AI:<NewLine><NewLine>    def __init__(self, brain, body):<NewLine>        self.brain = brain<NewLine>        self.body = body<NewLine><NewLine>    def __call__(self, inputs):<NewLine>        input = Variable(torch.from_numpy(np.array(inputs, dtype = np.float32)))<NewLine>        output = self.brain(input)<NewLine>        actions = self.body(output)<NewLine>        return actions.data.numpy()<NewLine><NewLine><NewLine><NewLine># Part 2 - Training the AI with Deep Convolutional Q-Learning<NewLine><NewLine>#Getting the simulink environment<NewLine># Creating Connection for sender and receiver socket<NewLine>env_simu = env.environment()<NewLine>env_simu.createServerSockets()<NewLine># Building an AI<NewLine>qnetwork = Qnetwork(1,20)<NewLine>softmax_body = SoftmaxBody(T = 1.0)<NewLine>ai = AI(brain = qnetwork, body = softmax_body)<NewLine>env_simu.sendAction(0)<NewLine><NewLine># Setting up Experience Replay<NewLine>n_steps = experience_replay_4_eligibility.NStepProgress(env = env_simu, ai = ai, n_step = 10)<NewLine>memory = experience_replay_4_eligibility.ReplayMemory(n_steps = n_steps, capacity = 10000)<NewLine><NewLine># Implementing Eligibility Trace<NewLine>def eligibility_trace(batch):<NewLine>    gamma = 0.99<NewLine>    inputs = []<NewLine>    targets = []<NewLine>    for series in batch:<NewLine>        input = Variable(torch.from_numpy(np.array([series[0].state, series[-1].state], dtype = np.float32)))<NewLine>        print('eli', type(input),'value',input)<NewLine>        output = qnetwork(input)<NewLine>        cumul_reward = 0.0 if series[-1].done else output[1].data.max()<NewLine>        for step in reversed(series[:-1]):<NewLine>            cumul_reward = step.reward + gamma * cumul_reward<NewLine>        state = series[0].state<NewLine>        target = output[0].data<NewLine>        target[series[0].action] = cumul_reward<NewLine>        inputs.append(state)<NewLine>        targets.append(target)<NewLine>    return torch.from_numpy(np.array(inputs, dtype = np.float32)), torch.stack(targets)<NewLine><NewLine># Making the moving average on 100 steps<NewLine>class MA:<NewLine>    def __init__(self, size):<NewLine>        self.list_of_rewards = []<NewLine>        self.size = size<NewLine>    def add(self, rewards):<NewLine>        if isinstance(rewards, list):<NewLine>            self.list_of_rewards += rewards<NewLine>        else:<NewLine>            self.list_of_rewards.append(rewards)<NewLine>        while len(self.list_of_rewards) &gt; self.size:<NewLine>            del self.list_of_rewards[0]<NewLine>    def average(self):<NewLine>        return np.mean(self.list_of_rewards)<NewLine>ma = MA(100)<NewLine><NewLine># Training the AI<NewLine>loss = nn.MSELoss()<NewLine>optimizer = optim.Adam(qnetwork.parameters(), lr = 0.001)<NewLine>nb_epochs = 100<NewLine>for epoch in range(1, nb_epochs + 1):<NewLine>    memory.run_steps(200)<NewLine>    for batch in memory.sample_batch(128):<NewLine>        print('hej')<NewLine>        inputs, targets = eligibility_trace(batch)<NewLine>        inputs, targets = Variable(inputs), Variable(targets)<NewLine>        predictions = qnetwork(inputs)<NewLine>        loss_error = loss(predictions, targets)<NewLine>        optimizer.zero_grad()<NewLine>        loss_error.backward()<NewLine>        optimizer.step()<NewLine>    rewards_steps = n_steps.rewards_steps()<NewLine>    ma.add(rewards_steps)<NewLine>    avg_reward = ma.average()<NewLine>    print(""Epoch: %s, Average Reward: %s"" % (str(epoch), str(avg_reward)))<NewLine>    if avg_reward &gt;= 1500:<NewLine>        print(""Congratulations, your AI wins"")<NewLine>        break<NewLine></code></pre><NewLine><p>And this is my experience replay code:</p><NewLine><pre><code class=""lang-auto""># Experience Replay<NewLine><NewLine># Importing the libraries<NewLine>import numpy as np<NewLine>from collections import namedtuple, deque<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine>from torch.autograd import Variable<NewLine><NewLine># Defining one Step<NewLine>Step = namedtuple('Step', ['state', 'action', 'reward'])<NewLine><NewLine># Making the AI progress on several (n_step) steps<NewLine>last_distance = 0<NewLine>class NStepProgress:<NewLine>    <NewLine>    def __init__(self, env, ai, n_step):<NewLine>        self.ai = ai<NewLine>        self.rewards = []<NewLine>        self.env = env<NewLine>        self.n_step = n_step<NewLine>    <NewLine>    def __iter__(self):<NewLine>        global last_distance<NewLine>        goalT1 = 20<NewLine>        state = self.env.receiveState()<NewLine>        state = state[0]<NewLine>        print('staaaate',state)<NewLine>        state = [state]<NewLine>        state = torch.Tensor(state).float().unsqueeze(0)<NewLine>        history = deque()<NewLine>        reward = 0.0<NewLine>        print('reward',type(reward))<NewLine>        print('state',type(state))<NewLine>        while True: # HUUUUSK DEN LUDER STATE TIL TENSOR ELLER et eller ANDET<NewLine>            print('state', type(np.array(state)),'value',np.array(state))<NewLine>            action = self.ai(np.array(state))<NewLine>            print('action output', type(action))<NewLine>            action = int(action[0][0])<NewLine>            self.env.sendAction(action+1)<NewLine>            next_state = self.env.receiveState()<NewLine>            next_state = next_state[0]<NewLine>            distance = ((abs(goalT1 - next_state)))<NewLine>            #next_state, r, is_done, _ = self.env.step(action)<NewLine>            <NewLine>            # Reward Policy<NewLine>            if 0 &lt;= distance &lt;= 0.2:<NewLine>                r = 1<NewLine>                if distance &lt; last_distance:<NewLine>                    r = r*0.5<NewLine>            elif distance &lt; last_distance:<NewLine>                r = 0.1<NewLine>            else:<NewLine>                r = -0.5*(distance)<NewLine><NewLine><NewLine>            reward += r<NewLine>            history.append(Step(state = state, action = action, reward = r))<NewLine>            while len(history) &gt; self.n_step + 1:<NewLine>                history.popleft()<NewLine>            if len(history) == self.n_step + 1:<NewLine>                yield tuple(history)<NewLine>            next_state = [next_state]<NewLine>            next_state = torch.Tensor(next_state).float().unsqueeze(0)<NewLine>            state = next_state<NewLine>            self.rewards.append(reward)<NewLine>            if r == 1:<NewLine>                if len(history) &gt; self.n_step + 1:<NewLine>                    history.popleft()<NewLine>                while len(history) &gt;= 1:<NewLine>                    yield tuple(history)<NewLine>                    history.popleft()<NewLine>                self.rewards.append(reward)<NewLine>                reward = 0.0<NewLine>                state = self.env.receiveState()<NewLine>                state = state[0]<NewLine>                state = [state]<NewLine>                state = torch.Tensor(state).float().unsqueeze(0)<NewLine>                history.clear()<NewLine>    <NewLine>    def rewards_steps(self):<NewLine>        rewards_steps = self.rewards<NewLine>        self.rewards = []<NewLine>        return rewards_steps<NewLine><NewLine># Implementing Experience Replay<NewLine><NewLine>class ReplayMemory:<NewLine>    <NewLine>    def __init__(self, n_steps, capacity = 10000):<NewLine>        self.capacity = capacity<NewLine>        self.n_steps = n_steps<NewLine>        self.n_steps_iter = iter(n_steps)<NewLine>        self.buffer = deque()<NewLine><NewLine>    def sample_batch(self, batch_size): # creates an iterator that returns random batches<NewLine>        ofs = 0<NewLine>        vals = list(self.buffer)<NewLine>        np.random.shuffle(vals)<NewLine>        while (ofs+1)*batch_size &lt;= len(self.buffer):<NewLine>            yield vals[ofs*batch_size:(ofs+1)*batch_size]<NewLine>            ofs += 1<NewLine><NewLine>    def run_steps(self, samples):<NewLine>        while samples &gt; 0:<NewLine>            entry = next(self.n_steps_iter) # 10 consecutive steps<NewLine>            self.buffer.append(entry) # we put 200 for the current episode<NewLine>            samples -= 1<NewLine>        while len(self.buffer) &gt; self.capacity: # we accumulate no more than the capacity (10000)<NewLine>            self.buffer.popleft()<NewLine></code></pre><NewLine><p>i know the problem is in the eligibility trace function when i use the batch from the memory.sample_batch(128), and then put two values into the numpy array but i do not know how this happen or how i can code around this… I hoped you could help me on this on one because i have been staring at this problem for 2 days…</p><NewLine></div>",https://discuss.pytorch.org/u/qLience,(Søren Koch),qLience,"March 14, 2018,  1:57pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote"" data-post=""1"" data-topic=""14911""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/qlience/40/3705_1.png"" width=""20""/> qLience:</div><NewLine><blockquote><NewLine><p>File “C:\Users\koch\Anaconda3\envs\py36_pytorch_kivy\lib\site-packages\torch\autograd\variable.py”, line 386, in matmul<br/><NewLine>return torch.matmul(self, other)<br/><NewLine>File “C:\Users\koch\Anaconda3\envs\py36_pytorch_kivy\lib\site-packages\torch\functional.py”, line 168, in matmul<br/><NewLine>return torch.mm(tensor1.unsqueeze(0), tensor2).squeeze_(0)<br/><NewLine>RuntimeError: size mismatch, m1: [1 x 2], m2: [1 x 30] at c:\anaconda2\conda-bld\pytorch_1513133520683\work\torch\lib\th\generic/THTensorMath.c:1416</p><NewLine></blockquote><NewLine></aside><NewLine><p>It sounds like you’re doing a matrix multiply where the first matrix is 1 by 2 while the second matrix is 1 by 30. The shapes of these matrices are incompatible.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>That error is from deep inside the pytorch source code. The offending line in the model seems to be <code>x = F.relu(self.fc1(state))</code></p><NewLine><p>My guess is that Qnetwork was initialised with an <code>input_size</code> that doesn’t match the size of the state it receives.</p><NewLine><p>Going a little furthur…</p><NewLine><pre><code class=""lang-auto"">input = Variable(torch.from_numpy(np.array([series[0].state, series[-1].state], dtype = np.float32)))<NewLine></code></pre><NewLine><p>somehow produces a 1d tensor containing only 2 elements.</p><NewLine><p>That must mean that <code>series[0].state</code> is a number rather than a vector of 20 numbers.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/richard; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jpeg729; <NewLine> ,"REPLY_DATE 1: March 14, 2018,  3:02pm; <NewLine> REPLY_DATE 2: March 15, 2018,  9:17pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
14779,GPU memory usage issue of A3C in GPU,2018-03-11T21:09:16.834Z,0,449,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I implemented A3C algorithm to play starcraft2. The shared model and local model in each worker is put in GPU by using <code>model.cuda()</code>. This method is much like <a href=""https://github.com/dgriff777/rl_a3c_pytorch"" rel=""nofollow noopener"">A3G</a>, the only difference is that the shared model in A3G is in CPU, not GPU.</p><NewLine><p>[Issue]<br/><NewLine>A3C needs to collect n-step experience to update the local model. I use <a href=""https://github.com/JIElite/pysc2-A3C/blob/master/sc2/worker.py#"" rel=""nofollow noopener"">a list</a> to store history n-step trajectory here. However, the memory cost of each worker in GPU is 1200MB, which seems to be much higher than the <a href=""https://github.com/JIElite/pysc2-A3C/blob/master/sc2/worker.py#L75"" rel=""nofollow noopener"">stored trajectory experience</a>. Is it reasonable? Or it just caused by bad programming?</p><NewLine><p>How can we reduce the memory usage in this case?</p><NewLine><p>Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/Jie_Han_Chen,(Jie Han Chen),Jie_Han_Chen,"March 11, 2018,  9:15pm",,,,,
14710,Can A3C share model in multiple GPU?,2018-03-10T08:59:16.153Z,1,1091,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Accroding to original A3C, there exist 1 shared global model and multiple local models corresponded to each worker process. A3C is designed for multi-core CPU for parallel processing with poor GPU usage.</p><NewLine><p>With little modification like <code>model.cuda()</code> we can run the model on GPU to speed up training.</p><NewLine><p>[Question]<br/><NewLine>If we have 2 GPUs in computer. Can we run the A3C just without modification to share the global model between each worker?</p><NewLine></div>",https://discuss.pytorch.org/u/Jie_Han_Chen,(Jie Han Chen),Jie_Han_Chen,"March 10, 2018,  8:59am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>See post here:</p><NewLine><aside class=""quote"" data-post=""4"" data-topic=""13941""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/dgriff/40/1361_1.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/trying-to-modify-this-code-to-use-cuda/13941/4"">Trying to modify this code to use cuda</a> <a class=""badge-wrapper bullet"" href=""/c/reinforcement-learning""><span class=""badge-category-bg"" style=""background-color: #AB9364;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""A section to discuss RL implementations, research, problems"">reinforcement-learning</span></a><NewLine></div><NewLine><blockquote><NewLine>    Hey Checkout my repo here I implemented a3c with gpu use and hog wild training. I call it A3G and it’s works very well it’s actually even faster at training Atari games than mass parallelized evolutionary algorithms with also much higher overall performance. <NewLine><NewLine><NewLine>update Just realized you had already visited repo and we spoke on issues on github. <img alt=""open_mouth"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/open_mouth.png?v=5"" title=""open_mouth""/><img alt=""rofl"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/rofl.png?v=5"" title=""rofl""/> <NewLine>Guess you can disregard then lol<NewLine>  </blockquote><NewLine></aside><NewLine><p>It will be slower if you share gpu to gpu. This way is faster cause you get benefits from gpu and cpu</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=5"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, <a class=""mention"" href=""/u/dgriff"">@dgriff</a>:</p><NewLine><p>In your A3G, we put the shared model in CPU. Why don’t we put the shared model in GPU?</p><NewLine><p>Thank you.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>cause cpu is faster for sharing model cause we can do “hogwild training”, sharing without locks asynchronous updates. The act of acquiring locks and releasing locks is actually quite time consuming. This is a benefit of CPU has over GPU. GPU requires locks for sharing in almost all cases except for atomic operations</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dgriff; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Jie_Han_Chen; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Jie_Han_Chen; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/dgriff; <NewLine> ,"REPLY_DATE 1: March 10, 2018,  9:06am; <NewLine> REPLY_DATE 2: March 10, 2018,  9:16am; <NewLine> REPLY_DATE 3: March 10, 2018,  4:06pm; <NewLine> REPLY_DATE 4: March 10, 2018,  5:51pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 2 Likes; <NewLine> 
14544,"&ldquo;RuntimeError: Variable data has to be a tensor, but got Variable&rdquo; with sample",2018-03-07T15:49:07.219Z,3,5306,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey there,</p><NewLine><p>i have some troubles when i want to use my batch from my experience replay. as shown below:</p><NewLine><pre><code class=""lang-auto"">, in &lt;lambda&gt;<NewLine>    return map(lambda x: Variable(torch.cat(x, 0)), samples)<NewLine>RuntimeError: Variable data has to be a tensor, but got Variable<NewLine></code></pre><NewLine><p>I push my experience seen in “def update” but when i want to use the batch from the experience replay shown sample (def ReplayMemory) but when i want to use it then i get the error but i have tried a lot of things to make it work but i want to hear your suggestion!!</p><NewLine><p>Here is my code:</p><NewLine><pre><code class=""lang-auto""># AI for pump<NewLine><NewLine># Importing the libraries<NewLine><NewLine>import numpy as np<NewLine>import random # random samples from different batches (experience replay)<NewLine>import os # For loading and saving brain<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim # for using stochastic gradient descent<NewLine>import torch.autograd as autograd # Conversion from tensor (advanced arrays) to avoid all that contains a gradient<NewLine># We want to put the tensor into a varaible taht will also contain a<NewLine># gradient and to this we need:<NewLine>from torch.autograd import Variable<NewLine># to convert this tensor into a variable containing the tensor and the gradient<NewLine><NewLine><NewLine># Initializing and setting the variance of a tensor of weights<NewLine>def normalized_columns_initializer(weights, std=1.0):<NewLine>    out = torch.randn(weights.size())<NewLine>    out *= std / torch.sqrt(out.pow(2).sum(1,keepdim=True).expand_as(out)) # thanks to this initialization, we have var(out) = std^2<NewLine>    return out<NewLine><NewLine># Initializing the weights of the neural network in an optimal way for the learning<NewLine>def weights_init(m):<NewLine>    classname = m.__class__.__name__ # python trick that will look for the type of connection in the object ""m"" (convolution or full connection)<NewLine>    if classname.find('Linear') != -1:<NewLine>        weight_shape = list(m.weight.data.size()) #?? list containing the shape of the weights in the object ""m""<NewLine>        fan_in = weight_shape[1] # dim1<NewLine>        fan_out = weight_shape[0] # dim0<NewLine>        w_bound = np.sqrt(6. / (fan_in + fan_out)) # weight bound<NewLine>        m.weight.data.uniform_(-w_bound, w_bound) # generating some random weights of order inversely proportional to the size of the tensor of weights<NewLine>        m.bias.data.fill_(0) # initializing all the bias with zeros<NewLine><NewLine># Creating the architecture of the Neural Network<NewLine>class Network(nn.Module): #inherinting from nn.Module<NewLine>    def __init__(self, input_size, nb_action): #[self,input neuroner, output neuroner]<NewLine>        super(Network, self).__init__() #inorder to use modules in torch.nn<NewLine>        # Input and output neurons<NewLine>        self.lstm = nn.LSTMCell(input_size, 30) # making an LSTM (Long Short Term Memory) to learn the temporal properties of the input<NewLine>        self.fcL = nn.Linear(30, nb_action) # full connection of the<NewLine>        self.apply(weights_init) # initilizing the weights of the model with random weights<NewLine>        self.fcL.weight.data = normalized_columns_initializer(self.fcL.weight.data, 0.01) # setting the standard deviation of the fcL tensor of weights to 0.01<NewLine>        self.fcL.bias.data.fill_(0) # initializing the actor bias with zeros<NewLine>        self.lstm.bias_ih.data.fill_(0) # initializing the lstm bias with zeros<NewLine>        self.lstm.bias_hh.data.fill_(0) # initializing the lstm bias with zeros<NewLine>        #self.train() # setting the module in ""train"" mode to activate the dropouts and batchnorms<NewLine>		<NewLine>    # For function that will activate neurons and perform forward propagation<NewLine>    def forward(self, inputs):<NewLine>        state, (hx, cx) = inputs <NewLine>        hx, cx = self.lstm(state, (hx, cx)) # the LSTM takes as input x and the old hidden &amp; cell states and ouputs the new hidden &amp; cell states<NewLine>        x = hx # getting the useful output, which are the hidden states (principle of the LSTM)<NewLine>        q_values = self.fcL(x)<NewLine>        return q_values, (hx, cx) <NewLine><NewLine><NewLine># Implementing Experience Replay<NewLine># We know that RL is based on MDP<NewLine># So going from one state(s_t) to the next state(s_t+1)<NewLine># We gonna put 100 transition between state into what we call the memory<NewLine># So we can use the distribution of experience to make a decision<NewLine>class ReplayMemory(object):<NewLine>    <NewLine>    def __init__(self, capacity):<NewLine>        self.capacity = capacity #100 transitions<NewLine>        self.memory = [] #memory to save transitions<NewLine>    <NewLine>    # pushing transitions into memory with append<NewLine>    #event=transition<NewLine>    def push(self, event):<NewLine>        self.memory.append(event)<NewLine>        if len(self.memory) &gt; self.capacity: #memory only contain 100 events<NewLine>            del self.memory[0] #delete first transition from memory if there is more that 100<NewLine>    <NewLine>    # taking random sample<NewLine>    def sample(self, batch_size):<NewLine>        #Creating variable that will contain the samples of memory<NewLine>        #zip =reshape function if list = ((1,2,3),(4,5,6)) zip(*list)= (1,4),(2,5),(3,6)<NewLine>        #                (state,action,reward),(state,action,reward)  <NewLine>        samples = zip(*random.sample(self.memory, batch_size))<NewLine>        #This is to be able to differentiate with respect to a tensor<NewLine>        #and this will then contain the tensor and gradient<NewLine>        #so for state,action and reward we will store the seperately into some<NewLine>        #bytes which each one will get a gradient<NewLine>        #so that eventually we'll be able to differentiate each one of them<NewLine>        return map(lambda x: Variable(torch.cat(x, 0)), samples)<NewLine><NewLine># Implementing Deep Q Learning<NewLine><NewLine>class Dqn():<NewLine>    <NewLine>    def __init__(self, params):<NewLine>        self.gamma = params.gamma #self.gamma gets assigned to input argument<NewLine>        self.tau = params.tau<NewLine>        # Sliding window of the evolving mean of the last 100 events/transitions<NewLine>        self.reward_window = []<NewLine>        #Creating network with network class<NewLine>        self.model = Network(params.input_size, params.action_size)<NewLine>        #creating memory with memory class<NewLine>        #We gonna take 100000 samples into memory and then we will sample from this memory to <NewLine>        #to get a snakk number of random transitions<NewLine>        self.memory = ReplayMemory(100000)<NewLine>        #creating optimizer (stochastic gradient descent)<NewLine>        self.optimizer = optim.Adam(self.model.parameters(), lr = params.lr) #learning rate<NewLine>        #input vector which is batch of input observations<NewLine>        #by unsqeeze we create a fake dimension to this is<NewLine>        #what the network expect for its inputs<NewLine>        #have to be the first dimension of the last_state<NewLine>        self.last_state = torch.Tensor(params.input_size).unsqueeze(0)<NewLine>        #Inilizing<NewLine>        self.last_action = 0<NewLine>        self.last_reward = 0<NewLine>        #Inilizing<NewLine>        self.hx = Variable(torch.zeros(1, 30))<NewLine>        self.cx = Variable(torch.zeros(1, 30))<NewLine>        self.initialise = 0 # Initialise to zero at first iteration hx cx<NewLine>    <NewLine>    def select_action(self, state):<NewLine>        #LSTM<NewLine>        if self.initialise == 0:<NewLine>            self.initialise +=1<NewLine>        else:  # The hx,cx from the previous iteration<NewLine>            self.cx = Variable(self.cx.data) <NewLine>            self.hx = Variable(self.hx.data) <NewLine>        q_values, (self.hx, self.cx) = self.model((Variable(state), (self.hx, self.cx)))<NewLine>        probs = F.softmax((q_values)*self.tau,dim=1)<NewLine>        #create a random draw from the probability distribution created from softmax<NewLine>        action = probs.multinomial()<NewLine>        return action.data[0,0]<NewLine>    <NewLine>    # See section 5.3 in AI handbook<NewLine>    def learn(self, batch_state, batch_next_state, batch_reward, batch_action, batch_hx, batch_cx):<NewLine>        outputs = self.model(batch_state,(batch_hx, batch_cx)).gather(1, batch_action.unsqueeze(1)).squeeze(1)<NewLine>        #next input for target see page 7 in attached AI handbook<NewLine>        next_outputs = self.model(batch_next_state,(batch_hx, batch_cx)).detach().max(1)[0]<NewLine>        target = self.gamma*next_outputs + batch_reward<NewLine>        #Using hubble loss inorder to obtain loss<NewLine>        td_loss = F.smooth_l1_loss(outputs, target)<NewLine>        #using  lass loss/error to perform stochastic gradient descent and update weights <NewLine>        self.optimizer.zero_grad() #reintialize the optimizer at each iteration of the loop<NewLine>        #This line of code that backward propagates the error into the NN<NewLine>        #td_loss.backward(retain_variables = True) #userwarning<NewLine>        td_loss.backward(retain_graph = True)<NewLine>		#And this line of code uses the optimizer to update the weights<NewLine>        self.optimizer.step()<NewLine>    <NewLine>    def update(self, reward, new_signal):<NewLine>        #Updated one transition and we have dated the last element of the transition<NewLine>        #which is the new state<NewLine>        new_state = torch.Tensor(new_signal).float().unsqueeze(0)<NewLine>        self.memory.push((self.last_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward]), self.hx, self.cx))<NewLine>        #After ending in a state its time to play a action<NewLine>        action = self.select_action(new_state)<NewLine>        if len(self.memory.memory) &gt; 100:<NewLine>            batch_state, batch_next_state, batch_action, batch_reward, batch_hx, batch_cx = self.memory.sample(100)<NewLine>            self.learn(batch_state, batch_next_state, batch_reward, batch_action, batch_hx, batch_cx)<NewLine>        self.last_action = action<NewLine>        self.last_state = new_state<NewLine>        self.last_reward = reward<NewLine>        self.reward_window.append(reward)<NewLine>        if len(self.reward_window) &gt; 1000:<NewLine>            del self.reward_window[0]<NewLine>        return action<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/qLience,(Søren Koch),qLience,"March 7, 2018,  3:49pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote"" data-post=""1"" data-topic=""14544""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/qlience/40/3705_1.png"" width=""20""/> qLience:</div><NewLine><blockquote><NewLine><p>return map(lambda x: Variable(torch.cat(x, 0)), samples)</p><NewLine></blockquote><NewLine></aside><NewLine><p>From the error message it sounds like the output of torch.cat(x, 0) is a Variable. You shouldn’t be wrapping that within another Variable.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>is there a way to convert my variable to a tensor?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">tensor = my_var.data<NewLine></code></pre><NewLine><p>be aware that this references the same data in memory, so inplace modifications are a really bad idea.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>With <code>my_var.data</code>, also keep in mind that you’ll be losing history about the computation graph and so you won’t be able to backprop through my_var from this path anymore.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>okay just solved it by:<br/><NewLine>removing the conversion Variable</p><NewLine><pre><code class=""lang-auto"">        return map(lambda x: torch.cat(x, 0), samples)<NewLine></code></pre><NewLine><p>so i push</p><NewLine><pre><code class=""lang-auto"">self.memory.push((Variable(self.last_state), Variable(new_state),Variable(torch.LongTensor([int(self.last_action)])), Variable(torch.Tensor([self.last_reward])), self.hx, self.cx))<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/richard; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/qLience; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jpeg729; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/richard; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/qLience; <NewLine> ,"REPLY_DATE 1: March 7, 2018,  4:42pm; <NewLine> REPLY_DATE 2: March 7, 2018,  4:18pm; <NewLine> REPLY_DATE 3: March 7, 2018,  4:41pm; <NewLine> REPLY_DATE 4: March 7, 2018,  4:44pm; <NewLine> REPLY_DATE 5: March 7, 2018,  4:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> 
14533,ValueError after running script for some time witjh NN with LSTM,2018-03-07T11:48:39.438Z,2,1185,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello there,</p><NewLine><p>I get the following from my  “def forward” Function.</p><NewLine><pre><code class=""lang-auto"">    state, (hx, cx) = inputs<NewLine>ValueError: too many values to unpack (expected 2)<NewLine></code></pre><NewLine><p>after running my code for some time.</p><NewLine><p>Code:</p><NewLine><pre><code class=""lang-auto""># AI for pump<NewLine><NewLine># Importing the libraries<NewLine><NewLine>import numpy as np<NewLine>import random # random samples from different batches (experience replay)<NewLine>import os # For loading and saving brain<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim # for using stochastic gradient descent<NewLine>import torch.autograd as autograd # Conversion from tensor (advanced arrays) to avoid all that contains a gradient<NewLine># We want to put the tensor into a varaible taht will also contain a<NewLine># gradient and to this we need:<NewLine>from torch.autograd import Variable<NewLine># to convert this tensor into a variable containing the tensor and the gradient<NewLine><NewLine><NewLine># Initializing and setting the variance of a tensor of weights<NewLine>def normalized_columns_initializer(weights, std=1.0):<NewLine>    out = torch.randn(weights.size())<NewLine>    out *= std / torch.sqrt(out.pow(2).sum(1,keepdim=True).expand_as(out)) # thanks to this initialization, we have var(out) = std^2<NewLine>    return out<NewLine><NewLine># Initializing the weights of the neural network in an optimal way for the learning<NewLine>def weights_init(m):<NewLine>    classname = m.__class__.__name__ # python trick that will look for the type of connection in the object ""m"" (convolution or full connection)<NewLine>    if classname.find('Linear') != -1:<NewLine>        weight_shape = list(m.weight.data.size()) #?? list containing the shape of the weights in the object ""m""<NewLine>        fan_in = weight_shape[1] # dim1<NewLine>        fan_out = weight_shape[0] # dim0<NewLine>        w_bound = np.sqrt(6. / (fan_in + fan_out)) # weight bound<NewLine>        m.weight.data.uniform_(-w_bound, w_bound) # generating some random weights of order inversely proportional to the size of the tensor of weights<NewLine>        m.bias.data.fill_(0) # initializing all the bias with zeros<NewLine><NewLine># Creating the architecture of the Neural Network<NewLine>class Network(nn.Module): #inherinting from nn.Module<NewLine>    def __init__(self, input_size, nb_action): #[self,input neuroner, output neuroner]<NewLine>        super(Network, self).__init__() #inorder to use modules in torch.nn<NewLine>        # Input and output neurons<NewLine>        self.lstm = nn.LSTMCell(input_size, 30) # making an LSTM (Long Short Term Memory) to learn the temporal properties of the input<NewLine>        self.fcL = nn.Linear(30, nb_action) # full connection of the<NewLine>        self.apply(weights_init) # initilizing the weights of the model with random weights<NewLine>        self.fcL.weight.data = normalized_columns_initializer(self.fcL.weight.data, 0.01) # setting the standard deviation of the fcL tensor of weights to 0.01<NewLine>        self.fcL.bias.data.fill_(0) # initializing the actor bias with zeros<NewLine>        self.lstm.bias_ih.data.fill_(0) # initializing the lstm bias with zeros<NewLine>        self.lstm.bias_hh.data.fill_(0) # initializing the lstm bias with zeros<NewLine>        self.train() # setting the module in ""train"" mode to activate the dropouts and batchnorms<NewLine><NewLine>		<NewLine>		<NewLine>    # For function that will activate neurons and perform forward propagation<NewLine>    def forward(self, inputs):<NewLine>        state, (hx, cx) = inputs <NewLine>        hx, cx = self.lstm(state, (hx, cx)) # the LSTM takes as input x and the old hidden &amp; cell states and ouputs the new hidden &amp; cell states<NewLine>        x = hx # getting the useful output, which are the hidden states (principle of the LSTM)<NewLine>        q_values = self.fcL(x)<NewLine>        return q_values, (hx, cx) <NewLine><NewLine><NewLine># Implementing Experience Replay<NewLine># We know that RL is based on MDP<NewLine># So going from one state(s_t) to the next state(s_t+1)<NewLine># We gonna put 100 transition between state into what we call the memory<NewLine># So we can use the distribution of experience to make a decision<NewLine>class ReplayMemory(object):<NewLine>    <NewLine>    def __init__(self, capacity):<NewLine>        self.capacity = capacity #100 transitions<NewLine>        self.memory = [] #memory to save transitions<NewLine>    <NewLine>    # pushing transitions into memory with append<NewLine>    #event=transition<NewLine>    def push(self, event):<NewLine>        self.memory.append(event)<NewLine>        if len(self.memory) &gt; self.capacity: #memory only contain 100 events<NewLine>            del self.memory[0] #delete first transition from memory if there is more that 100<NewLine>    <NewLine>    # taking random sample<NewLine>    def sample(self, batch_size):<NewLine>        #Creating variable that will contain the samples of memory<NewLine>        #zip =reshape function if list = ((1,2,3),(4,5,6)) zip(*list)= (1,4),(2,5),(3,6)<NewLine>        #                (state,action,reward),(state,action,reward)  <NewLine>        samples = zip(*random.sample(self.memory, batch_size))<NewLine>        #This is to be able to differentiate with respect to a tensor<NewLine>        #and this will then contain the tensor and gradient<NewLine>        #so for state,action and reward we will store the seperately into some<NewLine>        #bytes which each one will get a gradient<NewLine>        #so that eventually we'll be able to differentiate each one of them<NewLine>        return map(lambda x: Variable(torch.cat(x, 0)), samples)<NewLine><NewLine># Implementing Deep Q Learning<NewLine><NewLine>class Dqn():<NewLine>    <NewLine>    def __init__(self, params):<NewLine>        self.gamma = params.gamma #self.gamma gets assigned to input argument<NewLine>        self.tau = params.tau<NewLine>        # Sliding window of the evolving mean of the last 100 events/transitions<NewLine>        self.reward_window = []<NewLine>        #Creating network with network class<NewLine>        self.model = Network(params.input_size, params.action_size)<NewLine>        #creating memory with memory class<NewLine>        #We gonna take 100000 samples into memory and then we will sample from this memory to <NewLine>        #to get a snakk number of random transitions<NewLine>        self.memory = ReplayMemory(100000)<NewLine>        #creating optimizer (stochastic gradient descent)<NewLine>        self.optimizer = optim.Adam(self.model.parameters(), lr = params.lr) #learning rate<NewLine>        #input vector which is batch of input observations<NewLine>        #by unsqeeze we create a fake dimension to this is<NewLine>        #what the network expect for its inputs<NewLine>        #have to be the first dimension of the last_state<NewLine>        self.last_state = torch.Tensor(params.input_size).unsqueeze(0)<NewLine>        #Inilizing<NewLine>        self.last_action = 0<NewLine>        self.last_reward = 0<NewLine>    <NewLine>    def select_action(self, state):<NewLine>        #LSTM<NewLine>        initialise = True # Initialise to zero at first iteration<NewLine>        if initialise:<NewLine>            cx = Variable(torch.zeros(1, 30))<NewLine>            hx = Variable(torch.zeros(1, 30))<NewLine>        else:  # The hx,cx from the previous iteration<NewLine>            cx = Variable(cx.data) <NewLine>            hx = Variable(hx.data) <NewLine>        initialise = False<NewLine>        print('c')<NewLine>        print(cx)<NewLine>        print('h')<NewLine>        print(hx)<NewLine>        q_values, (hx,cx) = self.model((Variable(state), (hx,cx)))<NewLine>        probs = F.softmax((q_values)*self.tau,dim=1)<NewLine>        #create a random draw from the probability distribution created from softmax<NewLine>        action = probs.multinomial()<NewLine>        return action.data[0,0]<NewLine>    <NewLine>    # See section 5.3 in AI handbook<NewLine>    def learn(self, batch_state, batch_next_state, batch_reward, batch_action):<NewLine>        outputs = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)<NewLine>        #next input for target see page 7 in attached AI handbook<NewLine>        next_outputs = self.model(batch_next_state).detach().max(1)[0]<NewLine>        target = self.gamma*next_outputs + batch_reward<NewLine>        #Using hubble loss inorder to obtain loss<NewLine>        td_loss = F.smooth_l1_loss(outputs, target)<NewLine>        #using  lass loss/error to perform stochastic gradient descent and update weights <NewLine>        self.optimizer.zero_grad() #reintialize the optimizer at each iteration of the loop<NewLine>        #This line of code that backward propagates the error into the NN<NewLine>        #td_loss.backward(retain_variables = True) #userwarning<NewLine>        td_loss.backward(retain_graph = True)<NewLine>		#And this line of code uses the optimizer to update the weights<NewLine>        self.optimizer.step()<NewLine>    <NewLine>    def update(self, reward, new_signal):<NewLine>        #Updated one transition and we have dated the last element of the transition<NewLine>        #which is the new state<NewLine>        new_state = torch.Tensor(new_signal).float().unsqueeze(0)<NewLine>        self.memory.push((self.last_state, new_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward])))<NewLine>        #After ending in a state its time to play a action<NewLine>        action = self.select_action(new_state)<NewLine>        if len(self.memory.memory) &gt; 100:<NewLine>            batch_state, batch_next_state, batch_action, batch_reward = self.memory.sample(100)<NewLine>            self.learn(batch_state, batch_next_state, batch_reward, batch_action)<NewLine>        self.last_action = action<NewLine>        self.last_state = new_state<NewLine>        self.last_reward = reward<NewLine>        self.reward_window.append(reward)<NewLine>        if len(self.reward_window) &gt; 1000:<NewLine>            del self.reward_window[0]<NewLine>        return action<NewLine></code></pre><NewLine><p>any suggestions?</p><NewLine></div>",https://discuss.pytorch.org/u/qLience,(Søren Koch),qLience,"March 7, 2018, 11:49am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>batch_state</code> was a tuple, but got transformed into a variable in <code>memory.sample()</code></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Arg i see! how can avoid this by still using the experience replay?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The problem is not from memory.sample() (giving a tuple to a Variable yould raise an error). You forgot to also memorize the hidden and cell states (as separate tensors to memorize, don’t try to pass them as a tuple).</p><NewLine><p>Then, in <code>learn</code>, you do:</p><NewLine><pre><code class=""lang-auto"">self.model(batch_state)<NewLine></code></pre><NewLine><p>but you should have memorized batch_hidden and batch_cell and do:</p><NewLine><pre><code class=""lang-auto"">self.model(batch_state, (batch_hidden, batch_cell) )<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>of cause…</p><NewLine><p>You are a champ alexis! have a good day!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jpeg729; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/qLience; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/qLience; <NewLine> ,"REPLY_DATE 1: March 7, 2018,  1:48pm; <NewLine> REPLY_DATE 2: March 7, 2018,  1:28pm; <NewLine> REPLY_DATE 3: March 7, 2018,  1:48pm; <NewLine> REPLY_DATE 4: March 7, 2018,  1:49pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 2 Likes; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
14522,TypeError: an integer is required (got type tuple) from NN (LSTM implementation),2018-03-07T09:04:12.440Z,3,5920,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello there… I do not understand why i get this error in my “def select_action” function line:<br/><NewLine><code>q_values, (hx,cx) = self.model(Variable(state, (hx,cx)))</code></p><NewLine><p>Error:</p><NewLine><p>TypeError: an integer is required (got type tuple)</p><NewLine><pre><code class=""lang-auto""># AI for pump<NewLine><NewLine># Importing the libraries<NewLine><NewLine>import numpy as np<NewLine>import random # random samples from different batches (experience replay)<NewLine>import os # For loading and saving brain<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim # for using stochastic gradient descent<NewLine>import torch.autograd as autograd # Conversion from tensor (advanced arrays) to avoid all that contains a gradient<NewLine># We want to put the tensor into a varaible taht will also contain a<NewLine># gradient and to this we need:<NewLine>from torch.autograd import Variable<NewLine># to convert this tensor into a variable containing the tensor and the gradient<NewLine><NewLine><NewLine># Initializing and setting the variance of a tensor of weights<NewLine>def normalized_columns_initializer(weights, std=1.0):<NewLine>    out = torch.randn(weights.size())<NewLine>    out *= std / torch.sqrt(out.pow(2).sum(1,keepdim=True).expand_as(out)) # thanks to this initialization, we have var(out) = std^2<NewLine>    return out<NewLine><NewLine># Initializing the weights of the neural network in an optimal way for the learning<NewLine>def weights_init(m):<NewLine>    classname = m.__class__.__name__ # python trick that will look for the type of connection in the object ""m"" (convolution or full connection)<NewLine>    if classname.find('Linear') != -1:<NewLine>        weight_shape = list(m.weight.data.size()) #?? list containing the shape of the weights in the object ""m""<NewLine>        fan_in = weight_shape[1] # dim1<NewLine>        fan_out = weight_shape[0] # dim0<NewLine>        w_bound = np.sqrt(6. / (fan_in + fan_out)) # weight bound<NewLine>        m.weight.data.uniform_(-w_bound, w_bound) # generating some random weights of order inversely proportional to the size of the tensor of weights<NewLine>        m.bias.data.fill_(0) # initializing all the bias with zeros<NewLine><NewLine># Creating the architecture of the Neural Network<NewLine>class Network(nn.Module): #inherinting from nn.Module<NewLine>    <NewLine>    #Self - refers to the object that will be created from this class<NewLine>    #     - self here to specify that we're referring to the object<NewLine>    def __init__(self, input_size, nb_action): #[self,input neuroner, output neuroner]<NewLine>        super(Network, self).__init__() #inorder to use modules in torch.nn<NewLine>        # Input and output neurons<NewLine>        self.lstm = nn.LSTMCell(input_size, 30) # making an LSTM (Long Short Term Memory) to learn the temporal properties of the input<NewLine>        self.fcL = nn.Linear(30, nb_action) # full connection of the<NewLine>        self.apply(weights_init) # initilizing the weights of the model with random weights<NewLine>        self.fcL.weight.data = normalized_columns_initializer(self.fcL.weight.data, 0.01) # setting the standard deviation of the fcL tensor of weights to 0.01<NewLine>        self.fcL.bias.data.fill_(0) # initializing the actor bias with zeros<NewLine>        self.lstm.bias_ih.data.fill_(0) # initializing the lstm bias with zeros<NewLine>        self.lstm.bias_hh.data.fill_(0) # initializing the lstm bias with zeros<NewLine>        self.train() # setting the module in ""train"" mode to activate the dropouts and batchnorms<NewLine><NewLine>		<NewLine>		<NewLine>    # For function that will activate neurons and perform forward propagation<NewLine>    def forward(self, state):<NewLine>        inputs, (hx, cx) = state # getting separately the input images to the tuple (hidden states, cell states)<NewLine>        x = F.relu(self.lstm(inputs)) # forward propagating the signal from the input images to the 1st convolutional layer<NewLine>        hx, cx = self.lstm(x, (hx, cx)) # the LSTM takes as input x and the old hidden &amp; cell states and ouputs the new hidden &amp; cell states<NewLine>        x = hx # getting the useful output, which are the hidden states (principle of the LSTM)<NewLine>        return self.fcL(x), (hx, cx) # returning the output of the actor (Q(S,A)), and the new hidden &amp; cell states ((hx, cx))<NewLine><NewLine><NewLine># Implementing Experience Replay<NewLine># We know that RL is based on MDP<NewLine># So going from one state(s_t) to the next state(s_t+1)<NewLine># We gonna put 100 transition between state into what we call the memory<NewLine># So we can use the distribution of experience to make a decision<NewLine>class ReplayMemory(object):<NewLine>    <NewLine>    def __init__(self, capacity):<NewLine>        self.capacity = capacity #100 transitions<NewLine>        self.memory = [] #memory to save transitions<NewLine>    <NewLine>    # pushing transitions into memory with append<NewLine>    #event=transition<NewLine>    def push(self, event):<NewLine>        self.memory.append(event)<NewLine>        if len(self.memory) &gt; self.capacity: #memory only contain 100 events<NewLine>            del self.memory[0] #delete first transition from memory if there is more that 100<NewLine>    <NewLine>    # taking random sample<NewLine>    def sample(self, batch_size):<NewLine>        #Creating variable that will contain the samples of memory<NewLine>        #zip =reshape function if list = ((1,2,3),(4,5,6)) zip(*list)= (1,4),(2,5),(3,6)<NewLine>        #                (state,action,reward),(state,action,reward)  <NewLine>        samples = zip(*random.sample(self.memory, batch_size))<NewLine>        #This is to be able to differentiate with respect to a tensor<NewLine>        #and this will then contain the tensor and gradient<NewLine>        #so for state,action and reward we will store the seperately into some<NewLine>        #bytes which each one will get a gradient<NewLine>        #so that eventually we'll be able to differentiate each one of them<NewLine>        return map(lambda x: Variable(torch.cat(x, 0)), samples)<NewLine><NewLine># Implementing Deep Q Learning<NewLine><NewLine>class Dqn():<NewLine>    <NewLine>    def __init__(self, params):<NewLine>        self.gamma = params.gamma #self.gamma gets assigned to input argument<NewLine>        self.tau = params.tau<NewLine>        # Sliding window of the evolving mean of the last 100 events/transitions<NewLine>        self.reward_window = []<NewLine>        #Creating network with network class<NewLine>        self.model = Network(params.input_size, params.action_size)<NewLine>        #creating memory with memory class<NewLine>        #We gonna take 100000 samples into memory and then we will sample from this memory to <NewLine>        #to get a snakk number of random transitions<NewLine>        self.memory = ReplayMemory(100000)<NewLine>        #creating optimizer (stochastic gradient descent)<NewLine>        self.optimizer = optim.Adam(self.model.parameters(), lr = params.lr) #learning rate<NewLine>        #input vector which is batch of input observations<NewLine>        #by unsqeeze we create a fake dimension to this is<NewLine>        #what the network expect for its inputs<NewLine>        #have to be the first dimension of the last_state<NewLine>        self.last_state = torch.Tensor(params.input_size).unsqueeze(0)<NewLine>        #Inilizing<NewLine>        self.last_action = 0<NewLine>        self.last_reward = 0<NewLine>    <NewLine>    def select_action(self, state):<NewLine>        #LSTM<NewLine>        initialise = True # Initialise to zero at first iteration<NewLine>        if initialise:<NewLine>            cx = Variable(torch.zeros(1, 30))<NewLine>            hx = Variable(torch.zeros(1, 30))<NewLine>        else:  # The hx,cx from the previous iteration<NewLine>            cx = Variable(cx.data) <NewLine>            hx = Variable(hx.data) <NewLine>        initialise = False<NewLine>        q_values, (hx,cx) = self.model(Variable(state, (hx,cx)))<NewLine>        probs = F.softmax((q_values)*self.tau,dim=1)<NewLine>        #create a random draw from the probability distribution created from softmax<NewLine>        action = probs.multinomial()<NewLine>        return action.data[0,0]<NewLine>    <NewLine>    # See section 5.3 in AI handbook<NewLine>    def learn(self, batch_state, batch_next_state, batch_reward, batch_action):<NewLine>        outputs = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)<NewLine>        #next input for target see page 7 in attached AI handbook<NewLine>        next_outputs = self.model(batch_next_state).detach().max(1)[0]<NewLine>        target = self.gamma*next_outputs + batch_reward<NewLine>        #Using hubble loss inorder to obtain loss<NewLine>        td_loss = F.smooth_l1_loss(outputs, target)<NewLine>        #using  lass loss/error to perform stochastic gradient descent and update weights <NewLine>        self.optimizer.zero_grad() #reintialize the optimizer at each iteration of the loop<NewLine>        #This line of code that backward propagates the error into the NN<NewLine>        #td_loss.backward(retain_variables = True) #userwarning<NewLine>        td_loss.backward(retain_graph = True)<NewLine>		#And this line of code uses the optimizer to update the weights<NewLine>        self.optimizer.step()<NewLine>    <NewLine>    def update(self, reward, new_signal):<NewLine>        #Updated one transition and we have dated the last element of the transition<NewLine>        #which is the new state<NewLine>        new_state = torch.Tensor(new_signal).float().unsqueeze(0)<NewLine>        self.memory.push((self.last_state, new_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward])))<NewLine>        #After ending in a state its time to play a action<NewLine>        action = self.select_action(new_state)<NewLine>        if len(self.memory.memory) &gt; 100:<NewLine>            batch_state, batch_next_state, batch_action, batch_reward = self.memory.sample(100)<NewLine>            self.learn(batch_state, batch_next_state, batch_reward, batch_action)<NewLine>        self.last_action = action<NewLine>        self.last_state = new_state<NewLine>        self.last_reward = reward<NewLine>        self.reward_window.append(reward)<NewLine>        if len(self.reward_window) &gt; 1000:<NewLine>            del self.reward_window[0]<NewLine>        return action<NewLine>    <NewLine>    def score(self):<NewLine>        return sum(self.reward_window)/(len(self.reward_window)+1.)<NewLine>    <NewLine>    def save(self):<NewLine>        torch.save({'state_dict': self.model.state_dict(),<NewLine>                    'optimizer' : self.optimizer.state_dict(),<NewLine>                   }, 'last_brain.pth')<NewLine>    <NewLine>    def load(self):<NewLine>        if os.path.isfile('last_brain.pth'):<NewLine>            print(""=&gt; loading checkpoint... "")<NewLine>            checkpoint = torch.load('last_brain.pth')<NewLine>            self.model.load_state_dict(checkpoint['state_dict'])<NewLine>            self.optimizer.load_state_dict(checkpoint['optimizer'])<NewLine>            print(""done !"")<NewLine>        else:<NewLine>            print(""no checkpoint found..."")<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/qLience,(Søren Koch),qLience,"March 7, 2018,  9:09am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Mistake of parenthesis: you are feeding a Variable with a tuple of tensors. You should try:</p><NewLine><pre><code class=""lang-auto"">q_values, (hx,cx) = self.model ( (Variable(state), (hx, cx) ) )<NewLine></code></pre><NewLine><p>(I guess hx and cx already are Variables)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Arg yeez… thanks!</p><NewLine><p>but now i get the TypeError:<br/><NewLine>forward() missing 1 required positional argument: ‘hx’</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>When you did <code>x = F.relu(self.lstm(inputs))</code> in your forward pass, you forgot to provide the lstm with hidden and cell units.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I get this</p><NewLine><pre><code class=""lang-auto"">in forward<NewLine>    state, (hx, cx) = inputs<NewLine>ValueError: too many values to unpack (expected 2)<NewLine></code></pre><NewLine><p>with this</p><NewLine><pre><code class=""lang-auto"">    # For function that will activate neurons and perform forward propagation<NewLine>    def forward(self, inputs):<NewLine>        state, (hx, cx) = inputs <NewLine>        hx, cx = self.lstm(state, (hx, cx)) # the LSTM takes as input x and the old hidden &amp; cell states and ouputs the new hidden &amp; cell states<NewLine>        x = hx # getting the useful output, which are the hidden states (principle of the LSTM)<NewLine>        q_values = self.fcL(x)<NewLine>        return q_values, (hx, cx) <NewLine></code></pre><NewLine><p>when i run my code in some time</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/qLience; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/qLience; <NewLine> ,"REPLY_DATE 1: March 7, 2018, 11:49am; <NewLine> REPLY_DATE 2: March 7, 2018,  9:43am; <NewLine> REPLY_DATE 3: March 7, 2018, 11:49am; <NewLine> REPLY_DATE 4: March 7, 2018, 10:43am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
14405,The huge gap of training time between MacOS and Ubuntu 16.04LTS in multiprocessing,2018-03-05T17:41:32.126Z,0,519,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve implemented a popular reinforcement learning algorithm A3C, here is the source code: <a href=""https://github.com/JIElite/A3C"" rel=""nofollow noopener"">https://github.com/JIElite/A3C</a></p><NewLine><p>Sorry for lack of insufficient documentation, but I think it’s enough to reproduce this problem. The problem is when I use my <strong>Macbook Pro 15’(2017)</strong> to run main.py, it only takes <strong>20 seconds</strong> for full training. However, when I run the same code on my <strong>Ubuntu 16.04 LTS computer (i7-7700K)</strong> without any modification, it needs to take more than <strong>10 minutes</strong> to finish training. Even if I modify it to <strong>GPU</strong> version, it takes <strong>1 minute</strong> to training.</p><NewLine><p>What causes such huge gap in training time?</p><NewLine><p>Python version:</p><NewLine><ul><NewLine><li>Python version: 3.5.2</li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/Jie_Han_Chen,(Jie Han Chen),Jie_Han_Chen,"March 6, 2018,  5:44pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I use Unix command <code>time</code> to analyze the execution time.</p><NewLine><ol><NewLine><li><NewLine><p>Macbook Pro:<br/><NewLine>real    0m19.165s<br/><NewLine>user    2m14.593s<br/><NewLine>sys    0m1.469s</p><NewLine></li><NewLine><li><NewLine><p>Ubuntu:<br/><NewLine>real    2m29.506s<br/><NewLine>user    7m28.857s<br/><NewLine>sys    11m54.686s</p><NewLine></li><NewLine></ol><NewLine><p>It seems to take much time in sys. on Ubuntu. I finally add <code>os.environ[""OMP_NUM_THREADS""] = ""1""</code> to solve this issue, which was found at <a href=""https://github.com/ikostrikov/pytorch-a3c/issues/33"" rel=""nofollow noopener"">this issue: What is the purpose of `os.environ[‘OMP_NUM_THREADS’] = ‘1’ </a></p><NewLine><p>After I set OMP_NUM_THREADS to 1, the execution time of Ubuntu 16.04 LTS is:</p><NewLine><div class=""md-table""><NewLine><table><NewLine><thead><NewLine><tr><NewLine><th>real</th><NewLine><th>0m11.914s</th><NewLine></tr><NewLine></thead><NewLine><tbody><NewLine><tr><NewLine><td>user</td><NewLine><td>1m21.437s</td><NewLine></tr><NewLine><tr><NewLine><td>sys</td><NewLine><td>0m0.654s</td><NewLine></tr><NewLine></tbody><NewLine></table><NewLine></div><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Jie_Han_Chen; <NewLine> ,"REPLY_DATE 1: March 6, 2018,  9:10am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
14304,Policy Reinforcement learning with Pytorch,2018-03-03T17:37:12.416Z,0,872,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m currently working on a code using pygame and pytorch to implement a policy-based agent.<br/><NewLine>The goal is very simple for the moment : a car is in a straight road and has to stay in it. It can do nothing (action 0) / go left (action 1) or go right (action 2)</p><NewLine><p>I followed the tutorial <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""nofollow noopener"">https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py</a> to use it.</p><NewLine><p>But it words on the cart pole game but not on my case and I don’t understand why. It seems that my network is not learning.</p><NewLine><p>Here is my main code :</p><NewLine><pre><code>import numpy as np<NewLine>from model import Agent<NewLine>from car_game import CarGame<NewLine>import pygame<NewLine>import torch<NewLine>import torch.optim as optim<NewLine>from torch.autograd import Variable<NewLine>from torch.distributions import Categorical<NewLine>import matplotlib.pyplot as plt<NewLine><NewLine>GREEN = (20, 255, 140)<NewLine>GREY = (210, 210, 210)<NewLine>WHITE = (255, 255, 255)<NewLine><NewLine>SCREENWIDTH = 800<NewLine>SCREENHEIGHT = 600<NewLine><NewLine>size = (SCREENWIDTH, SCREENHEIGHT)<NewLine>screen = pygame.display.set_mode(size)<NewLine>pygame.display.set_caption(""Car Racing"")<NewLine><NewLine><NewLine>def select_action(state):<NewLine>    state = torch.from_numpy(state).float().unsqueeze(0)<NewLine>    probs = policy(Variable(state))<NewLine>    m = Categorical(probs)<NewLine>    action = m.sample()<NewLine>    policy.saved_log_probs.append(m.log_prob(action))<NewLine>    return action.data[0]<NewLine><NewLine><NewLine>def finish_episode(show=False):<NewLine>    R = 0<NewLine>    policy_loss = []<NewLine>    rewards = []<NewLine>    for r in policy.rewards[::-1]:<NewLine>        R = r + gamma * R<NewLine>        rewards.insert(0, R)<NewLine><NewLine>    rewards = torch.Tensor(rewards)<NewLine>    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)<NewLine>    for log_prob, reward in zip(policy.saved_log_probs, rewards):<NewLine>        policy_loss.append(-log_prob * reward)<NewLine>    optimizer.zero_grad()<NewLine>    policy_loss = torch.cat(policy_loss).sum()<NewLine>    policy_loss.backward()<NewLine>    optimizer.step()<NewLine>    if show:<NewLine>        print(""Reward : "", R, ' Policy Loss', policy_loss.data[0])<NewLine>    del policy.rewards[:]<NewLine>    del policy.saved_log_probs[:]<NewLine><NewLine><NewLine>def main():<NewLine>        global nb_episodes_before_dying<NewLine>        nb_episodes_before_dying = []<NewLine>        for i_episode in range(0, 100):<NewLine>            car_game = CarGame(speed=1, min_speed=0.5, screenheight=SCREENHEIGHT)<NewLine>            state = [car_game.playerCar.rect.x]<NewLine>            pygame.init()<NewLine>            # print(i_episode)<NewLine>            carryOn = True<NewLine>            nb_episodes = 0<NewLine>            while carryOn:<NewLine>                nb_episodes += 1<NewLine>                for event in pygame.event.get():<NewLine>                    if event.type == pygame.QUIT:<NewLine>                        carryOn = False<NewLine>                action = select_action(np.array(state))<NewLine>                # print(action)<NewLine><NewLine>                state, reward, done = car_game.play_one_step(action)<NewLine><NewLine>                policy.rewards.append(reward)<NewLine>                if done or nb_episodes &gt; 10000:<NewLine>                    nb_episodes_before_dying.append(nb_episodes)<NewLine>                    carryOn = False<NewLine>                car_game.all_sprites_list.update()<NewLine><NewLine>                # Drawing on Screen<NewLine>                screen.fill(GREEN)<NewLine>                # Draw The Road<NewLine>                pygame.draw.rect(screen, GREY, [300, 0, 200, SCREENHEIGHT])<NewLine>                # Draw Line painting on the road<NewLine>                pygame.draw.line(screen, WHITE, [400, 0], [400, SCREENHEIGHT], 5)<NewLine>                # Draw Line painting on the road<NewLine>                """"""pygame.draw.line(screen, WHITE, [240, 0], [240, SCREENHEIGHT], 5)<NewLine>                # Draw Line painting on the road<NewLine>                pygame.draw.line(screen, WHITE, [340, 0], [340, SCREENHEIGHT], 5)""""""<NewLine><NewLine>                # Now let's draw all the sprites in one go. (For now we only have 1 sprite!)<NewLine>                car_game.all_sprites_list.draw(screen)<NewLine><NewLine>                # Refresh Screen<NewLine>                pygame.display.flip()<NewLine><NewLine>                # Number of frames per secong e.g. 60<NewLine>                car_game.clock.tick(100)<NewLine><NewLine>            finish_episode(True)<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    policy = Agent()<NewLine>    optimizer = optim.Adam(policy.parameters(), lr=1e-2)<NewLine>    gamma = 0.99<NewLine>    main()<NewLine>    plt.plot(nb_episodes_before_dying)<NewLine>    plt.savefig('myfig.png')<NewLine></code></pre><NewLine><p>Here is my model code (very basic) :</p><NewLine><pre><code>import numpy as np<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine><NewLine><NewLine>class Agent(nn.Module):<NewLine>def __init__(self):<NewLine>    super(Agent, self).__init__()<NewLine>    self.layer1 = nn.Linear(1, 32)<NewLine>    self.output = nn.Linear(32, 3)<NewLine><NewLine>    self.rewards = []<NewLine>    self.saved_log_probs = []<NewLine><NewLine>def forward(self, x):<NewLine>    x = F.relu(self.layer1(x))<NewLine>    action_scores = self.output(x)<NewLine>    # print(action_scores)<NewLine>    return F.softmax(action_scores, dim=-1)<NewLine></code></pre><NewLine><p>And this is my last code file which implement my car game and process the game steps :</p><NewLine><pre><code>class CarGame():<NewLine><NewLine>def __init__(self, speed, min_speed, screenheight):<NewLine>    self.all_sprites_list = pygame.sprite.Group()<NewLine>    self.SPEED = speed<NewLine>    self.MIN_SPEED = min_speed<NewLine>    self.screenheight = screenheight<NewLine><NewLine>    self.playerCar = Car(RED, 60, 80, 70)<NewLine>    self.playerCar.rect.x = 430<NewLine>    self.playerCar.rect.y = screenheight - 100<NewLine><NewLine>    self.car1 = Car(PURPLE, 60, 80, random.randint(50, 100))<NewLine>    self.car1.rect.x = 310<NewLine>    self.car1.rect.y = -100<NewLine><NewLine>    self.car2 = Car(YELLOW, 60, 80, random.randint(50, 100))<NewLine>    self.car2.rect.x = 430<NewLine>    self.car2.rect.y = -600<NewLine><NewLine>    # Add the car to the list of objects<NewLine>    self.all_sprites_list.add(self.playerCar)<NewLine>    self.all_sprites_list.add(self.car1)<NewLine>    self.all_sprites_list.add(self.car2)<NewLine><NewLine>    self.all_coming_cars = pygame.sprite.Group()<NewLine>    self.all_coming_cars.add(self.car1)<NewLine>    self.all_coming_cars.add(self.car2)<NewLine><NewLine>    self.clock = pygame.time.Clock()<NewLine><NewLine>def play_one_step(self, action):<NewLine>    """""" Action is 0 (nothing), 1 (left) or 2 (right)<NewLine>    Update the game and return :<NewLine>        state, reward, done""""""<NewLine>    done = False<NewLine>    if action == 0:<NewLine>        pass<NewLine>    if action == 1:<NewLine>        self.playerCar.moveLeft(5)<NewLine>    if action == 2:<NewLine>        self.playerCar.moveRight(5)<NewLine><NewLine>    # Game Logic<NewLine>    for car in self.all_coming_cars:<NewLine>        car.moveForward(self.SPEED)<NewLine>        if car.rect.y &gt; self.screenheight:<NewLine>            car.changeSpeed(random.randint(50, 100))<NewLine>            car.repaint(random.choice(colorList))<NewLine>            car.rect.y = -200<NewLine><NewLine>    # Detect if out of pistes<NewLine>    if self.playerCar.rect.x &lt; 310:<NewLine>        print(""Car out"")<NewLine>        done = True<NewLine>    if self.playerCar.rect.x &gt; 440:<NewLine>        print(""Car out"")<NewLine>        done = True<NewLine>    return ([self.playerCar.rect.x], 1, done)<NewLine></code></pre><NewLine><p>I don’t have any problems in my logs, but when I print the output of my network I have very quickly : 0, 1, 0 --&gt; So my car does every time the same action and doesn’t train.<br/><NewLine>It’s done randomly, sometimes my car find directly the good action (do nothing because the road is straight …) but when left is at 1, it’s blocked and this forever.<br/><NewLine>Here is my log when my car go right since the begginning :</p><NewLine><pre><code>Car out<NewLine>Reward :  2.9701  Policy Loss 0.0<NewLine>Car out<NewLine>Reward :  2.9701  Policy Loss 0.0<NewLine>Car out<NewLine>Reward :  2.9701  Policy Loss 0.0<NewLine>Car out<NewLine>Reward :  2.9701  Policy Loss 0.0<NewLine>Car out<NewLine>Reward :  2.9701  Policy Loss 0.0<NewLine>Car out<NewLine>Reward :  2.9701  Policy Loss 0.0<NewLine>Car out<NewLine>Reward :  2.9701  Policy Loss 0.0<NewLine>Car out<NewLine>Reward :  2.9701  Policy Loss 0.0<NewLine></code></pre><NewLine><p>My reward and my policy loss don’t change …</p><NewLine><p>Do you have any idea please ?</p><NewLine><p>Thank you very much !</p><NewLine></div>",https://discuss.pytorch.org/u/guillaumegg10,(Guillaume),guillaumegg10,"March 3, 2018,  5:37pm",,,,,
14262,Implementing RNN and LSTM into DQN Pytorch code,2018-03-02T14:38:20.434Z,0,2502,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have some troubles finding some example on the great www to how i implement a recurrent neural network with LSTM layer into my current Deep q-network in Pytorch so it become a DRQN… Bear with me i am just getting started… Futhermore, I am NOT working with images processing, thereby CNN so do not worry about this. My states are purely temperatures values.</p><NewLine><p>Here is my code that i am currently train my DQN with:</p><NewLine><pre><code class=""lang-auto""># AI for Self Driving Car<NewLine><NewLine>#Settings to adjust inorder to get a better algorithm<NewLine># reward policy<NewLine>	# Less punishment to increase distance from goal<NewLine>	# if car is more than 10s to find goal then punish<NewLine># more hidden layers<NewLine># more hidden neurons<NewLine># gamma<NewLine># optimizer<NewLine><NewLine># Importing the libraries<NewLine><NewLine>import numpy as np<NewLine>import random # random samples from different batches (experience replay)<NewLine>import os # For loading and saving brain<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim # for using stochastic gradient descent<NewLine>import torch.autograd as autograd # Conversion from tensor (advanced arrays) to avoid all that contains a gradient<NewLine># We want to put the tensor into a varaible taht will also contain a<NewLine># gradient and to this we need:<NewLine>from torch.autograd import Variable<NewLine># to convert this tensor into a variable containing the tensor and the gradient<NewLine><NewLine><NewLine># Creating the architecture of the Neural Network<NewLine>class Network(nn.Module): #inherinting from nn.Module<NewLine>    <NewLine>    #Self - refers to the object that will be created from this class<NewLine>    #     - self here to specify that we're referring to the object<NewLine>    def __init__(self, input_size, nb_action): #[self,input neuroner, output neuroner]<NewLine>        super(Network, self).__init__() #inorder to use modules in torch.nn<NewLine>        # Input and output neurons<NewLine>        self.input_size = input_size<NewLine>        self.nb_action = nb_action<NewLine>        # Full connection between different layers of NN<NewLine>        # In this example its one input layer, one hidden layer and one output layer<NewLine>        # Using self here to specify that fc1 is a variable of my object<NewLine>        self.fc1 = nn.Linear(input_size, 40)<NewLine>        self.fc2 = nn.Linear(40, 30)<NewLine>		#Example of adding a hiddenlayer<NewLine>        # self.fcX = nn.Linear(30,30)<NewLine>        self.fc3 = nn.Linear(30, nb_action) # 30 neurons in hidden layer<NewLine>    <NewLine>    # For function that will activate neurons and perform forward propagation<NewLine>    def forward(self, state):<NewLine>        # rectifier function<NewLine>        x = F.relu(self.fc1(state))<NewLine>        x = F.relu(self.fc2(x))<NewLine>        q_values = self.fc3(x)<NewLine>        return q_values<NewLine><NewLine># Implementing Experience Replay<NewLine># We know that RL is based on MDP<NewLine># So going from one state(s_t) to the next state(s_t+1)<NewLine># We gonna put 100 transition between state into what we call the memory<NewLine># So we can use the distribution of experience to make a decision<NewLine>class ReplayMemory(object):<NewLine>    <NewLine>    def __init__(self, capacity):<NewLine>        self.capacity = capacity #100 transitions<NewLine>        self.memory = [] #memory to save transitions<NewLine>    <NewLine>    # pushing transitions into memory with append<NewLine>    #event=transition<NewLine>    def push(self, event):<NewLine>        self.memory.append(event)<NewLine>        if len(self.memory) &gt; self.capacity: #memory only contain 100 events<NewLine>            del self.memory[0] #delete first transition from memory if there is more that 100<NewLine>    <NewLine>    # taking random sample<NewLine>    def sample(self, batch_size):<NewLine>        #Creating variable that will contain the samples of memory<NewLine>        #zip =reshape function if list = ((1,2,3),(4,5,6)) zip(*list)= (1,4),(2,5),(3,6)<NewLine>        #                (state,action,reward),(state,action,reward)  <NewLine>        samples = zip(*random.sample(self.memory, batch_size))<NewLine>        #This is to be able to differentiate with respect to a tensor<NewLine>        #and this will then contain the tensor and gradient<NewLine>        #so for state,action and reward we will store the seperately into some<NewLine>        #bytes which each one will get a gradient<NewLine>        #so that eventually we'll be able to differentiate each one of them<NewLine>        return map(lambda x: Variable(torch.cat(x, 0)), samples)<NewLine><NewLine># Implementing Deep Q Learning<NewLine><NewLine>class Dqn():<NewLine>    <NewLine>    def __init__(self, input_size, nb_action, gamma, lrate, T):<NewLine>        self.gamma = gamma #self.gamma gets assigned to input argument<NewLine>        self.T = T<NewLine>        # Sliding window of the evolving mean of the last 100 events/transitions<NewLine>        self.reward_window = []<NewLine>        #Creating network with network class<NewLine>        self.model = Network(input_size, nb_action)<NewLine>        #creating memory with memory class<NewLine>        #We gonna take 100000 samples into memory and then we will sample from this memory to <NewLine>        #to get a snakk number of random transitions<NewLine>        self.memory = ReplayMemory(100000)<NewLine>        #creating optimizer (stochastic gradient descent)<NewLine>        self.optimizer = optim.Adam(self.model.parameters(), lr = lrate) #learning rate<NewLine>        #input vector which is batch of input observations<NewLine>        #by unsqeeze we create a fake dimension to this is<NewLine>        #what the network expect for its inputs<NewLine>        #have to be the first dimension of the last_state<NewLine>        self.last_state = torch.Tensor(input_size).unsqueeze(0)<NewLine>        #Inilizing<NewLine>        self.last_action = 0<NewLine>        self.last_reward = 0<NewLine>    <NewLine>    def select_action(self, state):<NewLine>        #Q value depends on state<NewLine>        #Temperature parameter T will be a positive number and the closer<NewLine>        #it is to ze the less sure the NN will when taking an action<NewLine>        #forexample<NewLine>        #softmax((1,2,3))={0.04,0.11,0.85} ==&gt; softmax((1,2,3)*3)={0,0.02,0.98} <NewLine>        #to deactivate brain then set T=0, thereby it is full random<NewLine>        probs = F.softmax((self.model(Variable(state, volatile = True))*self.T),dim=1) # T=100<NewLine>        #create a random draw from the probability distribution created from softmax<NewLine>        action = probs.multinomial()<NewLine>        print(probs.multinomial())<NewLine>        return action.data[0,0]<NewLine><NewLine>    # See section 5.3 in AI handbook<NewLine>    def learn(self, batch_state, batch_next_state, batch_reward, batch_action):<NewLine>        outputs = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)<NewLine>        #next input for target see page 7 in attached AI handbook<NewLine>        next_outputs = self.model(batch_next_state).detach().max(1)[0]<NewLine>        target = self.gamma*next_outputs + batch_reward<NewLine>        #Using hubble loss inorder to obtain loss<NewLine>        td_loss = F.smooth_l1_loss(outputs, target)<NewLine>        #using  lass loss/error to perform stochastic gradient descent and update weights <NewLine>        self.optimizer.zero_grad() #reintialize the optimizer at each iteration of the loop<NewLine>        #This line of code that backward propagates the error into the NN<NewLine>        #td_loss.backward(retain_variables = True) #userwarning<NewLine>        td_loss.backward(retain_graph = True)<NewLine>		#And this line of code uses the optimizer to update the weights<NewLine>        self.optimizer.step()<NewLine>    <NewLine>    def update(self, reward, new_signal):<NewLine>        #Updated one transition and we have dated the last element of the transition<NewLine>        #which is the new state<NewLine>        new_state = torch.Tensor(new_signal).float().unsqueeze(0)<NewLine>        self.memory.push((self.last_state, new_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward])))<NewLine>        #After ending in a state its time to play a action<NewLine>        action = self.select_action(new_state)<NewLine>        if len(self.memory.memory) &gt; 100:<NewLine>            batch_state, batch_next_state, batch_action, batch_reward = self.memory.sample(100)<NewLine>            self.learn(batch_state, batch_next_state, batch_reward, batch_action)<NewLine>        self.last_action = action<NewLine>        self.last_state = new_state<NewLine>        self.last_reward = reward<NewLine>        self.reward_window.append(reward)<NewLine>        if len(self.reward_window) &gt; 1000:<NewLine>            del self.reward_window[0]<NewLine>        return action<NewLine>    <NewLine>    def score(self):<NewLine>        return sum(self.reward_window)/(len(self.reward_window)+1.)<NewLine>    <NewLine>    def save(self):<NewLine>        torch.save({'state_dict': self.model.state_dict(),<NewLine>                    'optimizer' : self.optimizer.state_dict(),<NewLine>                   }, 'last_brain.pth')<NewLine>    <NewLine>    def load(self):<NewLine>        if os.path.isfile('last_brain.pth'):<NewLine>            print(""=&gt; loading checkpoint... "")<NewLine>            checkpoint = torch.load('last_brain.pth')<NewLine>            self.model.load_state_dict(checkpoint['state_dict'])<NewLine>            self.optimizer.load_state_dict(checkpoint['optimizer'])<NewLine>            print(""done !"")<NewLine>        else:<NewLine>            print(""no checkpoint found..."")<NewLine></code></pre><NewLine><p>I hope there is someone to help me out there!</p><NewLine></div>",https://discuss.pytorch.org/u/qLience,(Søren Koch),qLience,"March 2, 2018,  2:38pm",2 Likes,,,,
13941,Trying to modify this code to use cuda,2018-02-23T01:47:15.376Z,2,1256,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I found this code on github which looks like a pretty cool and simple a3c implementation using LSTM and multi-threading.  However, I am struggling trying to figure out how to modify it to use cuda.  I think the multi-threading part (and possibly the shared network part) is what is tripping me up, as I have never encountered code I couldn’t modify when necessary.  Anyone know what all needs to change here?  Also, if you could explain the tricky stuff (when changing to cuda) that would be great so we can all learn from it.</p><NewLine><p>Here is the original source:</p><NewLine><pre><code class=""lang-auto"">https://github.com/greydanus/baby-a3c<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">from __future__ import print_function<NewLine>import torch, os, gym, time, glob, argparse<NewLine>import numpy as np<NewLine>from scipy.signal import lfilter<NewLine>from scipy.misc import imresize  # preserves single-pixel info _unlike_ img = img[::2,::2]<NewLine><NewLine>import torch.nn as nn<NewLine>from torch.autograd import Variable<NewLine>import torch.nn.functional as F<NewLine>import torch.multiprocessing as mp<NewLine><NewLine>os.environ['OMP_NUM_THREADS'] = '1'<NewLine><NewLine>parser = argparse.ArgumentParser(description=None)<NewLine>parser.add_argument('--env', default='Pong-v0', type=str, help='gym environment')<NewLine>parser.add_argument('--processes', default=40, type=int, help='number of processes to train with')<NewLine>parser.add_argument('--render', default=True, type=bool, help='renders the atari environment')<NewLine>parser.add_argument('--test', default=False, type=bool, help='test mode sets lr=0, chooses most likely actions')<NewLine>parser.add_argument('--lstm_steps', default=20, type=int, help='steps to train LSTM over')<NewLine>parser.add_argument('--lr', default=1e-4, type=float, help='learning rate')<NewLine>parser.add_argument('--seed', default=1, type=int, help='seed random # generators (for reproducibility)')<NewLine>parser.add_argument('--gamma', default=0.99, type=float, help='discount for gamma-discounted rewards')<NewLine>parser.add_argument('--tau', default=1.0, type=float, help='discount for generalized advantage estimation')<NewLine>parser.add_argument('--horizon', default=0.99, type=float, help='horizon for running averages')<NewLine>args = parser.parse_args()<NewLine><NewLine>args.save_dir = '{}/'.format(args.env.lower())  # keep the directory structure simple<NewLine>if args.render:<NewLine>    args.processes = 1<NewLine><NewLine>args.test = True  # render mode -&gt; test mode w one process<NewLine><NewLine>if args.test:<NewLine>    args.lr = 0  # don't train in render mode<NewLine><NewLine>args.num_actions = gym.make(args.env).action_space.n  # get the action space of this game<NewLine>os.makedirs(args.save_dir) if not os.path.exists(args.save_dir) else None  # make dir to save models etc.<NewLine><NewLine>discount = lambda x, gamma: lfilter([1], [1, -gamma], x[::-1])[::-1]  # discounted rewards one liner<NewLine>prepro = lambda img: imresize(img[35:195].mean(2), (80, 80)).astype(np.float32).reshape(1, 80, 80) / 255.<NewLine><NewLine><NewLine>def printlog(args, s, end='\n', mode='a'):<NewLine>    print(s, end=end)<NewLine>    f = open(args.save_dir + 'log.txt', mode)<NewLine>    f.write(s + '\n')<NewLine>    f.close()<NewLine><NewLine><NewLine>class NNPolicy(torch.nn.Module):  # an actor-critic neural network<NewLine>    def __init__(self, channels, num_actions):<NewLine>        super(NNPolicy, self).__init__()<NewLine>        self.conv1 = nn.Conv2d(channels, 32, 3, stride=2, padding=1)<NewLine>        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)<NewLine>        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)<NewLine>        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)<NewLine>        self.lstm = nn.LSTMCell(32 * 5 * 5, 256)<NewLine>        self.critic_linear, self.actor_linear = nn.Linear(256, 1), nn.Linear(256, num_actions)<NewLine><NewLine>    def forward(self, inputs):<NewLine>        inputs, (hx, cx) = inputs<NewLine>        x = F.elu(self.conv1(inputs));<NewLine>        x = F.elu(self.conv2(x))<NewLine>        x = F.elu(self.conv3(x));<NewLine>        x = F.elu(self.conv4(x))<NewLine>        hx, cx = self.lstm(x.view(-1, 32 * 5 * 5), (hx, cx))<NewLine>        return self.critic_linear(hx), self.actor_linear(hx), (hx, cx)<NewLine><NewLine>    def try_load(self, save_dir):<NewLine>        paths = glob.glob(save_dir + '*.tar');<NewLine>        step = 0<NewLine>        if len(paths) &gt; 0:<NewLine>            ckpts = [int(s.split('.')[-2]) for s in paths]<NewLine>            ix = np.argmax(ckpts)<NewLine>            step = ckpts[ix]<NewLine>            self.load_state_dict(torch.load(paths[ix]))<NewLine>        print(""\tno saved models"") if step is 0 else print(""\tloaded model: {}"".format(paths[ix]))<NewLine>        return step<NewLine><NewLine><NewLine>class SharedAdam(torch.optim.Adam):  # extend a pytorch optimizer so it shares grads across processes<NewLine>    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):<NewLine>        super(SharedAdam, self).__init__(params, lr, betas, eps, weight_decay)<NewLine>        for group in self.param_groups:<NewLine>            for p in group['params']:<NewLine>                state = self.state[p]<NewLine>                state['shared_steps'], state['step'] = torch.zeros(1).share_memory_(), 0<NewLine>                state['exp_avg'] = p.data.new().resize_as_(p.data).zero_().share_memory_()<NewLine>                state['exp_avg_sq'] = p.data.new().resize_as_(p.data).zero_().share_memory_()<NewLine><NewLine>    def step(self, closure=None):<NewLine>            for group in self.param_groups:<NewLine>                for p in group['params']:<NewLine>                    if p.grad is None:<NewLine>                        continue<NewLine>                    self.state[p]['shared_steps'] += 1<NewLine>                    self.state[p]['step'] = self.state[p]['shared_steps'][0] - 1  # there's a ""step += 1"" later<NewLine>            super(SharedAdam, self).step(closure)<NewLine><NewLine><NewLine>torch.manual_seed(args.seed)<NewLine>shared_model = NNPolicy(channels=1, num_actions=args.num_actions).share_memory()<NewLine>shared_optimizer = SharedAdam(shared_model.parameters(), lr=args.lr)<NewLine><NewLine>info = {k: torch.DoubleTensor([0]).share_memory_() for k in ['run_epr', 'run_loss', 'episodes', 'frames']}<NewLine>info['frames'] += shared_model.try_load(args.save_dir) * 1e6<NewLine><NewLine>if int(info['frames'][0]) == 0:<NewLine>    printlog(args, '', end='', mode='w')  # clear log file<NewLine><NewLine><NewLine>def train(rank, args, info):<NewLine>    env = gym.make(args.env)  # make a local (unshared) environment<NewLine>    env.seed(args.seed + rank);<NewLine>    torch.manual_seed(args.seed + rank)  # seed everything<NewLine>    model = NNPolicy(channels=1, num_actions=args.num_actions)  # init a local (unshared) model<NewLine>    state = torch.Tensor(prepro(env.reset()))  # get first state<NewLine><NewLine>    start_time = last_disp_time = time.time()<NewLine>    episode_length, epr, eploss, done = 0, 0, 0, True  # bookkeeping<NewLine><NewLine>    while info['frames'][0] &lt;= 8e7 or args.test:  # openai baselines uses 40M frames...we'll use 80M<NewLine>        model.load_state_dict(shared_model.state_dict())  # sync with shared model<NewLine><NewLine>        cx = Variable(torch.zeros(1, 256)) if done else Variable(cx.data)  # lstm memory vector<NewLine>        hx = Variable(torch.zeros(1, 256)) if done else Variable(hx.data)  # lstm activation vector<NewLine>        values, logps, actions, rewards = [], [], [], []  # save values for computing gradientss<NewLine><NewLine>        for step in range(args.lstm_steps):<NewLine>            episode_length += 1<NewLine>            value, logit, (hx, cx) = model((Variable(state.view(1, 1, 80, 80)), (hx, cx)))<NewLine>            logp = F.log_softmax(logit)<NewLine><NewLine>            action = logp.max(1)[1].data if args.test else torch.exp(logp).multinomial().data[0]<NewLine>            state, reward, done, _ = env.step(action.numpy()[0])<NewLine>            if args.render:<NewLine>                env.render()<NewLine><NewLine>            state = torch.Tensor(prepro(state))<NewLine>            epr += reward<NewLine>            reward = np.clip(reward, -1, 1)  # reward<NewLine>            done = done or episode_length &gt;= 1e4  # keep agent from playing one episode too long<NewLine><NewLine>            info['frames'] += 1;<NewLine>            num_frames = int(info['frames'][0])<NewLine>            if num_frames % 2e6 == 0:  # save every 2M frames<NewLine>                printlog(args, '\n\t{:.0f}M frames: saved model\n'.format(num_frames / 1e6))<NewLine>                torch.save(shared_model.state_dict(), args.save_dir + 'model.{:.0f}.tar'.format(num_frames / 1e6))<NewLine><NewLine>            if done:  # update shared data. maybe print info.<NewLine>                info['episodes'] += 1<NewLine>                interp = 1 if info['episodes'][0] == 1 else 1 - args.horizon<NewLine>                info['run_epr'].mul_(1 - interp).add_(interp * epr)<NewLine>                info['run_loss'].mul_(1 - interp).add_(interp * eploss)<NewLine><NewLine>                if rank == 0 and time.time() - last_disp_time &gt; 60:  # print info ~ every minute<NewLine>                    elapsed = time.strftime(""%Hh %Mm %Ss"", time.gmtime(time.time() - start_time))<NewLine>                    printlog(args, 'time {}, episodes {:.0f}, frames {:.1f}M, run epr {:.2f}, run loss {:.2f}'<NewLine>                             .format(elapsed, info['episodes'][0], num_frames / 1e6, info['run_epr'][0],<NewLine>                                     info['run_loss'][0]))<NewLine>                    last_disp_time = time.time()<NewLine><NewLine>                episode_length, epr, eploss = 0, 0, 0<NewLine>                state = torch.Tensor(prepro(env.reset()))<NewLine><NewLine>            values.append(value)<NewLine>            logps.append(logp)<NewLine>            actions.append(action)<NewLine>            rewards.append(reward)<NewLine><NewLine>        next_value = Variable(torch.zeros(1, 1)) if done else model((Variable(state.unsqueeze(0)), (hx, cx)))[0]<NewLine>        values.append(Variable(next_value.data))<NewLine><NewLine>        loss = cost_func(torch.cat(values), torch.cat(logps), torch.cat(actions), np.asarray(rewards))<NewLine>        eploss += loss.data[0]<NewLine>        shared_optimizer.zero_grad();<NewLine>        loss.backward()<NewLine>        torch.nn.utils.clip_grad_norm(model.parameters(), 40)<NewLine><NewLine>        for param, shared_param in zip(model.parameters(), shared_model.parameters()):<NewLine>            if shared_param.grad is None: shared_param._grad = param.grad  # sync gradients with shared model<NewLine>        shared_optimizer.step()<NewLine><NewLine><NewLine>def cost_func(values, logps, actions, rewards):<NewLine>    np_values = values.view(-1).data.numpy()<NewLine><NewLine>    # generalized advantage estimation (a policy gradient method)<NewLine>    delta_t = np.asarray(rewards) + args.gamma * np_values[1:] - np_values[:-1]<NewLine>    gae = discount(delta_t, args.gamma * args.tau)<NewLine>    logpys = logps.gather(1, Variable(actions).view(-1, 1))<NewLine>    policy_loss = -(logpys.view(-1) * Variable(torch.Tensor(np.flip(gae, axis=0).copy()))).sum()<NewLine><NewLine>    # l2 loss over value estimator<NewLine>    rewards[-1] += args.gamma * np_values[-1]<NewLine>    discounted_r = discount(np.asarray(rewards), args.gamma)<NewLine>    discounted_r = Variable(torch.Tensor(np.flip(discounted_r, axis=0).copy()))<NewLine>    value_loss = .5 * (discounted_r - values[:-1, 0]).pow(2).sum()<NewLine><NewLine>    entropy_loss = -(-logps * torch.exp(logps)).sum()  # encourage lower entropy<NewLine>    return policy_loss + 0.5 * value_loss + 0.01 * entropy_loss<NewLine><NewLine><NewLine>processes = []<NewLine>for rank in range(args.processes):<NewLine>    p = mp.Process(target=train, args=(rank, args, info))<NewLine>    p.start()<NewLine>    processes.append(p)<NewLine>for p in processes:<NewLine>    p.join()<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/rbunn80110,(Robert Bunn),rbunn80110,"February 23, 2018,  1:48am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not sure that multiprocessing is a good idea with CUDA but it depends on what you’re doing here and the problems you’re running into.</p><NewLine><p>Generally, speaking, the strategy I have for converting models to CUDA is to change all tensors/variables to cuda by calling <code>.cuda()</code>, and all models to cuda with <code>model.cuda()</code>. What happens when you try to run the code after that?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s what I attempted to do and basically I go from one error to another in circles.  Fixing one error leads to another and it doesn’t appear I ever get closer to fixing the problem.  This code can be run as is, feel free to try it if you like.  I have converted many models to cuda in the past with no difficulty, so there is something here that I am just missing I guess.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey Checkout my repo here I implemented a3c with gpu use and hog wild training. I call it A3G and it’s works very well it’s actually even faster at training Atari games than mass parallelized evolutionary algorithms with also much higher overall performance.</p><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://assets-cdn.github.com/favicon.ico"" width=""32""/><NewLine><a href=""https://github.com/dgriff777/rl_a3c_pytorch"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""420"" src=""https://avatars1.githubusercontent.com/u/24666148?s=400&amp;v=4"" width=""420""/><NewLine><h3><a href=""https://github.com/dgriff777/rl_a3c_pytorch"" rel=""nofollow noopener"" target=""_blank"">dgriff777/rl_a3c_pytorch</a></h3><NewLine><p>rl_a3c_pytorch - A3C LSTM  Atari with Pytorch plus A3G design</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p><strong>update Just realized you had already visited repo and we spoke on issues on github. <img alt="":open_mouth:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/open_mouth.png?v=5"" title="":open_mouth:""/><img alt="":rofl:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/rofl.png?v=5"" title="":rofl:""/></strong><br/><NewLine><strong>Guess you can disregard then lol</strong></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/richard; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/rbunn80110; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/dgriff; <NewLine> ,"REPLY_DATE 1: February 23, 2018,  3:20am; <NewLine> REPLY_DATE 2: February 23, 2018,  3:29am; <NewLine> REPLY_DATE 3: February 24, 2018, 10:43pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
13762,Ladder Variational Autoencoders - Any help?,2018-02-18T16:19:57.510Z,0,444,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Does anyone of implementation of Ladder Variational Autoencoders.</p><NewLine><p>I did a search for it but didn’t found any.</p><NewLine><p>can anyone help me in this?</p><NewLine><p><a href=""http://papers.nips.cc/paper/6275-ladder-variational-autoencoders.pdf"" rel=""nofollow noopener"">http://papers.nips.cc/paper/6275-ladder-variational-autoencoders.pdf</a></p><NewLine></div>",https://discuss.pytorch.org/u/jmandivarapu1,(Jaya Krishna Mandivarapu),jmandivarapu1,"February 21, 2018,  2:48pm",,,,,
13702,"PyTorch Network Training, But Tensorflow (same) Network is Not. Why?",2018-02-15T21:24:16.272Z,0,972,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I realize this question might be more for a Tensorflow forum but having trouble getting answers and you guys might know.</p><NewLine><p>It’s been a week and I can not figure out why the PyTorch version trains and the Tensorflow version absolutely fails. As far as I can tell the networks are the same. At this point I know some specific things that are going wrong, but I don’t know why.</p><NewLine><h3>Simple Network</h3><NewLine><pre><code>Input: 4<NewLine>L1: (Input)4 -&gt; 128<NewLine>L2A: (L1)128 -&gt; 1 # Used for Mu<NewLine>L2B: (L1)128 -&gt; 1 # Used for Sigma<NewLine># Use Mu and Sigma to define Normal Distribution<NewLine># Use Normal Distribution for Continuous Action Sampling and Training<NewLine></code></pre><NewLine><p>I’m aware that due to the nature of PyTorch and Tensorflow the code looks a bit different, but at the network level, and at the update level I <em>think</em> they should both operate the same way.</p><NewLine><h3>A Picture is worth 1000 Words</h3><NewLine><p><a href=""https://i.imgur.com/ltFTMlm.png"" rel=""nofollow noopener""><img alt=""Comparison"" height=""500"" src=""https://i.imgur.com/ltFTMlm.png"" width=""250""/></a></p><NewLine><h3>CLUE NUMBER 1: Gradients in Tensorflow Blow Up</h3><NewLine><p>Above you can see that the gradients for PyTorch look good, but for Tensorflow they don’t (they blow up). I know there are ways I can use gradient clipping, and other tricks, but I’m hesitant to do that since I did not need to with PyTorch. There must be something different about the network.</p><NewLine><h3>CLUE NUMBER 2: Loss function in Tensorflow Goes Negative</h3><NewLine><p>Above you can see that the loss function for Tensorflow actually goes negative. This seems to be because the majority of the log_probs are positive, which is technically possible. But in PyTorch the majority of the log_probs are negative. Also in PyTorch it looks like loss is increasing but that is okay, I believe because as the episodes get longer, the returns for each step get longer too.</p><NewLine><h3>CLUE NUMBER 3: Sigmas on Tensorflow Start at one and drop to Zero.</h3><NewLine><p>If you look at the Sigmas on the Tensorflow graphs, they drop from an average of 1/episode to an average of 0/episode. This seems really important but I don’t know what to make of it.</p><NewLine><h3>Finally, Some Code. Both PyTorch and Tensorflow</h3><NewLine><p>I’ve Double-Double-Double checked and hyper-params are the same across the two.</p><NewLine><h1>PyTorch (Works. Trains easily within 1000 episodes):</h1><NewLine><pre><code># Code broken up into a Policy Class, and Agent Class and then the ""main.py"" which runs the code.<NewLine><NewLine>class Policy(nn.Module):<NewLine>    def __init__(self, hidden_size, num_inputs, action_space):<NewLine>        super(Policy, self).__init__()<NewLine>        self.action_space = action_space<NewLine>        num_outputs = action_space.shape[0]<NewLine>        self.linear1 = nn.Linear(num_inputs, hidden_size)<NewLine>        self.linear2 = nn.Linear(hidden_size, num_outputs)<NewLine>        self.linear2_ = nn.Linear(hidden_size, num_outputs)<NewLine><NewLine>    def forward(self, inputs):<NewLine>        x = inputs<NewLine>        x = self.linear1(x)<NewLine>        x = F.relu(x)<NewLine>        mu = self.linear2(x)<NewLine>        sigma_sq = self.linear2_(x)<NewLine>        sigma_sq = F.softplus(sigma_sq)<NewLine><NewLine>        return mu, sigma_sq<NewLine><NewLine><NewLine><NewLine>pi = Variable(torch.FloatTensor([math.pi]))<NewLine><NewLine>class Agent:<NewLine>    def __init__(self, hidden_size, num_inputs, action_space):<NewLine>        self.action_space = action_space<NewLine>        self.model = Policy(hidden_size, num_inputs, action_space)<NewLine>        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)<NewLine>        self.model.train()<NewLine><NewLine><NewLine>    # probability density of x given a normal distribution<NewLine>    # defined by mu and sigma<NewLine>    def normal(self, x, mu, sigma_sq):<NewLine>        a = (-1*(Variable(x)-mu).pow(2)/(2*sigma_sq)).exp()<NewLine>        b = 1/(2*sigma_sq*pi.expand_as(sigma_sq)).sqrt()<NewLine>        return a*b<NewLine><NewLine>    def select_action(self, state):<NewLine>        state = Variable(state)<NewLine>        mu, sigma_sq = self.model(state)<NewLine><NewLine>        # random scalar from normal distribution<NewLine>        # with mean 0 and std 1<NewLine>        random_from_normal = torch.randn(1)<NewLine><NewLine>        # modulate our normal (mu,sigma) with random_from_normal to pick an action.<NewLine>        # Note that if x = random_from_normal, then our action is just:<NewLine>        # mu + sigma * x<NewLine>        sigma = sigma_sq.sqrt()<NewLine>        action = (mu + sigma*Variable(random_from_normal)).data<NewLine><NewLine>        # calculate the probability density<NewLine>        prob = self.normal(action, mu, sigma_sq)<NewLine><NewLine>        log_prob = prob.log()<NewLine><NewLine>        return action, log_prob<NewLine><NewLine>    def discount_rewards(self, rewards, gamma):<NewLine>        stepReturn = torch.zeros(1, 1)<NewLine>        stepReturns = []<NewLine>        for i in range(len(rewards)):<NewLine>            stepReturn = gamma * stepReturn + rewards[i]<NewLine>            stepReturns.append(stepReturn)<NewLine>        return list(reversed(stepReturns))<NewLine><NewLine>    def update_parameters(self, rewards, log_probs, gamma):<NewLine>        discounted_rewards = self.discount_rewards(rewards, gamma)<NewLine>        loss = 0<NewLine>        for i in range(len(rewards)):<NewLine>            foo = log_probs[i]*Variable(discounted_rewards[i])<NewLine>            loss = loss + foo[0]<NewLine>        loss = loss / len(rewards)<NewLine>        loss = -loss<NewLine>        self.optimizer.zero_grad()<NewLine>        loss.backward()<NewLine>        self.optimizer.step()<NewLine><NewLine><NewLine><NewLine>agent = Agent(args.hidden_size, env.observation_space.shape[0], env.action_space)<NewLine>reward_sums = []<NewLine>for i_episode in range(args.num_episodes):<NewLine>    state = torch.Tensor([env.reset()])<NewLine>    log_probs = []<NewLine>    rewards = []<NewLine>    for t in range(args.num_steps):<NewLine>        action, log_prob = agent.select_action(state)<NewLine>        next_state, reward, done, _ = env.step(action.numpy()[0])<NewLine>        log_probs.append(log_prob)<NewLine>        rewards.append(reward)<NewLine>        state = torch.Tensor([next_state])<NewLine><NewLine>        if done:<NewLine>            break<NewLine><NewLine>    agent.update_parameters(rewards, log_probs, args.gamma)<NewLine></code></pre><NewLine><h1>TensorFlow Version (Does Not Train)</h1><NewLine><pre><code>class Policy:<NewLine>  def __init__(self, hparams, session):<NewLine>    self.session = session<NewLine>    optimizer = tf.train.AdamOptimizer(hparams['learning_rate'])<NewLine>    self.observations = tf.placeholder(tf.float32, shape=[None, 4], name=""observations"")<NewLine>    self.actions = tf.placeholder(tf.float32, name=""actions"")<NewLine>    self.returns = tf.placeholder(tf.float32, name=""returns"")<NewLine>    normal = self.build_graph(hparams)<NewLine>    self.action = normal.sample()<NewLine>    log_probs = normal.log_prob(self.actions)<NewLine>    loss = -tf.reduce_mean(tf.multiply(log_probs, self.returns))<NewLine>    self.trainMe = optimizer.minimize(loss)<NewLine><NewLine>  def build_graph(self, hparams):<NewLine>    hidden = tf.contrib.layers.fully_connected(<NewLine>            inputs=self.observations,<NewLine>            num_outputs=hparams['hidden_size'],<NewLine>            activation_fn=tf.nn.relu)<NewLine><NewLine>    mu = tf.contrib.layers.fully_connected(<NewLine>            inputs=hidden,<NewLine>            num_outputs=1,<NewLine>            activation_fn=None)<NewLine><NewLine>    sigma_sq = tf.contrib.layers.fully_connected(<NewLine>            inputs=hidden,<NewLine>            num_outputs=1,<NewLine>            activation_fn=None)<NewLine><NewLine>    sigma_sq = tf.nn.softplus(sigma_sq)<NewLine>    sigma = tf.sqrt(sigma_sq)<NewLine>    flat_sigma = tf.reshape(sigma,[-1])<NewLine>    flat_mu = tf.reshape(mu,[-1])<NewLine>    return tf.distributions.Normal(flat_mu, flat_sigma)<NewLine><NewLine>  def select_action(self, observation):<NewLine>    feed = { self.observations: [observation] }<NewLine>    return self.session.run(self.action, feed_dict=feed)<NewLine><NewLine>  def update_parameters(self, observations, actions, returns, ep_index):<NewLine>    feed = {<NewLine>      self.observations: observations,<NewLine>      self.actions: actions,<NewLine>      self.returns: returns,<NewLine>    }<NewLine>    self.session.run(self.trainMe, feed_dict = feed)<NewLine><NewLine><NewLine>class Agent:<NewLine>  def __init__(self, hparams):<NewLine>    self.hparams = hparams<NewLine>    self.env = gym.make('RoboschoolInvertedPendulum-v2')<NewLine><NewLine>  def run(self):<NewLine>    with tf.Graph().as_default(), tf.Session() as session:<NewLine>      policy = Policy(self.hparams, session)<NewLine>      session.run(tf.global_variables_initializer())<NewLine><NewLine>      for ep_index in range(self.hparams['max_episodes']):<NewLine>        observations, actions, rewards = self.policy_rollout(policy)<NewLine>        returns = self.discount_rewards(rewards)<NewLine>        policy.update_parameters(observations, actions, returns, ep_index)<NewLine><NewLine>  def policy_rollout(self, policy):<NewLine>    observation, reward, done = self.env.reset(), 0, False<NewLine>    observations, actions, rewards  = [], [], []<NewLine><NewLine>    while not done:<NewLine>      action = policy.select_action(observation)<NewLine>      observation, reward, done, _ = self.env.step(action)<NewLine>      observations.append(observation)<NewLine>      actions.append(action[0])<NewLine>      rewards.append(reward)<NewLine><NewLine>    return observations, actions, rewards<NewLine><NewLine>  def discount_rewards(self, rewards):<NewLine>    discounted_rewards = np.zeros_like(rewards)<NewLine>    running_add = 0<NewLine>    for t in reversed(range(0, len(rewards))):<NewLine>      running_add = running_add * self.hparams['gamma'] + rewards[t]<NewLine>      discounted_rewards[t] = running_add<NewLine>    return discounted_rewards<NewLine><NewLine><NewLine>hparams = {<NewLine>  'max_episodes': 5000,<NewLine>  'gamma': 0.99,<NewLine>  'hidden_size': 128,<NewLine>  'learning_rate': 0.001<NewLine>}<NewLine><NewLine>agent = Agent(hparams)<NewLine>agent.run()<NewLine></code></pre><NewLine><p><strong>EDIT</strong> Added Mu and Sigma to graphs above. I think they help tell the story of what is wrong. Now we just need to figure out WHY <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=5"" title="":slight_smile:""/></p><NewLine><p><strong>EDIT</strong> I’m aware that the initialization for the two networks is slightly different, primarily that the bias for TF is init to zero and for PyT bias is init to sampling from std. I updated TF to do the same thing and it did not help.</p><NewLine></div>",https://discuss.pytorch.org/u/keithmgould,(Keith Gould),keithmgould,"February 15, 2018,  9:25pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>At quick glance looks like tensorflow model updates after 5000 episodes which is long time between updates</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dgriff; <NewLine> ,"REPLY_DATE 1: February 20, 2018,  8:36pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
13784,Python 2.7 - torch.save/load - unknown error,2018-02-19T09:53:22.361Z,0,610,"<div class=""post"" itemprop=""articleBody""><NewLine><p>`File “C:\Anaconda2\lib\site-packages\torch\serialization.py”, line 192, in _save<br/><NewLine>serialized_storages[key]._write_file(f)</p><NewLine><p>RuntimeError: Unknown error`</p><NewLine><p>Getting this error when using torch.save() on python 2.7 and torch 0.1.12</p><NewLine><pre><code>     torch.save({""state_dict"": self.model.state_dict(),<NewLine>                ""optimizer"" : self.optimizer.state_dict(),<NewLine>               }, ""last_brain.pth"")<NewLine></code></pre><NewLine><p>and using</p><NewLine><p>checkpoint = torch.load(‘last_brain.pth’)<br/><NewLine>self.model.load_state_dict(checkpoint[‘state_dict’])<br/><NewLine>self.optimizer.load_state_dict(checkpoint[‘optimizer’])</p><NewLine><p>for load… this error:</p><NewLine><p>File “C:\Anaconda2\lib\site-packages\torch\serialization.py”, line 384, in _load<br/><NewLine>deserialized_objects[key]._set_from_file(f, offset)</p><NewLine><p>RuntimeError: Unknown error</p><NewLine></div>",https://discuss.pytorch.org/u/Aang_Maya,(Aang Maya),Aang_Maya,"February 19, 2018,  9:54am",,,,,
13101,Understanding REINFORCE implementation,2018-01-31T14:36:27.078Z,0,1802,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to reconcile the implementation of REINFORCE with the math. To begin, the R algorithm attempts to maximize the expected reward.</p><NewLine><p><img alt="""" height=""19"" src=""https://latex.codecogs.com/gif.latex?%5Cmathbb%7BE%7D%5Br(x)%5D"" width=""53""/></p><NewLine><p>How it’s commonly implemented in neural networks in code is by taking the gradient of reward times logprob.</p><NewLine><pre><code>loss = reward*logprob<NewLine>loss.backwards()<NewLine></code></pre><NewLine><p>In other words,</p><NewLine><p><img alt="""" height=""19"" src=""https://latex.codecogs.com/gif.latex?r(x)&amp;space;*&amp;space;%5Cnabla&amp;space;%5Clog&amp;space;P(x%7C%5Ctheta)"" width=""146""/></p><NewLine><p>Where theta are the parameters of the neural network. This makes sense sense because we’re using the logprob trick to transform the gradient of our expected loss.</p><NewLine><p><img alt="""" height=""19"" src=""https://latex.codecogs.com/gif.latex?%5Cnabla&amp;space;%5Cmathbb%7BE%7D%5Br(x)%5D&amp;space;=&amp;space;%5Cmathbb%7BE%7D%5B&amp;space;r(x)&amp;space;*&amp;space;%5Cnabla&amp;space;%5Clog&amp;space;P(x%7C%5Ctheta)%5D"" width=""261""/></p><NewLine><p>What I’m not comfortable with is interpreting reward*logprob as a “loss” because</p><NewLine><p><img alt="""" height=""19"" src=""https://latex.codecogs.com/gif.latex?%5Cnabla&amp;space;%5Br(x)&amp;space;%5Clog&amp;space;P(x%7C%5Ctheta)%5D"" width=""137""/></p><NewLine><p>with r(x) * log P(X) as a Monte Carlo sample of a loss inevitably suggests taking N samples of x</p><NewLine><p><img alt="""" height=""55"" src=""https://latex.codecogs.com/gif.latex?%5Cnabla&amp;space;%5Cleft&amp;space;%5B%5Cfrac%7B1%7D%7BN%7D&amp;space;%5Csum%5EN_%7Bi=1%7D&amp;space;r_i(x)&amp;space;%5Clog&amp;space;P(x_i%7C%5Ctheta)&amp;space;%5Cright%5D%5D"" width=""219""/></p><NewLine><p>This is an approximation to:</p><NewLine><p><img alt="""" height=""19"" src=""https://latex.codecogs.com/gif.latex?%5Cnabla&amp;space;%5Cmathbb%7BE%7D%5Br(x)&amp;space;*&amp;space;%5Clog&amp;space;P(x%7C%5Ctheta)%5D"" width=""164""/></p><NewLine><p>which is emphatically not what we started out with: <img alt="""" height=""19"" src=""https://latex.codecogs.com/gif.latex?%5Cmathbb%7BE%7D%5Br(x)%5D"" width=""53""/></p><NewLine><p>I believe there’s something subtle going on with taking the gradient through the sampling process but somehow you lose the sampling term in the math when you make Monte Carlo estimates.</p><NewLine></div>",https://discuss.pytorch.org/u/shenkev,(Shenkev),shenkev,"January 31, 2018,  3:55pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Prob not helping, but when you say the reward * log_prob, I think you mean the <strong>return</strong> * log_prob. They are different. Reward is what is given at each step of the episode. Return is the expected cumulative reward over the entire episode from the time t onward. In Sutton’s book the return is represented by ‘G’ in the pseudocode on page 271.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>In addition to what <a class=""mention"" href=""/u/keithmgould"">@keithmgould</a> said, keep in mind that you are calculating the expectation of your return with respect to the trajectory distribution, and this expectation is the objective function to be maximized.</p><NewLine><p>I recommend studying the first few slides of Sergey Levine’s <a href=""http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_4_policy_gradient.pdf"" rel=""nofollow noopener"">Policy Gradient lecture notes</a> to understand how the gradient of the objective is derived.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/keithmgould; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Sam; <NewLine> ,"REPLY_DATE 1: February 15, 2018,  9:39pm; <NewLine> REPLY_DATE 2: February 16, 2018,  2:07am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
12933,The probability of a path for a continuous action space is 0,2018-01-28T00:25:23.645Z,1,529,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi. When the action space A is continuous, and the policy, for example is a normal distribution, then the probability of a path is 0. I’m curious about how to deal with it when, for example implementing PPO and TRPO, where the probability of a path should be multiplied. There must be a formal explanation for it, but I haven’t found it <img alt="":frowning:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/frowning.png?v=5"" title="":frowning:""/></p><NewLine><p>Thanks for your help.</p><NewLine></div>",https://discuss.pytorch.org/u/Xiaodong_Jia,(Edward),Xiaodong_Jia,"January 28, 2018, 12:28am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The probability of an action from a normal distribution is always 0. Look at probability densities. When working with continuous actions, you can usually use the probability density of an action instead of the probability (which is used for discrete actions)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=5"" title="":slight_smile:""/><br/><NewLine>In fact, I’ve checked the implementations, and found the probability density is used instead. I’m not sure if this is a well-known fact, since I haven’t seen a paper mentioned it. Could you give me some formal reference of this fact?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>See section 13.7 of Sutton’s Reinforcement Learning Book.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/keithmgould; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Xiaodong_Jia; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/keithmgould; <NewLine> ,"REPLY_DATE 1: February 15, 2018,  9:42pm; <NewLine> REPLY_DATE 2: February 15, 2018,  9:57pm; <NewLine> REPLY_DATE 3: February 15, 2018, 10:01pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
13699,BatchNorm error eval() mode,2018-02-15T20:24:50.532Z,0,806,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey,<br/><NewLine>I am unable to carry out a forward pass on my network, it seems to throw an error at the BatchNorm layer, even with the model in eval() mode. I am unable to spot my mistake, I hope someone can help me.<br/><NewLine>I have attached the relevant code snippets below :-</p><NewLine><p>class Actor(nn.Module):<br/><NewLine>def <strong>init</strong>(self, env):<br/><NewLine>super(Actor, self).<strong>init</strong>()<br/><NewLine>self.stateDim = len(obs2state(env.reset().observation)) <span class=""hashtag"">#evaluates</span> to 37<br/><NewLine>self.actionDim = env.action_spec().shape[0]<br/><NewLine>self.norm0 = nn.BatchNorm1d(self.stateDim)</p><NewLine><hr/><NewLine><p><span class=""hashtag"">#main</span> function:<br/><NewLine>actor = Actor(env).cuda()<br/><NewLine>actor.eval()<br/><NewLine>action = actor(Variable(curState, volatile=True).cuda())</p><NewLine><p>Error:<br/><NewLine>raise ValueError(‘Expected more than 1 value per channel when training, got input size {}’.format(size))<br/><NewLine>.<br/><NewLine>.<br/><NewLine>running_mean should contain 140389837773688 elements not 37</p><NewLine></div>",https://discuss.pytorch.org/u/samlanka,(Sameera Lanka),samlanka,"February 15, 2018,  8:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Okay, the problem was because my input was of size (37,). Appending .view(1,-1) in the variable call helped.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/samlanka; <NewLine> ,"REPLY_DATE 1: February 15, 2018,  9:19pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
13358,DQN tutorial run error,2018-02-06T14:24:01.978Z,1,773,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Getting this error in running DQN tutorial.</p><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “reinforcement_q_learning.py”, line 109, in <br/><NewLine>plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),<br/><NewLine>File “reinforcement_q_learning.py”, line 105, in get_screen<br/><NewLine>return resize(screen).unsqueeze(0).type(Tensor)<br/><NewLine>NameError: global name ‘Tensor’ is not defined</p><NewLine></div>",https://discuss.pytorch.org/u/prateekagarwal3,(Prateek Agarwal),prateekagarwal3,"February 6, 2018,  2:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>Tensor</code> is defined in the first code snippet:</p><NewLine><pre><code class=""lang-auto""># if gpu is to be used<NewLine>use_cuda = torch.cuda.is_available()<NewLine>FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor<NewLine>LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor<NewLine>ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor<NewLine>Tensor = FloatTensor<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It worked thank you. I had commented some part of the code and this commented this also.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/prateekagarwal3; <NewLine> ,"REPLY_DATE 1: February 6, 2018,  3:12pm; <NewLine> REPLY_DATE 2: February 8, 2018,  5:15am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
13306,Result of slicing is an empty tensor,2018-02-05T18:50:10.007Z,2,984,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I am trying to create a variable-length sequence for a LSTM.</p><NewLine><p>Can anyone help me understand why this code works:</p><NewLine><pre><code class=""lang-auto"">lens = list(range(170,1,-1))<NewLine>xs = Variable(torch.randn(169, 200, 1))<NewLine>packed = torch.nn.utils.rnn.pack_padded_sequence(xs, lens, batch_first=True)<NewLine></code></pre><NewLine><p>and this code does not:</p><NewLine><pre><code class=""lang-auto"">lens = [294, 289, 288, 282, 273, 270, 261, 260, 240, 235, 231, 228, 228, 227, 226, 226, 199, 195, 194, 192, 190, 189, 177, 176, 165, 165, 161, 156, 153, 149, 149, 142, 142, 137, 136, 136, 135, 134, 134, 132, 131, 129, 122, 121, 121, 114, 113, 113, 112, 110, 109, 108, 107, 107, 106, 105, 105, 103, 102, 100, 99, 99, 98, 96, 95, 93, 92, 91, 91, 90, 88, 88, 87, 79, 78, 78, 77, 76, 75, 74, 73, 72, 72, 71, 71, 71, 71, 69, 69, 69, 68, 68, 68, 68, 68, 68, 67, 67, 66, 66, 65, 65, 64, 64, 64, 63, 63, 61, 61, 61, 61, 60, 60, 59, 59, 59, 59, 57, 57, 57, 57, 57, 56, 56, 55, 55, 54, 54, 54, 54, 54, 53, 53, 52, 52, 52, 51, 51, 51, 51, 51, 50, 50, 50, 49, 49, 49, 48, 47, 47, 47, 47, 46, 46, 46, 45, 44, 44, 44, 44, 43, 42, 39, 38, 36, 30, 30, 25, 23]<NewLine>xs = Variable(torch.randn(169, 200, 1))<NewLine>packed = torch.nn.utils.rnn.pack_padded_sequence(xs, lens, batch_first=True)<NewLine></code></pre><NewLine><p>The only difference is lens. In the first case I’ve created an artificial one using range() and in the other case is my real data.</p><NewLine><p>Thanks a lot for your help <img alt="":slight_smile:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=5"" title="":slight_smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/Rodrigo_Goncalves,(Rodrigo Goncalves),Rodrigo_Goncalves,"February 5, 2018,  6:50pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Because the first len has values all &lt;= 200, but the second has many &gt; 200, which is the maximum seq_len a tensor of shape 169,200,1 can have.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sure!!! Of course! I must be blind.</p><NewLine><p>Thanks a lot!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>No worries. I make stupid mistakes all the time <img alt="":smiley:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/smiley.png?v=5"" title="":smiley:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SimonW; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Rodrigo_Goncalves; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/SimonW; <NewLine> ,"REPLY_DATE 1: February 5, 2018,  6:52pm; <NewLine> REPLY_DATE 2: February 5, 2018,  6:53pm; <NewLine> REPLY_DATE 3: February 5, 2018,  6:54pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
12782,Actor Critic implementation problem,2018-01-24T10:11:37.050Z,4,3327,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello !</p><NewLine><p>I’m trying to implement an actor-critic algorithm using PyTorch. My understanding was that it was based on two separate agents, one actor for the policy and one critic for the state estimation, the former being used to adjust the weights that are represented by the reward in REINFORCE. I recently found a code in which both the agents have weights in common and I am somewhat lost.</p><NewLine><p>Let me first introduce the context;</p><NewLine><p>I previously implemented REINFORCE with baseline using the following strategy. I’m not sure it’s correct but the agent being successful in various environments, I thought it had to be. Here’s what I did:</p><NewLine><ul><NewLine><li>Play a full episode, record transition</li><NewLine><li>Discount reward</li><NewLine><li>Apply the baseline: For each state of the episode, I use the critic to evaluate the state and substract this value to the corresponding reward</li><NewLine><li>Update the critic : Target = R_state + y*V(next_state). Minimize loss between Target and estimated state value</li><NewLine><li>Update the policy: I use the adjusted reward vector as the weight vector of REINFORCE in which it is just the discounted reward. Hence: loss = - sum(log(selected_prob)*weights)</li><NewLine></ul><NewLine><p>REINFORCE with baseline seemed easier because you basically just replace the weights vector.<br/><NewLine>Afterwards, I was indeed expecting to follow a similar strategy with AC. Indeed:</p><NewLine><ul><NewLine><li>The critic update has the same form</li><NewLine><li>For the actor, I’d use this delta = R_state + y*V(next_state) - V(state) as weights, as explained in Sutton’s book</li><NewLine></ul><NewLine><p>While browsing the web for confirmation, looking for an understandable implementation on Github, I came across a code in which the actor and the critic have a common network and only the last layer brings specification. Though I’m highly in favor of sharing, I do not understand this setting and how the backprop works in this case.</p><NewLine><p>Here’s the link to my baseline implementation: <a href=""https://github.com/Mehd6384/RL/blob/master/Baseline.py"" rel=""nofollow noopener"">https://github.com/Mehd6384/RL/blob/master/Baseline.py</a><br/><NewLine>Here’s the link to the puzzling (but working) implementation in question: <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py"" rel=""nofollow noopener"">https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py</a></p><NewLine><p>Do the actor and the critic have to share the network ? Or no ? Or is it given the user’s taste ? How can the puzzling and working implementation join both the losses and backpropagate ?</p><NewLine><p>Thanks a lot !</p><NewLine></div>",https://discuss.pytorch.org/u/Mehdi,,Mehdi,"January 24, 2018, 10:11am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>A good reason for sharing code between the actor and the critic is that both need to understand the environment. The model basically does this…</p><NewLine><pre><code>shared_features = understanding_environment(input)<NewLine>action = actor(shared_features)<NewLine>value = critic(shared_features)<NewLine></code></pre><NewLine><p>shared_features is used in two different submodules and backpropagation simply adds the gradients that come from each submodule.</p><NewLine><p>The puzzling code sums the losses.</p><NewLine><pre><code>loss = torch.cat(policy_losses).sum() + torch.cat(value_losses).sum()<NewLine>loss.backward()<NewLine></code></pre><NewLine><p>PyTorch knows how to distribute the added loss correctly to each submodule when calculating the gradients.</p><NewLine><p>If the logic of backpropagation is fuzzy for you, can I suggest watching a <a href=""https://youtu.be/GZTvxoSHZIo?list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA&amp;t=192"" rel=""nofollow noopener"">lecture</a> by Andrej Karpathy that goes over the details.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey ! Thanks a lot for your answer.<br/><NewLine>I’m fine with backprop. Mostly my problem was that I just wasn’t sure whether the computation graph was sufficiently explicit for PyTorch to manage attributing the gradients given the loss was mixed.</p><NewLine><p>Then, I guess that in REINFORCE with baseline the model should also share some weights, shouldn’t it ?</p><NewLine><p>Thanks again !</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Something just occured to me. If we want PyTorch to be able to compute the gradients, that means that the whole computation, including action selection has to be done within the framework. I mean that in this case, you can’t compute the policy distribution, convert it to numpy, make your stochastic pick, use it, store it and later on, when the episode is over use it back, right?<br/><NewLine>Because then the computation graph would have holes and elements missing, am I right?</p><NewLine><p>Thanks !</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Exactly. If you want the gradient calculation to work you must use pytorch operations. If needed you can write your own. <a href=""http://pytorch.org/docs/0.3.0/notes/extending.html"" rel=""nofollow noopener"">http://pytorch.org/docs/0.3.0/notes/extending.html</a></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well, I’d better say hello to convenient good ol’ functions and start get accustomed to Tensors op. <img alt="":wink:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/wink.png?v=5"" title="":wink:""/><br/><NewLine>I’m highly grateful for your answers !</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jpeg729; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Mehdi; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Mehdi; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jpeg729; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Mehdi; <NewLine> ,"REPLY_DATE 1: January 24, 2018, 10:51am; <NewLine> REPLY_DATE 2: January 24, 2018,  2:59pm; <NewLine> REPLY_DATE 3: January 24, 2018,  3:03pm; <NewLine> REPLY_DATE 4: January 24, 2018,  3:07pm; <NewLine> REPLY_DATE 5: January 24, 2018,  3:09pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
12734,Basic CNN question,2018-01-23T14:34:26.028Z,0,331,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi !<br/><NewLine>I’d like to use CNN for reinforcement Learning and I have a question regarding the size of the first dense layer after the feature maps.</p><NewLine><p>I have a setup that seems to work for now: Input would be a (1,1,50,50) Tensor ie: One image with only one channel with dim 50x50. It goes in conv2d filter, a non-linear activation. Afterwards, it is flatten and sent to the dense layer to be used for action selection.<br/><NewLine>I’d like to send multiples image to train the network offline (ie: after finishing an episode, I want to pack all the states, all the images, and send them at once so I can get state estimations). Doing so, I always meet an error saying that the first dense layer after filters doesn’t have a good dimensionality.<br/><NewLine>Does it mean that I have to keep sending them one by one ?</p><NewLine><p>Thanks !</p><NewLine></div>",https://discuss.pytorch.org/u/Mehdi,,Mehdi,"January 23, 2018,  2:34pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You should be able to make batches of images of shape (n_images, 1, 50, 50) and train with those.</p><NewLine><p>My guess is that the code you use to flatten the cnn output is incorrect, but just happens to work when sending images 1 by 1. My guess is that your code turns the 4d output of the cnn into a 1d tensor, thus absorbing the batch dimension.</p><NewLine><p>This should work…</p><NewLine><pre><code>dense_input = cnn_output.view(cnn_output.size(0), -1)</code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yop, Indeed, it was exactly so !<br/><NewLine>Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jpeg729; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Mehdi; <NewLine> ,"REPLY_DATE 1: January 23, 2018,  4:09pm; <NewLine> REPLY_DATE 2: January 23, 2018,  4:09pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
12201,How to avoid this error？,2018-01-12T09:31:19.080Z,2,721,"<div class=""post"" itemprop=""articleBody""><NewLine><p>iteration 803…<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “train_do.py”, line 229, in <br/><NewLine>train(opt)<br/><NewLine>File “train_do.py”, line 134, in train<br/><NewLine>policy_loss = policy_loss.sum()<br/><NewLine>RuntimeError: value cannot be converted to type double without overflow: -inf</p><NewLine></div>",https://discuss.pytorch.org/u/guotong1988,(Tong Guo),guotong1988,"January 12, 2018,  9:31am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>The error is because your loss is <code>-inf</code> is that expected? If not you want to look why you loss goes to <code>-inf</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>the loss is about 200000 at last step</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>after 800 step, the loss go from 700000 to 200000</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/guotong1988; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/guotong1988; <NewLine> ,"REPLY_DATE 1: January 12, 2018,  9:41am; <NewLine> REPLY_DATE 2: January 12, 2018,  9:50am; <NewLine> REPLY_DATE 3: January 12, 2018,  9:52am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
7049,Store models in different GPUs for different subprocesses,2017-09-05T04:00:16.231Z,0,612,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I tried to implement A3C using torch.multiprocess and put my models into different gpus. However it told me I cannot use cuda when using multiprocess unless I switch to Python 3.4+ (I am currently using Python 2.7.6). The document recommand me to use DataParallel. But I want to implement asynchronous algorithm. In addition, DataParallel is implemented with threads, while my agents may have a lot of cpu operations and python objects modifications, which can be frequently blocked due to Python’s GIL and slow down the speed. So how can I correctly implemented A3C in Pytorch and store all my parameters in to cuda?</p><NewLine></div>",https://discuss.pytorch.org/u/dalegebit,(Dalegebit),dalegebit,"September 5, 2017,  4:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I got this to work and working fast if you have the GPUs:) For Pong it speed up convergence to 10mins on my dgx station compared to 45mins on CPU only.</p><NewLine><p>I have posted new A3C-GPU versions in repos below:</p><NewLine><p>discrete action spaces:<br/><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://assets-cdn.github.com/favicon.ico"" width=""16""/><NewLine><a href=""https://github.com/dgriff777/rl_a3c_pytorch"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars1.githubusercontent.com/u/24666148?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/dgriff777/rl_a3c_pytorch"" rel=""nofollow noopener"" target=""_blank"">dgriff777/rl_a3c_pytorch</a></h3><NewLine><p>rl_a3c_pytorch - Reinforcement learning A3C LSTM Atari with Pytorch</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>continuous action space version:<br/><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://assets-cdn.github.com/favicon.ico"" width=""16""/><NewLine><a href=""https://github.com/dgriff777/a3c_continuous"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars1.githubusercontent.com/u/24666148?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/dgriff777/a3c_continuous"" rel=""nofollow noopener"" target=""_blank"">dgriff777/a3c_continuous</a></h3><NewLine><p>a3c_continuous - A continuous action space version of A3C LSTM in pytorch</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dgriff; <NewLine> ,"REPLY_DATE 1: January 9, 2018,  4:26pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
11847,What&rsquo;s wrong with my reinforce and mnist?,2018-01-04T06:51:06.404Z,0,492,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I copy the code from <a href=""https://github.com/pytorch/examples/blob/master/mnist/main.py"" rel=""nofollow noopener"">https://github.com/pytorch/examples/blob/master/mnist/main.py</a></p><NewLine><p>And add these lines:</p><NewLine><pre><code class=""lang-auto"">    def reinforce():<NewLine>        model.train()<NewLine>        reward = torch.zeros([64,10]).cuda()<NewLine>        for batch_idx, (data, target) in enumerate(train_loader):<NewLine>            if len(target)&lt;64:<NewLine>                continue<NewLine>            temp_target = target.clone()<NewLine>            if args.cuda:<NewLine>                data, target = data.cuda(), target.cuda()<NewLine>            data, target = Variable(data), Variable(target)<NewLine>            optimizer.zero_grad()<NewLine>            output = model(data)<NewLine>            pred = output.data.max(1, keepdim=True)[1]<NewLine>    <NewLine>            for i in range(64):<NewLine>                temp1 = temp_target[i]<NewLine>                temp2 = pred[i].cpu().numpy()<NewLine>                if temp1!=temp2[0]:<NewLine>                    reward[i][temp_target[i]] = -1<NewLine>    <NewLine>            sample = torch.multinomial(output,10)<NewLine>            sample.reinforce(reward)<NewLine>            sample.backward()<NewLine>            # loss = F.nll_loss(output, target)<NewLine>            # loss.backward()<NewLine>            optimizer.step()<NewLine>            if batch_idx % args.log_interval == 0:<NewLine>                print(batch_idx)<NewLine>    <NewLine>    for epoch in range(1, args.epochs + 1):<NewLine>        train(epoch)<NewLine>        test()<NewLine>        reinforce()<NewLine>        test()<NewLine></code></pre><NewLine><p>The core idea is here</p><NewLine><pre><code class=""lang-auto"">            for i in range(64):<NewLine>                temp1 = temp_target[i]<NewLine>                temp2 = pred[i].cpu().numpy()<NewLine>                if temp1!=temp2[0]:<NewLine>                    reward[i][temp_target[i]] = -1<NewLine></code></pre><NewLine><p>I want to penalize if the predicted result and ground truth does not equal.</p><NewLine><p>The result is: After first <code>train()</code>, the accuracy is 94% and after the first <code>reinforce()</code> the accuracy is 11%</p><NewLine></div>",https://discuss.pytorch.org/u/guotong1988,(Tong Guo),guotong1988,"January 4, 2018,  6:51am",,,,,
11783,How can one implement multilayered LSTM with LSTMCell module?,2018-01-02T21:41:53.428Z,0,561,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The LSTM module has a parameter <em>num_layers</em> which enables us to create multilayered RNNs. Is there a way to connect LSTMCell modules together to get a similar effect, and what is the right way to do it?</p><NewLine></div>",https://discuss.pytorch.org/u/Erik_Janezic,(Erik Janežič),Erik_Janezic,"January 2, 2018,  9:41pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>for LSTMCell modules you will essentially do this manually with a python for-loop.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/smth; <NewLine> ,"REPLY_DATE 1: January 3, 2018,  4:21pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
11721,Can LSTM replace Replay Memory and or Elgibility Trace?,2017-12-31T14:15:39.155Z,0,555,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>As the title said, I am trying to make an ai that plays a fighting game and i don’t know if i should only implement LSTM, also all the tutorials i found are about using the OpenGym, if you know where i can find a tutorial that actually uses an actual game, that would be helpful , thank you for your time</p><NewLine></div>",https://discuss.pytorch.org/u/Inglor,(Taha Lamari),Inglor,"December 31, 2017,  2:15pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>For the question about an algorithm, I think this thread is useful.<br/><NewLine><aside class=""quote"" data-post=""1"" data-topic=""11111""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v2/letter/x/4da419/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/example-code-of-recurrent-policy-gradient/11111"">Example code of recurrent policy gradient?</a> <a class=""badge-wrapper bullet"" href=""/c/reinforcement-learning""><span class=""badge-category-bg"" style=""background-color: #AB9364;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""A section to discuss RL implementations, research, problems"">reinforcement-learning</span></a><NewLine></div><NewLine><blockquote><NewLine>    Is there an example code for recurrent policy gradient ? Will it be simply replacing MLP with RNN ?<NewLine>  </blockquote><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Ktakuya332C; <NewLine> ,"REPLY_DATE 1: December 31, 2017,  4:33pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
11609,Issue with REINFORCE implementation,2017-12-27T14:54:40.788Z,1,880,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,<br/><NewLine>I’ve been playing around with the REINFORCE algorithm, and decided to modify the example found <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""nofollow noopener"">here</a> to include a replay memory buffer. For some reason, the changes mean that the network no longer learns very well, even when the memory buffer is being used in the same way as the original lists holding the actions and log probs (as is currently implemented below). I can’t for the life of me figure out what is causing this, though. It’s driving me insane. Full code below. Please help!</p><NewLine><pre><code>&gt; import argparse<NewLine>&gt; import gym<NewLine>&gt; import numpy as np<NewLine>&gt; from itertools import count<NewLine>&gt; <NewLine>&gt; import torch<NewLine>&gt; import torch.nn as nn<NewLine>&gt; import torch.nn.functional as F<NewLine>&gt; import torch.optim as optim<NewLine>&gt; from torch.autograd import Variable<NewLine>&gt; from torch.distributions import Categorical<NewLine>&gt; <NewLine>&gt; import random<NewLine>&gt; from collections import namedtuple<NewLine>&gt; <NewLine>&gt; parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')<NewLine>&gt; <NewLine>&gt; parser.add_argument('--gamma', type=float, default=0.99, metavar='G', help='discount factor (default: 0.99)')<NewLine>&gt; parser.add_argument('--seed', type=int, default=543, metavar='N', help='random seed (default: 543)')<NewLine>&gt; parser.add_argument('--render', action='store_true', help='render the environment')<NewLine>&gt; parser.add_argument('--log-interval', type=int, default=10, metavar='N', help='interval between training status logs (default: 10)')<NewLine>&gt; parser.add_argument('--no-cuda', action='store_true', default=False, help='disables CUDA training')<NewLine>&gt; parser.add_argument('--memory-capacity', type=int, default=None, metavar='mc', help='batch replay memory capacity')<NewLine>&gt; parser.add_argument('--batch-size', type=int, default=64, metavar='b', help='batchsize for update function')<NewLine>&gt; <NewLine>&gt; args = parser.parse_args()<NewLine>&gt; <NewLine>&gt; args.cuda = not args.no_cuda and torch.cuda.is_available()<NewLine>&gt; torch.manual_seed(args.seed)<NewLine>&gt; if args.cuda:<NewLine>&gt;     torch.cuda.manual_seed(args.seed)<NewLine>&gt; <NewLine>&gt; env = gym.make('CartPole-v0')<NewLine>&gt; env.seed(args.seed)<NewLine>&gt; torch.manual_seed(args.seed)<NewLine>&gt; <NewLine>&gt; def discounted_reward(reward, gamma=0.99):<NewLine>&gt;     reward = reward[::-1]<NewLine>&gt;     disc_reward = []<NewLine>&gt;     cumulative = 0<NewLine>&gt;     for r in reward:<NewLine>&gt;         val = cumulative*gamma + r<NewLine>&gt;         disc_reward.insert(0, val)<NewLine>&gt;         cumulative = val<NewLine>&gt;     return disc_reward<NewLine>&gt; <NewLine>&gt; <NewLine>&gt; Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'log_prob'))<NewLine>&gt; class ReplayMemory(object):<NewLine>&gt;     def __init__(self, capacity=None):<NewLine>&gt;         self.capacity = capacity<NewLine>&gt;         self.memory = []<NewLine>&gt;         self.position = 0<NewLine>&gt; <NewLine>&gt;     def push(self, *args):<NewLine>&gt;         if self.capacity is not None:<NewLine>&gt;             if len(self.memory) &lt; self.capacity:<NewLine>&gt;                 self.memory.append(None)<NewLine>&gt;             self.memory[self.position] = Transition(*args)<NewLine>&gt;             self.position = (self.position + 1) % self.capacity<NewLine>&gt;         else:<NewLine>&gt;             self.memory.append(Transition(*args))<NewLine>&gt; <NewLine>&gt;     def sample(self, batch_size):<NewLine>&gt;         return random.sample(self.memory, batch_size)<NewLine>&gt; <NewLine>&gt;     def pull(self):<NewLine>&gt;         return self.memory<NewLine>&gt; <NewLine>&gt;     def __len__(self):<NewLine>&gt;         return len(self.memory)<NewLine>&gt; <NewLine>&gt;     def clear(self):<NewLine>&gt;         self.memory = []<NewLine>&gt;         self.position = 0<NewLine>&gt; <NewLine>&gt; <NewLine>&gt; class Policy(nn.Module):<NewLine>&gt;     def __init__(self, input_dim, hidden_dim, output_dim, capacity, use_gpu=True):<NewLine>&gt;         super(Policy, self).__init__()<NewLine>&gt;         self.input_dim = input_dim<NewLine>&gt;         self.hidden_dim = hidden_dim<NewLine>&gt;         self.output_dim = output_dim<NewLine>&gt;         self.use_gpu = use_gpu<NewLine>&gt; <NewLine>&gt;         self.affine1 = nn.Linear(input_dim, hidden_dim)<NewLine>&gt;         self.affine2 = nn.Linear(hidden_dim, output_dim)<NewLine>&gt; <NewLine>&gt;         self.memory = ReplayMemory(capacity)<NewLine>&gt; <NewLine>&gt;         self.state = []<NewLine>&gt;         self.action = []<NewLine>&gt;         self.next_state = []<NewLine>&gt;         self.reward = []<NewLine>&gt;         self.log_prob = []<NewLine>&gt; <NewLine>&gt;         if use_gpu:<NewLine>&gt;             self.Tensor = torch.cuda.FloatTensor<NewLine>&gt;         else:<NewLine>&gt;             self.Tensor = torch.FloatTensor<NewLine>&gt; <NewLine>&gt;     def forward(self, x):<NewLine>&gt;         x = F.relu(self.affine1(x))<NewLine>&gt;         action_scores = self.affine2(x)<NewLine>&gt;         return F.softmax(action_scores, dim=1)<NewLine>&gt; <NewLine>&gt;     def select_action(self, state):<NewLine>&gt;         probs = self.forward(state)<NewLine>&gt;         m = Categorical(probs)<NewLine>&gt;         action = m.sample()<NewLine>&gt;         log_prob = m.log_prob(action)<NewLine>&gt;         return action.data[0], log_prob.data[0]<NewLine>&gt; <NewLine>&gt;     def update(self, batch, optimizer):<NewLine>&gt;         rewards = torch.cat(batch.reward)<NewLine>&gt;         log_probs = torch.cat(batch.log_prob)<NewLine>&gt;         rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)<NewLine>&gt;         optimizer.zero_grad()<NewLine>&gt;         loss = (-log_probs*rewards).sum()<NewLine>&gt;         loss = Variable(torch.Tensor([loss]), requires_grad=True)<NewLine>&gt;         loss.backward()<NewLine>&gt;         optimizer.step()<NewLine>&gt; <NewLine>&gt; def main(args):<NewLine>&gt; <NewLine>&gt;     policy = Policy(4, 128, 2, args.memory_capacity, args.cuda)<NewLine>&gt;     <NewLine>&gt;     if args.cuda:<NewLine>&gt;         policy.cuda()<NewLine>&gt; <NewLine>&gt;     optimizer = optim.Adam(policy.parameters(), lr=1e-2)<NewLine>&gt;     running_reward = 10<NewLine>&gt;     for ep in count(1):<NewLine>&gt;         <NewLine>&gt;         policy.state = []<NewLine>&gt;         policy.action = []<NewLine>&gt;         policy.next_state = []<NewLine>&gt;         policy.reward = []<NewLine>&gt;         policy.log_prob = []<NewLine>&gt; <NewLine>&gt;         state = env.reset()<NewLine>&gt;         state = policy.Tensor([state])<NewLine>&gt;         <NewLine>&gt;         for t in range(10000):<NewLine>&gt;             action, log_prob = policy.select_action(Variable(state))<NewLine>&gt;             next_state, reward, done, _ = env.step(action)<NewLine>&gt;             <NewLine>&gt;             policy.state.append(state)<NewLine>&gt;             policy.action.append(policy.Tensor([action]))<NewLine>&gt;             policy.next_state.append(policy.Tensor([next_state]))<NewLine>&gt;             policy.reward.append(policy.Tensor([reward]))<NewLine>&gt;             policy.log_prob.append(policy.Tensor([log_prob]))<NewLine>&gt;             <NewLine>&gt;             if done:<NewLine>&gt;                 break<NewLine>&gt; <NewLine>&gt;             state = policy.Tensor([next_state])<NewLine>&gt; <NewLine>&gt;         running_reward = running_reward*0.99+t*0.01<NewLine>&gt;         disc_reward = discounted_reward(policy.reward)<NewLine>&gt;         <NewLine>&gt;         for s, a, ns, r, lp in zip(policy.state, policy.action, policy.next_state, disc_reward, policy.log_prob):<NewLine>&gt;             policy.memory.push(s, a, ns, r, lp)<NewLine>&gt;         <NewLine>&gt;         if args.memory_capacity is not None:<NewLine>&gt;             transitions = policy.memory.sample(args.batch_size)<NewLine>&gt;         else:<NewLine>&gt;             transitions = policy.memory.pull()<NewLine>&gt;         batch = Transition(*zip(*transitions))<NewLine>&gt;         policy.update(batch, optimizer)<NewLine>&gt;         <NewLine>&gt;         if args.memory_capacity is None:<NewLine>&gt;             policy.memory.clear()<NewLine>&gt;         <NewLine>&gt;         if ep % args.log_interval == 0:<NewLine>&gt;             print('Episode {}\tLast length: {:5d}\tAverage length: {:.2f}'.format(ep, t, running_reward))<NewLine>&gt;         if running_reward &gt; env.spec.reward_threshold:<NewLine>&gt;             print(""Solved! Running reward is now {} and the last episode runs to {} time steps!"".format(running_reward, t))<NewLine>&gt;             break<NewLine>&gt; <NewLine>&gt; <NewLine>&gt; if __name__ == '__main__':<NewLine>&gt;     main(args)</code></pre><NewLine></div>",https://discuss.pytorch.org/u/seanny1986,(Sean Morrison),seanny1986,"December 27, 2017,  2:55pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have not looked through all of your code, but I’d recommend splitting your <code>main()</code> function into some smaller chunks/subfunctions so it’s easier to debug :).</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I eventually figured it out – originally, my forward method was outputting the FloatTensor, and I was only storing the contents. When backpropagating the loss, I would wrap this in a variable, and use backward(), and the reward would increase, but nowhere near what it should have. I fixed it by returning and storing the variable (rather than the contents) and it worked as expected.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/richard; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/seanny1986; <NewLine> ,"REPLY_DATE 1: December 28, 2017,  3:38am; <NewLine> REPLY_DATE 2: December 30, 2017, 10:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
11111,Example code of recurrent policy gradient?,2017-12-13T13:32:14.896Z,0,2048,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is there an example code for recurrent policy gradient ? Will it be simply replacing MLP with RNN ?</p><NewLine></div>",https://discuss.pytorch.org/u/Xingdong_Zuo,(Xingdong Zuo),Xingdong_Zuo,"December 13, 2017,  1:32pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve got examples of recurrent policy gradients here in newly made repo for a3c continuous action spaces:<br/><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://assets-cdn.github.com/favicon.ico"" width=""32""/><NewLine><a href=""https://github.com/dgriff777/a3c_continuous"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""420"" src=""https://avatars1.githubusercontent.com/u/24666148?s=400&amp;v=4"" width=""420""/><NewLine><h3><a href=""https://github.com/dgriff777/a3c_continuous"" rel=""nofollow noopener"" target=""_blank"">dgriff777/a3c_continuous</a></h3><NewLine><p>a3c_continuous - A continuous action space version of A3C LSTM in pytorch</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>Can also see older discrete action spaces for Atari repo:<br/><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://assets-cdn.github.com/favicon.ico"" width=""32""/><NewLine><a href=""https://github.com/dgriff777/rl_a3c_pytorch"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""420"" src=""https://avatars1.githubusercontent.com/u/24666148?s=400&amp;v=4"" width=""420""/><NewLine><h3><a href=""https://github.com/dgriff777/rl_a3c_pytorch"" rel=""nofollow noopener"" target=""_blank"">dgriff777/rl_a3c_pytorch</a></h3><NewLine><p>rl_a3c_pytorch - Reinforcement learning A3C LSTM Atari with Pytorch</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dgriff; <NewLine> ,"REPLY_DATE 1: December 24, 2017, 12:18am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
7504,How to rewrite REINFORCE without using .reinforce()?,2017-09-17T23:12:46.526Z,2,1585,"<div class=""post"" itemprop=""articleBody""><NewLine><p>According to the formula of REINFORCE, we could define the loss function as log-probability of actions multiplied by the rewards. I am curious if it is to replace <code>action.reinforce(r)</code> as <code>loss = outputs of final layer * rewards</code> ?</p><NewLine></div>",https://discuss.pytorch.org/u/zuoxingdong,(Xingdong Zuo),zuoxingdong,"September 17, 2017, 11:12pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There’s an implementation here without using .reinforce. (<a href=""https://github.com/JamesChuanggg/pytorch-REINFORCE/blob/master/reinforce_discrete.py"" rel=""nofollow noopener"">https://github.com/JamesChuanggg/pytorch-REINFORCE/blob/master/reinforce_discrete.py</a>).</p><NewLine><p>basically, what you wrote is mostly correct, however you need to sample the categorical output (i.e the softmax output from the final output layer) to obtain an action, then you use the action to index your categorical output, and take log, then you can multiply the logprob by reward and sum it up like you wrote. The code snippets I’m talking about above are below:</p><NewLine><pre><code>probs = self.model(Variable(state)) # run net, get categorical probs output<NewLine>action = probs.multinomial().data # sample to get action index<NewLine>prob = probs[:, action[0,0]].view(1, -1) # index probs with action selection<NewLine>log_prob = prob.log() # compute log prob<NewLine><NewLine>ith_step_loss = -log_prob*reward<NewLine></code></pre><NewLine><p>You see the above loss implemented in line 52. Execept in this case they’re summing up loss across the entire trajectory, whereas I’ve written it above for one step action/reward.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can we use the nllloss() function augmented by the reward in this case (if I want to do it in an off-line batch training way)?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, you can use batch of -logprobs*reward in batch update mode, instead of summing up the loss as above. The current Pytorch 0.3 version torch.distribution can compute logprobs for batch of data.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/G-Wang; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/duguyiqiu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/G-Wang; <NewLine> ,"REPLY_DATE 1: September 18, 2017, 12:48am; <NewLine> REPLY_DATE 2: December 2, 2017,  7:07am; <NewLine> REPLY_DATE 3: December 18, 2017,  6:28pm; <NewLine> ",REPLY 1 LIKES: 4 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
11150,Can the timesteps T in deep reinforcement learning be trained?,2017-12-14T07:53:48.912Z,3,346,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Recently I’m trying to implement a deep reinforcement learning project which require a variable timesteps.I want to train a network to output a parameter T,and use T as the length or timesteps of policy gradient method or DQN method,I wonder if that’s implementable? I mean when we do back-propogate, can me back-propogate through timesteps T？</p><NewLine></div>",https://discuss.pytorch.org/u/zoharli,(Zoharli),zoharli,"December 14, 2017,  7:53am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Discrete values don’t have gradients. However, you can either soften it somehow or train with RL. I personally feel the second approach to be more promising.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Basically I think functions like torch.ceil() can handle the discrete problem. But I don’t know if there is a way to use the trainable parameter T to control the action length of DRL method.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, stepwise functions are never solution to such problems. ceil will prevent gradient flowing unless the input is exactly an integer.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SimonW; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/zoharli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/SimonW; <NewLine> ,"REPLY_DATE 1: December 14, 2017,  8:12am; <NewLine> REPLY_DATE 2: December 14, 2017,  8:27am; <NewLine> REPLY_DATE 3: December 14, 2017,  3:41pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
11106,RL for multi-joint robotic arm,2017-12-13T10:53:41.002Z,0,748,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I’ve been working on a RL setting to control a multi joint robotic arm. It is a simple setup, with a n-joints robot and a target to be reached.</p><NewLine><p>I’ve made various implementations, namely PG and DQN but so far, I’ve been met with cruel failure.<br/><NewLine>I have troubles figuring out the root of the problem because:</p><NewLine><ul><NewLine><li>The same implementation seems to work with gym environements (cartpole especially)</li><NewLine><li>I have very strange success rate shape, even with long training</li><NewLine><li>I have also tried various reward functions, some more guiding than other but to no avail</li><NewLine></ul><NewLine><p>On the figures, the x-axis values should be multiplied by x100 to obtain the real value</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/d1212ae79a2aba542de8e2bf29563fb6ef3d0496"" href=""https://discuss.pytorch.org/uploads/default/original/2X/d/d1212ae79a2aba542de8e2bf29563fb6ef3d0496.png"" title=""Figure.png""><img alt=""Figure"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/d/d1212ae79a2aba542de8e2bf29563fb6ef3d0496_2_10x10.png"" height=""258"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/d/d1212ae79a2aba542de8e2bf29563fb6ef3d0496_2_690x258.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/d/d1212ae79a2aba542de8e2bf29563fb6ef3d0496_2_690x258.png, https://discuss.pytorch.org/uploads/default/optimized/2X/d/d1212ae79a2aba542de8e2bf29563fb6ef3d0496_2_1035x387.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/d/d1212ae79a2aba542de8e2bf29563fb6ef3d0496.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Figure.png</span><span class=""informations"">1280×480 33.2 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>I’d be very grateful if somebody could help me figure out the problem. The following link to my repo: <a href=""https://github.com/Mehd6384/RL"" rel=""nofollow noopener"">https://github.com/Mehd6384/RL</a> has the guilty files and some explanations.<br/><NewLine>Thanks a lot !</p><NewLine></div>",https://discuss.pytorch.org/u/Mehdi,,Mehdi,"December 13, 2017, 10:53am",,,,,
10324,Implementing Sarsa weight updates,2017-11-24T00:05:12.009Z,1,1129,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have the following equation:</p><NewLine><pre><code class=""lang-auto"">w &lt;- w + a[R + gamma * q(S', A', w) - q(S, A, w)] * gradient of q(S, A, w)<NewLine></code></pre><NewLine><p>My code:</p><NewLine><pre><code class=""lang-python"">import gym<NewLine>import numpy as np<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine>from torch.autograd import backward, Variable<NewLine><NewLine>class Policy(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Policy, self).__init__()<NewLine>        self.fc1 = nn.Linear(5, 128)<NewLine>        self.fc2 = nn.Linear(128, 1)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = F.tanh(self.fc1(x))<NewLine>        return F.sigmoid(self.fc2(x))<NewLine><NewLine>env = gym.make('CartPole-v0')<NewLine>policy = Policy()<NewLine>optimizer = optim.Adam(policy.parameters())<NewLine><NewLine>s = env.reset()<NewLine>a = 0<NewLine><NewLine>s_next, reward, done, _ = env.step(a)<NewLine>a_next = 1<NewLine><NewLine>x = torch.from_numpy(np.append(s, a)).float().unsqueeze(0)<NewLine>x = Variable(x)<NewLine>q_sa = policy(x)<NewLine><NewLine>x_next= torch.from_numpy(np.append(s_next, a_next)).float().unsqueeze(0)<NewLine>x_next = Variable(x_next)<NewLine>q_sa_next = policy(x_next)<NewLine><NewLine>alpha = 1<NewLine>gamma = 1<NewLine>update = alpha * (reward + gamma * q_sa_next - q_sa)<NewLine>optimizer.zero_grad()<NewLine>update.backward()<NewLine>optimizer.step()<NewLine></code></pre><NewLine><p>Am I implementing <code>update</code> and propagating things correctly? I don’t quite understand how to handle the gradient part of the equation.</p><NewLine></div>",https://discuss.pytorch.org/u/dteoh,(Douglas Teoh),dteoh,"November 24, 2017, 12:05am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>q_sa_next</code> should have gradients stripped. ie call <code>Variable(q_sa_next.data)</code> or <code>q_sa_next.detach()</code>.</p><NewLine><p>if it was me, I’d simply call <code>.data</code> on it, to strip out the gradients. And then add <code>Variable</code> into the <code>update</code> line:</p><NewLine><pre><code>update = Variable(alpha * (reward + gamma * q_sa_next) - q_sa<NewLine></code></pre><NewLine><p>I never quite figured out how to backpropagate <code>update</code> directly though, so I do it like this:</p><NewLine><pre><code>crit = nn.MSELoss()<NewLine>loss = crit(q_sa, Variable(reward + gamma * q_sa_next))<NewLine>loss.backward()<NewLine></code></pre><NewLine><p>(Edit: I <em>guess</em> that to backpropagate <code>update</code>, we’d do something like <code>q_sa.backward(update)</code>. But as I say I just lazily do it using crit for now, and never tested this approach)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much, I’ve got a much better understanding now.</p><NewLine><p>The only thing I’m doing slightly different to try to reproduce the formula is to use <code>nn.L1Loss(size_average=False)</code>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Interesting. Thanks! <img alt="":slight_smile:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=5"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hmmm, so, you got me worried that I was using the wrong loss function <img alt="":slight_smile:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=5"" title="":slight_smile:""/></p><NewLine><p>So, I worked through it a bit <a href=""https://github.com/hughperkins/pub-prototyping/blob/master/maths/sarsa_updates.ipynb"" rel=""nofollow noopener"">https://github.com/hughperkins/pub-prototyping/blob/master/maths/sarsa_updates.ipynb</a></p><NewLine><p>… and I’m fairly sure of two things:</p><NewLine><ol><NewLine><li>should be MSE loss. Though there might be some issue with my reasoning above?, and</li><NewLine><li>to directly backprop <code>delta</code> without needing a loss function, we can do simply:</li><NewLine></ol><NewLine><p><code>q.backward(-delta)</code></p><NewLine><p>Thoughts?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Okay, I misunderstood, I did not consider the mathematical justification.</p><NewLine><p>I just thought that <code>R + gamma * q(S', A', w) - q(S, A, w)</code> is about moving the weights of <code>q(S, A, w)</code> towards <code>R + gamma * q(S', A', w)</code>, and that implementing it with any loss function that encoded this difference would do a similar thing.</p><NewLine><p>I don’t quite follow how:</p><NewLine><pre><code class=""lang-auto"">(1/2) * Q_w^2 - rQ_w - yQ'_w + C<NewLine></code></pre><NewLine><p>can turn into:</p><NewLine><pre><code class=""lang-auto"">(1/2) * (Q_w - (r + yQ'_w))^2 + C<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hughperkins; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dteoh; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/hughperkins; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/hughperkins; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/dteoh; <NewLine> ,"REPLY_DATE 1: November 25, 2017,  3:24pm; <NewLine> REPLY_DATE 2: November 27, 2017, 12:19am; <NewLine> REPLY_DATE 3: November 27, 2017, 12:34am; <NewLine> REPLY_DATE 4: November 27, 2017,  3:10am; <NewLine> REPLY_DATE 5: November 27, 2017,  3:42am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
2616,Experience replay for REINFORCE,2017-05-03T19:09:49.686Z,1,1095,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How can I implement experience replay for <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""nofollow noopener"">REINFORCE</a>  ?<br/><NewLine>I have an LSTM which after getting an input, outputs a series of actions (I use reinforce like in the link above to sample an action and give it a reward) but training it in an online fashion doesn’t seem to work well enough.<br/><NewLine>So is there some way I can use experience replay here ?</p><NewLine></div>",https://discuss.pytorch.org/u/Rinku_Jadhav2014,(Rinku Jadhav),Rinku_Jadhav2014,"May 3, 2017,  7:09pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Usually policy gradient methosd like reinforce are on-policy method which can not be updated from experience replay.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Your answer seems to suggest that on-policy methods do not use experience replay. Could you clarify?<br/><NewLine>Suppose I replace deep q-learning with SARSA (which is on-policy) and use experience replay then wouldn’t it be on-policy experience replay?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ruotianluo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Adi_R; <NewLine> ,"REPLY_DATE 1: May 4, 2017,  6:39am; <NewLine> REPLY_DATE 2: November 23, 2017,  9:03pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> 
10022,Different result between update via action.reinforce(reward) and direct BP,2017-11-16T09:34:16.212Z,0,683,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi I am working with PyTorch 0.2.0_3. I want to use direct back propagation rather than action.reinforce(reward) as it is more flexible. To be careful, I tried to compare the difference between two ways of update. I initialize an actor-critic model that will be updated via .reinforce and a deep copy of it that will be updated via direct BP. The action is chosen by the first model and the loss for the second model is calculated based on the corresponding action.</p><NewLine><p>I compare the difference of updates by calculating the l1 norm of the difference between the parameters of the two models, and the difference is not zero. Furthermore, the difference grows as the number of updates increases. I am wondering if I am not doing the comparison properly or if this can be a concern. Thank you.</p><NewLine><p>The following code is based on the actor-critic example provided in <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py"" rel=""nofollow noopener"">pytorch/examples</a>.</p><NewLine><pre><code>import argparse<NewLine>import copy<NewLine>import gym<NewLine>import numpy as np<NewLine>from itertools import count<NewLine>from collections import namedtuple<NewLine><NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine>import torch.autograd as autograd<NewLine>from torch.autograd import Variable<NewLine><NewLine><NewLine>parser = argparse.ArgumentParser(description='PyTorch actor-critic example')<NewLine>parser.add_argument('--gamma', type=float, default=0.99, metavar='G',<NewLine>                    help='discount factor (default: 0.99)')<NewLine>parser.add_argument('--seed', type=int, default=543, metavar='N',<NewLine>                    help='random seed (default: 1)')<NewLine>parser.add_argument('--check-interval', type=int, default=1, metavar='N',<NewLine>                    help='interval between checks of different ways of update')<NewLine>parser.add_argument('--check-times', type=int, default=10, metavar='N',<NewLine>                    help='total times to check the difference between two ways of update')<NewLine>args = parser.parse_args()<NewLine><NewLine><NewLine>env = gym.make('CartPole-v0')<NewLine>env.seed(args.seed)<NewLine>torch.manual_seed(args.seed) <NewLine><NewLine><NewLine>SavedInfo = namedtuple('SavedInfo', ['action', 'value', 'log_prob'])<NewLine><NewLine><NewLine>class Policy(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Policy, self).__init__()<NewLine>        self.affine1 = nn.Linear(4, 128)<NewLine>        self.action_head = nn.Linear(128, 2)<NewLine>        self.value_head = nn.Linear(128, 1)<NewLine><NewLine>        self.saved_info = []<NewLine>        self.rewards = []<NewLine><NewLine>    def forward(self, x):<NewLine>        x = F.relu(self.affine1(x))<NewLine>        action_scores = self.action_head(x)<NewLine>        state_values = self.value_head(x)<NewLine><NewLine>        return F.softmax(action_scores), state_values<NewLine><NewLine>    def select_action(self, state_):<NewLine>        state_ = torch.from_numpy(state_).float().unsqueeze(0)<NewLine>        probs, state_value = self.forward(Variable(state_, requires_grad=False))<NewLine>        action = probs.multinomial()<NewLine><NewLine>        return action, state_value, probs<NewLine><NewLine><NewLine>policy_reinforce = Policy()<NewLine>optimizer_reinforce = optim.Adam(policy_reinforce.parameters(), lr=3e-2)<NewLine><NewLine>policy_bp = copy.deepcopy(policy_reinforce)<NewLine>optimizer_bp = optim.Adam(policy_bp.parameters(), lr=3e-2)<NewLine><NewLine><NewLine>def finish_episode():<NewLine>    # r for values related to the model being updated via reinforce<NewLine>    # b for values related to the model being updated via direct BP<NewLine>    saved_info_r = policy_reinforce.saved_info<NewLine>    value_loss_r = 0<NewLine>    saved_info_b = policy_bp.saved_info<NewLine>    value_loss_b = 0<NewLine>    policy_loss_b = 0<NewLine><NewLine>    R = 0<NewLine>    cum_returns = []<NewLine><NewLine>    for r in policy_reinforce.rewards[::-1]:<NewLine>        R = r + args.gamma * R<NewLine>        cum_returns.insert(0, R)<NewLine><NewLine>    cum_returns = torch.Tensor(cum_returns)<NewLine>    cum_returns = (cum_returns - cum_returns.mean()) / (cum_returns.std() + np.finfo(np.float32).eps)<NewLine><NewLine>    for (action_r, value_r, log_prob_r), R in zip(saved_info_r, cum_returns):<NewLine>        adv_r = R - value_r.data[0, 0]<NewLine>        action_r.reinforce(adv_r)<NewLine>        value_loss_r += F.smooth_l1_loss(value_r, Variable(torch.Tensor([R])))<NewLine><NewLine>    optimizer_reinforce.zero_grad()<NewLine>    final_nodes = [value_loss_r] + list(map(lambda p: p.action, saved_info_r))<NewLine>    gradients = [torch.ones(1)] + [None] * len(saved_info_r)<NewLine>    autograd.backward(final_nodes, gradients)<NewLine>    optimizer_reinforce.step()<NewLine><NewLine>    for (_, value_b, log_prob_b), R in zip(saved_info_b, cum_returns):<NewLine>        adv_b = R - value_b.data[0, 0]<NewLine>        policy_loss_b -= log_prob_b * adv_b<NewLine>        value_loss_b += F.smooth_l1_loss(value_b, Variable(torch.Tensor([R])))<NewLine><NewLine>    optimizer_bp.zero_grad()<NewLine>    total_loss_b = policy_loss_b + value_loss_b<NewLine>    total_loss_b.backward()<NewLine>    optimizer_bp.step()<NewLine><NewLine>    del policy_reinforce.rewards[:]<NewLine>    del policy_reinforce.saved_info[:]<NewLine>    del policy_bp.rewards[:]<NewLine>    del policy_bp.saved_info[:]<NewLine><NewLine><NewLine>def check_difference():<NewLine>    reinforce_parameters = list(policy_reinforce.parameters())<NewLine>    bp_parameters = list(policy_bp.parameters())<NewLine>    difference = 0<NewLine><NewLine>    for i in range(len(reinforce_parameters)):<NewLine>        difference += (reinforce_parameters[i] - bp_parameters[i]).norm(1).data[0]<NewLine><NewLine>    return difference<NewLine><NewLine><NewLine>running_reward = 10<NewLine>check_done = 0<NewLine>differences = []<NewLine>for i_episode in count(1):<NewLine>    state = env.reset()<NewLine>    done = False<NewLine><NewLine>    for t in range(10000): # Don't infinite loop while learning<NewLine>        action_r, state_value_r, probs_r = policy_reinforce.select_action(state)<NewLine>        policy_reinforce.saved_info.append(SavedInfo(action_r, state_value_r,<NewLine>                                                     torch.log(probs_r.gather(1, Variable(action_r.data)))))<NewLine><NewLine>        _, state_value_b, probs_b = policy_bp.select_action(state)<NewLine>        policy_bp.saved_info.append(SavedInfo(None, state_value_b,<NewLine>                                              torch.log(probs_b.gather(1, Variable(action_r.data)))))<NewLine><NewLine>        state, reward, done, _ = env.step(action_r.data[0, 0])<NewLine><NewLine>        policy_reinforce.rewards.append(reward)<NewLine>        policy_bp.rewards.append(reward)<NewLine><NewLine>        if done:<NewLine>            break<NewLine><NewLine>    running_reward = running_reward * 0.99 + t * 0.01<NewLine>    finish_episode()<NewLine><NewLine>    if i_episode % args.check_interval == 0:<NewLine>        check_done += 1<NewLine>        result = check_difference()<NewLine>        differences.append(result)<NewLine><NewLine>    if check_done == args.check_times:<NewLine>        print(differences)<NewLine>        break</code></pre><NewLine></div>",https://discuss.pytorch.org/u/mufeili,(Mufei Li),mufeili,"November 16, 2017,  1:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think I have figured it out what’s going on. According to <a href=""https://github.com/pytorch/pytorch/blob/v0.2.0/torch/autograd/_functions/stochastic.py#L31"" rel=""nofollow noopener"">backward function for multinomial</a>, they used -r/(1e-6+p) directly in the backward process.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mufeili; <NewLine> ,"REPLY_DATE 1: July 3, 2018,  7:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
10108,Should I call backward() separately on each stochastic node?,2017-11-18T14:27:29.870Z,0,601,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a model which has a single sample from a multinomial pretty far upstream. Other than that it’s a standard supervised learning model, resulting in a single loss value at the end.</p><NewLine><p>I’ve got my head around the reinforce function. I now do the following:</p><NewLine><ul><NewLine><li>Compute a loss vector L over the batch (i.e. the batch loss, but not summed)</li><NewLine><li>Pass the negative of this loss to reinforce() on the Variable returned by torch.multinomial().</li><NewLine><li>Call backward() on the sum of the loss.</li><NewLine><li>Call optimizer.step()</li><NewLine></ul><NewLine><p>Doing this, the gradient over the parameters of the multinomial is None. Only if I call backward() also on the output of torch.multinomial() (after calling reinforce()) do I get a gradient. Is this the correct approach, or am I misunderstanding something?</p><NewLine></div>",https://discuss.pytorch.org/u/pbloem,(Peter Bloem),pbloem,"November 18, 2017,  2:27pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thats correct. but personally, I just do the calcs myself, since:</p><NewLine><ol><NewLine><li>then I dont have to think about this so much: it resembles REINFORCE more</li><NewLine><li>in next torch version, the api is very similar to doing it by hand, so migraiton will be easier</li><NewLine><li>makes it easier for non-pytorch people to read too</li><NewLine></ol><NewLine><p>The calcs by hand, compared with reinforce, is something like:</p><NewLine><pre><code>import torch<NewLine>from torch import autograd, nn, optim<NewLine>import torch.nn.functional as F<NewLine>from torch.autograd import Variable<NewLine>import numpy as np<NewLine><NewLine><NewLine>def run_model(x, h1):<NewLine>    torch.manual_seed(123)<NewLine>    params = list(h1.parameters())<NewLine><NewLine>    logits = h1(Variable(x))<NewLine>    probs = F.softmax(logits)<NewLine>    a = torch.multinomial(probs)<NewLine>    print('a', a)<NewLine>    return logits, probs, a<NewLine><NewLine><NewLine>def get_r():<NewLine>    torch.manual_seed(123)<NewLine>    r = torch.rand(4, 1)<NewLine>    return r<NewLine><NewLine><NewLine>def run_by_hand(params, x, h1):<NewLine>    print('')<NewLine>    print('=======')<NewLine>    print('by hand')<NewLine>    h1.zero_grad()<NewLine><NewLine>    logits, probs, a = run_model(x, h1)<NewLine>    g = torch.gather(probs, 1, Variable(a.data))<NewLine>    log_g = g.log()<NewLine>    a = a.data<NewLine><NewLine>    r = get_r()<NewLine>    r_loss = - (log_g * Variable(r)).sum()<NewLine>    r_loss.backward()<NewLine>    print('params.grad', params.grad)<NewLine><NewLine><NewLine>def run_pytorch_reinforce(params, x, h1):<NewLine>    print('')<NewLine>    print('=======')<NewLine>    print('pytorch reinforce')<NewLine>    h1.zero_grad()<NewLine>    x1, x2, a = run_model(x, h1)<NewLine>    r = get_r()<NewLine>    a.reinforce(r)<NewLine>    autograd.backward([a], [None])<NewLine>    print('params.grad', params.grad)<NewLine><NewLine><NewLine>def run():<NewLine>    N = 4<NewLine>    K = 1<NewLine>    C = 3<NewLine><NewLine>    torch.manual_seed(123)<NewLine>    x = torch.ones(N, K)<NewLine>    h1 = nn.Linear(K, C, bias=False)<NewLine>    params = list(h1.parameters())[0]<NewLine>    run_by_hand(params, x, h1)<NewLine>    run_pytorch_reinforce(params, x, h1)<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    run()</code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hughperkins; <NewLine> ,"REPLY_DATE 1: November 18, 2017,  2:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
9920,Unsure about input type of convulution layer,2017-11-14T11:49:42.303Z,1,2676,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there! I am studying convolution networks with reinforcemente learning to tackle some simple games. I already developed a simple network comprising only full connected hidden layers. The network for that task was:</p><NewLine><p>class NeuralNetworkLinear(nn.Module):</p><NewLine><pre><code>def __init__(self,input_size,output_size):<NewLine>    super(NeuralNetworkLinear, self).__init__()<NewLine>    n=200<NewLine>    self.fc1 = nn.Linear(input_size, n)<NewLine>    self.fc2 = nn.Linear(n, n)<NewLine>    self.fc3 = nn.Linear(n, output_size)<NewLine><NewLine>def forward(self, state):<NewLine>    x = F.relu(self.fc1(state))<NewLine>    x = F.relu(self.fc2(x))<NewLine>    q_values = self.fc3(x)<NewLine>    return q_values<NewLine></code></pre><NewLine><p>To make a forward pass I would do something like:</p><NewLine><pre><code>def select_action(self, state):<NewLine>    state = torch.Tensor(state).unsqueeze(0)<NewLine>    board=Variable(state)<NewLine>    probs = F.softmax(self.model(board)*self.temperature)<NewLine>    action = probs.multinomial()<NewLine></code></pre><NewLine><p>where state is a numpy 1D array comprising the X and Y position of some Bot and its target.</p><NewLine><p>Now I want to change my approach. I want to input the 2D grid to a convolution network. This grid is a numpy 2D array with values between 0 and 255, or 0 and 1, representing a “gray-scale” representation of the game world with 10x10 size. I copied this template convolution network from the pytorch tutorials:</p><NewLine><p>class CNN(nn.Module):</p><NewLine><pre><code>def __init__(self,input_size,output_size):<NewLine>    super(CNN, self).__init__()<NewLine>    self.conv1 = nn.Conv2d(1, 16, kernel_size=2, stride=1)<NewLine>    self.bn1 = nn.BatchNorm2d(16)<NewLine>    self.conv2 = nn.Conv2d(16, 32, kernel_size=2, stride=1)<NewLine>    self.bn2 = nn.BatchNorm2d(32)<NewLine>    self.conv3 = nn.Conv2d(32, 32, kernel_size=2, stride=1)<NewLine>    self.bn3 = nn.BatchNorm2d(32)<NewLine>    self.head = nn.Linear(448, 4)<NewLine><NewLine>def forward(self, state):<NewLine>    x = F.relu(self.bn1(self.conv1(state)))<NewLine>    x = F.relu(self.bn2(self.conv2(x)))<NewLine>    x = F.relu(self.bn3(self.conv3(x)))<NewLine>    return self.head(x.view(x.size(0), -1))<NewLine></code></pre><NewLine><p>But now, when i use this new network, i get the following error:</p><NewLine><p>“Expected 4D tensor as input, got 2D tensor instead.”. This error occur in “x = F.relu(self.bn1(self.conv1(state)))”</p><NewLine><p>I don’t really know how to apprach this. I just want to convert my numpy 2D array to the right input to the convolution network. Can someone help me with this?</p><NewLine><p>Thank you in advance <img alt="":wink:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/wink.png?v=5"" title="":wink:""/></p><NewLine></div>",https://discuss.pytorch.org/u/Francisco_Ferreira,(Francisco Ferreira),Francisco_Ferreira,"November 14, 2017, 11:49am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Convolutional neural networks expect 4D input where the inputs are:</p><NewLine><p>image_# x n_channel x width x height</p><NewLine><p>So if you want to input 1 grey-scale image you would have to reshape your input image to 1x1x10x10. You can use <code>.view()</code> on a torch tensor to reshape:</p><NewLine><pre><code>a = torch.randn((10, 10))<NewLine>print(a.size()) # torch.Size([10, 10])<NewLine><NewLine>a = a.view((1, 1, 10, 10))<NewLine>print(a.size()) # torch.Size([1, 1, 10, 10])<NewLine></code></pre><NewLine><p>Now the model should input the image without error.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <img alt="":wink:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/wink.png?v=5"" title="":wink:""/> that solved it</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/BartolomeD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Francisco_Ferreira; <NewLine> ,"REPLY_DATE 1: November 14, 2017, 12:55pm; <NewLine> REPLY_DATE 2: November 14, 2017,  2:23pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
9325,Reinforce deprecated?,2017-10-31T14:56:53.894Z,1,1356,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve being using action.reinforce(reward) for policy gradient based training, but it seems like there’s been a change recently and I get an error stating:</p><NewLine><p>File “/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/torch/autograd/variable.py”, line 209, in reinforce<br/><NewLine>if not isinstance(self.grad_fn, StochasticFunction):<br/><NewLine>NameError: name ‘StochasticFunction’ is not defined</p><NewLine><p>I read <a href=""https://github.com/pytorch/pytorch/issues/3340"" rel=""nofollow noopener"">on github</a> that .reinforce is being deprecated, and it’s suggested to use torch.distributions.</p><NewLine><p>Is there a reason for this change? Reinforce seemed relatively simple and intuitive. It’ll be great if the <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""nofollow noopener"">reinforce example from pytorch</a> is updated to reflect this change.</p><NewLine></div>",https://discuss.pytorch.org/u/notarobot,,notarobot,"October 31, 2017,  2:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://github.com/pytorch/pytorch/pull/3165"" rel=""nofollow noopener"">Here’s a good thread</a> on the reason for the change. I think it can be summarized in two points: support for multiple stochastic outputs is difficult, and improving performance with Variables.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you are on the 0.2 release, <code>reinforce</code> is still available. If you’re on <code>master</code> and have <code>torch.distributions</code> instead, the RL examples should now be as follows: <a href=""https://github.com/pytorch/examples/pull/249"" rel=""nofollow noopener"">https://github.com/pytorch/examples/pull/249</a></p><NewLine><p><code>torch.distributions</code> is much more general and suitable for a larger range of tasks - building the equivalent of <code>reinforce</code> using this is relatively simple (and arguably cleaner as it can be used to create a normal loss function to backpropagate).</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>This helps, thanks a lot!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/richard; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Kaixhin; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/notarobot; <NewLine> ,"REPLY_DATE 1: October 31, 2017,  5:51pm; <NewLine> REPLY_DATE 2: November 14, 2017, 10:59pm; <NewLine> REPLY_DATE 3: November 13, 2017,  8:23pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 3 Likes; <NewLine> REPLY 3 LIKES: ; <NewLine> 
9243,My REINFORCE model is not learning,2017-10-30T04:38:44.897Z,3,1278,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve been reading through “Hands-On Machine Learning with Scikit-Learn &amp; Tensorflow” and implementing the algorithms presented in the book using PyTorch instead of Tensorflow. (I’m a beginner and my PhD friends have recommended Pytorch to me!)</p><NewLine><p>I have been working on this algorithm (<a href=""https://github.com/ageron/handson-ml/blob/master/16_reinforcement_learning.ipynb"" rel=""nofollow noopener"">link</a>, go to “Policy Gradients”), but my implementation in Pytorch is simply not learning at all. Here’s the <a href=""https://nbviewer.jupyter.org/gist/yukw777/dfe7a60b3cc7317125913a6e4c725651"" rel=""nofollow noopener"">code</a>. Could anyone point out to me what I’m doing wrong? My hunch is that I’m missing logarithms somewhere, but not too sure.</p><NewLine></div>",https://discuss.pytorch.org/u/yukw777,,yukw777,"October 30, 2017,  4:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I tried to apply same concept in pong game but by directly scaling the gradients way. It does not learn.<br/><NewLine>Please check this: <a href=""https://discuss.pytorch.org/t/implementing-reinforce-using-gradient-scaling/9424"">Implementing reinforce using gradient scaling</a><br/><NewLine><a href=""https://gist.github.com/pj-parag/fce77fe977263bb9360d257c332fe44b"" rel=""nofollow noopener"">my implementation</a></p><NewLine><p>Thanks</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>pytorch lets you call <code>.reinforce()</code> directly on the output of the random variable. You need to sample the random variable using <code>torch.multinomial</code> rather than <code>np.random.multinomial</code>. You can see how this works at <a href=""https://discuss.pytorch.org/t/what-is-action-reinforce-r-doing-actually/1294/18?u=hughperkins"">What is action.reinforce(r) doing actually?</a> for example.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I actually got it working with <code>.reinforce()</code>, but then it’s calculating the gradient and updating the parameters every game right? The algorithm from the book plays the game 10 times, calculating the gradient for each, then calculates the mean of the gradients from those 10 games to update the parameters. Was wondering how I could do that with pytorch.</p><NewLine><p>I guess I could just use <code>.reinforce()</code> but I thought trying to implement the algorithm from the book in pytorch would be good practice.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/yukw777"">@yukw777</a>  I’d recommend upgrading to <code>master</code> which does reinforce differently, and in my opinion more closelhy resembles how REINFORCE paper presents the algorithm.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>For people looking around, this is the change <a class=""mention"" href=""/u/hughperkins"">@hughperkins</a> is referring to:<br/><NewLine><aside class=""onebox githubpullrequest""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/3165#issuecomment-341853173"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://github.com/colesbury"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""colesbury"" class=""thumbnail onebox-avatar"" height=""90"" src=""https://avatars1.githubusercontent.com/u/655866?v=4"" width=""90""/><NewLine></a><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/pull/3165"" rel=""nofollow noopener"" target=""_blank"">Replace StochasticFunctions v2</a><NewLine></h4><NewLine><div class=""date""><NewLine>  by <a href=""https://github.com/colesbury"" rel=""nofollow noopener"" target=""_blank"">colesbury</a><NewLine>  on <a href=""https://github.com/pytorch/pytorch/pull/3165"" rel=""nofollow noopener"" target=""_blank"">08:30PM - 18 Oct 17</a><NewLine></div><NewLine><div class=""github-commit-stats""><NewLine><strong>3 commits</strong><NewLine>  changed <strong>9 files</strong><NewLine>  with <strong>292 additions</strong><NewLine>  and <strong>191 deletions</strong>.<NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>And the doc on master branch reflects that: <a href=""http://pytorch.org/docs/master/distributions.html"" rel=""nofollow noopener"">http://pytorch.org/docs/master/distributions.html</a></p><NewLine><p>I definitely agree that this is a lot easier to understand as it is close to what’s described in the REINFORCE paper. Since the pytorch binary is still on 0.2.0 and doesn’t include this change, I guess we have to install pytorch from source for now.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/norm; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hughperkins; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/yukw777; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/hughperkins; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/yukw777; <NewLine> ,"REPLY_DATE 1: November 3, 2017,  4:43am; <NewLine> REPLY_DATE 2: November 3, 2017,  6:07am; <NewLine> REPLY_DATE 3: November 9, 2017,  5:01am; <NewLine> REPLY_DATE 4: November 11, 2017, 12:07pm; <NewLine> REPLY_DATE 5: November 12, 2017,  1:39am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
7341,vy007vikas/PyTorch-ActorCriticRL,2017-09-13T04:41:47.157Z,0,1435,"<div class=""post"" itemprop=""articleBody""><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://assets-cdn.github.com/favicon.ico"" width=""32""/><NewLine><a href=""https://github.com/vy007vikas/PyTorch-ActorCriticRL"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""400"" src=""https://avatars0.githubusercontent.com/u/5688236?s=400&amp;v=4"" width=""400""/><NewLine><h3><a href=""https://github.com/vy007vikas/PyTorch-ActorCriticRL"" rel=""nofollow noopener"" target=""_blank"">vy007vikas/PyTorch-ActorCriticRL</a></h3><NewLine><p>PyTorch-ActorCriticRL - PyTorch implementation of DDPG algorithm for continuous action reinforcement learning problem.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Hello everyone,<br/><NewLine>I have implemented DDPG in PyTorch for continuous action reinforcement learning and tested it on OpenAI envs like Pendulum-v0 and BiPedal Walker - v2. Have a look and feel free to ask any doubts, if you have. Hope it helps anyone who is stuck at these problems.</p><NewLine><p>Thanks,<br/><NewLine>Vikas Yadav</p><NewLine></div>",https://discuss.pytorch.org/u/vy007vikas,(Vikas Yadav),vy007vikas,"September 13, 2017,  4:41am",3 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey there.  Just tried running your software.  Hit an error right out of the gate.  What versions of Python and PyTorch did you use?</p><NewLine><p>Thanks.</p><NewLine><ul><NewLine><li>dan</li><NewLine></ul><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello.  I got it to work.  Let me know what version of PyTorch you wrote this for.  If my fixes were for a newer version, I’ll send you a pull request.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/danelliottster; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/danelliottster; <NewLine> ,"REPLY_DATE 1: November 10, 2017,  7:58pm; <NewLine> REPLY_DATE 2: November 10, 2017,  8:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
9640,Problems run multiple DQN models together,2017-11-08T08:16:54.814Z,0,246,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve been trying to implement multiple DQN models together. So I first defined my network like this,</p><NewLine><pre><code>class atari_model(nn.Module):<NewLine>    def __init__(self, in_channels=12, num_actions=18):<NewLine>        super(atari_model, self).__init__()<NewLine>        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)<NewLine>        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)<NewLine>        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)<NewLine>        self.fc4 = nn.Linear(7 * 7 * 64, 512)<NewLine>        self.fc5 = nn.Linear(512, num_actions)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = F.relu(self.conv1(x))<NewLine>        x = F.relu(self.conv2(x))<NewLine>        x = F.relu(self.conv3(x))<NewLine>        x = F.relu(self.fc4(x.view(x.size(0), -1)))<NewLine>        return self.fc5(x)<NewLine></code></pre><NewLine><p>then, I constructed a list of models,</p><NewLine><pre><code>models = []<NewLine>for i in range(8):<NewLine>    model = atari_model(12, 18)<NewLine>    model.cuda()<NewLine>    models.append(model)<NewLine>    optimizer = optim.Adam(model.parameters(), lr=0.001)<NewLine></code></pre><NewLine><p>then I collected some experiences in the environment and try to perform some updates using these models, and their target models. However, this does not work even if I just have one model, but instead the model is in a list. Does put the model in a list and then use it really cause any problems? If this causes problems, how to create any number of models by specifying the number elsewhere?</p><NewLine></div>",https://discuss.pytorch.org/u/Peter_Ham,(Peter Ham),Peter_Ham,"November 8, 2017,  8:24am",,,,,
1294,What is action.reinforce(r) doing actually?,2017-03-23T12:53:42.876Z,12,8665,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am studying RL with <b>reinforcement/reinforce.py</b> in <b>pytorch/examples</b>. I have some questions about it.</p><NewLine><ol><NewLine><li><NewLine><p>What does <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py#L68"" rel=""nofollow noopener"">action.reinforce®</a> internally do ?</p><NewLine></li><NewLine><li><NewLine><p>Below is <b>REINFORCE</b> update rule where v_t is a return. We need to do gradient “ascent” as below but if we use <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py#L71"" rel=""nofollow noopener"">optimizer.step</a>, it is gradient “descent”. Is [action.reinforce®]((<a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""nofollow noopener"">https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py</a>) multiplying log probability by <code>-r</code>? Then, it makes sense.<br/><NewLine><img height=""55"" src=""https://discuss.pytorch.org/uploads/default/original/1X/f4ecb1b345823f965cb555139cab35a65d5be478.png"" width=""454""/></p><NewLine></li><NewLine><li><NewLine><p>In <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py#L70"" rel=""nofollow noopener"">autograd.backward(model.saved_actions, [None for _ in model.saved_actions])</a>, what is the role of <code>None</code> here?</p><NewLine></li><NewLine></ol><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/yunjey,(Yunjey),yunjey,"March 23, 2017, 12:55pm",12 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>It finds the <code>.creator</code> of the output and calls <a href=""https://github.com/pytorch/pytorch/blob/master/torch/autograd/stochastic_function.py#L32-L44"">this method</a>. Basically, it just saves the reward in the <code>.reward</code> attribute of the creator function. Then, when the <code>backward</code> method is called, the <code>StochasticFunction</code> class will discard the <code>grad_output</code> it received and <a href=""https://github.com/pytorch/pytorch/blob/master/torch/autograd/stochastic_function.py#L17"">pass the saved reward</a> to the <code>backward</code> method.</li><NewLine><li>Yes, the gradient formulas are written in such a way that they negate the reward. You might not find <code>reward.neg()</code> there because they might have been slightly rewritten, but it’s still a gradient to be used with a descent.</li><NewLine><li>You need to give autograd the first list, so that it can discover all the stochastic nodes you want to optimize. The second list can look like that because the stochastic functions don’t need any gradients (they’ll discard them anyway), so you can give them None.</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/apaszke"">@apaszke</a> Thanks very much!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If I got it correctly, for Vanilla Policy Gradient which updates only once for each trajectory rather than for each step in REINFORCE, we have a sum up the log-probability of policy multiplied by advantage estimate for each time step, then compute a single objective of type <code>Variable</code> with calling <code>.backward()</code> to update the parameters ?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, you just somehow compute the total trajectory reward (probably by decaying the older rewards) and pass that to <code>.reward</code>. That’s all you have to do.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>In REINFORCE to reduce the variance one usually averages the loss over a number of sampled trajectories (batch_size). Do I understand correctly that in such a case I need to <code>action.reinforce(reward / batch_size)</code> for every action in each trajectory?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>action</code> and <code>reward</code> should both be tensors with <code>batch_size</code> as their first dimension</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is understandable but it does not explicitly answer my question: does <code>actions.reinforce(rewards)</code> correspond to the average or sum of errors (over the batch)?</p><NewLine><p>As a side note: I really like how the code simplifies with <code>reinforce</code> but even after reading all available information about it I still feel uncomfortable using it.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think I see what you’re asking now. The gradient estimates from all the examples in the batch are added together in the stochastic node (there’s no implicit division by the batch size) so, depending on your use case, you may want to manually divide your rewards by the batch size.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>Instead of replacing the grad_output, shouldn’t you scale grad_output by the reward.</li><NewLine><li>The output is softmax <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py#L43"" rel=""nofollow noopener"">https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py#L43</a> which I understand, but there is no cross-entropy loss in the case. Am I missing something?</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>In the naive REINFORCE method (which is used in the example), we use <code>\Delta log \pi_\theta v(t)</code> to do updating. Just forget cross-entropy loss. PyTorch provide <code>inforce()</code> method for <code>Variable</code> to bind the corresponding <code>v(t)</code> in the formula. You can try following code for checking:</p><NewLine><pre><code class=""lang-auto"">x = Variable(torch.Tensor([0.1, 0.9]), requires_grad=True)<NewLine>y = torch.multinomial(x, 1)  # here, make sure y.data[0] = 1<NewLine>r = torch.Tensor([2]).float()<NewLine>y.inforce(r)<NewLine>y.backward()<NewLine>print x.grad.data<NewLine></code></pre><NewLine><p>Which will give you the output of <code>[0.0, -2.2]</code>. And <code>-2.2</code> is exactly <code>1.0/0.9 * -v(t)</code> (Because d<code>logp = 1/p</code>)</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>I implemented Vanilla Policy Gradient in this way and it works. However using the .reinforce method seems cleaner</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/apaszke"">@apaszke</a></p><NewLine><p>In <a href=""https://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/stochastic.py"" rel=""nofollow noopener"">this file</a>, how could we understand the <code>backward</code> for Normal distribution.</p><NewLine><p>To make it simple, let’s say 1D, given a mean and std, we have a sample = mean + std*eps, where eps ~ N(0, 1).</p><NewLine><p>In the <code>backward</code>, the grad_mean = -reward*(sample - mean)/std**2.</p><NewLine><p>It is not very clear to me why it is like that. Since if we have a sample, according to the formula,  d_sample/d_mean = 1. So, grad_mean = upward_gradient * d_sample/d_mean</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/zuoxingdong"">@zuoxingdong</a> I recommend you read the paper linked at the top of the file you linked. It goes through all the derivations. The case of the normal distribution is derived in section 6, the formula you are asking about specifically is (13).</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/abhigenie92"">@abhigenie92</a><br/><NewLine>Agree. I am also not clear with this point. Do know how to achieve that? Is that understanding correct?</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>I do not notice the paper you mentioned, can you give a PDF file or the link. Thanks.</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Indeed it looks like the link was removed in the last commit, which is unfortunate. The paper was <a href=""http://incompleteideas.net/sutton/williams-92.pdf"" rel=""nofollow noopener"">Williams’92</a> aka “The REINFORCE paper”.</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just in case it’s useful, I can reproduce the results from calling <code>.reinforce</code> with the following code:</p><NewLine><pre><code>""""""<NewLine>Try some reinforce by hand and similar<NewLine>""""""<NewLine><NewLine>import torch<NewLine>from torch import autograd, nn, optim<NewLine>import torch.nn.functional as F<NewLine>from torch.autograd import Variable<NewLine>import numpy as np<NewLine><NewLine><NewLine>def run_model(x, h1):<NewLine>    torch.manual_seed(123)<NewLine>    params = list(h1.parameters())<NewLine><NewLine>    x1 = h1(Variable(x))<NewLine>    x2 = F.softmax(x1)<NewLine>    a = torch.multinomial(x2)<NewLine>    print('a', a)<NewLine>    return x1, x2, a<NewLine><NewLine><NewLine>def run_by_hand(params, x, h1):<NewLine>    print('')<NewLine>    print('=======')<NewLine>    print('by hand')<NewLine>    h1.zero_grad()<NewLine>    x1, x2, a = run_model(x, h1)<NewLine>    g = torch.gather(x2, 1, Variable(a.data))<NewLine>    log_g = g.log()<NewLine>    log_g.backward(- torch.ones(4, 1))<NewLine>    print('params.grad', params.grad)<NewLine><NewLine><NewLine>def run_pytorch_reinforce(params, x, h1):<NewLine>    print('')<NewLine>    print('=======')<NewLine>    print('pytorch reinforce')<NewLine>    h1.zero_grad()<NewLine>    x1, x2, a = run_model(x, h1)<NewLine>    a.reinforce(torch.ones(4, 1))<NewLine>    autograd.backward([a], [None])<NewLine>    print('params.grad', params.grad)<NewLine><NewLine><NewLine>def run():<NewLine>    N = 4<NewLine>    K = 1<NewLine>    C = 3<NewLine><NewLine>    torch.manual_seed(123)<NewLine>    x = torch.ones(N, K)<NewLine>    h1 = nn.Linear(K, C, bias=False)<NewLine>    params = list(h1.parameters())[0]<NewLine>    run_by_hand(params, x, h1)<NewLine>    run_pytorch_reinforce(params, x, h1)<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    run()<NewLine></code></pre><NewLine><p>Result:</p><NewLine><pre><code>=======<NewLine>by hand<NewLine>a Variable containing:<NewLine> 1<NewLine> 1<NewLine> 0<NewLine> 1<NewLine>[torch.LongTensor of size 4x1]<NewLine><NewLine>params.grad Variable containing:<NewLine> 0.6170<NewLine>-1.3288<NewLine> 0.7117<NewLine>[torch.FloatTensor of size 3x1]<NewLine><NewLine><NewLine>=======<NewLine>pytorch reinforce<NewLine>a Variable containing:<NewLine> 1<NewLine> 1<NewLine> 0<NewLine> 1<NewLine>[torch.LongTensor of size 4x1]<NewLine><NewLine>params.grad Variable containing:<NewLine> 0.6170<NewLine>-1.3288<NewLine> 0.7117<NewLine>[torch.FloatTensor of size 3x1]<NewLine></code></pre><NewLine><p>(Its actually almost the same amount of code in fact?)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/yunjey; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/zuoxingdong; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/wjaskowski; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/jekbradbury; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/wjaskowski; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/jekbradbury; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/abhigenie92; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Crazyai; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/volpato30; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/zuoxingdong; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/lucasb-eyer; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/norm; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/zeng; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/lucasb-eyer; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/hughperkins; <NewLine> ,"REPLY_DATE 1: March 23, 2017,  4:07pm; <NewLine> REPLY_DATE 2: March 24, 2017,  1:05am; <NewLine> REPLY_DATE 3: April 11, 2017,  5:57pm; <NewLine> REPLY_DATE 4: April 12, 2017,  2:16pm; <NewLine> REPLY_DATE 5: May 26, 2017,  6:55am; <NewLine> REPLY_DATE 6: May 26, 2017, 10:49pm; <NewLine> REPLY_DATE 7: May 28, 2017,  5:53pm; <NewLine> REPLY_DATE 8: May 27, 2017,  9:10am; <NewLine> REPLY_DATE 9: July 4, 2017,  6:50pm; <NewLine> REPLY_DATE 10: August 7, 2017, 12:50pm; <NewLine> REPLY_DATE 11: August 22, 2017,  9:42pm; <NewLine> REPLY_DATE 12: September 19, 2017, 12:18am; <NewLine> REPLY_DATE 13: September 23, 2017,  4:12pm; <NewLine> REPLY_DATE 14: October 25, 2017, 11:31am; <NewLine> REPLY_DATE 15: November 2, 2017,  8:18am; <NewLine> REPLY_DATE 16: November 2, 2017,  9:49am; <NewLine> REPLY_DATE 17: November 3, 2017,  6:12am; <NewLine> ",REPLY 1 LIKES: 13 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 2 Likes; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: 2 Likes; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: 1 Like; <NewLine> REPLY 17 LIKES: ; <NewLine> 
9424,Implementing reinforce using gradient scaling,2017-11-02T14:35:24.483Z,0,556,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to learn pong by scaling the loss gradients with rewards but it is not learning anything.<br/><NewLine>I have not done discounting because I think in some problems this might not be correct for example when producing a word sequence.</p><NewLine><pre><code>def update_grad(grad):<NewLine>        grad = torch.mul(grad, rewards_tensor)<NewLine>        return grad <NewLine></code></pre><NewLine><p>Here is my current implementation:<br/><NewLine><aside class=""onebox githubgist""><NewLine><header class=""source""><NewLine><a href=""https://gist.github.com/pj-parag/fce77fe977263bb9360d257c332fe44b"" rel=""nofollow noopener"" target=""_blank"">gist.github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://gist.github.com/pj-parag/fce77fe977263bb9360d257c332fe44b"" rel=""nofollow noopener"" target=""_blank"">https://gist.github.com/pj-parag/fce77fe977263bb9360d257c332fe44b</a></h4><NewLine><h5>gradient_scaling.py</h5><NewLine><pre><code class=""Python"">import numpy as np<NewLine>import cPickle as pickle<NewLine>import gym<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>from torch import optim<NewLine>from torch.autograd import Variable<NewLine>from torch import optim<NewLine>import torch.nn.functional as F<NewLine></code></pre><NewLine>This file has been truncated. <a href=""https://gist.github.com/pj-parag/fce77fe977263bb9360d257c332fe44b"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine><p><NewLine></p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>What is the correct way to do this? I am learning pytorch and I am very new to RL.</p><NewLine><p>Thanks a lot, <img alt="":slightly_smiling_face:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/slightly_smiling_face.png?v=5"" title="":slightly_smiling_face:""/></p><NewLine></div>",https://discuss.pytorch.org/u/norm,(norm),norm,"November 2, 2017,  2:35pm",,,,,
9235,Simple direct policy search suffers from wrong gradients?,2017-10-30T02:42:54.559Z,0,348,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Currently I am experimented a simple direct policy search in OpenAI Gym Pendulum-v0 in PyTorch.</p><NewLine><p>The approach is:</p><NewLine><ol><NewLine><li><NewLine><p>Initialize MLP policy with single hidden layer, 50 neurons, ReLu nonlinearity and output 1 continuous action value.</p><NewLine></li><NewLine><li><NewLine><p>True differentiable dynamics model (copy Gym code to PyTorch)</p><NewLine></li><NewLine><li><NewLine><p>Repeat</p><NewLine><pre><code>    3.1. Rollout trajectory for T=50 time steps by policy and compute accumulated costs J.<NewLine><NewLine>    3.2. J.backward() and update policy<NewLine></code></pre><NewLine></li><NewLine></ol><NewLine><p>First of all, the policy learns something reasonable by swing the pole and pull up. But then nothing better to be learned, even though there are still gradients.</p><NewLine><p>It is quite confusing where might be the problem, I’ve tried different hidden size, learning rates or max torques of Pendulum.</p><NewLine><p>BTW, the cost function is 0-1 saturation cost, i.e. <code>1 - exp(-0.5*x^2/s^2)</code> where <code>s</code> is a scaling factor.</p><NewLine></div>",https://discuss.pytorch.org/u/zuoxingdong,(Xingdong Zuo),zuoxingdong,"October 30, 2017,  2:42am",,,,,
8909,Pytorch DQN cannot work,2017-10-21T07:09:48.671Z,1,539,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to make DQN with pytorch to work with simple game like PongNoFrameskip-v4, I coded up the deep Q learning code here: <a href=""https://github.com/DontGiveUpEasily/pytorch-dqn"" rel=""nofollow noopener"">https://github.com/DontGiveUpEasily/pytorch-dqn</a> however, it runs pretty well on one machine and fails on every other machines. The machine that works pretty well has pytorch version 0.12.0 and python2.7.6 (not anaconda), while other machines have pytorch 0.2.0 and python 3.5. I downgraded pytorch version to 0.12.0 and python to 2.7 (no longer using anaconda) and tried again but it didn’t help.  What could be the reason?</p><NewLine></div>",https://discuss.pytorch.org/u/Peter_Ham,(Peter Ham),Peter_Ham,"October 21, 2017,  7:11am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you use the same seed (both openAI gym and Pytorch) when you tried to replicate your experiments?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>yes, exactly the same code, completely different results. Repeat for multiple times.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/valthom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Peter_Ham; <NewLine> ,"REPLY_DATE 1: October 21, 2017,  2:45pm; <NewLine> REPLY_DATE 2: October 21, 2017, 10:01pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
3710,Strange Error in Reinforcement Learning (DQN) tutorial,2017-06-05T13:23:10.957Z,0,947,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I met a strange problem in the PyTorch official <a href=""http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noopener"">Reinforcement Learning (DQN) tutorial </a>.</p><NewLine><p>It is OK when I run the reinforcement_q_learning.ipynb on my macbookpro(OSX), however, when I ran the last cell of it on my PC (Ubuntu16.04+1080ti+Cuda8.0+PyTorch+Gym), it errors out:</p><NewLine><pre><code>---------------------------------------------------------------------------<NewLine>ValueError                                Traceback (most recent call last)<NewLine>&lt;ipython-input-8-c9b5f0b71a6f&gt; in &lt;module&gt;()<NewLine>  3     # Initialize the environment and state<NewLine>  4     env.reset()<NewLine>----&gt; 5     last_screen = get_screen()<NewLine>  6     current_screen = get_screen()<NewLine>  7     state = current_screen - last_screen<NewLine><NewLine>&lt;ipython-input-5-3aa424675ed4&gt; in get_screen()<NewLine> 34     screen = torch.from_numpy(screen)<NewLine> 35     # Resize, and add a batch dimension (BCHW)<NewLine>---&gt; 36     return resize(screen).unsqueeze(0).type(Tensor)<NewLine> 37 <NewLine> 38 env.reset()<NewLine><NewLine>/usr/local/lib/python2.7/dist-packages/torchvision/transforms.pyc in __call__(self, img)<NewLine> 27     def __call__(self, img):<NewLine> 28         for t in self.transforms:<NewLine>---&gt; 29             img = t(img)<NewLine> 30         return img<NewLine> 31 <NewLine><NewLine>/usr/local/lib/python2.7/dist-packages/torchvision/transforms.pyc in __call__(self, pic)<NewLine> 77             pic = pic.mul(255).byte()<NewLine> 78         if torch.is_tensor(pic):<NewLine>---&gt; 79             npimg = np.transpose(pic.numpy(), (1, 2, 0))<NewLine> 80         assert isinstance(npimg, np.ndarray), 'pic should be Tensor or ndarray'<NewLine> 81         if npimg.shape[2] == 1:<NewLine><NewLine>/home/diaosiki/.local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc in transpose(a, axes)<NewLine>548 <NewLine>549     """"""<NewLine>--&gt; 550     return _wrapfunc(a, 'transpose', axes)<NewLine>551 <NewLine>552 <NewLine><NewLine>/home/diaosiki/.local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc in _wrapfunc(obj, method, *args, **kwds)<NewLine> 55 def _wrapfunc(obj, method, *args, **kwds):<NewLine> 56     try:<NewLine>---&gt; 57         return getattr(obj, method)(*args, **kwds)<NewLine> 58 <NewLine> 59     # An AttributeError occurs if the object does not have<NewLine><NewLine>ValueError: axes don't match array<NewLine></code></pre><NewLine><p>I could not figure out what’s the problem. Since the tutorial run on OSX, it should be my PC environment problem, yet other tutorials like <a href=""http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"" rel=""nofollow noopener"">Transfer Learning tutorial</a> run perfectly on my PC.</p><NewLine><p>Thanks for any help!</p><NewLine></div>",https://discuss.pytorch.org/u/siki,(diaosiki),siki,"June 5, 2017,  1:23pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>did you ever figure out this out? I ran into the same thing running the tutorial on ubuntu.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jwjohnson314; <NewLine> ,"REPLY_DATE 1: October 12, 2017,  1:07pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
8037,My DQN doesn&rsquo;t learn,2017-09-29T04:59:06.319Z,0,491,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m new to reinforcement learning and trying to implement DQN as the original paper proposed.</p><NewLine><p>But as the title, this DQN doesn’t seem to learn even after 1 million steps. Indeed the target value and the q value are going to close to each other, the accumulated reward (and loss) doesn’t increase.</p><NewLine><p>I cannot find out what is wrong. I will be happy if you can point out what is wrong in <a href=""https://github.com/moskomule/pytorch.rl.learning/blob/dqn/dl/dqn/dqn.py"" rel=""nofollow noopener"">my code</a>. Any advice is also welcome.</p><NewLine><p>Thank you for advance.</p><NewLine></div>",https://discuss.pytorch.org/u/moskomule,(Moskomule),moskomule,"September 29, 2017, 11:16am",1 Like,,,,
7524,DQN slower on GPU than CPU (tested on Breakout),2017-09-18T09:27:09.667Z,0,1071,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Dear all,</p><NewLine><p>I’m both new to pyTorch and RL in general.</p><NewLine><p>But recently I started to re-implement some of the most famous works.<br/><NewLine>I followed the tutorial of Denny Britz, but I used PyTorch to make it more interesting.</p><NewLine><p>I found out that my implementation is much faster when run on the CPU than on the GPU, which is strange.<br/><NewLine>I’m not sure, but I suppose that this is cause by the fact that I need to move the GPU memory many times to the CPU to sample some actions.<br/><NewLine>You can find my code in the git repo: <a href=""https://github.com/andompesta/MLTutorials/tree/master/RL/DeepQLearning"" rel=""nofollow noopener"">https://github.com/andompesta/MLTutorials/tree/master/RL/DeepQLearning</a></p><NewLine><p>Is my intuition right? do you have nay suggestions? More importantly I found that there is no equivalent in pyTorch fo the function np.random.choice. Have anyone implemented it in torch?</p><NewLine><p>Thanks in advance.</p><NewLine><p>Cheers,<br/><NewLine>Sandro</p><NewLine></div>",https://discuss.pytorch.org/u/andompesta,(Sandro Cavallari),andompesta,"September 18, 2017, 11:43am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>it’s possible that:</p><NewLine><ul><NewLine><li>your model is very small</li><NewLine><li>your GPU is not the fastest model</li><NewLine></ul><NewLine><p>either of these could explain the slowness.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/smth; <NewLine> ,"REPLY_DATE 1: September 28, 2017,  3:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
5944,Help editing loss or parameter update in Prioritized Experience Replay,2017-08-08T20:08:24.102Z,3,1716,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi. I’m trying to implement DQN with prioritized experience replay. (This paper: <a href=""https://arxiv.org/pdf/1511.05952.pdf"" rel=""nofollow noopener"">https://arxiv.org/pdf/1511.05952.pdf</a>). I need to multiply the gradients of the parameters by importance sampling weights before I update the neural network parameters. For my loss function I’ve been using huber loss.</p><NewLine><p>Here is a snippet of my code:</p><NewLine><pre><code>    self.optimizer.zero_grad()<NewLine>    loss.backward()<NewLine>    for param, weight in zip(self.qnet.parameters(), sampling_weights_batch):<NewLine>            param.grad.data *= weight<NewLine></code></pre><NewLine><p>Instead of redefining the smooth_l1_loss, I go through each parameter gradient and multiply it by the corresponding sampling weight which I stored in my replay memory. Is there a faster way to do this? Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/8Gitbrix,(Ashwin Jeyaseelan),8Gitbrix,"August 8, 2017,  8:09pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You will have to use ._grad in order to overwrite the gradient.</p><NewLine><p>But you should definitely prefer to change the loss computation (it would be much simpler and cleaner). The  smooth_l1_loss is immediate to rewrite by hand, and you just need a step to multiply with your weights before summing the batch dimension. Something like this:</p><NewLine><pre><code class=""lang-python"">class WeightedLoss(nn.Module):<NewLine>    def __init__(self):<NewLine>        [....]<NewLine><NewLine>    def forward(self, input, target, weights):<NewLine>        batch_loss = (torch.abs(input - target)&lt;1).float()*(input - target)**2 +\<NewLine>            (torch.abs(input - target)&gt;=1).float()*(torch.abs(input - target) - 0.5)<NewLine>        weighted_batch_loss = weight * loss <NewLine>        weighted_loss = weighted_batch_loss.sum()<NewLine>        return weighted_loss<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It worked. Thank you Alexis!!!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/8gitbrix"">@8Gitbrix</a>,</p><NewLine><p>I was wondering if your Prioritized Experience Replay code was available anywhere? Think it would make a really cool addition if it was possible to add it to the PyTorch tutorial for Q-learning?</p><NewLine><p><a class=""onebox"" href=""http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noopener"" target=""_blank"">http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html</a></p><NewLine><p>It’s very interesting <img alt="":blush:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/blush.png?v=5"" title="":blush:""/></p><NewLine><p>Best,</p><NewLine><p>Ajay</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Ajay. Unfortunately its not working, and the prioritized code is work related which I myself didn’t implement - but my reference was jaromiru’s code (worth checking out if you want to add it to the pytorch tutorial). If I do get something on my own I’ll let you know, or we can code something together!</p><NewLine><p>Ashwin</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Also, I have my own dqn code which I put on github: <a href=""https://github.com/8Gitbrix/DQN"" rel=""nofollow noopener"">https://github.com/8Gitbrix/DQN</a> . I tested its convergence to a local optimum on the game freeway, but I haven’t ran any rull tests on games since I only have a cpu on my macbook <img alt="":frowning:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/frowning.png?v=5"" title="":frowning:""/> . If anyone could test it and push the results that would be great.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Ashwin,</p><NewLine><p>there’s a good implementation of prioritized experience replay, (in TensorFlow unfortunately), by OpenAI - <a href=""https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py"" rel=""nofollow noopener"">https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py</a></p><NewLine><p>Would be fun to try to port it over to PyTorch <img alt="":grinning:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/grinning.png?v=5"" title="":grinning:""/> Seems that PER buffer is language agnostic?</p><NewLine><p>Best,</p><NewLine><p>Ajay</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/8Gitbrix; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/AjayTalati; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/8Gitbrix; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/8Gitbrix; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/AjayTalati; <NewLine> ,"REPLY_DATE 1: August 9, 2017,  9:33am; <NewLine> REPLY_DATE 2: August 10, 2017,  3:53pm; <NewLine> REPLY_DATE 3: September 23, 2017,  4:47pm; <NewLine> REPLY_DATE 4: September 23, 2017,  6:27pm; <NewLine> REPLY_DATE 5: September 23, 2017,  6:29pm; <NewLine> REPLY_DATE 6: September 24, 2017,  6:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> 
6449,DQN tutorial don&rsquo;t implement &lsquo;separate network&rsquo;?,2017-08-19T08:55:52.022Z,0,334,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>the papaer, DQN (2015), have 2 issue.</p><NewLine><p>First , replay memory and it implemented the tutorial DQN</p><NewLine><p>but, second, I think ‘separate network’ may not implemented the tutorial.<br/><NewLine>is that right?</p><NewLine><p>thanks</p><NewLine></div>",https://discuss.pytorch.org/u/Teddy_Kim,(Teddy Kim),Teddy_Kim,"August 19, 2017,  8:55am",,,,,
6264,Backward runtime error with network using the hidden state of an lstm cell while training,2017-08-15T19:00:54.767Z,1,1061,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I’m trying to implement Deep Q learning with an lstm cell. I’m implementing something similar to this paper:  <a href=""https://arxiv.org/pdf/1509.03044.pdf"" rel=""nofollow noopener"">https://arxiv.org/pdf/1509.03044.pdf</a>. The lstm cell + linear layer network learns the hidden state from rewards, which is then passed into a separate DQN network for predicting the Q value. But I’m having trouble training the networks because of a backward runtime error.</p><NewLine><p>Here is a snippet of my code:</p><NewLine><pre><code class=""lang-python""># train RNN state model:<NewLine>predict_reward, hidden_state = self.rnn_model(Variable(torch.from_numpy(state_batch).float()))<NewLine>reward_var = Variable(torch.from_numpy(reward_batch).float(), requires_grad=False)<NewLine>state_loss = nn.MSELoss()<NewLine>state_loss = state_loss(predict_reward, reward_var)<NewLine>self.rnn_optimizer.zero_grad()<NewLine>state_loss.backward()<NewLine>self.rnn_optimizer.step()<NewLine><NewLine># generate target q values<NewLine>target_q_output = self._generate_target_q_values(next_state_batch, reward_batch)<NewLine># get q net output after passing hidden_state into it<NewLine>q_output =  self.qnet(hidden_state)<NewLine>q_output = q_output[range(self._mini_batch_size), action_indexs]<NewLine>loss = F.smooth_l1_loss(q_output, target_q_output)<NewLine>self.optimizer.zero_grad()<NewLine>loss.backward()<NewLine>self.optimizer.step()<NewLine><NewLine></code></pre><NewLine><p>I get an error on the second last line (loss.backward()):</p><NewLine><p>line 370, in train_minibatch loss.backward()<br/><NewLine>File “/usr/local/lib/python3.6/site-packages/torch/autograd/variable.py”, line 156, in backward<br/><NewLine>torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)<br/><NewLine>File “/usr/local/lib/python3.6/site-packages/torch/autograd/<strong>init</strong>.py”, line 98, in backward<br/><NewLine>variables, grad_variables, retain_graph)<br/><NewLine>File “/usr/local/lib/python3.6/site-packages/torch/autograd/function.py”, line 91, in apply<br/><NewLine>return self._forward_cls.backward(self, *args)<br/><NewLine>File “/usr/local/lib/python3.6/site-packages/torch/autograd/_functions/basic_ops.py”, line 52, in backward<br/><NewLine>a, b = ctx.saved_variables<br/><NewLine><code>RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.</code></p><NewLine><p>I think the error is caused by passing the hidden state into the q network after calling backward on rnn_model (lstm network + linear layer). How can I fix this issue? Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/8Gitbrix,(Ashwin Jeyaseelan),8Gitbrix,"August 16, 2017, 12:33am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I imagine you have parameters that are optimized by both <code>rnn_optimizer</code> and <code>optimizer</code>. I think you probably want to ensure that the parameters optimized by each are disjoint?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah. My RNN model uses RMS prop and my dqn model uses adamax.<br/><NewLine>Earlier I declared:</p><NewLine><pre><code class=""lang-auto"">self.rnn_optimizer = optim.RMSprop(self.rnn_model.parameters())<NewLine>self.optimizer = optim.Adamax(self.qnet.parameters())<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I noticed that if I don’t save the hidden state in my rnn model, I don’t have any backward runtime error when calling loss.backward on the dqn network. But I need to save the hidden states in order for the rnn model to be useful.</p><NewLine><p>This is how I defined my rnn model:</p><NewLine><pre><code class=""lang-auto"">class LSTMState(nn.Module):<NewLine>    def __init__(self, input_size, mini_batch_size):<NewLine>        """"""<NewLine>        :param input_size: Number of features in state<NewLine>        This network has 1 output: reward.<NewLine>        The LSTM cell outputs its representation of the state, which will be passed into DQN.<NewLine>        """"""<NewLine>        super(LSTMState, self).__init__()<NewLine>        # initialize hidden state<NewLine>        self.h = Variable(torch.zeros(mini_batch_size, input_size))<NewLine>        # initialize cell state<NewLine>        self.c = Variable(torch.zeros(mini_batch_size, input_size))<NewLine>        self.hidden_dim = input_size<NewLine><NewLine>        # hidden size is number of features in the hidden state / size of output<NewLine>        # input size is the number of features in incoming input<NewLine>        self.lstm = nn.LSTMCell(input_size=input_size, hidden_size=input_size)<NewLine>        self.fc = nn.Linear(input_size, 1) # the linear layer output represents predicted reward<NewLine>        init.xavier_uniform(self.fc.weight)<NewLine>        # no need for an activation function, since magnitude can be big, and rewards can be negative<NewLine><NewLine>    def forward(self, x, train=False):<NewLine>        if train: # if training, we don't want to remember the hidden states?<NewLine>            # initialize hidden state<NewLine>            self.h = Variable(torch.zeros(x.size(0), self.hidden_dim))<NewLine>            # initialize cell state<NewLine>            self.c = Variable(torch.zeros(x.size(0), self.hidden_dim))<NewLine>            h, c = self.lstm(x, (self.h, self.c))<NewLine>            out = self.fc(self.h)<NewLine>            return out, h<NewLine>        else: # when testing, save the hidden and cell state<NewLine>            self.h, self.c = self.lstm(x, (self.h, self.c))<NewLine>            out = self.fc(self.h)<NewLine>            # linear layer output out is the predicted reward, and self.h is the predicted state information<NewLine>            return out, self.h<NewLine></code></pre><NewLine><p>Never mind. The error arises if I save the hidden and cell states while training. But with this ^ definition it works, although it runs terribly.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hughperkins; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/8Gitbrix; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/8Gitbrix; <NewLine> ,"REPLY_DATE 1: August 16, 2017,  7:21am; <NewLine> REPLY_DATE 2: August 16, 2017,  2:51pm; <NewLine> REPLY_DATE 3: August 16, 2017,  4:32pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
6267,Help with the creation of a reward function,2017-08-15T20:14:53.525Z,0,522,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am working on a hexapod that learns how to walk through RL. I have pretty much all of the code written, just working on training it. The issue is that I don’t feel like my reward function is good enough and need some help on how to create it.</p><NewLine><p>My basic structure is that the hexapod has a camera with a CNN that creates bounding boxes around people. I want the hexapod to maximize the area of the bounding box (meaning walk closer to the person) and minimize the distance between the center of the bounding box and the center of the frame (to make sure its walking towards the human). My current reward function is:</p><NewLine><pre><code>return (-1) * (distance**2 * area**2)<NewLine></code></pre><NewLine><p>I don’t like how I’m taking out the direction of the distance (meaning it has no idea if the bounding box is left or right from the center).</p><NewLine><p>I have no experience creating reward functions (this is my first) and would love some guidance on how to make it ‘correct’.</p><NewLine></div>",https://discuss.pytorch.org/u/traw1234,(Tug Witt),traw1234,"August 15, 2017,  8:15pm",,,,,
5634,Categorical DQN Implementation,2017-07-31T13:06:28.449Z,2,1327,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Just wanted to share an attempt at implementing in Pytorch the Categorical DQN algorithm published by DeepMind last week in <a href=""https://arxiv.org/abs/1707.06887"" rel=""nofollow noopener"">A distributional perspective on reinforcement learning</a>.</p><NewLine><p>It currently trains well on Catch, still running experiments on Atari. I am yet to find a good set of hyperparams for Atari games, will post updates about this.</p><NewLine><p>You can find the repo <a href=""https://github.com/floringogianu/categorical-dqn"" rel=""nofollow noopener"">here</a>, let me know if you have any suggestions / comments.</p><NewLine></div>",https://discuss.pytorch.org/u/florin,(Florin Gogianu),florin,"July 31, 2017,  1:06pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Wow, Rémi Munos is at Deepmind now <img alt="":astonished:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/astonished.png?v=5"" title="":astonished:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think this is his tenth paper or so at DeepMind <img alt="":slight_smile:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=5"" title="":slight_smile:""/> . Each one worth reading…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>He was teaching RL in my undergrad school in France. This course changed my life.<br/><NewLine>The paper is amazing, now I’m looking forward seeing how it reacts with some parallelism boost.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Haha, I have some async boilerplate lying around, was thinking the same.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/florin; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/florin; <NewLine> ,"REPLY_DATE 1: July 31, 2017,  1:33pm; <NewLine> REPLY_DATE 2: July 31, 2017,  1:56pm; <NewLine> REPLY_DATE 3: July 31, 2017,  2:17pm; <NewLine> REPLY_DATE 4: July 31, 2017,  3:17pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
5579,DQN Tutorial Issue,2017-07-30T01:30:07.926Z,0,412,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am currently trying to replicate in my own way the DQN tutorial; however for some reason as the DQN runs, the loss doesn’t change, and as such I believe the DQN isn’t being optimized. From looking at other similar questions, I think it has to deal with how I constructed the computational graph, but I don’t know exactly how to fix it from this particular situation.</p><NewLine><pre><code>def optimize_model():<NewLine>    batch = random.sample(D, batch_size)<NewLine>    state_batch = []<NewLine>    action_batch = []<NewLine>    reward_batch = []<NewLine>    next_state_batch = []<NewLine>    for state, action, reward, next_state in batch:<NewLine>        state_batch.append(state)<NewLine>        action_batch.append(action)<NewLine>        reward_batch.append(reward)<NewLine>        next_state_batch.append(next_state)<NewLine>    q_values = model(Variable(FloatTensor(state_batch)))<NewLine>    if len(D) &lt; 128:<NewLine>        q_values = q_values.gather(1, Variable(torch.LongTensor([action_batch])))<NewLine>    else:<NewLine>        q_values = q_values.gather(1, Variable(torch.t(torch.LongTensor([action_batch]))))<NewLine>    next_state_values = model(Variable(FloatTensor(next_state_batch), volatile=True)).max(1)[0]<NewLine>    next_state_values.volatile = False<NewLine>    expected_state_action_values = (next_state_values * gamma) + Variable(FloatTensor(reward_batch))<NewLine>    loss = torch.nn.functional.smooth_l1_loss(q_values, expected_state_action_values)<NewLine>    optimizer.zero_grad()<NewLine>    loss.backward()<NewLine>    optimizer.step()</code></pre><NewLine></div>",https://discuss.pytorch.org/u/jcava,(John Cava),jcava,"July 30, 2017,  1:30am",,,,,
5045,What do DQN or Actor-Critic learn for their CNN&rsquo;s?,2017-07-19T21:48:15.452Z,0,570,"<div class=""post"" itemprop=""articleBody""><NewLine><p>For example I could understand the loss from an Actor would learn to change the probability of an action at fully-connected layer, but at convolutional layer? What kind of features does it learn?</p><NewLine></div>",https://discuss.pytorch.org/u/Eddie_Li,(Eddie Li),Eddie_Li,"July 19, 2017,  9:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It is exactly the same idea, but instead of a fully connected on the whole input, it is several times the same small fully-connected on different parts of the input.</p><NewLine><p>For instance, if your channels are 4 consecutive black/white images with a moving ball rolling on the x axis, you will probably learn a 3D convolution (time * x * y) that will look like an edge detector projected on (x,y), and something close to an identity projected on (time*x).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> ,"REPLY_DATE 1: July 20, 2017,  8:54am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
916,Storing torch tensor for dqn memory issue,2017-03-06T20:50:57.543Z,9,2068,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am running into a ram leaking memory problem when I am saving my frames as pytorch tensor for a simple dqn implementation (inspired by <a href=""https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb"" rel=""nofollow noopener"">link</a>). Here is an quick example without the learning loop, trying to isolate my issue:</p><NewLine><pre><code class=""lang-python"">import resource<NewLine><NewLine>import argparse<NewLine>import gym<NewLine>import numpy as np<NewLine>from itertools import count<NewLine>from collections import namedtuple<NewLine>import os <NewLine><NewLine>import torch<NewLine>import random<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine>import torch.autograd as autograd<NewLine>from torch.autograd import Variable<NewLine>import torchvision.transforms as T<NewLine>import cv2<NewLine>import pickle<NewLine>import glob<NewLine>import time<NewLine>import subprocess<NewLine>from collections import namedtuple<NewLine><NewLine># Class<NewLine>class ReplayMemory(object):<NewLine>    '''<NewLine>    A simple class to wrap around the concept of memory<NewLine>    this helps for managing how much data is used. <NewLine>    '''<NewLine>    def __init__(self, capacity):<NewLine>        self.capacity = capacity<NewLine>        self.memory = []<NewLine>        self.position = 0<NewLine>        <NewLine>    def push(self, *args):<NewLine>        """"""Saves a transition.""""""<NewLine>        if len(self.memory) &lt; self.capacity:<NewLine>            self.memory.append(None) <NewLine>        self.memory[self.position] = Transition(*args)<NewLine>        self.position = (self.position + 1) % self.capacity<NewLine>        <NewLine>    def sample(self, batch_size):<NewLine>        return random.sample(self.memory, batch_size)<NewLine>    <NewLine>    def __len__(self):<NewLine>        return len(self.memory)<NewLine><NewLine># Functions<NewLine>def ProcessState(state,torchOutput=True):<NewLine>    img = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)<NewLine>    img = cv2.resize(img, (imageShape[1],imageShape[0])).astype('float32')<NewLine>    if torchOutput:<NewLine>        img = torch.from_numpy(img)<NewLine>    img /= 255<NewLine>    img -= 0.5 <NewLine>    img *= 2<NewLine>    return img<NewLine><NewLine># Variables<NewLine>Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))<NewLine><NewLine>imageShape = (110,80)<NewLine>env = gym.make('PongDeterministic-v3')<NewLine>action = 0 <NewLine>memory = ReplayMemory(32)<NewLine><NewLine># Example with pytorch<NewLine>for i_episode in range(25):<NewLine>    break<NewLine>    print 'Pytorch: Memory usage: %s (kb)' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss<NewLine>    obser = env.reset()<NewLine>    obser = ProcessState(obser)<NewLine><NewLine>    state = torch.ones((3,imageShape[0],imageShape[1]))    <NewLine>    state = torch.cat((state,obser.view(1,imageShape[0],imageShape[1])),0)<NewLine><NewLine>    for t in range(10000): <NewLine>        obser, reward, done, _ = env.step(0)<NewLine><NewLine>        #this is new observation getting process<NewLine>        obser = ProcessState(obser)<NewLine><NewLine>        state = torch.cat((state,obser.view(1,imageShape[0],imageShape[1])),0)<NewLine>        <NewLine>        memory.push(state[:-1], action, state[1:], reward, done)<NewLine><NewLine>        state = state[1:]<NewLine><NewLine>        if done:<NewLine>            break<NewLine># quit()<NewLine># memory = ReplayMemory(32)<NewLine># Numpy<NewLine>for i_episode in range(25):<NewLine>    print 'Numpy: Memory usage: %s (kb)' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss<NewLine>    obser = env.reset()<NewLine>    obser = ProcessState(obser,False)<NewLine><NewLine>    state = np.zeros((3,imageShape[0],imageShape[1]))<NewLine>    state = np.concatenate([state, obser.reshape((1,imageShape[0],imageShape[1]))])<NewLine><NewLine>    for t in range(10000): <NewLine>        obser, reward, done, _ = env.step(0)<NewLine><NewLine>        #this is new observation getting process<NewLine>        obser = ProcessState(obser,False)<NewLine><NewLine>        # state = torch.cat((state,obser.view(1,imageShape[0],imageShape[1])),0)<NewLine>        state = np.concatenate([state, obser.reshape((1,imageShape[0],imageShape[1]))])<NewLine>        <NewLine>        memory.push(state[:-1], action, state[1:], reward, done)<NewLine>        state = state[1:]<NewLine><NewLine>        if done:<NewLine>            break<NewLine></code></pre><NewLine><p>Here is the output I get for running the first loop (using pytorch) vs the second one, which is saving numpy arrays.</p><NewLine><pre><code class=""lang-auto"">jtremblay@office:~/code/Personal-git/dqn$ python memory_issue.py <NewLine>[2017-03-06 12:38:30,254] Making new env: PongDeterministic-v3<NewLine>Pytorch: Memory usage: 113432 (kb)<NewLine>Pytorch: Memory usage: 226380 (kb)<NewLine>Pytorch: Memory usage: 323796 (kb)<NewLine>Pytorch: Memory usage: 410124 (kb)<NewLine>Pytorch: Memory usage: 490116 (kb)<NewLine>Pytorch: Memory usage: 565884 (kb)<NewLine>Pytorch: Memory usage: 637428 (kb)<NewLine>Pytorch: Memory usage: 704220 (kb)<NewLine>Pytorch: Memory usage: 760188 (kb)<NewLine>Pytorch: Memory usage: 815892 (kb)<NewLine>Pytorch: Memory usage: 861828 (kb)<NewLine>Pytorch: Memory usage: 905388 (kb)<NewLine>Pytorch: Memory usage: 938916 (kb)<NewLine>Pytorch: Memory usage: 966900 (kb)<NewLine>Pytorch: Memory usage: 993036 (kb)<NewLine>Pytorch: Memory usage: 1001484 (kb)<NewLine>Pytorch: Memory usage: 1001484 (kb)<NewLine>Pytorch: Memory usage: 1001484 (kb)<NewLine>Pytorch: Memory usage: 1001484 (kb)<NewLine>Pytorch: Memory usage: 1001484 (kb)<NewLine>Pytorch: Memory usage: 1001484 (kb)<NewLine>Pytorch: Memory usage: 1001484 (kb)<NewLine>Pytorch: Memory usage: 1001484 (kb)<NewLine>Pytorch: Memory usage: 1001484 (kb)<NewLine>Pytorch: Memory usage: 1001484 (kb)<NewLine>jtremblay@office:~/code/Personal-git/dqn$ python memory_issue.py <NewLine>[2017-03-06 12:39:22,433] Making new env: PongDeterministic-v3<NewLine>Numpy: Memory usage: 113936 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine>Numpy: Memory usage: 130988 (kb)<NewLine><NewLine></code></pre><NewLine><p>As you can see the numpy saving is much more stable. This does not look like much but when you run my script with a replay size of one million frames it crashes quickly.</p><NewLine><p>Should I avoid storing torch tensor? I quite like keeping everything as a torch tensor to be honest. It saves me a few torch.from_numpy calls. Is there a way to release memory used by torch, I was not able to find anything on that subject in the documentation.</p><NewLine><p>I can provide more examples with learning loops if needed.</p><NewLine></div>",https://discuss.pytorch.org/u/jtremblay,(jtremblay),jtremblay,"March 6, 2017,  8:51pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s weird, I’ll look into that tomorrow. Thanks for posting the script.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I look forward hearing from you. Is there a way to for delete a tensor? I also tried to use storage object instead and it did not work. Also here is the version I am using:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>torch.__version__<NewLine>'0.1.9_2'<NewLine></code></pre><NewLine><p>[edit] I just tested with the most current version (‘0.1.10+ac9245a’) and i obverse the same problem.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It should get freed as soon as it goes out of scope (last reference to it is gone).</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok this is not what I am observing though. Am I keeping weird references to the tensor somewhere? I have “fixed” my script with storing numpy arrays, but I am loosing a lot in performance, about twice the time it took with storing tensor arrays. Is there a fast way to copy numpy arrays into a tensor reference of the same size, eg avoiding using torch.from_numpy()?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>torch.from_numpy</code> should be very fast - it only allocates a new torch.Tensor object that will reuse the same memory as the numpy array did. It’s nearly free <img alt="":confused:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/confused.png?v=5"" title="":confused:""/> I’m surprised that it makes any difference for you</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have done more test for the memory and you are right, the tensor gets clear from the memory when there is no more reference. I am trying to figure out in the script that I have shared where references are kept. Using clone where I could helped reducing the footprint, but there is still a leakage. Here is the updated code.</p><NewLine><pre><code class=""lang-python"">import resource<NewLine><NewLine>import argparse<NewLine>import gym<NewLine>import numpy as np<NewLine>from itertools import count<NewLine>from collections import namedtuple<NewLine>import os <NewLine><NewLine>import torch<NewLine>import random<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine>import torch.autograd as autograd<NewLine>from torch.autograd import Variable<NewLine>import torchvision.transforms as T<NewLine>import cv2<NewLine>import pickle<NewLine>import glob<NewLine>import time<NewLine>import subprocess<NewLine>from collections import namedtuple<NewLine><NewLine># Class<NewLine>class ReplayMemory(object):<NewLine>    '''<NewLine>    A simple class to wrap around the concept of memory<NewLine>    this helps for managing how much data is used. <NewLine>    '''<NewLine>    def __init__(self, capacity):<NewLine>        self.capacity = capacity<NewLine>        self.memory = []<NewLine>        self.position = 0<NewLine>        <NewLine>    def push(self, *args):<NewLine>        """"""Saves a transition.""""""<NewLine>        if len(self.memory) &lt; self.capacity:<NewLine>            self.memory.append(None) <NewLine>        self.memory[self.position] = Transition(*args)<NewLine>        self.position = (self.position + 1) % self.capacity<NewLine>        <NewLine>    def sample(self, batch_size):<NewLine>        return random.sample(self.memory, batch_size)<NewLine>    <NewLine>    def __len__(self):<NewLine>        return len(self.memory)<NewLine><NewLine># Functions<NewLine>def ProcessState(state,torchOutput=True):<NewLine>    img = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)<NewLine>    img = cv2.resize(img, (imageShape[1],imageShape[0])).astype('float32')<NewLine>    if torchOutput:<NewLine>        img = torch.from_numpy(img)<NewLine>    img /= 255<NewLine>    img -= 0.5 <NewLine>    img *= 2<NewLine>    return img<NewLine><NewLine># Variables<NewLine>Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))<NewLine><NewLine>imageShape = (110,80)<NewLine>env = gym.make('PongDeterministic-v3')<NewLine>action = 0 <NewLine>memory = []<NewLine>reward = 0<NewLine>done = False <NewLine># Example with pytorch<NewLine>for i_episode in range(25):<NewLine>    # break<NewLine>    print ('Pytorch: Memory usage: %s (kb)' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)<NewLine>    obser = env.reset()<NewLine>    obser = ProcessState(obser).clone()<NewLine><NewLine>    state = torch.ones((3,imageShape[0],imageShape[1])).clone()    <NewLine>    state = torch.cat((state.clone(),obser.view(1,imageShape[0],imageShape[1])),0).clone()<NewLine><NewLine>    for t in range(10000): <NewLine>        obser, reward, done, _ = env.step(0)<NewLine><NewLine>        #this is new observation getting process<NewLine>        obser = ProcessState(obser).clone()<NewLine><NewLine>        state = torch.cat((state.clone(),obser.view(1,imageShape[0],imageShape[1])),0).clone()<NewLine>        <NewLine>        memory.append({'state':state[:-1].clone(), 'action': action, 'state1':state[1:].clone(), <NewLine>            'reward':reward, 'done':done})<NewLine>        if len(memory) &gt; 32:<NewLine>            memory = memory[1:]<NewLine><NewLine>        state = state[1:].clone()<NewLine><NewLine>        if done:<NewLine>            break<NewLine># quit()<NewLine># memory = ReplayMemory(32)<NewLine># Numpy<NewLine>for i_episode in range(25):<NewLine>    print ('Numpy: Memory usage: %s (kb)' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)<NewLine>    obser = env.reset()<NewLine>    obser = ProcessState(obser,False)<NewLine><NewLine>    state = np.zeros((3,imageShape[0],imageShape[1]))<NewLine>    state = np.concatenate([state, obser.reshape((1,imageShape[0],imageShape[1]))])<NewLine><NewLine>    for t in range(10000): <NewLine>        obser, reward, done, _ = env.step(0)<NewLine><NewLine>        #this is new observation getting process<NewLine>        obser = ProcessState(obser,False)<NewLine><NewLine>        # state = torch.cat((state,obser.view(1,imageShape[0],imageShape[1])),0)<NewLine>        state = np.concatenate([state, obser.reshape((1,imageShape[0],imageShape[1]))])<NewLine>        <NewLine>        memory.append({'state':state[:-1], 'action': action, 'state1':state[1:], <NewLine>            'reward':reward, 'done':done})<NewLine>        if len(memory) &gt; 32:<NewLine>            memory = memory[1:]<NewLine>        state = state[1:]<NewLine><NewLine>        if done:<NewLine>            break<NewLine></code></pre><NewLine><p>Here is my output:</p><NewLine><pre><code class=""lang-auto"">(py3) jtremblay@office:~/code/Personal-git/dqn$ python memory_issue.py <NewLine>[2017-03-07 12:19:41,554] Making new env: PongDeterministic-v3<NewLine>Pytorch: Memory usage: 115628 (kb)<NewLine>Pytorch: Memory usage: 131180 (kb)<NewLine>Pytorch: Memory usage: 132500 (kb)<NewLine>Pytorch: Memory usage: 133556 (kb)<NewLine>Pytorch: Memory usage: 135932 (kb)<NewLine>Pytorch: Memory usage: 137252 (kb)<NewLine>Pytorch: Memory usage: 137780 (kb)<NewLine>Pytorch: Memory usage: 138308 (kb)<NewLine>Pytorch: Memory usage: 139364 (kb)<NewLine>Pytorch: Memory usage: 139892 (kb)<NewLine>Pytorch: Memory usage: 140420 (kb)<NewLine>Pytorch: Memory usage: 140684 (kb)<NewLine>Pytorch: Memory usage: 141476 (kb)<NewLine>Pytorch: Memory usage: 141476 (kb)<NewLine>Pytorch: Memory usage: 142004 (kb)<NewLine>Pytorch: Memory usage: 142268 (kb)<NewLine>Pytorch: Memory usage: 143060 (kb)<NewLine>Pytorch: Memory usage: 143588 (kb)<NewLine>Pytorch: Memory usage: 143852 (kb)<NewLine>Pytorch: Memory usage: 143852 (kb)<NewLine>Pytorch: Memory usage: 144116 (kb)<NewLine>Pytorch: Memory usage: 144380 (kb)<NewLine>Pytorch: Memory usage: 144380 (kb)<NewLine>Pytorch: Memory usage: 144644 (kb)<NewLine>Pytorch: Memory usage: 144644 (kb)<NewLine>Numpy: Memory usage: 144908 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine>Numpy: Memory usage: 154932 (kb)<NewLine></code></pre><NewLine><p>I have also try to create a simple code that would replicate the behaviour:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import resource<NewLine>import numpy as np<NewLine><NewLine>import argparse<NewLine>parser = argparse.ArgumentParser(description='Memory issue')<NewLine>parser.add_argument('--numpy',    action='store_true')<NewLine>args = parser.parse_args()<NewLine><NewLine><NewLine>a = [None for _ in range(10)]<NewLine># print (a)<NewLine>j = 0<NewLine><NewLine>if args.numpy:<NewLine>    state = np.ones((3,200,200))<NewLine>    state = np.concatenate([state,np.random.rand(1,200,200)],0)<NewLine>else:<NewLine>    state = torch.ones((3,200,200))<NewLine>    state = torch.cat((state,torch.rand(1,200,200)),0)<NewLine><NewLine>for i in range(5000):<NewLine>    if i % 400 is 0:<NewLine>        if args.numpy:<NewLine>            print ('Numpy Memory usage: %s (kb)' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)<NewLine>        else:<NewLine>            print ('Torch Memory usage: %s (kb)' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)<NewLine>    if j &gt; 8:<NewLine>        j = -1<NewLine>    j+=1<NewLine><NewLine>    if args.numpy:<NewLine>        state = np.concatenate([state,np.random.rand(1,200,200)],0)<NewLine>    else:<NewLine>        state = torch.cat((state,torch.rand(1,200,200)),0)<NewLine>    a[j] = state[0:3]<NewLine>    state = state[1:]<NewLine></code></pre><NewLine><p>Here is the output:</p><NewLine><pre><code class=""lang-auto"">(py3) jtremblay@office:~/code/Personal-git/dqn$ python memory_simple.py<NewLine>Torch Memory usage: 82428 (kb)<NewLine>Torch Memory usage: 95536 (kb)<NewLine>Torch Memory usage: 96024 (kb)<NewLine>Torch Memory usage: 96024 (kb)<NewLine>Torch Memory usage: 96024 (kb)<NewLine>Torch Memory usage: 96088 (kb)<NewLine>Torch Memory usage: 96660 (kb)<NewLine>Torch Memory usage: 96660 (kb)<NewLine>Torch Memory usage: 96660 (kb)<NewLine>Torch Memory usage: 96660 (kb)<NewLine>Torch Memory usage: 96660 (kb)<NewLine>Torch Memory usage: 96660 (kb)<NewLine>Torch Memory usage: 96660 (kb)<NewLine>Torch Memory usage: 96660 (kb)<NewLine>Torch Memory usage: 96808 (kb)<NewLine>Torch Memory usage: 96808 (kb)<NewLine>Torch Memory usage: 96808 (kb)<NewLine>Torch Memory usage: 96808 (kb)<NewLine>Torch Memory usage: 96808 (kb)<NewLine>Torch Memory usage: 96808 (kb)<NewLine>(py3) jtremblay@office:~/code/Personal-git/dqn$ python memory_simple.py --numpy<NewLine>Numpy Memory usage: 81132 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine>Numpy Memory usage: 96064 (kb)<NewLine></code></pre><NewLine><p>Really I am not sure how this helps or not. As for the time taken storing numpy arrays and then translate them into torch tensor, I will run more experiments and report them here.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I ran quick experiment running 4 data point in total and I get better performance if I am storing torch tensor rather than numpy. The experiment included running q network on 20 episodes in a deterministic environment</p><NewLine><p>Torch: 2 min and 2 min 12 sec.<br/><NewLine>Numpy 3 min 34 sec. and 3 min 26 sec.</p><NewLine><p>The only difference in the code includes converting the numpy array intor a torch array in order to run inference and learning process. I hope this helps.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Wow, that’s weird. I just ran your test script and I can’t reproduce the issue. If I increase the number of loop iterations the memory usage stabilizes at 93420KB for me (and 94440KB for numpy). Maybe it only happens in Python 2, I’ll need to try</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Nope, can’t reproduce. On Python 2.7 it takes a bit longer to stabilize, but it stops at  98668KB.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>i think this is just the kernel or the default memory allocator being smart / doing some caching. Some allocators / kernels do this, and I’ve seen this in other settings (unrelated to this).</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, the worrying part for me was that <a class=""mention"" href=""/u/jtremblay"">@jtremblay</a> mentioned that it went OOM, so I thought it might be a leak and not just an allocator strategy. But I can’t reproduce it in any way</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for running the test. The smaller snippet of code also stabilize for bigger loops on my machine (96876 kb). But the longer snippet of code (the one which uses the gym environment) does not stabilize. I am not sure why.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think I might have been wrong, I ran a very long experiment and after a while the memory usage stabilize. I am sorry about this.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>I decided to run a longer test with the script storing states coming from the gym environment:</p><NewLine><pre><code class=""lang-python"">import resource<NewLine><NewLine>import argparse<NewLine>import gym<NewLine>import numpy as np<NewLine>from itertools import count<NewLine>from collections import namedtuple<NewLine>import os <NewLine><NewLine>import torch<NewLine>import random<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine>import torch.autograd as autograd<NewLine>from torch.autograd import Variable<NewLine>import torchvision.transforms as T<NewLine>import cv2<NewLine>import pickle<NewLine>import glob<NewLine>import time<NewLine>import subprocess<NewLine>from collections import namedtuple<NewLine><NewLine># Functions<NewLine>def ProcessState(state,torchOutput=True):<NewLine>    img = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)<NewLine>    img = cv2.resize(img, (imageShape[1],imageShape[0])).astype('float32')<NewLine>    if torchOutput:<NewLine>        img = torch.from_numpy(img)<NewLine>    img /= 255<NewLine>    img -= 0.5 <NewLine>    img *= 2<NewLine>    return img<NewLine><NewLine># Variables<NewLine>Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))<NewLine><NewLine>imageShape = (110,80)<NewLine>env = gym.make('PongDeterministic-v3')<NewLine>action = 0 <NewLine>memory = []<NewLine>reward = 0<NewLine>done = False <NewLine># Example with pytorch<NewLine>for i_episode in range(5000):<NewLine>    if i_episode % 500 is 0:<NewLine>        print (str(i_episode)+' Pytorch: Memory usage: %s (kb)' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)<NewLine>    <NewLine>    obser = env.reset()<NewLine>    obser = ProcessState(obser).clone()<NewLine><NewLine>    state = torch.ones((3,imageShape[0],imageShape[1])).clone()    <NewLine>    state = torch.cat((state.clone(),obser.view(1,imageShape[0],imageShape[1])),0).clone()<NewLine><NewLine>    for t in range(10000): <NewLine>        obser, reward, done, _ = env.step(0)<NewLine><NewLine>        #this is new observation getting process<NewLine>        obser = ProcessState(obser).clone()<NewLine><NewLine>        state = torch.cat((state.clone(),obser.view(1,imageShape[0],imageShape[1])),0).clone()<NewLine>        <NewLine>        memory.append({'state':state[:-1].clone(), 'action': action, 'state1':state[1:].clone(), <NewLine>            'reward':reward, 'done':done})<NewLine>        if len(memory) &gt; 32:<NewLine>            memory = memory[1:]<NewLine><NewLine>        state = state[1:].clone()<NewLine><NewLine>        if done:<NewLine>            break<NewLine># quit()<NewLine># memory = ReplayMemory(32)<NewLine># Numpy<NewLine>for i_episode in range(50000):<NewLine>    if i_episode % 500 is 0:<NewLine>        print (str(i_episode)+' Numpy: Memory usage: %s (kb)' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)<NewLine>    obser = env.reset()<NewLine>    obser = ProcessState(obser,False)<NewLine><NewLine>    state = np.zeros((3,imageShape[0],imageShape[1]))<NewLine>    state = np.concatenate([state, obser.reshape((1,imageShape[0],imageShape[1]))])<NewLine><NewLine>    for t in range(10000): <NewLine>        obser, reward, done, _ = env.step(0)<NewLine><NewLine>        #this is new observation getting process<NewLine>        obser = ProcessState(obser,False)<NewLine><NewLine>        # state = torch.cat((state,obser.view(1,imageShape[0],imageShape[1])),0)<NewLine>        state = np.concatenate([state, obser.reshape((1,imageShape[0],imageShape[1]))])<NewLine>        <NewLine>        memory.append({'state':state[:-1], 'action': action, 'state1':state[1:], <NewLine>            'reward':reward, 'done':done})<NewLine>        if len(memory) &gt; 32:<NewLine>            memory = memory[1:]<NewLine>        state = state[1:]<NewLine><NewLine>        if done:<NewLine>            break<NewLine></code></pre><NewLine><p>I am still getting issues for storing pytorch tensor. For some reason a reference is kept to some tensors and they do not get clean out of the memory. Here is the output I got (The pytorch test ran for 6 hours). I stopped the numpy after 3000 episodes as it showed stabilities.</p><NewLine><pre><code class=""lang-auto"">(py3) jtremblay@office:~/code/Personal-git/dqn$ python memory_issue.py <NewLine>[2017-03-08 09:23:04,502] Making new env: PongDeterministic-v3<NewLine>0 Pytorch: Memory usage: 116916 (kb)<NewLine>500 Pytorch: Memory usage: 159720 (kb)<NewLine>1000 Pytorch: Memory usage: 172392 (kb)<NewLine>1500 Pytorch: Memory usage: 188232 (kb)<NewLine>2000 Pytorch: Memory usage: 204864 (kb)<NewLine>2500 Pytorch: Memory usage: 221232 (kb)<NewLine>3000 Pytorch: Memory usage: 236540 (kb)<NewLine>3500 Pytorch: Memory usage: 252908 (kb)<NewLine>4000 Pytorch: Memory usage: 268744 (kb)<NewLine>4500 Pytorch: Memory usage: 282472 (kb)<NewLine>(py3) jtremblay@office:~/code/Personal-git/dqn$ python memory_issue.py <NewLine>[2017-03-08 11:30:58,943] Making new env: PongDeterministic-v3<NewLine>0 Numpy: Memory usage: 116532 (kb)<NewLine>500 Numpy: Memory usage: 129508 (kb)<NewLine>1000 Numpy: Memory usage: 129508 (kb)<NewLine>1500 Numpy: Memory usage: 129508 (kb)<NewLine>2000 Numpy: Memory usage: 129508 (kb)<NewLine>2500 Numpy: Memory usage: 129508 (kb)<NewLine>3000 Numpy: Memory usage: 129508 (kb)<NewLine></code></pre><NewLine><p>There might be something that I do not understand while manipulating the tensors, in this context I am using torch.cat, clone and from_numpy. Using the same code with numpy arrays does create any instabilities with the memory. I thought using clone everywhere I could would force to freed any reference to previous tensors. If I do not use clone this is the result usage I get after 500 episodes:</p><NewLine><pre><code class=""lang-auto"">(py3) jtremblay@office:~/code/Personal-git/dqn$ python memory_issue.py <NewLine>[2017-03-08 12:51:20,207] Making new env: PongDeterministic-v3<NewLine>0 Pytorch: Memory usage: 115008 (kb)<NewLine>500 Pytorch: Memory usage: 492140 (kb)<NewLine></code></pre><NewLine><p>Without the clone call, the memory usage is quite large. Also I never invoke the copy function in the numpy part. I am extremely confused by this behaviour.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok, I’ve found and fixed the problem! Thanks for the report, the patch should be in master soon.</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>That is great <img alt="":stuck_out_tongue:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/stuck_out_tongue.png?v=5"" title="":stuck_out_tongue:""/> Thank you for double checking everything!</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>Quick question <a class=""mention"" href=""/u/apaszke"">@apaszke</a> and <a class=""mention"" href=""/u/jtremblay"">@jtremblay</a> :<br/><NewLine>Did you implement DQN with the same memory configuration as stated in the original paper (1 Mio. transitions)?</p><NewLine><p>I am currently facing a similar issue, where python starts to use up gigabytes of memory in an end-to-end learning setting an the python process gets killed at around 114 thousand transitions in memory.<br/><NewLine>I’m working on a MacbookPro with 16 gigs of RAM.</p><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/denizs"">@denizs</a><br/><NewLine>You can use this approach from the OpenAI baseline implementation that greatly optimizes memory requirements<br/><NewLine><a class=""onebox"" href=""https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers_deprecated.py#L152"" rel=""nofollow noopener"" target=""_blank"">https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers_deprecated.py#L152</a></p><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>Awesome thanks <img alt="":slight_smile:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=5"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jtremblay; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jtremblay; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/jtremblay; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/jtremblay; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/jtremblay; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/jtremblay; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/jtremblay; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/jtremblay; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/denizs; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/j.laute; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/denizs; <NewLine> ,"REPLY_DATE 1: March 6, 2017, 10:26pm; <NewLine> REPLY_DATE 2: March 6, 2017, 11:49pm; <NewLine> REPLY_DATE 3: March 7, 2017, 11:01am; <NewLine> REPLY_DATE 4: March 7, 2017,  5:21pm; <NewLine> REPLY_DATE 5: March 7, 2017,  8:13pm; <NewLine> REPLY_DATE 6: March 7, 2017,  8:29pm; <NewLine> REPLY_DATE 7: March 7, 2017,  8:58pm; <NewLine> REPLY_DATE 8: March 7, 2017,  9:07pm; <NewLine> REPLY_DATE 9: March 7, 2017,  9:34pm; <NewLine> REPLY_DATE 10: March 7, 2017,  9:45pm; <NewLine> REPLY_DATE 11: March 7, 2017,  9:53pm; <NewLine> REPLY_DATE 12: March 7, 2017, 10:07pm; <NewLine> REPLY_DATE 13: March 7, 2017, 10:12pm; <NewLine> REPLY_DATE 14: March 8, 2017,  9:05pm; <NewLine> REPLY_DATE 15: March 11, 2017, 11:26am; <NewLine> REPLY_DATE 16: March 13, 2017,  6:14pm; <NewLine> REPLY_DATE 17: July 18, 2017, 10:35pm; <NewLine> REPLY_DATE 18: July 19, 2017,  8:28am; <NewLine> REPLY_DATE 19: July 19, 2017,  8:41am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: 1 Like; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: 3 Likes; <NewLine> REPLY 16 LIKES: 1 Like; <NewLine> REPLY 17 LIKES: ; <NewLine> REPLY 18 LIKES: 1 Like; <NewLine> REPLY 19 LIKES: ; <NewLine> 
4798,Feature Add: Noisy Networks for Exploration,2017-07-11T18:27:33.168Z,3,2125,"<div class=""post"" itemprop=""articleBody""><NewLine><p>This is a request for a new feature in PyTorch, a linear layer with noisy weights as created in <a href=""https://arxiv.org/abs/1706.10295"" rel=""nofollow noopener"">Noisy Networks for Exploration</a>.  The feature is useful for exploration in RL problems.  I’ve already created an <a href=""https://github.com/pytorch/pytorch/issues/2024"" rel=""nofollow noopener"">issue</a> at the PyTorch Github, but it seems to have fallen through the cracks over there.  I’ve created a basic implementation of the layer at a link included therein.  It needs some tweaking and improvement, but I want to get the okay to go ahead with it before I put any more time in.</p><NewLine></div>",https://discuss.pytorch.org/u/jvmancuso,,jvmancuso,"July 11, 2017,  6:27pm",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems to be possible to create a noisyModule from any module, just looping on the parameters:</p><NewLine><pre><code class=""lang-python"">class NoisyModule(nn.Module):<NewLine>    def __init__(self, module):<NewLine>        super(NoisyModule, self).__init__()<NewLine>        self.module = module<NewLine>        for param in self.module.parameters():<NewLine>            sigma_param = Parameter(torch.Tensor(param.size())<NewLine>            epsilon_param = Parameter(torch.Tensor(param.size())<NewLine>            self.register_parameter(""sigma_""+param.name, sigma_param)<NewLine>            self.register_buffer(""epsilon_""+param.name, epsilon_param)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>How’s that noisy network been doing in your tests with RL?..</p><NewLine><p>Another change U can do to get some that noisy effect if use the randomized ReLu activation, “RReLu”, which performs quite well on Atari when I tried and performed better than Elu that a lot are using. Not only learned faster but lead to more robust and stable model</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Also, scaling the noise in proportion to variance it causes in outputs like in OpenAI version might help as well:<br/><NewLine><a class=""onebox"" href=""https://arxiv.org/abs/1706.01905"" rel=""nofollow noopener"" target=""_blank"">https://arxiv.org/abs/1706.01905</a></p><NewLine><p>^look at paragraph titled “Adaptive Noise Scaling” in Section 3</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>This isn’t quite so trivial.  First, we need to be able to turn off the noise for testing.  Second, we need to be able to choose when to resample the noise tensors, since for NoisyNet-A3C it’s supposed to be resampled after every k-step policy rollout.  But yes, it should be possible to implement for other kinds of layers, although defining the forward pass might be challenging for some.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I hadn’t heard of this paper, thanks!  One convenient feature of the noisy networks paper is that you optimize the variance parameters with the other parameters of the network, throwing away the need for an adaptive scaling schedule like that.  Looking forward to seeing a rigorous comparison of all these methods.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dgriff; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ethancaballero; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jvmancuso; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/jvmancuso; <NewLine> ,"REPLY_DATE 1: July 11, 2017, 10:34pm; <NewLine> REPLY_DATE 2: July 12, 2017,  4:42am; <NewLine> REPLY_DATE 3: July 12, 2017,  8:21am; <NewLine> REPLY_DATE 4: July 14, 2017,  2:25pm; <NewLine> REPLY_DATE 5: July 14, 2017,  2:02pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
1033,Continuous action A3C,2017-03-14T00:33:51.559Z,5,6697,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I wonder if anyone has got A3C working with continuous actions? I guessed it would be a good idea to ask first before trying to do it, as there’s probably a good reason why no one’s got it to work yet?</p><NewLine><p>I’m working on modifying, <a class=""mention"" href=""/u/ilya_kostrikov"">@Ilya_Kostrikov</a>, implementation,</p><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""32""/><NewLine><a href=""https://github.com/ikostrikov/pytorch-a3c"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""400"" src=""https://avatars1.githubusercontent.com/u/1212494?s=400&amp;v=4"" width=""400""/><NewLine><h3><a href=""https://github.com/ikostrikov/pytorch-a3c"" rel=""nofollow noopener"" target=""_blank"">ikostrikov/pytorch-a3c</a></h3><NewLine><p>PyTorch implementation of Asynchronous Advantage Actor Critic (A3C) from ""Asynchronous Methods for Deep Reinforcement Learning"". - ikostrikov/pytorch-a3c</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>So for example Open AIs pendulum, as only got a state/observation vector of 3, so there’s no need for any conv’s in the Actor-Critic module, basically I’m trying,</p><NewLine><pre><code class=""lang-auto"">lstm_out = 256<NewLine>enc_in = 3 # for pendulum<NewLine>enc_hidden = 200<NewLine>enc_out = lstm_out<NewLine><NewLine>class ActorCritic(nn.Module):<NewLine><NewLine>    def __init__(self , lstm_in ):<NewLine>        super(ActorCritic, self).__init__( )  <NewLine>        self.fc_enc_in  = nn.Linear(enc_in,enc_hidden) # enc_input_layer<NewLine>        self.fc_enc_out  = nn.Linear(enc_hidden,enc_out) # enc_output_layer             <NewLine>        self.lstm = nn.LSTMCell(lstm_in, lstm_out)<NewLine>        self.actor_mu = nn.Linear(lstm_out, 1)<NewLine>        self.actor_sigma = nn.Linear(lstm_out, 1)<NewLine>        self.critic_linear = nn.Linear(lstm_out, 1)<NewLine>        self.train()<NewLine><NewLine>    def forward(self, inputs):<NewLine>        <NewLine>        x, (hx, cx) = inputs<NewLine><NewLine>        x = F.relu(self.fc_enc_in(x))<NewLine>        x = self.fc_enc_out(x)<NewLine><NewLine>        hx, cx = self.lstm(x, (hx, cx))<NewLine>        x = hx<NewLine><NewLine>        return self.critic_linear(x), self.actor_mu(x), self.actor_sigma(x), (hx, cx)<NewLine></code></pre><NewLine><p>The initialisation code in main.py, then looks like,</p><NewLine><pre><code class=""lang-auto"">env = gym.envs.make(""Pendulum-v0"")<NewLine>lstm_in = 3    <NewLine>global_model = ActorCritic( lstm_in )<NewLine>global_model.share_memory()<NewLine>local_model = ActorCritic( lstm_in )<NewLine></code></pre><NewLine><p>And the training code is where I get confused (as usual) ???,</p><NewLine><pre><code class=""lang-auto"">env = gym.envs.make(""Pendulum-v0"")<NewLine>s0 = env.reset()<NewLine>done = True<NewLine>state = torch.from_numpy(s0).float().unsqueeze(0) <NewLine>value, mu, sigma, (hx, cx) = local_model((Variable(state), (hx, cx)))<NewLine><NewLine>#mu = mu.clamp(-1, 1) # constain to sensible values <NewLine>Softplus=nn.Softplus()     <NewLine>sigma = Softplus(sigma + 1e-5) # constrain to sensible values<NewLine>normal_dist = torch.normal(mu, sigma) <NewLine><NewLine>prob = normal_dist<NewLine>log_prob = torch.log(prob)<NewLine>entropy = 0.5 * (torch.log(2. * np.pi * sigma ) + 1.)<NewLine><NewLine>##--------------------------------------------------------------<NewLine># TODO Calculate the Gaussian neg log-likelihood, log(1/sqrt(2sigma^2pi)) - (x - mu)^2/(2*sigma^2)<NewLine># See - https://www.statlect.com/fundamentals-of-statistics/normal-distribution-maximum-likelihood<NewLine>#<NewLine>log_prob = torch.log(torch.pow( torch.sqrt(2. * sigma * np.pi) , -1)) - (normal_dist - mu)*(normal_dist - mu)*torch.pow((2. * sigma), -1)<NewLine>##--------------------------------------------------------------<NewLine><NewLine>action = Variable( prob.data )<NewLine><NewLine>#action=[0,]<NewLine>state, reward, done, _ = env.step([action.data[0][0]])<NewLine><NewLine></code></pre><NewLine><p>References,</p><NewLine><p>Reference - Deepmind A3C’s paper, <a href=""https://arxiv.org/pdf/1602.01783.pdf"" rel=""nofollow noopener"">https://arxiv.org/pdf/1602.01783.pdf</a><br/><NewLine>Section 9 - Continuous Action Control Using the MuJoCo Physics Simulator</p><NewLine><p>Here’s a diagram of the algorithm, from  <a href=""https://github.com/deeplearninc/relaax#distributed-a3c-architecture-with-continuous-actions"" rel=""nofollow noopener"">https://github.com/deeplearninc/relaax#distributed-a3c-architecture-with-continuous-actions</a></p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/39765a61a322e1e5ff85fa5348ae7322e4ffdb57"" href=""https://discuss.pytorch.org/uploads/default/original/1X/39765a61a322e1e5ff85fa5348ae7322e4ffdb57.png"" title=""image.png""><img data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/1X/39765a61a322e1e5ff85fa5348ae7322e4ffdb57_2_10x10.png"" height=""499"" src=""https://discuss.pytorch.org/uploads/default/optimized/1X/39765a61a322e1e5ff85fa5348ae7322e4ffdb57_2_690x499.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/1X/39765a61a322e1e5ff85fa5348ae7322e4ffdb57_2_690x499.png, https://discuss.pytorch.org/uploads/default/optimized/1X/39765a61a322e1e5ff85fa5348ae7322e4ffdb57_2_1035x748.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/1X/39765a61a322e1e5ff85fa5348ae7322e4ffdb57_2_1380x998.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">1547×1120 89.1 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/AjayTalati,(Ajay Talati),AjayTalati,"March 18, 2017,  4:20pm",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>How do you do a logarithm, in PyTorch?</p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; nnlog = nn.Log()<NewLine>Traceback (most recent call last):<NewLine>  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;<NewLine>AttributeError: module 'torch.nn' has no attribute 'Log'<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Many elementwise operations, especially traditional mathematical ones, are in the <code>torch</code> namespace (so <code>torch.log(var)</code> or <code>var.log()</code> here). If they don’t give an error when applied to a <code>Variable</code>, they’re differentiable. In general you don’t need to instantiate modules like <code>Softplus</code>; the versions in <code>nn</code> are provided to make it easier to use <code>nn.Sequential</code> and all parameterless modules in <code>nn</code> have a simpler functional equivalent in <code>F</code>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jekbradbury"">@jekbradbury</a> thank you very much !!!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well, I came back to this after a few days, and I’m still stuck. So any advice will make you a genius in my view?</p><NewLine><p>Here’s a post of my code as simple as I could make it as a big blob,</p><NewLine><aside class=""onebox githubgist""><NewLine><header class=""source""><NewLine><a href=""https://gist.github.com/AjayTalati/184fec867380f6fa22b9aa0951143dec"" rel=""nofollow noopener"" target=""_blank"">gist.github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://gist.github.com/AjayTalati/184fec867380f6fa22b9aa0951143dec"" rel=""nofollow noopener"" target=""_blank"">https://gist.github.com/AjayTalati/184fec867380f6fa22b9aa0951143dec</a></h4><NewLine><h5>main_single.py</h5><NewLine><pre><code class=""Python""># Reference - Deepmind A3C's paper, https://arxiv.org/pdf/1602.01783.pdf<NewLine># See section 9 - Continuous Action Control Using the MuJoCo Physics Simulator<NewLine>#<NewLine># Code based on https://github.com/pfre00/a3c<NewLine><NewLine>import argparse<NewLine>import torch<NewLine>import torch.multiprocessing as mp<NewLine>import gym<NewLine>import torch.nn as nn</code></pre><NewLine>This file has been truncated. <a href=""https://gist.github.com/AjayTalati/184fec867380f6fa22b9aa0951143dec"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine><p><NewLine></p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>I keep getting this error,</p><NewLine><pre><code class=""lang-auto"">File ""main_single.py"", line 174, in &lt;module&gt;<NewLine>value_loss = value_loss + advantage.pow(2)<NewLine>AttributeError: 'numpy.ndarray' object has no attribute 'pow'<NewLine></code></pre><NewLine><p>I don’t understand why <code>advantage</code> has become a <code>numpy.array</code> instead of a <code>torch.tensor</code> - it never occurred with the discrete action implementation?</p><NewLine><p>Any ideas what I’ve got wrong?</p><NewLine><p>Thanks a lot for your help,</p><NewLine><p>Best,</p><NewLine><p>Ajay</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>reward is probably returned from gym as a numpy object (I guess a scalar?) so I think you have to convert it?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, <a class=""mention"" href=""/u/jekbradbury"">@jekbradbury</a> thanks a lot!</p><NewLine><p>I tired conversion to a <code>torch tensor</code> but couldn’t get it to work - I’ll try again thought?</p><NewLine><p>What seems to help a little is, changing the code to</p><NewLine><pre><code class=""lang-auto"">    for t in reversed(range(len(rewards))):<NewLine>        R = torch.mul(R, args.gamma)  <NewLine>        R = torch.add(R, rewards[t])<NewLine>        advantage = R - values[t]<NewLine>        value_loss = value_loss + advantage.pow(2)<NewLine></code></pre><NewLine><p>Now I get the error,</p><NewLine><pre><code class=""lang-auto"">  File ""main_single.py"", line 185, in &lt;module&gt;<NewLine>    (policy_loss + 0.5 * value_loss).backward()<NewLine>  File ""/home/ajay/anaconda3/envs/pyphi/lib/python3.6/site-packages/torch/autograd/variable.py"", line 158, in backward<NewLine>    self._execution_engine.run_backward((self,), (gradient,), retain_variables)<NewLine>  File ""/home/ajay/anaconda3/envs/pyphi/lib/python3.6/site-packages/torch/autograd/stochastic_function.py"", line 13, in _do_backward<NewLine>    raise RuntimeError(""differentiating stochastic functions requires ""<NewLine>RuntimeError: differentiating stochastic functions requires providing a reward<NewLine></code></pre><NewLine><p>Which is perhaps a little bit better than before? I think <code>gym</code> environments are a bit strange?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>You forgot to call <code>.reinforce</code> on some of the stochastic outputs.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/apaszke"">@apaszke</a> Fan Q</p><NewLine><p>… this is padding to make this post 20 characters or more</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ajaytalati"">@AjayTalati</a>, It’s really nice to see someone also work on this, I’ve also implemented  continuous a3c and got some result on mujoco envs, you can check this out: <a href=""https://github.com/andrewliao11/pytorch-a3c-mujoco"" rel=""nofollow noopener"">https://github.com/andrewliao11/pytorch-a3c-mujoco</a></p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/andrewliao11"">@andrewliao11</a>,</p><NewLine><p>great stuff !!! That’s really cool, nice videos <img alt="":smile:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/smile.png?v=5"" title="":smile:""/></p><NewLine><p>I never managed to get it working very well, (I tried it on non-mujoco stuff), so went back to experimenting with the discrete actions version. Do you plan on experimenting with shared RMSProp?</p><NewLine><p>A3C is a great tool - you can apply it to a lot of stuff - it should be really helpful to you in the future!</p><NewLine><p>Kind regards,</p><NewLine><p>Ajay</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ll try ShareRMSProp in the near future!<br/><NewLine>However, I think continuous a3c is a little unstable (you can refer to the learning curve <a href=""https://github.com/andrewliao11/pytorch-a3c-mujoco/blob/master/asset/InvertedPendulum-v1.a3c.log.png"" rel=""nofollow noopener"">here</a>).<br/><NewLine>The problem might be the insufficient threads, which causes the async update fails (unable to reduce the correlation btn data)</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for sharing your code!<br/><NewLine>It seems that you keep exploring in your <code>test.py</code>, at line 79:</p><NewLine><pre><code class=""lang-auto"">action = (mu + sigma_sq.sqrt()*Variable(eps)).data<NewLine></code></pre><NewLine><p>But don’t you should just exploit with <code>action = mu</code>? It may explain the instability displayed by your learning curves</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/AjayTalati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jekbradbury; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/AjayTalati; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/AjayTalati; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/jekbradbury; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/AjayTalati; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/AjayTalati; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/andrewliao11; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/AjayTalati; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/andrewliao11; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> ,"REPLY_DATE 1: March 14, 2017,  8:02am; <NewLine> REPLY_DATE 2: March 14, 2017,  8:15am; <NewLine> REPLY_DATE 3: March 14, 2017,  8:19am; <NewLine> REPLY_DATE 4: March 18, 2017,  5:18pm; <NewLine> REPLY_DATE 5: March 18, 2017,  6:42pm; <NewLine> REPLY_DATE 6: March 18, 2017,  8:25pm; <NewLine> REPLY_DATE 7: March 18, 2017, 10:00pm; <NewLine> REPLY_DATE 8: March 19, 2017,  9:55pm; <NewLine> REPLY_DATE 9: April 14, 2017,  5:07pm; <NewLine> REPLY_DATE 10: April 14, 2017,  6:39pm; <NewLine> REPLY_DATE 11: April 15, 2017,  2:27am; <NewLine> REPLY_DATE 12: July 9, 2017,  7:40pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: 2 Likes; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: 1 Like; <NewLine> 
3592,Multithreading Segmentation fault,2017-05-31T18:22:20.394Z,5,2097,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to implement a multi-thread and single-GPU Actor-Critic training, where 1 thread is doing simulation and 1 thread is doing training.<br/><NewLine>The two threads will share a replay buffer (on CPU) and a model (on GPU).</p><NewLine><p>An example code is here in the gist <a href=""https://gist.github.com/zihangdai/fc8f76fbb8a0f6323a6b31e6d98ceb50"" rel=""nofollow noopener"">https://gist.github.com/zihangdai/fc8f76fbb8a0f6323a6b31e6d98ceb50</a></p><NewLine><p>If you run the code, it will occasionally give Segmentation fault. But real tasks, the Segmentation fault happens must faster.</p><NewLine><p>When I use the <code>net_lock</code> to lock the model, i.e., only one thread can use the model at a time, the problem goes away. However, this will be obviously less efficient. My hope is that both threads can use the model at the same time, and I don’t worry about the undetermined behavior problem for now.</p><NewLine><p>Any idea why this is happening?</p><NewLine></div>",https://discuss.pytorch.org/u/Zihang_Dai,(Zihang Dai),Zihang_Dai,"May 31, 2017,  6:22pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Some update:</p><NewLine><p>It seems the problem is that the forward and backward computation of Pytorch is not thread-safe. I’m not sure whether this is the problem introduced by Autograd. Can anyone confirm whether PyTorch is thread-safe or not?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>it should be thread-safe, and if it’s not, we will fix it.</p><NewLine><p>I’ve tested your example against the master branch of pytorch, and it did not produce segfaults.<br/><NewLine>I wonder if we fixed the issue you are seeing already.<br/><NewLine>Can you give the master branch a try to confirm?<br/><NewLine></p><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""32""/><NewLine><a href=""https://github.com/pytorch/pytorch#from-source"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""400"" src=""https://avatars2.githubusercontent.com/u/21003710?s=400&amp;v=4"" width=""400""/><NewLine><h3><a href=""https://github.com/pytorch/pytorch#from-source"" target=""_blank"">pytorch/pytorch</a></h3><NewLine><p>Tensors and Dynamic neural networks in Python with strong GPU acceleration - pytorch/pytorch</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Soumith, I just saw your reply and went to try my earlier script.</p><NewLine><p>I ran two versions of the actual training code, one with a thread lock on the model and the other without the lock. It turned out the one with lock is stilling running now (&gt;1 hour), and the one without lock gave Segmentation fault sooner or later (I tried more than once).</p><NewLine><p>I also tried the script I put in the gist and I also got the error.<br/><NewLine>To recover the error, I usually run the script for 20 times by running <code>for i in `seq 0 20`; do python multithread.py; done</code><br/><NewLine>(It’s better to put the shell script in a .sh file in case you wanna kill it).</p><NewLine><p>BTW, it happens with both GPU and CPU (make the network smaller so that the script is faster for running 20 runs).</p><NewLine><p>For your information, my environment is python2.7 with cuda 8.0.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/zihang_dai"">@Zihang_Dai</a> but what is your <code>print(torch.__version__)</code>?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I installed from the master branch and version is 0.1.12+1572173</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>i could reproduce this, and I opened an issue here: <a href=""https://discuss.pytorch.org/t/multithreading-segmentation-fault/3592/3"">Multithreading Segmentation fault</a><br/><NewLine>please follow along, and thanks a lot for the bug report.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Zihang_Dai; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Zihang_Dai; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Zihang_Dai; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/smth; <NewLine> ,"REPLY_DATE 1: June 3, 2017, 12:11am; <NewLine> REPLY_DATE 2: June 16, 2017,  1:37pm; <NewLine> REPLY_DATE 3: June 21, 2017,  6:13am; <NewLine> REPLY_DATE 4: June 21, 2017,  2:05pm; <NewLine> REPLY_DATE 5: June 21, 2017,  4:50pm; <NewLine> REPLY_DATE 6: June 21, 2017,  6:37pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
4114,Pointer Networks with RL - problem with action.reinforce(r),2017-06-18T19:46:07.783Z,0,1132,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m working on an implementation of <a href=""https://arxiv.org/abs/1611.09940"" rel=""nofollow noopener"">Neural Combinatorial Optimization with RL</a>, and I got a bit stuck on the reinforce update for the pointer network.</p><NewLine><p>Essentially, I’m using <code>torch.multinomial</code> to select from the input at each step of decoding to construct the output, which is a permutation of the inputs (FYI I call the elements from the input that the pointer network selects the “actions”). Then, I run the “actions” through a reward function, and call <code>action.reinforce(r)</code> for each “action”. However, what I think is happening is that since the actions are not directly the result of the call to <code>torch.multinomial</code> (the indices I used to select the actions from the input are), I’m getting the error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: reinforce() can be only called on outputs of stochastic functions<NewLine></code></pre><NewLine><p>Is there any way to get around this so I can still use pytorch’s <code>reinforce</code> method? Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/pemami4911,(Patrick E ),pemami4911,"June 18, 2017,  7:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Update: I ditched the <code>reinforce</code> method and I’m now simply computing the loss as <code>logprobs * (reward - baseline)</code>, averaged over batch size <code>B</code>, where <code>logprobs</code> is the sum of the log probabilities of the outputs of the pointer network. The reward is computed based on the actions, not the action indices, which are the outputs of <code>torch.multinomial</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Maybe helpful. I had a similar problem.</p><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/1311"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/1311"" rel=""nofollow noopener"" target=""_blank"">Stochasticity after reshaping operations</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2017-04-20"" data-format=""ll"" data-time=""19:15:14"" data-timezone=""UTC"">07:15PM - 20 Apr 17 UTC</span><NewLine></div><NewLine><div class=""date""><NewLine>          closed <span class=""discourse-local-date"" data-date=""2018-02-20"" data-format=""ll"" data-time=""21:04:11"" data-timezone=""UTC"">09:04PM - 20 Feb 18 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/khanhptnk"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""khanhptnk"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars3.githubusercontent.com/u/1854828?v=4"" width=""20""/><NewLine>          khanhptnk<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">I would love to see PyTorch maintaining Stochasticity of the output after reshaping. Right now, if you concat or just .view(-1)...</p><NewLine></div><NewLine><div class=""labels""><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pemami4911; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/khanhptnk; <NewLine> ,"REPLY_DATE 1: June 19, 2017,  8:58pm; <NewLine> REPLY_DATE 2: June 19, 2017, 11:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
4107,A3C error at lstm layer,2017-06-18T15:47:34.226Z,0,728,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey guys, need some help here. I’m trying to do an a3c agent and my implementation is based on this one[1].<br/><NewLine>My model and inputs has the same shape as [1]:</p><NewLine><p>My <code>__init__</code>:</p><NewLine><pre><code># num_inputs = 4<NewLine>self.conv1 = nn.Conv2d(num_inputs, 32, 5, stride=1, padding=2)<NewLine>self.conv2 = nn.Conv2d(32, 32, 5, stride=1, padding=1)<NewLine>self.conv3 = nn.Conv2d(32, 64, 4, stride=1, padding=1)<NewLine>self.conv4 = nn.Conv2d(64, 64, 3, stride=1, padding=1)<NewLine><NewLine>num_outputs = action_space.n # which is 4<NewLine><NewLine>self.lstm = nn.LSTMCell(1024, 512)<NewLine>self.critic_linear = nn.Linear(512, 1)<NewLine>self.actor_linear = nn.Linear(512, num_outputs)<NewLine></code></pre><NewLine><p>And my <code>forward</code>:</p><NewLine><pre><code>inputs, (hx, cx) = inputs<NewLine># inputs is something with shape [1, 4, 80, 80]<NewLine>x = F.relu(F.max_pool2d(self.conv1(inputs), kernel_size=2, stride=2))<NewLine>x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))<NewLine>x = F.relu(F.max_pool2d(self.conv3(x), kernel_size=2, stride=2))<NewLine>x = F.relu(F.max_pool2d(self.conv4(x), kernel_size=2, stride=2))<NewLine><NewLine>x = x.view(x.size(0), -1)<NewLine><NewLine>hx, cx = self.lstm(x, (hx, cx))<NewLine><NewLine>x = hx<NewLine><NewLine>return self.critic_linear(x), self.actor_linear(x), (hx, cx)<NewLine></code></pre><NewLine><p>I’m getting this error at  <code>hx, cx = self.lstm(x, (hx, cx))</code>:</p><NewLine><p><code>RuntimeError: size mismatch, m1: [1 x 256], m2: [512 x 2048] at /py/conda-bld/pytorch_1490980628440/work/torch/lib/TH/generic/THTensorMath.c:1229</code></p><NewLine><p>[1] <a href=""https://github.com/dgriff777/rl_a3c_pytorch/blob/master/model.py"" rel=""nofollow noopener"">https://github.com/dgriff777/rl_a3c_pytorch/blob/master/model.py</a></p><NewLine></div>",https://discuss.pytorch.org/u/darleybarreto,(Darley Barreto),darleybarreto,"June 18, 2017,  3:47pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>hey from looks of it when you are creating the hx, and cx Variables you have set the size incorrectly</p><NewLine><p>u will want something like this:</p><NewLine><p>cx = Variable(torch.zeros(1, 512))<br/><NewLine>hx = Variable(torch.zeros(1, 512))</p><NewLine><p>As I see you are using my repo as reference it would be those lines in the <a href=""http://train.py"" rel=""nofollow noopener"">train.py</a> and <a href=""http://test.py"" rel=""nofollow noopener"">test.py</a> file</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>You’re wright, I’m following three or so implementations and the error was a silly thing I did. Btw, your implementation is very good. Thank you.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>No problem and thank you. Yeah I had a lot of fun with it, its truly a great algorithm and one of my main goals was to see its true potential if implemented in which I thought was most optimal way while still keeping the generalized nature of it and it showed great potential.</p><NewLine><p>Small tip so you don’t waste time like I did worrying about the multiprocessing updating of parameters with out locking is with locks the acquiring and releasing locks just take too much time and slow it up too much that it negates any potential benefit. Trust in the Hogwild training! Its far superior.</p><NewLine><p>Anyways have fun and good luck!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dgriff; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/darleybarreto; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/dgriff; <NewLine> ,"REPLY_DATE 1: June 18, 2017,  6:09pm; <NewLine> REPLY_DATE 2: June 18, 2017,  7:11pm; <NewLine> REPLY_DATE 3: June 18, 2017,  8:07pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
3933,[resolved] Actor Critic with a large amount of possible actions,2017-06-11T21:59:06.227Z,0,932,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m working on a hexapod (18x servos) that teaches itself how to walk. I’ve gotten pretty familiar with actor critic implementations and how it works, but just about everything I’ve seen applies to a situation with a small set of possible actions (ie. left, right, forward). In my situation, there are 18 servos with at least 100 different possible values for each, so that would be a massive output on the network (10^36).</p><NewLine><p>My ideal output would be just one vector of 18 servo values. I know that the probability distribution of the actions is crucial, but what if I were to remove it? What if I took out the softmax from the last layer and just use the 18x output values? Would this work? If not, what other solutions could I try? Should I move to a different algo?</p><NewLine></div>",https://discuss.pytorch.org/u/traw1234,(Tug Witt),traw1234,"June 12, 2017, 10:21pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>could you first assign say 50 specific types movements for the hexapod then use that as possible actions</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Someone recommended that to me earlier on and I thought about. The issue I have with doing that is because my goal for the project was for it to learn the servo values on its own. I thought that it might end up doing better than a program written by hand would. If I can’t find out any solutions, I may end up doing that but ideally not.</p><NewLine><p><strong>EDIT</strong><br/><NewLine>So the robot will only end up making a few actions (turn left or right, move forward or backward). Ideally turning would be synchronous with the movements but that might be a later problem. What if I were to create an output of 4 18x1 vectors for each movement? Would it end up learning that each one is a specific movement? Or is this too bold to assume?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If I understand correctly, instead of learning the full 18-dimensional categorical distribution, you could learn an 18-dimensional Gaussian distribution by learning the mean and the diagonal of the covariance matrix instead. It seems like you would need to round the output of the Gaussian to the nearest valid discrete values but that should work.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mjacar"">@mjacar</a> So instead of choosing an action from the distribution, I would find the covariance matrix of the distribution and use that?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Right now your model outputs a softmax that represents a categorical distribution. Instead of doing that, have your model output the mean and standard deviation of a Gaussian that you can then sample from to choose your action.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>On the A3C paper, section 9 has a great description. This seems to work.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dgriff; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/traw1234; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mjacar; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/traw1234; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mjacar; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/traw1234; <NewLine> ,"REPLY_DATE 1: June 11, 2017, 10:28pm; <NewLine> REPLY_DATE 2: June 11, 2017, 11:06pm; <NewLine> REPLY_DATE 3: June 12, 2017,  2:09pm; <NewLine> REPLY_DATE 4: June 12, 2017,  3:21pm; <NewLine> REPLY_DATE 5: June 12, 2017,  3:34pm; <NewLine> REPLY_DATE 6: June 12, 2017, 10:20pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 2 Likes; <NewLine> REPLY 6 LIKES: ; <NewLine> 
3829,[announcement] Gridworld reinforcement learning tutorial,2017-06-07T22:18:12.777Z,1,1932,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I took the <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py"" rel=""nofollow noopener"">actor-critic example from the examples</a> and turned it into a tutorial with no gym dependencies, simulations running directly in the notebook. I’d like to know if I explained anything poorly or incorrectly or not enough, especially the parts about policy gradients.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/spro/practical-pytorch/blob/master/reinforce-gridworld/reinforce-gridworld.ipynb"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/spro/practical-pytorch/blob/master/reinforce-gridworld/reinforce-gridworld.ipynb"" rel=""nofollow noopener"" target=""_blank"">spro/practical-pytorch/blob/master/reinforce-gridworld/reinforce-gridworld.ipynb</a></h4><NewLine><pre><code class=""lang-ipynb"">{<NewLine> ""cells"": [<NewLine>  {<NewLine>   ""cell_type"": ""markdown"",<NewLine>   ""metadata"": {},<NewLine>   ""source"": [<NewLine>    ""![](https://i.imgur.com/eBRPvWB.png)\n"",<NewLine>    ""\n"",<NewLine>    ""# Practical PyTorch: Playing GridWorld with Reinforcement Learning (Policy Gradients with REINFORCE)\n"",<NewLine>    ""\n"",<NewLine>    ""In this project we'll teach a neural network to navigate through a dangerous grid world.\n"",<NewLine>    ""\n"",<NewLine>    ""![](http://i.imgur.com/XNGB7sr.gif)\n"",<NewLine>    ""\n"",<NewLine>    ""Training uses [policy gradients](http://www.scholarpedia.org/article/Policy_gradient_methods) via the REINFORCE algorithm and a simplified Actor-Critic method. A single network calculates both a policy to choose the next action (the actor) and an estimated value of the current state (the critic). Rewards are propagated through the graph with PyTorch's [`reinforce` method](http://pytorch.org/docs/autograd.html?highlight=reinforce#torch.autograd.Variable.reinforce).""<NewLine>   ]<NewLine>  },<NewLine>  {<NewLine>   ""cell_type"": ""markdown"",<NewLine>   ""metadata"": {},<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/spro/practical-pytorch/blob/master/reinforce-gridworld/reinforce-gridworld.ipynb"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>",https://discuss.pytorch.org/u/spro,(Sean Robertson),spro,"June 22, 2017,  3:55am",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Nice work! Looks pretty good at first glance. One suggestion that might help with the simplicity to readers is that you have a lot of nice clearly worded variables and then quite a few one letter variables. Maybe renaming some of those variable with more meaningful names might help with clarity for the reader.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry now after taking second look actor critic looks off to me. Is this performing well? Why discount factor of 0.9 not traditional 0.99. Also do have your rewards matched up incorrectly in different step direction to actions and values?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>According to the Sutton book this might be better described as <a href=""http://incompleteideas.net/sutton/book/bookdraft2017june.pdf"" rel=""nofollow noopener"">“REINFORCE with baseline” (page 342)</a> rather than actor-critic:</p><NewLine><blockquote><NewLine><p>Although the REINFORCE-with-baseline method learns both a policy and a state-value function, we do not consider it to be an actor-critic method because its state-value function is used only as a baseline, not as a critic. That is, it is not used for bootstrapping (updating a state from the estimated values of subsequent states), but only as a baseline for the state being updated.</p><NewLine></blockquote><NewLine><p>But it does perform pretty well. The rewards are given only at the end of the episode, and the discount factor was lowered because episodes are relatively short (~50 steps).</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>oh ok I see how that can work though doesn’t seem like it would be very robust in general purpose algorithms go. As for discount factor in my understanding and use of it a lower discount factor will just lead to grab rewards as soon as it can(High immediate reward values) and a higher discount factor leads to more importance on later rewards. Gridworld is a end goal objective game I believe(sorry never played lol) so would benefit with higher D factor in my opinion. Number of steps should not be an issue.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dgriff; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dgriff; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/spro; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/dgriff; <NewLine> ,"REPLY_DATE 1: June 9, 2017,  7:06am; <NewLine> REPLY_DATE 2: June 11, 2017,  8:29pm; <NewLine> REPLY_DATE 3: June 11, 2017,  9:07pm; <NewLine> REPLY_DATE 4: June 11, 2017, 10:04pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
2844,Tricks or variants of deep reinforce learning,2017-05-10T15:55:20.496Z,4,1747,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m a new learner to deep reinforce learning. In easy task, DRL is easy to converge. If the explore space get more complicated, what tricks or variants I can use to make sure my DRL will converge? Do those variants have the implementations of pytorch?</p><NewLine></div>",https://discuss.pytorch.org/u/Leechikara,(Leechikara),Leechikara,"May 10, 2017,  3:55pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Prayer?  Walking away from it and letting it run for a really long time?</p><NewLine><p>Have you looked at this: <a href=""http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#sphx-glr-intermediate-reinforcement-q-learning-py"" rel=""nofollow noopener"">http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#sphx-glr-intermediate-reinforcement-q-learning-py</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, I have read this tutorial before. But I use recurrent PG in task-oriented spoken dialog system. If the behavior of user is much easy, our algorithm is easy to converge. If the behavior of user is more complicated, it seems our algorithm has difficult in learning.<img height=""480"" src=""https://discuss.pytorch.org/uploads/default/original/1X/b5cae778968a4f889fd53ceac3074c0ab0c4e6fc.png"" width=""640""/><img height=""480"" src=""https://discuss.pytorch.org/uploads/default/original/1X/93b2ec96cbcef623e836945645ec8d4aa5ce0f68.png"" width=""640""/><img height=""480"" src=""https://discuss.pytorch.org/uploads/default/original/1X/4cc494118075ddf6ab2f585a686aed543c04f33e.png"" width=""640""/><br/><NewLine>The first figure shows the success rate of simple user, the second and third show the success rate of complicated user. It’s obvious that the same algorithm will be slower to converge in more difficult environment.<br/><NewLine>What I mean is if there exist some variants of DRL to make sure RL converge faster in complicated environment. And how can I make sure my DRL will do better after one update. In my experiment, I found I can not make sure my RL will get more success rate after one update.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Warning never dabbled on RL yet but my gut feeling would be:</p><NewLine><p>1/ Train on a simpler model, and use the pretrained weights to model more complex behaviour. (Aka fine-tuning, or even curriculum learning)</p><NewLine><p>2/ Try other RL like A3C.</p><NewLine><p>3/ Read <a href=""http://ufal.mff.cuni.cz/~straka/courses/npfl114/2016/sutton-bookdraft2016sep.pdf"" rel=""nofollow noopener"">Sutton’s RL book</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply <img alt="":grin:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/grin.png?v=5"" title="":grin:""/></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think you can check John Schulman’s <a href=""http://joschu.net/docs/nuts-and-bolts.pdf"" rel=""nofollow noopener"">nuts-and-bolts-drl</a></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you. I need to learn some tricks of RL. This is really helpful.<img alt="":blush:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/blush.png?v=5"" title="":blush:""/></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>David Silver’s video lectures and notes are a great reference to get you started</p><NewLine><p><a class=""onebox"" href=""http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"" rel=""nofollow noopener"" target=""_blank"">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/danelliottster; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Leechikara; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mratsim; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Leechikara; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/tigerneil; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Leechikara; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/dgriff; <NewLine> ,"REPLY_DATE 1: May 10, 2017,  6:14pm; <NewLine> REPLY_DATE 2: May 12, 2017,  2:18am; <NewLine> REPLY_DATE 3: May 12, 2017, 11:31am; <NewLine> REPLY_DATE 4: May 13, 2017,  2:53am; <NewLine> REPLY_DATE 5: June 4, 2017,  1:15pm; <NewLine> REPLY_DATE 6: June 5, 2017,  6:23am; <NewLine> REPLY_DATE 7: June 8, 2017, 10:48pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> 
3497,[resolved] RuntimeError: unsupported tensor type DQN Flappy Bird,2017-05-29T00:21:53.806Z,0,678,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone, I’m trying to do an agent which can play Flappy Bird game. I am following a pytorch example at</p><NewLine><p><a href=""http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noopener"">http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html</a>,<br/><NewLine>and getting an error.</p><NewLine><p>First, here is the file: <a href=""https://github.com/darleybarreto/deep-reinforcement-learning/blob/master/dqn/dqn.py"" rel=""nofollow noopener"">https://github.com/darleybarreto/deep-reinforcement-learning/blob/master/dqn/dqn.py</a><br/><NewLine>The shape of the Tensors that goes into the convlayer is (n, k, h, w), i.e, (1, k, 80, 80). At each player actions, the tensor shape is (1, 4, 80, 80), that is, I am stacking the four last screens and passing through the net.<br/><NewLine>Now, as in the tutorial, doing this line below:</p><NewLine><pre><code>transitions = memory.sample(BATCH_SIZE) [line 166]<NewLine>batch = Transition(*zip(*transitions)) [line 169]<NewLine>state_batch = Variable(torch.cat(batch.state)) [line 173]<NewLine></code></pre><NewLine><p>I’m ending up with a tensor of shape (512, 80, 80) and that was giving me an error.<br/><NewLine>So I did <code>state_batch.view(1, *state_batch.size())</code> [line 181] ending up with another error at<br/><NewLine>[line 185] <code>state_action_values = dqn(state_batch).gather(1, action_batch).</code></p><NewLine><p>Here is the error:<br/><NewLine>```line 39, in conv2d<br/><NewLine>return f(input, weight, bias)<br/><NewLine>RuntimeError: unsupported tensor type`</p><NewLine><p>Can someone explain to me whats going on?</p><NewLine></div>",https://discuss.pytorch.org/u/darleybarreto,(Darley Barreto),darleybarreto,"May 30, 2017,  4:00am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found whats wrong. When calling <code>pygame.surfarray.array3d(display_surface)</code> it gives me a ndarray of uint8 type (0 up to 255).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/darleybarreto; <NewLine> ,"REPLY_DATE 1: May 30, 2017,  1:16am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
3336,Reinforce weighted sampling,2017-05-22T13:57:59.422Z,0,829,"<div class=""post"" itemprop=""articleBody""><NewLine><p>If I understand correctly if we use a sampling method (i.e. torch.multinomial) and then use the .reinforce() method we will backpropagate the reward to whatever process that created the samples.</p><NewLine><p>My question is whether we can create a weighted sampler and use the reinforce to update this sampler as well</p><NewLine></div>",https://discuss.pytorch.org/u/mordith,,mordith,"May 22, 2017,  1:57pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>yes you can do that (I think).<br/><NewLine>See how the stochastic nodes are implemented:<br/><NewLine><a class=""onebox"" href=""https://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/stochastic.py"" target=""_blank"">https://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/stochastic.py</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/smth; <NewLine> ,"REPLY_DATE 1: May 28, 2017,  5:09pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
3441,Reinforcement learning with pytorch,2017-05-25T23:25:32.237Z,0,1641,"<div class=""post"" itemprop=""articleBody""><NewLine><p>First off want to say big fan of pytorch and thanks to all the pytorch guys for making such a great library for neural networks. And the above and beyond attentiveness to all the help I see you provide in this forum. Awesome!</p><NewLine><p>Just wanted to share my recent reinforcement learning repo with the A3C algorithm and training agent in Atari in openai gym. Only trained agent in Pong-v0, MsPacman-v0, and Breakout-v0 so far and had pretty good success so far with agent getting top scores on each in openai gym environment leaderboards. Probably will train in a couple more games but another shining example of pytorch’s stellar performance.</p><NewLine><p>Any feedback or questions always welcome <img alt="":smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smile.png?v=9"" title="":smile:""/></p><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""32""/><NewLine><a href=""https://github.com/dgriff777/rl_a3c_pytorch"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""420"" src=""https://avatars1.githubusercontent.com/u/24666148?s=400&amp;v=4"" width=""420""/><NewLine><h3><a href=""https://github.com/dgriff777/rl_a3c_pytorch"" rel=""nofollow noopener"" target=""_blank"">dgriff777/rl_a3c_pytorch</a></h3><NewLine><p>A3C LSTM  Atari with Pytorch plus A3G design. Contribute to dgriff777/rl_a3c_pytorch development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>",https://discuss.pytorch.org/u/dgriff,,dgriff,"May 25, 2017, 11:25pm",3 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks for sharing <img alt="":slight_smile:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=5"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/smth; <NewLine> ,"REPLY_DATE 1: May 28, 2017,  5:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
2852,ISSAC Simulator NVIDA,2017-05-10T21:42:14.159Z,0,261,"<div class=""post"" itemprop=""articleBody""><NewLine><p>What do community members think about ISSAC robot simulation of NVIDIA in GTC conference. The procedure mentioned<br/><NewLine>in the conference as follows:<br/><NewLine>Create n agents start actions in environment<br/><NewLine>Pick best performing agent among all agents<br/><NewLine>replace the best agent performance in all agents<br/><NewLine>Repeat step 2<br/><NewLine>Jensen mentions this enabled accelerating the learning process.</p><NewLine></div>",https://discuss.pytorch.org/u/sriharsha0806,(Sriharsha Annamaneni),sriharsha0806,"May 10, 2017,  9:42pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It looks interesting, similar to mujoco. Would love to play around with it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/smth; <NewLine> ,"REPLY_DATE 1: May 13, 2017,  4:30am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
2921,A question about the reinforcement learning example,2017-05-12T15:25:08.712Z,0,500,"<div class=""post"" itemprop=""articleBody""><NewLine><p>hi, guys</p><NewLine><p>Below is REINFORCE update rule.</p><NewLine><p><img height=""55"" src=""https://discuss.pytorch.org/uploads/default/original/1X/8c4a3abb17edf44acb1e9f13bc6db276eb50b241.png"" width=""454""/></p><NewLine><p>Instead of log probability, the reinforcement learning example uses the probability to compute the gradient. Is it correct ?</p><NewLine></div>",https://discuss.pytorch.org/u/xwgeng,,xwgeng,"May 12, 2017,  3:25pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The gradient of the log of the probability (in the case of the multinomial distribution that’s used in the example) is negative the reciprocal of the probability, which is what you’ll find in the multinomial function’s backward definition.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jekbradbury; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xwgeng; <NewLine> ,"REPLY_DATE 1: May 13, 2017, 12:22am; <NewLine> REPLY_DATE 2: May 13, 2017,  3:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
2324,Different outputs for the same seed in &ldquo;reinforce.py&rdquo;,2017-04-26T23:27:25.278Z,1,787,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am doing some experimentation in reinforcement learning using pytorch. I am not sure why <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""nofollow noopener"">reinforce.py in the RL example folder</a> produces different outputs for each run even though the seed is set to 543 by default. Here are the outputs from two runs:</p><NewLine><p>OUTPUT OF 1st RUN:</p><NewLine><p>Episode 10	Last length:    13	Average length: 10.64<br/><NewLine>Episode 20	Last length:    24	Average length: 11.37<br/><NewLine>Episode 30	Last length:   115	Average length: 15.63<br/><NewLine>Episode 40	Last length:    17	Average length: 19.16<br/><NewLine>Episode 50	Last length:    77	Average length: 22.33<br/><NewLine>Episode 60	Last length:    52	Average length: 24.56<br/><NewLine>Episode 70	Last length:    67	Average length: 28.63<br/><NewLine>Episode 80	Last length:   199	Average length: 40.23<br/><NewLine><strong>Episode 90	Last length:   116	Average length: 53.10</strong><br/><NewLine><strong>Episode 100	Last length:    32	Average length: 54.78</strong><br/><NewLine><strong>Episode 110	Last length:   112	Average length: 58.36</strong><br/><NewLine><strong>Episode 120	Last length:   199	Average length: 70.91</strong><br/><NewLine><strong>Episode 130	Last length:   199	Average length: 83.16</strong><br/><NewLine><strong>Episode 140	Last length:   199	Average length: 94.23</strong><br/><NewLine><strong>Episode 150	Last length:    69	Average length: 95.64</strong><br/><NewLine><strong>Episode 160	Last length:   199	Average length: 97.84</strong><br/><NewLine><strong>Episode 170	Last length:   199	Average length: 107.51</strong><br/><NewLine><strong>Episode 180	Last length:   199	Average length: 112.87</strong><br/><NewLine><strong>Episode 190	Last length:   199	Average length: 116.42</strong><br/><NewLine><strong>Episode 200	Last length:    66	Average length: 122.85</strong></p><NewLine><p>OUTPUT OF 2nd RUN:</p><NewLine><p>Episode 10	Last length:    13	Average length: 10.64<br/><NewLine>Episode 20	Last length:    24	Average length: 11.37<br/><NewLine>Episode 30	Last length:   115	Average length: 15.63<br/><NewLine>Episode 40	Last length:    17	Average length: 19.16<br/><NewLine>Episode 50	Last length:    77	Average length: 22.33<br/><NewLine>Episode 60	Last length:    52	Average length: 24.56<br/><NewLine>Episode 70	Last length:    67	Average length: 28.63<br/><NewLine>Episode 80	Last length:   199	Average length: 40.23<br/><NewLine><strong>Episode 90	Last length:   113	Average length: 53.07</strong><br/><NewLine><strong>Episode 100	Last length:   199	Average length: 62.14</strong><br/><NewLine><strong>Episode 110	Last length:   199	Average length: 74.02</strong><br/><NewLine><strong>Episode 120	Last length:    70	Average length: 78.82</strong><br/><NewLine><strong>Episode 130	Last length:    51	Average length: 79.03</strong><br/><NewLine><strong>Episode 140	Last length:   108	Average length: 78.31</strong><br/><NewLine><strong>Episode 150	Last length:   124	Average length: 81.46</strong><br/><NewLine><strong>Episode 160	Last length:   127	Average length: 86.14</strong><br/><NewLine><strong>Episode 170	Last length:    53	Average length: 87.84</strong><br/><NewLine><strong>Episode 180	Last length:    85	Average length: 86.07</strong><br/><NewLine><strong>Episode 190	Last length:    91	Average length: 86.13</strong><br/><NewLine><strong>Episode 200	Last length:   112	Average length: 87.26</strong></p><NewLine><p>Notice that the outputs started to differ beginning from episode 90. I also tried to set the same seed value to np.random but with no success.</p><NewLine><p>Any idea to what could be the source of the non-determinism in the script? Or is there a bug?</p><NewLine></div>",https://discuss.pytorch.org/u/mhalsharif,(alsharif),mhalsharif,"April 27, 2017, 12:07am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is an interesting one that I’ve seen before in similar situation, what’s going on is a little complex, and there is nothing you can do about it.</p><NewLine><p>I’m going to guess that you’re either running this on the GPU? Well, it turns out that reductions on the GPU (like sum for matrix multiplication and softmax) are actually slightly non-deterministic, in that the reduce operation will not be applied in the same order. Most reduction operations are commutative (they can be applied in any order and still get the same results), however because we are using floats (I’m guessing you’re using 32 bit?) we have a slightly different rounded value because we calculated in different order.</p><NewLine><p>Initially this difference in rounding is only slight, and doesn’t show up in your output at all (if you print out more decimal places you should see the effect earlier), but it accelerates as the fixed-precision errors accumulate, and eventually it will result in such significantly different values that it takes your optimisation off in a completely different direction, and we get significant divergences as above.</p><NewLine><p>There is nothing you can do about this if you are after repeatable results on the GPU, <strong>GPU computation is inherently stochastic</strong>. However you can try:</p><NewLine><ul><NewLine><li>running your experiments repeatedly (with different weight initialisations as well) and average them to get a more robust estimate of your score</li><NewLine><li>run your experiments for a shorter time (until before the divergence occurs)</li><NewLine><li>run your experiments on a single core CPU</li><NewLine></ul><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote"" data-post=""2"" data-topic=""2324""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""//discuss.pytorch.org/user_avatar/discuss.pytorch.org/tfwnicholson/40/564_1.png"" width=""20""/> tfwnicholson:</div><NewLine><blockquote><NewLine><p>There is nothing you can do about this if you are after repeatable results on the GPU, GPU computation is inherently stochastic. However you can try:</p><NewLine><ul><NewLine><li>running your experiments repeatedly (with different weight initialisations as well) and average them to get a more robust estimate of your score</li><NewLine><li>run your experiments for a shorter time (until before the divergence occurs)</li><NewLine><li>run your experiments on a single core CPU</li><NewLine></ul><NewLine></blockquote><NewLine></aside><NewLine><p>Hey Tom, thanks for the explanation. That would make sense if I used GPUs. But the results shown here are from running the script “as it is” on my macbook’s CPU. There is no threading or parallelism in this script so I assume it run on a single core. Could someone reproduce these results?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tfwnicholson; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mhalsharif; <NewLine> ,"REPLY_DATE 1: May 2, 2017,  5:55pm; <NewLine> REPLY_DATE 2: May 2, 2017,  8:19pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> 
2189,Doubts in Charecter level RNN tutorial,2017-04-24T12:16:20.487Z,0,548,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi I am going through PyTorch tutorial at</p><NewLine><p><a href=""http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#training"" rel=""nofollow noopener"">http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#training</a>.</p><NewLine><p>Inside the train() function the network parameters are updated with gradients using the below line.</p><NewLine><p>p.data.add_(-learning_rate, p.grad.data)</p><NewLine><p>the add function I saw in the source code accepts only one argument. How does the function accepts 2 arguments and multiplies the learning rate with gradients and add that to the tensor data.</p><NewLine></div>",https://discuss.pytorch.org/u/VISHNU_SUBRAMANIAN,(Vishnu Subramanian),VISHNU_SUBRAMANIAN,"April 24, 2017, 12:16pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>See the second signature of torch.add (there are two):</p><NewLine><p><a class=""onebox"" href=""http://pytorch.org/docs/torch.html#torch.add"" target=""_blank"">http://pytorch.org/docs/torch.html#torch.add</a></p><NewLine><p>it does: y = y + alpha * x</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/smth; <NewLine> ,"REPLY_DATE 1: April 29, 2017,  2:56pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
2271,Wnat pytorch compile with ICC,2017-04-26T04:57:47.399Z,1,581,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have GPU server and via nvidia-docker installed the pytorch run the pytorch test related scripts also. But want to measure the performance of GPU server with intel parallel studio (ICC).</p><NewLine><p>Please guide how to use ICC(intel c++ compiler) with pytorch to check performance on gpu server.</p><NewLine></div>",https://discuss.pytorch.org/u/niraj_vara,(Niraj Vara),niraj_vara,"April 26, 2017,  4:57am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>you’re on your own here. we dont use any non-standard build tools so you should hopefully figure it out.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>can you tell me which standard build tools you used for the permormance improvement ???</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/niraj_vara; <NewLine> ,"REPLY_DATE 1: April 27, 2017, 12:15am; <NewLine> REPLY_DATE 2: April 27, 2017, 10:36am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
1816,REINFORCE: Why centralize rewards?,2017-04-12T13:43:38.573Z,0,954,"<div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py#L65"" rel=""nofollow noopener"">This line</a> centralizes the rewards, is there a specific reason, since the original algorithm does not mention the centralization.</p><NewLine></div>",https://discuss.pytorch.org/u/freiburgzuo,(Xingdong Zuo),freiburgzuo,"April 12, 2017,  1:43pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is a classic trick used in a lot of different papers, normalizing the rewards really speeds up learning a lot.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ebetica; <NewLine> ,"REPLY_DATE 1: April 12, 2017,  3:12pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
1782,You can only reinforce a stochastic Function once,2017-04-10T18:46:06.935Z,1,643,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is this problem avoidable?</p><NewLine><p>If non-null rewards are very rare, it makes sense to “bootstrap” the same past action-reward more than once if the received reward wasn’t null.</p><NewLine><p>Is it possible to do it? Or should we just avoid <code>Variable.reinforce</code> in that case?</p><NewLine></div>",https://discuss.pytorch.org/u/alexis-jacq,(Alexis David Jacq),alexis-jacq,"April 10, 2017,  6:49pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could use something like this if you want to be able to reinforce vars cumulatively:</p><NewLine><pre><code class=""lang-python"">def reinforce(var, reward):<NewLine>    if var.creator.reward is torch.autograd.stochastic_function._NOT_PROVIDED:<NewLine>        var.creator.reward = reward<NewLine>    else:<NewLine>        var.creator.reward += reward<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, it works!</p><NewLine><p>By curiosity, what is the reason to reinforce a stochastic function only once in the usual way?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jekbradbury; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> ,"REPLY_DATE 1: April 10, 2017,  6:49pm; <NewLine> REPLY_DATE 2: April 10, 2017,  6:57pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> 
1566,What will happen if some part of the loss is negative?,2017-04-01T08:45:30.378Z,0,4226,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Im trying to implement a policy gradient method in RL and the target of my nn is to increase the probability of the actions that leads to positive results and decrease the probability of the actions that leads to negative results.<br/><NewLine>the loss function is something like ‘A*-logp’, where A is positive or negative. However, if the agent sampled a action that leads to a negative A, then the loss would be negative. In that case, I actually want the negative loss to be ‘lower’ because then the probability of that ‘bad’ action would decrease. But it didn’t work as I imagined <img alt="":cold_sweat:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/cold_sweat.png?v=5"" title="":cold_sweat:""/><br/><NewLine>here is part of my codes:</p><NewLine><pre><code>epprob = output.gather(1, epaction)<NewLine>loglik = torch.log(epprob)<NewLine>loss = -(loglik*discounted_epr).mean()<NewLine>optimizer.zero_grad()<NewLine>loss.backward()<NewLine>optimizer.step()<NewLine></code></pre><NewLine><p>What should I do now? Is there any tutorials about how to deal with the negative loss or about the loss functions with probability? I really appreciate your help.</p><NewLine></div>",https://discuss.pytorch.org/u/pointW,,pointW,"April 1, 2017,  8:45am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think what you want to do is exactly what <a href=""http://pytorch.org/docs/autograd.html?highlight=reinforce#torch.autograd.Variable.reinforce""><code>reinforce</code></a> does. You can find an example of usage here: <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"">https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> ,"REPLY_DATE 1: April 3, 2017,  3:18pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> 
1125,How to assign gradients to model parameters manually?,2017-03-16T17:28:24.147Z,3,1409,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In A3C, the gradients of main thread parameters are copied from training threads.<br/><NewLine>But the gradients of pytorch nn.module cannot be reassigned?<br/><NewLine>So how to implement this procedure?</p><NewLine><p>BTW. is there a function like tf.gather_nd() in pytorch?<br/><NewLine>to extract values in specific indexes of the tensor (multidimensional).<br/><NewLine>Cause right now, the torch.indes_select can only select from one dimension.</p><NewLine></div>",https://discuss.pytorch.org/u/onlytailei,(lei.tai),onlytailei,"April 6, 2017,  3:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>to answer your second question, you can create a mask tensor and index elements via this mask tensor:</p><NewLine><p>example:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine>x = torch.randn(2, 3, 2)<NewLine><NewLine>mask = x.gt(0.5)<NewLine>print(mask)<NewLine><NewLine># mask is a torch.ByteTensor of same shape and size as x<NewLine><NewLine>print(x[mask])<NewLine></code></pre><NewLine><p>Output:</p><NewLine><pre><code class=""lang-auto""><NewLine>(0 ,.,.) =<NewLine>  0  0<NewLine>  0  0<NewLine>  0  1<NewLine><NewLine>(1 ,.,.) =<NewLine>  0  1<NewLine>  1  0<NewLine>  1  0<NewLine>[torch.ByteTensor of size 2x3x2]<NewLine><NewLine><NewLine> 1.3140<NewLine> 0.8220<NewLine> 1.5169<NewLine> 0.9264<NewLine>[torch.FloatTensor of size 4]<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you smth.<br/><NewLine>If I have an index list where same indexes may repeat for several times.</p><NewLine><p>Then I think this method is useless.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you try <code>gather</code>?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>But gather always returns a tensor with same dimension as the input right?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, that’s the limitation.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/onlytailei; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/onlytailei; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/apaszke; <NewLine> ,"REPLY_DATE 1: March 16, 2017,  9:58pm; <NewLine> REPLY_DATE 2: March 17, 2017, 12:51pm; <NewLine> REPLY_DATE 3: March 18, 2017,  9:39pm; <NewLine> REPLY_DATE 4: March 20, 2017,  5:06pm; <NewLine> REPLY_DATE 5: March 21, 2017,  2:13pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
1122,Help for DQN (implementation of the paper),2017-03-16T14:53:26.315Z,0,729,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, I know it’s really bad to ask help for checking if the implementation is correct.</p><NewLine><p>But I am rather new in deep RL, I don’t know if there is any debugging trick or method systematically for checking if my implementation is right.</p><NewLine><p>Unlike most supervised learning algorithm, by inspecting loss, we might have some idea about how to tune hyper-parameters (as in cs231n). But we can only see mean reward in RL(as far as I know) and I don’t know how to tune based on that.</p><NewLine><p>I create this post because recently I rewrite <a href=""https://github.com/berkeleydeeprlcourse/homework/tree/master/hw3"" rel=""nofollow noopener"">berkerly-rl-hw3</a> (tensorflow) to <a href=""https://github.com/transedward/pytorch-dqn"" rel=""nofollow noopener"">pytorch-dqn</a> (pytorch). The previous one already create preprocessing… and other trick used in the paper. I only adapt it to be consistent with pytorch and rewrite the dqn training algorithm in pytorch.</p><NewLine><p>But the performance was quite different/disappointing according to <a href=""http://rll.berkeley.edu/deeprlcourse/docs/hw3.pdf"" rel=""nofollow noopener"">hw3 Section3</a> using the same hyper-parameters. I also tried the same setting with the paper, but still quite different. I never reached mean reward greater than 0.</p><NewLine><p>I tried inspecting the gradients and data before and after the part which I suspected there may contains bugs.</p><NewLine><p>I checked <a href=""https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb"" rel=""nofollow noopener"">pytorch dqn tutorial</a> already. I think I basically do the same as it does except I use target network as describe in paper.</p><NewLine><p>I hope someone who is very experience in RL or pytorch can give me some suggestion or check my code( just look at the algorithm part, at <a href=""https://github.com/transedward/pytorch-dqn/blob/master/dqn_learn.py"" rel=""nofollow noopener"">dqn_learn.py</a> ), even collaborate with me to reimplement this paper in pytorch.</p><NewLine></div>",https://discuss.pytorch.org/u/transedward,(Chen Hung Tu),transedward,"March 17, 2017,  2:45am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I got similar results with deepmind paper already.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/transedward; <NewLine> ,"REPLY_DATE 1: March 18, 2017,  5:12pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
673,Documentation for reinforce(),2017-02-22T21:44:33.332Z,1,1598,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I’m having trouble understanding how and when to use a.reinforce(), despite the examples. In particular, why do I need it if I want to implement REINFORCE ?</p><NewLine><p>Thanks for your help</p><NewLine></div>",https://discuss.pytorch.org/u/cartage,(Adams Adams),cartage,"February 22, 2017,  9:44pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You need to call <code>.reinforce()</code> on outputs of stochastic function (<code>.bernoulli()</code>, <code>.normal()</code> and <code>.uniform()</code> at the moemnt), if you want to have autograd estimate the gradient of the expectation of the reward. No need to use it if you don’t do any sampling.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could implement REINFORCE manually, it’s just a convenient way of doing that.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, that’s just what I needed !</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/cartage; <NewLine> ,"REPLY_DATE 1: February 22, 2017, 11:19pm; <NewLine> REPLY_DATE 2: February 22, 2017, 11:19pm; <NewLine> REPLY_DATE 3: February 22, 2017, 11:30pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
260,PyTorch with VizDoom sample,2017-01-30T00:49:50.733Z,0,966,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I just would like to share <a href=""https://github.com/akolishchak/doom-net-pytorch"" rel=""nofollow noopener"">https://github.com/akolishchak/doom-net-pytorch</a> as a PyTorch sample, ported it from Torch.</p><NewLine><p>Thank you for the great tool! It is a real time saver due to autograd. You have provided nice docs and quite useful examples. It turns out Torch is more memory efficient for my models though.</p><NewLine></div>",https://discuss.pytorch.org/u/andrew,,andrew,"January 30, 2017, 12:49am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>this is pretty great!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/smth; <NewLine> ,"REPLY_DATE 1: January 30, 2017,  4:35am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
189,Using previous output values as part of a loss function,2017-01-25T12:56:14.756Z,2,1008,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to implement TRPO, and I need the gradient of the network parameters w.r.t. the KL divergence between the current action distribution and the action distribution after the parameters are changed.</p><NewLine><p>Put simply, we have:<br/><NewLine>action_dstribution0 &lt;-- Network(state)</p><NewLine><p>We need to calculate something to the effect of:<br/><NewLine>autograd.backward(KL(Network(state), action_distribution0), [torch.ones(1)])</p><NewLine><p>What is the best way to implement this? How do I make sure action_distribution0 doesn’t get backpropagated through?</p><NewLine></div>",https://discuss.pytorch.org/u/yoonholee,(Yoonho Lee),yoonholee,"January 25, 2017, 12:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It sounds very similar to Q-learning - just use <code>.detach()</code> to prevent the gradients from getting propagated:</p><NewLine><pre><code class=""lang-auto"">action_distribution0 = model(state).detach()<NewLine># detach() blocks the gradient<NewLine># also action_distribution0 doesn't require grad now, so it can be a loss target<NewLine><NewLine># =========================================<NewLine># change the parameters of the network here<NewLine># =========================================<NewLine><NewLine>KL(Network(state), action_distribution0).backward()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This worked great, thanks!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>In the dcgan example, the G(z) output is copied over before pushing through D (<a href=""https://github.com/pytorch/examples/blob/master/dcgan/main.py#L213"" rel=""nofollow noopener"">link</a>).<br/><NewLine>Is this done in order to avoid backpropagation through G?<br/><NewLine>Would this be an equivalent way of doing things?</p><NewLine><pre><code>fake = netG(noise)<NewLine>fake = fake.detach() # this is not an inplace op, so need to re-assign?<NewLine>output = netD(fake)</code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>yes, detaching sounds better. When I wrote that example, <code>.detach()</code> was not available. Please send a PR to fix that <img alt="":slight_smile:"" class=""emoji"" src=""//discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=5"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/apaszke; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/yoonholee; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tomsercu; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/smth; <NewLine> ,"REPLY_DATE 1: January 25, 2017,  4:48pm; <NewLine> REPLY_DATE 2: January 26, 2017, 11:05am; <NewLine> REPLY_DATE 3: January 27, 2017,  4:57pm; <NewLine> REPLY_DATE 4: January 27, 2017,  6:43pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
