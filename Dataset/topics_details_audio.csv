id,title,created_at,reply_count,views,description,creator_link,creator_name,creator_alias,post_date,post_likes,replies,repliers_links,reply_dates,reply_likes
14960,About the audio category,2018-03-15T12:35:52.055Z,1,492,"<div class=""post"" itemprop=""articleBody""><NewLine><p>(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters. <strong>Until you edit this description or create topics, this category won’t appear on the categories page.</strong>)</p><NewLine><p>Use the following paragraphs for a longer description, or to establish category guidelines or rules:</p><NewLine><ul><NewLine><li><NewLine><p>Why should people use this category? What is it for?</p><NewLine></li><NewLine><li><NewLine><p>How exactly is this different than the other categories we already have?</p><NewLine></li><NewLine><li><NewLine><p>What should topics in this category generally contain?</p><NewLine></li><NewLine><li><NewLine><p>Do we need this category? Can we merge with another category, or subcategory?</p><NewLine></li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/apaszke,(Adam Paszke),apaszke,"March 15, 2018, 12:35pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It would be nice to have some links or something useful <img alt="":stuck_out_tongue:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/stuck_out_tongue.png?v=9"" title="":stuck_out_tongue:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://pytorch.org/audio"" rel=""nofollow noopener"">https://pytorch.org/audio</a> <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tyoc213; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vincentqb; <NewLine> ,"REPLY_DATE 1: July 1, 2020,  7:56pm; <NewLine> REPLY_DATE 2: July 17, 2020, 11:16pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
97566,CUDA out of memory when training audio RNN (GRU),2020-09-26T11:52:48.287Z,0,17,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I’m trying to train a simple audio classification model on Colab, but my GPU memory (running on a 16GB instance) use keeps expanding and getting out of control every few epochs. Here is the model definition and a minimal snippet of my training code:</p><NewLine><pre><code class=""lang-auto"">class ConvBlock(nn.Module):<NewLine>    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):<NewLine>        super().__init__()<NewLine>        self.layers = nn.Sequential(<NewLine>            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),<NewLine>            nn.BatchNorm2d(out_channels),<NewLine>            nn.ELU(),<NewLine>        )<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.layers(x)<NewLine>        return x<NewLine><NewLine><NewLine>class AudioClassifier(nn.Module):<NewLine>    def __init__(self, stereo=True, dropout=0.1):<NewLine>        super().__init__()<NewLine>        in_channels = 2 if stereo else 1<NewLine>        self.spec = MelspectrogramStretch(hop_length=None, <NewLine>                                          num_mels=128, <NewLine>                                          fft_length=2048, <NewLine>                                          norm='whiten', <NewLine>                                          stretch_param=[0.4, 0.4])<NewLine><NewLine>        self.features = nn.Sequential(*[<NewLine>            ConvBlock(in_channels=2, out_channels=32, kernel_size=3, stride=1),<NewLine>            nn.MaxPool2d(3,3),<NewLine>            nn.Dropout(p=dropout),<NewLine>            ConvBlock(in_channels=32, out_channels=64, kernel_size=3, stride=1),<NewLine>            nn.MaxPool2d(4,4),<NewLine>            nn.Dropout(p=dropout),<NewLine>            ConvBlock(in_channels=64, out_channels=64, kernel_size=3, stride=1),<NewLine>            nn.MaxPool2d(4,4),<NewLine>            nn.Dropout(p=dropout),<NewLine>        ])<NewLine>        self.min_len = 80896<NewLine>        self.gru_hidden_size = 64<NewLine>        self.gru_layers = 2<NewLine><NewLine>        self.rnn = nn.GRU(128, self.gru_hidden_size, num_layers=self.gru_layers) <NewLine>        self.ret = nn.Sequential(*[nn.Linear(self.gru_hidden_size,1), nn.Sigmoid()])<NewLine>  <NewLine>    <NewLine>    def modify_lengths(self, lengths):<NewLine>        def safe_param(elem):<NewLine>            return elem if isinstance(elem, int) else elem[0]<NewLine><NewLine>        for name, layer in self.features.named_children():<NewLine>            if isinstance(layer, (nn.Conv2d, nn.MaxPool2d)):<NewLine>                p, k, s = map(safe_param, [layer.padding, layer.kernel_size,layer.stride]) <NewLine>                lengths = ((lengths + 2*p - k)//s + 1).long()<NewLine><NewLine>        return torch.where(lengths &gt; 0, lengths, torch.tensor(1, device=lengths.device))<NewLine><NewLine>    def _many_to_one(self, t, lengths):<NewLine>        return t[torch.arange(t.size(0)), lengths - 1]<NewLine><NewLine>    def init_hidden(self, batch_size, device):<NewLine>        return torch.zeros(self.gru_layers, batch_size, self.gru_hidden_size, device=device)<NewLine><NewLine>    def forward(self, wave, lengths):<NewLine>        x = wave<NewLine>        raw_lengths = lengths<NewLine>        xt = x.float().transpose(1,2)<NewLine>        xt, lengths = self.spec(xt, raw_lengths)<NewLine>        xt = self.features(xt)<NewLine>        lengths = self.modify_lengths(lengths)<NewLine>        x = xt.transpose(1, -1)<NewLine><NewLine>        batch, time = x.size()[:2]<NewLine>        x = x.reshape(batch, time, -1)<NewLine>        lengths = lengths.clamp(max=x.shape[1])<NewLine><NewLine>        # Handle variable input size<NewLine>        x_pack = torch.nn.utils.rnn.pack_padded_sequence(x, lengths.clamp(max=x.shape[1]), batch_first=True)<NewLine>        x_pack, self.hidden = self.rnn(x_pack)<NewLine>        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x_pack, batch_first=True)<NewLine>        x = self._many_to_one(x, lengths)<NewLine>        x = self.ret(x)<NewLine>        return x<NewLine><NewLine><NewLine><NewLine>def train():<NewLine>    for epoch in range(1,epochs+1):<NewLine>        model.train()<NewLine>        batch_losses=[]<NewLine>        <NewLine>        for batch_idx, batch in enumerate(pbar):<NewLine>            optimizer.zero_grad()<NewLine>            wave, lengths, lbl = batch<NewLine>            model.hidden = model.init_hidden(BATCH_SIZE, device)<NewLine>            <NewLine>            pred = model(wave.to(device), lengths.to(device)).squeeze()<NewLine>            loss = loss_fn(pred, lbl.to(device))<NewLine>            <NewLine>            loss.backward()<NewLine>            batch_losses.append(loss.detach().item())<NewLine>            optimizer.step()<NewLine>            <NewLine>            del loss, pred, wave, lengths, lbl<NewLine></code></pre><NewLine><p>I have tried deleting the prediction and input variables every loop, made sure I was detaching every variable I keep, and added an <code>init_hidden()</code> function to refresh the hidden states. These have helped me get to around 3-4 epochs before crashing, but it still happens. I’m running a batch size of just 8, and the input audio files are at most 15-20 seconds long.</p><NewLine><p>Is there anything I can do without reducing the batch size even further? Sorry if there are any stupid mistakes there but I’m a CV guy just getting into audio processing. The code from the classifier was heavily inspired by <a class=""inline-onebox-loading"" href=""https://github.com/ksanjeevan/crnn-audio-classification"" rel=""nofollow noopener"">https://github.com/ksanjeevan/crnn-audio-classification</a>.</p><NewLine></div>",https://discuss.pytorch.org/u/glefundes,(Gabriel Lefundes),glefundes,"September 26, 2020, 11:52am",,,,,
97369,torchaudio.transforms.Spectrogram,2020-09-24T13:02:37.683Z,2,29,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Everybody,</p><NewLine><p>I am using the torchaudio.transforms.Spectrogram to get the Spectrogram of a sin wave which is as follows:<br/><NewLine>Fs = 400<br/><NewLine>freq = 5<br/><NewLine>sample = 400<br/><NewLine>x = np.arange(sample)<br/><NewLine>y = np.sin(2 * np.pi * freq * x / Fs)</p><NewLine><p>Then, I get the Spectrogram of the mentioned sin wave as follows:<br/><NewLine>specgram = torchaudio.transforms.Spectrogram(n_fft=256,<br/><NewLine>win_length=256,<br/><NewLine>hop_length=184,<br/><NewLine>window_fn=torch.hamming_window,<br/><NewLine>power=1,<br/><NewLine>normalized=True)</p><NewLine><p>output = specgram(torch.from_numpy(y))<br/><NewLine>As you see, the sin wave only has a frequency 5 Hz, so I expect the output does not change in different time bins, but I got the following figure which is so strange:</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/4b4ceb478bb352bd217683eaa5ad2a8ccdad42b2"" href=""https://discuss.pytorch.org/uploads/default/original/3X/4/b/4b4ceb478bb352bd217683eaa5ad2a8ccdad42b2.png"" title=""specg""><img alt=""specg"" data-base62-sha1=""aK8BJuXvecZP9fRilfAB3ofAARY"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/b/4b4ceb478bb352bd217683eaa5ad2a8ccdad42b2_2_10x10.png"" height=""295"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/b/4b4ceb478bb352bd217683eaa5ad2a8ccdad42b2_2_690x295.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/b/4b4ceb478bb352bd217683eaa5ad2a8ccdad42b2_2_690x295.png, https://discuss.pytorch.org/uploads/default/optimized/3X/4/b/4b4ceb478bb352bd217683eaa5ad2a8ccdad42b2_2_1035x442.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/4/b/4b4ceb478bb352bd217683eaa5ad2a8ccdad42b2_2_1380x590.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">specg</span><span class=""informations"">1514×649 8.23 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>I believe this is wrong, so I decided to used the signal library to get Spectrogram of the mention sin wave as follows:<br/><NewLine>frequencies_samples, time_segment_sample, spectrogram_of_vector = signal.spectrogram(<br/><NewLine>x=y,<br/><NewLine>fs=fs,<br/><NewLine>nperseg=256,<br/><NewLine>noverlap=184,<br/><NewLine>window=“hamming”,<br/><NewLine>detrend=False,<br/><NewLine>mode=‘magnitude’)<br/><NewLine>And, then by using  signal.spectrogram, I get the following figure:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/d476e368d3ebfb8672617f5d01d83bfc5686277b"" href=""https://discuss.pytorch.org/uploads/default/original/3X/d/4/d476e368d3ebfb8672617f5d01d83bfc5686277b.png"" title=""spec2""><img alt=""spec2"" data-base62-sha1=""ujxWkmMV49BHx2jeeXVW4D9C3J9"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/4/d476e368d3ebfb8672617f5d01d83bfc5686277b_2_10x10.png"" height=""298"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/4/d476e368d3ebfb8672617f5d01d83bfc5686277b_2_690x298.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/4/d476e368d3ebfb8672617f5d01d83bfc5686277b_2_690x298.png, https://discuss.pytorch.org/uploads/default/optimized/3X/d/4/d476e368d3ebfb8672617f5d01d83bfc5686277b_2_1035x447.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/d/4/d476e368d3ebfb8672617f5d01d83bfc5686277b_2_1380x596.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">spec2</span><span class=""informations"">1498×649 4.26 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>As we expected, the spectrogram should not change in different time bins because we have only one frequency.</p><NewLine><p>So, what is the matter with the spectrogram when I use torch library? why it changes over the time when I have only one frequency?</p><NewLine></div>",https://discuss.pytorch.org/u/Elahe_Rahimian,(Elahe Rahimian),Elahe_Rahimian,"September 24, 2020,  1:12pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This probably is from the phase shift incurred when the partitioning isn’t a multiple of the wave length in samples.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Many thanks for your reply,<br/><NewLine>I applied a window which is a multiple of the wave length in samples, and now it is as I expected.<br/><NewLine>However, I have another question.<br/><NewLine>The output of torchaudio.transforms.Spectrogram is  Exponent for the magnitude spectrogram. So, why we can see the effect of phase shift?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think the phase changes the jump at the discontinuity at the boundary of basic cell and that leads to varying degrees of uncleanliness of the Fourier transform. If you had a periodic thing, the phase shift would correspond to an argument shift of the (complex) coefficients but leaving the modules of the coefficients alone.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Many thanks for your reply.<br/><NewLine>I am just wondering why we do not see this effect when we get the spectrogram by Signal library (signal.spectrogram).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Elahe_Rahimian; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Elahe_Rahimian; <NewLine> ,"REPLY_DATE 1: September 24, 2020,  1:54pm; <NewLine> REPLY_DATE 2: September 24, 2020,  2:19pm; <NewLine> REPLY_DATE 3: September 24, 2020,  7:08pm; <NewLine> REPLY_DATE 4: September 25, 2020,  2:34am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
97259,Mel Spectrogram in torchaudio backprop,2020-09-23T13:34:46.973Z,1,26,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is the <a href=""https://pytorch.org/audio/transforms.html#melspectrogram"" rel=""nofollow noopener""><code>torchaudio.transforms.MelSpectrogram</code></a> class in <code>torchaudio</code> differentiable? As in, can I backpropagate through it? If so, can someone point to me towards some documentation on how this backpropagation works?</p><NewLine></div>",https://discuss.pytorch.org/u/Mahmoud_Abdelkhalek,(Mahmoud Abdelkhalek),Mahmoud_Abdelkhalek,"September 23, 2020,  1:34pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think so, just do <code>requires_grad_</code> on your input. In the end it goes through <code>torchaudio.transforms.functional.spectrogram</code> and uses the <code>torch.stft</code> function. This calls <code>torch.fft</code> (I think), which has a derivative defined.<br/><NewLine>There are several texts about how the inner parts of PyTorch work, <a href=""https://lernapparat.de/selective-excursion-into-pytorch-internals/"" rel=""nofollow noopener"">I wrote something simple a long time ago</a> and <a class=""mention"" href=""/u/ezyang"">@ezyang</a> has an awesome comprehensive <a href=""http://blog.ezyang.com/2019/05/pytorch-internals/"" rel=""nofollow noopener"">tour of PyTorch internals</a>.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot for the information! Especially the links about the inner workings of PyTorch!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Mahmoud_Abdelkhalek; <NewLine> ,"REPLY_DATE 1: September 23, 2020,  4:30pm; <NewLine> REPLY_DATE 2: September 23, 2020,  4:30pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
75086,PyAudio tutorials,2020-04-02T08:54:32.224Z,2,154,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>What is up with the audio tutorials?</p><NewLine><p>When trying to search for pytorch audio quickstart-ish code only found <code>https://pytorch.org/tutorials/beginner/audio_classifier_tutorial.html</code> which seems to be old and missing code.</p><NewLine><p>The one tutorial that is in pytorch/tutorials is <code>https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html</code> which only shows file loading and few transforms.</p><NewLine><p>Is there a way one could contribute on writing those?</p><NewLine></div>",https://discuss.pytorch.org/u/danielius,(Danielius Visockas),danielius,"April 2, 2020,  8:54am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Contributions are always welcome!<br/><NewLine>Could you open an issue <a href=""https://github.com/pytorch/tutorials/issues"">here</a> and describe what kind of tutorial you would like to write to get some feedback? <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Will do. Initially started to write and issue and its template got me here.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Has there been any progress on this front? This would be most helpful if so.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/danielius; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Steve_B; <NewLine> ,"REPLY_DATE 1: April 2, 2020,  9:22am; <NewLine> REPLY_DATE 2: April 2, 2020,  9:41am; <NewLine> REPLY_DATE 3: September 22, 2020,  7:10pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
96772,How much work should your dataset class do for you?,2020-09-18T19:37:31.852Z,0,30,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to determine if I should refactor the way may dataset <code>__getitem__</code> method pre-processes.</p><NewLine><p>Are there any heuristics beyond the obvious time complexity and code readability I should consider?</p><NewLine><p>For a given audio file that I load I want to:</p><NewLine><ol><NewLine><li>Pad the length to match a given interval size.</li><NewLine><li>Cut the audio into equal pre-determined interval lengths</li><NewLine><li>Create several types of spectrograms of each piece of this new sequence.</li><NewLine></ol><NewLine><p>It’s a lot. Is there any reason that I shouldn’t do all of these things right inside of <code>__getitem__</code>?</p><NewLine></div>",https://discuss.pytorch.org/u/Wesley_Neill,(Wesley Neill),Wesley_Neill,"September 18, 2020,  7:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If any of the tasks are independent of the audio file, you can perform them in the <code>__init__</code> method. Else, instead of doing everything in the <code>__getitem__</code> method, you can write custom transforms and use them in the order you want. I am not really sure if this is more efficient than doing everything in the <code>__getitem__</code> method but you can try and check if it is.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>__getitem__</code> is the process which is spawn with multiprocess.<br/><NewLine>The workload should be there (mainly the spectrograms)<br/><NewLine>As <a class=""mention"" href=""/u/hash-ir"">@hash-ir</a> mentions, any other task can be performed in the <code>__init__</code> function (or even outside the dataset class)</p><NewLine><p>It’s all about how many RAM do you have.<br/><NewLine>Do you need to read audio in <code>__getitem__</code>?<br/><NewLine>Well, if your dataset fits in your RAM you can preload it in init.<br/><NewLine>You can use any python function inside <code>__getitem__</code>, therefore it can still be readable.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hash-ir; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/JuanFMontesinos; <NewLine> ,"REPLY_DATE 1: September 18, 2020, 10:13pm; <NewLine> REPLY_DATE 2: September 18, 2020, 10:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
18512,Installing torchaudio on google colab,2018-05-22T14:01:56.400Z,2,2340,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to install the torchaudio library in google Colaboratory notebook. However I get this dependency error:</p><NewLine><pre><code class=""lang-auto"">running install<NewLine>running bdist_egg<NewLine>running egg_info<NewLine>creating torchaudio.egg-info<NewLine>writing torchaudio.egg-info/PKG-INFO<NewLine>writing dependency_links to torchaudio.egg-info/dependency_links.txt<NewLine>writing top-level names to torchaudio.egg-info/top_level.txt<NewLine>writing manifest file 'torchaudio.egg-info/SOURCES.txt'<NewLine>reading manifest file 'torchaudio.egg-info/SOURCES.txt'<NewLine>writing manifest file 'torchaudio.egg-info/SOURCES.txt'<NewLine>installing library code to build/bdist.linux-x86_64/egg<NewLine>running install_lib<NewLine>running build_ext<NewLine>building '_torch_sox' extension<NewLine>creating build<NewLine>creating build/temp.linux-x86_64-3.6<NewLine>creating build/temp.linux-x86_64-3.6/torchaudio<NewLine>x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fdebug-prefix-map=/build/python3.6-sXpGnM/python3.6-3.6.3=. -specs=/usr/share/dpkg/no-pie-compile.specs -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/lib/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/THC -I/usr/include/python3.6m -c torchaudio/torch_sox.cpp -o build/temp.linux-x86_64-3.6/torchaudio/torch_sox.o -DTORCH_EXTENSION_NAME=_torch_sox -std=c++11<NewLine>x86_64-linux-gnu-gcc: error: torchaudio/torch_sox.cpp: No such file or directory<NewLine>x86_64-linux-gnu-gcc: fatal error: no input files<NewLine>compilation terminated.<NewLine>error: command 'x86_64-linux-gnu-gcc' failed with exit status 1<NewLine></code></pre><NewLine><p>Is there any workaround to fix this issue ?</p><NewLine><p>Best Regards</p><NewLine></div>",https://discuss.pytorch.org/u/Ahmed_m,(Ahmed Mamoud),Ahmed_m,"May 22, 2018,  2:01pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just installed it via <code>pip</code> on Google Colab directly from the GitHub repo and it works fine.</p><NewLine><pre><code>pip install git+git://github.com/pytorch/audio<NewLine></code></pre><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/95c20cd74909bc9cf2b777a5fbbdc3344123741e"" href=""https://discuss.pytorch.org/uploads/default/original/2X/9/95c20cd74909bc9cf2b777a5fbbdc3344123741e.png"" title=""Screen Shot 2018-05-22 at 1.57.46 PM.png""><img alt=""46%20PM"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/9/95c20cd74909bc9cf2b777a5fbbdc3344123741e_2_10x10.png"" height=""151"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/9/95c20cd74909bc9cf2b777a5fbbdc3344123741e_2_690x151.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/9/95c20cd74909bc9cf2b777a5fbbdc3344123741e_2_690x151.png, https://discuss.pytorch.org/uploads/default/original/2X/9/95c20cd74909bc9cf2b777a5fbbdc3344123741e.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/9/95c20cd74909bc9cf2b777a5fbbdc3344123741e.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screen Shot 2018-05-22 at 1.57.46 PM.png</span><span class=""informations"">880×193 26 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>Note that you need to install the required dependencies via <code>apt-get</code> first as mentioned in the GitHub repo. Also you need to have PyTorch installed, otherwise you will get an error during the install.</p><NewLine><p>EDIT:</p><NewLine><p>The dependencies I mentioned above are the ones listed for Ubuntu Linux (<a href=""https://github.com/pytorch/audio#dependencies"" rel=""nofollow noopener"">https://github.com/pytorch/audio#dependencies</a>). Worked fine for me in the Colaboratory Notebook:</p><NewLine><pre><code>apt-get install sox libsox-dev libsox-fmt-all</code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually I was installing it manually and found that I should have installed ‘cffi’ first as additional dependency. However the direct installation via pip you have provided is awesome. Thaanks</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Tried the following:</p><NewLine><p><code>!sudo apt-get install sox libsox-dev libsox-fmt-all</code><br/><NewLine><code>!pip install git+git://github.com/pytorch/audio</code></p><NewLine><p>Got the following message:<br/><NewLine><code>Successfully built torchaudio</code></p><NewLine><p>However, got the following error while importing torchaudio:<br/><NewLine><code>import torchaudio</code><br/><NewLine><code>RuntimeError: Failed to parse the argument list of a type annotation: name 'Optional' is not defined</code></p><NewLine><p>Am I missing something?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>This worked finally:</p><NewLine><pre><code class=""lang-auto"">!git clone https://github.com/pytorch/audio.git<NewLine>os.chdir(""audio"")<NewLine>!git checkout 301e2e9<NewLine>!python setup.py install<NewLine></code></pre><NewLine><p>Source : <a href=""https://github.com/pytorch/audio/issues/71"" rel=""nofollow noopener"">https://github.com/pytorch/audio/issues/71</a></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>This works! Thankyou so much</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/rasbt; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Ahmed_m; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/100deep1001; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/100deep1001; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/PalaashAgrawal; <NewLine> ,"REPLY_DATE 1: May 22, 2018,  6:06pm; <NewLine> REPLY_DATE 2: May 23, 2018,  9:39am; <NewLine> REPLY_DATE 3: July 18, 2019,  2:20pm; <NewLine> REPLY_DATE 4: July 19, 2019,  7:04am; <NewLine> REPLY_DATE 5: September 17, 2020,  5:28pm; <NewLine> ",REPLY 1 LIKES: 4 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
96533,Using torch.stft / torch.istft on PackedSequence data,2020-09-16T17:06:01.568Z,0,23,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Will it be possible down the line to have <code>torch.stft</code> or <code>torch.istft</code> support PackedSequence objects?</p><NewLine><p>I have provided a minimum working example here:</p><NewLine><pre><code class=""lang-auto"">from torch.nn.utils.rnn import pad_sequence<NewLine>from torch.nn.utils.rnn import pack_padded_sequence<NewLine>from torch.nn.utils.rnn import pad_packed_sequence<NewLine><NewLine><NewLine>fft_size = 1024<NewLine>hop_size = 256<NewLine><NewLine><NewLine>batched_audio = torch.rand((3, 50000))<NewLine>window = torch.hann_window(fft_size)<NewLine>Y = torch.stft(batched_audio, n_fft=fft_size, hop_length=hop_size,<NewLine>               window=window)<NewLine><NewLine>print(torch.is_tensor(Y))<NewLine># Out: True<NewLine></code></pre><NewLine><p>In the above cell, all the audio is equal length (50000 samples). Below is an example with varying length audio (50000, 40000, and 30000 samples respectively).</p><NewLine><pre><code class=""lang-auto"">batched_audio = [torch.rand(50000), torch.rand(40000), torch.rand(30000)]<NewLine>lengths = [len(x) for x in batched_audio]<NewLine>padded_audio = pad_sequence(batched_audio, batch_first=True)<NewLine>packed_audio = pack_padded_sequence(padded_audio, lengths, batch_first=True)<NewLine><NewLine>Y = torch.stft(packed_audio, n_fft=fft_size, hop_length=hop_size,<NewLine>               window=window)<NewLine><NewLine># Out: AttributeError: 'PackedSequence' object has no attribute 'dim'<NewLine></code></pre><NewLine><p>This would be really handy to have to batch process variable-length audio; taking the STFT and ISTFT are important operations.</p><NewLine><p>I guess the workaround for this right now would be to pre-compute STFTs of all the waveforms before calling <code>pad_sequence</code>, but I’m trying to reduce the GPU memory footprint of my tests. I thought on-the-fly STFT computation would be more efficient.</p><NewLine></div>",https://discuss.pytorch.org/u/actuallyaswin,(Aswin Sivaraman),actuallyaswin,"September 16, 2020,  5:07pm",,,,,
95685,Sound event detection model not learning,2020-09-09T15:11:25.233Z,4,52,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone,</p><NewLine><p>I am fine tuning a model for sound event detection, taken from <a href=""https://github.com/qiuqiangkong/audioset_tagging_cnn"" rel=""nofollow noopener"">https://github.com/qiuqiangkong/audioset_tagging_cnn</a>, on the Urbansed dataset. In this task, the model should predict a [batch, n_classes, time_steps] matrix, with a value of one indicating the presence of an event at a certain time step.<br/><NewLine>However, my network does not seem to train. Specifically, after about the first 10 epochs, my loss stops decreasing. If i check the predictions of the model, the output is composed entirely of 0.5s.<br/><NewLine>I have tried:</p><NewLine><ul><NewLine><li>Changing the amount of l2 regularization, even turning it off completely</li><NewLine><li>Changing the learning rate</li><NewLine><li>Doing a mock training with only 2 samples to see if the network could learn the simple problem. The result was the same matrix of 0.5.</li><NewLine><li>Different optimizers (Adam and SGD so far)</li><NewLine><li>BCEWithLogitsLoss with reduction = mean and sum (using this loss as in theory multiple classes can be active at a time)</li><NewLine></ul><NewLine><p>My loss and optimizer:</p><NewLine><pre><code class=""lang-auto"">criterion = nn.BCEWithLogitsLoss(reduction='sum')<NewLine>optimizer = optim.SGD(model.parameters(), lr=0.001)<NewLine></code></pre><NewLine><p>My training loop:</p><NewLine><pre><code class=""lang-auto"">    for i, data in enumerate(dataloader_train):<NewLine>        inputs, labels = data<NewLine>        inputs = inputs.type(torch.FloatTensor)<NewLine>        optimizer.zero_grad()<NewLine>        outputs = model(inputs).cpu()<NewLine>        loss = 0<NewLine>        loss = criterion(outputs, labels)<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine>        running_loss += loss.item()<NewLine></code></pre><NewLine><p>I can’t figure out what’s wrong. Any thoughts?<br/><NewLine>Thanks,<br/><NewLine>Federico</p><NewLine></div>",https://discuss.pytorch.org/u/fcola,(Federico),fcola,"September 9, 2020,  3:11pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Try viewing ur data to see what it’s actually training with</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your suggestion. Tried to visualize the data and nothing seems out of place, the spectrograms appear correct.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think u should manually initalize ur networks weights and try again<br/><NewLine>Btw what is ur loss func, learning rate and optimizer?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, I will try that, although that would mean I cannot do transfer learning. As for loss function, optimizer and lr I’m using:</p><NewLine><pre><code class=""lang-auto"">criterion = nn.BCEWithLogitsLoss(reduction='sum')<NewLine>optimizer = optim.SGD(model.parameters(), lr=0.001)<NewLine></code></pre><NewLine><p>Although I also tried Adam and different learning rates and l2 regularization values.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well try using the Adams optimization included with the weights initialization (weight values should be very close to zero by not too small and uniformly distributed)<br/><NewLine>If it still continues not to learn then try changing network architecture</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Henry_Chibueze; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/fcola; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Henry_Chibueze; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/fcola; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Henry_Chibueze; <NewLine> ,"REPLY_DATE 1: September 9, 2020,  8:59pm; <NewLine> REPLY_DATE 2: September 10, 2020,  9:15am; <NewLine> REPLY_DATE 3: September 10, 2020,  2:27pm; <NewLine> REPLY_DATE 4: September 14, 2020,  6:40am; <NewLine> REPLY_DATE 5: September 14, 2020,  8:44am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
82445,Using pytorch vggish for audio classification tasks,2020-05-22T08:05:23.532Z,4,357,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am researching on using pretrained <a href=""https://github.com/tensorflow/models/tree/master/research/audioset/vggish"" rel=""nofollow noopener"">VGGish</a> model for audio classification tasks, ideally I could have a model classifying any of the classes defined in the google audioset. I came across a nice pytorch <a href=""https://github.com/harritaylor/torchvggish"" rel=""nofollow noopener"">port</a> for generating audio features. The original model generates only audio features as well. The original team suggests generally the following way to proceed:</p><NewLine><blockquote><NewLine><p><em>As a feature extractor</em> : VGGish converts audio input features into a semantically meaningful, high-level 128-D embedding which can be fed as input to a downstream classification model. The downstream model can be shallower than usual because the VGGish embedding is more semantically compact than raw audio features. So, for example, you could train a classifier for 10 of the AudioSet classes by using the released embeddings as features. Then, you could use that trained classifier with any arbitrary audio input by running the audio through the audio feature extractor and VGGish model provided here, passing the resulting embedding features as input to your trained model. <code>vggish_inference_demo.py</code> shows how to produce VGGish embeddings from arbitrary audio</p><NewLine></blockquote><NewLine><p>I’m not sure how to go about using getting the released embeddings and using them for training in pytorch. I’m also not sure how to translate the embeddings into classification. Could any one kindly share some pointers? Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/kepler62f,(Kepler62f),kepler62f,"May 22, 2020,  8:05am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Based on the description you’ve posted it seems the authors call the output features embeddings.<br/><NewLine>This might be a bit confusing, as there are <code>nn.Embedding</code> layers, which are apparenrently not meant here.</p><NewLine><p>If I understand the use case correctly, you could store each output feature of the <code>VGGish</code> model with its corresponding target, create a new classification model, and use these output features + targets to train this new classifier.</p><NewLine><p>According to their claim, this classifier can be “shallower”, as the “embeddings” are so great. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine><p>Let me know, if that makes sense.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>you could store each output feature of the  <code>VGGish</code>  model with its corresponding target, create a new classification model, and use these output features + targets to train this new classifier.</p><NewLine></blockquote><NewLine><p>Cool. Thanks for the tip! Would this seem like reasonable steps to train a new model?</p><NewLine><ol><NewLine><li>Download audio wav samples from <a href=""https://research.google.com/audioset/dataset/index.html"" rel=""nofollow noopener"">audioset</a>  as training data</li><NewLine><li>Send each audio wav sample thru the VGGish to get a corresponding 128-dimension vector (output feature)</li><NewLine><li>Define a Dataset comprising the VGGish output feature as input (x) and the corresponding target (y)</li><NewLine><li>Using nn.module, define a “shallow” model using single layer…say Linear() .</li><NewLine><li>train</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, your approach seems reasonable!<br/><NewLine>Let us know, how your experiments went. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I want to get a 128-dimension feature of my own video data, what should I do?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>For audio classification of video data, perhaps you could first extract the audio into wav file and slice them into short audio clips using a tool like FFMPEG with filename corresponding to the time segment of the video. The audio clips can be sent into the network to extract features.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kepler62f; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/STU; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/kepler62f; <NewLine> ,"REPLY_DATE 1: May 22, 2020,  8:50am; <NewLine> REPLY_DATE 2: May 26, 2020,  2:18am; <NewLine> REPLY_DATE 3: May 26, 2020,  8:00am; <NewLine> REPLY_DATE 4: August 19, 2020,  1:00pm; <NewLine> REPLY_DATE 5: September 12, 2020,  4:10am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
95799,Problems with Regression Network using mini-batches,2020-09-10T13:43:26.776Z,0,25,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a dataset that consists of 18 audio files with a duration of 5 minutes, annotated with affective labels (arousal) per timestep (25hz or per 0.04s). 9 of these files are used for evaluating performance.</p><NewLine><p>For each audio file I create a feature vector with MFCC (20 bins) and MFCC deltas (also 20) per labeled timestep with overlapping.</p><NewLine><h1>Dataset</h1><NewLine><pre><code class=""lang-auto"">def mfcc_features(filename, n_mfcc=20, n_mels=128, frame_time=0.08, hop_time=0.04):<NewLine>  filepath = os.path.join(gf.audio_path[0], filename+'.wav')<NewLine>  waveform, sample_rate = librosa.load(filepath, sr=None)<NewLine>  frame_length = int(sample_rate * frame_time)<NewLine>  hop_length = int(sample_rate * hop_time)<NewLine>  melkwargs = {""n_fft"" : frame_length, ""n_mels"" : n_mels, ""hop_length"": hop_length, <NewLine>              ""f_min"" : 0, ""f_max"" : None, ""window_fn"" : torch.hamming_window}<NewLine>  mfcc = torchaudio.transforms.MFCC(sample_rate=sample_rate, n_mfcc=n_mfcc,  dct_type=2, norm='ortho', log_mels=True, melkwargs=melkwargs)(torch.from_numpy(waveform))[:,:-1]<NewLine>  mfcc_deltas = torchaudio.functional.compute_deltas(mfcc, win_length=3)<NewLine>  feature_vector = torch.cat([mfcc, mfcc_deltas])<NewLine>  <NewLine>  return torch.FloatTensor(feature_vector).T<NewLine><NewLine>def label_vector(filename, target_value='arousal'):<NewLine>  target_values = ['arousal', 'valence']<NewLine>  if target_value not in target_values:<NewLine>    raise ValueError(""Invalid target value. Expected one of: %s"" % target_values)<NewLine><NewLine>  filepath = os.path.join(gf.gold_standard_path[target_values.index(target_value)], filename+'.csv')<NewLine>  df = pd.read_csv(filepath)<NewLine><NewLine>  return torch.FloatTensor(df['gold_standard'].values).unsqueeze(0).T<NewLine><NewLine>class AudioDataset(torch.utils.data.Dataset):<NewLine>  def __init__(self, list_IDs):<NewLine>    self.list_IDs = list_IDs<NewLine><NewLine>  def __len__(self):<NewLine>    return len(self.list_IDs)<NewLine>  <NewLine>  def __getitem__(self, index):<NewLine>    # Select sample<NewLine>    ID = self.list_IDs[index]<NewLine><NewLine>    # Load data and get label<NewLine>    X = mfcc_features(ID)<NewLine>    y = label_vector(ID)<NewLine><NewLine>    return X, y<NewLine></code></pre><NewLine><p>Therefore I will have an X and y for each file with the following shape:</p><NewLine><pre><code class=""lang-auto"">$ print(mfcc_features('P16').shape, label_vector('P16').shape)<NewLine>torch.Size([7500, 40]) torch.Size([7500, 1])<NewLine></code></pre><NewLine><p>I am currently having trouble using this dataset in a Network with mini-batches in the DataLoader.</p><NewLine><h1>Network architecture</h1><NewLine><pre><code class=""lang-auto"">import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine><NewLine>class Network(nn.Module):<NewLine>    def __init__(self, input_size, hidden_size):<NewLine>      super(Network, self).__init__()<NewLine>      self.dense_h1 = nn.Linear(in_features=input_size, out_features=hidden_size)<NewLine>      self.relu_h1 = nn.ReLU()<NewLine>      self.dropout = nn.Dropout(p=0.5)<NewLine>      self.dense_out = nn.Linear(in_features=hidden_size, out_features=1)<NewLine><NewLine>    def forward(self, x):<NewLine>      out = self.relu_h1(self.dense_h1(x))<NewLine>      out = self.dropout(out)<NewLine>      y_pred = self.dense_out(out)<NewLine>      return y_pred<NewLine></code></pre><NewLine><p>The rest of the code with the training sequence is as follows:</p><NewLine><h1>Initialization and training</h1><NewLine><pre><code class=""lang-auto"">use_cuda = torch.cuda.is_available()<NewLine>device = torch.device(""cuda:0"" if use_cuda else ""cpu"")<NewLine>torch.backends.cudnn.benchmark = True<NewLine><NewLine># Parameters<NewLine>params = {'batch_size': 3,<NewLine>          'shuffle': True,<NewLine>          'num_workers': 6}<NewLine><NewLine>learningRate = 1e-4<NewLine>max_epochs = 100<NewLine><NewLine># Model<NewLine>input_size, hidden_size = 40, 20<NewLine>model = Network(input_size, hidden_size)<NewLine><NewLine>if torch.cuda.is_available():<NewLine>    model.cuda()<NewLine><NewLine>criterion = ConcordanceCorrelationCoefficient()<NewLine>optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)<NewLine><NewLine># Datasets<NewLine>partition = { <NewLine>    ""train"": ['P39', 'P23', 'P41', 'P46', 'P37', 'P16', 'P21', 'P25', 'P56'],<NewLine>    ""validation"": ['P45', 'P26', 'P64', 'P34', 'P42', 'P65', 'P30', 'P19', 'P28']<NewLine>}<NewLine><NewLine># Generators<NewLine>training_set = AudioDataset(partition['train'])<NewLine>training_generator = torch.utils.data.DataLoader(training_set, **params)<NewLine><NewLine>validation_set = AudioDataset(partition['validation'])<NewLine>validation_generator = torch.utils.data.DataLoader(validation_set, **params)<NewLine><NewLine># Loop over epochs<NewLine>for epoch in range(max_epochs):<NewLine>    # Training<NewLine>    for local_batch, local_labels in training_generator:<NewLine>        # Transfer to GPU<NewLine>        local_batch, local_labels = local_batch.to(device), local_labels.to(device)<NewLine><NewLine>        # Forward pass: Compute predicted y by passing x to the model<NewLine>        outputs = model(local_batch)<NewLine><NewLine>        # Compute and print loss<NewLine>        loss = criterion(outputs, local_labels)<NewLine><NewLine>        # Zero gradients, perform a backward pass, and update the weights.<NewLine>        optimizer.zero_grad()<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine><NewLine>    # Validation<NewLine>    with torch.set_grad_enabled(False):<NewLine>        for local_batch, local_labels in validation_generator:<NewLine>            # Transfer to GPU<NewLine>            local_batch, local_labels = local_batch.squeeze().to(device), local_labels.squeeze().to(device)<NewLine><NewLine>            # Forward pass: Compute predicted y by passing x to the model<NewLine>            outputs = model(local_batch)<NewLine><NewLine>            # Compute and print loss<NewLine>            loss = criterion(outputs, local_labels)<NewLine>            print('Validation loss %.3f' % loss.item())<NewLine><NewLine>    print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))<NewLine></code></pre><NewLine><p>This sequence runs but the loss doesn’t really go down. I think it’s because the extra batch dimension creates a problem with my loss function, which is CCC. I have to use this metric for my research in addition to MSE.<br/><NewLine><code>torch.Size([3, 7500, 40]) torch.Size([3, 7500, 1])</code></p><NewLine><h1>Loss function</h1><NewLine><pre><code class=""lang-auto"">class ConcordanceCorrelationCoefficient(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(ConcordanceCorrelationCoefficient, self).__init__()<NewLine>        self.mean = torch.mean<NewLine>        self.var = torch.var<NewLine>        self.sum = torch.sum<NewLine>        self.sqrt = torch.sqrt<NewLine>        self.std = torch.std<NewLine><NewLine>    def forward(self, prediction, ground_truth):<NewLine>        mean_gt = self.mean(ground_truth, 0)<NewLine>        mean_pred = self.mean(prediction, 0)<NewLine>        var_gt = self.var(ground_truth, 0)<NewLine>        var_pred = self.var(prediction, 0)<NewLine>        v_pred = prediction - mean_pred<NewLine>        v_gt = ground_truth - mean_gt<NewLine>        cor = self.sum (v_pred * v_gt) / (self.sqrt(self.sum(v_pred ** 2)) * self.sqrt(self.sum(v_gt ** 2)))<NewLine>        sd_gt = self.std(ground_truth)<NewLine>        sd_pred = self.std(prediction)<NewLine>        numerator = 2 * cor * sd_gt * sd_pred<NewLine>        denominator= var_gt + var_pred + (mean_gt - mean_pred) ** 2<NewLine>        ccc = numerator / denominator<NewLine>        return 1-ccc<NewLine></code></pre><NewLine><h1>Questions</h1><NewLine><ul><NewLine><li>Does it make sense to use each file as a different mini-batch or should I go about this in a different way? I could try to concatenate all feature sets into one batch.</li><NewLine><li>How do I make the validation sequence more robust? Currently I only print the loss.</li><NewLine><li>Are there other machine learning techniques that I should adopt or try?</li><NewLine><li>Am I making it too complex for my use case? If so, what could I be doing differently?</li><NewLine></ul><NewLine><p>Thanks for taking your time to read my topic, if you have any questions about my code or project, feel free to ask them.</p><NewLine></div>",https://discuss.pytorch.org/u/bartvdbraak,(Bart van der Braak),bartvdbraak,"September 10, 2020,  1:43pm",,,,,
95146,Any new 1.60 Keyword spotting examples out there now we have torchaudio?,2020-09-04T07:19:00.909Z,3,65,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Just wondered if anyone has created or has any example code of a more recent pytorch using tourchaudio MFCC hopefully in a streaming model that can use Alsa sources?</p><NewLine><p>Or if anybody is up for the idea of kickstarting something.</p><NewLine><p>I quite like the Linto tensorflow HMG (Hotword model generator) as it allows you to create a profile of MFCC &amp; model parameters.<br/><NewLine></p><aside class=""onebox allowlistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""32""/><NewLine><a href=""https://github.com/linto-ai/linto-desktoptools-hmg"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""400"" src=""https://avatars1.githubusercontent.com/u/36000254?s=400&amp;v=4"" width=""400""/><NewLine><h3><a href=""https://github.com/linto-ai/linto-desktoptools-hmg"" rel=""nofollow noopener"" target=""_blank"">linto-ai/linto-desktoptools-hmg</a></h3><NewLine><p>GUI Tool to create, manage and test Keyword Spotting models using TF 2.0 - linto-ai/linto-desktoptools-hmg</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>Would be great to have something similar with Pytorch with a GUI where maybe you could test GRU, CRNN, DS-CNN models with datasets.<NewLine><p>But you always live in hope <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>Stuart</p><NewLine></div>",https://discuss.pytorch.org/u/rolyan_trauts,(rolyan trauts),rolyan_trauts,"September 4, 2020,  7:23am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Stuart,</p><NewLine><p>the idea sounds interesting.<br/><NewLine>Would you be interested to work on a tutorial for this kind of functionality?  <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah without a doubt, more than willing to put in the time.</p><NewLine><p>I was hoping Honk might get an update as a Honk2 and drop Librosa as it can be problematic on some platforms.<br/><NewLine></p><aside class=""onebox allowlistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""16""/><NewLine><a href=""https://github.com/castorini/honk"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars3.githubusercontent.com/u/26842848?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/castorini/honk"" rel=""nofollow noopener"" target=""_blank"">castorini/honk</a></h3><NewLine><p>PyTorch implementations of neural network models for keyword spotting - castorini/honk</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>But it would be a great addition as when it comes to opensource KWS that model creation is an easy process options are very limited.<br/><NewLine>Its sort of strange as its easier to find more complex ready to go installs of Kaldi / Deepspeech than a KWS where the models can be easily created from a dataset.<br/><NewLine>There are quite a few opensource options but often the models are blackboxes and why I posted HMG above as its not just the KWS but model creation tools that would be such a great contribution.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""95146"" data-username=""rolyan_trauts""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/rolyan_trauts/40/28320_2.png"" width=""20""/> rolyan_trauts:</div><NewLine><blockquote><NewLine><p>Yeah without a doubt, more than willing to put in the time.</p><NewLine></blockquote><NewLine></aside><NewLine><p>That sounds great!<br/><NewLine>Would you mind creating a feature request with your proposal <a href=""https://github.com/pytorch/audio/issues"">here</a> and tag <a class=""mention"" href=""/u/vincentqb"">@vincentqb</a> there so that he could take a look at it? <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Posted so fingers crossed.</p><NewLine><p>Many Thanks.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/rolyan_trauts; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/rolyan_trauts; <NewLine> ,"REPLY_DATE 1: September 7, 2020,  8:01am; <NewLine> REPLY_DATE 2: September 8, 2020, 12:55pm; <NewLine> REPLY_DATE 3: September 9, 2020,  8:40am; <NewLine> REPLY_DATE 4: September 9, 2020,  2:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
94626,Training pytorch model in Google Colab vs local GPUs results in less accurate models using same hyperparams. How to fix?,2020-08-31T00:50:14.477Z,2,65,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I recently discovered Google Colab and I uploaded my Pytorch project related to training models for processing audio. I got it training models using google’s TPUs, but I noticed that the models were less accurate than the ones I trained on my local machine. It turns out the the state dict weights and biases have about half the decimal places as the locally trained model. After some searching I read that Colab uses float16 By default instead of float32 precision to increase speed, but since the audio I’m training is in float32, it really needs to train in float32 precision. Is there a way to change this in Colab? Or is there a way to change my pytorch model to ensure float32 precision is kept? My model uses a stack of 1d convolutional layers, if that matters.</p><NewLine></div>",https://discuss.pytorch.org/u/Keith72,(Keith),Keith72,"August 31, 2020, 12:50am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>PyTorch uses float32 by default on CPU and GPU. I’m not deeply familiar with TPUs, but I guess you might be using <code>bfloat16</code> on them? Could you try to call <code>float()</code> on the model and inputs and check, if the TPU run is forcing you to use this format?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I went back and looked at the stat_dict of both the locally trained GPU model and the cloud TPU model, and they do have the same precision, around 4 decimal places, so what I said in the original question was incorrect.</p><NewLine><p>After some more reading it sounds like the built in bfloat16 type is part of what makes TPUs so fast, and I don’t understand all the math but I think it can produce the same range of values, so that might not be my issue.  I also should note that I’m using a pytorch_lightning module, although I wouldn’t think that would matter.  I might try calling float() to see if that changes the output, thanks!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here’s the reference I found on bfloat32:<br/><NewLine></p><aside class=""onebox allowlistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://cloud.google.com/blog/static/assets/favicon.ico"" width=""16""/><NewLine><a href=""https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus"" rel=""nofollow noopener"" target=""_blank"">Google Cloud Blog</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail"" height="""" src=""https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-20001000.png"" width=""""/><NewLine><h3><a href=""https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus"" rel=""nofollow noopener"" target=""_blank"">BFloat16: The secret to high performance on Cloud TPUs | Google Cloud Blog</a></h3><NewLine><p>How the high performance of Google Cloud TPUs is driven by Brain Floating Point Format, or bfloat16</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>While <code>bflaot16</code> uses the same range as <code>float32</code>, it does not provide the same “step size”.<br/><NewLine>As I’m not deeply familiar with this numerical format, I don’t know if you would have to adapt your model to this format.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Keith72; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Keith72; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: August 31, 2020,  1:22am; <NewLine> REPLY_DATE 2: August 31, 2020,  1:30am; <NewLine> REPLY_DATE 3: August 31, 2020,  1:34am; <NewLine> REPLY_DATE 4: August 31, 2020,  1:43am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
93827,Inverse MelSpectrogram,2020-08-23T22:52:22.316Z,0,50,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi I’m trying to make an autoencoder for speech data. The network’s input and output are Mel spectrograms. How can I obtain the audio waveform from the generated mel spectrogram?</p><NewLine></div>",https://discuss.pytorch.org/u/neovand,(Neo Mohsenvand),neovand,"August 23, 2020, 10:52pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""onebox"" href=""https://librosa.org/doc/latest/generated/librosa.feature.inverse.mel_to_audio.html?highlight=mel%20spectrogram"" rel=""nofollow noopener"" target=""_blank"">https://librosa.org/doc/latest/generated/librosa.feature.inverse.mel_to_audio.html?highlight=mel%20spectrogram</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/JuanFMontesinos; <NewLine> ,"REPLY_DATE 1: August 23, 2020, 11:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
92110,Conv1d internal weight calculations,2020-08-09T00:11:32.697Z,2,82,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am trying to figure out how a nn.conv1d processes an input for a specific example related to audio processing in a WaveNet model. I have input data of shape (1,1,8820), which passes through an input layer (1,16,1), to output a shape of (1,16,8820). That part I understand, because you can just multiply the two matrices. The next layer is a conv1d, kernel size=3, input channels=16, output channels=16, so the state dict shows a matrix with shape (16,16,3) for the weights. When the input of (1,16,8820) goes through that layer, the result is another (1,16,8820).  What multiplication steps occur within the layer to apply the weights to the audio data? In other words, if I wanted to apply the layer(forward calculations only) using only numpy for this example, how would I do that?</p><NewLine></div>",https://discuss.pytorch.org/u/Keith72,(Keith),Keith72,"August 9, 2020, 12:11am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/keith72"">@Keith72</a>, this is how pytorch conv1d actually do in your case:</p><NewLine><pre><code class=""lang-python"">x = torch.rand(1, 16, 8820)<NewLine>weight = torch.rand(16, 16, 3)<NewLine><NewLine># first pad zeros along the time dimension<NewLine>x = torch.pad(x, [1, 1])    #shape = (1, 16, 8822)<NewLine><NewLine>#unfolded, so you have 8820 moving windows with size = (16, 3)<NewLine>x = x.unfold(2, 3, 1)    #shape = (1, 16, 8820, 3)<NewLine><NewLine># matrix multiplication, I use tensordot for simplicity<NewLine>y = torch.tensordot(x,weight, dims=([1, 3], [1, 2]))     #shape = (1, 16, 8820)<NewLine></code></pre><NewLine><p>In numpy you can simply replace <code>pad</code> and <code>tensordot</code> with corresponding numpy function; for <code>unfold</code> you can use <code>numpy.lib.stride_tricks.as_strided</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the quick response! My initial implementation seems to fit using your steps, except the last step gave me a shape (16,1,8820), so I just swapped the first two dimensions. Now if I wanted to account for layer dilation, how would that work?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>That can be achieved easily by using indexing:</p><NewLine><pre><code class=""lang-python"">x = torch.pad(x, [1 * dilation] * 2)<NewLine>x = x.unfold(2, 2 * dilation + 1, 1)[..., ::dilation]    #shape = (1, 16, 8820, 3)<NewLine>...<NewLine></code></pre><NewLine><p>In numpy you can alternate the stride size of the ndarray to do dilated convolution. Here’s an example <a href=""https://github.com/yoyololicon/building-nn-with-numpy/blob/fffb87126e6a9fd4f580654403d2cf35693829ee/nn.py#L85"" rel=""nofollow noopener"">implemention</a>, you can check it for details.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>The example helps a lot, and thank you for taking the time to explain all that.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/yoyololicon; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Keith72; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/yoyololicon; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Keith72; <NewLine> ,"REPLY_DATE 1: August 10, 2020, 11:52am; <NewLine> REPLY_DATE 2: August 9, 2020, 11:59am; <NewLine> REPLY_DATE 3: August 10, 2020,  3:21am; <NewLine> REPLY_DATE 4: August 10, 2020, 11:52am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
92225,Are torch.transform.MelSpectrogram same as torch.compliance.kaldi.fbank,2020-08-10T04:23:00.942Z,0,43,"<div class=""post"" itemprop=""articleBody""><NewLine><p>By reading this <a href=""https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html"" rel=""nofollow noopener"">artical</a>, it looks like the fbank is just the mel scaled spectrogram. Could anyone confirm that?</p><NewLine><p>I understand the results from transform.MelSpectrogram and from compliance.fbank might not end up the same even with the same parameter settings. Are the general concepts between the two functions match?</p><NewLine></div>",https://discuss.pytorch.org/u/victkid,(Vic huang),victkid,"August 10, 2020,  4:23am",,,,,
91216,Load data from variables into classes,2020-07-31T10:59:35.945Z,1,77,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My task is to take an episode of a TV show and its subtitles. Then make the subtitle timings more accurate (from 200ms to 20ms). So I want to learn what is speech and what is not.</p><NewLine><p>I’ve now taken the audio, converted it into a spectrogram and separated each column of the spectogram to be a single data item. So now I have two arrays:</p><NewLine><pre><code class=""lang-python"">print(train_speech.size()) # torch.Size([93482, 201])<NewLine>print(train_silence.size()) # torch.Size([35038, 201])<NewLine></code></pre><NewLine><p>All I want to do is a simple multi-linear NN to make a difference. <code>train_speech</code> is FFT’s of people talking and <code>train_silence</code> is no talking (used subtitles for the distinction).</p><NewLine><p>My question is what DataLoader can I use to take these into torch?</p><NewLine></div>",https://discuss.pytorch.org/u/kristerv,(Krister Viirsaar),kristerv,"July 31, 2020, 10:59am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There is one <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader""><code>DataLoader</code></a>, which accepts a <code>Dataset</code> and provides different functionalities such as shuffling, creating batches using multiple workers, etc.</p><NewLine><p>To create a custom <code>Dataset</code> you could have a look at <a href=""https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"">this tutorial</a>. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>What I don’t get is that my data is already a simple tensor… It doesn’t make sense to me that I need to create a separate abstraction just to fetch numbers from a few arrays…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If your data is already stored as tensors, you can just use <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset""><code>TensorDataset</code></a> or completely skip the abstraction and just feed to data to your model.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Since I had two classes in separate variables I ended up making the custom class.</p><NewLine><pre><code class=""lang-auto"">class MyDataset(Dataset):<NewLine>    def __init__(self, speech, silence):<NewLine>        self.data = list(map(lambda x: (x, 1), speech)) + list(map(lambda x: (x, 0), silence))<NewLine>        <NewLine>    def __getitem__(self, index):<NewLine>        return self.data[index]<NewLine>    <NewLine>    def __len__(self):<NewLine>        return len(self.data)<NewLine><NewLine>train_ds = MyDataset(train_speech, train_silence)<NewLine>train_dl = DataLoader(train_ds, shuffle=True, batch_size=1024)<NewLine></code></pre><NewLine><p>Thanks for helping me get through this.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kristerv; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/kristerv; <NewLine> ,"REPLY_DATE 1: August 1, 2020,  7:24am; <NewLine> REPLY_DATE 2: August 3, 2020, 11:49am; <NewLine> REPLY_DATE 3: August 4, 2020,  8:52am; <NewLine> REPLY_DATE 4: August 5, 2020,  7:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
91345,How to reduce the storage of weighted shared models,2020-08-01T23:53:07.252Z,0,43,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Model training uses parameter sharing, but the saved size is the same as the model without sharing. How can we reduce the storage size of the model with shared weights?</p><NewLine></div>",https://discuss.pytorch.org/u/lyjzsyzlt,(永杰 吕),lyjzsyzlt,"August 1, 2020, 11:53pm",,,,,
90847,Torch and pytorch both installed?,2020-07-28T15:10:52.806Z,0,62,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!<br/><NewLine>I created a conda environment using a provided .yml file <a href=""https://github.com/loeweX/Greedy_InfoMax"" rel=""nofollow noopener"">on this repo</a>.<br/><NewLine>However, when installing torchaudio , torch 1.5.1 was automatically installed and I needed pytorch 1.0.0 (required for the code). I ended up having</p><NewLine><ul><NewLine><li><NewLine><p>torch 1.5.1</p><NewLine></li><NewLine><li><NewLine><p>pytorch 1.0.0<br/><NewLine>on the same environment. Is his a problem? I think it is, because now I’m getting the following error when I execute the audio training:</p><NewLine><blockquote><NewLine><p>File “/home/fjaviersaezm/anaconda3/envs/infomax/lib/python3.6/site-packages/torchaudio/functional.py”, line 40, in <br/><NewLine><span class=""mention"">@torch.jit.ignore</span></p><NewLine></blockquote><NewLine></li><NewLine></ul><NewLine><blockquote><NewLine><p>AttributeError: module ‘torch.jit’ has no attribute ‘ignore’</p><NewLine></blockquote><NewLine><p>Can I uninstall torch 1.5.1? How can I solve the “ignore” problem?<br/><NewLine>Thanks guys!</p><NewLine></div>",https://discuss.pytorch.org/u/fjsaezm,(Javier Sáez),fjsaezm,"July 28, 2020,  3:10pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>torchaudio requires more recent version of torch, see <a href=""https://github.com/pytorch/audio/blob/master/README.md#dependencies"" rel=""nofollow noopener"">dependencies</a>. It appears the environment is looking at pytorch 1.0 and functionalities required by torchaudio are missing.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vincentqb; <NewLine> ,"REPLY_DATE 1: July 28, 2020,  4:39pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
90642,Speech embedding LSTM model giving different results during inference(I am using model.eval() ),2020-07-27T07:12:53.127Z,0,61,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using an LSTM model with 3 hidden layers to produce speech embeddings. The input to the model are MFCC features taken on a window of 0.025 seconds and the hop length is 0.01 seconds. I have two inputs for inference and their size is as follows:</p><NewLine><ol><NewLine><li>torch.Size([54, 24, 40])</li><NewLine><li>torch.Size([439, 24, 40])</li><NewLine></ol><NewLine><p>The first 18  frames in both the inputs is same. I load the model as follows:</p><NewLine><pre><code class=""lang-auto"">embedder_net = SpeechEmbedder()<NewLine>embedder_net.load_state_dict(torch.load(hp.model.model_path))<NewLine>embedder_net.eval()<NewLine></code></pre><NewLine><p>The state_dict looks like following:</p><NewLine><pre><code class=""lang-auto"">LSTM_stack.weight_hh_l0 	 torch.Size([3072, 768])<NewLine>LSTM_stack.bias_ih_l0 	 torch.Size([3072])<NewLine>LSTM_stack.bias_hh_l0 	 torch.Size([3072])<NewLine>LSTM_stack.weight_ih_l1 	 torch.Size([3072, 768])<NewLine>LSTM_stack.weight_hh_l1 	 torch.Size([3072, 768])<NewLine>LSTM_stack.bias_ih_l1 	 torch.Size([3072])<NewLine>LSTM_stack.bias_hh_l1 	 torch.Size([3072])<NewLine>LSTM_stack.weight_ih_l2 	 torch.Size([3072, 768])<NewLine>LSTM_stack.weight_hh_l2 	 torch.Size([3072, 768])<NewLine>LSTM_stack.bias_ih_l2 	 torch.Size([3072])<NewLine>LSTM_stack.bias_hh_l2 	 torch.Size([3072])<NewLine>projection.weight 	 torch.Size([256, 768])<NewLine>projection.bias 	 torch.Size([256])<NewLine><NewLine></code></pre><NewLine><p>When I use the model for inference on the two inputs, I get results of the following size:</p><NewLine><ol><NewLine><li>torch.Size([54, 256])</li><NewLine><li>torch.Size([439, 256])</li><NewLine></ol><NewLine><p>Since the first 18 frames are same in both the inputs, I expect the first 18 embeddings in both results to be the same. But this is not the case.</p><NewLine><p>In fact the results in both cases are very different from the result I get when I just take frames 18 frames as the  input. Any idea why that would happen?</p><NewLine><p>Help appreciated</p><NewLine></div>",https://discuss.pytorch.org/u/ashu170292,(ASHUTOSH SINGH),ashu170292,"July 27, 2020,  7:16am",,,,,
89806,Question: Does torchaudio.load spawn a new process for loading mp3?,2020-07-19T20:00:23.189Z,0,46,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Lots of python libraries for reading mp3 files require spawning new processes that execute cli utilities. For example, AudioSegment runs ffpeg underneath. This is suboptimal for my particular use case because I convert files and I write to a database which works better with in memory files.</p><NewLine><p>From reading torchaudio source code I could not get a straightforward answer since it uses different backends and C++ wrapper code.</p><NewLine><p>Could you please confirm  torchaudio.load does/does not spawn a new process for loading mp3?</p><NewLine></div>",https://discuss.pytorch.org/u/antimora,(DT),antimora,"July 19, 2020,  8:00pm",,,,,
89415,Torchaudio onlinje streaming,2020-07-16T07:29:01.339Z,0,41,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is there a way how to do online audio streaming with torch audio?<br/><NewLine>I would like to get chunks of audio recorded from my microphone and calculate stft on the fly on the received chunks. Librosa has similar functionality but I was wondering if anything like this is also possible with torchaudio?</p><NewLine></div>",https://discuss.pytorch.org/u/Jan_Vainer,(Jan Vainer),Jan_Vainer,"July 16, 2020,  7:29am",,,,,
89368,Loading audio from BytesIO,2020-07-15T19:11:27.224Z,0,52,"<div class=""post"" itemprop=""articleBody""><NewLine><p>It appears that loading audio bytes from the memory currently the APIs do not support. Is there any way of using torchaudio for audio other than stored in files? My current use case to use DB to store audio as blobs.</p><NewLine></div>",https://discuss.pytorch.org/u/antimora,(DT),antimora,"July 15, 2020,  7:11pm",,,,,
88568,Applying Attention (Single and MultiHead Attention),2020-07-09T14:03:44.530Z,3,107,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Applying Attention from paper</p><NewLine><aside class=""onebox pdf""><NewLine><header class=""source""><NewLine><a href=""https://www.danielpovey.com/files/2018_interspeech_xvector_attention.pdf"" rel=""nofollow noopener"" target=""_blank"">danielpovey.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://www.danielpovey.com/files/2018_interspeech_xvector_attention.pdf"" rel=""nofollow noopener"" target=""_blank""><span class=""pdf-onebox-logo""></span></a><NewLine><h3><a href=""https://www.danielpovey.com/files/2018_interspeech_xvector_attention.pdf"" rel=""nofollow noopener"" target=""_blank"">2018_interspeech_xvector_attention.pdf</a></h3><NewLine><p class=""filesize"">266.24 KB</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Suppose my Hidden audio representation  shape is  (after few CNN operations/layers)</p><NewLine><pre><code class=""lang-auto""> H = torch.Size([128, 32, 64])    [Batch Size X FeatureDim X Length]<NewLine><NewLine></code></pre><NewLine><p>and I want to apply self-attention weights to the audio hidden frames as</p><NewLine><pre><code class=""lang-auto"">A = softmax(ReLU(AttentionWeight1 * (AttentionWeight2 * H))<NewLine></code></pre><NewLine><p>In order to learn these two self attention weight matrices. Do I need to register these two weights as Parameters in the init function like below</p><NewLine><pre><code class=""lang-auto""><NewLine>class Model(nn.Module):<NewLine>        def __init__(self, batch_size):<NewLine>           super(Model, self).__init__()<NewLine>           self.attention1   = nn.Parameter(torch.Tensor(self.batch_size,16, 32))<NewLine>           self.attention2   = nn.Parameter(torch.Tensor(self.batch_size,1, 16))  <NewLine></code></pre><NewLine><p>and in the forward do I need to do like this</p><NewLine><pre><code class=""lang-auto"">def forward(self, input):<NewLine>    ....<NewLine>   H = CNN(input)     #[B X Features X length]<NewLine>   attention = nn.Softmax(nn.ReLU(torch.mm(self.attention2, torch.mm(self.attention1, H))<NewLine>   H = H*attention<NewLine>   return H<NewLine><NewLine><NewLine></code></pre><NewLine><p>Please help. How can we apply attention here. A the above code is throwing error</p><NewLine><pre><code class=""lang-auto"">RuntimeError: matrices expected, got 3D, 3D tensors at /opt/conda/conda-bld/pytorch_1591914985702/work/aten/src/TH/generic/THTensorMath.cpp:36<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/shakeel608,(Shakeel Ahmad Sheikh),shakeel608,"July 9, 2020,  2:03pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>torch.mm</code> expects two matrices (2D tensors), while you seem to use two 3D tensors.<br/><NewLine>You could use <code>torch.bmm</code> or <code>torch.matmul</code> instead, which would work for these tensors.</p><NewLine><p>However, usually the parameters are not depending on the batch size.<br/><NewLine>Are you sure you want to initialize them with the batch size in <code>dim0</code>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a><br/><NewLine>How to make these weights learnable.<br/><NewLine>Am I doing it right here</p><NewLine><pre><code class=""lang-auto"">class Model(nn.Module):<NewLine>        def __init__(self, batch_size):<NewLine>           super(Model, self).__init__()<NewLine>           self.attention1   = nn.Parameter(torch.Tensor(self.batch_size,16, 32))<NewLine>           self.attention2   = nn.Parameter(torch.Tensor(self.batch_size,1, 16))  <NewLine></code></pre><NewLine><p>Attention Mechanism in Forward. The <strong>input</strong> here is the output after few CNN operations with Shape</p><NewLine><pre><code class=""lang-auto"">[BatchSize X DimFeature X Length]   = [128 X 32 X 64]<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">       """""" Get Attention Weights """"""<NewLine>        attn = input    <NewLine>        attention = attn.permute(0, 2, 1).matmul(self.attention1)<NewLine>        attention = attention.matmul(self.attention2)<NewLine>        attention = self.relu(attention)      <NewLine>        attention = attention.view(attention.size(0), -1)<NewLine>        attention = F.softmax(attention, 1)  <NewLine>        """""" Multiply Attention Weights with Audio Frames""""""<NewLine>        input = input * attention.unsqueeze(1)          #To Make it comptable with BatchSize X FeatureDim X FixedLength<NewLine><NewLine></code></pre><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> Is this okay now?<br/><NewLine>I have also another question when I use<code> nn.Softmax</code> in place of <code>F.softmax(attention, 1)</code>, why doesn’t it work ?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The code looks alright code-wise and you should be able to see valid gradients in <code>model.attention1.grad</code> and <code>model.attention2.grad</code> after a <code>backward()</code> call.</p><NewLine><p><code>nn.Softmax</code> should work like <code>F.softmax</code>, but you might have forgotten to create the module before calling it via:</p><NewLine><pre><code class=""lang-python"">nn.Softmax(dim=1)(input)<NewLine></code></pre><NewLine><p>What kind of error are you seeing with <code>nn.Softmax</code>?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your feedback.</p><NewLine><p>I think I was using the syntax wrongly  as below</p><NewLine><p><code>nn.Softmax(input, 1)</code></p><NewLine><p>but it is actually like this</p><NewLine><pre><code class=""lang-auto"">nn.Softmax(dim=1)(input)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/shakeel608; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/shakeel608; <NewLine> ,"REPLY_DATE 1: July 11, 2020,  9:08am; <NewLine> REPLY_DATE 2: July 13, 2020, 11:20am; <NewLine> REPLY_DATE 3: July 14, 2020,  2:25am; <NewLine> REPLY_DATE 4: July 15, 2020,  7:48am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
88899,Loading dataset into GPU,2020-07-12T18:58:05.477Z,0,60,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m using a dataset of audio feature vectors (each sample is represented with 40 features). At the moment I create a matrix in the <em>init</em>_ method of my custom dataset and push it to GPU:</p><NewLine><pre><code class=""lang-auto"">self.frame_array_device = torch.from_numpy(self.frame_array)<NewLine>self.frame_array_device.to(device)<NewLine></code></pre><NewLine><p>the dataset getitem method is defined as follows:</p><NewLine><pre><code class=""lang-auto"">def __getitem__(self, idx):<NewLine>        return self.frame_array_device[idx, :]<NewLine></code></pre><NewLine><p>Samples are drawn by the dataloader:</p><NewLine><pre><code class=""lang-auto"">train_loader = torch.utils.data.DataLoader(<NewLine>        train_dataset, batch_size=batch_size,shuffle=False)<NewLine></code></pre><NewLine><p>in the training loop I double check that the features are on the GPU:<br/><NewLine><code>batch_features = batch_features.to(device)</code></p><NewLine><p>However, training is embarassingly slow and CPU usage is at 100%…<br/><NewLine>I think that there is something wrong, could you give me some hint?<br/><NewLine>Thanks a lot</p><NewLine></div>",https://discuss.pytorch.org/u/icysapphire,(Icysapphire),icysapphire,"July 12, 2020,  6:58pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><em>I am newer to ML so take what I say with a grain</em></p><NewLine><p>Loading the data is what I think is the slowest part.  I notice that between epochs my CPU is always 100-200% even thought my device prints as <code>Device: Tesla P100-PCIE-16GB</code>. Then, during a short portion of every epoch my GPU also goes to 100%.</p><NewLine><p>Subscribed to see what experts have to say.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This line of code:</p><NewLine><pre><code class=""lang-python"">self.frame_array_device.to(device)<NewLine></code></pre><NewLine><p>won’t push the data to the <code>device</code>, if you don’t reassign the result via:</p><NewLine><pre><code class=""lang-python"">self.frame_array_device = self.frame_array_device.to(device)<NewLine></code></pre><NewLine><p>That being said, you could try to profile the data loading using the <code>data_time</code> object from the <a href=""https://github.com/pytorch/examples/blob/e7870c1fd4706174f52a796521382c9342d4373f/imagenet/main.py#L282"">ImageNet example</a> or alternatively you could create a random <code>CUDATensor</code> and just execute the training without the data loading at all to check the GPU utilization.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/David_Alford; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 12, 2020,  8:45pm; <NewLine> REPLY_DATE 2: July 13, 2020,  1:28am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
88378,Music Encoder model,2020-07-08T11:19:07.234Z,2,92,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello all,<br/><NewLine>I am trying to make a CNN model for Encoder Parameter Suggestion for Audio data<br/><NewLine>Input: Audio File(Length : 480000 samples)<br/><NewLine>Output: Parameter(Length : 469, eg :[0001111222001233312000] )</p><NewLine><p>I am getting very low Validation and Test Accuracy</p><NewLine><pre><code class=""lang-auto"">class AudioClassifier(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(AudioClassifier, self).__init__()<NewLine><NewLine>        self.conv11 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=Kernel_size_conv, stride=1,<NewLine>                                bias=True, padding=int((Kernel_size_conv - 1) / 2))<NewLine>        self.conv12 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=Kernel_size_conv, stride=1,<NewLine>                                bias=True, padding=int((Kernel_size_conv - 1) / 2))<NewLine>        self.conv13 = nn.Conv1d(in_channels=16, out_channels=4, kernel_size=Kernel_size_conv,<NewLine>                                stride=1024, bias=True)<NewLine><NewLine>    def forward(self, X):<NewLine>        out1 = torch.tanh(self.conv11(X))<NewLine>        out1 = torch.tanh(self.conv12(out1))<NewLine>        out1 = (self.conv13(out1))<NewLine>        return out1<NewLine></code></pre><NewLine><p>optimizer = torch.optim.Adam(model.parameters(),lr=1e-2,  weight_decay=1e-5)<br/><NewLine>loss_fn = nn.CrossEntropyLoss()</p><NewLine><pre><code class=""lang-auto"">    for epoch in range(E):<NewLine>        for batch in range(B)<NewLine>            pred_1= model(training_data)<NewLine>            loss = loss_fn(pred_1, Target)<NewLine>            optimizer.zero_grad()<NewLine>            loss.backward()<NewLine>            optimizer.step()<NewLine></code></pre><NewLine><p>Can Anybody suggest any other approach to rectify it.</p><NewLine></div>",https://discuss.pytorch.org/u/mukul_01,(Mukul Agarwal),mukul_01,"July 8, 2020, 11:26am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What about you training loss and training accuracy? If it is good, then your model is overfitting (which would be surprising given that it is quite simple), if not, then your model is probably too simple for the task</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Training loss is arnd 0.2 and Training Accuracy is on avg 92% .<br/><NewLine>Currently Every epoch Data is shuffled and passed to network.</p><NewLine><p>I cannot find any mistake which I might have made in coding.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are the classes balanced? If you have 1000 classes but 92% of your examples are of one class, then you can get an accuracy of 92% just by always predicting the same class. What kind of error does your model makes? Have a look at the predictions (training and validation) to check that they make sense</p><NewLine><p>If class imbalance is not an issue, then you might try different techniques against overfitting incl. (but not limited to): adding dropout, adding a regularisation term, decreasing the capacity of your network, etc.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><img alt=""Model_1_30"" data-base62-sha1=""alkxFXy1sHhHclm2o6P6Z5C79xU"" height=""480"" src=""https://discuss.pytorch.org/uploads/default/original/3X/4/8/487f08405906cf300b4552e269d9ef64f783ad22.png"" width=""640""/></p><NewLine><p>Even after reducing the parameters , Model is over-fitting after a approx 8-9 epochs and takes a while to reduce the training loss to zero.</p><NewLine><p>Any suggestions on this</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hard to say without seeing the data. How did you build your test set? Is it a sample from your original dataset or does it come from another source? What about the training set distribution (see my previous question)? How much example in each subset? With no information, it’s hard to make recommendations…</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Test data was chosen from the same dataset as Training set  .<br/><NewLine>Dataset is audio files int 16<br/><NewLine>Training set : 83 Audio files<br/><NewLine>Test set : 10 Audio files</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/qmeeus; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mukul_01; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/qmeeus; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mukul_01; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/qmeeus; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mukul_01; <NewLine> ,"REPLY_DATE 1: July 8, 2020, 12:25pm; <NewLine> REPLY_DATE 2: July 8, 2020, 12:49pm; <NewLine> REPLY_DATE 3: July 8, 2020,  1:19pm; <NewLine> REPLY_DATE 4: July 10, 2020, 11:16am; <NewLine> REPLY_DATE 5: July 10, 2020, 11:20am; <NewLine> REPLY_DATE 6: July 10, 2020, 12:23pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
87947,Tensorflow loss function,2020-07-04T14:24:45.037Z,1,90,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a dumpy question I have a loss function that depends on Tensorflow and Keras backend, the question is can I use this loss function to train my PyTorch network.</p><NewLine><p>Another question is there is any PyTorch function that equivalent to this TensorFlow command.<br/><NewLine>stft_true = tf.contrib.signal.stft(y_true,256,128,512,window_fn,pad_end=False)</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/Mohamed_Nabih,(Mohamed Nabih),Mohamed_Nabih,"July 4, 2020,  2:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey,</p><NewLine><p>Have you tried <a href=""https://pytorch.org/docs/master/generated/torch.stft.html#torch.stft""><code>torch.stft</code></a> that looks like the same thing.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes,  <a class=""mention"" href=""/u/alband"">@albanD</a><br/><NewLine>It seems the same function.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Mohamed_Nabih; <NewLine> ,"REPLY_DATE 1: July 4, 2020,  8:04pm; <NewLine> REPLY_DATE 2: July 9, 2020,  6:01pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
88362,How to feed audio into a model?&amp;Can a fully connected layer followed by a conv layer?,2020-07-08T09:39:01.974Z,0,63,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, almighty guys <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/>.<br/><NewLine>I’m a beginner in digital audio signal processing. And I met some questions during reading a <a href=""https://arxiv.org/abs/2006.15321"" rel=""nofollow noopener"">paper</a>.</p><NewLine><ol><NewLine><li>When I get the filter banks outputs from a 10s audio segment, should I send all of them into the model or just several frames?</li><NewLine><li>I’m a little confused about the model architecture in the paper. How does the dense layer connect with the encoder and decoder. Does it convert a  Bx1x1x128 tensor to a Bx128xhxw tensor?<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/f4f2289d9d4200578947424a9e1e793da00460bd"" href=""https://discuss.pytorch.org/uploads/default/original/3X/f/4/f4f2289d9d4200578947424a9e1e793da00460bd.png"" title=""image""><img alt=""image"" data-base62-sha1=""yWTkavHEMY5VOG3MttMcL5vmNaJ"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/4/f4f2289d9d4200578947424a9e1e793da00460bd_2_10x10.png"" height=""304"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/4/f4f2289d9d4200578947424a9e1e793da00460bd_2_690x304.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/4/f4f2289d9d4200578947424a9e1e793da00460bd_2_690x304.png, https://discuss.pytorch.org/uploads/default/original/3X/f/4/f4f2289d9d4200578947424a9e1e793da00460bd.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/f/4/f4f2289d9d4200578947424a9e1e793da00460bd.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">898×396 72 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><NewLine></li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/Michael-J98,,Michael-J98,"July 8, 2020,  9:39am",,,,,
88097,Query : Loop for batch per epoch,2020-07-06T11:28:20.615Z,1,95,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello all,<br/><NewLine>I have query regarding the placement of optimizer.zero_grad() and optimizer.step()</p><NewLine><p>for idx in range(epoch):<br/><NewLine>for kdx in range(batch):<br/><NewLine>y_pred = model(X)<br/><NewLine>loss = loss_fn(y_pred,y_target)<br/><NewLine>optimizer.zero_grad()<br/><NewLine>loss.backward()<br/><NewLine>optimizer.step()</p><NewLine><p>Do we need to call them once for every epoch or for every batch iteration?</p><NewLine></div>",https://discuss.pytorch.org/u/mukul_01,(Mukul Agarwal),mukul_01,"July 6, 2020, 11:31am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It depends on your use case.<br/><NewLine>The usual approach would be to zero out the old gradients, calculate the new gradients via <code>backward()</code>, and update the parameters via <code>optimizer.step()</code> for each batch (so once per iteration).</p><NewLine><p>However, you could simulate a larger batch size by accumulating the gradients using multiple batches and call the complete update steps after a specific number of iterations.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mukul_01; <NewLine> ,"REPLY_DATE 1: July 7, 2020,  7:53am; <NewLine> REPLY_DATE 2: July 7, 2020,  5:08pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
87709,How to create a RNN that applies *n* different recurrence relations to the input?,2020-07-02T09:29:51.483Z,1,101,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone,</p><NewLine><p>I am sorry for the long post but would really appreciate any help you guys can offer !</p><NewLine><p>I am trying to create a custom RNN that would apply <em>n</em> different recurrent connections (that are in fact <em>n</em> biquadratic filters) on the input. Another way of thinking about it would be to have <em>n</em> different RNN that works on the input and to concatenate thair results afterwards, however I believe that would lead to very poor performances (please tell me if I am wrong).</p><NewLine><p>For instance :</p><NewLine><p>I have a mini-batch of size [32, 16000], and want to apply 128 filters on it, which means that my output size is [32,128,16000].</p><NewLine><p>What I did so far is :</p><NewLine><ol><NewLine><li>Expand and clone the input so I have a tensor of size : [32, 128, 16000].</li><NewLine><li>Permute axes to get a size of [16000, 32, 128].</li><NewLine><li>Iterate on the sequence and use <strong>matrices products</strong> to compute the input, since the filters are linears. In fact, I use this recurrence relation that works for only one sequence of size N (except the first two samples ofc) :</li><NewLine></ol><NewLine><p><img alt=""image"" data-base62-sha1=""k4i8gXlziY1rDMFMf4ka1oDnUMp"" height=""58"" src=""https://discuss.pytorch.org/uploads/default/original/3X/8/c/8ca7b23e1df072aadec8be380a03444ccb1be391.png"" width=""454""/></p><NewLine><p>where the <em>a_i</em> and <em>b_i</em> are the learnable weights, <em>x[n]</em> is the <em>n-th</em> sample of the input, <em>y[n]</em> is the state at the time frame <em>n</em>, and <em>i</em> is for the <em>i-th</em> filter (or the <em>i-th</em> recurrence relation if you prefer).</p><NewLine><p>I already tried two methods to make it work (see below). The problem is that my versions are too slow and I don’t have a good enough understanding of <em>pytorch</em> to optimize them.</p><NewLine><p>So I would really appreciate any help you can provide on these points :</p><NewLine><ul><NewLine><li>Is there a better way of implementing an RNN with <em>n</em> different recurrence relations ?</li><NewLine><li>Do you see improvements I could make to my code (see below) that would yield to good performances ?</li><NewLine><li>May computing the outputs of the different RNNs in parallel and concatenate (with <em>torch.cat</em>) the results yield to better results ?</li><NewLine><li>May implementing in C++ (as it is the case for <em>pytorch</em> for the recurrence relation) become necessary to achieve good performance ?</li><NewLine></ul><NewLine><p>Links for the <em>pytorch</em> RNN :</p><NewLine><ul><NewLine><li><a href=""https://github.com/pytorch/pytorch/blob/d1623f4cc9334ade7376b3685bb91d3071e3b418/torch/nn/modules/rnn.py"" rel=""nofollow noopener"">RNN.py</a></li><NewLine><li><a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/RNN.cpp"" rel=""nofollow noopener"">RNN.cpp</a></li><NewLine><li><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/QuantizedLinear.cpp"" rel=""nofollow noopener"">QuantizeLinear.cpp</a> which seems to contain the function that achieve the loop on the sequence : <em>fbgemm_linear_int8_weight_fp32_activation</em>.</li><NewLine></ul><NewLine><p>Please, tell me if there is anything unclear or if you need more info.</p><NewLine><p>Thanks for reading this post, and thanks for any piece of advice you can provide !</p><NewLine><hr/><NewLine><p><strong>Code :</strong></p><NewLine><p><strong>In each of the following version, the loop on the sequence is the piece of code that is the longest to execute.</strong></p><NewLine><ul><NewLine><li>Version A :</li><NewLine></ul><NewLine><pre><code class=""lang-auto"">def forward(self, X) :<NewLine>    bs = X.size()[0]<NewLine>   <NewLine>    X = X.unsqueeze(1).expand(-1,self.kernels_number,-1).clone()<NewLine>    B0, A1, A2 = self.filters()<NewLine><NewLine>     if(self.is_cuda):<NewLine>      out = torch.zeros(self.points_per_sequence, bs, self.kernels_number).cuda()<NewLine>    else:<NewLine>      out = torch.zeros(self.points_per_sequence, bs, self.kernels_number)<NewLine><NewLine>    out[0] = torch.mul(X[0],B0) # [bs,1]*[1,128] = [bs,128]<NewLine>    out[1] = torch.mul(X[1],B0) - torch.mul(out[0],A1)<NewLine><NewLine>    for n in range(2, X.size()[0]):<NewLine>      out[n] = self.f_2(out[n-1], out[n-2], X[n], X[n-2], B0, A1, A2)<NewLine>    <NewLine>    out = torch.flip(out, dims=[2]).permute(1,2,0)<NewLine>    return out<NewLine></code></pre><NewLine><p>(Since I am using pass-band filters, I only need the three tensors B0, A1, A2 of size [1,n_channels] each, there are computed from only two weights but it does not matter here).</p><NewLine><p>The function self.f_2 :</p><NewLine><pre><code class=""lang-auto"">def f_2(self, y_1, y_2, x, x_2, b0, a1, a2):<NewLine>    """"""<NewLine>    Computing y[n] with y[n-1], y[n-2], x[n], x[n-1], x[n-2], b0, a1, a2    <NewLine>    Sizes : <NewLine>       x : [bs,128]<NewLine>       b0,a1,a2 : [1,128]<NewLine>       y_1, y_2 : [bs, 128]<NewLine>    """"""<NewLine>    return torch.mul(x-x_2,b0) - torch.mul(y_1,a1) - torch.mul(y_2,a2)<NewLine></code></pre><NewLine><p>I have not tried this version on the backward pass but the forward pass works.</p><NewLine><ul><NewLine><li>Version B :</li><NewLine></ul><NewLine><p>For this one, I used the function <em><a href=""https://pytorch.org/audio/_modules/torchaudio/functional.html#lfilter"" rel=""nofollow noopener"">lfilter</a></em>   from <em>torchaudio</em>. Since the filters are all differents, I started by looping over the filters and applying <em>lfilter</em> which did not work well : it took longer than the previous version and had RAM issues.</p><NewLine><p>Then I modified the function <em>lfilter</em> so it now accepts different filters. It now behaves, performance wise, as the version A.</p><NewLine><p>Here is my version of the <em>filter</em> :</p><NewLine><pre><code class=""lang-auto"">def m_lfilter(<NewLine>        waveform: torch.Tensor,<NewLine>        a_coeffs: torch.Tensor,<NewLine>        b_coeffs: torch.Tensor<NewLine>)  -&gt; torch.Tensor:<NewLine>    r""""""Perform an IIR filter by evaluating difference equation.<NewLine>    <NewLine>    NB : contrary to the original version this one does not requires normalized input and does not ouput normalized sequences.<NewLine><NewLine>    Args:<NewLine>        waveform (Tensor): audio waveform of dimension of `(..., number_of_filters, time)`.  <NewLine>        a_coeffs (Tensor): denominator coefficients of difference equation of dimension of `(n_order + 1)`.<NewLine>                                Lower delays coefficients are first, e.g. `number_of_filters*[a0, a1, a2, ...]`.<NewLine>                                Must be same size as b_coeffs (pad with 0's as necessary).<NewLine>        b_coeffs (Tensor): numerator coefficients of difference equation of dimension of `(n_order + 1)`.<NewLine>                                 Lower delays coefficients are first, e.g. `number_of_filters*[b0, b1, b2, ...]`.<NewLine>                                 Must be same size as a_coeffs (pad with 0's as necessary).<NewLine><NewLine>    Returns:<NewLine>        Tensor: Waveform with dimension of `(..., number_of_filters, time)`.  <NewLine><NewLine><NewLine>    Note : <NewLine>      The main difference with the original version is that we are not packing anymore  the batches (since we need to apply different filters)<NewLine>    """"""<NewLine><NewLine>    shape = waveform.size() # should returns [batch_size, number_of_filters, size_of_the_sequence]<NewLine><NewLine>    assert (a_coeffs.size(0) == b_coeffs.size(0))<NewLine>    assert (len(waveform.size()) == 3)<NewLine>    assert (waveform.device == a_coeffs.device)<NewLine>    assert (b_coeffs.device == a_coeffs.device)<NewLine>    device = waveform.device<NewLine>    dtype = waveform.dtype<NewLine>    n_channel,n_filters, n_sample = waveform.size()<NewLine>    n_order = a_coeffs.size(1)<NewLine>    assert (a_coeffs.size(0) == n_filters) # number of filters to apply - for each filter k, the coefs are in a_coeffs[k] and b_coeffs[k]<NewLine>    n_sample_padded = n_sample + n_order - 1<NewLine>    assert (n_order &gt; 0)<NewLine><NewLine>    # Pad the input and create output<NewLine>    padded_waveform = torch.zeros(n_channel, n_filters, n_sample_padded, dtype=dtype, device=device)<NewLine>    padded_waveform[:,:,(n_order - 1):] = waveform <NewLine>    padded_output_waveform = torch.zeros(n_channel, n_filters, n_sample_padded, dtype=dtype, device=device) # padded_output_waveform = torch.zeros(n_channel, n_sample_padded, dtype=dtype, device=device) <NewLine><NewLine>    # Set up the coefficients matrix<NewLine>    # Flip coefficients' order<NewLine>    a_coeffs_flipped = a_coeffs.flip(1).unsqueeze(0)<NewLine>    b_coeffs_flipped = b_coeffs.flip(1).t()<NewLine><NewLine>    # calculate windowed_input_signal in parallel<NewLine>    # create indices of original with shape (n_channel, n_order, n_sample)<NewLine>    window_idxs = torch.arange(n_sample, device=device).unsqueeze(0) + torch.arange(n_order, device=device).unsqueeze(1)<NewLine>    window_idxs = window_idxs.repeat(n_channel, 1, 1)<NewLine>    <NewLine>    window_idxs += (torch.arange(n_channel, device=device).unsqueeze(-1).unsqueeze(-1) * n_sample_padded)<NewLine>    window_idxs = window_idxs.long()<NewLine><NewLine>    # (n_filters, n_order) matmul (n_channel, n_order, n_sample) -&gt; (n_channel, n_filters, n_sample)<NewLine>    A = torch.take(padded_waveform, window_idxs).permute(0,2,1)  # taking the input coefs <NewLine>    input_signal_windows = torch.matmul(torch.take(padded_waveform, window_idxs).permute(0,2,1),b_coeffs_flipped).permute(1,0,2) <NewLine>    # input_signal_windows size : n_samples x batch_size x n_filters<NewLine>    <NewLine>    for i_sample, o0 in enumerate(input_signal_windows):<NewLine>        windowed_output_signal = padded_output_waveform[:, :, i_sample:(i_sample + n_order)].clone() # added clone here for back propagation<NewLine>        o0.sub_(torch.mul(windowed_output_signal,a_coeffs_flipped).sum(dim=2))<NewLine>        o0.div_(a_coeffs[:,0]) <NewLine><NewLine>        padded_output_waveform[:, : , i_sample + n_order - 1] = o0<NewLine><NewLine>    output = padded_output_waveform[:, :,(n_order - 1):]     <NewLine>    return output<NewLine></code></pre><NewLine><p>As for the the forward function :</p><NewLine><pre><code class=""lang-auto"">  def forward(self, X):<NewLine>      # creating filters<NewLine>    A, B = self.filters() # A = [[a1_0, a2_0, a3_0],...], A = [[b1_0, b2_0, b3_0],...] - size : [128, 3]<NewLine><NewLine>    X = X.unsqueeze(1).expand(-1,self.kernels_number,-1).clone()  # we have to expand the input to the size : [bs, n_filters, n_samples]<NewLine><NewLine>      # applying the filters<NewLine>    X = m_lfilter(X,A,B)<NewLine><NewLine>    return X<NewLine></code></pre><NewLine><p>This method works for the backward pass even if it takes ages to perform (I am working on implementing the TBPTT in parallel to improve these algorithms).</p><NewLine><hr/><NewLine></div>",https://discuss.pytorch.org/u/PaulC,,PaulC,"July 2, 2020, 11:53am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Pre-calculating non-recurrent term is a good approach. You can use Tensor.unfold to create (16000-2,32,3) feature tensor. You could then also apply 3x128 map to it (with conv1d or matmul), but most rnn implementations do just that with input-to-hidden matrix.</p><NewLine><p>Now, recurrent part is tricky. There are RNN implementations with independent hidden-to-hidden transitions - SRU, IndRNN among others, they could almost do what you want (i.e. they act as a stack of width 1 rnns), with some tweaks. But I’m not aware of implementations that look two steps back (maybe it is possible to emulate this somehow, not sure).</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""87709"" data-username=""PaulC""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/p/e9a140/40.png"" width=""20""/> PaulC:</div><NewLine><blockquote><NewLine><p>May implementing in C++ (as it is the case for <em>pytorch</em> for the recurrence relation) become necessary to achieve good performance ?</p><NewLine></blockquote><NewLine></aside><NewLine><p>And to this I would say yes. I’m sceptical about python loops with hundreds of steps already - timestep data slices are small, invocation overheads are huge, backward graph is a chain of small ops too. Actually, in my experience, such loops with GPU tensors are slower than with cpu tensors.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your answer ! It helps me <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/googlebot; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/PaulC; <NewLine> ,"REPLY_DATE 1: July 2, 2020,  9:38pm; <NewLine> REPLY_DATE 2: July 6, 2020,  7:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
86952,Default_collate,2020-06-25T20:48:06.756Z,1,167,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I tried to import default_collate from torch.utils as shown</p><NewLine><p>from torch.utils.data.dataloader import default_collate</p><NewLine><p>The problem is the default_collate is not found and I found _collate_fn_t are they similar functions?</p><NewLine></div>",https://discuss.pytorch.org/u/Mohamed_Nabih,(Mohamed Nabih),Mohamed_Nabih,"June 25, 2020,  8:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mohamed_nabih"">@Mohamed_Nabih</a>,</p><NewLine><p>I tried the import you shown and it works for me. Which version are you using?. Also, I remember I recently imported the same function using: <code>from torch.utils.data._utils.collate import default_collate</code>.</p><NewLine><p>To check if the import you used pointed at the same funciton, I checked with the following code:</p><NewLine><pre><code class=""lang-auto"">from torch.utils.data.dataloader import default_collate as dc1<NewLine>from torch.utils.data._utils.collate import default_collate as dc2<NewLine><NewLine>print(dc1 is dc2)<NewLine># &gt;&gt; True<NewLine></code></pre><NewLine><p>So yes, both ways import the actual <code>default_collate</code> method. I couldn’t find <code>_collate_fn_t</code>, but I think they refer to the same method (*.pyi files seem to be related with annotations, so they’re not actual declarations).</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/antonio_ossa"">@Antonio_Ossa</a><br/><NewLine>Thanks, Antonio for your help it works.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Antonio_Ossa; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Mohamed_Nabih; <NewLine> ,"REPLY_DATE 1: June 25, 2020, 11:45pm; <NewLine> REPLY_DATE 2: July 4, 2020,  1:38pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
86910,CNN with LSTM input shapes,2020-06-25T14:21:59.934Z,14,167,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to combine CNN and LSTM for the audio data.<br/><NewLine>Let us say the output of my CNN model is <code>torch.Size([8, 1, 10, 10]</code> which is [B X C_out X Frequency X Time ]<br/><NewLine>and the LSTM requires [L X B X InputSize].</p><NewLine><p>My question is what is the inputSize in LSTM and how shall I feed the output of CNN to the LSTM</p><NewLine><p>Please help <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a></p><NewLine></div>",https://discuss.pytorch.org/u/shakeel608,(Shakeel Ahmad Sheikh),shakeel608,"June 25, 2020,  2:33pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""86910"" data-username=""shakeel608""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/a587f6/40.png"" width=""20""/> shakeel608:</div><NewLine><blockquote><NewLine><p>My question is what is the inputSize in LSTM</p><NewLine></blockquote><NewLine></aside><NewLine><p>The mentioned <code>inputSize</code> in your shape information would correspond to the “feature” dimension.</p><NewLine><p>Since your CNN output is 4-dimensional, you would have to decide which dimensions are corresponding to the temporal dimensions and which to the features.</p><NewLine><p>Assuming you would like to use <code>C_out</code> and <code>Fequency</code> as the features, you could use:</p><NewLine><pre><code class=""lang-python"">x = torch.randn(8, 1, 10, 10)<NewLine>x = x.view(x.size(0), -1, x.size(3)) # [batch_size, features=channels*height, seq_len=width]<NewLine>x = x.permute(2, 0, 1) # [seq_len, batch_size, features]<NewLine></code></pre><NewLine><p>and pass it to the RNN.</p><NewLine><p>PS: Please don’t tag certain people, as this might discourage others to post a solution <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>, This is what I was looking for .</p><NewLine><p>Suppose after feeding the data input to the CNN model, it outputs variable length like the example below.</p><NewLine><pre><code class=""lang-auto"">CNN Shape==========&gt; torch.Size([2, 128, 5, 28])<NewLine>CNN Shape==========&gt; torch.Size([2, 128, 9, 28])<NewLine></code></pre><NewLine><p>Now When I am feeding this to the LSTM after performing the below mentioned operations</p><NewLine><pre><code class=""lang-auto"">x = x.view(x.size(0), -1, x.size(3)) # [batch_size, features=channels*height, seq_len=width]<NewLine>x = x.permute(2, 0, 1) # [seq_len, batch_size, features]<NewLine></code></pre><NewLine><p>It is giving the error<br/><NewLine><code>input.size(-1)</code> must be equal to input_size.</p><NewLine><p><em><strong>How the variable length from the CNNs is handled before feeding it the LSTM</strong></em></p><NewLine><p>Sure I will be careful in future.<br/><NewLine>Actually your explanations are very clear and to the point and I really enjoy those.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>In an RNN the temporal dimension is variable, not the feature dim.<br/><NewLine>You could use the channels (dim1) as the feature dimension and the <code>height*width</code> as the temporal dimension as a workaround.<br/><NewLine>However, based on your description you would like to use the width as the time dim: <code>[B X C_out X Frequency X Time ]</code>.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry<br/><NewLine>you are right<br/><NewLine>the temporal dimension is variable as</p><NewLine><pre><code class=""lang-auto"">CNN Shape==========&gt; torch.Size([16, 128, 40, 21])<NewLine>CNN Shape==========&gt; torch.Size([16, 128, 40, 28])<NewLine></code></pre><NewLine><p>This is what I get output from the CNN.<br/><NewLine>Now how do we handle this variable length in RNNs (this is on the fly training)</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Same as before: make sure to pass the inputs to the RNN as <code>[seq_len, batch_size, features]</code>.<br/><NewLine>If dim3 is now the time dimension and (dim1+dim2) are the features:</p><NewLine><pre><code class=""lang-python"">x = x.view(x.size(0), -1, x.size(3))<NewLine>x = x.permute(2, 0, 1)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi ptrblck,<br/><NewLine>During permutation I had to put (0, 2, 1) to match with <code>batch_size, seq_length, out_channels</code> in my case.<br/><NewLine>For video classification, my variable factor is batch_size so changing batch_size I can control the temporal part of a video. seq_length is coming from the previous block as a part of feature vector which I can’t change. I am little confused here. As temporal part should be controlled by seq_length not by the batch_size.<br/><NewLine>Please let me know. Thank you once again.</p><NewLine><p>Regards,<br/><NewLine>ananda2020</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>While the batch size can very, it doesn’t represent the temporal dimension, but just how many samples you are processing at once. If your <code>seq_length</code> is static, you are not working with a variable temporal dimension.</p><NewLine><p>Make sure to permute the input to the expected dimensions. By default RNNs expect an input of <code>[seq_len, batch_size, features]</code>. With <code>batch_first=True</code> the input should be <code>[batch_size, seq_len, features]</code>.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am extracting frames from videos. Then each frame is fed into the CNN to get the features and the output from CNN is fed into LSTM. So how can I change the seq_length?</p><NewLine><p>Thanks in advance.<br/><NewLine>Regards,<br/><NewLine>ananda2020</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>I assume your CNN creates features in the shape <code>[batch_size, features]</code> and you would like to use the batch size as the temporal dimension, since you made sure that the ordering of the input images is appropriate for the use case.<br/><NewLine>If that’s the case, just <code>unsqueeze</code> a fake batch dimension in dim1 and pass the outputs to the RNN.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you ptrblck. Yes, I named the frames such way that they are sequenced. Thank you once again.</p><NewLine><p>Regards,<br/><NewLine>Alakananda</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> , it worked as per the requirement</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi ptrblck,<br/><NewLine>I was wrong. My dataloader was not taking sequenced data. So I added a sample to get the sequence.<br/><NewLine>Thanks for posting the sampler code in another thread.</p><NewLine><p>Regards,<br/><NewLine>ananda2020</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> I have question regarding the hidden units of LSTM</p><NewLine><pre><code class=""lang-auto"">self.lstm = nn.LSTM(<NewLine>                    input_size = 64,<NewLine>                    hidden_size = 128,<NewLine>                    num_layers  = 2)<NewLine></code></pre><NewLine><p>Since the hidden units of LSTM are fixed 128. how does it handle the variable length inputs. It is bit confusing ?<br/><NewLine>Every time the input takes the batch, its input sequence length changes. so how does it handle it ?</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>The hidden size is defining the “feature dimension” of the input and is thus unrelated to the temporal dimension.<br/><NewLine><a href=""http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf"">This lecture on RNNs</a> gives you a good overview how these shapes are used.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a><br/><NewLine>this was very helpful</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/shakeel608; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/shakeel608; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ananda2020; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/ananda2020; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/ananda2020; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/shakeel608; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/ananda2020; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/shakeel608; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/shakeel608; <NewLine> ,"REPLY_DATE 1: June 26, 2020,  3:34am; <NewLine> REPLY_DATE 2: June 30, 2020,  9:42am; <NewLine> REPLY_DATE 3: June 30, 2020,  9:52am; <NewLine> REPLY_DATE 4: June 30, 2020, 10:16am; <NewLine> REPLY_DATE 5: June 30, 2020,  8:57pm; <NewLine> REPLY_DATE 6: June 30, 2020,  9:05pm; <NewLine> REPLY_DATE 7: June 30, 2020,  9:37pm; <NewLine> REPLY_DATE 8: June 30, 2020,  9:40pm; <NewLine> REPLY_DATE 9: July 1, 2020, 12:24am; <NewLine> REPLY_DATE 10: July 1, 2020, 12:27am; <NewLine> REPLY_DATE 11: July 1, 2020, 12:24pm; <NewLine> REPLY_DATE 12: July 1, 2020,  3:23pm; <NewLine> REPLY_DATE 13: July 1, 2020,  4:43pm; <NewLine> REPLY_DATE 14: July 1, 2020,  5:10pm; <NewLine> REPLY_DATE 15: July 2, 2020,  8:16am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: 1 Like; <NewLine> REPLY 15 LIKES: ; <NewLine> 
86714,Intermediate tensors in NeMo,2020-06-24T12:22:23.488Z,1,105,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I’m looking at QuartzNet in NeMo and trying to probe some of the internal tensors. I see that I can use <code>evaluated_tensors = neural_factory.infer(tensors=[a,b,c])</code>  to run inference and return the evaulations of a,b,c, but I can’t figure out how to get a list of the intermediate activation tensors, or to get a pointer to one.  I’m looking for a method of either the neural_factory or the individual models (like jasper_encoder) that would return a list of tensors that I can pass to infer().  Any ideas?<br/><NewLine>Thanks<br/><NewLine><em>Edit:</em>  I know infer() and the neural_factory object are NeMo, so maybe out of scope for this board, but the underlying model is based on a PyTorch module, so I’m hoping a general PyTorch method for getting internal activations will be useful here.  Here’s the inheritance tree for that encoder model.</p><NewLine><pre><code class=""lang-auto"">[nemo.collections.asr.jasper.JasperEncoder,<NewLine> nemo.backends.pytorch.nm.TrainableNM,<NewLine> nemo.core.neural_modules.NeuralModule,<NewLine> abc.ABC,<NewLine> torch.nn.modules.module.Module,<NewLine> object]</code></pre><NewLine></div>",https://discuss.pytorch.org/u/jhh3,(Jeremy H),jhh3,"June 24, 2020,  1:11pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>For a general PyTorch model you could use forward hooks to get the intermediate activations as described <a href=""https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/6"">here</a>.  Since NeMo seems to be using a PyTorch model internally, you would have to access its layers to register the hooks.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks ptrblck!  That worked.  It takes a little poking around to figure out the class structure, but not too much.  For example, using your example, I was able to use your example and this line:<br/><NewLine><code>encoder.encoder[1].mconv[0].conv.register_forward_hook(get_activation('B1(a).mconv.1D'))</code><br/><NewLine>to capture the output of the 1D convolution in the first block of a QuartzNet encoder model.<br/><NewLine>Again, thanks for the help.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>After messing with this for a while, I wanted to add one caveat that I encountered.  Some layers objects, like ReLU, are re-used throughout the network.  I guess it’s any layer without parameters, but I’m not sure.  The result is that if you put a hook on a ReLU layer, like <code>encoder.encoder[1].mout[0].register_forward_hook(get_activation('B1.mout.relu'))</code> in QuartzNet, <em>it gets called for every ReLU in the whole model</em>, not just the one you wanted.  So the final result in the dictionary is actually the ReLU output for the final model output, not the ReLU output associated with the layer where you registered the hook.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jhh3; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jhh3; <NewLine> ,"REPLY_DATE 1: July 1, 2020, 11:12am; <NewLine> REPLY_DATE 2: June 25, 2020,  8:00pm; <NewLine> REPLY_DATE 3: July 1, 2020, 11:10am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
85943,Audio Dataset - Load large file into memory in background,2020-06-18T14:47:30.769Z,0,158,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,</p><NewLine><p>I have a specific thing I want to achieve, and was wondering if it’s already possible using the current Dataset/DataLoader implementation. I saw a PR for a ChunkDataset API that may serve my needs, but it isn’t there yet.</p><NewLine><p>Data: Audio, with lots of small (1-10s) sound files.<br/><NewLine>I want to process this audio in terms of frames, but also incorporate a hop parameter (ie. take the first 1024 samples, then the next frame will start at 256 instead of 1024)</p><NewLine><p>What I want to do is concatenate all the short audio examples into long .wav files, of which two can fit into memory. I wrote code to index individual sounds and their respective frames and it works well.<br/><NewLine>The idea is to serve frames from one (long, e.g. 1GB) .wav file, and have another one loaded in the background. When all frames from the first file have been served, I replace the “current” file with the one that was loaded in the background, and load a new file.</p><NewLine><p>Everything works, except for the fact that the IO on loading a new file will block the <strong>getitem</strong> call, interrupting training. I was thinking of some async io structure, but lack some experience there in getting it to interop with the Dataset/Loader classes.<br/><NewLine>How can I do a non-blocking IO call to replace the current “buffered” file while keep serving frames?</p><NewLine></div>",https://discuss.pytorch.org/u/timlod,(Tim Loderhose),timlod,"June 18, 2020,  2:47pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>One way to manage async iterators is to use a <a href=""https://github.com/pytorch/audio/blob/master/torchaudio/datasets/utils.py#L353"" rel=""nofollow noopener"">background iterator</a> to prefetch ahead of time. But the exact setup depends on what you are trying to do of course <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>If you start with lots the small files (for a total of 1 GB), then you could create a dataset that reads them on <code>__getitem__</code>. You could either then cache the dataset in memory after loading, using a <a href=""https://github.com/pytorch/audio/pull/632/files#diff-8d22b987c96d9f00aba59dcf89bb5d90R29"" rel=""nofollow noopener"">cache</a>. And/or you could use a <a href=""https://github.com/pytorch/audio/blob/master/torchaudio/datasets/utils.py#L353"" rel=""nofollow noopener"">background iterator</a> to prefetch files ahead of time. The downside is lots of random disk seeks, but only on first read.</p><NewLine><p>If you have a single large (1 GB) data file with offsets, you could pay the price of loading it once in memory, and then the dataset simply knows about the offsets on <code>__getitem__</code>. To load the 1 GB async, you would need to read per block as an iterator, and you could use the background_iterator to push that in the background. You could still use the cache to keep the data in memory. The benefit would be faster starting time at the beginning since you don’t wait for the whole file to be loaded.</p><NewLine><p>Side note: You could also create a virtual RAM disk and copy the file(s) there once. Then everything after is done from RAM after that, so you could do lots of small file from there, or one big one. Reads are then fast.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply!</p><NewLine><p>I want to pay the price of loading the file once in memory and using the offsets (which I already have done), but the use case requires many of these large files, ie. the dataset could consist of 100 files of 1GB each. So what I imagined was always loading the next 1GB while the current 1GB is being served.<br/><NewLine>I don’t mind the penalty of waiting to load the first file, as long as subsequent loads do not affect training times - I want to make sure the GPU is utilised fully.</p><NewLine><p>I will have a look at the bg_iterator, which may work!</p><NewLine><p>edit: How would I have found audio/utils without going through the source/getting this recommendation?<br/><NewLine>Is there perhaps some documentation I missed?</p><NewLine><p>edit 2: bg_iterator did it! Fairly simple too, just had to create a generator for all big files and specify the generator with maxsize=1 so it will buffer the next item. All my other logic still works, generating the correct frames/batches. Thanks!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for pointing out that the torchaudio <a href=""https://pytorch.org/audio/datasets.html"" rel=""nofollow noopener"">documentation</a> needs to be updated to highlight <code>bg_iterator</code> <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>Created an <a href=""https://github.com/pytorch/audio/issues/733"" rel=""nofollow noopener"">issue</a> to track that</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vincentqb; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/timlod; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vincentqb; <NewLine> ,"REPLY_DATE 1: June 19, 2020,  9:04pm; <NewLine> REPLY_DATE 2: June 19, 2020,  9:04pm; <NewLine> REPLY_DATE 3: June 19, 2020,  9:04pm; <NewLine> ",REPLY 1 LIKES: 3 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
85517,Is there anything similar to grid_sample for 1D input?,2020-06-15T12:20:41.532Z,0,58,"<div class=""post"" itemprop=""articleBody""><NewLine><p>/is there any way I can make index selection diff wrt indices?</p><NewLine></div>",https://discuss.pytorch.org/u/torchlight,,torchlight,"June 15, 2020, 12:20pm",,,,,
84081,Is there a way to compute Kaldi pitch features with gradients?,2020-06-03T16:37:54.724Z,2,102,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to compute kaldi pitch features at the end of a network. Is there a way to compute gradients of the kaldi pitch feature extraction? (Maybe something similar to spectrogram of torchaudio).</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/Ofek_Cohen,(Ofek Cohen),Ofek_Cohen,"June 3, 2020,  5:22pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m not familiar with kaldi pitch, how do you compute it? If you use pytorch construct, the autograd should work just fine <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The original Kaldi library is written in C and usually called using .sh files. I do not know of a Pytorch function that implements it, although there are python envelopes such as pykaldi.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If there is no pytorch implementation, I’m afraid you’ll have to do one of the following:</p><NewLine><ul><NewLine><li>reimplement a new version with pytorch operators to use the autograd</li><NewLine><li>write a custom Function (<a href=""https://pytorch.org/docs/stable/notes/extending.html"">https://pytorch.org/docs/stable/notes/extending.html</a>) that uses the original library in the forward and for which you will specify the backward.</li><NewLine></ul><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Ofek_Cohen; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: June 3, 2020,  5:23pm; <NewLine> REPLY_DATE 2: June 3, 2020,  6:33pm; <NewLine> REPLY_DATE 3: June 3, 2020,  9:44pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
82864,How turn a torch audio to binary wav without saving to disk,2020-05-25T20:44:32.813Z,0,101,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I need to send some audio I use a torch based text to speech to generate as a JSON. I cannot find a way to turn it into a base64 without the following awful code:</p><NewLine><pre><code class=""lang-auto"">torchaudio.save('C:/Work/test_images/temp.wav', wav_array, sampling_rate)<NewLine>wav_file = open('C:/Work/test_images/temp.wav', 'rb')<NewLine></code></pre><NewLine><p>any ideas how to clean this up? there doesn’t appear to be any built-in functions with torch to do this, but I’m fairly new so would appreciate any help.</p><NewLine></div>",https://discuss.pytorch.org/u/quality,(Marcus Dunn),quality,"May 26, 2020,  1:01am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not deeply familiar with <code>torchaudio</code>, but would it be possible to use an <code>io.BytesIO</code> object for temporal storage instead of a file work?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: May 26, 2020,  7:58am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
81442,Accelerate Spectrograms with GPU and PyTorch?,2020-05-15T22:20:51.319Z,1,170,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am working with data that I spend a lot of time converting 1D signals to 2D spectrograms that are then fed to a CNN in Pytorch. I have a 2nd GPU and wanted to know if it was reasonable to accelerate my fft/spectrogram workflow through PyTorch on my GPU instead of what I am currently doing in my dataloader?</p><NewLine><p>What I am doing now:<br/><NewLine>-In my custom dataset (using torch.utils.data dataset class) I initialize and build my dataset of time-series audio data before training begins (fast). Then inside my <strong>getitem</strong> method I have some things that get done to the time-series data before I pass the data sample to a custom fft function that builds the spectrograms in the way I need and then returns that spectrogram as the data sample.<br/><NewLine>-My spectrogram function is custom built for various things I need but the main workhorse is built around the np.fft.fft() function, which is where I need to accelerate.<br/><NewLine>-I am using the “num_workers” kwarg in the PyTorch dataloader to better use my cores.</p><NewLine><p>What I would like to do:<br/><NewLine>Either in my <strong>getitem</strong> or elsewhere, I’d like to send it to my 2nd GPU (GTX 980ti) in hopes to faster generate the spectrograms then send them to my main GPU to pass the data through the model.</p><NewLine><p>Is this reasonable? Can I expect to see accelerations in contrast to a Ryzen 3950x and utilizing the “num_workers” kwarg in the dataloader? How would I do this? I’m open to ideas for other libraries and tools that exist out of PyTorch, but not sure where to start and seeking insight from this community.</p><NewLine></div>",https://discuss.pytorch.org/u/Mason7Acree,(Mason Acree),Mason7Acree,"May 15, 2020, 10:26pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I think the best way to speed this up would be to move it as preprocessing.<br/><NewLine>Have a seperate script that converts your audio data to the spectrogram and save them to disk.<br/><NewLine>Then your dataloader in the training script will just load the spectrograms directly.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Inconveniently this will not be accessible because I have certain transformations that are applied to my time-series data before I generate a spectrogram. Also this would eliminate any data augmentation that I have available in the time-series data as well.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>You can indeed operate on the main thread and to process that in a second gpu.<br/><NewLine>Even if I didn’t a proper profiling from my experience it may be worse to pre process them as the dimensionality is usually way higher. For example in my case an wavelength of 16k elements becomes into a 512x256x2.<br/><NewLine>Anyway I only recommend this is the main workload is the stft.if you have additional heavy preprocessing multiprocessing may be better</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Mason7Acree; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/JuanFMontesinos; <NewLine> ,"REPLY_DATE 1: May 15, 2020, 10:39pm; <NewLine> REPLY_DATE 2: May 19, 2020,  6:06pm; <NewLine> REPLY_DATE 3: May 19, 2020,  6:32pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
79252,How to compose torchaudio transforms,2020-05-01T12:26:06.596Z,0,113,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi<br/><NewLine>Is there an integrated solution to compose torchaudio transforms in the same style as torchvision do?</p><NewLine></div>",https://discuss.pytorch.org/u/Mohammad_Hassan_Soha,(Mohammad Hassan Sohan Ajini),Mohammad_Hassan_Soha,"May 1, 2020, 12:29pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>torchaudio doesn’t provide a dedicated compose transformation since 0.3.0 (see <a href=""https://github.com/pytorch/audio/releases"" rel=""nofollow noopener"">release notes</a>). Instead, one can simply apply them one after the other <code>x = transform1(x); x = transform2(x)</code>, or use <code>nn.Sequential(transform1, transform2)</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vincentqb; <NewLine> ,"REPLY_DATE 1: May 1, 2020,  3:33pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
77757,Installing torchaudio with pytorch 1.5.0 lead to downgrade,2020-04-21T21:13:12.272Z,12,740,"<div class=""post"" itemprop=""articleBody""><NewLine><p>while installing torchaudio with <code>conda install torchaudio -c pytorch</code>, anaconda downgrades torchvision (0.6.0 to 0.2.2) and pytorch (1.5 to 1.4). How to avoid this?</p><NewLine></div>",https://discuss.pytorch.org/u/Saurabh_Kataria,(Saurabh Kataria),Saurabh_Kataria,"April 21, 2020,  9:13pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Thanks for the report. Which OS are you using?</p><NewLine><p>It is torchaudio 0.5 that you are installing right?<br/><NewLine>If not, make sure that your conda is up to date as it sometimes prevent you from installing the latest version of libraries.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I can’t reproduce on MacOS</p><NewLine><pre><code class=""lang-auto"">❯ conda update -n base -c defaults conda<NewLine>❯ conda create -n 150and050 python=3.8<NewLine>❯ conda activate 150and050<NewLine>❯ conda install -c pytorch pytorch<NewLine>❯ conda install -c pytorch torchaudio<NewLine>❯ python -c ""import torch; print(torch.__version__); import torchaudio; print(torchaudio.__version__);""<NewLine>1.5.0<NewLine>0.5.0a0+3305d5c<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I can reproduce on linux:</p><NewLine><pre><code class=""lang-auto"">❯ conda update -n base -c defaults conda<NewLine>❯ conda create -n 150and050 python=3.8<NewLine>❯ conda activate 150and050<NewLine>❯ conda install -c pytorch pytorch<NewLine>❯ python -c ""import torch; print(torch.__version__);""<NewLine><NewLine>ModuleNotFoundError: No module named 'torch'<NewLine><NewLine>❯ conda install -c pytorch torchaudio<NewLine><NewLine>installs 0.4.0 and downgrades to 1.4.0<NewLine><NewLine>❯ python -c ""import torch; print(torch.__version__);""<NewLine><NewLine>ModuleNotFoundError: No module named 'torch'<NewLine><NewLine>❯ python -c ""import torchaudio; print(torchaudio.__version__);""<NewLine><NewLine>ModuleNotFoundError: No module named 'torchaudio'<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes linux binaries for torchaudio are not up yet. We are still working on that.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am also getting pytorch 1.4 and torchvision 0.6 on linux anaconda. My environment is as follows:</p><NewLine><pre><code class=""lang-auto"">name: Pytorch-gpu<NewLine>dependencies:<NewLine>  - python=3.7<NewLine>  - ipywidgets<NewLine>  - jupyterlab<NewLine>  - pytorch::pytorch<NewLine>  - pytorch::torchvision<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks so much. This is exactly what is happening to me. I’m on Debian GNU/Linux 9.4</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>We updated all the linux binaries now. So all should work fine. Can you double check that it works on your side?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Should we use anaconda again?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>All should be working now yes. Let us know if you see any issue.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Okey, I will try and let you know. I have been using the nightly version until now owing to the fact that when i installed torchvision this morning ,It does’t come with deeplabv3_resnet50 weights.</p><NewLine><p>Are deeplabv3_resnet50 weights in torchvision 0.6?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Torchvision 0.6.0a0+82fd1c8 is getting installed from anaconda, which is a prerelease</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you install from the pytorch channel, you should get the realease: <a href=""https://anaconda.org/pytorch/torchaudio"">https://anaconda.org/pytorch/torchaudio</a></p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>I was installing from pytorch official anaconda channel. I decided to change to pip and now is working!</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>worked perfectly using pytorch channel! thanks a lot.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vincentqb; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vincentqb; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Saurabh_Kataria; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/Saurabh_Kataria; <NewLine> ,"REPLY_DATE 1: April 21, 2020,  9:58pm; <NewLine> REPLY_DATE 2: April 21, 2020, 10:22pm; <NewLine> REPLY_DATE 3: April 21, 2020, 10:43pm; <NewLine> REPLY_DATE 4: April 21, 2020, 10:46pm; <NewLine> REPLY_DATE 5: April 22, 2020, 10:05am; <NewLine> REPLY_DATE 6: April 22, 2020,  3:15pm; <NewLine> REPLY_DATE 7: April 22, 2020,  6:01pm; <NewLine> REPLY_DATE 8: April 22, 2020,  8:39pm; <NewLine> REPLY_DATE 9: April 22, 2020,  8:41pm; <NewLine> REPLY_DATE 10: April 22, 2020,  8:44pm; <NewLine> REPLY_DATE 11: April 23, 2020,  7:39am; <NewLine> REPLY_DATE 12: April 23, 2020,  2:00pm; <NewLine> REPLY_DATE 13: April 23, 2020,  2:35pm; <NewLine> REPLY_DATE 14: April 23, 2020,  3:44pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 3 Likes; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: 1 Like; <NewLine> 
77501,"In sound event detection, how to augment synthetic labels?",2020-04-20T06:13:03.662Z,0,92,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m trying to participate in DCASE 2020 Task4, a sound event detection.<br/><NewLine>And while modifying the baseline code, I could find augmentation in training process.</p><NewLine><p>As you know, synthetic data have event label and onset, offset time.<br/><NewLine>So, when I augment data with time transform, I should adjust onset, offset labels to fit this.</p><NewLine><p>I’m stuck in here.<br/><NewLine>So my question is:</p><NewLine><ol><NewLine><li><NewLine><p>When data have been augmented and have two data in tuple, how to modify labels?</p><NewLine></li><NewLine><li><NewLine><p>When tuple have multi-data features, how pytorch link them with labels?<br/><NewLine>(ex. Data in Tuple : 4, label : 2)</p><NewLine></li><NewLine></ol><NewLine><p>Thank you for your help in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/kmh7576,,kmh7576,"April 20, 2020,  6:59am",,,,,
77099,Minimum number of training examples to properly finetune a seq2seq network,2020-04-16T20:09:49.027Z,0,73,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have trained a seq2seq model on some synthetic data, but am having trouble finetuning a very smallset of data in another domain. I have trained my seq2seq model on around 15k datapoints and my small dataset has around 50 datapoints.</p><NewLine></div>",https://discuss.pytorch.org/u/John_Lim,(John Lim),John_Lim,"April 16, 2020,  8:09pm",,,,,
74936,LSTM time series prediction network copies the input,2020-03-31T19:48:55.159Z,4,288,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am creating a model for music generation, but my proble is that for some reason modelpredicts a current step, and not a next step as a label says, even though i compute loss between model output and labels, and labels are shifted by 1 forward:</p><NewLine><p><img alt=""image"" data-base62-sha1=""9VFNNCzx69mZjBF26K3s5Ezs3eC"" height=""248"" src=""https://discuss.pytorch.org/uploads/default/original/3X/4/5/45988f4c319fd08045d95570a9f95b7404332b26.png"" width=""380""/><br/><NewLine>Sequence  - model input<br/><NewLine>Label - sequence shifted 1 step forward<br/><NewLine>Output - model output</p><NewLine><p>And if i feed in the label as input, it “predicts” the label, so model is basically repeating the input<br/><NewLine><img alt=""image"" data-base62-sha1=""qVfEQ3bip0kOsO0Dl241M8m4CWY"" height=""248"" src=""https://discuss.pytorch.org/uploads/default/original/3X/b/c/bcb15079b31c46c9c383bcefb15a2a68e5429ca0.png"" width=""386""/></p><NewLine><p>Dataset code:</p><NewLine><pre><code class=""lang-auto"">class h5FileDataset(Dataset):<NewLine>  def __init__(self, h5dir, seq_length):<NewLine>    self.h5dir = h5dir<NewLine>    self.seq_length = seq_length + 1<NewLine>    with h5py.File(h5dir,'r') as datafile:<NewLine>      self.length = len(datafile['audio']) // self.seq_length<NewLine>  def __len__(self):<NewLine>    return self.length<NewLine>  def __getitem__(self,idx):<NewLine>    with h5py.File(self.h5dir,'r') as datafile:<NewLine>      seq = datafile[""audio""][idx*self.seq_length:idx*self.seq_length+self.seq_length]<NewLine>    <NewLine>    feature = seq[0:len(seq)-1].astype('float32') #from 0 to second-to last element<NewLine>    label = seq[1:len(seq)].astype('float32') #from 1 to last element<NewLine><NewLine>    return feature,label<NewLine></code></pre><NewLine><p>Model code:</p><NewLine><pre><code class=""lang-auto"">class old_network(nn.Module):<NewLine>  def __init__(self, input_size=1, hidden_layer_size=1, output_size=1, seq_length_ = 1, batch_size_ = 128):<NewLine>        super().__init__()<NewLine>        self.hidden_layer_size = hidden_layer_size<NewLine>        self.batch_size = batch_size_<NewLine>        self.seq_length = seq_length_<NewLine><NewLine>        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first = False, num_layers = 2)<NewLine><NewLine>        self.linear1 = nn.Linear(hidden_layer_size, output_size)<NewLine>        self.linear2 = nn.Linear(hidden_layer_size, output_size)<NewLine>        #self.tanh1 = nn.Tanh()<NewLine>        self.tanh2 = nn.Tanh()<NewLine><NewLine>  def forward(self, input_seq):<NewLine>      lstm_out, _ = self.lstm(input_seq)<NewLine>      lstm_out = lstm_out.reshape(lstm_out.size(1),lstm_out.size(0),1) #reshape to batch,seq,feature<NewLine>      predictions = self.linear1(lstm_out)<NewLine>      #predictions2 = self.tanh1(predictions1)<NewLine>      predictions = self.linear2(predictions)<NewLine>      predictions = self.tanh2(predictions)<NewLine>      return predictions.reshape(predictions.shape[1],predictions.shape[0],1) #reshape to seq,batch,feature to match labels shape<NewLine></code></pre><NewLine><p>Training loop:</p><NewLine><pre><code class=""lang-auto"">epochs = 10<NewLine>batches = len(train_data_loader)<NewLine>losses = [[],[]]<NewLine>eval_iter = iter(eval_data_loader)<NewLine>print(""Starting training..."")<NewLine>try:<NewLine>  for epoch in range(epochs):<NewLine>    batch = 1<NewLine>    for seq, labels in train_data_loader:<NewLine>      start = time.time()<NewLine>      seq = seq.reshape(seq_length,batch_size,1).to(DEVICE)<NewLine>      labels = labels.reshape(seq_length,batch_size,1).to(DEVICE)<NewLine>      optimizer.zero_grad()<NewLine><NewLine>      y_pred = model(seq)<NewLine><NewLine>      loss = loss_function(y_pred, labels)<NewLine>      loss.backward()<NewLine>      optimizer.step()<NewLine>      <NewLine>      try:<NewLine>        eval_seq, eval_labels = next(eval_iter)<NewLine>      except StopIteration:<NewLine>        eval_iter = iter(eval_data_loader)<NewLine>        eval_seq, eval_labels = next(eval_iter)<NewLine>      eval_seq = eval_seq.reshape(seq_length,batch_size,1).to(DEVICE)<NewLine>      eval_labels = eval_labels.reshape(seq_length,batch_size,1).to(DEVICE)<NewLine><NewLine>      eval_y_pred = model(eval_seq)<NewLine><NewLine>      eval_loss = loss_function(eval_y_pred, eval_labels)<NewLine>      losses[1].append(eval_loss.item())<NewLine>      losses[0].append(loss.item())<NewLine><NewLine>      print_inline(""Batch {}/{} Time/batch: {:.4f}, Loss: {:.4f} Loss_eval: {:.4f}"".format(batch,batches,time.time()-start, loss.item(), eval_loss.item()))<NewLine>      batch += 1<NewLine><NewLine>      <NewLine><NewLine><NewLine>      if batch%50 == 0:<NewLine>            print(""\n Epoch: {}/{} Batch:{} Loss_train:{:.4f} Loss_eval: {:.4f}"".format(epoch,epochs,batch,loss.item(),eval_loss.item()))<NewLine>            <NewLine>            plt.close()<NewLine>            plt.plot(range(0,len(losses[0])),losses[0], label = ""Learning dataset"")<NewLine>            plt.plot(range(0,len(losses[1])),losses[1], label = ""Evaluation dataset"")<NewLine>            plt.legend()<NewLine>            plt.show()<NewLine>            torch.save({'model_state_dict':model.state_dict(), 'optimizer_state_dict' : optimizer.state_dict()},save_dir)<NewLine><NewLine>except KeyboardInterrupt:<NewLine>  plt.close()<NewLine>  plt.plot(range(0,len(losses[0])),losses[0], label = ""Learning dataset"")<NewLine>  plt.plot(range(0,len(losses[1])),losses[1], label = ""Evaluation dataset"")<NewLine>  plt.legend()<NewLine>  plt.show()<NewLine></code></pre><NewLine><p>I am kinda running out of ideas by this point, not sure what is wrong</p><NewLine></div>",https://discuss.pytorch.org/u/Topsoil,,Topsoil,"March 31, 2020, 11:18pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>No real ideas, just some comments:</p><NewLine><p>(1) I would never use <code>reshape()</code> or <code>view()</code> to adjust the tensor shape. I’ve seen to many cases where this was used incorrectly and broke the tensor. Just because the shape is correct in the network doesn’t throw an error doesn’t mean the tensor is correct. If possible, I always use <code>transpose()</code> or <code>permute()</code> since in almost all cases I only need to swap dimensions. For exanole, instead of</p><NewLine><pre><code>seq = seq.reshape(seq_length,batch_size,1).to(DEVICE)<NewLine></code></pre><NewLine><p>I would do</p><NewLine><pre><code>seq = seq.tranpose(1,0).to(DEVICE)<NewLine></code></pre><NewLine><p>or</p><NewLine><pre><code>seq = seq.permute(1,0,2).to(DEVICE)<NewLine></code></pre><NewLine><p>This ensures that dimensions are only swapped but never “torn apart” which can happen with <code>reshape()</code> or <code>view()</code>. The latter are mostly needed to maybe (un-)flatten tensors, but that’s not needed here.</p><NewLine><p>(2) I’m also not quite sure about</p><NewLine><pre><code>predictions = self.linear1(lstm_out)<NewLine></code></pre><NewLine><p>since the shape of <code>lstm_out</code> is <code>(batch_size, seq_len, features)</code>. I know that <code>nn.Linear</code> takes as input <code>(N,∗,H_in)</code> but I’m not sure if you really want go that way. Usually the last hidden state is used for prediction. So I would try:</p><NewLine><pre><code>lstm_out, (h, c) = self.lstm(input_seq)<NewLine>predictions = self.linear1(h[-1])<NewLine></code></pre><NewLine><p><code>h[-1]</code> is the last layer of the last hidden state.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is a classic result of using LSTM for time series analysis. LSTM is simply using the hidden state to relay back an earlier input without actually learning any patterns. In order to trick the LSTM into learning patterns, you can do the following</p><NewLine><ul><NewLine><li>Reduce step size</li><NewLine><li>Increase HiddenDim size</li><NewLine></ul><NewLine><pre><code class=""lang-auto"">%matplotlib inline<NewLine>import matplotlib.pyplot as plt<NewLine><NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine>import numpy as np<NewLine>from torch.autograd import Variable<NewLine><NewLine>class LSTMSimple(nn.Module):<NewLine>    def __init__(self,inputDim,hiddenDim,batchSize,outputDim):<NewLine>        super(LSTMSimple,self).__init__()<NewLine>        torch.manual_seed(1)<NewLine>        self.lstm=nn.LSTM(inputDim,hiddenDim,1).cuda()<NewLine>        # Hidden state is a tuple of two states, so we will have to initialize two tuples<NewLine>        self.state_h = torch.randn(1,batchSize,hiddenDim).cuda()<NewLine>        self.state_c = torch.rand(1,batchSize,hiddenDim).cuda()<NewLine>        self.linearModel=nn.Linear(hiddenDim,outputDim).cuda()<NewLine>        <NewLine>    def forward(self,inputs):<NewLine>        # LSTM<NewLine>        output, self.hidden = self.lstm(inputs, (self.state_h,self.state_c) )<NewLine>        self.state_h=self.state_h.detach()<NewLine>        self.state_c=self.state_c.detach()<NewLine>        # LINEAR MODEL<NewLine>        output=self.linearModel(output).cuda()<NewLine>        return output<NewLine><NewLine>def lossCalc(x,y):<NewLine>    return torch.sum(torch.add(x,-y))<NewLine>    <NewLine># Model Object<NewLine>batchSize=5<NewLine>inputDim=1<NewLine>outputDim=1<NewLine>stepSize=5<NewLine>hiddenDim=20<NewLine>model=LSTMSimple(inputDim,hiddenDim,batchSize,outputDim).cuda()<NewLine>loss = torch.nn.MSELoss()<NewLine>optimizer = optim.Adam(model.parameters(), lr=0.00001)<NewLine><NewLine># Input Data<NewLine>dataInput = np.random.randn(stepSize*batchSize,inputDim)<NewLine>dataY=np.insert(dataInput[1:],len(dataInput)-2,0)<NewLine>dataInput=Variable(torch.from_numpy(dataInput.reshape(stepSize,batchSize,inputDim).astype(np.float32))).cuda()<NewLine>dataY=Variable(torch.from_numpy(dataY.reshape(stepSize,batchSize,inputDim).astype(np.float32))).cuda()<NewLine>for epoch in range(10000):<NewLine>    optimizer.zero_grad()<NewLine>    dataOutput=model(dataInput).cuda()<NewLine>    curLoss=loss(dataOutput.view(batchSize*stepSize,outputDim),dataY.view(batchSize*stepSize,outputDim))<NewLine>    curLoss.backward()<NewLine>    optimizer.step()<NewLine>    if(epoch % 1000==0):<NewLine>        print(""For epoch {}, the loss is {}"".format(epoch,curLoss))<NewLine><NewLine>plt.plot(dataInput.cpu().detach().numpy().reshape(-1),color=""red"")<NewLine>plt.plot(dataOutput.cpu().detach().numpy().reshape(-1),color=""orange"")<NewLine>plt.plot(dataY.cpu().detach().numpy().reshape(-1),color=""green"")<NewLine>plt.figure()<NewLine></code></pre><NewLine><p><img alt=""image"" data-base62-sha1=""qivz8AnlH2la7HmdpMe7VuYZ7Uj"" height=""248"" src=""https://discuss.pytorch.org/uploads/default/original/3X/b/8/b850256595de9b4400ca1dd8a24d2e50d1459897.png"" width=""384""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>What do you mean exactly by reducing step size? Reducing input and output sequence lengths?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Tried increasing hidden dim to 100 and reducing seq_length to 250, still follows the input<br/><NewLine>Loss graph:<br/><NewLine><img alt=""image"" data-base62-sha1=""xVgLebE8xnXgxXZzYD0hdq7a2XC"" height=""248"" src=""https://discuss.pytorch.org/uploads/default/original/3X/e/d/edc10c073e7a8f02d4565974fe9e4f2c46bacc18.png"" width=""384""/></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Seq Len 250 is still very high for LSTM. Can you reduce it to 3 or 5 and retry</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh, i thought that lstm needs long sequences, especially in things like music, to capture all long-term dependencies</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vdw; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/anantguptadbl; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Topsoil; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Topsoil; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/anantguptadbl; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Topsoil; <NewLine> ,"REPLY_DATE 1: April 1, 2020,  5:55am; <NewLine> REPLY_DATE 2: April 1, 2020,  8:44am; <NewLine> REPLY_DATE 3: April 1, 2020, 12:43pm; <NewLine> REPLY_DATE 4: April 1, 2020,  1:10pm; <NewLine> REPLY_DATE 5: April 2, 2020,  5:54am; <NewLine> REPLY_DATE 6: April 5, 2020, 10:08am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
74169,Save the output of the network in .ark format,2020-03-23T21:25:06.332Z,1,103,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I trained a PyTorch model for speech recognition and I want to save the output of the model as .ark file<br/><NewLine>Can anyone give me a help?</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/Mohamed_Nabih,(Mohamed Nabih),Mohamed_Nabih,"March 23, 2020,  9:25pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not really pytroch related it seems ?</p><NewLine><p>Are you referring to this ? <a href=""https://fileinfo.com/extension/ark"" rel=""nofollow noopener"">https://fileinfo.com/extension/ark</a><br/><NewLine>If so… it’s a pretty arcane format, I’m pretty sure people switched to .zip and .gzip a few decade ago, if you need it for some reason you can use arc (<a href=""https://sourceforge.net/projects/arc/"" rel=""nofollow noopener"">https://sourceforge.net/projects/arc/</a>), at least on linux (but should work on OSX and windows as well I’d assume)</p><NewLine><p>Just output whatever you have to a text file and then use the arc tool to compress it. There’s also a python library for it seemingly: <a href=""https://pypi.org/project/arc/"" rel=""nofollow noopener"">https://pypi.org/project/arc/</a>, but I’m not sure if it actually works.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>So I need to save my network posteriors in any format can you give me a help on this</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/George3d6; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Mohamed_Nabih; <NewLine> ,"REPLY_DATE 1: March 24, 2020, 12:35am; <NewLine> REPLY_DATE 2: March 24, 2020, 12:54am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
73613,How can I download torchaudio?,2020-03-18T09:13:11.403Z,4,285,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am currently trying to download torchaudio for Conda to train an RNN on audio, but I can’t download it. I used the command <code>conda install -c pytorch torchaudio </code>, and also downloaded all of the required libraries, but when I try to download it, it says <code>PackagesNotFoundError: The following packages are not available from current channels: torchaudio</code>. Why does it happen, and how can I download torchaudio succesfully?</p><NewLine></div>",https://discuss.pytorch.org/u/Victor_Dobra,(Victor Dobra),Victor_Dobra,"March 19, 2020,  3:47pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Which python version are you using?<br/><NewLine>Also have you tried using pip or installing the nightly builds?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am using Python 3.7, and yes, I have tried to use pip and install the nightly builds and it didn’t work.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is surprising indeed, have you tried in an empty conda environment? You might have conflicting packages?</p><NewLine><p><a class=""mention"" href=""/u/vincentqb"">@vincentqb</a> is there something special about the packages?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, I have tried and it doesn’t work.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>pip install torchaudio</code> works at this moment, do you have an error with it?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have found out that torchaudio is not compatible with Windows 10, which I am using. Thank you for the help, anyway.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>What platform are you using? torchaudio is currently available for linux and macos. Windows support is in progress, see <a href=""https://github.com/pytorch/audio/issues/425"" rel=""nofollow noopener"">this</a>.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>It is usable but without mp3 support.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Victor_Dobra; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Victor_Dobra; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mpariente; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Victor_Dobra; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/vincentqb; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/peterjc123; <NewLine> ,"REPLY_DATE 1: March 18, 2020,  3:05pm; <NewLine> REPLY_DATE 2: March 19, 2020,  8:26am; <NewLine> REPLY_DATE 3: March 19, 2020,  3:48pm; <NewLine> REPLY_DATE 4: March 19, 2020,  5:24pm; <NewLine> REPLY_DATE 5: March 19, 2020,  7:01pm; <NewLine> REPLY_DATE 6: March 23, 2020, 11:20am; <NewLine> REPLY_DATE 7: March 23, 2020,  1:49pm; <NewLine> REPLY_DATE 8: March 23, 2020,  3:42pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> 
41859,RuntimeError: CUDA error: out of memory in Pytorch,2019-04-07T04:07:36.979Z,0,529,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am getting this error in the last batches while i am training a large speech dataset. I tried reducing batch sizes (16,8,4,2) but every time i got this error at the end of the epoch.<br/><NewLine>can someone give me the solution?</p><NewLine><h2>99%|█████████▉| 11706/11812 [29:32&lt;01:08,  1.55it/s, avg_loss=tensor(76.1413, device=‘cuda:0’), iter=11705, loss=tensor(117.4730, device=‘cuda:0’)]</h2><NewLine><p>RuntimeError                              Traceback (most recent call last)<br/><NewLine> in <br/><NewLine>3     start = time.time()<br/><NewLine>4<br/><NewLine>----&gt; 5     run_state = run_epoch(model, optimizer, train_ldr, *run_state)<br/><NewLine>6<br/><NewLine>7     msg = “Epoch {} completed in {:.2f} (s).”</p><NewLine><p>~/Hasan/Project/SpeechRNNT/speech-master/train.py in run_epoch(model, optimizer, train_ldr, it, avg_loss)<br/><NewLine>28         optimizer.zero_grad()<br/><NewLine>29         loss = model.loss(batch)<br/><NewLine>—&gt; 30         loss.backward()<br/><NewLine>31<br/><NewLine>32         grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 200)</p><NewLine><p>~/miniconda3/envs/ariyan/lib/python3.6/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)<br/><NewLine>91                 products. Defaults to <code>False</code>.<br/><NewLine>92         “”""<br/><NewLine>—&gt; 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)<br/><NewLine>94<br/><NewLine>95     def register_hook(self, hook):</p><NewLine><p>~/miniconda3/envs/ariyan/lib/python3.6/site-packages/torch/autograd/<strong>init</strong>.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)<br/><NewLine>88     Variable._execution_engine.run_backward(<br/><NewLine>89         tensors, grad_tensors, retain_graph, create_graph,<br/><NewLine>—&gt; 90         allow_unreachable=True)  # allow_unreachable flag<br/><NewLine>91<br/><NewLine>92</p><NewLine><p>~/Hasan/Project/SpeechRNNT/speech-master/transducer/functions/transducer.py in backward(self, *args)<br/><NewLine>78         grads = parent.backward(*args)[0]<br/><NewLine>79         if self.size_average:<br/><NewLine>—&gt; 80             grads = grads / grads.shape[0]<br/><NewLine>81         return grads, None, None, None<br/><NewLine>82</p><NewLine><p>RuntimeError: CUDA error: out of memory</p><NewLine></div>",https://discuss.pytorch.org/u/Ariyan_Hasan,(Ariyan Hasan),Ariyan_Hasan,"April 7, 2019,  4:07am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m wondering would you mind to share your implementation of transducer loss? Is it a python native code or did you used c/c++ extensions.</p><NewLine><p>Best,</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Yimeng_Zhu; <NewLine> ,"REPLY_DATE 1: March 13, 2020,  6:30pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
72534,Torchaudio in win10,2020-03-09T03:08:43.841Z,0,114,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Dear experts, when will the torchaudio be compatible with win10? I really need it. Thank you very much!</p><NewLine></div>",https://discuss.pytorch.org/u/YH-WEI,(YH-WEI),YH-WEI,"March 9, 2020,  3:09am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can track the progress <a href=""https://github.com/pytorch/audio/issues/425"" rel=""nofollow noopener"">here</a>. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vincentqb; <NewLine> ,"REPLY_DATE 1: March 9, 2020,  2:43pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
72526,Pytorchaudio Spectrogram Output Size:- Unexpected number of SFTs,2020-03-09T01:31:06.630Z,0,99,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am applying the pyaudio spectrogram to a 512 sample audio sample.</p><NewLine><p>An example below:-</p><NewLine><p>waveform = 512 samples<br/><NewLine>specgram = torchaudio.transforms.Spectrogram(hop_length=64)(waveform)</p><NewLine><p>I thought this would generate 512/64=8 Hops, so 8 SFTs for the spectrogram, however it generates 9.</p><NewLine><p>waveform = 512 samples<br/><NewLine>specgram = torchaudio.transforms.Spectrogram(hop_length=128)(waveform)</p><NewLine><p>I thought this would generate 512/128=4 SFTs for the spectrogram, however it generates 5.</p><NewLine><p>I guess it may start at the  -hop_length/2 and finishes at 512 + hop_length/2</p><NewLine><p>Pad is set to zero by default. If it is set to 32, it increases the number of SFTs by 1 as expected.</p><NewLine></div>",https://discuss.pytorch.org/u/Paul_Creaser,(Paul Creaser),Paul_Creaser,"March 9, 2020,  1:31am",,,,,
71980,The accuracy of the Model is constant,2020-03-04T11:13:24.271Z,7,292,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a problem with multi-classification I built this Model, but the test accuracy is constant at 4</p><NewLine><p>input_size = 13</p><NewLine><p>hidden1_size = 1024</p><NewLine><p>hidden2_size = 1024</p><NewLine><p>hidden3_size = 1024</p><NewLine><p>hidden4_size = 1024</p><NewLine><p>hidden5_size = 1024</p><NewLine><p>output_size = 1976</p><NewLine><p>class DNN(nn.Module):</p><NewLine><pre><code>def __init__(self, input_size, hidden1_size, hidden2_size, hidden3_size, hidden4_size, hidden5_size, output_size):<NewLine>    super(DNN, self).__init__()<NewLine>    self.fc1 = nn.Linear(input_size, hidden1_size)<NewLine><NewLine>    self.sig1 = nn.Sigmoid()<NewLine>    self.fc2 = nn.Linear(hidden1_size, hidden2_size)<NewLine><NewLine>    self.sig2 = nn.Sigmoid()<NewLine>    self.fc3 = nn.Linear(hidden2_size, hidden3_size)<NewLine><NewLine>    self.sig3 = nn.Sigmoid()<NewLine>    self.fc4 = nn.Linear(hidden3_size, hidden4_size)<NewLine><NewLine>    self.sig4 = nn.Sigmoid()<NewLine>    self.fc5 = nn.Linear(hidden4_size, hidden5_size)<NewLine><NewLine>    self.sig5 = nn.Sigmoid()<NewLine>    self.fc6 = nn.Linear(hidden5_size, output_size)<NewLine><NewLine>def forward(self, x):<NewLine>    out = self.fc1(x)<NewLine><NewLine>    out = self.sig1(out)<NewLine>    out = self.fc2(out)<NewLine><NewLine>    out = self.sig2(out)<NewLine>    out = self.fc3(out)<NewLine><NewLine>    out = self.sig3(out)<NewLine>    out = self.fc4(out)<NewLine><NewLine>    out = self.sig4(out)<NewLine>    out = self.fc5(out)<NewLine>  <NewLine>    out = self.sig5(out)<NewLine>    out = self.fc6(out)<NewLine><NewLine>    return out<NewLine></code></pre><NewLine><p>model = DNN(input_size, hidden1_size, hidden2_size, hidden3_size, hidden4_size, hidden5_size,<br/><NewLine>output_size)</p><NewLine><p>criterion = nn.CrossEntropyLoss()</p><NewLine><p>learning_rate = 0.008</p><NewLine><p>optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</p><NewLine><p>for epoch in range(1, 50):</p><NewLine><pre><code>for i, (X_train, y_train) in enumerate(train_loader):<NewLine><NewLine>    model.train()<NewLine><NewLine>    optimizer.zero_grad()<NewLine><NewLine>    outputs = model(Variable(X_train))<NewLine><NewLine>    loss = criterion(outputs, Variable(y_train))<NewLine><NewLine>    print('Iter %d/%d --&gt; loss %f' % (i, len(train_loader), loss.item()))<NewLine>    <NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine>correct = 0<NewLine>total = 0<NewLine>print('test')<NewLine>for X_test, y_test in test_loader:<NewLine>    model.eval()<NewLine>    out = model(Variable(X_test)).detach()<NewLine>    pred = out.max(dim=1)[1]#.argmax(dim=1, keepdim=True)<NewLine>    total += y_test.size(0)<NewLine>    correct += (pred.squeeze() == y_test).sum() # pred.eq(y_test.view_as(pre  d)).sum().item()<NewLine>accuracy = 100 * correct / total<NewLine>print('epoch: {}.  Accuracy: {}'.format(epoch, accuracy))</code></pre><NewLine></div>",https://discuss.pytorch.org/u/Mohamed_Nabih,(Mohamed Nabih),Mohamed_Nabih,"March 4, 2020, 12:57pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Try to overfit a small data sample (e.g. just 10 samples) to make sure you don’t have any hidden bugs in your code and that your model architecture works for this problem.<br/><NewLine>From my past experience I would claim, that relu activation functions might work better than sigmoids, so you could play around with the architecture and some hyperparameters.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a></p><NewLine><p>The problem is the network always estimates the most frequent class in the labels.<br/><NewLine>So, Do you have any idea to tackle this problem</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>In addition to using relu as activation, u should also add some dropouts. U can try to add weight to <code>CrossEntropyLoss</code> to reduce the over fitting caused by imbalanced data.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/g.m"">@G.M</a><br/><NewLine>Thanks a lot, but can you give me an example how can I add weights to BCE Loss</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Aren’t u using <code>CrossEntropyLoss</code>? I think u should use cross entropy for multi-class classification.<br/><NewLine>For the ordinary <code>BCELoss</code>, according to <a href=""https://pytorch.org/docs/1.0.1/nn.html#BCELoss"" rel=""nofollow noopener"">here</a>, the weight are just a number that is multiplied to each value in a batch, so the shape of <code>weight</code> must equal to the shape of a single batch.</p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; import torch as tc<NewLine>&gt;&gt;&gt; from torch import nn<NewLine>&gt;&gt;&gt; bsz = 10<NewLine>&gt;&gt;&gt; loss0 = nn.BCELoss(weight = tc.full([bsz], 0.5))  # ""weight"" can contain values of any number.<NewLine>&gt;&gt;&gt; loss1 = nn.BCELoss()<NewLine>&gt;&gt;&gt; inp, tar = tc.zeros(bsz), tc.ones(bsz)<NewLine>&gt;&gt;&gt; loss0(inp, tar)<NewLine>tensor(13.8155)<NewLine>&gt;&gt;&gt; loss1(inp, tar)<NewLine>tensor(27.6310)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/g.m"">@G.M</a><br/><NewLine>Sorry for the typo I already use CrossEntropyLoss so, can you edit the example according to CrossEntropy loss</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s ok <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/>. For CrossEntropy, it’s straightforward: provide a tensor of shape <code>[C]</code>( C is the number of classes, and the id of the classes ranges from <code>[0, C)</code> ). Each value represent the weight of each class, the weight here should be positive. For example:</p><NewLine><pre><code class=""lang-auto"">from torch import nn<NewLine>import torch as tc<NewLine>num_cls = 100<NewLine>weights = tc.rand([num_cls])<NewLine>loss = nn.CrossEntropyLoss(weight = weights)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/g.m"">@G.M</a><br/><NewLine>Thanks a lot, and this weight should be connected with any layers of the model or just implemented as you tell me.</p><NewLine><p>Another thing if I want to accelerate my model I use ReLU activation function and dropout layers and Increase the hidden layers this makes the loss to decrease and the accuracy increase but slowly due you have ideas how can I increase them rapidly</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>Usually, the weight of a class is something like the inverse of the frequency of the class.</li><NewLine><li>I think it is slow because dropout makes the model converges slower; my suggestion is to remove some Linearn layers. Removing some layers can accelerate the training process, reduce memory usage, and reduce over-fitting. Currently, u have 6 Linear layers, 2 or 3 should be enough <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/>.</li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Mohamed_Nabih; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/G.M; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Mohamed_Nabih; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/G.M; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Mohamed_Nabih; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/G.M; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Mohamed_Nabih; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/G.M; <NewLine> ,"REPLY_DATE 1: March 5, 2020,  7:59am; <NewLine> REPLY_DATE 2: March 5, 2020, 10:21am; <NewLine> REPLY_DATE 3: March 5, 2020, 11:15am; <NewLine> REPLY_DATE 4: March 5, 2020,  1:01pm; <NewLine> REPLY_DATE 5: March 5, 2020,  1:20pm; <NewLine> REPLY_DATE 6: March 5, 2020,  1:24pm; <NewLine> REPLY_DATE 7: March 5, 2020,  1:32pm; <NewLine> REPLY_DATE 8: March 5, 2020,  3:36pm; <NewLine> REPLY_DATE 9: March 6, 2020, 12:15am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> 
71154,Loss is not decreasing over the different iterations,2020-02-26T14:20:21.529Z,0,114,"<div class=""post"" itemprop=""articleBody""><NewLine><p>First, this is my Dataloader</p><NewLine><pre><code>X_train, X_test, y_train, y_test = train_test_split(feat, labels, test_size=0.2, random_state=1)<NewLine><NewLine>X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)<NewLine><NewLine>train = data_utils.TensorDataset(X_train, y_train)<NewLine><NewLine>train_loader = data_utils.DataLoader(train, batch_size=1000, shuffle=True)<NewLine><NewLine>test = data_utils.TensorDataset(X_test, y_test)<NewLine><NewLine>test_loader = data_utils.DataLoader(test, batch_size=1000, shuffle=False)<NewLine><NewLine>input_size = 13<NewLine>hidden1_size = 13<NewLine>hidden2_size = 64<NewLine>hidden3_size = 128<NewLine>hidden4_size = 256<NewLine>hidden5_size = 1024<NewLine>output_size = 3989<NewLine><NewLine>class DNN(nn.Module):<NewLine><NewLine>def __init__(self, input_size, hidden1_size, hidden2_size, hidden3_size, hidden4_size, hidden5_size, output_size):<NewLine>    super(DNN, self).__init__()<NewLine>    self.fc1 = nn.Linear(input_size, hidden1_size)<NewLine>    self.drp1 = nn.Dropout(p=0.2, inplace=False)<NewLine>    self.relu1 = nn.ReLU()<NewLine>    self.tan1 = nn.Tanh()<NewLine>    self.fc2 = nn.Linear(hidden1_size, hidden2_size)<NewLine>    self.drp2 = nn.Dropout(p=0.2, inplace=False)<NewLine>    self.relu2 = nn.ReLU()<NewLine>    self.tan2 = nn.Tanh()<NewLine>    self.fc3 = nn.Linear(hidden2_size, hidden3_size)<NewLine>    self.drp3 = nn.Dropout(p=0.2, inplace=False)<NewLine>    self.relu3 = nn.ReLU()<NewLine>    self.tan3 = nn.Tanh()<NewLine>    self.fc4 = nn.Linear(hidden3_size, hidden4_size)<NewLine>    self.drp4 = nn.Dropout(p=0.2, inplace=False)<NewLine>    self.relu4 = nn.ReLU()<NewLine>    self.tan4 = nn.Tanh()<NewLine>    self.fc5 = nn.Linear(hidden4_size, hidden5_size)<NewLine>    self.drp5 = nn.Dropout(p=0.2, inplace=False)<NewLine>    self.relu5 = nn.ReLU()<NewLine>    self.tan5 = nn.Tanh()<NewLine>    self.fc6 = nn.Linear(hidden5_size, output_size)<NewLine>    self.tan6 = nn.Tanh()<NewLine><NewLine>def forward(self, x):<NewLine>    out = self.fc1(x)<NewLine>    out = self.drp1(out)<NewLine>    out = self.relu1(out)<NewLine>    out = self.tan1(out)<NewLine>    out = self.fc2(out)<NewLine>    out = self.drp2(out)<NewLine>    out = self.relu2(out)<NewLine>    out = self.tan2(out)<NewLine>    out = self.fc3(out)<NewLine>    out = self.drp3(out)<NewLine>    out = self.relu3(out)<NewLine>    out = self.tan3(out)<NewLine>    out = self.fc4(out)<NewLine>    out = self.drp4(out)<NewLine>    out = self.relu4(out)<NewLine>    out = self.tan4(out)<NewLine>    out = self.fc5(out)<NewLine>    out = self.drp5(out)<NewLine>    out = self.relu5(out)<NewLine>    out = self.tan5(out)<NewLine>    out = self.fc6(out)<NewLine>    out = self.tan6(out)<NewLine>    return out<NewLine>  batch_size = 10<NewLine>  n_iterations = 50<NewLine>  no_eps = n_iterations / (13 / batch_size)<NewLine>  no_epochs = int(no_eps)<NewLine>  model = DNN(input_size, hidden1_size, hidden2_size, hidden3_size, hidden4_size, hidden5_size, output_size)<NewLine><NewLine> criterion = nn.CrossEntropyLoss()<NewLine> learning_rate = 0.0001<NewLine> optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)<NewLine>  iter = 0<NewLine> for epoch in range(no_epochs):<NewLine>for i, (X_train, y_train) in enumerate(train_loader):<NewLine>    optimizer.zero_grad()<NewLine>    outputs = model(Variable(X_train))<NewLine>    loss = criterion(outputs, Variable(y_train))<NewLine>    print('Iter %d --&gt; loss %f' % (i, loss.item()))<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine>correct = 0<NewLine>total = 0<NewLine>print('test')<NewLine>for X_test, y_test in test_loader:<NewLine>    outputs = model(Variable(X_test))<NewLine>    pred = outputs.argmax(dim=1, keepdim=True)<NewLine>    total += y_test.size(0)<NewLine>    correct += (pred.squeeze() == y_test).sum()  # pred.eq(y_test.view_as(pre  d)).sum().item()<NewLine><NewLine>accuracy = 100 * correct / total<NewLine><NewLine>print('Iteration: {}.  Accuracy: {}'.format(epoch, accuracy))<NewLine></code></pre><NewLine><pre><code class=""lang-auto""></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Mohamed_Nabih,(Mohamed Nabih),Mohamed_Nabih,"February 26, 2020,  4:12pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>you shouldn’t use two activation functions (here ReLU ad Tanh) , also if is a classifier you may use Sigmoid in final layer instead of tanh. And <code>Variable</code> is deprecated, pass the tensor directly instead</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss"" rel=""nofollow noopener"">CrossEntropyLoss</a> means you do not need an activation layer at the end of your network, also try Adam optimizer if SGD is not working, and as <a class=""mention"" href=""/u/simaiden"">@simaiden</a> mentioned, two act is not needed.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/klory"">@klory</a> <a class=""mention"" href=""/u/simaiden"">@simaiden</a><br/><NewLine>I did all of this but the loss didn’t improve</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/simaiden; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/klory; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Mohamed_Nabih; <NewLine> ,"REPLY_DATE 1: February 26, 2020,  7:06pm; <NewLine> REPLY_DATE 2: February 26, 2020,  7:34pm; <NewLine> REPLY_DATE 3: March 4, 2020,  6:13pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
71754,Sclicing torch Tensor,2020-03-02T18:13:22.793Z,10,143,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I need help<br/><NewLine>I have a torch tensor of size [1124823 x 13]  and I want from the center "" of the tensor to take five frames from the right and five frames from the left and concatenate them is there is any function do this?<br/><NewLine>"" N.B.""</p><NewLine></div>",https://discuss.pytorch.org/u/Mohamed_Nabih,(Mohamed Nabih),Mohamed_Nabih,"March 2, 2020,  6:22pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>You can use things like <code>t[base-5:base+5]</code> where base is whatever you call the center of your Tensor <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>sorry I can’t understand what is (t) and what is the base</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>t</code> would be your Tensor of size [1124823 x 13].<br/><NewLine>And base is the index (as a python number) of the center. So something like <code>base = t.size(0) // 2</code>.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you mean this</p><NewLine><p>base = feat.size(0)//2<br/><NewLine>x = feat[base-5: base:+5]</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes if your tensor is called <code>feat</code> that will work.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks albanD<br/><NewLine>But can I ask you if I want to go inside the center of each raw of the tensor take five elements from left and five from right</p><NewLine><p>i = 0</p><NewLine><p>j = 6</p><NewLine><p>base = feat.szie(0)//2</p><NewLine><p>for i in feat[i, j]:</p><NewLine><pre><code>x = feat[base - 5: base:+5]<NewLine><NewLine>i += 1</code></pre><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you want to take form the second dimension, you can use the indexing syntax like: <code>x = feat[:, base-5, base+5]</code>. Or you can use the specialized function to get a subset: <code>x = feat.narrow(1, base - 5, 10)</code>. The two will give exactly the same result.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not excatly, I want to go form the center of each raw take five numbers from the left and five numbers from right</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Rows are the second dimension for a 2D Tensor. So that should work.<br/><NewLine>Maybe you want to share an example with a given Tensor and which values you expect to get?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>x = feat.narrow(1, base - 5, 10)</p><NewLine><p><strong>IndexError: Dimension out of range (expected to be in range of [-13, 12], but got 562406)</strong></p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is because the center of the second dimension is not the same as the one in the first dimension.</p><NewLine><p>I am really confused about what you’re trying to do. I think an example of input/output that you want would help.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>The tensor size [ 1124823 x 13 ]. So I want to slice each raw from its center and take 10 elements, 5 from the right and five from left. So I need to iterate through every raw.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>What do you mean by “raw”? row?</p><NewLine><p>If you have a Tensor of size [ 1124823 x 13 ], a single row is a 1D Tensor of size 13.<br/><NewLine>So you want to do: <code>feat.narrow(1, feat.size(1)//2 -5, 10)</code>.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, sorry for this typo it is raw.</p><NewLine><p>But, Do I need a for loop to iterate for each raw</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, the operation above will give a Tensor of size <code>[ 1124823 x 10 ]</code> containing the result for every row.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Mohamed_Nabih; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Mohamed_Nabih; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Mohamed_Nabih; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Mohamed_Nabih; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Mohamed_Nabih; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/Mohamed_Nabih; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/Mohamed_Nabih; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: March 2, 2020,  8:05pm; <NewLine> REPLY_DATE 2: March 2, 2020,  8:19pm; <NewLine> REPLY_DATE 3: March 2, 2020,  8:48pm; <NewLine> REPLY_DATE 4: March 3, 2020, 10:48am; <NewLine> REPLY_DATE 5: March 3, 2020,  1:57pm; <NewLine> REPLY_DATE 6: March 3, 2020,  3:59pm; <NewLine> REPLY_DATE 7: March 3, 2020,  4:03pm; <NewLine> REPLY_DATE 8: March 3, 2020,  4:07pm; <NewLine> REPLY_DATE 9: March 3, 2020,  4:09pm; <NewLine> REPLY_DATE 10: March 3, 2020,  4:14pm; <NewLine> REPLY_DATE 11: March 3, 2020,  4:20pm; <NewLine> REPLY_DATE 12: March 3, 2020,  4:25pm; <NewLine> REPLY_DATE 13: March 3, 2020,  4:31pm; <NewLine> REPLY_DATE 14: March 3, 2020,  4:34pm; <NewLine> REPLY_DATE 15: March 3, 2020,  4:42pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> 
71470,Torchaudio.load normalization question,2020-02-28T18:11:30.112Z,1,163,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m new to audio signal processing and to pytorch and I’m having some trouble understanding this part of the docs of the torchaudio load function:</p><NewLine><blockquote><NewLine><p>normalization (bool, number, or callable, optional) – If boolean True, then output is divided by 1 &lt;&lt; 31 (assumes signed 32-bit audio), and normalizes to [-1, 1]. If number, then output is divided by that number If callable, then the output is passed as a parameter to the given function, then the output is divided by the result. (Default: True)</p><NewLine></blockquote><NewLine><p>From what I understand, the function assumes the file to have a <em>bit depth</em> of 32 bit, however that bit depth is rather rare. Does 32-bit audio mean indeed bit depth or something else?</p><NewLine><p>Also I don’t understand what is the meaning of <em>output is divided by 1 &lt;&lt; 31</em>. What is meant by output and what is meant by 1 &lt;&lt; 31?</p><NewLine><p>Thanks for your help</p><NewLine></div>",https://discuss.pytorch.org/u/Laurence_J,,Laurence_J,"February 28, 2020,  6:11pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I also assume 32-bit audio corresponds to the bits per sample.</p><NewLine><p><code>1 &lt;&lt; 31</code> is a left shift by 31 positions, so it translates to <code>1 &lt;&lt; 31 == 2**31  == 2147483648</code>, which would be the max value of each sample.<br/><NewLine>If I’m not mistaken, 32bit audio would have the range <code>[−2,147,483,648, 2,147,483,647]</code>, so you would get a minimal error for the max positive value.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>thank you very much for the reply,</p><NewLine><p>it must be true indeed that</p><NewLine><blockquote><NewLine><p>I also assume 32-bit audio corresponds to the bits per sample.</p><NewLine></blockquote><NewLine><p>as if i normalize with True I get a tensor with max and min values [-1,1]:<br/><NewLine>max: 0.0881<br/><NewLine>min: -0.1289</p><NewLine><p>while if I use normalization=16:<br/><NewLine>max: 11821056<br/><NewLine>min: -17301504</p><NewLine><p>and for normalization=False:<br/><NewLine>max: 1.8914e+08<br/><NewLine>min: -2.7682e+08</p><NewLine><p>indeed the std and avg of the data loaded using normalization True are between 0 and 1 so it seems like the correct normalization for the data I’m working with.</p><NewLine><p>Thank you very much again</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Laurence_J; <NewLine> ,"REPLY_DATE 1: February 29, 2020,  8:44am; <NewLine> REPLY_DATE 2: February 29, 2020, 10:30am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
70860,Target 3988 is out of bounds,2020-02-24T12:58:22.612Z,1,288,"<div class=""post"" itemprop=""articleBody""><NewLine><p>import torch<br/><NewLine>from torch.autograd import Variable<br/><NewLine>import torch.nn as nn<br/><NewLine>from fela import feat, labels<br/><NewLine>from Dataloader import train_loader, test_loader, X_train, X_test, X_val, y_train, y_test, y_val</p><NewLine><p>input_size = 13<br/><NewLine>hidden1_size = 13<br/><NewLine>hidden2_size = 64<br/><NewLine>hidden3_size = 128<br/><NewLine>hidden4_size = 256<br/><NewLine>hidden5_size = 1024<br/><NewLine>output_size = 3988</p><NewLine><p>class DNN(nn.Module):</p><NewLine><pre><code>def __init__(self, input_size, hidden1_size, hidden2_size, hidden3_size, hidden4_size, hidden5_size, output_size):<NewLine>    super(DNN, self).__init__()<NewLine>    self.fc1 = nn.Linear(input_size, hidden1_size)<NewLine>    self.drp1 = nn.Dropout(p=0.2, inplace=False)<NewLine>    self.relu1 = nn.ReLU()<NewLine>    self.fc2 = nn.Linear(hidden1_size, hidden2_size)<NewLine>    self.drp2 = nn.Dropout(p=0.2, inplace=False)<NewLine>    self.relu2 = nn.ReLU()<NewLine>    self.fc3 = nn.Linear(hidden2_size, hidden3_size)<NewLine>    self.drp3 = nn.Dropout(p=0.2, inplace=False)<NewLine>    self.relu3 = nn.ReLU()<NewLine>    self.fc4 = nn.Linear(hidden3_size, hidden4_size)<NewLine>    self.drp4 = nn.Dropout(p=0.2, inplace=False)<NewLine>    self.relu4 = nn.ReLU()<NewLine>    self.fc5 = nn.Linear(hidden4_size, hidden5_size)<NewLine>    self.drp5 = nn.Dropout(p=0.2, inplace=False)<NewLine>    self.relu5 = nn.ReLU()<NewLine>    self.fc6 = nn.Linear(hidden5_size, output_size)<NewLine><NewLine>def forward(self, x):<NewLine>    out = self.fc1(x)<NewLine>    out = self.drp1(out)<NewLine>    out = self.relu1(out)<NewLine>    out = self.fc2(out)<NewLine>    out = self.drp2(out)<NewLine>    out = self.relu2(out)<NewLine>    out = self.fc3(out)<NewLine>    out = self.drp3(out)<NewLine>    out = self.relu3(out)<NewLine>    out = self.fc4(out)<NewLine>    out = self.drp4(out)<NewLine>    out = self.relu4(out)<NewLine>    out = self.fc5(out)<NewLine>    out = self.drp5(out)<NewLine>    out = self.relu5(out)<NewLine>    out = self.fc6(out)<NewLine><NewLine>    return out<NewLine></code></pre><NewLine><p>batch_size = 10</p><NewLine><p>n_iterations = 50</p><NewLine><p>no_eps = n_iterations / (13 / batch_size)</p><NewLine><p>no_epochs = int(no_eps)</p><NewLine><p>model = DNN(input_size, hidden1_size, hidden2_size, hidden3_size, hidden4_size, hidden5_size, output_size)</p><NewLine><p>criterion = nn.CrossEntropyLoss()</p><NewLine><p>learning_rate = 0.0001</p><NewLine><p>optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</p><NewLine><p>iter = 0</p><NewLine><p>for epoch in range(no_epochs):</p><NewLine><pre><code>for i, (X_train, y_train) in enumerate(train_loader):<NewLine><NewLine>    optimizer.zero_grad()<NewLine><NewLine>    outputs = model(Variable(X_train))<NewLine><NewLine>    loss = criterion(outputs, Variable(y_train))<NewLine><NewLine>    print('Iter %d --&gt; loss %f' % (i, loss.item()))<NewLine><NewLine>    loss.backward()<NewLine><NewLine>    optimizer.step()<NewLine><NewLine>correct = 0<NewLine><NewLine>total = 0<NewLine><NewLine>for X_test, y_test in test_loader:<NewLine><NewLine>    outputs = model(Variable(X_test))<NewLine><NewLine>    pred = outputs.argmax(dim=1, keepdim=True)<NewLine><NewLine>    total += y_test.size(0)<NewLine><NewLine>    correct += (pred.squeeze() == y_test).sum()  # pred.eq(y_test.view_as(pre  d)).sum().item()<NewLine><NewLine>accuracy = 100 * correct / total<NewLine><NewLine>print('Iteration: {}.  Accuracy: {}'.format(epoch, accuracy))<NewLine></code></pre><NewLine><p>After some iterations, it gives me this error  {IndexError: Target 3988 is out of bounds}<br/><NewLine>“Hint I am using CPU not GPU”</p><NewLine></div>",https://discuss.pytorch.org/u/Mohamed_Nabih,(Mohamed Nabih),Mohamed_Nabih,"February 24, 2020, 12:58pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Can you share the full stack trace showing where the error happens?<br/><NewLine>If I had to bet, I would say that one of your <code>y_train</code> has an invalid value <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks albaD,<br/><NewLine>I checked it is actually the y_train size it was 3989</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Mohamed_Nabih; <NewLine> ,"REPLY_DATE 1: February 24, 2020,  3:25pm; <NewLine> REPLY_DATE 2: February 24, 2020, 10:41pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
70469,Loss and accuracy,2020-02-20T12:50:55.984Z,0,136,"<div class=""post"" itemprop=""articleBody""><NewLine><p>input_size = 13<br/><NewLine>hidden1_size = 13<br/><NewLine>hidden2_size = 128<br/><NewLine>hidden3_size = 64<br/><NewLine>output_size = 30</p><NewLine><p>class DNN(nn.Module):<br/><NewLine>def <strong>init</strong>(self, input_size, hidden1_size, hidden2_size, hidden3_size, output_size):<br/><NewLine>super(DNN, self).<strong>init</strong>()<br/><NewLine>self.fc1 = nn.Linear(input_size, hidden1_size)<br/><NewLine>self.relu1 = nn.ReLU()<br/><NewLine>self.fc2 = nn.Linear(hidden1_size, hidden2_size)<br/><NewLine>self.relu2 = nn.ReLU()<br/><NewLine>self.fc3 = nn.Linear(hidden2_size, hidden3_size)<br/><NewLine>self.relu3 = nn.ReLU()<br/><NewLine>self.fc4 = nn.Linear(hidden3_size, output_size)<br/><NewLine>self.relu4 = nn.ReLU()</p><NewLine><pre><code>def forward(self, x):<NewLine>    out = self.fc1(x)<NewLine>    out = self.relu1(out)<NewLine>    out = self.fc2(out)<NewLine>    out = self.relu2(out)<NewLine>    out = self.fc3(out)<NewLine>    out = self.relu3(out)<NewLine>    out = self.fc4(out)<NewLine>    out = self.relu4(out)<NewLine>    return out<NewLine></code></pre><NewLine><p>batch_size = 10<br/><NewLine>n_iterations = 50<br/><NewLine>no_eps = n_iterations / (13 / batch_size)<br/><NewLine>no_epochs = int(no_eps)<br/><NewLine>model = DNN(input_size, hidden1_size, hidden2_size, hidden3_size, output_size)</p><NewLine><p>criterion = nn.CrossEntropyLoss()<br/><NewLine>learning_rate = 0.01<br/><NewLine>optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</p><NewLine><p>iter = 0<br/><NewLine>for epoch in range(no_epochs):<br/><NewLine>for i, (X_train, y_train) in enumerate(train_loader):<br/><NewLine>y_train = torch.empty(batch_size, dtype=torch.long).random_(output_size)<br/><NewLine>optimizer.zero_grad()<br/><NewLine>outputs = model(X_train)<br/><NewLine>loss = criterion(outputs, y_train)<br/><NewLine>loss.backward()<br/><NewLine>optimizer.step()<br/><NewLine>iter += 1<br/><NewLine>if iter % 500 == 0:<br/><NewLine>correct = 0<br/><NewLine>total = 0<br/><NewLine>for X_test, y_test in test_loader:<br/><NewLine>outputs = model(X_test)<br/><NewLine>pred = outputs.argmax(dim=1, keepdim=True)<br/><NewLine>total += y_test.size(0)<br/><NewLine>correct += pred.eq(y_test.view_as(pred)).sum().item()<br/><NewLine>accuracy = 100 * correct / total<br/><NewLine>print(‘Iteration: {}. Loss: {}. Accuracy: {}’.format(iter, loss.item(), accuracy))</p><NewLine><hr/><NewLine><p>The output start with Iteration 500 not 1 why !!!<br/><NewLine>Iteration: 500. Loss: 3.403779983520508. Accuracy: 0.2761323337093925<br/><NewLine>Iteration: 1000. Loss: 3.4060192108154297. Accuracy: 0.3276070990724763<br/><NewLine>Iteration: 1500. Loss: 3.416713237762451. Accuracy: 0.4173101012337052<br/><NewLine>Iteration: 2000. Loss: 3.402294635772705. Accuracy: 0.34867708074959347<br/><NewLine>Iteration: 2500. Loss: 3.3952858448028564. Accuracy: 0.2755100135754692<br/><NewLine>Iteration: 3000. Loss: 3.4023067951202393. Accuracy: 0.3158719194042085</p><NewLine></div>",https://discuss.pytorch.org/u/Mohamed_Nabih,(Mohamed Nabih),Mohamed_Nabih,"February 20, 2020, 12:50pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You are increasing <code>iter</code> directly before the if condition to 1, which result in <code>iter % 500 == 0</code> when <code>iter</code> reaches multiples of 500.<br/><NewLine>If you want to print the very first iteration, increase <code>iter</code> after the condition.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: February 21, 2020,  6:17am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
70068,Read multiple .gz file and return it in one tensor,2020-02-17T16:04:09.150Z,1,152,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to read multiple .gz file and return its content in one tensor as follows:</p><NewLine><pre><code class=""lang-auto"">value_list = [ ]<NewLine><NewLine>with ReadHelper('ark: gunzip -c /home/mnabih/kaldi/egs/timit/s5/exp/mono_ali/*.gz|') as reader:<NewLine><NewLine>    for i, b in enumerate(reader):<NewLine><NewLine>        value = numpy.asarray(b[1])<NewLine><NewLine>        value_list.append(value)<NewLine><NewLine>        value = torch.from_numpy(value)<NewLine><NewLine>        print(type(value))<NewLine></code></pre><NewLine><p>it gives me multiple tensors, I tried this command to concatenate it</p><NewLine><pre><code>    values = torch.cat(value, dim = 0, out=None)<NewLine>But it gives me <NewLine></code></pre><NewLine><p>cat() received an invalid combination of arguments - got (Tensor, out=NoneType, dim=int), but expected one of:</p><NewLine><ul><NewLine><li>(tuple of Tensors tensors, name dim, Tensor out)</li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/Mohamed_Nabih,(Mohamed Nabih),Mohamed_Nabih,"February 17, 2020,  4:04pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you append tensors to the list and call <code>torch.tensor(value_list)</code>?<br/><NewLine>This should work:</p><NewLine><pre><code class=""lang-python"">value = np.random.randn(10, 10)<NewLine>value_list = []<NewLine>for _ in range(3):<NewLine>    value_list.append(value)<NewLine><NewLine>res = torch.tensor(value_list)<NewLine>print(res.shape)<NewLine>&gt; torch.Size([3, 10, 10])<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: February 18, 2020,  3:51am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
69722,"RuntimeError: Expected 4-dimensional input for 4-dimensional weight 32 1 7 7, but got 3-dimensional input of size [462, 2, 14] instead",2020-02-14T11:16:30.343Z,0,91,"<div class=""post"" itemprop=""articleBody""><NewLine><p>class CNN(nn.Module):</p><NewLine><pre><code>def __init__(self):<NewLine>    super(CNN, self).__init__()<NewLine><NewLine>    self.cnn1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=7, stride=4, padding=3)<NewLine>    self.relu1 = nn.ReLU()Preformatted text<NewLine>    self.maxpool1 = nn.MaxPool2d(kernel_size=2)<NewLine>    self.cnn2 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=5, stride=1, padding=2)<NewLine>    self.relu2 = nn.ReLU()<NewLine>    self.maxpool2 = nn.MaxPool2d(kernel_size=2)<NewLine>    self.fc1 = nn.Linear(3136, 10)<NewLine>    self.fcrelu = nn.ReLU()<NewLine><NewLine>def forward(self, x):<NewLine>    out = self.cnn1(x)<NewLine>    out = self.relu1(out)<NewLine>    out = self.maxpool1(out)<NewLine><NewLine>    out = self.cnn2(out)<NewLine>    out = self.relu2(out)<NewLine>    out = self.maxpool2(out)<NewLine><NewLine>    out = out.view(out.size(0), -1)<NewLine><NewLine>    out = self.fc1(out)<NewLine>    out = self.fcrelu(out)<NewLine><NewLine>    return out</code></pre><NewLine></div>",https://discuss.pytorch.org/u/Mohamed_Nabih,(Mohamed Nabih),Mohamed_Nabih,"February 14, 2020, 11:17am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>A 2D layer, e.g. <code>nn.Conv2d</code>, expects the input to have the shape <code>[batch_size, channels, height, width]</code>.<br/><NewLine>The <code>in_channels</code> of the first conv layer correspond to the <code>channels</code> in your input.</p><NewLine><p>Based on the definition of <code>self.cnn1</code> it seems you want to pass an input with a single channel, so you might want to call <code>x = x.unsqueeze(1)</code> to create the channel dimension.<br/><NewLine>This would only work, if your current input is defined as <code>[batch_size, height, width]</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: February 14, 2020,  7:43pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
69613,Return the tensor part from the generator class,2020-02-13T15:19:06.790Z,0,98,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><p>I am using this command to read the ark.files<br/><NewLine>train_feat = torchaudio.kaldi_io.read_mat_ark(’/home/mnabih/kaldi/egs/timit/s5/mfcc/cmvn_train.ark’)<br/><NewLine>The output type is Generator [(str, torch.Tensor)]<br/><NewLine>How can I extract the tensor part only to train my network.<br/><NewLine>Also,<br/><NewLine>d = {u: d for u, d in torchaudio.kaldi_io.read_mat_ark(’/home/mnabih/kaldi/egs/timit/s5/mfcc/cmvn_train.ark’)}</p><NewLine><p>it gives me class  with key: string and value: torch.tensor<br/><NewLine>I want to remove the key and using the tensor to train my network</p><NewLine></div>",https://discuss.pytorch.org/u/Mohamed_Nabih,(Mohamed Nabih),Mohamed_Nabih,"February 13, 2020,  3:19pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you tried?</p><NewLine><pre><code class=""lang-auto"">g = torchaudio.kaldi_io.read_mat_ark(’/home/mnabih/kaldi/egs/timit/s5/mfcc/cmvn_train.ark’)<NewLine><NewLine>d = {u: d for u, d in g}  # dictionary<NewLine>t = [d for _, d in g]  # list of tensors<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vincentqb; <NewLine> ,"REPLY_DATE 1: February 14, 2020,  7:19pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
68897,Audio datatype (does float16 make sense?),2020-02-06T18:21:09.993Z,3,170,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,<br/><NewLine>I’m an intermediate PyTorch user (only did a vision project before) who wants to use torchaudio for something new.<br/><NewLine>I’m making currently making a dataset loader for the NSynth dataset (16bit 16kHz PCM wav files) specific to my application.<br/><NewLine>As I’ll be training on an RTX GPU, and the data is originally 16bit, I was wondering if it would be smart to use float16 in this case. I’m not exactly sure about this though, given my limited knowledge of signal processing. torchaudio.load returns 32bit floats by default and does not give the option to load float16, so I was wondering whether there exists a theoretical reason.</p><NewLine></div>",https://discuss.pytorch.org/u/timlod,(Tim Loderhose),timlod,"February 6, 2020,  6:21pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think the data loading should be performed in FP16, as you might end up with some quantization noise.<br/><NewLine>If I’m not mistaken, the 16bit audio files represent 65536 different levels.<br/><NewLine>Since FP16 cannot represent all integers &gt;2048 (<a href=""https://en.wikipedia.org/wiki/Half-precision_floating-point_format"">Wikipedia - FP16</a>), you’ll lose some information.</p><NewLine><p>That being said, once you’ve loaded and preprocessed the data, you could still use FP16 for the model training. Have a look at <a href=""https://github.com/NVIDIA/apex"">apex/amp</a> for an automatic mixed-precision approach.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually, 64bit float can maintain the original information without any loss. According to my  experience, the amount of information lost from using float32 is actually quite small.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>FP32 is able to represent all integers in <code>[-16777216, 16777216]</code>, which should thus work for these audio files. Why would you lose information in this use case or did I misunderstood your explanation?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I did some googling and found that indeed I was wrong. My original conclusion was based on the fact that when I used librosa to load wav files and set the precision to fp32, some values were rounded. But in the case of fp64, the exact value are shown. I guess perhaps that was due to display issue? I’m not sure what’s mechanism behind that.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks! Makes a lot of sense. I think I’ll stay away from apex for now, I don’t want to overcomplicate things. I guess I can always start with FP32 and move to FP16 later and compare.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/G.M; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/G.M; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/timlod; <NewLine> ,"REPLY_DATE 1: February 7, 2020,  2:25am; <NewLine> REPLY_DATE 2: February 7, 2020,  2:37am; <NewLine> REPLY_DATE 3: February 7, 2020,  2:48am; <NewLine> REPLY_DATE 4: February 7, 2020,  9:58am; <NewLine> REPLY_DATE 5: February 7, 2020,  1:31pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
68476,A spectrogram inversion package for PyTorch,2020-02-03T06:21:21.929Z,1,234,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello folks~</p><NewLine><p>I made an PyTorch package that included some classic spectrogram inversion algorithms (like Griffin-Lim) to recover phase information given only the magnitude response of audio. I would like to invite everyone to take a look and use it.</p><NewLine><p>You can find its repository here: <a href=""https://github.com/yoyololicon/spectrogram-inversion"" rel=""nofollow noopener"">https://github.com/yoyololicon/spectrogram-inversion</a><br/><NewLine>The docs: <a href=""https://spectrogram-inversion.readthedocs.io/"" rel=""nofollow noopener"">https://spectrogram-inversion.readthedocs.io/</a>.<br/><NewLine>Any issues and contributions are wellcome.</p><NewLine><p>Cheers <img alt="":slightly_smiling_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slightly_smiling_face.png?v=9"" title="":slightly_smiling_face:""/></p><NewLine></div>",https://discuss.pytorch.org/u/yoyololicon,,yoyololicon,"February 3, 2020,  6:21am",4 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for sharing your code! <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/><br/><NewLine>May I ask the naive question, when this would be used?<br/><NewLine>Could we e.g. create a spectrum using a GAN and use your inversion algos to create the waveform?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Exactly, as long as the spectrum follow the regular fourier representation.<br/><NewLine>We assume the target magnitude spectrum that used in training is obtained using <code>torch.stft</code>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for sharing your code indeed! <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/> Are there additions you’d also like to see in the implementation available in the master branch of <a href=""https://github.com/pytorch/audio/pull/365"" rel=""nofollow noopener"">torchaudio</a>?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/yoyololicon; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vincentqb; <NewLine> ,"REPLY_DATE 1: February 4, 2020,  2:55am; <NewLine> REPLY_DATE 2: February 4, 2020,  6:13am; <NewLine> REPLY_DATE 3: February 6, 2020,  5:38pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
67465,Integrate Kaldi with pytorch,2020-01-23T13:06:16.838Z,0,260,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Dear All;</p><NewLine><p>I am running Kaldi ASR toolkit and fit MFCC features from speech Dataset and stored it in .ark, .scp and CMVN files, so how I can train my Network based on these files</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/Mohamed_Nabih,(Mohamed Nabih),Mohamed_Nabih,"January 23, 2020,  1:06pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could use a library like <a href=""https://pypi.org/project/kaldiio/"">kaldiio</a> to load these samples and create a custom <code>Dataset</code> and pass it to a <code>DataLoader</code> as explained in <a href=""https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"">this tutorial</a>.</p><NewLine><p>Once you have the <code>Dataset</code> ready, you could continue working with the architecture or your model.<br/><NewLine>I’m not completely sure how the data is stored, but since you are dealing with MFCC data, I assume you could treat it as “image” data?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://pytorch.org/audio/"" rel=""nofollow noopener"">torchaudio</a> supports <a href=""https://pytorch.org/audio/kaldi_io.html"" rel=""nofollow noopener"">ark and scp</a>, offers <a href=""https://pytorch.org/audio/transforms.html#mfcc"" rel=""nofollow noopener"">MFCC</a>, and also has a template for <a href=""https://pytorch.org/audio/datasets.html"" rel=""nofollow noopener"">datasets with DataLoader</a> <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vincentqb; <NewLine> ,"REPLY_DATE 1: January 24, 2020,  6:03am; <NewLine> REPLY_DATE 2: January 27, 2020, 11:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
57043,DataLoader Runtime Error with Melspectrograms and workers,2019-09-28T19:18:10.437Z,0,588,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello! I am constantly running into a RuntimeError that I cannot seem to understand, and I would be very grateful if someone can explain what is happening and how I can fix it.</p><NewLine><p>I am creating a dataset class and dataloader for music genre classification. The main issue arises when I include a mel-spectrogram transform from torchaudio.</p><NewLine><p>Here is a script that shows the error I have.</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import torch<NewLine>from torch.utils.data import Dataset<NewLine>from torch.utils import data<NewLine>import torchaudio<NewLine>import pandas as pd<NewLine><NewLine>sr = 22050<NewLine><NewLine>class Mp3Dataset(Dataset):<NewLine>    """"""<NewLine>    Mp3 dataset class to work with the FMA dataset.<NewLine>    Input:<NewLine>    df - pandas dataframe containing track_id and genre.<NewLine>    audio_path - directory with mp3 files<NewLine>    duration - how much of the songs to sample<NewLine>    """"""<NewLine><NewLine>    def __init__(self, df: pd.DataFrame, audio_path: str, duration: float):<NewLine><NewLine>        self.audio_path = audio_path<NewLine>        self.IDs = df['track_id'].astype(str).to_list()<NewLine>        self.genre_list = df.genre.to_list()<NewLine>        self.duration = duration<NewLine><NewLine>        self.E = torchaudio.sox_effects.SoxEffectsChain()<NewLine>        self.E.append_effect_to_chain(""trim"", [0, self.duration])<NewLine>        self.E.append_effect_to_chain(""rate"", [sr])<NewLine>        self.E.append_effect_to_chain(""channels"", [""1""])<NewLine><NewLine>        self.mel = torchaudio.transforms.MelSpectrogram(sample_rate=sr)<NewLine><NewLine>    def __len__(self):<NewLine>        return len(self.IDs)<NewLine><NewLine>    def __getitem__(self, index):<NewLine>        ID = self.IDs[index]<NewLine>        genre = self.genre_list[index]<NewLine><NewLine>        # sox: set input file<NewLine>        self.E.set_input_file(self.get_path_from_ID(ID))<NewLine><NewLine>        # use sox to read in the file using my effects<NewLine>        waveform, _ = self.E.sox_build_flow_effects()  # size: [1, len * sr]<NewLine><NewLine>        melspec = self.mel(waveform)<NewLine><NewLine>        return melspec, genre<NewLine><NewLine>    def get_path_from_ID(self, ID):<NewLine>        """"""<NewLine>        Gets the audio path from the ID using the FMA dataset format<NewLine>        """"""<NewLine>        track_id = ID.zfill(6)<NewLine><NewLine>        return os.path.join(self.audio_path, track_id[:3], track_id + '.mp3')<NewLine><NewLine>if __name__ == '__main__':<NewLine><NewLine>    # my path to audio files<NewLine>    audio_path = os.path.join('data', 'fma_small')<NewLine><NewLine>    # my dataframe that has track_id and genre info<NewLine>    df = pd.read_csv('data/fma_metadata/small_track_info.csv')<NewLine><NewLine>    torchaudio.initialize_sox()<NewLine><NewLine>    dataset = Mp3Dataset(df, audio_path, 1.0)<NewLine><NewLine>    params = {'batch_size': 8, 'shuffle': True, 'num_workers': 2}<NewLine><NewLine>    dataset_loader = data.DataLoader(dataset, **params)<NewLine><NewLine>    print(next(iter(dataset_loader)))<NewLine><NewLine>    torchaudio.shutdown_sox()<NewLine><NewLine></code></pre><NewLine><p>When I run this code, I get the following errors thrown at me:</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>~/Documents/data_sci.nosync/fma/min_ex.py in &lt;module&gt;<NewLine>     71     dataset_loader = data.DataLoader(dataset, **params)<NewLine>     72<NewLine>---&gt; 73     print(next(iter(dataset_loader)))<NewLine>     74<NewLine>     75     torchaudio.shutdown_sox()<NewLine><NewLine>/usr/local/anaconda3/envs/pytorch_fma/lib/python3.7/site-packages/torch/utils/data/dataloader.py in __next__(self)<NewLine>    817             else:<NewLine>    818                 del self.task_info[idx]<NewLine>--&gt; 819                 return self._process_data(data)<NewLine>    820<NewLine>    821     next = __next__  # Python 2 compatibility<NewLine><NewLine>/usr/local/anaconda3/envs/pytorch_fma/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _process_data(self, data)<NewLine>    844         self._try_put_index()<NewLine>    845         if isinstance(data, ExceptionWrapper):<NewLine>--&gt; 846             data.reraise()<NewLine>    847         return data<NewLine>    848<NewLine><NewLine>/usr/local/anaconda3/envs/pytorch_fma/lib/python3.7/site-packages/torch/_utils.py in reraise(self)<NewLine>    367             # (https://bugs.python.org/issue2651), so we work around it.<NewLine>    368             msg = KeyErrorMessage(msg)<NewLine>--&gt; 369         raise self.exc_type(msg)<NewLine><NewLine>RuntimeError: Caught RuntimeError in DataLoader worker process 0.<NewLine>Original Traceback (most recent call last):<NewLine>  File ""/usr/local/anaconda3/envs/pytorch_fma/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py"", line 178, in _worker_loop<NewLine>    data = fetcher.fetch(index)<NewLine>  File ""/usr/local/anaconda3/envs/pytorch_fma/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 47, in fetch<NewLine>    return self.collate_fn(data)<NewLine>  File ""/usr/local/anaconda3/envs/pytorch_fma/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py"", line 80, in default_collate<NewLine>    return [default_collate(samples) for samples in transposed]<NewLine>  File ""/usr/local/anaconda3/envs/pytorch_fma/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py"", line 80, in &lt;listcomp&gt;<NewLine>    return [default_collate(samples) for samples in transposed]<NewLine>  File ""/usr/local/anaconda3/envs/pytorch_fma/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py"", line 56, in default_collate<NewLine>    return torch.stack(batch, 0, out=out)<NewLine>RuntimeError: stack(): functions with out=... arguments don't support automatic differentiation, but one of the arguments requires grad.<NewLine></code></pre><NewLine><p>Two things I observe with this script:</p><NewLine><ol><NewLine><li>Removing the mel-spectrogram transform and outputting the waveform instead removes this error.</li><NewLine><li>Changing <code>num_workers</code> to <code>0</code> removes this error as well.</li><NewLine></ol><NewLine><p>To be clear, these observations are independent. Performing only one of these modifications removes the error. However, I would like both workers and a melspectrogram transform</p><NewLine><p>I am using PyTorch 1.2.0 and Torchaudio 0.3.0+bf88aef, as well as python 3.7.4.</p><NewLine></div>",https://discuss.pytorch.org/u/awray3,(awray3),awray3,"September 28, 2019,  7:18pm",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have the exact same problem.<br/><NewLine>Until now, I did not find a solution.<br/><NewLine><strong>Edit, posting possible solution:</strong><br/><NewLine>Using python 3.6.9<br/><NewLine>Updating to torch 1.3.1<br/><NewLine>and torchaudio 0.4.0<br/><NewLine>did the trick for me.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/patientzero; <NewLine> ,"REPLY_DATE 1: January 22, 2020, 12:03pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
66909,Spectrogram to RGB pictures for Resnet: faint image with mp3 files,2020-01-16T22:50:10.921Z,1,322,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I am trying to generate pictures from audio spectrogram. I couldn’t find specific examples on internet and I attempted to put together a solution myself.</p><NewLine><p>I managed to implement an algorithm that can generate pictures passing files encoded mp3 or wav.<br/><NewLine>At high level everything seems to work ok for Wav files but for mp3 I seem to generate a picture where the spectrum is faint (compared to the one generated by the wav file).<br/><NewLine>I generated the files using Audacity and I saved the track to mp3 or wav.<br/><NewLine>Below is the code. I am using Jupiterlab on Sagemaker for the runtime environment.</p><NewLine><p>This seems to be a standard use case in audio classification modelling. Can somebody help explain the reason behind this and whether there is any resource that could have code that can convert audio to RGB pictures for Resnet ingestion ?</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torchaudio<NewLine>import matplotlib.pyplot as plt<NewLine><NewLine>def normalize_input(tensor):<NewLine>    # Subtract the mean, and scale to the interval [-1,1]<NewLine>    tensor_minusmean = tensor - tensor.mean()<NewLine>    return tensor_minusmean/tensor_minusmean.abs().max()<NewLine><NewLine><NewLine>filename = ""/home/ec2-user/SageMaker/GiuseppeProjects/GiuseppeTest.mp3""<NewLine><NewLine>waveform, sample_rate = torchaudio.load(filename)<NewLine>#waveform = np.delete(waveform, (1), axis=0)<NewLine>waveform = normalize_input(waveform)<NewLine>print(""Shape of waveform: {}"".format(waveform.size()))<NewLine>print(""Sample rate of waveform: {}"".format(sample_rate))<NewLine><NewLine>#print(waveform)<NewLine><NewLine>plt.figure()<NewLine>plt.plot(waveform.t().numpy())<NewLine>plt.show()<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">import sys<NewLine>from torchvision import transforms<NewLine>import torchvision<NewLine>from skimage.util import img_as_ubyte<NewLine>from skimage import exposure<NewLine>from sklearn import preprocessing<NewLine>from PIL import Image<NewLine>import numpy as np<NewLine><NewLine><NewLine>transform_spectra = transforms.Compose([<NewLine>            transforms.ToPILImage(),<NewLine>            transforms.Resize((224,224)), <NewLine>            transforms.RandomVerticalFlip(1)<NewLine>        ])<NewLine><NewLine>def make0min(tensornd):<NewLine>    tensor = tensornd.numpy()<NewLine>    res = np.where(tensor == 0, 1E-19 , tensor)<NewLine>    return torch.from_numpy(res)<NewLine><NewLine>def normalize(tensor):<NewLine>    tensor_minusmean = tensor - tensor.mean()<NewLine>    return tensor_minusmean/tensor_minusmean.abs().max()<NewLine><NewLine>def normalize_nd(tensor):<NewLine>    tensor_minusmean = tensor - tensor.mean()    <NewLine>    return tensor_minusmean/np.absolute(tensor_minusmean).max()<NewLine>    <NewLine>def spectrogrameToImage(waveform):<NewLine>    specgram = torchaudio.transforms.Spectrogram(n_fft=400, win_length=None, hop_length=None, pad=0,window_fn=torch.hann_window, power=2, normalized=True, wkwargs=None)(waveform )<NewLine>    specgram= make0min(specgram)<NewLine>    specgram = specgram.log2()[0,:,:].numpy()<NewLine>    <NewLine>    <NewLine>    np.set_printoptions(linewidth=300)<NewLine>    np.set_printoptions(threshold=sys.maxsize)<NewLine><NewLine>    specgram= normalize_nd(specgram)<NewLine>    specgram = img_as_ubyte(specgram)<NewLine>    specgramImage = transform_spectra(specgram)<NewLine>    return specgramImage<NewLine><NewLine>def print_spec(spec):<NewLine>    torch.set_printoptions(linewidth=150)<NewLine>    torch.set_printoptions(profile=""full"")<NewLine>    #spec = torchvision.transforms.ToTensor()(spec)<NewLine>    spec = np.array(spec)<NewLine>    #print (spec)<NewLine>    <NewLine>    <NewLine>waveform = normalize(waveform)<NewLine>spec = spectrogrameToImage(waveform)<NewLine>spec = spec.convert('RGB')<NewLine><NewLine>plt.figure()<NewLine>plt.imshow(spec)<NewLine></code></pre><NewLine><p><img alt=""mp3vswav"" data-base62-sha1=""txYGQEaeYlLtEbVtqqqOmlj94jU"" height=""225"" src=""https://discuss.pytorch.org/uploads/default/original/3X/c/f/cf16560655da14fa98671dda0b860b824f70c8aa.png"" width=""400""/></p><NewLine></div>",https://discuss.pytorch.org/u/Giuseppe_Sarno,(Giuseppe Sarno),Giuseppe_Sarno,"January 16, 2020, 10:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I seem to have achieved better results by eliminating the first 10 data points of the spectrogram. It looks like there is a clear difference in the spectrogram for the first data points which causes this problem when normalizing the picture.<br/><NewLine>I assume this is something with the difference between MP3 and WAV that anything to do with Spectrogram ?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you printed a sample from each waveform? Printing some statistics like mean, std dev, range, of each, and some information about the difference would also help. I expect the difference to come from the waveform and not the spectrogram.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Giuseppe_Sarno; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vincentqb; <NewLine> ,"REPLY_DATE 1: January 18, 2020, 11:53am; <NewLine> REPLY_DATE 2: January 21, 2020,  6:20pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
66566,Torchaudio Spectrogram returning some data points as 0 and Log2 inf,2020-01-13T21:26:48.653Z,2,232,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I followed the tutorial at <a href=""https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html</a><br/><NewLine>and applied log2 to the spectrogram. However one of my mp3 files is as such the Spectrogram returns data = 0. This makes the log2 to return inf.<br/><NewLine>Is this expected ? or is there something it can be done to avoid this scenario?</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Giuseppe_Sarno,(Giuseppe Sarno),Giuseppe_Sarno,"January 13, 2020,  9:45pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is expected. Do you need to apply the log transformation? If so, you can also add a little value before taking log:</p><NewLine><pre><code class=""lang-auto"">epsilon = 1e-6<NewLine>log2(epsilon + specgram)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""66566""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/v/46a35a/40.png"" width=""20""/> vincentqb:</div><NewLine><blockquote><NewLine><p>epsilon = 1e-6</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thank you, will need to do something like this.</p><NewLine><p>The odd problem I have is the following:</p><NewLine><p>I am converting / comparing 2 sound files. One WAV and the other the MP3 of the same file then create the spectrogram and convert to image.<br/><NewLine>I get the issue with the MP3 version of the file while everything is ok for WAV (noticed this consistently with other audios).<br/><NewLine>The other key difference is that because the first data points for each row are very small compared to the others when I normalize the pictures to RGB (0 -255) the image is very faint. While the pictures generated with a Wav file is good. I normalize the waveform, spectrum and the image.<br/><NewLine>Are there techniques to avoid this problem ? any particular reason why the MP3 version has this different behavior ?</p><NewLine><p>Thanks,</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you provide a minimal code and files to reproduce? Is the mp3 converted from the wav or vice-versa?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi vincentqb,<br/><NewLine>I created a new topic as the problem is now different to the question on this topic <a href=""https://discuss.pytorch.org/t/spectrogram-to-rgb-pictures-for-resnet-faint-image-with-mp3-files/66909"">spectrogram-to-rgb-pictures-for-resnet-faint-image-with-mp3-files</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vincentqb; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Giuseppe_Sarno; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vincentqb; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Giuseppe_Sarno; <NewLine> ,"REPLY_DATE 1: January 15, 2020,  4:10pm; <NewLine> REPLY_DATE 2: January 14, 2020, 11:12pm; <NewLine> REPLY_DATE 3: January 15, 2020,  5:47pm; <NewLine> REPLY_DATE 4: January 16, 2020, 10:51pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
65530,Unable to install latest torchaudio using pip,2020-01-01T17:51:18.332Z,2,549,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to install the latest torchaudio version from <a href=""https://download.pytorch.org/whl/nightly/torch_nightly.html"" rel=""nofollow noopener"">this link</a> using the following pip command:</p><NewLine><pre><code class=""lang-auto"">pip install torchaudio_nightly -f https://download.pytorch.org/whl/nightly/torch_nightly.html<NewLine></code></pre><NewLine><p>This downloads and installs torchaudio version <code>0.4.0.dev20190801</code>. However, on the given link there are multiple wheels available for a torchaudio 0.4.0 dev version with a later date (all the way up to 20200101). How can I install one of the later versions (preferably the latest one) using pip?</p><NewLine></div>",https://discuss.pytorch.org/u/torslim,(torslim),torslim,"January 3, 2020, 10:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you have some dependency that fix your torchaudio version to this one? like torch_nightly?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, when using a completely empty environment and installing torchaudio_nightly using the command above it the latest version pip downloads is 0.4.0-dev20190801, see <a href=""https://colab.research.google.com/drive/1esAfqlbSoSqY0fG8OZgPvX9IAsi21BC4"" rel=""nofollow noopener"">this Colab notebook</a>. There clearly are torchaudio nightly versions available from a newer data as can be seen on the html nightly link. Conda has a newer version available from <a href=""https://anaconda.org/pytorch-nightly/torchaudio"" rel=""nofollow noopener"">this link</a> (v0.4.0.dev20200102).</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/vincentqb"">@vincentqb</a> who is working on audio</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/torslim; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: January 2, 2020, 11:39am; <NewLine> REPLY_DATE 2: January 2, 2020,  6:15pm; <NewLine> REPLY_DATE 3: January 3, 2020, 10:10am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
65443,Can I work with torchaudio in windows,2019-12-31T07:40:10.368Z,0,768,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I can’t import torchaudio module in python 3 base conda does it support windows or it works then how do I use I am new in pytorch</p><NewLine></div>",https://discuss.pytorch.org/u/Vijay,(Vijay),Vijay,"December 31, 2019,  7:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for asking! We don’t officially offer binaries yet for torchaudio in windows, see <a href=""https://github.com/pytorch/audio/issues/50"" rel=""nofollow noopener"">open issue</a>. You can try compiling from source using the instructions <a href=""https://github.com/pytorch/audio/blob/master/README.md"" rel=""nofollow noopener"">here</a>. Please do post about issues you may run into while doing so <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vincentqb; <NewLine> ,"REPLY_DATE 1: January 2, 2020,  4:33pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
62400,Torchaudio.load() in C++,2019-11-28T09:32:08.462Z,0,304,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to convert ogg-file to <code>torch.Tensor</code> using <code>torchaudio.load</code> function, but in deployment I need to do it in C++ (I converted original model to TorchScript). Can I use it from C++? Or what can I do as alternative?</p><NewLine></div>",https://discuss.pytorch.org/u/Oktai15,(Oktai Tatanov),Oktai15,"November 28, 2019,  9:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>torchaudio does not yet have a C++ interface. However, <code>torchaudio.load</code> is a wrapper around <a href=""https://github.com/pytorch/audio/blob/master/torchaudio/torch_sox.cpp"" rel=""nofollow noopener"">https://github.com/pytorch/audio/blob/master/torchaudio/torch_sox.cpp</a> which you could use.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vincentqb; <NewLine> ,"REPLY_DATE 1: December 2, 2019, 11:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
60781,Module &lsquo;torchaudio.transforms&rsquo; has no attribute &lsquo;DownmixMono&rsquo;,2019-11-12T20:09:19.379Z,0,624,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I’m using <strong>torchaudio</strong> v0.3.1 and I’m getting  this error while I’m running torchaudio official example in google colab,</p><NewLine><blockquote><NewLine><p><code>module 'torchaudio.transforms' has no attribute 'DownmixMono'</code></p><NewLine></blockquote><NewLine><p><a href=""https://pytorch.org/tutorials/beginner/audio_classifier_tutorial.html?highlight=audio"" rel=""nofollow noopener"">https://pytorch.org/tutorials/beginner/audio_classifier_tutorial.html?highlight=audio</a><br/><NewLine><a href=""https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/audio_classifier_tutorial.ipynb"" rel=""nofollow noopener"">https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/audio_classifier_tutorial.ipynb</a></p><NewLine></div>",https://discuss.pytorch.org/u/arminarj,(Armin Arjmand),arminarj,"November 12, 2019,  8:09pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>DownmixMono</code> has been deprecated, see <a href=""https://github.com/pytorch/audio/releases"" rel=""nofollow noopener"">breaking changes</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you open to degrade, this works to me.</p><NewLine><p><code>conda install pytorch==1.1.0 torchvision==0.3.0 cudatoolkit=10.0 torchaudio=0.2.0 -c pytorch</code></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vincentqb; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/huckiyang; <NewLine> ,"REPLY_DATE 1: December 7, 2019,  7:14pm; <NewLine> REPLY_DATE 2: November 24, 2019,  2:21am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
61392,Cannot import torchaudio,2019-11-18T19:42:54.503Z,1,296,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have an error when trying to import torchaudio:</p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; import torchaudio<NewLine>---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-3-4cf0a64f61c0&gt; in &lt;module&gt;<NewLine>----&gt; 1 import torchaudio<NewLine><NewLine>/opt/conda/lib/python3.6/site-packages/torchaudio-0.4.0a0+256458f-py3.6-linux-x86_64.egg/torchaudio/__init__.py in &lt;module&gt;<NewLine>      5 import _torch_sox<NewLine>      6 <NewLine>----&gt; 7 from torchaudio import transforms, datasets, kaldi_io, sox_effects, compliance, _docs<NewLine>      8 <NewLine>      9 try:<NewLine><NewLine>/opt/conda/lib/python3.6/site-packages/torchaudio-0.4.0a0+256458f-py3.6-linux-x86_64.egg/torchaudio/transforms.py in &lt;module&gt;<NewLine>      4 import torch<NewLine>      5 from typing import Optional<NewLine>----&gt; 6 from . import functional as F<NewLine>      7 from .compliance import kaldi<NewLine>      8 <NewLine><NewLine>/opt/conda/lib/python3.6/site-packages/torchaudio-0.4.0a0+256458f-py3.6-linux-x86_64.egg/torchaudio/functional.py in &lt;module&gt;<NewLine>    637 <NewLine>    638 @torch.jit.script<NewLine>--&gt; 639 def highpass_biquad(waveform, sample_rate, cutoff_freq, Q=0.707):<NewLine>    640     # type: (Tensor, int, float, float) -&gt; Tensor<NewLine>    641     r""""""Designs biquad highpass filter and performs filtering.  Similar to SoX implementation.<NewLine><NewLine>/opt/conda/lib/python3.6/site-packages/torch/jit/__init__.py in script(obj, optimize, _frames_up, _rcb)<NewLine>   1209         if _rcb is None:<NewLine>   1210             _rcb = _gen_rcb(obj, _frames_up)<NewLine>-&gt; 1211         fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))<NewLine>   1212         # Forward docstrings<NewLine>   1213         fn.__doc__ = obj.__doc__<NewLine><NewLine>RuntimeError: <NewLine>Arguments for call are not valid.<NewLine>The following operator variants are available:<NewLine>  <NewLine>  aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -&gt; (Tensor(a!)):<NewLine>  Expected a value of type 'Tensor' for argument 'self' but instead found type 'float'.<NewLine>  <NewLine>  aten::div.Tensor(Tensor self, Tensor other) -&gt; (Tensor):<NewLine>  Expected a value of type 'Tensor' for argument 'self' but instead found type 'float'.<NewLine>  <NewLine>  aten::div.Scalar(Tensor self, Scalar other) -&gt; (Tensor):<NewLine>  Expected a value of type 'Tensor' for argument 'self' but instead found type 'float'.<NewLine>  <NewLine>  aten::div(int a, int b) -&gt; (float):<NewLine>  Expected a value of type 'int' for argument 'a' but instead found type 'float'.<NewLine>  <NewLine>  aten::div(float a, float b) -&gt; (float):<NewLine>  Expected a value of type 'float' for argument 'b' but instead found type 'int'.<NewLine>  <NewLine>  div(float a, Tensor b) -&gt; (Tensor):<NewLine>  Expected a value of type 'Tensor' for argument 'b' but instead found type 'int'.<NewLine>  <NewLine>  div(int a, Tensor b) -&gt; (Tensor):<NewLine>  Expected a value of type 'int' for argument 'a' but instead found type 'float'.<NewLine><NewLine>The original call is:<NewLine>at /opt/conda/lib/python3.6/site-packages/torchaudio-0.4.0a0+256458f-py3.6-linux-x86_64.egg/torchaudio/functional.py:654:9<NewLine>        sample_rate (int): sampling rate of the waveform, e.g. 44100 (Hz)<NewLine>        cutoff_freq (float): filter cutoff frequency<NewLine>        Q (float): https://en.wikipedia.org/wiki/Q_factor<NewLine><NewLine>    Returns:<NewLine>        output_waveform (torch.Tensor): Dimension of `(channel, time)`<NewLine>    """"""<NewLine><NewLine>    GAIN = 1.<NewLine>    w0 = 2 * math.pi * cutoff_freq / sample_rate<NewLine>         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>    A = math.exp(GAIN / 40.0 * math.log(10))<NewLine>    alpha = math.sin(w0) / 2. / Q<NewLine>    mult = _dB2Linear(max(GAIN, 0))<NewLine><NewLine>    b0 = (1 + math.cos(w0)) / 2<NewLine>    b1 = -1 - math.cos(w0)<NewLine>    b2 = b0<NewLine>    a0 = 1 + alpha<NewLine>    a1 = -2 * math.cos(w0)<NewLine></code></pre><NewLine><p>My setup: Ubuntu running from Docker, pytorch 1.3.0a0+24ae9b5, torchaudio 0.4.0a0+256458f</p><NewLine></div>",https://discuss.pytorch.org/u/ygabuev,(Yuriy),ygabuev,"November 18, 2019,  8:05pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Since you are compiling from source, please make sure you have the latest version (of pytorch and torchaudio) from master. This may have been fixed by <a href=""https://github.com/pytorch/audio/pull/326"" rel=""nofollow noopener"">https://github.com/pytorch/audio/pull/326</a>.</p><NewLine><p>Note to self: This should not be related to <a href=""https://github.com/pytorch/audio/pull/339"" rel=""nofollow noopener"">https://github.com/pytorch/audio/pull/339</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I thought I had the latest versions, but I didn’t. Thank you!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vincentqb; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ygabuev; <NewLine> ,"REPLY_DATE 1: November 18, 2019,  9:33pm; <NewLine> REPLY_DATE 2: November 18, 2019,  9:34pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
60986,Use packed padded sequence to RNN cell,2019-11-14T14:33:17.618Z,0,126,"<div class=""post"" itemprop=""articleBody""><NewLine><p>HI ! I’m biginner to pytorch!</p><NewLine><p>And I’m trying to use packed padded sequence to torch.nn.LSTMCell,</p><NewLine><p>but there is no tutorials about using packed padded sequences to torch.nn.LSTMCELL(not torch.nn.LSTM)</p><NewLine><p>is there any way to use packed padded sequences to torch.nn.LSTMCELL?</p><NewLine></div>",https://discuss.pytorch.org/u/zhdkffk533,,zhdkffk533,"November 14, 2019,  2:33pm",,,,,
38204,Problem installing torch audio on Windows 10 conda,2019-02-25T15:35:00.475Z,5,2249,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In Anaconda Python 3.6.7 with PyTorch installed, on Windows 10, I do this sequence:</p><NewLine><pre><code class=""lang-auto"">conda install -c conda-forge librosa<NewLine>conda install -c groakat sox<NewLine></code></pre><NewLine><p>then in a fresh download from <a href=""https://github.com/pytorch/audio"" rel=""nofollow noopener"">https://github.com/pytorch/audio</a> I do</p><NewLine><p><code>python setup.py install</code></p><NewLine><p>and it runs for a while and ends like this:</p><NewLine><pre><code class=""lang-auto"">torchaudio/torch_sox.cpp(3): fatal error C1083: Cannot open include file: 'sox.h': No such file or directory<NewLine>error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.15.26726\\bin\\HostX86\\x64\\cl.exe' failed with exit status 2<NewLine></code></pre><NewLine><p>I think I’m almost there and this is doable.  Please help!</p><NewLine><p>Note this is also an open GitHub issue of long standing: <a href=""https://github.com/pytorch/audio/issues/50"" rel=""nofollow noopener"">https://github.com/pytorch/audio/issues/50</a></p><NewLine></div>",https://discuss.pytorch.org/u/Lars_Ericson,(Lars Ericson),Lars_Ericson,"February 25, 2019,  3:53pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not a windows user. But I think the problem is with the environment setup. It is not able to locate the sox.h file, which is installed in your conda environment. You are facing this problem as you are currently not in the folder in which you installed conda. Try moving to the folder of your environment and run the command again.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Kushaj, I also cross-posted on StackOverflow and I got an answer over there which looks like a firm and definitive negative for Windows: <a href=""https://stackoverflow.com/questions/54872876/how-to-install-torch-audio-on-windows-10-conda"" rel=""nofollow noopener"">https://stackoverflow.com/questions/54872876/how-to-install-torch-audio-on-windows-10-conda</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you do a check for me. Open terminal and import numpy or any conda package. Tell me if it works or not.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>It works fine:</p><NewLine><pre><code class=""lang-auto"">&gt;python<NewLine>Python 3.6.7 (default, Feb 24 2019, 05:34:16) [MSC v.1900 64 bit (AMD64)] on win32<NewLine>Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.<NewLine>&gt;&gt;&gt; import numpy<NewLine></code></pre><NewLine><p>This is an open, logged issue for Windows.  I’m hoping a PyTorch guru will take pity on Windows folks and fix it.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I checked the problem in some detail, I think for now you will not be able to do so on Windows. But if you want a trick, you can run an Ubuntu VM with the libraries installed and try to communicate between Windows and the VM.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Other people have suggested WSL Linux on Windows but there are still problems accessing GPU with that: <a href=""http://www.erogol.com/using-windows-wsl-for-deep-learning-development/"" rel=""nofollow noopener"">http://www.erogol.com/using-windows-wsl-for-deep-learning-development/</a></p><NewLine><p>For my purposes (training speech recognition in Swahili), I am giving up on PyTorch in favor of this Tensorflow-based project done for Udacity which has no missing pieces on Windows: <a href=""https://github.com/simoninithomas/DNN-Speech-Recognizer"" rel=""nofollow noopener"">https://github.com/simoninithomas/DNN-Speech-Recognizer</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>For the record, I was trying to reproduce this OpenNMT-py speech training demo on Windows: <a href=""http://opennmt.net/OpenNMT-py/speech2text.html"" rel=""nofollow noopener"">http://opennmt.net/OpenNMT-py/speech2text.html</a></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I managed to compile torchaudio with sox in Windows 10, but is a bit tricky.</p><NewLine><p>Unfortunately the sox_effects are not usable, this error shows up:<br/><NewLine><code>RuntimeError: Error opening output memstream/temporary file</code></p><NewLine><p>But you can use the other torchaudio functionalities.</p><NewLine><p>The steps I followed for Windows 10 64bit are:</p><NewLine><p>#############################</p><NewLine><h1>TORCHAUDIO WINDOWS10 64bit</h1><NewLine><p>#############################</p><NewLine><p><strong>Note:</strong> I mix some command lines unix-like syntax, you can use file explorer or whatever</p><NewLine><h3>preliminar arrangements</h3><NewLine><ol><NewLine><li><NewLine><p>Download sox sources<br/><NewLine><code>$ git clone git://git.code.sf.net/p/sox/code sox</code></p><NewLine></li><NewLine><li><NewLine><p>Download other sox source to get lpc10</p><NewLine></li><NewLine></ol><NewLine><pre><code class=""lang-auto"">$ git clone https://github.com/chirlu/sox/tree/master/lpc10 sox2<NewLine>$ cp -R sox2/lpc10 sox<NewLine></code></pre><NewLine><ol start=""3""><NewLine><li>IMPORTANT get VisualStudio2019 and BuildTools installed</li><NewLine></ol><NewLine><h3>lpc10 lib</h3><NewLine><p>4.0. Create a VisualStudio CMake project for lpc10 and build it<br/><NewLine><em>Start window -&gt; open local folder -&gt; sox/lpc10</em><br/><NewLine><em>(it reads CMakeLists.txt automatically)</em><br/><NewLine><em>Build-&gt;build All</em></p><NewLine><p>4.2. Copy lpc10.lib to sox</p><NewLine><pre><code class=""lang-auto"">$ mkdir -p sox/src/out/build/x64-Debug<NewLine>$ cp sox/lpc10/out/build/x64-Debug/lpc10.lib sox/src/out/build/x64-Debug<NewLine></code></pre><NewLine><h3>gsm lib</h3><NewLine><p>5.0. Create a CMake project for libgsm and compile it as before with lpc10</p><NewLine><p>5.1. Copy gsm.lib to sox</p><NewLine><pre><code class=""lang-auto"">$ mkdir -p sox/src/out/build/x64-Debug<NewLine>$ cp sox/libgsm/out/build/x64-Debug/gsm.lib sox/src/out/build/x64-Debug<NewLine></code></pre><NewLine><h3>sox lib</h3><NewLine><p>6.0. Create a CMake project for sox in VS</p><NewLine><p>6.1. Edit some files:</p><NewLine><p>CMakeLists.txt: (add at the very beginning)</p><NewLine><p><code> project(sox)</code></p><NewLine><p>sox_i.h: (add under stdlib.h include line)</p><NewLine><pre><code class=""lang-auto"">#include &lt;wchar.h&gt; /* For off_t not found in stdio.h */<NewLine>#define UINT16_MAX  ((int16_t)-1)<NewLine>#define INT32_MAX  ((int32_t)-1)<NewLine></code></pre><NewLine><p>sox.c: (add under time.h include line)</p><NewLine><pre><code>`#include &lt;sys/timeb.h&gt;`<NewLine></code></pre><NewLine><p>6.2. Build sox with VisualStudio</p><NewLine><p>6.3. Copy the libraries where python will find them, I use a <strong>conda environment</strong>:</p><NewLine><pre><code class=""lang-auto"">$ cp sox/src/out/build/x64-Debug/libsox.lib envs\&lt;envname&gt;\libs\sox.lib<NewLine>$ cp sox/src/out/build/x64-Debug/gsm.lib envs\&lt;envname&gt;\libs<NewLine>$ cp sox/src/out/build/x64-Debug/lpc10.lib envs\&lt;envname&gt;\libs<NewLine></code></pre><NewLine><h3>torchaudio</h3><NewLine><p><code>$ activate &lt;envname&gt;</code></p><NewLine><p>7.0. Download torchaudio from github<br/><NewLine><code>$ git clone https://github.com/pytorch/audio thaudio</code></p><NewLine><p>7.1. Update setup.py, after the “else:” statement of “if IS_WHEEL…”<br/><NewLine><code>$ vi thaudio/setup.py</code></p><NewLine><p><span class=""hashtag"">#if</span> IS_WHEEL…</p><NewLine><pre><code class=""lang-auto"">else:<NewLine>    audio_path = os.path.dirname(os.path.abspath(__file__))<NewLine><NewLine>    # Add include path for sox.h, I tried both with the same outcome<NewLine>    include_dirs += [os.path.join(audio_path, '../sox/src')]<NewLine>    #include_dirs += [os.path.join(audio_path, 'torchaudio/sox')]<NewLine><NewLine>    # Add more libraries<NewLine><NewLine>    #libraries += ['sox']<NewLine>    libraries += ['sox','gsm','lpc10']<NewLine></code></pre><NewLine><p>7.2. Edit sox.cpp from torchaudio because dynamic arrays are not allowed:</p><NewLine><pre><code class=""lang-auto"">$ vi thaudio/torchaudio/torch_sox.cpp<NewLine><NewLine> //char* sox_args[max_num_eopts];<NewLine> char* sox_args[20]; //Value of MAX_EFFECT_OPTS<NewLine></code></pre><NewLine><p>7.3. Build and install</p><NewLine><pre><code class=""lang-auto"">$ cd thaudio<NewLine>$ python setup.py install<NewLine></code></pre><NewLine><p>It will print out tons of warnings about type conversion and some library conflict with MSVCRTD but “works”.</p><NewLine><p>And thats all.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Kushaj; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Lars_Ericson; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Kushaj; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Lars_Ericson; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Kushaj; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Lars_Ericson; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Lars_Ericson; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/maremoto; <NewLine> ,"REPLY_DATE 1: February 25, 2019,  8:31pm; <NewLine> REPLY_DATE 2: February 25, 2019,  8:51pm; <NewLine> REPLY_DATE 3: February 25, 2019,  8:55pm; <NewLine> REPLY_DATE 4: February 26, 2019,  2:02am; <NewLine> REPLY_DATE 5: February 26, 2019, 12:30pm; <NewLine> REPLY_DATE 6: February 26, 2019,  2:49pm; <NewLine> REPLY_DATE 7: February 26, 2019,  3:39pm; <NewLine> REPLY_DATE 8: October 31, 2019, 10:43am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> 
59498,"Channels in MFCC, and how to use them",2019-10-29T18:33:12.010Z,0,167,"<div class=""post"" itemprop=""articleBody""><NewLine><p>what are channels in mfcc, because when i use transform.mfcc i get the output as [2, n_mfcc, time], my question is what are the channels, and are the number of channels consistent in similar type of audio (basically in a dataset), how do use channel in models concatenate mfcc end-to-end?</p><NewLine></div>",https://discuss.pytorch.org/u/Rohan_Kumar,(Rohan Kumar),Rohan_Kumar,"October 29, 2019,  6:33pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The conventions we use for dimensions are given in the <a href=""https://github.com/pytorch/audio/blob/master/README.md"" rel=""nofollow noopener"">README</a>. In particular, a waveform is (channel, time), and <code>MFCC</code> : (channel, time) -&gt; (channel, mfcc, time), and so MFCC is applied per channel. Your original waveform must therefore have had 2 channels.</p><NewLine><p>In the datasets we provide, the number of channels are the same across all waveforms.</p><NewLine><p>Since the output of MFCC is just a tensor, you can use torch.cat to concatenate two MFCCs along a given axis.</p><NewLine><p>Is that what you were asking?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s pretty much what I asked, thank you. I still want to know what channels are actually. And if you use log2 with mfcc, which I think is used, how do you handle the ban values, currently I have replaced them with 0s.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vincentqb; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Rohan_Kumar; <NewLine> ,"REPLY_DATE 1: October 30, 2019,  8:04pm; <NewLine> REPLY_DATE 2: October 29, 2019,  7:52pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
57208,Siamese Network for gender recognition,2019-09-30T23:26:05.661Z,0,257,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone, recently I started to read about Siamese Nets and I wanted to try this type of model on a gender recognition task. My data is a .csv dataset containing ~3000 n-vectors of audio features (n=20). The loss function I’m using is the <a href=""http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf"" rel=""nofollow noopener"">Contrastive Loss</a>. Here is my model:</p><NewLine><pre><code class=""lang-auto"">class ContrastiveLoss(torch.nn.Module):<NewLine>  """"""<NewLine>  Contrastive loss function.<NewLine>  Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf<NewLine>  """"""<NewLine>  <NewLine>  def __init__(self, margin=1.0):<NewLine>      super(ContrastiveLoss, self).__init__()<NewLine>      self.margin = margin<NewLine><NewLine>  def forward(self, output1, output2, label):<NewLine>      euclidean_distance = F.pairwise_distance(output1, output2)<NewLine>      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance.double(), 2) +<NewLine>                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance.double(), min=0.0), 2))<NewLine><NewLine><NewLine>      return loss_contrastive<NewLine><NewLine>class SiameseMLP(nn.Module):<NewLine>  def __init__(self):<NewLine>    super(SiameseMLP, self).__init__()<NewLine>    self.layers = nn.Sequential(<NewLine>        nn.Linear(20, 256),<NewLine>        nn.ReLU(),<NewLine>        nn.Dropout(p=0.1),<NewLine>        nn.Linear(256, 256),<NewLine>        nn.ReLU(),<NewLine>        nn.Dropout(p=0.1),<NewLine>        nn.Linear(256, 256),<NewLine>        nn.ReLU(),<NewLine>        nn.Dropout(p=0.1),<NewLine>        nn.Linear(256, 2)<NewLine>    )<NewLine><NewLine>  def forward_once(self, x):<NewLine>    x = x.view(-1, 20)<NewLine>    x = self.layers(x)<NewLine>    return x<NewLine><NewLine>  def forward(self, x_1, x_2):<NewLine>    y_1 = self.forward_once(x_1)<NewLine>    y_2 = self.forward_once(x_2)<NewLine>    return y_1, y_2<NewLine></code></pre><NewLine><p>I started to do some tests and for now I can’t reach a good value for the training loss, keeping it around ~0.33. When I calculate the dissimilarity between pairs, I get random values so I think the model is not learning completely. Do you have any suggest? Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/dengio,,dengio,"September 30, 2019, 11:26pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am not sure of the exact reason.</p><NewLine><p>Below are some points that I could think of:</p><NewLine><ol><NewLine><li>You do not need tow square the euclidean distance, as pairwise distance already gives that.</li><NewLine></ol><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""57208""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/d/f05b48/40.png"" width=""20""/> dengio:</div><NewLine><blockquote><NewLine><p>torch.pow(euclidean_distance.double(), 2)</p><NewLine></blockquote><NewLine></aside><NewLine><ol start=""2""><NewLine><li><NewLine><p>Check if your dataset is balanced/imbalanced.</p><NewLine></li><NewLine><li><NewLine><p>I see that you have used only 2-dimensions in the final layer. Try using more (64, 128, 256 etc.,)<br/><NewLine>I am not sure if reducing 20 dimensions to 2 dimensions is not generalizing well.</p><NewLine></li><NewLine></ol><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""57208""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/d/f05b48/40.png"" width=""20""/> dengio:</div><NewLine><blockquote><NewLine><p>nn.Linear(256, 2)</p><NewLine></blockquote><NewLine></aside><NewLine><ol start=""4""><NewLine><li><NewLine><p>Try normalizing the final descriptor (<code>F.normalize()</code>)</p><NewLine></li><NewLine><li><NewLine><p>Play-around with the margin parameter of siamese (contrastive) loss.<br/><NewLine>Also, try triplet loss.</p><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/InnovArul; <NewLine> ,"REPLY_DATE 1: October 1, 2019,  1:14am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
56929,How to compute the MelSpectrogram on batches of data?,2019-09-26T20:35:14.082Z,0,173,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey there,</p><NewLine><p>I can’t figure out how to run the MelSpectrogram (from torchaudio) on batches of data.</p><NewLine><p>I checked the torchaudio docs:</p><NewLine><p><a class=""onebox"" href=""https://pytorch.org/audio/_modules/torchaudio/transforms.html#MelSpectrogram"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/audio/_modules/torchaudio/transforms.html#MelSpectrogram</a></p><NewLine><p>and the waveform needs to be in this format: (channel, time)</p><NewLine><p>which makes sense for <em>one</em> wave, but what if we have batches of data?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/vgoklani,(Vishal Goklani),vgoklani,"September 26, 2019,  9:47pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This transformation might only work on a single sample (like <code>torchvision.transforms</code>), as it’s usual use case would probably be to apply it in the <code>__getitem__</code> method on each data sample.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 30, 2019,  8:09am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
17430,How to iterate over VCTK dataset elements?,2018-05-03T09:44:22.314Z,1,647,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!<br/><NewLine>I am trying to go through VCTK dataset in this way:</p><NewLine><pre><code class=""lang-auto"">train_set = datasets.VCTK(root = 'processed/training.pt', download = True, transform = transforms.PadTrim(max_len=30000))<NewLine>training_data_loader = DataLoader(dataset = train_set,<NewLine>            num_workers=opt.nThreads, batch_size=opt.batchSize,shuffle=True)<NewLine><NewLine>for batch_idx, batch in enumerate(training_data_loader):<NewLine>    print(batch_idx)<NewLine>    ........<NewLine><NewLine></code></pre><NewLine><p>However, it prints only 0 and shows the following error:</p><NewLine><pre><code class=""lang-auto"">0<NewLine>Traceback (most recent call last):<NewLine>  File ""main_audio.py"", line 153, in &lt;module&gt;<NewLine>    for batch_idx, batch in enumerate(training_data_loader):<NewLine>  File ""/mnt/home/20140941/.conda/envs/opt_anaconda/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 281, in __next__<NewLine>    return self._process_next_batch(batch)<NewLine>  File ""/mnt/home/20140941/.conda/envs/opt_anaconda/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 301, in _process_next_batch<NewLine>    raise batch.exc_type(batch.exc_msg)<NewLine>IndexError: Traceback (most recent call last):<NewLine>  File ""/mnt/home/20140941/.conda/envs/opt_anaconda/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 55, in _worker_loop<NewLine>    samples = collate_fn([dataset[i] for i in batch_indices])<NewLine>  File ""build/bdist.linux-x86_64/egg/torchaudio/datasets/vctk.py"", line 126, in __getitem__<NewLine>    audio, target = self.data[index], self.labels[index]<NewLine>IndexError: tuple index out of range<NewLine><NewLine></code></pre><NewLine><p>How can I solve this problem?</p><NewLine></div>",https://discuss.pytorch.org/u/YK11,(Yeskendir ),YK11,"May 4, 2018,  8:28am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I assume you are using the torchaudio library.  The root should be a folder not a file.  Otherwise, you’ll have to make a custom <code>torch.utils.data.Dataset</code>.  Also, did you check to see that it actually downloads the files?  The VCTK dataset is very large and takes a long time to download.  You should probably just run the first line in a REPL and see if the dataset gets downloaded correctly.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply! I fixed root and all data was downloaded. However, there is still problem.</p><NewLine><pre><code class=""lang-auto"">train_set = datasets.VCTK(root = '.', download = True, transform = transforms.PadTrim(max_len=30000))<NewLine>training_data_loader = DataLoader(dataset = train_set,<NewLine>            num_workers=opt.nThreads, batch_size=2,shuffle=True)<NewLine><NewLine>for batch_idx, batch in enumerate(training_data_loader, 0):<NewLine>    print(batch_idx)<NewLine></code></pre><NewLine><p>The length of training set is 44257. When I run code it prints integers from 1 to 20 (supposed to print to 22129). And shows similar errror.</p><NewLine><pre><code class=""lang-auto"">    for batch_idx, batch in enumerate(training_data_loader, 0):<NewLine>  File ""/mnt/home/20140941/.conda/envs/opt_anaconda/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 281, in __next__<NewLine>    return self._process_next_batch(batch)<NewLine>  File ""/mnt/home/20140941/.conda/envs/opt_anaconda/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 301, in _process_next_batch<NewLine>    raise batch.exc_type(batch.exc_msg)<NewLine>IndexError: Traceback (most recent call last):<NewLine>  File ""/mnt/home/20140941/.conda/envs/opt_anaconda/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 55, in _worker_loop<NewLine>    samples = collate_fn([dataset[i] for i in batch_indices])<NewLine>  File ""build/bdist.linux-x86_64/egg/torchaudio/datasets/vctk.py"", line 126, in __getitem__<NewLine>    audio, target = self.data[index], self.labels[index]<NewLine>IndexError: tuple index out of range<NewLine><NewLine></code></pre><NewLine><p>Please, help!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you ever get an answer for this? I’m running into the exact same issue now…</p><NewLine><p>It seems to work as expected when I just iterate through the dataset directly though (rather than using the loader).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dhpollack; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/YK11; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tks; <NewLine> ,"REPLY_DATE 1: May 4, 2018,  8:47am; <NewLine> REPLY_DATE 2: May 6, 2018,  6:18am; <NewLine> REPLY_DATE 3: August 13, 2019,  5:08pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
53173,LPC analysis for speech signals,2019-08-12T13:39:26.971Z,0,193,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>Is there any plan to provide torchaudio with a new feature of calculating the LPC analysis parameters for speech signals? Or at least converting MFCC to LPC?</p><NewLine></div>",https://discuss.pytorch.org/u/Ahmed_m,(Ahmed Mamoud),Ahmed_m,"August 12, 2019,  1:40pm",,,,,
48582,Error occurred when I tried to import torchaudio,2019-06-21T05:11:34.042Z,4,2318,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I installed torchaudio according to the instruction by <a href=""https://github.com/pytorch/audio"" rel=""nofollow noopener"">https://github.com/pytorch/audio</a>. Nevertheless, when I tried to import torchaudio, the following error message popped up:</p><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “test.py”, line 1, in <br/><NewLine>import torchaudio<br/><NewLine>File “/Users/q7/Desktop/Wavenet/audio-master/torchaudio/<strong>init</strong>.py”, line 7, in <br/><NewLine>from torchaudio import transforms, datasets, kaldi_io, sox_effects, legacy, compliance<br/><NewLine>File “/Users/q7/Desktop/Wavenet/audio-master/torchaudio/transforms.py”, line 6, in <br/><NewLine>from . import functional as F<br/><NewLine>File “/Users/q7/Desktop/Wavenet/audio-master/torchaudio/functional.py”, line 108, in <br/><NewLine><span class=""mention"">@torch.jit.script</span><br/><NewLine>File “/anaconda3/lib/python3.7/site-packages/torch/jit/<strong>init</strong>.py”, line 824, in script<br/><NewLine>fn = torch._C._jit_script_compile(ast, _rcb, get_default_args(obj))<br/><NewLine>File “/anaconda3/lib/python3.7/site-packages/torch/jit/annotations.py”, line 55, in get_signature<br/><NewLine>return parse_type_line(type_line)<br/><NewLine>File “/anaconda3/lib/python3.7/site-packages/torch/jit/annotations.py”, line 97, in parse_type_line<br/><NewLine>raise RuntimeError(“Failed to parse the argument list of a type annotation: {}”.format(str(e)))<br/><NewLine>RuntimeError: Failed to parse the argument list of a type annotation: name ‘Optional’ is not defined</p><NewLine><p>Can someone propose a solution? Thanks a lot.</p><NewLine></div>",https://discuss.pytorch.org/u/Kai_Qu1,(Kai Qu),Kai_Qu1,"June 21, 2019,  5:11am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am having the same problem. Using torch 1.1.0.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>There was bad commit in torchaudio. Solution: old version with pytorch 1.0.<br/><NewLine>pip3 install git+https://github.com/pytorch/audio@d92de5b</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""48582""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/maksim_pershin/40/13364_2.png"" width=""20""/> Maksim_Pershin:</div><NewLine><blockquote><NewLine><p>pip3 install git+https://github.com/pytorch/audio@d92de5b</p><NewLine></blockquote><NewLine></aside><NewLine><p>Any other alternative?<br/><NewLine>This fails on current version<br/><NewLine><code>transform = torchaudio.transforms.DownmixMono(channels_first=True)</code><br/><NewLine><code>__init__() got an unexpected keyword argument 'channels_first'</code></p><NewLine><p>The  <code>__init__</code>  method just contains a  <code>pass</code>  statement.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I also have the same issue.</p><NewLine><p>edit:<br/><NewLine>Used Maksim_Pershin’s old version successfully for now.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you try this?</p><NewLine><pre><code class=""lang-auto"">import sys<NewLine>sys.version<NewLine>'3.6.8 (default, Jan 14 2019, 11:02:34) \n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]'<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">!cat /usr/local/cuda/version.txt<NewLine>CUDA Version 10.0.130<NewLine></code></pre><NewLine><pre><code class=""lang-auto""># Install dependencies<NewLine>!apt-get install sox libsox-dev libsox-fmt-all<NewLine></code></pre><NewLine><p>Go to <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noopener"">the following link and check for links corresponding to your python and CUDA versions</a><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" href=""https://user-images.githubusercontent.com/25292456/61643300-2d95c880-acc0-11e9-9c17-136ef854a1e4.png"" rel=""nofollow noopener"" title=""61643300-2d95c880-acc0-11e9-9c17-136ef854a1e4.png""><img alt=""image"" height=""252"" src=""https://user-images.githubusercontent.com/25292456/61643300-2d95c880-acc0-11e9-9c17-136ef854a1e4.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">61643300-2d95c880-acc0-11e9-9c17-136ef854a1e4.png</span><span class=""informations"">1135×416</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><pre><code class=""lang-auto"">!pip3 install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl<NewLine>!pip3 install https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl<NewLine></code></pre><NewLine><pre><code class=""lang-auto""># install torchaudio<NewLine>!pip install git+https://github.com/pytorch/audio.git@v0.2.0<NewLine></code></pre><NewLine><pre><code class=""lang-auto""># Run torchaudio<NewLine>import torchaudio<NewLine>print(torchaudio.__version__)  # prints 0.2.0a0+7d7342f<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/rfalcon100; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Maksim_Pershin; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/100deep1001; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/johan010; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/100deep1001; <NewLine> ,"REPLY_DATE 1: June 22, 2019, 12:27pm; <NewLine> REPLY_DATE 2: June 22, 2019,  3:40pm; <NewLine> REPLY_DATE 3: July 19, 2019, 12:27pm; <NewLine> REPLY_DATE 4: July 24, 2019, 10:08am; <NewLine> REPLY_DATE 5: July 29, 2019, 12:10pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
48367,Torchaudio import error,2019-06-19T07:44:00.072Z,0,832,"<div class=""post"" itemprop=""articleBody""><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/5210368c0ed0f57c4f7c105960a0fdb12e0b8a0f"" href=""https://discuss.pytorch.org/uploads/default/original/2X/5/5210368c0ed0f57c4f7c105960a0fdb12e0b8a0f.png"" title=""Screen Shot 2019-06-19 at 3.39.31 PM.png""><img alt=""31%20PM"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/5210368c0ed0f57c4f7c105960a0fdb12e0b8a0f_2_10x10.png"" height=""284"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/5210368c0ed0f57c4f7c105960a0fdb12e0b8a0f_2_689x284.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/5210368c0ed0f57c4f7c105960a0fdb12e0b8a0f_2_689x284.png, https://discuss.pytorch.org/uploads/default/optimized/2X/5/5210368c0ed0f57c4f7c105960a0fdb12e0b8a0f_2_1033x426.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/5/5210368c0ed0f57c4f7c105960a0fdb12e0b8a0f_2_1378x568.png 2x"" width=""689""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screen Shot 2019-06-19 at 3.39.31 PM.png</span><span class=""informations"">2296×946 260 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>Hi, I tried to install torchaudio by using the command “conda install -c derickl torchaudio” from <a href=""https://anaconda.org/derickl/torchaudio"" rel=""nofollow noopener"">https://anaconda.org/derickl/torchaudio</a>, but when I tried to import torchaudio, the error message in the screenshot popped up. Can someone help to solve this? Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/Kai_Qu,(Kai Qu),Kai_Qu,"June 19, 2019,  7:44am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I ran into an issue like that before. I think upgrading the pytorch version helped but can’t remember exactly</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The error message indicates a mismatch between the PyTorch version expected by torchaudio and the one installed.</p><NewLine><p>If possible, following the instructions on the web page would likely work best: <a href=""https://github.com/pytorch/audio/#dependencies"" rel=""nofollow noopener"">https://github.com/pytorch/audio/#dependencies</a></p><NewLine><p>Generally, compiling the auxiliary libraries (torchvision, torchaudio) is much easier and faster than it is PyTorch itself, but I don’t know about mac specifics.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>it looks like the package is unofficial one and they did not declare minimum and maximum pytorch version properly.</p><NewLine><p><a class=""mention"" href=""/u/jamarshon"">@jamarshon</a> is soon uploading official torchaudio packages to conda channel</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have the same problem. Has this been resolved?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jamarshon; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jmsosullivan; <NewLine> ,"REPLY_DATE 1: June 19, 2019, 12:49pm; <NewLine> REPLY_DATE 2: June 19, 2019,  1:10pm; <NewLine> REPLY_DATE 3: June 19, 2019,  1:55pm; <NewLine> REPLY_DATE 4: July 28, 2019,  6:30pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
51189,nn.LSTMCell inside nn.LSTM?,2019-07-21T08:38:36.067Z,1,408,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Does this <code>rnn = nn.LSTM(input_size, hidden_size)</code> mean, it has  <code>hidden_size</code> number of <code>nn.LSTMCell()</code> cells inside?</p><NewLine></div>",https://discuss.pytorch.org/u/gt_tugsuu,(GT),gt_tugsuu,"July 21, 2019,  8:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, hidden_size is the hidden dimension of the LSTM cell.</p><NewLine><p>LSTM is functionally equivalent to writing a loop over your input and feeding it to LSTMCell (modulo the API differences).  LSTM exists because the cuDNN library provides a more efficient implementation when you know the full input at the start of the computation.  Also, LSTM is more efficient when you are using more than one layer of LSTM cells.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jlquinn"">@jlquinn</a><br/><NewLine>Thanks for replying.<br/><NewLine>So does it mean it’s just one LSTMCell? (in this case)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jlquinn; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/gt_tugsuu; <NewLine> ,"REPLY_DATE 1: July 22, 2019,  6:23pm; <NewLine> REPLY_DATE 2: July 22, 2019,  7:08pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
41135,How does torchaudio.transforms.DownmixMono work?,2019-03-28T16:09:01.577Z,9,898,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am getting confused when I use torchaudio.transforms.DownmixMono.<br/><NewLine>first, I load my data with sound = torchaudio.load(). This is correct that sound[0] is two channel data with torch.Size is ([2, 132300]) and sound[1] = 22050, which is the sample rate.Then I use soundData = torchaudio.transforms.DownmixMono(sound[0]) to downsample. But the result looks weird with torch.Size([2, 1]). If I understand it correctly, I can get soundData, which has only one channel? What’s wrong with that?</p><NewLine><p>I check the document as well,  the input format should be: tensor (Tensor): Tensor of audio of size (c x n) or (n x c), what does (c x n) mean?</p><NewLine></div>",https://discuss.pytorch.org/u/wzehui,(Wang Zehui),wzehui,"March 28, 2019,  4:09pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It looks like you are passing your data as <code>[channels, length]</code>, so you should pass <code>channels_first=True</code>. While the docs says:</p><NewLine><blockquote><NewLine><p>channels_first (bool): Downmix across channels dimension.  Default: <code>True</code></p><NewLine></blockquote><NewLine><p>The default seems to be in fact <code>None</code>, which results in dim1 as the default channel dimension:</p><NewLine><pre><code class=""lang-python"">channels_first = None<NewLine>ch_dim = int(not channels_first)<NewLine>print(ch_dim)<NewLine>&gt; 1<NewLine></code></pre><NewLine><p><code>c x n</code> should correspond to <code>[channels, length]</code>.</p><NewLine><p>Thanks for reporting this problem! I’ve created an issue <a href=""https://github.com/pytorch/audio/issues/93"" rel=""nofollow noopener"">here</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply! This is exactly where the problem is!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>,<br/><NewLine>How should I pass the <em>channels_first</em>  keyword to the DownmixMono  function?</p><NewLine><p>I get the following error.<br/><NewLine><code>torchaudio.transforms.DownmixMono()(sound[0],channels_first = True)</code><br/><NewLine><code>__init__() got an unexpected keyword argument 'channels_first'</code></p><NewLine><p>However this works but I get wrong dimensions as mentioned by OP:<br/><NewLine><code>torchaudio.transforms.DownmixMono()(sound[0])</code></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>You should pass it while initializing the transformation:</p><NewLine><pre><code class=""lang-python"">transform = torchaudio.transforms.DownmixMono(channels_first=True)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> Thanks for the reply;<br/><NewLine>I tried that too… but can’t get it work<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/7751c33e5bb0ca6cf18c6786a6dde2bd95d1e037"" href=""https://discuss.pytorch.org/uploads/default/original/2X/7/7751c33e5bb0ca6cf18c6786a6dde2bd95d1e037.png"" title=""image.png""><img alt=""image"" data-base62-sha1=""h1xZFC2HGvoCD27770bTwHV62b5"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/7/7751c33e5bb0ca6cf18c6786a6dde2bd95d1e037_2_10x10.png"" height=""190"" src=""https://discuss.pytorch.org/uploads/default/original/2X/7/7751c33e5bb0ca6cf18c6786a6dde2bd95d1e037.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">795×220 8.61 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p><code>transform = torchaudio.transforms.DownmixMono(channels_first=True) __init__() got an unexpected keyword argument 'channels_first'</code></p><NewLine><p>Just to add up; I downloaded torchaudio as follows:</p><NewLine><pre><code class=""lang-auto"">!git clone https://github.com/pytorch/audio.git<NewLine>os.chdir(""audio"")<NewLine>!git checkout 301e2e9<NewLine>!python setup.py install<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>This argument was introduced after your specified commit hash.<br/><NewLine>If you look at the file, you’ll see that the <code>__init__</code> method just contains a <code>pass</code> statement.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sure… got it! Thanks for the help, I will look for more recent hashes.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>  Tried all possible hashes but didn’t succeed.</p><NewLine><p>Could you guide to a stable version/hash? I am on Google Colab.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Isn’t the current master working (<code>5c9d33d</code>)?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>No,</p><NewLine><p>Here’s  what I tried:</p><NewLine><pre><code class=""lang-auto"">!apt-get install sox libsox-dev libsox-fmt-all<NewLine>!git clone https://github.com/pytorch/audio.git<NewLine>import os<NewLine>os.chdir(""audio"")<NewLine>!git checkout 5c9d33d  #301e2e9 d92de5b<NewLine>!python setup.py install<NewLine>import torchaudio<NewLine></code></pre><NewLine><p>Gives me following error:<br/><NewLine><code>RuntimeError: Failed to parse the argument list of a type annotation: name 'Optional' is not defined</code></p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just installed the current master without a problem and it seems your error might be related to <a href=""https://github.com/pytorch/audio/issues/121"" rel=""nofollow noopener"">this issue</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/wzehui; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/100deep1001; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/100deep1001; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/100deep1001; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/100deep1001; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/100deep1001; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: March 28, 2019,  8:52pm; <NewLine> REPLY_DATE 2: March 29, 2019, 10:01am; <NewLine> REPLY_DATE 3: July 19, 2019, 10:08am; <NewLine> REPLY_DATE 4: July 19, 2019, 10:31am; <NewLine> REPLY_DATE 5: July 19, 2019, 10:50am; <NewLine> REPLY_DATE 6: July 19, 2019, 10:58am; <NewLine> REPLY_DATE 7: July 19, 2019, 11:14am; <NewLine> REPLY_DATE 8: July 19, 2019, 12:19pm; <NewLine> REPLY_DATE 9: July 19, 2019, 12:46pm; <NewLine> REPLY_DATE 10: July 19, 2019,  2:01pm; <NewLine> REPLY_DATE 11: July 19, 2019,  2:07pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: 1 Like; <NewLine> 
48921,LSTM CTC model not learning in PyTorch,2019-06-25T14:04:57.811Z,4,867,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! I was working on an ASR model in tensorflow keras, but now I want to swich to pytorch. I’m trying to reimplement keras model in pytorch, but I think, I did a mistake, because the same model on  the same data does not learn in pytorch.</p><NewLine><p>Here is a full jupyter notebook of my problem: <a href=""https://github.com/dnnagy/asr_tf_pytorch/blob/master/asr_tf_pytorch.ipynb"" rel=""nofollow noopener"">notebook on github</a></p><NewLine><p>As You can see, the TF model overfits the random data, as expected, but the pytorch model does not learn anything.</p><NewLine><p>I’m using pytorch 1.1.0 with CUDA, and Tensorflow 2.0.0-beta1</p><NewLine></div>",https://discuss.pytorch.org/u/dnnagy,(Nagy Dániel),dnnagy,"June 25, 2019,  8:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>initialization of weights will matter here.</p><NewLine><p>the <code>nn.Linear</code> layers that you created, initialize their weights to something other than the default initialization and see if it makes a difference. You can use <a href=""https://pytorch.org/docs/stable/nn.html#torch-nn-init"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/nn.html#torch-nn-init</a> for convenience to try different initializations.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could You post a simple example on how to properly initialize weights?<br/><NewLine>Do I have to re-initialize LSTM weights after each epoch or sample?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok, I initialized all my Linear weights based on <a href=""https://discuss.pytorch.org/t/weight-initilzation/157/15"">this comment</a>, but pytorch is still not learning:</p><NewLine><pre><code class=""lang-auto"">def weight_init(m): <NewLine>  if isinstance(m, nn.Linear):<NewLine>    size = m.weight.size()<NewLine>    fan_out = size[0] # number of rows<NewLine>    fan_in = size[1] # number of columns<NewLine>    variance = np.sqrt(2.0/(fan_in + fan_out))<NewLine>    m.weight.data.normal_(0.0, variance)<NewLine>    <NewLine>baseline_model = FCBaseline(SEGMENT_WIDTH, SEGMENT_HEIGHT, SEGMENT_CHANNELS, num_classes)<NewLine>baseline_model.apply(weight_init)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could be that I missed it but it seems like a possible reason is that you forgot to zero the gradients before/after running a batch. You only seem to do it at the start. Try adding the following INSIDE your training loop:</p><NewLine><pre><code class=""lang-auto"">optimizer.zero_grad()<NewLine></code></pre><NewLine><p>Does this solve your issue?</p><NewLine><p>See <a href=""https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"" rel=""nofollow noopener"">here</a> for an example or <a href=""https://discuss.pytorch.org/t/why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch/4903"">here for the reason why this is needed.</a></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank You, this solved my issue. I forgot to zero out gradients after each minbach. Now works fine.</p><NewLine><pre><code class=""lang-auto""># Optimizer needs the gradients of this minibatch only, so zero out prev grads.<NewLine>optimizer.zero_grad()<NewLine>loss.backward() # Calculates derivatives with autograd<NewLine>optimizer.step() # Update weights<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dnnagy; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/dnnagy; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/a3VonG; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/dnnagy; <NewLine> ,"REPLY_DATE 1: June 25, 2019,  2:13pm; <NewLine> REPLY_DATE 2: June 25, 2019,  2:15pm; <NewLine> REPLY_DATE 3: June 25, 2019,  2:30pm; <NewLine> REPLY_DATE 4: June 27, 2019,  4:48pm; <NewLine> REPLY_DATE 5: June 27, 2019,  4:48pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
49074,Parallel Implementation of Gaussian mixture models for working with multiple GPU&rsquo;s,2019-06-27T02:57:12.073Z,0,382,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!<br/><NewLine>I’m currently working on language recognition and I wanted to implement a Gaussian mixture model which can run parallelly on multiple GPUs(I have 2 GPU’s of 11 Gb each) . As of now, I have implemented a naive GMM comprising normal def functions which does everything on CPU without any parallelization and optimization(Like I don’t need to store the grads of tensors as well) .I wanted to get an idea or some suggestions about how I should go ahead with <strong>Parallelization implementation of GMM!</strong><br/><NewLine>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/Sedherthe,(Soma Siddhartha),Sedherthe,"June 27, 2019,  2:57am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Would you like to apply some data parallelism (cloning the model on each GPU and splitting the data) or would you like to shard the model somehow (different GPUs compute different parts of your model)?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I was thinking of the first way similar to nn.Dataparallel approach! This seems possible to me…<br/><NewLine>I’m not sure how I can go about if I want to implement it in the second way though.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Sedherthe; <NewLine> ,"REPLY_DATE 1: June 27, 2019, 10:17am; <NewLine> REPLY_DATE 2: June 27, 2019, 11:00am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
48482,360 hr Librispeech training for neural network,2019-06-20T05:02:05.459Z,0,254,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I want to train librispeech which contains variable length of samples.<br/><NewLine>How to use Dataloader for this ( should I Cut it into framewise and then train) or is their any methods for loading different sample size.<br/><NewLine>If I follow to cut it into frames wise,  its a huge number files which it able to handle properly by the GPU.<br/><NewLine>Is their any method to write a customized way to train the neural network.</p><NewLine></div>",https://discuss.pytorch.org/u/pytorch_user12,(pytorch_user12),pytorch_user12,"June 20, 2019,  5:02am",,,,,
48270,A network has to trained with huge number ofdataset,2019-06-18T13:36:49.621Z,0,151,"<div class=""post"" itemprop=""articleBody""><NewLine><p>What is the best way to train a network with large database contain billions of samples ??<br/><NewLine>When the same network is trained with a small dataset in GPU, its taking less time.<br/><NewLine>But when the datsamples are increased its taking too much time irrespective of same number of batch size, no_worker and same dataloader also.<br/><NewLine>I am not able to figure out, why the total batch size execution time is not matching with epoch time.<br/><NewLine>I am just following this link <a href=""https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html</a>.<br/><NewLine>MY network is simple DNN.</p><NewLine></div>",https://discuss.pytorch.org/u/pytorch_user12,(pytorch_user12),pytorch_user12,"June 18, 2019,  1:36pm",,,,,
47945,The time complexity of fft function,2019-06-14T12:18:15.333Z,2,369,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am curious about the fft built-in function.</p><NewLine><p>Is this function calculate the DFT coefficient directly?</p><NewLine><p>What is the time complexity of fft function if we do not use GPU?</p><NewLine><p>Is this function use divide-and-conquer algorithm for calculating fft?</p><NewLine><p>Thanks,</p><NewLine></div>",https://discuss.pytorch.org/u/658130434af411741ceb,(Seonghyeon Lee),658130434af411741ceb,"June 14, 2019, 12:18pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi 성현!</p><NewLine><aside class=""quote no-group quote-modified"" data-full=""true"" data-post=""1"" data-topic=""47945""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/658130434af411741ceb/40/13100_2.png"" width=""20""/> 658130434af411741ceb:</div><NewLine><blockquote><NewLine><p>…<br/><NewLine>What is the time complexity of fft function if we do not use GPU?</p><NewLine><p>Is this function use divide-and-conquer algorithm for calculating fft?</p><NewLine></blockquote><NewLine></aside><NewLine><p>I haven’t actually looked at the code, but the time complexity<br/><NewLine>should be <code>n log n</code>.</p><NewLine><p>After all, the function in question is <code>torch.fft</code>, where “fft”<br/><NewLine>stands for “fast Fourier transform,” which uses what you call<br/><NewLine>the “divide-and-conquer” algorithm and runs in <code>n log n</code>.</p><NewLine><p>It would be false advertising if <code>torch.fft</code> instead used the<br/><NewLine>sft* algorithm.</p><NewLine><p>*) “Slow Fourier transform”</p><NewLine><p>Best regards.</p><NewLine><p>K. Frank</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>So much thanks for reply <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>I have one more question.</p><NewLine><p>Can we get the gradient of fft function with respect to input?</p><NewLine><p>I think it is possible, but I want to make sure things.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello 성현!</p><NewLine><aside class=""quote no-group quote-modified"" data-full=""true"" data-post=""3"" data-topic=""47945""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/658130434af411741ceb/40/13100_2.png"" width=""20""/> 658130434af411741ceb:</div><NewLine><blockquote><NewLine><p>…<br/><NewLine>Can we get the gradient of fft function with respect to input?</p><NewLine><p>I think it is possible, but I want to make sure things.</p><NewLine></blockquote><NewLine></aside><NewLine><p>It looks like the gradient is supported.  Try:</p><NewLine><pre><code class=""lang-nohighlight"">&gt;&gt;&gt; import torch<NewLine>&gt;&gt;&gt; t = torch.randn (1, 8, 2)<NewLine>&gt;&gt;&gt; t.requires_grad = True<NewLine>&gt;&gt;&gt; torch.fft (t, 1)<NewLine>tensor([[[-2.7232,  3.8741],<NewLine>         [-2.9743, -2.1404],<NewLine>         [ 1.1234,  4.4275],<NewLine>         [ 1.7661,  1.6113],<NewLine>         [-3.6401,  3.6872],<NewLine>         [ 0.0582, -3.4854],<NewLine>         [-1.6034,  0.3976],<NewLine>         [ 0.3413,  0.8839]]], grad_fn=&lt;FftWithSizeBackward&gt;)<NewLine></code></pre><NewLine><p>(I haven’t actually tried this, but I would imagine that<br/><NewLine><code>FftWithSizeBackward</code> works correctly.)</p><NewLine><p>Best.</p><NewLine><p>K. Frank</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, backward of fft is implemented (and for rfft, ifft, irfft as well).</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>About the speed, on CUDA, the cufft package is used, on CPU, MKL is used. They are both highly-optimized libraries, so it should be very fast.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/KFrank; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/658130434af411741ceb; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/KFrank; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/SimonW; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/SimonW; <NewLine> ,"REPLY_DATE 1: June 14, 2019,  3:25pm; <NewLine> REPLY_DATE 2: June 17, 2019,  4:14am; <NewLine> REPLY_DATE 3: July 9, 2019,  3:14am; <NewLine> REPLY_DATE 4: June 17, 2019,  8:38pm; <NewLine> REPLY_DATE 5: June 17, 2019,  8:39pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
47120,PyTorch bindings for Warp-ctc: c10/cuda/CUDAGuard.h: No such file or directory,2019-06-05T10:28:43.504Z,0,151,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to install <a href=""https://github.com/SeanNaren/warp-ctc"" rel=""nofollow noopener"">warp-CTC</a> for <a href=""https://github.com/SeanNaren/deepspeech.pytorch"" rel=""nofollow noopener"">deepSpeech2</a>.</p><NewLine><p>I’m using python 3.7.2,  pytorch 1.1.0, cuda 9.0.</p><NewLine><p>I get the following error:</p><NewLine><blockquote><NewLine><p>src/binding.cpp:10:33: fatal error: c10/cuda/CUDAGuard.h: No such file or directory</p><NewLine></blockquote><NewLine><p>I’d appreciate any direction for solving this, thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Tzeviya,,Tzeviya,"June 5, 2019, 10:28am",,,,,
46060,Why is reflection and reflect pad only implemented for certain dimensions?,2019-05-23T23:42:24.826Z,0,651,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Quoting from the documentation:</p><NewLine><blockquote><NewLine><p>Replicate padding is implemented for padding the last 3 dimensions of 5D input tensor, or the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor. Reflect padding is only implemented for padding the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor.</p><NewLine></blockquote><NewLine><p>Is this by choice?</p><NewLine></div>",https://discuss.pytorch.org/u/Rakshit_Kothari,(Rakshit Kothari),Rakshit_Kothari,"May 23, 2019, 11:42pm",2 Likes,,,,
45997,New audio package for PyTorch,2019-05-23T09:00:42.825Z,0,266,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>We released our <em>audtorch</em> package yesterday and would like to invite everyone to take a look and use it.</p><NewLine><p>You can find its repository here: <a href=""https://github.com/audeering/audtorch"" rel=""nofollow noopener"">https://github.com/audeering/audtorch</a>. We welcome all new issues and contributions <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>We will also synchronize with the rest of the audio community that’s working on <a href=""https://github.com/keunwoochoi/torchaudio-contrib"" rel=""nofollow noopener"">torchaudio-contrib</a> and try to promote usage of PyTorch in the audio community in general.</p><NewLine><p>Cheers <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/></p><NewLine></div>",https://discuss.pytorch.org/u/atriantafy,(Andreas Triantafyllopoulos),atriantafy,"May 23, 2019,  9:00am",7 Likes,,,,
45931,Cuda out of memory with enough Cuda memory,2019-05-22T16:23:30.605Z,1,568,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Dear all,<br/><NewLine>Currently, I have training the GRU network for speech recognition using pytorch. The training is successfully finished. However, the evaluation time, the Cuda out of memory runtime error occurs.<br/><NewLine>The actual error is RuntimeError: CUDA out of memory. Tried to allocate 1.72 GiB (GPU 0; 11.92 GiB total capacity; 5.72 GiB already allocated; 1.65 GiB free; 4.04 GiB cached)</p><NewLine><p>My Cuda has around (4.04GB + 1.65GB ) unused memory but Cuda is unable to allocate 1.72GB memory, which is unreasonable.<br/><NewLine>Please suggest possible solutions to overcome this error.</p><NewLine><p>With best regards,</p><NewLine></div>",https://discuss.pytorch.org/u/torchtes,(Kina),torchtes,"May 22, 2019,  4:23pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello there,</p><NewLine><p>There are a few things that you can do to lower your memory footprint. First is to run your validation code with <code>torch.no_grad()</code> so to not save any gradients. Are you by any chance running your validation code inside your training loop? If so, there might be a few tensors that you could delete from the training loop, perhaps <code>del training_input, del training_output, del ...</code>. Lastly, I found that putting this once before training lowers my memory footprint but I don’t know it’s inner workings.<br/><NewLine><code>torch.backends.cudnn.benchmark = True # Optimizes cudnn</code></p><NewLine><p>I don’t know about the cache though, good luck <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much Oli.<br/><NewLine>I already done the first two memory footprint methods, but i did not used the third one i will try and just come up with the result.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Oli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/torchtes; <NewLine> ,"REPLY_DATE 1: May 23, 2019,  5:55am; <NewLine> REPLY_DATE 2: May 23, 2019,  6:44am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> 
44062,SpecAugment w/torchaudio,2019-05-01T00:41:12.555Z,0,878,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I just published a Pytorch implementation of Google’s <a href=""https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html"" rel=""nofollow noopener"">new data augmentation technique</a> in <a href=""https://github.com/zcaceres/spec_augment"" rel=""nofollow noopener"">SpecAugment with torch audio</a>. Thought it might be of interest to some people working on audio in the forum.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/812c3779110a5d92db7fc8ba246b34966d8df780"" href=""https://discuss.pytorch.org/uploads/default/original/2X/8/812c3779110a5d92db7fc8ba246b34966d8df780.jpeg"" title=""freqmask.jpg""><img alt=""freqmask"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/8/812c3779110a5d92db7fc8ba246b34966d8df780_2_10x10.png"" height=""192"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/8/812c3779110a5d92db7fc8ba246b34966d8df780_2_689x192.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/8/812c3779110a5d92db7fc8ba246b34966d8df780_2_689x192.jpeg, https://discuss.pytorch.org/uploads/default/optimized/2X/8/812c3779110a5d92db7fc8ba246b34966d8df780_2_1033x288.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/8/812c3779110a5d92db7fc8ba246b34966d8df780_2_1378x384.jpeg 2x"" width=""689""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">freqmask.jpg</span><span class=""informations"">2296×642 621 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>To implement one of the transforms we ported a Tensorflow function <code>sparse_image_warp</code> to Pytorch. FWIW this was my deep dive into Pytorch and I found the experience really enjoyable with good docs and APIs that were easy to match with their TF counterparts <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/zachc,(Zach C),zachc,"May 1, 2019, 12:41am",10 Likes,,,,
43956,Phase Unwrapping,2019-04-29T19:02:30.253Z,0,267,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I would like to know how to implement a phase unwrapping like “numpy.unwrap” with the backward functionality? Could it be possible to support “torch.stft” with magnitude and phase instead of real and imaginary components to make life easier?</p><NewLine><p>Regards</p><NewLine></div>",https://discuss.pytorch.org/u/Ahmed_m,(Ahmed Mamoud),Ahmed_m,"April 29, 2019,  7:02pm",,,,,
39550,"Error in CUDA memory allocation, no matter the GPU size",2019-03-11T22:53:56.373Z,0,424,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):                      <NewLine>  File ""main.py"", line 49, in &lt;module&gt;<NewLine>    solver.exec()<NewLine>  File ""/project/src/solver.py"", line 195, in exec<NewLine>    self.valid()<NewLine>  File ""/project/src/solver.py"", line 234, in valid<NewLine>    ctc_pred, state_len, att_pred, att_maps = self.asr_model(x, ans_len+VAL_STEP,state_len=state_len)<NewLine>  File ""/home/anaconda2/envs/dlp/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/project/src/asr.py"", line 61, in forward<NewLine>    encode_feature,encode_len = self.encoder(audio_feature,state_len)<NewLine>  File ""/home/anaconda2/envs/dlp/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/project/home/src/asr.py"", line 314, in forward<NewLine>    input_x,enc_len = self.vgg_extractor(input_x,enc_len)<NewLine>  File ""/home/anaconda2/envs/dlp/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/project/src/asr.py"", line 554, in forward<NewLine>    feature = self.pool2(feature) # BSx128xT/4xD/4<NewLine>  File ""/home/anaconda2/envs/dlp/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/anaconda2/envs/dlp/lib/python3.6/site-packages/torch/nn/modules/pooling.py"", line 148, in forward<NewLine>    self.return_indices)<NewLine>  File ""/home/anaconda2/envs/dlp/lib/python3.6/site-packages/torch/_jit_internal.py"", line 132, in fn<NewLine>    return if_false(*args, **kwargs)<NewLine>  File ""/home/anaconda2/envs/dlp/lib/python3.6/site-packages/torch/nn/functional.py"", line 425, in _max_pool2d<NewLine>    input, kernel_size, stride, padding, dilation, ceil_mode)[0]<NewLine>  File ""/home/anaconda2/envs/dlp/lib/python3.6/site-packages/torch/nn/functional.py"", line 417, in max_pool2d_with_indices<NewLine>    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, _stride, padding, dilation, ceil_mode)<NewLine>RuntimeError: CUDA out of memory. Tried to allocate 24.12 MiB (GPU 0; 10.91 GiB total capacity; 9.25 GiB already allocated; 17.44 MiB free; 41.97 MiB cached)<NewLine></code></pre><NewLine><p>I keep increasing the GPU size, using 2 gpus but i still get this error even when batch size is 1.</p><NewLine><p>How could I resolve it?</p><NewLine></div>",https://discuss.pytorch.org/u/Rafael_R,(jean),Rafael_R,"March 11, 2019, 10:53pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This also means it is not using the 2 GPUs.</p><NewLine><p>How could I use 2 GPUs ( using nn.DataParallel?), and any way to remove this error?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you doing this in jupyter or py script?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Rafael_R; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Kushaj; <NewLine> ,"REPLY_DATE 1: March 11, 2019, 11:39pm; <NewLine> REPLY_DATE 2: March 14, 2019,  3:56pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
36319,Selecting element on dimension from list of indexes,2019-02-03T21:45:22.923Z,1,6724,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m implementing an LSTM on audio clips of different sizes. After going through padding/packing, the output of the lstm is:</p><NewLine><p><code>a.shape = torch.Size([16, 1580, 201])</code></p><NewLine><p>with <code>(batch, padded sequence, feature)</code>. I also have a list of the actual lengths of the sequences:</p><NewLine><p><code>lengths = [1580, 959, 896, 881, 881, 881, 881, 881, 881, 881, 881, 881, 881, 335, 254, 219]</code>.</p><NewLine><p>What I would like to do is for every element in the batch select the output of the last element in the sequence, and end up with a tensor of shape:</p><NewLine><p><code>torch.Size([16, 201])</code></p><NewLine><p>(independent of the variable sequence length the examples have). So far I’ve been using:<br/><NewLine><code>torch.cat([a[i:i+1][:,ind-1] for i,ind in enumerate(lengths)], dim=0)</code></p><NewLine><p>but I was wondering if there’s a proper PyTorch function for such use case?</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/ksanjeevan,(Kiran Sanjeevan),ksanjeevan,"February 3, 2019, 10:12pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The following indexing should work:</p><NewLine><pre><code class=""lang-python"">x = torch.randn(16, 1580, 201)<NewLine>idx = torch.tensor(<NewLine>    [1580, 959, 896, 881, 881, 881, 881, 881, 881, 881, 881, 881, 881, 335, 254, 219]<NewLine>)<NewLine>idx = idx - 1  # 0-based index<NewLine>y = x[torch.arange(x.size(0)), idx]<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Awesome, that’s exactly what I needed thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ksanjeevan; <NewLine> ,"REPLY_DATE 1: February 4, 2019,  6:04am; <NewLine> REPLY_DATE 2: February 4, 2019,  6:00am; <NewLine> ",REPLY 1 LIKES: 10 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
28818,Swapaxes to meet input for lstm throws an error,2018-11-05T13:04:30.767Z,4,384,"<div class=""post"" itemprop=""articleBody""><NewLine><p>LSTM is expecting (seq_len, batch, input_size) but when I do that PyTorch throws an error:</p><NewLine><pre><code class=""lang-auto"">ValueError: Expected input batch_size (20) to match target batch_size (27).<NewLine></code></pre><NewLine><p>20 is seq_len and 27 is batch_size.<br/><NewLine>Input shape:</p><NewLine><pre><code class=""lang-auto"">torch.Size([20, 27, 87])<NewLine></code></pre><NewLine><p>(seq_len, batch, input_size)  accordingly.<br/><NewLine>My model:</p><NewLine><pre><code class=""lang-auto"">class RNN(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(RNN, self).__init__()<NewLine>        self.lstm1 = nn.LSTM(input_size=87, hidden_size=256)<NewLine>        self.lstm2 = nn.LSTM(input_size=256, hidden_size=128)<NewLine>        self.lstm3 = nn.LSTM(input_size=128, hidden_size=64)<NewLine>        self.lstm4 = nn.LSTM(input_size=64, hidden_size=32)<NewLine>        self.fc1 = nn.Linear(in_features=32, out_features=128)<NewLine>        self.fc2 = nn.Linear(in_features=128, out_features=64)<NewLine>        self.fc3 = nn.Linear(in_features=64, out_features=32)<NewLine>        self.fc4 = nn.Linear(in_features=32, out_features=3)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = torch.tanh(self.lstm1(x)[0])<NewLine>        x = torch.tanh(self.lstm2(x)[0])<NewLine>        x = torch.tanh(self.lstm3(x)[0])<NewLine>        x = torch.tanh(self.lstm4(x)[0])<NewLine>        x = F.relu(self.fc1(x))<NewLine>        x = F.relu(self.fc2(x))<NewLine>        x = F.relu(self.fc3(x))<NewLine>        x = self.fc4(x)<NewLine>        return x<NewLine></code></pre><NewLine><p>There are two questions:</p><NewLine><ol><NewLine><li>What’s wrong with seq_len in my case?</li><NewLine><li>Is it ok to swapaxes to meet requirments or it mixes important things:</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">X_train = X_train.swapaxes(1,0)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/andreiliphd,(Andrei Li),andreiliphd,"November 5, 2018,  1:05pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think it depends on your use case.<br/><NewLine>Currently you are feeding a tensor with the shape <code>[seq_len, batch_size, hidden_size]</code> into your linear layer.<br/><NewLine>Usually the input to a linear layer should be <code>[batch_size, *, in_features]</code>. The <code>*</code> means any number of additional dimensions, where the same linear operation will be performed on.<br/><NewLine>In this case, your output won’t differ, but will be shaped <code>[seq_len, batch_size, out_features]</code>.<br/><NewLine>Since your target is most likely in the shape <code>[batch_size, *]</code>, you’ll get an error trying to calculate the loss.<br/><NewLine>You could <code>permute</code> the output or the activation in your model to set the batch dimension as dim0.</p><NewLine><p>In case your target is only a single class for the whole sequence, probably you would like to get the activation of the last step from your LSTM.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Changed to dim0 labels, got error:</p><NewLine><pre><code class=""lang-auto"">ValueError: Expected target size (27, 3), got torch.Size([27])<NewLine></code></pre><NewLine><p>Manual:</p><NewLine><pre><code class=""lang-auto"">Target: (N)(N) where each value is 0≤targets[i]≤C−10≤targets[i]≤C−1, or<NewLine><NewLine>(N,d1,d2,...,dK)(N,d1,d2,...,dK) with K≥2K≥2 in the case of K-dimensional loss.<NewLine></code></pre><NewLine><p>So, manual allow [batch_size, * ] for target.<br/><NewLine>The question is how it should be encoded. I used the following data to encode labels and nn works:</p><NewLine><pre><code class=""lang-auto"">y_train_torch.shape<NewLine>torch.Size([27, 3])<NewLine>tensor([[1, 0, 0],<NewLine>        [1, 0, 0],<NewLine>        [1, 0, 0],<NewLine>        [1, 0, 0],<NewLine>        [1, 0, 0],<NewLine>        [1, 0, 0],<NewLine>        [1, 0, 0],<NewLine>        [1, 0, 0],<NewLine>        [1, 0, 0],<NewLine>        [1, 0, 0],<NewLine>        [0, 1, 0],<NewLine>        [0, 1, 0],<NewLine>        [0, 1, 0],<NewLine>        [0, 1, 0],<NewLine>        [0, 1, 0],<NewLine>        [0, 1, 0],<NewLine>        [0, 1, 0],<NewLine>        [0, 1, 0],<NewLine>        [0, 1, 0],<NewLine>        [0, 1, 0],<NewLine>        [0, 0, 1],<NewLine>        [0, 0, 1],<NewLine>        [0, 0, 1],<NewLine>        [0, 0, 1],<NewLine>        [0, 0, 1],<NewLine>        [0, 0, 1],<NewLine>        [0, 0, 1]])<NewLine></code></pre><NewLine><p>Feeding into nn any other shape causes an error.<br/><NewLine>I strongly believe in PyTorch and on my first task where I analyzed financial reports of Russian companies to define target capitalization it performed really well. Slightly better than TensorFlow. Keras caused an overfit.<br/><NewLine>I also strongly believe that I am monkey playing with collider and I want to learn.<br/><NewLine>Considering our case with seq_len it seems that it doesn’t really matter whether batch come first or seq_len.<br/><NewLine>But it’s nn and it should fit the data it should learn and give me accuracy at least on training data. Of course my dataset is really small but… I said: ""One, two and three’ to mic and trying to classify it into three categories. Made it 30 times. Keras is fine and I reached 100% val acc and 100% test acc for 6 epochs but I want to use flexible instrument such as PyTorch for my tasks.<br/><NewLine>model(input) in PyTorch gives me that(see below) after 3000 epochs my doubt that I incorrectly encoded labels:</p><NewLine><pre><code class=""lang-auto"">tensor([[[ -7.7228,   3.0183,  11.2289],<NewLine>         [  0.1328,  -3.4348,   4.6932],<NewLine>         [-10.8275, -10.2396,   0.1168],<NewLine>         ...,<NewLine>         [-10.7704, -10.0659,   0.1782],<NewLine>         [-10.8403, -10.1490,   0.1689],<NewLine>         [-10.7978, -10.1629,   0.1400]],<NewLine><NewLine>        [[-11.1168,   4.3190,  15.7578],<NewLine>         [  0.3338,  -5.1119,   6.4511],<NewLine>         [-15.0107, -15.3557,  -0.3449],<NewLine>         ...,<NewLine>         [-14.9699, -15.3302,  -0.3514],<NewLine>         [-14.9923, -15.2761,  -0.3178],<NewLine>         [-14.9932, -15.3604,  -0.3547]],<NewLine><NewLine>        [[-11.9856,   4.5941,  16.8911],<NewLine>         [  0.4712,  -5.5082,   6.8625],<NewLine>         [-16.2449, -17.0981,  -0.5629],<NewLine>         ...,<NewLine>         [-16.2539, -17.0999,  -0.5594],<NewLine>         [-16.2275, -17.0364,  -0.5457],<NewLine>         [-16.2737, -17.1193,  -0.5595]],<NewLine><NewLine>        ...,<NewLine><NewLine>        [[  0.2076,  -5.2667,  17.3286],<NewLine>         [ -7.1703, -13.7633,  25.5701],<NewLine>         [-17.3713, -18.6646,  -0.7112],<NewLine>         ...,<NewLine>         [-17.3655, -18.6440,  -0.7082],<NewLine>         [-17.3683, -18.6579,  -0.7098],<NewLine>         [-17.3624, -18.6468,  -0.7087]],<NewLine><NewLine>        [[  0.2093,  -5.2638,  17.3253],<NewLine>         [ -7.1709, -13.7636,  25.5766],<NewLine>         [-17.3682, -18.6617,  -0.7112],<NewLine>         ...,<NewLine>         [-17.3644, -18.6427,  -0.7081],<NewLine>         [-17.3678, -18.6579,  -0.7099],<NewLine>         [-17.3617, -18.6466,  -0.7088]],<NewLine><NewLine>        [[  0.2100,  -5.2618,  17.3232],<NewLine>         [ -7.1747, -13.7550,  25.5766],<NewLine>         [-17.3694, -18.6632,  -0.7113],<NewLine>         ...,<NewLine>         [-17.3630, -18.6415,  -0.7081],<NewLine>         [-17.3680, -18.6584,  -0.7100],<NewLine>         [-17.3609, -18.6461,  -0.7089]]], grad_fn=&lt;ThAddBackward&gt;)<NewLine><NewLine></code></pre><NewLine><p>Visual inspection and calculation gives my acc 0% on training set and 33 percent acc on test set.<br/><NewLine>And big thank you for answer <a href=""https://discuss.pytorch.org/u/ptrblck"">ptrblck</a>.<br/><NewLine>Hope to find the truth in my problem.<br/><NewLine>Ready to post any data for the task at my disposal.<br/><NewLine>Full code:</p><NewLine><pre><code class=""lang-auto"">import librosa<NewLine>from os import listdir<NewLine>import numpy as np<NewLine>from sklearn.model_selection import train_test_split<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>def loadSound(path):<NewLine>    soundList = listdir(path)<NewLine>    loadedSound = []<NewLine>    for sound in soundList:<NewLine>        Y, sr = librosa.load(path + sound)<NewLine>        loadedSound.append(librosa.feature.mfcc(Y, sr=sr))   <NewLine>    return np.array(loadedSound)<NewLine>one = loadSound('./voice_123/one/')<NewLine>one = loadSound('./voice_123/one/')<NewLine>two = loadSound('./voice_123/two/')<NewLine>three = loadSound('./voice_123/three/')<NewLine>X = np.concatenate((one, two, three), axis=0)<NewLine>one_label = np.concatenate((np.ones(10), np.zeros(10), np.zeros(10)))<NewLine>two_label = np.concatenate((np.zeros(10), np.ones(10), np.zeros(10)))<NewLine>three_label = np.concatenate((np.zeros(10), np.zeros(10), np.ones(10)))<NewLine>y = np.concatenate((one_label[:, None], two_label[:, None], three_label[:, None]), axis=1)<NewLine>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=False)<NewLine># X_train = X_train.swapaxes(1,0)<NewLine># X_test = X_test.swapaxes(1,0)<NewLine>X_train_torch = torch.from_numpy(X_train).float()<NewLine>X_test_torch = torch.from_numpy(X_test).float()<NewLine>y_train_torch = torch.from_numpy(y_train).long()<NewLine>y_test_torch = torch.from_numpy(y_test).long()<NewLine>class RNN(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(RNN, self).__init__()<NewLine>        self.lstm1 = nn.LSTM(input_size=87, hidden_size=256)<NewLine>        self.lstm2 = nn.LSTM(input_size=256, hidden_size=128)<NewLine>        self.lstm3 = nn.LSTM(input_size=128, hidden_size=64)<NewLine>        self.lstm4 = nn.LSTM(input_size=64, hidden_size=32)<NewLine>        self.fc1 = nn.Linear(in_features=32, out_features=128)<NewLine>        self.fc2 = nn.Linear(in_features=128, out_features=64)<NewLine>        self.fc3 = nn.Linear(in_features=64, out_features=32)<NewLine>        self.fc4 = nn.Linear(in_features=32, out_features=3)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = torch.tanh(self.lstm1(x)[0])<NewLine>        x = torch.tanh(self.lstm2(x)[0])<NewLine>        x = torch.tanh(self.lstm3(x)[0])<NewLine>        x = torch.tanh(self.lstm4(x)[0])<NewLine>        x = F.relu(self.fc1(x))<NewLine>        x = F.relu(self.fc2(x))<NewLine>        x = F.relu(self.fc3(x))<NewLine>        x = self.fc4(x)<NewLine>        return x<NewLine>model = RNN()<NewLine>model(X_train_torch)<NewLine>loss_fn = torch.nn.CrossEntropyLoss()<NewLine>learning_rate = 0.00001<NewLine>optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)<NewLine>for t in range(3000):<NewLine>    y_pred = model(X_train_torch)<NewLine>    loss = loss_fn(y_pred, y_train_torch)<NewLine>    print(t, loss.item())<NewLine>    optimizer.zero_grad()<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine>for t in range(3000):<NewLine>    y_pred = model(X_train_torch)<NewLine>    loss = loss_fn(y_pred, y_train_torch)<NewLine>    print(t, loss.item())<NewLine>    optimizer.zero_grad()<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine>learning_rate = 0.0001<NewLine>optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)<NewLine>for t in range(3000):<NewLine>    y_pred = model(X_train_torch)<NewLine>    loss = loss_fn(y_pred, y_train_torch)<NewLine>    print(t, loss.item())<NewLine>    optimizer.zero_grad()<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine></code></pre><NewLine><p>And I can provide my: “One, two and three”.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Based on your description, it seems you are trying to classify your dataset into one of three classes.<br/><NewLine>You are right about the docs of <code>nn.CrossEntropyLoss</code>, i.e. the target might be multi-dimensional.<br/><NewLine>However, if you compare the shapes of the input (model output) and the target, you see that the channel dimension is missing (<code>C</code> in the docs).<br/><NewLine>In your case, your target should have the shape <code>[batch_size]</code> and contain the class indices, i.e. values in the range <code>[0, 2]</code>.<br/><NewLine>Just call <code>y_train_torch = torch.argmax(y_train_torch)</code> and you should be fine.</p><NewLine><p>The multi-dimensional use case is useful for e.g. segmentation tasks, where each pixel belongs to one particular class.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>What you mean is that:</p><NewLine><pre><code class=""lang-auto"">tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,<NewLine>        2, 2, 2])<NewLine></code></pre><NewLine><p>I did it around 10 times and couldn’t believe my eyes till I read docs.<br/><NewLine>Actually [batch_size, *] is possible, I believe.<br/><NewLine>torch.argmax(y_train_torch) gives on my y_train_torch:</p><NewLine><pre><code class=""lang-auto"">tensor(20)<NewLine></code></pre><NewLine><p>Okay. I am closer to truth. I checked the shape of input and output tensors they are fine.<br/><NewLine>But:</p><NewLine><pre><code class=""lang-auto"">model_for = model(X_train_torch)<NewLine>for number in range(27):<NewLine>    print(model_for[number].argmax())<NewLine></code></pre><NewLine><p>Gives:</p><NewLine><pre><code class=""lang-auto"">tensor(1)<NewLine>tensor(1)<NewLine>tensor(1)<NewLine>tensor(1)<NewLine>tensor(1)<NewLine>tensor(1)<NewLine>tensor(1)<NewLine>tensor(1)<NewLine>tensor(1)<NewLine>tensor(2)<NewLine>tensor(2)<NewLine>tensor(2)<NewLine>tensor(2)<NewLine>tensor(2)<NewLine>tensor(2)<NewLine>tensor(2)<NewLine>tensor(2)<NewLine>tensor(10)<NewLine>tensor(0)<NewLine>tensor(0)<NewLine>tensor(11)<NewLine>tensor(5)<NewLine>tensor(11)<NewLine>tensor(11)<NewLine>tensor(5)<NewLine>tensor(5)<NewLine>tensor(5)<NewLine></code></pre><NewLine><p>Why is 5,10 and 11? I didn’t say that <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/><br/><NewLine>Labels are fine. Not sure but without it. It’s not working and asking me to give exact dimensions of the labels.<br/><NewLine>What if the problem is in seq_len and batch?<br/><NewLine>But PyTorch is giving me the error in the case when I correctly did a model that batch should be second.<br/><NewLine>So, even in the case of messing up with seq_len I have a bunch of Linear layers that should play the game and make everything perfect. Even if there is a mistake. Correct me if I am wrong.<br/><NewLine>Therefore, the problem might be in: seq_len.<br/><NewLine>Therefore, I have to change slicing of lstm to something else for model to be happy.<br/><NewLine>How?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, my bad. It should be <code>torch.argmax(y_train_torch, dim=1)</code>.<br/><NewLine>This will give you the right class indices.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>model(X_train_torch)[0] gives:</p><NewLine><pre><code class=""lang-auto"">tensor([[-355.4695, -424.0195, -681.5226],<NewLine>        [-339.5056, -432.3004, -696.3847],<NewLine>        [-359.5927, -438.7826, -705.6765],<NewLine>        [-346.2557, -439.3127, -707.5528],<NewLine>        [-359.3715, -437.6559, -703.8282],<NewLine>        [-356.7079, -443.8343, -714.3834],<NewLine>        [-357.2831, -442.1734, -711.5266],<NewLine>        [-355.7645, -442.3993, -712.0264],<NewLine>        [-359.3816, -440.9185, -709.2794],<NewLine>        [-357.2108, -443.9724, -714.5976],<NewLine>        [-355.9514, -443.8340, -714.3595],<NewLine>        [-359.4459, -440.7137, -708.9507],<NewLine>        [-359.6165, -440.2903, -708.2195],<NewLine>        [-357.3955, -443.4591, -713.6996],<NewLine>        [-359.5545, -442.3593, -711.6683],<NewLine>        [-356.2823, -443.2863, -713.4790],<NewLine>        [-359.6244, -439.4549, -706.8243],<NewLine>        [-358.2084, -445.1720, -716.4835],<NewLine>        [-359.5984, -437.4826, -703.5777],<NewLine>        [-356.8849, -442.9969, -712.9752]], grad_fn=&lt;SelectBackward&gt;)<NewLine></code></pre><NewLine><p>which is shape:</p><NewLine><pre><code class=""lang-auto"">torch.Size([20, 3])<NewLine></code></pre><NewLine><p>Made a change as <a href=""https://discuss.pytorch.org/u/ptrblck"">ptrblck</a> suggested and I have become closer to truth data of output. At least it looks better. Going deeper in what <a href=""https://discuss.pytorch.org/u/ptrblck"">ptrblck</a> said.</p><NewLine><pre><code class=""lang-auto"">tensor([[0.0890, 0.0380, 0.0673],<NewLine>        [0.0890, 0.0379, 0.0674],<NewLine>        [0.0891, 0.0380, 0.0674],<NewLine>        [0.0891, 0.0379, 0.0674],<NewLine>        [0.0891, 0.0379, 0.0674],<NewLine>        [0.0892, 0.0380, 0.0675],<NewLine>        [0.0891, 0.0380, 0.0675],<NewLine>        [0.0891, 0.0378, 0.0673],<NewLine>        [0.0891, 0.0378, 0.0673],<NewLine>        [0.0891, 0.0379, 0.0676],<NewLine>        [0.0890, 0.0379, 0.0674],<NewLine>        [0.0890, 0.0379, 0.0675],<NewLine>        [0.0891, 0.0379, 0.0674],<NewLine>        [0.0892, 0.0379, 0.0675],<NewLine>        [0.0891, 0.0379, 0.0674],<NewLine>        [0.0892, 0.0380, 0.0675],<NewLine>        [0.0891, 0.0380, 0.0675],<NewLine>        [0.0891, 0.0379, 0.0675],<NewLine>        [0.0891, 0.0379, 0.0674],<NewLine>        [0.0892, 0.0379, 0.0675]], grad_fn=&lt;SelectBackward&gt;)<NewLine></code></pre><NewLine><p>Obviously problem with seq_len because 20 is seq_len not a batch_size:</p><NewLine><pre><code class=""lang-auto"">torch.Size([20, 3])<NewLine></code></pre><NewLine><p>Trying to change input data to match manual [seq_len, batch_size, *]. Got an error:</p><NewLine><pre><code class=""lang-auto"">ValueError: Expected input batch_size (20) to match target batch_size (27).<NewLine></code></pre><NewLine><p>Will try to debug.<br/><NewLine>Shape of output from model(input) is still:</p><NewLine><pre><code class=""lang-auto"">torch.Size([20, 27, 3])<NewLine></code></pre><NewLine><p>Strange we did self.lstm1(x)[0]) that should destroy dimension 20.<br/><NewLine>Obviously, I should change the code in lstm part of my model to match dimension, it seems that tanh make it fit for the next lstm which is not right because I want to feed it to Linear. As was suggested to me earlier. Will try to debug the model.<br/><NewLine>According to the manual tanh doesn’t change output:</p><NewLine><pre><code class=""lang-auto"">input (Tensor) – the input tensor<NewLine>out (Tensor, optional) – the output tensor<NewLine></code></pre><NewLine><p>Will try to look manual for lstm.<br/><NewLine>Changing model to:</p><NewLine><pre><code class=""lang-auto"">        x = torch.tanh(self.lstm1(x))<NewLine>        x = torch.tanh(self.lstm2(x))<NewLine>        x = torch.tanh(self.lstm3(x))<NewLine>        x = torch.tanh(self.lstm4(x)[0])<NewLine></code></pre><NewLine><p>Didn’t help. Got error:</p><NewLine><pre><code class=""lang-auto"">TypeError: tanh(): argument 'input' (position 1) must be Tensor, not tuple<NewLine></code></pre><NewLine><p>What if I try:</p><NewLine><pre><code class=""lang-auto"">    class RNN(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(RNN, self).__init__()<NewLine>        self.lstm1 = nn.LSTM(input_size=87, hidden_size=256)<NewLine>        self.lstm2 = nn.LSTM(input_size=256, hidden_size=128)<NewLine>        self.lstm3 = nn.LSTM(input_size=128, hidden_size=64)<NewLine>        self.lstm4 = nn.LSTM(input_size=64, hidden_size=32)<NewLine>        self.fc1 = nn.Linear(in_features=32, out_features=128)<NewLine>        self.fc2 = nn.Linear(in_features=128, out_features=64)<NewLine>        self.fc3 = nn.Linear(in_features=64, out_features=32)<NewLine>        self.fc4 = nn.Linear(in_features=32, out_features=3)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = torch.tanh(self.lstm1(x)[0])<NewLine>        x = torch.tanh(self.lstm2(x)[0])<NewLine>        x = torch.tanh(self.lstm3(x)[0])<NewLine>        x = torch.tanh(self.lstm4(x)[0][0])<NewLine>        x = F.relu(self.fc1(x))<NewLine>        x = F.relu(self.fc2(x))<NewLine>        x = F.relu(self.fc3(x))<NewLine>        x = self.fc4(x)<NewLine>        return x<NewLine></code></pre><NewLine><p>Oh my God, it finally worked <img alt="":smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smile.png?v=6"" title="":smile:""/><br/><NewLine>And acc:</p><NewLine><pre><code class=""lang-auto"">Training accuracy: 100.0%<NewLine>Testing accuracy: 100.0%<NewLine><NewLine></code></pre><NewLine><p>Thank you very much ptrblck,all is good!</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Awesome <a class=""mention"" href=""/u/andreiliphd"">@andreiliphd</a>!<br/><NewLine>It was a pleasure to see how you managed to get rid of all the bugs! <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>it’s good.i have the same problem.Thanks you help me to slove the problem.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/andreiliphd; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/andreiliphd; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/andreiliphd; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/780748976; <NewLine> ,"REPLY_DATE 1: November 5, 2018,  3:43pm; <NewLine> REPLY_DATE 2: November 5, 2018,  6:55pm; <NewLine> REPLY_DATE 3: November 5, 2018,  8:33pm; <NewLine> REPLY_DATE 4: November 5, 2018,  9:24pm; <NewLine> REPLY_DATE 5: November 5, 2018,  9:20pm; <NewLine> REPLY_DATE 6: November 6, 2018, 10:00am; <NewLine> REPLY_DATE 7: November 6, 2018, 11:32am; <NewLine> REPLY_DATE 8: January 27, 2019,  1:04pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
34269,Sequence-Wise Batch-Norm,2019-01-09T15:00:45.485Z,0,438,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a question regarding sequence-wise batch-normalization for RNNs, as described in the paper: <a href=""https://arxiv.org/abs/1510.01378"" rel=""nofollow noopener"">https://arxiv.org/abs/1510.01378</a></p><NewLine><p><img alt=""CodeCogsEqn2"" height=""54"" src=""https://discuss.pytorch.org/uploads/default/original/2X/d/d80cd35f3519bcb562594a2115b3165cbc656cd8.gif"" width=""152""/></p><NewLine><p>[Note: I swapped the indices in the formula to be consistent with sequence-first format]</p><NewLine><p>Assuming variable length sequences, what would be the best way to implement this as a layer in PyTorch?</p><NewLine><p>My first idea was to manually compute the inner sum (i.e. along the time axis for each sequence independently), create a new vector of size <code>(N, H)</code> where <code>N</code> is the mini-batch size and <code>H</code> the number of features, with this vector containing the <em>sum</em> of all outputs for each sequence, and then call <code>BatchNorm1d</code> on that.</p><NewLine><p>However, this will not work properly for the standard deviation (and probably for other things as well).</p><NewLine><p>Has anyone implemented this already in PyTorch? What is the best way to do this?</p><NewLine></div>",https://discuss.pytorch.org/u/atriantafy,(Andreas Triantafyllopoulos),atriantafy,"January 9, 2019,  3:00pm",,,,,
34018,Spectrogram data load,2019-01-07T12:39:26.766Z,2,825,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I worked on speech enhancement with VCTK database.</p><NewLine><p>I want to load data and apply the pre-processing method simultaneously and efficiently. pre-processing is performed in just one python function.</p><NewLine><p>My problem is, when I use the dataloader, it load just one wave file per loading. And it return the different amount of training data after pre-processing because the wave files which have different time length are chopped into input size in pre-processing. It means for every iteration, network will be trained with small and different batch size, and it is time consuming.</p><NewLine><p>So I want to loading and pre-processing simultaneously for training with same batch size ( It should be stacked for several wav file). Now, I saved all preprocessed data as npy file. Is there more efficient way for loading data.</p><NewLine></div>",https://discuss.pytorch.org/u/1c9d70faac66efabd051,(Hyunjae),1c9d70faac66efabd051,"January 7, 2019, 12:39pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can create a custom <code>collate_fn</code> in your dataloader that pads or trims the output from the Dataset properly and then stacks the padded/trimmed tensors into a batch.  Or if you are using an rnn then you can put it into a <a href=""https://pytorch.org/docs/stable/nn.html#packedsequence"" rel=""nofollow noopener""><code>PackedSequence</code></a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>thank you for reply.</p><NewLine><p>could you give me some simple example code or link for stacking tensor into a batch…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can search the forum for padding packed sequences.  But the gist of it is that the collate function takes a list of outputs from the <code>Dataset</code>, then you unpack that, get the lengths, then pad based on the maximum length in the batch.</p><NewLine><pre><code class=""lang-auto"">def collate_spectrograms_fn(batch):<NewLine>  # assuming sig has size (c, l, n_ftt)<NewLine>  sigs, targets = zip(*batch)<NewLine>  lengths = torch.tensor([sig.size(1) for sig in sigs], dtype=torch.long)<NewLine>  max_len = lengths.max()<NewLine>  sigs = [pad(sig, (0, 0, 0, ma_len - sig.size(1)) for sig in sigs]<NewLine>  return torch.stack(sigs), torch.cat(targets), lengths<NewLine></code></pre><NewLine><p>I haven’t tested that but that’s the general idea.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dhpollack; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/1c9d70faac66efabd051; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/dhpollack; <NewLine> ,"REPLY_DATE 1: January 7, 2019,  2:25pm; <NewLine> REPLY_DATE 2: January 8, 2019,  6:06am; <NewLine> REPLY_DATE 3: January 8, 2019,  8:44am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
32118,Las attetion problem,2018-12-14T09:31:10.539Z,0,173,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m trying to implement a modified version of LAS model. Everything works fine the loss and wer is decreasing but after the second epochs the attention seems to be wrong because it’s attending not only the right part of the listener features but also the end of the sequence.</p><NewLine><p>Here is an attention plot: <a href=""https://i.imgur.com/iCp404F.jpg"" rel=""nofollow noopener"">https://i.imgur.com/iCp404F.jpg</a><br/><NewLine>I think in one point it should attend one part of the sequence not two, its also hard for softmax.</p><NewLine><p>Does anyone have any idea what can cause this? common structural problems or anything?</p><NewLine></div>",https://discuss.pytorch.org/u/Andras,(Andras),Andras,"December 14, 2018,  9:31am",,,,,
29222,HogWild on 2 or more GPUs,2018-11-09T21:38:41.636Z,0,240,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I would love to convert Sean Naren’s DeepSpeech to Hogwild on Cuda (2 or more GPUs).<br/><NewLine>I did not find a working implementation of Hogwild on GPUs. Is this possible, and if yes, is there a reference implementation I can use as an example?</p><NewLine><p>Thanks &amp; kind regards<br/><NewLine>Ernst</p><NewLine></div>",https://discuss.pytorch.org/u/zapphod42,(Ernst),zapphod42,"November 9, 2018,  9:38pm",,,,,
28803,How to properly implement this architecture?,2018-11-05T10:46:39.370Z,1,341,"<div class=""post"" itemprop=""articleBody""><NewLine><p>At the moment my model gives me an error:<br/><NewLine><code>TypeError: tanh(): argument 'input' (position 1) must be Tensor, not tuple</code></p><NewLine><p>If there is a solution?<br/><NewLine>How to implement this model in PyTorch?</p><NewLine><p>The model is following:</p><NewLine><pre><code class=""lang-auto"">class RNN(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(RNN, self).__init__()<NewLine>        self.lstm1 = nn.LSTM(input_size=87, hidden_size=256)<NewLine>        self.lstm2 = nn.LSTM(input_size=256, hidden_size=128)<NewLine>        self.lstm3 = nn.LSTM(input_size=128, hidden_size=64)<NewLine>        self.lstm4 = nn.LSTM(input_size=64, hidden_size=32)<NewLine>        self.fc1 = nn.Linear(in_features=32, out_features=128)<NewLine>        self.fc2 = nn.Linear(in_features=128, out_features=64)<NewLine>        self.fc3 = nn.Linear(in_features=64, out_features=32)<NewLine>        self.fc4 = nn.Linear(in_features=32, out_features=3)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = torch.tanh(self.lstm1(x))<NewLine>        x = torch.tanh(self.lstm2(x))<NewLine>        x = torch.tanh(self.lstm3(x))<NewLine>        x = torch.tanh(self.lstm4(x))<NewLine>        x = F.relu(self.fc1(x))<NewLine>        x = F.relu(self.fc2(x))<NewLine>        x = F.relu(self.fc3(x))<NewLine>        x = self.fc4(x)<NewLine>        return x<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/andreiliphd,(Andrei Li),andreiliphd,"November 5, 2018, 10:46am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Good morning <a class=""mention"" href=""/u/andreiliphd"">@andreiliphd</a>,<br/><NewLine>I believe LSTM outputs siomething like this : output, (h_n, c_n)<br/><NewLine>Maybe you only want to apply your tanh on the “output” ? So you should do :</p><NewLine><pre><code class=""lang-auto"">torch.tanh(self.lstm1(x)[0])<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Good morning <a href=""https://discuss.pytorch.org/u/lelouedec"">lelouedec</a>!</p><NewLine><p>Thank you very much for your help!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/lelouedec; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/andreiliphd; <NewLine> ,"REPLY_DATE 1: November 5, 2018, 12:21pm; <NewLine> REPLY_DATE 2: November 5, 2018, 12:51pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
27671,Gradient of a variable became NaN after first batch,2018-10-20T17:58:43.401Z,0,983,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,<br/><NewLine>I’m developing a neural vocoder. My loss is based on STFT. However, when I switch to pytorch 0.4.1, the loss became NaN after my first batch. I tried to reduce learning rate, however, the error still happens.<br/><NewLine>I created a simple code to test it:</p><NewLine><pre><code class=""lang-auto"">import numpy as np<NewLine>import torch<NewLine><NewLine>def cal_spec(signal, n_fft=2048, hop_length=256, win_length=1024):<NewLine>    window = torch.hann_window(win_length).cuda()<NewLine>    complex_spectrogram = torch.stft(<NewLine>            signal, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window, center=False)<NewLine>    power_spectrogram = complex_spectrogram[:, :, :, 0] ** 2 + complex_spectrogram[:, :, :, 1] ** 2<NewLine>    return torch.sqrt(power_spectrogram)<NewLine><NewLine>grads = {}<NewLine>def add_grad(name, x):<NewLine>    grads[name] = x<NewLine><NewLine>def reg(name):<NewLine>    return lambda x: add_grad(name, x)<NewLine><NewLine><NewLine># file can be downloaded from:<NewLine># https://drive.google.com/file/d/1qxTIKLcSShBcfX3kIgtf5scJSlQKtrPa/view?usp=sharing<NewLine>d = np.load('a.npy.npz')<NewLine>x = torch.tensor(d['pred'], requires_grad=True)<NewLine>y = torch.tensor(d['target'], requires_grad=False)<NewLine><NewLine>pred_spec = cal_spec(x)<NewLine>target_spec = cal_spec(y)<NewLine>x.register_hook(reg('x'))<NewLine>pred_spec.register_hook(reg('pred_spec'))<NewLine><NewLine>loss = torch.mean(torch.abs(pred_spec - target_spec))<NewLine>loss.backward()<NewLine></code></pre><NewLine><p>After inspecting, I see that the gradient of pred_spec is fine. However, the gradient of x is NaN.<br/><NewLine>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/tuan3w,(Tuan Nguyen),tuan3w,"October 20, 2018,  6:03pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Maybe the non-differentiability of sqrt at 0 causes you trouble? If it does you could add a small constant (but beware, the square root of a small constant isn’t nearly as small) or just drop the square root and operate with the squares.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/tom"">@tom</a>,<br/><NewLine>Thanks for your reply. And yes, the non-differentiability of sqrt at 0 causes the problem. In pytorch 0.4.0, I didn’t see the problem. Maybe there’s some changes in STFT calculation in torch from 0.4.0 to 0.4.1</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tuan3w; <NewLine> ,"REPLY_DATE 1: October 20, 2018,  6:49pm; <NewLine> REPLY_DATE 2: October 21, 2018, 12:36am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
27554,Can‘t convergence as good as Keras or Tensorflow,2018-10-19T03:04:52.418Z,0,450,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I‘m a beginner of Pytorch ,and I try to build a lstm acoustic model, I used merlin’s frontend to prepare data,but my  result is not as good as keras or tensorflow,here is my code，Did I make any mistakes in building the model and training?</p><NewLine><pre><code class=""lang-auto"">class LSTM(nn.Module):<NewLine>    def __init__(self, embedding_dim, hidden_dim, output_size):<NewLine>        super(LSTM, self).__init__()<NewLine>        self.fc1=nn.Linear(embedding_dim,hidden_dim)<NewLine>        self.fc2=nn.Linear(hidden_dim,hidden_dim)<NewLine>        self.hidden_dim = hidden_dim<NewLine>        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2,batch_first=True)<NewLine>        self.hidden2out = nn.Linear(hidden_dim, output_size)<NewLine>        self.dropout_layer = nn.Dropout(p=0.1)<NewLine><NewLine><NewLine>    def init_hidden(self, batch_size):<NewLine>        return (autograd.Variable(torch.randn(2, batch_size, self.hidden_dim)).cuda(),<NewLine>                autograd.Variable(torch.randn(2, batch_size, self.hidden_dim)).cuda())<NewLine><NewLine><NewLine>    def forward(self,input, lengths):<NewLine>        self.hidden = self.init_hidden(batch_size)<NewLine>        input1=torch.tanh(self.fc1(input))<NewLine>        input2=torch.tanh(self.fc2(input1))<NewLine>        packed_input = pack_padded_sequence(input2, lengths,batch_first=True)<NewLine>        outputs, (ht, ct) = self.lstm(packed_input, self.hidden)<NewLine>        opt,_=pad_packed_sequence(outputs,batch_first=True)<NewLine>        outputs=self.hidden2out(opt)<NewLine>        return outputs<NewLine><NewLine>model =LSTM(ins,ins,outs).to(device)<NewLine>optimizer = optim.Adam(model.parameters(), lr=0.002)<NewLine>for epoch in range(25):  # again, normally you would NOT do 300 epochs, it is toy data<NewLine>    L = 1<NewLine>    overall_loss = 0<NewLine><NewLine>    for iteration in range(int(len(train_x.keys()) / batch_size) + 1):<NewLine>        x_batch, y_batch, utt_length_batch = get_batch(train_x, train_y,keys_list,iteration,batch_size)<NewLine>        if utt_length_batch == []:<NewLine>            continue<NewLine>        else:<NewLine>            L += 1<NewLine>        max_length_batch = max(utt_length_batch)<NewLine>        x_batch = data_utils.transform_data_to_3d_matrix(x_batch, max_length=max_length_batch, shuffle_data=False)<NewLine>        y_batch = data_utils.transform_data_to_3d_matrix(y_batch, max_length=max_length_batch, shuffle_data=False)<NewLine>    #    for i in range(len(x_batch)):<NewLine>    #        for s in range(len(x_batch[i])):<NewLine>    #            temp_x_batch[s][i][:]=x_batch[i][s][:]<NewLine>     #           temp_y_batch[s][i][:]=y_batch[i][s][:]<NewLine>        inputs = torch.from_numpy(x_batch).float().to(device)<NewLine>        tags = torch.from_numpy(y_batch).float().to(device)<NewLine><NewLine>        # Also, we need to clear out the hidden state of the LSTM,<NewLine>        # detaching it from its history on the last instance.<NewLine>        model.zero_grad()<NewLine><NewLine>        # Step 2. Get our inputs ready for the network, that is, turn them into<NewLine>        # Tensors of word indices.<NewLine><NewLine>        # Step 3. Run our forward pass.<NewLine>        #output,hidden = model(inputs,utt_length_batch)<NewLine><NewLine>        pred = model(torch.autograd.Variable(inputs), utt_length_batch)<NewLine>        loss=criterion(pred,tags)<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine>        overall_loss += loss<NewLine>    print(overall_loss/L)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/K_Li,(Wenjie Li),K_Li,"October 19, 2018,  3:04am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Maybe there are some minor differences between your PyTorch and Keras/TF code.<br/><NewLine>Could you post the Keras code and also some dummy input and target tensors, i.e. <code>data = torch.randn(...)</code> so that we could compare the results and debug the code?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: October 19, 2018,  9:01am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
22349,"When installing torchaudio , I met error &ldquo;compiler (c++) may be ABI-incompatible with PyTorch&rdquo;",2018-08-03T03:13:33.876Z,0,716,"<div class=""post"" itemprop=""articleBody""><NewLine><p>it shows :::</p><NewLine><p>Your compiler (c++) may be ABI-incompatible with PyTorch!     Please use a compiler that is ABI-compatible with GCC 4.9 and above.</p><NewLine><p>however, I have updated my GCC version ,and it’s already satisfied .<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/74fddb7a5f4d8b6e9163c9cc649dde87bb294248"" href=""https://discuss.pytorch.org/uploads/default/original/2X/7/74fddb7a5f4d8b6e9163c9cc649dde87bb294248.png"" title=""image.png""><img alt=""image"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/7/74fddb7a5f4d8b6e9163c9cc649dde87bb294248_2_10x10.png"" height=""96"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/7/74fddb7a5f4d8b6e9163c9cc649dde87bb294248_2_690x96.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/7/74fddb7a5f4d8b6e9163c9cc649dde87bb294248_2_690x96.png, https://discuss.pytorch.org/uploads/default/optimized/2X/7/74fddb7a5f4d8b6e9163c9cc649dde87bb294248_2_1035x144.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/7/74fddb7a5f4d8b6e9163c9cc649dde87bb294248_2_1380x192.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">1814×254 62.6 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/alphadl,(Liam),alphadl,"August 3, 2018,  3:13am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>How did you installed pytorch?<br/><NewLine>This test is designed to find if the compiler used to build the binaries is compatible with the compiler you use for your extensions.<br/><NewLine>Compiling pytorch from source will solve the problem for sure. Otherwise <a class=""mention"" href=""/u/smth"">@smth</a> should be able to tell you which compiler was used to build the package (depending on how you installed pytorch).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: August 3, 2018,  9:40am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
22269,Can i use deep learning to measure the similarity between two variable length voice sequences?,2018-08-02T07:40:27.872Z,0,457,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Usually we use DTW(Dynamic Time warpping) to measure the similarity between two variabel voice sequences. However DTW is time-cunsuming and not easy to run in the GPU since too much control in it. I wish to find a deep learning algorithm to measure the similarty. Anybody has ideas? Thanks:)</p><NewLine></div>",https://discuss.pytorch.org/u/Feixiang_Xu,(Feixiang Xu),Feixiang_Xu,"August 2, 2018,  7:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What kind of input are you using for DTW?<br/><NewLine>Some kind of mel frequency cepstral coefficient?<br/><NewLine>If the DTW works sufficiently well and you have the data, you could try to train a model to predict the DTW score for two voice sequences.<br/><NewLine>If your DTW results aren’t really good, you would have to get somehow the “ground truth” for your data.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: August 4, 2018,  2:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
22044,It seems my CNN model does not train,2018-07-31T00:17:55.721Z,2,612,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a CNN network that designed for speech emotion recognition task and I am trying to learn the network from the scratch in an end-to-end manner. I used concordance correlation coefficient (CCC) as objective function ( I am trying to maximize CCC or minimize 1-CCC). But, It seems my network does not learn. My input is audio sample extracted from video clip. There are totally 23 video clip with 5 minute duration that the audio frames of 14 person considered as training data. The audio sample extracted each 40 ms that results 7500 audio frame for each video clip. At 16 kHz sampling rate, this correspond to 640 sample in each frame. Each frame labeled with arousal and valence. The structure of my training network is as follow:</p><NewLine><pre><code>for i_batch, sample_batched in enumerate(train_loader):<NewLine>    data_time.update(time.time() - since)<NewLine>    ground_truth, audio =  sample_batched['landmark'],   sample_batched['audio'] <NewLine>    ground_truth = Variable (ground_truth, requires_grad = False)<NewLine>    audio = Variable (audio, requires_grad = True)               <NewLine>     # Define model.<NewLine>    optimizer.zero_grad()<NewLine>    prediction = model(audio)<NewLine>    mse_mean = 0<NewLine>    ccc_mean = 0<NewLine>    for i, name in enumerate(['arousal', 'valence']):<NewLine>        gt_single = ground_truth[:,i]<NewLine>        gt_single = gt_single.float()<NewLine>        pred_single = prediction[:, i]<NewLine>        ccc_loss = criterion1(pred_single , gt_single)<NewLine>        if i==0:<NewLine>            ccc_arousal = ccc_loss.item()<NewLine>        else:<NewLine>            ccc_valence = ccc_loss.item()<NewLine>        ccc_mean += ccc_loss<NewLine>    CCC_arousal.update(ccc_arousal, audio.size(0))           <NewLine>    CCC_valence.update(ccc_valence, audio.size(0))<NewLine>    losses.update((ccc_mean/2).item(), audio.size(0))<NewLine>    (ccc_mean/2).backward(retain_graph=True)<NewLine>    optimizer.step()<NewLine></code></pre><NewLine><p>the loss value (1-CCC) is around 1 from the beginning and does not change considerably after even 50 epoch. Can anyone help me about this issue?</p><NewLine></div>",https://discuss.pytorch.org/u/a.par.s,(Ali),a.par.s,"July 31, 2018, 12:17am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>One way to debug it is to try to overfit your network by training with only 1 mini batch repeatedly. If your network is designed properly, your network should converge very quickly and get a loss of nearly 0. Otherwise, there might be serious design issues in your network.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for reply. As your suggestion I used 1 mini batch with 500 sample and trained my network. Unfortunately, the loss did not change considerably. My prediction model is as follow:<br/><NewLine>class RecurrentModel(nn.Module):<br/><NewLine>def <strong>init</strong>(self, input_size = args.input_size, hidden_units=256, number_of_outputs=2):<br/><NewLine>super(RecurrentModel, self).<strong>init</strong>()<br/><NewLine>self.hidden = hidden_units<br/><NewLine>self. input_size = input_size<br/><NewLine>self. n_outputs = number_of_outputs<br/><NewLine>self.lstm = nn.LSTM(input_size = self.input_size, hidden_size = self.hidden, num_layers = 2, batch_first=True)<br/><NewLine>self.linear = nn.Linear (hidden_units, number_of_outputs)<br/><NewLine>def forward(self, net):<br/><NewLine>batch_size, seq_length, num_features = list (net.size())<br/><NewLine>outputs, _ = self.lstm (net)<br/><NewLine>prediction = self.linear(outputs[0])<br/><NewLine>return torch.reshape(prediction, (batch_size*seq_length, self.n_outputs))</p><NewLine><p>class AudioModel(nn.Module):<br/><NewLine>def <strong>init</strong>(self):<br/><NewLine>super(AudioModel, self).<strong>init</strong>()<br/><NewLine>self.drop = nn.Dropout()<br/><NewLine>self.conv1 = nn.Conv2d(1, 40, (1,20), padding=(0,9))<br/><NewLine>self.relu1 = nn.ReLU()<br/><NewLine>self.pool1 = nn.MaxPool2d((1, 2), (1, 2))<br/><NewLine>self.conv2 = nn.Conv2d(20, 40, (1,160), padding=(0,80) ) <span class=""hashtag"">#padding</span> = (0,38)<br/><NewLine>self.relu2 = nn.ReLU()<br/><NewLine>self.pool2 = nn.MaxPool2d((1, 10), (1, 10))<br/><NewLine>def forward(self, audio_frames, conv_filters = 40):</p><NewLine><pre><code>    batch_seq_length, num_features = list (audio_frames.size())<NewLine>    seq_length = args.seq_length<NewLine>    batch_size = args.batch_size<NewLine>    rnn = RecurrentModel ()<NewLine>    audio_input = torch.reshape(audio_frames, [1, 1, batch_size * seq_length, num_features])<NewLine>    net = self.drop(audio_input)<NewLine>    net = self.conv1(net)<NewLine>    net = self.relu1(net)<NewLine>    # Subsampling of the signal to 8KhZ.<NewLine>    net = self.pool1(net)<NewLine>    net = self.conv2(net)<NewLine>    net = self.relu2(net)<NewLine>    net = torch.reshape(net, (1,batch_size * seq_length,<NewLine>                           num_features // 2, conv_filters)) #(num_features // 3)+37<NewLine>    net = self.pool2(net)<NewLine>    net = torch.reshape(net, (batch_size, seq_length,  num_features // 2 * 4)) <NewLine>    net = rnn (net)<NewLine>    return net<NewLine></code></pre><NewLine><p>batch_size and seq_length are equal to 25 and 1, respectively.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>This suggests your network is not designed properly. I would suggest you simplify your model to a couple of layers to start with and train it with 1 mini batch. Once confirmed the network can reliably converge, you can start adding more layers.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/John_Smith; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/a.par.s; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/John_Smith; <NewLine> ,"REPLY_DATE 1: July 31, 2018,  2:05am; <NewLine> REPLY_DATE 2: July 31, 2018,  6:34am; <NewLine> REPLY_DATE 3: July 31, 2018, 11:16pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
22121,Multi gpu errors on Conv1D - float object cannot be interpreted as an integer,2018-07-31T16:59:55.551Z,0,1034,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>Firstly I followed tutorial for multi-gpu. <a href=""https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html</a></p><NewLine><p>It works for me. Unfortunately I tried to implement my own dataset and my own network architecture for multi-gpu training</p><NewLine><p>Single GPU - works fine (i think)<br/><NewLine>Multi gpu for pytorch 0.4.0 I had an error:</p><NewLine><p>RuntimeError: Expected tensor for argument <span class=""hashtag"">#1</span> ‘input’ to have the same device as tensor for argument <span class=""hashtag"">#2</span> ‘weight’; but device 1 does not equal 0 (while checking arguments for cudnn_convolution)</p><NewLine><p>For pytorch 0.4.1 I have now an error</p><NewLine><p>TypeError: ‘float’ object cannot be interpreted as an integer</p><NewLine><p>My nn module subclass forward method. Simple dilated convolution, batchnorm and relu.</p><NewLine><blockquote><NewLine><p>def forward(self, x):<br/><NewLine>out = self.dil_conv(x)<br/><NewLine>out = self.bn1(out)<br/><NewLine>out = self.relu(out)</p><NewLine></blockquote><NewLine><p>Error in this function. I use torch.nn.DataParallel(net)</p><NewLine><blockquote><NewLine><p>def forward(self, input):<br/><NewLine>return F.conv1d(input, self.weight, self.bias, self.stride,<br/><NewLine>self.padding, self.dilation, self.groups)</p><NewLine></blockquote><NewLine><p>Error for conv1d</p><NewLine><blockquote><NewLine><p>TypeError: ‘float’ object cannot be interpreted as an integer</p><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/Rafal_Pilarczyk,(Rafał Pilarczyk),Rafal_Pilarczyk,"July 31, 2018,  4:59pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok I think that previously by mistake I add padding as floating number, but it was casted to int so that’s why everything looked fine and now it has changed after upgrade. Probably during conv1d init there should be type validators or am I wrong?</p><NewLine><p>Now i have the same error. Shold I use inputs.cuda() or rather inputs.to(device) for Multi-gpu? In the example I can see inputs.to(device), but this approach gives me error below. I tried to find this kind of error, but solutions were not clear or didn’t work for me.</p><NewLine><blockquote><NewLine><p>RuntimeError: Expected tensor for argument <span class=""hashtag"">#1</span> ‘input’ to have the same device as tensor for argument <span class=""hashtag"">#2</span> ‘weight’; but device 1 does not equal 0 (while checking arguments for cudnn_convolution)</p><NewLine></blockquote><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Rafal_Pilarczyk; <NewLine> ,"REPLY_DATE 1: July 31, 2018,  7:53pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
20140,Unable to install pytorch: fatal error: &lsquo;atomic&rsquo; file not found #include &lt;atomic&gt;,2018-06-23T08:38:21.147Z,0,710,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey all, having some trouble with the pytorch audio installation.</p><NewLine><p>$ python setup.py install</p><NewLine><p>running install<br/><NewLine>running bdist_egg<br/><NewLine>running egg_info<br/><NewLine>writing torchaudio.egg-info/PKG-INFO<br/><NewLine>writing dependency_links to torchaudio.egg-info/dependency_links.txt<br/><NewLine>writing top-level names to torchaudio.egg-info/top_level.txt<br/><NewLine>reading manifest file ‘torchaudio.egg-info/SOURCES.txt’<br/><NewLine>writing manifest file ‘torchaudio.egg-info/SOURCES.txt’<br/><NewLine>installing library code to build/bdist.macosx-10.7-x86_64/egg<br/><NewLine>running install_lib<br/><NewLine>running build_py<br/><NewLine>running build_ext<br/><NewLine>building ‘_torch_sox’ extension<br/><NewLine>gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/anaconda3/include -arch x86_64 -I/anaconda3/include -arch x86_64 -I/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/anaconda3/include/python3.6m -c torchaudio/torch_sox.cpp -o build/temp.macosx-10.7-x86_64-3.6/torchaudio/torch_sox.o -DTORCH_EXTENSION_NAME=_torch_sox -std=c++11<br/><NewLine>In file included from torchaudio/torch_sox.cpp:1:<br/><NewLine>In file included from /anaconda3/lib/python3.6/site-packages/torch/lib/include/torch/torch.h:5:<br/><NewLine>In file included from /anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/ATen.h:5:<br/><NewLine>In file included from /anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Allocator.h:6:<br/><NewLine>/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Retainable.h:3:10: fatal error: ‘atomic’ file not found<br/><NewLine>#include <br/><NewLine>^~~~~~~~<br/><NewLine>1 error generated.<br/><NewLine>error: command ‘gcc’ failed with exit status 1</p><NewLine><p>This is after installing sox and cloning the repo. Is there something I am missing?<br/><NewLine>I also tried upgrading my g++ to g+±8, but with no luck.</p><NewLine><p>The corresponding issue is here: <a href=""https://github.com/pytorch/audio/issues/52"" rel=""nofollow noopener"">https://github.com/pytorch/audio/issues/52</a>. Just thought I could also ask the community.</p><NewLine><p>Any help appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/3210jr,(Ally Salim Jr),3210jr,"June 23, 2018,  8:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>A solution from <span class=""mention"">@soumith</span> :</p><NewLine><p>Try<br/><NewLine>MACOSX_DEPLOYMENT_TARGET=10.10 CC=clang CXX=clang++ python setup.py install</p><NewLine><p>Worked for me. Thanks.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/3210jr; <NewLine> ,"REPLY_DATE 1: June 23, 2018,  5:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
18575,About PyTorch&rsquo;s GPU Acceleration,2018-05-23T12:48:13.694Z,1,556,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When using the PyTorch, if you want to use the GPU only need to add ‘.cuda()’ in the corresponding place, or do you need other instructions in addition to this operation?<br/><NewLine>Why, when I only use ‘.cuda()’, I can be sure that he is running on the GPU, but I can’t feel the speed increase.</p><NewLine></div>",https://discuss.pytorch.org/u/vipchengrui,(Vipchengrui),vipchengrui,"May 23, 2018, 12:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You should make sure to call <code>.cuda()</code> or <code>.to(device='cuda')</code> on your data and your model.<br/><NewLine>Depending on the model architecture, pushing it to the GPU won’t give you any speedups.<br/><NewLine>This is usually the case for small models.</p><NewLine><p>Could you post your model architecture?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>U can also check the bottleneck of your code. Sometimes, the data loading or other preprocessing will occupy some time.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your answer</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/lkywk; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vipchengrui; <NewLine> ,"REPLY_DATE 1: May 23, 2018, 12:49pm; <NewLine> REPLY_DATE 2: May 23, 2018,  1:19pm; <NewLine> REPLY_DATE 3: June 11, 2018,  7:43am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
18345,How to convert audio (e.g. wav) to tensor and back?,2018-05-19T08:41:49.884Z,0,1046,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!<br/><NewLine>It is easy to convert audio (wav) to tensor using .load() command. But I am interested in reverse operation. Can someone help me?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/YK11,(Yeskendir ),YK11,"May 19, 2018,  8:41am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Isn’t it <code>torchaudio.save('amazing_sound.wav', sound, sample_rate)</code> ?</p><NewLine><p>By the way, you can find the doc here: <a href=""http://pytorch.org/audio/"">http://pytorch.org/audio/</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexis-jacq; <NewLine> ,"REPLY_DATE 1: May 19, 2018, 10:19pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
17868,How to split torchaudio dataset (e.g. VCTK) into training and testing sets?,2018-05-10T12:42:47.075Z,0,354,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How to split torchaudio dataset (e.g. VCTK) into training and testing sets?</p><NewLine></div>",https://discuss.pytorch.org/u/YK11,(Yeskendir ),YK11,"May 10, 2018, 12:42pm",,,,,
17503,Framewise Audio Data Loader for large Audio Corpus,2018-05-04T11:33:44.463Z,0,876,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to create a data loader for audio dataset. I have a bunch of audio files and those are listed in a csv file. To create a data loader, I need to inherit Dataset class and implement <strong>getitem</strong> and <strong>len</strong> methods. I want to load and process audio data on the fly and additionally my DNN model is not sequence wise. I need to load a set of audio files pre-process it and divide it into frames of constant size. At the time of input(to DNN) I need to take a minibatch of audio frames(not whole audio sequence).</p><NewLine><p><strong>getitem</strong> method takes an index and return the data frame corresponding to the index. All my audio file paths are in a CSV file and I want the Dataset loader to input it and load, pre-process and divide it into frames on the fly.</p><NewLine><p>What do i do to make ‘index’ variable of <strong>getitem</strong> correspond to audio data frames ?</p><NewLine><p>Please help.</p><NewLine></div>",https://discuss.pytorch.org/u/Aditay,(Aditay Tripathi),Aditay,"May 4, 2018, 11:33am",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can check <a href=""https://github.com/pytorch/audio/blob/master/torchaudio/datasets/yesno.py"" rel=""nofollow noopener"">yesno dataset implementation</a> for a general idea about how to build a custom dataset.</p><NewLine><p>Regarding paths in csv, you can maybe create a preprocessed file which contains the audio files as tensors similar to how they have done it in <a href=""https://github.com/pytorch/audio/blob/master/torchaudio/datasets/yesno.py#L117"" rel=""nofollow noopener"">here</a>.</p><NewLine><p>Well a simple trick is to do you own calculations and set your own length in the <code>__init__</code> of you dataset class, and return that length in the <code>__len__</code> method. And not worry about the <code>index</code> argument. So the <code>dataloader</code> will call your <code>__getitem__</code> <code>length-1</code> times and you can randomly pick the frames you want from your data, irrespective of the <code>index</code> value.</p><NewLine><p>As for trimming the audio, normalizing, stft calculation, etc, you can use any of the available <a href=""http://pytorch.org/audio/transforms.html"" rel=""nofollow noopener"">transforms</a>, or write your own transforms which you can pass to your custom dataset.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Nabarun_Goswami; <NewLine> ,"REPLY_DATE 1: May 9, 2018, 11:08am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> 
17342,Module &lsquo;torch&rsquo; has no attribute &lsquo;hann_window&rsquo; when importing torchaudio,2018-05-02T06:53:43.653Z,0,371,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have installed torchaudio on top of pytorch 0.3.1 for python3.5.<br/><NewLine>But When I try to import torchaudioI get the following error:</p><NewLine><p>AttributeError: module ‘torch’ has no attribute ‘hann_window’</p><NewLine><p>Please help.</p><NewLine></div>",https://discuss.pytorch.org/u/Aditay,(Aditay Tripathi),Aditay,"May 2, 2018,  6:53am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Pytorch 0.3.1 does not have the audio related functions. You should install 0.4, that has those required functions <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=5"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Nabarun_Goswami; <NewLine> ,"REPLY_DATE 1: May 9, 2018, 10:51am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
17333,Comparison of torch stft with librosa,2018-05-02T04:30:04.841Z,0,969,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I was evaluating the stft function in pytorch vs librosa. I found that the precision is only upto 2 decimal places. I am wondering if that can be improved to atleast 4-5 decimal places.</p><NewLine><p>Also, regarding the speed of execution,</p><NewLine><pre><code class=""lang-auto"">librosa is ~2x faster than pytorch on CPU (Didn't expect this, thought CPU times should be similar.)<NewLine>pytorch is ~4x faster than librosa on GPU (Expected this.)<NewLine><NewLine>Rig: TitanX pascal, 4-core CPU, pytorch 0.5.0a0+8fbab83<NewLine></code></pre><NewLine><p>Do these numbers look ok? Or is there a possibility I am doing something wrong?</p><NewLine><p>Regards<br/><NewLine>Nabarun</p><NewLine></div>",https://discuss.pytorch.org/u/Nabarun_Goswami,(Nabarun Goswami),Nabarun_Goswami,"May 2, 2018,  4:30am",2 Likes,,,,
