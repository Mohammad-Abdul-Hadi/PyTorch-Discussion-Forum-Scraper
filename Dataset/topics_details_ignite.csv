id,title,created_at,reply_count,views,description,creator_link,creator_name,creator_alias,post_date,post_likes,replies,repliers_links,reply_dates,reply_likes
45318,About the ignite category,2019-05-15T13:13:40.472Z,0,198,"<div class=""post"" itemprop=""articleBody""><NewLine><p>(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)</p><NewLine><p>Use the following paragraphs for a longer description, or to establish category guidelines or rules:</p><NewLine><ul><NewLine><li><NewLine><p>Why should people use this category? What is it for?</p><NewLine></li><NewLine><li><NewLine><p>How exactly is this different than the other categories we already have?</p><NewLine></li><NewLine><li><NewLine><p>What should topics in this category generally contain?</p><NewLine></li><NewLine><li><NewLine><p>Do we need this category? Can we merge with another category, or subcategory?</p><NewLine></li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/smth,,smth,"May 15, 2019,  1:13pm",,,,,
96695,Evaluate multiple models with one evaluator results weird metrics,2020-09-18T05:50:24.645Z,1,28,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello.</p><NewLine><p>I have several similar models.(with little variation of some hyperparameters)<br/><NewLine>I am trying to evaluate these models for same dataset with one evaluator, to reduce eval time.</p><NewLine><p>So I made custom evaluator like below:</p><NewLine><pre><code class=""lang-auto"">metrics_group = {<NewLine>    experiment_name: {'Top-1': Accuracy(output_transform=lambda output: (output[0][experiment_name], output[1][experiment_name])),<NewLine>                      'Top-5': TopKCategoricalAccuracy(output_transform=lambda output: (output[0][experiment_name], output[1][experiment_name]))}<NewLine>    for experiment_name in experiment_settings.keys()<NewLine>}<NewLine><NewLine>def _inference(engine, batch):<NewLine>    data = {experiment_name: _prepare_batch(batch,<NewLine>                                            device=experiment_settings[experiment_name]['device_config']['host'],<NewLine>                                            non_blocking=True)<NewLine>            for experiment_name in experiment_components.keys()}<NewLine>    y_preds = {}<NewLine>    with torch.no_grad():<NewLine>        for experiment_name, experiment_component in experiment_components.items():<NewLine>            experiment_component['model'].eval()<NewLine>            y_preds[experiment_name] = experiment_component['model'](data[experiment_name][0]) # Is this work simultaneously? IDK...<NewLine>        return y_preds, {experiment_name: xy[1] for experiment_name, xy in data.items()}<NewLine><NewLine>evaluator = Engine(_inference)<NewLine><NewLine>for experiment_name, metrics in metrics_group.items():<NewLine>    for metric_name, metric in metrics.items():<NewLine>        metric.attach(evaluator, '{0}/{1}'.format(experiment_name, metric_name))<NewLine></code></pre><NewLine><p>As you can see, I attached  ‘Top-1’ and ‘Top-5’ to measure every model but the results says:</p><NewLine><pre><code class=""lang-auto"">print(evaluator.state.metrics)<NewLine>{'A/Top-1': 0.1328125,<NewLine> 'A/Top-5': 0.6015625,<NewLine> 'B/Top-1': 0.1328125,<NewLine> 'B/Top-5': 0.6015625,<NewLine> 'C/Top-1': 0.1328125,<NewLine> 'C/Top-5': 0.6015625,<NewLine> 'D/Top-1': 0.1328125,<NewLine> 'D/Top-5': 0.6015625}<NewLine></code></pre><NewLine><p>I confused with this result so I checked evaluator’s output, but:</p><NewLine><pre><code class=""lang-auto"">print(evaluator.state.output[0]['A']<NewLine>tensor([[-0.3352, -2.0896, -0.1886,  ...,  0.3233,  0.5214, -0.0945],<NewLine>        [-0.3316, -2.0419, -0.1706,  ...,  0.2498,  0.5802, -0.0909],<NewLine>        [-0.3056, -2.0395, -0.2438,  ...,  0.2266,  0.5328, -0.1037],<NewLine>        ...,<NewLine>        [-0.3001, -2.0332, -0.2573,  ...,  0.3248,  0.5248, -0.0653],<NewLine>        [-0.3233, -2.0452, -0.1502,  ...,  0.2362,  0.5626, -0.0756],<NewLine>        [-0.3304, -2.0787, -0.1769,  ...,  0.2427,  0.5589, -0.0379]],<NewLine>       device='cuda:0')<NewLine><NewLine>print(evaluator.state.output[0]['B'])<NewLine>tensor([[ 0.9059, -0.0701,  2.3905,  ...,  0.9909,  2.3744,  0.6785],<NewLine>        [ 0.8492, -0.0840,  2.3493,  ...,  0.9192,  2.3537,  0.6840],<NewLine>        [ 0.8844, -0.1049,  2.3237,  ...,  0.8993,  2.3218,  0.6488],<NewLine>        ...,<NewLine>        [ 0.8758, -0.1526,  2.3674,  ...,  1.0029,  2.3428,  0.6295],<NewLine>        [ 0.8461, -0.0652,  2.3746,  ...,  0.9019,  2.2553,  0.6627],<NewLine>        [ 0.9023, -0.0956,  2.3242,  ...,  0.9092,  2.2694,  0.6629]],<NewLine>       device='cuda:0')<NewLine><NewLine>print(torch.eq(evaluator.state.output[0]['A'], evaluator.state.output[0]['B']))<NewLine>tensor([[False, False, False,  ..., False, False, False],<NewLine>        [False, False, False,  ..., False, False, False],<NewLine>        [False, False, False,  ..., False, False, False],<NewLine>        ...,<NewLine>        [False, False, False,  ..., False, False, False],<NewLine>        [False, False, False,  ..., False, False, False],<NewLine>        [False, False, False,  ..., False, False, False]], device='cuda:0')<NewLine></code></pre><NewLine><p>I think there’s no problem with  ‘predicting’ phase.<br/><NewLine>Is there a problem with attaching metrics? Any suggestion will be welcome.</p><NewLine></div>",https://discuss.pytorch.org/u/FruitVinegar,(NHK),FruitVinegar,"September 18, 2020,  5:50am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/fruitvinegar"">@FruitVinegar</a> I think the problem is related to output transform definition with lambdas. Lambdas can not store <code>experiment_name</code> internally, so all <code>output_transform</code> fetch the last <code>experiment_name</code>.<br/><NewLine>For example, take a look:</p><NewLine><pre><code class=""lang-python"">ot_list = []<NewLine><NewLine>for n in [""a"", ""b"", ""c"", ""d""]:<NewLine>    ot_list.append(lambda _: print(n))<NewLine><NewLine>for o in ot_list:<NewLine>    o(None)<NewLine>&gt; d<NewLine>&gt; d<NewLine>&gt; d<NewLine>&gt; d<NewLine></code></pre><NewLine><p>In order to do what you would like I’d use functools.partial</p><NewLine><pre><code class=""lang-python"">from functools import partial<NewLine><NewLine>ot_list = []<NewLine><NewLine>def ot_func(output, exp_name):<NewLine>    print(output, exp_name)<NewLine><NewLine><NewLine>for n in [""a"", ""b"", ""c"", ""d""]:<NewLine>    ot_list.append(partial(ot_func, exp_name=n))<NewLine><NewLine>for o in ot_list:<NewLine>    o(1)<NewLine><NewLine>&gt; 1 a<NewLine>&gt; 1 b<NewLine>&gt; 1 c<NewLine>&gt; 1 d<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh my. The problem was totally not related to ignite.<br/><NewLine>Thanks for your help!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/FruitVinegar; <NewLine> ,"REPLY_DATE 1: September 18, 2020,  7:29am; <NewLine> REPLY_DATE 2: September 18, 2020,  7:29am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
91221,How access inputs in custom Ignite Metric?,2020-07-31T13:02:11.283Z,5,78,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have implemented a custom Ignite Metric based on this <a href=""https://pytorch.org/ignite/metrics.html#how-to-create-a-custom-metric"" rel=""nofollow noopener"">tutorial</a>.</p><NewLine><pre><code class=""lang-auto"">def update(self, output):<NewLine>        y_pred, y = output<NewLine></code></pre><NewLine><p>How to access inputs (x values) in the update function?</p><NewLine></div>",https://discuss.pytorch.org/u/odats,(Oleh Dats),odats,"July 31, 2020,  1:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The simplest way to add <code>x</code> is to pass it into the output as <code>(y_pred, y, x)</code>.</p><NewLine><p>Another way (more clearly implemented) is to reimplement <code>iteration_completed()</code> method : <a href=""https://github.com/pytorch/ignite/blob/6faa6ac1e3a46c79e0dfcfd976439b86329717b0/ignite/metrics/metric.py#L198"" rel=""nofollow noopener"">https://github.com/pytorch/ignite/blob/6faa6ac1e3a46c79e0dfcfd976439b86329717b0/ignite/metrics/metric.py#L198</a></p><NewLine><p>and pass to <code>update</code> method everything it needs without “hiding” things: output, input, etc…</p><NewLine><p>Let me know if it answers your question. Thanks</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>how to pass <code>x</code> into the output? And not to break other metrics.</p><NewLine><pre><code class=""lang-auto"">evaluator = create_supervised_evaluator(<NewLine>        model, metrics={<NewLine>            ""loss"": Loss(criterion),<NewLine>            ""accuracy"": Accuracy(), <NewLine>            ""accuracy_pix2pix"": PixelToPixelAccuracy()}, # custom metric to extend<NewLine>    )<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Pass all necessary data y, predictions, x to all metrics and use <code>output_transform</code> to filter out required args to existing metrics.</p><NewLine><p>Probably, more elegant way to do this is :</p><NewLine><ul><NewLine><li>return output as a dictionary with keys like “y” - target, “y_pred” - predictions, “x” for input x.</li><NewLine><li>override <code>_required_output_keys</code> for your custom metric as a tuple <code>(""y"", ""y_pred"", ""x"")</code><NewLine></li><NewLine></ul><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/ignite/blob/6faa6ac1e3a46c79e0dfcfd976439b86329717b0/ignite/metrics/metric.py#L129"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/ignite/blob/6faa6ac1e3a46c79e0dfcfd976439b86329717b0/ignite/metrics/metric.py#L129"" rel=""nofollow noopener"" target=""_blank"">pytorch/ignite/blob/6faa6ac1e3a46c79e0dfcfd976439b86329717b0/ignite/metrics/metric.py#L129</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""119"" style=""counter-reset: li-counter 118 ;""><NewLine><li>Args:</li><NewLine><li>    output_transform (callable, optional): a callable that is used to transform the</li><NewLine><li>        :class:`~ignite.engine.engine.Engine`'s ``process_function``'s output into the</li><NewLine><li>        form expected by the metric. This can be useful if, for example, you have a multi-output model and</li><NewLine><li>        you want to compute the metric with respect to one of the outputs.</li><NewLine><li>        By default, metrics require the output as ``(y_pred, y)`` or ``{'y_pred': y_pred, 'y': y}``.</li><NewLine><li>    device (str of torch.device, optional): optional device specification for internal storage.</li><NewLine><li><NewLine></li><li>""""""</li><NewLine><li><NewLine></li><li class=""selected"">_required_output_keys = (""y_pred"", ""y"")</li><NewLine><li><NewLine></li><li>def __init__(</li><NewLine><li>    self, output_transform: Callable = lambda x: x, device: Optional[Union[str, torch.device]] = None,</li><NewLine><li>):</li><NewLine><li>    self._output_transform = output_transform</li><NewLine><li><NewLine></li><li>    # Check device if distributed is initialized:</li><NewLine><li>    if idist.get_world_size() &gt; 1:</li><NewLine><li><NewLine></li><li>        # check if reset and update methods are decorated. Compute may not be decorated</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Thus, I think it can be possible to fetch needed args without using output transform…</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you please provide short code example?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here is a colab with an example : <a href=""https://colab.research.google.com/drive/1-EL_YGLPzEPIw_6jU-tWRRLXnx0lN106?usp=sharing"" rel=""nofollow noopener"">https://colab.research.google.com/drive/1-EL_YGLPzEPIw_6jU-tWRRLXnx0lN106?usp=sharing</a></p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine>from ignite.metrics import Metric, Accuracy<NewLine>from ignite.engine import create_supervised_evaluator<NewLine><NewLine><NewLine>class CustomMetric(Metric):<NewLine><NewLine>    _required_output_keys = (""y_pred"", ""y"", ""x"")<NewLine><NewLine>    def __init__(self, *args, **kwargs):<NewLine>        super().__init__(*args, **kwargs)<NewLine>    <NewLine>    def update(self, output):<NewLine>        print(""CustomMetric: output="")<NewLine>        for i, o in enumerate(output):<NewLine>            print(i, o.shape)<NewLine><NewLine>    def reset(self):<NewLine>        pass<NewLine><NewLine>    def compute(self):<NewLine>        return 0.0<NewLine><NewLine><NewLine><NewLine>model = nn.Linear(10, 3)<NewLine><NewLine>metrics = {<NewLine>    ""Accuracy"": Accuracy(),<NewLine>    ""CustomMetric"": CustomMetric()<NewLine>}<NewLine><NewLine>evaluator = create_supervised_evaluator(<NewLine>    model, <NewLine>    metrics=metrics, <NewLine>    output_transform=lambda x, y, y_pred: {""x"": x, ""y"": y, ""y_pred"": y_pred}<NewLine>)<NewLine><NewLine>data = [<NewLine>    (torch.rand(4, 10), torch.randint(0, 3, size=(4, ))),<NewLine>    (torch.rand(4, 10), torch.randint(0, 3, size=(4, ))),<NewLine>    (torch.rand(4, 10), torch.randint(0, 3, size=(4, )))<NewLine>]<NewLine>res = evaluator.run(data)<NewLine></code></pre><NewLine><p>Output:</p><NewLine><pre><code class=""lang-auto"">CustomMetric: output=<NewLine>0 torch.Size([4, 3])<NewLine>1 torch.Size([4])<NewLine>2 torch.Size([4, 10])<NewLine>CustomMetric: output=<NewLine>0 torch.Size([4, 3])<NewLine>1 torch.Size([4])<NewLine>2 torch.Size([4, 10])<NewLine>CustomMetric: output=<NewLine>0 torch.Size([4, 3])<NewLine>1 torch.Size([4])<NewLine>2 torch.Size([4, 10])<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/odats"">@odats</a> since v0.4.2 which will be released Sep 22-25th, please consider using <code>required_output_keys</code> as a public class attribute instead of a private one <code>_required_output_keys</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/odats; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/odats; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: July 31, 2020,  1:46pm; <NewLine> REPLY_DATE 2: July 31, 2020,  4:37pm; <NewLine> REPLY_DATE 3: July 31, 2020,  5:03pm; <NewLine> REPLY_DATE 4: July 31, 2020,  5:08pm; <NewLine> REPLY_DATE 5: August 3, 2020,  8:15am; <NewLine> REPLY_DATE 6: September 16, 2020,  2:05pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> 
94541,Problems with Dice Loss in Pytorch Ignite,2020-08-30T02:10:32.680Z,0,61,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am having issues with Dice Loss and Pytorch Ignite. I am trying to reproduce the result of <a href=""https://github.com/ternaus/TernausNet"" rel=""nofollow noopener"">Ternausnet</a> using dice loss but my gradients keep being zero and loss just does not improve or shows very strange results (negative, nan, etc). I am not sure where to look for a possible source of the issue. Below is the code for DiceLoss:</p><NewLine><pre><code class=""lang-auto"">from torch import nn<NewLine>from torch.nn import functional as F<NewLine><NewLine># from catalyst.contrib.nn import DiceLoss<NewLine>import torch<NewLine><NewLine><NewLine>class DiceLoss(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine><NewLine>    def forward(self, logits, targets, eps=1, threshold=None):<NewLine><NewLine>        # comment out if your model contains a sigmoid or<NewLine>        # equivalent activation layer<NewLine>        proba = torch.sigmoid(logits)<NewLine>        proba = proba.view(proba.shape[0], 1, -1)<NewLine>        targets = targets.view(targets.shape[0], 1, -1)<NewLine>        if threshold:<NewLine>            proba = (proba &gt; threshold).float()<NewLine>        # flatten label and prediction tensors<NewLine><NewLine>        intersection = torch.sum(proba * targets, dim=1)<NewLine>        summation = torch.sum(proba, dim=1) + torch.sum(targets, dim=1)<NewLine>        dice = (2.0 * intersection + eps) / (summation + eps)<NewLine>        # print(intersection, summation, dice)<NewLine>        return (1 - dice).mean()<NewLine><NewLine></code></pre><NewLine><p>and here is the model (unet11 backbone code taken from ternausnet):</p><NewLine><pre><code class=""lang-auto"">import pytorch_lightning as pl<NewLine>import torch<NewLine>from torch import nn<NewLine>from torchvision import models<NewLine>from carvana_unet.utils import DiceLoss<NewLine><NewLine>def conv3x3(in_: int, out: int) -&gt; nn.Module:<NewLine>    return nn.Conv2d(in_, out, 3, padding=1)<NewLine><NewLine><NewLine>class ConvRelu(nn.Module):<NewLine>    def __init__(self, in_: int, out: int) -&gt; None:<NewLine>        super().__init__()<NewLine>        self.conv = conv3x3(in_, out)<NewLine>        self.activation = nn.ReLU(inplace=True)<NewLine><NewLine>    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:<NewLine>        x = self.conv(x)<NewLine>        x = self.activation(x)<NewLine>        return x<NewLine><NewLine><NewLine>class DecoderBlock(nn.Module):<NewLine>    def __init__(<NewLine>        self, in_channels: int, middle_channels: int, out_channels: int<NewLine>    ) -&gt; None:<NewLine>        super().__init__()<NewLine><NewLine>        self.block = nn.Sequential(<NewLine>            ConvRelu(in_channels, middle_channels),<NewLine>            nn.ConvTranspose2d(<NewLine>                middle_channels,<NewLine>                out_channels,<NewLine>                kernel_size=3,<NewLine>                stride=2,<NewLine>                padding=1,<NewLine>                output_padding=1,<NewLine>            ),<NewLine>            nn.ReLU(inplace=True),<NewLine>        )<NewLine><NewLine>    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:<NewLine>        return self.block(x)<NewLine><NewLine><NewLine>class UNet11Lightning(pl.LightningModule):<NewLine>    def __init__(<NewLine>        self, num_filters: int = 32, pretrained: bool = True, loss_fn=DiceLoss<NewLine>    ) -&gt; None:<NewLine>        """"""<NewLine>        Args:<NewLine>            num_filters:<NewLine>            pretrained:<NewLine>                False - no pre-trained network is used<NewLine>                True  - encoder is pre-trained with VGG11<NewLine>        """"""<NewLine>        super().__init__()<NewLine>        self.loss_fn = loss_fn()<NewLine>        self.pool = nn.MaxPool2d(2, 2)<NewLine><NewLine>        self.encoder = models.vgg11(pretrained=pretrained).features<NewLine><NewLine>        self.relu = self.encoder[1]<NewLine>        self.conv1 = self.encoder[0]<NewLine>        self.conv2 = self.encoder[3]<NewLine>        self.conv3s = self.encoder[6]<NewLine>        self.conv3 = self.encoder[8]<NewLine>        self.conv4s = self.encoder[11]<NewLine>        self.conv4 = self.encoder[13]<NewLine>        self.conv5s = self.encoder[16]<NewLine>        self.conv5 = self.encoder[18]<NewLine><NewLine>        self.center = DecoderBlock(<NewLine>            num_filters * 8 * 2, num_filters * 8 * 2, num_filters * 8<NewLine>        )<NewLine>        self.dec5 = DecoderBlock(<NewLine>            num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 8<NewLine>        )<NewLine>        self.dec4 = DecoderBlock(<NewLine>            num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 4<NewLine>        )<NewLine>        self.dec3 = DecoderBlock(<NewLine>            num_filters * (8 + 4), num_filters * 4 * 2, num_filters * 2<NewLine>        )<NewLine>        self.dec2 = DecoderBlock(<NewLine>            num_filters * (4 + 2), num_filters * 2 * 2, num_filters<NewLine>        )<NewLine>        self.dec1 = ConvRelu(num_filters * (2 + 1), num_filters)<NewLine><NewLine>        self.final = nn.Conv2d(num_filters, 1, kernel_size=1)<NewLine><NewLine>    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:<NewLine>        conv1 = self.relu(self.conv1(x))<NewLine>        conv2 = self.relu(self.conv2(self.pool(conv1)))<NewLine>        conv3s = self.relu(self.conv3s(self.pool(conv2)))<NewLine>        conv3 = self.relu(self.conv3(conv3s))<NewLine>        conv4s = self.relu(self.conv4s(self.pool(conv3)))<NewLine>        conv4 = self.relu(self.conv4(conv4s))<NewLine>        conv5s = self.relu(self.conv5s(self.pool(conv4)))<NewLine>        conv5 = self.relu(self.conv5(conv5s))<NewLine><NewLine>        center = self.center(self.pool(conv5))<NewLine><NewLine>        dec5 = self.dec5(torch.cat([center, conv5], 1))<NewLine>        dec4 = self.dec4(torch.cat([dec5, conv4], 1))<NewLine>        dec3 = self.dec3(torch.cat([dec4, conv3], 1))<NewLine>        dec2 = self.dec2(torch.cat([dec3, conv2], 1))<NewLine>        dec1 = self.dec1(torch.cat([dec2, conv1], 1))<NewLine>        return self.final(dec1)<NewLine><NewLine>    def configure_optimizers(self):<NewLine>        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)<NewLine>        return optimizer<NewLine><NewLine>    def training_step(self, batch, batch_idx):<NewLine>        x, y = batch[""features""].cuda(), batch[""target""].cuda()<NewLine>        y_hat = self(x)<NewLine>        loss = self.loss_fn(y_hat, y)<NewLine>        result = pl.TrainResult(loss)<NewLine>        result.log(""train_loss"", loss)<NewLine>        return {""loss"": loss}<NewLine><NewLine>    def validation_step(self, batch, batch_idx):<NewLine>        x, y = batch[""features""].cuda(), batch[""target""].cuda()<NewLine>        y_hat = self(x)<NewLine>        loss = self.loss_fn(y_hat, y)<NewLine>        result = pl.EvalResult(checkpoint_on=loss)<NewLine>        result.log(""val_loss"", loss, prog_bar=True, on_step=True)<NewLine>        tensorboard_log = {""val_loss"": loss}<NewLine>        return {""loss"": loss, ""log"": tensorboard_log}<NewLine><NewLine>    def test_step(self, batch, batch_idx):<NewLine>        x = batch[""features""].cuda()<NewLine>        y_hat = self(x)<NewLine>        return torch.nn.functional.sigmoid(y_hat)<NewLine><NewLine></code></pre><NewLine><p>can anybody help me find the source of the issue or point me into the direction of where to look? I have tried other losses (like BCE with logits which was decreasing into -inf) and nothing seems to work.</p><NewLine></div>",https://discuss.pytorch.org/u/notacode,,notacode,"August 30, 2020,  9:13am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Moving to <code>Ignite</code> category for better visibility. CC <a class=""mention"" href=""/u/vfdev-5"">@vfdev-5</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/notacode"">@notacode</a> checkout <a href=""https://pytorch.org/ignite/quickstart.html"" rel=""nofollow noopener"">pytorch-ignite quick-start</a> on how to use pytorch-ignite without <code>pl.LightningModule</code> <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: August 30, 2020,  9:14am; <NewLine> REPLY_DATE 2: August 30, 2020, 10:32am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
93090,How to pass additional parameters to forward function during training and validation?,2020-08-17T14:01:35.269Z,0,48,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I use Ignite to train LSTM model based on <a href=""https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb"" rel=""nofollow noopener"">this tutorial</a>.</p><NewLine><p><code>def forward(self, source, target, teacher_force_ratio=0.5):</code></p><NewLine><p>How to pass <code>target</code> and <code>teacher_force_ratio</code> during training (<code>teacher_force_ratio=0.5</code>) and validation process (<code>teacher_force_ratio=0)</code>?</p><NewLine><p>I have checked <a href=""https://pytorch.org/ignite/_modules/ignite/engine.html#create_supervised_trainer"" rel=""nofollow noopener"">create_supervised_trainer</a> and could not find such possibility.</p><NewLine><p>Should I extend Dataset class:<br/><NewLine><code>return (source, target) -&gt; return ((source, target, teacher_force_ratio), target)</code></p><NewLine></div>",https://discuss.pytorch.org/u/odats,(Oleh Dats),odats,"August 17, 2020,  2:03pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/odats"">@odats</a></p><NewLine><p>You can create a trainer with a custom <code>train_step</code> (or <code>update_fn</code>) as here:<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/ignite/concepts.html#engine"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/ignite/concepts.html#engine</a></p><NewLine><p>HTH</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: August 18, 2020,  6:10am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
92253,How to set the same random seed for all workers?,2020-08-10T08:56:15.098Z,12,104,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Without setting a random seed the data loader returns the same random data for each epoch:<br/><NewLine><code>epoch 1: worker1-&gt;[2], worker2-&gt;[2], epoch 2:  worker1-&gt;[2], worker2-&gt;[2],...</code></p><NewLine><p>When I set a random seed in worker_init_fn function I get random data for each worker:<br/><NewLine><code>epoch 1: worker1-&gt;[2], worker2-&gt;[4], epoch 2:  worker1-&gt;[7], worker2-&gt;[6],...</code></p><NewLine><p>How to set a random seed to get a new random data each epoch but the same random data for each worker?<br/><NewLine><code>epoch 1: worker1-&gt;[2], worker2-&gt;[2], epoch 2:  worker1-&gt;[7], worker2-&gt;[7],...</code></p><NewLine><pre><code class=""lang-auto"">def worker_init_fn(worker_id):                         <NewLine>    print(torch.utils.data.get_worker_info().seed)<NewLine>    print(torch.initial_seed())<NewLine>    # set seed<NewLine>    np.random.seed(int(torch.utils.data.get_worker_info().seed)%(2**32-1))<NewLine><NewLine>train_loader = DataLoader(DatasetRandom(), batch_size=2, num_workers=4, worker_init_fn=worker_init_fn)<NewLine></code></pre><NewLine><p>I use Ignite for training.</p><NewLine></div>",https://discuss.pytorch.org/u/odats,(Oleh Dats),odats,"August 10, 2020, 10:27am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/odats"">@odats</a> can you set a seed each epoch for that ?</p><NewLine><pre><code class=""lang-python"">@trainer.on(Events.EPOCH_STARTED)<NewLine>def set_epoch_seed():<NewLine>    set_seed(trainer.state.epoch)<NewLine></code></pre><NewLine><p>If this does not work for you, please provide a minimal code snippet to see the problem.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>set_seed</code> is no defined, where I can find this method?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can use <a href=""https://github.com/pytorch/ignite/blob/master/ignite/utils.py#L142"" rel=""nofollow noopener"">ignite.utils.manual_seed</a>, but I wanted to say that set the seed of your random generator.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">@trainer.on(Events.EPOCH_STARTED)<NewLine>def set_epoch_seed():<NewLine>    ignite.utils.manual_seed(trainer.state.epoch)<NewLine></code></pre><NewLine><p>Yes, it works. But it has 2 issues:</p><NewLine><ol><NewLine><li>validation data loader returns the same random values as training loader</li><NewLine><li>it always returns the same values (because seed values are always the same 1,2,3)</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please, provide a minimal code snippet to run it and see the problem in details</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""92253"" data-username=""odats""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/odats/40/21970_2.png"" width=""20""/> odats:</div><NewLine><blockquote><NewLine><p>Yes, it works. But it has 2 issues:</p><NewLine><ol><NewLine><li>validation data loader returns the same random values as training loader</li><NewLine><li>it always returns the same values (because seed values are always the same 1,2,3)</li><NewLine></ol><NewLine></blockquote><NewLine></aside><NewLine><pre><code class=""lang-auto"">class DatasetRandom(Dataset):   <NewLine>    def __len__(self):<NewLine>        return 8<NewLine>    def __getitem__(self, idx):<NewLine>        #print(torch.utils.data.get_worker_info().seed)<NewLine>        return (np.random.randint(1000, size=1), idx%2)<NewLine><NewLine>def worker_init_fn(worker_id):                         <NewLine>    #print(torch.utils.data.get_worker_info().seed)<NewLine>    #print(torch.initial_seed())<NewLine>    np.random.seed(int(torch.utils.data.get_worker_info().seed)%(2**32-1))<NewLine><NewLine>@trainer.on(Events.EPOCH_STARTED)<NewLine>def set_epoch_seed():<NewLine>    #print('seed', torch.initial_seed())<NewLine>    # manual_seed(trainer.state.epoch)<NewLine>    manual_seed(int(torch.initial_seed())%(2**32-1))<NewLine><NewLine>train_loader = DataLoader(DatasetRandom(), batch_size=1, num_workers=2)<NewLine>val_loader = DataLoader(DatasetRandom(), batch_size=1, num_workers=2, worker_init_fn=worker_init_fn)<NewLine></code></pre><NewLine><ol><NewLine><li>add worker_init_fn=worker_init_fn to val_loader</li><NewLine><li>manual_seed(int(torch.initial_seed())%(2**32-1))</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have found issue and updated my code. I have posted working solution. Thank you for the support.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I have found issue and updated my code. I have posted working solution. Thank you for the support.</p><NewLine></blockquote><NewLine><p><a class=""mention"" href=""/u/odats"">@odats</a> great !</p><NewLine><p>However, seeing your <code>DatasetRandom</code> implementation, it seems a bit weird to have the same random data each epoch as you said it in the very begining:</p><NewLine><blockquote><NewLine><p>Without setting a random seed the data loader returns the same random data for each epoch:<br/><NewLine><code>epoch 1: worker1-&gt;[2], worker2-&gt;[2], epoch 2:  worker1-&gt;[2], worker2-&gt;[2],...</code></p><NewLine></blockquote><NewLine><p>Probably, somewhere you should have some unwanted random seed synchonization. Which ignite version you are using, btw ?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>It is about pytorch. I thought Ignine has some elegant solution for this behavior. As you suggested add callback<code> @trainer.on(Events.EPOCH_STARTED)</code></p><NewLine><pre><code class=""lang-auto"">class DatasetRandom(Dataset):    <NewLine>    def __len__(self):<NewLine>        return 2<NewLine>    def __getitem__(self, idx):<NewLine>        return (np.random.randint(1000, size=1), idx%2)<NewLine><NewLine>train_loader = DataLoader(DatasetRandom(), batch_size=1, num_workers=2)<NewLine><NewLine>for e in range(2): <NewLine>    print('epoch',e)<NewLine>    for i, (images, labels) in enumerate(train_loader):<NewLine>        print(i, images, labels)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">epoch 0<NewLine>0 tensor([[492]]) tensor([0])<NewLine>1 tensor([[492]]) tensor([1])<NewLine>epoch 1<NewLine>0 tensor([[492]]) tensor([0])<NewLine>1 tensor([[492]]) tensor([1])<NewLine></code></pre><NewLine><p>Ignite: 0.4.1, PyTorch: 1.4.0</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I thought Ignine has some elegant solution for this behavior.</p><NewLine></blockquote><NewLine><p>In v0.3.0 we had something similar to <code>set_epoch_seed</code> in the Engine automatically, but we found that it has a lot of side effects like you see with evaluator etc.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Btw, seeing the implementation of DataLoader’s  worker loop : <a href=""https://github.com/pytorch/pytorch/blob/23db54acdf455866206d2de36c33fb75e177cb4c/torch/utils/data/_utils/worker.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/23db54acdf455866206d2de36c33fb75e177cb4c/torch/utils/data/_utils/worker.py</a></p><NewLine><p>A seed is setup for torch and python random (not numpy random) to randomize data each time dataloader iterator is created, so if you replace your <code>np.random.randint(1000, size=1)</code> by <code>random.randint(0, 1000)</code>, data will be random for each epoch.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, but for numpy I should go with original solution?</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>It depends on what is behind <code>np.randint</code> calls in your real case. I was thinking about data augmentations that can be parametrized with numpy, so if you have a control of that it would be more simple to replace randomness generation…</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/odats; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/odats; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/odats; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/odats; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/odats; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/odats; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: August 10, 2020,  2:33pm; <NewLine> REPLY_DATE 2: August 10, 2020, 12:49pm; <NewLine> REPLY_DATE 3: August 10, 2020,  1:01pm; <NewLine> REPLY_DATE 4: August 10, 2020,  1:33pm; <NewLine> REPLY_DATE 5: August 10, 2020,  2:08pm; <NewLine> REPLY_DATE 6: August 10, 2020,  2:33pm; <NewLine> REPLY_DATE 7: August 10, 2020,  2:34pm; <NewLine> REPLY_DATE 8: August 10, 2020,  2:49pm; <NewLine> REPLY_DATE 9: August 10, 2020,  2:58pm; <NewLine> REPLY_DATE 10: August 10, 2020,  3:28pm; <NewLine> REPLY_DATE 11: August 10, 2020,  3:46pm; <NewLine> REPLY_DATE 12: August 10, 2020,  4:17pm; <NewLine> REPLY_DATE 13: August 10, 2020,  4:56pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: 1 Like; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> 
90919,Distributed error this morning,2020-07-29T05:11:35.776Z,0,56,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m not sure why but today when I run the same code that I ran last night I get this error:</p><NewLine><pre><code class=""lang-auto"">   from ignite.utils import setup_logger<NewLine>  File ""/home/mia/anaconda3/envs/idk/lib/python3.7/site-packages/ignite/__init__.py"", line 2, in &lt;module&gt;<NewLine>    import ignite.distributed<NewLine>  File ""/home/user/anaconda3/envs/idk/lib/python3.7/site-packages/ignite/distributed/__init__.py"", line 1, in &lt;module&gt;<NewLine>    from ignite.distributed.auto import *<NewLine>  File ""/home/user/anaconda3/envs/idk/lib/python3.7/site-packages/ignite/distributed/auto.py"", line 10, in &lt;module&gt;<NewLine>    from ignite.distributed import utils as idist<NewLine>  File ""/home/useranaconda3/envs/idk/lib/python3.7/site-packages/ignite/distributed/utils.py"", line 8, in &lt;module&gt;<NewLine>    from ignite.distributed.comp_models import (<NewLine>  File ""/home/user/anaconda3/envs/idk/lib/python3.7/site-packages/ignite/distributed/comp_models/__init__.py"", line 2, in &lt;module&gt;<NewLine>    from ignite.distributed.comp_models.native import has_native_dist_support<NewLine>  File ""/home/user/anaconda3/envs/idk/lib/python3.7/site-packages/ignite/distributed/comp_models/native.py"", line 22, in &lt;module&gt;<NewLine>    class _NativeDistModel(ComputationModel):<NewLine>  File ""/home/user/anaconda3/envs/idk/lib/python3.7/site-packages/ignite/distributed/comp_models/native.py"", line 305, in _NativeDistModel<NewLine>    ""AND"": dist.ReduceOp.BAND,<NewLine>AttributeError: type object 'torch.distributed.ReduceOp' has no attribute 'BAND'<NewLine></code></pre><NewLine><p>Any idea?</p><NewLine></div>",https://discuss.pytorch.org/u/pytorchnewbie,,pytorchnewbie,"July 29, 2020,  5:11am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pytorchnewbie"">@pytorchnewbie</a> what is your pytorch version ?<br/><NewLine>It should be &gt;= 1.3</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: July 29, 2020,  7:15am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
90810,"Computing every iteration, every 20 epochs",2020-07-28T10:47:57.346Z,3,68,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!</p><NewLine><p>What if I wanted to compute something on every batch output (so every iteration), but I only wanted to do this every 20 epochs? How could I do this? Is there a sort of “detach_event_handler” as a counterpart to attach_event_handler? Or is there some other way to do this? Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/pytorchnewbie,,pytorchnewbie,"July 28, 2020, 10:51am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I might have misunderstood your question but could you not just use:</p><NewLine><pre><code class=""lang-auto"">if(epoch%20 == 0):<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think I didn’t explain well my question.</p><NewLine><p>Consider the following:</p><NewLine><p>trainer.add_event_handler(Events.ITERATION_COMPLETED(every=1), foo)</p><NewLine><p>I want to run foo() on every iteration, but only every 20 epochs.</p><NewLine><p>I cant just do:</p><NewLine><p>if (epoch%20==0):</p><NewLine><p>trainer.add_event_handler(Events.ITERATION_COMPLETED(every=1), foo)</p><NewLine><p>Because the first time epoch gets to 20, the handler will be attached to the trainer and then foo() will be called on every single iteration from then on, whether epoch%20 or not.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pytorchnewbie"">@pytorchnewbie</a> I think the point was about using <code>if epoch %20 ==0</code> inside <code>foo</code> which is one of possible implementations.</p><NewLine><p>If you would like to setup customized event filtering other than <code>every</code>, please see the usage of <code>event_filter </code> argument : <a href=""https://pytorch.org/ignite/engine.html#ignite.engine.events.Events"" rel=""nofollow noopener"">https://pytorch.org/ignite/engine.html#ignite.engine.events.Events</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>either this.<br/><NewLine>Or, as far as I understand you want to call foo() on every iteration of every 20th epoch right?<br/><NewLine>So this:</p><NewLine><pre><code class=""lang-auto"">for epoch in range(n_epochs):    #your epoch loop<NewLine>    for iteration, data in enumerate(your_dataloader, 0):     #your iteration loop<NewLine>        if((epoch+1)%20 == 0): <NewLine>            foo() #function gets called on every iteration of every 20th epoch<NewLine></code></pre><NewLine><p>should work.<br/><NewLine><a class=""mention"" href=""/u/pytorchnewbie"">@pytorchnewbie</a> Is this not the behavior you asked for?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ralo4"">@RaLo4</a> The question is marked with “ignite” category and about how to do that with PyTorch-Ignite</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/RaLo4; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/RaLo4; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: July 28, 2020, 11:10am; <NewLine> REPLY_DATE 2: July 28, 2020, 11:33am; <NewLine> REPLY_DATE 3: July 28, 2020,  1:45pm; <NewLine> REPLY_DATE 4: July 28, 2020,  3:08pm; <NewLine> REPLY_DATE 5: July 28, 2020,  3:52pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
90306,Toutorial for ignite,2020-07-23T16:21:19.636Z,0,61,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Can anyone suggest me good toutorial for Pytorch ignite.<br/><NewLine>I searched but can’t find anything.</p><NewLine></div>",https://discuss.pytorch.org/u/Aman_Gupta,(Aman Gupta),Aman_Gupta,"July 23, 2020,  4:21pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/aman_gupta"">@Aman_Gupta</a> how about</p><NewLine><ul><NewLine><li><a href=""https://pytorch.org/ignite/quickstart.html"" rel=""nofollow noopener"">https://pytorch.org/ignite/quickstart.html</a></li><NewLine><li><a href=""https://pytorch.org/ignite/concepts.html"" rel=""nofollow noopener"">https://pytorch.org/ignite/concepts.html</a></li><NewLine></ul><NewLine><p>We have also various tutorials with Jupyter Notebooks:</p><NewLine><ul><NewLine><li><a href=""https://pytorch.org/ignite/examples.html#notebooks"" rel=""nofollow noopener"">https://pytorch.org/ignite/examples.html#notebooks</a></li><NewLine></ul><NewLine><p>Let me know if it does not help you to start. Thanks</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for sharing<br/><NewLine>I think jupyter notebook may help</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Aman_Gupta; <NewLine> ,"REPLY_DATE 1: July 23, 2020,  4:44pm; <NewLine> REPLY_DATE 2: July 23, 2020, 11:30pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
90002,Add_event_handler arguments not passing correctly?,2020-07-21T08:36:16.197Z,7,70,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!</p><NewLine><p>So when I do:</p><NewLine><pre><code class=""lang-auto"">def log_engine_output(<NewLine>    engine: Engine,<NewLine>    fields: Dict[LOG_OP, Union[List[str], List[VisPlot], List[VisImg]]],<NewLine>    epoch_num=None,<NewLine>) -&gt; None:<NewLine>    """"""Log numerical fields in the engine output dictionary to stdout""""""<NewLine><NewLine>    print(epoch_num)<NewLine><NewLine>    for mode in fields.keys():<NewLine>        if (<NewLine>            mode is LOG_OP.NUMBER_TO_VISDOM<NewLine>            or mode is LOG_OP.VECTOR_TO_VISDOM<NewLine>            or mode is LOG_OP.IMAGE_TO_VISDOM<NewLine>        ):<NewLine>            mode(<NewLine>                engine,<NewLine>                engine.state.vis,<NewLine>                fields[mode],<NewLine>                engine_attr=""output"",<NewLine>                epoch_num=epoch_num,<NewLine>            )<NewLine>        else:<NewLine>            mode(engine, fields[mode], engine_attr=""output"")<NewLine><NewLine><NewLine>trainer.add_event_handler(<NewLine>            Events.EPOCH_COMPLETED(every=1),<NewLine>            log_engine_output,<NewLine>            {<NewLine>                # Log fields as message in logfile<NewLine>                LOG_OP.LOG_MESSAGE: [""loss""],<NewLine>                # Log fields as separate data files<NewLine>                LOG_OP.SAVE_IN_DATA_FILE: [""loss""],<NewLine>                # Plot fields to Visdom<NewLine>                LOG_OP.NUMBER_TO_VISDOM: [<NewLine>                    # First plot, key is ""p1""<NewLine>                    VisPlot(<NewLine>                        ""loss"",<NewLine>                        plot_key=""p1"",<NewLine>                        split=""nll_1"",<NewLine>                        title=""Train loss: "",<NewLine>                        x_label=""Iters"",<NewLine>                        y_label=""nll"",<NewLine>                        env=cfg.env,<NewLine>                    ),<NewLine>                ],<NewLine>            },<NewLine>            trainer.state.epoch,<NewLine>        )<NewLine></code></pre><NewLine><p>The epoch_num that is printed is always 0. That’s not right since it prints every epoch - why would this happen?</p><NewLine><p>I’m also confusing about the structure of doing trainer.add_event_handler() vs. the decorator of<br/><NewLine><span class=""mention"">@trainer.on</span>(Events.EPOCH_COMPLETED(every=1)).</p><NewLine><p>I thought that the only difference between the decorator and the add_event_handler(some_function, variable1, variable2) was that with add_event_handler you could pass in the variables that are the arguments to the function (variable 1, variable 2), whereas with the decorator <span class=""mention"">@train</span> you can only pass in “engine” as your argument. Is this correct? If so, why isn’t passing the arguments working for me?</p><NewLine><p>Also - what is the situation with add_event_handler automatically checking to see if the first argument it finds (after the function),is  “engine”? From the tutorial, “The first argument can be optionally engine, but not necessary.”.</p><NewLine><pre><code class=""lang-auto"">def log_metrics(engine, title):<NewLine>    print(""Epoch: {} - {} accuracy: {:.2f}""<NewLine>           .format(trainer.state.epoch, title, engine.state.metrics[""acc""]))<NewLine><NewLine>@trainer.on(Events.EPOCH_COMPLETED)<NewLine>def evaluate(trainer):<NewLine>    with evaluator.add_event_handler(Events.COMPLETED, log_metrics, ""train""):<NewLine>        evaluator.run(train_loader)<NewLine></code></pre><NewLine><p>Here it automatically passes “evaluator” as the “engine” argument to log_metrics. However, here:</p><NewLine><pre><code class=""lang-auto"">trainer = Engine(update_model)<NewLine><NewLine>trainer.add_event_handler(Events.STARTED, lambda _: print(""Start training""))<NewLine># or<NewLine>@trainer.on(Events.STARTED)<NewLine>def on_training_started(engine):<NewLine>    print(""Another message of start training"")<NewLine># or even simpler, use only what you need !<NewLine>@trainer.on(Events.STARTED)<NewLine>def on_training_started():<NewLine>    print(""Another message of start training"")<NewLine><NewLine># attach handler with args, kwargs<NewLine>mydata = [1, 2, 3, 4]<NewLine><NewLine>def on_training_ended(data):<NewLine>    print(""Training is ended. mydata={}"".format(data))<NewLine><NewLine>trainer.add_event_handler(Events.COMPLETED, on_training_ended, mydata)<NewLine></code></pre><NewLine><p>The first argument is not “engine”. Does this mean that somehow ignite checks the function passed into add_event_handler to see if the first argument is called “engine”? And if so, does the nomenclature have to be exactly called “engine” in order for it to pass the engine that add_event_handler is attached to to the function?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/pytorchnewbie,,pytorchnewbie,"July 21, 2020,  8:36am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/pytorchnewbie"">@pytorchnewbie</a></p><NewLine><blockquote><NewLine><p>The epoch_num that is printed is always 0. That’s not right since it prints every epoch - why would this happen?</p><NewLine></blockquote><NewLine><p>This a correct behaviour as passing <code>trainer.state.epoch</code> it is passed as value and not reference. I agree that it would be nice to have it as reference, but I have no idea how to do that in python.</p><NewLine><p>You need to fetch epoch inside the handler with <code>trainer.state.epoch</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I’m also confusing about the structure of doing trainer.add_event_handler() vs. the decorator of<br/><NewLine><span class=""mention"">@trainer.on</span>(Events.EPOCH_COMPLETED(every=1)).</p><NewLine></blockquote><NewLine><p>Normally and the intention is that both should be the same and provide the same functionnality. If it is not the case, it is a bug.</p><NewLine><p>You can also pass variable using the decorator:</p><NewLine><pre><code class=""lang-python"">from ignite.engine import Engine, Events<NewLine><NewLine>trainer = Engine(lambda e, b: None)<NewLine><NewLine><NewLine>@trainer.on(Events.ITERATION_COMPLETED, [0, 1, 2])<NewLine>def print_my_data(_mydata):<NewLine>    print(""print_my_data : "", _mydata)<NewLine>    <NewLine>trainer.run([0])<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""90002"" data-username=""pytorchnewbie""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/p/4da419/40.png"" width=""20""/> pytorchnewbie:</div><NewLine><blockquote><NewLine><p>The first argument is not “engine”. Does this mean that somehow ignite checks the function passed into add_event_handler to see if the first argument is called “engine”? And if so, does the nomenclature have to be exactly called “engine” in order for it to pass the engine that add_event_handler is attached to to the function?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, internally, we check the signature of attached function and see if we can bind the function with and without the Engine.<br/><NewLine>There are following options possible:</p><NewLine><ol><NewLine><li>with engine only</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">@trainer.on(event)<NewLine>def handler(engine):<NewLine>    assert engine == trainer<NewLine></code></pre><NewLine><ol start=""2""><NewLine><li>with engine and additional variables</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">@trainer.on(event, v1, v2)<NewLine>def handler(engine, v1, v2):<NewLine>    assert engine == trainer<NewLine></code></pre><NewLine><ol start=""3""><NewLine><li>without engine</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">@trainer.on(event)<NewLine>def handler():<NewLine>    pass<NewLine></code></pre><NewLine><ol start=""4""><NewLine><li>without engine, but with variables</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">@trainer.on(event, v1, v2)<NewLine>def handler(v1, v2):<NewLine>    pass<NewLine></code></pre><NewLine><blockquote><NewLine><p>And if so, does the nomenclature have to be exactly called “engine” in order for it to pass the engine that add_event_handler is attached to to the function?</p><NewLine></blockquote><NewLine><p>Yes, if you would like to pass engine, it has to be the first argument.<br/><NewLine>Hope it is more clear <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>What does it mean passed by value and not reference? Value to me means at that moment what is the value of trainer.state.epoch, but isn’t this handler triggered completely anew every epoch? Therefore every epoch the value of trainer.state.epoch changes?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>What does it mean passed by value and not reference?</p><NewLine></blockquote><NewLine><p>It means that the function gets the value of  <code>trainer.state.epoch</code>.</p><NewLine><blockquote><NewLine><p>Therefore every epoch the value of trainer.state.epoch changes?</p><NewLine></blockquote><NewLine><p>yes, it is updated internally by <code>Engine</code> during the run.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>To respond to : “Yes, internally, we check the signature of attached function and see if we can bind the function with and without the Engine.”</p><NewLine><p>Strange… it doesn’t seem to work in this case:</p><NewLine><p>def print_epoch(engine):<br/><NewLine>print(""The training epoch is: "" + str(engine.state.epoch))</p><NewLine><p>trainer.add_event_handler(<br/><NewLine>Events.EPOCH_COMPLETED(every=1), print_epoch()<br/><NewLine>)</p><NewLine><p>The error is : “TypeError: print_epoch() missing 1 required positional argument: ‘engine’”</p><NewLine><p>I get the same error if I do:</p><NewLine><p>trainer.add_event_handler(<br/><NewLine>Events.EPOCH_COMPLETED(every=1), print_epoch(), trainer<br/><NewLine>)</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""90002"" data-username=""vfdev-5""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/vfdev-5/40/3535_2.png"" width=""20""/> vfdev-5:</div><NewLine><blockquote><NewLine><p>Therefore every epoch the value of trainer.state.epoch changes?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Exactly… so then normally the value of trainer.state.epoch shouldn’t be 0 every single time the event handler is triggered?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>It should be like that</p><NewLine><pre><code class=""lang-python"">def print_epoch(engine):<NewLine>    print(""The training epoch is: "" + str(engine.state.epoch))<NewLine><NewLine>trainer.add_event_handler(<NewLine>    Events.EPOCH_COMPLETED(every=1), print_epoch<NewLine>)<NewLine></code></pre><NewLine><p><code>print_epoch</code> shouldn’t be called on the same line as <code>trainer.add_event_handler</code>.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ofcourse silly mistake I apologize</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""8"" data-topic=""90002"" data-username=""pytorchnewbie""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/p/4da419/40.png"" width=""20""/> pytorchnewbie:</div><NewLine><blockquote><NewLine><p>Exactly… so then normally the value of trainer.state.epoch shouldn’t be 0 every single time the event handler is triggered?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Well, it is more like the following:</p><NewLine><pre><code class=""lang-python"">def foo(x):<NewLine>    print(x)<NewLine>    <NewLine><NewLine>handlers = []<NewLine><NewLine>def add_handler(fn, args):<NewLine>    handlers.append((fn, args))<NewLine>    <NewLine>    <NewLine>def execute():<NewLine>    for h, args in handlers:<NewLine>        h(args)<NewLine>        <NewLine><NewLine>epoch = 0<NewLine><NewLine># args are registered with value 0<NewLine>add_handler(foo, epoch)<NewLine><NewLine>execute()<NewLine><NewLine># even if we update the variable, registered args are not changed<NewLine>epoch = 1<NewLine><NewLine>execute()<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: July 21, 2020,  8:52am; <NewLine> REPLY_DATE 2: July 21, 2020,  8:57am; <NewLine> REPLY_DATE 3: July 21, 2020,  9:13am; <NewLine> REPLY_DATE 4: July 21, 2020,  9:34am; <NewLine> REPLY_DATE 5: July 21, 2020,  9:37am; <NewLine> REPLY_DATE 6: July 21, 2020,  9:38am; <NewLine> REPLY_DATE 7: July 21, 2020,  9:38am; <NewLine> REPLY_DATE 8: July 21, 2020,  9:40am; <NewLine> REPLY_DATE 9: July 21, 2020,  9:40am; <NewLine> REPLY_DATE 10: July 21, 2020,  9:44am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> 
89422,_inference within create_supervised_trainer?,2020-07-16T08:51:13.359Z,0,56,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>In one of ignite’s issues (<a href=""https://github.com/pytorch/ignite/pull/48/files"" rel=""nofollow noopener"">https://github.com/pytorch/ignite/pull/48/files</a>) I saw that the _inference was passed in to the trainer engine. Why is this option not in the docs? Is it not recommended to do this? Why not?</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/pytorchnewbie,,pytorchnewbie,"July 17, 2020, 11:39am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Moving to <code>Ignite</code> category for better visibility and CC’ing <a class=""mention"" href=""/u/vfdev-5"">@vfdev-5</a> <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Seems like it is a very old PR and things changed since than.</p><NewLine><p>Currently, we do not provide such abstractions like <code>Trainer</code> etc as there is no unique definition of it as it can be task/config dependent etc. We left that to the user to compose its trainer using one or more <code>Engine</code>s for training/validation etc.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: July 17, 2020, 11:40am; <NewLine> REPLY_DATE 2: July 17, 2020, 12:01pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
89326,Storing values in metric or returning them in engine&rsquo;s dict for plotting purposes,2020-07-15T13:38:18.521Z,17,94,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! I have a question about how to best plot things. Suppose I have a model and that during inference I am getting the error per example in a batch. I then want to correlate the error of the predicted y with a property of the input x. So ultimately I’m plotting a huge amount of property(x) and error(y,y_pred) on a scatterplot, and then calculating their correlation. I assume that the best way to do this would be to log every batch x properties and y errors using a custom metric, and then outputing the correlation between the property(x) and error(y,y_pred) for all x,y pairs at the end through the compute() function.</p><NewLine><p>However, what if I also wanted a scatterplot of all the property(x) and error(y,y_pred) for all pairs x,y? I understand that metric stores the history, but would it be too convoluted to also plot all the property(x) and error(y,y_pred) from within Metric’s compute() function? It seems complicated because I create my visdom Visualizer in a file called “train.py” (where I also run my evaluator) and the custom Metric is in its own file called metric.py.</p><NewLine><p>The only other option I can think of is to store all the property(x) and error(y,y_pred) outside of the Metric.py file by returning the property(x) and error(y,y_pred) in the dict returned by the evaluator engine in train.py, and then creating an event handler that will save those values every iteration until the end of the validation set. After this, all the property(x) and error(y,y_pred) are saved in a file, and I just repull them all to do a scatter plot.</p><NewLine><p>I was wondering what is the best/more ignite/most elegant way to do this.</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/pytorchnewbie,,pytorchnewbie,"July 15, 2020,  1:38pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/pytorchnewbie"">@pytorchnewbie</a>,</p><NewLine><p>IMO, it would be better to separate the logic of metrics computation and logging for visualization.</p><NewLine><pre><code class=""lang-python"">def inference_step(e, batch):<NewLine>    # ...<NewLine>    x = batch[""x""]<NewLine>    y_pred = model(x)<NewLine>    return {<NewLine>        ""y_pred"": y_pred,<NewLine>    }<NewLine><NewLine>infer_engine = Engine(inference_step)<NewLine>infer_engine.state.viz_data = {}  # data storage for visualization<NewLine><NewLine>@infer_engine.on(Events.ITERATION_COMPLETED)<NewLine>def compute_iterationwise_error(engine):<NewLine>    batch = engine.state.batch<NewLine>    x = batch[""x""]<NewLine>    y = batch[""y""]<NewLine>    y_pred = engine.state.output[""y_pred""]<NewLine><NewLine>    err = compute_error(y,y_pred)<NewLine>    engine.state.viz_data[""err""] = err<NewLine>    engine.state.viz_data[""x_properties""] = get_properties(x)<NewLine><NewLine><NewLine>plot_every = 1 # 2 or 10<NewLine><NewLine>@infer_engine.on(Events.ITERATION_COMPLETED(every=plot_every))<NewLine>def plot_infer_data(engine):<NewLine>    viz_data = engine.state.viz_data<NewLine>    visdom_plot_data(viz_data[""x_properties""], viz_data[""err""])<NewLine></code></pre><NewLine><p>For me, it is not clear from the description, how often you would like to compute a metric:</p><NewLine><ul><NewLine><li>epoch-wise (1 per the dataset)</li><NewLine><li>batch-wise (1 per batch)</li><NewLine></ul><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks! To answer your question: I want to update per epoch, since this is the validation set (so theres only one epoch so i just want the relationship of the validation set)</p><NewLine><p>It seems strange to use a function compute_iterationwise_error within which you get the error and the properties and storing them in a dict every iteration since this is extremely similar to using a Metric. Is the only reason you’re doing this this way because Metric doesn’t allow you to return dicts? The compute() needs to give a scalar?</p><NewLine><p>Additionally, at the end of accumulating all these errors and properties, I want to compute the correlation between them. This is a final scalar based on all the collected data - this seems like Metric could be especially useful.</p><NewLine><p>Finally, if you plot the values in real time, so one property and one error per iteration, then thats great but what if i wanted to then udpate the title of the plot at the very end to include the correlation between the properties and the errors? I dont think visdom has a function for just updating the title of a plot on its own without adding an additional data point…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>It seems strange to use a function compute_iterationwise_error within which you get the error and the properties and storing them in a dict every iteration since this is extremely similar to using a Metric. Is the only reason you’re doing this this way because Metric doesn’t allow you to return dicts? The compute() needs to give a scalar?</p><NewLine></blockquote><NewLine><p>Yes, true. But as I didn’t understand what you wanted to plot, I thought about batchwise logging…</p><NewLine><blockquote><NewLine><p>Additionally, at the end of accumulating all these errors and properties, I want to compute the correlation between them. This is a final scalar based on all the collected data - this seems like Metric could be especially useful.</p><NewLine></blockquote><NewLine><p>Maybe, you can use <code>EpochMetric</code> to accumulate errors and properties and setup <code>compute_fn</code> to compute the correlation. See <a href=""https://pytorch.org/ignite/metrics.html#ignite.metrics.EpochMetric"" rel=""nofollow noopener"">docs</a>.<br/><NewLine>Then computed value you can separately log with visdom…</p><NewLine><blockquote><NewLine><p>Finally, if you plot the values in real time, so one property and one error per iteration, then thats great but what if i wanted to then udpate the title of the plot at the very end to include the correlation between the properties and the errors? I dont think visdom has a function for just updating the title of a plot on its own without adding an additional data point…</p><NewLine></blockquote><NewLine><p>You can do it like that:</p><NewLine><pre><code class=""lang-auto"">w = vis.line(X=np.array([1, 2, 3]), Y=np.array([1, 2, 3]), opts=dict(xlabel=""x"", ylabel=""y"", title=""title-to-change""))<NewLine>vis.update_window_opts(w, opts={""title"": ""new_title""})<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>""Yes, true. But as I didn’t understand what you wanted to plot, I thought about batchwise logging… ""</p><NewLine><p>—&gt; ok but so if I use metric, then I won’t have access to the list to plot. So I’m still stuck because I won’t be able to plot the error, property data points (i.e. “separately log with visdom” would require me to use your solution above anyways). It woulndn’t make sense to use both a metric using EpochLoss and all the code you wrote above with storing infer_engine.state.viz_data since now both a metric and this dictionary are storing the data points…</p><NewLine><p>However, the solution you provided works, because I can just add a function that computes the correlation on infer_engine.state.viz_data.</p><NewLine><p>I was just hoping for you to confirm that the only reason we are not using metric is specifically because we want to plot the error, property data points - there is no way to do this if we use Metric right?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well, you can manually pick collected errors/target from <code>EpochMetric</code> like <code>EpochMetric._predictions</code> and <code>EpochMetric._targets</code>. But this may look hacky…</p><NewLine><p>Probably, do it manually, as there are logics mixed in what you would like to do.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>What do you mean by manually in your third sentence? Is it different from manually in your first? Why is it hacky?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Why is it hacky?</p><NewLine></blockquote><NewLine><p>Because we access private members (<code>EpochMetric._*</code>)…</p><NewLine><blockquote><NewLine><p>What do you mean by manually ?</p><NewLine></blockquote><NewLine><p>Idea is to setup <code>EpochMetric</code> like that :</p><NewLine><pre><code class=""lang-python"">def inference_step(e, batch):<NewLine>    # ...<NewLine>    x = batch[""x""]<NewLine>    y_pred = model(x)<NewLine><NewLine>    # compute error and properties<NewLine>    err = compute_error(y,y_pred)<NewLine>    x_properties = get_properties(x)<NewLine><NewLine>    return {<NewLine>        ""y_pred"": y_pred,<NewLine>        ""err"": err,<NewLine>        ""x_properties"": x_properties,<NewLine>    }<NewLine><NewLine>infer_engine = Engine(inference_step)<NewLine><NewLine>em = EpochMetric(<NewLine>    compute_fn=compute_corr, <NewLine>    output_transform=lambda out: out[""err""], out[""x_properties""]<NewLine>)<NewLine># we set em._predictions as x_properties<NewLine># and em._targets as err<NewLine><NewLine>em.attach(infer_engine, ""overall_correlation"")<NewLine><NewLine>@infer_engine.on(Events.COMPLETED)<NewLine>def plot_infer_data(engine):<NewLine>    visdom_plot_data(em._targets, em._predictions)<NewLine></code></pre><NewLine><p>I didn’t check if this works, but here is the idea…</p><NewLine><blockquote><NewLine><p>What do you mean by manually in your third sentence? Is it different from manually in your first?</p><NewLine></blockquote><NewLine><p>There are two options to do almost the same thing. First option is to collect data manually as in the first answer. Second option is to use EpochMetric and plot data after the computation.<br/><NewLine>I would prefer the 1st option as I can control everything…</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah I see, but so where would the plotting go in here? do you recommend doing em._predictions and em._targets?</p><NewLine><p>Also, I’m concerned about the warning on EpochMetrics: “Current implementation does not work with distributed computations. Results are not gather across all devices and computed results are valid for a single device only.”</p><NewLine><ul><NewLine><li>Does device here mean machine or gpu? Can I stay on one machine and use multiple gpus?</li><NewLine></ul><NewLine><p>-Does this warning apply if I create my own custom metric like such?</p><NewLine><pre><code class=""lang-auto"">class Mape(Metric):<NewLine>    def __init__(self, output_transform=lambda x: x):<NewLine><NewLine>        self._num_examples = None<NewLine>        self._sum_percentages = None<NewLine>        super(Mape, self).__init__(output_transform=output_transform)<NewLine><NewLine>    @reinit__is_reduced<NewLine>    def reset(self):<NewLine>        self._num_examples = 0<NewLine>        self._sum_percentages = 0<NewLine>        super(Mape, self).reset()<NewLine><NewLine>    @reinit__is_reduced<NewLine>    def update(self, output):<NewLine>        y_pred, y = output<NewLine><NewLine>        errors = torch.abs(y_pred - y.view_as(y_pred))<NewLine>        errors_divided = errors / y.view_as(y_pred) * 100<NewLine><NewLine>        # pdb.set_trace()<NewLine><NewLine>        self._num_examples += y.shape[0] * y.shape[1]<NewLine>        self._sum_percentages += torch.sum(errors_divided).item()<NewLine><NewLine>    @sync_all_reduce(""_num_examples"", ""_sum_percentages"")<NewLine>    def compute(self):<NewLine>        if self._num_examples == 0:<NewLine>            raise NotComputableError(<NewLine>                ""CustomAccuracy must have at least one example before it can be computed.""<NewLine>            )<NewLine>        return self._sum_percentages / self._num_examples<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>The problem of <code>EpochMetric</code> within distributed data parallelism is that each process collects its own list of <code>_predictions</code> and <code>_targets</code>. There is no syncronization of the whole seen histories across participating processes. We can also imagine that this can potentially raise OOM when we do <code>all_gather</code> operation (which is missing currently, but we still plan to add it).</p><NewLine><p>In your <code>Mape</code> implementation, <code>_num_examples</code> and <code>_sum_percentages</code> looks like to be scalars, so <code>sync_all_reduce</code> will collect the data across the processes and in compute method all processes will manipulate “total” <code>self._sum_percentages</code>, <code>self._num_examples</code>. So, it is OK.</p><NewLine><blockquote><NewLine><p>Does device here mean machine or gpu?</p><NewLine></blockquote><NewLine><p>Device means single gpu here</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Got it - so is there a difference between EpochMetric and custom metrics with respect to distributed training? And what case scenarios (i.e. not scalars) can I not use ignite if I’m using distributed training? For example, I’m assuming the following won’t work?</p><NewLine><pre><code class=""lang-auto"">class Correlation(Metric):<NewLine>    def __init__(self, output_transform=lambda x: x):<NewLine><NewLine>        self._list_of_ETC = []<NewLine>        self._list_of_errors = []<NewLine>        super(Correlation, self).__init__(output_transform=output_transform)<NewLine><NewLine>    @reinit__is_reduced<NewLine>    def reset(self):<NewLine>        self._list_of_ETC = []<NewLine>        self._list_of_errors = []<NewLine>        super(Correlation, self).reset()<NewLine><NewLine>    @reinit__is_reduced<NewLine>    def update(self, output):<NewLine><NewLine>        mini_list_of_ETC, mini_list_of_errors = output<NewLine><NewLine>        self._list_of_ETC.append(mini_list_of_ETC)<NewLine>        self._list_of_errors.append(mini_list_of_errors)<NewLine>        pdb.set_trace()<NewLine><NewLine>    @sync_all_reduce(""_list_of_ETC"", ""_list_of_errors"")<NewLine>    def compute(self):<NewLine><NewLine>        if len(self._list_of_ETC) == 0:<NewLine>            raise NotComputableError(<NewLine>                ""ETC must have at least 1 element.""<NewLine>            )<NewLine><NewLine>        if len(self._list_of_errors) == 0:<NewLine>            raise NotComputableError(<NewLine>                ""ETC must have at least 1 element.""<NewLine>            )<NewLine><NewLine>        if len(self._list_of_ETC) != len(self._list_of_errors):<NewLine>            raise NotComputableError(<NewLine>                ""Number of ETC must be equal to number of errors!""<NewLine>            )<NewLine><NewLine>        corr, p_value = pearsonr(self._list_of_ETC, self._list_of_errors)<NewLine><NewLine>        return corr<NewLine><NewLine></code></pre><NewLine><p>Is there anyway to use Ignite and distributed training in this case? I’m 99% of the time going to be using more than 1 gpu in the near future.<br/><NewLine>Thanks!</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>And what case scenarios (i.e. not scalars) can I not use ignite if I’m using distributed training?</p><NewLine></blockquote><NewLine><p>There are some abstractions that are not working yet in DDP: <code>EpochMetric</code>, <code>LRFinder</code>, all contrib regression metrics that use <code>EpochMetric</code>.</p><NewLine><blockquote><NewLine><p>so is there a difference between EpochMetric and custom metrics with respect to distributed training?</p><NewLine></blockquote><NewLine><p>Yes, in the way of how to collect data across processes. By default we collect internal metric’s data via all reduce operation (sum across processes). In case of non-scalar mertic’s data, it is not defined how to perform a sum of those objects and certainly it wont give a correct result (think about computing median for example).</p><NewLine><blockquote><NewLine><p>For example, I’m assuming the following won’t work?</p><NewLine></blockquote><NewLine><p>No, it wont work, as you need to collect <code>_list_of_ETC</code> and <code>_list_of_errors</code> accross processes, so for example total <code>_list_of_errors</code> should be a concat of <code>_list_of_errors</code> by rank, right ?</p><NewLine><p>You have to use all gather op to do that</p><NewLine><pre><code class=""lang-auto"">import torch.distributed as dist<NewLine><NewLine>class Correlation(Metric):<NewLine><NewLine>   def compute(self):<NewLine>       # roughly something like that<NewLine>       list_of_ETC = torch.tensor(_list_of_ETC).to(this_current_device)<NewLine>       dist.all_gather(_list_of_ETC) <NewLine>       self._list_of_ETC = _list_of_ETC.tolist() <NewLine>       # etc<NewLine></code></pre><NewLine><p>PS: btw if you would like to help us with this issue : <a href=""https://github.com/pytorch/ignite/issues/978"" rel=""nofollow noopener"">https://github.com/pytorch/ignite/issues/978</a><br/><NewLine>such that everyone could benefit of using <code>EpochMetric</code> in DDP…<br/><NewLine>If so, you can submit a draft PR and we could guide you on how to better implement and test it…</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see but the example that I gave you above with the Correlation class is not part of EpochMetric. Therefore, can I modify your sentence to say that currently, there is no way of getting Metric to work across multiple gpus in all case scenarios easily?</p><NewLine><p>And is it safe to say that to fix this, every time the user needs to make a custom metric that does <em>not</em> use <span class=""mention"">@sync_all_reduce</span> and instead uses dist.all_gather?</p><NewLine><p>Sure I’ll look into it, it’ll depend on time and if someone can help me because I’m still just learning <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Therefore, can I modify your sentence to say that currently, there is no way of getting Metric to work across multiple gpus in all case scenarios easily?</p><NewLine></blockquote><NewLine><p>Well, majority of ignite’s metrics like Accuracy, Precision, Recall, ConfusionMatrix etc are working in DDP for their use-cases <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><blockquote><NewLine><p>And is it safe to say that to fix this, every time the user needs to make a custom metric that does <em>not</em> use <span class=""mention"">@sync_all_reduce</span> and instead uses dist.all_gather?</p><NewLine></blockquote><NewLine><p>It depends on the metric and how it should be computed: how to compute accumulators for an iteration. If in update function you do <code>+=</code> on tensors, scalars etc. <code>@sync_all_reduce </code> will work without problems and the result will be correct. If you do something else like concat of lists etc, <code>@sync_all_reduce </code> wont work and at compute time we had to collect all the data.</p><NewLine><blockquote><NewLine><p>Sure I’ll look into it, it’ll depend on time and if someone can help me because I’m still just learning</p><NewLine></blockquote><NewLine><p>We can help with that. Just if anyone could initialize the work and put some code etc…</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>OK I see. I’m currently working 2 jobs so I’m not sure if I’ll have time but if you outline what needs to be done I can see if i can squeeze it in! Can you let me know concretely what needs to be done?</p><NewLine><p>Thanks</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see. Anyway, i’ll update <a href=""https://github.com/pytorch/ignite/issues/978"" rel=""nofollow noopener"">the issue related to that</a> where I’ll describe what to do in details…</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks! One last thing:</p><NewLine><p>I find it strange that there is this problem with distributed training because apparently when you put a model on multiple gpus in pytorch, all the distributed stuff dissapears once it returns its prediction according to: <a href=""https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html#create-model-and-dataparallel"" rel=""nofollow noopener"">https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html#create-model-and-dataparallel</a> .</p><NewLine><p>Therefore, once I return y_pred in the infer_dict of the training engine, nothing is distributed anymore?</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, there are at least 2 data parallelisms you can do with PyTorch provided classes :</p><NewLine><ul><NewLine><li><NewLine><a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel"" rel=""nofollow noopener"">DataParallel</a>(DP)</li><NewLine><li><NewLine><a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel"" rel=""nofollow noopener"">DistributedDataParallel</a>(DDP), <a href=""https://pytorch.org/docs/stable/notes/ddp.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/notes/ddp.html</a><NewLine></li><NewLine></ul><NewLine><p>What we discussed before on collecting data across participating processes etc is applied only for DDP. With DP there is a single process running, so there is no need to collect data etc.</p><NewLine><p>However, PyTorch doc suggests to use DDP and it is faster than DP.</p><NewLine><p>PS: See also how ignite simplifies usage of DDP : <a href=""https://pytorch.org/ignite/distributed.html"" rel=""nofollow noopener"">https://pytorch.org/ignite/distributed.html</a></p><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks! but before you said you can’t use current implementation of Ignite on multiple gpus. But you can, by using DataParallel!</p><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, you are right, you can use ignite and its API with DP on multiple GPUs.</p><NewLine><p>PS: I had a shortcut between multiple devices (GPUs) and DDP…</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: July 15, 2020,  2:12pm; <NewLine> REPLY_DATE 2: July 16, 2020,  8:03am; <NewLine> REPLY_DATE 3: July 16, 2020,  8:45am; <NewLine> REPLY_DATE 4: July 16, 2020,  8:57am; <NewLine> REPLY_DATE 5: July 16, 2020,  8:56am; <NewLine> REPLY_DATE 6: July 16, 2020,  9:00am; <NewLine> REPLY_DATE 7: July 16, 2020,  9:10am; <NewLine> REPLY_DATE 8: July 16, 2020,  9:10am; <NewLine> REPLY_DATE 9: July 16, 2020,  9:19am; <NewLine> REPLY_DATE 10: July 16, 2020,  9:21am; <NewLine> REPLY_DATE 11: July 16, 2020,  9:37am; <NewLine> REPLY_DATE 12: July 16, 2020,  9:43am; <NewLine> REPLY_DATE 13: July 16, 2020,  9:52am; <NewLine> REPLY_DATE 14: July 16, 2020, 10:18am; <NewLine> REPLY_DATE 15: July 16, 2020, 10:41am; <NewLine> REPLY_DATE 16: July 16, 2020, 10:50am; <NewLine> REPLY_DATE 17: July 16, 2020, 11:12am; <NewLine> REPLY_DATE 18: July 16, 2020, 11:12am; <NewLine> REPLY_DATE 19: July 16, 2020, 11:14am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> REPLY 18 LIKES: ; <NewLine> REPLY 19 LIKES: 1 Like; <NewLine> 
89408,Inefficiency in metric calculation,2020-07-16T06:21:18.274Z,1,63,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Suppose I have two metrics, mse and correlation. Every 2 epochs of the training, I want to run the evaluator and collect the MSE metric. Also, at the end of training (800 epochs later), I want to run the evaluator and collect and the correlation metric. However, the metrics are attached to the evaluator engine. Therefore, every time evaluator is run, both metrics are calculated. This seems inefficient, since I only need to calculate correlation on the evaluator run at the end of the 800 training epochs, not 400 times (800/2) during training. Thoughts?</p><NewLine></div>",https://discuss.pytorch.org/u/pytorchnewbie,,pytorchnewbie,"July 16, 2020,  7:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can attach/detach metrics, so I think you can just attach it at the end of the training:</p><NewLine><pre><code class=""lang-python"">@trainer.on(Events.EPOCH_COMPLETED(every=2))<NewLine>def compute_mse():<NewLine>    evaluator.run(validation)<NewLine><NewLine>@trainer.on(Events.COMPLETED)<NewLine>def compute_correlation():<NewLine>    correlation_metric.attach(evaluator, ""corr"")<NewLine>    evaluator.run(validation)<NewLine>    # optionally<NewLine>    correlation_metric.detach(evaluator)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Beautiful! This is exactly what I wanted to know. Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> ,"REPLY_DATE 1: July 16, 2020,  8:59am; <NewLine> REPLY_DATE 2: July 16, 2020,  9:00am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
89263,Non-ML models with ignite,2020-07-15T05:53:45.406Z,0,63,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In research, a lot of the time we will be comparing machine learning approaches to non-machine learning approaches. For the latter, there isn’t usually a notion of “iterations” or “epochs”. The model does what it does and outputs an error. Since the idea would be to code all the experiments in the same framework, I was wondering how to use ignite for non-learning based approaches/if it makes sense to do that. For instance, does it make sense to use ignite’s “metrics” class for a non-learning based approach? Does using any of ignite make sense for non-learning approaches?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/pytorchnewbie,,pytorchnewbie,"July 15, 2020,  5:53am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/pytorchnewbie"">@pytorchnewbie</a></p><NewLine><blockquote><NewLine><p>The model does what it does and outputs an error. Since the idea would be to code all the experiments in the same framework, I was wondering how to use ignite for non-learning based approaches/if it makes sense to do that. For instance, does it make sense to use ignite’s “metrics” class for a non-learning based approach? Does using any of ignite make sense for non-learning approaches?</p><NewLine></blockquote><NewLine><p>Probably, it depends on details etc (if you could provide some, it would help), but I think this can be just considered as inference phase: a model (any kind of) is used to compute predictions over the batches of the test dataset and we wish to save the outputs</p><NewLine><pre><code class=""lang-python"">def inference_step(engine, batch):<NewLine>    sample, meta_data, what_ever_data = batch<NewLine>    result = non_ml_model(sample)<NewLine>    return {<NewLine>        ""result"": result,        <NewLine>    }<NewLine><NewLine>inference_engine = Engine(inference_step)<NewLine><NewLine>@inference_engine.on(Events.ITERATION_COMPLETED)<NewLine>def save_result(engine):<NewLine>    sample, meta_data, what_ever_data = engine.state.batch<NewLine>    result = engine.state.output[""result""]<NewLine>    # do something to save the result<NewLine><NewLine></code></pre><NewLine><p>Another example of using Engine can be to compute mean/std of the dataset (no ML-model at all):</p><NewLine><pre><code class=""lang-auto"">from ignite.metrics import Average<NewLine><NewLine>def compute_mean_std(engine, batch):<NewLine>    b, c, *_ = batch['image'].shape<NewLine>    data = batch['image'].reshape(b, c, -1).to(dtype=torch.float64)<NewLine>    mean = torch.mean(data, dim=-1).sum(dim=0)<NewLine>    mean2 = torch.mean(data ** 2, dim=-1).sum(dim=0)<NewLine>    return {""mean"": mean, ""mean^2"": mean2}<NewLine><NewLine>compute_engine = Engine(compute_mean_std)<NewLine>img_mean = Average(output_transform=lambda output: output['mean'])<NewLine>img_mean.attach(compute_engine, 'mean')<NewLine>img_mean2 = Average(output_transform=lambda output: output['mean^2'])<NewLine>img_mean2.attach(compute_engine, 'mean2')<NewLine>state = compute_engine.run(train_loader)<NewLine>state.metrics['std'] = torch.sqrt(state.metrics['mean2'] - state.metrics['mean'] ** 2)<NewLine>mean = state.metrics['mean'].tolist()<NewLine>std = state.metrics['std'].tolist()<NewLine></code></pre><NewLine><p><a class=""onebox"" href=""https://pytorch.org/ignite/engine.html#ignite.engine.engine.Engine"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/ignite/engine.html#ignite.engine.engine.Engine</a></p><NewLine><p>Let me know if this helps. Otherwise, please provide a minimal code snippet to understand better the question…</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you so much! I am figuring out something about the non-ml model and will respond to this in a day or two with more details.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> ,"REPLY_DATE 1: July 15, 2020,  8:22am; <NewLine> REPLY_DATE 2: July 15, 2020,  1:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
88954,Difference between MeanSquaredError &amp; Loss (where loss = mse),2020-07-13T08:07:13.843Z,3,200,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!</p><NewLine><p>So I am trying to calculate the average mean squared error over my validation dataset. I’ve done this in two ways: using Ignite’s Loss metric, where the loss_fn = nn.MSELoss() and then using Ignite’s MeanSquaredError metric, as can be seen in the code snippets below:</p><NewLine><pre><code class=""lang-auto""><NewLine>loss_fn = torch.nn.MSELoss()<NewLine><NewLine>metrics = {<NewLine>        ""mse"": Loss(<NewLine>            loss_fn,<NewLine>            output_transform=lambda infer_dict: (infer_dict[""y_pred""], infer_dict[""y""]),<NewLine>        ),<NewLine>    }<NewLine><NewLine>    for name, metric in metrics.items():<NewLine>        metric.attach(engine, name)<NewLine></code></pre><NewLine><p>vs.</p><NewLine><pre><code class=""lang-auto"">metrics = {<NewLine>        ""mse"": MeanSquaredError(<NewLine>            output_transform=lambda infer_dict: (infer_dict[""y_pred""], infer_dict[""y""]),<NewLine>        ),<NewLine>    }<NewLine><NewLine>    for name, metric in metrics.items():<NewLine>        metric.attach(engine, name)<NewLine></code></pre><NewLine><p>I obtain two different results, as can be sceen from the two images below.</p><NewLine><p>MeanSquaredError (the top left is evaluation MSE error, and top right is training MSE error):</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/eae94b8610d9467f615e2752b87c73895dd93915"" href=""https://discuss.pytorch.org/uploads/default/original/3X/e/a/eae94b8610d9467f615e2752b87c73895dd93915.png"" title=""meansquarederror""><img alt=""meansquarederror"" data-base62-sha1=""xw7yJYGItGjbVgplutzc8HiUdbT"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/a/eae94b8610d9467f615e2752b87c73895dd93915_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/a/eae94b8610d9467f615e2752b87c73895dd93915_2_529x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/a/eae94b8610d9467f615e2752b87c73895dd93915_2_529x500.png, https://discuss.pytorch.org/uploads/default/optimized/3X/e/a/eae94b8610d9467f615e2752b87c73895dd93915_2_793x750.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/e/a/eae94b8610d9467f615e2752b87c73895dd93915_2_1058x1000.png 2x"" width=""529""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">meansquarederror</span><span class=""informations"">1568×1482 246 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>Loss:</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/970ae2450248c2389d2d7b154feeaa5b68b8c6b0"" href=""https://discuss.pytorch.org/uploads/default/original/3X/9/7/970ae2450248c2389d2d7b154feeaa5b68b8c6b0.png"" title=""loss""><img alt=""loss"" data-base62-sha1=""lybpPsk5o6iebsE2dMIXQoxmKNW"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/7/970ae2450248c2389d2d7b154feeaa5b68b8c6b0_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/7/970ae2450248c2389d2d7b154feeaa5b68b8c6b0_2_524x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/7/970ae2450248c2389d2d7b154feeaa5b68b8c6b0_2_524x500.png, https://discuss.pytorch.org/uploads/default/optimized/3X/9/7/970ae2450248c2389d2d7b154feeaa5b68b8c6b0_2_786x750.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/9/7/970ae2450248c2389d2d7b154feeaa5b68b8c6b0_2_1048x1000.png 2x"" width=""524""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">loss</span><span class=""informations"">1532×1460 243 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>We can see that MSE has an error of the order of “M” whereas Loss has an error of the order of “K”. What accounts for this difference?</p><NewLine><p>Initially I thought that its probably because Loss() probably calculates the average mean squared error per batch, and then takes the average of the averages, whereas MeanSquaredError (from what I saw in the source code) keeps track of all squared errors, and takes the average of all the squared errors across all the batches (so it only does one average, not average of averages). However, since the batch size is constant, the two results should be numerically equivalent. For example:</p><NewLine><p>( (((7^2) + (8^2) + (13^2))/3) + (((3^2) + (6^2) + (11^2))/3)  )  / 2</p><NewLine><p>is the same as:</p><NewLine><p>(7^2) + (8^2) + (13^2) + (3^2) + (6^2) + (11^2)/6</p><NewLine><p>because both groups in the first example have 3 elements in them.</p><NewLine><p>Therefore, what accounts for the difference in magnitudes of the error functions?</p><NewLine><p>Thanks so much!</p><NewLine></div>",https://discuss.pytorch.org/u/pytorchnewbie,,pytorchnewbie,"July 13, 2020,  8:07am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ignite’s Metric API allows to inspect what happens batch by batch. All metrics allow to do the following:</p><NewLine><pre><code class=""lang-auto"">mse_metric = MeanSquaredError()<NewLine><NewLine>mse_metric.reset()<NewLine><NewLine>mse_metric.update((y_pred1, y1))<NewLine># check result of the 1st batch<NewLine>print(mse_metric.compute())<NewLine><NewLine>mse_metric.update((y_pred2, y2))<NewLine># check result of the 1st and 2nd batch<NewLine>print(mse_metric.compute())<NewLine></code></pre><NewLine><p>In this way you can compare both on your predictions and targets and see where comes from the difference.</p><NewLine><p>HTH</p><NewLine><p>PS.<br/><NewLine>Shouldn’t <code>loss_fn = torch.nn.MSELoss()</code> have reduction “sum” ?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>So I did that and it turns out that MeanSquaredError metric has a different output than the Loss = torch.nn.MSELoss() even on the first batch.</p><NewLine><p>From running:</p><NewLine><pre><code class=""lang-auto"">        loss.backward()<NewLine>        optimizer.step()<NewLine><NewLine>        mse_metric.reset()<NewLine>        loss_metric.reset()<NewLine><NewLine>        mse_metric.update((y_pred, y))<NewLine>        loss_metric.update((y_pred, y))<NewLine><NewLine>        print(mse_metric.compute())<NewLine>        print(loss_metric.compute())<NewLine></code></pre><NewLine><p>I got :</p><NewLine><p>8.566025390625<br/><NewLine>0.05948628857731819</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have printed both update() steps after one iteration. They both have the same # _num_examples but loss has a different ._sum (37521646.875) than MeanSquaredErrors’ _sum_of_squared_errors (5403117056.0)… Is there a way to inspect even deeper the reason for this?</p><NewLine><p>On a separate note, the _num_examples is wrong. In both cases, Loss and MeanSquaredError, te _num_examples is just the first element of the y.shape[0]. However, I want to do MSE on a tensor of [200,144], so the num_examples shouldn’t be 200, it should be 200*144. Is there a reason for why Ignite metrics only include the batch # as the num_examples?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Also no it should not be “sum” because “Loss” documentation specifically says that it expects it to be the average loss. The source code then explains why it says: self._sum += average_loss.item() * n</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok i finally solved it . Loss uses torch.nn.MSELoss() which takes the sum of the errors of the (200,144) and then divides by 144, and this is then the ._sum value. The MeanSquaredError also takes the sum of the error of the (200,144), giving the _sum_of_squared_errors  value. But then, during compute(), both consider the num_of_examples to be 200 so then they both divide by 200. So Loss is basically = MeanSquaredError/144. I think this is probably something alot of people could get confused about  -  it might be worth specifying that MeanSquaredError only divides by the batch size to get the mean…</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for pointing out this difference ! Yes, both metrics should be definitely made coherent between each other and the result of <code>nn.MSELoss</code> … and we also need to update the docs.</p><NewLine><p>If you would like to help us by opening an issue on that, it would be great !</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sure!! Ill have to learn how to do that first though haha.</p><NewLine><p>Just a last thought: I do think the way MeanSquaredError does it is superior to the way Loss (well, pytorch’s torch.nn.MSELoss) does it. The assumption makes more sense. If I have a tensor of (200,144) and take the total squared error, it is more likely that the user wants the mean error w respect to the batch than w respect to the dimensions of 144…</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> ,"REPLY_DATE 1: July 13, 2020,  8:18am; <NewLine> REPLY_DATE 2: July 13, 2020,  8:23am; <NewLine> REPLY_DATE 3: July 13, 2020, 10:29am; <NewLine> REPLY_DATE 4: July 13, 2020, 10:25am; <NewLine> REPLY_DATE 5: July 13, 2020, 11:10am; <NewLine> REPLY_DATE 6: July 13, 2020, 11:19am; <NewLine> REPLY_DATE 7: July 13, 2020, 11:27am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> 
86080,Nested event handlers,2020-06-19T14:25:22.073Z,6,141,"<div class=""post"" itemprop=""articleBody""><NewLine><p>What if I want to run my evaluation engine (which itself has event handlers) with my evaluation dataloader every 20 epochs of the training engine? Right now this isn’t possible, so I made a wrapper function that allows for this below. However, I was wondering if there was an ignite way of doing this? I couldn’t find any.</p><NewLine><pre><code class=""lang-auto"">def _lf_two(<NewLine>    log_fn: Callable[[Engine, List[str]], None],<NewLine>    inner_engine: Engine,<NewLine>    loader: DataLoader,<NewLine>    **kwargs<NewLine>) -&gt; Callable[[Engine], Any]:<NewLine>    """"""Returns a lambda calling custom log function with two engines (e.g. the training loop and validation loop)""""""<NewLine>    return lambda outer_engine: log_fn(<NewLine>        inner_engine, loader, epoch_num=outer_engine.state.epoch, **kwargs<NewLine>    )<NewLine><NewLine>def run_engine_and_log_output(<NewLine>    engine: Engine, loader: DataLoader, fields: List[str], epoch_num=None,<NewLine>) -&gt; None:<NewLine><NewLine>    engine.run(loader,max_epochs=1, epoch_length=10)<NewLine>    ###Here i grab things from engine.state.output and do what i want###<NewLine><NewLine>trainer.add_event_handler(<NewLine>        Events.EPOCH_COMPLETED(every=20),<NewLine>        _lf_two(<NewLine>            run_engine_and_log_output,<NewLine>            evaluator,<NewLine>            val_loader))<NewLine></code></pre><NewLine><p>Also, I was wondering if I then wanted to run the evaluation outside of the training loop, and this time with different event handlers, would best practice be to create two different evaluation engines? One to pass into the training loop event handler into (__lf_two()), and one to run after my train.run ?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/pytorchnewbie,,pytorchnewbie,"June 19, 2020,  2:39pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Maybe I do not correctly understand the issue, but if you would like to run model evaluation every 20-th epochs and in the end of the training it is simply done like :</p><NewLine><pre><code class=""lang-python""><NewLine>trainer = ...<NewLine>evaluator = ...<NewLine><NewLine><NewLine>@trainer.on(Events.EPOCH_COMPLETED(every=20) | Events.COMPLETED)<NewLine>def validate_model():<NewLine>    state = evaluator.run(val_loader)<NewLine>    print(trainer.state.epoch, state.metrics)<NewLine><NewLine></code></pre><NewLine><p><a class=""mention"" href=""/u/pytorchnewbie"">@pytorchnewbie</a>, what do you think? Does it answer your question ?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry for the late reply!</p><NewLine><p>Well sort of - I want to do :</p><NewLine><p>handler = ModelCheckpoint(<br/><NewLine>“models/”,<br/><NewLine>“checkpoint”,<br/><NewLine>score_function=score_function,<br/><NewLine>n_saved=None<br/><NewLine>)</p><NewLine><p>evaluator.add_event_handler(Events.EPOCH_COMPLETED(every=50), handler, {‘textcnn’: model})</p><NewLine><p>except I want it to be the EPOCH_COMPLETED(every=50) of the training epoch, not the evaluator epoch…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>except I want it to be the EPOCH_COMPLETED(every=50) of the training epoch, not the evaluator epoch…</p><NewLine></blockquote><NewLine><p>It means that you would like to save a model (not the best one) every 50-th epoch completed. This is easy following the <a href=""https://pytorch.org/ignite/handlers.html#ignite.handlers.ModelCheckpoint"" rel=""nofollow noopener"">doc of ModelCheckpoint</a></p><NewLine><pre><code class=""lang-python"">handler = ModelCheckpoint(“models/”, “checkpoint”, n_saved=None)<NewLine><NewLine>trainer.add_event_handler(EPOCH_COMPLETED(every=50), handler)<NewLine></code></pre><NewLine><p>so, here is a complete working example</p><NewLine><pre><code class=""lang-python"">!rm -rf /tmp/models<NewLine><NewLine>import os<NewLine>from ignite.engine import Engine, Events<NewLine>from ignite.handlers import ModelCheckpoint<NewLine>from torch import nn<NewLine>trainer = Engine(lambda engine, batch: None)<NewLine>handler = ModelCheckpoint('/tmp/models', 'myprefix', n_saved=None, create_dir=True)<NewLine>model = nn.Linear(3, 3)<NewLine>trainer.add_event_handler(Events.EPOCH_COMPLETED(every=50), handler, {'mymodel': model})<NewLine>trainer.run([0], max_epochs=200)<NewLine><NewLine>!ls /tmp/models<NewLine></code></pre><NewLine><p>HTH</p><NewLine><p>PS. there was a typo in the docstring which i’ve just fixed</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>oh oops sorry I forgot to mention that I want score_function to running the evaluation loop once, because I want the MSE from that. However, in that case I need to pass in the evaluator engine somehow.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pytorchnewbie"">@pytorchnewbie</a> <code>score_function</code> option is used to save only the best N models.</p><NewLine><p>To resume, there are two options how to save models:</p><NewLine><ol><NewLine><li>either based on any event. For example in like previous answer, model is always saved every 50-th epochs no matter its quality.</li><NewLine><li>either based on a score. For example, evaluator engine provides a metric score, thus on the moment to save or not a model, <code>Checkpoint</code> verifies the current score, compares with registered score(s) and decides if we need or not to save current model.</li><NewLine></ol><NewLine><p>If you would like combine both, you need to provide your own <code>score_function</code> that would take into account event number and your score.</p><NewLine><pre><code class=""lang-python"">trainer = ...<NewLine>evaluator = ...<NewLine><NewLine>def score_function(_):<NewLine>    # MAKE SURE THAT MSE SCORE IS AVAILABLE WHEN IT IS CALLED<NewLine>    mse = evaluator.state.metrics[""mse""]<NewLine>    epoch = trainer.state.epoch<NewLine>    # Create your own logic when to save the model<NewLine>    # ... model with highest scores will be retained.<NewLine>    return some_score<NewLine><NewLine>to_save = {'model': model}<NewLine>handler = Checkpoint(..., score_function=score_function, ...)<NewLine><NewLine>trainer.add_event_handler(Events.COMPLETED, handler)<NewLine></code></pre><NewLine><p>HTH</p><NewLine><p>PS. If you want just to display eval score in the filename while saving a model every 50 epochs, unfortunately, this should be done by overriding the way you write the file…</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply.</p><NewLine><p>I want option <span class=""hashtag"">#2</span>.<br/><NewLine>However, it doesn’t work. I have written:</p><NewLine><pre><code class=""lang-auto"">def score_function(engine):<NewLine>        return evaluator.state.metrics[""mse""]<NewLine><NewLine>handler = ModelCheckpoint(<NewLine>        ""models/"", ""checkpoint"", score_function=score_function, n_saved=5<NewLine>    )<NewLine><NewLine>trainer.add_event_handler(<NewLine>        Events.EPOCH_COMPLETED(every=10), handler, {""textcnn"": model}<NewLine>    )<NewLine></code></pre><NewLine><p>The problem is that it saves the first 5 models every 10 epochs, so the model at epoch 10,20,30,40,50 . However, it doesn’t then replace them as the training continues (although the mse does drop)…</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please, see the docs about the score function:</p><NewLine><blockquote><NewLine><p>Objects with highest scores will be retained.</p><NewLine></blockquote><NewLine><p>You need to return for example:</p><NewLine><pre><code class=""lang-auto"">def score_function(engine):<NewLine>    return -evaluator.state.metrics[""mse""]<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah yes, thank you!! Really appreciate it</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi again. I apologize in advance but I’ve tried solving this for 2 hours now. I’m not sure what I did but suddenly the attaching doesn’t work anymore. It doesn’t recognize “mse” :</p><NewLine><p><img alt=""finalerr"" data-base62-sha1=""PDmoOiTeEoDyvfXlBlHl7CONPM"" height=""92"" src=""https://discuss.pytorch.org/uploads/default/original/3X/0/5/05d6397ade1e03c55a7bc44ddd881c465cf484fe.png"" width=""580""/></p><NewLine><p>Which  is weird because it should since below is what I call in train():</p><NewLine><pre><code class=""lang-auto"">model = get_network(cfg)<NewLine><NewLine>    to_save = {<NewLine>        ""model"": model,<NewLine>    }<NewLine><NewLine>    def score_function(engine):<NewLine>        return -evaluator.state.metrics[""mse""]<NewLine><NewLine>    handler = ModelCheckpoint(<NewLine>        ""models/"", ""checkpoint"", score_function=score_function, n_saved=5<NewLine>    )<NewLine><NewLine>    # Data Loaders<NewLine>    train_loader, val_loader = get_dataloaders(cfg, num_workers=cfg.data_loader_workers)<NewLine><NewLine>    # Your training loop<NewLine>    trainer = create_training_loop(model, cfg, ""trainer"", device=device)<NewLine><NewLine>    # Your evaluation loop<NewLine>    evaluator = create_evaluation_loop(model, cfg, ""evaluator"", device=device)<NewLine><NewLine><NewLine>    trainer.add_event_handler(Events.EPOCH_COMPLETED(every=10), handler, {""mlp"": model})<NewLine></code></pre><NewLine><p>And then here is a screenshot of  the actual code of create_evaluation_loop(), as well as the terminal where I use pdb to show that despite the for loop  where I attach running,  afterwards the engine still doesn’t have anything in :  engine.state.metrics …</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/05f7acb76868e29d696bae2e09178e54226969df"" href=""https://discuss.pytorch.org/uploads/default/original/3X/0/5/05f7acb76868e29d696bae2e09178e54226969df.jpeg"" title=""pdbscreen""><img alt=""pdbscreen"" data-base62-sha1=""QN1JdHUkuH5X0BTYccActUjIN9"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/5/05f7acb76868e29d696bae2e09178e54226969df_2_10x10.png"" height=""408"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/5/05f7acb76868e29d696bae2e09178e54226969df_2_690x408.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/5/05f7acb76868e29d696bae2e09178e54226969df_2_690x408.jpeg, https://discuss.pytorch.org/uploads/default/optimized/3X/0/5/05f7acb76868e29d696bae2e09178e54226969df_2_1035x612.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/0/5/05f7acb76868e29d696bae2e09178e54226969df_2_1380x816.jpeg 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">pdbscreen</span><span class=""informations"">2144×1268 776 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>You need to make sure that validation is run before saving the model.</p><NewLine><p>Here is a synthetic example you can play with to understand how it works and fix your problem</p><NewLine><pre><code class=""lang-python"">!rm -rf /tmp/ignite-mse-example/<NewLine><NewLine>import torch<NewLine><NewLine><NewLine>from ignite.engine import Engine, Events<NewLine>from ignite.handlers import Checkpoint, DiskSaver<NewLine>from ignite.metrics import MeanSquaredError<NewLine><NewLine><NewLine>num_epochs = 50<NewLine>num_train_samples_per_epoch = 25<NewLine>num_val_samples_per_epoch = 10<NewLine><NewLine>batch_size = 4<NewLine>num_features = 5<NewLine>val_targets = torch.rand(num_val_samples_per_epoch, batch_size, num_features)<NewLine>val_preds = val_targets + torch.rand(num_val_samples_per_epoch, batch_size, num_features) * 0.01<NewLine><NewLine><NewLine>trainer = Engine(lambda e, b: None)<NewLine><NewLine><NewLine>def validation_step(e, b):<NewLine>    i = e.state.iteration - 1    <NewLine>    err = (num_epochs - trainer.state.epoch - 1) * torch.rand(batch_size, num_features)    <NewLine>    y_preds = val_preds[i, ...] + err<NewLine>    y = val_targets[i, ...]<NewLine>    return y_preds, y<NewLine>    <NewLine><NewLine>evaluator = Engine(validation_step)<NewLine><NewLine>mse_metric = MeanSquaredError()<NewLine>mse_metric.attach(evaluator, ""mse"")<NewLine><NewLine><NewLine>val_every = 5<NewLine><NewLine>@trainer.on(Events.EPOCH_COMPLETED(every=val_every))<NewLine>def run_validation():<NewLine>    print(""{} : Run validation ..."".format(trainer.state.epoch))<NewLine>    val_data = range(num_val_samples_per_epoch)<NewLine>    evaluator.run(val_data)<NewLine>    print(""Val MSE:"", evaluator.state.metrics[""mse""])<NewLine><NewLine><NewLine>def score_function(_):<NewLine>    # !!! MAKE SURE THAT MSE SCORE IS ALREADY COMPUTED !!!<NewLine>    mse = evaluator.state.metrics[""mse""]<NewLine>    return -mse<NewLine><NewLine>to_save = {'model': torch.nn.Linear(10, 10)}<NewLine>handler = Checkpoint(<NewLine>    to_save, <NewLine>    DiskSaver(""/tmp/ignite-mse-example""), <NewLine>    n_saved=3, <NewLine>    score_function=score_function, <NewLine>    score_name=""val_mse"",<NewLine>    global_step_transform=lambda _1, _2: trainer.state.epoch<NewLine>)<NewLine><NewLine>trainer.add_event_handler(Events.EPOCH_COMPLETED(every=val_every), handler)<NewLine><NewLine><NewLine>train_data = range(num_train_samples_per_epoch)<NewLine>trainer.run(train_data, max_epochs=num_epochs)<NewLine><NewLine>!ls /tmp/ignite-mse-example/<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Got it, makes great sense. Thanks alot!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> ,"REPLY_DATE 1: June 20, 2020,  8:27am; <NewLine> REPLY_DATE 2: July 3, 2020,  5:42pm; <NewLine> REPLY_DATE 3: July 3, 2020,  9:22pm; <NewLine> REPLY_DATE 4: July 6, 2020,  8:48am; <NewLine> REPLY_DATE 5: July 6, 2020,  9:20am; <NewLine> REPLY_DATE 6: July 6, 2020,  9:27am; <NewLine> REPLY_DATE 7: July 6, 2020,  9:43am; <NewLine> REPLY_DATE 8: July 6, 2020, 10:06am; <NewLine> REPLY_DATE 9: July 12, 2020,  5:40pm; <NewLine> REPLY_DATE 10: July 12, 2020,  6:39pm; <NewLine> REPLY_DATE 11: July 13, 2020,  7:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: 1 Like; <NewLine> 
88559,Evaluation metrics,2020-07-09T12:22:57.590Z,2,86,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! This is a simple question but for some reason I’m having a hard time finding the answer. It says here (<a href=""https://pytorch.org/ignite/metrics.html"" rel=""nofollow noopener"">https://pytorch.org/ignite/metrics.html</a>) that metrics doesn’t store the memory of the entire output history of the model.</p><NewLine><p>However, the evaluation loop predicts y values (a regressive MLP), and the MSE is computed with the ground truth y values per batch. Therefore, the “mse” returned by the evaluation metrics thing gets overriden every batch. I want to have the “mse” of the entire validation set - this seems not compatible with the current setup.</p><NewLine><pre><code class=""lang-auto"">def create_evaluation_loop(<NewLine>    model: nn.Module, cfg: DictConfig, name: str, device=""cpu"",<NewLine>) -&gt; Engine:<NewLine><NewLine>    # Loss<NewLine>    if cfg.sum:<NewLine>        loss_fn = torch.nn.MSELoss(reduction=""sum"")<NewLine>    else:<NewLine>        loss_fn = torch.nn.MSELoss()<NewLine><NewLine>    def _inference(engine, batch):<NewLine>        model.eval()<NewLine>        with torch.no_grad():<NewLine>            x, y = batch<NewLine>            x, y = x.to(device), y.to(device)<NewLine>            y_pred = model(x)<NewLine><NewLine>            if cfg.house:<NewLine>                factor = 6.0<NewLine>            else:<NewLine>                factor = 1777.0<NewLine><NewLine>            # pdb.set_trace()<NewLine><NewLine>            y = y * factor<NewLine>            y_pred = y_pred * factor<NewLine><NewLine>            if cfg.envelope:<NewLine>                y_hat_env = torch.tensor(np.abs(hilbert(y.cpu().detach().numpy())))<NewLine>                y_pred_hat_env = torch.tensor(<NewLine>                    np.abs(hilbert(y_pred.cpu().detach().numpy()))<NewLine>                )<NewLine>                mse_val = loss_fn(y_hat_env, y_pred_hat_env).item()<NewLine>            else:<NewLine>                mse_val = loss_fn(y, y_pred).item()<NewLine><NewLine>        # Anything you want to log must be returned in this dictionary<NewLine>        # pdb.set_trace()<NewLine>        infer_dict = {<NewLine>            ""loss"": mse_val,<NewLine>            ""y_pred"": y_pred,<NewLine>            ""y"": y,<NewLine>            ""ypred_first"": [y[0], y_pred[0]],  # * mean + stdv,  # * mean + stdv,<NewLine>        }<NewLine><NewLine>        return infer_dict<NewLine><NewLine>    engine = Engine(_inference)<NewLine><NewLine>    engine.logger = setup_logger(name=name)<NewLine><NewLine>    metrics = {<NewLine>        ""mse"": Loss(<NewLine>            loss_fn,<NewLine>            output_transform=lambda infer_dict: (infer_dict[""y_pred""], infer_dict[""y""]),<NewLine>        ),<NewLine>    }<NewLine><NewLine>    for name, metric in metrics.items():<NewLine>        metric.attach(engine, name)<NewLine><NewLine>    return engine<NewLine><NewLine></code></pre><NewLine><p>Also,  what is the  difference between the trainer which returns an “output” and the evaluator which returns the infer_dict? The difference comes when you do “metrics = {<br/><NewLine>“mse”: Loss(<br/><NewLine>loss_fn,<br/><NewLine>output_transform=lambda infer_dict: (infer_dict[“y_pred”], infer_dict[“y”]),<br/><NewLine>),<br/><NewLine>}” and then “attach” the engine and name together… but im not really quite clear on what that does. I know for fact that with the trainer, the output dictionary is very online in that it stores no history. So  I’d assume the “attach” stores the history, but then the link above specifically says that it doesn’t?</p><NewLine><p>Thanks for your time!</p><NewLine></div>",https://discuss.pytorch.org/u/pytorchnewbie,,pytorchnewbie,"July 9, 2020, 12:22pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pytorchnewbie"">@pytorchnewbie</a> sorry I saw the question but forgot to answer.</p><NewLine><p>If I understand correctly the problem, you would like to compute MSE metric (<a href=""https://pytorch.org/ignite/metrics.html#ignite.metrics.MeanSquaredError"" rel=""nofollow noopener"">https://pytorch.org/ignite/metrics.html#ignite.metrics.MeanSquaredError</a>).</p><NewLine><p>To do that create a validation engine and attach MSE metric to that:</p><NewLine><pre><code class=""lang-python"">from ignite.engine import create_supervised_evaluator<NewLine>from ignite.metrics import MeanSquaredError<NewLine><NewLine>val_metrics = {<NewLine>    ""mse"": MeanSquaredError(),<NewLine>}<NewLine>evaluator = create_supervised_evaluator(model, metrics=val_metrics)<NewLine><NewLine>res = evaluator.run(val_loader)<NewLine>print(res.metrics[""mse""])<NewLine></code></pre><NewLine><blockquote><NewLine><p>Also, what is the difference between the trainer which returns an “output” and the evaluator which returns the infer_dict?</p><NewLine></blockquote><NewLine><p>Output of an engine is not restricted at all, but should be coherent with how we would like to use it after inside attached handlers/metrics. See <a href=""https://pytorch.org/ignite/concepts.html"" rel=""nofollow noopener"">concepts</a> for more details.<br/><NewLine>If we would like to log batch loss during the training, then we need logically to return its value. But in addition if we would like to do something with predictions or compute another error metric etc we can also put everything into output and work with the output inside attached handler.<br/><NewLine>Same for validation engine. If we use it to compute metrics, we need to output predictions and target. If we set it to do inference=compute prediction and write the prediction after inside a save_handler. We output only predictions…</p><NewLine><p>Hope it is more clear.</p><NewLine><blockquote><NewLine><p>So I’d assume the “attach” stores the history, but then the link above specifically says that it doesn’t?</p><NewLine></blockquote><NewLine><p>It depends on metric. Some metric can compute in online manner: storing only certain internal variables. For example, MSE = 1/N sum ( sample_err^2 ). It can be computed as accumulation of all sample_err^2 and finally divided by number of seen samples. So, practically, we do not store a list of <code>[y_pred1, y_pred2, ...]</code> and <code>[y1, y2, ...]</code> to compute sample_err etc.<br/><NewLine>But there are metrics, e.g. median “something” error that can not be computed like that and we need to store two lists and execute computation on stored history.</p><NewLine><p>You can also compute any metric on the training dataset during the training (see <a href=""https://pytorch.org/ignite/quickstart.html#id1"" rel=""nofollow noopener"">here</a> how to do this). But, in this case you should be aware of the fact that model is also changing. So, it means that first predictions in the begining of an epoch will be worse than the last ones, but everything will be accounted to compute final overall training dataset metric. For example, accuracy on an epoch of 3 batches (bs=16) can give the following: <code>num_correct_per_batch = [0, 4, 8]</code>. Final accuracy is (0 + 4 + 8) / (16 + 16 + 16). However, on validation it could give (8 + 8 + 8)/(16 + 16 + 16).</p><NewLine><p>Please, let me know if it answers your questions.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!  thanks for your answer but not quite…</p><NewLine><p>I’m asking:  the metrics  is calculated per batch, but I want the mse on the entire validation dataset. How do I do this? Do i have to store all the predictions somehow? How do I “store”  them if so?</p><NewLine><p>Also, when you  say “it depends on the metric”. This sounds  like behind the scenes  ignite figures out whether to hold on the the storage of the history of predictions or not. <span class=""hashtag"">#1</span>) Does that mean that ignite’ MSE will automatically calculate the MSE on the entire validation set, not just one batch at a time? <span class=""hashtag"">#2</span>) But how do I do it  for a personal metric that ignite doesn’t have already?</p><NewLine><p>Thanks!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I’m asking: the metrics is calculated per batch, but I want the mse on the entire validation dataset. How do I do this? Do i have to store all the predictions somehow? How do I “store” them if so?</p><NewLine></blockquote><NewLine><p>There is <code>EpochMetric</code> which computes a metric using user’s function on the entire history. <a href=""https://pytorch.org/ignite/metrics.html#ignite.metrics.EpochMetric"" rel=""nofollow noopener"">https://pytorch.org/ignite/metrics.html#ignite.metrics.EpochMetric</a></p><NewLine><blockquote><NewLine><p>This sounds like behind the scenes ignite figures out whether to hold on the the storage of the history of predictions or not.</p><NewLine></blockquote><NewLine><p>Yes, each metric implementation know how to compute itself.</p><NewLine><blockquote><NewLine><ol><NewLine><li>Does that mean that ignite’ MSE will automatically calculate the MSE on the entire validation set, not just one batch at a time?</li><NewLine></ol><NewLine></blockquote><NewLine><p>Yes, MSE metric attached to an engine compute MSE on the entire input data.</p><NewLine><blockquote><NewLine><p><span class=""hashtag"">#2</span>) But how do I do it for a personal metric that ignite doesn’t have already?</p><NewLine></blockquote><NewLine><p>See here : <a href=""https://pytorch.org/ignite/metrics.html#how-to-create-a-custom-metric"" rel=""nofollow noopener"">https://pytorch.org/ignite/metrics.html#how-to-create-a-custom-metric</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pytorchnewbie; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: July 10, 2020, 12:24am; <NewLine> REPLY_DATE 2: July 10, 2020,  5:43am; <NewLine> REPLY_DATE 3: July 10, 2020,  7:50am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
83714,Change model&rsquo;s attribute during training,2020-06-01T14:27:13.353Z,0,101,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello.<br/><NewLine>I’d like to change my model’s attribute during training.<br/><NewLine>In order to that, now I’m calling my model as global variable.</p><NewLine><p>For example:</p><NewLine><pre><code class=""lang-auto"">trainer = create_supervised_trainer(~)<NewLine><NewLine>@trainer.on(Events.EPOCH_STARTED(once=10)<NewLine>def change_attribute(engine):<NewLine>    global model<NewLine>    model.my_attribute = new_attribute<NewLine></code></pre><NewLine><p>Is there more clear way using trainer?<br/><NewLine>I investigated <code>trainer.__dict__</code> and <code>trainer.state.__dict__</code>, but there’s no attributes that keeps model.</p><NewLine></div>",https://discuss.pytorch.org/u/FruitVinegar,(NHK),FruitVinegar,"June 1, 2020,  2:27pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/fruitvinegar"">@FruitVinegar</a></p><NewLine><p>Trainer defined as</p><NewLine><pre><code class=""lang-auto"">trainer = create_supervised_trainer(~)<NewLine></code></pre><NewLine><p>is just a configured Engine. Engine, by default, does not store any info on model, optimizer, loss etc.<br/><NewLine>This can be done if needed like that:</p><NewLine><pre><code class=""lang-python"">trainer = ...<NewLine># trainer.state is not None in recent nightly releases: 0.4.0.dev202005XX<NewLine>trainer.state.model = model<NewLine>trainer.state.optimizer = optimizer<NewLine># otherwise, in stable v0.3.0, you need to set attributes in a handler attached to `Events.STARTED`<NewLine><NewLine>@trainer.on(Events.EPOCH_STARTED(once=10)<NewLine>def change_attribute(engine):<NewLine>    trainer.state.model.my_attribute = new_attribute<NewLine></code></pre><NewLine><p>About using Engine, see <a href=""https://pytorch.org/ignite/concepts.html#engine"" rel=""nofollow noopener"">https://pytorch.org/ignite/concepts.html#engine</a> .<br/><NewLine>About State, please, see the documentation on it : <a href=""https://pytorch.org/ignite/engine.html#ignite.engine.State"" rel=""nofollow noopener"">https://pytorch.org/ignite/engine.html#ignite.engine.State</a> and <a href=""https://pytorch.org/ignite/concepts.html#state"" rel=""nofollow noopener"">https://pytorch.org/ignite/concepts.html#state</a></p><NewLine><blockquote><NewLine><p>A good practice is to use <code>State</code> also as a storage of user data created in update or handler functions. For example, we would like to save new_attribute in the state:</p><NewLine></blockquote><NewLine><pre><code class=""lang-auto"">def user_handler_function(engine): <NewLine>    engine.state.new_attribute = 12345<NewLine></code></pre><NewLine><p>HTH</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: June 1, 2020,  3:17pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
83171,RuntimeError: multi-target not supported custom dataset in ignite,2020-05-28T03:23:23.331Z,2,90,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello! I am trying to train a neural network with a preprocessed CIFAR10 : I managed to pass them from a .npy extension to a .pt extension and load them on the code</p><NewLine><pre><code class=""lang-auto"">train_images=torch.from_numpy(torch.load('train_images.pt')).type(torch.LongTensor)<NewLine>test_images=torch.from_numpy(torch.load('test_images.pt')).type(torch.LongTensor)<NewLine>train_labels=torch.from_numpy(torch.load('train_labels.pt')).type(torch.LongTensor)<NewLine>test_labels=torch.from_numpy(torch.load('test_labels.pt')).type(torch.LongTensor)<NewLine><NewLine><NewLine>train_transform = Compose([<NewLine>    ToPILImage(mode='RGB'),<NewLine>    Resize(image_size, BICUBIC),<NewLine>    RandomAffine(degrees=2, translate=(0.02, 0.02), scale=(0.98, 1.02), shear=2, fillcolor=(124,117,104)),<NewLine>    RandomHorizontalFlip(),<NewLine>    Pad(4),<NewLine>    ToTensor(),<NewLine>    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])<NewLine>])<NewLine><NewLine>test_transform = Compose([<NewLine>    ToPILImage(mode='RGB'),<NewLine>    Resize(image_size, BICUBIC),<NewLine>    Pad(4),    <NewLine>    ToTensor(),<NewLine>    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])<NewLine>])<NewLine><NewLine>class CIFAR100Dataset(Dataset):<NewLine>    <NewLine>    def __init__(self, images, labels=None, transforms=None):<NewLine>        self.X = images<NewLine>        self.y = labels<NewLine>        self.transforms=train_transform<NewLine>    <NewLine>    def __len__(self):<NewLine>        return(len(self.X))<NewLine>        <NewLine>    def __getitem__(self, i):<NewLine>        data = self.X[i]<NewLine>        data = np.asarray(data).astype(np.uint8).reshape(32, 32, 3)<NewLine>        if self.transforms:<NewLine>            data = self.transforms(data)<NewLine>        if self.y is not None:<NewLine>            return (data, self.y[i])<NewLine>        else:<NewLine>            return data<NewLine><NewLine>train_dataset = CIFAR100Dataset(train_images, train_labels, train_transform)<NewLine>test_dataset = CIFAR100Dataset(test_images, test_labels, test_transform)<NewLine><NewLine></code></pre><NewLine><p>The problem is…when I try to train I get this message:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: multi-target not supported at /opt/conda/conda-bld/pytorch_1587428270644/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:18<NewLine></code></pre><NewLine><p>So, I take a look into my target data and the size is [100,100], so I change it to [100] with</p><NewLine><pre><code class=""lang-auto"">y=(torch.max(y, 1)[1]).type(torch.LongTensor).cuda()<NewLine></code></pre><NewLine><p>in this part of the code</p><NewLine><pre><code class=""lang-auto"">def update_fn(engine, batch):<NewLine>    model.train()<NewLine>    <NewLine>    x = convert_tensor(batch[0], device=device, non_blocking=True)<NewLine>    y = convert_tensor(batch[1], device=device, non_blocking=True)<NewLine>    y=(torch.max(y, 1)[1]).type(torch.LongTensor).cuda()<NewLine>    y_pred = model(x)<NewLine>    # Compute loss <NewLine>    loss = criterion(y_pred,y)  <NewLine><NewLine>    optimizer.zero_grad()<NewLine>    if use_amp:<NewLine>        with amp.scale_loss(loss, optimizer, loss_id=0) as scaled_loss:<NewLine>            scaled_loss.backward()<NewLine>    else:<NewLine>        loss.backward()<NewLine>    optimizer.step()<NewLine>    <NewLine>    return {<NewLine>        ""batchloss"": loss.item(),<NewLine>    <NewLine></code></pre><NewLine><p>If I print y (my target)</p><NewLine><pre><code class=""lang-auto"">tensor([27, 40, 90, 26, 50, 41, 27, 93, 84, 82, 76, 85, 93, 57, 68, 89, 25, 18,<NewLine>         9, 18, 40,  7, 26, 84, 64, 73, 43, 74, 49, 18, 22, 26, 31,  7, 67, 42,<NewLine>         3, 96, 53, 38, 47, 99, 26, 55, 64, 22, 29, 81, 90, 19, 16, 79, 93, 17,<NewLine>        95, 42, 34, 25, 29, 47,  2, 43, 32, 94, 13,  9, 14, 45, 90, 92,  2,  9,<NewLine>        11, 62, 75, 54,  7, 45, 68,  5, 24, 66, 36, 72, 43, 68,  4, 56, 57, 31,<NewLine>        52, 27, 49, 19,  2, 88, 33, 11, 48, 59], device='cuda:0')<NewLine><NewLine></code></pre><NewLine><p>The shape is now fine</p><NewLine><pre><code class=""lang-auto"">torch.Size([100])<NewLine></code></pre><NewLine><p>Note: I used this same change in another code that is not ignite and it works:</p><NewLine><pre><code class=""lang-auto"">    model.eval()<NewLine>    for data, target in test_loader:<NewLine>        # move tensors to GPU if CUDA is available<NewLine>        <NewLine>       # if contador ==35000<NewLine>        if train_on_gpu:<NewLine>            data, target = data.cuda(), target.cuda()<NewLine>          <NewLine>        # forward pass: compute predicted outputs by passing inputs to the model<NewLine><NewLine>        output = model(data)<NewLine>        values,indices=torch.max(target,1)<NewLine>        target=indices<NewLine></code></pre><NewLine><p>So, my question here is…Why is not working on ignite? I am missing something on the update_fn function?</p><NewLine><p>Regards,</p><NewLine></div>",https://discuss.pytorch.org/u/Tanya_Boone,(Tanya Boone),Tanya_Boone,"May 28, 2020,  3:23am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The shape and the values of <code>y</code> look correct, so Ignite shouldn’t raise an issue.<br/><NewLine>Are you sure you’ve rerun the complete code (or cells)?<br/><NewLine>If so, could you directly print the shapes of <code>y_pred</code> and <code>y</code> before passing them to the criterion?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, this is what I am doing</p><NewLine><pre><code class=""lang-auto""># x.shape<NewLine>torch.Size([100, 3, 40, 40])<NewLine><NewLine>#y values<NewLine>tensor([ 1, 30, 63, 70, 37, 29, 40, 40, 16, 15,  3, 31, 34, 42, 96, 71, 53, 25,<NewLine>        61, 64,  5, 34, 68, 23, 21, 32, 98, 86, 41, 15, 82, 77, 44, 79, 42, 28,<NewLine>         9, 86, 17, 42, 98, 25, 66, 28, 15, 56, 45, 13, 86, 70, 37, 94, 68, 65,<NewLine>         2, 90, 24, 11, 14, 18, 29, 46, 44,  7, 12, 31, 51, 80, 56, 35, 79, 92,<NewLine>        89, 25, 17, 80, 90, 83, 19, 44, 17, 57, 98, 29, 20, 21, 29, 46, 82, 51,<NewLine>        35, 51,  2, 27, 70, 17,  6, 35, 23, 48], device='cuda:0')<NewLine>#y shape<NewLine>torch.Size([100])<NewLine><NewLine>#y_pred<NewLine>torch.Size([100, 100])<NewLine></code></pre><NewLine><p>I am comparing with the same code but regular CIFAR100 dataset (not custom) and I get the same sizes<br/><NewLine>(well in this case the batch size is 800)</p><NewLine><pre><code class=""lang-auto"">torch.Size([800, 3, 40, 40])<NewLine>torch.Size([800])<NewLine>torch.Size([800, 100])<NewLine><NewLine></code></pre><NewLine><p>I am sure I am running the latest file, but I don’t know is not working on ignite… <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>That should be correct. Let’s see, if <a class=""mention"" href=""/u/vfdev-5"">@vfdev-5</a> knows, what might be going on.<br/><NewLine>Could you post a reproducible code snippet including the Ignite code?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/tanya_boone"">@Tanya_Boone</a> for me the problem is not related to ignite but with datatypes, criterion used etc…</p><NewLine><p>Please, post a minimal code, maybe with random data to perform a single training step.</p><NewLine><p>PS: <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> thanks for helping on the issue</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Tanya_Boone; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: May 28, 2020,  6:03am; <NewLine> REPLY_DATE 2: May 28, 2020,  6:15am; <NewLine> REPLY_DATE 3: May 28, 2020,  6:17am; <NewLine> REPLY_DATE 4: May 28, 2020,  7:37am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
82506,Average time per epoch calculation on pytorch ignite,2020-05-22T15:53:23.224Z,2,149,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I need to save a file with the average elapsed time per epoch and I found this function on the documentation:</p><NewLine><p><a class=""onebox"" href=""https://pytorch.org/ignite/_modules/ignite/handlers/timing.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/ignite/_modules/ignite/handlers/timing.html</a></p><NewLine><pre><code class=""lang-auto""> t = Timer(average=True)<NewLine>for _ in range(10):<NewLine>       work()<NewLine>       idle()<NewLine>       t.step()<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">def save_time(engine):<NewLine>     torch.save(t.value(),'time.pt´)<NewLine><NewLine>trainer.add_event_handler(Events.EPOCH_STARTED(every=3), save_time)<NewLine></code></pre><NewLine><p>this works but It shows the elapsed time in 3 epochs and I suppose the file overwrites every 3 epochs but <strong>what I want is to calculate the average time just once for the first epoch and stop the timer</strong>. I don’t want to divide timer value between 3, also I don’t want to generate one file per epoch. One option is to run my code with  just one epoch but I am running several models and I would like to generate this just once every time I am going to train.</p><NewLine><p>How can I achieve this? <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/></p><NewLine><p>pd: the documentation show an example doing it by iteration, but I am unable to translate this example to epoch. I believe the documentation should extend to epoch example as well.</p><NewLine><p>Regards,</p><NewLine></div>",https://discuss.pytorch.org/u/Tanya_Boone,(Tanya Boone),Tanya_Boone,"May 22, 2020,  3:53pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/tanya_boone"">@Tanya_Boone</a> Thank you for this question.</p><NewLine><p>I think that several things could help you.</p><NewLine><p>First, you can use <code>Timer</code> with filtered event. I think that in your case, you should try the following code to reset the timer every 3 epochs (started)</p><NewLine><pre><code class=""lang-python"">timer.attach(trainer,<NewLine>             start=Events.EPOCH_STARTED(every=3),<NewLine>             resume=Events.ITERATION_STARTED,<NewLine>             pause=Events.ITERATION_COMPLETED)<NewLine></code></pre><NewLine><p>An other option is to use directly <code>Timer</code> (without <code>step</code> to do not average)</p><NewLine><pre><code class=""lang-auto"">timer = Timer()<NewLine><NewLine>@trainer.on(Events.STARTED)<NewLine>def time_start():<NewLine>    # reset the timer at the beginning<NewLine>    timer.reset()<NewLine><NewLine>@trainer.on(Events.EPOCH_STARTED(every=3))<NewLine>def time_start():<NewLine>    # use value() to get the time<NewLine>    print(""timer of 3 epochs :"", timer.value())<NewLine>    # reset the timer from now for 3 next epochs<NewLine>    timer.reset()<NewLine></code></pre><NewLine><p>Last point I would like to mention, we recently added epoch timers in <code>trainer.state.timers</code>. That is another option which should help you to mesure epoch times</p><NewLine><pre><code class=""lang-python"">@trainer.on(Events.EPOCH_COMPLETED)<NewLine>def print_timers():<NewLine>    # trainer.state.timers is a dict<NewLine>    print(""timers="", trainer.state.timers)<NewLine></code></pre><NewLine><p>If you need more support, don’t hesitate to ask, it would be a pleasure to help <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>HTH</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks Sylvain !</p><NewLine><p><a class=""mention"" href=""/u/tanya_boone"">@Tanya_Boone</a> please let us know if Sylvain’s answer solves your problem. Thanks !</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""82506"" data-username=""sdesrozis""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/sdesrozis/40/24628_2.png"" width=""20""/> sdesrozis:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">timer.attach(trainer,<NewLine>             start=Events.EPOCH_STARTED(every=3),<NewLine>             resume=Events.ITERATION_STARTED,<NewLine>             pause=Events.ITERATION_COMPLETED)<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>Hello,</p><NewLine><p>I am trying with this method</p><NewLine><pre><code class=""lang-auto"">timer.attach(trainer,start=Events.EPOCH_STARTED(every=1),<NewLine>    resume=Events.ITERATION_STARTED,<NewLine>    pause=Events.ITERATION_COMPLETED)<NewLine></code></pre><NewLine><p>After I want to print the value</p><NewLine><p>print(“timers=”, trainer.state.timers)</p><NewLine><p>Engine run is terminating due to exception: ‘State’ object has no attribute 'timers"". I changed to timer but I am getting the same error <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>This feature is not in stable 0.3.0. You have to use git version or nightly version. Sorry to miss that point.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>To install nightly release: <a href=""https://github.com/pytorch/ignite#nightly-releases"" rel=""nofollow noopener"">https://github.com/pytorch/ignite#nightly-releases</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/sdesrozis; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Tanya_Boone; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/sdesrozis; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: May 22, 2020,  7:55pm; <NewLine> REPLY_DATE 2: May 23, 2020, 11:20am; <NewLine> REPLY_DATE 3: May 23, 2020,  1:33pm; <NewLine> REPLY_DATE 4: May 23, 2020,  1:37pm; <NewLine> REPLY_DATE 5: May 23, 2020,  5:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
81854,Training ELU network with pytorch ignite,2020-05-18T16:03:14.066Z,1,106,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I am trying to train this network on pytorch ignite but the loss doesn’t increase a single point after more than 20 epochs…</p><NewLine><p>I am working with Deep Neural Networks and I’ve been trying to emulate ELU network</p><NewLine><pre><code class=""lang-auto"">class CIFARClassifierELUPaper(nn.Module):<NewLine>    def __init__(self, num_classes=100):<NewLine>        super(CIFARClassifierELUPaper, self).__init__()<NewLine>        self.main = nn.Sequential(<NewLine>            # Block 1<NewLine>            nn.Conv2d(3, 384, 3, padding=1), # 32<NewLine>            nn.ELU(),<NewLine>            nn.MaxPool2d(2, 2), # 16<NewLine>            # Block 2<NewLine>            nn.Conv2d(384, 384, 1, padding=0), # 16<NewLine>            nn.ELU(),<NewLine>            nn.Conv2d(384, 384, 2, padding=1), # 17<NewLine>            nn.ELU(),<NewLine>            nn.Conv2d(384, 640, 2, padding=1), # 18<NewLine>            nn.ELU(),<NewLine>            nn.MaxPool2d(2, 2), # 9<NewLine>            nn.Dropout2d(0.1),<NewLine>            # Block 3<NewLine>            nn.Conv2d(640, 640, 1, padding=0), # 9<NewLine>            nn.ELU(),<NewLine>            nn.Conv2d(640, 768, 2, padding=1), # 10<NewLine>            nn.ELU(),<NewLine>            nn.Conv2d(768, 768, 2, padding=1), # 11<NewLine>            nn.ELU(),<NewLine>            nn.Conv2d(768, 768, 2, padding=1), # 12<NewLine>            nn.ELU(),<NewLine>            nn.MaxPool2d(2, 2), # 6<NewLine>            nn.Dropout2d(0.2),<NewLine>            # Block 4<NewLine>            nn.Conv2d(768, 768, 1, padding=0), # 6<NewLine>            nn.ELU(),<NewLine>            nn.Conv2d(768, 896, 2, padding=1), # 7<NewLine>            nn.ELU(),<NewLine>            nn.Conv2d(896, 896, 2, padding=1), # 8<NewLine>            nn.ELU(),<NewLine>            nn.MaxPool2d(2, 2), # 4<NewLine>            nn.Dropout2d(0.3),<NewLine>            # Block 5num_classes<NewLine>            nn.Conv2d(896, 896, 1, padding=0), # 4<NewLine>            nn.ELU(),<NewLine>            nn.Conv2d(896, 1024, 2, padding=1), # 5<NewLine>            nn.ELU(),<NewLine>            nn.Conv2d(1024, 1024, 2, padding=1), # 6<NewLine>            nn.ELU(),<NewLine>            nn.MaxPool2d(2, 2), # 3<NewLine>            nn.Dropout2d(0.4),<NewLine>            # Block 6<NewLine>            nn.Conv2d(1024, 1024, 1, padding=0), # 3<NewLine>            nn.ELU(),<NewLine>            nn.Conv2d(1024, 1152, 2, padding=0), # 2<NewLine>            nn.ELU(),<NewLine>            nn.MaxPool2d(2, 2), # 1<NewLine>            nn.Dropout2d(0.5),<NewLine>            # Block 7<NewLine>            nn.Conv2d(1152, 1152, 1, padding=0), #  1<NewLine>            nn.ELU(),<NewLine>            nn.Dropout2d(0.0),<NewLine>            # Block 8<NewLine>            nn.Conv2d(1152, num_classes, 1, padding=0), # 1<NewLine>        )<NewLine> <NewLine>    def forward(self, x):<NewLine>        return self.main(x).view(x.size(0),-1)<NewLine><NewLine>model=CIFARClassifierELUPaper()<NewLine></code></pre><NewLine><p>I am trying to train CIFAR 100 and I am training with:</p><NewLine><p>SGD<br/><NewLine>learning rate 0.1<br/><NewLine>ZCA whitening and all the others preprocessing (Random horizontal flip and randomcrops) steps</p><NewLine><p>but both eval and train losses doesn’t increase or decrease ever (I tested with 20 epochs of 80)…</p><NewLine><p>This are the metrics<br/><NewLine>] The metrics on training are {‘Loss’: 4.6055209875106815, ‘Accuracy’: 0.0082, ‘Precision’: 8.2e-05, ‘Recall’: 0.01, ‘Top-5 Accuracy’: 0.0474}<br/><NewLine>The metrics on testing are {‘Loss’: 4.6053833532333375, ‘Accuracy’: 0.01, ‘Precision’: 0.0001, ‘Recall’: 0.01, ‘Top-5 Accuracy’: 0.05}</p><NewLine><p>Should I wait longer or there is something wrong with my network? I trained this before getting 64% but on the other one the only difference is that I added batch normalization…but I want to train from the begging and try to get the 73% …Also the  the publication suggest that BN is not necessary, so it shouldn’t be a problem right?</p><NewLine></div>",https://discuss.pytorch.org/u/Tanya_Boone,(Tanya Boone),Tanya_Boone,"May 18, 2020,  4:03pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/tanya_boone"">@Tanya_Boone</a></p><NewLine><p>could you please share complete training script to be able to run it locally or in Colab ?</p><NewLine><p>Probably, input data normalization and high learning rate may be responsible for that …</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sure,</p><NewLine><pre><code class=""lang-auto"">from torchvision import datasets<NewLine>import torchvision.transforms as transforms<NewLine>from torch.utils.data.sampler import SubsetRandomSampler<NewLine>import os<NewLine>import numpy as np<NewLine>import torch.optim as optim<NewLine>import random<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>from torch.autograd import Variable<NewLine>from collections import OrderedDict<NewLine>import math<NewLine>from contextlib import redirect_stdout<NewLine>from itertools import chain<NewLine>import torch.nn.functional as F<NewLine>from torch.optim.lr_scheduler import ExponentialLR,MultiStepLR<NewLine>from torchvision.datasets.cifar import CIFAR100, CIFAR10<NewLine>from torchvision.transforms import Compose, RandomCrop, Pad, RandomHorizontalFlip, Resize, RandomAffine,LinearTransformation<NewLine>from torchvision.transforms import ToTensor, Normalize<NewLine>from torch.utils.data import Subset<NewLine>from PIL.Image import BICUBIC<NewLine>from torch.utils.data import DataLoader<NewLine>import matplotlib.pylab as plt<NewLine>from apex import amp<NewLine>from ignite.utils import convert_tensor<NewLine>from ignite.engine import Engine, Events, create_supervised_evaluator<NewLine>from ignite.metrics import RunningAverage, Accuracy, Precision, Recall, Loss, TopKCategoricalAccuracy<NewLine>#from ignite.contrib.handlers import TensorboardLogger<NewLine>#from ignite.contrib.handlers.tensorboard_logger import OutputHandler, OptimizerParamsHandler<NewLine>from datetime import datetime<NewLine>from ignite.contrib.handlers import CustomPeriodicEvent<NewLine>import logging<NewLine>from ignite.handlers import ModelCheckpoint, EarlyStopping, TerminateOnNan<NewLine>from ignite.engine import Engine, Events<NewLine>from ignite.handlers import Checkpoint, DiskSaver<NewLine>from ignite.contrib.handlers import ProgressBar<NewLine>import dill<NewLine>import pickle <NewLine>from ignite.engine import create_supervised_trainer<NewLine>assert torch.cuda.is_available()<NewLine>assert torch.backends.cudnn.enabled, ""NVIDIA/Apex:Amp requires cudnn backend to be enabled.""<NewLine>torch.backends.cudnn.benchmark = True<NewLine><NewLine>device = ""cuda""<NewLine><NewLine>path = "".""<NewLine>image_size = 32<NewLine><NewLine>train_transform = Compose([<NewLine>    RandomCrop(32),<NewLine>    RandomHorizontalFlip(),<NewLine>    Pad(4),<NewLine>    ToTensor(),<NewLine>    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),<NewLine>    #LinearTransformation(transformation_matrix, mean_vector), <NewLine> ])<NewLine><NewLine><NewLine>test_transform = Compose([ <NewLine>    Pad(4),   <NewLine>    ToTensor(),<NewLine>    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) <NewLine>])<NewLine><NewLine>train_dataset = CIFAR100(root=path, train=True, transform=train_transform, download=True)<NewLine>test_dataset = CIFAR100(root=path, train=False, transform=test_transform, download=True)<NewLine><NewLine>train_eval_indices = [random.randint(0, len(train_dataset) - 1) for i in range(len(test_dataset))]<NewLine>train_eval_dataset = Subset(train_dataset, train_eval_indices)<NewLine><NewLine><NewLine>batch_size = 100<NewLine>num_workers = 10<NewLine><NewLine>train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, <NewLine>                          shuffle=True, drop_last=True, pin_memory=True)<NewLine><NewLine>test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, <NewLine>                         shuffle=False, drop_last=False, pin_memory=True)<NewLine><NewLine>eval_train_loader = DataLoader(train_eval_dataset, batch_size=batch_size, num_workers=num_workers, <NewLine>                               shuffle=False, drop_last=False, pin_memory=True)<NewLine><NewLine>def seed_everything(seed):<NewLine>    random.seed(seed)<NewLine>    os.environ['PYTHONHASHSEED'] = str(seed)<NewLine>    np.random.seed(seed)<NewLine>    torch.manual_seed(seed)<NewLine>    torch.cuda.manual_seed(seed)<NewLine>    torch.backends.cudnn.deterministic = True<NewLine><NewLine>seed_everything(17)<NewLine><NewLine>class CIFARClassifierELUPaper(nn.Module):<NewLine><NewLine>    def __init__(self, num_classes=100):<NewLine>        super(CIFARClassifierELUPaper, self).__init__()<NewLine>        <NewLine>        self.main = nn.Sequential(<NewLine>            # Block 1<NewLine>            nn.Conv2d(3, 384, 3, padding=1), # 32<NewLine>            nn.ELU(),<NewLine>            #nn.BatchNorm2d(384,eps=1e-4,momentum=0.1),<NewLine>            nn.MaxPool2d(2, 2), # 16<NewLine>            # Block 2<NewLine>            nn.Conv2d(384, 384, 1, padding=0), # 16<NewLine>            nn.ELU(),<NewLine>           # nn.BatchNorm2d(384,eps=1e-4,momentum=0.1),<NewLine>            nn.Conv2d(384, 384, 2, padding=1), # 17<NewLine>            nn.ELU(),<NewLine>            #nn.BatchNorm2d(384,eps=1e-4,momentum=0.1),<NewLine>            nn.Conv2d(384, 640, 2, padding=1), # 18<NewLine>            nn.ELU(),<NewLine>            #nn.BatchNorm2d(640,eps=1e-4,momentum=0.1),<NewLine>            nn.MaxPool2d(2, 2), # 9<NewLine>            nn.Dropout2d(0.1),<NewLine>            # Block 3<NewLine>            nn.Conv2d(640, 640, 1, padding=0), # 9<NewLine>            nn.ELU(),<NewLine>           # nn.BatchNorm2d(640,eps=1e-4,momentum=0.1),<NewLine>            nn.Conv2d(640, 768, 2, padding=1), # 10<NewLine>            nn.ELU(),<NewLine>            #nn.BatchNorm2d(768,eps=1e-4,momentum=0.1),<NewLine>            nn.Conv2d(768, 768, 2, padding=1), # 11<NewLine>            nn.ELU(),<NewLine>            #nn.BatchNorm2d(768,eps=1e-4,momentum=0.1),<NewLine>            nn.Conv2d(768, 768, 2, padding=1), # 12<NewLine>            nn.ELU(),<NewLine>            #nn.BatchNorm2d(768,eps=1e-4,momentum=0.1),<NewLine>            nn.MaxPool2d(2, 2), # 6<NewLine>            nn.Dropout2d(0.2),<NewLine>            # Block 4<NewLine>            nn.Conv2d(768, 768, 1, padding=0), # 6<NewLine>            nn.ELU(),<NewLine>            #nn.BatchNorm2d(768,eps=1e-4,momentum=0.1),<NewLine>            nn.Conv2d(768, 896, 2, padding=1), # 7<NewLine>            nn.ELU(),<NewLine>            #nn.BatchNorm2d(896,eps=1e-4,momentum=0.1),<NewLine>            nn.Conv2d(896, 896, 2, padding=1), # 8<NewLine>            nn.ELU(),<NewLine>            nn.MaxPool2d(2, 2), # 4<NewLine>            nn.Dropout2d(0.3),<NewLine>            # Block 5num_classes<NewLine>            nn.Conv2d(896, 896, 1, padding=0), # 4<NewLine>            nn.ELU(),<NewLine>           # nn.BatchNorm2d(896,eps=1e-4,momentum=0.1),<NewLine>            nn.Conv2d(896, 1024, 2, padding=1), # 5<NewLine>            nn.ELU(),<NewLine>            #nn.BatchNorm2d(1024,eps=1e-4,momentum=0.1),<NewLine>            nn.Conv2d(1024, 1024, 2, padding=1), # 6<NewLine>            nn.ELU(),<NewLine>            #nn.BatchNorm2d(1024,eps=1e-4,momentum=0.1),<NewLine>            nn.MaxPool2d(2, 2), # 3<NewLine>            nn.Dropout2d(0.4),<NewLine>            # Block 6<NewLine>            nn.Conv2d(1024, 1024, 1, padding=0), # 3<NewLine>            nn.ELU(),<NewLine>            #nn.BatchNorm2d(1024,eps=1e-4,momentum=0.1),<NewLine>            nn.Conv2d(1024, 1152, 2, padding=0), # 2<NewLine>            nn.ELU(),<NewLine>            #nn.BatchNorm2d(1152,eps=1e-4,momentum=0.1),<NewLine>            nn.MaxPool2d(2, 2), # 1<NewLine>            nn.Dropout2d(0.5),<NewLine>            # Block 7<NewLine>            nn.Conv2d(1152, 1152, 1, padding=0), #  1<NewLine>            nn.ELU(),<NewLine>            #nn.BatchNorm2d(1152,eps=1e-4,momentum=0.1),<NewLine>            nn.Dropout2d(0.0),<NewLine>            # Block 8<NewLine>            nn.Conv2d(1152, num_classes, 1, padding=0) # 1<NewLine>        )<NewLine> <NewLine>    def forward(self, x):<NewLine>        return self.main(x).view(x.size(0),-1)<NewLine><NewLine>model=CIFARClassifierELUPaper()<NewLine><NewLine>print(model)<NewLine>    <NewLine>print_num_params(model)<NewLine><NewLine>def ZCA_whitening(data):<NewLine>    <NewLine>    data=data.view(data.size(0),-1)<NewLine>    X_norm=data/255<NewLine>    X_norm.mean(axis=0).shape<NewLine>    cov = np.cov(X_norm.cpu(), rowvar=True)   <NewLine>    U,S,V = np.linalg.svd(cov)<NewLine>    epsilon = 0.1<NewLine>    X_ZCA = U.dot(np.diag(1.0/np.sqrt(S + epsilon))).dot(U.T).dot(X_norm.cpu())   <NewLine>    X_ZCA_rescaled = (X_ZCA - X_ZCA.min()) / (X_ZCA.max() - X_ZCA.min())   <NewLine>    <NewLine>    return X_ZCA_rescaled<NewLine><NewLine><NewLine>def zca_prepare_batch(batch, device=None, **kwargs):<NewLine>    <NewLine>    <NewLine>    data, target = batch<NewLine>    zca_data = ZCA_whitening(data).reshape(data.shape)<NewLine>    <NewLine>    zca_data = Variable(torch.from_numpy(zca_data))<NewLine>    <NewLine>    zca_data = zca_data.to(device)<NewLine><NewLine>    target = target.to(device)<NewLine><NewLine>    return zca_data, target<NewLine><NewLine><NewLine>model = model.cuda()<NewLine><NewLine>criterion = nn.CrossEntropyLoss()<NewLine>lr = 0.01<NewLine><NewLine>optimizer = optim.SGD(model.parameters(),lr,momentum=0.9,weight_decay=0.0005,nesterov=True)<NewLine><NewLine>#lr_scheduler = ExponentialLR(optimizer, gamma=0.975)<NewLine>lr_scheduler = MultiStepLR(optimizer, milestones=[70], gamma=0.1)<NewLine><NewLine>use_amp = True<NewLine><NewLine>#Load state dict<NewLine>filename='checkpoint_testxxx.pth'<NewLine>if os.path.isfile(filename):<NewLine>    print(""=&gt; loading checkpoint '{}'"".format(filename))<NewLine>    checkpoint = torch.load(filename)<NewLine>    model.load_state_dict(checkpoint['model'])<NewLine>    optimizer.load_state_dict(checkpoint['optimizer'])<NewLine>    #lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])<NewLine>    #print('lr scheduler is')<NewLine>    #print(checkpoint['lr_scheduler'])<NewLine>    print(""=&gt; loaded checkpoint "")<NewLine><NewLine><NewLine><NewLine># Initialize Amp<NewLine>model, optimizer = amp.initialize(model, optimizer, opt_level=""O2"", num_losses=1)<NewLine>   <NewLine>    <NewLine>batch = next(iter(train_loader))<NewLine><NewLine>batch = None<NewLine>torch.cuda.empty_cache()<NewLine><NewLine>trainer = create_supervised_trainer(<NewLine>    model, <NewLine>    optimizer, <NewLine>    criterion, <NewLine>    device=""cuda"",<NewLine>    prepare_batch=zca_prepare_batch<NewLine>)<NewLine><NewLine><NewLine>#def output_transform(out):<NewLine>#    return out['batchloss']<NewLine><NewLine>#RunningAverage(output_transform=output_transform).attach(trainer, ""batchloss"")<NewLine><NewLine>print('Start experiment')<NewLine><NewLine>trainer.add_event_handler(Events.EPOCH_COMPLETED, lambda engine: lr_scheduler.step())<NewLine><NewLine><NewLine>resume_epoch = 69 # zero-based<NewLine><NewLine><NewLine>def resume_training(engine):<NewLine>    engine.state.iteration = resume_epoch * len(engine.state.dataloader)<NewLine>    engine.state.epoch = resume_epoch<NewLine>    print('Las iteraciones son')<NewLine>    print(engine.state.iteration)<NewLine>    print('El epoch actual es')<NewLine>    print(engine.state.epoch)<NewLine><NewLine>#trainer.add_event_handler(Events.STARTED, resume_training)<NewLine><NewLine>metrics = {<NewLine>    'Loss': Loss(criterion),<NewLine>    'Accuracy': Accuracy(),<NewLine>    'Precision': Precision(average=True),<NewLine>    'Recall': Recall(average=True),<NewLine>    'Top-5 Accuracy': TopKCategoricalAccuracy(k=5)<NewLine>}<NewLine><NewLine>evaluator = create_supervised_evaluator(model, metrics=metrics, device=device,prepare_batch=zca_prepare_batch, non_blocking=True)<NewLine>train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device,prepare_batch=zca_prepare_batch, non_blocking=True)<NewLine><NewLine><NewLine>cpe = CustomPeriodicEvent(n_epochs=3)<NewLine>cpe.attach(trainer)<NewLine><NewLine><NewLine>def run_evaluation(engine):<NewLine>    train_evaluator.run(eval_train_loader)<NewLine>    evaluator.run(test_loader)<NewLine><NewLine>def save_model_and_metrics(engine):<NewLine>#    print('el num de epochs actual es')<NewLine>#    print(trainer.state.epoch)<NewLine>    epoch=engine.state.epoch<NewLine>    if len(str(epoch))==1:<NewLine>        epoch='00'+str(epoch)<NewLine>    if len(str(epoch))==2:<NewLine>        epoch='0'+str(epoch)<NewLine>        <NewLine><NewLine>trainer.add_event_handler(cpe.Events.EPOCHS_3_STARTED, run_evaluation)<NewLine>trainer.add_event_handler(Events.COMPLETED, run_evaluation)<NewLine><NewLine><NewLine>trainer.add_event_handler(Events.ITERATION_COMPLETED, TerminateOnNan())<NewLine><NewLine>training_saver = ModelCheckpoint(""checkpoint_190520"",<NewLine>                                 filename_prefix='checkpoint',<NewLine>                                 save_interval=None,  # Save every 1000 iterations<NewLine>                                 n_saved=None,<NewLine>                                 atomic=True,<NewLine>                                 save_as_state_dict=True,<NewLine>                                 require_empty=False,<NewLine>                                 create_dir=True)<NewLine>                                 <NewLine>#Changed from Events.ITERATION_COMPLETED to Events.EPOCH_COMPLETED. EDIT: changed to EPOCH_STARTED every 10                                                                  <NewLine>trainer.add_event_handler(Events.EPOCH_STARTED(every=3), <NewLine>                          training_saver, <NewLine>                          {<NewLine>                              ""model"": model,<NewLine>                              ""optimizer"": optimizer,<NewLine>                              ""lr_scheduler"": lr_scheduler<NewLine>                          })<NewLine># Store the best model<NewLine>def default_score_fn(engine):<NewLine>    score = engine.state.metrics['Accuracy']<NewLine>    return score<NewLine><NewLine><NewLine># Add early stopping<NewLine>es_patience = 10<NewLine>es_handler = EarlyStopping(patience=es_patience, score_function=default_score_fn, trainer=trainer)<NewLine>#evaluator.add_event_handler(Events.COMPLETED, es_handler)<NewLine>#setup_logger(es_handler._logger)<NewLine><NewLine># Clear cuda cache between training/testing<NewLine>def empty_cuda_cache(engine):<NewLine>    torch.cuda.empty_cache()<NewLine>    import gc<NewLine>    gc.collect()<NewLine><NewLine>trainer.add_event_handler(Events.EPOCH_COMPLETED, empty_cuda_cache)<NewLine>evaluator.add_event_handler(Events.COMPLETED, empty_cuda_cache)<NewLine>#train_evaluator.add_event_handler(Events.COMPLETED, empty_cuda_cache)  <NewLine><NewLine>num_epochs =80<NewLine><NewLine>ProgressBar(persist=True).attach(trainer)<NewLine><NewLine>trainer.run(train_loader, max_epochs=num_epochs)<NewLine><NewLine>print('The results are')<NewLine>print(train_evaluator.state.metrics) <NewLine>print(evaluator.state.metrics) <NewLine><NewLine># Dill routine<NewLine>    <NewLine>model_copy=dill.dumps(model)<NewLine>torch.save(model_copy,'complete_model_final.pt')<NewLine>torch.save(train_evaluator.state.metrics,'metrics_final.pt')<NewLine><NewLine><NewLine><NewLine><NewLine><NewLine><NewLine><NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>As you can find in various source on how to debug CNN and training:</p><NewLine><ul><NewLine><li>try to overfit the model on a single batch</li><NewLine></ul><NewLine><p>Seems like this model without BN has a difficulty even to overfit a single batch.<br/><NewLine>I tried a larger LR -&gt; 0.1 and loss on a single batch stucked on ~4.0<br/><NewLine>With BN’s seems like it can overfit a batch and can train (as you also observed).<br/><NewLine>If you want to remove BN’s, probably you need to check conv weights initialization schema and find out an appropriate one.</p><NewLine><p>It seems like you have some bugs:</p><NewLine><ol><NewLine><li>input data is 40x40</li><NewLine></ol><NewLine><pre><code class=""lang-python"">train_transform = Compose([<NewLine>    RandomCrop(32),<NewLine>    RandomHorizontalFlip(),<NewLine>    Pad(4),<NewLine>    ToTensor(),<NewLine>    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),<NewLine>    #LinearTransformation(transformation_matrix, mean_vector), <NewLine> ])<NewLine></code></pre><NewLine><p>usually, they do pad first and than random crop of 32</p><NewLine><ol start=""2""><NewLine><li>data is normalized -&gt; do not need to divide by 255 in whitening</li><NewLine></ol><NewLine><pre><code class=""lang-python"">def ZCA_whitening(data):<NewLine>    <NewLine>    data=data.view(data.size(0),-1)<NewLine>    X_norm=data/255  # &lt;--- ???<NewLine></code></pre><NewLine><p>HTH</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Tanya_Boone; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: May 18, 2020,  6:00pm; <NewLine> REPLY_DATE 2: May 18, 2020,  6:04pm; <NewLine> REPLY_DATE 3: May 19, 2020,  2:38pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
81688,Combining metrics in ignite.metrics,2020-05-17T20:19:15.817Z,0,90,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In the following (1) code block, for each metric (accuracy, precision, recall, f1), I create a metric class to record <code>(y_pred, y)</code> and calculate the score at the end. My question is can I create a new metric class that combines the four metrics. And I just need to update it once in a loop. For example, see the code block (2).</p><NewLine><p>(1) What I use now.</p><NewLine><pre><code class=""lang-python"">from ignite.metrics import Accuracy, Precision, Recall, Fbeta<NewLine><NewLine>accuracy = Accuracy()<NewLine>precision = Precision()<NewLine>recall = Recall()<NewLine>f1 = Fbeta(beta=1.0, average=False, precision=precision, recall=recall)<NewLine><NewLine>for X, y in dataloader:<NewLine>    y_pred = model(X)<NewLine><NewLine>    # calculate loss, backward, and update weights<NewLine><NewLine>    accuracy.update((y_pred, y))<NewLine>    precision.update((y_pred, y))<NewLine>    recall.update((y_pred, y))<NewLine><NewLine>print(f""Accuracy: {accuracy.compute()}"")<NewLine>print(f""Precision: {precision.compute()}"")<NewLine>print(f""Recall: {recall.compute()}"")<NewLine>print(f""F1: {f1.compute()}"")<NewLine></code></pre><NewLine><p>(2) What I want.</p><NewLine><pre><code class=""lang-python"">import CustomMetric  # metric combining Accuracy, Precision, Recall, and F1<NewLine><NewLine>metric = CustomMetric()<NewLine><NewLine>for X, y in dataloader:<NewLine>    y_pred = model(X)<NewLine><NewLine>    # calculate loss, backward, and update weights<NewLine><NewLine>    metric.update((y_pred, y))<NewLine><NewLine>scores = metric.compute()<NewLine>print(f""Accuracy: {scores['accuracy']}"")<NewLine>print(f""Precision: {score['precision']}"")<NewLine>print(f""Recall: {score['recall']}"")<NewLine>print(f""F1: {scores['f1']}"")<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/stvhuang,,stvhuang,"May 17, 2020,  8:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/stvhuang"">@stvhuang</a> you can do it like this :</p><NewLine><pre><code class=""lang-python"">class MetricsGroup:<NewLine>        <NewLine>    def __init__(self, metrics_dict):<NewLine>        self.metrics = metrics_dict<NewLine>        <NewLine>    def update(self, output):<NewLine>        for name, metric in self.metrics.items():<NewLine>            metric.update(output)<NewLine>            <NewLine>    def compute(self):<NewLine>        output = {}<NewLine>        for name, metric in self.metrics.items():<NewLine>            output[name] = metric.compute()<NewLine>        return output<NewLine><NewLine><NewLine>import torch<NewLine>from ignite.metrics import Accuracy, Precision, Recall, Fbeta<NewLine><NewLine>p = Precision()<NewLine>r = Recall()<NewLine>m_group = MetricsGroup({<NewLine>    ""accuracy"": Accuracy(),<NewLine>    ""precision"": p,<NewLine>    ""recall"": r,<NewLine>    ""f1"": Fbeta(beta=1.0, average=False, precision=p, recall=r)<NewLine>})<NewLine><NewLine>for _ in range(10):<NewLine>    <NewLine>    y = torch.randint(0, 4, size=(32, ))<NewLine>    y_pred = torch.rand(32, 4)<NewLine><NewLine>    m_group.update((y_pred, y))<NewLine><NewLine>scores = m_group.compute()<NewLine>print(f""Accuracy: {scores['accuracy']}"")<NewLine>print(f""Precision: {scores['precision']}"")<NewLine>print(f""Recall: {scores['recall']}"")<NewLine>print(f""F1: {scores['f1']}"")<NewLine></code></pre><NewLine><p>HTH</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: May 18, 2020, 11:07pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
72335,How to use Learning Rate scheduler in Ignite?,2020-03-06T19:24:42.949Z,2,677,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to use</p><NewLine><pre><code class=""lang-auto"">from ignite.contrib.handlers.param_scheduler import LRScheduler<NewLine>from torch.optim.lr_scheduler import StepLR<NewLine></code></pre><NewLine><p>So far I can’t find any full file example on this.</p><NewLine><p>I try to implement myself. However, look like I missing something because the model does not train (loss keep on both training and validation dataset, without lr schedule model train OK)</p><NewLine><p>The code:</p><NewLine><pre><code class=""lang-auto"">from args_util import my_args_parse<NewLine>from data_flow import get_train_val_list, get_dataloader, create_training_image_list, create_image_list<NewLine>from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator<NewLine>from ignite.metrics import Loss, MeanAbsoluteError, MeanSquaredError<NewLine>from ignite.engine import Engine<NewLine>from ignite.handlers import Checkpoint, DiskSaver<NewLine>from crowd_counting_error_metrics import CrowdCountingMeanAbsoluteError, CrowdCountingMeanSquaredError<NewLine>from visualize_util import get_readable_time<NewLine><NewLine>import torch<NewLine>from torch import nn<NewLine>from models import CompactCNN<NewLine>import os<NewLine>from ignite.contrib.handlers.param_scheduler import LRScheduler<NewLine>from torch.optim.lr_scheduler import StepLR<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">    # create data loader<NewLine>    train_loader, val_loader, test_loader = get_dataloader(train_list, None, test_list, dataset_name=dataset_name)<NewLine><NewLine>    print(""len train_loader "", len(train_loader))<NewLine><NewLine>    # model<NewLine>    model = CompactCNN()<NewLine>    model = model.to(device)<NewLine><NewLine>    # loss function<NewLine>    loss_fn = nn.MSELoss(reduction='sum').to(device)<NewLine><NewLine>    optimizer = torch.optim.Adam(model.parameters(), args.lr,<NewLine>                                weight_decay=args.decay)<NewLine><NewLine>    step_scheduler = StepLR(optimizer, step_size=100, gamma=0.1)<NewLine>    lr_scheduler = LRScheduler(step_scheduler)<NewLine><NewLine>    trainer = create_supervised_trainer(model, optimizer, loss_fn, device=device)<NewLine>    evaluator = create_supervised_evaluator(model,<NewLine>                                            metrics={<NewLine>                                                'mae': CrowdCountingMeanAbsoluteError(),<NewLine>                                                'mse': CrowdCountingMeanSquaredError(),<NewLine>                                                'nll': Loss(loss_fn)<NewLine>                                            }, device=device)<NewLine>    print(model)<NewLine><NewLine>    print(args)<NewLine><NewLine>    if len(args.load_model) &gt; 0:<NewLine>        load_model_path = args.load_model<NewLine>        print(""load mode "" + load_model_path)<NewLine>        to_load = {'trainer': trainer, 'model': model, 'optimizer': optimizer, 'lr_scheduler': lr_scheduler}<NewLine>        checkpoint = torch.load(load_model_path)<NewLine>        Checkpoint.load_objects(to_load=to_load, checkpoint=checkpoint)<NewLine>        print(""load model complete"")<NewLine>        for param_group in optimizer.param_groups:<NewLine>            param_group['lr'] = args.lr<NewLine>            print(""change lr to "", args.lr)<NewLine>    else:<NewLine>        print(""do not load, keep training"")<NewLine>        trainer.add_event_handler(Events.ITERATION_COMPLETED, lr_scheduler)<NewLine><NewLine><NewLine>    @trainer.on(Events.ITERATION_COMPLETED(every=50))<NewLine>    def log_training_loss(trainer):<NewLine>        timestamp = get_readable_time()<NewLine>        print(timestamp + "" Epoch[{}] Loss: {:.2f}"".format(trainer.state.epoch, trainer.state.output))<NewLine><NewLine><NewLine>    @trainer.on(Events.EPOCH_COMPLETED)<NewLine>    def log_training_results(trainer):<NewLine>        evaluator.run(train_loader)<NewLine>        metrics = evaluator.state.metrics<NewLine>        timestamp = get_readable_time()<NewLine>        print(timestamp + "" Training set Results - Epoch: {}  Avg mae: {:.2f} Avg mse: {:.2f} Avg loss: {:.2f}""<NewLine>              .format(trainer.state.epoch, metrics['mae'], metrics['mse'], metrics['nll']))<NewLine><NewLine><NewLine>    @trainer.on(Events.EPOCH_COMPLETED)<NewLine>    def log_validation_results(trainer):<NewLine>        evaluator.run(test_loader)<NewLine>        metrics = evaluator.state.metrics<NewLine>        timestamp = get_readable_time()<NewLine>        print(timestamp + "" Validation set Results - Epoch: {}  Avg mae: {:.2f} Avg mse: {:.2f} Avg loss: {:.2f}""<NewLine>              .format(trainer.state.epoch, metrics['mae'], metrics['mse'], metrics['nll']))<NewLine><NewLine><NewLine><NewLine>    # docs on save and load<NewLine>    to_save = {'trainer': trainer, 'model': model, 'optimizer': optimizer, 'lr_scheduler': lr_scheduler}<NewLine>    save_handler = Checkpoint(to_save, DiskSaver('saved_model/' + args.task_id, create_dir=True, atomic=True),<NewLine>                              filename_prefix=args.task_id,<NewLine>                              n_saved=5)<NewLine><NewLine>    trainer.add_event_handler(Events.EPOCH_COMPLETED(every=3), save_handler)<NewLine><NewLine>    trainer.run(train_loader, max_epochs=args.epochs)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/ThaiThien,(Thai Thien),ThaiThien,"March 6, 2020,  7:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/thaithien"">@ThaiThien</a><br/><NewLine><a href=""https://github.com/pytorch/ignite/blob/master/examples/contrib/cifar10/main.py#L70"" rel=""nofollow noopener"">here</a> is an example of using another LR scheduler (PiecewiseLinear), but the idea is the same. In the example, a helper function is used to attach scheduler to the trainer and internally it does the following : <a href=""https://github.com/pytorch/ignite/blob/master/ignite/contrib/engines/common.py#L113"" rel=""nofollow noopener"">https://github.com/pytorch/ignite/blob/master/ignite/contrib/engines/common.py#L113</a></p><NewLine><p>So, in your code, probably you need to do the following:</p><NewLine><pre><code class=""lang-python""># Should be attached in any case: loading or not loading<NewLine># else:  <NewLine>#        print(""do not load, keep training"")<NewLine>trainer.add_event_handler(Events.ITERATION_STARTED, lr_scheduler)<NewLine></code></pre><NewLine><p>HTH</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t understand why I have put it out of the if-else statement:</p><NewLine><p>1/ If the model is not load (case Else: ) then the <code>trainer.add_event_handler(Events.ITERATION_STARTED, lr_scheduler)</code><br/><NewLine>is execute<br/><NewLine>2/ if the model was load, then it should load the lr_scheduler too<br/><NewLine><code> to_save = {'trainer': trainer, 'model': model, 'optimizer': optimizer, 'lr_scheduler': lr_scheduler}</code><br/><NewLine>and<br/><NewLine><code>to_load = {'trainer': trainer, 'model': model, 'optimizer': optimizer, 'lr_scheduler': lr_scheduler}</code></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I don’t understand why I have put it out of the if-else statement:</p><NewLine></blockquote><NewLine><p>Because, loading existing checkpoint (model, optimizer, trainer, lr_scheduler) does not setup event handlers as lr scheduler. Yes, <code>Checkpoint.load_objects(to_load=to_load, checkpoint=checkpoint)</code> only setups the states for all those objects.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think what wrong is I should use</p><NewLine><p><code>trainer.add_event_handler(Events.EPOCH_STARTED, lr_scheduler)</code></p><NewLine><p>instead of</p><NewLine><p><code>trainer.add_event_handler(Events.ITERATION_STARTED, lr_scheduler)</code></p><NewLine><p>Because in 1 epoch there are so many ITERATION (batch size of 1 for example) that make learning rate drop so fast with my step_size=100</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, that’s true that if attached to iterations, then lr scheduler step-like params should be multiplied by epoch lenght.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/thaithien"">@ThaiThien</a> Hey thai, I am working on same approach with you. Did you obtain positive results from your try ? Or do you have different suggestion instead ?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>My approach above is suppose to be correct. I do check (write log) the learing rate each epoch to make sure it is schedule as expect. (use mlflow or comet.ml for fast log with chart, or just write to file)</p><NewLine><p>For my use-case, actually learning rate scheduler does not help much (something fishy here). Result as same as no scheduler (fixed lr).</p><NewLine><p>So, I don’t use scheduler anymore.</p><NewLine><p><a class=""mention"" href=""/u/sefa_alp"">@Sefa_Alp</a>, please make experiment and compare.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ThaiThien; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ThaiThien; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Sefa_Alp; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ThaiThien; <NewLine> ,"REPLY_DATE 1: March 6, 2020,  7:56pm; <NewLine> REPLY_DATE 2: March 7, 2020,  4:42pm; <NewLine> REPLY_DATE 3: March 7, 2020,  9:00pm; <NewLine> REPLY_DATE 4: March 11, 2020,  5:11pm; <NewLine> REPLY_DATE 5: March 11, 2020,  8:21pm; <NewLine> REPLY_DATE 6: April 24, 2020,  6:24pm; <NewLine> REPLY_DATE 7: May 6, 2020,  5:18pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
68471,Saving full model with pytorch ignite,2020-02-03T05:26:45.515Z,4,369,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I was able to run successfully this tutorial with pytorch ignite</p><NewLine><p><a class=""onebox"" href=""https://www.kaggle.com/hmendonca/multitask-efficientnetb4-ignite-amp-clr-aptos19"" rel=""nofollow noopener"" target=""_blank"">https://www.kaggle.com/hmendonca/multitask-efficientnetb4-ignite-amp-clr-aptos19</a></p><NewLine><p>but in order to do a pruning method I need to save the whole model (state dict is not useful), and I try to save with torch.save(model,‘model1.pth’) for example and I get the following message:</p><NewLine><p>File “”, line 1, in<br/><NewLine>torch.save(model,‘modelo_ejemplo1.pth’)</p><NewLine><p>File “C:\ProgramData\Anaconda3\lib\site-packages\torch\serialization.py”, line 260, in save<br/><NewLine>return _with_file_like(f, “wb”, lambda f: _save(obj, f, pickle_module, pickle_protocol))</p><NewLine><p>File “C:\ProgramData\Anaconda3\lib\site-packages\torch\serialization.py”, line 185, in _with_file_like<br/><NewLine>return body(f)</p><NewLine><p>File “C:\ProgramData\Anaconda3\lib\site-packages\torch\serialization.py”, line 260, in<br/><NewLine>return _with_file_like(f, “wb”, lambda f: _save(obj, f, pickle_module, pickle_protocol))</p><NewLine><p>File “C:\ProgramData\Anaconda3\lib\site-packages\torch\serialization.py”, line 332, in _save<br/><NewLine>pickler.dump(obj)</p><NewLine><p>AttributeError: Can’t pickle local object ‘_initialize…patch_forward…new_fwd’</p><NewLine><p>I’ve been looking a lot about this error message, but anyone seems to use ignite a lot…</p><NewLine><p>I can save the model like this</p><NewLine><p>to_save = {‘model’: model}<br/><NewLine>handler = Checkpoint(to_save, DiskSaver(‘D:\Neural_Nets\EfficientNet\Pytorch\CIFAR10b’, create_dir=True,require_empty=False), n_saved=2)<br/><NewLine>trainer.add_event_handler(Events.EPOCH_STARTED, handler)</p><NewLine><p>but I can only save the weights…I want to save the whole model like I do with other codes that don’t use this pytorch ignite…can you help me with this issue? I think ignite is more efficient than my other way of training, that is why I want to change to it but if I can’t save the model there is no case <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/Tanya_Boone,(Tanya Boone),Tanya_Boone,"February 3, 2020,  5:26am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/tanya_boone"">@Tanya_Boone</a></p><NewLine><blockquote><NewLine><p>torch.save(model,‘model1.pth’)<br/><NewLine>AttributeError: Can’t pickle local object ‘_initialize…patch_forward…new_fwd’</p><NewLine></blockquote><NewLine><p>seems like your model can not be saved with <code>torch.save</code>.</p><NewLine><p>Maybe you need to replace some lambda function in there, if there are some…<br/><NewLine>Can you share the model definition to take a look ?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think is the model because I can save it with other type of training but when I switch to ignite is when I get this kind of error…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Tanya! It’s a bit far but could you share a model that you can not save with ignite ? Thank you very much for your help <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, for example, efficientnet</p><NewLine><p>class Swish(nn.Module):<br/><NewLine>def forward(self, x):<br/><NewLine>return x * torch.sigmoid(x)</p><NewLine><p>class Flatten(nn.Module):<br/><NewLine>def forward(self, x):<br/><NewLine>return x.reshape(x.shape[0], -1)</p><NewLine><p>class SqueezeExcitation(nn.Module):</p><NewLine><pre><code>def __init__(self, inplanes, se_planes):<NewLine>    super(SqueezeExcitation, self).__init__()<NewLine>    self.reduce_expand = nn.Sequential(<NewLine>        nn.Conv2d(inplanes, se_planes, <NewLine>                  kernel_size=1, stride=1, padding=0, bias=True),<NewLine>        Swish(),<NewLine>        nn.Conv2d(se_planes, inplanes, <NewLine>                  kernel_size=1, stride=1, padding=0, bias=True),<NewLine>        nn.Sigmoid()<NewLine>    )<NewLine><NewLine>def forward(self, x):<NewLine>    x_se = torch.mean(x, dim=(-2, -1), keepdim=True)<NewLine>    x_se = self.reduce_expand(x_se)<NewLine>    return x_se * x<NewLine></code></pre><NewLine><p>class MBConv(nn.Module):<br/><NewLine>def <strong>init</strong>(self, inplanes, planes, kernel_size, stride,<br/><NewLine>expand_rate=1.0, se_rate=0.25,<br/><NewLine>drop_connect_rate=0.2):<br/><NewLine>super(MBConv, self).<strong>init</strong>()</p><NewLine><pre><code>    expand_planes = int(inplanes * expand_rate)<NewLine>    se_planes = max(1, int(inplanes * se_rate))<NewLine><NewLine>    self.expansion_conv = None        <NewLine>    if expand_rate &gt; 1.0:<NewLine>        self.expansion_conv = nn.Sequential(<NewLine>            nn.Conv2d(inplanes, expand_planes, <NewLine>                      kernel_size=1, stride=1, padding=0, bias=False),<NewLine>            nn.BatchNorm2d(expand_planes, momentum=0.01, eps=1e-3),<NewLine>            Swish()<NewLine>        )<NewLine>        inplanes = expand_planes<NewLine><NewLine>    self.depthwise_conv = nn.Sequential(<NewLine>        nn.Conv2d(inplanes, expand_planes,<NewLine>                  kernel_size=kernel_size, stride=stride, <NewLine>                  padding=kernel_size // 2, groups=expand_planes,<NewLine>                  bias=False),<NewLine>        nn.BatchNorm2d(expand_planes, momentum=0.01, eps=1e-3),<NewLine>        Swish()<NewLine>    )<NewLine><NewLine>    self.squeeze_excitation = SqueezeExcitation(expand_planes, se_planes)<NewLine>    <NewLine>    self.project_conv = nn.Sequential(<NewLine>        nn.Conv2d(expand_planes, planes, <NewLine>                  kernel_size=1, stride=1, padding=0, bias=False),<NewLine>        nn.BatchNorm2d(planes, momentum=0.01, eps=1e-3),<NewLine>    )<NewLine><NewLine>    self.with_skip = stride == 1<NewLine>    self.drop_connect_rate = torch.tensor(drop_connect_rate, requires_grad=False)<NewLine><NewLine>def _drop_connect(self, x):        <NewLine>    keep_prob = 1.0 - self.drop_connect_rate<NewLine>    drop_mask = torch.rand(x.shape[0], 1, 1, 1) + keep_prob<NewLine>    drop_mask = drop_mask.type_as(x)<NewLine>    drop_mask.floor_()<NewLine>    return drop_mask * x / keep_prob<NewLine>    <NewLine>def forward(self, x):<NewLine>    z = x<NewLine>    if self.expansion_conv is not None:<NewLine>        x = self.expansion_conv(x)<NewLine><NewLine>    x = self.depthwise_conv(x)<NewLine>    x = self.squeeze_excitation(x)<NewLine>    x = self.project_conv(x)<NewLine>    <NewLine>    # Add identity skip<NewLine>    if x.shape == z.shape and self.with_skip:            <NewLine>        if self.training and self.drop_connect_rate is not None:<NewLine>            self._drop_connect(x)<NewLine>        x += z<NewLine>    return x<NewLine></code></pre><NewLine><p>def init_weights(module):<br/><NewLine>if isinstance(module, nn.Conv2d):<br/><NewLine>nn.init.kaiming_normal_(module.weight, a=0, mode=‘fan_out’)<br/><NewLine>elif isinstance(module, nn.Linear):<br/><NewLine>init_range = 1.0 / math.sqrt(module.weight.shape[1])<br/><NewLine>nn.init.uniform_(module.weight, a=-init_range, b=init_range)</p><NewLine><p>class EfficientNet(nn.Module):</p><NewLine><pre><code>def _setup_repeats(self, num_repeats):<NewLine>    return int(math.ceil(self.depth_coefficient * num_repeats))<NewLine><NewLine>def _setup_channels(self, num_channels):<NewLine>    num_channels *= self.width_coefficient<NewLine>    new_num_channels = math.floor(num_channels / self.divisor + 0.5) * self.divisor<NewLine>    new_num_channels = max(self.divisor, new_num_channels)<NewLine>    if new_num_channels &lt; 0.9 * num_channels:<NewLine>        new_num_channels += self.divisor<NewLine>    return new_num_channels<NewLine><NewLine>def __init__(self, num_classes, <NewLine>             width_coefficient=1.0,<NewLine>             depth_coefficient=1.0,<NewLine>             se_rate=0.25,<NewLine>             dropout_rate=0.2,<NewLine>             drop_connect_rate=0.2):<NewLine>    super(EfficientNet, self).__init__()<NewLine>    <NewLine>    self.width_coefficient = width_coefficient<NewLine>    self.depth_coefficient = depth_coefficient<NewLine>    self.divisor = 8<NewLine>            <NewLine>    list_channels = [32, 16, 24, 40, 80, 112, 192, 320, 1280]<NewLine>    list_channels = [self._setup_channels(c) for c in list_channels]<NewLine>            <NewLine>    list_num_repeats = [1, 2, 2, 3, 3, 4, 1]<NewLine>    list_num_repeats = [self._setup_repeats(r) for r in list_num_repeats]        <NewLine>    <NewLine>    expand_rates = [1, 6, 6, 6, 6, 6, 6]<NewLine>    strides = [1, 2, 2, 2, 1, 2, 1]<NewLine>    kernel_sizes = [3, 3, 5, 3, 5, 5, 3]<NewLine><NewLine>    # Define stem:<NewLine>    self.stem = nn.Sequential(<NewLine>        nn.Conv2d(3, list_channels[0], kernel_size=3, stride=2, padding=1, bias=False),<NewLine>        nn.BatchNorm2d(list_channels[0], momentum=0.01, eps=1e-3),<NewLine>        Swish()<NewLine>    )<NewLine>    <NewLine>    # Define MBConv blocks<NewLine>    blocks = []<NewLine>    counter = 0<NewLine>    num_blocks = sum(list_num_repeats)<NewLine>    for idx in range(7):<NewLine>        <NewLine>        num_channels = list_channels[idx]<NewLine>        next_num_channels = list_channels[idx + 1]<NewLine>        num_repeats = list_num_repeats[idx]<NewLine>        expand_rate = expand_rates[idx]<NewLine>        kernel_size = kernel_sizes[idx]<NewLine>        stride = strides[idx]<NewLine>        drop_rate = drop_connect_rate * counter / num_blocks<NewLine>        <NewLine>        name = ""MBConv{}_{}"".format(expand_rate, counter)<NewLine>        blocks.append((<NewLine>            name,<NewLine>            MBConv(num_channels, next_num_channels, <NewLine>                   kernel_size=kernel_size, stride=stride, expand_rate=expand_rate, <NewLine>                   se_rate=se_rate, drop_connect_rate=drop_rate)<NewLine>        ))<NewLine>        counter += 1<NewLine>        for i in range(1, num_repeats):                <NewLine>            name = ""MBConv{}_{}"".format(expand_rate, counter)<NewLine>            drop_rate = drop_connect_rate * counter / num_blocks                <NewLine>            blocks.append((<NewLine>                name,<NewLine>                MBConv(next_num_channels, next_num_channels, <NewLine>                       kernel_size=kernel_size, stride=1, expand_rate=expand_rate, <NewLine>                       se_rate=se_rate, drop_connect_rate=drop_rate)                                    <NewLine>            ))<NewLine>            counter += 1<NewLine>    <NewLine>    self.blocks = nn.Sequential(OrderedDict(blocks))<NewLine>    <NewLine>    # Define head<NewLine>    self.head = nn.Sequential(<NewLine>        nn.Conv2d(list_channels[-2], list_channels[-1], <NewLine>                  kernel_size=1, bias=False),<NewLine>        nn.BatchNorm2d(list_channels[-1], momentum=0.01, eps=1e-3),<NewLine>        Swish(),<NewLine>        nn.AdaptiveAvgPool2d(1),<NewLine>        Flatten(),<NewLine>        nn.Dropout(p=dropout_rate),<NewLine>        nn.Linear(list_channels[-1], num_classes)<NewLine>    )<NewLine><NewLine>    self.apply(init_weights)<NewLine>    <NewLine>def forward(self, x):<NewLine>    f = self.stem(x)<NewLine>    f = self.blocks(f)<NewLine>    y = self.head(f)<NewLine>    return y<NewLine></code></pre><NewLine><p>model = EfficientNet(num_classes=10,<br/><NewLine>width_coefficient=1.4, depth_coefficient=1.8,<br/><NewLine>dropout_rate=0.4)<br/><NewLine>resolution = 380<br/><NewLine>img_stats  = [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]]</p><NewLine><p>but I managed to save it with this library called dill</p><NewLine><p>For example,</p><NewLine><p>import dill</p><NewLine><h1>Dill routine</h1><NewLine><p>model_copy=dill.dumps(model)<br/><NewLine>torch.save(model_copy,‘model_ignite_original.pt’)</p><NewLine><p>and I dont get any errors…</p><NewLine><p>when I want to load it I do it like this</p><NewLine><pre><code>model1 = torch.load(model_name)<NewLine>model=dill.loads(model1)</code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry for this late response. I will look more precisely your code, thank very much !</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Tanya_Boone; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/sdesrozis; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Tanya_Boone; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/sdesrozis; <NewLine> ,"REPLY_DATE 1: February 3, 2020,  8:10am; <NewLine> REPLY_DATE 2: February 5, 2020, 10:18pm; <NewLine> REPLY_DATE 3: March 29, 2020,  2:53pm; <NewLine> REPLY_DATE 4: March 29, 2020,  3:01pm; <NewLine> REPLY_DATE 5: April 23, 2020, 10:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
77344,Ignite - dataloader with more than input-target pair and custom evaluator,2020-04-18T17:08:37.270Z,4,153,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My dataloader returns image, target and a weight map for my loss function. For this, I’ve written an update function for the trainer engine (because I have to pass that weight map to the loss function). But, during evaluation, I don’t know how (or if) it’s possible to include this weight map and attach loss as a metric to the evaluator. <code>prepare_batch</code> expects an input-target pair (I cant’ return more than 2 items) and I don’t think I can pass the weight map through <code>output_transform</code>…</p><NewLine><p>Any ideas?</p><NewLine></div>",https://discuss.pytorch.org/u/xen0f0n,(Xenofon Karagiannis),xen0f0n,"April 18, 2020,  5:08pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/xen0f0n"">@xen0f0n</a><br/><NewLine>So, output transform argument of Loss metric : <a href=""https://pytorch.org/ignite/metrics.html#ignite.metrics.Loss"" rel=""nofollow noopener"">https://pytorch.org/ignite/metrics.html#ignite.metrics.Loss</a><br/><NewLine>does not work in your case ?</p><NewLine><p>If it’s the case, please, provide a minimal code snippet to understand the issue. Thanks!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/vfdev-5"">@vfdev-5</a> I’m not sure I can use Loss from metrics, no.<br/><NewLine>So, my update_model function for the trainer engine is</p><NewLine><pre><code class=""lang-auto"">def update_model(engine, batch):<NewLine>        inputs, targets, weight_map = batch<NewLine>        inputs = inputs.to(device)<NewLine>        targets = targets.to(device)<NewLine>        weight_map = weight_map.to(device)<NewLine>        optimizer.zero_grad()<NewLine>        outputs = net(inputs)<NewLine>        loss = criterion(outputs, targets, weight_map)<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine>        return loss.item()<NewLine></code></pre><NewLine><p>I need to unpack the batch, that has 3 items, and follow the same scheme for evaluation. If I could unpack the batch inside <code>loss_fn</code> (in metrics.Loss), or using <code>prepare_batch</code> in <code>create_supervised_evaluator</code> that would do the trick.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If I understand your case correctly, the following would work</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine>from ignite.engine import Engine, create_supervised_evaluator, Events<NewLine>from ignite.metrics import Loss<NewLine><NewLine>data = [<NewLine>    # inputs, targets, weight_map<NewLine>    [torch.rand(1), torch.rand(1), torch.rand(2)],<NewLine>    [torch.rand(1), torch.rand(1), torch.rand(2)],<NewLine>    [torch.rand(1), torch.rand(1), torch.rand(2)],<NewLine>    [torch.rand(1), torch.rand(1), torch.rand(2)]<NewLine>]<NewLine><NewLine>net = nn.Linear(1, 1)<NewLine>optimizer = torch.optim.SGD(net.parameters(), lr=0.001)<NewLine><NewLine>class WMapLoss(nn.Module):<NewLine>    <NewLine>    def forward(self, y_pred, y, weight_map):<NewLine>        return (y_pred - y).sum() + weight_map.sum()<NewLine><NewLine><NewLine>criterion = WMapLoss()<NewLine><NewLine><NewLine>def update_model(engine, batch):<NewLine>    inputs, targets, weight_map = batch<NewLine>    optimizer.zero_grad()<NewLine>    outputs = net(inputs)<NewLine>    loss = criterion(outputs, targets, weight_map)<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine>    return loss.item()<NewLine><NewLine><NewLine>def eval_fn(engine, batch):<NewLine>    net.eval()<NewLine>    with torch.no_grad():<NewLine>        inputs, targets, weight_map = batch<NewLine>        outputs = net(inputs)<NewLine>        return outputs, targets, {""weight_map"": weight_map}<NewLine><NewLine><NewLine>trainer = Engine(update_model)<NewLine>evaluator = Engine(eval_fn)<NewLine><NewLine>metrics={""val loss"": Loss(criterion)}<NewLine><NewLine>for name, metric in metrics.items():<NewLine>    metric.attach(evaluator, name)<NewLine><NewLine><NewLine>@trainer.on(Events.EPOCH_COMPLETED)<NewLine>def run_validation():<NewLine>    evaluator.run(data)<NewLine><NewLine>trainer.run(data, max_epochs=2)<NewLine>print(evaluator.state.metrics)<NewLine>&gt; {'val loss': 1.027790516614914}<NewLine></code></pre><NewLine><p>Tell me if this works or does not work for you.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>It works! Thanks!<br/><NewLine>Now, could you explain why?! <img alt="":stuck_out_tongue:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/stuck_out_tongue.png?v=9"" title="":stuck_out_tongue:""/><br/><NewLine>Why does the <code>eval_fn</code> return a dictionary? (kwargs for some function?)</p><NewLine><p>According to the docs:<br/><NewLine><em>process_function (callable): A function receiving a handle to the engine and the current batch in each iteration, and returns data to be stored in the engine’s state.</em></p><NewLine><p>How does <code>Loss</code> receive the <code>weight_map</code>?</p><NewLine><p>And finally, for other metrics that only need outputs and target, do I have to run another evaluator again to log them?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>There is no magic there <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>To be able to pass additional attribute to <code>Loss</code> we need to set the output in format <code>(prediction, target, kwargs)</code> as according to the <a href=""https://pytorch.org/ignite/metrics.html#ignite.metrics.Loss"" rel=""nofollow noopener"">docs</a>:</p><NewLine><blockquote><NewLine><p>output_transform (callable): a callable that is used to transform the<br/><NewLine>:class:<code>~ignite.engine.Engine</code>'s <code>process_function</code>'s output into the<br/><NewLine>form expected by the metric.<br/><NewLine>This can be useful if, for example, you have a multi-output model and<br/><NewLine>you want to compute the metric with respect to one of the outputs.<br/><NewLine>The output is expected to be a tuple <code>(prediction, target)</code> or<br/><NewLine>(prediction, target, kwargs) where kwargs is a dictionary of extra<br/><NewLine>keywords arguments. If extra keywords arguments are provided they are passed to <code>loss_fn</code>.</p><NewLine></blockquote><NewLine><p>You can also see how it is interpreted in the update function: <a href=""https://pytorch.org/ignite/_modules/ignite/metrics/loss.html#Loss"" rel=""nofollow noopener"">https://pytorch.org/ignite/_modules/ignite/metrics/loss.html#Loss</a></p><NewLine><p>Therefore, <code>eval_fn</code> as being engine’s processing function returns the output exactly in the good format for the Loss function.</p><NewLine><p>For other metrics, you need to filter out the last element (dict) with <code>output_transform</code> and attach to this evaluator (no need another one):</p><NewLine><pre><code class=""lang-python"">acc = Accuracy(output_transform=lambda out: out[0], out[1])<NewLine>acc.attach(evaluator, ""Accuracy"")<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xen0f0n; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/xen0f0n; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: April 18, 2020,  5:29pm; <NewLine> REPLY_DATE 2: April 18, 2020,  6:00pm; <NewLine> REPLY_DATE 3: April 19, 2020,  8:22am; <NewLine> REPLY_DATE 4: April 18, 2020, 10:41pm; <NewLine> REPLY_DATE 5: April 18, 2020, 10:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
75886,How to make ignite trainer handle error batch instead of stop the program?,2020-04-08T15:34:58.679Z,0,79,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Sometime, the single sample that does not work with model (dimension mismake) or None in batch (because error in loading data due to corrupt).</p><NewLine><p>Instead of throwing exception and stop the program, how to just do handle (log, write something) and carry-on with Ignite trainer.</p><NewLine><p>What should I do ? If possible, please give example in code.</p><NewLine></div>",https://discuss.pytorch.org/u/ThaiThien,(Thai Thien),ThaiThien,"April 8, 2020,  3:35pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/thaithien"">@ThaiThien</a> currently there is no proper way to skip a single iteration (Maybe it could worth a feature request).<br/><NewLine>However, still what can be done is the following:</p><NewLine><pre><code class=""lang-python""><NewLine>def process_function(engine, batch):<NewLine>     if not is_correct_batch(batch):<NewLine>         # let's return the previous output =&gt; wont break other handlers using state.output<NewLine>         # but metrics computation will be impacted... <NewLine>         return engine.state.output<NewLine><NewLine>     # otherwise, continue as if everyhing is OK<NewLine>        <NewLine>engine = Engine(process_function)<NewLine></code></pre><NewLine><p>Hope this could help with your problem.</p><NewLine><p>PS: feel free to open a feature request on the github if a proper handling of such situation is needed for you (and maybe others). Thanks</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: April 8, 2020,  4:00pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
74522,"Ignite, Correct way of using the library, how to pass model to callable",2020-03-27T11:25:07.666Z,3,133,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,<br/><NewLine>I’m new to pytorch and ignite libraries, and I’m trying to figure out which is the best way to use them.<br/><NewLine>I read almost all the tutorial on the ignite library but still I don’t get which is the correct way of use it.</p><NewLine><p>In every tutorial I see that the function used to define an <code>engine</code> is calling inside <code>model</code> but this last is not passed to it as parameters,<br/><NewLine>what I would have thought is that the correct way to use ignite is to “hide” everything to the final user, so I would have defined every stuff to do during the training inside a fit function of a certain model class</p><NewLine><p>but since I cannot pass parameters to the functions passed to the engine this result more difficult (""'I’m actually passing the model as optional parameter but kinda feel that it is not the correct way"")</p><NewLine><p>can anyone explain me the abstract way to use the ignite library in the correct way ??</p><NewLine><p>Thank you all !</p><NewLine></div>",https://discuss.pytorch.org/u/damicoedoardo,(Edoardo D'Amico),damicoedoardo,"March 27, 2020, 11:25am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/damicoedoardo"">@damicoedoardo</a>  basical usage of ignite is the following where we highly use global scope variables in majority of handlers:</p><NewLine><pre><code class=""lang-python""><NewLine>def main():<NewLine><NewLine>    train_loader, val_loader = get_dataloader(...)<NewLine>    optimizer = ...<NewLine>    model = ... <NewLine>    criterion = ...<NewLine><NewLine>    def train_step(engine, batch):<NewLine>        model.train()  # model is defined in the scope of main<NewLine>        optimizer.zero_grad()   # optimizer is defined in the scope of main<NewLine>        x, y = prepare_batch(batch)<NewLine>        y_pred = model(x)<NewLine>        loss = criterion(y_pred, y)   # criterion is defined in the scope of main<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine>        return loss.item()        <NewLine><NewLine><NewLine>     trainer = Engine(train_step)<NewLine>     evaluator = create_supervised_evaluator(model, metrics={'acc': Accuracy()})<NewLine><NewLine>    @trainer.on(Events.EPOCH_COMPLETED)<NewLine>    def run_validation(engine):<NewLine>        evaluator.run(val_loader)<NewLine><NewLine>    trainer.run(train_loader, max_epochs=10)<NewLine></code></pre><NewLine><p>If you wish not to use global scope like above, it is possible to do this like</p><NewLine><ol><NewLine><li>define a custom structure <code>Trainer</code> and pass necessary things to it</li><NewLine></ol><NewLine><pre><code class=""lang-python""><NewLine>class SpecificModelTrainer:<NewLine>    <NewLine>    def __init__(self, model, optimizer, criterion, *args, **kwargs):<NewLine>         # ... setup all to self.*<NewLine><NewLine>    def train_step(self, engine, batch):<NewLine>        self.model.train()  # model is defined in the scope of main<NewLine>        self.optimizer.zero_grad()   # optimizer is defined in the scope of main<NewLine>        x, y = self.prepare_batch(batch)<NewLine>        y_pred = self.model(x)<NewLine>        loss = self.criterion(y_pred, y)   # criterion is defined in the scope of main<NewLine>        loss.backward()<NewLine>        self.optimizer.step()<NewLine>        return loss.item()        <NewLine>         <NewLine><NewLine>def main():<NewLine>    train_loader, val_loader = get_dataloader(...)<NewLine>    optimizer = ...<NewLine>    model = ... <NewLine>    criterion = ...<NewLine><NewLine>    model_trainer = SpecificModelTrainer(...)<NewLine><NewLine>    trainer_engine = Engine(model_trainer.train_step)<NewLine><NewLine>    ...<NewLine></code></pre><NewLine><ol start=""2""><NewLine><li>use <code>trainer.state</code> for that</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">def main():<NewLine><NewLine>    train_loader, val_loader = get_dataloader(...)<NewLine>    optimizer = ...<NewLine>    model = ... <NewLine>    criterion = ...<NewLine><NewLine>    def train_step(engine, batch):<NewLine>        state = engine.state<NewLine>        state.model.train()  # model is defined in the scope of main<NewLine>        state.optimizer.zero_grad()   # optimizer is defined in the scope of main<NewLine>        x, y = state.prepare_batch(batch)<NewLine>        y_pred = state.model(x)<NewLine>        loss = state.criterion(y_pred, y)   # criterion is defined in the scope of main<NewLine>        loss.backward()<NewLine>        state.optimizer.step()<NewLine>        return loss.item()        <NewLine><NewLine><NewLine>     trainer = Engine(train_step)<NewLine>     evaluator = create_supervised_evaluator(model, metrics={'acc': Accuracy()})<NewLine><NewLine>    @trainer.on(Events.STARTED, model, optimizer, criterion, prepare_batch)<NewLine>    def setup_state(engine, m, o, c, p):<NewLine>         engine.state.model = m<NewLine>         engine.state.optimizer = o<NewLine>         engine.state.criterion = c<NewLine>         engine.state.prepare_batch = p<NewLine><NewLine>    @trainer.on(Events.EPOCH_COMPLETED)<NewLine>    def run_validation(engine):<NewLine>        evaluator.run(val_loader)<NewLine><NewLine>    trainer.run(train_loader, max_epochs=10)<NewLine></code></pre><NewLine><p>HTH</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much for the explanation and example of usage!<br/><NewLine>forgot to say, very good work on Ignite guys !</p><NewLine><p>Keep going!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not to mention how good ignite is, I’d suggest you to checkout pytorch lightning as well.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>And, please, don’t forget others: skorch, fastai, catalyst, …</p><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://i2.wp.com/neptune.ai/wp-content/uploads/2019/03/cropped-Artboard-12.png?fit=32%2C32&amp;ssl=1"" width=""32""/><NewLine><a href=""https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem"" rel=""nofollow noopener"" target=""_blank"" title=""02:41PM - 26 February 2020"">neptune.ai – 26 Feb 20</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""aspect-image"" style=""--aspect-ratio:690/383;""><img class=""thumbnail"" height=""383"" src=""https://i1.wp.com/neptune.ai/wp-content/uploads/PyTorch-blue.png?fit=1171%2C651&amp;ssl=1"" width=""690""/></div><NewLine><h3><a href=""https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem"" rel=""nofollow noopener"" target=""_blank"">8 Creators and Core Contributors Talk About Their Model Training Libraries...</a></h3><NewLine><p>There are 6 high-level training APIs in the PyTorch Ecosystem. Which one should you choose? I asked the authors to explain the differences between those libraries and that is how this post was created!</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/damicoedoardo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/braindotai; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: March 27, 2020, 12:59pm; <NewLine> REPLY_DATE 2: March 27, 2020,  1:21pm; <NewLine> REPLY_DATE 3: March 27, 2020,  1:25pm; <NewLine> REPLY_DATE 4: March 27, 2020,  2:25pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 2 Likes; <NewLine> 
74464,"Ignite metrics (recall, precision, etc) - thresholded_output_transform",2020-03-26T16:21:09.743Z,1,120,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m working on a binary task and I’m figuring out Recall, Precision, etc, as implemented by Ignite. As per <a href=""https://pytorch.org/ignite/_modules/ignite/metrics/recall.html"" rel=""nofollow noopener"">documentation</a>:</p><NewLine><p><em>“In binary and multilabel cases, the elements of <code>y</code> and <code>y_pred</code> should have 0 or 1 values. Thresholding of predictions can be done as below:”</em></p><NewLine><pre><code class=""lang-auto"">def thresholded_output_transform(output):<NewLine>            y_pred, y = output<NewLine>            y_pred = torch.round(y_pred)<NewLine>            return y_pred, y<NewLine></code></pre><NewLine><p>Am I correct to assume that  <code>output</code>  is what is returned by the model?<br/><NewLine>I’m using BCEWithLogitsLoss, so I have to pass logits through a sigmoid function. I have to use a sigmoid here as well, right?</p><NewLine><pre><code class=""lang-auto"">def thresholded_output_transform(output):<NewLine>            y_pred, y = output <NewLine>            y_pred = torch.sigmoid(y_pred)<NewLine>            y_pred = torch.clamp(y_pred, 0., 1.)<NewLine>            y_pred = torch.round(y_pred)<NewLine>            return y_pred, y<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/xen0f0n,(Xenofon Karagiannis),xen0f0n,"March 26, 2020,  4:23pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Output is what is returned by evaluator’s <code>process_function</code> :  <a href=""https://pytorch.org/ignite/metrics.html"" rel=""nofollow noopener"">https://pytorch.org/ignite/metrics.html</a></p><NewLine><blockquote><NewLine><p>I’m using BCEWithLogitsLoss, so I have to pass logits through a sigmoid function. I have to use a sigmoid here as well, right?</p><NewLine></blockquote><NewLine><p>Well, it depends on your choice how to define whether logit’s value is sufficient to be interpreted as 1.<br/><NewLine>Yes, it is more convenient to pass to “probabilties” using sigmoid and then apply a thresholding (0.5) as you put to your version of <code>thresholded_output_transform</code>.</p><NewLine><p>Here is another way to apply the thresholding:</p><NewLine><pre><code class=""lang-python"">t = 0.55<NewLine><NewLine>def thresholded_output_transform(output):<NewLine>    y_pred, y = output <NewLine>    y_pred = torch.sigmoid(y_pred)<NewLine>    y_pred = (y_pred &gt; t).float()<NewLine>    return y_pred, y<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/vfdev-5"">@vfdev-5</a> nice! Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xen0f0n; <NewLine> ,"REPLY_DATE 1: March 26, 2020,  5:00pm; <NewLine> REPLY_DATE 2: March 26, 2020,  5:02pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
74369,How to use Ignite with multiple output model / loss?,2020-03-25T18:19:58.807Z,0,157,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a model with multiple outputs. How to use multiple losses?</p><NewLine><pre><code class=""lang-auto"">import torch.nn as nn<NewLine><NewLine>class NeuralNetwork(nn.Module):<NewLine>  def __init__(self):<NewLine>    super(NeuralNetwork, self).__init__()<NewLine>    self.linear1 = nn.Linear(in_features = 3, out_features = 1)<NewLine>    self.linear2 = nn.Linear(in_features = 3,out_features = 2)<NewLine><NewLine>  def forward(self, x):<NewLine>    output1 = self.linear1(x)<NewLine>    output2 = self.linear2(x)<NewLine>    return output1, output2<NewLine></code></pre><NewLine><p>output1 - CrossEntropyLoss, output2 - MSE</p><NewLine><p>For one output it is easy:</p><NewLine><pre><code class=""lang-auto"">criterion = nn.CrossEntropyLoss()<NewLine>trainer = create_supervised_trainer(model, optimizer, criterion, device=device)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/odats,(Oleh Dats),odats,"March 25, 2020,  6:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Try to use this, this will work</p><NewLine><p>def forward(self, x):<br/><NewLine>out= self.linear1(x)<br/><NewLine>out = self.linear2(out)<br/><NewLine>return out</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/odats"">@odats</a> please see these issues labelled as ‘question’:</p><NewLine><ul><NewLine><li><a href=""https://github.com/pytorch/ignite/issues/353"" rel=""nofollow noopener"">https://github.com/pytorch/ignite/issues/353</a></li><NewLine><li><a href=""https://github.com/pytorch/ignite/issues/819"" rel=""nofollow noopener"">https://github.com/pytorch/ignite/issues/819</a></li><NewLine></ul><NewLine><p>HTH</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Muhammad_Izaz; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: March 25, 2020,  6:47pm; <NewLine> REPLY_DATE 2: March 26, 2020, 11:17am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
74146,Why random functions return same values during the training based on Ignite?,2020-03-23T15:16:10.780Z,4,127,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I use Ignite for training my model.<br/><NewLine>np.random.randint, torch.randint… called from  <span class=""mention"">@trainer.on</span>(Events.EPOCH_COMPLETED(every=1)) return the same values.<br/><NewLine>It looks like a seed is hardcoded. Why do they have it and how to cancel it?</p><NewLine></div>",https://discuss.pytorch.org/u/odats,(Oleh Dats),odats,"March 23, 2020,  3:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>CC <a class=""mention"" href=""/u/vfdev-5"">@vfdev-5</a>, who is the main author of the library.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/odats"">@odats</a> this is done to have reproducble trainings = user can resume the training from a checkpoint (based on epoch or iteration) and have “almost” the same training behaviour. Please, see this : <a href=""https://pytorch.org/ignite/concepts.html#resume-training"" rel=""nofollow noopener"">https://pytorch.org/ignite/concepts.html#resume-training</a></p><NewLine><p>To alter the seed, it is possible to specify the seed as the argument in the run:</p><NewLine><pre><code class=""lang-python"">trainer.run(dataloader, seed=1234)<NewLine></code></pre><NewLine><p>That being said we think to change this behaviour and master/nightly releases already setup the seed using torch default random generator : <a href=""https://github.com/pytorch/ignite/pull/799"" rel=""nofollow noopener"">https://github.com/pytorch/ignite/pull/799</a></p><NewLine><p>Hope this helps. Feel free to ask other questions.</p><NewLine><p>PS. <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> thanks for the notification ! I should have missed the email somehow…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have some randomness in Dataset get item. I do random shuffling of values. It looks like seed is not hardcoded there and I get true random data for each training step. Can you please confirm?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I have some randomness in Dataset get item. I do random shuffling of values. It looks like seed is not hardcoded there and I get true random data for each training step. Can you please confirm?</p><NewLine></blockquote><NewLine><p>Sorry, I do not get what you would like me to confirm.<br/><NewLine>On ignite’s side, seed is used to make random state “synchronization” at each dataloader restart (it corresponds in most of the cases to the epoch size). In this way, for a given iteration/epoch the dataflow can be the same (for a given seed). More precisely it is something like</p><NewLine><pre><code class=""lang-auto"">for e in range(num_epochs):<NewLine>    set_seed(seed + e)<NewLine>    do_single_epoch_iterations(dataloader)<NewLine></code></pre><NewLine><p>Hope this answers your question</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I mean in my custom dataset I have a code:</p><NewLine><pre><code class=""lang-auto"">class MyDataset(Dataset):<NewLine>   def __getitem__(self, idx):<NewLine>      item = self.dataset.iloc[idx]<NewLine>      new_item = np.random.permutation(item)<NewLine>      return new_item<NewLine></code></pre><NewLine><p>By default each time it returns random seed. My question is: will my dataset, for each epoch, return new randomly permuted examples?</p><NewLine><p>I have found some information:<br/><NewLine>However, while resuming from iteration, random data augmentations are not synchronized in the middle of the epoch and thus batches remaining until the end of en epoch can effectively be different of those from the initial run.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>IMO, there is no problems with your <code>getitem</code>'s permutations, they all will be generated randomly as supposed.</p><NewLine><blockquote><NewLine><p>By default each time it returns random seed.</p><NewLine></blockquote><NewLine><p><code>new_item = np.random.permutation(item)</code> it returns a random value not random state’s seed, right ?</p><NewLine><blockquote><NewLine><p>I have found some information:<br/><NewLine>However, while resuming from iteration, random data augmentations are not synchronized in the middle of the epoch and thus batches remaining until the end of en epoch can effectively be different of those from the initial run.</p><NewLine></blockquote><NewLine><p>This info is about when you try to resume/restart the training from a checkpoint that saved trainer’s state in the middle of epoch (e.g. I store checkpoints every 1000 iterations and my epoch is 5432 iterations).</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for clarification. Everything is clear now. It would be nice to have some documentation about this behaviour.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I agree we need to better update the documentation.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/odats; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/odats; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/odats; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: March 24, 2020,  5:10am; <NewLine> REPLY_DATE 2: March 24, 2020,  7:15am; <NewLine> REPLY_DATE 3: March 24, 2020,  8:25am; <NewLine> REPLY_DATE 4: March 24, 2020,  9:30am; <NewLine> REPLY_DATE 5: March 24, 2020,  9:39am; <NewLine> REPLY_DATE 6: March 24, 2020, 10:03am; <NewLine> REPLY_DATE 7: March 24, 2020, 10:05am; <NewLine> REPLY_DATE 8: March 24, 2020, 10:08am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> 
72453,Defining LSTM Model in Ignite,2020-03-08T04:40:12.011Z,1,124,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello all,</p><NewLine><p>I have a fully functional LSTM neural network in Keras. I found ignite as the closest alternative to Keras for PyTorch(which I have to use instead of Tensorflow due to some requirements), and need to implement exactly the same model in Ignite. I have my training data as a CSV file, and am attaching the code that needs to be converted below.</p><NewLine><pre><code class=""lang-auto"">import keras <NewLine>import pandas as pd<NewLine>import numpy as np<NewLine>import csv<NewLine>from unidecode import unidecode<NewLine>from keras.preprocessing.text import Tokenizer<NewLine>def _removeNonAscii(s): <NewLine>    return """".join(i for i in s if ord(i)&lt;128)<NewLine><NewLine>x_train = []<NewLine>y_train = []<NewLine>with open('/Users/anubhav/Desktop/dataset_/final.csv') as f:<NewLine>    reader = csv.reader(f)<NewLine>    for row in reader:<NewLine>        x_train.append(_removeNonAscii(str(row[0])))<NewLine>        y_train.append(str(row[1]))<NewLine><NewLine>tk = Tokenizer()<NewLine>tk.fit_on_texts(x_train)<NewLine>xtrain = tk.texts_to_sequences(x_train)<NewLine><NewLine>flattened = []<NewLine>for b in xtrain:<NewLine>    for t in b:<NewLine>        flattened.append(t)<NewLine>flattened<NewLine>npFlat = np.asarray(flattened)<NewLine><NewLine>ytrain = []<NewLine>for i in range(0, (len(y_train)-1)):<NewLine>    try:<NewLine>        ytrain.append(int(y_train[i]))<NewLine>    except ValueError:<NewLine>        print(""Error"", i)<NewLine><NewLine>vocabulary_size = len(tk.word_index)+1<NewLine>print(vocabulary_size)<NewLine><NewLine>model = keras.Sequential()<NewLine>model.add(keras.layers.Embedding(input_dim=vocabulary_size+1,<NewLine>         output_dim=50))<NewLine>model.add(keras.layers.LSTM(units=50,return_sequences=True))<NewLine>model.add(keras.layers.LSTM(units=10))<NewLine>model.add(keras.layers.Dropout(0.5))<NewLine>model.add(keras.layers.Dense(8))<NewLine>model.add(keras.layers.Dense(1, activation=""sigmoid""))<NewLine>model.compile(optimizer='adam', loss='mean_squared_error')<NewLine><NewLine>yTRain = np.asarray(ytrain)<NewLine>trainingReshape = np.zeros(401,)<NewLine>target = np.append(yTRain, trainingReshape)<NewLine><NewLine>model.fit(npFlat, target, epochs=29, batch_size=7, callbacks=[keras.callbacks.EarlyStopping(patience=5)])<NewLine></code></pre><NewLine><p>How would I convert this Keras model to Pytorch Ignite? This is really urgent, and help would be much appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/renegadels,(Anubhav B),renegadels,"March 8, 2020,  4:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/renegadels"">@renegadels</a></p><NewLine><blockquote><NewLine><p>How would I convert this Keras model to Pytorch Ignite?</p><NewLine></blockquote><NewLine><p>Maybe this tutorial can help you :<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/ignite/blob/master/examples/notebooks/TextCNN.ipynb"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/ignite/blob/master/examples/notebooks/TextCNN.ipynb"" rel=""nofollow noopener"" target=""_blank"">pytorch/ignite/blob/master/examples/notebooks/TextCNN.ipynb</a></h4><NewLine><pre><code class=""lang-ipynb"">{<NewLine> ""cells"": [<NewLine>  {<NewLine>   ""cell_type"": ""markdown"",<NewLine>   ""metadata"": {},<NewLine>   ""source"": [<NewLine>    ""# Convolutional Neural Networks for Sentence Classification using Ignite""<NewLine>   ]<NewLine>  },<NewLine>  {<NewLine>   ""cell_type"": ""markdown"",<NewLine>   ""metadata"": {},<NewLine>   ""source"": [<NewLine>    ""This is a tutorial on using Ignite to train neural network models, setup experiments and validate models.\n"",<NewLine>    ""\n"",<NewLine>    ""In this experiment, we'll be replicating [\n"",<NewLine>    ""Convolutional Neural Networks for Sentence Classification by Yoon Kim](https://arxiv.org/abs/1408.5882)! This paper uses CNN for text classification, a task typically reserved for RNNs, Logistic Regression, Naive Bayes.\n"",<NewLine>    ""\n"",<NewLine>    ""We want to be able to classify IMDB movie reviews and predict whether the review is positive or negative. IMDB Movie Review dataset comprises of 25000 positive and 25000 negative examples. The dataset comprises of text and label pairs. This is binary classification problem. We'll be using PyTorch to create the model, torchtext to import data and Ignite to train and monitor the models!\n"",<NewLine>    ""\n"",<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/ignite/blob/master/examples/notebooks/TextCNN.ipynb"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot <a class=""mention"" href=""/u/vfdev-5"">@vfdev-5</a>. Will check it out and get back to you.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/renegadels; <NewLine> ,"REPLY_DATE 1: March 8, 2020, 10:44am; <NewLine> REPLY_DATE 2: March 8, 2020, 12:19pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
71734,Epoch_length wrong number of iterations,2020-03-02T15:40:30.252Z,7,111,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to iterate over the training samples (172 samples) a fixed number of times (e.g. 512)</p><NewLine><p><code>trainer.run(train_loader, max_epochs=500, epoch_length=512)</code></p><NewLine><p>and in my Dataset subclass implementation I’ve  defined <code>__len__</code>  to return the number of samples (172 samples).<br/><NewLine>But epoch_length does not stop loading at 512, but at 688 samples (4x172). I’ve noticed this behavior by printing a counter when <code>__getitem__</code> is called.<br/><NewLine>Any thoughts?</p><NewLine></div>",https://discuss.pytorch.org/u/xen0f0n,(Xenofon Karagiannis),xen0f0n,"March 2, 2020,  3:40pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/xen0f0n"">@xen0f0n</a>, what is the expected behaviour for you ?<br/><NewLine>I think what happens is a normal behaviour:<br/><NewLine>Your data provider has 172 samples, when engine is asked to measure an epoch as 512 iterations it does not mean that data provider should be restarted after 512 iterations.</p><NewLine><pre><code class=""lang-auto"">data<NewLine>|-----|-----|-----|-----|-----|-----|-----|-----|<NewLine>epoch<NewLine>|----------|----------|----------|----------|<NewLine></code></pre><NewLine><p>So, trainer asks for data from the data provider and on 512th iteration, data loader has been restarted previously twice and provides 168th sample, then on 513th iteration (and epoch + 1), provided data corresponds to 169th sample from data loader.</p><NewLine><p>What do you think ?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/vfdev-5"">@vfdev-5</a>  Well, it would make sense if on 513th I was on epoch + 1.<br/><NewLine>But I don’t think that’s the case. At the completion of each epoch I log some metrics and this takes place on iteration 688 and not before. Hence, that’s when the epoch ends, right?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>OK, we need to check versions and the code. Here is my snippet</p><NewLine><pre><code class=""lang-python"">import ignite<NewLine>print(ignite.__version__)<NewLine><NewLine>from ignite.engine import Engine<NewLine><NewLine>trainer = Engine(lambda e, b: print(""{} - {} : {}"".format(e.state.epoch, e.state.iteration, b)))<NewLine><NewLine>data = list(range(172))<NewLine>trainer.run(data, epoch_length=512, max_epochs=10)<NewLine></code></pre><NewLine><p>with the output</p><NewLine><pre><code class=""lang-auto"">0.4.0.dev20200302<NewLine>1 - 1 : 0<NewLine>1 - 2 : 1<NewLine>1 - 3 : 2<NewLine>1 - 4 : 3<NewLine>1 - 5 : 4<NewLine>1 - 6 : 5<NewLine>1 - 7 : 6<NewLine>1 - 8 : 7<NewLine>1 - 9 : 8<NewLine>1 - 10 : 9<NewLine>1 - 11 : 10<NewLine>...<NewLine>1 - 510 : 165<NewLine>1 - 511 : 166<NewLine>1 - 512 : 167<NewLine>2 - 513 : 168<NewLine>2 - 514 : 169<NewLine>2 - 515 : 170<NewLine>2 - 516 : 171<NewLine>2 - 517 : 0<NewLine>2 - 518 : 1<NewLine>2 - 519 : 2<NewLine>2 - 520 : 3<NewLine>2 - 521 : 4<NewLine>2 - 522 : 5<NewLine>2 - 523 : 6<NewLine>...<NewLine>10 - 5112 : 123<NewLine>10 - 5113 : 124<NewLine>10 - 5114 : 125<NewLine>10 - 5115 : 126<NewLine>10 - 5116 : 127<NewLine>10 - 5117 : 128<NewLine>10 - 5118 : 129<NewLine>10 - 5119 : 130<NewLine>10 - 5120 : 131<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/vfdev-5"">@vfdev-5</a> I got the same output. But I still can’t explain that the evaluation happening on EPOCH_COMPLETED<br/><NewLine><code>@trainer.on(Events.EPOCH_COMPLETED)</code><br/><NewLine>takes place NOT on iteration 512 but on 688. I’ve also tried this for <code>epoch_length=3</code>, and <code>engine.state.iteration</code> is 172 when logging.</p><NewLine><pre><code class=""lang-auto""><NewLine>@trainer.on(Events.EPOCH_COMPLETED)<NewLine>    def evaluate(trainer):<NewLine>        with evaluator.add_event_handler(Events.COMPLETED, log_metrics, ""train""):<NewLine>            evaluator.run(train_loader)<NewLine></code></pre><NewLine><p>It would make more sense to me if the both <code>Events.ITERATION_COMPLETED(every=512)</code> and <code>Events.EPOCH_COMPLETED(every=1)</code> fired the “same time”.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/vfdev-5"">@vfdev-5</a> it seems I’ve made a mistake… Trainer accepts <code>epoch_length</code> as argument, BUT I’ve created an evaluator engine  with <code>create_supervised_evaluator</code> and although <code>trainer.state</code> is correct, <code>evaluator.state</code> doesn’t have the same <code>epoch</code> and <code>iteration</code>.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>So, could you find where is a problem in your code ?</p><NewLine><blockquote><NewLine><p>although <code>trainer.state</code> is correct, <code>evaluator.state</code> doesn’t have the same <code>epoch</code> and <code>iteration</code> .</p><NewLine></blockquote><NewLine><p><code>evaluator.state</code> normally contains nothing interesting except metrics. As we usually run it<br/><NewLine>like <code>evaluator.run(train_loader)</code> it makes 1 epoch defined by <code>len(train_loader)</code> and its should contain according to your example, the following:</p><NewLine><pre><code class=""lang-auto"">iteration: 172<NewLine>epoch: 1<NewLine>epoch_length: 172<NewLine>max_epochs: 1<NewLine></code></pre><NewLine><p>And it is correct, evaluator did 1 epoch and 172 iterations as asked. And it does not contain any training information.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/vfdev-5"">@vfdev-5</a> I am a bit confused. I use the same evaluator for both <code>train_loader</code> and <code>val_loader</code>. Are the metrics calculated during training, or does the evaluator iterate the set again after the training epoch (with other random transforms) , just without updating the weights?</p><NewLine><p>On the train set, the metrics should take into account the samples of the whole epoch (512 samples) and not just the <code>len(train_loader)</code>. Having <code>batch_size=1</code>,  I should have 512 CE losses and calculate the mean. And IoU should be averaged for 512 samples (not 172).</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I use the same evaluator for both <code>train_loader</code> and <code>val_loader</code> . Are the metrics calculated during training, or does the evaluator iterate the set again after the training epoch (with other random transforms) , just without updating the weights?</p><NewLine></blockquote><NewLine><p>Evaluator does not compute metrics during the training. Yes, evaluator iterates the set again and metrics are computed with “fixed” model (without updating the weights).</p><NewLine><blockquote><NewLine><p>On the train set, the metrics should take into account the samples of the whole epoch (512 samples)</p><NewLine></blockquote><NewLine><p>in this case you need to just set <code>epoch_length=512</code> in <code>evaluator.run(train_loader, epoch_length=512)</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xen0f0n; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/xen0f0n; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/xen0f0n; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/xen0f0n; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: March 2, 2020,  8:04pm; <NewLine> REPLY_DATE 2: March 2, 2020,  8:08pm; <NewLine> REPLY_DATE 3: March 2, 2020,  8:11pm; <NewLine> REPLY_DATE 4: March 2, 2020,  8:49pm; <NewLine> REPLY_DATE 5: March 2, 2020,  9:03pm; <NewLine> REPLY_DATE 6: March 2, 2020,  9:16pm; <NewLine> REPLY_DATE 7: March 2, 2020,  9:32pm; <NewLine> REPLY_DATE 8: March 2, 2020, 10:18pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> 
70714,Naming .pth checkpoints using PyTorch Ignite ModelCheckpoint handler,2020-02-22T22:49:54.997Z,2,165,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I use the ModelCheckpoint handler in Ignite to save the model using these lines</p><NewLine><pre><code class=""lang-auto"">ckpt_handler = ModelCheckpoint(f'./experiments/{log_num}/checkpoints', filename_prefix='epoch_', n_saved=None, create_dir=True)<NewLine>trainer.add_event_handler(Events.EPOCH_COMPLETED(every=2), ckpt_handler, to_save)<NewLine></code></pre><NewLine><p>but the .pth filename follow this pattern --&gt; epoch__checkpoint_344.pth (344 refers to iterations).</p><NewLine><p>I would to include the number of epoch into the filename… Any idea how’s that done?</p><NewLine></div>",https://discuss.pytorch.org/u/xen0f0n,(Xenofon Karagiannis),xen0f0n,"February 22, 2020, 10:49pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/xen0f0n"">@xen0f0n</a> it can be done by adding a <code>global_step_transform=lambda e, _: e.state.epoch</code></p><NewLine><pre><code class=""lang-python"">checkpointer = ModelCheckpoint(*args, **kwargs, global_step_transform=lambda e, _: e.state.epoch)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <a class=""mention"" href=""/u/vfdev-5"">@vfdev-5</a>! Not sure how this works… I tried accessing trainer.state.epoch (before trainer.run) and state was None. Is this what that lambda does? Accessing trainer engine state after trainer.run?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, state at the begining of the training is None as it is not defined.<br/><NewLine>When you attached</p><NewLine><pre><code class=""lang-auto"">trainer.add_event_handler(Events.EPOCH_COMPLETED(every=2), ckpt_handler, to_save)<NewLine></code></pre><NewLine><p>once every 2 epoch, <code>ckpt_handler</code> is triggered to save what to save. Its argument <code>global_step_transform</code> is an optional callable that</p><NewLine><blockquote><NewLine><p>global step transform function to output a desired global step. Input of the function is (engine, event_name). Output of function should be an integer. Default is None, global_step based on attached engine. If provided, uses function output as global_step.</p><NewLine></blockquote><NewLine><p>So it is just executed during the training, where trainer’s state is initialized and can be used.</p><NewLine><p>Please, see the docs for info : <a href=""https://pytorch.org/ignite/handlers.html#ignite.handlers.Checkpoint"" rel=""nofollow noopener"">https://pytorch.org/ignite/handlers.html#ignite.handlers.Checkpoint</a></p><NewLine><p>Hope this helps <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xen0f0n; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: February 22, 2020, 11:16pm; <NewLine> REPLY_DATE 2: February 22, 2020, 11:15pm; <NewLine> REPLY_DATE 3: February 22, 2020, 11:27pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
65056,Ignite TypeError: &lsquo;Events&rsquo; object is not callable,2019-12-26T12:27:59.201Z,2,256,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I get this error when I try to execute the example <a href=""https://github.com/pytorch/ignite/blob/master/examples/mnist/mnist_with_tensorboardx.py"" rel=""nofollow noopener"">https://github.com/pytorch/ignite/blob/master/examples/mnist/mnist_with_tensorboardx.py</a></p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""/content/mnist_with_tensorboardx.py"", line 130, in &lt;module&gt;<NewLine>    args.log_interval, args.log_dir)<NewLine>  File ""/content/mnist_with_tensorboardx.py"", line 76, in run<NewLine>    @trainer.on(Events.ITERATION_COMPLETED(every=log_interval))<NewLine>TypeError: 'Events' object is not callable<NewLine></code></pre><NewLine><p>I’m working on google colab so I had to install some packages and I also used  TensorBoard notebook extension:</p><NewLine><pre><code class=""lang-auto"">!pip install --pre pytorch-ignite<NewLine>!pip install 'grpcio==1.24.3'<NewLine>!pip install -q tf-nightly-2.0-preview<NewLine>%load_ext tensorboard <NewLine>!python /content/mnist_with_tensorboardx.py --log_dir=/tmp/tensorboard_logs<NewLine>%tensorboard --logdir {/tmp/tensorboard_logs}<NewLine></code></pre><NewLine><p>According to this issue <a href=""https://github.com/pytorch/ignite/issues/687"" rel=""nofollow noopener"">https://github.com/pytorch/ignite/issues/687</a>, the problem is solved when the latest nightly version is used however for me it’s not the case.</p><NewLine></div>",https://discuss.pytorch.org/u/Upgrade_Yourself,(Upgrade Yourself),Upgrade_Yourself,"December 26, 2019, 12:30pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/upgrade_yourself"">@Upgrade_Yourself</a> I replied on this issue here too : <a href=""https://github.com/pytorch/ignite/issues/687"" rel=""nofollow noopener"">https://github.com/pytorch/ignite/issues/687</a><br/><NewLine>Please, tell me if it works now for you.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I haven’t yet looked at the code so I don’t know what you changed exactly but yes it works. That was a quick answer <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually, there is nothing changed in the code, except a typo introduced while previously refactored some stuff… You can see the change here too : <a href=""https://github.com/pytorch/ignite/commit/8f9ca9059e3e5341527f4f6d05923508b20812f0"" rel=""nofollow noopener"">https://github.com/pytorch/ignite/commit/8f9ca9059e3e5341527f4f6d05923508b20812f0</a></p><NewLine><p>Glad, that it works for you now.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Upgrade_Yourself; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vfdev-5; <NewLine> ,"REPLY_DATE 1: December 26, 2019, 12:36pm; <NewLine> REPLY_DATE 2: December 26, 2019, 12:37pm; <NewLine> REPLY_DATE 3: December 26, 2019, 12:39pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
48904,Implement custom metric in ignite,2019-06-25T10:53:01.598Z,1,557,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Are there some examples on how to implement a custom metric in Ignite?</p><NewLine></div>",https://discuss.pytorch.org/u/Luca_Pamparana,(Luca Pamparana),Luca_Pamparana,"June 25, 2019, 10:53am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/luca_pamparana"">@Luca_Pamparana</a> to implement a custom metric is simple: you need to override <code>ignite.metrics.Metric</code> class and reimplement 3 methods: <code>reset</code>, <code>update</code> and <code>compute</code>:</p><NewLine><pre><code class=""lang-python"">from ignite.metrics import Metric<NewLine><NewLine><NewLine>class MyMetric(Metric):<NewLine><NewLine>    def __init__(self, output_transform=lambda x: x):<NewLine>        self._var1 = None<NewLine>        self._var2 = None<NewLine>        super(MyMetric, self).__init__(output_transform=output_transform)<NewLine><NewLine>    def reset(self):<NewLine>        self._var1 = 0<NewLine>        self._var2 = 0<NewLine><NewLine>    def update(self, output):<NewLine>        y_pred, y = output<NewLine>        # ... your custom implementation to update internal state on after a single iteration<NewLine>        # e.g. self._var2 += y.shape[0]<NewLine>              <NewLine>    def compute(self):<NewLine>        # compute the metric using the internal variables<NewLine>        # res = self._var1 / self._var2<NewLine>        res = 0<NewLine>        return res<NewLine></code></pre><NewLine><p>For other examples, take a look at the source code :<br/><NewLine>MSE: <a href=""https://github.com/pytorch/ignite/blob/master/ignite/metrics/mean_squared_error.py"" rel=""nofollow noopener"">https://github.com/pytorch/ignite/blob/master/ignite/metrics/mean_squared_error.py</a><br/><NewLine>etc</p><NewLine><p>HTH</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for that reply!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Luca_Pamparana; <NewLine> ,"REPLY_DATE 1: June 25, 2019,  1:00pm; <NewLine> REPLY_DATE 2: June 26, 2019,  7:24pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> 
