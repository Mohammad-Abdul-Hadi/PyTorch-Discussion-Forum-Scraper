id,title,created_at,reply_count,views,description,creator_link,creator_name,creator_alias,post_date,post_likes,replies,repliers_links,reply_dates,reply_likes
17547,About the glow category,2018-05-05T00:22:39.490Z,1,744,"<div class=""post"" itemprop=""articleBody""><NewLine><p>This category is for the Glow neural network accelerator compiler: <a href=""https://github.com/pytorch/glow"" rel=""nofollow noopener"">https://github.com/pytorch/glow</a></p><NewLine></div>",https://discuss.pytorch.org/u/smth,,smth,"January 11, 2019,  2:44pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>im thinking along the same lines. who created this category and what is it for?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>also, this thread looks useful: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/glow-memory-leaking-in-opencl-and-build-issues/30950"">[GLOW] Memory leaking in opencl and build issues</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Beatrice_Paige; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Beatrice_Paige; <NewLine> ,"REPLY_DATE 1: December 26, 2018,  1:26pm; <NewLine> REPLY_DATE 2: December 26, 2018,  1:29pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
75974,Is Conv1D fully supported now?,2020-04-09T05:53:22.744Z,1,115,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Back in Oct 2019 Glow did not support ONNX models with Conv1D . A workaround was to use Conv2D instead. Now I see a recent merge for Conv1D (<a href=""https://github.com/pytorch/glow/pull/4317"" rel=""nofollow noopener"">https://github.com/pytorch/glow/pull/4317</a>). Does it mean there is a Conv1D support?</p><NewLine></div>",https://discuss.pytorch.org/u/antimora,(DT),antimora,"April 9, 2020,  5:53am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, it should work!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jfix"">@jfix</a>, I can confirm the Conv1D in ONNX is supported. I have a follow up question, however.</p><NewLine><p>Since Conv1D uses Conv2D by transposing/reshaping tensor, can you please confirm transposing and reshaping operations are cheap? I see lots of extra/new reshape and transpose nodes in graph.dot, whereas ONNX does not contain any.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/antimora; <NewLine> ,"REPLY_DATE 1: April 9, 2020,  6:11am; <NewLine> REPLY_DATE 2: September 24, 2020,  5:40am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
75973,Optimization guide using model-compiler,2020-04-09T05:49:29.229Z,1,147,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am exploring Glow to optimize my model that has a bunch of conv + batch normalization + relu. And unfortunately I do not see a good guide for this. I looked through available options ( <code>./build/bin/model-compiler -help-list</code> ) but I am lost which I should be applying.</p><NewLine><p>Here is a partial screenshot of the model:</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/9436eeec97a955b1062436d35f0fb8a1cd20a75a"" href=""https://discuss.pytorch.org/uploads/default/original/3X/9/4/9436eeec97a955b1062436d35f0fb8a1cd20a75a.png"" title=""image""><img alt=""image"" data-base62-sha1=""l9amiAkTGS1ldCp4nANXGIHtPOG"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/4/9436eeec97a955b1062436d35f0fb8a1cd20a75a_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/4/9436eeec97a955b1062436d35f0fb8a1cd20a75a_2_319x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/4/9436eeec97a955b1062436d35f0fb8a1cd20a75a_2_319x500.png, https://discuss.pytorch.org/uploads/default/optimized/3X/9/4/9436eeec97a955b1062436d35f0fb8a1cd20a75a_2_478x750.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/9/4/9436eeec97a955b1062436d35f0fb8a1cd20a75a_2_638x1000.png 2x"" width=""319""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">666×1042 60.3 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>Does model-compiler automatically apply optimizations like fusing nodes? Or one needs to apply manually?</p><NewLine><p>I am using PyTorch 1.4 and exporting my model in ONNX format.</p><NewLine></div>",https://discuss.pytorch.org/u/antimora,(DT),antimora,"April 9, 2020,  5:49am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Optimizations are done automatically. Some are dependent on the backend you’re running on. In the model snippet you have here I’d expect the BatchNorms to be optimized away (fused into Conv’s weights). You should be able to pass <code>-dump-graph-DAG=graph.dot</code> to see the Glow graph after optimizations are done. Some things such as Relu fusing may be done at lower levels of IR which are a bit harder to see (e.g. might be done in LLVM IR for the CPU backend or other LLVM-based backends).</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/antimora"">@antimora</a> I am also facing problem in finding good guide/tutorial? Have you got any reference or share your own experience?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Nothing beyond what Jordan has suggested. I can confirm via graph.dot some of the nodes, such as batch norm, are fused.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tkg.rahul; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/antimora; <NewLine> ,"REPLY_DATE 1: April 9, 2020,  6:09am; <NewLine> REPLY_DATE 2: September 15, 2020,  6:43pm; <NewLine> REPLY_DATE 3: September 24, 2020,  5:35am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
93081,Build for wsl2 ubuntu 20.04,2020-08-17T12:14:28.251Z,1,71,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all! I am trying to build glow so i can start learning and contributing <img alt="":grinning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/grinning.png?v=9"" title="":grinning:""/></p><NewLine><p>are these warnings below ignorable?<br/><NewLine>also, could you point to the right direction where i can build a subset of the project? or i have to use debug/release everytime to ensure everything goes nicely</p><NewLine><p>muhd@mud:/opensource/build_Debug$ ninja all<br/><NewLine>[218/653] Building CXX object thirdparty/folly/CMakeFiles/folly_base.dir/folly/synchronization/detail/Hardware.cpp.o<br/><NewLine>/home/muhd/opensource/glow/thirdparty/folly/folly/synchronization/detail/Hardware.cpp:122:72: warning: unused parameter ‘status’ [-Wunused-parameter]<br/><NewLine>[[noreturn]] FOLLY_DISABLE_SANITIZERS static void rtmAbortFunc(uint8_t status) {<br/><NewLine>^<br/><NewLine>1 warning generated.<br/><NewLine>[258/653] NodeGen: Generating nodes.<br/><NewLine>Writing node descriptors to:<br/><NewLine>/home/muhd/opensource/build_Debug/glow/AutoGenNodes.h<br/><NewLine>/home/muhd/opensource/build_Debug/glow/AutoGenNodes.cpp<br/><NewLine>/home/muhd/opensource/build_Debug/glow/AutoGenNodes.def<br/><NewLine>/home/muhd/opensource/build_Debug/glow/AutoGenNodesImport.h<br/><NewLine>/home/muhd/opensource/build_Debug/glow/AutoGenNodesExport.h<br/><NewLine>[262/653] InstrGen: Generating instructions.<br/><NewLine>Writing instr descriptors to:<br/><NewLine>/home/muhd/opensource/build_Debug/glow/AutoGenInstr.h<br/><NewLine>/home/muhd/opensource/build_Debug/glow/AutoGenInstr.cpp<br/><NewLine>/home/muhd/opensource/build_Debug/glow/AutoGenInstr.def<br/><NewLine>/home/muhd/opensource/build_Debug/glow/AutoGenIRBuilder.h<br/><NewLine>/home/muhd/opensource/build_Debug/glow/AutoGenIRBuilder.cpp<br/><NewLine>/home/muhd/opensource/build_Debug/glow/AutoGenIRGen.h<br/><NewLine>[320/653] Building CXX object lib/LLVMIRCodeGen/CMakeFiles/LLVMIRCodeGen.dir/GlowJIT.cpp.o<br/><NewLine>/home/muhd/opensource/glow/lib/LLVMIRCodeGen/GlowJIT.cpp:139:7: warning: ‘LegacyLocalCXXRuntimeOverrides’ is deprecated: ORCv1 utilities (utilities with the ‘Legacy’ prefix) are deprecated. Please use the ORCv2 LocalCXXRuntimeOverrides utility instead [-Wdeprecated-declarations]<br/><NewLine>cxxSymbolOverride_(<br/><NewLine>^<br/><NewLine>/usr/lib/llvm-9/include/llvm/ExecutionEngine/Orc/ExecutionUtils.h:224:33: note: ‘LegacyLocalCXXRuntimeOverrides’ has been explicitly marked deprecated here<br/><NewLine>LegacyLocalCXXRuntimeOverrides::LegacyLocalCXXRuntimeOverrides(<br/><NewLine>^<br/><NewLine>/home/muhd/opensource/glow/lib/LLVMIRCodeGen/GlowJIT.cpp:183:7: warning: ‘LegacyRTDyldObjectLinkingLayer’ is deprecated: ORCv1 layers (layers with the ‘Legacy’ prefix) are deprecated. Please use ORCv2 (see docs/ORCv2.rst) [-Wdeprecated-declarations]<br/><NewLine>objectLayer_(ES_,<br/><NewLine>^<br/><NewLine>/usr/lib/llvm-9/include/llvm/ExecutionEngine/Orc/RTDyldObjectLinkingLayer.h:356:3: note: ‘LegacyRTDyldObjectLinkingLayer’ has been explicitly marked deprecated here<br/><NewLine>LLVM_ATTRIBUTE_DEPRECATED(<br/><NewLine>^<br/><NewLine>/usr/lib/llvm-9/include/llvm/Support/Compiler.h:277:23: note: expanded from macro ‘LLVM_ATTRIBUTE_DEPRECATED’<br/><NewLine>decl  <strong>attribute</strong> ((deprecated(message)))<br/><NewLine>^<br/><NewLine>/home/muhd/opensource/glow/lib/LLVMIRCodeGen/GlowJIT.cpp:191:7: warning: ‘LegacyIRCompileLayer’ is deprecated: ORCv1 layers (layers with the ‘Legacy’ prefix) are deprecated. Please use the ORCv2 IRCompileLayer instead [-Wdeprecated-declarations]<br/><NewLine>compileLayer_(objectLayer_, SimpleCompiler(TM_)) {<br/><NewLine>^<br/><NewLine>/usr/lib/llvm-9/include/llvm/ExecutionEngine/Orc/IRCompileLayer.h:66:3: note: ‘LegacyIRCompileLayer’ has been explicitly marked deprecated here<br/><NewLine>LLVM_ATTRIBUTE_DEPRECATED(<br/><NewLine>^<br/><NewLine>/usr/lib/llvm-9/include/llvm/Support/Compiler.h:277:23: note: expanded from macro ‘LLVM_ATTRIBUTE_DEPRECATED’<br/><NewLine>decl  <strong>attribute</strong> ((deprecated(message)))<br/><NewLine>^<br/><NewLine>/home/muhd/opensource/glow/lib/LLVMIRCodeGen/GlowJIT.cpp:233:49: warning: ‘LegacyCtorDtorRunner’ is deprecated: ORCv1 utilities (utilities with the ‘Legacy’ prefix) are deprecated. Please use the ORCv2 CtorDtorRunner utility instead [-Wdeprecated-declarations]<br/><NewLine>LegacyCtorDtorRunner&lt;decltype(compileLayer_)&gt; ctorRunner(std::move(ctorNames),<br/><NewLine>^<br/><NewLine>/usr/lib/llvm-9/include/llvm/ExecutionEngine/Orc/ExecutionUtils.h💯3: note: ‘LegacyCtorDtorRunner’ has been explicitly marked deprecated here<br/><NewLine>LLVM_ATTRIBUTE_DEPRECATED(<br/><NewLine>^<br/><NewLine>/usr/lib/llvm-9/include/llvm/Support/Compiler.h:277:23: note: expanded from macro ‘LLVM_ATTRIBUTE_DEPRECATED’<br/><NewLine>decl  <strong>attribute</strong> ((deprecated(message)))<br/><NewLine>^<br/><NewLine>4 warnings generated.<br/><NewLine>[653/653] Linking CXX executable tests/ConcatBench<br/><NewLine>muhd@mud:/opensource/build_Debug$</p><NewLine></div>",https://discuss.pytorch.org/u/muhdmud,(muhdmud),muhdmud,"August 17, 2020, 12:14pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hmm, think these warnings can be ignored. Please feel free to upstream fixes if you’d like! <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>What subset are you interested in? You could always disable e.g. the OpenCL and/or CPU backends when you build, in order to build/test a bit less, e.g. via <code>-DGLOW_WITH_CPU=OFF -DGLOW_WITH_OPENCL=OFF</code> when you run <code>cmake</code>. But not sure how you’re using Glow/if that helps you at all.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for the reply! When you say upstream fix, do you mean fix this error so that we can use wsl2 for the future?</p><NewLine><p>The subset i am interested in, is to do with this issue [Support for LogSoftmax operator is missing] <a href=""https://github.com/pytorch/glow/issues/4399#"" rel=""nofollow noopener"">https://github.com/pytorch/glow/issues/4399#</a>, was trying to implement it when i faced this alert. so since its ignorable now should be fine <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/muhdmud; <NewLine> ,"REPLY_DATE 1: September 14, 2020,  6:48am; <NewLine> REPLY_DATE 2: September 24, 2020,  4:17am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
95745,Torch_glow tests fail unless run separately,2020-09-10T06:38:02.226Z,0,35,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>upon running python3.6 setup.py test --run_cmake all the tests are failing</p><NewLine><p>============================================================= test session starts =============================================================<br/><NewLine>platform linux – Python 3.6.12, pytest-2.8.7, py-1.4.31, pluggy-0.3.1<br/><NewLine>rootdir: /home/kpit/glow_compiler/glow_new/torch_glow, inifile:<br/><NewLine>plugins: colcon-core-0.5.10, cov-2.2.1<br/><NewLine>collected 243 items</p><NewLine><p>tests/functionality/blacklist_test.py …<br/><NewLine>tests/functionality/conv_to_glow_test.py .FFF<br/><NewLine>tests/functionality/fused_linear_test.py .<br/><NewLine>tests/functionality/glow_compile_spec_test.py .<br/><NewLine>tests/functionality/jit_vs_glow_path_test.py FFF<br/><NewLine>tests/functionality/load_backend_specific_options_test.py F<br/><NewLine>tests/functionality/max_fusion_merge_size_test.py …<br/><NewLine>tests/functionality/min_graph_size_test.py .<br/><NewLine>tests/functionality/only_tensor_outputs_test.py .<br/><NewLine>tests/functionality/quantized_cut_in_the_middle_test.py F<br/><NewLine>tests/functionality/randomize_constants_test.py .<br/><NewLine>tests/functionality/remove_exceptions_test.py .<br/><NewLine>tests/functionality/set_glow_backend_test.py .<br/><NewLine>tests/functionality/to_glow_selective_test.py .<br/><NewLine>tests/nodes/Int_test.py FF<br/><NewLine>tests/nodes/adaptive_avg_pool2d_test.py FFF<br/><NewLine>tests/nodes/add_test.py FFFFFFF<br/><NewLine>tests/nodes/addmm_test.py FFF<br/><NewLine>tests/nodes/avgpool2d_test.py FF<br/><NewLine>tests/nodes/avgpool3d_test.py FF<br/><NewLine>tests/nodes/batchnorm2d_test.py FF<br/><NewLine>tests/nodes/batchnorm3d_test.py FF<br/><NewLine>tests/nodes/bmm_test.py F<br/><NewLine>tests/nodes/cat_test.py FF.<br/><NewLine>tests/nodes/clamp_test.py F<br/><NewLine>tests/nodes/constant_chunk_test.py FF<br/><NewLine>tests/nodes/contiguous_test.py F<br/><NewLine>tests/nodes/conv2d_test.py FsF<br/><NewLine>tests/nodes/conv3d_test.py FFF<br/><NewLine>tests/nodes/div_test.py FFFFFF<br/><NewLine>tests/nodes/dropout_test.py FF<br/><NewLine>tests/nodes/embedding_bag_test.py F<br/><NewLine>tests/nodes/exp_test.py F<br/><NewLine>tests/nodes/flatten_test.py FFFFFF<br/><NewLine>tests/nodes/floor_div_test.py sFFFFF<br/><NewLine>tests/nodes/floor_test.py F<br/><NewLine>tests/nodes/gelu_test.py F<br/><NewLine>tests/nodes/getattr_test.py F<br/><NewLine>tests/nodes/layernorm_test.py FF<br/><NewLine>tests/nodes/linear_test.py FFF<br/><NewLine>tests/nodes/masked_fill_test.py FFFFF<br/><NewLine>tests/nodes/matmul_test.py FFFFFFFF<br/><NewLine>tests/nodes/max_test.py F<br/><NewLine>tests/nodes/maxpool2d_test.py FF<br/><NewLine>tests/nodes/mean_test.py FF<br/><NewLine>tests/nodes/min_test.py F<br/><NewLine>tests/nodes/mm_test.py F<br/><NewLine>tests/nodes/mul_test.py FFFFFF<br/><NewLine>tests/nodes/norm_test.py FFF<br/><NewLine>tests/nodes/num_to_tensor_test.py FF<br/><NewLine>tests/nodes/permute_test.py F<br/><NewLine>tests/nodes/pow_test.py F<br/><NewLine>tests/nodes/prelu_test.py F<br/><NewLine>tests/nodes/quantized_add_relu_test.py FFF<br/><NewLine>tests/nodes/quantized_add_test.py FFF<br/><NewLine>tests/nodes/quantized_avgpool2d_test.py FF<br/><NewLine>tests/nodes/quantized_avgpool3d_test.py FF<br/><NewLine>tests/nodes/quantized_batchnorm2d_test.py FF<br/><NewLine>tests/nodes/quantized_batchnorm3d_relu_test.py F<br/><NewLine>tests/nodes/quantized_batchnorm3d_test.py FF<br/><NewLine>tests/nodes/quantized_conv2d_relu_test.py FFF<br/><NewLine>tests/nodes/quantized_conv2d_test.py FFFFFF<br/><NewLine>tests/nodes/quantized_conv3d_relu_test.py FFF<br/><NewLine>tests/nodes/quantized_conv3d_test.py FFFF<br/><NewLine>tests/nodes/quantized_linear_test.py FFFs<br/><NewLine>tests/nodes/quantized_maxpool_test.py FF<br/><NewLine>tests/nodes/quantized_mul_test.py FFFF<br/><NewLine>tests/nodes/quantized_relu_test.py FF<br/><NewLine>tests/nodes/reciprocal_test.py FF<br/><NewLine>tests/nodes/relu_test.py FF<br/><NewLine>tests/nodes/reshape_test.py F<br/><NewLine>tests/nodes/roi_align_rotated_test.py FFFFFFFF<br/><NewLine>tests/nodes/roi_align_test.py FFFFFFF<br/><NewLine>tests/nodes/rsub_test.py FFFFFF<br/><NewLine>tests/nodes/sigmoid_test.py FF<br/><NewLine>tests/nodes/size_test.py .<br/><NewLine>tests/nodes/slice_test.py F<br/><NewLine>tests/nodes/softmax_test.py FF.<br/><NewLine>tests/nodes/sqrt_test.py FF<br/><NewLine>tests/nodes/squeeze_test.py FFFF<br/><NewLine>tests/nodes/stack_test.py F<br/><NewLine>tests/nodes/sub_test.py FFFFFF<br/><NewLine>tests/nodes/tanh_test.py FF<br/><NewLine>tests/nodes/to_test.py FFF<br/><NewLine>tests/nodes/topk_test.py F<br/><NewLine>tests/nodes/transpose_test.py FFFFFF.<br/><NewLine>tests/nodes/typeas_test.py FFFFFF<br/><NewLine>tests/nodes/unsqueeze_test.py FFFFFF<br/><NewLine>tests/nodes/upsample_test.py FFFFFF<br/><NewLine>tests/nodes/view_test.py F</p><NewLine><p>However upon running python3.6 -m pytest nodes<br/><NewLine>219 passed, 3 skipped in 19.03 seconds (All most all test passed)</p><NewLine><p>Requesting help on the same?</p><NewLine></div>",https://discuss.pytorch.org/u/tkg.rahul,(Rahul),tkg.rahul,"September 10, 2020,  6:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for reporting this – would you mind creating an issue on Github for this to move the conversation there?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: September 17, 2020, 11:59pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
81422,Model bundle performance / multi-core inference,2020-05-15T18:18:20.024Z,4,188,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!</p><NewLine><p>I am trying to use Glow in order to increase inference speed for a custom pytorch model based on mobilenet architecture. The model was converted to onnx format and I modified <a href=""https://github.com/pytorch/glow/tree/master/examples/bundles/resnet50"" rel=""nofollow noopener"">resnet50 example</a> to get a model bundle for my model.<br/><NewLine>Inference time for original traced pytorch model is <strong>~27ms</strong> per image. For the compiled bundle inference time is <strong>~800ms</strong>.<br/><NewLine>Taking in to account that glow uses only 1 core during inference and I have 12-core (24 thread) CPU, the expected performance for multi-core inference using glow is going to be 800 ms / 24 ~ <strong>33 ms</strong> (it is the lowest estimate).</p><NewLine><p>My questions:</p><NewLine><ol><NewLine><li>Performance of the compiled model is worse comparing with the original pytorch model. Is it expected? Have I missed some optimization steps? Should I use another way to get better performance?</li><NewLine><li>How can I run multi-core inference using Glow with CPU backend?</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/sheh,(Alexander Shekhovtsov),sheh,"May 15, 2020,  6:22pm",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><ol><NewLine><li>Performance of the compiled model is worse comparing with the original pytorch model. Is it expected? Have I missed some optimization steps? Should I use another way to get better performance?</li><NewLine></ol><NewLine></blockquote><NewLine><p>PyTorch works hard to implement optimized kernels for CPU. In Glow, we haven’t spent much time recently working on optimizing our Conv kernels for our CPU backend. We have focused much more on custom HW AI accelerators such as the NNPI and Habana backends in Glow. IIRC, last we focused on CPU perf a couple years ago it was originally tested on a few specific models of x86 CPUs. So your perf is going to depend on what CPU you’re using.</p><NewLine><blockquote><NewLine><ol start=""2""><NewLine><li>How can I run multi-core inference using Glow with CPU backend?</li><NewLine></ol><NewLine></blockquote><NewLine><p>We don’t currently support this. See prior discussion here: <a href=""https://github.com/pytorch/glow/issues/1749"" rel=""nofollow noopener"">https://github.com/pytorch/glow/issues/1749</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""81422"" data-username=""jfix""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/jfix/40/21510_2.png"" width=""20""/> jfix:</div><NewLine><blockquote><NewLine><p>PyTorch works hard to implement optimized kernels for CPU. In Glow, we haven’t spent much time recently working on optimizing our Conv kernels for our CPU backend. We have focused much more on custom HW AI accelerators such as the NNPI and Habana backends in Glow. IIRC, last we focused on CPU perf a couple years ago it was originally tested on a few specific models of x86 CPUs. So your perf is going to depend on what CPU you’re using.</p><NewLine></blockquote><NewLine></aside><NewLine><p><a class=""mention"" href=""/u/jfix"">@jfix</a> does this mean any evaluation board having any kind of cpu i.e. arm cortex won’t be able to take advantage of glow in inference?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can absolutely run on CPU architectures, and performance should still be pretty good, just might not be as good as other frameworks. You could always modify/add to the Glow CPU backend’s libjit with kernels for ops you care about that are better targeted for ARM.</p><NewLine><p>I know NXP has had a lot of success targeting ARM Cortex with Glow: <a href=""https://media.nxp.com/news-releases/news-release-details/industrys-first-mcu-based-implementation-glow-neural-network"" rel=""nofollow noopener"">https://media.nxp.com/news-releases/news-release-details/industrys-first-mcu-based-implementation-glow-neural-network</a> – maybe worth posting an issue on Glow’s GH with tagging mciprian13 from NXP to see if you can find out more.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot <a class=""mention"" href=""/u/jfix"">@jfix</a> for quick response. I will check NXP offerings.<br/><NewLine>Does glow still uses one cpu core for inference? I can see multi core inference issue is still open on github.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, we didn’t really invest much in multicore execution. We have focused on accelerators such as NNPI and Habana. I don’t expect it’d be hard to go data parallel and split a batch by the number of cores you have.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tkg.rahul; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/tkg.rahul; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: May 28, 2020,  9:55pm; <NewLine> REPLY_DATE 2: September 16, 2020,  9:40am; <NewLine> REPLY_DATE 3: September 16, 2020,  6:56pm; <NewLine> REPLY_DATE 4: September 17, 2020,  8:05am; <NewLine> REPLY_DATE 5: September 17, 2020, 10:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
96388,How to use Glow,2020-09-15T18:04:57.541Z,2,57,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Everyone,</p><NewLine><p>I am new to Glow and PyTorch (trying to learn both). I have trained a very simple Neural network based classifier in PyTorch(C++). I want to do inferencing on PI3 and want to take advantage of glow. Requesting you to please let me know what all steps I need to follow to achieve this.</p><NewLine><p>While checking example/mnist.cpp I observed that training, testing and validation is done using glow, however I was under the impression that glow has inference support only?</p><NewLine><p>Is there any example where glow is applied on computational graph?</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/tkg.rahul,(Rahul),tkg.rahul,"September 15, 2020,  6:05pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I am new to Glow and PyTorch (trying to learn both). I have trained a very simple Neural network based classifier in PyTorch(C++). I want to do inferencing on PI3 and want to take advantage of glow. Requesting you to please let me know what all steps I need to follow to achieve this.</p><NewLine></blockquote><NewLine><p>I think the best way here would be to use our AOT executable bundles, see <a href=""https://github.com/pytorch/glow/blob/master/docs/AOT.md"" rel=""nofollow noopener"">here</a> for more info.</p><NewLine><blockquote><NewLine><p>While checking example/mnist.cpp I observed that training, testing and validation is done using glow, however I was under the impression that glow has inference support only?</p><NewLine></blockquote><NewLine><p>Glow supports training to some extent – we support autograd and execution of many grad ops. However we have focused much more on inference.</p><NewLine><blockquote><NewLine><p>Is there any example where glow is applied on computational graph?</p><NewLine></blockquote><NewLine><p>Not sure what you’re asking about specifically here – you can find lots of simple examples of computational graphs we build directly in Glow IR in our <a href=""https://github.com/pytorch/glow/blob/master/tests/unittests/OperatorTest.cpp"" rel=""nofollow noopener""><code>OperatorTest.cpp</code></a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""96388"" data-username=""jfix""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/jfix/40/21510_2.png"" width=""20""/> jfix:</div><NewLine><blockquote><NewLine><p>Not sure what you’re asking about specifically here – you can find lots of simple examples of computational graphs we build directly in Glow IR in our <a href=""https://github.com/pytorch/glow/blob/master/tests/unittests/OperatorTest.cpp"" rel=""nofollow noopener""> <code>OperatorTest.cpp</code> </a>.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thanks for the response <a class=""mention"" href=""/u/jfix"">@jfix</a>. I will check OperatorTest.cpp.<br/><NewLine>Originally I wanted to know if there is any example which takes a pre trained model and converts this to computational graphs for which glow optimisation can be applied.<br/><NewLine>Trained Model -&gt; computational graph -&gt; glow optimisation -&gt; binary</p><NewLine><p>Apologies in advance if again I have confused you.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>So a lot of Glow’s focus is on computational graphs, we use a Node-based Graph representation as our high level IR, and that is what we focus much of our optimizations on (GraphOptimizer.cpp). In OperatorTest.cpp you can find many such graphs that we build and run – they haven’t been trained on real models, it’s more for unit test and general functionality. For real models, we have e.g. many CV examples that are run through the <code>image-classifier</code> tool. For many of these you can take the Function in the code and run e.g. <code>function-&gt;dumpDAG(""graph.dot"")</code>, and then use that <a href=""https://en.wikipedia.org/wiki/DOT_(graph_description_language)"" rel=""nofollow noopener"">dot file</a> to visualize the graph. Hope this helps.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tkg.rahul; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: September 16, 2020,  6:59pm; <NewLine> REPLY_DATE 2: September 17, 2020,  5:36am; <NewLine> REPLY_DATE 3: September 17, 2020, 10:44pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
95412,Ninjia test error,2020-09-07T09:21:30.164Z,2,54,"<div class=""post"" itemprop=""articleBody""><NewLine><p>/tests/CPUOperatorTest<br/><NewLine>[ FAILED ] 1 test, listed below:<br/><NewLine>[ FAILED ] CPU/OperatorStatelessTest.MFCC_1x17_Float/0, where GetParam() = (“CPU”)</p><NewLine><p>./tests/CPUMLTest<br/><NewLine>[ SKIPPED ] CPU/MLTest.learnSparseLengthsSumEmbeddings/0</p><NewLine><p>LLVMIRCodeGen/libjit/libjit.cpp:902: void (anonymous namespace)::find_min_max_f(float *, dim_t, float &amp;, float &amp;): Assertion `!std::isnan(tensor[i]) &amp;&amp; “NaN value found!”’ failed.</p><NewLine><p>Partition failed: the number of given(Interpreter) devices(1) is fewer than the required minimal partitions(2).</p><NewLine><p>Please help me, I am new in glow compiler, I do not know how to start learning this compiler, so I run some test and get 3 errors, please give me some advice for learning this compiler</p><NewLine></div>",https://discuss.pytorch.org/u/hisbug,,hisbug,"September 7, 2020,  9:21am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/hisbug"">@hisbug</a>, for the error on CPUMLTest, I think that is just a buggy test. Do you see the error every time you run it, or is it inconsistent?</p><NewLine><p>For the error on the Operator test, I’m not sure why you are seeing this error. What cmd are you running exactly?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jfix"">@jfix</a> Thanks for replying, CPUMLTest has two outputs, it is inconsistent,<br/><NewLine>my cmd is:</p><NewLine><pre><code class=""lang-auto"">/home/hye/tmp/build_Debug/tests/CPUMLTest --gtest_output=xml:CPUMLTest.xml<NewLine></code></pre><NewLine><p>most of the time, the output like this:</p><NewLine><pre><code class=""lang-auto"">[----------] Global test environment tear-down<NewLine>[==========] 20 tests from 1 test suite ran. (17844 ms total)<NewLine>[  PASSED  ] 19 tests.<NewLine>[  SKIPPED ] 1 test, listed below:<NewLine>[  SKIPPED ] CPU/MLTest.learnSparseLengthsSumEmbeddings/0<NewLine></code></pre><NewLine><p>sometimes the output like this:</p><NewLine><pre><code class=""lang-auto"">CPUMLTest: /home/hye/tmp/glow/lib/Backends/CPU/../../LLVMIRCodeGen/libjit/libjit.cpp:902: void (anonymous namespace)::find_min_max_f(float *, dim_t, float &amp;, float &amp;): Assertion `!std::isnan(tensor[i]) &amp;&amp; ""NaN value found!""' failed.      <NewLine>#0 0x000000000300d21e llvm::sys::PrintStackTrace(llvm::raw_ostream&amp;) (/home/hye/tmp/build_Debug/tests/CPUMLTest+0x300d21e)                                                                                                                    <NewLine>#1 0x000000000300b114 llvm::sys::RunSignalHandlers() (/home/hye/tmp/build_Debug/tests/CPUMLTest+0x300b114)                                                                                                                                    <NewLine>#2 0x000000000300b295 SignalHandler(int) (/home/hye/tmp/build_Debug/tests/CPUMLTest+0x300b295)                                                                                                                                                <NewLine>#3 0x00007f5fd261eb20 __restore_rt (/lib64/libpthread.so.0+0x14b20)                                                                                                                                                                           <NewLine>#4 0x00007f5fd2124625 raise (/lib64/libc.so.6+0x3c625)                                                                                                                                                                                        <NewLine>#5 0x00007f5fd210d8d9 abort (/lib64/libc.so.6+0x258d9)                                                                                                                                                                                        <NewLine>#6 0x00007f5fd210d7a9 _nl_load_domain.cold (/lib64/libc.so.6+0x257a9)                                                                                                                                                                         <NewLine>#7 0x00007f5fd211ca66 (/lib64/libc.so.6+0x34a66)                                                                                                                                                                                              <NewLine>#8 0x00007f5fd33726b8 <NewLine></code></pre><NewLine><p>For the Operator test, my cmd :</p><NewLine><pre><code class=""lang-auto"">/home/hye/tmp/build_Debug/tests/CPUOperatorTest --gtest_output=xml:CPUOperatorTest.xml<NewLine></code></pre><NewLine><p>The third failure is unit test 40, my cmd:</p><NewLine><pre><code class=""lang-auto"">/home/hye/tmp/build_Debug/tests/InterpreterMLTest --gtest_output=xml:InterpreterMLTest.xml<NewLine></code></pre><NewLine><p>output is:</p><NewLine><pre><code class=""lang-auto"">[----------] Global test environment tear-down<NewLine>[==========] 20 tests from 1 test suite ran. (20686 ms total)<NewLine>[  PASSED  ] 19 tests.<NewLine>[  SKIPPED ] 1 test, listed below:<NewLine>[  SKIPPED ] Interpreter/MLTest.learnSparseLengthsSumEmbeddings/0<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m a bit confused on what the issue is for the CPUOperatorTest. But the MLTest issues should be fixed as of <a href=""https://github.com/pytorch/glow/commit/730c0a96ff6088aaa020c49981762f766c3a0cbf"" rel=""nofollow noopener"">this commit</a> which just landed.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hisbug; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: September 14, 2020,  6:44am; <NewLine> REPLY_DATE 2: September 14, 2020,  9:17am; <NewLine> REPLY_DATE 3: September 14, 2020,  7:37pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
92686,Getting an error while built a bundle for ONNX model,2020-08-13T12:49:51.595Z,1,64,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I am getting an error while built a bundle for the ONNX model. I have downloaded the pre-trained model from here <a href=""https://github.com/onnx/models/tree/master/vision/classification/mnist"" rel=""nofollow noopener"">https://github.com/onnx/models/tree/master/vision/classification/mnist</a><br/><NewLine>I have followed the instruction from here <a href=""https://github.com/pytorch/glow/blob/master/docs/AOT.md"" rel=""nofollow noopener"">https://github.com/pytorch/glow/blob/master/docs/AOT.md</a><br/><NewLine>My system OS is Windows 10.</p><NewLine><p>I have used below command<br/><NewLine>model-compiler -model=E:\Glow\onnx_mnist_model\mnist-8.onnx -emit-bundle=E:\Glow\converted_onnxModel -backend=CPU</p><NewLine><p>It’s through the below error<br/><NewLine>WARNING: Logging before InitGoogleLogging() is written to STDERR<br/><NewLine>F0813 17:15:02.143579  3536 Error.cpp:119] exitOnError(Error) got an unexpected ErrorValue:<br/><NewLine>Error code: MODEL_LOADER_INVALID_PROTOBUF<br/><NewLine>Error message: Failed to parse ModelProto<br/><NewLine>Error return stack:</p><NewLine></div>",https://discuss.pytorch.org/u/mr2020,(Mohammad Rafakath),mr2020,"August 13, 2020, 12:49pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you been able to load the protobuf elsewhere? That error points to some issue due to the format of the protobuf itself, i.e. it isn’t being parsed correctly into C++ before Glow can even start taking a look at it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: September 14, 2020,  6:51am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
75868,Compiling Python Code,2020-04-08T13:52:37.317Z,1,145,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m new to glow and am trying to understand exactly what it does and how it can be used.</p><NewLine><p>From reading the paper I get the impression that normal pytorch code can be compiled an optimized, but from what I see, the closest thing to this is the torch_glow library. Is this the case or have I missed something?</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/kempj,(Jeremy Kemp),kempj,"April 8, 2020,  1:52pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/kempj"">@kempj</a>, Glow has multiple front ends. That is, we can load models from ONNX or Caffe2 protobufs, or through TorchScript from PyTorch (<code>torch_glow</code>). Or you could even write your model directly in C++ with our C++ APIs if you wanted <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/>.</p><NewLine><p><code>torch_glow</code> is under active development/is less mature than our ONNX and Caffe2 protobuf loaders. If you have problems with <code>torch_glow</code>, another path from PyTorch to Glow is to export your model to ONNX and then load into Glow.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/jfix"">@jfix</a>, as per your Apr 8 comment torch_glow is under active dev. Requesting you to comment on current state of troch_glow as per current date?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s definitely still under active development. It’s gotten more mature, but not sure if it’s easily buildable still. You can see the latest build instructions from a couple months ago <a href=""https://github.com/pytorch/glow/blob/master/docs/pytorch.md"" rel=""nofollow noopener"">here</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tkg.rahul; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: April 8, 2020,  8:37pm; <NewLine> REPLY_DATE 2: September 9, 2020,  5:12pm; <NewLine> REPLY_DATE 3: September 14, 2020,  6:46am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
40560,How to perform quantization of a model in PyTorch?,2019-03-21T22:08:40.928Z,7,5691,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone!</p><NewLine><p>I have trained the model <code>MobileNetV2 + SSD Lite</code> in PyTorch from ‘<a href=""https://github.com/qfgaohao/pytorch-ssd/blob/master/vision/ssd/mobilenet_v2_ssd_lite.py"" rel=""nofollow noopener"">https://github.com/qfgaohao/pytorch-ssd/blob/master/vision/ssd/mobilenet_v2_ssd_lite.py</a>’. Now, I want use it in Raspberry Pi3.</p><NewLine><p>I converted ‘.pth’ model into Caffe2 model through <code>ONNX</code> representation and I got two files: init_net.pb and predict_net.pb for Caffe2 framework.</p><NewLine><p>As far as I know, to accelerate the model on mobile systems such as Rpi3(B/B+) I should use the <code>QNNPACK</code> lib which allows make the low-precision inference using operators with <code>int8</code> data type.</p><NewLine><p>How to perform quantization of this model?<br/><NewLine>How can I make low-precision inference using <code>QNNPACK</code>?<br/><NewLine>Maybe there are some tutorials about it?</p><NewLine><p>Thnx.</p><NewLine></div>",https://discuss.pytorch.org/u/r3krut,(Mikhail),r3krut,"March 21, 2019, 11:10pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/r3krut"">@r3krut</a>,</p><NewLine><p>This category is for Glow, which is a different PyTorch backend from Caffe2 (which <a href=""https://github.com/pytorch/QNNPACK#end-to-end-benchmarking"" rel=""nofollow noopener"">""natively integrates <code>QNNPACK</code>""</a>). Glow primarily targets neural network accelerators, though it does have a CPU backend and supports automatic profiling + quantization. If you want to use <code>QNNPACK</code>, I believe all you need to do is make sure your model (e.g. <code>predict_net</code>) is using operators such as <code>Int8Conv</code>, <code>Int8FC</code>, etc. and your Caffe2 model would use it.</p><NewLine><p>However, if your network is not quantized and/or you don’t want to install Caffe2 on your Raspberry Pi, you could use try to use Glow to profile your model, quantize it, and then save what we call an ahead-of-time compiled “bundle”, which is just a binary to copy to your Raspberry Pi3 to run (see docs <a href=""https://github.com/pytorch/glow/blob/master/docs/Quantization.md#how-to-perform-nn-conversion"" rel=""nofollow noopener"">here</a> and <a href=""https://github.com/pytorch/glow/blob/master/docs/AOT.md#creating-standalone-executable-bundles"" rel=""nofollow noopener"">here</a>). Note that it may not perform as well as <code>QNNPACK</code>; we are more focused on accelerator backends right now.</p><NewLine><p>Thanks,<br/><NewLine>Jordan</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to do a quantization of .pb in pytorch and get a quantized .pb directly? Like bazel quantization tools ? Any tutorial will be appreciated!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thnx for answer <a class=""mention"" href=""/u/jfix"">@jfix</a> , but I’m a bit confused.<br/><NewLine>At the moment my model <code>predict_net.pb</code> does not use operators such as <code>Int8Conv</code> and etc.<br/><NewLine>How do I force my model to use these operations?<br/><NewLine>Should I change operators such as <code>Conv</code> to <code>Int8Conv</code> manually?<br/><NewLine>Which file should I change? <code>predict_net.pb</code> or <code>predcit_net.pbtxt</code>?</p><NewLine><p>Example of some <code>op</code> from my <code>predict_net.pbtxt</code>:</p><NewLine><pre><code class=""lang-auto"">op {<NewLine>  input: ""0""<NewLine>  input: ""1""<NewLine>  output: ""497""<NewLine>  name: """"<NewLine>  type: ""Conv""<NewLine>  arg {<NewLine>    name: ""strides""<NewLine>    ints: 2<NewLine>    ints: 2<NewLine>  }<NewLine>  arg {<NewLine>    name: ""pads""<NewLine>    ints: 1<NewLine>    ints: 1<NewLine>    ints: 1<NewLine>    ints: 1<NewLine>  }<NewLine>  arg {<NewLine>    name: ""kernels""<NewLine>    ints: 3<NewLine>    ints: 3<NewLine>  }<NewLine>  arg {<NewLine>    name: ""group""<NewLine>    i: 1<NewLine>  }<NewLine>  arg {<NewLine>    name: ""dilations""<NewLine>    ints: 1<NewLine>    ints: 1<NewLine>  }<NewLine>}<NewLine></code></pre><NewLine><p>Do I have to make changes here or not? Do replace from <code>type: ""Conv""</code> on <code>type: ""Int8Conv""</code>?<br/><NewLine>Help me deal with this, please. Thnx.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>So it looks like your model is only in float right now. You cannot just simply replace <code>Conv</code> with <code>In8tConv</code> etc. – in order to use quantization you need to know the quantization parameters to use for each operator. In Glow we call this <code>scale</code> and <code>offset</code>; in Caffe2 it’s called <code>Y_scale</code> and <code>Y_zero_point</code>. These are usually based on actual values you expect to flow through your graph.</p><NewLine><p>If you don’t know what the scales/offsets should be (likely the case), one option would be to use Glow’s profiling and quantization to quantize automatically. Like I said in my previous comment:</p><NewLine><blockquote><NewLine><p>However, if your network is not quantized and/or you don’t want to install Caffe2 on your Raspberry Pi, you could use try to use Glow to profile your model, quantize it, and then save what we call an ahead-of-time compiled “bundle”, which is just a binary to copy to your Raspberry Pi3 to run (see docs <a href=""https://github.com/pytorch/glow/blob/master/docs/Quantization.md#how-to-perform-nn-conversion"" rel=""nofollow noopener"">here </a>and <a href=""https://github.com/pytorch/glow/blob/master/docs/AOT.md#creating-standalone-executable-bundles"" rel=""nofollow noopener"">here </a>). Note that it may not perform as well as  <code>QNNPACK</code> ; we are more focused on accelerator backends right now.</p><NewLine></blockquote><NewLine><p>Again, this is not <code>QNNPACK</code>; Glow does not use it. If you are interested in using <code>QNNPACK</code> and Caffe2 on your Raspberry Pi then you could try asking the question in a separate category.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>We always first import Caffe2 or ONNX protos, and generate them into Glow IR, and then profile/quantize the Glow IR from there. However once you’re in Glow IR, there is no current way to generate anything back out to Caffe2/ONNX/PyTorch protos, whether quantized or not.</p><NewLine><p>If this might fit your needs, you can always follow <a href=""https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html"" rel=""nofollow noopener"">this tutorial</a> to get ONNX or Caffe2 from your PyTorch model, which you can then import to Glow.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Is there an example of taking a float32 ONNX graph, quantizing it with Glow and generate Glow IR? Is there a Python interface to Glow profile/quantization?</p><NewLine><p>Thanks,<br/><NewLine>Sikandar</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can follow the instructions <a href=""https://github.com/pytorch/glow/blob/master/docs/Quantization.md#how-to-perform-nn-conversion"" rel=""nofollow noopener"">here</a> on how to gather a profile of a model and then quantize the model. You just need an ONNX proto to load into Glow – see the page on Testing <a href=""https://github.com/pytorch/glow/blob/master/docs/Testing.md#model-loader"" rel=""nofollow noopener"">here</a> which discusses how to load a model using one of our example proto model Loaders. We have some limtited support for python via PyTorch through the ONNXIFI interface – you can find info <a href=""https://github.com/pytorch/glow/blob/master/docs/Onnxifi.md"" rel=""nofollow noopener"">here</a>. Otherwise it’s relatively straightforward to run the Testing Loader examples I linked to above after you’ve built in C++ and quantize/run your model.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/jfix"">@jfix</a>, it seems glow ./bin/image-classifier, currently support image classification type model only?<br/><NewLine>Is there a way to quantize Image Generator type model? or other kind of model? Thanks.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/eric4337"">@eric4337</a>, we currently also have a NMT model driver called <code>text-translator</code>, but it’s for pre-unrolled NMT models. We also have a <code>model-runner</code> driver but it’s very simple and just for testing, for models without any non-Constant inputs.</p><NewLine><p>If you want to try other models you need to create your own driver, probably based on <code>tools/loader/ImageClassifier.cpp</code> if you’re interested in image-based models. This mostly would mean you are able to correctly load in and out the inputs/outputs based on their expected shape(s) and datatype(s). Also, depending on the model you may need to add additional operator support.</p><NewLine><p>Once you have those things done you can quantize the model.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Look what is available here: <a href=""https://github.com/opencv/openvino_training_extensions/tree/develop/pytorch_toolkit/nncf"" rel=""nofollow noopener"">https://github.com/opencv/openvino_training_extensions/tree/develop/pytorch_toolkit/nncf</a>.</p><NewLine><p>This is a Quantization Aware Training in PyTorch with ability to export the quantized model to ONNX.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>Can you export the quantized model to onnx? thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Stella; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/r3krut; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/symashayak; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/eric4337; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/AlexKoff88; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/blueskywwc; <NewLine> ,"REPLY_DATE 1: March 25, 2019,  4:24pm; <NewLine> REPLY_DATE 2: March 30, 2019,  4:29am; <NewLine> REPLY_DATE 3: April 4, 2019,  4:29pm; <NewLine> REPLY_DATE 4: April 5, 2019,  6:19am; <NewLine> REPLY_DATE 5: April 5, 2019,  6:24am; <NewLine> REPLY_DATE 6: April 26, 2019,  6:25pm; <NewLine> REPLY_DATE 7: April 28, 2019,  3:36am; <NewLine> REPLY_DATE 8: July 17, 2019,  8:19am; <NewLine> REPLY_DATE 9: July 18, 2019, 12:47am; <NewLine> REPLY_DATE 10: August 6, 2019,  4:21pm; <NewLine> REPLY_DATE 11: July 23, 2020,  2:29am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> 
89074,Resnet50 Pytorch Model Accuracy Loss,2020-07-14T01:41:31.277Z,1,91,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I took the resnet50 PyTorch model from torchvision and exported to ONNX. When I ran it using image-classifier on first 1000 images of imagenet data set, i am seeing almost 20% accuracy loss from the resnet50 caffe2 model (on same 1000 images). It makes me wonder if the options i am using for running pytorch model is not correct. I am using “-use-imagenet-normalization” “-compute-softmax” (pytorch model does not softmax in the end) and “-image-mode=0to1”. Does input to pytorch model gets normalized in the same way as caffe2 model?</p><NewLine><p>What can I check to debug this further?</p><NewLine></div>",https://discuss.pytorch.org/u/mai202,,mai202,"July 14, 2020,  1:41am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""89074"" data-username=""mai202""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/278dde/40.png"" width=""20""/> mai202:</div><NewLine><blockquote><NewLine><p>Does input to pytorch model gets normalized in the same way as caffe2 model?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Where do you have the caffe2 model from?</p><NewLine><p>As a debugging step I would recommend to pass an input with constant values to both models and compare the outputs (e.g. <code>torch.ones()</code>).<br/><NewLine>If the outputs are equal, the issue is most likely created in the preprocessing of the data.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I got caffe model from “<a href=""http://fb-glow-assets.s3.amazonaws.com/models"" rel=""nofollow noopener"">http://fb-glow-assets.s3.amazonaws.com/models</a>” which is the url in glow to download models.<br/><NewLine><a href=""https://github.com/pytorch/glow/blob/master/utils/download_datasets_and_models.py"" rel=""nofollow noopener"">https://github.com/pytorch/glow/blob/master/utils/download_datasets_and_models.py</a></p><NewLine><p>I have also tried models from onnx zoo, <a href=""https://github.com/onnx/models/tree/master/vision/classification/resnet/model"" rel=""nofollow noopener"">https://github.com/onnx/models/tree/master/vision/classification/resnet/model</a><br/><NewLine>“resnet50-v2-7.onnx” is showing same accuracy loss.</p><NewLine><p>For same input, models does show different outputs. I have not tried with torch.ones but for same image.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the update. Unfortunately, I cannot be of much help, as I don’t know how these models (in Glow and ONNX) were created.<br/><NewLine>Just by reading through the ONNX link, it seems that the preprocessing is <a href=""https://github.com/onnx/models/tree/master/vision/classification/resnet#preprocessing"">identical</a> to the <code>torchvision.models</code>.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for getting back to me.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mai202; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mai202; <NewLine> ,"REPLY_DATE 1: July 15, 2020,  9:32am; <NewLine> REPLY_DATE 2: July 17, 2020, 12:44am; <NewLine> REPLY_DATE 3: July 17, 2020, 11:29am; <NewLine> REPLY_DATE 4: July 18, 2020,  1:08am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
89451,Slow quantized inference on Cortex-A72,2020-07-16T14:10:04.986Z,1,57,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I compiled a TFLite Flatbuffer file into a bundle executable and got a major increase in inference time.<br/><NewLine>What could be the reason for such behavior?</p><NewLine><ul><NewLine><li>Device: Raspberry Pi 4 (Cortex-A72)</li><NewLine><li>Inference Time: (min, mean, max)<br/><NewLine>**  Float: 1.41ms; 1.80ms; 12.05ms;<br/><NewLine>**  Int8  : 3.14ms; 3.84ms; 18.66ms;</li><NewLine></ul><NewLine><p>How can I fix that?</p><NewLine></div>",https://discuss.pytorch.org/u/MaxS1996,(Zulu02),MaxS1996,"July 16, 2020,  2:10pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/maxs1996"">@MaxS1996</a>, I’d suggest reading this issue – it will give you a better understanding of why using quantization doesn’t necessarily mean that performance gets better: <a href=""https://github.com/pytorch/glow/issues/4505"" rel=""nofollow noopener"">https://github.com/pytorch/glow/issues/4505</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: July 20, 2020,  1:36pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
88399,How to use the AOT bundle in pytorch?,2020-07-08T13:35:29.901Z,2,111,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I was trying to use the bundle that is generated using GLOW. In the instructions it mentions that I need to link the executable to my application. How can I link that executable to pytorch file. I wish to run some tests for pytorch execution vs glow AOT bundle execution. <a href=""https://github.com/pytorch/glow/blob/master/docs/AOT.md"" rel=""nofollow noopener"">https://github.com/pytorch/glow/blob/master/docs/AOT.md</a></p><NewLine></div>",https://discuss.pytorch.org/u/tazz,(Tanmay),tazz,"July 8, 2020,  1:35pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you want to run it driven via PyTorch, you’d need to still create a C++ wrapper (see <a href=""https://github.com/pytorch/glow/blob/master/docs/AOT.md#how-to-use-the-bundle"" rel=""nofollow noopener"">here</a> for how to use the bundle that’s generated) that PyTorch has a bridge to execute on.</p><NewLine><p>But to take a step back, it might make sense to just run via <a href=""https://github.com/pytorch/glow/blob/master/docs/pytorch.md"" rel=""nofollow noopener""><code>torch_glow</code></a> and without a bundle – not sure if there’s a specific reason you wanted to use a bundle here.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jfix"">@jfix</a>…the whole idea was to run inference in pytorch and compare it to the AOT bundle that is generated using glow. I assume that that pytorch has been focusing on inference on cpu and multi-core whereas  glow is focused more on hardware accelerators. I did have a question if I had to compare execution of both on cuda gpu what would be a good way to get some results?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""88399"" data-username=""tazz""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/t/8491ac/40.png"" width=""20""/> tazz:</div><NewLine><blockquote><NewLine><p>the whole idea was to run inference in pytorch and compare it to the AOT bundle that is generated using glow.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Sure, but you don’t need to use an AOT bundle to do so. If you go through the torch_glow path, nothing is AOT, it’s done compiled just-in-time.</p><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""88399"" data-username=""tazz""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/t/8491ac/40.png"" width=""20""/> tazz:</div><NewLine><blockquote><NewLine><p>I did have a question if I had to compare execution of both on cuda gpu what would be a good way to get some results?</p><NewLine></blockquote><NewLine></aside><NewLine><p>We don’t have support for CUDA right now directly. Theoretically you could try to target CUDA via our LLVM backend, but it’s been much more targeted toward CPUs, and I don’t know of anyone who has tried this. We also have an OpenCL backend, but it’s not a big focus of ours.</p><NewLine><p>In open source we have focused more on inference accelerators backends like NNPI and Habana. There are also open-source users that contribute PRs usually targeted for LLVM-based backends, but those backends are all kept private by those contributors.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tazz; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: July 8, 2020,  7:31pm; <NewLine> REPLY_DATE 2: July 9, 2020,  1:49pm; <NewLine> REPLY_DATE 3: July 10, 2020,  4:28am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
44055,Cross-compiling for ARM and other archs,2019-04-30T22:55:13.228Z,1,786,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have been able to build my own custom model-runner based on the glow-provided model-runner and image-classifier (refer to <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/glow-with-generic-model-builder/42559"">Glow with generic model builder</a>). Using the custom builder, I am able to output a bundle. I am also able to use the bundle to run the custom network (built a custom runner based on ResNet50 example).</p><NewLine><p>My question now is: is it possible to cross-build for ARM and other architectures, or must I build on the target? In general, it looks like we should be able to build for anything that LLVM supports, but I certainly do not want to build on the target.</p><NewLine><p>I am focusing only on the cpu backend for now.</p><NewLine><p>The more precise question can be phrased as follows. I understand that the *.weights file is just a binary weights file, so we should be able to load it on any architecture. How can we make sure that the *.o bundle file is compatible with the target architecture? How can we direct the builder to compile it for a specific target?</p><NewLine><p>Any pointers would be appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/wny,,wny,"April 30, 2019, 11:11pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah, never mind! There is the <code>-target</code> option that seems to be the same as the default LLVM option.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/wny"">@wny</a>,</p><NewLine><p>Could you explain in more details how could I cross-compile Glow for aarch64 with -target option?</p><NewLine><p>I have added it in CMakeLists.txt as:</p><NewLine><pre><code class=""lang-auto"">set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -target=aarch64"")<NewLine>set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -target=aarch64"")<NewLine></code></pre><NewLine><p>But it seems to fail during command: <code>cmake -G Ninja -DCMAKE_BUILD_TYPE=Release ../glow</code> with the following error:</p><NewLine><pre><code class=""lang-auto"">-- Cannot find glog automatically. Using legacy find.<NewLine>-- Found glog (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libglog.so)<NewLine>-- Using 64b tensor dimensions.<NewLine>-- Libjit option 'fast-math' enabled.<NewLine>-- Found LLVM 8.0.0<NewLine>-- Using LLVMConfig.cmake in: /usr/lib/llvm-8/cmake<NewLine>CMake Error at /usr/share/cmake-3.10/Modules/FindPackageHandleStandardArgs.cmake:137 (message):<NewLine>  Could NOT find Threads (missing: Threads_FOUND)<NewLine>Call Stack (most recent call first):<NewLine>  /usr/share/cmake-3.10/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE)<NewLine>  /usr/share/cmake-3.10/Modules/FindThreads.cmake:205 (FIND_PACKAGE_HANDLE_STANDARD_ARGS)<NewLine>  thirdparty/folly/CMakeLists.txt:101 (find_package)<NewLine><NewLine><NewLine>-- Configuring incomplete, errors occurred!<NewLine>See also ""/home/osboxes/Desktop/glow_Release/CMakeFiles/CMakeOutput.log"".<NewLine>See also ""/home/osboxes/Desktop/glow_Release/CMakeFiles/CMakeError.log"".<NewLine></code></pre><NewLine><p>Thank you in advance!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You want to use <code>-target</code> when you run e.g. <code>image-classifier</code> or <code>model-compiler</code>, etc., which are the tools that can be used to create the binary you’re going to deploy to another machine with a different CPU architecture. <code>-target</code> should not be used as a CMake flag – you still want to build Glow itself for your local machine’s architecture.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wny; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/atomic; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: April 30, 2019, 11:22pm; <NewLine> REPLY_DATE 2: June 2, 2020, 11:01am; <NewLine> REPLY_DATE 3: June 25, 2020,  7:40pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
85954,How do I run the inference in model I have written in pytorch on glow?,2020-06-18T15:54:40.898Z,2,141,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I exported a mobilenetv2 model form pytorch that I have trained to do binary classification in the onnx format. I understand the the image classifier example is used to load the model and the image and test it.<br/><NewLine>But are there any other steps or sample code you have with better documentation that will help me understand to utilize the model better?</p><NewLine><p>I understand you have the model-compiler tool to compile the model to an executable. Also is there a way to deploy this model on GPU? What steps do I need to follow for that?</p><NewLine></div>",https://discuss.pytorch.org/u/tazz,(Tanmay),tazz,"June 18, 2020,  3:54pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, are you interested in ahead-of-time compilation (called creating a “bundle” in Glow terminology), or in JIT compilation? You can use <code>image-classifier</code> for both purposes, or <code>model-compiler</code> for ahead of time.</p><NewLine><p>There are some examples for <a href=""https://github.com/pytorch/glow/blob/master/docs/AOT.md"" rel=""nofollow noopener"">AOT here</a> and <a href=""https://github.com/pytorch/glow/blob/master/docs/Testing.md"" rel=""nofollow noopener"">JIT here</a>.</p><NewLine><p>For GPUs, the best we have right now is our OpenCL backend. Otherwise there’s not much open source GPU support currently, as we have focused Glow more on supporting specialized accelerator HW for inference such as Intel’s NNPI, Habana’s Goya, etc.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/jfix"">@jfix</a> ,</p><NewLine><p>I was able to execute the image classifier tool. Its seems that there seems to be a lot more information for AOT.  Although a basic command line example with the parameters that the image-classifier to might make the process easier. Thanks!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’d suggest taking a look at the <a href=""https://github.com/pytorch/glow/tree/master/examples/bundles/resnet50"" rel=""nofollow noopener"">Resnet50 bundle example</a> – it simply has a <code>main.cpp</code> and <code>CMake</code> that drives all of the commands you need to create an AOT bundle and run it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tazz; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: June 25, 2020,  2:00am; <NewLine> REPLY_DATE 2: June 25, 2020,  1:54pm; <NewLine> REPLY_DATE 3: June 25, 2020,  6:30pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
86579,Unable to find models or images when I run ./test/images/run.sh?,2020-06-23T15:22:23.005Z,1,91,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have setup a virtual environment . After that, I ran the python script download_datasets_and_models.py in the utils folder. Now, I am trying to run the run.sh script in the test/images folder and I get the following error</p><NewLine><pre><code class=""lang-auto"">(venv-glow) [tkavathe@ adas2-linux build 11:18 AM]$   ./tests/images/run.sh<NewLine>Model: .<NewLine>Running 1 thread(s).<NewLine>WARNING: Logging before InitGoogleLogging() is written to STDERR<NewLine>F0623 11:19:01.687749  5214 Error.cpp:119] exitOnError(Error) got an unexpected ErrorValue: <NewLine>Error code: MODEL_LOADER_INVALID_PROTOBUF<NewLine>Error message: Can't find the model or network files for squeezenet.<NewLine>Error return stack:<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:895<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:4463<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:4474<NewLine>*** Check failure stack trace: ***<NewLine>./bin/image-classifier[0x9acbbf]<NewLine>./bin/image-classifier[0x9aafe2]<NewLine>./bin/image-classifier[0x9ad298]<NewLine>/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7fa9e0c0b390]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(gsignal+0x38)[0x7fa9de400428]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(abort+0x16a)[0x7fa9de40202a]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(+0x9e49)[0x7fa9e2210e49]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(+0xb5cd)[0x7fa9e22125cd]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google10LogMessage9SendToLogEv+0x283)[0x7fa9e2214433]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google10LogMessage5FlushEv+0xbb)[0x7fa9e221215b]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google15LogMessageFatalD2Ev+0xe)[0x7fa9e2214e1e]<NewLine>./bin/image-classifier[0x2b321a1]<NewLine>./bin/image-classifier[0x8a14de]<NewLine>./bin/image-classifier[0x4ec7bc]<NewLine>./bin/image-classifier[0x4e8394]<NewLine>/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb8c80)[0x7fa9dea63c80]<NewLine>/lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba)[0x7fa9e0c016ba]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7fa9de4d241d]<NewLine>./tests/images/run.sh: line 29:  5211 Aborted                 ./bin/image-classifier $1 $indices -image-mode=neg128to127 -m=squeezenet$model -model-input-name=data$suffix<NewLine>Model: .<NewLine>Running 1 thread(s).<NewLine>WARNING: Logging before InitGoogleLogging() is written to STDERR<NewLine>F0623 11:19:01.723178  5220 Error.cpp:119] exitOnError(Error) got an unexpected ErrorValue: <NewLine>Error code: MODEL_LOADER_INVALID_PROTOBUF<NewLine>Error message: Can't find the model or network files for squeezenet.<NewLine>Error return stack:<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:895<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:4463<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:4474<NewLine>*** Check failure stack trace: ***<NewLine>./bin/image-classifier[0x9acbbf]<NewLine>./bin/image-classifier[0x9aafe2]<NewLine>./bin/image-classifier[0x9ad298]<NewLine>/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7ff157684390]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(gsignal+0x38)[0x7ff154e79428]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(abort+0x16a)[0x7ff154e7b02a]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(+0x9e49)[0x7ff158c89e49]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(+0xb5cd)[0x7ff158c8b5cd]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google10LogMessage9SendToLogEv+0x283)[0x7ff158c8d433]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google10LogMessage5FlushEv+0xbb)[0x7ff158c8b15b]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google15LogMessageFatalD2Ev+0xe)[0x7ff158c8de1e]<NewLine>./bin/image-classifier[0x2b321a1]<NewLine>./bin/image-classifier[0x8a14de]<NewLine>./bin/image-classifier[0x4ec7bc]<NewLine>./bin/image-classifier[0x4e8394]<NewLine>/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb8c80)[0x7ff1554dcc80]<NewLine>/lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba)[0x7ff15767a6ba]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7ff154f4b41d]<NewLine>./tests/images/run.sh: line 29:  5217 Aborted                 ./bin/image-classifier $1 $indices -image-mode=neg128to127 -m=squeezenet$model -model-input-name=data$suffix<NewLine>Model: squeezenet<NewLine>Running 1 thread(s).<NewLine>WARNING: Logging before InitGoogleLogging() is written to STDERR<NewLine>F0623 11:19:01.733387  5226 Error.cpp:119] exitOnError(Error) got an unexpected ErrorValue: <NewLine>Error code: MODEL_LOADER_INVALID_PROTOBUF<NewLine>Error message: Can't find the model or network files for squeezenet/model.onnx.<NewLine>Error return stack:<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:895<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:4463<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:4474<NewLine>*** Check failure stack trace: ***<NewLine>./bin/image-classifier[0x9acbbf]<NewLine>./bin/image-classifier[0x9aafe2]<NewLine>./bin/image-classifier[0x9ad298]<NewLine>/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7f2364b95390]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(gsignal+0x38)[0x7f236238a428]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(abort+0x16a)[0x7f236238c02a]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(+0x9e49)[0x7f236619ae49]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(+0xb5cd)[0x7f236619c5cd]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google10LogMessage9SendToLogEv+0x283)[0x7f236619e433]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google10LogMessage5FlushEv+0xbb)[0x7f236619c15b]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google15LogMessageFatalD2Ev+0xe)[0x7f236619ee1e]<NewLine>./bin/image-classifier[0x2b321a1]<NewLine>./bin/image-classifier[0x8a14de]<NewLine>./bin/image-classifier[0x4ec7bc]<NewLine>./bin/image-classifier[0x4e8394]<NewLine>/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb8c80)[0x7f23629edc80]<NewLine>/lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba)[0x7f2364b8b6ba]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7f236245c41d]<NewLine>./tests/images/run.sh: line 29:  5225 Aborted                 ./bin/image-classifier $1 $indices -image-mode=neg128to127 -m=squeezenet$model -model-input-name=data$suffix<NewLine>Model: squeezenet<NewLine>Running 1 thread(s).<NewLine>WARNING: Logging before InitGoogleLogging() is written to STDERR<NewLine>F0623 11:19:01.744666  5238 Error.cpp:119] exitOnError(Error) got an unexpected ErrorValue: <NewLine>Error code: MODEL_LOADER_INVALID_PROTOBUF<NewLine>Error message: Can't find the model or network files for squeezenet/model.onnx.<NewLine>Error return stack:<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:895<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:4463<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:4474<NewLine>*** Check failure stack trace: ***<NewLine>./bin/image-classifier[0x9acbbf]<NewLine>./bin/image-classifier[0x9aafe2]<NewLine>./bin/image-classifier[0x9ad298]<NewLine>/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7fb67f8d5390]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(gsignal+0x38)[0x7fb67d0ca428]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(abort+0x16a)[0x7fb67d0cc02a]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(+0x9e49)[0x7fb680edae49]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(+0xb5cd)[0x7fb680edc5cd]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google10LogMessage9SendToLogEv+0x283)[0x7fb680ede433]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google10LogMessage5FlushEv+0xbb)[0x7fb680edc15b]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google15LogMessageFatalD2Ev+0xe)[0x7fb680edee1e]<NewLine>./bin/image-classifier[0x2b321a1]<NewLine>./bin/image-classifier[0x8a14de]<NewLine>./bin/image-classifier[0x4ec7bc]<NewLine>./bin/image-classifier[0x4e8394]<NewLine>/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb8c80)[0x7fb67d72dc80]<NewLine>/lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba)[0x7fb67f8cb6ba]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7fb67d19c41d]<NewLine>./tests/images/run.sh: line 29:  5230 Aborted                 ./bin/image-classifier $1 $indices -image-mode=neg128to127 -m=squeezenet$model -model-input-name=data$suffix<NewLine>Model: squeezenet<NewLine>Running 1 thread(s).<NewLine>WARNING: Logging before InitGoogleLogging() is written to STDERR<NewLine>F0623 11:19:01.755416  5241 Error.cpp:119] exitOnError(Error) got an unexpected ErrorValue: <NewLine>Error code: MODEL_LOADER_INVALID_PROTOBUF<NewLine>Error message: Can't find the model or network files for squeezenet/model.onnx.<NewLine>Error return stack:<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:895<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:4463<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:4474<NewLine>*** Check failure stack trace: ***<NewLine>./bin/image-classifier[0x9acbbf]<NewLine>./bin/image-classifier[0x9aafe2]<NewLine>./bin/image-classifier[0x9ad298]<NewLine>/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7f2007f71390]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(gsignal+0x38)[0x7f2005766428]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(abort+0x16a)[0x7f200576802a]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(+0x9e49)[0x7f2009576e49]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(+0xb5cd)[0x7f20095785cd]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google10LogMessage9SendToLogEv+0x283)[0x7f200957a433]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google10LogMessage5FlushEv+0xbb)[0x7f200957815b]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google15LogMessageFatalD2Ev+0xe)[0x7f200957ae1e]<NewLine>./bin/image-classifier[0x2b321a1]<NewLine>./bin/image-classifier[0x8a14de]<NewLine>./bin/image-classifier[0x4ec7bc]<NewLine>./bin/image-classifier[0x4e8394]<NewLine>/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb8c80)[0x7f2005dc9c80]<NewLine>/lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba)[0x7f2007f676ba]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7f200583841d]<NewLine>./tests/images/run.sh: line 29:  5240 Aborted                 ./bin/image-classifier $1 $indices -image-mode=neg128to127 -m=squeezenet$model -model-input-name=data$suffix<NewLine>Model: squeezenet<NewLine>Running 1 thread(s).<NewLine>WARNING: Logging before InitGoogleLogging() is written to STDERR<NewLine>F0623 11:19:01.766302  5244 Error.cpp:119] exitOnError(Error) got an unexpected ErrorValue: <NewLine>Error code: MODEL_LOADER_INVALID_PROTOBUF<NewLine>Error message: Can't find the model or network files for squeezenet/model.onnx.<NewLine>Error return stack:<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:895<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:4463<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/ONNXModelLoader.cpp:4474<NewLine>*** Check failure stack trace: ***<NewLine>./bin/image-classifier[0x9acbbf]<NewLine>./bin/image-classifier[0x9aafe2]<NewLine>./bin/image-classifier[0x9ad298]<NewLine>/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7fea46016390]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(gsignal+0x38)[0x7fea4380b428]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(abort+0x16a)[0x7fea4380d02a]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(+0x9e49)[0x7fea4761be49]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(+0xb5cd)[0x7fea4761d5cd]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google10LogMessage9SendToLogEv+0x283)[0x7fea4761f433]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google10LogMessage5FlushEv+0xbb)[0x7fea4761d15b]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google15LogMessageFatalD2Ev+0xe)[0x7fea4761fe1e]<NewLine>./bin/image-classifier[0x2b321a1]<NewLine>./bin/image-classifier[0x8a14de]<NewLine>./bin/image-classifier[0x4ec7bc]<NewLine>./bin/image-classifier[0x4e8394]<NewLine>/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb8c80)[0x7fea43e6ec80]<NewLine>/lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba)[0x7fea4600c6ba]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7fea438dd41d]<NewLine>./tests/images/run.sh: line 29:  5243 Aborted                 ./bin/image-classifier $1 $indices -image-mode=neg128to127 -m=squeezenet$model -model-input-name=data$suffix<NewLine>Model: squeezenet<NewLine>Running 1 thread(s).<NewLine>WARNING: Logging before InitGoogleLogging() is written to STDERR<NewLine>F0623 11:19:01.784961  5247 Error.cpp:119] exitOnError(Error) got an unexpected ErrorValue: <NewLine>Error message: Can't find the model or network files for squeezenet/predict_net.pb<NewLine>Error return stack:<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/Caffe2ModelLoader.cpp:251<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/Caffe2ModelLoader.cpp:2076<NewLine>/local/mnt2/workspace2/Virtual_Env/venv-glow/glow/lib/Importer/Caffe2ModelLoader.cpp:2098<NewLine>*** Check failure stack trace: ***<NewLine>./bin/image-classifier[0x9acbbf]<NewLine>./bin/image-classifier[0x9aafe2]<NewLine>./bin/image-classifier[0x9ad298]<NewLine>/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7f3b5812d390]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(gsignal+0x38)[0x7f3b55922428]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(abort+0x16a)[0x7f3b5592402a]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(+0x9e49)[0x7f3b59732e49]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(+0xb5cd)[0x7f3b597345cd]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google10LogMessage9SendToLogEv+0x283)[0x7f3b59736433]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google10LogMessage5FlushEv+0xbb)[0x7f3b5973415b]<NewLine>/usr/lib/x86_64-linux-gnu/libglog.so.0(_ZN6google15LogMessageFatalD2Ev+0xe)[0x7f3b59736e1e]<NewLine>./bin/image-classifier[0x2b321a1]<NewLine>./bin/image-classifier[0x7648ae]<NewLine>./bin/image-classifier[0x4ec715]<NewLine>./bin/image-classifier[0x4e8394]<NewLine>/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb8c80)[0x7f3b55f85c80]<NewLine>/lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba)[0x7f3b581236ba]<NewLine>/lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7f3b559f441d]<NewLine>./tests/images/run.sh: line 56:  5246 Aborted                 ./bin/image-classifier tests/images/imagenet/*.png -expected-labels=${imagenetIdxValues} -image-mode=neg128to127 -m=squeezenet/predict_net.pb -m=squeezenet/init_net.pb -model-input-name=data ""$@""<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/tazz,(Tanmay),tazz,"June 23, 2020,  3:53pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you run ls in the directory you’re running <code>./test/images/run.sh</code>? Do you see a folder called <code>squeezenet/</code>? Does it have anything inside it?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/jfix"">@jfix</a> ,</p><NewLine><p>I was able to get the run.sh file running. I this case there was no option to specify the directory for the models. Also another suggestion would be to mention the directory structure during the installation procedure. As I see a lot of problems related to installation and working with things.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tazz; <NewLine> ,"REPLY_DATE 1: June 25, 2020,  2:14am; <NewLine> REPLY_DATE 2: June 25, 2020,  1:54pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
86610,Build Glow on Windows,2020-06-23T20:40:19.704Z,0,79,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I would like to build a Glow on my Windows x64 machine. For now, I’m working inside a virtual machine.</p><NewLine><p>I went to <a href=""https://github.com/pytorch/glow"" rel=""nofollow noopener"">https://github.com/pytorch/glow</a> and I followed <strong>docs/GlowWindowsBuildx86.md</strong>.<br/><NewLine>I used the <strong>prebuild</strong> version of LLVM and I did other steps successfully, but I have a problem with building the Glow.<br/><NewLine>There are missing <strong>pthread.h</strong> and failed tests <strong>HAVE_GNU_POSIX_REGEX</strong> and <strong>HAVE_POSIX_REGEX</strong> during the execution of <em>CMake</em> command and a lot of warnings and linker errors(multiple definitions of symbols, etc.) during the <em>build</em> step.<br/><NewLine>Also, only four executable files are generated, but I suppose there should be a lot.</p><NewLine><p>Can you please tell me are there some additional steps that should be done in order to compile Glow properly.</p><NewLine><p>I installed the newest Windows SDK, CMake, Python, and Visual Studio Community 2019 with C/C++ Development Kit.</p><NewLine><p>Thanks in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/milivoje96,,milivoje96,"June 23, 2020,  8:40pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’d suggesting posting an issue on GitHub and tagging mciprian13 and ayermolo. They’re some of the primary users that run Windows.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: June 25, 2020,  2:16am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
85829,Can Glow compile training of dynamically scheduled networks?,2020-06-17T18:22:56.820Z,0,82,"<div class=""post"" itemprop=""articleBody""><NewLine><p>If I understand things correctly, Glow can be used to compile both the inference and the training of <em>statically</em> scheduled networks.<br/><NewLine>Can it also be used to compile the training of <em>dynamically</em> scheduled networks?</p><NewLine></div>",https://discuss.pytorch.org/u/dbanas,(David Banas),dbanas,"June 17, 2020,  6:22pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you define what exactly you mean by static and dynamic scheduling here? Mostly likely the answer is no – currently Glow does not support dynamic shapes or control flow, so in order to support e.g. different batch sizes you need to compile different models (though you could dynamically dispatch to different compiled models for different batch sizes at runtime).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: June 25, 2020,  2:02am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
85549,Can you compare MLIR vs. GLOW?,2020-06-15T18:08:23.731Z,2,350,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello guys,</p><NewLine><p>before going to deep study of GLOW, can you please somehow elaborate on MLIR and compare it to GLOW?<br/><NewLine>What are the benefits or possible drawbacks of GLOW vs. MLIR?<br/><NewLine>Are these comparable technologies or these serve other purposes indeed?</p><NewLine></div>",https://discuss.pytorch.org/u/Peter_Peter,(Peter Peter),Peter_Peter,"June 15, 2020,  6:08pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""85549"" data-username=""Peter_Peter""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/peter_peter/40/1360_2.png"" width=""20""/> Peter_Peter:</div><NewLine><blockquote><NewLine><p>Are these comparable technologies or these serve other purposes indeed?</p><NewLine></blockquote><NewLine></aside><NewLine><p>There’s some overlap, but in general they serve different purposes. MLIR is compiler infrastructure – it’s intended to be a useful multi-level IR that can be used across different SW stacks. Meanwhile, Glow is itself a compiler framework that has its own two levels of Glow-specific IR, and also includes optimizations written specifically for Glow IR, has features such as AOT compilation for LLVM-based backends, a high-performance runtime for server-oriented deployment, etc. In theory we could for example replace Glow’s IR with MLIR (but this is not something we’re planning).</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello Jordan,</p><NewLine><p>What you mean by  <strong>LLVM-based backends</strong>?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Glow uses LLVM in two ways: 1) Across the whole compiler as a support library e.g. for many of its efficient data structures, CL options, serialization, etc. 2) Separately, backends may use LLVM for low-level codegen for specific architectures once they go below Glow Instruction IR. E.g. the Glow CPU backend uses this, and other private backends may as well (maintained privately by OSS users so you don’t see them on Github).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Peter_Peter; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: June 15, 2020,  8:05pm; <NewLine> REPLY_DATE 2: June 20, 2020,  3:23pm; <NewLine> REPLY_DATE 3: June 25, 2020,  1:55am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
39568,Quantization example resnet50,2019-03-12T02:39:48.614Z,17,2723,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I am a beginner of glow. When trying quantization example for resnet50</p><NewLine><pre><code class=""lang-auto"">./bin/image-classifier tests/images/imagenet/*.png -image-mode=0to1 -m=resnet50 -model-input-name=gpu_0/data -dump-profile=""profile.yaml""<NewLine></code></pre><NewLine><p>what is the file “resnet50” I should prepare for this example?    resnet50.pth file?<br/><NewLine>cuz it shows “ONNXModelLoader.cpp line161 error”</p><NewLine><p>Thx!</p><NewLine></div>",https://discuss.pytorch.org/u/weiwei_lee,(Weiwei Lee),weiwei_lee,"March 12, 2019,  2:39am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/weiwei_lee"">@weiwei_lee</a> – resnet50 here represents the directory containing Caffe2 or ONNX protobufs. We have a script to download some from <code>utils/download_caffe2_models.sh</code> and <code>utils/download_onnx_models.sh</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jfix"">@jfix</a>  Thx for ur answer!</p><NewLine><p>another question, how to quantize pytorch model instead?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/weiwei_lee"">@weiwei_lee</a> I’m not exactly sure what the question is – do you have your own model defined in PyTorch that you want to run with Glow, and then use quantization for it?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jfix"">@jfix</a><br/><NewLine>Yes, that’s my question!<br/><NewLine>I wanna quantize my pytorch model directly, instead of ONNX or Caffe2 model.<br/><NewLine>Does that work?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/weiwei_lee"">@weiwei_lee</a> You can do this e.g. by exporting from PyTorch into ONNX, and then load the ONNX proto representation of the model into Glow – see <a href=""https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html"" rel=""nofollow noopener"">this tutorial</a>. This of course is dependent on what we support in our ONNX importer and in Glow, so not everything you export will work right out of the box.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok, Thx <a class=""mention"" href=""/u/jfix"">@jfix</a></p><NewLine><p>I successfully get profile.yaml and use it to quantize model<br/><NewLine>however, how to save “quantized model” ?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/weiwei_lee"">@weiwei_lee</a> By save, do you mean you want to save the compiled quantized model as a standalone executable for a CPU? We have directions on that <a href=""https://github.com/pytorch/glow/blob/master/docs/AOT.md#creating-standalone-executable-bundles"" rel=""nofollow noopener"">here</a>. If not, please clarify what you’re looking for.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jfix"">@jfix</a></p><NewLine><p>Yes, that is what I want.<br/><NewLine>Is there any document explain more detail about quantization flow and rule in inference stage?<br/><NewLine>Because I am curious that in glow,</p><NewLine><ol><NewLine><li>it load model and yaml to transform weight of model into int8?</li><NewLine><li>Input to each layer is float or int8?</li><NewLine><li>is the multiplication rule like tflite using gemmlowp to do int8*int8?</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/weiwei_lee"">@weiwei_lee</a></p><NewLine><p>Yes, please take a look at all of our docs – I think they mostly seem that they will answer your questions. <a href=""https://github.com/pytorch/glow/blob/master/docs/Quantization.md#quantization-in-glow"" rel=""nofollow noopener"">Here is the doc on quantization</a>. You gather a profile on the floating point graph with whatever inputs you want, it dumps the yaml file, then you load it back in and it will quantize the graph given the profile you gathered. Assuming the CPU backend supports the node in quantized form, all inputs will be int8. Otherwise conversions to/from int8 will be performed. You can look at the graph that is generated to see what is and is not quantized, via the command line option <code>-dump-graph-DAG=""graph.dot""</code>.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jfix"">@jfix</a></p><NewLine><p>Thanks for your clearly explanation!</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to quantize another pytorch model with the approach described in this post. I have glow build and model converted to .onnx , what should I put as the -model-input-name? when i do<br/><NewLine>./bin/image-classifier tests/images/imagenet/*.png -image-mode=0to1 -m=‘path/to/my/model’ -model-input-name=gpu_0/data -dump-profile=“profile.yaml”<br/><NewLine>I got error file: ~/glow/lib/Importer/ProtobufLoader.cpp line: 33 message: There is no tensor registered with name 0.<br/><NewLine>I assume it’s the input name was incorrect?</p><NewLine><p>Thanks!</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, it’s probably due to <code>-model-input-name</code>. You’ll need to take a look at the proto itself to find out the name of the external input that is the image itself, which is the input to the first operator of the model. In most of our image classification models, this is <code>data</code> or <code>gpu_0/data</code>. You can see some examples in our image classifier run script <a href=""https://github.com/pytorch/glow/blob/master/tests/images/run.sh"" rel=""nofollow noopener"">here</a>.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jfix"">@jfix</a> Thank you for the quick respond! How do I get the input name of the model?  I have tried print(model) and for name, param in model.named_parameters(): print (name, param.data.shape) , both start from the actual layers, not including the input.  sorry it’s a basic question.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could use something like <a href=""https://electronjs.org/apps/netron"" rel=""nofollow noopener"">Netron</a> to view your protobuf, and view what the very first operator’s input is (see the image below, for the very start of a Caffe2 Resnet50 model – you’d use <code>gpu_0/data</code>). Otherwise you should be able to just inspect the protobuf text of the model to see what the input name of the first operator is – it should be external input.</p><NewLine><p><img alt=""08%20PM"" height=""217"" src=""https://discuss.pytorch.org/uploads/default/original/2X/5/55cd761bbb47b05d963597581fee30830047d639.png"" width=""241""/></p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your help <a class=""mention"" href=""/u/jfix"">@jfix</a>! That works wonders!<br/><NewLine>However got an error eventually glow/lib/Importer/ONNXModelLoader.cpp line: 896 error code: MODEL_LOADER_UNSUPPORTED_OPERATOR message: Failed to load operator. I guess it means some of the operations in my model are supported for quantization at this point?</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah, some op is unsupported – what options were you using for the <code>image-classifier</code>? Was this when trying to get the profile (<code>-dump-profile</code>), or load the profile for quantization (<code>-load-profile</code>)? Have you tried just running it in fp32 (i.e. without the mentioned options)?</p><NewLine><p>For <code>MODEL_LOADER_UNSUPPORTED_OPERATOR</code>, the operator is unsupported in fp32 too, and we’d need to add support to the importer for it. We need to improve our error messages here – in the meantime could you just add a simple print statement just before the <code>RETURN_ERR()</code> in <code>ONNXModelLoader::LoadOperator()</code> to print the <code>typeName</code> to see what it is reporting as unsupported.</p><NewLine><p>Also, feel free to open an issue for supporting whatever op it is on Github!</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/jfix"">@jfix</a>  for clarifying it! I got the error when doing ./bin/image-classifier tests/images/imagenet/*.png -image-mode=0to1 -m=‘path/to/my/model’ -model-input-name=gpu_0/data -dump-profile=“profile.yaml” .<br/><NewLine>The model I am trying to quantize with is mobilefacenet, made with grouped convolutions and dense blocks.</p><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p>Got it, cool. Well like I said, if you find out what op is being reported as unsupported please open an issue with it on GH.</p><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Look what is available here: <a href=""https://github.com/opencv/openvino_training_extensions/tree/develop/pytorch_toolkit/nncf"" rel=""nofollow noopener"">https://github.com/opencv/openvino_training_extensions/tree/develop/pytorch_toolkit/nncf</a>.</p><NewLine><p>This is a Quantization Aware Training in PyTorch with ability to export the quantized model to ONNX.</p><NewLine><p>There are many results there including ResNet-50 ready to use config for quantization.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/weiwei_lee; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/weiwei_lee; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/weiwei_lee; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/weiwei_lee; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/weiwei_lee; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/Stella; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/Stella; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/Stella; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/Stella; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/AlexKoff88; <NewLine> ,"REPLY_DATE 1: March 12, 2019,  6:05am; <NewLine> REPLY_DATE 2: March 12, 2019,  6:24am; <NewLine> REPLY_DATE 3: March 12, 2019,  6:38am; <NewLine> REPLY_DATE 4: March 12, 2019,  6:50am; <NewLine> REPLY_DATE 5: March 12, 2019,  3:13pm; <NewLine> REPLY_DATE 6: March 13, 2019,  2:16am; <NewLine> REPLY_DATE 7: March 13, 2019,  5:20am; <NewLine> REPLY_DATE 8: March 13, 2019,  8:21am; <NewLine> REPLY_DATE 9: March 13, 2019,  8:48pm; <NewLine> REPLY_DATE 10: March 14, 2019,  1:11am; <NewLine> REPLY_DATE 11: April 4, 2019,  3:58am; <NewLine> REPLY_DATE 12: April 5, 2019,  6:29am; <NewLine> REPLY_DATE 13: April 7, 2019,  3:01am; <NewLine> REPLY_DATE 14: April 7, 2019,  5:20am; <NewLine> REPLY_DATE 15: April 8, 2019,  7:54pm; <NewLine> REPLY_DATE 16: April 9, 2019, 10:43pm; <NewLine> REPLY_DATE 17: April 11, 2019,  5:15am; <NewLine> REPLY_DATE 18: April 11, 2019,  4:55pm; <NewLine> REPLY_DATE 19: August 6, 2019,  4:22pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> REPLY 18 LIKES: ; <NewLine> REPLY 19 LIKES: ; <NewLine> 
79965,Initialize weights as a constant array?,2020-05-06T10:03:48.250Z,0,109,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m interested in using the Glow stand-alone bundles for microcontroller applications but I ran into the following memory issue. When declaring the constant weight array</p><NewLine><pre><code class=""lang-auto"">uint8_t constantWeight[CONSTANT_MEM_SIZE] = {<NewLine>#include ""MODEL_NAME.weights.txt""<NewLine>};<NewLine></code></pre><NewLine><p>the array is not defined as constant. You cannot simply add a <code>const</code> keyword because the model entry function expects a non-constant format. This is a problem because non-constant variables are stored in the (too small) RAM segment instead of the flash. Is there a simple way to work around this problem?</p><NewLine></div>",https://discuss.pytorch.org/u/byersin,,byersin,"May 6, 2020, 10:04am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Check out  <a href=""http://bit.ly/deep-C"" rel=""nofollow noopener"">bit.ly/deep-C </a> for an easy approach to put pytorch &amp; onnx models into microcontroller applications</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not sure I understand the issue. Why do you need to mark it as <code>const</code>? Your microcontroller application requires the weights to be <code>const</code>? Can you not just create a constant reference to use in APIs other than the model entry function?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/srohit0; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: May 14, 2020,  4:14pm; <NewLine> REPLY_DATE 2: May 28, 2020,  9:48pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
80480,Load an onnx model with ConvTranspose2d group &gt; 1,2020-05-09T17:55:50.470Z,6,234,"<div class=""post"" itemprop=""articleBody""><NewLine><p>A pretrained model has the following layer</p><NewLine><pre><code class=""lang-auto"">nn.ConvTranspose2d(64, 64, kernel_size=2, <NewLine>      stride=2, padding=0, output_padding=0, groups=64, bias=False)<NewLine></code></pre><NewLine><p>the model was converted to onnx.</p><NewLine><p>During compilation using command:</p><NewLine><pre><code class=""lang-auto"">./build/bin/model-compiler -g -model model.onnx -emit-bundle ./bundle -backend=CPU<NewLine></code></pre><NewLine><p>an assertion fails:</p><NewLine><pre><code class=""lang-auto"">  assert(filterDims.n % group == 0 &amp;&amp; filterDims.h == kdim.height &amp;&amp;<NewLine>         filterDims.w == kdim.width &amp;&amp; filterDims.c == idim.c / group &amp;&amp;<NewLine>         ""Invalid filter dims"");<NewLine></code></pre><NewLine><p>for my case:</p><NewLine><pre><code class=""lang-auto"">idim = (1, 64, 15, 20)<NewLine>filterDims = (64, 1, 2, 2)<NewLine>group = 64<NewLine></code></pre><NewLine><p>so <code>filterDims.n % group != 0</code> and <code>filterDims.c != idim.c / group</code>.</p><NewLine><p>What wrong with the layer parameters and what the assertion checks?</p><NewLine></div>",https://discuss.pytorch.org/u/sheh,(Alexander Shekhovtsov),sheh,"May 10, 2020,  3:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What does <code>model-compiler</code> do and when is it used?<br/><NewLine>Do you know, how <code>kdim</code> is defined and what it stands for?<br/><NewLine>It seems that it’s another placeholder for the kernel and tries to compare the shapes with the <code>weight</code> tensor in the transposed convolution?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m testing glow compiler, <code>model-compiler</code> creates model <a href=""https://github.com/pytorch/glow/blob/1e7d13a591abcdc4035363e11717febd38b0def3/docs/AOT.md#compile-a-bundle-for-a-floating-point-model"" rel=""nofollow noopener"">bundle</a> and can profile model.<br/><NewLine><code>kdim</code> means “kernel dim”.</p><NewLine><p>Sorry, I should have posted the whole <a href=""https://github.com/pytorch/glow/blob/07caebd08dae4017a11c0460e769bc0aec31651f/lib/Graph/Graph.cpp#L691"" rel=""nofollow noopener"">method code</a></p><NewLine><pre><code class=""lang-auto"">static void assertConvTransposeDims(NodeValue input, NodeValue filter,<NewLine>                                    NodeValue bias,<NewLine>                                    llvm::ArrayRef&lt;unsigned_t&gt; kernels,<NewLine>                                    llvm::ArrayRef&lt;unsigned_t&gt; strides,<NewLine>                                    llvm::ArrayRef&lt;unsigned_t&gt; pads,<NewLine>                                    unsigned_t group) {<NewLine>  ShapeNHWC idim = ShapeNHWC(input.dims());<NewLine>  (void)idim;<NewLine>  ShapeHW kdim(kernels);<NewLine>  (void)kdim;<NewLine>  assert(idim.c % group == 0 &amp;&amp; ""channels number must be divisible by groups"");<NewLine><NewLine>  // NOTE: here the N in NHWC is abnormal because it is the number of filters<NewLine>  // (and therefore the number of output channels of the conv) and not the<NewLine>  // batch size. The rest of the dimensions are representative of the input<NewLine>  // dimensions to the convolution.<NewLine>  ShapeNHWC filterDims(filter.dims());<NewLine>  (void)filterDims;<NewLine><NewLine>  assert(filterDims.n % group == 0 &amp;&amp; filterDims.h == kdim.height &amp;&amp;<NewLine>         filterDims.w == kdim.width &amp;&amp; filterDims.c == idim.c / group &amp;&amp;<NewLine>         ""Invalid filter dims"");<NewLine><NewLine>  assert(bias.getType()-&gt;size() == filterDims.n &amp;&amp; ""Invalid bias size"");<NewLine>}<NewLine></code></pre><NewLine><p>It looks like ConvTranspose2d must have <code>out_channels=in_channels*group</code> to pass the assertion.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>in_channels</code> and <code>out_channels</code> should be both divisible by <code>groups</code>.<br/><NewLine>If your code is working fine in PyTorch, I’m a bit confused if <code>glow</code> adds some other checks to the transposed convolutions.</p><NewLine><p>Did you narrow it down to a <code>nn.ConvTranpose2d</code> layer, which creates the issue in the export?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I’m a bit confused if  <code>glow</code>  adds some other checks to the transposed convolutions.</p><NewLine></blockquote><NewLine><p>Moreover the <a href=""https://github.com/pytorch/glow/blob/07caebd08dae4017a11c0460e769bc0aec31651f/lib/Graph/Graph.cpp#L736"" rel=""nofollow noopener"">same check</a> works for Conv2d layers.</p><NewLine><p>I managed to convert the model with <code>group=1</code>. Obviously, I have to retrain the model with new layer.</p><NewLine><p>Anyway I wonder what’s wrong with original case in my initial post.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""80480"" data-username=""sheh""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/ba9def/40.png"" width=""20""/> sheh:</div><NewLine><blockquote><NewLine><p>Moreover the <a href=""https://github.com/pytorch/glow/blob/07caebd08dae4017a11c0460e769bc0aec31651f/lib/Graph/Graph.cpp#L736"">same check </a> works for Conv2d layers.</p><NewLine></blockquote><NewLine></aside><NewLine><p>That’s what I’m wondering. If your model works fine in your PyTorch code, I would assume that it should also pass all Glow checks.<br/><NewLine>Are you seeing this error for all settings other than <code>groups=1</code>?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve tested with <code>group=2</code> and <code>group=32</code> – it fails on the same assertion <code>assertConvTransposeDims</code>.<br/><NewLine>If I comment the assertion it fails somewhere later.</p><NewLine><p>Should I create a github issue?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, please create an issue, as I’m not familiar enough with Glow’s internals.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Github issue<br/><NewLine></p><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/glow/issues/4516"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/glow</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/glow/issues/4516"" rel=""nofollow noopener"" target=""_blank"">Load an onnx model with ConvTranspose2d group &gt; 1</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-05-14"" data-format=""ll"" data-time=""14:47:55"" data-timezone=""UTC"">02:47PM - 14 May 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/sheh"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""sheh"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars3.githubusercontent.com/u/1386851?v=4"" width=""20""/><NewLine>          sheh<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">Original discussion<NewLine>A pretrained model has the following layer<NewLine>nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2, <NewLine> padding=0, output_padding=0, groups=64, bias=False)<NewLine>the model was converted to onnx.<NewLine>During...</p><NewLine></div><NewLine><div class=""labels""><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/sheh; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/sheh; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/sheh; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/sheh; <NewLine> ,"REPLY_DATE 1: May 10, 2020,  7:03am; <NewLine> REPLY_DATE 2: May 10, 2020,  9:34am; <NewLine> REPLY_DATE 3: May 11, 2020, 12:11am; <NewLine> REPLY_DATE 4: May 11, 2020,  9:43am; <NewLine> REPLY_DATE 5: May 11, 2020,  6:02pm; <NewLine> REPLY_DATE 6: May 12, 2020,  3:48pm; <NewLine> REPLY_DATE 7: May 12, 2020, 11:04pm; <NewLine> REPLY_DATE 8: May 18, 2020, 10:45am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
76670,CPUOperatorTest failed. How can I fix this?,2020-04-14T02:05:44.051Z,0,80,"<div class=""post"" itemprop=""articleBody""><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/9945427446d601c56f75670d3a24a1b69fc9ea1e"" href=""https://discuss.pytorch.org/uploads/default/original/3X/9/9/9945427446d601c56f75670d3a24a1b69fc9ea1e.png"" title=""捕""><img alt=""捕"" data-base62-sha1=""lRTrl2RtwKgb087An1nDwcQvmPA"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/9/9945427446d601c56f75670d3a24a1b69fc9ea1e_2_10x10.png"" height=""409"" src=""https://discuss.pytorch.org/uploads/default/original/3X/9/9/9945427446d601c56f75670d3a24a1b69fc9ea1e.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">捕</span><span class=""informations"">703×417 49.7 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/Will_Liu,,Will_Liu,"April 14, 2020,  2:07am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/will_liu"">@Will_Liu</a> Thanks for reporting this. Can you run the <code>CPUOperatorTest</code> by itself to determine which test specifically is failing?</p><NewLine><p>Also, please provide the environment you’re running in – this test suite covered under CI so it must be a portability issue.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: April 14, 2020,  2:20am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
75054,Glow: Compiling a model with varying size input,2020-04-01T22:44:21.904Z,0,124,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Does the glow’s bundle compiler support input with varying size? Like varying image size input?</p><NewLine></div>",https://discuss.pytorch.org/u/antimora,(DT),antimora,"April 1, 2020, 10:44pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think this depends on the model. E.g. I’ve seen C2 models where the shapes of the network are inferred based on the input size, so you have to specify that at least when compiling. But other models may have specific shape expectations in the model and so aren’t flexible, you’d hit an error if you try different image sizes that are not compatible.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: April 4, 2020, 12:58am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
73317,Translating PyTorch into Glow IR,2020-03-15T22:02:29.275Z,1,183,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Does glow provide a way to translate PyTorch model directly into Glow IR without first converting into ONNX ? I am trying to find out if ONNX op support a dependency ?</p><NewLine><p>Cheers,<br/><NewLine>Yetanadur</p><NewLine></div>",https://discuss.pytorch.org/u/yetanadur,(Ananth Durbha),yetanadur,"March 15, 2020, 10:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/yetanadur"">@yetanadur</a>,</p><NewLine><p>We have a path that goes through TorchScript, but it is still under active development and so does not have as much support or stability yet. Please see this doc for more info: <a href=""https://github.com/pytorch/glow/blob/master/docs/pytorch.md"" rel=""nofollow noopener"">https://github.com/pytorch/glow/blob/master/docs/pytorch.md</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the pointer Jordan!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/yetanadur; <NewLine> ,"REPLY_DATE 1: March 16, 2020,  4:06pm; <NewLine> REPLY_DATE 2: March 19, 2020,  3:53am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
70189,Tutorial for Hardware Designer,2020-02-18T14:40:09.784Z,2,269,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi and I’m new to the Glow compiler. In fact, I’m designing an accelerator and I hope to write a Glow backend for it. Is there any good reference or tutorial for me?</p><NewLine><p>Thanks a lot!</p><NewLine></div>",https://discuss.pytorch.org/u/Xusine1131,(Shanqing Lin),Xusine1131,"February 18, 2020,  2:40pm",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am also interested in this topic.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/xusine1131"">@Xusine1131</a> <a class=""mention"" href=""/u/leejaymin"">@leejaymin</a> – thanks for your interest! We had a really simple example PR put up ~6 weeks ago to help people get started here. It didn’t land but you can take a look there, as well as a little bit of the discussion there. Hopefully this is a good place to get started, and if you have suggestions or document some of the things that opti-mix mentioned in the comments.</p><NewLine><aside class=""onebox githubpullrequest""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/glow/pull/3991"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/glow</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Pull Request""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 12 16"" width=""60""><path d=""M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/glow/pull/3991"" rel=""nofollow noopener"" target=""_blank"">Simple example backend that performs individual SLS ops</a><NewLine></h4><NewLine><div class=""branches""><NewLine><code>pytorch:master</code> ← <code>bertmaher:example_sls</code><NewLine></div><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-01-08"" data-format=""ll"" data-time=""23:36:36"" data-timezone=""UTC"">11:36PM - 08 Jan 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/bertmaher"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""bertmaher"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars1.githubusercontent.com/u/4441820?v=4"" width=""20""/><NewLine>          bertmaher<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""1 commits changed 4 files with 538 additions and 0 deletions""><NewLine><a href=""https://github.com/pytorch/glow/pull/3991/files"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+538</span><NewLine><span class=""removed"">-0</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jfix"">@jfix</a> Thanks for your example! That’s a good reference, and it’s quite neat.<br/><NewLine>By the way, is there any code for self-defined the Glow Runtime?  I think a runtime might be necessary as well. Thanks!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think we have any examples for building out a bigger runtime other than what you see in the Example backend for implementing an <code>ExampleFunction</code>. In general the runtime tends to be more important for JIT compilation/server uses cases, whereas many open source users care more about ahead of time compilation of a binary. CC: <a class=""mention"" href=""/u/gcatron"">@gcatron</a></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks! I think I have understood the concept of runtime.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/leejaymin; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Xusine1131; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Xusine1131; <NewLine> ,"REPLY_DATE 1: February 18, 2020,  2:58pm; <NewLine> REPLY_DATE 2: February 20, 2020,  8:00pm; <NewLine> REPLY_DATE 3: February 21, 2020,  1:41am; <NewLine> REPLY_DATE 4: February 21, 2020,  4:27am; <NewLine> REPLY_DATE 5: February 21, 2020,  5:21am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
68531,Error when compiling glow on Ubuntu 18.04,2020-02-03T17:03:45.460Z,2,186,"<div class=""post"" itemprop=""articleBody""><NewLine><p>It seems llvm headers are not in the include path even though I provide the -DLLVM_DIR path to LLVM config i.e /lib/cmake/llvm</p><NewLine><p>Are there other options I need to provide to enable llvm include headers are correctly sourced ?</p><NewLine><p>When I run ninja all, I get errors like</p><NewLine><pre><code class=""lang-auto"">glow/include/glow/Graph/PlaceholderBindings.h:44:36: error: no template named 'StringMap' in namespace 'llvm'<NewLine>  using PlaceholderNameMap = llvm::StringMap&lt;Placeholder *&gt;;<NewLine></code></pre><NewLine><p>I had ran cmake as follows<br/><NewLine>cmake -G Ninja -DCMAKE_BUILD_TYPE=Debug …/glow -DLLVM_DIR=&lt;path/to/llvm&gt;/local/lib/cmake/llvm</p><NewLine></div>",https://discuss.pytorch.org/u/yetanadur,(Ananth Durbha),yetanadur,"February 3, 2020,  5:03pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I interpret the error differently: it looks like your version of LLVM is not the version needed by Glow (as it is complaining StringMap is missing, not that the header is missing.)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sure. Basically, I had verified that the necessary header llvm/ADT/StringMap.h is present in my llvm path and the file does have StringMap symbol defined inside llvm namespace. Hence, I feel the issue is perhaps, the header file path is not correctly setup. When I look at the CC commands, I see <code>-isystem &lt;llvm/include/path&gt;</code> - is this sufficient ? I was expecting it should have been - <code>-I&lt;llvm/include/path&gt;</code> ?</p><NewLine><p>(btw, I am using llvm 9.0)</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like glow/include/glow/Quantization/Base/Base.h is using llvm::StringMap without including header file for it.</p><NewLine><p>When I explicitly added ""<span class=""hashtag"">#include</span> “llvm/ADT/StringMap.h” to this file, I was able to compile. But there are few other such files as well. So I am not sure the right place to add this include to address all these files depending on it.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for catching this <a class=""mention"" href=""/u/yetanadur"">@yetanadur</a> – I’d suggest just adding it to the file if it uses <code>StringMap</code>. Better to have dependences listed explicitly anyway. Feel free to put up a PR with a fix <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ezyang; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/yetanadur; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/yetanadur; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: February 3, 2020,  9:54pm; <NewLine> REPLY_DATE 2: February 3, 2020, 10:58pm; <NewLine> REPLY_DATE 3: February 3, 2020, 11:21pm; <NewLine> REPLY_DATE 4: February 21, 2020,  4:23am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
67343,Video inference using glow,2020-01-22T07:16:52.965Z,0,149,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Guys,<br/><NewLine>I am new to this world of glow,<br/><NewLine>I can use ffmpeg or gstreamer to do video decode and infernence.<br/><NewLine>I<br/><NewLine>want to use glow for the same. Could you help me, if there is some sample code/docs,  to explain, How video decode and inference could be achieved using glow.</p><NewLine><p>Thanks<br/><NewLine>Udit</p><NewLine></div>",https://discuss.pytorch.org/u/Udit_Kumar,(Udit Kumar),Udit_Kumar,"January 22, 2020,  7:16am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/udit_kumar"">@Udit_Kumar</a>,</p><NewLine><p>Thanks for your interest! Do you have an initial starting point, e.g. some ONNX model that you want to run? And do you have a specific architecture you’d like to run on?</p><NewLine><p>We have some documentation on how to run models such as image classification <a href=""https://github.com/pytorch/glow/blob/master/docs/Testing.md#image-classification"" rel=""nofollow noopener"">here</a>. However this is not going to satisfy your desire for video specifically. We do intend to improve our video based operators and coverage over the next half year or so.</p><NewLine><p>Thanks,<br/><NewLine>Jordan</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: January 27, 2020,  9:27pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
63925,Error on MAC OS: Unable to load the JIT library,2019-12-13T04:22:15.217Z,0,130,"<div class=""post"" itemprop=""articleBody""><NewLine><p>“unable to load the jit library” error occurs in the recent Glow version (12/13/2019).</p><NewLine><p>In version (8/11 2019), there is no runtime error under both backends: interpreter and CPU.<br/><NewLine>However, the recent Glow version is not working with “CPU backend”. In case of interpreter, it is still working.</p><NewLine><p>I used llvm@7 for compiling Glow.</p><NewLine><p>The detailed error and program arguments are below.</p><NewLine><pre><code class=""lang-bash"">cat_285.png<NewLine>-m=conv_batchnorm.onnx<NewLine>-image-mode=0to1<NewLine>-model-input-name=input.1<NewLine>-time<NewLine>-use-imagenet-normalization<NewLine>-backend=CPU<NewLine></code></pre><NewLine><pre><code class=""lang-bash"">WARNING: Logging before InitGoogleLogging() is written to STDERR<NewLine>F1213 12:51:48.990640 176840704 LLVMIRGen.cpp:194] Check failed: llmodule_.get() Unable to load the JIT library.<NewLine>*** Check failure stack trace: ***<NewLine>0  image-classifier         0x00000001042b1c55 llvm::sys::PrintStackTrace(llvm::raw_ostream&amp;) + 40<NewLine>1  image-classifier         0x00000001042b2088 SignalHandler(int) + 180<NewLine>2  libsystem_platform.dylib 0x00007fff64782b5d _sigtramp + 29<NewLine>3  libsystem_platform.dylib 000000000000000000 _sigtramp + 2609370304<NewLine>4  libsystem_c.dylib        0x00007fff6463c6a6 abort + 127<NewLine>5  libglog.0.dylib          0x00000001088ef991 google::InstallFailureFunction(void (*)()) + 0<NewLine>6  libglog.0.dylib          0x00000001088eeae7 google::LogMessage::SendToLog() + 723<NewLine>7  libglog.0.dylib          0x00000001088ef349 google::LogMessage::Flush() + 175<NewLine>8  libglog.0.dylib          0x00000001088f29ef google::LogMessageFatal::~LogMessageFatal() + 15<NewLine>9  libglog.0.dylib          0x00000001088efc77 google::LogMessageFatal::~LogMessageFatal() + 9<NewLine>10 image-classifier         0x0000000104410096 glow::LLVMIRGen::initCodeGen() + 422<NewLine>11 image-classifier         0x000000010443bebf glow::LLVMBackend::compileIRWithoutConstants(glow::IRFunction*) const + 271<NewLine>12 image-classifier         0x000000010443c639 glow::LLVMBackend::compile(glow::Function*, glow::BackendOptions const&amp;) const + 393<NewLine>13 image-classifier         0x00000001043467e0 glow::runtime::Provisioner::provision(std::__1::vector&lt;glow::runtime::DAG, std::__1::allocator&lt;glow::runtime::DAG&gt; &gt;&amp;, glow::Module&amp;, glow::CompilationContext&amp;) + 4096<NewLine>14 image-classifier         0x0000000103f3f207 glow::runtime::HostManager::addNetwork(std::__1::unique_ptr&lt;glow::Module, std::__1::default_delete&lt;glow::Module&gt; &gt;, glow::CompilationContext&amp;, bool) + 4007<NewLine>15 image-classifier         0x0000000103e9ad37 glow::Loader::compile(glow::CompilationContext&amp;) + 2055<NewLine>16 image-classifier         0x0000000103edc4e8 buildAndCompileAndGetInAndOutPair(glow::Loader&amp;, glow::PlaceholderBindings&amp;, glow::Type const*) + 456<NewLine>17 image-classifier         0x0000000103edb45e main::$_2::operator()(unsigned long, unsigned long) const + 1342<NewLine>18 image-classifier         0x0000000103edaf13 main::$_0::operator()() const + 35<NewLine>19 image-classifier         0x0000000103edaead decltype(std::__1::forward&lt;main::$_0&gt;(fp)()) std::__1::__invoke&lt;main::$_0&gt;(main::$_0&amp;&amp;) + 29<NewLine>20 image-classifier         0x0000000103edae15 void std::__1::__thread_execute&lt;std::__1::unique_ptr&lt;std::__1::__thread_struct, std::__1::default_delete&lt;std::__1::__thread_struct&gt; &gt;, main::$_0&gt;(std::__1::tuple&lt;std::__1::unique_ptr&lt;std::__1::__thread_struct, std::__1::default_delete&lt;std::__1::__thread_struct&gt; &gt;, main::$_0&gt;&amp;, std::__1::__tuple_indices&lt;&gt;) + 37<NewLine>21 image-classifier         0x0000000103eda6e3 void* std::__1::__thread_proxy&lt;std::__1::tuple&lt;std::__1::unique_ptr&lt;std::__1::__thread_struct, std::__1::default_delete&lt;std::__1::__thread_struct&gt; &gt;, main::$_0&gt; &gt;(void*) + 99<NewLine>22 libsystem_pthread.dylib  0x00007fff6478b2eb _pthread_body + 126<NewLine>23 libsystem_pthread.dylib  0x00007fff6478e249 _pthread_start + 66<NewLine>24 libsystem_pthread.dylib  0x00007fff6478a40d thread_start + 13<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/leejaymin,(jemin),leejaymin,"December 13, 2019,  4:22am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>For completeness, this is being <a href=""https://github.com/pytorch/glow/issues/3874"" rel=""nofollow noopener"">tracked here</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: December 30, 2019,  3:56pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
64922,Extending Select to support Int32,2019-12-24T21:00:55.841Z,1,129,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How can one go about extending Select to work on tensors with element type int32, specifically in the interpreter backend? Or, is it not recommended to extend a single op on a single backend?</p><NewLine></div>",https://discuss.pytorch.org/u/alannnna,(Alanna),alannnna,"December 24, 2019, 11:30pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/alannnna"">@alannnna</a>, Totally reasonable to extend this. I don’t think we have a doc here on what to do, but off the top of my head it should be as simple as:</p><NewLine><ul><NewLine><li>Update <code>Interpreter::isOpSupported()</code> to include <code>ElemKind::Int32ITy</code><NewLine></li><NewLine><li>Update <code>BoundInterpreterFunction::fwdElementExpInst()</code> to support it</li><NewLine><li>(probably not necessary) May need to update <code>SelectNode::verify()</code> if there is any verification that fails</li><NewLine></ul><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: December 28, 2019,  1:51pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
63863,How to execute a onnx model having LSTM feature with Glow compiler,2019-12-12T12:33:13.522Z,0,449,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Here we’re using  a LSTM model which predicts the sentence by giving input of string, it works fine with torchvision without using glow compiler.<br/><NewLine>How to convert the model into onnx format and what are the steps for running onnx model using Glow compiler?</p><NewLine></div>",https://discuss.pytorch.org/u/Compiler-team-1,(Compiler Team 1),Compiler-team-1,"December 12, 2019, 12:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you specifically want to go the ONNX path, you should be able to export to ONNX using instructions <a href=""https://pytorch.org/docs/stable/onnx.html"" rel=""nofollow noopener"">here</a>.</p><NewLine><p>Once it’s in ONNX format you can load it through our Loader class, and run it on any Glow backend (assuming it has sufficient support) using instructions <a href=""https://github.com/pytorch/glow/blob/master/docs/Testing.md#model-loader"" rel=""nofollow noopener"">here</a>. However note that it sounds like you will need to build your own custom Loader class for this (perhaps similar to our <code>text-translator</code> which you can find info about in the above link).</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am running a LSTM model which is in onnx format but i am getting error as below:</p><NewLine><p>./bin/model-compiler -m /home/hemanth/lstm_new.onnx -emit-bundle=/home/Desktop<br/><NewLine>WARNING: Logging before InitGoogleLogging() is written to STDERR<br/><NewLine>F1223 11:42:51.549001 30189 Error.cpp:119] exitOnError(Error) got an unexpected ErrorValue:<br/><NewLine>Error message: No node under name lstm_h<br/><NewLine>Error return stack:<br/><NewLine>/home/glow/lib/Importer/ProtobufLoader.cpp:110<br/><NewLine>/home/glow/include/glow/Importer/CommonOperatorLoader.h:584<br/><NewLine>/home/glow/include/glow/Importer/CommonOperatorLoader.h:1128<br/><NewLine>/home/glow/lib/Importer/ONNXModelLoader.cpp:1763<br/><NewLine>/home/glow/lib/Importer/ONNXModelLoader.cpp:1940<br/><NewLine>/home/glow/lib/Importer/ONNXModelLoader.cpp:2028<br/><NewLine>/home/glow/lib/Importer/ONNXModelLoader.cpp:2042<br/><NewLine>*** Check failure stack trace: ***<br/><NewLine><span class=""hashtag"">#0</span> 0x00000000006a512a llvm::sys::PrintStackTrace(llvm::raw_ostream&amp;) (./bin/model-compiler+0x6a512a)<br/><NewLine><span class=""hashtag"">#1</span> 0x00000000006a30bc llvm::sys::RunSignalHandlers() (./bin/model-compiler+0x6a30bc)<br/><NewLine><span class=""hashtag"">#2</span> 0x00000000006a3227 SignalHandler(int) (./bin/model-compiler+0x6a3227)<br/><NewLine><span class=""hashtag"">#3</span> 0x00007fa0fcc41390 __restore_rt (/lib/x86_64-linux-gnu/libpthread.so.0+0x11390)<br/><NewLine><span class=""hashtag"">#4</span> 0x00007fa0fbdcb428 gsignal (/lib/x86_64-linux-gnu/libc.so.6+0x35428)<br/><NewLine><span class=""hashtag"">#5</span> 0x00007fa0fbdcd02a abort (/lib/x86_64-linux-gnu/libc.so.6+0x3702a)<br/><NewLine><span class=""hashtag"">#6</span> 0x00007fa0fca0ae49 (/usr/lib/x86_64-linux-gnu/libglog.so.0+0x9e49)<br/><NewLine><span class=""hashtag"">#7</span> 0x00007fa0fca0c5cd (/usr/lib/x86_64-linux-gnu/libglog.so.0+0xb5cd)<br/><NewLine><span class=""hashtag"">#8</span> 0x00007fa0fca0e433 google::LogMessage::SendToLog() (/usr/lib/x86_64-linux-gnu/libglog.so.0+0xd433)<br/><NewLine><span class=""hashtag"">#9</span> 0x00007fa0fca0c15b google::LogMessage::Flush() (/usr/lib/x86_64-linux-gnu/libglog.so.0+0xb15b)<br/><NewLine><span class=""hashtag"">#10</span> 0x00007fa0fca0ee1e google::LogMessageFatal::~LogMessageFatal() (/usr/lib/x86_64-linux-gnu/libglog.so.0+0xde1e)<br/><NewLine><span class=""hashtag"">#11</span> 0x00000000025cd9ca glow::detail::exitOnError(char const*, unsigned long, glow::detail::GlowError) (./bin/model-compiler+0x25cd9ca)<br/><NewLine><span class=""hashtag"">#12</span> 0x00000000005ff7db glow::ONNXModelLoader::ONNXModelLoader(std::__cxx11::basic_string&lt;char, std::char_traits, std::allocator &gt; const&amp;, llvm::ArrayRef&lt;char const*&gt;, llvm::ArrayRef&lt;glow::Type const*&gt;, glow::Function&amp;, glow::detail::GlowError*, bool) (./bin/model-compiler+0x5ff7db)<br/><NewLine><span class=""hashtag"">#13</span> 0x0000000000454540 main (./bin/model-compiler+0x454540)<br/><NewLine><span class=""hashtag"">#14</span> 0x00007fa0fbdb6830 __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x20830)<br/><NewLine><span class=""hashtag"">#15</span> 0x00000000004fb909 _start (./bin/model-compiler+0x4fb909)<br/><NewLine>Aborted (core dumped)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Compiler-team-1; <NewLine> ,"REPLY_DATE 1: December 13, 2019, 12:03am; <NewLine> REPLY_DATE 2: December 23, 2019,  6:19am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
62762,Partitioning multiple networks,2019-12-02T16:09:15.971Z,0,195,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m interested in using/extending glow to run multiple networks concurrently. From what I found out so far, the compilation process for some network works as follows:</p><NewLine><ol><NewLine><li>A caffe2 or onnx network is loaded into one glow::function inside a glow:module</li><NewLine><li>This module can be added to the HostManager where the compilation, optimizations and partitioning happen</li><NewLine></ol><NewLine><p>Therefor, when multiple networks are added, the DAGs of the single networks are created independantly from each other, right? If that is correct, do the overall physically available resources need to be partitioned in advance for the networks?</p><NewLine><p>What I want to achieve is to do the partitioning, taking all the loaded networks into account. Is that something that should even be supported by glow sooner or later, or is that not desired?<br/><NewLine>Maybe the most simple solution, concerning the current situation would be to just load multiple networks into one module, but that seems a bit hacky to me?</p><NewLine></div>",https://discuss.pytorch.org/u/lpolari,,lpolari,"December 2, 2019,  4:09pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/lpolari"">@lpolari</a>, you are correct, when multiple networks are added the DAGs for each network are independent from each other. Physical resources are taken into account while adding a network so advanced partitioning may not be necessary.<br/><NewLine>When loading partitions onto a device the Glow Runtime will try to evenly distribute used space on the devices.<br/><NewLine>Example:<br/><NewLine>3 Devices with 10GB of memory<br/><NewLine>Network A Partitioned: A1[6GB], A2[5GB]<br/><NewLine>Network B Partitioned B1[4GB], B2[8GB]<br/><NewLine>Assuming you load A then B.<br/><NewLine>The device loading will looks like:<br/><NewLine>Dev1: A1, B1<br/><NewLine>Dev2: A2<br/><NewLine>Dev3: B2<br/><NewLine>The limitation here is that Glow decides one network at a time so you might end up with a less than optimal assignment of resources.<br/><NewLine>To get “optimal” will depend a bit on your use case. In general glow only partitions a network if it will not fit on a single device. So you usually won’t end up with many different networks loaded on a single device.<br/><NewLine>If you are running many smaller networks then partitioning shouldn’t be an issue but you will have contention for devices at inference time.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/gcatron; <NewLine> ,"REPLY_DATE 1: December 9, 2019, 11:16pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> 
18933,Map high-level (graph) IR to low-level IR,2018-05-31T00:09:36.239Z,0,1265,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I started looking into Glow compiler this week.</p><NewLine><p>What is the recommended way to dump the dataflow graph for, say, the <a href=""https://github.com/pytorch/glow/blob/master/docs/Example.md"" rel=""nofollow noopener"">LeNet example here</a> in dot format?</p><NewLine><p>I see in <a href=""https://github.com/pytorch/glow/blob/master/docs/IR.md"" rel=""nofollow noopener"">Glow IR documentation</a>:</p><NewLine><blockquote><NewLine><p>“The compiler has a debug method for dumping a graphical representation of the graph into a dotty file. The method is called ‘dumpDAG’. The images above were generated with this method. The textual representation of the graph is less informative and it looks like this…”</p><NewLine></blockquote><NewLine><p>However, I’m not sure I see a cmdline option to dump such a representation. Is that right? Do I need to hack it? I see that a <code>-dump-llvm-ir</code> option is available to <em>dump the LLVM-IR of the jitted code</em>.</p><NewLine><p>My objective is to understand (visually) the mapping of the high-level constructs captured by the high-level IR to the linear algebra primitives with which the low-level IR is comprised.</p><NewLine><p>-SR</p><NewLine></div>",https://discuss.pytorch.org/u/soumyarooproy,(Soumyaroop Roy),soumyarooproy,"May 31, 2018, 12:34am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Made some progress here. Documenting for posterity.</p><NewLine><p>In the context of the LeNet example <a href=""https://github.com/pytorch/glow/blob/master/docs/Example.md"" rel=""nofollow noopener"">here</a>, described in C++, adding the following will dump the network DAG:</p><NewLine><pre><code class=""lang-auto"">    F-&gt;dumpDAG(""lenet.dot"");<NewLine></code></pre><NewLine><p>The dumped DAG is the one that’s shown in the link above. Run the following to generate the image:</p><NewLine><pre><code class=""lang-auto"">dot -Tpng -O lenet.dot<NewLine></code></pre><NewLine><p>To dump the low-level IR for the compilation unit — a function at a time as I understand (is that right?) — add the following:</p><NewLine><pre><code class=""lang-auto"">    // Compile for inference<NewLine>    EE.compile(CompilationMode::Infer, F);<NewLine>    EE.getIR().dumpDAG(""lenet_infer_ir.dot"");<NewLine></code></pre><NewLine><p>Alternately, to dump the training graph and the IR DAG for the training network, add the following:</p><NewLine><pre><code class=""lang-auto"">    // Set up training of the network<NewLine>    EE.getConfig().learningRate = 0.1;<NewLine>    EE.getConfig().momentum = 0;<NewLine>    EE.getConfig().batchSize = minibatchSize;<NewLine><NewLine>    Function *TF = glow::differentiate(F, EE.getConfig());<NewLine>    // Compile for training<NewLine>    EE.compile(CompilationMode::Train, TF);<NewLine>    TF-&gt;dumpDAG(""lenet_train.dot"");<NewLine>    EE.getIR().dumpDAG(""lenet_train_ir.dot"");<NewLine></code></pre><NewLine><p>-SR</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>To add on the answer, if you run something that use the loader (Loader.cpp), you can use the command line option <code>-dumpGraphDAG=&lt;filename.dot&gt;</code> to dump the graph using the .dot format.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can we get the <strong>dumpDAG</strong> output for the python model too like in C++?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/soumyarooproy; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/qcolombet; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Sharath_R; <NewLine> ,"REPLY_DATE 1: June 1, 2018, 10:57pm; <NewLine> REPLY_DATE 2: August 28, 2018, 11:52pm; <NewLine> REPLY_DATE 3: December 8, 2019,  7:40pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
58727,Creating bundles error,2019-10-20T14:50:59.101Z,0,194,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to follow the instructions in <a href=""https://github.com/pytorch/glow/blob/master/docs/AOT.md"" rel=""nofollow noopener"">https://github.com/pytorch/glow/blob/master/docs/AOT.md</a></p><NewLine><p>When I reached this step, something went wrong，</p><NewLine><ul><NewLine><li><code>cmake -G ninja &lt;other cmake flags&gt; -DGLOW_WITH_BUNDLES=ON -DGLOW_WITH_CPU=ON</code></li><NewLine><li><NewLine><code>ninja RunResNet50Bundle</code><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/366605ff842f0e5e31c0c5c5280d68944a5bfaa9"" href=""https://discuss.pytorch.org/uploads/default/original/3X/3/6/366605ff842f0e5e31c0c5c5280d68944a5bfaa9.png"" title=""捕获.PNG""><img alt=""%E6%8D%95%E8%8E%B7"" data-base62-sha1=""7LemN2dUBrYCq4iy9stw5tYvtQR"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/3/6/366605ff842f0e5e31c0c5c5280d68944a5bfaa9_2_10x10.png"" height=""105"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/3/6/366605ff842f0e5e31c0c5c5280d68944a5bfaa9_2_690x105.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/3/6/366605ff842f0e5e31c0c5c5280d68944a5bfaa9_2_690x105.png, https://discuss.pytorch.org/uploads/default/optimized/3X/3/6/366605ff842f0e5e31c0c5c5280d68944a5bfaa9_2_1035x157.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/3/6/366605ff842f0e5e31c0c5c5280d68944a5bfaa9.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">捕获.PNG</span><span class=""informations"">1046×160 105 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><NewLine><br/><NewLine>Error creating directory “/bundles/resnet50”</li><NewLine></ul><NewLine><p>Is this because I am running the command in the wrong directory?</p><NewLine></div>",https://discuss.pytorch.org/u/burning2,,burning2,"October 20, 2019,  2:50pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry for the delayed response! It looks like <code>GLOW_BINARY_DIR</code> is not being set correctly. I believe it needs to be set in your environment if you’re not building the whole project via <code>-DGLOW_WITH_BUNDLES=ON</code>. In your case it’s probably <code>/home/liss/projects/glow/</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: December 2, 2019,  7:21pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
56065,Undefined reference to glow::Stats(),2019-09-16T14:30:41.206Z,0,390,"<div class=""post"" itemprop=""articleBody""><NewLine><h2>Ubuntu 16.0.4 version<br/><NewLine>LLVM 8.0.1<br/><NewLine>CLANG 8<br/><NewLine>sudo ninja all<br/><NewLine>[0/1] Re-running CMake…<br/><NewLine>– Found glog with new-style glog target.<br/><NewLine>– Found LLVM 8.0.1<br/><NewLine>– Using LLVMConfig.cmake in: /usr/local/lib/cmake/llvm<br/><NewLine>Adding CPU backend.<br/><NewLine>Adding CMakeFiles backend.<br/><NewLine>Adding Interpreter backend.</h2><NewLine><h2>– ******** Summary ********<br/><NewLine>–   CMake version         : 3.5.1<br/><NewLine>–   CMake command         : /usr/bin/cmake<br/><NewLine>–   System                : Linux<br/><NewLine>–   C++ compiler          : /usr/bin/c++<br/><NewLine>–   C++ compiler version  : 5.5.0<br/><NewLine>–   CXX flags             :  -Wall -Wnon-virtual-dtor -fno-exceptions -fno-rtti -Wno-psabi -Wnon-virtual-dtor<br/><NewLine>–   Build type            : Release<br/><NewLine>–   Compile definitions   : GIT_SHA1=“ef7f916”;GIT_DATE=“2019-09-12”;WITH_PNG;GLOW_WITH_LLVMIRCODEGEN=1;GLOW_WITH_CPU=1;GOOGLE_PROTOBUF_NO_RTTI;ONNX_NAMESPACE=glow_onnx<br/><NewLine>–   CMAKE_PREFIX_PATH     : /usr/bin<br/><NewLine>–   CMAKE_INSTALL_PREFIX  : /usr/local<br/><NewLine>–   CMAKE_MODULE_PATH     : ~glow/cmake/modules</h2><NewLine><h2>–   ONNX version          : 1.5.0<br/><NewLine>–   ONNX NAMESPACE        : glow_onnx<br/><NewLine>–   ONNX_BUILD_TESTS      : OFF<br/><NewLine>–   ONNX_BUILD_BENCHMARKS : OFF<br/><NewLine>–   ONNX_USE_LITE_PROTO   : OFF<br/><NewLine>–   ONNXIFI_DUMMY_BACKEND : OFF<br/><NewLine>–   ONNXIFI_ENABLE_EXT    : OFF</h2><NewLine><p>–   Protobuf compiler     : /usr/bin/protoc<br/><NewLine>–   Protobuf includes     : /usr/include<br/><NewLine>–   Protobuf libraries    : optimized;/usr/lib/x86_64-linux-gnu/libprotobuf.so;debug;/usr/lib/x86_64-linux-gnu/libprotobuf.so;-pthread<br/><NewLine>–   BUILD_ONNX_PYTHON     : OFF<br/><NewLine>– Failed to find LLVM FileCheck<br/><NewLine>– git Version: v1.5.0<br/><NewLine>– Version: 1.5.0<br/><NewLine>– Performing Test HAVE_STD_REGEX – success<br/><NewLine>– Performing Test HAVE_GNU_POSIX_REGEX – failed to compile<br/><NewLine>– Performing Test HAVE_POSIX_REGEX – success<br/><NewLine>– Performing Test HAVE_STEADY_CLOCK – success<br/><NewLine>Skipping adding test en2gr_cpu_test because it requires a models directory. Configure with -DGLOW_MODELS_DIR.<br/><NewLine>Skipping adding test en2gr_quantization_test because it requires a models directory. Configure with -DGLOW_MODELS_DIR.<br/><NewLine>Skipping adding test en2gr_cpu_partition_test because it requires a models directory. Configure with -DGLOW_MODELS_DIR.<br/><NewLine>Skipping adding test en2gr_cpu_config_test because it requires a models directory. Configure with -DGLOW_MODELS_DIR.<br/><NewLine>Skipping adding test resnet_runtime_test because it requires a models directory. Configure with -DGLOW_MODELS_DIR.<br/><NewLine>– Configuring done<br/><NewLine>– Generating done<br/><NewLine>– Build files have been written to: ~/glow<br/><NewLine>[1/111] Linking CXX executable bin/resnet-verify<br/><NewLine>FAILED: bin/resnet-verify<br/><NewLine>: &amp;&amp; /usr/bin/c++   -Wall -Wnon-virtual-dtor -fno-exceptions -fno-rtti -Wno-psabi -O3 -DNDEBUG -march=native -ffast-math -fno-finite-math-only   examples/CMakeFiles/resnet-verify.dir/resnet-verify.cpp.o  -o bin/resnet-verify  lib/ExecutionEngine/libExecutionEngine.a lib/Graph/libGraph.a lib/Importer/libImporter.a lib/Runtime/HostManager/libHostManager.a lib/Partitioner/libPartitioner.a lib/Runtime/Provisioner/libProvisioner.a lib/Runtime/Executor/libExecutor.a lib/Optimizer/GraphOptimizer/libGraphOptimizer.a lib/Backends/libBackends.a lib/Quantization/libQuantization.a lib/Backend/libBackend.a lib/ExecutionContext/libExecutionContext.a lib/CodeGen/libCodeGen.a lib/IR/libIR.a lib/Optimizer/GraphOptimizerPipeline/libGraphOptimizerPipeline.a /usr/local/lib/libLLVMCore.a /usr/local/lib/libLLVMBinaryFormat.a lib/Converter/libConverter.a lib/Graph/libGraph.a lib/Quantization/Base/libQuantizationBase.a lib/Support/TensorPool/libTensorPool.a lib/Base/libBase.a /usr/lib/x86_64-linux-gnu/libpng.so lib/Support/libSupport.a /usr/local/lib/libglog.a /usr/local/lib/libLLVMSupport.a -lz -lrt -ldl -ltinfo -lpthread -lm /usr/local/lib/libLLVMDemangle.a lib/Importer/build_onnx/libonnx_proto.a -pthread /usr/lib/x86_64-linux-gnu/libprotobuf.so &amp;&amp; :<br/><NewLine>lib/Runtime/HostManager/libHostManager.a(HostManager.cpp.o): In function <code>glow::runtime::HostManager::exportMemoryCounters()': HostManager.cpp:(.text+0x601): undefined reference to</code>glow::Stats()’<br/><NewLine>HostManager.cpp:(.text+0x618): undefined reference to <code>glow::StatsExporterRegistry::setCounter(llvm::StringRef, long)' HostManager.cpp:(.text+0x61d): undefined reference to</code>glow::Stats()’<br/><NewLine>HostManager.cpp:(.text+0x634): undefined reference to <code>glow::StatsExporterRegistry::setCounter(llvm::StringRef, long)' HostManager.cpp:(.text+0x639): undefined reference to</code>glow::Stats()’<br/><NewLine>HostManager.cpp:(.text+0x650): undefined reference to <code>glow::StatsExporterRegistry::setCounter(llvm::StringRef, long)' lib/Runtime/HostManager/libHostManager.a(HostManager.cpp.o): In function</code>glow::runtime::HostManager::clearHost()’:<br/><NewLine>HostManager.cpp:(.text+0x3521): undefined reference to <code>glow::Stats()' HostManager.cpp:(.text+0x3537): undefined reference to</code>glow::StatsExporterRegistry::setCounter(llvm::StringRef, long)’<br/><NewLine>HostManager.cpp:(.text+0x353c): undefined reference to <code>glow::Stats()' HostManager.cpp:(.text+0x3552): undefined reference to</code>glow::StatsExporterRegistry::setCounter(llvm::StringRef, long)’<br/><NewLine>HostManager.cpp:(.text+0x3557): undefined reference to <code>glow::Stats()' HostManager.cpp:(.text+0x356d): undefined reference to</code>glow::StatsExporterRegistry::setCounter(llvm::StringRef, long)’<br/><NewLine>lib/Base/libBase.a(Image.cpp.o): In function <code>glow::getPngInfo(char const*)': Image.cpp:(.text+0x1b5): undefined reference to</code>png_set_longjmp_fn’<br/><NewLine>lib/Base/libBase.a(Image.cpp.o): In function <code>glow::writePngImage(glow::Tensor*, char const*, std::pair&lt;float, float&gt;, llvm::ArrayRef&lt;float&gt;, llvm::ArrayRef&lt;float&gt;)': Image.cpp:(.text+0x5e6): undefined reference to</code>png_set_longjmp_fn’<br/><NewLine>Image.cpp:(.text+0x641): undefined reference to <code>png_set_longjmp_fn' Image.cpp:(.text+0x6c8): undefined reference to</code>png_set_longjmp_fn’<br/><NewLine>Image.cpp:(.text+0x8a6): undefined reference to <code>png_set_longjmp_fn' lib/Base/libBase.a(Image.cpp.o):Image.cpp:(.text+0xa4f): more undefined references to</code>png_set_longjmp_fn’ follow<br/><NewLine>collect2: error: ld returned 1 exit status<br/><NewLine>I am getting this error please help me to fix this!</p><NewLine></div>",https://discuss.pytorch.org/u/ponnam_sairam,(ponnam sairam),ponnam_sairam,"October 16, 2019, 10:34am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What’s your CMake command line?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually, I think this is a straight-up bug in our CMake.  I dunno why it works in CI.  <a href=""https://github.com/pytorch/glow/pull/3517"" rel=""nofollow noopener"">https://github.com/pytorch/glow/pull/3517</a> should fix it.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code>								GLOW INSTALLATION STEPS On Ubuntu (16.04)<NewLine></code></pre><NewLine><hr/><NewLine><p>Glow Compiler Prerequisites:</p><NewLine><pre><code>	Operating system	:	Ubuntu 16.04LTS<NewLine><NewLine>	RAM			:	Minimum 16GB<NewLine><NewLine>	SWAP MEMORY		:	Minimum 12GB to 20GB<NewLine><NewLine>	Memory Needed		:	70GB<NewLine><NewLine>	Total Memory needed 	:	Minimum 150GB(LLVM&amp;GLOW)<NewLine></code></pre><NewLine><hr/><NewLine><p>Glow Compiler Dependencies:</p><NewLine><pre><code>	LLVM 8.0.1<NewLine><NewLine>	Clang 8.0.1<NewLine><NewLine>	Anaconda 3<NewLine>`	<NewLine>	Pytorch if GPU is used need to install CUDA 10.1 and cuDNN 7.1		<NewLine></code></pre><NewLine><hr/><NewLine><p>Glow Compiler Process</p><NewLine><p>Step1:</p><NewLine><pre><code>Download glow repository from git hub<NewLine><NewLine>	$git clone https://github.com/pytorch/glow.git<NewLine><NewLine>	$cd glow<NewLine></code></pre><NewLine><p>Step2:</p><NewLine><pre><code>#Glow depends on a few submodules: googletest, onnx, and a library for FP16 conversions.<NewLine><NewLine>#To get them, from the glow directory, run:<NewLine><NewLine>	$git submodule update --init --recursive<NewLine></code></pre><NewLine><p>Step3:</p><NewLine><pre><code>#If Protobuf is not installed install it by using shell script<NewLine><NewLine>#version should be 2.6.1 <NewLine><NewLine>#PATH: glow/utils/<NewLine><NewLine>#run shell script <NewLine><NewLine>	$./install_protobuf.sh<NewLine></code></pre><NewLine><p>Step4:</p><NewLine><pre><code>#Create a build directory in glow<NewLine><NewLine>	$mkdir build<NewLine><NewLine>#Change working directory to build<NewLine><NewLine>	$cd build<NewLine><NewLine>#Now run cmake in Release mode providing Glow source directory as path<NewLine><NewLine>	$cmake -DCMAKE_BUILD_TYPE=Release ../<NewLine><NewLine>#This will build files into ......( It will take 4 to 8 hours or more based on RAM and SWAP memory)<NewLine><NewLine>#if cmake is not installed install it by running following command<NewLine><NewLine>	$sudo apt install cmake<NewLine></code></pre><NewLine><p>step5:</p><NewLine><pre><code>#run make command to compile the source code<NewLine><NewLine>	$make<NewLine></code></pre><NewLine><p>Step6:</p><NewLine><pre><code>#run make install to install the library<NewLine><NewLine>	$make install<NewLine></code></pre><NewLine><hr/><NewLine><p>Testing Glow:</p><NewLine><pre><code>#A few test programs that use Glow's C++ API are found under the examples/ subdirectory. The mnist, cifar10, fr2en and ptb programs train and 	<NewLine><NewLine>run digit recognition, image classification and language modeling benchmarks, respectively.<NewLine><NewLine>#To run these programs, build Glow in Release mode, then run the following commands to download the cifar10, mnist and ptb databases.<NewLine><NewLine>$python ../glow/utils/download_datasets_and_models.py --all-datasets<NewLine><NewLine>#Now run the examples. Note that the databases should be in the current working directory.<NewLine><NewLine>	$./bin/mnist	<NewLine><NewLine>	$./bin/cifar10<NewLine><NewLine>	$./bin/fr2en<NewLine><NewLine>	$./bin/ptb<NewLine><NewLine>	$./bin/char-rnn<NewLine><NewLine>#If everything goes well you should see:<NewLine><NewLine>	mnist: pictures from the mnist digits database<NewLine>	<NewLine>cifar10: image classifications that steadily improve<NewLine>	<NewLine>fr2en: an interactive French-to-English translator<NewLine>	<NewLine>ptb: decreasing perplexity on the dataset as the network trains<NewLine>	<NewLine>char-rnn: generates random text based on some document<NewLine></code></pre><NewLine><p>These steps worked and installed Glow compiler in my ubuntu system.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Glow Compiler Dependencies:</p><NewLine><pre><code>	LLVM 8.0.1<NewLine><NewLine>	Clang 8.0.1<NewLine><NewLine>	Anaconda 3<NewLine>`	<NewLine>	Pytorch if GPU is used need to install CUDA 10.1 and cuDNN 7.1</code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Bert_Maher; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Bert_Maher; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Compiler-team-1; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Compiler-team-1; <NewLine> ,"REPLY_DATE 1: September 16, 2019,  4:48pm; <NewLine> REPLY_DATE 2: September 16, 2019, 10:55pm; <NewLine> REPLY_DATE 3: October 16, 2019, 10:34am; <NewLine> REPLY_DATE 4: October 16, 2019, 10:35am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
57589,Habana backend compile error,2019-10-07T01:58:35.017Z,0,218,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I was trying to build Glow with Habana backend option like below:</p><NewLine><ul><NewLine><li><code>cmake -G Ninja -DCMAKE_BUILD_TYPE=Debug -DLLVM_DIR=/usr/local/opt/llvm@7/lib/cmake/llvm -DGLOW_WITH_HABANA=ON ..</code></li><NewLine></ul><NewLine><p>However, the error occurred during cmake sync.<br/><NewLine>The error message is the following:</p><NewLine><pre><code class=""lang-bash"">CMake Error: The following variables are used in this project, but they are set to NOTFOUND.<NewLine>Please set them or make sure they are set and tested correctly in the CMake files:<NewLine>/Users/jeminlee/development/glow/lib/Backends/Habana/SYNAPSE_INCLUDE_DIR<NewLine>   used as include directory in directory /Users/jeminlee/development/glow/lib/Backends/Habana<NewLine>/Users/jeminlee/development/glow/tests/unittests/SYNAPSE_INCLUDE_DIR<NewLine>   used as include directory in directory /Users/jeminlee/development/glow/tests/unittests<NewLine></code></pre><NewLine><p>I would like to know whether there is the right way to compile Glow on Habana backend.</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/leejaymin,(jemin),leejaymin,"October 7, 2019,  1:59am",,,,,
52273,Debugging Glow in terms of Glow IR,2019-08-01T05:58:24.629Z,0,274,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have watched on 2018 LLVM Developers’ meeting: Glow LLVM_based machine learning compiler.<br/><NewLine>In this talk, they mentioned that Glow-IR-level debugging is available.</p><NewLine><p>The following screenshot represents the debugging example on Glow IR.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/94d266ffc60491838d5bb6ad434263ba5827c61f"" href=""https://discuss.pytorch.org/uploads/default/original/2X/9/94d266ffc60491838d5bb6ad434263ba5827c61f.jpeg"" title=""image.jpg""><img alt=""image"" data-base62-sha1=""lexrVXqJmPcbrScML6ahlGRNh2n"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/9/94d266ffc60491838d5bb6ad434263ba5827c61f_2_10x10.png"" height=""499"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/9/94d266ffc60491838d5bb6ad434263ba5827c61f_2_532x499.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/9/94d266ffc60491838d5bb6ad434263ba5827c61f_2_532x499.jpeg, https://discuss.pytorch.org/uploads/default/original/2X/9/94d266ffc60491838d5bb6ad434263ba5827c61f.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/9/94d266ffc60491838d5bb6ad434263ba5827c61f.jpeg 2x"" width=""532""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.jpg</span><span class=""informations"">752×706 168 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>Unlike C and Assembly modes, I wonder how I could enter the debugging mode for Glow-IR.</p><NewLine></div>",https://discuss.pytorch.org/u/leejaymin,(jemin),leejaymin,"August 1, 2019,  6:02am",,,,,
48459,List of high and low level IR,2019-06-20T00:50:02.590Z,1,307,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How do I get a list of all IR ops in glow?</p><NewLine></div>",https://discuss.pytorch.org/u/Milindn,(MilindN),Milindn,"June 20, 2019, 12:50am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can look to a couple places in code depending on what exactly you’re looking for.</p><NewLine><p>You can see all of Glow’s Nodes <a href=""https://github.com/pytorch/glow/blob/master/tools/ClassGen/NodeGen.cpp#L37"" rel=""nofollow noopener"">here</a>. But it generally makes more sense to see if a Node is supported on a specific backend, and with a specific precision.</p><NewLine><p>So you can see whether a particular backend supports a Node with a specific precision by looking at <code>Backend::isOpSupported()</code>. For example, <a href=""https://github.com/pytorch/glow/blob/2605951a3fabe63c23e3da159b8b71864a547458/lib/Backends/CPU/CPUBackend.cpp#L43"" rel=""nofollow noopener"">here is that function for our CPU backend</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks Jordan. Can you point me to any description (document or source code) for memory management pass within the compiler?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sure! Just to be clear, we mainly do memory-related management/optimizations with our low-level IR, meaning only backends which use our low-level IR benefit from them. That includes our CPU, OpenCL, and Interpreter backends, but not Habana which skips low-level IR. Here are some links you might find useful:</p><NewLine><ul><NewLine><li><a href=""https://github.com/pytorch/glow/blob/bdf0a045738fff1195055354123a8e29b3a9baac/lib/IR/ChildMemSizeBasedScheduler.cpp"" rel=""nofollow noopener"">Our default low-level IR scheduler which tries to minimize memory usage</a></li><NewLine><li>Our IR Optimizer does many memory related optimizations such as <a href=""https://github.com/pytorch/glow/blob/bdf0a045738fff1195055354123a8e29b3a9baac/lib/Optimizer/IROptimizer/IROptimizer.cpp#L1174"" rel=""nofollow noopener"">buffer sharing</a>, <a href=""https://github.com/pytorch/glow/blob/bdf0a045738fff1195055354123a8e29b3a9baac/lib/Optimizer/IROptimizer/IROptimizer.cpp#L1331"" rel=""nofollow noopener"">skipping unnecessary intermediate allocations</a>, <a href=""https://github.com/pytorch/glow/blob/bdf0a045738fff1195055354123a8e29b3a9baac/lib/Optimizer/IROptimizer/IROptimizer.cpp#L256"" rel=""nofollow noopener"">alloc activation sinking</a>, and many more found in that file, many (but not all) of which are listed/described <a href=""https://github.com/pytorch/glow/blob/master/docs/Optimizations.md#set-of-supported-ir-optimizations"" rel=""nofollow noopener"">here</a><NewLine></li><NewLine><li><a href=""https://github.com/pytorch/glow/blob/master/docs/IR.md#low-level-ir"" rel=""nofollow noopener"">And here’s a general description of our low-level IR</a></li><NewLine></ul><NewLine><p>Hope that helps!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Milindn; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: June 20, 2019,  1:09am; <NewLine> REPLY_DATE 2: July 23, 2019, 12:47am; <NewLine> REPLY_DATE 3: July 26, 2019,  7:47am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
50461,Glow example run error,2019-07-12T17:27:16.984Z,1,267,"<div class=""post"" itemprop=""articleBody""><NewLine><p>cloned the repo. All unit tests pass. Tried running the mnist and run in to this issue, any ideas?</p><NewLine><p>$:/hdd1/glow/build_Debug$ ./bin/mnist<br/><NewLine>WARNING: Logging before InitGoogleLogging() is written to STDERR<br/><NewLine>I0712 13:22:43.377008 18779 mnist.cpp:46] Loading the mnist database.<br/><NewLine>I0712 13:22:53.249351 18779 mnist.cpp:81] Loaded 50000 images.<br/><NewLine>mnist: …/lib/Graph/NodeValue.cpp:25: glow::NodeValue::NodeValue(glow::Node*): Assertion `(!N || (N-&gt;getNumResults() == 1)) &amp;&amp; “Constructing a value for a multi-res node”’ failed.<br/><NewLine>Aborted</p><NewLine></div>",https://discuss.pytorch.org/u/glossyfungus,,glossyfungus,"July 12, 2019,  5:28pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for reporting this! It was broken by a PR from last week. I’ve put up a fix <a href=""https://github.com/pytorch/glow/pull/3232"" rel=""nofollow noopener"">here</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Works with the fix. Thanks for providing a solution rightaway.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/glossyfungus; <NewLine> ,"REPLY_DATE 1: July 12, 2019, 11:13pm; <NewLine> REPLY_DATE 2: July 14, 2019,  5:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
49015,No int8 4x speedup is observed,2019-06-26T13:37:28.454Z,3,414,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How can I verify that my model was properly quantized with int8 instructions? Are there something stored or logged for me to check? I collected profile and run model in OpenCL mode and did not see significant performance difference. I use Pascal 1050 Ti  which is 6.1 compute compatible device which must support efficient Int8 vector dot product operation (d4pa??). I clearly see GPU boost over CPU backend but almost no quanitization caused speedup. What is the trick?</p><NewLine></div>",https://discuss.pytorch.org/u/Marat,(Закиров Марат),Marat,"June 26, 2019,  1:39pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>One reason could be that different models will benefit differently from quantization. If your model’s weights aren’t huge then perhaps you were already close to compute bound in the float version.</p><NewLine><p>Another reason could be that we haven’t spent a ton of time optimizing our OpenCL kernels. Perhaps they aren’t using the best instructions possible. This is something we would love for improvement on. You can find the kernels in <a href=""https://github.com/pytorch/glow/tree/master/lib/Backends/OpenCL"" rel=""nofollow noopener"">the <code>.cl</code> files located here</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>If your model’s weights aren’t huge then perhaps you were already close to compute bound in the float version.</p><NewLine></blockquote><NewLine><p>Sounds weird. I have model with completely stupid weights like {-1, 0, 1} and observe no performance gain after I apply --load-profile option.</p><NewLine><pre><code class=""lang-auto"">class Net(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Net, self).__init__()<NewLine>        # 1 input image channel, 6 output channels, 3x3 square convolution<NewLine>        # kernel<NewLine>        self.conv1 = nn.Conv2d(3, 8, 3)<NewLine>        self.conv2 = nn.Conv2d(8, 16, 3)<NewLine>        self.conv3 = nn.Conv2d(16, 32, 3)<NewLine>        self.conv4 = nn.Conv2d(32, 64, 3)<NewLine>        self.conv5 = nn.Conv2d(64, 128, 3)<NewLine>        self.conv6 = nn.Conv2d(128, 1000, 3)<NewLine><NewLine>        # Replce all weights by dummy 0 or 1<NewLine>        entities = [self.conv1, self.conv2, self.conv3, self.conv4, self.conv5, self.conv6]<NewLine><NewLine>        for e in entities:<NewLine>            e.weight[:] = torch.randint(-1, 1, e.weight.size())<NewLine>            e.bias[:]   = torch.randint(-1, 1, e.bias.size())<NewLine><NewLine>    def forward(self, x):<NewLine>        # Max pooling over a (2, 2) window<NewLine>        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))<NewLine>        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))<NewLine>        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))<NewLine>        x = F.max_pool2d(F.relu(self.conv4(x)), (2, 2))<NewLine>        x = F.max_pool2d(F.relu(self.conv5(x)), (2, 2))<NewLine>        x = F.max_pool2d(F.relu(self.conv6(x)), (2, 2))<NewLine>        x = x.view(16, 1000)<NewLine>        return x<NewLine></code></pre><NewLine><p>And collected profile</p><NewLine><pre><code class=""lang-auto"">---<NewLine>- nodeOutputName:  'learned_101:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          127<NewLine>- nodeOutputName:  'learned_81:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          127<NewLine>- nodeOutputName:  'learned_41:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          127<NewLine>- nodeOutputName:  'learned_21:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          127<NewLine>- nodeOutputName:  'relu5:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'A291:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'learned_9:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          127<NewLine>- nodeOutputName:  'zero5:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'relu4:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'A261:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'learned_5:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          127<NewLine>- nodeOutputName:  'zero4:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'learned_11:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          127<NewLine>- nodeOutputName:  'A252:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          127<NewLine>- nodeOutputName:  'data:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          -128<NewLine>- nodeOutputName:  'learned_61:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          127<NewLine>- nodeOutputName:  'learned_3:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          127<NewLine>- nodeOutputName:  'learned_1:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          127<NewLine>- nodeOutputName:  'zero2:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'A241:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'learned_7:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          127<NewLine>- nodeOutputName:  'relu2:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'A201:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'A222:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          127<NewLine>- nodeOutputName:  'A181:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'A192:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          127<NewLine>- nodeOutputName:  'A211:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'relu:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'A141:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'A151:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'save_output:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'A301:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'A132:0'<NewLine>  scale:           0.0705882<NewLine>  offset:          127<NewLine>- nodeOutputName:  'zero:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'A162:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          127<NewLine>- nodeOutputName:  'A271:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'A282:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          127<NewLine>- nodeOutputName:  'learned_01:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          127<NewLine>- nodeOutputName:  'A131:0'<NewLine>  scale:           0.00392157<NewLine>  offset:          -128<NewLine>- nodeOutputName:  'output1:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'zero1:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'relu3:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'A231:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'relu1:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'A171:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>- nodeOutputName:  'zero3:0'<NewLine>  scale:           0.1<NewLine>  offset:          0<NewLine>...<NewLine><NewLine></code></pre><NewLine><p>I do not believe that glow as AI compiler do not have any debugging tools which includes logs and some intermediate IR representations for observations and analysis.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Sounds weird. I have model with completely stupid weights like {-1, 0, 1} and observe no performance gain after I apply --load-profile option.</p><NewLine></blockquote><NewLine><p>I did not mean the literal values of the weights, I meant the byte size of the weights themselves, e.g. how many bytes the weights of each conv layer take up. One benefit of quantization is that it shrinks the number of bytes the weights take up by 4x.</p><NewLine><blockquote><NewLine><p>I do not believe that glow as AI compiler do not have any debugging tools which includes logs and some intermediate IR representations for observations and analysis.</p><NewLine></blockquote><NewLine><p>We have a Graph based high level IR which you can dump a dot file of a DAG representation of it. This is via command line option <code>-dump-graph-DAG=""file.dot""</code>. We also have the ability to dump our serialized low-level Instruction IR to stdout, via command line option <code>-dump-ir</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Marat; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: June 26, 2019,  5:39pm; <NewLine> REPLY_DATE 2: June 27, 2019, 12:18pm; <NewLine> REPLY_DATE 3: June 27, 2019,  4:03pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
48902,What is gpu_0/data?,2019-06-25T10:11:27.634Z,1,280,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I build GLOW and now want to test in GPU mode. Recently I work with image-classifier tool but I failed to understand what is</p><NewLine><pre><code class=""lang-auto"">-model-input-name=gpu_0/data<NewLine></code></pre><NewLine><p>???<br/><NewLine>I pass ONNX model file and PNG picture what role model-input-name plays?</p><NewLine><p>Fill line is</p><NewLine><pre><code class=""lang-auto"">./bin/image-classifier tests/images/imagenet/*.png -image-mode=0to1 -m=resnet50 -model-input-name=gpu_0/data?<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Marat,(Закиров Марат),Marat,"June 25, 2019, 10:12am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found fix using lldb debugger if I pass</p><NewLine><pre><code class=""lang-auto"">-model-input-name=gpu_0/data_0<NewLine></code></pre><NewLine><p>instead it works in my case.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>How to specify appropriate -model-input-name when using -input-image-list-file option of image-classifier tool?</p><NewLine><p>If use gpu_0/data_0 I will get</p><NewLine><pre><code class=""lang-auto"">Reshape<NewLine>name : OC2_DUMMY_0<NewLine>Input : float&lt;9 x 2048 x 1 x 1&gt;<NewLine>Dims : [1, 2048]<NewLine>users : 1<NewLine>Result : float&lt;1 x 2048&gt;<NewLine><NewLine>Reshape into a different size<NewLine>For comparison `LHS Equal RHS` with:<NewLine>LHS: 2048<NewLine>RHS: 18432<NewLine>Encountered an error, exiting.<NewLine>location: ../lib/Importer/ONNXModelLoader.cpp:1265 message: Function verification failed.<NewLine></code></pre><NewLine><p>Which is not good</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please see <a href=""https://discuss.pytorch.org/t/quantization-example-resnet50/39568/13"">this comment here and the couple after it</a> about the model input name. It has nothing to do with the input image list – it’s the name of the input in the protobuf representing the model itself.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Marat; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Marat; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: June 25, 2019, 10:41am; <NewLine> REPLY_DATE 2: June 25, 2019, 11:55am; <NewLine> REPLY_DATE 3: June 25, 2019,  9:30pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
46568,Possible to compile to LLVM-IR?,2019-05-29T16:43:03.580Z,4,668,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Would it be possible to compile a model to LLVM-IR instead of the GLOW Low-Level IR?</p><NewLine></div>",https://discuss.pytorch.org/u/samkg,,samkg,"May 29, 2019,  4:44pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>For context, currently we use LLVM-IR as part of our CPU Backend, which is LLVM based. We compile our <code>""libjit.cpp""</code> and other similar cpp files, which contain kernels for each operator in across different precisions, to LLVM IR. And then when we load a model we generate high-level Glow IR (Nodes), then from it we generate low-level Glow IR (Instructions).  Then we iterate over the Glow low-level IR and copy in kernels from our previously generated kernels that are in LLVM IR.</p><NewLine><p>So, you’re wondering about skipping just the low-level IR and going from high-level IR to LLVM IR? What is the benefit/purpose here? I believe it would be possible but it would take a decent amount of work to write all the logic to map down, and I am unsure of the benefit of doing so. Would you still be using our libjit kernels?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am looking for a way to extract that LLVM IR from the CPU Backend - is there any such way to do this?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>We have a flag <code>-dump-llvm-ir</code> which you can use to dump the llvm IR to stdout. Note that this only works for llvm-based backends, e.g. our CPU backend. For example:</p><NewLine><pre><code class=""lang-auto"">./bin/image-classifier tests/images/imagenet/cat_285.png -use-imagenet-normalization -image-mode=0to1 -m=resnet50 -model-input-name=gpu_0/data -dump-llvm-ir -cpu<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi jfix,<br/><NewLine>The solution you provided doesn’t work for me.</p><NewLine><ol><NewLine><li>if you don’t specify “-cpu”, no llvm ir is printed out.</li><NewLine><li>even after added “-cpu”, only libjit’s llvm ir got dumped, no model ir was dumped out. In the standalone bundle, the main.cpp calls an extern function resnet50(…), but I can’t find such function in the dumped llvm ir. The only thing close to it is something called “<span class=""mention"">@jitmain</span>”. Will jitmain got renamed to resnet50 later on? Otherwise how main.o link against it?<br/><NewLine>Is there a way to dump out the whole llvm ir of both the model itself, and the libjit?</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>RE: 1, Yeah sorry that was my fault. You have to be using an LLVM based backend for <code>-dump-llvm-ir</code> to work correctly.</p><NewLine><p>I do not know all of the details on LLVM-based backends – I would suggest asking on a GH issue via <a href=""https://github.com/pytorch/glow/issues/new"" rel=""nofollow noopener"">this link</a>, and someone more knowledgable about LLVM backends and bundles will be able to answer.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>You’re doing it right, actually <code>-cpu -dump-llvm-ir</code> is the right way to dump LLVM IR generated by the CPU backend.  The problem is the output is pretty overwhelming because you get all of libjit’s IR dumped in addition to the IR generated for your model.</p><NewLine><p>You’ll see two big sections in the dumped output: “before optimizations” and “after optimizations”.  The “after” section will just have your model code, since it’s after we do inlining, specialization and prune unused functions.  Look for <span class=""mention"">@jitmain</span> (or <span class=""mention"">@main</span>) in either section to see where the model code starts.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/samkg; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/maxima; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Bert_Maher; <NewLine> ,"REPLY_DATE 1: May 29, 2019,  8:15pm; <NewLine> REPLY_DATE 2: May 30, 2019,  6:51pm; <NewLine> REPLY_DATE 3: June 21, 2019, 11:11pm; <NewLine> REPLY_DATE 4: June 12, 2019,  6:55pm; <NewLine> REPLY_DATE 5: June 21, 2019, 11:20pm; <NewLine> REPLY_DATE 6: June 22, 2019,  4:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 2 Likes; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> 
48263,Running Pytorch models on host x86,2019-06-18T12:57:03.500Z,5,474,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry for possibly stupid question. But my question emerges naturally due to lack (in my opinion) of complete step-by-step documentation for release transition process.</p><NewLine><p>To the best of my knowledge the only way to effectively (using GPU + quantization to int8 + some compiler optimizations) execute PyTorch models on host devices is:</p><NewLine><ol><NewLine><li>Make JIT code from PyTorch models</li><NewLine><li>Save  this model in ONNX format on disk</li><NewLine><li>Make an application or use existing to load and run pytorch model in ONNX format saved previously</li><NewLine></ol><NewLine><p>My question is how much code will be autonomous? What libraries and other environment features will it require to run on host x86 server with GPU on the customer side?</p><NewLine></div>",https://discuss.pytorch.org/u/Marat,(Закиров Марат),Marat,"June 18, 2019, 12:59pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not stupid at all! <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>Right now we support the <a href=""https://github.com/pytorch/glow/blob/master/docs/Onnxifi.md"" rel=""nofollow noopener"">ONNXIFI interface</a> which allows PyTorch/Caffe2 to use Glow as an execution backend. Through ONNXIFI (technically FOXI as noted there) the model is passed as an ONNX or C2 proto to Glow, loaded/compiled/quantized/etc., and run on one of our backends.</p><NewLine><p>We also have a <a href=""https://github.com/pytorch/glow/pull/3069"" rel=""nofollow noopener"">very basic PR up here</a> (has not yet landed) that creates an actual Glow backend for PyTorch, which goes more directly from PyTorch IR to Glow.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Building GLOW (<a href=""https://github.com/pytorch/glow"" rel=""nofollow noopener"">https://github.com/pytorch/glow</a>) is extreme pain which also includes additional pain of building custom llvm (<a href=""https://solarianprogrammer.com/2013/01/17/building-clang-libcpp-ubuntu-linux/"" rel=""nofollow noopener"">https://solarianprogrammer.com/2013/01/17/building-clang-libcpp-ubuntu-linux/</a>). I am still trying to build GLOW first I find issue with libpng&amp;zlib next I have problem with protobuf. Are your suggestion simple?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m sorry you’ve had such a bad experience. If you have feedback we’d happily take it to improve developer experience.</p><NewLine><p>If you’re on Ubuntu 18.04 (like the link you posted says), I believe you should be able to use <code>apt-get</code> to install all of our dependencies. Have you followed the <a href=""https://github.com/pytorch/glow#ubuntu"" rel=""nofollow noopener"">instructions on our README</a>? It says it’s been tested on 16.04 at least. Or are you building everything manually?</p><NewLine><p>The instructions for installing and using ONNXIFI are on that page, but I haven’t tried it myself. That page does say it can be a little tricky but I do not know the details there. If you try and run into issues you can always reach out for help.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have the following issue with protobuf</p><NewLine><pre><code class=""lang-auto"">lib/Importer/libImporter.a(caffe2.pb.cc.o):(.data.rel.ro+0xaf0): undefined reference to `google::protobuf::Message::InitializationErrorString[abi:cxx11]() const'<NewLine>lib/Importer/libImporter.a(caffe2.pb.cc.o):(.data.rel.ro+0xb90): undefined reference to `google::protobuf::Message::GetTypeName[abi:cxx11]() const'<NewLine>lib/Importer/libImporter.a(caffe2.pb.cc.o):(.data.rel.ro+0xbc8): undefined reference to `google::protobuf::Message::InitializationErrorString[abi:cxx11]() const'<NewLine>lib/Importer/libImporter.a(caffe2.pb.cc.o):(.data.rel.ro+0xc68): undefined reference to `google::protobuf::Message::GetTypeName[abi:cxx11]() const'<NewLine>lib/Importer/libImporter.a(caffe2.pb.cc.o):(.data.rel.ro+0xca0): undefined reference to `google::protobuf::Message::InitializationErrorString[abi:cxx11]() const'<NewLine>clang-8: error: linker command failed with exit code 1 (use -v to see invocation)<NewLine>[214/282] Linking CXX executable bin/char-rnn<NewLine>ninja: build stopped: subcommand failed.<NewLine></code></pre><NewLine><p>And I have following version of protobuf it is 3.6.1 which is &gt;= 2.6.1</p><NewLine><pre><code class=""lang-auto"">marat@moon:~/glow/build_Debug$ which protoc<NewLine>/home/marat/anaconda3/bin/protoc<NewLine>marat@moon:~/glow/build_Debug$ protoc --version<NewLine>libprotoc 3.6.1<NewLine>marat@moon:~/glow/build_Debug$<NewLine></code></pre><NewLine><p>I also will be very appreciated if you say where did you take llvm-8 because I failed to find it by apt in my ubuntu 16.04</p><NewLine><p>UPDATE</p><NewLine><p>I removed anaconda form $PATH and it helped. But I do not know how it will work: pytorch with version of 3.6.1 and glow compiled with 2.6.1. So it seems that problem on compilation stage accrued due to libraries versions mismatch .</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I also trying to build GLOW with</p><NewLine><pre><code class=""lang-auto"">cmake -G Ninja -DCMAKE_BUILD_TYPE=Release -DGLOW_WITH_CPU=1 -DGLOW_WITH_OPENCL=1 ..<NewLine></code></pre><NewLine><p>But recently have some problems</p><NewLine><pre><code class=""lang-auto"">CMake Error at lib/Backends/OpenCL/CMakeLists.txt:32 (add_library):<NewLine>  Target ""OpenCLBackend"" links to target ""OpenCL::OpenCL"" but the target was<NewLine>  not found.  Perhaps a find_package() call is missing for an IMPORTED<NewLine>  target, or an ALIAS target is missing?<NewLine><NewLine></code></pre><NewLine><p>That I supposed to do?</p><NewLine><p>I believe that problem come from</p><NewLine><pre><code class=""lang-auto"">target_link_libraries(OpenCLBackend<NewLine>                      PUBLIC<NewLine>                      OpenCL::OpenCL)<NewLine><NewLine></code></pre><NewLine><p>from</p><NewLine><pre><code class=""lang-auto"">lib/Backends/OpenCL/CMakeLists.txt<NewLine></code></pre><NewLine><p>find_package finds OpenCL successfully (no error messages about that)</p><NewLine><p>That is  “OpenCL::OpenCL” ?</p><NewLine><p>UPDATE</p><NewLine><p>This issue solved by using more recent cmake, <strong>cmake 3.5.2 DID NOT WORK</strong> please update cmake minimal required</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the feedback! Would you mind posting these problems as GH issues via <a href=""https://github.com/pytorch/glow/issues/new"" rel=""nofollow noopener"">this link</a>? We will be sure to fix them.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Marat; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Marat; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Marat; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: June 18, 2019,  3:58pm; <NewLine> REPLY_DATE 2: June 19, 2019, 12:09pm; <NewLine> REPLY_DATE 3: June 19, 2019,  4:11pm; <NewLine> REPLY_DATE 4: June 20, 2019,  2:02pm; <NewLine> REPLY_DATE 5: June 21, 2019,  2:35pm; <NewLine> REPLY_DATE 6: June 21, 2019, 11:23pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
47925,Error in glow examples,2019-06-14T06:11:21.485Z,0,246,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I built glow from github and when I ran the examples (./bin/mnist) , I’m getting errors as follows<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/3747b02e65299d64a5191f33624fcc56be626a88"" href=""https://discuss.pytorch.org/uploads/default/original/2X/3/3747b02e65299d64a5191f33624fcc56be626a88.png"" title=""image.png""><img alt=""image"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/3/3747b02e65299d64a5191f33624fcc56be626a88_2_10x10.png"" height=""226"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/3/3747b02e65299d64a5191f33624fcc56be626a88_2_690x226.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/3/3747b02e65299d64a5191f33624fcc56be626a88_2_690x226.png, https://discuss.pytorch.org/uploads/default/optimized/2X/3/3747b02e65299d64a5191f33624fcc56be626a88_2_1035x339.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/3/3747b02e65299d64a5191f33624fcc56be626a88.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">1150×377 64.8 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine>Any idea on how to solve this?</p><NewLine></div>",https://discuss.pytorch.org/u/Sharath_R,(Sharath R),Sharath_R,"June 14, 2019,  6:11am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I believe there is a PR up to fix this issue <a href=""https://github.com/pytorch/glow/pull/3097"" rel=""nofollow noopener"">here</a>. It should land tomorrow.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: June 14, 2019,  6:30am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
47739,How to generate LLVM IR from the pytorch code,2019-06-12T13:05:16.131Z,2,379,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve seen in forums that people used -dump-ir command on the command line to get it. Have no idea where to specify it. Can anyone explain? or give a sample code to generate the IR?</p><NewLine></div>",https://discuss.pytorch.org/u/ritesh_gupta,(Ritesh Gupta),ritesh_gupta,"June 12, 2019,  1:05pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>-dump-ir</code> is a command line option specified in <code>lib/Optimizer/IROptimizer.cpp</code>. It should be available on the command line for any binary that links against the <code>Optimizer</code> lib. For example, you can specify it when running the <code>image-classifier</code>. There are examples of running the <code>image-classifier</code> at <code>tests/images/run.sh</code>, so you could append it to any of those. It also should work when running unit tests that link against <code>Optimizer</code>, e.g. <code>./tests/OperatorTest -dump-ir</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/jfix"">@jfix</a>. I’ll try to execute and get back to you. If you have some good tutorial on this please share I need it.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I would suggest reading our <a href=""https://github.com/pytorch/glow/blob/master/docs/Testing.md"" rel=""nofollow noopener"">doc on Testing</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ritesh_gupta; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: June 12, 2019,  6:19pm; <NewLine> REPLY_DATE 2: June 13, 2019,  5:11pm; <NewLine> REPLY_DATE 3: June 13, 2019,  5:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
42252,How to output the Low-level IR instructions to a file in Glow?,2019-04-11T05:39:51.373Z,5,424,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I wish to generate the low-level IR and put it into a file for a specific test such as mnist.<br/><NewLine>I found the IRGen.cpp but there seems no function can export or save the low-level IR.<br/><NewLine>Is there any chance for me to do so?</p><NewLine><p>The low-level IR may look like:</p><NewLine><pre><code class=""lang-auto"">declare {<NewLine>  %input = weight float&lt;8 x 28 x 28 x 1&gt;, broadcast, 0.0<NewLine>  %filter = weight float&lt;16 x 5 x 5 x 1&gt;, xavier, 25.0<NewLine>  %filter0 = weight float&lt;16&gt;, broadcast, 0.100<NewLine>  %weights = weight float&lt;10 x 144&gt;, xavier, 144.0<NewLine>  %bias = weight float&lt;10&gt;, broadcast, 0.100<NewLine>  %selected = weight index&lt;8 x 1&gt;<NewLine>  ...<NewLine>  %result = weight float&lt;8 x 10&gt;<NewLine>}<NewLine><NewLine>program {<NewLine>  %allo = alloc float&lt;8 x 28 x 28 x 16&gt;<NewLine>  %conv = convolution [5 1 2 16] @out %allo, @in %input, @in %filter3, @in %bias0<NewLine>  %allo0 = alloc float&lt;8 x 28 x 28 x 16&gt;<NewLine>  %relu = relu @out %allo0, @in %allo<NewLine>  %allo1 = alloc index&lt;8 x 9 x 9 x 16 x 2&gt;<NewLine>  %allo2 = alloc float&lt;8 x 9 x 9 x 16&gt;<NewLine>  %pool = pool max [3 3 0] @out %allo2, @in %allo0, @inout %allo1<NewLine>  ...<NewLine>  %deal6 = dealloc @out %allo6<NewLine>  %deal7 = dealloc @out %allo7<NewLine>  %deal8 = dealloc @out %allo8<NewLine>  %deal9 = dealloc @out %allo9<NewLine>}<NewLine></code></pre><NewLine><p>Thank you so much!</p><NewLine></div>",https://discuss.pytorch.org/u/Vyronas,(Vyronas),Vyronas,"April 11, 2019,  5:39am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can use the command line option <code>-dump-ir</code>, which will dump the low-level IR after all optimizations have been applied.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Should I use this command line while using cmake to compile?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Should I use this command line while using cmake to compile?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>No. It’s built into the IR optimizer as a command line option when running different binaries. For example you can add it to the command line when running the <code>image-classifier</code> if you’re running <code>mnist</code>.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I got it.  Thank you so much!</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, <a class=""mention"" href=""/u/vyronas"">@Vyronas</a> I also need dump Low level optimized IR to a file. Can you help in this regard? I’ve tried -dump-ir, but it is not generating any IR.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Vyronas; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Vyronas; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Vyronas; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ritesh_gupta; <NewLine> ,"REPLY_DATE 1: April 12, 2019, 11:03pm; <NewLine> REPLY_DATE 2: April 12, 2019, 11:22pm; <NewLine> REPLY_DATE 3: April 13, 2019,  5:51pm; <NewLine> REPLY_DATE 4: April 13, 2019,  6:16pm; <NewLine> REPLY_DATE 5: April 13, 2019,  6:29pm; <NewLine> REPLY_DATE 6: June 12, 2019,  7:10am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
45945,MLIR - a common intermediate representation (IR),2019-05-22T19:35:49.243Z,1,1204,"<div class=""post"" itemprop=""articleBody""><NewLine><p>MLIR: <a href=""https://github.com/tensorflow/mlir"" rel=""nofollow noopener"">https://github.com/tensorflow/mlir</a></p><NewLine><p>MLIR’s intention seems to be an IR lowering framework. In my opinion, this has great synergy with the multiple levels of IR that Glow currently provides.</p><NewLine><p>Does Glow have any intention / interest of integration or use-of MLIR?</p><NewLine></div>",https://discuss.pytorch.org/u/PeterCDMcLean,(Peter Cd Mc Lean),PeterCDMcLean,"May 22, 2019,  7:35pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Peter, we don’t have any plans for MLIR for now. It could make sense to load MLIR into Glow (converting MLIR into Glow IR), which would allow us to use Glow’s optimization stack, and target any of our backends. Did you have something in particular in mind for Glow + MLIR?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: May 27, 2019, 12:25am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
44988,Glow master branch ninja test,2019-05-11T13:23:53.099Z,0,212,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I git cloned <a href=""https://github.com/pytorch/glow"" rel=""nofollow noopener"">https://github.com/pytorch/glow</a><br/><NewLine>and followed the build instructions. Tried with both release and debug build mode</p><NewLine><p>When I ran ninja test, it gave segfault for the tests.</p><NewLine><p>I am running on Ubuntu18.04.02</p><NewLine><p>I am missing something?</p><NewLine><pre><code class=""lang-auto"">The following tests FAILED:<NewLine>	  1 - BackendCorrectnessTest (SEGFAULT)<NewLine>	  2 - BackendTest (SEGFAULT)<NewLine>	  6 - DeviceManagerTest (SEGFAULT)<NewLine>	 11 - GradCheckTest (SEGFAULT)<NewLine>	 17 - HyphenTest (SEGFAULT)<NewLine>	 21 - MLTest (SEGFAULT)<NewLine>	 25 - OperatorTest (SEGFAULT)<NewLine>	 28 - QuantizationTest (SEGFAULT)<NewLine>	 29 - TensorsTest (SEGFAULT)<NewLine>	 31 - TraceEventsTest (SEGFAULT)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Andre_Chang,(Andre Chang),Andre_Chang,"May 11, 2019,  1:33pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Andre, no idea what’s going wrong here without additional details. Can you run one of these tests, e.g. OperatorTest, perhaps with a debugger, and give more details, like a backtrace?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: May 27, 2019, 12:21am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
45342,Compiling Tests for RISC-V,2019-05-15T19:40:47.320Z,2,618,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to have tests be created for RISC-V. In particular I’m looking at the GemmTest. From what I understand because GemmTest depends on libjit_matmul_f I should be building libjit targeting RISC-V. However, doing only that does not result in the test/GemmTest binary being built for RISC-V. Is there something else I should be doing?</p><NewLine><p>From the build log I can see that GemmTest gets compiled twice. The first time GemmTest.cpp is compiled into GemmTest.cpp.o. Then again later on, after some supporting libraries are built I assume, GemmTest.cpp.o is compiled into the executable GemmTest binary. Would someone be kind enough to explain what each compilation step does, and which step(s) I should be looking at in order to build GemmTest for another target?</p><NewLine></div>",https://discuss.pytorch.org/u/derekz.tu,(Derek Tu),derekz.tu,"May 15, 2019,  7:40pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey Derek,</p><NewLine><p>GemmTest is actually a pretty straightforward CMake executable, so you can ignore a bunch of the libjit complexity.  The bits of CMake that are most interesting here are:</p><NewLine><ul><NewLine><li>the GemmTest definition itself: <a href=""https://github.com/pytorch/glow/blob/master/tests/unittests/CMakeLists.txt#L129"" rel=""nofollow noopener"">https://github.com/pytorch/glow/blob/master/tests/unittests/CMakeLists.txt#L129</a><NewLine></li><NewLine><li>the CPURuntimeNative library which compiles libjit as a typical library: <a href=""https://github.com/pytorch/glow/blob/master/lib/Backends/CPU/CMakeLists.txt#L65"" rel=""nofollow noopener"">https://github.com/pytorch/glow/blob/master/lib/Backends/CPU/CMakeLists.txt#L65</a><NewLine></li><NewLine></ul><NewLine><p>Do note that these targets are disabled when building with MSVC; I don’t remember why since I haven’t built on Windows myself.</p><NewLine><p>Bert</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Bert,</p><NewLine><p>Thanks for the reply. So from these two CMakeLists.txt I can see that GemmTest depends on CPURuntimeNative, etc, and CPURuntimeNative is built using the libjit files. I know for the libjit files you can build them for a different architecture by<br/><NewLine>adding ""-target "" into: <a href=""https://github.com/pytorch/glow/blob/71f2ec208b1306f422da7fb1562254eb225d32ae/lib/Backends/CPU/CMakeLists.txt#L16"" rel=""nofollow noopener"">https://github.com/pytorch/glow/blob/71f2ec208b1306f422da7fb1562254eb225d32ae/lib/Backends/CPU/CMakeLists.txt#L16</a></p><NewLine><p>However, I am unable to find such an option/figure out what to change that will allow me to have GemmTest be compiled for another architecture ie ARM or RISC-V. Is this possible? Or would it better for me to write my own test case? In which case again are there any pointers on how I would build it for another architecture already supported by the LLVM?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah, I see the confusion - the cmake options you’re pointing to are for building libjit as bytecode, to be later loaded by the Glow runtime.  You want to configure the cmake project for cross-compilation to build a RISC-V binary</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yep is it possible to configure the cmake project for cross-compilations to build a RISC-V binary? Or would it be better for me to try and emit a RISC-V bundle through model-runner/image-classifier?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can definitely configure CMake to do cross compilation though I’ve not done it with Glow myself (here’s, e.g., an example of how to cross-compile llvm/clang itself: <a href=""https://llvm.org/docs/HowToCrossCompileLLVM.html"" rel=""nofollow noopener"">https://llvm.org/docs/HowToCrossCompileLLVM.html</a>).  The main things to do are to set --target and --sysroot appropriately in your CMAKE_CXX_FLAGS.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Bert_Maher; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/derekz.tu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Bert_Maher; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/derekz.tu; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Bert_Maher; <NewLine> ,"REPLY_DATE 1: May 15, 2019,  8:00pm; <NewLine> REPLY_DATE 2: May 15, 2019,  9:28pm; <NewLine> REPLY_DATE 3: May 15, 2019,  9:50pm; <NewLine> REPLY_DATE 4: May 15, 2019, 11:31pm; <NewLine> REPLY_DATE 5: May 16, 2019,  3:51am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
44061,Unable to compile for targets other than x86_64,2019-05-01T00:32:44.064Z,2,283,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have tried emitting bundles for targets other than <code>x86_64</code> to no avail. Even <code>i386</code> is failing. It has something to do with tensor types. I suppose different targets expect/support different tensor types; however, I made sure that my network is using Float32. So this shouldn’t be a problem for <code>i386</code>. The network looks like this:</p><NewLine><pre><code class=""lang-auto"">graph(%input_data : Float(1, 2)<NewLine>      %1 : Float(8, 2)<NewLine>      %2 : Float(8)<NewLine>      %3 : Float(8, 8)<NewLine>      %4 : Float(8)<NewLine>      %5 : Float(8, 8)<NewLine>      %6 : Float(8)<NewLine>      %7 : Float(1, 8)<NewLine>      %8 : Float(1)) {<NewLine>  %9 : Float(1, 8) = onnx::Gemm[alpha=1, beta=1, transB=1](%input_data, %1, %2), scope: FeedForwardNN/Linear<NewLine>  %10 : Float(1, 8) = onnx::Relu(%9), scope: FeedForwardNN/ReLU<NewLine>  %11 : Float(1, 8) = onnx::Gemm[alpha=1, beta=1, transB=1](%10, %3, %4), scope: FeedForwardNN/ReLU<NewLine>  %12 : Float(1, 8) = onnx::Relu(%11), scope: FeedForwardNN/ReLU<NewLine>  %13 : Float(1, 8) = onnx::Gemm[alpha=1, beta=1, transB=1](%12, %5, %6), scope: FeedForwardNN/ReLU<NewLine>  %14 : Float(1, 8) = onnx::Relu(%13), scope: FeedForwardNN/ReLU<NewLine>  %15 : Float(1, 1) = onnx::Gemm[alpha=1, beta=1, transB=1](%14, %7, %8), scope: FeedForwardNN/ReLU<NewLine>  %output : Float(1, 1) = onnx::Sigmoid(%15), scope: FeedForwardNN/Sigmoid<NewLine>  return (%output);<NewLine>}<NewLine></code></pre><NewLine><p>The traceback that I get when trying to build is:</p><NewLine><pre><code class=""lang-auto"">(base) wny@wny-Macbook ~/sandbox/nn/xgmr/build/debug/bin $ ./x-model-runner -model=/Users/wny/sandbox/nn/pytorch_tuts/ffnn.onnx -network-name=x -model-input-name=input_data -cpu -emit-bundle=../output -target i386<NewLine>Assertion failed: ((i &gt;= FTy-&gt;getNumParams() || FTy-&gt;getParamType(i) == args[i]-&gt;getType()) &amp;&amp; ""Calling a function with a bad signature: argument type mismatch.""), function createCall, file ../../lib/LLVMIRCodeGen/LLVMIRGen.cpp, line 558.<NewLine>0  x-model-runner       0x000000010721e0bc llvm::sys::PrintStackTrace(llvm::raw_ostream&amp;) + 37<NewLine>1  x-model-runner       0x000000010721e4be SignalHandler(int) + 192<NewLine>2  libsystem_platform.dylib 0x00007fff713b7b3d _sigtramp + 29<NewLine>3  libsystem_platform.dylib 0x000000011a0f8938 _sigtramp + 2832469528<NewLine>4  libsystem_c.dylib        0x00007fff712751c9 abort + 127<NewLine>5  libsystem_c.dylib        0x00007fff7123d868 basename_r + 0<NewLine>6  x-model-runner       0x00000001072c7136 glow::LLVMIRGen::createCall(llvm::IRBuilder&lt;llvm::ConstantFolder, llvm::IRBuilderDefaultInserter&gt;&amp;, llvm::Function*, llvm::ArrayRef&lt;llvm::Value*&gt;) + 422<NewLine>7  x-model-runner       0x00000001072d04e3 glow::LLVMIRGen::generateLLVMIRForInstr(llvm::IRBuilder&lt;llvm::ConstantFolder, llvm::IRBuilderDefaultInserter&gt;&amp;, glow::Instruction const*) + 1603<NewLine>8  x-model-runner       0x0000000107272111 glow::CPULLVMIRGen::generateLLVMIRForInstr(llvm::IRBuilder&lt;llvm::ConstantFolder, llvm::IRBuilderDefaultInserter&gt;&amp;, glow::Instruction const*) + 2113<NewLine>9  x-model-runner       0x00000001072c92ed glow::LLVMIRGen::generateLLVMIRForModule(llvm::IRBuilder&lt;llvm::ConstantFolder, llvm::IRBuilderDefaultInserter&gt;&amp;) + 445<NewLine>10 x-model-runner       0x00000001072718c8 glow::CPULLVMIRGen::generateLLVMIRForModule(llvm::IRBuilder&lt;llvm::ConstantFolder, llvm::IRBuilderDefaultInserter&gt;&amp;) + 40<NewLine>11 x-model-runner       0x00000001072c448b glow::LLVMIRGen::performCodeGen() + 139<NewLine>12 x-model-runner       0x000000010727c2c2 glow::BundleSaver::save(llvm::StringRef, llvm::StringRef, llvm::StringRef, llvm::SmallVectorImpl&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; const&amp;, llvm::StringRef, llvm::StringRef) <NewLine>+ 450<NewLine>13 x-model-runner       0x00000001072eccd0 glow::LLVMBackend::save(glow::Function*, llvm::StringRef, llvm::StringRef) const + 832<NewLine>14 x-model-runner       0x00000001071124a4 glow::ExecutionEngine::save(glow::Function*, glow::CompilationOptions const&amp;, llvm::StringRef, llvm::StringRef) + 148<NewLine>15 x-model-runner       0x0000000106f27e6f glow::Loader::compile(glow::PlaceholderBindings&amp;) + 2415<NewLine>16 x-model-runner       0x0000000106f4cf85 buildNetwork(glow::Loader&amp;, glow::Type const*) + 117<NewLine>17 x-model-runner       0x0000000106f4d4a2 main + 242<NewLine>18 libdyld.dylib            0x00007fff711cced9 start + 1<NewLine>Abort trap: 6<NewLine></code></pre><NewLine><p>Ideally, I’d like to be able to compile for the ARM targets (and others supported by LLVM), but understanding why compilation fails even for <code>i386</code> would be a progress.</p><NewLine></div>",https://discuss.pytorch.org/u/wny,,wny,"May 1, 2019, 12:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for checking this out! Please <a href=""https://github.com/pytorch/glow/issues/new"" rel=""nofollow noopener"">post this as an issue on Github here</a> and we can get the right people to take a look.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok, for the benefit of those who might hit this issue, it is being tracked here: <a href=""https://github.com/pytorch/glow/issues/2836"" rel=""nofollow noopener"">https://github.com/pytorch/glow/issues/2836</a>.</p><NewLine><p>To summarize: essentially the CPU backend kernels, which is <code>libjit</code> in <code>backends/CPU/libjit</code> has to be compiled for the correct architecture. This can be achieved by adding appropriate flags in <code>backends/CPU/CMakeLIsts.txt</code> for the <code>CPURuntimeCompilationOptions</code>. The sources in <code>libjit</code> may have to be modified/customized for the specific architecture (e.g. some includes may need to be changed or removed).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/wny; <NewLine> ,"REPLY_DATE 1: May 1, 2019,  2:28pm; <NewLine> REPLY_DATE 2: May 2, 2019,  6:36pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
42772,Using glow with model made using PyTorch Python API,2019-04-16T21:27:50.923Z,2,609,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a model defined and trained using the Python API of PyTorch. I want to optimize it for inference. How exactly do I proceed?</p><NewLine><p>I can’t seem to find any documentation or examples (the ones on the glow repo are cpp only).</p><NewLine></div>",https://discuss.pytorch.org/u/chauhankaranraj,(Karanraj Chauhan),chauhankaranraj,"April 16, 2019,  9:27pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I want to optimize it for inference.</p><NewLine></blockquote><NewLine><p>I’m not sure what this means. Do you have a specific Glow backend you want to run on?</p><NewLine><blockquote><NewLine><p>I can’t seem to find any documentation or examples (the ones on the glow repo are cpp only).</p><NewLine></blockquote><NewLine><p>To use Glow you need to use C++, at least to build the project and use it out of the box. This may or may not be possible depending on your model – for example we have an <code>image-classifier</code> driver for running on any of our backends, so as long as you can get your model in ONNX or Caffe2 protobuf form we can load it there.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""42772""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v3/letter/j/ce73a5/40.png"" width=""20""/> jfix:</div><NewLine><blockquote><NewLine><p>I’m not sure what this means. Do you have a specific Glow backend you want to run on?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Apologies if I’m being vague. I want to reduce the inference time so I can process more frames per second (it’s an image processing project). I will be running this on CPU.</p><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""42772""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v3/letter/j/ce73a5/40.png"" width=""20""/> jfix:</div><NewLine><blockquote><NewLine><p>so as long as you can get your model in ONNX or Caffe2 protobuf form we can load it there.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I have exported the model to ONNX using torch.onnx.export. How should I load it?</p><NewLine><p>Thanks for your help. Much appreciated.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""42772""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v3/letter/c/f17d59/40.png"" width=""20""/> chauhankaranraj:</div><NewLine><blockquote><NewLine><p>I have exported the model to ONNX using torch.onnx.export. How should I load it?</p><NewLine></blockquote><NewLine></aside><NewLine><p>You can follow the directions on our docs – for example for running image classification models you may be able to use our <code>image-classifier</code>. You can find more info <a href=""https://github.com/pytorch/glow/blob/master/docs/Testing.md#model-loader"" rel=""nofollow noopener"">here</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/chauhankaranraj; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: April 17, 2019,  5:38pm; <NewLine> REPLY_DATE 2: April 17, 2019,  7:31pm; <NewLine> REPLY_DATE 3: April 18, 2019,  2:45am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
42693,No tensor registered with name,2019-04-16T05:27:15.739Z,4,356,"<div class=""post"" itemprop=""articleBody""><NewLine><p>From <code>model-runner</code> I get the error <code>ProtobufLoader.cpp line: 33 message: There is no tensor registered with name input_data</code>. My graph (ONNX) looks like this:</p><NewLine><pre><code class=""lang-auto"">graph(%input_data : Float(1, 2)<NewLine>      %1 : Float(8, 2)<NewLine>      %2 : Float(8)<NewLine>      %3 : Float(8, 8)<NewLine>      %4 : Float(8)<NewLine>      %5 : Float(8, 8)<NewLine>      %6 : Float(8)<NewLine>      %7 : Float(1, 8)<NewLine>      %8 : Float(1)) {<NewLine>  %9 : Float(1, 8) = onnx::Gemm[alpha=1, beta=1, transB=1](%input_data, %1, %2), scope: FeedForwardNN/Linear<NewLine>  %10 : Float(1, 8) = onnx::Relu(%9), scope: FeedForwardNN/ReLU<NewLine>  %11 : Float(1, 8) = onnx::Gemm[alpha=1, beta=1, transB=1](%10, %3, %4), scope: FeedForwardNN/ReLU<NewLine>  %12 : Float(1, 8) = onnx::Relu(%11), scope: FeedForwardNN/ReLU<NewLine>  %13 : Float(1, 8) = onnx::Gemm[alpha=1, beta=1, transB=1](%12, %5, %6), scope: FeedForwardNN/ReLU<NewLine>  %14 : Float(1, 8) = onnx::Relu(%13), scope: FeedForwardNN/ReLU<NewLine>  %15 : Float(1, 1) = onnx::Gemm[alpha=1, beta=1, transB=1](%14, %7, %8), scope: FeedForwardNN/ReLU<NewLine>  %output : Float(1, 1) = onnx::Sigmoid(%15), scope: FeedForwardNN/Sigmoid<NewLine>  return (%output);<NewLine>}<NewLine></code></pre><NewLine><p>The command line looks like this:</p><NewLine><p><code>/bin $ ./model-runner -model=./ffnn.onnx -network-name=""ffnn"" -cpu -emit-bundle=./ -verbose</code></p><NewLine><p>I have confirmed that the graph (ONNX) is constructed correctly by importing it in Python, then using the <code>caffe2</code> backend to run inference.</p><NewLine><p>I have also tried creating the <code>init_net.pb</code> and <code>predict_net.pb</code>, but I get the same error when specifying <code>*.pb</code> instead of <code>*.onnx</code></p><NewLine><p>I am guessing this has something to do with not specifying the input for <code>model-runner</code> (the way we do, for instance, with <code>image-classifier</code>). But then again, there is no way to specify input for <code>model-runner</code>.</p><NewLine><p>Any ideas would be appreciated.</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/wny,,wny,"April 16, 2019,  6:44am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I am guessing this has something to do with not specifying the input for  <code>model-runner</code>  (the way we do, for instance, with  <code>image-classifier</code> ). But then again, there is no way to specify input for  <code>model-runner</code> .</p><NewLine></blockquote><NewLine><p>Yeah sorry for the confusion – you’ve diagnosed the problem correctly. As I noted in <a href=""https://discuss.pytorch.org/t/glow-with-generic-model-builder/42559/3?u=jfix"">my other reply in the other thread</a>, the ModelRunner is more of a toy, and your model must not have any inputs, and exactly one output.</p><NewLine><p>I would suggest creating your own model loader/runner, perhaps initially based on <code>ModelRunner</code> since it’s the simplest, which is customized for your model. It would have a single input with name <code>input_data</code> and a single output named <code>output</code>. You can look at <code>ImageClassifier</code> to see how it creates its <code>Caffe2ModelLoader</code>/<code>ONNXModelLoader</code> with <code>inputName</code> along with a Tensor Type <code>inputImageType</code>, and then calls <code>updateInputPlaceholders()</code> to update the Tensor for the input before running.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Got it. Thanks! It sounds to me like one could write a base class that clients could subclass from, and override some methods to create custom inputs, to build their own custom models. This can then be made part of the Glow distribution.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah this was partially the purpose of the <code>Loader</code> class which <code>ModelRunner</code>/<code>ImageClassifier</code>/<code>TextTranslator</code> derive from. Perhaps more things could be pulled into <code>Loader</code>, or into <code>Caffe2ModelLoader</code>/<code>ONNXModelLoader</code>. If you have any ideas we always welcome PRs <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yep, I think there is quite a bit of glue logic that can be encapsulated away. I may give it some thought later, once I have more experience with glow.</p><NewLine><p>A bit off topic: my impression is that currently there is really only the cpu backend; is this correct? In that case, does Glow perform optimizations like taking advantage of CPU SIMD architecture? In particular quantization would make the network amenable to that sort of optimization it seems. If so, can we verify those optimizations by dumping low level IR or even LLVM asm output (the latter would probably be more appropriate in this case).</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>A bit off topic: my impression is that currently there is really only the cpu backend; is this correct?</p><NewLine></blockquote><NewLine><p>We have an OpenCL backend that is also under development, but it hasn’t had a ton of work done recently. We additionally have the Habana backend, but that is only useful if you have a Habana accelerator <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/>. Lastly we have the Interpreter backend, but it’s intended as a reference implementation and not super performant.</p><NewLine><blockquote><NewLine><p>In that case, does Glow perform optimizations like taking advantage of CPU SIMD architecture? In particular quantization would make the network amenable to that sort of optimization it seems. If so, can we verify those optimizations by dumping low level IR or even LLVM asm output (the latter would probably be more appropriate in this case).</p><NewLine></blockquote><NewLine><p>Yes, the CPU backend supports vectorized implementations for most of our op kernels. This is not visible in Glow’s low-level IR, but it is visible in LLVM IR and asm. There are two useful command-line options to use here: <code>-dump-llvm-ir</code> and <code>-dump-llvm-asm</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/wny; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/wny; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: April 16, 2019,  5:13pm; <NewLine> REPLY_DATE 2: April 16, 2019,  5:23pm; <NewLine> REPLY_DATE 3: April 16, 2019,  5:38pm; <NewLine> REPLY_DATE 4: April 16, 2019,  8:11pm; <NewLine> REPLY_DATE 5: April 16, 2019,  8:59pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
42559,Glow with generic model builder,2019-04-14T17:50:00.942Z,1,678,"<div class=""post"" itemprop=""articleBody""><NewLine><p>There are plenty of examples for using the image-classifier builder, but I have not found any examples on using the generic model builder. In particular:</p><NewLine><ol><NewLine><li>What would be the input format to the generic builder?</li><NewLine><li>I would guess that I would have to write my own input handler. Where would I do that?</li><NewLine><li>Relating to 2, is there a high level class that I can subclass from to create my own builder?</li><NewLine></ol><NewLine><p>Ultimately, is there a barebones example somewhere?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/wny,,wny,"April 14, 2019,  5:50pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hate answering my own question, but here goes… (I still have more questions below, though):</p><NewLine><p>From Glow’s <code>tools/ModelRunner.cpp</code>:</p><NewLine><pre><code class=""lang-cpp"">int main(int argc, char **argv) {<NewLine>  PlaceholderBindings bindings;<NewLine>  // The loader verifies/initializes command line parameters, and initializes<NewLine>  // the ExecutionEngine and Function.<NewLine>  Loader loader(argc, argv);<NewLine><NewLine>  // Create the model based on the input net, and get SaveNode for the output.<NewLine>  std::unique_ptr&lt;ProtobufLoader&gt; LD;<NewLine>  if (!loader.getCaffe2NetDescFilename().empty()) {<NewLine>    LD.reset(new Caffe2ModelLoader(loader.getCaffe2NetDescFilename(),<NewLine>                                   loader.getCaffe2NetWeightFilename(), {}, {},<NewLine>                                   *loader.getFunction()));<NewLine>  } else {<NewLine>    LD.reset(new ONNXModelLoader(loader.getOnnxModelFilename(), {}, {},<NewLine>                                 *loader.getFunction()));<NewLine>  }<NewLine>  Placeholder *output = EXIT_ON_ERR(LD-&gt;getSingleOutput());<NewLine>  auto *outputT = bindings.allocate(output);<NewLine><NewLine>  // Compile the model, and perform quantization/emit a bundle/dump debug info<NewLine>  // if requested from command line.<NewLine>  loader.compile(bindings);<NewLine><NewLine>  // If in bundle mode, do not run inference.<NewLine>  if (!emittingBundle()) {<NewLine>    loader.runInference(bindings);<NewLine><NewLine>    llvm::outs() &lt;&lt; ""Model: "" &lt;&lt; loader.getFunction()-&gt;getName() &lt;&lt; ""\n"";<NewLine><NewLine>    // Print out the result of output operator.<NewLine>    outputT-&gt;getHandle().dump();<NewLine><NewLine>    // If profiling, generate and serialize the quantization infos now that we<NewLine>    // have run inference to gather the profile.<NewLine>    if (profilingGraph()) {<NewLine>      loader.generateAndSerializeQuantizationInfos(bindings);<NewLine>    }<NewLine>  }<NewLine><NewLine>  return 0;<NewLine>}<NewLine></code></pre><NewLine><p>It looks like the generic model-runner does not accept input. Compare this with <code>tools/ImageClassifier.cpp</code>. We can probably modify it to accept custom input, or build on top of model-runner to create custom builders. My question, however, is this:</p><NewLine><p><em>Since model-runner does not accept input, how does it perform quantization (if asked) and other optimizations?</em></p><NewLine><p>In particular, documentation talks about the backend “watching” network activation as data flows through it, and makes decisions about which optimizations to make and how to correctly quantize the network.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Correct – ModelRunner does not accept input. It assumes a model with no inputs and a single output.</p><NewLine><blockquote><NewLine><ol start=""2""><NewLine><li>I would guess that I would have to write my own input handler. Where would I do that?</li><NewLine><li>Relating to 2, is there a high level class that I can subclass from to create my own builder?<br/><NewLine>Ultimately, is there a barebones example somewhere?</li><NewLine></ol><NewLine></blockquote><NewLine><p>You’d need to create your own version that uses a specific number of inputs/outputs given your use case, as well as load the inputs yourself into the input tensors. I think the most barebones example we have is ModelRunner. You then could look to ImageClassifier to observe how we are loading in inputs and outputs. TextTranslator also exhibits multiple inputs and outputs for a model. I.e. for each, you can take a look at calls to create <code>Caffe2ModelLoader</code>/<code>ONNXModelLoader</code> to see how to use multiple inputs/outputs, how they’re used later, etc.</p><NewLine><blockquote><NewLine><p><em>Since model-runner does not accept input, how does it perform quantization (if asked) and other optimizations?</em></p><NewLine></blockquote><NewLine><p>Good question – it would only be able to quantize given the Constant inputs already specified. However, the ModelRunner is really there as a simple tester for operator support, and is not used for serious models where you would want to gather a profile across many different inputs and then test accuracy across some test set.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wny; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: April 16, 2019,  6:46am; <NewLine> REPLY_DATE 2: April 16, 2019,  5:03pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
40570,Dump value from quantized weight,2019-03-22T01:31:54.368Z,0,247,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In “<a href=""https://github.com/pytorch/glow/blob/master/docs/AOT.md#creating-standalone-executable-bundles"" rel=""nofollow noopener"">https://github.com/pytorch/glow/blob/master/docs/AOT.md#creating-standalone-executable-bundles</a>”</p><NewLine><p>We could create an quantized weightfile “.weight”. I wonder how to dump value from this file?  Thx</p><NewLine></div>",https://discuss.pytorch.org/u/Ken,,Ken,"March 22, 2019,  1:31am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ken"">@Ken</a>,</p><NewLine><p>I don’t think there’s a good way to dump the values from the <code>.weights</code> file out of the box. It’s a binary blob of the weights. You could look at one of the <code>main.cpp</code> drivers mentioned <a href=""https://github.com/pytorch/glow/blob/master/docs/AOT.md#a-step-by-step-example-of-the-resnet50-network-model"" rel=""nofollow noopener"">here</a>, e.g. the Resnet50 bundle’s <a href=""https://github.com/pytorch/glow/tree/master/examples/bundles/resnet50/main.cpp"" rel=""nofollow noopener"">main.cpp</a>, and try to inspect it somehow when loading it in. However it would probably be better/easier to modify <a href=""https://github.com/pytorch/glow/tree/master/lib/LLVMIRCodeGen/BundleSaver.cpp"" rel=""nofollow noopener"">BundleSaver.cpp</a> when you’re generating the <code>.weights</code> file to inspect it then.</p><NewLine><p>Thanks,<br/><NewLine>Jordan</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: March 25, 2019,  4:35pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
39593,Object detector uisng Glow,2019-03-12T07:10:40.695Z,0,415,"<div class=""post"" itemprop=""articleBody""><NewLine><p>HI All,</p><NewLine><p>I am planning to port MObilenetV2 SSD object detector using GLOW on my RK3399 ARM64 board.<br/><NewLine>Can anyone guide me to relevant document/sample code on porting a detector using GLOW,<br/><NewLine>I have converted mobilenet ssd .pth model to .onnx model. Now want to use this ONNX model using GLOW</p><NewLine><p>Regards<br/><NewLine>Pallab Sarkar</p><NewLine></div>",https://discuss.pytorch.org/u/palcode,(Pallab Sarkar),palcode,"March 12, 2019,  7:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/palcode"">@palcode</a> – I believe this will require supporting a new <a href=""https://github.com/pytorch/glow/blob/master/docs/Testing.md#model-loader"" rel=""nofollow noopener"">loader</a>, likely somewhat similar to our existing <code>image-classifier</code>. Additionally, there may be operators that Glow and/or our ONNX importer do not support, so you would need to add support there, e.g. see <a href=""https://github.com/pytorch/glow/blob/master/docs/NewOperators.md#how-to-implement-a-new-operator-in-glow"" rel=""nofollow noopener"">this doc on adding new ops</a>.</p><NewLine><p>We definitely welcome contributions – if you’d like to add support for the model to Glow feel free to open an issue to track it, as well as open PRs with any relevant changes to complete the issue. Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: March 12, 2019,  3:23pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
38790,Is it possible to install Pytorch GPU+CUDA+cudnn in windows by Docker Image?,2019-03-03T11:32:02.987Z,2,2960,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have to deploy a model on windows and trouble to install some dependencies, so it is possible by Docker Image(No Cuda installed)?</p><NewLine></div>",https://discuss.pytorch.org/u/Yakin_Rubaiyat,(Yakin Rubaiyat),Yakin_Rubaiyat,"March 3, 2019, 11:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes you can deploy on windows using Docker. This is what makes docker so powerful. The method is same, pull the image and make a container to run it.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I doubt that you can use GPU+CUDA+cudnn in Docker on Windows. You need <a href=""https://github.com/NVIDIA/nvidia-docker"" rel=""nofollow noopener"">“NVIDIA Container Runtime for Docker”</a> which allows you to use the hosts GPU in your container. But on Windows Linux runs in a VM, which in turn has no access to the GPU. See also here: <a href=""https://github.com/NVIDIA/nvidia-docker/wiki/Frequently-Asked-Questions#is-microsoft-windows-supported"" rel=""nofollow noopener"">https://github.com/NVIDIA/nvidia-docker/wiki/Frequently-Asked-Questions#is-microsoft-windows-supported</a>.</p><NewLine><p>Running deep learning tasks in a VM can be very slow, because the Advanced Vector Extensions (AVX) commands are not available in your virtual host (at least on my Mac), which slows down the learning rate a lot.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>So even if you have nvidia driver installed on windows, you still will not be able to use GPU in docker? If I understand correctly, the instructions would become different when a linux host would try to communicate with a windows driver.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, you are not able to access the GPU from inside the container in Windows. This is, because the container runs inside a a stripped down Linux host which runs inside a virtual machine. And the virtual machine doesn’t have access to the GPU. You may experiment with <a href=""https://docs.docker.com/machine/drivers/vsphere/"" rel=""nofollow noopener"">VMWare vSphere as a docker-machine driver</a> and try using <a href=""https://blogs.vmware.com/apps/2018/09/using-gpus-with-virtual-machines-on-vsphere-part-2-vmdirectpath-i-o.html"" rel=""nofollow noopener"">Passthrough mechanism</a>.</p><NewLine><p>The situation is different when you run Linux on bare metal. Then you can access the GPU from inside the container when you run the instance with the nvidia-docker runtime.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I didn’t knew things are so complicated on Windows. But thanks for the learning experience.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Kushaj; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/OleRoel; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Kushaj; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/OleRoel; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Kushaj; <NewLine> ,"REPLY_DATE 1: March 3, 2019,  3:44pm; <NewLine> REPLY_DATE 2: March 4, 2019, 11:15pm; <NewLine> REPLY_DATE 3: March 5, 2019,  8:57am; <NewLine> REPLY_DATE 4: March 5, 2019,  9:35am; <NewLine> REPLY_DATE 5: March 5, 2019,  9:37am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 2 Likes; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
38925,Standalone executable bundles for training,2019-03-04T23:36:52.335Z,0,303,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The performance of Glow is quite impressive to me with a first very basic MNIST train run. Now I wanted to do more advanced train runs using CNNs and am wondering, if I now need to implement the CNN by hand in Glow or simply use the <code>image-classifier</code> as described in the glow <a href=""https://github.com/pytorch/glow/blob/master/docs/AOT.md"" rel=""nofollow noopener"">documentation</a>. The documentation only tells about using the  bundle for trained networks, or did I miss something?</p><NewLine></div>",https://discuss.pytorch.org/u/OleRoel,(Ole Roel),OleRoel,"March 4, 2019, 11:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You’re right – the bundles only support inference right now. We currently are more focused on inference over training. We have had questions about this before – see <a href=""https://github.com/pytorch/glow/issues/1595"" rel=""nofollow noopener"">this issue on GH</a> for a bit more details.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: March 5, 2019, 12:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
34206,[GLOW]Questions about using glow for training!,2019-01-09T03:15:24.301Z,2,936,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want use glow to train NN on a new chip. I wanna ask a few question about Training. Thanks for helping!</p><NewLine><ol><NewLine><li>In the example/mnist.cpp, I found the code use same data to train and infer, is that right?</li><NewLine><li>How do I specify a loss function before training? How do I know how many steps I should train before success?</li><NewLine><li>I can load a modle from caffee2 or onnx and use it to infer, But if I want to load a net from caffee2 or onnx and use it to train, what should I do? Should I modify the code of "" image-iclassifier"" to support train mode?</li><NewLine></ol><NewLine><p>Thank you very much for your help!</p><NewLine></div>",https://discuss.pytorch.org/u/xiaoshiyi,(xiaoqian),xiaoshiyi,"January 9, 2019,  3:16am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Fair warning, the Glow team is pretty focused on inference right now, so you’ll likely encounter some rough edges in training.  Keep in mind Glow currently has no way to save training results, so you’re limited to training and performing inference in a single process.  To answer your questions, though:</p><NewLine><blockquote><NewLine><ul><NewLine><li>In the example/mnist.cpp, I found the code use same data to train and infer, is that right?</li><NewLine></ul><NewLine></blockquote><NewLine><p>Yes, that’s currently what it’s doing.  There’s no reason for it though; you could easily separate the data into train and test sets.</p><NewLine><blockquote><NewLine><ul><NewLine><li>How do I specify a loss function before training? How do I know how many steps I should train before success?</li><NewLine></ul><NewLine></blockquote><NewLine><p>You’ll need to specify the loss function as part of the graph (e.g. Softmax, CrossEntryopyLoss, etc.).</p><NewLine><blockquote><NewLine><ul><NewLine><li>I can load a modle from caffee2 or onnx and use it to infer, But if I want to load a net from caffee2 or onnx and use it to train, what should I do? Should I modify the code of "" image-iclassifier"" to support train mode?</li><NewLine></ul><NewLine></blockquote><NewLine><p>So, we’ve never actually tried training from a c2 or onnx model.  In theory one could make it work but I’m not sure the loader knows how to create a graph for training.  (Specifically, I think it might make all the weights constant).  You can give this a shot by modifying image-classifier but I’d be very surprised if it works out-of-the-box.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply very much!</p><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""34206""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/bert_maher/40/7722_2.png"" width=""20""/> Bert_Maher:</div><NewLine><blockquote><NewLine><p>Keep in mind Glow currently has no way to save training results, so you’re limited to training and performing inference in a single process.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Glow can save_weights, But it has no way to save the model, which make supporting training pretty hard. Is there any way to modify the code to save the trained model?</p><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""34206""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/bert_maher/40/7722_2.png"" width=""20""/> Bert_Maher:</div><NewLine><blockquote><NewLine><p>Fair warning, the Glow team is pretty focused on inference right now</p><NewLine></blockquote><NewLine></aside><NewLine><p>I found nearly all AI compiler focused on inference other than training. Why? Is it too hard to support training? Or nobody need AI compiler for training?</p><NewLine><p>If I wanna make glow to be a backend for a AI framework such as Tensorflow, is it possible?</p><NewLine><p>Thank you very much!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""34206""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v2/letter/x/e47c2d/40.png"" width=""20""/> xiaoshiyi:</div><NewLine><blockquote><NewLine><p>Is there any way to modify the code to save the trained model?</p><NewLine></blockquote><NewLine></aside><NewLine><p>It absolutely can be done. It would just take some work <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/> But we would be more than happy to see improvements here, so please feel free to send PRs! See <a href=""https://github.com/pytorch/glow/issues/1714"" rel=""nofollow noopener"">this issue we have open</a>.</p><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""34206""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v2/letter/x/e47c2d/40.png"" width=""20""/> xiaoshiyi:</div><NewLine><blockquote><NewLine><p>I found nearly all AI compiler focused on inference other than training. Why? Is it too hard to support training? Or nobody need AI compiler for training?</p><NewLine></blockquote><NewLine></aside><NewLine><p>One reason AI compilers might not support training is that inference is just an easier place to start. That doesn’t mean training is too hard, though. We would like to eventually expand our training support.</p><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""34206""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v2/letter/x/e47c2d/40.png"" width=""20""/> xiaoshiyi:</div><NewLine><blockquote><NewLine><p>If I wanna make glow to be a backend for a AI framework such as Tensorflow, is it possible?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, as long as you can get it into a format Glow can import, such as ONNX proto. See <a href=""https://github.com/pytorch/glow/issues/2423"" rel=""nofollow noopener"">this post</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Bert_Maher; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xiaoshiyi; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: January 9, 2019,  6:23pm; <NewLine> REPLY_DATE 2: January 26, 2019, 10:17am; <NewLine> REPLY_DATE 3: March 5, 2019, 12:00am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
36721,Glow internship/trainee opportunities,2019-02-08T19:00:36.884Z,0,344,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Are there internship/trainee opportunities to work on Glow?</p><NewLine></div>",https://discuss.pytorch.org/u/Rinat_Shigapov,(Rinat Shigapov),Rinat_Shigapov,"February 8, 2019,  7:00pm",,,,,
34994,How can i get more image examples in tests/image/imagenet,2019-01-18T08:02:29.645Z,2,249,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! I want to get more images which is png files in tests/image/imagenet but i don’t know how to get images.</p><NewLine><p>I tried to use imagenet files but my files are JPEG files, so it doesn’t work with ./image-classifier.</p><NewLine><p>thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/be3b7e87906db510acc8,(asdasd11x),be3b7e87906db510acc8,"January 18, 2019,  8:02am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, you should be able to convert the jpeg files to png pretty easily – e.g. you can try following directions <a href=""https://askubuntu.com/questions/457604/bulk-converting-images-from-one-format-to-another"" rel=""nofollow noopener"">here</a>, or google for how to do so.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for answering me.</p><NewLine><p>I tried to convert my JPEG file to png file by both ‘convert’ and 'mogrify command.<br/><NewLine>But when I execute the image-classifier, errors come again.</p><NewLine><p>my command : ./bin_opt/image-classifier tests/images/i.png -image-mode=0to255 -m=inception_v2 -model-input-name=data “$@” -opencl -time -iterations=1</p><NewLine><p>error:<br/><NewLine><span class=""hashtag"">#0</span> 0x0000000000628eba llvm::sys::PrintStackTrace(llvm::raw_ostream&amp;) (./bin_opt/image-classifier+0x628eba)<br/><NewLine><span class=""hashtag"">#1</span> 0x000000000062700e llvm::sys::RunSignalHandlers() (./bin_opt/image-classifier+0x62700e)<br/><NewLine><span class=""hashtag"">#2</span> 0x000000000062715e SignalHandler(int) (./bin_opt/image-classifier+0x62715e)<br/><NewLine><span class=""hashtag"">#3</span> 0x00007f7d435b2d10 __restore_rt (/lib/x86_64-linux-gnu/libpthread.so.0+0x10d10)<br/><NewLine><span class=""hashtag"">#4</span> 0x00000000004e89db glow::readPngImageAndPreprocess(llvm::StringRef, glow::ImageNormalizationMode, glow::ImageChannelOrder, glow::ImageLayout, bool) (./bin_opt/image-classifier+0x4e89db)<br/><NewLine><span class=""hashtag"">#5</span> 0x00000000004ba562 loadImagesAndPreprocess(llvm:<img alt="":cl:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/cl.png?v=6"" title="":cl:""/>:list&lt;std::string, bool, llvm:<img alt="":cl:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/cl.png?v=6"" title="":cl:""/>:parser<a>std::string</a> &gt; const&amp;, glow::Tensor*) (./bin_opt/image-classifier+0x4ba562)<br/><NewLine><span class=""hashtag"">#6</span> 0x00000000004ba955 main (./bin_opt/image-classifier+0x4ba955)<br/><NewLine><span class=""hashtag"">#7</span> 0x00007f7d429cba40 __libc_start_main /build/buildd/glibc-2.21/csu/libc-start.c:323:0<br/><NewLine><span class=""hashtag"">#8</span> 0x00000000004b5bb9 _start (./bin_opt/image-classifier+0x4b5bb9)</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t have enough info here to help debug… Can you compile in debug mode and run with a debugger, to try to print a more informative stack trace, e.g. with line numbers? Are you able to run with the test images that come with Glow (located at <code>tests/images/imagenet/</code>)?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/be3b7e87906db510acc8; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: January 18, 2019,  4:51pm; <NewLine> REPLY_DATE 2: January 20, 2019, 11:43pm; <NewLine> REPLY_DATE 3: January 21, 2019,  9:48pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
34760,[GLOW] Question about executing image-classifier,2019-01-16T02:23:09.482Z,0,462,"<div class=""post"" itemprop=""articleBody""><NewLine><p>hi i have a question about executing /bin/image-classifier.</p><NewLine><p>I built the glow successfully and ran the image-classifier.</p><NewLine><p>But it doesn’t work ;(</p><NewLine><p>I think build is successful because i check build test by ‘ninja test’ and all pass.</p><NewLine><p>Error code is :<br/><NewLine>Error at file /path/glow/lib/Importer/ONNXModelLoader.cpp line 172 “Can’t find the model or network files.”<br/><NewLine>Encountered an error, exiting.</p><NewLine><p>I already downloaded all models by utils/download_onnx_models.sh.</p><NewLine><p>What should I do ??? Plz help me !</p><NewLine></div>",https://discuss.pytorch.org/u/3ae8f0c86c640bb8e662,(enjoy_man99),3ae8f0c86c640bb8e662,"January 16, 2019,  2:24am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you’re running a caffe2 model you might need to also run utils/download_caffe2_models.sh.  What’s the full command line you are using to run image-classifier?  You need to specify the path to the models you’ve downloaded.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Bert_Maher; <NewLine> ,"REPLY_DATE 1: January 16, 2019,  4:34am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> 
33692,Unusual performance results for ResNet50,2019-01-03T15:29:10.519Z,0,523,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have found some very unusual behavior when comparing time of inference run for ResNet50 in image classifier for various backends. I’d very much appreciate any comment why such thing might have happened.</p><NewLine><p>My line looks like this:</p><NewLine><pre><code class=""lang-auto"">./bin/image-classifier &lt;testImages&gt; -use-imagenet-normalization -image_mode=0to1 -m=resnet50 -model_input_name=gpu_0/data -&lt;cpu|interpreter|opencl&gt; -time<NewLine></code></pre><NewLine><p>Where &lt;testImages&gt; is directory with 1/16/32/64 images depending on the tested batch size. To have a bit clearer picture I have used measured time to calculate images/second value.</p><NewLine><p>And now the strange part…</p><NewLine><p>For OCL I can see logarithmic increase, but for all other backends value is constant. It completely doesn’t matter how big batch size I throw at it, images/second value is the same (+/- small noise).</p><NewLine><p>On generated dot file everything looks reasonable, that is batch size is propagated.</p><NewLine><p>Have you also experienced similar behavior? Or maybe I’m just doing my measurements wrong?</p><NewLine></div>",https://discuss.pytorch.org/u/speryt,(Sebastian),speryt,"January 3, 2019,  3:29pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Below you can see chart presenting number of images/second with increase of batch size for each of backends.</p><NewLine><p><img alt=""chart"" height=""348"" src=""https://discuss.pytorch.org/uploads/default/original/2X/e/e24c35ce29b9d644d1278b63cf49370237492dfc.png"" width=""594""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Sebastian, I’m assuming this is because the OpenCL backend takes advantage of parallelism across the multiple images in the batch, whereas our CPU and Interpreter do not and are always running on a single CPU core. So I would expect to always see about the same images/second for the CPU and Interpreter, whereas the OpenCL backend will benefit from this parallelism up until whatever device you’re using has all of its parallel resources exhausted, at which point images/second would plateau. This appears to be what you are seeing here.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks Jordan! That makes a lot of sense now. I am almost sure that I read about that limitation. Apparently I must have forgot.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>By the way, there have been some discussions about multi-threaded CPU kernels – you can follow along in <a href=""https://github.com/pytorch/glow/issues/1749"" rel=""nofollow noopener"">GH issue #1749</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/speryt; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/speryt; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jfix; <NewLine> ,"REPLY_DATE 1: January 4, 2019, 12:25pm; <NewLine> REPLY_DATE 2: January 8, 2019,  9:52am; <NewLine> REPLY_DATE 3: January 8, 2019, 12:42pm; <NewLine> REPLY_DATE 4: January 8, 2019,  6:35pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> 
31457,Few questions about TensorView - Reshape difference,2018-12-06T15:20:06.932Z,0,485,"<div class=""post"" itemprop=""articleBody""><NewLine><p>While analyzing inferMixedNet unit test with OpenCL backend I noticed something strange/unexpected and I’d appreciate some explanation.</p><NewLine><p>Generated low-level IR looks as follows (code part only)</p><NewLine><pre><code class=""lang-auto"">code {<NewLine>  0 %tr_res = allocactivation  { Ty: float&lt;2 x 16 x 16 x 3&gt;} // size: 6144 // Users: @in 2, @out 5, @out 1<NewLine>  1 %tr = transpose @out %tr_res, @in %var { Shuffle: [0, 2, 3, 1]}<NewLine>  2 %tr_res2 = tensorview @in %tr_res { Ty: float&lt;2 x 768&gt;, Offsets: [0, 0, 0, 0]} // Users: @in 4<NewLine>  3 %fc_add_bias_res = allocactivation  { Ty: float&lt;2 x 16&gt;} // size: 128 // Users: @in 6, @out 4, @in 10, @out 9, @out 11, @in 9, @in 8, @out 6<NewLine>  4 %fc_dot = matmul @out %fc_add_bias_res, @in %tr_res2, @in %weights<NewLine>  5 %dealloc = deallocactivation @out %tr_res // size: 6144<NewLine>  6 %fc_add_bias = batchedadd @out %fc_add_bias_res, @in %fc_add_bias_res, @in %bias<NewLine>  7 %tanh_res = allocactivation  { Ty: float&lt;2 x 16&gt;} // size: 128 // Users: @in 13, @out 10, @out 14, @in 10, @out 8<NewLine>  8 %tanh = tanh @out %tanh_res, @in %fc_add_bias_res<NewLine>  9 %sig = sigmoid @out %fc_add_bias_res, @in %fc_add_bias_res<NewLine>  10 %add = elementadd @out %tanh_res, @in %tanh_res, @in %fc_add_bias_res<NewLine>  11 %dealloc3 = deallocactivation @out %fc_add_bias_res // size: 128<NewLine>  12 %fc_dot1_res = allocactivation  { Ty: float&lt;2 x 16&gt;} // size: 128 // Users: @in 16, @out 15, @out 17, @in 15, @out 13<NewLine>  13 %fc_dot1 = matmul @out %fc_dot1_res, @in %tanh_res, @in %weights1<NewLine>  14 %dealloc4 = deallocactivation @out %tanh_res // size: 128<NewLine>  15 %fc_add_bias1 = batchedadd @out %fc_dot1_res, @in %fc_dot1_res, @in %bias1<NewLine>  16 %SM = softmax @out %ret, @in %fc_dot1_res<NewLine>  17 %dealloc7 = deallocactivation @out %fc_dot1_res // size: 128<NewLine>}<NewLine></code></pre><NewLine><p>I have problems in understanding what TensorView operation is doing (at line <span class=""hashtag"">#2</span>)?<br/><NewLine>In OpenCL backend sources it is only used in allocateMemory, because during execute() loop it is nop. Also, the code in allocateMemory is not clear when it comes to this operation tbh.</p><NewLine><p>From what I see in IR it looks like this operation is behaving as <strong>reshape</strong> only changing the size of input. However, this introduced much more questions. Why not just use reshape instruction? What’s even more confusing is the fact that OCL backend in execute() doesn’t have Reshape. Actually, even there is no ReshapeInst (at least I couldn’t spot it in ClassGen files - only node). To add to this - dot file with this network has Reshape block in place where TensorView should be <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/></p><NewLine><p>I noticed same behavior even in cases where createReshape is explicitly written, e.g. in inferComplexNet1. Instead of Reshape instruction in IR I see TensorView instructions.</p><NewLine><p>To summarize, what I’d like to understand:</p><NewLine><ol><NewLine><li><NewLine><p>What TensorView instruction is doing?</p><NewLine></li><NewLine><li><NewLine><p>What is the difference between TensorView and Reshape?</p><NewLine></li><NewLine><li><NewLine><p>How I can get the output size of TensorView? In other words, how I can tell to what dimensions I should reshape my Tensor. Should it be something like TV-&gt;getTy()-&gt;getDims()?</p><NewLine></li><NewLine><li><NewLine><p>TensorView operation has only one operand - <span class=""mention"">@in</span> and no <span class=""mention"">@out</span>, yet in next instructions (in example above line <span class=""hashtag"">#4</span>) it is used as an input. How I can take <strong>the name of the output</strong>? Normally I’d do something like TV-&gt;getDest()-&gt;getName(), but here it’s not possible. Any suggestion?</p><NewLine></li><NewLine></ol><NewLine><p>Thanks for support!</p><NewLine></div>",https://discuss.pytorch.org/u/speryt,(Sebastian),speryt,"December 6, 2018,  3:20pm",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Good questions.  You’re exactly right that TensorView is behaving as a reshape (in fact, if you look at <a href=""https://github.com/pytorch/glow/blob/master/lib/IR/IRGen.cpp#L117"" rel=""nofollow noopener"">IRGen for Reshape</a> you can see that we generate TensorView for Reshape).</p><NewLine><p>TensorView is a generic IR instruction that represents taking a “view” into a memory region of a tensor without creating any new allocation. Reshaping is one example (change in type without change in data), another is slicing (looking into a subregion without copying the data).  Since it deals with underlying memory it doesn’t exist at the Graph level, only the IR level, which is why you see <code>Reshape</code> in the dot files but not in the IR.</p><NewLine><p>I’m not actually sure about getting the view size or “output” name of a TV.  I’d probably look at the generated class implementation in AutoGenInstr.h .  I also kind of agree the way we use <span class=""mention"">@out</span> params and names on the RHS in the IR is pretty confusing <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Sebastian – just to be super clear, as you noticed there is no Reshape Instruction. Reshape Nodes are always implemented via Tensorview Instructions during IRGen, as Bert noted/linked to. <a href=""https://github.com/pytorch/glow/blob/master/docs/IR.md#low-level-ir"" rel=""nofollow noopener"">IRGen generates Instructions (low-level) IR from the Node (high-level) IR</a>.</p><NewLine><p>Another example where you would see one Node implemented via different Instructions is the Concat node (<a href=""https://github.com/pytorch/glow/blob/master/lib/IR/IRGen.cpp#L249-L285"" rel=""nofollow noopener"">see the IRGen case here</a>). A Concat is implemented as one or more InsertTensor Instructions. There is no Concat Instruction.</p><NewLine><p>For getting the shape and name, the Instruction class derives from Value, and Value derives from both Named and Typed. So for the name, you should be able to do <code>TV-&gt;getName()</code>. And for the shape, you should be able to do <code>TV-&gt;dims()</code>. There should be example of both of these in use in <code>lib/Optimizer/IROptimizer.cpp</code>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for info. It helped a lot!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""31457""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v2/letter/s/6bbea6/40.png"" width=""20""/> speryt:</div><NewLine><blockquote><NewLine><p>What is the difference between TensorView and Reshape?</p><NewLine></blockquote><NewLine></aside><NewLine><p><code>torch.view</code>  has existed for a long time. It will return a tensor with the new shape. The returned tensor will share the underling data with the original tensor. See the <a href=""http://pytorch.org/docs/master/tensors.html?highlight=view#torch.Tensor.view"" rel=""nofollow noopener"">documentation here</a>.</p><NewLine><p>On the other hand, it seems that  <code>torch.reshape</code> . this method will</p><NewLine><blockquote><NewLine><p>Return a tensor with the same data and number of elements as input, but with the specified shape. When possible, the returned tensor will be a view of input. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.</p><NewLine></blockquote><NewLine><p>It means that  <code>torch.reshape</code>  may return a copy or a view of the original tensor. You can not count on that to return a view or a copy. According to the developer:</p><NewLine><blockquote><NewLine><p>if you need a copy use clone if you need the same storage use view. The semantics of reshape are that it may or may not share the storage and you don’t know beforehand.</p><NewLine></blockquote><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Bert_Maher; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jfix; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/speryt; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Beatrice_Paige; <NewLine> ,"REPLY_DATE 1: January 4, 2019, 12:27pm; <NewLine> REPLY_DATE 2: January 4, 2019, 12:27pm; <NewLine> REPLY_DATE 3: December 7, 2018,  2:30pm; <NewLine> REPLY_DATE 4: December 26, 2018,  1:35pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
31272,Using NCHW format as default for Glow,2018-12-04T15:37:12.836Z,1,600,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’d like to ask if there is any way (or plan for) adding to Glow ability to use NCHW by default instead of NHWC? For now it is said in some places that Glow’s default is NHWC, but at the same time, many times methods to generate NCHW layout are used.</p><NewLine><p>I strongly doubt that all of the backends will use NHWC, doing conversion instead - for example convertConvToNCHWConv. Such approach will significantly affect performance by introducing all of those Transpose operations. Instead it should be allowed from the very beginning to choose which format will be used. E.g. Tensorflow allows to chose how formatting will be done.</p><NewLine></div>",https://discuss.pytorch.org/u/speryt,(Sebastian),speryt,"December 4, 2018,  3:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You’re right that many backends will use NCHW, but the conversions usually have no runtime cost – the graph optimizer completely eliminates transposes that cancel.  I just brought up an experimental NCHW backend and no transposes were needed to run ResNet or ResNeXt :-).</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/bert_maher"">@Bert_Maher</a> thanks for info. That sound’s really interesting. If I get it correctly low-level IR, which is input for my backend, should not have transpose operation (or significantly reduced number of those), while at the same time using properly formatted convolution.</p><NewLine><p>So for example for convolution that works only with NCHW I should just use conversion approach from OpenCL and it should work with high and low level optimizations taking care of improving performance? That’s really impressive <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yep, that’s exactly right!  The OpenCL approach is a good place to look for the NCHW.  And please do let us know if you’re seeing transposes that aren’t eliminated with that approach; it’s always possible to improve the optimizer.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Bert_Maher; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/speryt; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Bert_Maher; <NewLine> ,"REPLY_DATE 1: December 4, 2018,  5:30pm; <NewLine> REPLY_DATE 2: December 4, 2018,  6:49pm; <NewLine> REPLY_DATE 3: January 4, 2019, 12:26pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
30950,[GLOW] Memory leaking in opencl and build issues,2018-11-30T11:48:23.968Z,3,592,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m having memory issues running the mnist and cifar10 examples, and I had problems compiling which might be related.</p><NewLine><p>The memory issue is that when I run e.g. the <em>cifar10</em> training example with -opencl, the <strong>GPU</strong> memory usage grows at a rate of 200MB per second (looking at nvidia-smi) until the lack of memory crashes the program.</p><NewLine><p>I tried updating my nvidia libraries and opencl libraries but the behaviour is the same.<br/><NewLine>Any ideas how to debug this problem? The <em>mnist</em> example actually makes it to the final predictions and they are right, so the program is running correctly in that respect.</p><NewLine><p>The building issues I had might be related, firstly for some reason llvm-link-6.0 was found by cmake while the rest finds llvm-7.0, so I had to manually set that to llvm-link-7 (I’m running debian btw.)<br/><NewLine>Then I had to make sure the code within the FACEBOOK_INTERNAL &amp;&amp; LLVM_VERSION_PATCH &lt; 20181009 is run, and not the regular llvm-7 code. It seems that the LLVM_VERSION_PATCH variable is not set while I apparently need it.</p><NewLine><p>Any help would be appreciated! The glow library seems otherwise perfect to use!</p><NewLine><p>-Marijn</p><NewLine></div>",https://discuss.pytorch.org/u/marijnfs,(Marijn Stollenga),marijnfs,"November 30, 2018,  1:55pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>What example are you talking about exactly? What is this <code>-opencl</code> option supposed to do?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi sorry I should be more clear, I’m talking in this case about the GLOW framework (which I guess is the backend of pytorch?). When you build that framework (from <a href=""https://github.com/pytorch/glow"" rel=""nofollow noopener"">https://github.com/pytorch/glow</a>) it builds several binaries that implement some simple training, but I get these memory leaks when I run it with ‘-opencl’ which is a flag that selects the opencl backend (as opposed to regular cpu backend).</p><NewLine><p>I’m running the latest nvidia driver and have tried different versions, because opencl driver support is notorious for such issues I believe, but I would really like to find a fix for this.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s not actually the backend for pytorch <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/> I thinks it’s more caffe2 related?<br/><NewLine>I’m not sure who is knowledgeable about this, <a class=""mention"" href=""/u/smth"">@smth</a> might know who to ask?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see, yeah I’m not sure where it fits in the ecosystem, just that they point to this discussion forum to discuss issues. There is an interesting talk about it BTW <a href=""https://www.youtube.com/watch?v=cTz7c5dn5Gc"" rel=""nofollow noopener"">https://www.youtube.com/watch?v=cTz7c5dn5Gc</a></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for reporting this – we can continue the discussion on the Github issue (<a href=""https://github.com/pytorch/glow/issues/2104"" rel=""nofollow noopener"">https://github.com/pytorch/glow/issues/2104</a>).</p><NewLine><p>Just to give you an idea of where Glow fits in the stack, it’s an optional, experimental backend for Caffe2.  It can also (sort of) be used as a standalone framework, but it’s really intended to sit underneath C2 and provide a backend for hardware accelerators.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/marijnfs; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/marijnfs; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Bert_Maher; <NewLine> ,"REPLY_DATE 1: November 30, 2018,  1:35pm; <NewLine> REPLY_DATE 2: November 30, 2018,  1:38pm; <NewLine> REPLY_DATE 3: November 30, 2018,  1:53pm; <NewLine> REPLY_DATE 4: November 30, 2018,  2:01pm; <NewLine> REPLY_DATE 5: November 30, 2018,  5:58pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
28452,Datatype of Bias (FP32?) in FP16 models,2018-10-31T14:02:17.059Z,0,310,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In Int8 models, bias term is int32,<br/><NewLine>In similar vein, in FP16 models,  does the bias term need higher precision (FP32)?</p><NewLine></div>",https://discuss.pytorch.org/u/jujubi,,jujubi,"October 31, 2018,  2:03pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Right now this is FP16 too, but it’s likely we will push higher precision as well here.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/qcolombet; <NewLine> ,"REPLY_DATE 1: November 21, 2018,  8:26pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
28844,FP16*FP16 in convolution generating FP32,2018-11-05T21:40:56.209Z,0,307,"<div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/qcolombet"">@qcolombet</a></p><NewLine><p>Looking at the FP16 implementation of convolution.<br/><NewLine>sum += float(<br/><NewLine>filterW.at({d, fx, fy, fd}) *<br/><NewLine>inW.at({n, (size_t)ox, (size_t)oy, g * inCperG + fd}));</p><NewLine><p>The filterW and inW are FP16.  sum is Float.</p><NewLine><p>The * defined in glow/Support/Float16.h returns FP16.</p><NewLine><p>Shouldnt we need to define a version of * that returns Float instead when multiplying 2 FP16 quantities, as we are accumulating in FP32?</p><NewLine></div>",https://discuss.pytorch.org/u/jujubi,,jujubi,"November 5, 2018,  9:40pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There is no right or wrong answer in that case. I can see some hardware doing the full float computation while others only accumulate in fp32 while effectively doing all the computations in fp16. The interpreter does the latter.</p><NewLine><p>The idea for the basic fp16 operators is that all the intermediate results are casted back to fp16.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/qcolombet; <NewLine> ,"REPLY_DATE 1: November 5, 2018, 10:42pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
19152,Multiple backends for single network,2018-06-04T08:19:11.390Z,1,786,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to use different backends for different layers in glow? For example, if network includes two layers Convolution-&gt;Softmax then is it possible to use OpenCL for Convolution layer and CPU for Softmax?</p><NewLine></div>",https://discuss.pytorch.org/u/prasshantg,(Prashant Gaikwad),prasshantg,"June 4, 2018,  8:19am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Since this question has been unanswered for a month, I will attempt to answer it. Glow developers/anyone else, please feel free to correct me wherever I go wrong.</p><NewLine><p>If I understand your question correctly, you are asking if one can use Glow to generate code for a parallel hardware so as to distribute compute and data, something similar to distributed TensorFlow/PyTorch. Is that accurate?</p><NewLine><p>If that is the case, you can model your system in OpenCL such that your CPU is the host and your parallel hardware — a GPU, a cluster of CPUs, or any accelerator, etc. — is the device (provided there are OpenCL drivers for that device). Then, you could layout your convolution kernel on the parallel device and do the softmax on your CPU (or another device). The OpenCL drivers will take care of copying the code and data. The memory model of your system (shared vs. distributed, for instance), communication links (PCIe vs. nvlink, etc.) will dictate the choice of your compute model, how your lay out code and data, the granularity level of your kernels, etc.</p><NewLine><p>The CPU backend as it is today (and what I perceive its intent to be) is to support running the network on a (single-core) CPU (X86, ARM, etc.).</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>The CPU backend as it is today (and what I perceive its intent to be) is to support running the network on a (single-core) CPU (X86, ARM, etc.).</p><NewLine></blockquote><NewLine><p>That is correct.</p><NewLine><p>That said, we are looking into adding support for partitioning the input across device.<br/><NewLine>See:<br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/glow/issues/1268"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/glow</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://github.com/bertmaher"" rel=""nofollow noopener""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars1.githubusercontent.com/u/4441820?v=2&amp;s=96"" width=""60""/><NewLine></a><NewLine><h4><a href=""https://github.com/pytorch/glow/issues/1268"" rel=""nofollow noopener"" target=""_blank"">Issue: Graph partitioning for multi-device execution</a></h4><NewLine><div class=""date"" style=""margin-top:10px;""><NewLine><div class=""user"" style=""margin-top:10px;""><NewLine>	opened by <a href=""https://github.com/bertmaher"" rel=""nofollow noopener"" target=""_blank"">bertmaher</a><NewLine>	on <a href=""https://github.com/pytorch/glow/issues/1268"" rel=""nofollow noopener"" target=""_blank"">2018-07-12</a><NewLine></div><NewLine><div class=""user""><NewLine></div><NewLine></div><NewLine><pre class=""content"" style=""white-space: pre-wrap;"">Partitioning a Function into multiple subgraphs breaks a lot of assumptions in our current architecture. PR #1176 shows a proof-of-concept for...</pre><NewLine><div class=""labels""><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/soumyarooproy; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/qcolombet; <NewLine> ,"REPLY_DATE 1: July 6, 2018,  9:39pm; <NewLine> REPLY_DATE 2: August 29, 2018, 12:11am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
