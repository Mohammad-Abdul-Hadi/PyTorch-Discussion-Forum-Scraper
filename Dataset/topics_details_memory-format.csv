id,title,created_at,reply_count,views,description,creator_link,creator_name,creator_alias,post_date,post_likes,replies,repliers_links,reply_dates,reply_likes
73663,About the Memory Format category,2020-03-18T17:32:35.898Z,0,116,"<div class=""post"" itemprop=""articleBody""><NewLine><p>(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)</p><NewLine><p>Use the following paragraphs for a longer description, or to establish category guidelines or rules:</p><NewLine><ul><NewLine><li><NewLine><p>Why should people use this category? What is it for?</p><NewLine></li><NewLine><li><NewLine><p>How exactly is this different than the other categories we already have?</p><NewLine></li><NewLine><li><NewLine><p>What should topics in this category generally contain?</p><NewLine></li><NewLine><li><NewLine><p>Do we need this category? Can we merge with another category, or subcategory?</p><NewLine></li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/albanD,(Alban D),albanD,"March 18, 2020,  5:32pm",,,,,
97241,Fastest way of dataloading large datasets,2020-09-23T10:22:33.286Z,2,49,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am working a classification problem, which includes the feature generation at first and then classification. Due to my problem constrain, i generate the features first and then train classifier separately. However, dataloading has become the bottleneck of my pipeline and progress.</p><NewLine><p>I save the generated datasamples as list of tensors, as .pt file having dimensions [(50,10,10,10), (1)], the last (1) being the associated label tensor.<br/><NewLine>I use a standard dataset and dataloader code:</p><NewLine><pre><code class=""lang-auto"">class LR_Dataset(Dataset):<NewLine>    <NewLine>    def __init__(self, filepath):<NewLine>        self.filepath = filepath<NewLine>        self.filenames = os.listdir(self.filepath)<NewLine><NewLine>    def __len__(self):<NewLine>    	return len(self.filenames)<NewLine><NewLine>    def __getitem__(self, idx):<NewLine>    	x,y = torch.load(os.path.join(self.filepath,self.filenames[idx]))<NewLine>    	return x,y<NewLine>        <NewLine><NewLine>def dataloader(filepath, batch_size, num_workers=0):<NewLine>    dataset = LR_Dataset(filepath)<NewLine>    return DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)<NewLine><NewLine></code></pre><NewLine><p>Note that: the dataset folder contains ~150,000 .pt files. So i doubt if reading .pt file is taking the time, or is it because the .pt files contain tensors and not numpy arrays.</p><NewLine><p>I am facing a tough deadline. Any help would be greatly appreciated.</p><NewLine><p>Thank you</p><NewLine></div>",https://discuss.pytorch.org/u/Dipayan_Das,(Dipayan Das),Dipayan_Das,"September 23, 2020, 10:23am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a></p><NewLine><p>Please help! <img alt="":pray:t2:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/pray/2.png?v=9"" title="":pray:t2:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think there is any overhead from using tensors instead of np arrays.</p><NewLine><p>You could try increasing <code>num_workers &gt; 0</code> to use multiprocessing in <code>DataLoader</code></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>As <a class=""mention"" href=""/u/dipayan_das"">@Dipayan_Das</a> explained multiple workers might give you a speedup.<br/><NewLine>If that’s not helping, you could try to preload the complete dataset, which should take approx. 27GB, if my calculation is right.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot for considering, but I have a limited RAM of 16GB and 8GB GPU. Any other method other than loading whole data to RAM shall do. Additionally, i have indeed experimented with num_workers, however, the effect is not at all significant.</p><NewLine><p>Thank you</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Dipayan_Das; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/suraj.pt; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Dipayan_Das; <NewLine> ,"REPLY_DATE 1: September 23, 2020, 10:25am; <NewLine> REPLY_DATE 2: September 23, 2020,  1:10pm; <NewLine> REPLY_DATE 3: September 24, 2020, 12:07am; <NewLine> REPLY_DATE 4: September 24, 2020,  4:59pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
95522,RuntimeError: CUDA error: out of memory happens randomly,2020-09-08T07:44:17.459Z,0,52,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am getting RuntimeError: CUDA error: out of memory error randomly while running my testing code. Sometime when I add print(torch.cuda.device_count()) it is fixed but happens again.</p><NewLine><p>Some people suggest decreasing batch size but mine is already 1.</p><NewLine></div>",https://discuss.pytorch.org/u/lycaenidae,(Lycaenidae),lycaenidae,"September 8, 2020,  7:44am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you using variable input shapes in your script?<br/><NewLine>If so, do you know the largest expected shape for your use case?<br/><NewLine>If not, what kind of model are you using and are you using the latest stable PyTorch release?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 11, 2020,  6:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
95288,Illegal memory access,2020-09-05T14:58:27.531Z,5,84,"<div class=""post"" itemprop=""articleBody""><NewLine><p>i have an error after make one loss func</p><NewLine><pre><code class=""lang-auto"">class Latent_Classifier(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Latent_Classifier, self).__init__()<NewLine>        <NewLine>        self.encoder = nn.Sequential(<NewLine>            nn.Linear(128, 750),<NewLine>            nn.LeakyReLU(0.2),<NewLine>            nn.Linear(750, 750),<NewLine>            nn.Linear(750, 1)<NewLine>        )<NewLine>        <NewLine>    def forward(self,latent_z):<NewLine>        x1 = self.encoder(latent_z)<NewLine>        print(x1.size())<NewLine>        _eps = 1e-15<NewLine>        loss = -(x1 + _eps).log().mean()-(1 - x1 + _eps).log().mean()<NewLine>        <NewLine>        return loss<NewLine></code></pre><NewLine><p>i use this func as</p><NewLine><pre><code class=""lang-auto"">classifier = Latent_Classifier()<NewLine><NewLine>f_classifier = classifier(latent_f)<NewLine>lm_classifier = classifier(latent_l)<NewLine><NewLine>loss = 4000*(f_loss + m_loss) + 30 * (f_classifier + lm_classifier) + 2000 * lm_loss<NewLine><NewLine>loss.backward()<NewLine></code></pre><NewLine><p>in loss.backward() i got an error msg</p><NewLine><p>CUDA error: an illegal memory access was encountered</p><NewLine><p>before using classifier loss, i have no error msg</p><NewLine><p>is an error in function Latent_Classifier?</p><NewLine><p>when i executed it using ‘torch.device(“cpu”)’ not cuda:0 it works well</p><NewLine></div>",https://discuss.pytorch.org/u/DoKyung_Lim,(DoKyung Lim),DoKyung_Lim,"September 5, 2020,  3:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you rerun the script with:</p><NewLine><pre><code class=""lang-python"">CUDA_LAUNCH_BLOCKING=1 python script.py args<NewLine></code></pre><NewLine><p>and post the stack trace here, please?<br/><NewLine>The illegal memory access might have been created by a previous CUDA operation and your loss could be a red herring.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>it’s my traceback message</p><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “train2.py”, line 121, in <br/><NewLine>loss.backward()<br/><NewLine>File “/home/hhhoh/.local/lib/python3.6/site-packages/torch/tensor.py”, line 18                              5, in backward<br/><NewLine>torch.autograd.backward(self, gradient, retain_graph, create_graph)<br/><NewLine>File “/home/hhhoh/.local/lib/python3.6/site-packages/torch/autograd/<strong>init</strong>.p                              y”, line 127, in backward<br/><NewLine>allow_unreachable=True)  # allow_unreachable flag<br/><NewLine>RuntimeError: CUDA error: an illegal memory access was encountered<br/><NewLine>Exception raised from copy_kernel_cuda at /pytorch/aten/src/ATen/native/cuda/Cop                              y.cu:200 (most recent call first):<br/><NewLine>frame <span class=""hashtag"">#0:</span> c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f130683                              e1e2 in /home/hhhoh/.local/lib/python3.6/site-packages/torch/lib/libc10.so)<br/><NewLine>frame <span class=""hashtag"">#1:</span>  + 0x1e63b08 (0x7f1308b0ab08 in /home/hhhoh/.local/l                              ib/python3.6/site-packages/torch/lib/libtorch_cuda.so)<br/><NewLine>frame <span class=""hashtag"">#2:</span>  + 0xc282b9 (0x7f13424cf2b9 in /home/hhhoh/.local/li                              b/python3.6/site-packages/torch/lib/libtorch_cpu.so)<br/><NewLine>frame <span class=""hashtag"">#3:</span>  + 0xc25f28 (0x7f13424ccf28 in /home/hhhoh/.local/li                              b/python3.6/site-packages/torch/lib/libtorch_cpu.so)<br/><NewLine>frame <span class=""hashtag"">#4:</span> at::native::copy_(at::Tensor&amp;, at::Tensor const&amp;, bool) + 0x44 (0x7f13                              424cf144 in /home/hhhoh/.local/lib/python3.6/site-packages/torch/lib/libtorch_cp                              u.so)<br/><NewLine>frame <span class=""hashtag"">#5:</span> at::Tensor::copy_(at::Tensor const&amp;, bool) const + 0x115 (0x7f1342bba0                              95 in /home/hhhoh/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)<br/><NewLine>frame <span class=""hashtag"">#6:</span>  + 0x37e647e (0x7f134508d47e in /home/hhhoh/.local/l                              ib/python3.6/site-packages/torch/lib/libtorch_cpu.so)<br/><NewLine>frame <span class=""hashtag"">#7:</span> at::Tensor::copy_(at::Tensor const&amp;, bool) const + 0x115 (0x7f1342bba0                              95 in /home/hhhoh/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)<br/><NewLine>frame <span class=""hashtag"">#8:</span> at::native::to(at::Tensor const&amp;, c10::TensorOptions const&amp;, bool, boo                              l, c10::optional<a>c10::MemoryFormat</a>) + 0xb54 (0x7f134270b564 in /home/hhhoh/.loc                              al/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)<br/><NewLine>frame <span class=""hashtag"">#9:</span>  + 0x128850a (0x7f1342b2f50a in /home/hhhoh/.local/l                              ib/python3.6/site-packages/torch/lib/libtorch_cpu.so)<br/><NewLine>frame <span class=""hashtag"">#10:</span>  + 0x2e749da (0x7f134471b9da in /home/hhhoh/.local/                              lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)<br/><NewLine>frame <span class=""hashtag"">#11:</span>  + 0x10ea412 (0x7f1342991412 in /home/hhhoh/.local/                              lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)<br/><NewLine>frame <span class=""hashtag"">#12:</span> at::Tensor::to(c10::TensorOptions const&amp;, bool, bool, c10::optional&lt;c                              10::MemoryFormat&gt;) const + 0x146 (0x7f1342bedf56 in /home/hhhoh/.local/lib/pytho                              n3.6/site-packages/torch/lib/libtorch_cpu.so)<br/><NewLine>frame <span class=""hashtag"">#13:</span>  + 0x336a970 (0x7f1344c11970 in /home/hhhoh/.local/                              lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)<br/><NewLine>frame <span class=""hashtag"">#14:</span> torch::autograd::Engine::evaluate_function(std::shared_ptr&lt;torch::aut                              ograd::GraphTask&gt;&amp;, torch::autograd::Node*, torch::autograd::InputBuffer&amp;, std::                              shared_ptr<a>torch::autograd::ReadyQueue</a> const&amp;) + 0x3fd (0x7f1344c173fd in /home                              /hhhoh/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)<br/><NewLine>frame <span class=""hashtag"">#15:</span> torch::autograd::Engine::thread_main(std::shared_ptr&lt;torch::autograd:                              :GraphTask&gt; const&amp;) + 0x451 (0x7f1344c18fa1 in /home/hhhoh/.local/lib/python3.6/                              site-packages/torch/lib/libtorch_cpu.so)<br/><NewLine>frame <span class=""hashtag"">#16:</span> torch::autograd::Engine::thread_init(int, std::shared_ptr&lt;torch::auto                              grad::ReadyQueue&gt; const&amp;, bool) + 0x89 (0x7f1344c11119 in /home/hhhoh/.local/lib                              /python3.6/site-packages/torch/lib/libtorch_cpu.so)<br/><NewLine>frame <span class=""hashtag"">#17:</span> torch::autograd::python::PythonEngine::thread_init(int, std::shared_p                              tr<a>torch::autograd::ReadyQueue</a> const&amp;, bool) + 0x4a (0x7f13523b14ba in /home/hh                              hoh/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)<br/><NewLine>frame <span class=""hashtag"">#18:</span>  + 0xbd6df (0x7f135350d6df in /usr/lib/x86_64-linux                              -gnu/libstdc++.so.6)<br/><NewLine>frame <span class=""hashtag"">#19:</span>  + 0x76db (0x7f13559496db in /lib/x86_64-linux-gnu/                              libpthread.so.0)<br/><NewLine>frame <span class=""hashtag"">#20:</span> clone + 0x3f (0x7f1355c82a3f in /lib/x86_64-linux-gnu/libc.so.6)</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The illegal memory access was most likely triggered before the copy kernel, so the blocking launch is apparently not working.<br/><NewLine>Could you post an executable code snippet, which would reproduce this issue?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you mean all code related execution?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, if possible narrow down the minimal code snippet, which reproduces the error.<br/><NewLine>I.e. remove all data loading, metric calculation etc., use random inputs and try to isolate the illegal memory access to a few lines.<br/><NewLine>What’s currently hard to debug is that your code apparently seems to run fine on the CPU and that the blocking launch isn’t properly working in your setup.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>i change classifier to Latent_Classifier().to(device) and error is cleared</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/DoKyung_Lim; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/DoKyung_Lim; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/DoKyung_Lim; <NewLine> ,"REPLY_DATE 1: September 5, 2020,  6:59pm; <NewLine> REPLY_DATE 2: September 5, 2020,  9:59pm; <NewLine> REPLY_DATE 3: September 7, 2020,  2:54am; <NewLine> REPLY_DATE 4: September 7, 2020,  4:23pm; <NewLine> REPLY_DATE 5: September 7, 2020,  6:38pm; <NewLine> REPLY_DATE 6: September 8, 2020, 12:40pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
95012,RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same,2020-09-03T04:15:12.132Z,0,38,"<div class=""post"" itemprop=""articleBody""><NewLine><p><img alt=""Screenshot from 2020-09-03 12-10-19"" data-base62-sha1=""lctPeiKM5CLfglrepWRuinYtUly"" height=""1"" src=""https://discuss.pytorch.org/uploads/default/original/3X/9/4/9496d58ad2725d67cbab9ccaf8e56f1791282b90.png"" width=""1""/><br/><NewLine>I have checked my input and model are on GPU. However, it seemed not working. Can anyone help me with the bug I’m encountering?</p><NewLine></div>",https://discuss.pytorch.org/u/allenhung1025,(Allenhung1025),allenhung1025,"September 3, 2020,  4:17am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you post your code so that we could have a look, please?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 5, 2020,  7:10am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
94650,"How can l clear the old cache in GPU, when training different groups of data continuously?",2020-08-31T06:36:15.187Z,0,71,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I need to train the image classifiers in different groups. When one group is trained, the script will automatically load and train the next group by “for” statements.<br/><NewLine>But an error occurred saying there was not enough memory in GPU when it start to train the next group. How can I clear the GPU memory used by the last group training before the script start train the next group?<br/><NewLine>l have try to use torch.cuda.empty_cache() after each group training finished but it doesn’t work.</p><NewLine><pre><code class=""lang-auto"">    time.sleep(5)<NewLine>    del model<NewLine>    del loss<NewLine>    gc.collect()<NewLine>    torch.cuda.empty_cache()#清空GPU缓存<NewLine>    # print(torch.cuda.memory_stats(0))<NewLine>    time.sleep(10)<NewLine>    torch.cuda.empty_cache()#清空GPU缓存<NewLine>    # print(torch.cuda.memory_stats(0))<NewLine></code></pre><NewLine><p>Pytorch’s version is 1.6.0<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/8b94ad2e444c53dd5cb1ad62fe8334543856d612"" href=""https://discuss.pytorch.org/uploads/default/original/3X/8/b/8b94ad2e444c53dd5cb1ad62fe8334543856d612.png"" title=""image""><img alt=""image"" data-base62-sha1=""jUMUbO0ch7piBwgn6DJh9yXlSjo"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/b/8b94ad2e444c53dd5cb1ad62fe8334543856d612_2_10x10.png"" height=""390"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/b/8b94ad2e444c53dd5cb1ad62fe8334543856d612_2_690x390.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/b/8b94ad2e444c53dd5cb1ad62fe8334543856d612_2_690x390.png, https://discuss.pytorch.org/uploads/default/optimized/3X/8/b/8b94ad2e444c53dd5cb1ad62fe8334543856d612_2_1035x585.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/8/b/8b94ad2e444c53dd5cb1ad62fe8334543856d612_2_1380x780.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1884×1065 1.04 MB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/tcexeexe,(tcexeexe),tcexeexe,"August 31, 2020,  6:55am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you delete all references to the model and other tensors, the memory can be freed or reused. <a href=""https://discuss.pytorch.org/t/clearing-the-gpu-is-a-headache/84762/4"">Here</a> is a small example.<br/><NewLine>Make sure you are not storing the model output, loss etc. without detaching it, as this would keep the computation graph with all intermediate tensors alive.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 1, 2020,  4:54am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
91986,Cuda out of memory Error using retain_graph=True,2020-08-07T15:01:35.032Z,4,134,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Dear all,</p><NewLine><p>I can not figure out how to get rid of the out of memory error:<br/><NewLine>RuntimeError: CUDA out of memory. Tried to allocate 7.50 MiB (GPU 0; 11.93 GiB total capacity; 5.47 GiB already allocated; 4.88 MiB free; 81.67 MiB cached).</p><NewLine><p>In fact due to the recurrent architecture of my network I have to ‘retain_graph=True’ Otherwise I get the error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.<NewLine></code></pre><NewLine><p>I keep running into this error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.<NewLine></code></pre><NewLine><p>Here is the main of my function</p><NewLine><pre><code class=""lang-auto"">for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data<NewLine>        states = None #torch.empty().to(device)<NewLine>        for idx, image in enumerate(loader):<NewLine>            <NewLine>            # Step 1. Remember that Pytorch accumulates gradients.<NewLine>            <NewLine>            # We need to clear them out before each instance<NewLine>            # Step 3. Run our forward pass.<NewLine><NewLine>            tensor = image[0].clone().to(device)<NewLine>            <NewLine>            if states is None:<NewLine>                states = prednet.get_initial_states(tensor)<NewLine>            prednet.zero_grad()<NewLine>            # tensor = tensor.reshape(tensor.shape[0],1,tensor.shape[1],tensor.shape[2],tensor.shape[3])<NewLine>            tag_scores, states = prednet(tensor, states)<NewLine>            # Step 4. Compute the loss, gradients, and update the parameters by<NewLine>            #  calling optimizer.step()<NewLine>            <NewLine>            loss = loss_function(tag_scores, torch.zeros_like(tag_scores))<NewLine>            print(loss)<NewLine>            loss.backward(retain_graph=True)<NewLine>            for state in states:<NewLine>                    state.detach()<NewLine>            optimizer.step()<NewLine>            print('1 backward')<NewLine>            torch.cuda.empty_cache()<NewLine></code></pre><NewLine><p>Here is the forward function:</p><NewLine><pre><code class=""lang-auto"">def forward(self, a, states = None):<NewLine><NewLine><NewLine>            r_tm1 = states[:self.nb_layers]<NewLine>            c_tm1 = states[self.nb_layers:2*self.nb_layers]<NewLine>            e_tm1 = states[2*self.nb_layers:3*self.nb_layers]<NewLine><NewLine>            if self.extrap_start_time is not None:<NewLine>                t = states[-1].copy()<NewLine>                a = torch.switch(t &gt;= self.t_extrap, states[-2], a)  # if past self.extrap_start_time, the previous prediction will be treated as the actual<NewLine><NewLine>            c = []<NewLine>            r = []<NewLine>            e = []<NewLine><NewLine><NewLine>            for l in reversed(range(self.nb_layers)):<NewLine>                inputs = [r_tm1[l], e_tm1[l]]<NewLine>                if l &lt; self.nb_layers - 1:<NewLine>                    inputs.append(r_up)<NewLine><NewLine>                inputs = torch.cat(inputs, self.channel_axis)<NewLine>                # print(inputs.shape)<NewLine><NewLine>                i = self.conv_layers['i'][l](inputs)<NewLine>                f = self.conv_layers['f'][l](inputs)<NewLine>                o = self.conv_layers['o'][l](inputs)<NewLine><NewLine>                # print('i',torch.isnan(i).any())<NewLine>                # print('f',torch.isnan(f).any())<NewLine>                # print('o',torch.isnan(o).any())<NewLine>                # print('c',torch.isnan(o).any())<NewLine>                # print('c',torch.isnan(self.conv_layers['c'][l](inputs)).any())<NewLine>                _c = f * c_tm1[l] + i * self.conv_layers['c'][l](inputs)<NewLine>                _r = o * self.LSTM_activation(_c)<NewLine>                c.insert(0, _c)<NewLine>                r.insert(0, _r)<NewLine><NewLine>                if l &gt; 0:<NewLine>                    r_up = self.upsample(_r)<NewLine><NewLine><NewLine>            for l in range(self.nb_layers):<NewLine>                ahat = self.conv_layers['ahat'][l](r[l])<NewLine><NewLine>                if l == 0:<NewLine>                    value = torch.Tensor([self.pixel_max]).to(device)<NewLine>                    ahat = torch.min(ahat, value.expand_as(ahat))<NewLine>                    frame_prediction = ahat<NewLine><NewLine>                # compute errors<NewLine><NewLine>                e_up = self.error_activation(ahat - a)<NewLine>                e_down = self.error_activation(a - ahat)<NewLine><NewLine>                e.append(torch.cat((e_up, e_down), dim=self.channel_axis))<NewLine>                if l &lt; self.nb_layers - 1:<NewLine>                    a = self.conv_layers['a'][l](e[l])<NewLine>                    a = self.pool(a)  # target for next layer<NewLine><NewLine>            if self.output_mode == 'prediction':<NewLine>                output = frame_prediction<NewLine><NewLine>            else:<NewLine>                for l in range(self.nb_layers):<NewLine>                    layer_error = torch.mean(torch.flatten(e[l],start_dim=1), dim=-1, keepdim = True)<NewLine>                    if l == 0:<NewLine>                        all_error = layer_error<NewLine>                    else:<NewLine>                         all_error = torch.cat((all_error, layer_error), dim=-1)<NewLine><NewLine>                if self.output_mode == 'error' and image_n ==0:<NewLine>                    output = all_error<NewLine>                    output = output.unsqueeze(1)<NewLine>                # elif self.output_mode == 'error':<NewLine>                #     all_error = all_error.unsqueeze(1)<NewLine>                #     output = torch.cat((output, all_error), dim=1)<NewLine>                else:<NewLine>                    output = torch.cat((torch.flatten(frame_prediction, start_dim=1), all_error), dim=-1)<NewLine><NewLine>            states = r + c + e<NewLine>            if self.extrap_start_time is not None:<NewLine>                states += [frame_prediction, t + 1]<NewLine>            # return output, states<NewLine><NewLine>            return output, states<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Martin_Barry,(Martin Barry),Martin_Barry,"August 7, 2020,  3:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I assume you need the <code>retain_graph=True</code> setting, since you are not detaching the <code>states</code> tensor.<br/><NewLine>If this is your use case, you would have to lower the batch size to be able to store all computation graphs on the device.<br/><NewLine>If you don’t need to backpropagate through multiple steps, you might want to detach <code>states</code> via:</p><NewLine><pre><code class=""lang-python"">tag_scores, states = prednet(tensor, states.detach())<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your answer! I indeed need the backpropagation through time. But even reducing the batch will only delay the rise of memory error. Also I already tried to use</p><NewLine><pre><code class=""lang-auto"">for state in states:<NewLine>                    state.detach()<NewLine></code></pre><NewLine><p>It doest not change the out of memory error after 5-10 batches.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>state.detach()</code> is not an inplace method and you would have to reassign the result as:</p><NewLine><pre><code class=""lang-python"">state = state.detach()<NewLine></code></pre><NewLine><p>If that doesn’t help, could you post an executable code snippet?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey! thank you so much for your time. Still not working. Yet I am working on a 2-3GB database and the network is “fairly” complex. I can send you the code, but make an executable snippet that reproduces the error would take me lot of time. Especially since I do not know how to make one, that would take me a long while. I will put the solution here if I ever find one.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>But isn’t there a way to set retain_graph=False from time to time to save memory ? I wanted to do something like this but every time I do this at a given step I get</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>The issue is raised, since the intermediate tensors are already freed (after you’ve used <code>retain_graph=True</code>), while your <code>backward()</code> call tries to backpropagate through operations where these intermedates are already deleted. Detaching the tensor would solve the problem (the backward pass would stop at this point and will not backpropagate further), but I understand that it might not be trivial if the code base is complicated.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Martin_Barry; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Martin_Barry; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Martin_Barry; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: August 10, 2020,  7:03am; <NewLine> REPLY_DATE 2: August 10, 2020,  1:24pm; <NewLine> REPLY_DATE 3: August 10, 2020,  4:22pm; <NewLine> REPLY_DATE 4: August 11, 2020,  8:12am; <NewLine> REPLY_DATE 5: August 11, 2020,  8:22am; <NewLine> REPLY_DATE 6: August 13, 2020,  8:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
89135,LSTM CPU memory leak on specific batchsize and hidden size,2020-07-14T10:54:06.963Z,0,100,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I’ve encountered a memory leak on a LSTM model and condensed the issue into the following code.</p><NewLine><p>I’ve tried the following solutions:</p><NewLine><ol><NewLine><li>Detach hidden state using repackage_hidden() <a href=""https://discuss.pytorch.org/t/help-clarifying-repackage-hidden-in-word-language-model/226/8"">https://discuss.pytorch.org/t/help-clarifying-repackage-hidden-in-word-language-model/226/8</a><NewLine></li><NewLine><li>gc.collect()</li><NewLine><li>torch.nograd()</li><NewLine></ol><NewLine><p>However, problem still persists using pytorch 1.5.1, and my machine has 64GB of ram. Any help is appreciated!</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import psutil, os, gc<NewLine><NewLine>def repackage_hidden(h):<NewLine>    """"""Wraps hidden states in new Tensors, to detach them from their history.""""""<NewLine><NewLine>    if isinstance(h, torch.Tensor):<NewLine>        return h.detach()<NewLine>    else:<NewLine>        return tuple(repackage_hidden(v) for v in h)<NewLine><NewLine># Doesn't leak memory<NewLine># batch = 3<NewLine># hidden_size = 256<NewLine><NewLine># batch = 3<NewLine># hidden_size = 512<NewLine><NewLine># batch = 6<NewLine># hidden_size = 256<NewLine><NewLine><NewLine># Leaks memory<NewLine>batch = 6<NewLine>hidden_size = 512<NewLine><NewLine>rnn = nn.LSTM(320, hidden_size, num_layers=5, bidirectional=True)<NewLine>x = torch.randn(5, batch, 320)<NewLine>h0 = torch.randn(10, batch, hidden_size)<NewLine>c0 = torch.randn(10, batch, hidden_size)<NewLine>with torch.no_grad():<NewLine>    for i in range(1000):<NewLine>        print(i, psutil.Process(os.getpid()).memory_info().rss)    <NewLine>        output, hidden = rnn(x, (h0, c0))<NewLine>        hidden = repackage_hidden(hidden)<NewLine>        gc.collect()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/pol1,,pol1,"July 15, 2020,  4:16am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>For the sake of completeness: issues seems to be solved <a href=""https://discuss.pytorch.org/t/lstm-on-cpu-wont-release-memory-when-all-refs-deleted/89026/16"">here</a>. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 16, 2020,  4:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
89026,LSTM on CPU won&rsquo;t release memory when all refs deleted,2020-07-13T16:51:54.924Z,8,254,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey,</p><NewLine><p>Merely instantiating a bunch of LSTMs on a CPU device seems to allocate memory in such a way that it’s never released, even after gc.collect(). The same code run on the GPU releases the memory after a <code>torch.cuda.empty_cache()</code>. I haven’t been able to find any equivalent of <code>empty_cache()</code> for the CPU.</p><NewLine><p>Is this expected behavior? My actual use-case involves training several models at once on CPU cores in a Kubernetes deployment, and involving LSTMs in any way fills memory until the Kubernetes OOM killer evicts the pod. The models themselves are quite small (and if I load a trained model in, they take up very little memory), but all memory temporarily used during training stays filled once training is done.</p><NewLine><p>Code:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import gc<NewLine><NewLine>device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')<NewLine>throwaway = torch.ones((1,1)).to(device) # load CUDA context<NewLine><NewLine>class Encoder(nn.Module):<NewLine>    def __init__(self, input_dim, hidden_dim, n_layers, dropout_perc):<NewLine>        super().__init__()<NewLine>        self.hidden_dim, self.n_layers = (hidden_dim, n_layers)<NewLine>        self.rnn = nn.LSTM(input_dim,hidden_dim,n_layers,dropout=dropout_perc)<NewLine>    def forward(self,x):<NewLine>        outputs, (hidden, cell) = self.rnn(x)<NewLine>        return hidden, cell<NewLine><NewLine>pile=[]<NewLine>for i in range(500):<NewLine>    pile.append(Encoder(102,64,4,0.5).to(device))<NewLine><NewLine>del pile<NewLine>gc.collect()<NewLine>if torch.cuda.is_available():<NewLine>    torch.cuda.empty_cache()<NewLine></code></pre><NewLine><p>I’m running PyTorch 1.5.1 and Python 3.8.3 on Ubuntu 18.04 LTS.</p><NewLine></div>",https://discuss.pytorch.org/u/StrawVulcan,,StrawVulcan,"July 13, 2020,  4:51pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Simpler models don’t seem to exhibit this behavior. For example, this code fully deallocates the memory once all the references are deleted:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import gc<NewLine><NewLine>class Bloat(nn.Module):<NewLine>    def __init__(self, size):<NewLine>        super().__init__()<NewLine>        self.fc = nn.Linear(size,size)<NewLine>    def forward(self,x):<NewLine>        return self.fc(x)<NewLine><NewLine>pile = []<NewLine>for i in range(10):<NewLine>    pile.append(Bloat(2**12))<NewLine><NewLine>del pile<NewLine>gc.collect()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you only seeing the increase on system memory usage, if you are using the GPU or also a CPU implementation only?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for replying <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> .</p><NewLine><p>When using <code>torch.device('cpu')</code> the memory usage of allocating the LSTM module <code>Encoder</code> increases and never comes back down.<br/><NewLine>When using <code>torch.device('cuda:0')</code> the memory usage of the same comes down out of the GPU, and most of it comes down out of the system RAM as well. (I just did the experiment, and there was 16M unaccountably still allocated in system RAM).</p><NewLine><p>So the problem seems at least <em>mostly</em> restricted to <code>torch.device('cpu')</code>.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh, I just realized you might mean a CPU-only distribution of PyTorch? I’m using a CUDA-enabled version, but with a CPU device. I haven’t tried with a CPU-only version of PyTorch because I do train on a GPU occasionally, though if this bug (if it is a bug) isn’t on a <em>CPU only</em> version of PyTorch, I can definitely switch to that for the actual deployment.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just tried this on my Mac using the CPU-only distribution of PyTorch 1.5.1 for MacOS, and it <em>did</em> free all of its memory after the <code>del pile; gc.collect()</code>. So perhaps this bug only affects using a CPU device on the GPU-capable distribution of PyTorch. That at least gives me an angle of attack, though it would be far more convenient if the GPU-capable distribution of PyTorch behaved itself in CPU-mode for development and testing.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah, blast. I just tried <code>1.5.1+cpu</code> on <em>Linux</em>, and it didn’t free the memory. The Mac version freed the memory after the <code>del pile; gc.collect()</code>, but the Linux version didn’t.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Also just tried <code>1.6.0.dev20200625+cpu</code> on Linux, and it didn’t free the memory. The resident set only every increases, never decreases, until I kill the Python process itself.</p><NewLine><p>Also interesting: I can see that there aren’t any references to tensors left with this code. The result is only the <code>throwaway</code> tensor after <code>del pile</code>:</p><NewLine><pre><code class=""lang-auto"">for obj in gc.get_objects():<NewLine>    try:<NewLine>        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):<NewLine>            print(f'type:{type(obj)}; shape:{obj.size()}; grad_fn:{repr(obj.grad_fn)}; requires_grad:{obj.requires_grad}')<NewLine>    except Exception as e:<NewLine>        pass<NewLine></code></pre><NewLine><p>So it’s not that there’s still some kind of lingering reference.</p><NewLine><p>One last thing: If I do this multiple times, say filling <code>pile</code>, deleting <code>pile</code>, and then filling it again, the memory usage doesn’t go up significantly until I fill <code>pile</code> <em>more</em> than I filled it the last time. So I think this is some kind of ever-expanding heap issue.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the debugging so far.<br/><NewLine>Could you post, how you are measuring the allocated system memory? Are you checking it in a system utility or inside the Python process directly?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m also having the same CPU memory issue with LSTMs, strangely it is only affected if the batch size and hidden size are above a certain level. In the example code below, the memory measurement is on the Python process directly.</p><NewLine><p><a href=""https://discuss.pytorch.org/t/lstm-cpu-memory-leak-on-specific-batchsize-and-hidden-size/89135"">https://discuss.pytorch.org/t/lstm-cpu-memory-leak-on-specific-batchsize-and-hidden-size/89135</a></p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> I’m measuring allocated system memory by watching the memory for the Python process in htop rise and fall as I run my script in a repl.</p><NewLine><p>I also just did a longer experiment using the MacOS version of CPU-only PyTorch 1.5.1 (which is the only place I’ve seen the LSTM memory released correctly so far). The memory usage remained bounded (below 400MB), and it was completely released at each stage.</p><NewLine><p>I just started running the same script on the same data, with the only difference being the <em>Linux</em> version of CPU-only PyTorch 1.5.1 (this one specifically: <a href=""https://download.pytorch.org/whl/cpu/torch-1.5.1%2Bcpu-cp38-cp38-linux_x86_64.whl"" rel=""nofollow noopener"">https://download.pytorch.org/whl/cpu/torch-1.5.1%2Bcpu-cp38-cp38-linux_x86_64.whl</a>). In about five minutes it has already consumed a <em>gigabyte</em>, and continues to climb.</p><NewLine><p>I’ll see if I can figure out what’s different between the two versions of CPU-only PyTorch tomorrow, but I’m out of my depth if it’s an MKL-DNN bug or something.</p><NewLine><p>By the way, the script I’m currently running involves my actual application code, including training, which I can’t share. However, if it would be helpful, I’d be happy to craft a minimal example I <em>can</em> share that exhibits the same behavior. It doesn’t appear hard to replicate.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pol1"">@pol1</a> That’s interesting. My own parameters for the LSTM module are like so in my leaking example:</p><NewLine><pre><code class=""lang-auto"">batch_size = 16<NewLine>hidden_size = 64<NewLine>n_layers = 4<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Does reducing it to batch_size=1 and hidden_size=16 help? Reducing them worked in my minimal example</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ll give it a shot tomorrow.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>A minimal code snippet to reproduce this issue would be great, but your initial code might also do the job?<br/><NewLine>Please create an issue <a href=""https://github.com/pytorch/pytorch/issues"">here</a> with the code snippet, a brief description, and link to this topic for further information, so that we can track and fix it.</p><NewLine><p>CC <a class=""mention"" href=""/u/pol1"">@pol1</a> in case you would like to add your information to the same issue.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Was able to stabilize the leak by setting <code>OMP_NUM_THREADS=4</code>. More info is here: <a href=""https://github.com/pytorch/pytorch/issues/32008"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/32008</a></p><NewLine><p>A fix has been merged and looks to be available in future version 1.6. The nightly build works for this similar case: <a href=""https://github.com/pytorch/pytorch/issues/40973"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/40973</a>`</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hmm… I’ll definitely try the workarounds in this post, but if 1.6 is supposed to have resolved it, my issue may be different. The <code>1.6.0.dev20200625+cpu</code> nightly leaked in my above allocation test.</p><NewLine><p>Do you know if the fix for the issue you linked would have been in by then? It’s the latest 1.6 nightly I could find.</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think I’m convinced I have a distinct issue now. Neither using 1.6, nor <code>MKL_DISABLE_FAST_MM=1</code>, nor <code>OMP_NUM_THREADS=4</code> or a combination thereof solves the leak in my allocation test. I’ll file a PyTorch issue now.</p><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here’s the new issue: <a href=""https://github.com/pytorch/pytorch/issues/41486"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/41486</a></p><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pol1"">@pol1</a> reducing the hidden size didn’t solve the problem in my allocation test, so I’m pretty sure I have a different problem.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/StrawVulcan; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/StrawVulcan; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/StrawVulcan; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/StrawVulcan; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/StrawVulcan; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/StrawVulcan; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/pol1; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/StrawVulcan; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/StrawVulcan; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/pol1; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/StrawVulcan; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/pol1; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/StrawVulcan; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/StrawVulcan; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/StrawVulcan; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/StrawVulcan; <NewLine> ,"REPLY_DATE 1: July 13, 2020,  5:14pm; <NewLine> REPLY_DATE 2: July 14, 2020,  9:48am; <NewLine> REPLY_DATE 3: July 14, 2020,  6:47pm; <NewLine> REPLY_DATE 4: July 14, 2020,  2:39pm; <NewLine> REPLY_DATE 5: July 14, 2020,  5:35pm; <NewLine> REPLY_DATE 6: July 14, 2020,  6:04pm; <NewLine> REPLY_DATE 7: July 14, 2020,  6:21pm; <NewLine> REPLY_DATE 8: July 15, 2020,  3:32am; <NewLine> REPLY_DATE 9: July 15, 2020,  3:44am; <NewLine> REPLY_DATE 10: July 15, 2020,  4:13am; <NewLine> REPLY_DATE 11: July 15, 2020,  4:17am; <NewLine> REPLY_DATE 12: July 15, 2020,  4:22am; <NewLine> REPLY_DATE 13: July 15, 2020,  4:24am; <NewLine> REPLY_DATE 14: July 15, 2020,  5:19am; <NewLine> REPLY_DATE 15: July 15, 2020,  6:57am; <NewLine> REPLY_DATE 16: July 15, 2020,  1:46pm; <NewLine> REPLY_DATE 17: July 15, 2020,  5:32pm; <NewLine> REPLY_DATE 18: July 15, 2020,  6:07pm; <NewLine> REPLY_DATE 19: July 15, 2020,  6:07pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: 1 Like; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: 1 Like; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> REPLY 18 LIKES: ; <NewLine> REPLY 19 LIKES: ; <NewLine> 
88258,Group convolution output order,2020-07-07T15:12:55.662Z,0,77,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to apply a group 1x1 convolution where one (whole) input is convolved with multiple sets of filters (ngroups), that is: conv = nn.Conv2d(ch, ch*ngroups, 1, groups=ch).</p><NewLine><p>What is the correct way to separate the result of this convolution into respective groups?</p><NewLine><p>conv(input).view(bs, ch, ngroups, h, w), or conv(input).view(bs, ngroups, ch, h, w)</p><NewLine></div>",https://discuss.pytorch.org/u/trougnouf,(Benoit Brummer),trougnouf,"July 7, 2020,  3:30pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think it’s view(bs, ch, ngroups, h, w) based on the following experiment:</p><NewLine><pre><code class=""lang-auto"">bs = = h = w = 1<NewLine>ch = 3<NewLine>ngroups = 4<NewLine><NewLine>&gt;&gt;&gt; input = torch.zeros(bs, 3, h, w)<NewLine>&gt;&gt;&gt; input[0,1] = 1<NewLine><NewLine>&gt;&gt;&gt; conv = torch.nn.Conv2d(ch, ch*ngroups, 1, groups=ch)<NewLine><NewLine>&gt;&gt;&gt; conv.bias<NewLine>Parameter containing:<NewLine>tensor([ 0.7412,  0.8724, -0.6242, -0.1312, -0.1991, -0.1502,  0.1400,  0.9860,<NewLine>        -0.7265,  0.2633,  0.3402, -0.7472], requires_grad=True)<NewLine><NewLine>&gt;&gt;&gt; conv(input)<NewLine>tensor([[[[ 0.7412]], #(=bias)<NewLine>         [[ 0.8724]], #(=bias)<NewLine>         [[-0.6242]], #(=bias)<NewLine>         [[-0.1312]], #(=bias)<NewLine>         [[ 0.7028]], # not bias -&gt; ch1<NewLine>         [[-0.3131]], # not bias -&gt; ch1<NewLine>         [[ 0.0071]], # not bias -&gt; ch1<NewLine>         [[ 0.6137]], # not bias -&gt; ch1<NewLine>         [[-0.7265]], #(=bias)<NewLine>         [[ 0.2633]], #(=bias)<NewLine>         [[ 0.3402]], #(=bias)<NewLine>         [[-0.7472]]]], grad_fn=&lt;MkldnnConvolutionBackward&gt;)<NewLine><NewLine>&gt;&gt;&gt;conv(input).view(bs, ch, ngroups, 1, 1)[0, 1]<NewLine>Tensor([[[ 0.7028]],<NewLine><NewLine>        [[-0.3131]],<NewLine><NewLine>        [[ 0.0071]],<NewLine><NewLine>        [[ 0.6137]]], grad_fn=&lt;SelectBackward&gt;)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/trougnouf; <NewLine> ,"REPLY_DATE 1: July 7, 2020,  3:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
86920,RAM Fills Up Despite Using Train Function From PyTorch,2020-06-25T16:04:17.981Z,0,68,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi There,<br/><NewLine>I’m trying to build an image classifier but there seems to be a persistent issue of memory filling. As soon as I execute this function, the memory starts to fill up and before it’s epoch 2, the kernel crashes.</p><NewLine><p>The following is the training function:</p><NewLine><pre><code class=""lang-python"">def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):<NewLine>    since = time.time()<NewLine><NewLine>    val_acc_history = []<NewLine>    <NewLine>    print(""Device = "", device)<NewLine>    <NewLine>    best_model_wts = copy.deepcopy(model.state_dict())<NewLine>    best_acc = 0.0<NewLine><NewLine>    for epoch in range(num_epochs):<NewLine>        print('Epoch {}/{}'.format(epoch, num_epochs - 1))<NewLine>        print('-' * 10)<NewLine><NewLine>        # Each epoch has a training and validation phase<NewLine>        for phase in ['train', 'val']:<NewLine>            if phase == 'train':<NewLine>                model.train()  # Set model to training mode<NewLine>            else:<NewLine>                model.eval()   # Set model to evaluate mode<NewLine><NewLine>            running_loss = 0.0<NewLine>            running_corrects = 0<NewLine><NewLine>            # Iterate over data.<NewLine>            for inputs, labels in dataloaders[phase]:<NewLine>                inputs = inputs.to(device)<NewLine>                labels = labels.to(device)<NewLine><NewLine>                # zero the parameter gradients<NewLine>                optimizer.zero_grad()<NewLine><NewLine>                # forward<NewLine>                # track history if only in train<NewLine>                with torch.set_grad_enabled(phase == 'train'):<NewLine>                    # Get model outputs and calculate loss<NewLine>                    # Special case for inception because in training it has an auxiliary output. In train<NewLine>                    #   mode we calculate the loss by summing the final output and the auxiliary output<NewLine>                    #   but in testing we only consider the final output.<NewLine>                    if is_inception and phase == 'train':<NewLine>                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958<NewLine>                        outputs, aux_outputs = model(inputs)<NewLine>                        loss1 = criterion(outputs, labels)<NewLine>                        loss2 = criterion(aux_outputs, labels)<NewLine>                        loss = loss1 + 0.4*loss2<NewLine>                    else:<NewLine>                        outputs = model(inputs)<NewLine>                        loss = criterion(outputs, labels)<NewLine>                        optimizer.zero_grad()<NewLine><NewLine>                    _, preds = torch.max(outputs, 1)<NewLine>                    del inputs<NewLine>                    del labels<NewLine><NewLine>                    # backward + optimize only if in training phase<NewLine>                    if phase == 'train':<NewLine>                        loss.backward()<NewLine>                        optimizer.step()<NewLine><NewLine>                # statistics<NewLine>                running_loss += loss.item() * inputs.size(0)<NewLine>                running_corrects += torch.sum(preds == labels.data)<NewLine>                <NewLine>                loss.detach()<NewLine><NewLine>            epoch_loss = running_loss / len(dataloaders[phase].dataset)<NewLine>            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)<NewLine><NewLine>            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))<NewLine><NewLine>            # deep copy the model<NewLine>            if phase == 'val' and epoch_acc &gt; best_acc:<NewLine>                best_acc = epoch_acc<NewLine>                best_model_wts = copy.deepcopy(model.state_dict())<NewLine>            if phase == 'val':<NewLine>                val_acc_history.append(epoch_acc)<NewLine><NewLine>        print()<NewLine><NewLine>    time_elapsed = time.time() - since<NewLine>    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))<NewLine>    print('Best val Acc: {:4f}'.format(best_acc))<NewLine><NewLine>    # load best model weights<NewLine>    model.load_state_dict(best_model_wts)<NewLine>    return model, val_acc_history<NewLine></code></pre><NewLine><p>Then I print some parameters and move the model to GPU</p><NewLine><pre><code class=""lang-python"">model_ft, input_size = initialize_model(model_name, n_classes, use_pretrained = True)<NewLine><NewLine><NewLine># Move Model To GPU<NewLine>model_ft = model_ft.to(device)<NewLine><NewLine>print(""Params to learn:"")<NewLine><NewLine>params_to_update = model_ft.parameters()<NewLine>for name,param in model_ft.named_parameters():<NewLine>    if param.requires_grad == True:<NewLine>        print(""\t"",name)<NewLine>            <NewLine>optimizer_ft = optim.Adam(params_to_update, learning_rate)<NewLine>criterion = nn.CrossEntropyLoss()<NewLine></code></pre><NewLine><p>Using the very function defined above, I train this model, which then crashes my kernel.</p><NewLine><pre><code class=""lang-python"">trained_model, hist = train_model(model_ft, dataloaders, criterion, optimizer_ft, num_epochs = epochs)<NewLine><NewLine>print(model_ft)<NewLine></code></pre><NewLine><p>Please tell me if further information is required so I can get this issue resolved.</p><NewLine><p>Thanks in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/Haris_Rashid,(Haris Rashid),Haris_Rashid,"June 25, 2020,  4:04pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you remove the <code>.data</code> usage, as it’s not recommended and might yield unwanted side effects.<br/><NewLine>This shouldn’t create the increased memory usage, but is nevertheless recommended.<br/><NewLine>Also, the <code>loss.detach()</code> operation won’t have any effect, as it’s not an inplace operation (also shouldn’t create the memory issue).</p><NewLine><p>To debug the increased memory usage, could you add print statements into your code and check, where the memory is increasing?<br/><NewLine>You could use:</p><NewLine><pre><code class=""lang-python"">print(torch.cuda.memory_allocated() / 1024**2)<NewLine></code></pre><NewLine><p>Usually users forget to <code>detach()</code> tensors before storing them in lists, so that the complete computation graph will be stored as well.<br/><NewLine>However, the <code>epoch_acc</code> should already be detached. You could check if by printing the tensor’s <code>.grad_fn</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: June 26, 2020,  9:38am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
85840,Hi how can i interpolate an image by scipy,2020-06-17T20:15:02.457Z,0,68,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi how can i interpolate an image by scipy</p><NewLine></div>",https://discuss.pytorch.org/u/Golam_Moctader_Ronok,(Golam Moctader Ronok),Golam_Moctader_Ronok,"June 17, 2020,  8:15pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp2d.html""><code>scipy.interpolate.interp2d</code></a> might work.<br/><NewLine>Note that this question is not related to PyTorch so you might get faster and better answers on StackOverflow or a dedicated forum for other libraries. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: June 18, 2020,  9:45am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
85686,Reshape tensor while keeping data in a given dimension,2020-06-16T20:43:27.044Z,4,188,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I would like to reshape my data, which represents a single image, such that the width and height (dims 2,3) are lowered and the batch size increases; effectively making small crops as batches out of an image.<br/><NewLine>I want a specific dimension, the channels (1) to contain the same data.</p><NewLine><p>I initially wrote a pair of functions to do something like this, but I fear it’s not backprop compatible</p><NewLine><pre><code class=""lang-auto"">def img_to_batch(imgtensor, patch_size: int):<NewLine>    _, ch, height, width = imgtensor.shape<NewLine>    assert height%patch_size == 0 and width%patch_size == 0, 'img_to_batch: dims must be dividable by patch_size. {}%{}!=0'.format(imgtensor.shape, patch_size)<NewLine>    bs = math.ceil(height/patch_size) * math.ceil(width/patch_size)<NewLine>    btensor = torch.zeros([bs,ch,patch_size, patch_size], device=imgtensor.device, dtype=imgtensor.dtype)<NewLine>    xstart = ystart = 0<NewLine>    for i in range(bs):<NewLine>        btensor[i] = imgtensor[:, :, ystart:ystart+patch_size, xstart:xstart+patch_size]<NewLine>        xstart += patch_size<NewLine>        if xstart+patch_size &gt; width:<NewLine>            xstart = 0<NewLine>            ystart += patch_size<NewLine>    return btensor<NewLine><NewLine>def batch_to_img(btensor, height: int, width: int, ch=3):<NewLine>    imgtensor = torch.zeros([1, ch, height, width], device=btensor.device, dtype=btensor.dtype)<NewLine>    patch_size = btensor.shape[-1]<NewLine>    xstart = ystart = 0<NewLine>    for i in range(btensor.size(0)):<NewLine>        imgtensor[0, :, ystart:ystart+patch_size, xstart:xstart+patch_size] = btensor[i]<NewLine>        xstart += patch_size<NewLine>        if xstart+patch_size &gt; width:<NewLine>            xstart = 0<NewLine>            ystart += patch_size<NewLine>    return imgtensor<NewLine></code></pre><NewLine><p>There is a simple view / reshape function in the torch library, but when I use it the channels do not keep their respective data. eg (I would like the data shown to contain the same elements even if out of order):</p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; imgtens = torch.rand(1,3,4,4)<NewLine>&gt;&gt;&gt; imgtens[:,0,:,:]<NewLine>tensor([[[0.6830, 0.2091, 0.8786, 0.6002],<NewLine>         [0.0325, 0.7217, 0.1479, 0.3478],<NewLine>         [0.0880, 0.8705, 0.0929, 0.7978],<NewLine>         [0.7604, 0.2658, 0.3518, 0.1969]]])<NewLine>&gt;&gt;&gt; reshaped = imgtens.view([4,3,2,2])<NewLine>&gt;&gt;&gt; reshaped[:,0,:,:]<NewLine>tensor([[[0.6830, 0.2091],<NewLine>         [0.8786, 0.6002]],<NewLine><NewLine>        [[0.7604, 0.2658],<NewLine>         [0.3518, 0.1969]],<NewLine><NewLine>        [[0.3787, 0.0042],<NewLine>         [0.3481, 0.2722]],<NewLine><NewLine>        [[0.4175, 0.8700],<NewLine>         [0.1930, 0.7646]]])<NewLine></code></pre><NewLine><p>I read that the fold/unfold function may be able to help by creating a sliding window.</p><NewLine></div>",https://discuss.pytorch.org/u/trougnouf,(Benoit Brummer),trougnouf,"June 16, 2020,  9:33pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""85686"" data-username=""trougnouf""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/trougnouf/40/8006_2.png"" width=""20""/> trougnouf:</div><NewLine><blockquote><NewLine><p>I read that the fold/unfold function may be able to help by creating a sliding window</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes it works, actually, in your case you are extracting patches using sliding window then permuting channels.</p><NewLine><p>Here is the code that will do the job for your arbitrary example:</p><NewLine><pre><code class=""lang-auto"">x.unfold(2, 2, 2)[0].unfold(2, 2, 2).contiguous().view(3, -1, 2, 2).permute((1, 0, 2, 3))<NewLine></code></pre><NewLine><p>But if you have question how it works, here is another post I have explained the calculations:<br/><NewLine><aside class=""quote quote-modified"" data-post=""1"" data-topic=""51459""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v4/letter/z/aeb1de/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/change-the-dimension-of-tensor/51459"">Change the dimension of tensor</a><NewLine></div><NewLine><blockquote><NewLine>    Hi, <NewLine>I have a tensor with dimension [1, 1, 4, 6] like this: <NewLine>a = torch.tensor([[[ 1,  2,  3,  4,  5,  6],<NewLine>                   [ 7,  8,  9, 10, 11, 12],<NewLine>                   [13, 14, 15, 16, 17, 18],<NewLine>                   [19, 20, 21, 22, 23, 24]]])<NewLine><NewLine>I want to change it to a tensor like this: <NewLine>[[ [[1, 2],<NewLine>    [7, 8]],<NewLine><NewLine>   [[3, 4],<NewLine>    [9, 10]],<NewLine><NewLine>   [[5, 6],<NewLine>    [11, 12]],<NewLine><NewLine>   [[13, 14],<NewLine>    [19, 20]],<NewLine><NewLine>   [[15, 16],<NewLine>    [21, 22]],<NewLine><NewLine>   [[17, 18],<NewLine>    [23, 24]] ]]<NewLine>   <NewLine><NewLine>Is it possible? <NewLine>if we use this cod…<NewLine>  </blockquote><NewLine></aside><NewLine></p><NewLine><p>Bests</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <a class=""mention"" href=""/u/nikronic"">@Nikronic</a> !<br/><NewLine>If I understand this correctly, then the img_to_batch function I wrote above would be simply:</p><NewLine><pre><code class=""lang-auto"">def img_to_batch(imgtensor, patch_size: int):<NewLine>    _, ch, height, width = imgtensor.shape<NewLine>    assert height%patch_size == 0 and width%patch_size == 0, (<NewLine>        'img_to_batch: dims must be dividable by patch_size. {}%{}!=0'.format(<NewLine>            imgtensor.shape, patch_size))<NewLine>    return imgtensor.unfold(2,patch_size,patch_size).unfold(<NewLine>        3,patch_size,patch_size).contiguous().view(<NewLine>            ch,-1,patch_size,patch_size).permute((1,0,2,3))       <NewLine></code></pre><NewLine><p>Note that I made a slight modification from the equivalent of “x.unfold(2, 2, 2)[0].unfold(2,” to “x.unfold(2, 2, 2).unfold(3,”, I think that it can then also handle arbitrary batch size (and just like the channels they wouldn’t get mixed up). Correct me if I’m wrong.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I do not think you can do this as <code>tensor.unfold</code> depending on some criteria creates another dimension like <code>.unsqueeze(0)</code> and because of that <code>.view</code> and <code>.permute</code> will no longer be valid due to number of dimension mismatch.</p><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""85686"" data-username=""trougnouf""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/trougnouf/40/8006_2.png"" width=""20""/> trougnouf:</div><NewLine><blockquote><NewLine><p>“x.unfold(2, 2, 2)[0].unfold(2,” to “x.unfold(2, 2, 2).unfold(3,”</p><NewLine></blockquote><NewLine></aside><NewLine><p>Actually, I have never thought of generalizing <code>fold</code> and <code>unfold</code> method due to its tricky behavior (or at least my bad understanding).</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>You are right, the following test shows that data from channel 1 can be placed in channel 0</p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; img = torch.rand(4,3,8,8)<NewLine>&gt;&gt;&gt; img_to_batch(img,4)[4,0]<NewLine>tensor([[0.4258, 0.3276, 0.8221, 0.6588],<NewLine>        [0.5438, 0.9239, 0.0490, 0.7193],<NewLine>        [0.5852, 0.7115, 0.0703, 0.1770],<NewLine>        [0.4305, 0.4190, 0.2891, 0.0326]])<NewLine>&gt;&gt;&gt; img[0,1]<NewLine>tensor([[0.4258, 0.3276, 0.8221, 0.6588, 0.9918, 0.6219, 0.4951, 0.4356],<NewLine>        [0.5438, 0.9239, 0.0490, 0.7193, 0.6819, 0.0627, 0.0361, 0.3178],<NewLine>        [0.5852, 0.7115, 0.0703, 0.1770, 0.3855, 0.0666, 0.7337, 0.0240],<NewLine>        [0.4305, 0.4190, 0.2891, 0.0326, 0.3457, 0.7378, 0.5640, 0.7104],<NewLine>        [0.3787, 0.2371, 0.4585, 0.6150, 0.7169, 0.6518, 0.4671, 0.1212],<NewLine>        [0.8061, 0.4295, 0.1194, 0.7166, 0.7526, 0.8067, 0.1612, 0.2812],<NewLine>        [0.3896, 0.8208, 0.5835, 0.6830, 0.0191, 0.7138, 0.9124, 0.7285],<NewLine>        [0.0963, 0.4236, 0.2779, 0.8006, 0.1528, 0.5168, 0.6543, 0.7928]])<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think I got this <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/> I moved the channel axis to the front s.t. it would be safe and it passes my manual tests.</p><NewLine><pre><code class=""lang-auto"">def img_to_batch(img, patch_size: int):<NewLine>    _, ch, height, width = img.shape<NewLine>    assert height%patch_size == 0 and width%patch_size == 0, (<NewLine>        'img_to_batch: dims must be dividable by patch_size. {}%{}!=0'.format(<NewLine>            img.shape, patch_size))<NewLine>    assert img.dim() == 4<NewLine>    return img.unfold(2, patch_size, patch_size).unfold(<NewLine>        3, patch_size, patch_size).transpose(1,0).reshape(<NewLine>            ch, -1, patch_size, patch_size).transpose(1,0)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Nikronic; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/trougnouf; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Nikronic; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/trougnouf; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/trougnouf; <NewLine> ,"REPLY_DATE 1: June 17, 2020,  3:53pm; <NewLine> REPLY_DATE 2: June 17, 2020, 12:11am; <NewLine> REPLY_DATE 3: June 17, 2020, 12:23am; <NewLine> REPLY_DATE 4: June 17, 2020, 12:50pm; <NewLine> REPLY_DATE 5: June 17, 2020,  3:53pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
85348,Sparse tensor multiply a diagonal tensor,2020-06-13T16:54:25.544Z,0,111,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I would like to multiply a (symmetric) sparse tensor with a diagonal tensor. Since both tensors are sparse and the output is also sparse, I expect that this can be done very efficiently. However, my current solution use torch.sparse.mm and I have to convert one of tensor to be dense, which is not very memory efficient.</p><NewLine><p>Do you have some good suggestions? Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/Chen-Cai-OSU,(Chen-Cai-OSU),Chen-Cai-OSU,"June 13, 2020,  4:54pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sparse matrix multiplication doesn’t seem to be implemented yet as seen in <a href=""https://github.com/pytorch/pytorch/issues/5262"">this feature request</a>, so I assume you would have to use a dense matrix for one tensor.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you. I end up transferring the tensor to cpu and use scipy.sparse to do the job. Hope there will be more features to support common sparse tensor computation.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Chen-Cai-OSU; <NewLine> ,"REPLY_DATE 1: June 14, 2020, 10:01am; <NewLine> REPLY_DATE 2: June 14, 2020,  4:41pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
82136,Copy_() and memory format,2020-05-20T08:38:11.392Z,7,529,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I know that in pytorch 1.5 <code>to()</code> and <code>clone()</code> can preserve formats and therefore we can send non-contiguous tensors between devices.</p><NewLine><p>I wonder, what is the case for <code>copy_()</code>?  can we send non-contagious tensors with it?</p><NewLine><p>If not, is there any suggested workaround for avoiding copy?<br/><NewLine>for example</p><NewLine><pre><code class=""lang-auto"">a = torch.randn(10,1, device=""cuda:1"").share_memory_()<NewLine>b = torch.randn(10,2, device=""cuda:0"")<NewLine>b = torch.transpose(b, 0,1)<NewLine>a.copy_(b)  # is it OK?<NewLine></code></pre><NewLine><p>In the example above we want to avoid using <code>to()</code>/<code>clone()</code> to avoid creating a new tensor and moving it to shared memory.</p><NewLine></div>",https://discuss.pytorch.org/u/seliad,(Saar Eliad),seliad,"May 20, 2020,  8:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p><code>.copy_()</code> will not change the contiguity of any Tensor.<br/><NewLine>It will just read the content from b and write it to a. Not changing the size/strides.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>a.copy_</code> is inplace operation and it never changes strides (and memory format) of <code>a</code>. So the result of <code>a.copy_(b)</code> going to be <code>a</code> with data of <code>b</code> and strides of <code>a</code>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks! Is there a way to copy the stride?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>If <code>a</code> is the same size as <code>b</code>, you can aggressively restride <code>a</code> with <code>a.as_strided_(b.shape, b.stride())</code> and do <code>a.copy_(b)</code> as next step. But this will surely break autograd.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>For (good) reasons, <code>.as_strided()</code> is actually supported by the autograd, so that will work <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/></p><NewLine><p>But even beyond that, here, since you override all the content of <code>a</code> anyway with the copy, all the gradients will flow towards <code>b</code> and the original value of <code>a</code> will just get 0s.<br/><NewLine>So autograd will work just fine <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I somewhat still get data corruption, doing just the forward pass and calling <code>as_strided_</code> when sizes are equal.<br/><NewLine>If I replace <code>copy_()</code> with <code>to()</code>, its totally OK.</p><NewLine><p>I wonder if its related to cuda streams or something?<br/><NewLine><code>a.copy_(b)</code> when a,b are on different devices?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well, it’s not because they have the same size that the as_strided will be valid.<br/><NewLine>Is <code>a</code> contiguous ? If <code>a</code> has some overlapping memory already, then it’s backing memory won’t be big enough <img alt="":confused:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=9"" title="":confused:""/></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I made sure its contiguous too<br/><NewLine>Here is what I did.</p><NewLine><pre><code class=""lang-auto"">device = ...<NewLine>ranks = [....]<NewLine>saved = [None for rank in ranks]<NewLine>a = saved[rank]<NewLine>if a is not None and b.size() == a.size() and b.storage_offset() == a.storage_offset() and b.stride() == a.stride() and b.is_contiguous() and a.is_contiguous():<NewLine>    # no need to call as_strided_<NewLine>    a.copy_(b)<NewLine>else:<NewLine>    a = b.to(device)<NewLine>    saved[rank] = a<NewLine></code></pre><NewLine><p>when we replace that ugly if with <code>if False:</code>  everything works.<br/><NewLine>(what happens next is sending a through queue and cloning the the receiver, and then normal neural net)</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>One if the risks of <code>as_strided</code> is that you can do fairly bad stuff. In particular, these are no checks and you can end up reading out of bounds of the original Tensor’s values (or even out of the Storage backing it).</p><NewLine><p>In the ocd above, if you just do <code>.to()</code> the first time and <code>.copy_()</code> afterwards. The layout of a will just be the layout of the first b. Is that not ok?<br/><NewLine>Why is it so important to keep b’s layout at each iteration?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>I thought it would be OK (that’s exactly what I did at first, just the <code>is not None</code> check)<br/><NewLine>but it didn’t work.<br/><NewLine>Then I gradually increased the checks.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>So you mean this does not work?</p><NewLine><pre><code class=""lang-auto"">device = ...<NewLine>ranks = [....]<NewLine>saved = [None for rank in ranks]<NewLine>a = saved[rank]<NewLine>if a is not None:<NewLine>    a.copy_(b)<NewLine>else:<NewLine>    a = b.to(device)<NewLine>    saved[rank] = a<NewLine></code></pre><NewLine><p>What is the issue you see with this?</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>exactly.</p><NewLine><p>I tried 2 networks.</p><NewLine><ol><NewLine><li>WideResnet: it works fine with <code>copy_()</code>. all <code>b</code> tensors are contiguous there.</li><NewLine><li>GPT2 (from huggingface): does not work. I know that some <code>b</code> tensors are non-contiguous.</li><NewLine></ol><NewLine><p>For the GPT2 the task is zero-shot on wikitext2.<br/><NewLine>With <code>to(device)</code> I restore the perplexity from the paper (~29)<br/><NewLine>With <code>copy_()</code> the perplexity explodes.(high crazy numbers, like 8400, 121931 and so on)</p><NewLine><p>I tired to look at the tensors with the debugger, they look fine.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ho ok… that looks more like a multiprocessing issue then if the Tensors looks fine?<br/><NewLine>Layout won’t change anything about the values computed (unless bugs). So you should not see any difference here !</p><NewLine><p>Can you make a small code sample that repro this?</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you please try to <code>cuda.synchronize()</code> after <code>copy_</code> calls? When you are looking at tensors in debugger you are actually synchronizing to make them (data) observable.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I found the bug. It was indeed was some multiprocessing/threading/multi-cuda-stream/ issue + contiguous we discussed.<br/><NewLine>Everything solved and <code>copy_()</code> works. Thanks <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>In  case it will help someone:<br/><NewLine>I actually do notice some deadlocks when combining <code>to()</code> and <code>copy_()</code> like mentioned above.<br/><NewLine>Deadlock happens inside (i.e compiled cuda code) of <code>to()</code> at the second call, that is<br/><NewLine><code>to()</code>, <code>copy_()</code>, <code>copy_()</code>,…,<code>copy_()</code>, <code>to()</code>, deadlock.<br/><NewLine>Anyway, I got frustrated and now I’m using only <code>to()</code>, as its quite minor optimization I wasted too much time on.</p><NewLine><p>I think its related to RTX2080ti not supporting p2p (I looked at the cuda code that does the <code>to()</code>  few weeks ago, and I think it assumes something about p2p, but I did not bother to check it thoroughly.)</p><NewLine><p>I can’t share my code (yet), but as soon as I will I’ll share the full example</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>If you have a repro that deadlock on a given hardware (and is reproducible on other similar card to be sure it’s not a faulty card), please open an issue on github!  Thanks.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/VitalyFedyunin; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/seliad; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/VitalyFedyunin; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/seliad; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/seliad; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/seliad; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/seliad; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/VitalyFedyunin; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/seliad; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/seliad; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: May 20, 2020,  4:14pm; <NewLine> REPLY_DATE 2: May 20, 2020,  3:50pm; <NewLine> REPLY_DATE 3: May 20, 2020,  3:54pm; <NewLine> REPLY_DATE 4: May 20, 2020,  4:14pm; <NewLine> REPLY_DATE 5: May 20, 2020,  4:34pm; <NewLine> REPLY_DATE 6: May 20, 2020,  4:39pm; <NewLine> REPLY_DATE 7: May 20, 2020,  4:41pm; <NewLine> REPLY_DATE 8: May 20, 2020,  4:53pm; <NewLine> REPLY_DATE 9: May 20, 2020,  5:06pm; <NewLine> REPLY_DATE 10: May 20, 2020,  5:28pm; <NewLine> REPLY_DATE 11: May 20, 2020,  6:12pm; <NewLine> REPLY_DATE 12: May 20, 2020,  6:34pm; <NewLine> REPLY_DATE 13: May 20, 2020,  6:46pm; <NewLine> REPLY_DATE 14: May 20, 2020,  6:51pm; <NewLine> REPLY_DATE 15: May 23, 2020,  1:00pm; <NewLine> REPLY_DATE 16: June 12, 2020,  8:04am; <NewLine> REPLY_DATE 17: June 12, 2020,  1:54pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: 1 Like; <NewLine> REPLY 15 LIKES: 1 Like; <NewLine> REPLY 16 LIKES: 1 Like; <NewLine> REPLY 17 LIKES: ; <NewLine> 
85092,Out Of Memory error in GPU when performing random search,2020-06-11T18:01:28.675Z,0,81,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am training an lstm model and currently I am performing random search.</p><NewLine><p>Initially after each random search I was emptying the cache (<code>torch.cuda.empty_cache()</code>), however I was getting the OOM error after some number of random searches (usually around 3).</p><NewLine><p>Then I read that in order for the memory to be freed I need to do <code>del variable</code> first. However, even after that I continued having the same issue.  I am tracing the allocated gpu memory ( <code>torch.cuda.</code> <code>memory_allocated()</code>) and I can see that after each random search the memory is being freed. Although, when a new random search starts the memory allocated is a bit higher than the previous random search. I don’t think that this is caused by some variable that is not erased. Is there something that I am missing?</p><NewLine><p><img alt=""image"" data-base62-sha1=""xd7jKFIbuvzi2hEnWo3dMrgWi93"" height=""167"" src=""https://discuss.pytorch.org/uploads/default/original/3X/e/8/e8c35acb9f4e28ca809761a1b6431fdf4b202ec9.png"" width=""379""/></p><NewLine></div>",https://discuss.pytorch.org/u/ire,,ire,"June 11, 2020,  7:35pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What is the random search doing? Could it change some hyperparameters and thus increase the model parameters, which could yield the OOM issue?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: June 12, 2020,  8:41am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
84546,GPU running out of memory at only 50% consumption,2020-06-07T17:43:01.151Z,0,133,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m getting the following error (during the backward pass):<br/><NewLine>RuntimeError: CUDA out of memory. Tried to allocate 4.93 GiB (GPU 0; 23.65 GiB total capacity; 9.11 GiB already allocated; 10.44 GiB free; 12.34 GiB reserved in total by PyTorch) (malloc at /opt/conda/conda-bld/pytorch_1587428398394/work/c10/cuda/CUDACachingAllocator.cpp:289)</p><NewLine><p>I have exclusive access to this GPU while the process is running. It seems like PyTorch isn’t reserving enough memory? Clearly there is enough free memory (10.44 GiB), unless memory fragmentation is so bad that I can’t use ~50% of the GPU memory. This always happens on the backward pass - I’ve even tried using torch.utils.checkpoint, but it didn’t make much of a difference since the forward pass does not take up nearly as much memory even with grad.<br/><NewLine>My model uses Transformer layers as well as sparse-dense matrix multiplication, with variable-length sequences, so I do expect some fragmentation, but could it be causing it to this degree when I call loss.backward()? How could I fix this?<br/><NewLine>Does backpropagation on sparse-dense matrix multiplication w.r.t. the sparse tensor return a sparse tensor? If it is returning a dense tensor, this could be the reason for high memory consumption.</p><NewLine></div>",https://discuss.pytorch.org/u/santient,(Santiago Benoit),santient,"June 7, 2020,  5:43pm",,,,,
83338,Gpu memory cost,2020-05-29T08:00:15.611Z,1,84,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">      out = self.conv1(x)<NewLine>      out = self.norm1(out)<NewLine>      out = self.relu(out)<NewLine><NewLine>      out = self.conv2(out)<NewLine>      out = self.norm2(out)<NewLine>      out = self.relu(out)<NewLine><NewLine>      out = self.conv3(out)<NewLine>      out = self.norm3(out)<NewLine></code></pre><NewLine><ol><NewLine><li><NewLine><code>training</code> of  conv and norm is false</li><NewLine><li>track_running_stats of norm is True</li><NewLine></ol><NewLine><p>When I use pycharm to debug the code, from <code>conv1</code> to <code>conv3</code>, the gpu memory not increase. But I exec <code>norm3</code>, the gpu memory increase.<br/><NewLine>If <code>training</code> of  conv and norm is True, I test that I exec every sentence, the gpu memory increase.</p><NewLine><p>When I exec every sentence， that create new out, why the gpu memory not increase?<br/><NewLine>But why I exec norm3 the gpu memory increase?</p><NewLine></div>",https://discuss.pytorch.org/u/as754770178,(As754770178),as754770178,"May 29, 2020,  8:00am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see the requires_grad of weight and bias in norm is False.</p><NewLine><p>The code in <code>mmdet/models/backbones/resnet.py</code> of  <a href=""https://github.com/open-mmlab/mmdetection"" rel=""nofollow noopener"">mmdetection</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Keep in mind that the GPU api is asynchronous. So you might want to add a <code>torch.cuda.syncrhonize()</code> after the line to make sure it finished executing.</p><NewLine><p>Also since, you reuse the <code>out</code> variable, the old Tensor that out was pointing to is not reachable anymore and can be deleted when you’re not training (when training, it needs to be kept around to be able to compute the backward).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/as754770178; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: May 29, 2020,  8:20am; <NewLine> REPLY_DATE 2: May 29, 2020,  2:04pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
80847,Best way to load a lot of training data,2020-05-12T12:11:55.285Z,3,287,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I am trying to create a model that understand patterns in human voice and have a lot of voice samples (133K different files overall size 40GB). I run a lot of preprocessing and then generate a feature cube which I want to give to Pytorch model.</p><NewLine><p>So far I have been doing the preprocessing and cube generation offline so that I create the feature cubes and write them to a “*.pt” file using Torch.save().<br/><NewLine>I have currently only used 5K samples that generated an *.pt file of 1GB (I would then expect a file 26GB big).<br/><NewLine>I then do a Torch.load on the training host loading everything in memory using TensorDataset and DataLoader</p><NewLine><pre><code class=""lang-auto"">features = torch.load(featpath)<NewLine>labels   =  torch.load(labpath)<NewLine>dataset = torch.utils.data.TensorDataset(features,labels)<NewLine>return torch.utils.data.DataLoader(dataset,shuffle=True,batch_size=batch_size,num_workers=num_workers)<NewLine></code></pre><NewLine><p>It worked ok with fewer samples and it probably will work if I give enough memory / disk space (I use S3 and Sagemaker)</p><NewLine><p>I am just wondering am I doing the right thing ? Is there a best practices for large datasets ? Streaming from disk ? or do they have to be all loaded in memory ? I am assuming here that DataSet/Loader do this.</p><NewLine></div>",https://discuss.pytorch.org/u/Giuseppe_Sarno,(Giuseppe Sarno),Giuseppe_Sarno,"May 12, 2020, 12:21pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>As long as you have enough memory to do so, there’s no problem with what you are doing. If the data were to get too large, the easiest solution would be to split the samples into individual files and only load them in a custom Dataset <code>__getitem__</code> method.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks just to clarify.<br/><NewLine>The <strong>getitem</strong> will have to flip from one to the other when one has finished the cycle ?<br/><NewLine>This means that for every epoch I would be flipping from one to another and back to the first a the beginning of next epoch?<br/><NewLine>Would be nice if there was an example but this seems reasonable in case I run out of memory. I still do not have a lot of good handle on Datasets.</p><NewLine><p>Thanks you again.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well, to avoid unnecessary pain, you would split the data so that each sample tensor will have its own <code>.pt</code> file. Then, in the <code>__getitem__</code>, you call the load, like this</p><NewLine><pre><code class=""lang-auto"">class MyDataset(torch.util.data.Dataset):<NewLine>   def __init__(self, root):<NewLine>        self.root = root<NewLine>        self.files = os.listdir(root) # take all files in the root directory<NewLine>   def __len__(self):<NewLine>        return len(self.files)<NewLine>   def __getitem__(self, idx):<NewLine>        sample, label = torch.load(os.path.join(self.root, self.files[idx])) # load the features of this sample<NewLine>        return sample, label<NewLine></code></pre><NewLine><p>And then use this dataset in the same way. Keeping the data in several chunks rather than individual samples would be … problematic.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, hopefully I will fit in memory but this seems a pretty simple solution.<br/><NewLine>Would there be any issue with Shuffling and DataLoader ? I wonder how would the shuffling work in this case.</p><NewLine><p>Thanks again.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>With shuffling enabled, the dataloader randomizes the <code>idx</code> parameter of <code>__getitem__</code>, effectively choosing a random file each time.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/futscdav; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Giuseppe_Sarno; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/futscdav; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Giuseppe_Sarno; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/futscdav; <NewLine> ,"REPLY_DATE 1: May 12, 2020, 12:22pm; <NewLine> REPLY_DATE 2: May 12, 2020, 12:34pm; <NewLine> REPLY_DATE 3: May 12, 2020,  5:23pm; <NewLine> REPLY_DATE 4: May 12, 2020,  5:16pm; <NewLine> REPLY_DATE 5: May 12, 2020,  5:21pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
79584,How to automatically free CUDA memory when using same reference (variable name) in torch operation?,2020-05-03T22:57:41.375Z,0,118,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, here is one toy code about this issue:</p><NewLine><pre><code class=""lang-auto"">import torch <NewLine><NewLine>torch.cuda.set_device(3)<NewLine>a = torch.rand(10000, 10000).cuda()<NewLine># monitor cuda:3 by ""watch -n 0.01 nvidia-smi""<NewLine>a = torch.add(a, 0.0)<NewLine># keep monitoring<NewLine></code></pre><NewLine><p>When using same variable name “a” in torch.add, I find the old a’s memory is not freed in cuda, it still exists even though the reference is updated, and I cannot reach tp original memory since the reference is gone. How could I make the memory of old reference automatically freed when use same variable name in left/right of torch operations?</p><NewLine></div>",https://discuss.pytorch.org/u/themoonboy,,themoonboy,"May 3, 2020, 11:00pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>a</code> will be freed automatically, if no reference points to this variable.<br/><NewLine>Note that PyTorch uses a memory caching mechanism, so <code>nvidia-smi</code> will show all allocated and cached memory as well as the memory used by the CUDA context.</p><NewLine><p>Here is a small example to demonstrate this behavior:</p><NewLine><pre><code class=""lang-python""># Should be empty<NewLine>print('allocated ', torch.cuda.memory_allocated() / 1024**2)<NewLine>print('cached ', torch.cuda.memory_cached() / 1024**2)<NewLine>&gt; allocated  0.0<NewLine>&gt; cached  0.0<NewLine><NewLine># Initial setup<NewLine>a = torch.rand(1024, 1024, 128).cuda()<NewLine>print('allocated ', torch.cuda.memory_allocated() / 1024**2)<NewLine>print('cached ', torch.cuda.memory_cached() / 1024**2)<NewLine>&gt; allocated  512.0<NewLine>&gt; cached  512.0<NewLine><NewLine># torch.add will use a temp variable, as it's not inplace<NewLine>a = torch.add(a, 0.0)<NewLine>print('allocated ', torch.cuda.memory_allocated() / 1024**2)<NewLine>print('cached ', torch.cuda.memory_cached() / 1024**2)<NewLine>&gt; allocated  512.0<NewLine>&gt; cached  1024.0<NewLine><NewLine># Delete reference<NewLine>a = 1.<NewLine>print('allocated ', torch.cuda.memory_allocated() / 1024**2)<NewLine>print('cached ', torch.cuda.memory_cached() / 1024**2)<NewLine>&gt; allocated  0.0<NewLine>&gt; cached  1024.0<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: May 22, 2020, 10:50pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
78429,Optimizer closure causes out-of-memory error,2020-04-25T20:50:05.808Z,1,102,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I met an out-of-memory error when I use closure in my new optimizer, which needs the value of the loss f(x) to update the parameters x in each iteration. The scheme is like</p><NewLine><blockquote><NewLine><p>g(x) = f’(x)/f(x)<br/><NewLine>x = x - a*g(x)</p><NewLine></blockquote><NewLine><p>So I define a closure function before optimizer.step</p><NewLine><pre><code class=""lang-python"">def closure():<NewLine>    optimizer.zero_grad()<NewLine>    outputs = net(inputs)<NewLine>    loss = criterion(outputs, targets)<NewLine>    loss.backward()<NewLine>    return loss, outputs<NewLine>loss, outputs = optimizer.step(closure) <NewLine></code></pre><NewLine><p>It works fine when I apply this optimizer to train a CNN model for MNIST. However, when I use this optimizer to train ResNet34 for cifar10, even on an HPC cluster, the program will be killed after a few iterations because of an out-of-memory error.</p><NewLine><p>I think the memory of the compute node (128G) is large enough, and it works fine when I change the optimizer to torch.optim.SGD, with the same other settings. The corresponding code for SGD is:</p><NewLine><pre><code class=""lang-python"">optimizer.zero_grad()<NewLine>outputs = net(inputs)<NewLine>loss = criterion(outputs, targets)<NewLine>loss.backward()<NewLine>optimizer.step()<NewLine></code></pre><NewLine><p>So the only difference I can notice is the use of closure function in the new optimizer.</p><NewLine><p>I have two questions:</p><NewLine><ol><NewLine><li>Did I use the closure function correctly? So far this new optimizer works fine for some smaller data like MNIST, but since closure is rarely used in optimizers, and there are not many examples, so I am not so sure if I used the closure correctly.</li><NewLine><li>Is the out-of-memory error caused by the use of closure function? It’s not clear for me how the closure and optimizer.step work in PyTorch so I have no idea where this out-of-memory error comes from.</li><NewLine></ol><NewLine><p>Any help is highly appreciated! Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/Tia,,Tia,"April 25, 2020,  8:50pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not sure how your new optimizer is defined, but e.g. <code>LBFGS</code> (which also requires a closure) can be memory intensive as stated in the <a href=""https://pytorch.org/docs/stable/optim.html#torch.optim.LBFGS"">docs</a>:</p><NewLine><blockquote><NewLine><p>This is a very memory intensive optimizer (it requires additional  <code>param_bytes * (history_size + 1)</code>  bytes). If it doesn’t fit in memory try reducing the history size, or use a different algorithm.</p><NewLine></blockquote><NewLine><ol><NewLine><li>If your optimizer’s <code>step</code> method expects the <code>output</code> and <code>loss</code> as a return value, then the <code>closure</code> looks correct. You could compare it to <a href=""https://github.com/pytorch/examples/blob/69d2798ec7fb4f87b320a1848203da5346675b95/time_sequence_prediction/train.py#L57-L64"">this example</a>.</li><NewLine><li>Might be and it depends, what <code>optimizer.step</code> is doing with the <code>closure</code>. If you are storing the history of the losses and outputs, then an increased memory usage is expected.</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply! I later realized that I just need the value of the the loss, not all the metadata. so instead of saving loss and outputs, I changed my code as</p><NewLine><pre><code class=""lang-python"">def closure():<NewLine>    optimizer.zero_grad()<NewLine>    outputs = net(inputs)<NewLine>    loss = criterion(outputs, targets)<NewLine>    loss.backward()<NewLine>    return loss.detach(), outputs.detach()<NewLine>loss, outputs = optimizer.step(closure)<NewLine></code></pre><NewLine><p>That works fine, no out-of-memory error.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Tia; <NewLine> ,"REPLY_DATE 1: April 26, 2020,  6:57pm; <NewLine> REPLY_DATE 2: April 26, 2020,  6:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
77087,Smaller torch saved model,2020-04-16T18:48:33.878Z,5,248,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a trained NTS-NET that uses 108 MB on file storage. In my server I do not have enough space but is only some MB. So I compress “state_dict” using “tar.gz” and I arrive to 100 MB. It i just enought. So to load the model I use the funcion</p><NewLine><pre><code class=""lang-auto"">import pickle<NewLine>import tarfile<NewLine><NewLine>from torch.serialization import _load, _open_zipfile_reader<NewLine><NewLine><NewLine>def torch_load_targz(filep_ath):<NewLine>    tar = tarfile.open(filep_ath, ""r:gz"")<NewLine>    member = tar.getmembers()[0]<NewLine>    with tar.extractfile(member) as untar:<NewLine>        with _open_zipfile_reader(untar) as zipfile:<NewLine>            torch_loaded = _load(zipfile, None, pickle)<NewLine>    return torch_loaded<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    torch_load_targz(""../models/nts_net_state.tar.gz"")<NewLine>    # equivalet for torch.load(""../models/nts_net_state.pt"")  for .tar.gz <NewLine></code></pre><NewLine><p>So at the end I read the torch model from tar.gz directly. But in this way the prediction are too slow.<br/><NewLine>Exist some better solution at this problem?</p><NewLine><p>(I’m using torch-1.4.0, and python 3.6)</p><NewLine></div>",https://discuss.pytorch.org/u/nicolalandro,(Nicolalandro),nicolalandro,"April 16, 2020,  6:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is the loading of the <code>state_dict</code> slow or the model predictions?<br/><NewLine>The latter shouldn’t be influenced by how the <code>state_dict</code> is loaded or are you reloading it in every iteration?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The slow part is the extract tar.gz and it save only 8MB into the state_dict stored file.<br/><NewLine>I’m speaking about only predictions (of small data) so I do not have iterations inside a single call.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It is expected that unzipping a file will take longer than e.g. reading a binary file.<br/><NewLine>What kind of system are you using that you are running out of memory for 108MB?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>No. I fill my server memory with:</p><NewLine><ul><NewLine><li>with torch and torchvision and other libraries</li><NewLine><li>and 108MB of trained model</li><NewLine></ul><NewLine><p>For example I see that transform a tensorflow model using tensorflow-lite the size in MB of the model can be reduced a lot. I was wondering for something like that using pytorch. If exist some other way may be it is less slow.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could try to <a href=""https://pytorch.org/docs/stable/quantization.html"">quantize</a> your model to reduce the size.<br/><NewLine>However, I’m not sure, how experimental this feature is at the moment,</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you! I try this way! And I wondering also for other.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/nicolalandro; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/nicolalandro; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/nicolalandro; <NewLine> ,"REPLY_DATE 1: April 17, 2020,  3:25am; <NewLine> REPLY_DATE 2: April 17, 2020,  8:03am; <NewLine> REPLY_DATE 3: April 17, 2020,  9:29am; <NewLine> REPLY_DATE 4: April 17, 2020, 10:03am; <NewLine> REPLY_DATE 5: April 18, 2020,  1:46am; <NewLine> REPLY_DATE 6: April 18, 2020,  6:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
77235,Phantom PyTorch Data on GPU,2020-04-17T21:04:01.666Z,0,131,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a Python script which is running a PyTorch model on a specific device, passed by using the <code>.to()</code> function and then the integer number of the device. I have four devices available so I alternate between 0, 1, 2, 3. This integer is set to a variable <code>args.device_id</code> that I use around my code.</p><NewLine><p>I’m getting this situation where I see some phantom data appearing on a GPU device which I didn’t specify ever. See the picture below (this output is from <code>gpustat</code> which is a <code>nvidia-smi</code> wrapper):<br/><NewLine><img alt=""image"" data-base62-sha1=""7JtbCTg1yfZaU3ALxf8LNgx6xQ4"" height=""82"" src=""https://discuss.pytorch.org/uploads/default/original/3X/3/6/36331062d378ce1740f03f6de79162a3cc3c70c0.png"" width=""673""/></p><NewLine><p>Note that when I force-quit my script, all the GPUs return to zero usage. So there are no other programs running from other users.</p><NewLine><p>I specifically added <a href=""https://discuss.pytorch.org/t/how-to-debug-causes-of-gpu-memory-leaks/6741/3"">this snippet</a> to my main training/test loop, based on a previously-found PyTorch Discuss thread regarding finding-all-tensors. <code>gc</code> here is the Python garbage collection built-in module.</p><NewLine><pre><code class=""lang-auto"">...<NewLine>for obj in gc.get_objects():<NewLine>    try:<NewLine>        if (torch.is_tensor(obj) or<NewLine>            (hasattr(obj, 'data') and torch.is_tensor(obj.data))):<NewLine>            print(type(obj), obj.size(), obj.device)<NewLine>    except:<NewLine>        pass<NewLine>...<NewLine></code></pre><NewLine><p>Here is the result that I see on the command prompt:</p><NewLine><pre><code class=""lang-auto"">2020-04-17 20:56:06 [INFO] Process ID: 3761, Using GPU device 1...<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1000, 63, 513]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1000, 15872]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([51920]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([51920]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1, 51920]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1, 51920]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1, 51920]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1, 513, 203, 2]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1, 203, 513]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1, 513, 203, 2]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1, 203, 513]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1, 203, 513, 2]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1, 203, 513]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1, 203, 513]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([]) cpu<NewLine>&lt;class 'torch.nn.parameter.Parameter'&gt; torch.Size([2048]) cuda:1<NewLine>&lt;class 'torch.nn.parameter.Parameter'&gt; torch.Size([2048]) cuda:1<NewLine>&lt;class 'torch.nn.parameter.Parameter'&gt; torch.Size([2048, 512]) cuda:1<NewLine>&lt;class 'torch.nn.parameter.Parameter'&gt; torch.Size([2048, 512]) cuda:1<NewLine>&lt;class 'torch.nn.parameter.Parameter'&gt; torch.Size([2048]) cuda:1<NewLine>&lt;class 'torch.nn.parameter.Parameter'&gt; torch.Size([2048]) cuda:1<NewLine>&lt;class 'torch.nn.parameter.Parameter'&gt; torch.Size([2048, 512]) cuda:1<NewLine>&lt;class 'torch.nn.parameter.Parameter'&gt; torch.Size([2048, 512]) cuda:1<NewLine>&lt;class 'torch.nn.parameter.Parameter'&gt; torch.Size([2048]) cuda:1<NewLine>&lt;class 'torch.nn.parameter.Parameter'&gt; torch.Size([2048]) cuda:1<NewLine>&lt;class 'torch.nn.parameter.Parameter'&gt; torch.Size([2048, 512]) cuda:1<NewLine>&lt;class 'torch.nn.parameter.Parameter'&gt; torch.Size([2048, 513]) cuda:1<NewLine>&lt;class 'torch.nn.parameter.Parameter'&gt; torch.Size([513, 512]) cuda:1<NewLine>&lt;class 'torch.nn.parameter.Parameter'&gt; torch.Size([513]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1000, 16000]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1000, 16000]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1000, 16000]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1000, 63, 513]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1000, 63, 513]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1000, 63, 513]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1000, 63, 513, 2]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1000, 63, 513]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1000]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1000]) cuda:1<NewLine>&lt;class 'torch.Tensor'&gt; torch.Size([1000, 2]) cuda:1<NewLine></code></pre><NewLine><p>So everything on this list (except for one random outlier) all say “cuda:1”. How can I identify what data is sitting in “cuda:3” then? Appreciate the help. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>Another tid-bit, the phantom data always builds up to 577MiB exactly, never more than that. This behavior occurs even if I set the classic <code>os.environ['CUDA_VISIBLE_DEVICES']</code> variable at the top of the script.</p><NewLine></div>",https://discuss.pytorch.org/u/actuallyaswin,(Aswin Sivaraman),actuallyaswin,"April 17, 2020,  9:10pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you set the env variable in your terminal via:</p><NewLine><pre><code class=""lang-python"">CUDA_VISIBLE_DEVICES=1,3 python script.py args<NewLine></code></pre><NewLine><p>If you are using the <code>os.environ</code> method inside your script, you would have to make sure it’s called before any <code>torch</code> imports.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: April 18, 2020,  4:27am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
74875,Assignment to split tensor causes memory leak,2020-03-31T06:58:11.017Z,1,125,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to train a network for smoothing a point cloud.<br/><NewLine>It is applied independently for each point and has the surrounding points as input.<br/><NewLine>The function below performs the actual smoothing by splitting the points (<em>x</em>) and the neighborhoods (<em>y</em>) into batches and then calling the model.</p><NewLine><pre><code class=""lang-auto"">def do_smoothing(x, y, model, batch_size):<NewLine>    model.eval()<NewLine>    torch.no_grad()   # Make sure that no gradient memory is allocated<NewLine>    print('Used Memory: {:3f}'.format(torch.cuda.memory_allocated('cuda') * 1e-9 ))<NewLine>    xbatches = list(torch.split(x, batch_size))<NewLine>    ybatches = list(torch.split(y, batch_size))<NewLine>    assert(len(xbatches) == len(ybatches))<NewLine>    print('Used Memory: {:3f}'.format(torch.cuda.memory_allocated('cuda') * 1e-9 ))<NewLine>    for i in range(len(xbatches)):<NewLine>        print(""Batch {} [Memory: {:3f}]"".format(i + 1, torch.cuda.memory_allocated('cuda') * 1e-9))<NewLine>        tmp = self.forward(_xbatches[i])<NewLine>        xbatches[i] += tmp<NewLine>        del tmp<NewLine>        print(""Batch {} [Memory: {:3f}]"".format(i + 1, torch.cuda.memory_allocated('cuda') * 1e-9))<NewLine>        print(""------------------------------------"")<NewLine>    return x<NewLine></code></pre><NewLine><p>Unfortunately, the memory consumption is very high leading to an out of memory error. The console output is:</p><NewLine><pre><code class=""lang-auto"">Used Memory: 0.256431<NewLine>Used Memory: 0.256431<NewLine>Prior batch 1 [Memory: 0.256431]<NewLine>Post batch 1 [Memory: 2.036074]<NewLine>------------------------------------<NewLine>Prior batch 2 [Memory: 2.036074]<NewLine> -&gt; Out of memory error<NewLine></code></pre><NewLine><p>If I remove the line <code>xbatches[i] += tmp</code> the allocated memory is not changing (as expected).<br/><NewLine>If I also remove the line <code>del tmp</code> on the other hand the code once again allocates huge amounts of GPU memory.</p><NewLine><p>I assume that <a href=""https://pytorch.org/docs/stable/torch.html#torch.split"" rel=""nofollow noopener"">torch.split</a> creates views onto the tensor and therefore the update should not use additional memory.<br/><NewLine>Did I miss anything or is this unintended behavior?</p><NewLine><p>I am using pytorch 1.4.0 with CUDA 10.1 on a Windows 10 machine.</p><NewLine><p>Thanks in advance for any tips.</p><NewLine></div>",https://discuss.pytorch.org/u/DominikP,(Dominik Penk),DominikP,"March 31, 2020,  6:58am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you try to use <code>torch.no_grad()</code> in a <code>with</code> statement:</p><NewLine><pre><code class=""lang-python"">model.eval()<NewLine>with torch.no_grad():<NewLine>    xbatches = ...<NewLine></code></pre><NewLine><p>If I’m not mistaken, your current approach shouldn’t change the gradient behavior.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much, this indeed fixes the problem.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/DominikP; <NewLine> ,"REPLY_DATE 1: March 31, 2020,  7:51am; <NewLine> REPLY_DATE 2: March 31, 2020,  7:51am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
