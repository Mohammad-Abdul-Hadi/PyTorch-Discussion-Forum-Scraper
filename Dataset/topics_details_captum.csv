id,title,created_at,reply_count,views,description,creator_link,creator_name,creator_alias,post_date,post_likes,replies,repliers_links,reply_dates,reply_likes
68657,About the Captum category,2020-02-04T17:02:37.072Z,0,101,"<div class=""post"" itemprop=""articleBody""><NewLine><p>(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters.)</p><NewLine><p>Use the following paragraphs for a longer description, or to establish category guidelines or rules:</p><NewLine><ul><NewLine><li><NewLine><p>Why should people use this category? What is it for?</p><NewLine></li><NewLine><li><NewLine><p>How exactly is this different than the other categories we already have?</p><NewLine></li><NewLine><li><NewLine><p>What should topics in this category generally contain?</p><NewLine></li><NewLine><li><NewLine><p>Do we need this category? Can we merge with another category, or subcategory?</p><NewLine></li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/albanD,(Alban D),albanD,"February 4, 2020,  5:02pm",,,,,
96963,Conceptually Linked Interpretability,2020-09-21T05:39:31.154Z,0,20,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The goal of “Conceptual Link” is to optimize workflows and improve human interpretability.</p><NewLine><p>In essence what I suggest is a conceptual link between;<br/><NewLine>• model or rather suite of tools<br/><NewLine>• training: datasets, word embedding &amp; libraries<br/><NewLine>• use-case potential/scenario.</p><NewLine><p>A great way to explain the idea of “conceptual linking” for virtual assistants would be through this image.</p><NewLine><p><img alt=""giphy"" data-base62-sha1=""lFc2KenBazRC4gD0JRZTO7WOzJo"" height=""375"" src=""https://discuss.pytorch.org/uploads/default/original/3X/9/7/97d5beaf0bd918e1313216d646e44d17010083ba.gif"" width=""500""/></p><NewLine><p>Cartoon personification of career choices, is a good set to establish conceptually linked interpretability for your models, training process/datasets, and use case scenarios.</p><NewLine><p>It is but a drop in the ocean. The tip of the iceberg if you will.</p><NewLine><p>Also keep in mind this initial set is ideal for NLP – as a platform facilitating customizable iterations of Virtual Assistants (VA).</p><NewLine><p>The idea of conceptual link is not limited to Personification nor VA. Conceptual Links for other use case scenarios (self driving, anlysis engines ect) can be represented as animals, machines or inanimate objects.</p><NewLine><p>This platform and the idea of users customizing and creating VA should facilitate creative expression – as in a VA built from an entirely fictional character or a VA created by user on the front end as if it is a work of art.</p><NewLine><p>Another benefit of a platform built around creating VA is a potential solution for managing deepfakes. In theory a user could record their likeness into a VA for many reasons. One reason would be to establish control and ownership of their likeness. After a double verification process confirms that the user is the real deal run a Linear-Feedback Shift Register ID with non linear encryption tweaks to create a secure fast and efficient cyclic redundancy check for the entire platform.</p><NewLine><p>Before I reach to far ahead.</p><NewLine><p>I share this idea with the hope that this approach will not only improve your workflows but significantly improve presentation of your projects.</p><NewLine><p>Now for the moment of zen.</p><NewLine><p>In the context of a mobile game (with AR features) built around raising, training and battling virtual creatures there are many processes you can leverage to get users to  contribute tagged data for machine learning models.<br/><NewLine>What if you feed the virtual creatures by taking photos of a requested object? Producing sounds of a requested type or nature? Draw requested shapes on the virtual screen?</p><NewLine><p>Suffice it to say these datasets could be partially guided via game design in effort to produce potentially valuable datastreams for machine learning.</p><NewLine><p>How valuable?</p><NewLine><p>I am uncertain. I get that quality over quantity is ideal for initial training runs but for testing or overlapping analysis of an emergent user generated dataset… I don’t understand enough about the way these systems work to determine a value for such a dataset at this time.</p><NewLine><p>In essence the easiest approach would be utilizing the game experience to generate a stream of unique iterations for well classified objects – with random levels of junk data mixed in.</p><NewLine><p>Then there is the training aspect of the game experience which could be designed in some way to provide valuable data to aid computer vision models while being fun for the user at the same time.</p><NewLine><p>Next up is the evolution process which could be designed to generate another machine learning dataset.</p><NewLine><p>Then last but certainly not least is the battle mode. I will leave this one wide open for your imagination to run wild with.</p><NewLine><p>I have several ideas regarding clever use case scenarios for machine learning software, but I need to do more research to determine viability of those ideas while also figuring out where to even post them here…</p><NewLine><p>I look forward to learning more about pytorch.</p><NewLine><p>One more question; for computer vision without 3D spatial mapping changing nothing about a well trained highly accurate image analysis algorithm what would happen if you put it in front of a Mandelbrot fractal zoom?</p><NewLine><p>Food for thought…</p><NewLine></div>",https://discuss.pytorch.org/u/cubytes,(Derik West),cubytes,"September 21, 2020,  5:39am",,,,,
89496,GradCam - Zero output,2020-07-16T19:43:34.921Z,1,100,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone,</p><NewLine><p>I just started to dig into Explainable ML/DNN field and was trying to play with “captum” a bit when I found a problem that I cannot explain.</p><NewLine><p>N.B.<br/><NewLine>Since I can’t upload more than image and share more than 2 links (D:), ill share 2 images by links.</p><NewLine><p>I’ve a custom model to perform classification on MNIST (it reaches 97% accuracy and since I use it as a playground im not interested to improve it) with this architecture:</p><NewLine><pre><code class=""lang-auto"">MnistNet(<NewLine>  (FeatureExtractor): Sequential(<NewLine>    (0): Conv2d(1, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))<NewLine>    (1): ReLU()<NewLine>    (2): Conv2d(8, 16, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))<NewLine>    (3): ReLU()<NewLine>    (4): Conv2d(16, 20, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))<NewLine>    (5): ReLU()<NewLine>    (6): Conv2d(20, 24, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))<NewLine>    (7): ReLU()<NewLine>  )<NewLine>  (Classifier): Sequential(<NewLine>    (0): Flatten()<NewLine>    (1): Linear(in_features=1944, out_features=256, bias=True)<NewLine>    (2): Dropout(p=0.3, inplace=False)<NewLine>    (3): ReLU()<NewLine>    (4): Linear(in_features=256, out_features=64, bias=True)<NewLine>    (5): Dropout(p=0.3, inplace=False)<NewLine>    (6): ReLU()<NewLine>    (7): Linear(in_features=64, out_features=10, bias=True)<NewLine>  )<NewLine>)<NewLine></code></pre><NewLine><p>Now I’m trying to perform GuidedGradCam against an image of a four which is rightfully classified with 0.999 probability.</p><NewLine><p>After I perform a “run” of Guided GradCam using every possible target (0-9) i get these results:<br/><NewLine><a href=""https://freeimage.host/i/dfkz0X"" rel=""nofollow noopener"">Results of GuidedGradCam</a><br/><NewLine>Now, even without considering the poor general results, the real problem is the image for the target 4 (the true label and the real model output), in fact it’s “blank”!!</p><NewLine><p>I interpreted this result as something caused by the final ReLU pass in the GradCam so to test it, i implemented my own version of GradCam (not GuidedGradCam!) and this is the result with the same input:<br/><NewLine><a href=""https://freeimage.host/i/dfknsI"" rel=""nofollow noopener"">Result of custom GradCam</a><br/><NewLine>Not how the results are similar (target 4 and 9 still blank) with expected differences (its GradCam without matmul from DeepLift, so a “less resolution” is expected as it has written in the paper).</p><NewLine><p>Finally i test what happens if i take out the ReLU from my own version of GradCam. In other words i modify the last line of code such that the return now is:</p><NewLine><pre><code class=""lang-auto"">gradcam = torch.sum(A_k*alpha_k, dim=(0))<NewLine></code></pre><NewLine><p>and not</p><NewLine><pre><code class=""lang-auto"">gradcam = nn.ReLU()(torch.sum(A_k*alpha_k, dim=(0)))<NewLine></code></pre><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/859d2a6fce1630cbd2684a5778ab94369b244b83"" href=""https://discuss.pytorch.org/uploads/default/original/3X/8/5/859d2a6fce1630cbd2684a5778ab94369b244b83.png"" title=""not_relu_results""><img alt=""not_relu_results"" data-base62-sha1=""j40eeSaEVTm9ex3sGLyEbPEvR3d"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/5/859d2a6fce1630cbd2684a5778ab94369b244b83_2_10x10.png"" height=""251"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/5/859d2a6fce1630cbd2684a5778ab94369b244b83_2_517x251.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/5/859d2a6fce1630cbd2684a5778ab94369b244b83_2_517x251.png, https://discuss.pytorch.org/uploads/default/optimized/3X/8/5/859d2a6fce1630cbd2684a5778ab94369b244b83_2_775x376.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/8/5/859d2a6fce1630cbd2684a5778ab94369b244b83_2_1034x502.png 2x"" width=""517""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">not_relu_results</span><span class=""informations"">1161×565 55.3 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>Now target 4 and 9 have very “important” outputs with really low values (up to -50 and -14). It seems that the output of GradCam should be considered in its absolute value without performing ReLU.</p><NewLine><p><strong>Question</strong><br/><NewLine>Given the above context and results, has anyone a good explanations of why this happens and why using ReLU make sense in the original GradCam?</p><NewLine></div>",https://discuss.pytorch.org/u/Francesco_Grimaldi,(Francesco Grimaldi),Francesco_Grimaldi,"July 16, 2020,  7:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/francesco_grimaldi"">@Francesco_Grimaldi</a>, this is a surprising result. To understand the cause of this further, I’d recommend looking at the activations and gradients separately of the last convolutional layer, since the product of these leads to GradCAM. You can do this with Captum using LayerActivation and dividing LayerGradientXActivation by LayerActivation for the gradient.</p><NewLine><p>It could be that gradients of the target class of 4 with respect to the final conv output is mostly negative, potentially because the output is saturated, so the gradients are not representative of the “importance” of the corresponding activation. Looking at the gradients and activations separately will be helpful to further understand this.</p><NewLine><p>Also, Captum does offer GradCAM itself as LayerGradCAM (since it technically outputs attributions with shape matching the layer output) with an argument relu_attributions to decide whether to apply a ReLU to the final attribution.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank your very much for the answer <em>vivekm</em>. I’ll try to use both my version and the Captum one to see if they are consistent and try to inspect this particular case a bit more (99% of the times using relu works fine).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vivekm; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Francesco_Grimaldi; <NewLine> ,"REPLY_DATE 1: August 3, 2020,  9:51pm; <NewLine> REPLY_DATE 2: August 4, 2020,  9:15am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
91267,Captum to explain Albert-based mutli-class classification,2020-07-31T19:53:07.078Z,0,51,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone, I have trained a multi-class classification model with pre-trained Albert LM. I am now experimenting with Captum to see the attribution score of each class/label. I expect to see positive/max attribution score for model prediction class/label, and negative and smaller scores for the rest of classes/labels. However, I don’t really see such a correlation there. Not sure if I don’t use Captum correctly or my expectation is simply not correct. Anyone has experience with this? Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Kevin_Song,(Kevin Song),Kevin_Song,"July 31, 2020,  7:53pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Kevin, thank you for the question! Do you have any example code snippets that you could post here ?<br/><NewLine>Jupyter or google colab notebooks would be great too.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Narine; <NewLine> ,"REPLY_DATE 1: August 3, 2020,  9:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
86564,Global explanations in captum?,2020-06-23T13:56:15.485Z,0,96,"<div class=""post"" itemprop=""articleBody""><NewLine><p>are there any global explanations algorithms embedded in captum?<br/><NewLine>is there any plan to add some anytime soon?<br/><NewLine>which ones?</p><NewLine></div>",https://discuss.pytorch.org/u/ccarmel,(Ccarmel),ccarmel,"June 23, 2020,  1:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ccarmel"">@ccarmel</a>, We are currently working onTCAV - Testing with Concept Activation Vectors.<br/><NewLine>Besides that we can always aggregate sample level attributions to get a global view, but this of course it tied to the data.<br/><NewLine>Do you have any global explanation algorithms for Neural Networks in mind ?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Narine; <NewLine> ,"REPLY_DATE 1: July 7, 2020,  3:30am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
79156,Captum tutorials for parameters attribution,2020-04-30T18:11:42.497Z,2,134,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’ve been looking into <a href=""https://captum.ai/tutorials/"" rel=""nofollow noopener"">Captum library</a> to do some parameter/neuron attribution. I.e., given a model and an input, quantify neurons’ impact on model’s prediction. Captum seems to have some tutorials for feature attribution but not any for parameter attribution. Is that the case or am I overlooking it ?</p><NewLine></div>",https://discuss.pytorch.org/u/TinfoilHat0,,TinfoilHat0,"May 2, 2020,  5:37am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/tinfoilhat0"">@TinfoilHat0</a>! Thank you for the question! What do you exactly mean by parameter ? Do you mean the weights of the neurons ?  Current attribution for neurons is meant for neuron / layer activation.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>yes thats what i meant. there are no tutorials for layer/neuron attribution as far as i could see.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/tinfoilhat0"">@TinfoilHat0</a>, you can take a look at this getting started <a href=""https://captum.ai/tutorials/Titanic_Basic_Interpret"" rel=""nofollow noopener"">tutorial</a> on the Titanic dataset, it covers example usage of Captum for layer and neuron attribution.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks Vivek. This had evaded my attention.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Narine; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/TinfoilHat0; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vivekm; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/TinfoilHat0; <NewLine> ,"REPLY_DATE 1: May 10, 2020, 12:33am; <NewLine> REPLY_DATE 2: May 10, 2020,  2:46am; <NewLine> REPLY_DATE 3: May 19, 2020, 11:53am; <NewLine> REPLY_DATE 4: May 19, 2020, 11:53am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
79787,Captum can not work in Conv1d network?,2020-05-05T06:49:05.759Z,0,108,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I built a conv1d neural network to classify 1D data series.  The data format is (N, 1, 5000).<br/><NewLine>The accuracy is ~98%. I tried to use <a href=""https://captum.ai/tutorials/"" rel=""nofollow noopener"">captum</a> to see which portion of the data is more informative. However, I can not make it work. I suspect that captum can not work on conv1d networks?</p><NewLine><p>I followed the tutorial and here is the code:</p><NewLine><pre><code class=""lang-auto"">X_test.requires_grad_()<NewLine>ig = IntegratedGradients(model)<NewLine>attributions, delta = ig.attribute(X_test,  target=0, return_convergence_delta=True)<NewLine>print('IG Attributions:', attributions)<NewLine>print('Convergence Delta:', delta)<NewLine></code></pre><NewLine><p>The error msg:</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>IndexError                                Traceback (most recent call last)<NewLine>&lt;ipython-input-88-18f4428b41d3&gt; in &lt;module&gt;<NewLine>      1 X_test.requires_grad_()<NewLine>      2 ig = IntegratedGradients(model)<NewLine>----&gt; 3 attributions, delta = ig.attribute(X_test,  target=0, return_convergence_delta=True)<NewLine>      4 print('IG Attributions:', attributions)<NewLine>      5 print('Convergence Delta:', delta)<NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/captum/attr/_core/integrated_gradients.py in attribute(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)<NewLine>    282             internal_batch_size=internal_batch_size,<NewLine>    283             forward_fn=self.forward_func,<NewLine>--&gt; 284             target_ind=expanded_target,<NewLine>    285         )<NewLine>    286 <NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/captum/attr/_utils/batching.py in _batched_operator(operator, inputs, additional_forward_args, target_ind, internal_batch_size, **kwargs)<NewLine>    162         )<NewLine>    163         for input, additional, target in _batched_generator(<NewLine>--&gt; 164             inputs, additional_forward_args, target_ind, internal_batch_size<NewLine>    165         )<NewLine>    166     ]<NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/captum/attr/_utils/batching.py in &lt;listcomp&gt;(.0)<NewLine>    161             **kwargs<NewLine>    162         )<NewLine>--&gt; 163         for input, additional, target in _batched_generator(<NewLine>    164             inputs, additional_forward_args, target_ind, internal_batch_size<NewLine>    165         )<NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/captum/attr/_utils/gradient.py in compute_gradients(forward_fn, inputs, target_ind, additional_forward_args)<NewLine>     94     with torch.autograd.set_grad_enabled(True):<NewLine>     95         # runs forward pass<NewLine>---&gt; 96         outputs = _run_forward(forward_fn, inputs, target_ind, additional_forward_args)<NewLine>     97         assert outputs[0].numel() == 1, (<NewLine>     98             ""Target not provided when necessary, cannot""<NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/captum/attr/_utils/common.py in _run_forward(forward_func, inputs, target, additional_forward_args)<NewLine>    501         *(*inputs, *additional_forward_args)<NewLine>    502         if additional_forward_args is not None<NewLine>--&gt; 503         else inputs<NewLine>    504     )<NewLine>    505     return _select_targets(output, target)<NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)<NewLine>    540     def __call__(self, *input, **kwargs):<NewLine>    541         for hook in self._forward_pre_hooks.values():<NewLine>--&gt; 542             result = hook(self, input)<NewLine>    543             if result is not None:<NewLine>    544                 if not isinstance(result, tuple):<NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/captum/attr/_core/deep_lift.py in pre_hook(module, baseline_inputs_add_args)<NewLine>    503         def pre_hook(module: Module, baseline_inputs_add_args: Tuple) -&gt; Tuple:<NewLine>    504             inputs = baseline_inputs_add_args[0]<NewLine>--&gt; 505             baselines = baseline_inputs_add_args[1]<NewLine>    506             additional_args = None<NewLine>    507             if len(baseline_inputs_add_args) &gt; 2:<NewLine><NewLine>IndexError: tuple index out of range<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/jgongac,(aaron),jgongac,"May 5, 2020,  6:49am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Issue has been resolved in the GitHub issue <a href=""https://github.com/pytorch/captum/issues/370"" rel=""nofollow noopener"">thread</a> (adding link here for future reference).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vivekm; <NewLine> ,"REPLY_DATE 1: May 18, 2020,  3:19pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
74494,NoiseTunnel: CUDA out of memory when trying out,2020-03-27T04:49:49.058Z,4,221,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was trying out captum. I am getting CUDA out of memory when I try NoiseTunnel option in captum.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/2af1c8aede0de607333b2f5a3c87ab496af4de8e"" href=""https://discuss.pytorch.org/uploads/default/original/3X/2/a/2af1c8aede0de607333b2f5a3c87ab496af4de8e.png"" title=""image""><img alt=""image"" data-base62-sha1=""67U4ycLHIrUpeQsdg2lRrVbCVJA"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/a/2af1c8aede0de607333b2f5a3c87ab496af4de8e_2_10x10.png"" height=""124"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/a/2af1c8aede0de607333b2f5a3c87ab496af4de8e_2_690x124.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/a/2af1c8aede0de607333b2f5a3c87ab496af4de8e_2_690x124.png, https://discuss.pytorch.org/uploads/default/optimized/3X/2/a/2af1c8aede0de607333b2f5a3c87ab496af4de8e_2_1035x186.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/2/a/2af1c8aede0de607333b2f5a3c87ab496af4de8e.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1287×233 31.1 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>Can someone help in solving where I am going wrong…</p><NewLine></div>",https://discuss.pytorch.org/u/nareshr8,(Naresh Rangan),nareshr8,"March 27, 2020,  4:49am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Your device might not have enough memory and thus you are seeing the OOM issue.<br/><NewLine>Could you explain, what <code>noise_tunnel</code> does and could you try to lower the batch size or number of samples you are passing to this method?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am using GeForce GTX 1070 Ti, which has 8 GB of memory. I was just trying out the example <a href=""https://captum.ai/tutorials/Resnet_TorchVision_Interpret"" rel=""nofollow noopener"">here</a>. I am using just one image.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you check the memory usage via <code>nvidia-smi</code> and make sure the device is empty and stop other processes, if necessary?<br/><NewLine>I haven’t tried the code, so I’m not sure how large the expected memory footprint is.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I saw that it as using just around 300-400 MB before running this line. And on execution, it went out of memory.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/nareshr8"">@nareshr8</a>, you can reduce n_samples to limit the number of perturbed examples and n_steps to reduce the number of integral approximation steps.<br/><NewLine>With our recent improvements (the PR got merged a couple of days ago) you should be able to adjust internal_batch_size and run IG for very large number of steps.<br/><NewLine><aside class=""onebox githubcommit""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/captum/commit/455483a34e6012a09fb22346f969072c478de68b"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/captum</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Commit""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M10.86 7c-.45-1.72-2-3-3.86-3-1.86 0-3.41 1.28-3.86 3H0v2h3.14c.45 1.72 2 3 3.86 3 1.86 0 3.41-1.28 3.86-3H14V7h-3.14zM7 10.2c-1.22 0-2.2-.98-2.2-2.2 0-1.22.98-2.2 2.2-2.2 1.22 0 2.2.98 2.2 2.2 0 1.22-.98 2.2-2.2 2.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/captum/commit/455483a34e6012a09fb22346f969072c478de68b"" rel=""nofollow noopener"" target=""_blank"">Internal Batching Improvements (#333)</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        committed <span class=""discourse-local-date"" data-date=""2020-04-01"" data-format=""ll"" data-time=""18:48:26"" data-timezone=""UTC"">06:48PM - 01 Apr 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/vivekmig"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""vivekmig"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars0.githubusercontent.com/u/11067177?v=4"" width=""20""/><NewLine>          vivekmig<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""changed 18 files with 337 additions and 85 deletions""><NewLine><a href=""https://github.com/pytorch/captum/commit/455483a34e6012a09fb22346f969072c478de68b"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+337</span><NewLine><span class=""removed"">-85</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><pre class=""github-content"" style=""white-space: normal;"">Summary:<NewLine>This PR improves internal batching to avoid creating an expanded tensor initially and simply recursively compute each subset of steps and...</pre><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot <a class=""mention"" href=""/u/narine"">@Narine</a> it worked for me. Thanks.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/nareshr8; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/nareshr8; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Narine; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/nareshr8; <NewLine> ,"REPLY_DATE 1: March 27, 2020,  4:55am; <NewLine> REPLY_DATE 2: March 27, 2020,  5:01am; <NewLine> REPLY_DATE 3: March 27, 2020,  5:07am; <NewLine> REPLY_DATE 4: March 27, 2020,  5:12am; <NewLine> REPLY_DATE 5: April 4, 2020,  1:19pm; <NewLine> REPLY_DATE 6: April 4, 2020,  1:19pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 3 Likes; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> 
70584,Manually calculating integrated gradient,2020-02-21T10:29:13.107Z,4,457,"<div class=""post"" itemprop=""articleBody""><NewLine><p>in the paper formula is,<br/><NewLine><img alt=""Screenshot (485)"" data-base62-sha1=""w6x6O6zmzwmHclvncLhuX6JdAFb"" height=""75"" src=""https://discuss.pytorch.org/uploads/default/original/3X/e/1/e102a68ff9da9ebeaf0d8be48aba37bb39c4c9f1.png"" width=""461""/></p><NewLine><p>how do we manually calculate this value, for example,</p><NewLine><pre><code class=""lang-auto"">from captum.attr import IntegratedGradients<NewLine>import torch, torch.nn as nn, torch.nn.functional as F<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">class ToyModel(nn.Module):<NewLine>    r""""""<NewLine>    Example toy model from the original paper (page 10)<NewLine><NewLine>    https://arxiv.org/pdf/1703.01365.pdf<NewLine><NewLine><NewLine>    f(x1, x2) = RELU(ReLU(x1) - 1 - ReLU(x2))<NewLine>    """"""<NewLine><NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine><NewLine>    def forward(self, input1, input2):<NewLine>        relu_out1 = F.relu(input1)<NewLine>        relu_out2 = F.relu(input2)<NewLine>        return F.relu(relu_out1 - 1 - relu_out2)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">net = ToyModel()<NewLine>net.eval()<NewLine># defining model input tensors<NewLine>input1 = torch.tensor([3.0], requires_grad=True)<NewLine>input2 = torch.tensor([1.0], requires_grad=True)<NewLine><NewLine># defining baselines for each input tensor<NewLine>baseline1 = torch.tensor([0.0])<NewLine>baseline2 = torch.tensor([0.0])<NewLine><NewLine># defining and applying integrated gradients on ToyModel and the<NewLine>ig = IntegratedGradients(net)<NewLine>attributions, approximation_error = ig.attribute((input1, input2),<NewLine>                                                 baselines=(baseline1, baseline2),<NewLine>                                                 method='gausslegendre',<NewLine>                                                 return_convergence_delta=True)<NewLine>attributions<NewLine></code></pre><NewLine><p>gives<br/><NewLine>(tensor([1.5000], grad_fn=),<br/><NewLine>tensor([-0.5000], grad_fn=))</p><NewLine><p>so, here baseline is 0, 0 input is 3, 1 if our function is,</p><NewLine><pre><code class=""lang-auto"">f(x1, x2) = x1 - 1 - x2<NewLine></code></pre><NewLine><p>do we replace x1 with x1*alpha, then differentiate wrt x1, so we get alpha, then integrate, so we get alpha**2 / 2 with alpha from 0 to 1, that is 1/2</p><NewLine><p>and same thing for x2, replace x2 with x2*alpha, then differentiate wrt x2, so we get -alpha, then integrate, so we get -alpha**2 / 2 with alpha from 0 to 1, that is -1/2</p><NewLine><p>then multiply these with input, which gives 1.5, -0.5.</p><NewLine><p>what is the intuition behind using this technique, and how does one understand this formula in a better way?</p><NewLine></div>",https://discuss.pytorch.org/u/vainaijr,,vainaijr,"February 21, 2020, 10:29am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""70584"" data-username=""vainaijr""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/vainaijr/40/19433_2.png"" width=""20""/> vainaijr:</div><NewLine><blockquote><NewLine><p>what is the intuition behind using this technique, and how does one understand this formula in a better way?</p><NewLine></blockquote><NewLine></aside><NewLine><p>So the goal of the attribution exercise we are doing is to attribute the difference between F(x) and F(x’) to the inidividual components of the input difference (x-x’).<br/><NewLine>What happens is that we are following the line form x’ to x. If we integrate the derivative in the direction of this line that, we get the difference between F(x) and F(x’), this is the fundamental theorem of calculus.<br/><NewLine>But now, the “derivative in the direction of this line” can be written as a scalar product of the gradient of F with the direction vector ((x-x’) / |x-x’|). The integrated gradient now collects the gradient of F parts separately (before the scalar product).<br/><NewLine>This and a change of variables leads to the formula you cite above.<br/><NewLine>Inherent in the construction is sum_i IntegratedGrads_i(x) = F(x) - F(x’), so we are indeed decomposing the difference.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think you explain correct but I am still confused, could you give an example with a function like f(x) = x**2.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>A 1d example won’t do. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/><br/><NewLine>But here is a simple 2d one.<br/><NewLine>Take f(x, y) = x * exp(y). We can plot this in 3d or as a countour plot:</p><NewLine><pre><code class=""lang-python"">f = lambda x, y: x * y.exp()<NewLine>xx = torch.linspace(-1,1)[None].expand(100,100)<NewLine>yy = torch.linspace(-1,1)[:, None].expand(100,100)<NewLine>zz = f(xx, yy)<NewLine>x0 = torch.tensor([-0.5, 0.5])<NewLine>y0 = torch.tensor([-0.25, 0.5])<NewLine>z0 = f(x0, y0)<NewLine>fig = pyplot.figure()<NewLine>ax = fig.gca(projection='3d')<NewLine>ax.scatter3D(x0[0], y0[0], z0[0], color=['k'], s=20)<NewLine>ax.scatter3D(x0[1], y0[1], z0[1], color=['k'], s=20)<NewLine>ax.set_transform<NewLine>ax.plot_surface(xx.numpy(),yy.numpy(),zz.numpy(), cmap=pyplot.cm.coolwarm)<NewLine>ax.view_init(elev=40)<NewLine>pyplot.figure()<NewLine>pyplot.contourf(xx.numpy(),yy.numpy(),zz.numpy(), cmap=pyplot.cm.coolwarm, levels=20)<NewLine>pyplot.plot(x0, y0)<NewLine></code></pre><NewLine><p>3d:</p><NewLine><p><img alt=""image"" data-base62-sha1=""lXnO4wxO7rJuqFYPXSKfXDzEJUh"" height=""231"" src=""https://discuss.pytorch.org/uploads/default/original/3X/9/9/99e41fe4371553b142303dc63ba6b5c32f83d6a5.png"" width=""349""/></p><NewLine><p>Contour plot:<br/><NewLine><img alt=""image"" data-base62-sha1=""ypk69eM2uACFMmlt8VGHxlacLt6"" height=""252"" src=""https://discuss.pytorch.org/uploads/default/original/3X/f/1/f126c0df844d31a5ea945a017a67cf55f55b9128.png"" width=""398""/></p><NewLine><p>Now I added two points (in the 3d) and a line in the countour plot. We take the top right end of the line as x and the lower bottom as x’.</p><NewLine><p>We can parametrize this line:</p><NewLine><pre><code class=""lang-python"">x_line = (x0[0] + torch.linspace(0, 1, 1000) * (x0[1] - x0[0])).requires_grad_()<NewLine>y_line = (y0[0] + torch.linspace(0, 1, 1000) * (y0[1] - y0[0])).requires_grad_()<NewLine></code></pre><NewLine><p>Note that this looks a like the x’ + α(x-x’) you have as an argument to ∂F/∂x_i in the integral in your equation (1).</p><NewLine><p>And indeed we can compute the gradients for each point on the line:</p><NewLine><pre><code class=""lang-python"">z_line = f(x_line, y_line)<NewLine>z_line.sum().backward()<NewLine></code></pre><NewLine><p>We can approximate the integral over 0…1 by taking the mean over the <code>.grad</code>s. Thus we can calculate the integrated gradients:</p><NewLine><pre><code class=""lang-python"">ig_x = (x0[1] - x0[0]) * x_line.grad.mean()<NewLine>ig_y = (y0[1] - y0[0]) * y_line.grad.mean()<NewLine></code></pre><NewLine><p>This gives <code>ig_x</code> as 1.1599 and <code>ig_y</code> as 0.0540.</p><NewLine><p>As a sanity check, we can compare <code>ig_x + ig_y</code> with <code>z0[1]-z0[0]</code> and indeed they seem to differ by 0.0001, which looks good.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks for your reply, I carry experiment and find</p><NewLine><pre><code class=""lang-auto"">ig_x + ig_y<NewLine></code></pre><NewLine><p>and</p><NewLine><pre><code class=""lang-auto"">z0[1] - z0[0]<NewLine></code></pre><NewLine><p>to be equal to,</p><NewLine><pre><code class=""lang-auto"">1.21376102688577<NewLine></code></pre><NewLine><p>when integrating, with alpha from 0 to 1.</p><NewLine><p>in the paper, they also have this figure,<br/><NewLine><img alt=""Screenshot (489)"" data-base62-sha1=""swmEmoRLY7G4O44tMNjnx4GpRt0"" height=""438"" src=""https://discuss.pytorch.org/uploads/default/original/3X/c/7/c7e577fb1b096f649c0e338faeb15546caacf4f2.png"" width=""620""/><br/><NewLine>does this mean, that instead of parameterization of a line,  we could parameterize a curve also, like a sigmoid curve, or a sinusoidal curve, or a circle and that would be a different attribution method.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""70584"" data-username=""vainaijr""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/vainaijr/40/19433_2.png"" width=""20""/> vainaijr:</div><NewLine><blockquote><NewLine><p>does this mean, that instead of parameterization of a line, we could parameterize a curve also, like a sigmoid curve, or a sinusoidal curve, or a circle and that would be a different attribution method.</p><NewLine></blockquote><NewLine></aside><NewLine><p>So there are several things to be considered here:</p><NewLine><ul><NewLine><li>The line parametrization (x’ + α(x-x’)) is of constant speed along the path. This enables multiplying with (x_i-x’_i) at the last step rather than having to do this separately for each “time step” in the integral. But done properly such a change of in the parametrization of the path would not change the result.</li><NewLine><li>As the figure illustrates, if you took a path different than the straight line, then you would get a different attribution. While the “sum of the two attribution parts” is again the difference in function values by the fundamental theorem of calculus, you can get a wildly different split by coordinate. e.g. if you take F = min(x_1, x_2) as your function and consider paths between (0, 1) and (1, 0), where the function is both 0. You could go along the coordinate axes to get (depending on which you do first) an attribution of (0, 0) or (1, -1). You can also get anything in between by interleaving directions.</li><NewLine><li>One of the bad parts of all this (with having to arbitrarily choose the line) is that it will no <em>compose</em> well, i.e. if you declare the last conv layer of your network the features and now you want to attribute the change in features to the change in input and the change in output to the change in features, you’ll get something completely different between the two, because it’s unlikely that the straight line in input space corresponds to a straight line in feature-space.</li><NewLine><li>In the end, it is some sort of non-definedness issue around the choice of path. The next question could be how to get around this. The straight line is straightforward to pick in Euclidean space, as would be geodesics (but even then it might not be unique if you have several) for manifolds. You could also try to get avoid the choice of a single path by introducing some probability measure on the paths and then integrating over that. But by then you’re in a much more complex setting (and I’m not sure anyone has done this).</li><NewLine></ul><NewLine><p>Hmhm. Now the last post might have been peak happiness. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vainaijr; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/vainaijr; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: February 21, 2020, 11:37am; <NewLine> REPLY_DATE 2: February 21, 2020,  3:04pm; <NewLine> REPLY_DATE 3: February 22, 2020,  8:10am; <NewLine> REPLY_DATE 4: February 22, 2020,  1:17pm; <NewLine> REPLY_DATE 5: February 22, 2020,  2:15pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 3 Likes; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> 
70399,Integrated Gradients with Captum in a sequential model,2020-02-20T01:49:47.058Z,5,270,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am using Integrated Gradients with a NN model that has both sequential (LSTM) and non sequential layers (MLP). I was following the basic steps as given in the examples and ran into a weird error.<br/><NewLine>Code snippet:<br/><NewLine>test_tens.requires_grad_()<br/><NewLine>attr = ig.attribute(test_tens, target=1, return_convergence_delta=False)<br/><NewLine>attr = attr.detach().numpy()</p><NewLine><p>error:<br/><NewLine>in line 2 of the above snippet:<br/><NewLine>"" index 1 is out of bounds for dimension 1 with size 1 ""</p><NewLine><p>Probably some problem around the target thing, but not sure what to do</p><NewLine></div>",https://discuss.pytorch.org/u/key_crusher,,key_crusher,"February 20, 2020,  1:49am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for posting the issue, <a class=""mention"" href=""/u/key_crusher"">@key_crusher</a>! What are the shapes of the output of your model and <code>test_tens</code>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for the reply <a class=""mention"" href=""/u/narine_kokhlikyan"">@Narine_Kokhlikyan</a>. The output dimensions = [batch_size, 1]. The shape of the test_tens = [batch_size, 103], here 103 is the feature count.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see! If the output dim is: output dimensions = [batch_size, 1]. then you might want to use <code>target=0</code>. Try that and let me know if you see the same error.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <a class=""mention"" href=""/u/narine_kokhlikyan"">@Narine_Kokhlikyan</a>. This worked but I still don’t understand this. Doesn’t the target value mean the value to which we want to check the contribution of the corresponding features ?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Target value is basically the index to your output. In a specific case it could be predicted class index.<br/><NewLine>It looks like you have only one dimensional output per example that’s why it can only be 0 or you don’t even need to specify it and leave <code>None</code></p><NewLine><p>Here you can find a definition of it: <a href=""https://christophm.github.io/interpretable-ml-book/terminology.html"" rel=""nofollow noopener"">https://christophm.github.io/interpretable-ml-book/terminology.html</a></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>So, if I understand this correctly, had I used SoftMax instead of Sigmoid, then my output dimensions would have been [ batch_size, num_classes ] and then the target value would have been used to determine the contribution to the specific class and it could take values from 0 to num_classes - 1</p><NewLine><p>Is this correct ? <a class=""mention"" href=""/u/narine_kokhlikyan"">@Narine_Kokhlikyan</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, that sounds right!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Narine_Kokhlikyan; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/key_crusher; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Narine_Kokhlikyan; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/key_crusher; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Narine_Kokhlikyan; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/key_crusher; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Narine_Kokhlikyan; <NewLine> ,"REPLY_DATE 1: February 20, 2020,  2:05am; <NewLine> REPLY_DATE 2: February 20, 2020,  2:08am; <NewLine> REPLY_DATE 3: February 20, 2020,  2:10am; <NewLine> REPLY_DATE 4: February 20, 2020,  2:13am; <NewLine> REPLY_DATE 5: February 20, 2020,  2:22am; <NewLine> REPLY_DATE 6: February 20, 2020,  2:22am; <NewLine> REPLY_DATE 7: February 20, 2020,  2:23am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> 
68646,Integrated gradients using with pack_padded_sequence returns error,2020-02-04T15:49:10.548Z,0,331,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I am using integrated gradient (IG) package from Captum package, which I apply one LSTM on varying length sequences and then I try to get IG from the trained model using the following line of code:</p><NewLine><blockquote><NewLine><p>attr, delta = ig.attribute((data, seq_lengths), target=1, return_convergence_delta=True)</p><NewLine></blockquote><NewLine><p>but I am getting the following error:</p><NewLine><blockquote><NewLine><p>RuntimeError: <code>lengths</code> array must be sorted in decreasing order when <code>enforce_sorted</code> is True. You can pass <code>enforce_sorted=False</code> to pack_padded_sequence and/or pack_sequence to sidestep this requirement if you do not need ONNX exportability.</p><NewLine></blockquote><NewLine><p>however, I have sorted the lengths of the array in each batch in decreasing order.<br/><NewLine>please note that If I use this IG without using pack_padded_sequence it works perfectly.</p><NewLine><p>regarding the previous error, I set enforce_sorted=False in pack_padded_sequence but I am getting another error:</p><NewLine><blockquote><NewLine><p>RuntimeError: Length of all samples has to be greater than 0, but found an element in ‘lengths’ that is &lt;= 0</p><NewLine></blockquote><NewLine><p>Here is the length of all the samples which none of them are less than zero:</p><NewLine><blockquote><NewLine><p>tensor([23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,<br/><NewLine>23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,<br/><NewLine>23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,<br/><NewLine>23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,<br/><NewLine>23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,<br/><NewLine>23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,<br/><NewLine>22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 21, 21, 21, 20,<br/><NewLine>14, 10])</p><NewLine></blockquote><NewLine><p>any help would be much appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/Mostafa_Alishahi,(Mostafa Alishahi),Mostafa_Alishahi,"February 4, 2020,  5:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for the question <a class=""mention"" href=""/u/mostafa_alishahi"">@Mostafa_Alishahi</a>!<br/><NewLine>Replied on github: <a href=""https://github.com/pytorch/captum/issues/275"" rel=""nofollow noopener"">https://github.com/pytorch/captum/issues/275</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Narine_Kokhlikyan; <NewLine> ,"REPLY_DATE 1: February 12, 2020, 10:29am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
