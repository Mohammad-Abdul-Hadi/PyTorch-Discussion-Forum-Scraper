id,title,created_at,reply_count,views,description,creator_link,creator_name,creator_alias,post_date,post_likes,replies,repliers_links,reply_dates,reply_likes
34819,About the jit category,2019-01-16T15:11:20.868Z,0,385,"<div class=""post"" itemprop=""articleBody""><NewLine><p>A category for <a href=""https://pytorch.org/docs/stable/jit.html"">TorchScript</a> and the PyTorch JIT compiler</p><NewLine></div>",https://discuss.pytorch.org/u/smth,,smth,"December 4, 2019,  8:28pm",,,,,
97578,Question regarding PyTorch&rsquo;s Backwards Compatibility guarantee between releases,2020-09-27T00:53:36.489Z,0,19,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Does PyTorch guarantee backwards compatibility between minor releases (ie., 1.X-&gt;1.Y for all Y &gt; X) for models/operator behaviors? In other words, would all the models defined and saved using an older 1.X version be loadable and runnable, with same behavior, using latest 1.Y version? Or, is this best effort which implies that some models will fail with upgrade of PyTorch version from 1.X-&gt;1.Y</p><NewLine><p>Similarly, does PyTorch JIT guarantee backwards-compatibility for traced/scripted models between releases?</p><NewLine><p>I didn’t find any documentation on backwards compatibility guarantee and I see backwards incompatible changes being noted in the release notes of each version release. Could someone help answer this or point to a document?</p><NewLine></div>",https://discuss.pytorch.org/u/vdantu,(Vamshidhar Dantu (dan21c)),vdantu,"September 27, 2020, 12:53am",,,,,
83244,Torch.jit.trace is not working with mask rcnn,2020-05-28T14:18:18.793Z,6,259,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How could I get a torchscript version of <code>torchvision.models.detection.</code> <code>maskrcnn_resnet50_fpn</code>?</p><NewLine><p>torch.jit.script and torch.jit.tarce are not working with this model</p><NewLine><p>With torch.jit.script</p><NewLine><pre><code class=""lang-auto"">model = torch.load(modelname+""-best.pth"")<NewLine>model=model.cuda()<NewLine>model.eval()<NewLine>print(img)<NewLine>with torch.no_grad():<NewLine>    print(model(img))<NewLine>    traced_cell = torch.jit.script(model, (img))<NewLine>torch.jit.save(traced_cell, modelname+""-torchscript.pth"")<NewLine><NewLine>loaded_trace = torch.jit.load(modelname+""-torchscript.pth"")<NewLine>loaded_trace.eval()<NewLine>with torch.no_grad():<NewLine>    print(loaded_trace(img))<NewLine>    <NewLine>TensorMask(torch.argmax(loaded_trace(img),1)).show()<NewLine></code></pre><NewLine><p>Output:</p><NewLine><pre><code class=""lang-auto"">TensorImage([[[[0.8961, 0.9132, 0.8789,  ..., 0.2453, 0.1939, 0.2282],<NewLine>          [0.8276, 0.9132, 0.8618,  ..., 0.2282, 0.1939, 0.2282],<NewLine>          [0.8961, 0.9132, 0.8789,  ..., 0.2282, 0.2282, 0.2453],<NewLine>          ...,<NewLine>          [0.8961, 0.8618, 0.9132,  ..., 0.4508, 0.4166, 0.3994],<NewLine>          [0.9303, 0.9132, 0.9474,  ..., 0.4166, 0.4166, 0.4508],<NewLine>          [0.9646, 0.8789, 0.9303,  ..., 0.3994, 0.3994, 0.3994]],<NewLine><NewLine>         [[1.0455, 1.0630, 1.0280,  ..., 0.3803, 0.3277, 0.3627],<NewLine>          [0.9755, 1.0630, 1.0105,  ..., 0.3627, 0.3277, 0.3627],<NewLine>          [1.0455, 1.0630, 1.0280,  ..., 0.3627, 0.3627, 0.3803],<NewLine>          ...,<NewLine>          [1.0455, 1.0105, 1.0630,  ..., 0.5903, 0.5553, 0.5378],<NewLine>          [1.0805, 1.0630, 1.0980,  ..., 0.5553, 0.5553, 0.5903],<NewLine>          [1.1155, 1.0280, 1.0805,  ..., 0.5378, 0.5378, 0.5378]],<NewLine><NewLine>         [[1.2631, 1.2805, 1.2457,  ..., 0.6008, 0.5485, 0.5834],<NewLine>          [1.1934, 1.2805, 1.2282,  ..., 0.5834, 0.5485, 0.5834],<NewLine>          [1.2631, 1.2805, 1.2457,  ..., 0.5834, 0.5834, 0.6008],<NewLine>          ...,<NewLine>          [1.2631, 1.2282, 1.2805,  ..., 0.8099, 0.7751, 0.7576],<NewLine>          [1.2980, 1.2805, 1.3154,  ..., 0.7751, 0.7751, 0.8099],<NewLine>          [1.3328, 1.2457, 1.2980,  ..., 0.7576, 0.7576, 0.7576]]]],<NewLine>       device='cuda:0')<NewLine>[{'boxes': tensor([[412.5222, 492.3208, 619.7662, 620.9233]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.1527], device='cuda:0'), 'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          ...,<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')}]<NewLine>---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-23-7216a0dac5a0&gt; in &lt;module&gt;<NewLine>     12 loaded_trace.eval()<NewLine>     13 with torch.no_grad():<NewLine>---&gt; 14     print(loaded_trace(img))<NewLine>     15 <NewLine>     16 TensorMask(torch.argmax(loaded_trace(img),1)).show()<NewLine><NewLine>~/anaconda3/envs/pro1/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)<NewLine>    556             result = self._slow_forward(*input, **kwargs)<NewLine>    557         else:<NewLine>--&gt; 558             result = self.forward(*input, **kwargs)<NewLine>    559         for hook in self._forward_hooks.values():<NewLine>    560             hook_result = hook(self, input, result)<NewLine><NewLine>RuntimeError: forward() Expected a value of type 'List[Tensor]' for argument 'images' but instead found type 'TensorImage'.<NewLine>Position: 1<NewLine>Value: TensorImage([[[[0.8961, 0.9132, 0.8789,  ..., 0.2453, 0.1939, 0.2282],<NewLine>          [0.8276, 0.9132, 0.8618,  ..., 0.2282, 0.1939, 0.2282],<NewLine>          [0.8961, 0.9132, 0.8789,  ..., 0.2282, 0.2282, 0.2453],<NewLine>          ...,<NewLine>          [0.8961, 0.8618, 0.9132,  ..., 0.4508, 0.4166, 0.3994],<NewLine>          [0.9303, 0.9132, 0.9474,  ..., 0.4166, 0.4166, 0.4508],<NewLine>          [0.9646, 0.8789, 0.9303,  ..., 0.3994, 0.3994, 0.3994]],<NewLine><NewLine>         [[1.0455, 1.0630, 1.0280,  ..., 0.3803, 0.3277, 0.3627],<NewLine>          [0.9755, 1.0630, 1.0105,  ..., 0.3627, 0.3277, 0.3627],<NewLine>          [1.0455, 1.0630, 1.0280,  ..., 0.3627, 0.3627, 0.3803],<NewLine>          ...,<NewLine>          [1.0455, 1.0105, 1.0630,  ..., 0.5903, 0.5553, 0.5378],<NewLine>          [1.0805, 1.0630, 1.0980,  ..., 0.5553, 0.5553, 0.5903],<NewLine>          [1.1155, 1.0280, 1.0805,  ..., 0.5378, 0.5378, 0.5378]],<NewLine><NewLine>         [[1.2631, 1.2805, 1.2457,  ..., 0.6008, 0.5485, 0.5834],<NewLine>          [1.1934, 1.2805, 1.2282,  ..., 0.5834, 0.5485, 0.5834],<NewLine>          [1.2631, 1.2805, 1.2457,  ..., 0.5834, 0.5834, 0.6008],<NewLine>          ...,<NewLine>          [1.2631, 1.2282, 1.2805,  ..., 0.8099, 0.7751, 0.7576],<NewLine>          [1.2980, 1.2805, 1.3154,  ..., 0.7751, 0.7751, 0.8099],<NewLine>          [1.3328, 1.2457, 1.2980,  ..., 0.7576, 0.7576, 0.7576]]]],<NewLine>       device='cuda:0')<NewLine>Declaration: forward(__torch__.torchvision.models.detection.mask_rcnn.___torch_mangle_1723.MaskRCNN self, Tensor[] images, Dict(str, Tensor)[]? targets=None) -&gt; ((Dict(str, Tensor), Dict(str, Tensor)[]))<NewLine>Cast error details: Unable to cast Python instance to C++ type (compile in debug mode for details)<NewLine></code></pre><NewLine><p>With torch.jit.trace</p><NewLine><pre><code class=""lang-auto"">modelname=""maskrcnn""<NewLine>model = torch.load(modelname+""-best.pth"")<NewLine>model=model.cuda()<NewLine>model.eval()<NewLine>print(img)<NewLine>with torch.no_grad():<NewLine>    print(model(img))<NewLine>    traced_cell = torch.jit.trace(model, (img))<NewLine>torch.jit.save(traced_cell, modelname+""-torchscript.pth"")<NewLine><NewLine>loaded_trace = torch.jit.load(modelname+""-torchscript.pth"")<NewLine>loaded_trace.eval()<NewLine>with torch.no_grad():<NewLine>    print(loaded_trace(img))<NewLine>    <NewLine>TensorMask(torch.argmax(loaded_trace(img),1)).show()<NewLine></code></pre><NewLine><p>Output</p><NewLine><pre><code class=""lang-auto"">TensorImage([[[[0.8961, 0.9132, 0.8789,  ..., 0.2453, 0.1939, 0.2282],<NewLine>          [0.8276, 0.9132, 0.8618,  ..., 0.2282, 0.1939, 0.2282],<NewLine>          [0.8961, 0.9132, 0.8789,  ..., 0.2282, 0.2282, 0.2453],<NewLine>          ...,<NewLine>          [0.8961, 0.8618, 0.9132,  ..., 0.4508, 0.4166, 0.3994],<NewLine>          [0.9303, 0.9132, 0.9474,  ..., 0.4166, 0.4166, 0.4508],<NewLine>          [0.9646, 0.8789, 0.9303,  ..., 0.3994, 0.3994, 0.3994]],<NewLine><NewLine>         [[1.0455, 1.0630, 1.0280,  ..., 0.3803, 0.3277, 0.3627],<NewLine>          [0.9755, 1.0630, 1.0105,  ..., 0.3627, 0.3277, 0.3627],<NewLine>          [1.0455, 1.0630, 1.0280,  ..., 0.3627, 0.3627, 0.3803],<NewLine>          ...,<NewLine>          [1.0455, 1.0105, 1.0630,  ..., 0.5903, 0.5553, 0.5378],<NewLine>          [1.0805, 1.0630, 1.0980,  ..., 0.5553, 0.5553, 0.5903],<NewLine>          [1.1155, 1.0280, 1.0805,  ..., 0.5378, 0.5378, 0.5378]],<NewLine><NewLine>         [[1.2631, 1.2805, 1.2457,  ..., 0.6008, 0.5485, 0.5834],<NewLine>          [1.1934, 1.2805, 1.2282,  ..., 0.5834, 0.5485, 0.5834],<NewLine>          [1.2631, 1.2805, 1.2457,  ..., 0.5834, 0.5834, 0.6008],<NewLine>          ...,<NewLine>          [1.2631, 1.2282, 1.2805,  ..., 0.8099, 0.7751, 0.7576],<NewLine>          [1.2980, 1.2805, 1.3154,  ..., 0.7751, 0.7751, 0.8099],<NewLine>          [1.3328, 1.2457, 1.2980,  ..., 0.7576, 0.7576, 0.7576]]]],<NewLine>       device='cuda:0')<NewLine>[{'boxes': tensor([[412.5222, 492.3208, 619.7662, 620.9233]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.1527], device='cuda:0'), 'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          ...,<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')}]<NewLine>/opt/conda/conda-bld/pytorch_1587452831668/work/torch/csrc/utils/python_arg_parser.cpp:760: UserWarning: This overload of nonzero is deprecated:<NewLine>	nonzero(Tensor input, *, Tensor out)<NewLine>Consider using one of the following signatures instead:<NewLine>	nonzero(Tensor input, *, bool as_tuple)<NewLine>/home/david/anaconda3/envs/proy/lib/python3.7/site-packages/torch/tensor.py:467: RuntimeWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).<NewLine>  'incorrect results).', category=RuntimeWarning)<NewLine>/home/david/anaconda3/envs/proy/lib/python3.7/site-packages/fastai2/torch_core.py:272: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<NewLine>  res = getattr(super(TensorBase, self), fn)(*args, **kwargs)<NewLine>/opt/conda/conda-bld/pytorch_1587452831668/work/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.<NewLine>/home/david/anaconda3/envs/proy/lib/python3.7/site-packages/torchvision/models/detection/rpn.py:164: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).<NewLine>  torch.tensor(image_size[1] / g[1], dtype=torch.int64, device=device)] for g in grid_sizes]<NewLine>---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-15-44b7a9360e87&gt; in &lt;module&gt;<NewLine>      6 with torch.no_grad():<NewLine>      7     print(model(img))<NewLine>----&gt; 8     traced_cell = torch.jit.trace(model, (img))<NewLine>      9 torch.jit.save(traced_cell, modelname+""-torchscript.pth"")<NewLine>     10 <NewLine><NewLine>~/anaconda3/envs/proy/lib/python3.7/site-packages/torch/jit/__init__.py in trace(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)<NewLine>    881         return trace_module(func, {'forward': example_inputs}, None,<NewLine>    882                             check_trace, wrap_check_inputs(check_inputs),<NewLine>--&gt; 883                             check_tolerance, strict, _force_outplace, _module_class)<NewLine>    884 <NewLine>    885     if (hasattr(func, '__self__') and isinstance(func.__self__, torch.nn.Module) and<NewLine><NewLine>~/anaconda3/envs/proy/lib/python3.7/site-packages/torch/jit/__init__.py in trace_module(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)<NewLine>   1035             func = mod if method_name == ""forward"" else getattr(mod, method_name)<NewLine>   1036             example_inputs = make_tuple(example_inputs)<NewLine>-&gt; 1037             module._c._create_method_from_trace(method_name, func, example_inputs, var_lookup_fn, strict, _force_outplace)<NewLine>   1038             check_trace_method = module._c._get_method(method_name)<NewLine>   1039 <NewLine><NewLine>~/anaconda3/envs/proy/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)<NewLine>    554                 input = result<NewLine>    555         if torch._C._get_tracing_state():<NewLine>--&gt; 556             result = self._slow_forward(*input, **kwargs)<NewLine>    557         else:<NewLine>    558             result = self.forward(*input, **kwargs)<NewLine><NewLine>~/anaconda3/envs/proy/lib/python3.7/site-packages/torch/nn/modules/module.py in _slow_forward(self, *input, **kwargs)<NewLine>    540                 recording_scopes = False<NewLine>    541         try:<NewLine>--&gt; 542             result = self.forward(*input, **kwargs)<NewLine>    543         finally:<NewLine>    544             if recording_scopes:<NewLine><NewLine>~/anaconda3/envs/proy/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py in forward(self, images, targets)<NewLine>     68         if isinstance(features, torch.Tensor):<NewLine>     69             features = OrderedDict([('0', features)])<NewLine>---&gt; 70         proposals, proposal_losses = self.rpn(images, features, targets)<NewLine>     71         detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)<NewLine>     72         detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)<NewLine><NewLine>~/anaconda3/envs/proy/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)<NewLine>    554                 input = result<NewLine>    555         if torch._C._get_tracing_state():<NewLine>--&gt; 556             result = self._slow_forward(*input, **kwargs)<NewLine>    557         else:<NewLine>    558             result = self.forward(*input, **kwargs)<NewLine><NewLine>~/anaconda3/envs/proy/lib/python3.7/site-packages/torch/nn/modules/module.py in _slow_forward(self, *input, **kwargs)<NewLine>    540                 recording_scopes = False<NewLine>    541         try:<NewLine>--&gt; 542             result = self.forward(*input, **kwargs)<NewLine>    543         finally:<NewLine>    544             if recording_scopes:<NewLine><NewLine>~/anaconda3/envs/proy/lib/python3.7/site-packages/torchvision/models/detection/rpn.py in forward(self, images, features, targets)<NewLine>    486         proposals = self.box_coder.decode(pred_bbox_deltas.detach(), anchors)<NewLine>    487         proposals = proposals.view(num_images, -1, 4)<NewLine>--&gt; 488         boxes, scores = self.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)<NewLine>    489 <NewLine>    490         losses = {}<NewLine><NewLine>~/anaconda3/envs/proy/lib/python3.7/site-packages/torchvision/models/detection/rpn.py in filter_proposals(self, proposals, objectness, image_shapes, num_anchors_per_level)<NewLine>    392 <NewLine>    393         # select top_n boxes independently per level before applying nms<NewLine>--&gt; 394         top_n_idx = self._get_top_n_idx(objectness, num_anchors_per_level)<NewLine>    395 <NewLine>    396         image_range = torch.arange(num_images, device=device)<NewLine><NewLine>~/anaconda3/envs/proy/lib/python3.7/site-packages/torchvision/models/detection/rpn.py in _get_top_n_idx(self, objectness, num_anchors_per_level)<NewLine>    372                 pre_nms_top_n = min(self.pre_nms_top_n(), num_anchors)<NewLine>    373             _, top_n_idx = ob.topk(pre_nms_top_n, dim=1)<NewLine>--&gt; 374             r.append(top_n_idx + offset)<NewLine>    375             offset += num_anchors<NewLine>    376         return torch.cat(r, dim=1)<NewLine><NewLine>RuntimeError: expected device cuda:0 but got device cpu<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/WaterKnight,(David Lacalle),WaterKnight,"May 28, 2020,  2:55pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>what version of PyTorch/Torchvision are you using? Afaik this should work on the latest for both (cc <a class=""mention"" href=""/u/fmassa"">@fmassa</a>)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">print(torch.__version__)<NewLine><NewLine>print(torchvision.__version__)<NewLine><NewLine>1.6.0.dev20200421 <NewLine>0.7.0a0+6e47842<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you check, if passing the inputs as a list would solve the error, as claimed in the error message for scripting?<br/><NewLine>This code works for me:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torchvision<NewLine>model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False)<NewLine>model.eval()<NewLine>scripted_model = torch.jit.script(model)<NewLine>out = scripted_model([torch.randn(3, 224, 224), torch.randn(3, 400, 400)])<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have tried the following:</p><NewLine><pre><code class=""lang-auto"">modelname=""maskrcnn""<NewLine>model = torch.load(modelname+""-best.pth"")<NewLine>model=model.cuda()<NewLine>model.eval()<NewLine>with torch.no_grad():<NewLine>    print(model(img.clone()))<NewLine>    traced_cell = torch.jit.script(model)<NewLine>traced_cell.save(modelname+""-torchscript.pth"")<NewLine><NewLine>loaded_trace = torch.jit.load(modelname+""-torchscript.pth"")<NewLine>loaded_trace.eval()<NewLine>with torch.no_grad():<NewLine>    print(loaded_trace([img[0]]))<NewLine></code></pre><NewLine><p>However, the output of the model is looking different now!</p><NewLine><pre><code class=""lang-auto"">[{'boxes': tensor([[412.5222, 492.3208, 619.7662, 620.9233]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.1527], device='cuda:0'), 'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          ...,<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')}]<NewLine>({}, [{'scores': tensor([0.1527], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'boxes': tensor([[412.5222, 492.3208, 619.7662, 620.9233]], device='cuda:0'), 'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          ...,<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.],<NewLine>          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')}])<NewLine>code/__torch__/torchvision/models/detection/mask_rcnn.py:42: UserWarning: RCNN always returns a (Losses, Detections) tuple in scripting<NewLine></code></pre><NewLine><p>It is returning a tuple now! <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not sure, why the tuple is returned, but as a workaround you could just remove the first element, as the others yield the same result.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, i am doing it right now. However, is quite strange!</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>For multiple images, the first <code>dict</code> will also be empty, so I might be missing something obvious, but I don’t know what it could be used for.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>If anyone is still interested, PyTorch throws a UserWarning explaining what’s what:</p><NewLine><p><code>code/__torch__/torchvision/models/detection/keypoint_rcnn.py:86: UserWarning: RCNN always returns a (Losses, Detections) tuple in scripting</code></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/lysuhin; <NewLine> ,"REPLY_DATE 1: May 28, 2020,  4:23pm; <NewLine> REPLY_DATE 2: May 28, 2020,  4:27pm; <NewLine> REPLY_DATE 3: May 29, 2020,  9:11am; <NewLine> REPLY_DATE 4: May 29, 2020, 10:01am; <NewLine> REPLY_DATE 5: May 29, 2020, 10:16am; <NewLine> REPLY_DATE 6: May 29, 2020, 10:17am; <NewLine> REPLY_DATE 7: May 29, 2020, 10:19am; <NewLine> REPLY_DATE 8: September 25, 2020,  3:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
97239,Torch.jit.script in nightly version,2020-09-23T10:08:51.672Z,4,55,"<div class=""post"" itemprop=""articleBody""><NewLine><p>From PyTorch 1.6 to 1.7.0.dev20200914, I am able to export the PyTorch Lightning Module torch script.<br/><NewLine>But starting from 1.7.0.dev20200915, it started to show error like this.</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>Wrong type for attribute assignment. Expected None but got Any:<NewLine></code></pre><NewLine><p>Could this be there are some changes to the JIT? If so, how can I fix it so it can be exported in 1.6 and upcoming 1.7.</p><NewLine><p>Reproducible Link:<br/><NewLine><a class=""onebox"" href=""https://colab.research.google.com/drive/1DuVwIZ1XHPdyzIUiQcP5jG1O13Zv6K7z?usp=sharing"" rel=""nofollow noopener"" target=""_blank"">https://colab.research.google.com/drive/1DuVwIZ1XHPdyzIUiQcP5jG1O13Zv6K7z?usp=sharing</a></p><NewLine><p>Thank you in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/ydcjeff,(Jeff Yang),ydcjeff,"September 24, 2020,  9:03am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>CC <a class=""mention"" href=""/u/williamfalcon"">@williamFalcon</a> in case this breaks Lightning.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> Thank you for your reply. Actually I am working on a PR of Lightning to support PyTorch 1.7.<br/><NewLine>The tests were passing prior nightly version but failing on the recent nightly version.</p><NewLine><p>Can you share some links about JIT changes?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t know which change might have caused this error, but based on the stack trace I would start with <code>torch/jit/_recursive.py</code>, since <code>concrete_type._create_methods_and_properties</code> seems to be failing.</p><NewLine><p>Based on the history of this file, there was <a href=""https://github.com/pytorch/pytorch/pull/42390"">this PR</a> on Sept. 14th which would fit into the first breaking nightly release.<br/><NewLine>This PR also introduced <code>_create_methods_and_properties</code> as seen <a href=""https://github.com/pytorch/pytorch/commit/e7d782e724c76bb0572023d52ee7438a40a7a262#diff-fb8eed23326063aef5f5843c34d460ce"">here</a>, so it would be my best guess.</p><NewLine><p>Maybe <a class=""mention"" href=""/u/splitinfinity"">@SplitInfinity</a> could help out as the author of this PR.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I will look into it. Thank you for your reply</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>We added support for module and class type properties recently, and I think the issue here is that a previously existing property that included code incompatible with TorchScript is now failing to compile. There are two options:</p><NewLine><ol><NewLine><li>Modify the property definition so that it is TorchScript compatible.</li><NewLine><li>Exclude the property from compilation. This can be done by adding a class attributed named <code>__ignored_properties__</code> and setting it to a list of the names of properties that should be ignored. <a href=""https://github.com/pytorch/pytorch/pull/45233/files#diff-facd1187e022303568f653cb6b745110R1170"" rel=""nofollow noopener"">You can see an example in this PR</a>. In the same stack of PRs, there is another that enables the usual <code>@ignore</code> and <code>@unused</code> syntax work with properties, I plan to land that by the end of the week.</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/splitinfinity"">@SplitInfinity</a>, Thank you for your reply.<br/><NewLine>Setting <code>__ignored_properties__</code> solves the issue, also this is good to see <code>@ignore</code> and <code>@unused</code> are WIP too.</p><NewLine><p>Lastly, Thank you for your work on the PRs.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ydcjeff; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ydcjeff; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/SplitInfinity; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ydcjeff; <NewLine> ,"REPLY_DATE 1: September 24, 2020,  8:49am; <NewLine> REPLY_DATE 2: September 24, 2020,  9:10am; <NewLine> REPLY_DATE 3: September 25, 2020,  5:06am; <NewLine> REPLY_DATE 4: September 24, 2020,  9:24am; <NewLine> REPLY_DATE 5: September 25, 2020,  5:06am; <NewLine> REPLY_DATE 6: September 25, 2020,  5:08am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 2 Likes; <NewLine> REPLY 6 LIKES: ; <NewLine> 
93112,How does the JIT get the function types?,2020-08-17T16:25:01.452Z,11,112,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Does it look at the raw source on disk? How can we give it a <em>custom</em> callable type for a function and force it to recompile a copy of the function body with the new types (but a new name)?</p><NewLine></div>",https://discuss.pytorch.org/u/Enamex,,Enamex,"August 17, 2020,  4:25pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Does it look at the raw source on disk?</p><NewLine></blockquote><NewLine><p>Yes. JIT uses the <code>inspect</code> module to look at the source code for the function, and determines the function schema (types of the inputs and outputs) based on type annotations (either Python3 or mypy style).</p><NewLine><blockquote><NewLine><p>How can we give it a  <em>custom</em>  callable type for a function and force it to recompile a copy of the function body with the new types (but a new name)?</p><NewLine></blockquote><NewLine><p>Could you provide a code sample to clarify this question? What is it that you want to do?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group quote-modified"" data-post=""2"" data-topic=""93112"" data-username=""SplitInfinity""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/5e9695/40.png"" width=""20""/> SplitInfinity:</div><NewLine><blockquote><NewLine><blockquote><NewLine><p>How can we give it a <em>custom</em> callable type for a function and force it to recompile a copy of the function body with the new types (but a new name)?</p><NewLine></blockquote><NewLine><p>Could you provide a code sample to clarify this question? What is it that you want to do?</p><NewLine></blockquote><NewLine></aside><NewLine><p>I’d like to be able to say something like:</p><NewLine><pre><code class=""lang-auto"">def generic_loop(self, inputs, state):<NewLine>    "" ^^^ I'm trying to reuse this. ^^^ ""<NewLine>    outs = []<NewLine>    for x in inputs:<NewLine>        out, state = self.fwd(x, state)<NewLine>        outs.append(out)<NewLine>    return outs, state<NewLine>class Cell(nn.Module):<NewLine>    def __init__(self, forward_sig):<NewLine>        self.forward = apply_cell_sig(generic_loop, forward_sig)<NewLine>        # self.forward : (TCustomCell, List[TCustomInput], TCustomState) -&gt; TCustomOut<NewLine><NewLine>class CustomCell(Cell):<NewLine>    def __init__(self):<NewLine>        super().__init__(<NewLine>            get_cell_sig(<NewLine>                CustomCell, TCustomInput, TCustomState, TCustomOut))<NewLine>    def fwd(self, input: TCustomInput, state: TCustomState):<NewLine>        return blah(input, state)<NewLine></code></pre><NewLine><p>More generally, I’m looking for a way to treat a function body as a “template” that I can provide specific types for, and ask the JIT explicitly to give me a new version for it. So it doesn’t just see its full name and go “I’ve already seen this; here’s the cached version”.</p><NewLine><p>By your confirmation though, this’s probably very difficult. I couldn’t get the JIT to accept a function that takes in a <code>nn.Module</code> instance as an argument either (besides <code>self.</code> accesses) so there’re multiple issues here.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>More generally, I’m looking for a way to treat a function body as a “template” that I can provide specific types for, and ask the JIT explicitly to give me a new version for it. So it doesn’t just see its full name and go “I’ve already seen this; here’s the cached version”.</p><NewLine></blockquote><NewLine><p>I don’t think it is a publicly advertised feature, but you could try using <code>@torch.jit._overload</code>. You can find examples of how to use this decorator in <code>test/test_jit.py</code>, like <code>test_function_overloads</code>. The caveat here is that the one body you supply will be used with all overloaded signatures because there is no concept of <code>TypeVar</code> in the JIT typing system. So you might have do type refinement to write one body that works for all types (see <code>my_conv</code> in <code>test_function_overloading_isinstance</code>, also in <code>test_jit.py</code>).</p><NewLine><blockquote><NewLine><p>By your confirmation though, this’s probably very difficult. I couldn’t get the JIT to accept a function that takes in a  <code>nn.Module</code>  instance as an argument either (besides  <code>self.</code>  accesses) so there’re multiple issues here.</p><NewLine></blockquote><NewLine><p>Yeah, modules cannot be passed around because there is no <code>Module</code> type in the JIT type system. There is no way to annotate a function correctly to make this work.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""93112"" data-username=""SplitInfinity""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/5e9695/40.png"" width=""20""/> SplitInfinity:</div><NewLine><blockquote><NewLine><p>I don’t think it is a publicly advertised feature, but you could try using <code>@torch.jit._overload</code> . You can find examples of how to use this decorator in <code>test/test_jit.py</code> , like <code>test_function_overloads</code> . The caveat here is that the one body you supply will be used with all overloaded signatures because there is no concept of <code>TypeVar</code> in the JIT typing system. So you might have do type refinement to write one body that works for all types (see <code>my_conv</code> in <code>test_function_overloading_isinstance</code> , also in <code>test_jit.py</code> ).</p><NewLine></blockquote><NewLine></aside><NewLine><p>This looks promising!</p><NewLine><p>Can you elaborate on what’s happening here?</p><NewLine><pre><code class=""lang-auto"">        # TODO: pyflakes currently does not compose @overload annotation with other<NewLine>        # decorators. This is fixed on master but not on version 2.1.1.<NewLine>        # Next version update remove noqa and add @typing.overload annotation<NewLine><NewLine>        @torch.jit._overload  # noqa: F811<NewLine>        def test_simple(x1):  # noqa: F811<NewLine>            # type: (int) -&gt; int<NewLine>            pass<NewLine><NewLine>        @torch.jit._overload  # noqa: F811<NewLine>        def test_simple(x1):  # noqa: F811<NewLine>            # type: (float) -&gt; float<NewLine>            pass<NewLine><NewLine>        def test_simple(x1):  # noqa: F811<NewLine>            return x1<NewLine><NewLine>        def invoke_function():<NewLine>            return test_simple(1.0), test_simple(.5)<NewLine><NewLine>        self.checkScript(invoke_function, ())<NewLine><NewLine>        # testing that the functions are cached<NewLine>        compiled_fns_1 = torch.jit._script._get_overloads(test_simple)<NewLine>        compiled_fns_2 = torch.jit._script._get_overloads(test_simple)<NewLine>        # ^^^ HERE HERE HERE ^^^ #<NewLine>        for a, b in zip(compiled_fns_1, compiled_fns_2):<NewLine>            self.assertIs(a.graph, b.graph)<NewLine></code></pre><NewLine><p>Why could the two successive invocations of <code>torch.jit._script._get_overloads(test_simple)</code> get different results? At that point, there’s only one thing attached to the <code>test_simple</code> name.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""93112"" data-username=""SplitInfinity""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/5e9695/40.png"" width=""20""/> SplitInfinity:</div><NewLine><blockquote><NewLine><p>Yeah, modules cannot be passed around because there is no <code>Module</code> type in the JIT type system. There is no way to annotate a function correctly to make this work.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I was hoping there’d be a way to work around this specifically <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/> The JIT already works with a <code>self</code> argument of an <code>nn.Module</code> subtype. How can we make this available for free functions? This restriction greatly limits the composability of free functions if I want to use <code>script</code>. Honestly not sure at that point if there’d be a worthwhile speed either. This’s unrelated, but I’ve had some trouble understanding what the JIT does with what it sees, and intuiting where my code might be giving it more trouble than it can handle.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Instead of that, you could also look into</p><NewLine><ul><NewLine><li>Inject your code into linecache under different module names like IPython does. This isn’t exactly documented, but clean as far as the JIT is concerned.</li><NewLine><li>Get the AST using <code>git_jit_def</code>, meddle with the AST, and use <code>_jit_script_compile</code> to compile yourself (you probably need the resolver too), so you basically mimic torch.jit.script except the caching. This is fairly invasive into JIT internals, but at least it is straightforward to your cause.</li><NewLine></ul><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""93112"" data-username=""Enamex""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/enamex/40/3329_2.png"" width=""20""/> Enamex:</div><NewLine><blockquote><NewLine><p>I was hoping there’d be a way to work around this specifically <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/> The JIT already works with a <code>self</code> argument of an <code>nn.Module</code> subtype. How can we make this available for free functions?  This restriction greatly limits the composability of free functions if I want to use <code>script</code> .</p><NewLine></blockquote><NewLine></aside><NewLine><p>The short answer is not really. The JIT supports classes insofar as they are static - if you JIT-compile Modules, you don’t give it the class source but rather an instance. It will then go through the data members and see and process their types, assuming they will be fixed (which isn’t the case in Python in general).<br/><NewLine>Maybe you might find more success in taking the “JIT” part more literally, e.g. JIT-compiling a local function from a dynamic function.</p><NewLine><aside class=""quote no-group""><NewLine><blockquote><NewLine><p>Honestly not sure at that point if there’d be a worthwhile speed either. This’s unrelated, but I’ve had some trouble understanding what the JIT does with what it sees, and intuiting where my code might be giving it more trouble than it can handle.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I’m affraid the JIT isn’t a magic bullet for optimization but “only” does specific things that are highly desirable.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""8"" data-topic=""93112"" data-username=""tom""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tom/40/3162_2.png"" width=""20""/> tom:</div><NewLine><blockquote><NewLine><p>Maybe you might find more success in taking the “JIT” part more literally, e.g. JIT-compiling a local function from a dynamic function.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Can you elaborate on this a bit more?</p><NewLine><aside class=""quote no-group"" data-post=""8"" data-topic=""93112"" data-username=""tom""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tom/40/3162_2.png"" width=""20""/> tom:</div><NewLine><blockquote><NewLine><p>I’m affraid the JIT isn’t a magic bullet for optimization but “only” does specific things that are highly desirable.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Absolutely. I meant that what those “specific things” are is a bit unclear to me. And doesn’t seem to be a goal of the documentation.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""93112"" data-username=""Enamex""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/enamex/40/3329_2.png"" width=""20""/> Enamex:</div><NewLine><blockquote><NewLine><p>Why could the two successive invocations of <code>torch.jit._script._get_overloads(test_simple)</code> get different results? At that point, there’s only one thing attached to the <code>test_simple</code> name.</p><NewLine></blockquote><NewLine></aside><NewLine><p>There are three - the two overloads and the implementation. And as you can see in the test, they should not have different results in terms of the ordering of functions nor the graphs produced by compiling said functions.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""9"" data-topic=""93112"" data-username=""Enamex""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/enamex/40/3329_2.png"" width=""20""/> Enamex:</div><NewLine><blockquote><NewLine><p>Can you elaborate on this a bit more?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Well, so one thing could be to actually make it a method of the class of self before you JIT that class / the method.</p><NewLine><aside class=""quote no-group"" data-post=""9"" data-topic=""93112"" data-username=""Enamex""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/enamex/40/3329_2.png"" width=""20""/> Enamex:</div><NewLine><blockquote><NewLine><p>Absolutely. I meant that what those “specific things” are is a bit unclear to me. And doesn’t seem to be a goal of the documentation.</p><NewLine></blockquote><NewLine></aside><NewLine><p>My chance to sell advanced PyTorch courses. Your chance to be a hero. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>More seriously we discuss this a bit in section 15.3.1 Interacting with the PyTorch JIT / What to expect from moving beyond classic Python/PyTorch of <a href=""https://pytorch.org/deep-learning-with-pytorch"" rel=""nofollow noopener"">out book that you can download in exchange for anwering a few questions</a>.<br/><NewLine>To summarize that, the main use-cases I see are</p><NewLine><ul><NewLine><li>exporting stuff,</li><NewLine><li>getting rid of the GIL for nicer multithreading in deployment,</li><NewLine><li>optimize certain patterns (e.g. pointwise ops in RNNs and elsewhere are one thing we had relatively early), but people are working on expanding this (e.g. to cover reductions).</li><NewLine></ul><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>There are three bodies in the text, but I don’t see why the two <code>torch.jit._script._get_overloads(test_sample)</code> calls <em>could</em> return different compiled objects <em>even if</em> the cache was disabled.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""8"" data-topic=""93112"" data-username=""tom""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tom/40/3162_2.png"" width=""20""/> tom:</div><NewLine><blockquote><NewLine><p>The short answer is not really. The JIT supports classes insofar as they are static - if you JIT-compile Modules, <strong>you don’t give it the class source but rather an instance</strong>. It will then go through the data members and see and process their types, assuming they will be fixed (which isn’t the case in Python in general).</p><NewLine></blockquote><NewLine></aside><NewLine><p>This bit’s interesting. I might’ve been misunderstanding the cache behavior here. If I <code>script</code> different instances of the same class at different times, does it use the cache or treat every instance separately? <code>script</code>, not <code>trace</code>. Because if it treats instances separately, could assigning different modules to attributes be a workaround for JITing functions that make use of modules from arguments (by not passing them through arguments…)?</p><NewLine><aside class=""quote no-group"" data-post=""8"" data-topic=""93112"" data-username=""tom""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tom/40/3162_2.png"" width=""20""/> tom:</div><NewLine><blockquote><NewLine><p>Maybe you might find more success in taking the “JIT” part more literally, e.g. JIT-compiling a local function from a dynamic function.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Which is why I’d imagine being able to “JIT” the same function with different types. Am I misinterpreting this?</p><NewLine><aside class=""quote no-group"" data-post=""11"" data-topic=""93112"" data-username=""tom""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tom/40/3162_2.png"" width=""20""/> tom:</div><NewLine><blockquote><NewLine><p>More seriously we discuss this a bit in section 15.3.1 Interacting with the PyTorch JIT / What to expect from moving beyond classic Python/PyTorch of <a href=""https://pytorch.org/deep-learning-with-pytorch"" rel=""nofollow noopener"">out book that you can download in exchange for anwering a few questions </a>.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Looks great! Thanks for the rec.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>If I  <code>script</code>  different instances of the same class at different times, does it use the cache or treat every instance separately?</p><NewLine></blockquote><NewLine><p>Instances are not cached, but their types in the JIT system are. Every time you script an <code>nn.Module</code>, a type is created for in the JIT type system. This is reused if multiple instances of the same module are scripted in the same program, or even the same instance is scripted twice. The <code>ScriptModule</code> returned by <code>torch.jit.script</code> is always fresh and never from a cache, but it might refer to a type object that is cached.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SplitInfinity; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Enamex; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/SplitInfinity; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Enamex; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Enamex; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Enamex; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/SplitInfinity; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/Enamex; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/Enamex; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/SplitInfinity; <NewLine> ,"REPLY_DATE 1: September 10, 2020,  5:58am; <NewLine> REPLY_DATE 2: September 15, 2020,  7:28pm; <NewLine> REPLY_DATE 3: September 15, 2020,  7:48pm; <NewLine> REPLY_DATE 4: September 16, 2020, 11:30am; <NewLine> REPLY_DATE 5: September 16, 2020, 11:34am; <NewLine> REPLY_DATE 6: September 16, 2020, 12:41pm; <NewLine> REPLY_DATE 7: September 16, 2020, 12:53pm; <NewLine> REPLY_DATE 8: September 16, 2020,  5:00pm; <NewLine> REPLY_DATE 9: September 16, 2020,  5:42pm; <NewLine> REPLY_DATE 10: September 16, 2020,  6:50pm; <NewLine> REPLY_DATE 11: September 17, 2020,  6:05pm; <NewLine> REPLY_DATE 12: September 17, 2020,  8:47pm; <NewLine> REPLY_DATE 13: September 25, 2020,  4:58am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> 
96856,Create a constant inside a graph,2020-09-19T16:41:43.444Z,4,63,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey,<br/><NewLine>I’m trying to edit a graph representation (<code>torch._C.Graph</code>) via python API.<br/><NewLine>I want to add a node at the begging of the graph that returns a CUDA device.</p><NewLine><p>this is what I did:</p><NewLine><pre><code class=""lang-python"">cuda_node = graph.create('prim::Constant[value=""cuda:0""]')<NewLine>cuda_value = next(cuda_node.outputs())<NewLine>cuda_value.setType(torch._C.DeviceObjType.get())<NewLine>graph.prependNode(cuda_node)<NewLine></code></pre><NewLine><p>This code has been successfully ran, but I get this error when I run the graph function by <code>torch._C._create_function_from_graph(...)</code> :</p><NewLine><pre><code class=""lang-auto"">RuntimeError: 0 INTERNAL ASSERT FAILED at ""/pytorch/torch/csrc/jit/ir/alias_analysis.cpp"":465, please report a bug to PyTorch. We don't have an op for prim::Constant[value=""cuda:0""] but it isn't a special case.  Argument types: <NewLine></code></pre><NewLine><p>How can I create this kind of node without experience this issue ?<br/><NewLine>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Omer_B,(Omer B),Omer_B,"September 19, 2020,  4:41pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems you are using internal methods from the <code>_C</code> namespace, so I’m unsure if your use case is supported out of the box (without breaking stuff).<br/><NewLine>What’s your use case you need to manipulate the graph and cannot recreate it?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> I’m not sure that recreate a graph replica will help in that case, since that I still don’t know how to create a <code>prim::Constant</code> with the attribute <code>value=""cuda:0""</code>.</p><NewLine><p>my mistake was that there is no <code>prim::Constant[value=""cuda:0""]</code> node kind, I need to create the node by:</p><NewLine><pre><code class=""lang-python"">cuda_node = graph.create('prim::Constant')<NewLine></code></pre><NewLine><p>and then I need to set for <code>cuda_node</code> an attribute <code>value=""cuda:0""</code> but I’m not sure how.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>This code snippet would create a <code>prim::Constant</code> on the GPU:</p><NewLine><pre><code class=""lang-python"">@torch.jit.script<NewLine>def fun(x):<NewLine>    y = x + torch.tensor(1, device='cuda')<NewLine>    return y<NewLine><NewLine>print(fun.graph)<NewLine>&gt; graph(%x.1 : Tensor):<NewLine>  %7 : bool = prim::Constant[value=0]()<NewLine>  %21 : Device = prim::Constant[value=""cuda""]()<NewLine>  %5 : None = prim::Constant()<NewLine>  %2 : int = prim::Constant[value=1]() # &lt;ipython-input-36-11fdcf9e4060&gt;:9:25<NewLine>  %8 : Tensor = aten::tensor(%2, %5, %21, %7) # &lt;ipython-input-36-11fdcf9e4060&gt;:9:12<NewLine>  %y.1 : Tensor = aten::add(%x.1, %8, %2) # &lt;ipython-input-36-11fdcf9e4060&gt;:9:8<NewLine>  return (%y.1)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> thanks, seems like I can take this example and use <code>createClone</code> to create my desired node in my graph. I guess there is no exposed API to set attributes for nodes, right?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""96856"" data-username=""Omer_B""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/omer_b/40/23472_2.png"" width=""20""/> Omer_B:</div><NewLine><blockquote><NewLine><p>I guess there is no exposed API to set attributes for nodes, right?</p><NewLine></blockquote><NewLine></aside><NewLine><p>I don’t know, so lets wait for some experts here. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> actually, it takes too much time that I can’t afford to copy an entire graph just for 1 node modification. if there will be an option to set attributes for nodes it would be great.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Omer_B; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Omer_B; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Omer_B; <NewLine> ,"REPLY_DATE 1: September 22, 2020,  5:44am; <NewLine> REPLY_DATE 2: September 22, 2020,  7:04am; <NewLine> REPLY_DATE 3: September 22, 2020,  7:27am; <NewLine> REPLY_DATE 4: September 22, 2020, 12:38pm; <NewLine> REPLY_DATE 5: September 22, 2020, 11:18pm; <NewLine> REPLY_DATE 6: September 23, 2020,  6:26pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
85243,Faster RCNN C++,2020-06-12T16:54:08.557Z,0,238,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Looking to see if anyone has succesfully deployed a Torchvision Faster RCNN (or Mask RCNN) model to C++ via torchscript/libtorch.</p><NewLine><p>In python</p><NewLine><pre><code class=""lang-auto"">model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)<NewLine>script_model = torch.jit.script(model)<NewLine>script_model.save(""model.pt"")<NewLine></code></pre><NewLine><p>In C++</p><NewLine><pre><code class=""lang-auto"">module = torch::jit::load(""model.pt"");<NewLine></code></pre><NewLine><p>I linked to library with QMake in QT Creator with this .pro in Windows 10</p><NewLine><pre><code class=""lang-auto"">QT -= gui<NewLine><NewLine>CONFIG += c++14 console no_keywords<NewLine>CONFIG -= app_bundle<NewLine><NewLine>QMAKE_CXXFLAGS += -D_GLIBCXX_USE_CXX11_ABI=0<NewLine>QMAKE_LFLAGS += -INCLUDE:?warp_size@cuda@at@@YAHXZ<NewLine><NewLine>DEFINES += QT_DEPRECATED_WARNINGS<NewLine><NewLine>SOURCES += \<NewLine>        main.cpp<NewLine><NewLine>LIBS += -LD:\src\PyTorch_Playground\c++\testLibTorch\libtorch-win-shared-with-deps-debug-latest\libtorch\lib \<NewLine>-lc10 -lc10_cuda -ltorch_cuda -ltorch_cpu -ltorch<NewLine><NewLine>INCLUDEPATH += D:\src\PyTorch_Playground\c++\testLibTorch\libtorch-win-shared-with-deps-latest\libtorch\include<NewLine>INCLUDEPATH += D:\src\PyTorch_Playground\c++\testLibTorch\libtorch-win-shared-with-deps-latest\libtorch\include\torch\csrc\api\include<NewLine><NewLine></code></pre><NewLine><p>This works when the model loaded is a ResNet model as well as FCN-ResNet. Both run and give correct outputs but RCNN models throw an exception</p><NewLine><p><code>schemas.size() &gt; 0 INTERNAL ASSERT FAILED at ""..\..\torch\csrc\jit\frontend\schema_matching.cpp"":491, please report a bug to PyTorch.</code></p><NewLine><p>Torchscript RCNN model has no problem running in python.</p><NewLine><p>There is a few month old github issue (of which I added my comments) <a href=""https://github.com/pytorch/pytorch/issues/35881"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/35881</a></p><NewLine><p>but since this is seriously blocking my transition from tensorflow to pytorch I’m hoping crossposting it here will attract more eyes. Also I’m not sure if it’s a bug or a mistake on my part or I’m not linking correctly. Also wondering if people have ever had this working perhaps on previous versions. I’m using nightly builds because I was playing with AMP and autocast.</p><NewLine></div>",https://discuss.pytorch.org/u/Jeremy_Tavrisov,(Jeremy Tavrisov),Jeremy_Tavrisov,"June 12, 2020,  4:54pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying the same thing. Were you able to do it?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Nope. I have made no progress on this</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Fairly new to PyTorch and for me loading faster-rcnn model is failing as well. Tried following the directions here: <a href=""https://pytorch.org/tutorials/advanced/cpp_export.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/cpp_export.html</a></p><NewLine><p>An exception is thrown here:</p><NewLine><p>module = torch::jit::load(“Path_to_model”);</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Has anyone figured this out?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Dipam_Vasani; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Jeremy_Tavrisov; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/DSD; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Jeremy_Tavrisov; <NewLine> ,"REPLY_DATE 1: June 27, 2020,  1:05am; <NewLine> REPLY_DATE 2: June 27, 2020,  1:22am; <NewLine> REPLY_DATE 3: July 5, 2020, 12:22am; <NewLine> REPLY_DATE 4: September 23, 2020, 12:16pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
97025,Is there a way to prefix in load_state_dict,2020-09-21T16:51:36.269Z,0,23,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Created a class contains a number of nn.Modules.  Looks like doing so adds a prefix to state_dict keys. Is there a way to specify a prefix during load_state_dict () ? Or what is the recommended way to hande such situation ?</p><NewLine></div>",https://discuss.pytorch.org/u/whatdhack,(Whatdhack),whatdhack,"September 21, 2020,  4:51pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could create a new <code>dict</code> by assigning the <code>state_dict</code> keys and values to the new keys with the desired prefix.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 23, 2020,  8:27am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
97150,Batch norm mix precision jit bug,2020-09-22T16:47:03.808Z,0,21,"<div class=""post"" itemprop=""articleBody""><NewLine><p>batch norm can receive half tensor and also return half tensor in normal cases,</p><NewLine><p>but after tracing, and under torch.no_grad(), it return float instead of half (see img)</p><NewLine><pre><code class=""lang-auto"">bn_layer = torch.nn.BatchNorm2d(10).cuda().float()<NewLine>x = torch.rand(1,10,10,10).cuda().half()<NewLine>o1 = bn_layer(x)<NewLine>print(o1.dtype)<NewLine><NewLine>with torch.no_grad():<NewLine>    trace = torch.jit.trace(bn_layer, torch.rand(1,10,10,10).cuda().half())<NewLine>    <NewLine>with torch.no_grad():<NewLine>    o2 = trace(x)<NewLine>print(o2.dtype)<NewLine><NewLine>o3 = trace(x)<NewLine>print(o3.dtype)<NewLine></code></pre><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/dd806f07ae1b4be0405d23cc164828bfc1854882"" href=""https://discuss.pytorch.org/uploads/default/original/3X/d/d/dd806f07ae1b4be0405d23cc164828bfc1854882.png"" title=""image""><img alt=""image"" data-base62-sha1=""vBuGIek5hiMAtnKbjjxaEnmx5Rw"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/d/dd806f07ae1b4be0405d23cc164828bfc1854882_2_10x10.png"" height=""317"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/d/dd806f07ae1b4be0405d23cc164828bfc1854882_2_690x317.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/d/dd806f07ae1b4be0405d23cc164828bfc1854882_2_690x317.png, https://discuss.pytorch.org/uploads/default/optimized/3X/d/d/dd806f07ae1b4be0405d23cc164828bfc1854882_2_1035x475.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/d/d/dd806f07ae1b4be0405d23cc164828bfc1854882_2_1380x634.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1624×748 73.3 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/Fangzou_Liao,(Fangzou Liao),Fangzou_Liao,"September 22, 2020,  4:47pm",,,,,
97046,Where do we find the tests for PyTorch backwards compatibility tests?,2020-09-21T20:51:27.841Z,0,24,"<div class=""post"" itemprop=""articleBody""><NewLine><p>PT seems to support backwards compatibility for JIT saved models but doesn’t guarantee forward compatability for the same. Are these backwards compatibility tests a part of the regression suite? Where do we find this suite?</p><NewLine></div>",https://discuss.pytorch.org/u/vdantu,(Vamshidhar Dantu (dan21c)),vdantu,"September 21, 2020,  8:51pm",,,,,
95679,Getting Unknown type annotation error when JIT saving,2020-09-09T14:22:25.829Z,1,75,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When JIT saving “model.pt” of a complex pytorch model with many custom classes, I am encountering the error that pytorch doesn’t know the type annotation of one of those custom classes. In other words, the following code (drastically summarized from original) fails on the seventh line:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from gan import Generator<NewLine>from gan.blocks import SpadeBlock<NewLine><NewLine>generator = Generator()<NewLine>generator.load_weights(""path/to/weigts"")<NewLine>jitted = torch.jit.script(generator)<NewLine>torch.jit.save(jitted, ""model.pt"")<NewLine></code></pre><NewLine><p>Error:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""pth2onnx.py"", line 72, in &lt;module&gt;<NewLine>    to_torch_jit(generator)<NewLine>  File ""pth2onnx.py"", line 24, in to_torch_jit<NewLine>    jitted = torch.jit.script(generator)<NewLine>  File ""/home/a.nieuwland/.conda/envs/python3.6/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1516, in script<NewLine>    return torch.jit._recursive.create_script_module(obj, torch.jit._recursive.infer_methods_to_compile)<NewLine>  File ""/home/a.nieuwland/.conda/envs/python3.6/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 310, in create_script_module<NewLine>    concrete_type = concrete_type_store.get_or_create_concrete_type(nn_module)<NewLine>  File ""/home/a.nieuwland/.conda/envs/python3.6/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 269, in get_or_create_concrete_type<NewLine>    concrete_type_builder = infer_concrete_type_builder(nn_module)<NewLine>  File ""/home/a.nieuwland/.conda/envs/python3.6/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 138, in infer_concrete_type_builder<NewLine>    sub_concrete_type = concrete_type_store.get_or_create_concrete_type(item)<NewLine>  File ""/home/a.nieuwland/.conda/envs/python3.6/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 269, in get_or_create_concrete_type<NewLine>    concrete_type_builder = infer_concrete_type_builder(nn_module)<NewLine>  File ""/home/a.nieuwland/.conda/envs/python3.6/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 138, in infer_concrete_type_builder<NewLine>    sub_concrete_type = concrete_type_store.get_or_create_concrete_type(item)<NewLine>  File ""/home/a.nieuwland/.conda/envs/python3.6/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 269, in get_or_create_concrete_type<NewLine>    concrete_type_builder = infer_concrete_type_builder(nn_module)<NewLine>  File ""/home/a.nieuwland/.conda/envs/python3.6/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 126, in infer_concrete_type_builder<NewLine>    attr_type = infer_type(name, item)<NewLine>  File ""/home/a.nieuwland/.conda/envs/python3.6/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 99, in infer_type<NewLine>    attr_type = torch.jit.annotations.ann_to_type(class_annotations[name], _jit_internal.fake_range())<NewLine>  File ""/home/a.nieuwland/.conda/envs/python3.6/lib/python3.6/site-packages/torch/jit/annotations.py"", line 303, in ann_to_type<NewLine>    raise ValueError(""Unknown type annotation: '{}'"".format(ann))<NewLine>ValueError: Unknown type annotation: '&lt;class 'gan.blocks.SpadeBlock'&gt;'<NewLine></code></pre><NewLine><p>The type it complains about is indeed a class we ourselves have programmed and used in the loaded <code>Generator</code>. I would appreciate pointers on what could cause this or how to investigate this!</p><NewLine><p>I tried the following:</p><NewLine><ul><NewLine><li>explicitly importing <code>SpadeBlock</code> in the script that calls torch.jit.script</li><NewLine><li>ensured it inherits from <code>nn.Module</code> (as does <code>Generator</code>)</li><NewLine><li>ensured the gan package is installed, using <code>pip install --user -e &lt;directory&gt;</code><NewLine></li><NewLine></ul><NewLine><p>Any ideas? Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/ArthurNieuwland,(Arthur Nieuwland),ArthurNieuwland,"September 9, 2020,  2:22pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Which version of <code>torch</code> are you using? If it is anything below 1.6, I think you need to script <code>SpadeBlock</code> as well by decorating its definition with <code>@torch.jit.script</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for responding.</p><NewLine><p>I checked, but I already had <code>torch==1.6.0</code> installed. I’m using Python 3.6.0, it it matters. Just to be sure, I reinstalled using <code>pip uninstall torch; pip install --user torch==1.6.0</code> and now the error changed to be even stranger. Now it can’t identify <code>nn.Module</code>!</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""pth2onnx.py"", line 72, in &lt;module&gt;<NewLine>    to_torch_jit(generator)<NewLine>  File ""pth2onnx.py"", line 24, in to_torch_jit<NewLine>    jitted = torch.jit.script(generator)<NewLine>  File ""/home/a.nieuwland/.conda/envs/python3.6/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1516, in script<NewLine>    return torch.jit._recursive.create_script_module(obj, torch.jit._recursive.infer_methods_to_compile)<NewLine>  File ""/home/a.nieuwland/.conda/envs/python3.6/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 310, in create_script_module<NewLine>    concrete_type = concrete_type_store.get_or_create_concrete_type(nn_module)<NewLine>  File ""/home/a.nieuwland/.conda/envs/python3.6/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 269, in get_or_create_concrete_type<NewLine>    concrete_type_builder = infer_concrete_type_builder(nn_module)<NewLine>  File ""/home/a.nieuwland/.conda/envs/python3.6/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 126, in infer_concrete_type_builder<NewLine>    attr_type = infer_type(name, item)<NewLine>  File ""/home/a.nieuwland/.conda/envs/python3.6/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 99, in infer_type<NewLine>    attr_type = torch.jit.annotations.ann_to_type(class_annotations[name], _jit_internal.fake_range())<NewLine>  File ""/home/a.nieuwland/.conda/envs/python3.6/lib/python3.6/site-packages/torch/jit/annotations.py"", line 303, in ann_to_type<NewLine>    raise ValueError(""Unknown type annotation: '{}'"".format(ann))<NewLine>ValueError: Unknown type annotation: '&lt;class 'torch.nn.modules.module.Module'&gt;'<NewLine></code></pre><NewLine><p>EDIT: After testing some more the difference in unknown type annotation appears to because I used a different generator class in the second export. Is it possible pytorch’s JIT doesn’t support <a href=""https://docs.python.org/3/library/typing.html"" rel=""nofollow noopener"">type hints</a>? I’m using those heavily.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you post a small example that I can play with to investigate?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I run this exact script and get the Unknown type annotation nn.Module error. It includes a successful export to onnx just as a sanity check that there isn’t something wrong with the model.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine><NewLine>class DcganGenerator(nn.Module):<NewLine>    __main: nn.Module<NewLine><NewLine>    def __init__(self, shape_originals, shape_targets):<NewLine>        super().__init__()<NewLine>        num_channels = shape_originals[0]<NewLine>        shape_originals  # TODO upsample to this shape<NewLine>        ngf = 64<NewLine>        ndf = 64<NewLine><NewLine>        self.__main = nn.Sequential(<NewLine>            nn.ConvTranspose2d(num_channels, ngf * 8, 4, 1, 0, bias=False),<NewLine>            nn.BatchNorm2d(ngf * 8),<NewLine>            nn.PReLU(),<NewLine>            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),<NewLine>            nn.BatchNorm2d(ngf * 4),<NewLine>            nn.PReLU(),<NewLine>            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),<NewLine>            nn.BatchNorm2d(ngf * 2),<NewLine>            nn.PReLU(),<NewLine>            nn.ConvTranspose2d(ngf * 2, ngf, 4, 1, 1, bias=False),<NewLine>            nn.BatchNorm2d(ngf),<NewLine>            nn.PReLU(),<NewLine>            nn.ConvTranspose2d(ngf, num_channels, 4, 1, 1, bias=False),<NewLine>            nn.PReLU(),<NewLine>        )<NewLine><NewLine>    def forward(self, inputs):<NewLine>        return {""generated"": self.__main(inputs)}<NewLine><NewLine>def load_generator():<NewLine>    print(""Instantiating generator"")<NewLine>    g = DcganGenerator([3, 128, 128], [3, 512, 512])<NewLine>    print(g)<NewLine>    return g<NewLine><NewLine><NewLine>def to_torch_jit(generator):<NewLine>    print(""Converting to JIT pytorch"")<NewLine>    try:<NewLine>        jitted = torch.jit.script(generator)<NewLine>        torch.jit.save(jitted, ""model.pt"")<NewLine><NewLine>        print(""Created model.pt"")<NewLine>    except Exception as e:<NewLine>        print(f""Failed. {e}"")<NewLine><NewLine><NewLine>def to_onnx(generator):<NewLine>    print(""Converting to ONNX"")<NewLine>    try:<NewLine>        torch.onnx.export(<NewLine>            generator,<NewLine>            torch.rand(1, 3, 128, 128),<NewLine>            ""model.onnx"",<NewLine>            export_params=True,<NewLine>            opset_version=11,<NewLine>            input_names=[""in""],<NewLine>            output_names=[""out""],<NewLine>        )<NewLine>        print(""Created model.onnx"")<NewLine>    except Exception as e:<NewLine>        print(f""Failed. {e}"")<NewLine><NewLine><NewLine>generator = load_generator()<NewLine>print()<NewLine>to_torch_jit(generator)<NewLine>to_onnx(generator)<NewLine></code></pre><NewLine><p>Saved as jit-it.py, output:</p><NewLine><pre><code class=""lang-auto"">$ python3 jit-it.py <NewLine>Instantiating generator<NewLine>DcganGenerator(<NewLine>  (_DcganGenerator__main): Sequential(<NewLine>    (0): ConvTranspose2d(3, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)<NewLine>    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>    (2): PReLU(num_parameters=1)<NewLine>    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)<NewLine>    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>    (5): PReLU(num_parameters=1)<NewLine>    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)<NewLine>    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>    (8): PReLU(num_parameters=1)<NewLine>    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>    (11): PReLU(num_parameters=1)<NewLine>    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>    (13): PReLU(num_parameters=1)<NewLine>  )<NewLine>)<NewLine><NewLine>Converting to JIT pytorch<NewLine>Failed. Unknown type annotation: '&lt;class 'torch.nn.modules.module.Module'&gt;'<NewLine>Converting to ONNX<NewLine>Created model.onnx<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Your example works for me if I remove the declaration of <code>__main</code> outside of <code>__init__</code> (<code>nn.Module</code> is not a supported type annotation) and rename it to <code>_main</code> (probably some special handling for identifiers that begin with two <code>__</code>).</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ohhh. Thanks for helping figure that out! Works here now too.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SplitInfinity; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ArthurNieuwland; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/SplitInfinity; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ArthurNieuwland; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/SplitInfinity; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ArthurNieuwland; <NewLine> ,"REPLY_DATE 1: September 9, 2020,  8:40pm; <NewLine> REPLY_DATE 2: September 10, 2020,  1:28pm; <NewLine> REPLY_DATE 3: September 10, 2020,  5:08pm; <NewLine> REPLY_DATE 4: September 11, 2020,  8:23am; <NewLine> REPLY_DATE 5: September 11, 2020, 10:48pm; <NewLine> REPLY_DATE 6: September 15, 2020,  2:06pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
95978,Declarative specification of the Torchscript IR,2020-09-11T22:28:51.474Z,2,89,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi - We are trying to implement a lowering of Torchscript to MLIR/ATen dialect as part of the LLVM npcomp incubator effort [1]. Is there a declarative specification of the Torchscript IR we could leverage for this work ?  Thanks</p><NewLine><p>[1] <a href=""https://llvm.discourse.group/t/npcomp-next-steps-for-torch-ir-aten-dialect/1777"" rel=""nofollow noopener"">https://llvm.discourse.group/t/npcomp-next-steps-for-torch-ir-aten-dialect/1777</a></p><NewLine></div>",https://discuss.pytorch.org/u/powderluv,(Powderluv),powderluv,"September 11, 2020, 10:28pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think the closest thing to what you are looking for is the <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/OVERVIEW.md#core-program-representation"" rel=""nofollow noopener"">core program representation section of the JIT overview documentation.</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks - that is a really helpful and nicely written doc.</p><NewLine><p>Are there any facilities for systematically enumerating the ops and how they are materialized in the IR? (i.e. I’ve got a harness that traces each op and then extracts the corresponding IR to make such a mapping, but it would be better if this was in some machine-usable source form somewhere)</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If by ops, you mean things like convolutions, not really. Most ops of that sort show up in the IR as <code>aten::op</code>, but they are not a formally defined part of the JIT IR. There is no formal restriction on which ops are “allowed” and which ops are not. They map to operators in ATen, some of which can be user-defined and registered outside of the core PyTorch library.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks - that was my understanding but was just double checking that I wasn’t missing something obvious.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SplitInfinity; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Stella_Laurenzo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/SplitInfinity; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Stella_Laurenzo; <NewLine> ,"REPLY_DATE 1: September 11, 2020, 10:53pm; <NewLine> REPLY_DATE 2: September 11, 2020, 11:12pm; <NewLine> REPLY_DATE 3: September 11, 2020, 11:21pm; <NewLine> REPLY_DATE 4: September 11, 2020, 11:26pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
95845,TorchScript: how can I access functions that were exported using @torch.jit.export?,2020-09-10T22:30:32.119Z,1,37,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When converting my model to TorchScript, I am using the decorator <span class=""mention"">@torch.jit.export</span> to mark some functions besides forward() to be exported by torch.jit.script().</p><NewLine><p>When loading the TorchScript model in Python, I can indeed access these functions.</p><NewLine><p>My question is regarding C++: since these functions are not included in the standard module interface, I need them to appear in a header file somewhere. Is such a header generated, and if not, how does one call the exported functions from C++?</p><NewLine><p>Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/rfejgin,(Roy F),rfejgin,"September 10, 2020, 10:32pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can see in <code>torch/csrc/jit/api/module.h</code> that <code>torch::jit::Module::forward</code> is implemented like so:</p><NewLine><pre><code class=""lang-auto"">  IValue forward(std::vector&lt;IValue&gt; inputs) {<NewLine>    return get_method(""forward"")(std::move(inputs));<NewLine>  }<NewLine></code></pre><NewLine><p>You should be able to call exported functions like so:</p><NewLine><pre><code class=""lang-auto"">auto mod = torch::jit::load(...);<NewLine>auto exported_method = mod.get_method(&lt;exported_method_name&gt;)<NewLine>auto result = exported_method(...);<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks. Yes, I just found an earlier question that was asked on this (it seems to be missing from docs at the moment). My function does not take any arguments, but it looks like you still need to pass in an empty vector of IValues. For future reference, here’s what I ended up doing:</p><NewLine><pre><code class=""lang-auto"">    std::vector&lt;torch::jit::IValue&gt; stack;<NewLine>    module.get_method(""my_function_name"")(stack);<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SplitInfinity; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/rfejgin; <NewLine> ,"REPLY_DATE 1: September 10, 2020, 10:47pm; <NewLine> REPLY_DATE 2: September 10, 2020, 10:51pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
95789,My pytorch conversion to mobile is not working&hellip;!,2020-09-10T11:30:07.573Z,0,26,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have OCR NN really…huge and<br/><NewLine>cuz I want to use pytorch on android…<br/><NewLine>I tried to convert pytorch model to pytorch mobile…<br/><NewLine>(But… If I can just uses pytorch library in JAVA??)<br/><NewLine>Here is what I tried…</p><NewLine><pre><code class=""lang-auto"">recognizer2 = recognizer<NewLine>recognizer2.eval()<NewLine>example = img_cv_grey, horizontal_list, free_list,\<NewLine>                            decoder, beamWidth, batch_size,\<NewLine>                            workers, allowlist, blocklist, detail,\<NewLine>                            paragraph, contrast_ths, adjust_contrast,\<NewLine>                            filter_ths, False<NewLine>    torch.jit.trace(recognizer2, example)<NewLine></code></pre><NewLine><p>The example is sample input(altough I don know why it is needed… Can’t we just convert to the mobile without sample input? it is very anoying…)</p><NewLine><p>the error message is…</p><NewLine><blockquote><NewLine><hr/><NewLine><p>RuntimeError                              Traceback (most recent call last)<br/><NewLine> in ()<br/><NewLine>----&gt; 1 result = convert_torch_mobile(“drive/My Drive/colab/easy_ocr_test/Capture1.PNG”)</p><NewLine><p>2 frames<br/><NewLine> in convert_torch_mobile(image, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, paragraph, min_size, contrast_ths, adjust_contrast, filter_ths, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, add_margin)<br/><NewLine>15     detector2.eval()<br/><NewLine>16     example = img_cv_grey, horizontal_list, free_list,                            decoder, beamWidth, batch_size,                            workers, allowlist, blocklist, detail,                            paragraph, contrast_ths, adjust_contrast,                            filter_ths, False<br/><NewLine>—&gt; 17     torch.jit.trace(recognizer2, example)<br/><NewLine>18<br/><NewLine>19</p><NewLine><p>/usr/local/lib/python3.6/dist-packages/torch/jit/<strong>init</strong>.py in trace(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)<br/><NewLine>953         return trace_module(func, {‘forward’: example_inputs}, None,<br/><NewLine>954                             check_trace, wrap_check_inputs(check_inputs),<br/><NewLine>–&gt; 955                             check_tolerance, strict, _force_outplace, _module_class)<br/><NewLine>956<br/><NewLine>957     if (hasattr(func, ‘<strong>self</strong>’) and isinstance(func.<strong>self</strong>, torch.nn.Module) and</p><NewLine><p>/usr/local/lib/python3.6/dist-packages/torch/jit/<strong>init</strong>.py in trace_module(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)<br/><NewLine>1107             func = mod if method_name == “forward” else getattr(mod, method_name)<br/><NewLine>1108             example_inputs = make_tuple(example_inputs)<br/><NewLine>-&gt; 1109             module._c._create_method_from_trace(method_name, func, example_inputs, var_lookup_fn, strict, _force_outplace)<br/><NewLine>1110             check_trace_method = module._c._get_method(method_name)<br/><NewLine>1111</p><NewLine><p>RuntimeError: Tracer cannot infer type of (array([[255, 255, 255, …, 255, 255, 255],<br/><NewLine>[255, 255, 255, …, 255, 255, 255],<br/><NewLine>[255, 255, 255, …, 255, 255, 255],<br/><NewLine>…,<br/><NewLine>[255, 255, 255, …, 255, 255, 255],<br/><NewLine>[255, 255, 255, …, 255, 255, 255],<br/><NewLine>[255, 255, 255, …, 255, 255, 255]], dtype=uint8), [[59, 89, 15, 31], [97, 295, 11, 31], [58, 326, 32, 58], [332, 746, 34, 58], [61, 169, 59, 79], [197, 295, 59, 79], [318, 460, 56, 80], [481, 559, 59, 79], [581, 663, 59, 79], [99, 131, 83, 99], [145, 210, 77, 103], [231, 445, 83, 99], [58, 504, 128, 154], [11, 45, 143, 159], [58, 774, 152, 176], [58, 172, 174, 198], [197, 295, 177, 197], [318, 404, 174, 198], [411, 471, 177, 197], [494, 602, 174, 198], [625, 711, 177, 197], [65, 181, 199, 219], [199, 275, 199, 219], [297, 489, 203, 219], [58, 352, 244, 273], [59, 551, 273, 293], [59, 169, 295, 315], [197, 279, 295, 315], [305, 445, 295, 315], [469, 575, 295, 315], [599, 683, 295, 315], [67, 181, 317, 337], [197, 305, 317, 337], [327, 569, 321, 337], [60, 288, 366, 390], [58, 724, 390, 414], [59, 169, 413, 433], [197, 281, 413, 433], [305, 437, 413, 433], [461, 565, 413, 433], [591, 675, 413, 433], [96, 182, 434, 458], [199, 307, 437, 457], [327, 523, 439, 457], [58, 242, 484, 508], [58, 472, 508, 532], [58, 172, 530, 554], [197, 273, 533, 553], [297, 437, 533, 553], [461, 535, 533, 551], [561, 645, 533, 553], [97, 181, 555, 575], [199, 305, 555, 575], [362, 440, 552, 576], [456, 506, 552, 576], [532, 795, 555, 575], [635, 659, 613, 631], [696, 734, 610, 634], [335, 499, 605, 641], [539, 561, 615, 631]], [], ‘greedy’, 5, 1, 0, None, None, 1, False, 0.1, 0.5, 0.003, False)<br/><NewLine>:Only tensors and (possibly nested) tuples of tensors, lists, or dictsare supported as inputs or outputs of traced functions, but instead got value of type ndarray.</p><NewLine></blockquote><NewLine><p>And my model is</p><NewLine><blockquote><NewLine><p>DataParallel(<br/><NewLine>(module): Model(<br/><NewLine>(FeatureExtraction): ResNet_FeatureExtractor(<br/><NewLine>(ConvNet): ResNet(<br/><NewLine>(conv0_1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn0_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(conv0_2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn0_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(relu): ReLU(inplace=True)<br/><NewLine>(maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<br/><NewLine>(layer1): Sequential(<br/><NewLine>(0): BasicBlock(<br/><NewLine>(conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(relu): ReLU(inplace=True)<br/><NewLine>(downsample): Sequential(<br/><NewLine>(0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/><NewLine>(1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>)<br/><NewLine>)<br/><NewLine>)<br/><NewLine>(conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<br/><NewLine>(layer2): Sequential(<br/><NewLine>(0): BasicBlock(<br/><NewLine>(conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(relu): ReLU(inplace=True)<br/><NewLine>(downsample): Sequential(<br/><NewLine>(0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/><NewLine>(1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>)<br/><NewLine>)<br/><NewLine>(1): BasicBlock(<br/><NewLine>(conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(relu): ReLU(inplace=True)<br/><NewLine>)<br/><NewLine>)<br/><NewLine>(conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(maxpool3): MaxPool2d(kernel_size=2, stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)<br/><NewLine>(layer3): Sequential(<br/><NewLine>(0): BasicBlock(<br/><NewLine>(conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(relu): ReLU(inplace=True)<br/><NewLine>(downsample): Sequential(<br/><NewLine>(0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/><NewLine>(1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>)<br/><NewLine>)<br/><NewLine>(1): BasicBlock(<br/><NewLine>(conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(relu): ReLU(inplace=True)<br/><NewLine>)<br/><NewLine>(2): BasicBlock(<br/><NewLine>(conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(relu): ReLU(inplace=True)<br/><NewLine>)<br/><NewLine>(3): BasicBlock(<br/><NewLine>(conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(relu): ReLU(inplace=True)<br/><NewLine>)<br/><NewLine>(4): BasicBlock(<br/><NewLine>(conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(relu): ReLU(inplace=True)<br/><NewLine>)<br/><NewLine>)<br/><NewLine>(conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(layer4): Sequential(<br/><NewLine>(0): BasicBlock(<br/><NewLine>(conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(relu): ReLU(inplace=True)<br/><NewLine>)<br/><NewLine>(1): BasicBlock(<br/><NewLine>(conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(relu): ReLU(inplace=True)<br/><NewLine>)<br/><NewLine>(2): BasicBlock(<br/><NewLine>(conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/><NewLine>(bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(relu): ReLU(inplace=True)<br/><NewLine>)<br/><NewLine>)<br/><NewLine>(conv4_1): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), bias=False)<br/><NewLine>(bn4_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(conv4_2): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1), bias=False)<br/><NewLine>(bn4_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>)<br/><NewLine>)<br/><NewLine>(AdaptiveAvgPool): AdaptiveAvgPool2d(output_size=(None, 1))<br/><NewLine>(SequenceModeling): Sequential(<br/><NewLine>(0): BidirectionalLSTM(<br/><NewLine>(rnn): LSTM(512, 512, batch_first=True, bidirectional=True)<br/><NewLine>(linear): Linear(in_features=1024, out_features=512, bias=True)<br/><NewLine>)<br/><NewLine>(1): BidirectionalLSTM(<br/><NewLine>(rnn): LSTM(512, 512, batch_first=True, bidirectional=True)<br/><NewLine>(linear): Linear(in_features=1024, out_features=512, bias=True)<br/><NewLine>)<br/><NewLine>)<br/><NewLine>(Prediction): Linear(in_features=512, out_features=1568, bias=True)<br/><NewLine>)<br/><NewLine>)</p><NewLine></blockquote><NewLine><p>Why it happens? What I want is just use pytorch in JAVA… any help please? Thanks…</p><NewLine></div>",https://discuss.pytorch.org/u/SungmanHong,(Sungman Hong),SungmanHong,"September 10, 2020, 11:30am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>My simpled error output is this</p><NewLine><p><code>torch.jit.trace(recognizer2, example)</code></p><NewLine><blockquote><NewLine><p>RuntimeError: Type ‘Tuple[int, int, float, float, float, int, float, float, float, float, float, float, bool]’ cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced</p><NewLine></blockquote><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SungmanHong; <NewLine> ,"REPLY_DATE 1: September 10, 2020, 11:39am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
92847,"Torch.jit.trace works, but torch.jit.script does not",2020-08-14T22:37:42.598Z,0,54,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it expected that for some module torch.jit.trace works well, while torch.jit.script fails. In particular, I get an error, related to TorchScript treating all objects as Tensors by default, i.e.</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>Tensor cannot be used as a tuple:<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Vlad_A,(Vlad A),Vlad_A,"August 14, 2020, 10:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, this is expected.</p><NewLine><p><code>torch.jit.trace</code> constructs a JIT IR graph by observing the operations that are performed on <code>Tensors</code> by the <code>Module</code> being traced with little concern for the Python language constructs that are used to express these operations. This means that it can produce JIT graphs successfully for a wide variety of programs, but those graphs may not always be faithful representations of the corresponding programs. For example, tracing is unable to capture control flow because it observes tensor operations and runtime, and that means the graph it constructs will contain one branch of a given instance of if statement but not the other.</p><NewLine><p><code>torch.jit.script</code> produces a JIT graph from a program by compiling it. However, it places a number of restrictions on the programs it can compile, such as requiring that they be statically typed (through heavy use of type annotations) and use only a specified subset of Python language features.</p><NewLine><p>To answer your question, <code>torch.jit.script</code> assumes any argument without an annotated type is a <code>Tensor</code>. To be able to script your <code>Module</code>, you will need to add type annotations to all methods and attributes and to make sure you are not using any unsupported Python language features.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SplitInfinity; <NewLine> ,"REPLY_DATE 1: September 10, 2020,  6:11am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
94050,Convert string to int/float in torch script,2020-08-25T12:26:28.669Z,0,65,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Currently, I cannot convert simple python strings like “0” or “1” to an integer or float in torchscript. Trying <code>int('0')</code> will result in the error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError:<NewLine>Arguments for call are not valid.<NewLine></code></pre><NewLine><p>And it expects only a tensor, a bool, a float or a number to be converted to int.</p><NewLine><p>With float, it only allows <code>""-inf""</code> or <code>""inf""</code> to be converted.</p><NewLine><p>I tried encoding with bytes, but that isn’t allowed either. Do you have any suggestions for a JIT/Torchscript compatible way to convert strings to ints/floats?</p><NewLine></div>",https://discuss.pytorch.org/u/Dieblitzen,,Dieblitzen,"August 25, 2020, 12:26pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Which version of <code>torch</code> are you using? I just tried this with <code>torch-1.6</code> (the latest release) and it works:<br/><NewLine><em>Code</em></p><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine>def fn() -&gt; int:<NewLine>    a = int('0')<NewLine>    return a<NewLine><NewLine>s = torch.jit.script(fn)<NewLine>print(s())<NewLine></code></pre><NewLine><p><em>Output</em></p><NewLine><pre><code class=""lang-auto"">0<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SplitInfinity; <NewLine> ,"REPLY_DATE 1: September 10, 2020,  6:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
95683,How to add annotation to work with JIT for nn.Sequential,2020-09-09T14:51:14.595Z,0,35,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">class Model(mm.Module):<NewLine>    def __init__(self):<NewLine>        # Init<NewLine><NewLine>    def forward(self, x):<NewLine>        # type: (List[Tensor]) -&gt; List[Tensor]<NewLine>        # Code<NewLine></code></pre><NewLine><p>If we need to define type of input x that if different from Tensor, we could do some thing like above. However, i am confused about how to add annotation if i use</p><NewLine><pre><code class=""lang-auto"">class Model(mm.Module):<NewLine>    def __init__(self):<NewLine>        # Add sequential module<NewLine>        self.layer = nn.Sequential([...])<NewLine><NewLine>    def forward(self, x):<NewLine>        # type: (List[Tensor]) -&gt; List[Tensor]<NewLine>        # Code<NewLine></code></pre><NewLine><p>If i define an attribute self.layer of type nn.Sequential and this nn.Sequential requires other data type for example List[Tensor], how can i annotate this to work with torch.jit?</p><NewLine></div>",https://discuss.pytorch.org/u/voqtuyen,(Voqtuyen),voqtuyen,"September 9, 2020,  2:51pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is not possible at the moment. What you can do is extend or make your own version of <code>Sequential</code> with the type annotation you need on its <code>forward</code> method.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SplitInfinity; <NewLine> ,"REPLY_DATE 1: September 9, 2020,  9:13pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
95624,"When to use jit.trace, and when to use jit.script?",2020-09-09T05:54:05.713Z,0,39,"<div class=""post"" itemprop=""articleBody""><NewLine><p>After I read <a href=""https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html"" rel=""nofollow noopener"">this toturial</a>, which I learnt is that I should use <code>jit.script</code> to transfer my module into TorchScript if there are conditional expressions or other uncertainties during execution in its forward propagation.</p><NewLine><p>If my comprehension is correct, then can I say that it’s OK to use <code>jit.script</code> any time to replace <code>jit.trace</code>? Since I figure no drawback of <code>jit.script</code> against <code>jit.trace</code>.</p><NewLine><p>Also I cannot understand why the toturial calls somthing like <code>jit.trace(jit.script(module))</code> then. Does this make any sense? Can’t I just use <code>jit.script(module)</code> alone?</p><NewLine></div>",https://discuss.pytorch.org/u/Flicic_Suo,(Flicic Suo),Flicic_Suo,"September 9, 2020,  5:54am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah, jit.trace is of limited use. It can process some python code without requiring modifications (type annotations, etc.), but at the same time non-trivial code may fail to work correctly, if any non-tensor non-constants get captured.</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""95624"" data-username=""Flicic_Suo""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/flicic_suo/40/17471_2.png"" width=""20""/> Flicic_Suo:</div><NewLine><blockquote><NewLine><p>Also I cannot understand why the toturial calls somthing like <code>jit.trace(jit.script(module))</code> then. Does this make any sense?</p><NewLine></blockquote><NewLine></aside><NewLine><p>No, this doesn’t make sense. Note that jit.script(module) returns a new module, old one remains in uncompiled state.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/googlebot; <NewLine> ,"REPLY_DATE 1: September 9, 2020, 10:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
63913,&lsquo;module&rsquo; object is not iterable:,2019-12-13T02:29:09.917Z,4,612,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How can I iterate nn.squential() module like a list:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>'module' object is not iterable:<NewLine>at /home/dai/scripts/card_ocr_cpu/detector/model_torchscript.py:83:8<NewLine>    def forward(self, x):<NewLine>        out = []<NewLine>        for i, m in enumerate(self.features):<NewLine>        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...  &lt;--- HERE<NewLine>            x = m(x)<NewLine>            if i in [3, 6, 13, 18]:<NewLine>                out.append(x)<NewLine>        return out<NewLine>'__torch__.extractor.forward' is being compiled since it was called from '__torch__.EAST.forward'<NewLine>at /home/dai/scripts/card_ocr_cpu/detector/model_torchscript.py:209:8<NewLine>    def forward(self, x, train:bool=False):<NewLine>        x1,x2,x3,x4 = self.extractor(x)<NewLine>        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        x = self.merge(x1,x2,x3,x4)<NewLine>        score,geo = self.output(x)<NewLine>        if not train:<NewLine>            boxes = get_boxes_torch(score,geo,score_thresh=0.95,nms_thresh=0.2)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"December 13, 2019,  2:29am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you try to add all modules into an <code>nn.ModuleList</code> instead of <code>nn.Sequential</code>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you , I fixed this problem by removing <code>enumerate()</code>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>nn.ModuleList</code> can’t be enumerated or subscripted in a <code>jit.script</code> <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>It can be enumerated, it can’t currently be subscripted.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you sure? I’m trying to iterate over a ModuleList (without enumerate) and get the error “‘<strong>torch</strong>.torch.nn.modules.container.ModuleList’ object is not iterable”. Latest pytorch version</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dalalaa; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Enamex; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/kwanUm; <NewLine> ,"REPLY_DATE 1: December 13, 2019,  4:54am; <NewLine> REPLY_DATE 2: December 31, 2019, 12:42pm; <NewLine> REPLY_DATE 3: December 31, 2019, 12:36pm; <NewLine> REPLY_DATE 4: January 3, 2020,  8:45pm; <NewLine> REPLY_DATE 5: September 9, 2020,  8:36am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
85109,torch.jit.frontend.NotSupportedError: Compiled functions can&rsquo;t take variable number of arguments or use keyword-only arguments with defaults:,2020-06-11T21:38:54.716Z,2,257,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I am trying to save a model using torch.jit.script in the following code:</p><NewLine><pre><code class=""lang-auto"">    with torch.jit.optimized_execution(True):<NewLine>    	traced_script_module = torch.jit.script(model)# save the converted model<NewLine>    	traced_script_module.save(""yolov3.pt"")<NewLine></code></pre><NewLine><p>But I got this error :</p><NewLine><pre><code class=""lang-auto"">torch.jit.frontend.NotSupportedError: Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults:<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 87<NewLine>    def forward(self, *input):<NewLine>                      ~~~~~~ &lt;--- HERE<NewLine>        r""""""Defines the computation performed at every call.<NewLine><NewLine></code></pre><NewLine><p>The code of building the model can be found here:<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/eriklindernoren/PyTorch-YOLOv3/blob/master/models.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/eriklindernoren/PyTorch-YOLOv3/blob/master/models.py"" rel=""nofollow noopener"" target=""_blank"">eriklindernoren/PyTorch-YOLOv3/blob/master/models.py</a></h4><NewLine><pre><code class=""lang-py"">from __future__ import division<NewLine><NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>from torch.autograd import Variable<NewLine>import numpy as np<NewLine><NewLine>from utils.parse_config import *<NewLine>from utils.utils import build_targets, to_cpu, non_max_suppression<NewLine><NewLine>import matplotlib.pyplot as plt<NewLine>import matplotlib.patches as patches<NewLine><NewLine><NewLine>def create_modules(module_defs):<NewLine>    """"""<NewLine>    Constructs module list of layer blocks from module configuration in module_defs<NewLine>    """"""<NewLine>    hyperparams = module_defs.pop(0)<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/eriklindernoren/PyTorch-YOLOv3/blob/master/models.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>How can I solve this issue?</p><NewLine></div>",https://discuss.pytorch.org/u/baheytharwat,(Bahey Tharwat),baheytharwat,"June 11, 2020,  9:38pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is the DarkNet the thing you want to script for? Can you provide a smaller example for easier navigation?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, I am trying to script YOLOv3 based on Darknet<br/><NewLine>The main two classes are YOLOLayer and Darknet</p><NewLine><pre><code class=""lang-auto"">class YOLOLayer(nn.Module):<NewLine>    """"""Detection layer""""""<NewLine><NewLine>    def __init__(self, anchors, num_classes, img_dim=416):<NewLine>        super(YOLOLayer, self).__init__()<NewLine>        self.anchors = anchors<NewLine>        self.num_anchors = len(anchors)<NewLine>        self.num_classes = num_classes<NewLine>        self.ignore_thres = 0.5<NewLine>        self.mse_loss = nn.MSELoss()<NewLine>        self.bce_loss = nn.BCELoss()<NewLine>        self.obj_scale = 1<NewLine>        self.noobj_scale = 100<NewLine>        self.metrics = {}<NewLine>        self.img_dim = img_dim<NewLine>        self.grid_size = 0  # grid size<NewLine><NewLine><NewLine>    def forward(self, x, targets=None, img_dim=None):<NewLine><NewLine>        # Tensors for cuda support<NewLine>        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor<NewLine>        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor<NewLine>        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor<NewLine><NewLine>        self.img_dim = img_dim<NewLine>        num_samples = x.size(0)<NewLine>        grid_size = x.size(2)<NewLine><NewLine>        prediction = (<NewLine>            x.view(num_samples, self.num_anchors, self.num_classes + 5, grid_size, grid_size)<NewLine>            .permute(0, 1, 3, 4, 2)<NewLine>            .contiguous()<NewLine>        )<NewLine><NewLine>        # Get outputs<NewLine>        x = torch.sigmoid(prediction[..., 0])  # Center x<NewLine>        y = torch.sigmoid(prediction[..., 1])  # Center y<NewLine>        w = prediction[..., 2]  # Width<NewLine>        h = prediction[..., 3]  # Height<NewLine>        pred_conf = torch.sigmoid(prediction[..., 4])  # Conf<NewLine>        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.<NewLine><NewLine>        # If grid size does not match current we compute new offsets<NewLine>        if grid_size != self.grid_size:<NewLine>            self.compute_grid_offsets(grid_size, cuda=x.is_cuda)<NewLine><NewLine>        # Add offset and scale with anchors<NewLine>        pred_boxes = FloatTensor(prediction[..., :4].shape)<NewLine>        pred_boxes[..., 0] = x.data + self.grid_x<NewLine>        pred_boxes[..., 1] = y.data + self.grid_y<NewLine>        pred_boxes[..., 2] = torch.exp(w.data) * self.anchor_w<NewLine>        pred_boxes[..., 3] = torch.exp(h.data) * self.anchor_h<NewLine><NewLine>        output = torch.cat(<NewLine>            (<NewLine>                pred_boxes.view(num_samples, -1, 4) * self.stride,<NewLine>                pred_conf.view(num_samples, -1, 1),<NewLine>                pred_cls.view(num_samples, -1, self.num_classes),<NewLine>            ),<NewLine>            -1,<NewLine>        )<NewLine><NewLine>        if targets is None:<NewLine>            return output, 0<NewLine>        else:<NewLine>            iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf = build_targets(<NewLine>                pred_boxes=pred_boxes,<NewLine>                pred_cls=pred_cls,<NewLine>                target=targets,<NewLine>                anchors=self.scaled_anchors,<NewLine>                ignore_thres=self.ignore_thres,<NewLine>            )<NewLine><NewLine>            # Loss : Mask outputs to ignore non-existing objects (except with conf. loss)<NewLine>            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])<NewLine>            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])<NewLine>            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])<NewLine>            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])<NewLine>            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])<NewLine>            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])<NewLine>            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj<NewLine>            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])<NewLine>            total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls<NewLine><NewLine>            # Metrics<NewLine>            cls_acc = 100 * class_mask[obj_mask].mean()<NewLine>            conf_obj = pred_conf[obj_mask].mean()<NewLine>            conf_noobj = pred_conf[noobj_mask].mean()<NewLine>            conf50 = (pred_conf &gt; 0.5).float()<NewLine>            iou50 = (iou_scores &gt; 0.5).float()<NewLine>            iou75 = (iou_scores &gt; 0.75).float()<NewLine>            detected_mask = conf50 * class_mask * tconf<NewLine>            precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + 1e-16)<NewLine>            recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + 1e-16)<NewLine>            recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + 1e-16)<NewLine><NewLine>            self.metrics = {<NewLine>                ""loss"": to_cpu(total_loss).item(),<NewLine>                ""x"": to_cpu(loss_x).item(),<NewLine>                ""y"": to_cpu(loss_y).item(),<NewLine>                ""w"": to_cpu(loss_w).item(),<NewLine>                ""h"": to_cpu(loss_h).item(),<NewLine>                ""conf"": to_cpu(loss_conf).item(),<NewLine>                ""cls"": to_cpu(loss_cls).item(),<NewLine>                ""cls_acc"": to_cpu(cls_acc).item(),<NewLine>                ""recall50"": to_cpu(recall50).item(),<NewLine>                ""recall75"": to_cpu(recall75).item(),<NewLine>                ""precision"": to_cpu(precision).item(),<NewLine>                ""conf_obj"": to_cpu(conf_obj).item(),<NewLine>                ""conf_noobj"": to_cpu(conf_noobj).item(),<NewLine>                ""grid_size"": grid_size,<NewLine>            }<NewLine><NewLine>            return output, total_loss<NewLine><NewLine><NewLine>class Darknet(nn.Module):<NewLine>    """"""YOLOv3 object detection model""""""<NewLine><NewLine>    def __init__(self, config_path, img_size=416):<NewLine>        super(Darknet, self).__init__()<NewLine>        self.module_defs = parse_model_config(config_path)<NewLine>        self.hyperparams, self.module_list = create_modules(self.module_defs)<NewLine>        self.yolo_layers = [layer[0] for layer in self.module_list if hasattr(layer[0], ""metrics"")]<NewLine>        self.img_size = img_size<NewLine>        self.seen = 0<NewLine>        self.header_info = np.array([0, 0, 0, self.seen, 0], dtype=np.int32)<NewLine><NewLine>    def forward(self, x, targets=None):<NewLine>        img_dim = x.shape[2]<NewLine>        loss = 0<NewLine>        layer_outputs, yolo_outputs = [], []<NewLine>        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):<NewLine>            if module_def[""type""] in [""convolutional"", ""upsample"", ""maxpool""]:<NewLine>                x = module(x)<NewLine>            elif module_def[""type""] == ""route"":<NewLine>                x = torch.cat([layer_outputs[int(layer_i)] for layer_i in module_def[""layers""].split("","")], 1)<NewLine>            elif module_def[""type""] == ""shortcut"":<NewLine>                layer_i = int(module_def[""from""])<NewLine>                x = layer_outputs[-1] + layer_outputs[layer_i]<NewLine>            elif module_def[""type""] == ""yolo"":<NewLine>                x, layer_loss = module[0](x, targets, img_dim)<NewLine>                loss += layer_loss<NewLine>                yolo_outputs.append(x)<NewLine>            layer_outputs.append(x)<NewLine>        yolo_outputs = to_cpu(torch.cat(yolo_outputs, 1))<NewLine>        return yolo_outputs if targets is None else (loss, yolo_outputs)<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/wanchaol"">@wanchaol</a>,<br/><NewLine>Is there anything that I can do from my side to help you navigate through the code and realize my problem?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>HI <a class=""mention"" href=""/u/baheytharwat"">@baheytharwat</a>, sorry I haven’t got time to debug the issue out as the model is a big large, I will continue do it next week. It would be nice if you could provide a minimal repro, that could help on debug and fix <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,<br/><NewLine><a class=""mention"" href=""/u/baheytharwat"">@baheytharwat</a> Did you manage to solve this?<br/><NewLine>Does anyone have some ideas on how to debug it?<br/><NewLine>Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/baheytharwat; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/baheytharwat; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Oded_Golden; <NewLine> ,"REPLY_DATE 1: June 11, 2020,  9:53pm; <NewLine> REPLY_DATE 2: June 11, 2020, 10:01pm; <NewLine> REPLY_DATE 3: June 13, 2020,  5:02pm; <NewLine> REPLY_DATE 4: June 15, 2020,  3:29am; <NewLine> REPLY_DATE 5: September 8, 2020,  2:01pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
95393,Debugging backward errors,2020-09-07T03:29:57.080Z,1,37,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a big composite module, that jit compiles and executes forward() ok, but fails in backward(). The big issue is that there is no error with jit disabled, and set_detect_anomaly() is not too helpful in jit mode.</p><NewLine><p>I’m 90% sure the error itself is related to how jit incorrectly enables requires_grad in multiple scenarios. But are there any techniques to localize it?</p><NewLine><p>For reference, here is exception text:</p><NewLine><pre><code class=""lang-auto"">The following operation failed in the TorchScript interpreter.<NewLine>Traceback of TorchScript (most recent call last):<NewLine>  File ""&lt;string&gt;"", line 138, in &lt;backward op&gt;<NewLine>                   dim: int):<NewLine>            def backward(grad_outputs: List[Tensor]):<NewLine>                grad_self = torch.stack(grad_outputs, dim)<NewLine>                            ~~~~~~~~~~~ &lt;--- HERE<NewLine>                return grad_self, None<NewLine>RuntimeError: sizes() called on undefined Tensor<NewLine></code></pre><NewLine><p>So, some generated code, look like for unbind() operation, where outputs have inconsistent requires_grad?</p><NewLine><p>Also note how “dim:int” line cutoff does a bad service.</p><NewLine><p>And console:</p><NewLine><p>[W …\torch\csrc\autograd\python_anomaly_mode.cpp:104] Warning: Error detected in struct torch::jit::`anonymous namespace’::DifferentiableGraphBackward. Traceback of forward call that caused the error:<br/><NewLine>…<br/><NewLine>has traceback that stops at jit module</p><NewLine></div>",https://discuss.pytorch.org/u/googlebot,(Alex),googlebot,"September 7, 2020,  3:29am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>To me the error looks like you have an undefined Tensor (aka None) showing up unexpectedly.<br/><NewLine>I would try to cut down the model (or just the JITed part) a bit to zoom into where it happens. (You can see a method I use to grab a submodule <a href=""https://lernapparat.de/transformers-pytorch-tvm/"" rel=""nofollow noopener"">when searching for DebugWrap in my blog Post on PyTorch and TVM</a>).<br/><NewLine>Naturally, we would be most grateful if you found a reproducing snippet that you can share to fix this in PyTorch.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I couldn’t easily divide that module (issue with mutable objects in mixed mode). What helped somewhat was running backward() in profiler context, exporting chrome trace and looking at last successful ops, but that’s not a good or reliable solution. Luckily, I identified unbind from the above message, and indeed unbind was the problem.</p><NewLine><p>Now, for some reason I failed to reproduce the failure in a small script, however I think it is somehow related to “<a href=""https://github.com/pytorch/pytorch/issues/36608"" rel=""nofollow noopener"">Backward through view of unbind output</a>” issue and seems to only fail in “legacy” jit mode (which is 1.6 default), so perhaps this will be handled in next release.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/googlebot; <NewLine> ,"REPLY_DATE 1: September 7, 2020,  6:52am; <NewLine> REPLY_DATE 2: September 7, 2020, 11:26am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
95158,[Torchscript] Serialization - Loss of Information,2020-09-04T09:00:28.170Z,0,48,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone.<br/><NewLine>I am working with Torchscript on PyTorch 1.5.0.<br/><NewLine>I want to automatically grab input shape &amp; dtype information from a deserialized, traced torchscript model with the following code line:</p><NewLine><pre><code class=""lang-auto"">[(i.debugName().split('.')[0], i.type().sizes(), i.type().scalarType()) for i in <NewLine>list(reloaded_module.graph.inputs())[1:]]<NewLine></code></pre><NewLine><p>The Problem is that this gives me None for both the size and the scalarType when i deserialize the model.<br/><NewLine>On the other hand, when i execute the same line of code on a non-deserialized model (e.g. a just-in-time traced model that has not been serialized&amp;deserialized) i get the desired results.</p><NewLine><p>You should be able to reproduce this through just a few lines of code.</p><NewLine><pre><code class=""lang-auto"">model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)<NewLine>input_image = ""get an image from somewhere""<NewLine><NewLine>traced_path = ""./test.pt""<NewLine>traced_module = torch.jit.trace(model,input_batch)<NewLine>traced_module.save(traced_path)<NewLine><NewLine>reloaded_module = torch.jit.load(traced_path)<NewLine><NewLine>print([(i.debugName().split('.')[0], i.type().sizes(), i.type().scalarType()) for i in  list(traced_module.graph.inputs())[1:]])<NewLine>print([(i.debugName().split('.')[0], i.type().sizes(), i.type().scalarType()) for i in  list(reloaded_module.graph.inputs())[1:]])<NewLine><NewLine></code></pre><NewLine><p>Any suggestions on solutions? Am i missing something or is this actually a bug in Torch-Serialization/Deserialization?</p><NewLine><p>Best regards,<br/><NewLine>Knight3</p><NewLine></div>",https://discuss.pytorch.org/u/Knight3,(Knight3),Knight3,"September 4, 2020,  9:00am",,,,,
95044,[TorchScript] PackedSequence issue,2020-09-03T10:12:02.148Z,0,45,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to export LSTM based pytorch model to TorchScript script but I failed because of such error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>Arguments for call are not valid.<NewLine>The following variants are available:<NewLine>  <NewLine>  forward__0(__torch__.torch.nn.modules.rnn.LSTM self, Tensor input, (Tensor, Tensor)? hx=None) -&gt; ((Tensor, (Tensor, Tensor))):<NewLine>  Expected a value of type 'Tensor' for argument 'input' but instead found type '__torch__.torch.nn.utils.rnn.PackedSequence'.<NewLine>  <NewLine>  forward__1(__torch__.torch.nn.modules.rnn.LSTM self, __torch__.torch.nn.utils.rnn.PackedSequence input, (Tensor, Tensor)? hx=None) -&gt; ((__torch__.torch.nn.utils.rnn.PackedSequence, (Tensor, Tensor))):<NewLine>  Expected a value of type 'Optional[Tuple[Tensor, Tensor]]' for argument 'hx' but instead found type 'Tensor (inferred)'.<NewLine>  Inferred the value for argument 'hx' to be of type 'Tensor' because it was not annotated with an explicit type.<NewLine><NewLine>The original call is:<NewLine>  File """", line 24<NewLine>                                 enforce_sorted=False)<NewLine><NewLine>        output, feat = self.lstm(x, feat)<NewLine>                       ~~~~~~~~~ &lt;--- HERE<NewLine>        output = pad_packed_sequence(output,<NewLine>                                     batch_first=True,<NewLine></code></pre><NewLine><p>Script to reproduce:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch import nn<NewLine>from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence<NewLine><NewLine><NewLine>class LSTMBlock(nn.Module):<NewLine>    def __init__(self, input_size, hidden_size):<NewLine>        super().__init__()<NewLine>        self.input_size = input_size<NewLine>        self.hidden_size = hidden_size<NewLine><NewLine>        self.lstm = nn.LSTM(input_size,<NewLine>                            hidden_size,<NewLine>                            batch_first=True,<NewLine>                            bidirectional=False)<NewLine><NewLine>    def forward(self, x, input_lengths, feat=None):<NewLine>        total_length = x.shape[1]<NewLine>        x = pack_padded_sequence(x,<NewLine>                                 input_lengths,<NewLine>                                 batch_first=True,<NewLine>                                 enforce_sorted=False)<NewLine><NewLine>        output, feat = self.lstm(x, feat)<NewLine>        output = pad_packed_sequence(output,<NewLine>                                     batch_first=True,<NewLine>                                     total_length=total_length)[0]<NewLine><NewLine>        return output, input_lengths, feat<NewLine><NewLine>model = LSTMBlock(100, 100)<NewLine>script = torch.jit.script(model)<NewLine></code></pre><NewLine><p>Is there a way how to handle models trained with packed_sequence?</p><NewLine></div>",https://discuss.pytorch.org/u/Bartlomiej_Roszak,(Bartłomiej Roszak),Bartlomiej_Roszak,"September 3, 2020, 10:12am",,,,,
62337,FasterRCNN Resnet50 JIT Trace,2019-11-27T17:46:44.588Z,6,1722,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m trying to trace FasterRCNN to use in Pytorch Mobile on iOS.</p><NewLine><p>I simply trace as shown below:</p><NewLine><pre><code class=""lang-auto"">model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)<NewLine>model.eval()<NewLine><NewLine>input_tensor = torch.rand(1,3,224,224)<NewLine>script_model = torch.jit.trace(model, input_tensor)<NewLine>script_model.save(""models/fRCNN_resnet50.pt"")<NewLine></code></pre><NewLine><p>I receive a “Only tensors or tuples of tensors can be output from traced functions (getOutput at …/torch/csrc/jit/tracer.cpp:209)” error as shown below</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Only tensors or tuples of tensors can be output from traced functions (getOutput at ../torch/csrc/jit/tracer.cpp:209)<NewLine>frame #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) + 135 (0x128cc29e7 in libc10.dylib)<NewLine>frame #1: torch::jit::tracer::TracingState::getOutput(c10::IValue const&amp;) + 1785 (0x120164069 in libtorch.dylib)<NewLine>frame #2: torch::jit::tracer::exit(std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt; const&amp;) + 232 (0x120167108 in libtorch.dylib)<NewLine>frame #3: torch::jit::tracer::createGraphByTracing(pybind11::function const&amp;, torch::jit::tracer::TypedStack, pybind11::function const&amp;, bool, torch::jit::script::Module*) + 916 (0x11c957914 in libtorch_python.dylib)<NewLine>frame #4: void pybind11::cpp_function::initialize&lt;torch::jit::script::initJitScriptBindings(_object*)::$_16, void, torch::jit::script::Module&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, pybind11::function, pybind11::tuple, pybind11::function, bool, pybind11::name, pybind11::is_method, pybind11::sibling&gt;(torch::jit::script::initJitScriptBindings(_object*)::$_16&amp;&amp;, void (*)(torch::jit::script::Module&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, pybind11::function, pybind11::tuple, pybind11::function, bool), pybind11::name const&amp;, pybind11::is_method const&amp;, pybind11::sibling const&amp;)::'lambda'(pybind11::detail::function_call&amp;)::__invoke(pybind11::detail::function_call&amp;) + 197 (0x11c993185 in libtorch_python.dylib)<NewLine>frame #5: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 3324 (0x11c4cd92c in libtorch_python.dylib)<NewLine>frame #6: _PyCFunction_FastCallDict + 183 (0x10e3d0167 in Python)<NewLine>frame #7: call_function + 184 (0x10e452d28 in Python)<NewLine>frame #8: _PyEval_EvalFrameDefault + 27511 (0x10e44f597 in Python)<NewLine>frame #9: _PyEval_EvalCodeWithName + 2447 (0x10e45388f in Python)<NewLine>frame #10: fast_function + 545 (0x10e454141 in Python)<NewLine>frame #11: call_function + 401 (0x10e452e01 in Python)<NewLine>frame #12: _PyEval_EvalFrameDefault + 27511 (0x10e44f597 in Python)<NewLine>frame #13: _PyEval_EvalCodeWithName + 2447 (0x10e45388f in Python)<NewLine>frame #14: fast_function + 545 (0x10e454141 in Python)<NewLine>frame #15: call_function + 401 (0x10e452e01 in Python)<NewLine>frame #16: _PyEval_EvalFrameDefault + 27511 (0x10e44f597 in Python)<NewLine>frame #17: _PyEval_EvalCodeWithName + 2447 (0x10e45388f in Python)<NewLine>frame #18: PyEval_EvalCode + 100 (0x10e448954 in Python)<NewLine>frame #19: builtin_exec + 548 (0x10e445fe4 in Python)<NewLine>frame #20: _PyCFunction_FastCallDict + 491 (0x10e3d029b in Python)<NewLine>frame #21: call_function + 439 (0x10e452e27 in Python)<NewLine>frame #22: _PyEval_EvalFrameDefault + 27511 (0x10e44f597 in Python)<NewLine>frame #23: gen_send_ex + 183 (0x10e3a7fe7 in Python)<NewLine>frame #24: _PyEval_EvalFrameDefault + 11552 (0x10e44b740 in Python)<NewLine>frame #25: gen_send_ex + 183 (0x10e3a7fe7 in Python)<NewLine>frame #26: _PyEval_EvalFrameDefault + 11552 (0x10e44b740 in Python)<NewLine>frame #27: gen_send_ex + 183 (0x10e3a7fe7 in Python)<NewLine>frame #28: _PyCFunction_FastCallDict + 560 (0x10e3d02e0 in Python)<NewLine>frame #29: call_function + 439 (0x10e452e27 in Python)<NewLine>frame #30: _PyEval_EvalFrameDefault + 27511 (0x10e44f597 in Python)<NewLine>frame #31: fast_function + 381 (0x10e45409d in Python)<NewLine>frame #32: call_function + 401 (0x10e452e01 in Python)<NewLine>frame #33: _PyEval_EvalFrameDefault + 27511 (0x10e44f597 in Python)<NewLine>frame #34: fast_function + 381 (0x10e45409d in Python)<NewLine>frame #35: call_function + 401 (0x10e452e01 in Python)<NewLine>frame #36: _PyEval_EvalFrameDefault + 27511 (0x10e44f597 in Python)<NewLine>frame #37: _PyEval_EvalCodeWithName + 2447 (0x10e45388f in Python)<NewLine>frame #38: _PyFunction_FastCallDict + 763 (0x10e45445b in Python)<NewLine>frame #39: _PyObject_FastCallDict + 247 (0x10e3873e7 in Python)<NewLine>frame #40: _PyObject_Call_Prepend + 149 (0x10e387505 in Python)<NewLine>frame #41: PyObject_Call + 96 (0x10e387220 in Python)<NewLine>frame #42: _PyEval_EvalFrameDefault + 28250 (0x10e44f87a in Python)<NewLine>frame #43: _PyEval_EvalCodeWithName + 2447 (0x10e45388f in Python)<NewLine>frame #44: fast_function + 545 (0x10e454141 in Python)<NewLine>frame #45: call_function + 401 (0x10e452e01 in Python)<NewLine>frame #46: _PyEval_EvalFrameDefault + 27670 (0x10e44f636 in Python)<NewLine>frame #47: gen_send_ex + 183 (0x10e3a7fe7 in Python)<NewLine>frame #48: builtin_next + 92 (0x10e446bcc in Python)<NewLine>frame #49: _PyCFunction_FastCallDict + 491 (0x10e3d029b in Python)<NewLine>frame #50: call_function + 439 (0x10e452e27 in Python)<NewLine>frame #51: _PyEval_EvalFrameDefault + 27511 (0x10e44f597 in Python)<NewLine>frame #52: _PyEval_EvalCodeWithName + 2447 (0x10e45388f in Python)<NewLine>frame #53: fast_function + 545 (0x10e454141 in Python)<NewLine>frame #54: call_function + 401 (0x10e452e01 in Python)<NewLine>frame #55: _PyEval_EvalFrameDefault + 27511 (0x10e44f597 in Python)<NewLine>frame #56: gen_send_ex + 183 (0x10e3a7fe7 in Python)<NewLine>frame #57: builtin_next + 92 (0x10e446bcc in Python)<NewLine>frame #58: _PyCFunction_FastCallDict + 491 (0x10e3d029b in Python)<NewLine>frame #59: call_function + 439 (0x10e452e27 in Python)<NewLine>frame #60: _PyEval_EvalFrameDefault + 27511 (0x10e44f597 in Python)<NewLine>frame #61: _PyEval_EvalCodeWithName + 2447 (0x10e45388f in Python)<NewLine>frame #62: fast_function + 545 (0x10e454141 in Python)<NewLine>frame #63: call_function + 401 (0x10e452e01 in Python)<NewLine></code></pre><NewLine><p>Could someone explain to me how to properly trace or script a pretrained object detection model such as this one? I don’t know which steps I might be missing if any!</p><NewLine></div>",https://discuss.pytorch.org/u/HussainHaris,(Haris),HussainHaris,"November 27, 2019,  5:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The MaskRCNN based models don’t support tracing due to the error you saw, but thanks to <a href=""https://github.com/pytorch/vision/pull/1407"" rel=""nofollow noopener"">this PR</a> you can use <code>torch.jit.script</code> to compile the whole network.</p><NewLine><p>You’ll need to get the most current version of torchvision by <a href=""https://github.com/pytorch/vision#installation"" rel=""nofollow noopener"">building it from source</a>, then you can do</p><NewLine><pre><code class=""lang-python"">model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)<NewLine>model.eval()<NewLine>script_model = torch.jit.script(model)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""62337""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/driazati/40/15393_2.png"" width=""20""/> driazati:</div><NewLine><blockquote><NewLine><p>building it from source</p><NewLine></blockquote><NewLine></aside><NewLine><p>Hi! I know some time has passed between now and when this was response was written. Would recompiling from source still be the solution for this? Will try tonight! Thanks</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>We had a release recently, so it should work so long as you’re using the latest versions of PyTorch and torchvision.</p><NewLine><pre><code class=""lang-auto""># Clear out old versions<NewLine>pip uninstall torch<NewLine>pip uninstall torch<NewLine>pip uninstall torchvision<NewLine><NewLine># Install the most recent versions<NewLine>pip install torch torchvision<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""62337""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/driazati/40/15393_2.png"" width=""20""/> driazati:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">script_model = torch.jit.script(model)<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>Okay, I was able to successfuly script the model as a pt file. I proceeded to place this pt file in an iOS project via pytorch mobile bridging headers. I also changed the settings to accept an 800x800 input image since the model specifies that.</p><NewLine><p>However when I try to run predict/inference on Swift/iOS I get the following error.</p><NewLine><pre><code class=""lang-auto"">2020-01-29 12:43:32.086469-0500 torchMobile[11047:2729940] <NewLine>Unknown builtin op: torchvision::_new_empty_tensor_op.<NewLine>Could not find any similar ops to torchvision::_new_empty_tensor_op. This op may not exist or may not be currently supported in TorchScript.<NewLine>:<NewLine>at /Users/hxh85ki/Desktop/Projects/thdEnv/lib/python3.6/site-packages/torchvision/ops/new_empty_tensor.py:16:11<NewLine>        output (Tensor)<NewLine>    """"""<NewLine>    return torch.ops.torchvision._new_empty_tensor_op(x, shape)<NewLine>           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>Serialized at code/__torch__/torchvision/ops/new_empty_tensor.py:4:7<NewLine>def _new_empty_tensor(x: Tensor,<NewLine>    shape: List[int]) -&gt; Tensor:<NewLine>  _0 = ops.torchvision._new_empty_tensor_op(x, shape)<NewLine>       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>  return _0<NewLine>'_new_empty_tensor' is being compiled since it was called from 'interpolate'<NewLine>Serialized at code/__torch__/torchvision/ops/misc.py:25:2<NewLine>    align_corners: Optional[bool]=None) -&gt; Tensor:<NewLine>  _1 = __torch__.torchvision.ops.misc._output_size<NewLine>  _2 = __torch__.torchvision.ops.new_empty_tensor._new_empty_tensor<NewLine>  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>  _3 = uninitialized(Tensor)<NewLine>  if torch.gt(torch.numel(input), 0):<NewLine>'interpolate' is being compiled since it was called from 'GeneralizedRCNNTransform.resize'<NewLine>Serialized at code/__torch__/torchvision/models/detection/transform.py:79:4<NewLine>    target: Optional[Dict[str, Tensor]]) -&gt; Tuple[Tensor, Optional[Dict[str, Tensor]]]:<NewLine>    _18 = __torch__.torchvision.models.detection.transform.resize_boxes<NewLine>    _19 = __torch__.torchvision.ops.misc.interpolate<NewLine>    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>    _20 = __torch__.torchvision.models.detection.transform.resize_keypoints<NewLine>    _21 = uninitialized(Tuple[Tensor, Optional[Dict[str, Tensor]]])<NewLine>'GeneralizedRCNNTransform.resize' is being compiled since it was called from 'GeneralizedRCNNTransform.forward'<NewLine>at /Users/hxh85ki/Desktop/Projects/thdEnv/lib/python3.6/site-packages/torchvision/models/detection/transform.py:47:34<NewLine>                                 ""of shape [C, H, W], got {}"".format(image.shape))<NewLine>            image = self.normalize(image)<NewLine>            image, target_index = self.resize(image, target_index)<NewLine>                                  ~~~~~~~~~~~ &lt;--- HERE<NewLine>            images[i] = image<NewLine>            if targets is not None and target_index is not None:<NewLine>Serialized at code/__torch__/torchvision/models/detection/transform.py:29:33<NewLine>        pass<NewLine>      image0 = (self).normalize(image, )<NewLine>      _2 = (self).resize(image0, target_index, )<NewLine>                                 ~~~~~~~~~~~~ &lt;--- HERE<NewLine>      image1, target_index0, = _2<NewLine>      _3 = torch._set_item(images0, i, image1)<NewLine>Fatal error: Can't find the model file!: file /Users/hxh85ki/thd-visual-ai/pytorchMobile/torchMobile/torchMobile/ViewController.swift, line 110<NewLine>2020-01-29 12:43:32.088293-0500 torchMobile[11047:2729940] Fatal error: Can't find the model file!: file /Users/hxh85ki/thd-visual-ai/pytorchMobile/torchMobile/torchMobile/ViewController.swift, line 110<NewLine></code></pre><NewLine><p>My model file is definitely in the project and I’m able to successfully able to run any normal image classification models on iOS. But this is my first attempt with Object Detection using Faster RCNN. Where am I going wrong? Any help would be appreciated. If necessary I could also take it to the mobile thread.</p><NewLine><p>From my understanding, if I can trace/script a pt file for mobile, pytorch mobile should be able to run it? This is a vanilla Faster RCNN Resnet50 fpn.</p><NewLine><p>Thanks, Haris</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>We don’t currently build and ship torchvision or torchaudio for mobile.  We’re looking into it.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/hussainharis"">@HussainHaris</a> David is right, the torchvision c++ APIs are not supported on mobile yet.  If your local pytorch version is 1.4.0, you can use the python API below to examine the ops used by your model</p><NewLine><pre><code class=""lang-auto"">torch.jit.export_opnames(traced_script_module)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/david_reiss"">@David_Reiss</a> I try to load faster rcnn model with nightly libtorch on Win10. But it failed. Is it support win10?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, how is shipping torchvision models for object detection or segmentation such as this FasterRCNN different from say shipping a resnet50 on mobile? Isn’t resnet50 and other CV models torchvision models? Just so I can understand better!</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Detection models use special ops that are not part of the PyTorch core.  Some segmentation architectures (like U-Net) work fine without those ops.  We are looking into supporting them.</p><NewLine><p>Win10 is supported.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve also run into this same issue using the pre-trained Faster RCNN model from pytorch, saved using</p><NewLine><pre><code class=""lang-auto"">model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)<NewLine>model.eval()<NewLine>script_module = torch.jit.script(model)<NewLine>script_module.save(""../models/frcnn_torchscript.pt"")<NewLine></code></pre><NewLine><p>Module.load using the Java wrappers in Android Studio gives:</p><NewLine><pre><code class=""lang-auto"">Unknown builtin op: torchvision::_new_empty_tensor_op.<NewLine>    Could not find any similar ops to torchvision::_new_empty_tensor_op. This op may not exist or may not be currently supported in TorchScript.<NewLine></code></pre><NewLine><p>Is this something I could fix by compiling the latest Pytorch Android from source?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>As David stated,  F-RCNN is a detection model using special ops not part of the Pytorch Core and is therefore not directly scriptable for mobile deployment unfortunately</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok thanks <a class=""mention"" href=""/u/hussainharis"">@HussainHaris</a>. Do you have any insight on what <a href=""https://github.com/pytorch/vision/pull/1407/files#diff-9b4079a8972dd58b35089da703a0a35f"" rel=""nofollow noopener"">this change</a> did and what I could do to build it into the android libraries?  It seems like there <em>is</em> a torchvision package available now (<a href=""https://github.com/pytorch/android-demo-app/blob/master/PyTorchDemoApp/app/build.gradle#L39"" rel=""nofollow noopener"">used in this example I was following</a>) which uses at least some of the vision ops, so maybe there is a way for me to hack the missing ops in and unblock myself for a while?</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""13"" data-topic=""62337"" data-username=""mrpropellers""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mrpropellers/40/21514_2.png"" width=""20""/> mrpropellers:</div><NewLine><blockquote><NewLine><p>this change</p><NewLine></blockquote><NewLine></aside><NewLine><p>I didn’t see this actually, maybe <a class=""mention"" href=""/u/david_reiss"">@David_Reiss</a> could shed some light on these changes for MaskRCNN scriptability?</p><NewLine><p>Please let me know <a class=""mention"" href=""/u/mrpropellers"">@mrpropellers</a> of any updates with this attempt. Currently I’m trying to add MaskRCNN Resnet50 via <a href=""https://medium.com/tensorflow/mlir-a-new-intermediate-representation-and-compiler-framework-beba999ed18d"" rel=""nofollow noopener"">MLRI/Tensorflow Lite’s experimental convertor</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/HussainHaris; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/HussainHaris; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ekilic; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/HussainHaris; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mrpropellers; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/HussainHaris; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/mrpropellers; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/HussainHaris; <NewLine> ,"REPLY_DATE 1: November 27, 2019,  7:33pm; <NewLine> REPLY_DATE 2: January 28, 2020,  7:37pm; <NewLine> REPLY_DATE 3: January 31, 2020,  4:44pm; <NewLine> REPLY_DATE 4: January 29, 2020,  5:53pm; <NewLine> REPLY_DATE 5: January 31, 2020,  4:44pm; <NewLine> REPLY_DATE 6: January 31, 2020,  4:44pm; <NewLine> REPLY_DATE 7: January 30, 2020, 10:45am; <NewLine> REPLY_DATE 8: January 31, 2020,  4:43pm; <NewLine> REPLY_DATE 9: February 20, 2020,  8:07pm; <NewLine> REPLY_DATE 10: March 10, 2020, 12:02am; <NewLine> REPLY_DATE 11: March 10, 2020, 12:22am; <NewLine> REPLY_DATE 12: March 10, 2020, 12:51am; <NewLine> REPLY_DATE 13: March 10, 2020,  4:09pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> 
53883,When should I use Tracing rather than Scripting?,2019-08-21T03:05:43.143Z,6,2094,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I went through the official doc of TorchScript (<a href=""https://pytorch.org/docs/stable/jit.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/jit.html</a>) but didn’t understand clearly what is the advantage of Tracing over Scripting.</p><NewLine><p>Af far as I understood, both <code>jit.script</code> and <code>jit.trace</code> can convert existing <code>nn.Module</code> instances into TorchScript. However Tracing cannot handle control flow such as if/for and it also requires an example input. The inability to handle control flow sounds like a huge deal breaker.</p><NewLine><p>The only disadvantage of Scripting I noticed is that it cannot handle several builtin modules like RNN/GRU.</p><NewLine><p>Are there any reasons to use Tracing over Scripting?</p><NewLine><p>Thank you.</p><NewLine><p>BTW, can LSTM be scripted? It is a little odd that RNN and GRU cannot be scripted but LSTM can.</p><NewLine></div>",https://discuss.pytorch.org/u/lucidfrontier45,(Shiqiao Du),lucidfrontier45,"August 21, 2019,  3:06am",5 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Tracing lets you use dynamic behavior in Python since it just records tensor operations. This may work better for your use case, but as you pointed out there are some fundamental limitations such as the inability to trace control flow / Python values. They both should be pretty easy to try out on your codebase (i.e. call <code>trace()</code> or <code>script()</code> on a module). If tracing does not work out of the box it is unlikely that you can maintain the semantics of your model and get it working under tracing (i.e. if you use control flow). Scripting may require some work to get your model using only <a href=""https://pytorch.org/docs/master/jit.html#torchscript-language-reference"" rel=""nofollow noopener"">features supported by the compiler</a>, but you will probably be able to get it working with some code changes.</p><NewLine><p><code>GRU</code> and <code>LSTM</code> can both be compiled on master/nightly, (AFAIK only <code>LSTM</code> is in the v1.2.0 release), we haven’t had many requests for <code>RNN</code> yet. If you’d like to see it (or something else in PyTorch) be script-able that <a href=""https://pytorch.org/docs/master/jit.html#builtin-functions"" rel=""nofollow noopener"">isn’t already</a>, please file an issue on GitHub.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/driazati"">@driazati</a> Thank you for reply.</p><NewLine><p>Do you mean that Tracing can handle almost all Python features/libraries except for for/while/if kind flows while Scripting can only handle a subset of Python features (aka TorchScript)? I still suspect there are many Python features that cannot be used with Tracing and therefore the difference between Tracing and Scripting is very small. Could you kindly give me an example where we should use Tracing over Scripting?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Tracing can handle anything that uses only PyTorch tensors and PyTorch operations. If someone passed a PyTorch tensor to a Pandas dataframe and did some operations, tracing wouldn’t capture that (though neither would script at this point), so there are limitations. If the only data flowing around your computations are tensors and there is no control flow, tracing is probably the way to go. Otherwise, use scripting.</p><NewLine><p>The <a href=""https://github.com/facebookresearch/pytext"" rel=""nofollow noopener"">pytext</a> library uses a mix of <a href=""https://github.com/facebookresearch/pytext/blob/2db0cc9d266b4ae45c5911a530f950bb49f79273/pytext/models/doc_model.py#L137"" rel=""nofollow noopener"">scripting</a> and <a href=""https://github.com/facebookresearch/pytext/blob/739aada896883235599d2589745b635347132904/pytext/models/test/bilstm_test.py"" rel=""nofollow noopener"">tracing</a>, and it all generally works well since they can be <a href=""https://pytorch.org/docs/master/jit.html#mixing-tracing-and-scripting"" rel=""nofollow noopener"">mixed together</a> pretty seamlessly.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have got similar confusion.</p><NewLine><p>Seems that even If “the only data flowing around your computations are tensors and there is no control flow”, we can still use scripting. Does this mean we can actually use scripting for all cases? Any speed difference between the two?</p><NewLine><p>Thanks.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Right, the only thing that would work in tracing but not scripting is use of Python language features and dynamic behavior that script mode doesn’t support. Since scripting compiles your code, you may have to do some work to make the compiler happy (e.g. add type annotations). But it is easy to try, just pass your module to <code>torch.jit.script</code>. The two compile to the same IR under the hood, so the speed should be about the same.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I don’t understand what do you mean by dynamic behaviour and python language features that is supported in trace but not in script? also</p><NewLine><p>what is the difference between Torchscript compiler and JIT compiler?</p><NewLine><pre><code class=""lang-auto"">Scripting a function or  `nn.Module`  will inspect the source code, <NewLine>compile it as TorchScript code using the TorchScript compiler. <NewLine></code></pre><NewLine><pre><code class=""lang-auto"">Trace a function and return an executable <NewLine>that will be optimized using just-in-time compilation.<NewLine></code></pre><NewLine><p>I request you to explain those in detail.</p><NewLine><p>Thanks.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, would you be able to give an example of dynamic behavior / Python language features that script mode doesn’t support? It sounds a bit abstract to me compared to the control flow statements that I know only work with script mode.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>It was also abstract for me in which can trace would be superior/more useful than scripting. I finally found an example in my code</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>from torch import nn<NewLine><NewLine><NewLine>class MyModule(nn.Module):<NewLine>    def __init__(self, return_b=False):<NewLine>        super().__init__()<NewLine>        self.return_b = return_b<NewLine><NewLine>    def forward(self, x):<NewLine>        a = x + 2<NewLine>        if self.return_b:<NewLine>            b = x + 3<NewLine>            return a, b<NewLine>        return a<NewLine><NewLine><NewLine>model = MyModule(return_b=True)<NewLine><NewLine># Will work<NewLine>traced = torch.jit.trace(model, (torch.randn(10, ), ))<NewLine><NewLine># Will fail<NewLine>scripted = torch.jit.script(model)<NewLine></code></pre><NewLine><p>This can easily be changed to be scriptable, but if you know the control flow is static once the model is exported, tracing will work just fine.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/lucidfrontier45; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/vincentzlt; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Midhilesh; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/austinmw; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mpariente; <NewLine> ,"REPLY_DATE 1: August 23, 2019,  9:23pm; <NewLine> REPLY_DATE 2: August 26, 2019,  4:43am; <NewLine> REPLY_DATE 3: August 26, 2019,  5:52pm; <NewLine> REPLY_DATE 4: March 31, 2020,  2:52am; <NewLine> REPLY_DATE 5: March 31, 2020,  3:51am; <NewLine> REPLY_DATE 6: June 10, 2020, 11:43am; <NewLine> REPLY_DATE 7: June 23, 2020,  1:53pm; <NewLine> REPLY_DATE 8: September 2, 2020,  6:51pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
94857,Retrieve the device type and device ID from c++ programmatically in a JIT GraphPass,2020-09-01T19:44:36.729Z,0,46,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is there a way to retrieve the device type and device ID of a model, from inside a custom JIT graph pass? In other words, if a customer does a <code>model.to(""cuda:0"")</code>, is there any way to retrieve “cuda, 0” from inside a graph pass? Please let me know.</p><NewLine></div>",https://discuss.pytorch.org/u/vdantu,(Vamshidhar Dantu (dan21c)),vdantu,"September 1, 2020,  7:44pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This depends on the state of the graph.<br/><NewLine>For example, in a freshly traced graph, you have the values (mostly) with as complete tensor type and that includes the device. Similarly dimensioned tensors (with scalar type, dimension (1d 2d 3d etc) and device, requires grad) have that information, but more incomplete values don’t.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can be done in a custom graph pass during inference. Graph pass only has reference to <code>std::shared_ptr&lt;Graph&gt;</code> . But assuming that this is during inference, is it possible to retrieve the device-type and device-id (in case of cuda)? This could be scripted model or traced model.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vdantu; <NewLine> ,"REPLY_DATE 1: September 1, 2020,  8:16pm; <NewLine> REPLY_DATE 2: September 1, 2020,  8:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
94708,Traced function with addmm slower than Python,2020-08-31T14:31:59.647Z,4,45,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am playing with very simple dense layer implementation using <code>torch.addmm</code> and it seems that <code>torch.jit.trace</code> transforms <code>addmm</code> op to sequence of <code>mm</code> and <code>add</code> ops, leading to performance drop on CPU:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch.autograd import profiler<NewLine>torch.set_num_threads(1)<NewLine><NewLine>def dense_layer(input, w, b):<NewLine>    return torch.addmm(input=b, mat1=input, mat2=w)<NewLine><NewLine>if __name__ == '__main__':<NewLine>    torch.random.manual_seed(1234)<NewLine>    a = torch.randn(100000, 10)<NewLine>    b = torch.randn(10, 10)<NewLine>    c = torch.randn(10)<NewLine><NewLine>    with profiler.profile() as prof:<NewLine>        for i in range(1000):<NewLine>            dense_layer(a, b, c)<NewLine>    print(prof.key_averages().table(sort_by='cpu_time_total', row_limit=5))<NewLine><NewLine>    traced = torch.jit.trace(dense_layer, (a, b, c))<NewLine>    with profiler.profile() as prof2:<NewLine>        for i in range(1000):<NewLine>            traced(a, b, c)<NewLine>    print(prof2.key_averages().table(sort_by='cpu_time_total', row_limit=5))<NewLine></code></pre><NewLine><p>And the output of this script on EC2 Windows with <code>Intel Xeon E5-2686 v4 @ 2.30GHz</code>:</p><NewLine><pre><code class=""lang-auto"">--------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  <NewLine>Name            Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  <NewLine>--------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  <NewLine>addmm           99.89%           5.603s           100.00%          5.609s           5.609ms          1000             <NewLine>expand          0.05%            2.927ms          0.09%            4.772ms          4.772us          1000             <NewLine>as_strided      0.03%            1.845ms          0.03%            1.845ms          1.845us          1000             <NewLine>--------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  <NewLine>Self CPU time total: 5.609s<NewLine><NewLine>-----------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  <NewLine>Name         Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  <NewLine>-----------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  <NewLine>add          65.54%           5.190s           65.81%           5.212s           5.212ms          1000             <NewLine>mm           33.91%           2.685s           34.19%           2.708s           2.708ms          1000             <NewLine>empty        0.29%            23.118ms         0.29%            23.118ms         11.559us         2000             <NewLine>-----------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  <NewLine>Self CPU time total: 7.919s<NewLine></code></pre><NewLine><p>Same script run on Fedora 32 with <code>Intel Core i7-8700K CPU @ 3.70GHz</code>:</p><NewLine><pre><code class=""lang-auto"">--------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  <NewLine>Name            Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  <NewLine>--------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  <NewLine>addmm           99.87%           1.863s           100.00%          1.866s           1.866ms          1000             <NewLine>expand          0.06%            1.164ms          0.10%            1.895ms          1.895us          1000             <NewLine>as_strided      0.04%            731.363us        0.04%            731.363us        0.731us          1000             <NewLine>--------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  <NewLine>Self CPU time total: 1.866s<NewLine><NewLine>-----------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  <NewLine>Name         Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  <NewLine>-----------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  <NewLine>add          73.35%           1.430s           73.40%           1.431s           1.431ms          1000             <NewLine>mm           26.50%           516.765ms        26.60%           518.530ms        518.530us        1000             <NewLine>empty        0.09%            1.726ms          0.09%            1.726ms          0.863us          2000             <NewLine>-----------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  <NewLine>Self CPU time total: 1.950s<NewLine></code></pre><NewLine><p>On Fedora machine difference is less pronounced, but still takes place. What’s the reason of this transformation? Because at least for CPU both <code>mm</code> and <code>addmm</code> call same <code>gemm</code> so it seems more reasonable to expand vector to cover whole matrix and call gemm afterwards. Is it because of focus on GPU? Is there any way to produce CPU-effective trace for such case?</p><NewLine></div>",https://discuss.pytorch.org/u/fatvlad,(Vlad Fatenko),fatvlad,"August 31, 2020,  2:31pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There is a secret linear function <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><pre><code class=""lang-auto"">def dense_layer(input, w, b):<NewLine>    return torch.ops.aten.linear(input, w, b)<NewLine></code></pre><NewLine><p>more seriously, I think that whatever causes addmm to be split could be considered a bug.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Secret function also being split in trace mode <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/><br/><NewLine>I will raise the issue then, however this does not look unintentional.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah, my bad. If you script that, it works. (even if you trace the scripted function, I think)</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, it does. Seems like scripting plain <code>addmm</code> version still does not work.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Moreover, running <code>torch.jit.script(dense_layer)</code> with the former implementation raises error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: The following operation failed in the TorchScript interpreter.<NewLine>Traceback of TorchScript (most recent call last):<NewLine>  File ""&lt;string&gt;"", line 3, in dense_layer<NewLine><NewLine>      def addmm(self: Tensor, mat1: Tensor, mat2: Tensor, beta: number = 1.0, alpha: number = 1.0):<NewLine>          return self + mat1.mm(mat2)<NewLine>                        ~~~~~~~ &lt;--- HERE<NewLine><NewLine>      def batch_norm(input : Tensor, running_mean : Optional[Tensor], running_var : Optional[Tensor], training : bool, momentum : float, eps : float) -&gt; Tensor:<NewLine>RuntimeError: Dimension out of range (expected to be in range of [-1, 0], but got 1)<NewLine></code></pre><NewLine><p>Edit: was just using wrong signature without keywords.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/fatvlad; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/fatvlad; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/fatvlad; <NewLine> ,"REPLY_DATE 1: September 3, 2020, 11:44am; <NewLine> REPLY_DATE 2: August 31, 2020,  3:39pm; <NewLine> REPLY_DATE 3: August 31, 2020,  3:42pm; <NewLine> REPLY_DATE 4: August 31, 2020,  3:47pm; <NewLine> REPLY_DATE 5: August 31, 2020,  4:03pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
94642,"In profiling mode, prim::DifferentiableGraph is present in the graph of a scripted module but not the same traced module",2020-08-31T03:49:24.307Z,0,32,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I have a module composed of conv2d and relu.</p><NewLine><p>In profiling mode, we can find one <code>prim::DifferentiableGraph</code> node in the scripted module. However, no <code>prim::DifferentiableGraph</code> is found if we trace this module.</p><NewLine><p>Could someone explain to me why there is this different behaviour between script and trace?</p><NewLine><p>The code:</p><NewLine><pre><code class=""lang-auto""><NewLine>from __future__ import division<NewLine><NewLine>import argparse<NewLine><NewLine>import torch<NewLine><NewLine>import torch.nn as nn<NewLine><NewLine>class MyModule(nn.Module):<NewLine><NewLine>    def __init__(self, in_channels, out_channels, **kwargs):<NewLine><NewLine>        super(MyModule, self).__init__()<NewLine><NewLine>        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)<NewLine><NewLine>        self.relu = nn.ReLU()<NewLine><NewLine>    def forward(self, x):<NewLine><NewLine>        return self.relu(self.conv(x))<NewLine><NewLine>def test(mode):<NewLine><NewLine>    print(""*"" * 10, mode, ""*"" * 10)<NewLine><NewLine>    ConvRelu = MyModule(3, 32, kernel_size = 3, stride = 1)<NewLine><NewLine>    x = torch.randn((1, 3, 8, 8))<NewLine><NewLine>    x.requires_grad = True<NewLine><NewLine>    if mode == 'script':<NewLine><NewLine>        m = torch.jit.script(ConvRelu)<NewLine><NewLine>    else:<NewLine><NewLine>        m = torch.jit.trace(ConvRelu, x)<NewLine><NewLine>    print('Conv2d+Relu Graph:\n', m.graph_for(x))<NewLine><NewLine>    print('Conv2d+Relu Graph:\n', m.graph_for(x))<NewLine><NewLine>if __name__ == '__main__':<NewLine><NewLine>    parser = argparse.ArgumentParser()<NewLine><NewLine>    parser.add_argument('--mode', '-m', required=True, choices=['script', 'trace'], help=""to script or to trace the module"")<NewLine><NewLine>    args = parser.parse_args()<NewLine><NewLine>    torch._C._jit_set_profiling_mode(True)<NewLine><NewLine>    torch._C._jit_set_profiling_executor(True)<NewLine><NewLine>    test(args.mode)<NewLine><NewLine></code></pre><NewLine><p>Script:</p><NewLine><pre><code class=""lang-auto""><NewLine>python script_trace.py -m script<NewLine><NewLine></code></pre><NewLine><pre><code class=""lang-auto""><NewLine>********** script **********<NewLine><NewLine>Conv2d+Relu Graph:<NewLine><NewLine> graph(%self : __torch__.MyModule,<NewLine><NewLine>      %x.1 : Tensor):<NewLine><NewLine>  %2 : int[] = prim::Constant[value=[0, 0]]()<NewLine><NewLine>  %3 : int[] = prim::Constant[value=[1, 1]]()<NewLine><NewLine>  %4 : int = prim::Constant[value=1]() # /home/pytorch/torch/nn/modules/conv.py:343:47<NewLine><NewLine>  %5 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name=""conv""](%self)<NewLine><NewLine>  %6 : Tensor = prim::GetAttr[name=""weight""](%5)<NewLine><NewLine>  %7 : Tensor? = prim::GetAttr[name=""bias""](%5)<NewLine><NewLine>  %8 : Tensor = prim::profile(%x.1)<NewLine><NewLine>  %9 : Tensor = prim::profile(%6)<NewLine><NewLine>  %10 : Tensor = aten::conv2d(%8, %9, %7, %3, %2, %3, %4) # /home/pytorch/torch/nn/modules/conv.py:345:15<NewLine><NewLine>  %11 : Tensor = prim::profile(%10)<NewLine><NewLine>  %result.2 : Tensor = aten::relu(%11) # /home/pytorch/torch/nn/functional.py:1063:17<NewLine><NewLine>  %13 : Tensor = prim::profile(%result.2)<NewLine><NewLine>   = prim::profile()<NewLine><NewLine>  return (%13)<NewLine><NewLine>Conv2d+Relu Graph:<NewLine><NewLine> graph(%self : __torch__.MyModule,<NewLine><NewLine>      %x.1 : Tensor):<NewLine><NewLine>  %4 : int = prim::Constant[value=1]() # /home/pytorch/torch/nn/modules/conv.py:343:47<NewLine><NewLine>  %3 : int[] = prim::Constant[value=[1, 1]]()<NewLine><NewLine>  %2 : int[] = prim::Constant[value=[0, 0]]()<NewLine><NewLine>  %21 : int = prim::BailoutTemplate_0()<NewLine><NewLine>  %18 : Float(1, 3, 8, 8) = prim::BailOut[index=0](%21, %x.1, %self)<NewLine><NewLine>  %5 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name=""conv""](%self)<NewLine><NewLine>  %6 : Tensor = prim::GetAttr[name=""weight""](%5)<NewLine><NewLine>  %19 : Float(32, 3, 3, 3) = prim::BailOut[index=1](%21, %6, %5, %18)<NewLine><NewLine>  %7 : Tensor? = prim::GetAttr[name=""bias""](%5)<NewLine><NewLine>  %10 : Tensor = aten::conv2d(%18, %19, %7, %3, %2, %3, %4) # /home/pytorch/torch/nn/modules/conv.py:345:15<NewLine><NewLine>  %20 : Float(1, 32, 6, 6) = prim::BailOut[index=2](%21, %10)<NewLine><NewLine>  %result.2 : Float(1, 32, 6, 6) = prim::DifferentiableGraph_1(%20)<NewLine><NewLine>  return (%result.2)<NewLine><NewLine>with prim::BailoutTemplate_0 = graph(%self : __torch__.MyModule,<NewLine><NewLine>      %x.1 : Tensor):<NewLine><NewLine>  %2 : Float(1, 3, 8, 8) = prim::BailOut[index=0](%x.1, %self)<NewLine><NewLine>  %3 : int[] = prim::Constant[value=[0, 0]]()<NewLine><NewLine>  %4 : int[] = prim::Constant[value=[1, 1]]()<NewLine><NewLine>  %5 : int = prim::Constant[value=1]() # /home/pytorch/torch/nn/modules/conv.py:343:47<NewLine><NewLine>  %6 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name=""conv""](%self)<NewLine><NewLine>  %7 : Tensor = prim::GetAttr[name=""weight""](%6)<NewLine><NewLine>  %8 : Float(32, 3, 3, 3) = prim::BailOut[index=1](%7, %6, %2)<NewLine><NewLine>  %9 : Tensor? = prim::GetAttr[name=""bias""](%6)<NewLine><NewLine>  %10 : Tensor = aten::conv2d(%2, %8, %9, %4, %3, %4, %5) # /home/pytorch/torch/nn/modules/conv.py:345:15<NewLine><NewLine>  %11 : Float(1, 32, 6, 6) = prim::BailOut[index=2](%10)<NewLine><NewLine>  %result.2 : Float(1, 32, 6, 6) = aten::relu(%11) # /home/pytorch/torch/nn/functional.py:1063:17<NewLine><NewLine>  return (%result.2)<NewLine><NewLine>with prim::DifferentiableGraph_1 = graph(%0 : Float(1, 32, 6, 6)):<NewLine><NewLine>  %result.3 : Float(1, 32, 6, 6) = aten::relu(%0) # /home/pytorch/torch/nn/functional.py:1063:17<NewLine><NewLine>  return (%result.3)<NewLine><NewLine></code></pre><NewLine><p>We can find one <code>prim::DifferentiableGraph</code> node when we print the graph for the second time.</p><NewLine><p>Trace:</p><NewLine><pre><code class=""lang-auto""><NewLine>python script_trace.py -m trace<NewLine><NewLine></code></pre><NewLine><pre><code class=""lang-auto""><NewLine>********** trace **********<NewLine><NewLine>Conv2d+Relu Graph:<NewLine><NewLine> graph(%self.1 : __torch__.MyModule,<NewLine><NewLine>      %input.1 : Tensor):<NewLine><NewLine>  %7 : None = prim::Constant(), scope: __module.conv<NewLine><NewLine>  %6 : int[] = prim::Constant[value=[1, 1]]()<NewLine><NewLine>  %5 : int[] = prim::Constant[value=[0, 0]]()<NewLine><NewLine>  %4 : bool = prim::Constant[value=0](), scope: __module.conv # /home/pytorch/torch/nn/modules/conv.py:346:0<NewLine><NewLine>  %3 : int = prim::Constant[value=1](), scope: __module.conv # /home/pytorch/torch/nn/modules/conv.py:346:0<NewLine><NewLine>  %2 : bool = prim::Constant[value=1](), scope: __module.conv # /home/pytorch/torch/nn/modules/conv.py:346:0<NewLine><NewLine>  %23 : int = prim::BailoutTemplate_0()<NewLine><NewLine>  %20 : Float(1, 3, 8, 8) = prim::BailOut[index=0](%23, %input.1, %self.1)<NewLine><NewLine>  %8 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name=""conv""](%self.1)<NewLine><NewLine>  %9 : Tensor = prim::GetAttr[name=""weight""](%8)<NewLine><NewLine>  %21 : Float(32, 3, 3, 3) = prim::BailOut[index=1](%23, %9, %20)<NewLine><NewLine>  %input : Tensor = aten::_convolution(%20, %21, %7, %6, %5, %6, %4, %5, %3, %4, %4, %2), scope: __module.conv # /home/pytorch/torch/nn/modules/conv.py:346:0<NewLine><NewLine>  %22 : Float(1, 32, 6, 6) = prim::BailOut[index=2](%23, %input)<NewLine><NewLine>  %14 : Float(1, 32, 6, 6) = aten::relu(%22), scope: __module.relu # /home/pytorch/torch/nn/functional.py:1063:0<NewLine><NewLine>  return (%14)<NewLine><NewLine>with prim::BailoutTemplate_0 = graph(%self.1 : __torch__.MyModule,<NewLine><NewLine>      %input.1 : Tensor):<NewLine><NewLine>  %2 : Float(1, 3, 8, 8) = prim::BailOut[index=0](%input.1, %self.1)<NewLine><NewLine>  %3 : bool = prim::Constant[value=1](), scope: __module.conv # /home/pytorch/torch/nn/modules/conv.py:346:0<NewLine><NewLine>  %4 : int = prim::Constant[value=1](), scope: __module.conv # /home/pytorch/torch/nn/modules/conv.py:346:0<NewLine><NewLine>  %5 : bool = prim::Constant[value=0](), scope: __module.conv # /home/pytorch/torch/nn/modules/conv.py:346:0<NewLine><NewLine>  %6 : int[] = prim::Constant[value=[0, 0]]()<NewLine><NewLine>  %7 : int[] = prim::Constant[value=[1, 1]]()<NewLine><NewLine>  %8 : None = prim::Constant(), scope: __module.conv<NewLine><NewLine>  %9 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name=""conv""](%self.1)<NewLine><NewLine>  %10 : Tensor = prim::GetAttr[name=""weight""](%9)<NewLine><NewLine>  %11 : Float(32, 3, 3, 3) = prim::BailOut[index=1](%10, %2)<NewLine><NewLine>  %input : Tensor = aten::_convolution(%2, %11, %8, %7, %6, %7, %5, %6, %4, %5, %5, %3), scope: __module.conv # /home/pytorch/torch/nn/modules/conv.py:346:0<NewLine><NewLine>  %13 : Float(1, 32, 6, 6) = prim::BailOut[index=2](%input)<NewLine><NewLine>  %14 : Float(1, 32, 6, 6) = aten::relu(%13), scope: __module.relu # /home/pytorch/torch/nn/functional.py:1063:0<NewLine><NewLine>  return (%14)<NewLine><NewLine>Conv2d+Relu Graph:<NewLine><NewLine> graph(%self.1 : __torch__.MyModule,<NewLine><NewLine>      %input.1 : Tensor):<NewLine><NewLine>  %7 : None = prim::Constant(), scope: __module.conv<NewLine><NewLine>  %6 : int[] = prim::Constant[value=[1, 1]]()<NewLine><NewLine>  %5 : int[] = prim::Constant[value=[0, 0]]()<NewLine><NewLine>  %4 : bool = prim::Constant[value=0](), scope: __module.conv # /home/pytorch/torch/nn/modules/conv.py:346:0<NewLine><NewLine>  %3 : int = prim::Constant[value=1](), scope: __module.conv # /home/pytorch/torch/nn/modules/conv.py:346:0<NewLine><NewLine>  %2 : bool = prim::Constant[value=1](), scope: __module.conv # /home/pytorch/torch/nn/modules/conv.py:346:0<NewLine><NewLine>  %23 : int = prim::BailoutTemplate_0()<NewLine><NewLine>  %20 : Float(1, 3, 8, 8) = prim::BailOut[index=0](%23, %input.1, %self.1)<NewLine><NewLine>  %8 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name=""conv""](%self.1)<NewLine><NewLine>  %9 : Tensor = prim::GetAttr[name=""weight""](%8)<NewLine><NewLine>  %21 : Float(32, 3, 3, 3) = prim::BailOut[index=1](%23, %9, %20)<NewLine><NewLine>  %input : Tensor = aten::_convolution(%20, %21, %7, %6, %5, %6, %4, %5, %3, %4, %4, %2), scope: __module.conv # /home/pytorch/torch/nn/modules/conv.py:346:0<NewLine><NewLine>  %22 : Float(1, 32, 6, 6) = prim::BailOut[index=2](%23, %input)<NewLine><NewLine>  %14 : Float(1, 32, 6, 6) = aten::relu(%22), scope: __module.relu # /home/pytorch/torch/nn/functional.py:1063:0<NewLine><NewLine>  return (%14)<NewLine><NewLine>with prim::BailoutTemplate_0 = graph(%self.1 : __torch__.MyModule,<NewLine><NewLine>      %input.1 : Tensor):<NewLine><NewLine>  %2 : Float(1, 3, 8, 8) = prim::BailOut[index=0](%input.1, %self.1)<NewLine><NewLine>  %3 : bool = prim::Constant[value=1](), scope: __module.conv # /home/pytorch/torch/nn/modules/conv.py:346:0<NewLine><NewLine>  %4 : int = prim::Constant[value=1](), scope: __module.conv # /home/pytorch/torch/nn/modules/conv.py:346:0<NewLine><NewLine>  %5 : bool = prim::Constant[value=0](), scope: __module.conv # /home/pytorch/torch/nn/modules/conv.py:346:0<NewLine><NewLine>  %6 : int[] = prim::Constant[value=[0, 0]]()<NewLine><NewLine>  %7 : int[] = prim::Constant[value=[1, 1]]()<NewLine><NewLine>  %8 : None = prim::Constant(), scope: __module.conv<NewLine><NewLine>  %9 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name=""conv""](%self.1)<NewLine><NewLine>  %10 : Tensor = prim::GetAttr[name=""weight""](%9)<NewLine><NewLine>  %11 : Float(32, 3, 3, 3) = prim::BailOut[index=1](%10, %2)<NewLine><NewLine>  %input : Tensor = aten::_convolution(%2, %11, %8, %7, %6, %7, %5, %6, %4, %5, %5, %3), scope: __module.conv # /home/pytorch/torch/nn/modules/conv.py:346:0<NewLine><NewLine>  %13 : Float(1, 32, 6, 6) = prim::BailOut[index=2](%input)<NewLine><NewLine>  %14 : Float(1, 32, 6, 6) = aten::relu(%13), scope: __module.relu # /home/pytorch/torch/nn/functional.py:1063:0<NewLine><NewLine>  return (%14)<NewLine><NewLine></code></pre><NewLine><p>In this case, there is no <code>prim::DifferentiableGraph</code> node in the graph.</p><NewLine><p>PyTorch commit:</p><NewLine><pre><code class=""lang-auto""><NewLine>commit b58f89b2e4b4a6dc9fbc0c00e608de0f4db52267<NewLine><NewLine></code></pre><NewLine><p>changes made to the threshold:</p><NewLine><pre><code class=""lang-auto""><NewLine>diff --git a/torch/csrc/jit/runtime/graph_executor_impl.h b/torch/csrc/jit/runtime/graph_executor_impl.h<NewLine><NewLine>index a2fd10c..f4f43e2 100644<NewLine><NewLine>--- a/torch/csrc/jit/runtime/graph_executor_impl.h<NewLine><NewLine>+++ b/torch/csrc/jit/runtime/graph_executor_impl.h<NewLine><NewLine>@@ -40,8 +40,8 @@ bool getAutodiffSubgraphInlining();<NewLine><NewLine> <NewLine><NewLine> // Tunable parameters for deciding when to create/keep subgraphs of<NewLine><NewLine> // differentiable code<NewLine><NewLine>-const size_t autodiffSubgraphNodeThreshold = 2;<NewLine><NewLine>-const size_t autodiffSubgraphInlineThreshold = 5;<NewLine><NewLine>+const size_t autodiffSubgraphNodeThreshold = 1;<NewLine><NewLine>+const size_t autodiffSubgraphInlineThreshold = 1;<NewLine><NewLine></code></pre><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/chunyuan-w,(Chunyuan W),chunyuan-w,"August 31, 2020,  3:49am",,,,,
94433,Order of nodes in scripted graph,2020-08-28T15:43:32.261Z,1,61,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Suppose I have the following graph and example code.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine>class Net(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Net, self).__init__()<NewLine><NewLine>    def forward(self, x):<NewLine>        z = x.squeeze(0)<NewLine>        z.add_(2)<NewLine>        x.mul_(5)<NewLine>        return x<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    script = torch.jit.script(Net().eval())<NewLine>    print(str(script))<NewLine>    print(""-"" * 10)<NewLine>    for node in script.graph.nodes():<NewLine>        print(node)<NewLine></code></pre><NewLine><p>Is the loop guaranteed to iterate over the nodes in an order that preserves the semantics of the in-place operations in the graph?<br/><NewLine>In this simple example its just enough that the add is looped over before the mul, but the general case is more important to me.<br/><NewLine>I do get the correct order when running this script, but I’m interested if this is guaranteed.</p><NewLine></div>",https://discuss.pytorch.org/u/ttipri,(Ian),ttipri,"August 28, 2020,  3:44pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Reodering is done with checking dependencies, including side effects:<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/58a7e73a95865e5c02a581703c39edd671495a6c/torch/csrc/jit/ir/alias_analysis.h#L98-L108"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/58a7e73a95865e5c02a581703c39edd671495a6c/torch/csrc/jit/ir/alias_analysis.h#L98-L108"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/58a7e73a95865e5c02a581703c39edd671495a6c/torch/csrc/jit/ir/alias_analysis.h#L98-L108</a></h4><NewLine><pre class=""onebox""><code class=""lang-h""><ol class=""start lines"" start=""98"" style=""counter-reset: li-counter 97 ;""><NewLine><li>// Move 'n' (already in the graph) after 'movePoint' in the topological order.</li><NewLine><li>//</li><NewLine><li>// Tries to preserve value dependencies, so other nodes might be moved. We</li><NewLine><li>// make two guarantees about the postcondition of the node list:</li><NewLine><li>//   - `n` is directly after `movePoint`.</li><NewLine><li>//   - only nodes between `n` and `movePoint` have been moved.</li><NewLine><li>//</li><NewLine><li>// Returns `false` if it's impossible to move `n` after `MovePoint` without</li><NewLine><li>// violating dependencies, otherwise executes the move and returns `true`</li><NewLine><li>TORCH_API bool moveAfterTopologicallyValid(Node* n, Node* movePoint);</li><NewLine><li>TORCH_API bool moveBeforeTopologicallyValid(Node* n, Node* movePoint);</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks! Greatly appreciated (:</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ttipri; <NewLine> ,"REPLY_DATE 1: August 28, 2020,  6:49pm; <NewLine> REPLY_DATE 2: August 28, 2020,  6:50pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
94197,Tracing Torchvision Models,2020-08-26T15:33:16.015Z,0,50,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello All,<br/><NewLine>I am trying to trace the <code>fasterrcnn-resnet50-fpn</code> and save it in a .pt file later to be read and used by libtorch. I went through the thread- <a href=""https://discuss.pytorch.org/t/fasterrcnn-resnet50-jit-trace/62337/15"">jit trace of fasterRCNN</a> and saw that the PR to add scriptability of fasterRCNN was merged<br/><NewLine>I am trying trace it using the following code (pytorch 1.6.0)</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torchvision<NewLine>from PIL import Image<NewLine><NewLine>model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True).eval()<NewLine>example = tuple(list(torch.rand((3, 640, 480))))<NewLine>traced_Script = torch.jit.trace(model, example_inputs=example)<NewLine>traced_Script.save(""/Models/fasterrcnn.pt"")<NewLine></code></pre><NewLine><p>and run into the following error -</p><NewLine><pre><code class=""lang-auto"">TypeError: forward() takes from 2 to 3 positional arguments but 4 were given<NewLine></code></pre><NewLine><p>The torchvision documentation says we are supposed to provide the input as list so I went ahead with that,<br/><NewLine>Could anyone please tell what could be going wrong<br/><NewLine>TIA</p><NewLine></div>",https://discuss.pytorch.org/u/a_d,,a_d,"August 26, 2020,  3:33pm",,,,,
92188,How to define and save a custom operator with a dynamic schema?,2020-08-09T19:56:53.491Z,2,99,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,<br/><NewLine>Is it possible to define and save a custom operator which can potentially take any number of inputs of any type and similarly return any number of outputs of any type? For example, these could be nodes representing/containing subgraph attribute.</p><NewLine><p>Also, I wanted to understand why PyTorch JIT IR doesn’t support saving operator node’s attributes?</p><NewLine><p>P.S: This topic is similar to <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/saving-fusiongroup-as-part-of-scriptmodule/62263"">Saving FusionGroup as part of ScriptModule</a></p><NewLine></div>",https://discuss.pytorch.org/u/vdantu,(Vamshidhar Dantu (dan21c)),vdantu,"August 10, 2020,  3:00am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Vamshi,</p><NewLine><p>For any number of inputs/outputs of any type, have you tried List[Any]? Your operator would need some way of knowing the exact type of each element though, which can be done by passing some metadata input or attribute.</p><NewLine><p>I am not entirely sure what you mean by “JIT IR doesn’t support saving operator node’s attributes”, since node attributes are saved as far as I know. Please clarify.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/yanan_cao"">@Yanan_Cao</a>: Thanks for the response. I had a couple of questions about this</p><NewLine><ol><NewLine><li>I couldn’t find any example of any operator with schema <code>List[Any]</code> in 1.4 or 1.5. Could you please point to any example that I can follow?</li><NewLine><li>List by definition expect all elements of same type right? So, how could this be used if the operator takes in “int, bool, float, tensor, lists” in any order and any number?</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>Does it have to be 1.4 or 1.5?</li><NewLine></ol><NewLine><p>Here is an example, not sure if it is in 1.5 or 1.6<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/8094228f26cdef24529bffadd6a6e43b56df2d6e/torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp#L674"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/8094228f26cdef24529bffadd6a6e43b56df2d6e/torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp#L674"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/8094228f26cdef24529bffadd6a6e43b56df2d6e/torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp#L674</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""664"" style=""counter-reset: li-counter 663 ;""><NewLine><li>// round((x_e + x_r)/2)*2 = x_e + round(x_r/2)*2, where x_e is an even integer,</li><NewLine><li>// x_r is either 0.5 of 1.5, round(x_r/2)*2 results a 0 or 2, so the final</li><NewLine><li>// result will always be a even number. Due to symmetricity, it also applies to</li><NewLine><li>// negative cases.</li><NewLine><li>double round_to_even(double a) {</li><NewLine><li>  // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)</li><NewLine><li>  return a - std::floor(a) == 0.5 ? (std::round(a * 0.5) * 2.0) : std::round(a);</li><NewLine><li>}</li><NewLine><li><NewLine></li><li>RegisterOperators reg2({</li><NewLine><li class=""selected"">    // registered as Any[] so that heterogenous tuples can be called with len()</li><NewLine><li>    Operator(</li><NewLine><li>        ""aten::len.any(Any[] a) -&gt; int"",</li><NewLine><li>        listLen,</li><NewLine><li>        aliasAnalysisFromSchema()),</li><NewLine><li><NewLine></li><li>// these ops have a specialized implementation for the list element type</li><NewLine><li>#define CREATE_SPECIALIZED_LIST_OPS(decl_type, value_type) \</li><NewLine><li>  Operator(                                                \</li><NewLine><li>      ""aten::remove."" decl_type ""("" decl_type              \</li><NewLine><li>      ""[](a!) self,                                                           \</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><ol start=""2""><NewLine><li>I think it can be heterogenous. Indeed it is not a common use case though, let us know if you hit any rough edges.</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/yanan_cao"">@Yanan_Cao</a>: Thanks for the pointers. These were very helpful. Using the type <code>Any</code> seems like a flexible way. It wasn’t supported in 1.4.x though.</p><NewLine><p>I am defining my custom operator as varargs.</p><NewLine><pre><code class=""lang-auto"">my::Customop(...) -&gt; (...)<NewLine></code></pre><NewLine><p>This seems to work to save multiple inputs and multiple outputs of different types. Is this a recommended way to represent an operator, or should I look out for any corner case?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Given your requirement, I think this is probably the only way to represent the schema of your operator. Yes, you should look out for corner cases given that the version you are using is slightly older.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Yanan_Cao; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vdantu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Yanan_Cao; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/vdantu; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Yanan_Cao; <NewLine> ,"REPLY_DATE 1: August 11, 2020,  4:22pm; <NewLine> REPLY_DATE 2: August 11, 2020,  5:04pm; <NewLine> REPLY_DATE 3: August 19, 2020,  6:02am; <NewLine> REPLY_DATE 4: September 27, 2020,  2:33am; <NewLine> REPLY_DATE 5: August 25, 2020, 11:59pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
94056,RuntimeError: output 1 of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer,2020-08-25T13:32:18.346Z,0,55,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello all!</p><NewLine><p>I’m trying to convert a pytorch model(<a href=""https://github.com/ZTao-z/resnet-ssd#use-a-pre-trained-ssd-network-for-detection"" rel=""nofollow noopener"">https://github.com/ZTao-z/resnet-ssd#use-a-pre-trained-ssd-network-for-detection</a>) to onnx.</p><NewLine><p>This is the code I’m trying to use:</p><NewLine><pre><code class=""lang-auto"">from ssd import build_ssd<NewLine>import os<NewLine>import sys<NewLine>import time<NewLine>import torch<NewLine>import torch.onnx<NewLine>import torch.nn as nn<NewLine>import torch.backends.cudnn as cudnn<NewLine>import numpy as np<NewLine>import argparse<NewLine><NewLine>torch.set_default_tensor_type('torch.cuda.FloatTensor')<NewLine><NewLine>if __name__ == '__main__':<NewLine>     ssd_net = build_ssd('train', 300, 21)<NewLine><NewLine>     if args.resume:<NewLine>         print('Resuming training, loading {}...'.format(args.resume))<NewLine>         ssd_net.load_weights(args.resume)<NewLine>     ssd_net = ssd_net.cuda().eval()<NewLine><NewLine>     dummy_input = torch.randn(1, 3, 300, 300, device='cuda', requires_grad=True)<NewLine>     for param in ssd_net.parameters():<NewLine>         param.requires_grad = False<NewLine><NewLine>#     for name, param in ssd_net.named_parameters():<NewLine>#         print(name, param)<NewLine>#         break<NewLine><NewLine>     torch.onnx.export(ssd_net, dummy_input, ""onnx_model_name.onnx"", verbose=True, export_params=True)<NewLine><NewLine></code></pre><NewLine><p>This is the error:</p><NewLine><pre><code class=""lang-auto"">Resuming training, loading ssd300_mAP_77.43_v2.pth...<NewLine>Loading weights into state dict...<NewLine>Finished!<NewLine>Traceback (most recent call last):<NewLine>  File ""onnx_conversion.py"", line 38, in &lt;module&gt;<NewLine>    torch.onnx.export(ssd_net, dummy_input, ""onnx_model_name.onnx"", verbose=True, export_params=True)<NewLine>  File ""/home/user_name/venv/lib/python3.6/site-packages/torch/onnx/__init__.py"", line 208, in export<NewLine>    custom_opsets, enable_onnx_checker, use_external_data_format)<NewLine>  File ""/home/user_name/venv/lib/python3.6/site-packages/torch/onnx/utils.py"", line 92, in export<NewLine>    use_external_data_format=use_external_data_format)<NewLine>  File ""/home/user_name/venv/lib/python3.6/site-packages/torch/onnx/utils.py"", line 530, in _export<NewLine>    fixed_batch_size=fixed_batch_size)<NewLine>  File ""/home/user_name/venv/lib/python3.6/site-packages/torch/onnx/utils.py"", line 366, in _model_to_graph<NewLine>    graph, torch_out = _trace_and_get_graph_from_model(model, args)<NewLine>  File ""/home/user_name/venv/lib/python3.6/site-packages/torch/onnx/utils.py"", line 319, in _trace_and_get_graph_from_model<NewLine>    torch.jit._get_trace_graph(model, args, strict=False, _force_outplace=False, _return_inputs_states=True)<NewLine>  File ""/home/user_name/venv/lib/python3.6/site-packages/torch/jit/__init__.py"", line 338, in _get_trace_graph<NewLine>    outs = ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)<NewLine>  File ""/home/user_name/venv/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/user_name/venv/lib/python3.6/site-packages/torch/jit/__init__.py"", line 426, in forward<NewLine>    self._force_outplace,<NewLine>RuntimeError: output 1 ( 0.0133  0.0133  0.0700  0.0700<NewLine> 0.0133  0.0133  0.1025  0.1025<NewLine> 0.0133  0.0133  0.0990  0.0495<NewLine> 0.0133  0.0133  0.0495  0.0990<NewLine> 0.0400  0.0133  0.0700  0.0700<NewLine>------------------ 8723 more lines ------------------<NewLine> 0.5000  0.5000  0.8700  0.8700<NewLine> 0.5000  0.5000  0.9558  0.9558<NewLine> 0.5000  0.5000  1.0000  0.6152<NewLine> 0.5000  0.5000  0.6152  1.0000<NewLine>[ CUDAFloatType{8732,4} ]) of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer.<NewLine></code></pre><NewLine><p>There is no problem with invoking the pytorch model &amp; loading state_dict.<br/><NewLine>The problem comes with conversion to onnx model.</p><NewLine><p>Any help is greatly appreciated!</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/spabho,,spabho,"August 25, 2020,  1:32pm",1 Like,,,,
93658,Get intermediate outputs from torchscript graph,2020-08-21T18:30:14.263Z,0,43,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to get the values of intermediate ops in the torchscript graph? Or is the only way to modify the source model to keep intermediate outputs in the actual output (or something like this)?</p><NewLine></div>",https://discuss.pytorch.org/u/flynntax,,flynntax,"August 21, 2020,  6:30pm",,,,,
93137,Creating a torchscript wrapper?,2020-08-17T20:20:52.995Z,0,62,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m aware that torchscript does not support mixed serialization but is there anyway to easily save a torchscript file with a preprocessing wrapper? The actual input to my models are vectorized features derived from a text file and the features chosen are model hyperparameters, as such I wanted to hide that process under the hood allowing users to pass a raw text file and have the preprocessing steps wrapped around torchscript object but I can’t see how to do this without mixed serialization?</p><NewLine><p>Is there a work around?</p><NewLine></div>",https://discuss.pytorch.org/u/Michael_Moran,(Michael Moran),Michael_Moran,"August 17, 2020,  8:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just make a class wrapper. First, make all preprocessing functions torchscript friendly, then you can create a class like:</p><NewLine><pre><code class=""lang-python"">class Wrapper(nn.Module):<NewLine>    def __init__(self, preprocess_fn, model):<NewLine>       self.preprocess_fn = preprocess_fn<NewLine>       self.model = model<NewLine><NewLine>    def forward(self, x): # remember, by default, x is expected to be a torch.Tensor. <NewLine>        return self.model(self.preprocess_fn(x))<NewLine><NewLine>wrapper = Wrapper(your_preprocess_function, your_model)<NewLine>wrapper_ts = torch.jit.script(wrapper)<NewLine></code></pre><NewLine><p>And that’s all!<br/><NewLine>I did the same approach but to integrate some post processing steps without needing to deploy extra python files.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vferrer; <NewLine> ,"REPLY_DATE 1: August 21, 2020,  8:24am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
93433,Model computation time measurement when using Torchscript in C++ (reposted),2020-08-20T02:33:04.739Z,0,63,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello</p><NewLine><p>In order to measure the computation time of the deep learning model when using GPU, we have to keep in mind about GPU operations asynchronicity. Thus, we should use torch.cuda.Event or torch.autograd.profiler in Python.</p><NewLine><p>I found torch::autograd::profiler in libtorch but there is no API documentation to know how to use functions in the library. This looks like C++ version of torch.autograd.profiler which to measure model computation time. However, there are many examples to measure computation time in Python but it is hard to find any example to measure computation time when using GPU in C++.</p><NewLine><p>(This might be because the majority of users except me are familiar to use C++ and C++ library. <img alt="":sweat_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/sweat_smile.png?v=9"" title="":sweat_smile:""/>)</p><NewLine><p><strong>Could someone provide a simple example to measure computation time of Torchscript model considering both CPU time and GPU time in C++?</strong></p><NewLine><p>FYI, in order to correctly measure computation time in Python3, please refer this link:<br/><NewLine><aside class=""quote"" data-post=""1"" data-topic=""26964""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/daulbaev/40/1824_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/how-to-measure-time-in-pytorch/26964"">How to measure time in PyTorch</a><NewLine></div><NewLine><blockquote><NewLine>    I have seen lots of ways to measure time in PyTorch. But what is the most proper way to do it now (both for cpu and cuda)? <NewLine>Should I clear the memory cache if I use timeit? <NewLine>And is it possible to get accurate results if I’m computing on a cluster? And is it a way to make this results reproducible? <NewLine>And what is better: timeit or profiler?<NewLine>  </blockquote><NewLine></aside><NewLine></p><NewLine><p>Many thanks!</p><NewLine><ul><NewLine><li>I copied this question <a href=""https://discuss.pytorch.org/t/model-computation-time-measurement-when-using-torchscript-in-c/93218"">here</a><NewLine></li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/Jungmo_Ahn,(Jungmo Ahn),Jungmo_Ahn,"August 20, 2020,  2:33am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">std::chrono::steady_clock::time_point begin = std::chrono::steady_clock::now();<NewLine>model.forward(inputs).toTensor();<NewLine>cudaDeviceSynchronize(); // declared in cuda.h<NewLine>std::chrono::steady_clock::time_point begin = std::chrono::steady_clock::now();<NewLine></code></pre><NewLine><p>This is my solution to measure elapsed time for model computation when using GPU.<br/><NewLine>If there is a better way, plz suggest me.</p><NewLine><p>Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Jungmo_Ahn; <NewLine> ,"REPLY_DATE 1: August 20, 2020,  1:29pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
93472,"c10::Error: Couldn&rsquo;t find an operator for aten::dropout(Tensor input, float p, bool train)",2020-08-20T09:46:14.465Z,0,54,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I got an error with <code>c10::Error: Couldn't find an operator for aten::dropout(Tensor input, float p, bool train)</code> when I tried to load a torchscript model by libtorch c++ api. The linking libtorch library is compiled with <code>script/build_pytorch_android.sh</code> script.</p><NewLine><pre><code class=""lang-auto"">target platform: android arm64-v8a<NewLine>pytorch tag: v1.6.0<NewLine></code></pre><NewLine><p>And if I linked the program with x86 prebuilt libtorch library, and run it in x86 platform, all worked well.</p><NewLine></div>",https://discuss.pytorch.org/u/Jonson,(叶俊贤),Jonson,"August 20, 2020, 12:27pm",,,,,
93468,Different Output from Libtorch c++than pytorch,2020-08-20T09:12:31.033Z,1,50,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m using the same traced model in both c++ and python but I’m getting different outputs.</p><NewLine><p>Python Code</p><NewLine><pre><code class=""lang-auto"">import cv2<NewLine>import numpy as np <NewLine>import torch<NewLine>import torchvision<NewLine>from torchvision import transforms as trans<NewLine><NewLine><NewLine># device for pytorch<NewLine>device = torch.device('cuda:0')<NewLine><NewLine>torch.set_default_tensor_type('torch.cuda.FloatTensor')<NewLine><NewLine>model = torch.jit.load(""traced_facelearner_model_new.pt"")<NewLine>model.eval()<NewLine><NewLine># read the example image used for tracing<NewLine>image=cv2.imread(""videos/example.jpg"")<NewLine><NewLine>test_transform = trans.Compose([<NewLine>            trans.ToTensor(),<NewLine>            trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])<NewLine>        ])       <NewLine><NewLine>resized_image = cv2.resize(image, (112, 112))<NewLine><NewLine>tens = test_transform(resized_image).to(device).unsqueeze(0)<NewLine>output = model(tens)<NewLine>print(output)<NewLine></code></pre><NewLine><p>C++ code</p><NewLine><pre><code class=""lang-auto"">#include &lt;iostream&gt;<NewLine>#include &lt;algorithm&gt; <NewLine>#include &lt;opencv2/opencv.hpp&gt;<NewLine>#include &lt;torch/script.h&gt;<NewLine><NewLine><NewLine>int main()<NewLine>{<NewLine>	try<NewLine>	{<NewLine>		torch::jit::script::Module model = torch::jit::load(""traced_facelearner_model_new.pt"");<NewLine>		model.to(torch::kCUDA);<NewLine>		model.eval();<NewLine><NewLine>		cv::Mat visibleFrame = cv::imread(""example.jpg"");<NewLine><NewLine>		cv::resize(visibleFrame, visibleFrame, cv::Size(112, 112));<NewLine>		at::Tensor tensor_image = torch::from_blob(visibleFrame.data, { 1, visibleFrame.rows, visibleFrame.cols, 3 }, at::kByte);<NewLine>		tensor_image = tensor_image.permute({ 0, 3, 1, 2 });<NewLine>		tensor_image = tensor_image.to(at::kFloat);<NewLine><NewLine>		tensor_image[0][0] = tensor_image[0][0].sub(0.5).div(0.5);<NewLine>		tensor_image[0][1] = tensor_image[0][1].sub(0.5).div(0.5);<NewLine>		tensor_image[0][2] = tensor_image[0][2].sub(0.5).div(0.5);<NewLine><NewLine>		tensor_image = tensor_image.to(torch::kCUDA);<NewLine>		std::vector&lt;torch::jit::IValue&gt; input;<NewLine>		input.emplace_back(tensor_image);<NewLine>		// Execute the model and turn its output into a tensor.<NewLine>		auto output = model.forward(input).toTensor();<NewLine>		output = output.to(torch::kCPU);<NewLine>		std::cout &lt;&lt; ""Embds: "" &lt;&lt; output &lt;&lt; std::endl;<NewLine><NewLine>		std::cout &lt;&lt; ""Done!\n"";<NewLine>	}<NewLine>	catch (std::exception e)<NewLine>	{<NewLine>		std::cout &lt;&lt; ""exception"" &lt;&lt; e.what() &lt;&lt; std::endl;<NewLine>	}<NewLine>}<NewLine></code></pre><NewLine><p>Python output:<br/><NewLine>tensor([[-1.6270e+00, -7.8417e-02, -3.4403e-01, -1.5171e+00, -1.3259e+00,<br/><NewLine>-1.1877e+00, -2.0234e-01, -1.0677e+00,  8.8365e-01,  7.2514e-01,<br/><NewLine>2.3642e+00, -1.4473e+00, -1.6696e+00, -1.2191e+00,  6.7770e-01,<br/><NewLine>-3.0230e-01, -1.5904e+00,  1.7433e+00, -7.8862e-01,  3.9448e-01,<br/><NewLine>-1.7189e+00,  1.1014e+00, -2.2981e+00, -5.1542e-01, -1.1593e-01,<br/><NewLine>6.5024e-01, -6.8557e-01, -7.0064e-01, -1.0784e+00, -7.7883e-01,<br/><NewLine>1.3773e+00, -1.5619e+00, -2.0540e-01,  1.2147e+00,  7.3867e-01,<br/><NewLine>1.1110e+00,  1.0524e-01, -1.1249e+00, -5.0620e-01, -5.2198e-01,<br/><NewLine>1.3556e+00, -1.5315e+00,  1.0446e-01,  9.1795e-01,  2.7186e+00,<br/><NewLine>-6.9594e-01,  7.4122e-01,  1.4757e+00,  1.2925e-01, -2.6900e-01,<br/><NewLine>1.5588e+00, -1.0609e+00, -2.0121e-01, -6.8162e-01,  1.1572e-01,<br/><NewLine>-1.7430e-01, -1.4399e+00,  1.4873e+00,  1.1772e+00,  8.0879e-01,<br/><NewLine>-1.3121e-01, -2.0003e+00, -7.4500e-02, -4.1007e-01, -1.2315e+00,<br/><NewLine>-1.1150e+00, -2.1979e+00, -1.2252e+00, -1.5357e+00,  2.3477e+00,<br/><NewLine>-1.9694e+00,  1.8873e+00,  3.2776e-01, -7.6457e-01, -1.7912e+00,<br/><NewLine>5.7192e-01, -2.5461e-01, -6.7235e-01, -3.1392e+00, -8.8816e-01,<br/><NewLine>-6.2070e-01, -7.2750e-01,  2.4999e-01,  1.1434e+00,  1.0114e+00,<br/><NewLine>3.4786e-01,  9.9722e-01, -4.8731e-01, -5.6572e-01,  1.2642e+00,<br/><NewLine>-4.4803e-01, -1.4394e+00, -1.8629e-01,  5.3590e-01,  1.4678e+00,<br/><NewLine>8.5147e-02, -2.0793e+00, -2.8566e-01,  2.9678e-01, -3.4123e-01,<br/><NewLine>3.1120e-01,  7.2252e-01,  2.7816e+00,  1.0683e+00, -3.1785e+00,<br/><NewLine>-6.7824e-01, -1.7665e-02,  5.2761e-01,  1.1141e-01, -1.6249e+00,<br/><NewLine>-2.0966e+00,  1.2752e+00, -8.8363e-01, -1.9442e+00,  1.5579e+00,<br/><NewLine>5.6738e-01, -3.4520e-01,  9.1841e-01,  7.5063e-02, -1.6585e+00,<br/><NewLine>2.5177e-01, -1.3581e+00,  3.4045e-01,  1.2807e+00, -3.7098e-01,<br/><NewLine>5.8744e-01,  9.2038e-01, -4.1502e-01, -1.4006e+00,  1.3954e+00,<br/><NewLine>-1.1765e+00,  1.3100e+00,  2.1202e+00,  3.0595e+00,  1.7250e-01,<br/><NewLine>-5.0746e-01, -1.1361e+00,  1.3437e+00, -8.2815e-02, -1.0477e+00,<br/><NewLine>8.5581e-01,  2.4402e+00,  1.6616e+00, -1.9156e+00,  4.2771e-01,<br/><NewLine>1.7761e+00,  1.5104e-01, -2.7037e-01, -6.1427e-02, -1.0483e+00,<br/><NewLine>-2.2830e-01,  3.9742e-01, -6.7260e-01,  2.4361e+00, -7.6196e-01,<br/><NewLine>1.0965e+00,  1.4753e+00,  8.5338e-01,  4.5726e-01, -1.8667e-01,<br/><NewLine>-1.1761e+00, -8.8821e-02,  1.3202e-01,  1.5002e+00, -4.9365e-01,<br/><NewLine>-1.0977e+00, -2.9104e-02, -3.5381e-01, -2.2095e-01,  9.3996e-01,<br/><NewLine>-1.0770e+00,  9.3767e-01,  2.2430e+00, -7.1536e-01, -7.0468e-01,<br/><NewLine>-2.1124e+00, -2.7435e+00,  1.7995e+00,  4.1688e-01,  4.2249e-01,<br/><NewLine>1.1487e-01, -1.1160e-01,  2.0495e+00, -1.6678e+00, -2.2310e+00,<br/><NewLine>3.1619e-01, -1.0459e-01, -5.3289e-01, -3.8420e-01, -1.3272e+00,<br/><NewLine>-4.5785e-01, -1.3917e+00,  1.3051e-01, -1.6694e+00,  2.3753e+00,<br/><NewLine>7.4885e-01,  2.2261e+00,  3.5489e-01,  2.2460e+00, -7.0667e-01,<br/><NewLine>-3.1920e-01,  2.7467e-01, -1.4723e-01,  2.2449e-01,  3.0860e-01,<br/><NewLine>-5.6551e-01,  1.3486e+00, -1.0313e+00, -1.8844e-01, -5.4212e-01,<br/><NewLine>-8.9150e-01,  2.1663e-01, -2.3341e-02,  5.4041e-01, -2.8048e-01,<br/><NewLine>-8.5421e-01, -1.3455e+00, -5.4566e-03,  3.3249e-01,  3.2633e-02,<br/><NewLine>-7.2821e-01, -2.1179e+00, -4.3671e-01,  1.6922e-01, -1.5222e+00,<br/><NewLine>-8.1076e-01, -4.5145e-01,  1.0031e+00,  3.8981e-01, -7.5108e-01,<br/><NewLine>1.2772e+00,  1.0216e+00, -8.8832e-02,  7.2678e-01,  2.3863e-01,<br/><NewLine>-7.2614e-01, -9.3102e-01,  1.0179e-01, -3.1820e-01,  1.7549e+00,<br/><NewLine>2.4568e-02, -2.4448e-01,  6.6527e-01,  8.9161e-01,  2.4075e-01,<br/><NewLine>7.7993e-01, -2.9786e-01,  3.7189e-01, -1.8534e+00,  1.2161e+00,<br/><NewLine>-1.4340e-01, -8.4045e-01, -1.7490e-02, -6.3605e-02, -2.6961e-01,<br/><NewLine>-6.0356e-02,  1.6479e-02,  8.4313e-02,  1.2867e+00, -1.8166e+00,<br/><NewLine>-4.4236e-01,  1.9492e+00,  7.5414e-02, -1.1048e+00,  3.2055e-01,<br/><NewLine>1.6554e+00,  1.6603e+00,  5.2739e-01, -8.8670e-02, -3.8753e-01,<br/><NewLine>1.1036e+00, -8.2550e-02,  1.5303e+00,  7.2115e-01,  6.3496e-01,<br/><NewLine>-5.9476e-01, -1.7111e+00, -7.4406e-02,  1.2575e+00,  1.0652e+00,<br/><NewLine>3.3742e-01, -6.1574e-01, -7.7878e-01, -1.5626e+00,  2.0075e+00,<br/><NewLine>7.8007e-01,  2.3359e+00, -5.8407e-01, -3.6670e-02, -1.8357e+00,<br/><NewLine>-8.5492e-01, -7.9237e-02, -3.4835e+00,  1.8853e-01, -6.3243e-01,<br/><NewLine>-1.4143e-01, -1.5573e+00,  1.3054e+00,  7.2289e-02, -3.3197e-01,<br/><NewLine>-4.2815e-01, -9.9560e-01,  4.8308e-02, -1.0704e+00,  4.6133e-02,<br/><NewLine>-2.7710e-01,  6.3607e-01, -1.2849e-01, -5.8321e-01, -6.4198e-01,<br/><NewLine>6.8877e-01,  4.4855e-01, -9.9281e-01, -1.9603e-01, -1.3646e-01,<br/><NewLine>-1.5132e+00, -1.8551e+00,  2.9994e+00,  1.9747e+00, -8.8294e-01,<br/><NewLine>1.0297e+00,  5.4850e-01,  2.2204e+00, -1.9871e-02,  1.6224e+00,<br/><NewLine>-1.3714e+00, -1.9999e-01, -1.8371e-01,  9.8869e-01,  1.7765e+00,<br/><NewLine>2.1239e+00,  1.6547e-01, -3.8542e-01,  1.1274e+00, -3.9524e+00,<br/><NewLine>-1.8184e-01, -9.8598e-01, -1.2485e-01, -7.8307e-01,  1.5246e+00,<br/><NewLine>-2.3675e-01,  7.5133e-01, -1.8204e+00,  1.1964e+00,  6.9412e-01,<br/><NewLine>-3.4246e+00, -6.2488e-01, -2.0008e-01, -1.4634e-01,  3.6126e-01,<br/><NewLine>-6.2960e-01,  1.2811e+00, -2.0820e-01, -2.6770e-01,  1.0875e+00,<br/><NewLine>-1.8656e+00, -1.7223e+00, -1.6199e+00, -1.6023e+00,  1.1000e-03,<br/><NewLine>5.5017e-01,  1.9496e+00,  7.6847e-01, -1.2796e+00,  2.4125e+00,<br/><NewLine>-1.0207e+00,  1.4682e+00,  6.9706e-04, -3.1195e-01,  8.4523e-01,<br/><NewLine>1.1639e+00,  1.0964e+00,  8.0490e-01,  3.7047e-01,  4.5071e-01,<br/><NewLine>1.0288e+00, -1.0690e+00, -1.0394e+00, -6.6745e-01, -2.9959e-01,<br/><NewLine>1.2548e+00, -1.3682e+00, -1.3584e+00, -1.2101e+00, -9.2314e-01,<br/><NewLine>-1.6717e+00,  1.9204e-01, -5.1889e-01,  6.6319e-01, -3.5625e-02,<br/><NewLine>3.5143e+00,  7.8116e-01, -8.7697e-01, -3.8530e-01,  2.0860e+00,<br/><NewLine>-1.5915e+00, -8.9022e-01, -5.0295e-01, -1.2801e+00,  1.8433e-01,<br/><NewLine>-6.9138e-01,  7.6171e-01,  2.1874e-01, -9.5043e-01,  1.3584e+00,<br/><NewLine>-1.0811e+00,  3.7449e-01,  1.4505e+00,  1.4932e+00, -1.0532e+00,<br/><NewLine>-3.7828e-01,  1.7716e+00,  1.8390e-01, -1.4419e+00,  1.0288e+00,<br/><NewLine>-1.6216e-01, -1.9189e+00, -1.0210e+00,  7.4068e-01,  7.0265e-01,<br/><NewLine>1.6574e+00,  3.3080e-01, -2.9631e+00,  1.9505e-01, -2.5233e-01,<br/><NewLine>-2.0795e+00, -1.4711e+00, -1.9923e+00,  3.1158e+00,  2.3007e+00,<br/><NewLine>-1.4851e+00, -1.3739e+00, -3.8031e-01,  1.3879e+00,  6.2704e-01,<br/><NewLine>4.0849e-01,  5.2626e-01, -5.3517e-01,  6.4794e-01,  1.3874e+00,<br/><NewLine>1.1729e+00, -6.2420e-02,  1.6669e-01,  3.7647e-02, -1.8886e+00,<br/><NewLine>7.9953e-01,  9.9094e-02,  3.3523e-01,  6.6596e-01, -2.0243e+00,<br/><NewLine>6.9878e-01,  1.0356e+00,  4.0730e-01, -4.5905e-01,  2.0120e+00,<br/><NewLine>-5.4535e-02, -1.4968e+00,  1.5344e-01, -2.9665e-01,  3.0098e-01,<br/><NewLine>5.8679e-01,  2.0437e-01, -1.8587e+00,  6.7893e-02,  7.3112e-01,<br/><NewLine>3.5927e-01,  1.2785e+00,  4.0530e-01,  8.8397e-01,  1.0595e+00,<br/><NewLine>-6.2867e-01,  9.6102e-01, -1.6319e+00,  3.6489e-01, -4.1222e-01,<br/><NewLine>1.8157e+00, -2.3874e+00, -2.0938e+00, -5.5133e-01,  1.8377e+00,<br/><NewLine>-1.0041e+00,  7.4509e-02,  1.0751e+00,  1.6144e+00, -7.9048e-01,<br/><NewLine>-8.2033e-01, -3.3595e+00,  1.1192e+00, -3.6376e-01, -5.9706e-02,<br/><NewLine>-1.5762e+00, -7.6090e-01, -5.4732e-01, -2.5771e-01, -5.6112e-02,<br/><NewLine>-8.0445e-01, -1.9105e+00,  4.5630e-01,  2.2545e+00, -1.7567e+00,<br/><NewLine>-1.3612e+00,  1.2470e+00,  3.2429e-01,  1.2829e+00,  2.1712e+00,<br/><NewLine>1.6078e+00,  1.1831e+00,  7.4726e-02,  3.6741e-01, -6.8770e-01,<br/><NewLine>-7.1650e-01,  1.7661e-01]], device=‘cuda:0’,<br/><NewLine>grad_fn=)</p><NewLine><p>C++ output<br/><NewLine>Embds: Columns 1 to 8 -84.6285  -14.7203   17.7419   47.0915   31.8170   57.6813    3.6089  -38.0543</p><NewLine><p>Columns 9 to 16   3.3444  -95.5730   90.3788  -10.8355    2.8831  -14.3861    0.8706  -60.7844</p><NewLine><p>Columns 17 to 24  30.0367  -43.1165   -5.6550   33.2033   -1.1758  105.3884   -9.8710   17.8346</p><NewLine><p>Columns 25 to 32  17.0933   66.6854  119.4765   79.3748   30.2875  -77.4174    0.3317   -4.0767</p><NewLine><p>Columns 33 to 40  -2.8686  -30.3538  -51.4344  -54.1199  -94.5696  -33.0847  -19.5770   54.3094</p><NewLine><p>Columns 41 to 48   9.1542    1.8090   84.0233  -34.8189   79.6485  109.4215   10.2912  -47.0976</p><NewLine><p>Columns 49 to 56  37.7219  -15.3790  -16.3427   22.2094 -110.2703  -47.8214  -40.3721   49.5144</p><NewLine><p>Columns 57 to 64   7.0735  -69.1642  -87.2891    2.4904 -114.2314  -34.6742   77.0583   47.5493</p><NewLine><p>Columns 65 to 72 -12.7955  -12.1884  -70.9220   61.2372  -23.0823  -14.9402   13.1899   77.5274</p><NewLine><p>Columns 73 to 80  14.8980    3.9681  -12.4636   -2.8313  -26.5012   18.7349  -81.2809   27.7805</p><NewLine><p>Columns 81 to 88   4.6502  -18.6308  -65.8188   -7.8959  -84.8021   18.9902   55.9421   -3.1461</p><NewLine><p>Columns 89 to 96 -68.0309 -121.0718  -39.6810   79.0844   44.7410    5.4263  -55.5766  -46.9981</p><NewLine><p>Columns 97 to 104 107.5576  -64.8779  -38.2952   27.7137   -3.9070   27.3118   -6.6422  -13.3164</p><NewLine><p>Columns 105 to 112 104.2085    0.5082  -78.4771  -19.8312  -38.7756  -52.0113   55.9654  -14.9233</p><NewLine><p>Columns 113 to 120  -9.7707   52.0167  -44.6636  -98.1208    4.3471   72.7285    1.8963  -15.4767</p><NewLine><p>Columns 121 to 128 -15.4205  -42.2256  170.4943  -79.3618   -1.6385   11.5500   59.1987  -65.9982</p><NewLine><p>Columns 129 to 136  -9.0985   33.3904   98.2815  -74.2509   11.8020  -89.1567   34.4861   43.4928</p><NewLine><p>Columns 137 to 144 -56.4307   11.7731  -16.7437   31.0511  -46.6434  -20.9232   26.8300    3.2606</p><NewLine><p>Columns 145 to 152  61.6599  -21.9810  -70.2742  -15.0909  -41.5298  -30.9954  -76.2638    0.6642</p><NewLine><p>Columns 153 to 160   2.6916   47.7454   26.7200   21.0140  -44.8855   -6.4925  -65.3175  -45.4141</p><NewLine><p>Columns 161 to 168 -17.8177  -31.5315  -32.9688   11.2705  -58.3355  -83.6264  -56.9800  -41.5826</p><NewLine><p>Columns 169 to 176  14.9421  -66.3415  -19.4020   -8.9205   34.7736   -1.2142  -22.5419   40.3070</p><NewLine><p>Columns 177 to 184  51.2629   37.0988  -84.1648  112.5778  -51.5290   56.4389  -17.4903   42.5482</p><NewLine><p>Columns 185 to 192  57.6678  -29.1431   63.6813   17.9877  -59.6995   31.1782  -43.9503   42.7553</p><NewLine><p>Columns 193 to 200  29.6934  -19.0927  -74.4936  -90.7978  -75.4938   41.4866    9.0591   52.9187</p><NewLine><p>Columns 201 to 208 -89.2584  -50.5271  -46.8471  -67.3429   -1.2110   21.3874   86.3426  -33.9398</p><NewLine><p>Columns 209 to 216  46.3358   17.8981 -100.1674  -50.8498  -55.5474  -42.1486    2.6009   79.9036</p><NewLine><p>Columns 217 to 224  73.3729   41.6763  -82.8588   -2.8996   17.4613 -166.8535   68.3080   42.2190</p><NewLine><p>Columns 225 to 232 -75.3225  -27.0393   40.7027  133.1041  -10.1574   85.9142  -17.5571  -11.0445</p><NewLine><p>Columns 233 to 240 -46.6592   36.1900  -25.5837   23.5690  111.7863  116.6611   -3.4232  -14.3296</p><NewLine><p>Columns 241 to 248 -10.1717  -26.3160  110.0413  -74.1527   66.8889   54.4394   -8.4007  -80.9817</p><NewLine><p>Columns 249 to 256 -52.5828    0.9547  -78.9718   19.8881   68.5607    4.6896   82.5919   11.0848</p><NewLine><p>Columns 257 to 264 -48.9090   49.7747  -90.9747  -22.6597   82.9919  -31.0079   33.3777  -80.8728</p><NewLine><p>Columns 265 to 272  20.9312   24.9726   58.8175  -57.3928  -36.9511   41.7683  -22.7457   18.0902</p><NewLine><p>Columns 273 to 280  33.3806   12.2698  -48.8019  -64.5811  -22.4971   13.0827   25.2252  -69.3366</p><NewLine><p>Columns 281 to 288 -31.1383    9.3472  -41.4773  -45.0921  -29.0197   20.8469  -18.5003  101.1813</p><NewLine><p>Columns 289 to 296  21.4998  -41.0139   13.0072   14.5900   47.8082    8.7939   -1.6898  -65.2906</p><NewLine><p>Columns 297 to 304  98.5455  -36.5257  -13.4876   31.5104   67.0052   20.0974   80.6973  -59.4268</p><NewLine><p>Columns 305 to 312  -9.8725  109.9801  -11.7113   76.0156   19.4814  -54.8399  -58.3198  -22.0197</p><NewLine><p>Columns 313 to 320 -11.4874  -40.5763  -90.6195   61.3063    2.9030  -38.8599   49.8093   63.7094</p><NewLine><p>Columns 321 to 328 -57.7285   41.2222   35.4600   21.2505   29.7755   40.5168  -36.1677  -35.7411</p><NewLine><p>Columns 329 to 336  55.7660   46.6989   56.3559 -109.1042  -56.7988  -16.9920   32.8174   50.5294</p><NewLine><p>Columns 337 to 344  13.8572   92.8637   59.6933   -0.8193  -69.0457   14.8087   20.9237   29.3850</p><NewLine><p>Columns 345 to 352 -59.0192  -19.3695  -47.4750    1.2323  -18.9492  -63.6595   46.3948    1.5139</p><NewLine><p>Columns 353 to 360  80.1003 -116.6856   18.4157   43.6484   14.6691  -26.1271  -60.0532   10.0214</p><NewLine><p>Columns 361 to 368 -17.5375   11.3292   -6.1891   -2.1459  -24.8204    0.0574  147.1159   56.4644</p><NewLine><p>Columns 369 to 376  20.6844   99.9769   -2.2026   45.3141   -5.9111   22.8332  -26.9914  -54.8931</p><NewLine><p>Columns 377 to 384  13.0211  -22.7115  -55.9605 -102.6626  -41.1080   37.0626   64.1098  -87.8013</p><NewLine><p>Columns 385 to 392  -4.5324  116.3614  -13.5869   29.3998  -29.8993  -19.1788   89.5348   33.3830</p><NewLine><p>Columns 393 to 400  47.5617  -47.8952 -115.5733   18.6636   70.4700   38.7836   52.9221  -26.4590</p><NewLine><p>Columns 401 to 408  57.7344  -46.9924 -107.3308 -104.5425   93.0818  -38.1794   28.5326   63.8123</p><NewLine><p>Columns 409 to 416 -21.0296  -53.7937   46.5247   10.2387  -12.8996   85.9877   53.1290   48.6895</p><NewLine><p>Columns 417 to 424 -66.8464   -2.3867   22.6467    7.4483   21.0441  -94.1917  -42.1939   15.9525</p><NewLine><p>Columns 425 to 432  53.8263  113.8375   61.6334 -104.5839  -20.7676   78.8139  -22.6948 -127.5196</p><NewLine><p>Columns 433 to 440  26.8981   20.7751   38.6938    0.1248  -14.7045  -67.0021  -51.5681   -8.1669</p><NewLine><p>Columns 441 to 448  19.7874  -48.3975  -32.2947   81.1478   48.5060  -85.6838  -17.2948    4.0231</p><NewLine><p>Columns 449 to 456  17.8500  173.0746   -8.2571   20.8623   -7.1263   78.6013   18.4043    6.9401</p><NewLine><p>Columns 457 to 464 -55.3688   28.4737   21.1565  142.7567  -89.0954  -30.7984   62.5072   26.2824</p><NewLine><p>Columns 465 to 472 -40.7608  -53.0610  -23.0218    2.4569   58.6491  -60.6084   15.7515  -54.9259</p><NewLine><p>Columns 473 to 480 -44.9702   -8.3017  -71.4793  -84.7397 -114.3832  -15.3010   54.4510  -32.4508</p><NewLine><p>Columns 481 to 488  75.7713   22.8518  -35.4634  -48.0759  -31.5085   -8.1592    6.5577  -23.7090</p><NewLine><p>Columns 489 to 496  -0.2302  -68.3007   26.5670  -28.0143  -21.5935  -55.7180   -5.6677   56.4317</p><NewLine><p>Columns 497 to 504  61.9337    9.6666  -12.2558  -60.3430  -30.2482   31.4843   71.7933   -8.8972</p><NewLine><p>Columns 505 to 512  36.8830  -31.1061   51.6818    8.2866    1.7214   -2.9263  -37.4330   48.5854<br/><NewLine>[ CPUFloatType{1,512} ]</p><NewLine><p>Using<br/><NewLine>Pytorch 1.6.0<br/><NewLine>Libtorch 1.6.0<br/><NewLine>Windows 10<br/><NewLine>Cuda 10.1</p><NewLine></div>",https://discuss.pytorch.org/u/Arki99,(Raveesh Nandakumar),Arki99,"August 20, 2020,  9:13am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>instead of using</p><NewLine><pre><code class=""lang-auto"">tensor_image[0][0] = tensor_image[0][0].sub(0.5).div(0.5);<NewLine>		tensor_image[0][1] = tensor_image[0][1].sub(0.5).div(0.5);<NewLine>		tensor_image[0][2] = tensor_image[0][2].sub(0.5).div(0.5);<NewLine></code></pre><NewLine><p>Just try <code>tensor_image = tensor_image.sub(0.5).div(0.5)</code></p><NewLine><p>Also, it’s better to monitor the output tensor’s <code>sum</code> or <code>mean</code> rather than all the values in the output tensor.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I tried that but I’m still facing the same issue.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/chetan_patil; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Arki99; <NewLine> ,"REPLY_DATE 1: August 20, 2020, 10:08am; <NewLine> REPLY_DATE 2: August 20, 2020, 10:13am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
92709,PyTorch is officially binded with Rust (tch-rs),2020-08-13T16:20:31.324Z,1,210,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The rust community has grown so much over the years, a library ‘tch-rs’ (<a href=""https://docs.rs/tch/0.2.0/tch/"" rel=""nofollow noopener"">https://docs.rs/tch/0.2.0/tch/</a>) has had a successful build (<a href=""https://travis-ci.org/LaurentMazare/tch-rs"" rel=""nofollow noopener"">https://travis-ci.org/LaurentMazare/tch-rs</a>) to bind C++ API of PyTorch.</p><NewLine><p>I am posting this to have the PyTorch community expand and self aware if anyone else has experience with rust and open to hear if anyone has tried to deploy PyTorch algorithms with tch-rs.</p><NewLine><aside class=""onebox allowlistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""16""/><NewLine><a href=""https://github.com/LaurentMazare/tch-rs"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars0.githubusercontent.com/u/1041292?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/LaurentMazare/tch-rs"" rel=""nofollow noopener"" target=""_blank"">LaurentMazare/tch-rs</a></h3><NewLine><p>Rust bindings for the C++ api of PyTorch. Contribute to LaurentMazare/tch-rs development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>",https://discuss.pytorch.org/u/rchavezj,(Rchavezj),rchavezj,"August 13, 2020,  4:20pm",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>what is the benefit for binding with rust language ?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Right now Libtorch C++ API deployment is somewhat difficult unless you have an open source library/software (i.e. torchserve) than can wrap serialize model’s into production instead of python web frameworks (flask/django/fastapi). Torchserve is nice for it has an RFC for a high performance Cpp PyTorch serving platform</p><NewLine><p>If you look at the techempower benchmarks, you see in the top 10 we have 4/10 libraries made in rust. This site is used to measure who has the fastest request done for web frameworks.<br/><NewLine></p><aside class=""onebox allowlistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://www.techempower.com/benchmarks/../images/icon.png"" width=""16""/><NewLine><a href=""https://www.techempower.com/benchmarks/"" rel=""nofollow noopener"" target=""_blank"">www.techempower.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""aspect-image"" style=""--aspect-ratio:529/500;""><img class=""thumbnail"" height=""500"" src=""https://www.techempower.com/benchmarks/img/filter-panel.png"" width=""529""/></div><NewLine><h3><a href=""https://www.techempower.com/benchmarks/"" rel=""nofollow noopener"" target=""_blank"">TechEmpower Web Framework Performance Comparison</a></h3><NewLine><p>Performance comparison of a wide spectrum of web application frameworks and platforms using community-contributed test implementations.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Right now two popular web frameworks from rust, Rocket and Actix, are gaining traction in the web community. Once you finish creating your algorithms in pytorch using pytorch library, torchscript the model to be loaded up using tch-rs.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mingfeng.zhang; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/rchavezj; <NewLine> ,"REPLY_DATE 1: August 20, 2020,  5:59pm; <NewLine> REPLY_DATE 2: August 19, 2020,  3:41pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
93272,Freeze_module pass for 1.4.x version of PyTorch,2020-08-18T19:38:23.320Z,1,54,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I see that there is a freeze_module pass in 1.5.x and later in PyTorch.</p><NewLine><p>Is there a similar pass in 1.4.x version of PyTorch which inlines the graph and resolves the weights and biases in the PyTorch graph into constants?</p><NewLine><p>Does running <code>_jit_pass_lower_graph()</code>, and converting all the tensor parameters returned by this pass into constants in the graph have the same effect?</p><NewLine></div>",https://discuss.pytorch.org/u/vdantu,(Vamshidhar Dantu (dan21c)),vdantu,"August 18, 2020,  7:41pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The <code>freeze_module</code> pass was added to 1.5, so there isn’t anything 100% equivalent in 1.4 The <code>_jit_pass_lower_graph</code> does something similar but freezing goes beyond it to try to simplify the graph based on the changes, and to verify that the conversion to constants was a correct transform.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/zdevito"">@zdevito</a> :  Thanks for the reply. If I had to achieve something similar to <code>freeze_module</code> in PyTorch 1.4.x, is running <code>_jit_pass_lower_graph()</code> and converting the returned parameters into graph constants a good approach? Or is there a better mechanism to achieve this? Porting <code>freeze_module</code>  to 1.4.x seems cumbersome as it relies on AliasDB APIs which don’t exist in 1.4.x.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zdevito; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vdantu; <NewLine> ,"REPLY_DATE 1: August 18, 2020,  8:34pm; <NewLine> REPLY_DATE 2: August 18, 2020,  8:44pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
93249,Converting torchscript models to version 1 instead of version 3?,2020-08-18T15:59:18.949Z,0,37,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Amazon Elastic Inference supports PyTorch and currently uses a modified version of 1.3.1. However, the JIT it uses seems to be a really old version as it says it requires version 1 torchscript models. The models that I’m able to create locally are version 3, and it seems to be that way no matter what version of PyTorch I use. When trying to script() or trace() the models on AWS, it seems to not actually work.</p><NewLine><p>Ideally, I’d be able to convert the models I have locally to a version 1 torchscript models ao I could copy them to my AWS server and run inference with them. Does anybody know how to do that?</p><NewLine></div>",https://discuss.pytorch.org/u/joeyballentine,(Joey B),joeyballentine,"August 18, 2020,  3:59pm",,,,,
92551,Converting a tensor to a Python number for ONNX convertation,2020-08-12T11:58:14.018Z,7,91,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to convert lstm-decoder with beam search to onnx, but I got some TracerWarnings. Is it possible to operate with int-number item from 1-size input tensor in my future code like this?</p><NewLine><pre><code class=""lang-python"">def forward(self, img_features, beam_size):<NewLine>    '''<NewLine>    Arguments:<NewLine>        img_features (tensor): Extracted features from the encoder module. (batch_size, feature_pixels = 7x7, encoder_dim = 2048)<NewLine>        beam_size (tensor): Number of top candidates to consider for beam search. (int, = 3 or =6 or =9)<NewLine>    '''<NewLine>    item = beam_size.item()<NewLine>    #some code...<NewLine>    output = torch.tensor([item])<NewLine>    return output<NewLine></code></pre><NewLine><p>Now I have this warnings and constant output of Onnx-model:</p><NewLine><pre><code class=""lang-auto"">TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<NewLine>  item = beam_size.item()<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.<NewLine>  output = torch.tensor([item])<NewLine></code></pre><NewLine><p>I tried using <span class=""mention"">@torch.jit.script</span> but it didn’t help too</p><NewLine></div>",https://discuss.pytorch.org/u/PlaeryinBol,(Plaeryin Bol),PlaeryinBol,"August 12, 2020, 11:59am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you post your real use case, i.e. how the Python integer is used?<br/><NewLine>Based on your current code snippet you should be able to just use the tensor instead of converting it to an integer, but I assume it’s just an example and not your real use case.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok, it’s just a bit of my problems with Onnx-convertation of lstm beam-search, and converting a tensor to a Python number is the first of them. When I export this for Onnx, I got the constant output from the model with incorrect tracing. I found, that I should script it with torch.script, but those examples not easy to understand <img alt="":thinking:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/thinking.png?v=9"" title="":thinking:""/></p><NewLine><pre><code class=""lang-auto"">    def forward(self, img_features, beam_size):<NewLine>        """"""<NewLine>        Function to generate the caption for the corresponding encoded image using beam search to provide the most optimal caption <NewLine>        combination. <NewLine>        <NewLine>        Arguments:<NewLine>            img_features (tensor): Extracted features from the encoder module. (batch_size, feature_pixels = 7x7, encoder_dim = 2048)<NewLine>            beam_size (tensor): Number of top candidates to consider for beam search. (int = 3 or 6 or 9)<NewLine>            <NewLine>        Output:<NewLine>            sentence (tensor): ordered list of words of the final optimal caption (list)<NewLine><NewLine>        """"""<NewLine>        beam_size = beam_size.item()#!Converting a tensor to a Python number<NewLine>        <NewLine>        prev_words = torch.zeros(beam_size, 1).long()<NewLine>        <NewLine>        sentences = prev_words<NewLine>        <NewLine>        top_preds = torch.zeros(beam_size, 1)<NewLine>        <NewLine>        completed_sentences = []<NewLine>        completed_sentences_preds = []<NewLine>        <NewLine>        step = 1<NewLine>        h, c = self.get_init_lstm_state(img_features)<NewLine><NewLine>        while True:<NewLine>            embedding = self.embedding(prev_words).squeeze(1)<NewLine>            context = self.attention(img_features, h)[0]<NewLine>            gate = self.sigmoid(self.f_beta(h))<NewLine>            gated_context = gate * context<NewLine><NewLine>            lstm_input = torch.cat((embedding, gated_context), dim=1)<NewLine>            h, c = self.lstm(lstm_input, (h, c))<NewLine>            output = self.deep_output(h)<NewLine>            output = top_preds.expand_as(output) + output<NewLine><NewLine>            if step == 1:<NewLine>                top_preds, top_words = output[0].topk(beam_size, 0, True, True)<NewLine>            else:<NewLine>                top_preds, top_words = output.view(-1).topk(beam_size, 0, True, True)<NewLine>            prev_word_idxs = top_words / output.size(1)<NewLine>            next_word_idxs = top_words % output.size(1)<NewLine><NewLine>            sentences = torch.cat((sentences[prev_word_idxs], next_word_idxs.unsqueeze(1)), dim=1)<NewLine><NewLine>            incomplete = [idx for idx, next_word in enumerate(next_word_idxs) if next_word != 1]#!Converting a tensor to a Python boolean<NewLine>            complete = list(set(range(len(next_word_idxs))) - set(incomplete))#!Converting a tensor to a Python index<NewLine><NewLine>            if len(complete) &gt; 0:<NewLine>                completed_sentences.extend(sentences[complete].tolist())#!Converting a tensor to a Python list<NewLine>                completed_sentences_preds.extend(top_preds[complete])#!Converting a tensor to a Python index<NewLine>            beam_size -= len(complete)<NewLine><NewLine>            if beam_size == 0:<NewLine>                break<NewLine>            sentences = sentences[incomplete]<NewLine>            h = h[prev_word_idxs[incomplete]]<NewLine>            c = c[prev_word_idxs[incomplete]]<NewLine>            img_features = img_features[prev_word_idxs[incomplete]]<NewLine>            top_preds = top_preds[incomplete].unsqueeze(1)<NewLine>            prev_words = next_word_idxs[incomplete].unsqueeze(1)<NewLine><NewLine>            if step &gt; 50:<NewLine>                break<NewLine>            step += 1<NewLine><NewLine>        idx = completed_sentences_preds.index(max(completed_sentences_preds))#!Converting a tensor to a Python boolean<NewLine>        sentence = completed_sentences[idx]<NewLine><NewLine>        sentence = torch.tensor(sentence)#!torch.tensor results are registered as constants<NewLine>        return sentence<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is scripting working for you now or are you still facing some issues?<br/><NewLine>I’m not an expert in using ONNX, but could you try to use tensors instead of Python literals via e.g.:</p><NewLine><pre><code class=""lang-python"">beam_size = torch.tensor([5])<NewLine>prev_words = torch.zeros(beam_size, 1).long()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, your case is working, but in this line, for example:</p><NewLine><pre><code class=""lang-auto"">top_preds, top_words = output[0].topk(beam_size, 0, True, True)<NewLine></code></pre><NewLine><p>‘topk()’ first argument must be int, not Tensor, so I still need this option (converatation tensor to a single number).<br/><NewLine>I try to script it like this:</p><NewLine><pre><code class=""lang-auto"">        from torch import Tensor<NewLine>        @torch.jit.script<NewLine>        def get_item(x: Tensor):<NewLine>            item = x.item()<NewLine>            return item<NewLine>        item = get_item(beam_size)<NewLine></code></pre><NewLine><p>but got this error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: get_item() Expected a value of type 'Tensor' for argument 'x' but instead found type 'int'.<NewLine>Position: 0<NewLine>Value: 1<NewLine>Declaration: get_item(Tensor x) -&gt; (Scalar)<NewLine>Cast error details: Unable to cast Python instance of type &lt;class 'int'&gt; to C++ type 'at::Tensor'<NewLine></code></pre><NewLine><p>What do you think about possible solution for this problems?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>The last error message points towards an integer input, while a tensor is expected.<br/><NewLine>Did you accidentally pass <code>x.item()</code> to <code>get_item()</code>?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, I’m forgot to save changes in module <img alt="":smile_cat:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smile_cat.png?v=9"" title="":smile_cat:""/></p><NewLine><p>Now, when I try to export to Onnnx this model with above <code>get_item(beam_size)</code> scripted function, I got<br/><NewLine><code>RuntimeError: Tracer cannot set value trace for type Int. Supported types are tensor, tensor list, and tuple of tensors.</code></p><NewLine><p>Is it problem from Onnx-part, or am I doing something wrong with torch.script and it leads to errors?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not sure, but the standalone PyTorch script seems to work:</p><NewLine><pre><code class=""lang-python"">from torch import Tensor<NewLine>@torch.jit.script<NewLine>def get_item(x: Tensor):<NewLine>    item = x.item()<NewLine>    return item<NewLine><NewLine>beam_size = torch.randn(1)<NewLine>item = get_item(beam_size)<NewLine>print(get_item.graph)<NewLine>&gt; graph(%x.1 : Tensor):<NewLine>  %item.1 : Scalar = aten::item(%x.1) # &lt;ipython-input-109-1655baf12d25&gt;:4:11<NewLine>  return (%item.1)<NewLine>print(item)<NewLine>&gt; 0.18689797818660736<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, this is work in practice, but not in onnx-export(</p><NewLine><p>Anyway, thanks for your help, you are best <img alt="":kissing_heart:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/kissing_heart.png?v=9"" title="":kissing_heart:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/PlaeryinBol; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/PlaeryinBol; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/PlaeryinBol; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/PlaeryinBol; <NewLine> ,"REPLY_DATE 1: August 15, 2020,  6:21am; <NewLine> REPLY_DATE 2: August 17, 2020,  8:24am; <NewLine> REPLY_DATE 3: August 18, 2020,  4:40am; <NewLine> REPLY_DATE 4: August 18, 2020,  7:20am; <NewLine> REPLY_DATE 5: August 18, 2020,  7:25am; <NewLine> REPLY_DATE 6: August 18, 2020,  8:12am; <NewLine> REPLY_DATE 7: August 18, 2020,  8:17am; <NewLine> REPLY_DATE 8: August 18, 2020,  1:55pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> 
92735,Torch.jit.script(module) vs @torch.jit.script decorator,2020-08-13T19:23:23.476Z,0,84,"<div class=""post"" itemprop=""articleBody""><NewLine><p>What is the purpose of <span class=""mention"">@torch.jit.script</span> decorator? Why is adding the decorator “<span class=""mention"">@torch.jit.script</span>” results in an error, while I can call torch.jit.script on that module, e.g. this fails:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine>@torch.jit.script<NewLine>class MyCell(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super(MyCell, self).__init__()<NewLine>        self.linear = torch.nn.Linear(4, 4)<NewLine><NewLine>    def forward(self, x, h):<NewLine>        new_h = torch.tanh(self.linear(x) + h)<NewLine>        return new_h, new_h<NewLine><NewLine>my_cell = MyCell()<NewLine>x, h = torch.rand(3, 4), torch.rand(3, 4)<NewLine>traced_cell = torch.jit.script(my_cell, (x, h))<NewLine>print(traced_cell)<NewLine>traced_cell(x, h)<NewLine><NewLine>""C:\Users\Administrator\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\jit\__init__.py"", line 1262, in script<NewLine>    raise RuntimeError(""Type '{}' cannot be compiled since it inherits""<NewLine>RuntimeError: Type '&lt;class '__main__.MyCell'&gt;' cannot be compiled since it inherits from nn.Module, pass an instance instead<NewLine></code></pre><NewLine><p>While the following code works well:</p><NewLine><pre><code class=""lang-auto"">class MyCell(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super(MyCell, self).__init__()<NewLine>        self.linear = torch.nn.Linear(4, 4)<NewLine><NewLine>    def forward(self, x, h):<NewLine>        new_h = torch.tanh(self.linear(x) + h)<NewLine>        return new_h, new_h<NewLine><NewLine>my_cell = MyCell()<NewLine>x, h = torch.rand(3, 4), torch.rand(3, 4)<NewLine>traced_cell = torch.jit.script(my_cell, (x, h))<NewLine>print(traced_cell)<NewLine>traced_cell(x, h)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Vlad_A,(Vlad A),Vlad_A,"August 13, 2020,  7:23pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>@torch.jit.script</code> can be used as a decorator on functions to script them. So, these two code snippets are roughly equivalent:</p><NewLine><p><strong>Decorator</strong></p><NewLine><pre><code class=""lang-auto"">@torch.jit.script<NewLine>def fn(a: int):<NewLine>   return a + 1<NewLine><NewLine>fn(3) # fn here is a scripted function<NewLine></code></pre><NewLine><p><strong>Function Call</strong></p><NewLine><pre><code class=""lang-auto"">def fn(a: int):<NewLine>   return a + 1<NewLine><NewLine>s_fn = torch.jit.script(fn)<NewLine>s_fn(3) # s_fn here is a scripted function<NewLine></code></pre><NewLine><p>This decorator can also be used on classes that extend <code>object</code> to script them (known as <a href=""https://pytorch.org/docs/stable/jit.html#torchscript-classes"" rel=""nofollow noopener"">Torchscript classes</a>).</p><NewLine><p>Because only <em>instances</em> of <code>Modules</code> can be scripted, <code>@torch.jit.script</code> cannot be used as a decorator on a subclass of <code>Module</code>. You must create an instance of the <code>Module</code> and pass it to <code>torch.jit.script</code> in order to script it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SplitInfinity; <NewLine> ,"REPLY_DATE 1: August 13, 2020,  8:03pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
92674,Torch.jit.trace_module: how to capture modification of passed dict?,2020-08-13T11:11:27.348Z,0,51,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Fairseq Transformer models modify the passed incremental state dict without returning it (OTOH huggingface Transformers return it). ONNX export seems to drop these operations. Is it possible to still export these correctly?</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>from typing import Any, Dict, List, Optional<NewLine>from torch import Tensor<NewLine> <NewLine>class InplaceModule(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.embeddings = torch.nn.Linear(1, 10, bias=False)<NewLine>        <NewLine>    def forward(self, x, state: Dict[str, Dict[str, Tensor]]):<NewLine>        state['new_level_1'] = dict()<NewLine>        state['new_level_1']['new_level_2'] = x<NewLine>        return self.embeddings(x)<NewLine>    <NewLine>dummy_state = {'level_1': {'level_2': torch.tensor([[3]], dtype=torch.float32)}}<NewLine>dummy_state = torch.jit.annotate(Dict[str, Dict[str, Optional[torch.Tensor]]], dummy_state)<NewLine>dummy_input = torch.tensor([[3]], dtype=torch.float32)<NewLine>model = torch.jit.trace_module(InplaceModule(), dict(forward=(dummy_input, dummy_state,)))<NewLine>print(model.code)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">def forward(self,<NewLine>    input: Tensor,<NewLine>    argument_2: Dict[str, Dict[str, Tensor]]) -&gt; Tensor:<NewLine>  return (self.embeddings).forward(input, )<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/vadimkantorov,(Vadim Kantorov),vadimkantorov,"August 13, 2020,  1:16pm",,,,,
92504,Parameters of the quantized model will miss in state_dict after being traced in pytorch1.6. Is it a bug or feature?,2020-08-12T03:54:05.531Z,0,48,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>from torch.quantization import QuantStub, DeQuantStub<NewLine><NewLine>class QuantizableModel(nn.Module):<NewLine>    def __init__(self, *args, **kwargs):<NewLine>        super(QuantizableModel, self).__init__()<NewLine>        # self.module = ConvBNReLU(3, 64)<NewLine>        self.conv = nn.Conv2d(3, 2, 3, 1, 1, groups=1, bias=True)<NewLine>        self.quant = QuantStub()<NewLine>        self.dequant = DeQuantStub()<NewLine><NewLine>        # weight initialization<NewLine>        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out')<NewLine>        nn.init.zeros_(self.conv.bias)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine>        x = self.conv(x)<NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    model = QuantizableModel().eval()<NewLine>    inp = torch.randn(1, 3, 224, 224)<NewLine>    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine>    torch.quantization.prepare(model, inplace=True)<NewLine>    # # Dummy calibration<NewLine>    model(inp)<NewLine>    torch.quantization.convert(model, inplace=True)<NewLine>    print(""before traced: "", model.state_dict().keys())<NewLine>    traced_model = torch.jit.trace(model, inp).eval()<NewLine>    print(""after traced: "", traced_model.state_dict().keys())<NewLine></code></pre><NewLine><p>The output of running the above code in pytorch 1.5 is:</p><NewLine><pre><code class=""lang-auto"">before traced:  odict_keys(['conv.weight', 'conv.scale', 'conv.zero_point', 'conv.bias', 'quant.scale', 'quant.zero_point'])<NewLine>after traced:  odict_keys(['conv._packed_params', 'quant.scale', 'quant.zero_point'])<NewLine></code></pre><NewLine><p>The output of running the same code in pytorch 1.6 is:</p><NewLine><pre><code class=""lang-auto"">before traced:  odict_keys(['conv.weight', 'conv.bias', 'conv.scale', 'conv.zero_point', 'quant.scale', 'quant.zero_point'])<NewLine>after traced:  odict_keys(['quant.scale', 'quant.zero_point'])<NewLine></code></pre><NewLine><p>Parameters of the quantized model will miss in state_dict after being traced in pytorch1.6. Is it a bug or feature?</p><NewLine></div>",https://discuss.pytorch.org/u/Wenlong_Shi,(Wenlong Shi),Wenlong_Shi,"August 12, 2020,  2:53pm",,,,,
91848,FBGEMM &amp; Python Performance,2020-08-06T11:49:20.802Z,0,58,"<div class=""post"" itemprop=""articleBody""><NewLine><p>So I’m in the process of deploying a torch.jit traced model into an x86 environment. I’d ideally like to do this in Python, but I see the tutorials are in C++. Is it possible to unlock the full performance of the FBGEMM backend from Python, or should I really be using C++?</p><NewLine><p>A 20-30% performance hit is acceptable, but if C++ will be multiple times faster I’ll go with that.</p><NewLine></div>",https://discuss.pytorch.org/u/sbsky,,sbsky,"August 6, 2020, 11:49am",,,,,
91135,Missing ops in scripted function,2020-07-30T18:37:09.333Z,0,96,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,</p><NewLine><p>I’ve come across some interesting behavior regarding TorchScript and index ops. The following two functions do the same job. However, upon inspection of the TorchScript code, one can see that in the latter the JIT compiler completely removes an assignment operator: <code>data_sigma[data_sigma &lt; 1e-12].fill_(1.0)</code></p><NewLine><pre><code class=""lang-auto"">In [13]: @torch.jit.script<NewLine>    ...: def fit(data: Tensor):<NewLine>    ...:     # Reduce all but the last dimension<NewLine>    ...:     # pylint:disable=unnecessary-comprehension<NewLine>    ...:     reduction_dims = [i for i in range(data.dim() - 1)]<NewLine>    ...:     # pylint:enable=unnecessary-comprehension<NewLine>    ...:<NewLine>    ...:     data_mu = torch.mean(data, dim=reduction_dims, keepdim=True)<NewLine>    ...:     data_sigma = torch.std(data, dim=reduction_dims, keepdim=True)<NewLine>    ...:     data_sigma = torch.where(data_sigma &lt; 1e-12, torch.ones_like(data_sigma), data_sigma)<NewLine>    ...:     return data_mu, data_sigma<NewLine>    ...:<NewLine><NewLine>In [14]: print(fit.code)<NewLine>def fit(data: Tensor) -&gt; Tuple[Tensor, Tensor]:<NewLine>  reduction_dims = annotate(List[int], [])<NewLine>  for i in range(torch.sub(torch.dim(data), 1)):<NewLine>    _0 = torch.append(reduction_dims, i)<NewLine>  data_mu = torch.mean(data, reduction_dims, True, dtype=None)<NewLine>  data_sigma = torch.std(data, reduction_dims, True, True)<NewLine>  _1 = torch.lt(data_sigma, 9.9999999999999998e-13)<NewLine>  _2 = torch.ones_like(data_sigma, dtype=None, layout=None, device=None, pin_memory=None, memory_format=None)<NewLine>  data_sigma0 = torch.where(_1, _2, data_sigma)<NewLine>  return (data_mu, data_sigma0)<NewLine><NewLine><NewLine>In [15]: @torch.jit.script<NewLine>    ...: def fit(data: Tensor):<NewLine>    ...:     # Reduce all but the last dimension<NewLine>    ...:     # pylint:disable=unnecessary-comprehension<NewLine>    ...:     reduction_dims = [i for i in range(data.dim() - 1)]<NewLine>    ...:     # pylint:enable=unnecessary-comprehension<NewLine>    ...:<NewLine>    ...:     data_mu = torch.mean(data, dim=reduction_dims, keepdim=True)<NewLine>    ...:     data_sigma = torch.std(data, dim=reduction_dims, keepdim=True)<NewLine>    ...:     #data_sigma = torch.where(data_sigma &lt; 1e-12, torch.ones_like(data_sigma), data_sigma)<NewLine>    ...:     data_sigma[data_sigma &lt; 1e-12].fill_(1.0)<NewLine>    ...:     return data_mu, data_sigma<NewLine>    ...:<NewLine><NewLine>In [16]: print(fit.code)<NewLine>def fit(data: Tensor) -&gt; Tuple[Tensor, Tensor]:<NewLine>  reduction_dims = annotate(List[int], [])<NewLine>  for i in range(torch.sub(torch.dim(data), 1)):<NewLine>    _0 = torch.append(reduction_dims, i)<NewLine>  data_mu = torch.mean(data, reduction_dims, True, dtype=None)<NewLine>  data_sigma = torch.std(data, reduction_dims, True, True)<NewLine>  return (data_mu, data_sigma)<NewLine></code></pre><NewLine><p>Does anyone have a good explanation for this behavior? I’m worried now that the same may be happening in other parts of my code. Such assignments are important when avoiding numerical precision errors.</p><NewLine><p>Cheers,<br/><NewLine>Ângelo</p><NewLine></div>",https://discuss.pytorch.org/u/angelolovatto,(Ângelo Lovatto),angelolovatto,"July 30, 2020,  6:39pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, this is definitely concerning—do you mind filing a bug on Github and we can follow up there? Thanks!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually: I believe this is correct behavior. Indexing a tensor with another tensor produces a copy of the original tensor. So in the code:</p><NewLine><pre><code class=""lang-auto"">data_sigma[data_sigma &lt; 1e-12].fill_(1.0)<NewLine></code></pre><NewLine><p>you are performing <code>fill_</code> on a copy of <code>data_sigma</code>, then immediately throwing it away. The TorchScript compiler is correctly recognizing that this is work that you never use the result of, and thus we remove it as an optimization.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: July 30, 2020,  7:07pm; <NewLine> REPLY_DATE 2: August 29, 2020,  7:45pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
90739,Torch JIT trace problem,2020-07-27T23:38:10.669Z,0,114,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The code is as follows:</p><NewLine><pre><code class=""lang-auto"">class DemoTargetObjectFeatureProcessor(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super(DemoTargetObjectFeatureProcessor, self).__init__()<NewLine><NewLine>    def forward(self, target_object_inputs):<NewLine>        target_object_size = target_object_inputs[:, 0] * target_object_inputs[:, 1]<NewLine>        target_object_size = target_object_size.unsqueeze(-1)<NewLine>        return torch.cat((target_object_inputs, target_object_size), dim=1)<NewLine><NewLine>    @torch.jit.export<NewLine>    def forward_1(self, target_object_inputs_dict):<NewLine>        target_object_inputs = target_object_inputs_dict['a']<NewLine>        return self.forward(target_object_inputs)<NewLine><NewLine>target_object_inputs = torch.tensor([[1,1], [2,2]])<NewLine>target_object_inputs_dict = {<NewLine>    ""a"": torch.tensor([[1,1], [2,2]])<NewLine>}<NewLine><NewLine>fp = DemoTargetObjectFeatureProcessor()<NewLine>fp.forward(target_object_inputs)<NewLine>fp.forward_1(target_object_inputs_dict)<NewLine><NewLine>module = torch.jit.trace(fp, target_object_inputs)<NewLine>module = torch.jit.trace(fp.forward, target_object_inputs)<NewLine><NewLine>module = torch.jit.trace(fp.forward_1, target_object_inputs_dict)<NewLine></code></pre><NewLine><p>I’m trying to pass a named dictionary of tensors into the TorchScript module from “module = torch.jit.trace(fp.forward_1, target_object_inputs_dict)”, but I will get the error as follows:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""test_trace.py"", line 50, in &lt;module&gt;<NewLine>    module = torch.jit.trace(fp.forward_1, target_object_inputs_dict)<NewLine>  File ""/home/lei.chen/.pyenv/versions/3.6.6/lib/python3.6/site-packages/torch/jit/__init__.py"", line 893, in trace<NewLine>    raise AttributeError(""trace doesn't support compiling individual module's functions.\n""<NewLine>AttributeError: trace doesn't support compiling individual module's functions.<NewLine>Please use trace_module<NewLine></code></pre><NewLine><p>If i’m trying to use</p><NewLine><pre><code class=""lang-auto"">module = torch.jit.script(fp)<NewLine>module.forward_1(target_object_inputs_dict)<NewLine></code></pre><NewLine><p>I will get the follow error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError:<NewLine>Unsupported operation: indexing tensor with unsupported index type 'str'. Only ints, slices, and tensors are supported:<NewLine>  File ""test_trace.py"", line 36<NewLine>    @torch.jit.export<NewLine>    def forward_1(self, target_object_inputs_dict):<NewLine>        target_object_inputs = target_object_inputs_dict['a']<NewLine>                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        return self.forward(target_object_inputs)<NewLine></code></pre><NewLine><p>How can we pass a named dictionary of tensors into TorchScript? I want to run model inference in C++ backend using the exported TorchScript. From the PyTorch unit tests <a href=""https://github.com/pytorch/pytorch/blob/5136ed0e44c65cb3747a1f22f77ccf09d54c125c/test/cpp/api/jit.cpp#L73"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/5136ed0e44c65cb3747a1f22f77ccf09d54c125c/test/cpp/api/jit.cpp#L73</a>, it seems that we can pass a c10::Dict into TorchScript module.</p><NewLine></div>",https://discuss.pytorch.org/u/rayleichen,(Lei Chen),rayleichen,"July 27, 2020, 11:38pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Any TorchScript expert can help here? I also find this issue in PyTorch github page: <a href=""https://github.com/pytorch/pytorch/issues/16847"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/16847</a>. It’s claimed that “Closing as we support this in the tracer now.”, but many ppls experienced the error from dictionary as input tensors.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/rayleichen; <NewLine> ,"REPLY_DATE 1: July 31, 2020,  1:34am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
89568,How to convert darts to ONNX,2020-07-17T09:38:16.174Z,6,130,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi.<br/><NewLine>I want to convert <a href=""https://github.com/quark0/darts/tree/master/cnn"" rel=""nofollow noopener"">darts/cnn</a>’s model to TFlite, finaly.<br/><NewLine>First of all, I tried to convert it to ONNX by below code.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import genotypes<NewLine>from model import NetworkCIFAR as Network<NewLine><NewLine>genotype = eval(""genotypes.%s"" % 'DARTS')<NewLine>model = Network(36, 10, 20, True, genotype)<NewLine>model.load_state_dict(torch.load('./weights.pt'))<NewLine>model = model.cuda()<NewLine><NewLine>onnx_model_path = './darts_model.onnx'<NewLine>dummy_input = torch.randn(8,3,32,32)<NewLine>input_names = ['image_array']<NewLine>output_names = ['category']<NewLine>torch.onnx.export(model,dummy_input, onnx_model_path,<NewLine>                  input_names=input_names, output_names=output_names)<NewLine></code></pre><NewLine><p>However, it couldn’t convert.<br/><NewLine>Error is below.</p><NewLine><pre><code class=""lang-auto""> Traceback (most recent call last):<NewLine>  File ""&lt;stdin&gt;"", line 2, in &lt;module&gt;<NewLine>  File ""/home/XXXX/darts/cnn/eval-EXP-20200710-150423/venv/lib/python3.6/site-packages/torch/onnx/__init__.py"", line 168, in export<NewLine>    custom_opsets, enable_onnx_checker, use_external_data_format)<NewLine>  File ""/home/XXXX/darts/cnn/eval-EXP-20200710-150423/venv/lib/python3.6/site-packages/torch/onnx/utils.py"", line 69, in export<NewLine>    use_external_data_format=use_external_data_format)<NewLine>  File ""/home/XXXX/darts/cnn/eval-EXP-20200710-150423/venv/lib/python3.6/site-packages/torch/onnx/utils.py"", line 488, in _export<NewLine>    fixed_batch_size=fixed_batch_size)<NewLine>  File ""/home/XXXX/darts/cnn/eval-EXP-20200710-150423/venv/lib/python3.6/site-packages/torch/onnx/utils.py"", line 334, in _model_to_graph<NewLine>    graph, torch_out = _trace_and_get_graph_from_model(model, args, training)<NewLine>  File ""/home/XXXX/darts/cnn/eval-EXP-20200710-150423/venv/lib/python3.6/site-packages/torch/onnx/utils.py"", line 291, in _trace_and_get_graph_from_model<NewLine>    torch.jit._get_trace_graph(model, args, _force_outplace=False, _return_inputs_states=True)<NewLine>  File ""/home/XXXX/darts/cnn/eval-EXP-20200710-150423/venv/lib/python3.6/site-packages/torch/jit/__init__.py"", line 278, in _get_trace_graph<NewLine>    outs = ONNXTracedModule(f, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)<NewLine>  File ""/home/XXXX_darts/cnn/eval-EXP-20200710-150423/venv/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/XXXX/darts/cnn/eval-EXP-20200710-150423/venv/lib/python3.6/site-packages/torch/jit/__init__.py"", line 361, in forward<NewLine>    self._force_outplace,<NewLine>  File ""/home/XXXX/darts/cnn/eval-EXP-20200710-150423/venv/lib/python3.6/site-packages/torch/jit/__init__.py"", line 351, in wrapper<NewLine>    out_vars, _ = _flatten(outs)<NewLine>RuntimeError: Only tuples, lists and Variables supported as JIT inputs/outputs. Dictionaries and strings are also accepted but their usage is not recommended. But got unsupported type NoneType<NewLine></code></pre><NewLine><p>Does onnx.export not correspond to darts?<br/><NewLine>Could you please tell me how to fix it to convert to ONNX?</p><NewLine><p>Finally, I am asking this same question in darts’s issue, sorry.</p><NewLine><p>Thank you!</p><NewLine><p>My environment</p><NewLine><ul><NewLine><li>Ubuntu 16.04</li><NewLine><li>Python 3.6.10</li><NewLine><li>CUDA 9.0</li><NewLine><li>Pytorch 0.3.1(to search model), 1.5.1(to convert to ONNX)</li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/crook52,,crook52,"July 17, 2020,  9:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Based on the error message, it seems that an intermediate activation is <code>None</code> instead of a valid tensor.<br/><NewLine>Is your model working fine in PyTorch eager mode and JIT (without the ONNX export)?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply.<br/><NewLine>Is it correct to understand PyTorch eager mode as normal mode?<br/><NewLine>If so, the output of the model(input) is below. I think it is correct.</p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; out = model(dummy_input)<NewLine>&gt;&gt;&gt; out<NewLine>(tensor([[-0.0391, -0.0840,  0.1382, -0.0397,  0.0157, -0.0448, -0.0603, -0.0823,<NewLine>          0.0025, -0.0009],<NewLine>        [ 0.0257,  0.0031, -0.1880,  0.0768, -0.1047, -0.0392,  0.1393, -0.0419,<NewLine>         -0.0437,  0.0032],<NewLine>        [ 0.0090,  0.0097, -0.0768, -0.0383, -0.0220, -0.2048, -0.1315,  0.0117,<NewLine>         -0.0538, -0.0613],<NewLine>        [ 0.0438, -0.1284,  0.0325,  0.0441,  0.0736,  0.1941, -0.0407, -0.0634,<NewLine>          0.1074, -0.0407],<NewLine>        [ 0.0440,  0.0194,  0.0147,  0.0859,  0.2149, -0.0393,  0.1640,  0.0369,<NewLine>         -0.1021, -0.1820],<NewLine>        [-0.3142, -0.0726, -0.0694, -0.1064, -0.1595,  0.2461,  0.1174,  0.2102,<NewLine>          0.1790,  0.2188],<NewLine>        [-0.0522,  0.0327, -0.1626, -0.0955,  0.0625, -0.0061,  0.0662,  0.0667,<NewLine>          0.1003,  0.0635],<NewLine>        [ 0.1054, -0.0456,  0.0922,  0.0559,  0.1422, -0.1924, -0.2107, -0.0572,<NewLine>         -0.0424, -0.1007]], device='cuda:0', grad_fn=&lt;AddmmBackward&gt;), tensor([[-0.0779, -0.4483,  0.5459, -0.4263,  0.3033, -0.0147,  0.1823,  0.2561,<NewLine>          0.3321, -0.8131],<NewLine>        [-0.1185, -0.1932,  0.2465,  0.3930, -0.0634,  0.2440, -0.0587, -0.5931,<NewLine>          0.0938,  0.2163],<NewLine>        [ 0.0699, -0.2207,  0.5958, -0.0778, -0.1024, -0.1841,  0.5211,  0.0760,<NewLine>          0.2308,  0.1463],<NewLine>        [ 0.1858, -0.0432,  0.3188, -0.0905,  0.1415, -0.6925,  0.1487, -0.2300,<NewLine>          1.0883,  0.1186],<NewLine>        [-0.1471,  0.1120,  0.3354, -0.3918,  0.0748, -0.5318,  0.0106, -0.4543,<NewLine>          1.2513,  0.1778],<NewLine>        [ 0.4499,  0.0425,  0.3949, -0.8790, -0.1463, -0.4942, -0.4362, -0.3380,<NewLine>          0.3257,  0.2104],<NewLine>        [ 0.2962,  0.0098,  0.6569,  0.0520,  0.1627, -0.4044,  0.2104, -0.2278,<NewLine>          0.2411,  0.0337],<NewLine>        [ 0.0591, -0.0795,  0.9120, -0.5483, -0.2887, -0.2304, -0.3799, -0.5769,<NewLine>          0.5903, -0.6071]], device='cuda:0', grad_fn=&lt;AddmmBackward&gt;))<NewLine><NewLine></code></pre><NewLine><p>Regarding the JIT, I don’t know much about it, so could you please tell me how to check that?<br/><NewLine>Here’s the <a href=""https://github.com/quark0/darts/blob/master/cnn/model.py"" rel=""nofollow noopener"">code</a>, is it possible to convert to ONNX even if ‘foward’ has ‘if’ in it?</p><NewLine><pre><code class=""lang-auto"">class NetworkCIFAR(nn.Module):<NewLine><NewLine>  def __init__(self, C, num_classes, layers, auxiliary, genotype):<NewLine>    super(NetworkCIFAR, self).__init__()<NewLine>    self._layers = layers<NewLine>    self._auxiliary = auxiliary<NewLine><NewLine>    stem_multiplier = 3 #RGB<NewLine>    C_curr = stem_multiplier*C #C is input_channels<NewLine>    self.stem = nn.Sequential(<NewLine>      nn.Conv2d(3, C_curr, 3, padding=1, bias=False),<NewLine>      nn.BatchNorm2d(C_curr)<NewLine>    )<NewLine>    <NewLine>    C_prev_prev, C_prev, C_curr = C_curr, C_curr, C<NewLine>    self.cells = nn.ModuleList()<NewLine>    reduction_prev = False<NewLine>    for i in range(layers):<NewLine>      if i in [layers//3, 2*layers//3]:<NewLine>        C_curr *= 2<NewLine>        reduction = True<NewLine>      else:<NewLine>        reduction = False<NewLine>      cell = Cell(genotype, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)<NewLine>      reduction_prev = reduction<NewLine>      self.cells += [cell]<NewLine>      C_prev_prev, C_prev = C_prev, cell.multiplier*C_curr<NewLine>      if i == 2*layers//3:<NewLine>        C_to_auxiliary = C_prev<NewLine><NewLine>    if auxiliary:<NewLine>      self.auxiliary_head = AuxiliaryHeadCIFAR(C_to_auxiliary, num_classes)<NewLine>    self.global_pooling = nn.AdaptiveAvgPool2d(1)<NewLine>    self.classifier = nn.Linear(C_prev, num_classes)<NewLine><NewLine>  def forward(self, input):<NewLine>    logits_aux = None<NewLine>    s0 = s1 = self.stem(input)<NewLine>    for i, cell in enumerate(self.cells):<NewLine>      s0, s1 = s1, cell(s0, s1, self.drop_path_prob)<NewLine>      if i == 2*self._layers//3:<NewLine>        if self._auxiliary and self.training:<NewLine>          logits_aux = self.auxiliary_head(s1)<NewLine>    out = self.global_pooling(s1)<NewLine>    logits = self.classifier(out.view(out.size(0),-1))<NewLine>    return logits, logits_aux<NewLine></code></pre><NewLine><p>I’m sorry for all of the questions.<br/><NewLine>Thank you for your time.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""89568"" data-username=""crook52""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/c/d26b3c/40.png"" width=""20""/> crook52:</div><NewLine><blockquote><NewLine><p>Is it correct to understand PyTorch eager mode as normal mode?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, sorry for the unclear naming. By “eager” mode I meant the normal Python usage.</p><NewLine><p>Good to see the model is working generally.<br/><NewLine>Could you , for the sake of debugging, remove the <code>logits_aux</code> from the <code>forward</code> and just return the <code>logits</code> and retry to export the model?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>It looks like it fails in tracing, can you try <code>torch.jit.trace</code> and see if it work or not?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m sorry for my late reply.<br/><NewLine>After removing logits_aux  as you advised, it worked!!!<br/><NewLine>I cannot thank you enough!</p><NewLine><p>However, why it couldn’t convert with logits_aux??</p><NewLine><pre><code class=""lang-auto"">cclass AuxiliaryHeadCIFAR(nn.Module):<NewLine><NewLine>  def __init__(self, C, num_classes):<NewLine>    """"""assuming input size 8x8""""""<NewLine>    super(AuxiliaryHeadCIFAR, self).__init__()<NewLine>    self.features = nn.Sequential(<NewLine>      nn.ReLU(inplace=True),<NewLine>      nn.AvgPool2d(5, stride=3, padding=0, count_include_pad=False), # image size = 2 x 2<NewLine>      nn.Conv2d(C, 128, 1, bias=False),<NewLine>      nn.BatchNorm2d(128),<NewLine>      nn.ReLU(inplace=True),<NewLine>      nn.Conv2d(128, 768, 2, bias=False),<NewLine>      nn.BatchNorm2d(768),<NewLine>      nn.ReLU(inplace=True)<NewLine>    )<NewLine>    self.classifier = nn.Linear(768, num_classes)<NewLine><NewLine>  def forward(self, x):<NewLine>    x = self.features(x)<NewLine>    x = self.classifier(x.view(x.size(0),-1))<NewLine>    return x<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply, and sorry for my late reply.</p><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""89568"" data-username=""ebarsoum""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ebarsoum/40/5515_2.png"" width=""20""/> ebarsoum:</div><NewLine><blockquote><NewLine><p>It looks like it fails in tracing,</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, torch.jit.trace doesn’t work.</p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; torch.jit.trace(model,dummy_input)<NewLine>/home/XXXX/darts/cnn/eval-EXP-20200710-150423/utils.py:105: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<NewLine>  mask = Variable(torch.cuda.FloatTensor(x.size(0), 1, 1, 1).bernoulli_(keep_prob))<NewLine>Traceback (most recent call last):<NewLine>  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;<NewLine>  File ""/home/XXXX/darts/cnn/eval-EXP-20200710-150423/venv/lib/python3.6/site-packages/torch/jit/__init__.py"", line 875, in trace<NewLine>    check_tolerance, _force_outplace, _module_class)<NewLine>  File ""/home/XXXX/darts/cnn/eval-EXP-20200710-150423/venv/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1027, in trace_module<NewLine>    module._c._create_method_from_trace(method_name, func, example_inputs, var_lookup_fn, _force_outplace)<NewLine>RuntimeError: Only tensors, lists and tuples of tensors can be output from traced functions<NewLine></code></pre><NewLine><p>If I remove logits_aux following ptrblck’s advice, it work well.<br/><NewLine>I don’t know why. So, I would appreciate it if you could inform when you find it.<br/><NewLine>Thank you!</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>My best guess is that tracing the model didn’t go through the conditions where <code>aux_logits</code> is set to a tensor, so that it stayed <code>None</code> until the <code>return</code> statement. This could happen, e.g. if you called <code>model.eval()</code> before exporting it.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your comment!<br/><NewLine>The point is that I have no choice but to erase the lights_aux, tracing model.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/crook52; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ebarsoum; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/crook52; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/crook52; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/crook52; <NewLine> ,"REPLY_DATE 1: July 19, 2020,  8:23am; <NewLine> REPLY_DATE 2: July 20, 2020,  8:02am; <NewLine> REPLY_DATE 3: July 28, 2020,  6:20am; <NewLine> REPLY_DATE 4: July 21, 2020,  4:37am; <NewLine> REPLY_DATE 5: July 28, 2020,  6:37am; <NewLine> REPLY_DATE 6: July 28, 2020,  6:41am; <NewLine> REPLY_DATE 7: July 28, 2020,  9:10am; <NewLine> REPLY_DATE 8: July 30, 2020,  1:20am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
82415,Debugging runtime error module-&gt;forward(inputs) libtorch 1.4,2020-05-22T03:00:57.735Z,7,571,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a question related to this project <a href=""https://github.com/NathanUA/U-2-Net/blob/7e5ff7d4c3becfefbb6e3d55916f48c7f7f5858d/u2net_test.py#L104"" rel=""nofollow noopener"">https://github.com/NathanUA/U-2-Net/blob/7e5ff7d4c3becfefbb6e3d55916f48c7f7f5858d/u2net_test.py#L104</a></p><NewLine><p>I can trace the net like this:</p><NewLine><pre><code>traced_script_module = torch.jit.trace(net, inputs_test)<NewLine>traced_script_module.save(""traced_model.pt"")<NewLine>print(inputs_test.size()) # shows (1, 3, 320, 320)<NewLine></code></pre><NewLine><p>Now I’m trying to run the model in a C++ application. I was able to do this in a prior project <a href=""https://github.com/DBraun/PyTorchTOP-cpumem"" rel=""nofollow noopener"">https://github.com/DBraun/PyTorchTOP-cpumem</a> I used CMake and built in debug mode by doing<br/><NewLine><code>SET DEBUG=1</code> before the CMake instructions.</p><NewLine><p>In the C++ project for U-2-Net, I can load the model into a module with no errors. When I call</p><NewLine><pre><code>torchinputs.clear();<NewLine>torchinputs.push_back(torch::ones({1, 3, 320, 320 }, torch::kCUDA).to(at::kFloat));<NewLine>module.forward(torchinputs); // error<NewLine></code></pre><NewLine><p>I get</p><NewLine><pre><code>Unhandled exception at 0x00007FFFD8FFA799 in TouchDesigner.exe: Microsoft C++ exception: std::runtime_error at memory location 0x000000EA677F1B30. occurred<NewLine></code></pre><NewLine><p>The error is at <a href=""https://github.com/pytorch/pytorch/blob/4c0bf93a0e61c32fd0432d8e9b6deb302ca90f1e/torch/csrc/jit/api/module.h#L112"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/4c0bf93a0e61c32fd0432d8e9b6deb302ca90f1e/torch/csrc/jit/api/module.h#L112</a> It says <code>inputs</code> has size 0. However, I’m pretty sure I’ve passed non-empty data (1, 3, 320,320) to module-&gt;forward() <a href=""https://github.com/DBraun/PyTorchTOP-cpumem/blob/f7cd16cb84021a7fc3681cad3a66c2bd7551a572/src/PyTorchTOP.cpp#L294"" rel=""nofollow noopener"">https://github.com/DBraun/PyTorchTOP-cpumem/blob/f7cd16cb84021a7fc3681cad3a66c2bd7551a572/src/PyTorchTOP.cpp#L294</a></p><NewLine><p>This is the stack trace at module-&gt;forward(torchinputs)</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/765f53f1c30856e251e8011d95aced362fba404b"" href=""https://discuss.pytorch.org/uploads/default/original/3X/7/6/765f53f1c30856e251e8011d95aced362fba404b.png"" title=""stacktrace""><img alt=""stacktrace"" data-base62-sha1=""gTazZSquxNEAXYf0fqJoRqMgQoH"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/7/6/765f53f1c30856e251e8011d95aced362fba404b_2_10x10.png"" height=""212"" src=""https://discuss.pytorch.org/uploads/default/original/3X/7/6/765f53f1c30856e251e8011d95aced362fba404b.png"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">stacktrace</span><span class=""informations"">939×289 10.6 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>I thought it might be a DLL issue but I’ve copied all DLLs from libtorch/lib</p><NewLine><p>I can confirm GPU stuff is available and that when I traced the module I was using CUDA.</p><NewLine><pre><code>LoadLibraryA(""c10_cuda.dll"");<NewLine>LoadLibraryA(""torch_cuda.dll"");<NewLine><NewLine>try {<NewLine>	std::cout &lt;&lt; ""CUDA:   "" &lt;&lt; torch::cuda::is_available() &lt;&lt; std::endl;<NewLine>	std::cout &lt;&lt; ""CUDNN:  "" &lt;&lt; torch::cuda::cudnn_is_available() &lt;&lt; std::endl;<NewLine>	std::cout &lt;&lt; ""GPU(s): "" &lt;&lt; torch::cuda::device_count() &lt;&lt; std::endl;<NewLine>}<NewLine>catch (std::exception&amp; ex) {<NewLine>	std::cout &lt;&lt; ex.what() &lt;&lt; std::endl;<NewLine>}<NewLine></code></pre><NewLine><p>Trying to fix the runtime exception on <code>module-&gt;forward</code>, I thought maybe <code>@torch.jit.script</code> needed to be in some of the functions in the U-2-Net project like here <a href=""https://github.com/NathanUA/U-2-Net/blob/7e5ff7d4c3becfefbb6e3d55916f48c7f7f5858d/model/u2net.py#L24"" rel=""nofollow noopener"">https://github.com/NathanUA/U-2-Net/blob/7e5ff7d4c3becfefbb6e3d55916f48c7f7f5858d/model/u2net.py#L24</a> I was worried about calling shape[2:] in a function without the <code>@torch.jit.script</code> Should I not be worried?</p><NewLine><p>Any advice is appreciated!</p><NewLine><p>I’ve also followed all the instructions here <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/an-unhandled-exceptionmicrosoft-c-exception-c10-error-at-memory-location/82284/4"">An unhandled exceptionMicrosoft C ++ exception: c10 :: Error at memory location</a></p><NewLine></div>",https://discuss.pytorch.org/u/DBraun,(David),DBraun,"May 23, 2020, 12:28am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you moved your model to CUDA? The model will be on CPU by default if you call torch::jit::load.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your suggestion. I tried</p><NewLine><pre><code>module = torch::jit::load(""traced_model.pt"", torch::kCUDA);<NewLine>module.to(torch::kCUDA);<NewLine></code></pre><NewLine><p>but got the same results. I have the debug dlls and library etc, perfectly ready for some more debugging. Anything more I can do to help?</p><NewLine><p>I’m stepping through line-by-line. I noticed that the module.forward() call takes about 18 seconds before the exception and this happens even when I know I’m giving it a wrongly sized Tensor:</p><NewLine><pre><code>torchinputs.push_back(torch::ones({1, 1, 1, 1}, torch::kCUDA).to(torch::kFloat)); // intentionally wrong size<NewLine>module.forward(torchinputs);<NewLine></code></pre><NewLine><p>If I change everything in my code to cpu, it doesn’t throw a runtime error. So I must be not succeeding in making sure everything is CUDA. I also tried following everything here <a href=""https://github.com/pytorch/pytorch/issues/19302"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/19302</a></p><NewLine><p>Why isn’t this sufficient for having everything in CUDA?</p><NewLine><pre><code>auto module = torch::jit::load(""traced_model.pt"", torch::kCUDA);<NewLine>for (auto p : module.parameters()) {<NewLine>	std::cout &lt;&lt; p.device() &lt;&lt; std::endl; // cuda:0<NewLine>}<NewLine><NewLine>auto finalinput = torch::ones({ 1, 3, 320, 320 }, torch::TensorOptions().dtype(torch::kFloat).device(torch::kCUDA));<NewLine>std::cout &lt;&lt; ""finalinput device: "" &lt;&lt; finalinput.device() &lt;&lt; std::endl; // cuda:0<NewLine>torchinputs.push_back(finalinput);<NewLine>auto forward_result = module.forward(torchinputs); // std::runtime_error<NewLine></code></pre><NewLine><p>^ and changing merely both of those two references to kCPU instead of kCUDA doesn’t throw an error.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I read everything here <a href=""https://pytorch.org/tutorials/advanced/cpp_export.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/cpp_export.html</a> and tried at::kCUDA instead of torch::kCUDA. I tried the nightly debug 1.5 libtorch but encountered other problems that I couldn’t solve, so I need to stick with 1.4 for now.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>same issue with you. I use unet for inference.(libtorch1.5.0 cuda9.2)</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi , I was facing the same issues (trying to run in Unreal Engine 4.25.</p><NewLine><p>Did you manage to solve this?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not yet. Maybe the info on the github issue will help you although it didn’t work for me <a href=""https://github.com/NathanUA/U-2-Net/issues/29"" rel=""nofollow noopener"">https://github.com/NathanUA/U-2-Net/issues/29</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Putting <code>LoadLibraryA(""torch_cuda.dll"");</code> early in my code allowed me to start using the nightly debug build of libtorch 1.5, but I’m still stuck on <code>module.forward</code>.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/307cacfc6a3f688ced10015a21725749fa77e907"" href=""https://discuss.pytorch.org/uploads/default/original/3X/3/0/307cacfc6a3f688ced10015a21725749fa77e907.png"" title=""2020-07-26 21_27_53-Microsoft Visual Studio""><img alt=""2020-07-26 21_27_53-Microsoft Visual Studio"" data-base62-sha1=""6UW2aTP710rLYvFQh0eVtLEqb5l"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/3/0/307cacfc6a3f688ced10015a21725749fa77e907_2_10x10.png"" height=""227"" src=""https://discuss.pytorch.org/uploads/default/original/3X/3/0/307cacfc6a3f688ced10015a21725749fa77e907.png"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">2020-07-26 21_27_53-Microsoft Visual Studio</span><span class=""informations"">1163×384 26.4 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>I also put <code>-INCLUDE:?warp_size@cuda@at@@YAHXZ</code> in Linker&gt;All Options&gt;Additional Options.</p><NewLine><p>and <code>LoadLibraryA(""c10_cuda.dll"");</code> early too. Is there something else I can try?</p><NewLine><p>Update again: I can trace a style transfer model and that works in my code here, but the traced U2Net model doesn’t work.</p><NewLine><p>Below is how I wrapped the model. Is it ok to use <code>x[:,0,:,:]</code> or does that break the jit trace? I’m also concerned about this line <a href=""https://github.com/NathanUA/U-2-Net/blob/b77cd6da3204efcb03e18e15dd3b9eb24d47f969/model/u2net.py#L24"" rel=""nofollow noopener"">https://github.com/NathanUA/U-2-Net/blob/b77cd6da3204efcb03e18e15dd3b9eb24d47f969/model/u2net.py#L24</a></p><NewLine><pre><code>def normPRED(d):<NewLine>    ma = torch.max(d)<NewLine>    mi = torch.min(d)<NewLine><NewLine>    dn = (d-mi)/(ma-mi)<NewLine><NewLine>    return dn<NewLine><NewLine><NewLine>class ModelWrapper(nn.Module):<NewLine><NewLine>    def __init__(self, u2netmodel):<NewLine>            super(ModelWrapper,self).__init__()<NewLine><NewLine>            self.u2netmodel = u2netmodel<NewLine><NewLine>    def forward(self, x):<NewLine><NewLine>            # my code doesn't use ToTensorLab in the data loader<NewLine>            # https://github.com/NathanUA/U-2-Net/blob/b77cd6da3204efcb03e18e15dd3b9eb24d47f969/data_loader.py#L208<NewLine>            # so do the normalization in this wrapper<NewLine><NewLine>            x = x / torch.max(x)<NewLine><NewLine>            r = (x[:,0,:,:]-0.485)/0.229<NewLine>            g = (x[:,1,:,:]-0.456)/0.224<NewLine>            b = (x[:,2,:,:]-0.406)/0.225<NewLine><NewLine>            img = torch.stack((r,g,b), 1)<NewLine><NewLine>            d1,d2,d3,d4,d5,d6,d7= self.u2netmodel(img)<NewLine><NewLine>            return normPRED(d1)<NewLine><NewLine># paraphrasing u2net_test.py<NewLine>net = U2NET(3,1)<NewLine>net.cuda()<NewLine>wrapper = ModelWrapper(net)<NewLine>wrapper.cuda()<NewLine>wrapper.to('cuda') # just in case?<NewLine>print('is cuda: ' + str(next(wrapper.parameters()).is_cuda)) # True<NewLine><NewLine>inputs_test = data_test['image']<NewLine>inputs_test = inputs_test.type(torch.FloatTensor)<NewLine>inputs_test = inputs_test.cuda()<NewLine>print(""inputs size: "" + str(inputs_test.size())) # [1, 3, 320, 320]<NewLine><NewLine>d1 = wrapper(inputs_test)<NewLine><NewLine># save d1 as an image and the image is great!<NewLine># save_output(img_name_list[i_test],pred,prediction_dir)<NewLine><NewLine>traced = torch.jit.trace(wrapper, inputs_test)<NewLine>traced.save(""traced_model.pt"")</code></pre><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>If I use torch.jit.script instead of torch.jit.trace:</p><NewLine><pre><code>sm = torch.jit.script(wrapper)<NewLine>torch.jit.save(sm, ""traced_model.pt"")<NewLine></code></pre><NewLine><p>I realized I could put print statements and that they would show up in the c++ console. I put some surrounding the execution of <code>torch.stack</code> in my <code>ModelWrapper</code>. It turns out this is the moment it throws the exception. Same thing happens when trying to write it with <code>torch.cat</code> followed by <code>torch.unsqueeze</code> (the cat fails).</p><NewLine><p>Weirdly I am able to do this inside the ModuleWrapper’s forward:<br/><NewLine><code>something = torch.stack([torch.randn([2, 3, 4]), torch.randn([2, 3, 4])])</code></p><NewLine><p>When I removed the <code>torch.stack</code> call from ModelWrapper, using print statements I was able to pinpoint a failure within the <code>u2netmodel</code> forward on a call to <code>cat</code>. <a href=""https://github.com/NathanUA/U-2-Net/blob/b77cd6da3204efcb03e18e15dd3b9eb24d47f969/model/u2net.py#L87"" rel=""nofollow noopener"">https://github.com/NathanUA/U-2-Net/blob/b77cd6da3204efcb03e18e15dd3b9eb24d47f969/model/u2net.py#L87</a></p><NewLine><p>So my question now is why can’t I do <code>torch.stack</code> or <code>torch.cat</code> in this module? Is it because these things allocate memory whereas calls to Conv2D don’t allocate? I’ve used print statements to make sure everything’s on cuda etc. Something about cloning? Python execution of the same model is working fine.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>I realized I should just try the following code:</p><NewLine><pre><code>auto thing1 = torch::ones({ 1, 3, 5, 5 }, torch::kCUDA).to(torch::kFloat32);<NewLine>auto thing2 = torch::ones({ 1, 3, 5, 5 }, torch::kCUDA).to(torch::kFloat32);<NewLine>auto thing3 = torch::cat({ thing1, thing2 }, 1);<NewLine></code></pre><NewLine><p>I get this error on <code>torch::cat</code></p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/4f6898080d78374bc1cdda75a4efde24cca011ba"" href=""https://discuss.pytorch.org/uploads/default/original/3X/4/f/4f6898080d78374bc1cdda75a4efde24cca011ba.png"" title=""image""><img alt=""image"" data-base62-sha1=""bktOjqgKupFj28yYhxqnW0QZyMW"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/f/4f6898080d78374bc1cdda75a4efde24cca011ba_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/original/3X/4/f/4f6898080d78374bc1cdda75a4efde24cca011ba.png"" width=""461""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1043×1131 44.3 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p><code>Unhandled exception at 0x00007FFA2C4FA799 in TouchDesigner.exe: Microsoft C++ exception: c10::Error at memory location 0x00000046B7DF54D0.</code><br/><NewLine>at this line:<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/7cdf786a07b2ca434983a0a508e2e42c75a4697d/aten/src/ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h#L100"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/7cdf786a07b2ca434983a0a508e2e42c75a4697d/aten/src/ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h#L100"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/7cdf786a07b2ca434983a0a508e2e42c75a4697d/aten/src/ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h#L100</a></h4><NewLine><pre class=""onebox""><code class=""lang-h""><ol class=""start lines"" start=""90"" style=""counter-reset: li-counter 89 ;""><NewLine><li><NewLine></li><li>template&lt;class FuncPtr, class... ParametersBeforeTensorOptions, class... ParametersAfterTensorOptions&gt;</li><NewLine><li>struct with_scattered_tensor_options_&lt;FuncPtr, guts::typelist::typelist&lt;ParametersBeforeTensorOptions...&gt;, guts::typelist::typelist&lt;ParametersAfterTensorOptions...&gt;&gt; final {</li><NewLine><li>    static decltype(auto) wrapper(</li><NewLine><li>                ParametersBeforeTensorOptions... parameters_before,</li><NewLine><li>                optional&lt;ScalarType&gt; scalar_type,</li><NewLine><li>                optional&lt;Layout&gt; layout,</li><NewLine><li>                optional&lt;Device&gt; device,</li><NewLine><li>                optional&lt;bool&gt; pin_memory,</li><NewLine><li>                ParametersAfterTensorOptions... parameters_after) {</li><NewLine><li class=""selected"">        return (*FuncPtr::func_ptr())(</li><NewLine><li>            std::forward&lt;ParametersBeforeTensorOptions&gt;(parameters_before)...,</li><NewLine><li>            TensorOptions().dtype(scalar_type).device(device).layout(layout).pinned_memory(pin_memory),</li><NewLine><li>            std::forward&lt;ParametersAfterTensorOptions&gt;(parameters_after)...</li><NewLine><li>        );</li><NewLine><li>    }</li><NewLine><li>};</li><NewLine><li><NewLine></li><li>}</li><NewLine><li><NewLine></li><li>template&lt;class FuncPtr&gt;</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Note that I’m running this code inside a DLL compiled for TouchDesigner and TouchDesigner is using CUDA 10.1 to match my libtorch build. If I run the same code inside a simple exe, there’s no error. Any clue what’s going on or clue to proceed? Thank you!</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>It was an issue with DLLs…</p><NewLine><p>TouchDesigner has its own DLLs in <code>C:/Program Files/Derivative/TouchDesigner/bin</code>. These DLLs get loaded when TouchDesigner opens.</p><NewLine><p>My custom plugin is in Documents/Derivative/Plugins and all of the libtorch DLLs are also there. My thought was that having everything in this Plugins folder would be sufficient: If my custom dll looked for a dependent DLL it would find it there as a sibling. However, I needed to paste the libtorch DLLs into TouchDesigner’s own bin folder. I didn’t trace down which specific DLL was the dealbreaker. Whichever one is most relevant to torch::cat I guess…</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/peterjc123; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/DBraun; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/DBraun; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/zhijian_li; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Arnon_Kahani; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/DBraun; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/DBraun; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/DBraun; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/DBraun; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/DBraun; <NewLine> ,"REPLY_DATE 1: May 23, 2020,  3:11am; <NewLine> REPLY_DATE 2: May 25, 2020,  5:36pm; <NewLine> REPLY_DATE 3: May 26, 2020,  3:54pm; <NewLine> REPLY_DATE 4: June 4, 2020,  6:59am; <NewLine> REPLY_DATE 5: July 7, 2020,  6:29pm; <NewLine> REPLY_DATE 6: July 7, 2020,  7:34pm; <NewLine> REPLY_DATE 7: July 27, 2020,  9:17pm; <NewLine> REPLY_DATE 8: July 28, 2020,  8:08pm; <NewLine> REPLY_DATE 9: July 29, 2020,  8:43pm; <NewLine> REPLY_DATE 10: July 29, 2020, 11:29pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> 
90850,Custom operation with quantization,2020-07-28T15:29:04.227Z,0,35,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I want to run MLP model on my accelerator.<br/><NewLine>My plan is to add a custom jit operation that operates like the nn.Linear layer but faster by using my accelerator.<br/><NewLine>However, the accelerator only supports int8 operation, so I need some quantization as well.</p><NewLine><p>How can I add the custom operation that utilizes torch.quantization?</p><NewLine><p>Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/zzoon91,(Junhyeok),zzoon91,"July 28, 2020,  3:29pm",,,,,
89933,Operator to convert intlist to tensor?,2020-07-20T17:59:00.861Z,2,100,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to cast intlist (int []) to a tensor and vice versa in the JIT scriptmodule’s graph? Curious to know if we can insert any operator which does this for us.</p><NewLine></div>",https://discuss.pytorch.org/u/vdantu,(Vamshidhar Dantu (dan21c)),vdantu,"July 20, 2020,  5:59pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>tensor, probably:</p><NewLine><pre><code class=""lang-python"">@torch.jit.script <NewLine>def fn(l : List[int]): <NewLine>     return torch.tensor(l) <NewLine>fn.graph                                                                                                                                                                                                  <NewLine></code></pre><NewLine><p>gives</p><NewLine><pre><code class=""lang-auto"">graph(%l.1 : int[]):<NewLine>  %4 : bool = prim::Constant[value=0]()<NewLine>  %2 : None = prim::Constant()<NewLine>  %5 : Tensor = aten::tensor(%l.1, %2, %2, %4)<NewLine>  return (%5)<NewLine></code></pre><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Nice example. Thanks <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I was going through the source code and found the following two operators</p><NewLine><pre><code class=""lang-auto"">""aten::_list_to_tensor(int[] self) -&gt; Tensor""<NewLine><NewLine>""aten::_tensor_to_list(Tensor self) -&gt; int[]""<NewLine></code></pre><NewLine><p>I don’t see these used anywhere in the code itself. Can these be used to convert an int[] to Tensor and if needed a Tensor to int[]?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>you can use::</p><NewLine><pre><code class=""lang-auto"">torch.jit.script<NewLine>def foo(x: torch.Tensor):<NewLine>    out: List[int] = x.tolist()<NewLine>    return out<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Were you able to script this method? I had tried this and got errors before.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vdantu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vdantu; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/vdantu; <NewLine> ,"REPLY_DATE 1: July 20, 2020,  6:55pm; <NewLine> REPLY_DATE 2: July 21, 2020,  3:55am; <NewLine> REPLY_DATE 3: July 25, 2020, 11:24pm; <NewLine> REPLY_DATE 4: July 28, 2020, 12:02am; <NewLine> REPLY_DATE 5: July 28, 2020, 12:35am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
89044,This is an invalid model. Graph output (1) does not exist in the graph,2020-07-13T20:15:12.516Z,0,131,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Does anyone know why this code gives the following error when loading the ONNX model?</p><NewLine><pre><code class=""lang-auto"">@torch.jit.script<NewLine>def bar(x):<NewLine>    zero = torch.tensor(0.0, dtype=torch.float32)<NewLine>    one = torch.tensor(1.0, dtype=torch.float32)<NewLine>    if x.eq(zero):<NewLine>        y = zero<NewLine>    else:<NewLine>        y = one<NewLine>    return y<NewLine>        <NewLine><NewLine>class Foo(nn.Module):<NewLine>    <NewLine>    def forward(self, x):<NewLine>        return bar(x)<NewLine>    <NewLine>foo = Foo()<NewLine><NewLine>dummy_x = torch.tensor(0.0, dtype=torch.float32)<NewLine><NewLine>torch.onnx.export(foo, dummy_x, ""./foo.onnx"", input_names=[""x""], output_names=[""y""])<NewLine><NewLine>foo_onnx = onnxruntime.InferenceSession(""./foo.onnx"")<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: D:\2\s\onnxruntime\core\graph\graph.cc:912 onnxruntime::Graph::InitializeStateFromModelFileGraphProto This is an invalid model. Graph output (1) does not exist in the graph.<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/anico,,anico,"July 13, 2020,  8:15pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just tested this repro with PyTorch and ONNXRuntime nightly.<br/><NewLine>The error I see is:<br/><NewLine>onnxruntime.capi.onnxruntime_pybind11_state.InvalidGraph: [ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. Type Error: Type ‘tensor(float)’ of input parameter (x) of operator (Equal) in node (Equal_3) is invalid.</p><NewLine><p>However, by changing dtype to torch.int in the test, this ONNXRuntime error is no longer thrown.<br/><NewLine>Looks like ONNX Equal op takes float input data type, so this might be an issue with ONNXRuntime.<br/><NewLine>Will follow op on this.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually setting opset_version=11 would fix this issue. ONNX Equal op supports float types starting from opset 11.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Got it. The change is as follows:</p><NewLine><pre><code class=""lang-auto"">torch.onnx.export(foo, dummy_x, ""D:/foo.onnx"", input_names=[""x""], output_names=[""y""], opset_version=11)<NewLine></code></pre><NewLine><p>Now it runs correctly:</p><NewLine><pre><code class=""lang-auto"">x = np.asarray(1.0, dtype=np.float32)<NewLine>foo_onnx.run([""y""], {""x"": x})<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/neginraoof; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/neginraoof; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/anico; <NewLine> ,"REPLY_DATE 1: July 24, 2020,  5:17pm; <NewLine> REPLY_DATE 2: July 27, 2020,  6:42pm; <NewLine> REPLY_DATE 3: July 27, 2020,  7:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
89647,Linking issues with torch,2020-07-17T20:51:05.818Z,0,88,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am building a custom C++ shared library that uses pytorch C++ libraries. But I am getting one undefined symbol - “typeinfo for torch::jit::GraphAttr”. I have included torch and c10 into my target_link_libraries. Am I missing additional pytorch libraries to link ?</p><NewLine></div>",https://discuss.pytorch.org/u/yetanadur,(Ananth Durbha),yetanadur,"July 18, 2020,  1:46am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>resolved the issue. It seems when I build pytorch code with<br/><NewLine><code>python setup.py install </code> the shared object executables are installed without execution permission on files - as<br/><NewLine><code>-rw-r--r-- </code> . After manually correcting file permissions with<br/><NewLine><code>chmod +x &lt;file.so&gt;</code> , the link issues are resolved.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/yetanadur; <NewLine> ,"REPLY_DATE 1: July 25, 2020,  1:16am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
90188,Unpacking Quantized model in C++ with torchscript format,2020-07-22T19:59:16.728Z,2,160,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi I am working on a quantized model in C++. I have trained and quantized the model in Python and loaded to C++ (post training quantization). I wonder if I can parse the jitted model parameters (torchscript format) in C++ ? I could not find any layer-unpacking modules in  torch::jit::script::Module .<br/><NewLine>After loading the model, I can dump the scriptModule modules and parameters using (torch::jit::script::Module) m-&gt;dump():</p><NewLine><pre><code>--------------------------------------------------------------------------------------------------------------------<NewLine>dumping module module __torch__.Net {<NewLine>	  parameters {<NewLine>	  }<NewLine>	  attributes {<NewLine>		training = False<NewLine>	(Here-&gt;)  fc1 = &lt;__torch__.torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU object at 0x5555566b75f0&gt;<NewLine>		Relu1 = &lt;__torch__.torch.nn.modules.linear.Identity object at 0x5555566b3d40&gt;<NewLine>		fc2 = &lt;__torch__.torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU object at 0x5555566a7030&gt;<NewLine>		Relu2 = &lt;__torch__.torch.nn.modules.linear.Identity object at 0x5555567b1440&gt;<NewLine>		droput2 = &lt;__torch__.torch.nn.modules.dropout.Dropout object at 0x5555567b1640&gt;<NewLine>		fc3 = &lt;__torch__.torch.nn.quantized.modules.linear.Linear object at 0x5555567b22c0&gt;<NewLine>		quant = &lt;__torch__.torch.nn.quantized.modules.Quantize object at 0x5555567b2ad0&gt;<NewLine>		dequant = &lt;__torch__.torch.nn.quantized.modules.DeQuantize object at 0x5555567b8350&gt;<NewLine>		logMax = &lt;__torch__.torch.nn.modules.activation.LogSoftmax object at 0x5555567b87c0&gt;<NewLine>	  }<NewLine>	  methods {<NewLine>		method forward {<NewLine>		  graph(%self.1 : __torch__.Net,<NewLine>				%x.1 : Tensor):<NewLine>			%7 : int = prim::Constant[value=-1]() # ~//MNIST_PyTorch_Quantize.py:50:19<NewLine>			%8 : int = prim::Constant[value=784]() # ~//MNIST_PyTorch_Quantize.py:50:23<NewLine>			%3 : __torch__.torch.nn.quantized.modules.Quantize = prim::GetAttr[name=""quant""](%self.1)<NewLine>			%x0.1 : Tensor = prim::CallMethod[name=""forward""](%3, %x.1) # :0:0<NewLine>			%9 : int[] = prim::ListConstruct(%7, %8)<NewLine>			%x1.1 : Tensor = aten::view(%x0.1, %9) # ~//MNIST_PyTorch_Quantize.py:50:12<NewLine>			%12 : __torch__.torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU = prim::GetAttr[name=""fc1""](%self.1)<NewLine>			%x2.1 : Tensor = prim::CallMethod[name=""forward""](%12, %x1.1) # :0:0<NewLine>			%16 : __torch__.torch.nn.modules.linear.Identity = prim::GetAttr[name=""Relu1""](%self.1)<NewLine>			%x3.1 : Tensor = prim::CallMethod[name=""forward""](%16, %x2.1) # :0:0<NewLine>			%20 : __torch__.torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU = prim::GetAttr[name=""fc2""](%self.1)<NewLine>			%x4.1 : Tensor = prim::CallMethod[name=""forward""](%20, %x3.1) # :0:0<NewLine>			%24 : __torch__.torch.nn.modules.linear.Identity = prim::GetAttr[name=""Relu2""](%self.1)<NewLine>			%x5.1 : Tensor = prim::CallMethod[name=""forward""](%24, %x4.1) # :0:0<NewLine>			%28 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=""droput2""](%self.1)<NewLine>			%x6.1 : Tensor = prim::CallMethod[name=""forward""](%28, %x5.1) # :0:0<NewLine>			%32 : __torch__.torch.nn.quantized.modules.linear.Linear = prim::GetAttr[name=""fc3""](%self.1)<NewLine>			%x7.1 : Tensor = prim::CallMethod[name=""forward""](%32, %x6.1) # :0:0<NewLine>			%36 : __torch__.torch.nn.quantized.modules.DeQuantize = prim::GetAttr[name=""dequant""](%self.1)<NewLine>			%x8.1 : Tensor = prim::CallMethod[name=""forward""](%36, %x7.1) # :0:0<NewLine>			%40 : __torch__.torch.nn.modules.activation.LogSoftmax = prim::GetAttr[name=""logMax""](%self.1)<NewLine>			%42 : Tensor = prim::CallMethod[name=""forward""](%40, %x8.1) # :0:0<NewLine>			return (%42)<NewLine>		}<NewLine>	  }<NewLine>	  submodules {<NewLine>		module __torch__.torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU {<NewLine>		  parameters {<NewLine>		  }<NewLine>		  attributes {<NewLine>			training = False<NewLine>			in_features = 784<NewLine>			out_features = 512<NewLine>			scale = 0.048926487565040588<NewLine>			zero_point = 0<NewLine>			_packed_params = &lt;__torch__.torch.nn.quantized.modules.linear.LinearPackedParams object at 0x5555566bcc40&gt;<NewLine>		  }<NewLine>		  methods {<NewLine>			method forward {<NewLine>			  graph(%self.1 : __torch__.torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU,<NewLine>					%input.1 : Tensor):<NewLine>				%4 : __torch__.torch.nn.quantized.modules.linear.LinearPackedParams = prim::GetAttr[name=""_packed_params""](%self.1)<NewLine>				%5 : Tensor = prim::GetAttr[name=""_packed_params""](%4)<NewLine>				%7 : float = prim::GetAttr[name=""scale""](%self.1)<NewLine>				%9 : int = prim::GetAttr[name=""zero_point""](%self.1)<NewLine>				%Y_q.1 : Tensor = quantized::linear_relu(%input.1, %5, %7, %9) # ~/python3.7/site-packages/torch/nn/intrinsic/quantized/modules/linear_relu.py:29:14<NewLine>					return (%Y_q.1)<NewLine>			}<NewLine>		  }<NewLine>		  submodules {<NewLine>		   (+)        module __torch__.torch.nn.quantized.modules.linear.LinearPackedParams {<NewLine>		  }<NewLine>		}<NewLine>		 module __torch__.torch.nn.modules.linear.Identity {} (+)<NewLine>		 module __torch__.torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU {} (+)<NewLine>		 module __torch__.torch.nn.modules.linear.Identity {} (+)<NewLine>		 module __torch__.torch.nn.modules.dropout.Dropout {} (+)<NewLine>		 module __torch__.torch.nn.quantized.modules.linear.Linear {} (+) <NewLine>		 module __torch__.torch.nn.quantized.modules.Quantize {} (+) <NewLine>		 module __torch__.torch.nn.quantized.modules.DeQuantize {}  (+)<NewLine>		 module __torch__.torch.nn.modules.activation.LogSoftmax {} (+)<NewLine>		} //   end of submodules <NewLine>		}  //   end of  dumping module module __torch__.Net<NewLine>	--------------------------------------------------------------------------------------------------------<NewLine></code></pre><NewLine><p>Notes: (+) means there are collapsed lines omitted to save space.<br/><NewLine>Torch version 1.6.0+.<br/><NewLine>My Questions:<br/><NewLine>1- During model load, are the module layers packed again to a format like in torch::nn::Linear and torch::nn::Conv1d … etc? how can I access them?<br/><NewLine>2- Are the pointers printed in the dump (like the line marked with “(Here-&gt;)”  ) for Python objects and methods or they are C++ objects and methods? are they cast-able to the formats in torch::nn:* ?<br/><NewLine>3- What is the recommended procedure to structure the model again in terms of the number of layers , the attributes\configuration of each layer and the corresponding trained weights from the jitted\torchscript format ?</p><NewLine></div>",https://discuss.pytorch.org/u/k.osama,(karim M.),k.osama,"July 22, 2020,  8:12pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>1- During model load, are the module layers packed again to a format like in torch::nn::Linear and torch::nn::Conv1d … etc? how can I access them?</p><NewLine></blockquote><NewLine><p>what is <code>torch::nn::Linear</code>? are you talking about the pytorch c++ API? the weights are packed in linear modules, you can use <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/linear.py#L34"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/linear.py#L34</a> to get the unpacked weights.</p><NewLine><blockquote><NewLine><p>Are the pointers printed in the dump (like the line marked with “(Here-&gt;)” ) for Python objects and methods or they are C++ objects and methods? are they cast-able to the formats in torch::nn:* ?</p><NewLine></blockquote><NewLine><p>These are TorchScript objects I think. I’m not sure about the relationship between the pytorch c++ API and TorchScript, cc <a class=""mention"" href=""/u/michael_suo"">@Michael_Suo</a> could you comment on this?</p><NewLine><blockquote><NewLine><p>What is the recommended procedure to structure the model again in terms of the number of layers , the attributes\configuration of each layer and the corresponding trained weights from the jitted\torchscript format ?</p><NewLine></blockquote><NewLine><p>Not sure I understand the question, could you be more concrete?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""90188"" data-username=""jerryzh168""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/jerryzh168/40/15217_2.png"" width=""20""/> jerryzh168:</div><NewLine><blockquote><NewLine><p>what is <code>torch::nn::Linear</code> ? are you talking about the pytorch c++ API? the weights are packed in linear modules, you can use <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/linear.py#L34"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/linear.py#L34 </a> to get the unpacked weights.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, I mean C++ API. As mentioned in description, I have trained the model in Python and exported it as torchscript to C++. Is the function “_weight_bias” accessible in torchscript format () (I can see that it is has the decorator <span class=""mention"">@torch.jit.export</span>) ? If yes, can you show please how to use it with loaded torchscript models ?<br/><NewLine>Rephrase 3-: Given a model that is trained in python, saved as “torchscript”  and loaded into C++ front end, I want to extract the number of layers, the type of each layer (Linear,conv1d…), the size of each layer and weights associated with each layer… How can I do that ?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""90188"" data-username=""k.osama""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/k.osama/40/27010_2.png"" width=""20""/> k.osama:</div><NewLine><blockquote><NewLine><p>torchscript format () (I can see that it is has the decorator <span class=""mention"">@torch.jit.export</span>) ? If yes, can you show please how to use</p><NewLine></blockquote><NewLine></aside><NewLine><p>here is an example calling method in a TorchScript Moudule: <a href=""https://codebrowser.bddppq.com/pytorch/pytorch/torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp.html#978"" rel=""nofollow noopener"">https://codebrowser.bddppq.com/pytorch/pytorch/torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp.html#978</a></p><NewLine><p>if by C++ API you mean TorchScript then this should work. There is another C++ API that is authoring models in C++(<a href=""https://pytorch.org/docs/stable/cpp_index.html#authoring-models-in-c"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/cpp_index.html#authoring-models-in-c</a>) which I’m not familiar with.</p><NewLine><p>for question 3: API of torch::jit::Module can be found in <a href=""https://codebrowser.bddppq.com/pytorch/pytorch/torch/csrc/jit/api/module.h.html#torch::jit::Module"" rel=""nofollow noopener"">https://codebrowser.bddppq.com/pytorch/pytorch/torch/csrc/jit/api/module.h.html#torch::jit::Module</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/k.osama; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: July 22, 2020,  9:26pm; <NewLine> REPLY_DATE 2: July 23, 2020, 10:03pm; <NewLine> REPLY_DATE 3: July 24, 2020,  8:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
90338,Problem saving nn.Module as a TorchScript module (DLRM model),2020-07-23T23:23:49.446Z,0,77,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am trying to create a TorchScript module of Facebook’s <a href=""https://github.com/facebookresearch/dlrm"" rel=""nofollow noopener"">deep learning recommendation model (DLRM)</a> using torch.jit.script() method. The conversion fails owing to the following runtime error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>cannot call a value of type 'Tensor':<NewLine>  File ""dlrm_s_pytorch.py"", line 275<NewLine>        # return x<NewLine>        # approach 2: use Sequential container to wrap all layers<NewLine>        return layers(x)<NewLine>               ~~~~~~ &lt;--- HERE<NewLine>'DLRM_Net.apply_mlp' is being compiled since it was called from 'DLRM_Net.sequential_forward'<NewLine>  File ""dlrm_s_pytorch.py"", line 343<NewLine>    def sequential_forward(self, dense_x, lS_o, lS_i):<NewLine>        # process dense features (using bottom mlp), resulting in a row vector<NewLine>        x = self.apply_mlp(dense_x, self.bot_l)<NewLine>        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        # debug prints<NewLine>        # print(""intermediate"")<NewLine>'DLRM_Net.sequential_forward' is being compiled since it was called from 'DLRM_Net.forward'<NewLine>  File ""dlrm_s_pytorch.py"", line 337<NewLine>    def forward(self, dense_x, lS_o, lS_i):<NewLine>        if self.ndevices &lt;= 1:<NewLine>            return self.sequential_forward(dense_x, lS_o, lS_i)<NewLine>                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        else:<NewLine>            return self.parallel_forward(dense_x, lS_o, lS_i)<NewLine></code></pre><NewLine><p>To recreate the error:</p><NewLine><ol><NewLine><li>Clone the DLRM repository and install the requirements.</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">&lt;activate virtual environment&gt;<NewLine>git clone https://github.com/facebookresearch/dlrm.git<NewLine>cd dlrm<NewLine>pip install requirements.txt<NewLine></code></pre><NewLine><ol start=""2""><NewLine><li>Add the following line in dlrm_s_pytorch.py at after line 179 to solve a type conversion issue:</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">n = n.item()<NewLine></code></pre><NewLine><ol start=""3""><NewLine><li>Add the following snippet in dlrm_s_pytorch.py after the architecture object is initialized:</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">dlrm_jit = torch.jit.script(dlrm)<NewLine>sys.exit() # successful exit after compiling, no need to train<NewLine></code></pre><NewLine><ol start=""4""><NewLine><li>Run the below command:</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">python  dlrm_s_pytorch.py --arch-sparse-feature-size=32 --arch-embedding-size=""70446-298426-33086-133729-61823"" --data-size=20480  --arch-mlp-bot=""256-256-128-32"" --arch-mlp-top=""256-64-1"" --max-ind-range=400000 --data-generation=random --loss-function=bce --nepochs=5 --round-targets=True --learning-rate=1.0  --mini-batch-size=2048<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/pollo_loco,,pollo_loco,"July 24, 2020,  5:25pm",,,,,
90093,Unable to increase performance for custom CNN using torchscript,2020-07-22T06:25:40.623Z,0,67,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I tried to visualize the runtime performance improvement made by convolution layer which I implemented from scratch Vs torchscript version of convolution layer Vs torch.nn.conv2d() module for 100 iterations with input (128,3,28,28), out_channel =64, kernel size=3.</p><NewLine><p>Convolution layer from scratch in CUDA -&gt; 9.366 seconds<br/><NewLine>torchscript convolution layer from scratch in CUDA -&gt; 6.636 seconds<br/><NewLine>torch.nn.conv2d() -&gt; 475.614 milliseconds.</p><NewLine><p><strong>Is there any problem in my approach? and how to optimize even more?</strong> . I request you to help me with this problem.</p><NewLine><p>My code</p><NewLine><pre><code class=""lang-auto"">class conv2D(nn.Module):<NewLine>  def __init__(self, in_channel, out_channel, kernel_size):<NewLine>    super(conv2D,self).__init__()<NewLine>    self.weight = torch.nn.Parameter(torch.ones(out_channel,in_channel,kernel_size, kernel_size))<NewLine>    self.bias = torch.nn.Parameter(torch.zeros(out_channel))<NewLine>    self.kernel_size = kernel_size<NewLine>    self.in_channel = in_channel<NewLine>    self.out_channel = out_channel<NewLine><NewLine>  def forward(self, image):<NewLine>    img_height = image.shape[3]<NewLine>    img_width = image.shape[2]<NewLine>    batch_size = image.shape[0]<NewLine>    out_height = img_height-self.kernel_size+1<NewLine>    out_width = img_width-self.kernel_size+1<NewLine><NewLine>    output = torch.zeros(batch_size,self.out_channel,out_width,out_height)<NewLine>    for k in range(batch_size):<NewLine>      for i in range(out_height):<NewLine>        for j in range(out_width):<NewLine>          temp = torch.sum(image[k,:,j:j+self.kernel_size,i:i+self.kernel_size]*self.weight,dim=(1,2,3))<NewLine>          output[k,:,i,j]=torch.add(temp,self.bias)<NewLine>      return output<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">x = torch.ones(128,3,28,28).to(""cuda"")<NewLine>c = conv2D(3,64,3).to(""cuda"")<NewLine>c_s = torch.jit.script(c).to(""cuda"")<NewLine>c_s(x)<NewLine></code></pre><NewLine><p>Scripting the model and running with a sample input to get an optimized graph</p><NewLine><pre><code class=""lang-auto"">with torch.autograd.profiler.profile(use_cuda=True) as prof:<NewLine>  with torch.no_grad():<NewLine>    for i in range(100):<NewLine>      c(x)<NewLine>print(prof.table())<NewLine></code></pre><NewLine><p>Profiling both the scripted and normal method.</p><NewLine><pre><code class=""lang-auto"">with torch.autograd.profiler.profile(use_cuda=True) as prof:<NewLine>  with torch.no_grad():<NewLine>    for i in range(100):<NewLine>      c_s(x)<NewLine>print(prof.table())<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Midhilesh,(Midhilesh),Midhilesh,"July 22, 2020,  6:25am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Convolutions are not easy to optimize and I’m not sure if there is any JIT compiler in the wild, which is currently able to optimize these layers to a competitive performance (please let me know, if you find it <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/> ).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 24, 2020,  9:54am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
87555,How to visualize the profile guided optimization in torchscript?,2020-07-01T08:19:23.812Z,4,179,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am currently studying torchscript. I came across the technique called <strong>profile guided optimization</strong> is being carried out in torchscript which gets every information about the tensor and it’s operation.</p><NewLine><p>Profile guided optimization uses <code>Prim:Profile</code> to these information. My doubt is is there any way to visualize the graph (Intermediate Representation) with <code>Prim:Profile</code> and <code>Prim:guard</code> in pytorch 1.5?</p><NewLine><p>Please help to with my doubt</p><NewLine></div>",https://discuss.pytorch.org/u/Midhilesh,(Midhilesh),Midhilesh,"July 1, 2020,  8:19am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, yes you can run:</p><NewLine><pre><code class=""lang-auto"">old_prof_exec_state = torch._C._jit_set_profiling_executor(True)<NewLine>old_prof_mode_state = torch._C._jit_set_profiling_mode(True)<NewLine>torch._C._jit_set_num_profiled_runs(num_runs)<NewLine></code></pre><NewLine><p>run your scripted function/module for some number of times,</p><NewLine><p>then run:</p><NewLine><p><code>torch.jit.last_executed_optimized_graph()</code></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi sir,<br/><NewLine>I can visualize <code>Prim:Profile</code>.</p><NewLine><p>Can you please explain me in detail the solution you have provided?. I cannot understand the set of commands you have given.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>old_prof_exec_state = torch._C._jit_set_profiling_executor(True)<br/><NewLine>old_prof_mode_state = torch._C._jit_set_profiling_mode(True)</p><NewLine></blockquote><NewLine><p>this enables the profiling executor.</p><NewLine><blockquote><NewLine><p>torch._C._jit_set_num_profiled_runs(num_runs)</p><NewLine></blockquote><NewLine><p>how many profiling runs we want to do before we optimize the graph.</p><NewLine><pre><code class=""lang-auto"">@torch.jit.script<NewLine>def foo(x):<NewLine>    if x.size(0) == 1:<NewLine>        return 1<NewLine>    else:<NewLine>        return 2<NewLine><NewLine>old_prof_exec_state = torch._C._jit_set_profiling_executor(True)<NewLine>old_prof_mode_state = torch._C._jit_set_profiling_mode(True)<NewLine>torch._C._jit_set_num_profiled_runs(1)<NewLine><NewLine>foo(torch.rand([1, 2]))<NewLine>print(torch.jit.last_executed_optimized_graph())<NewLine>foo(torch.rand([1, 2]))<NewLine>print(torch.jit.last_executed_optimized_graph())<NewLine></code></pre><NewLine><p>gives</p><NewLine><pre><code class=""lang-auto"">graph(%x.1 : Tensor):<NewLine>  %1 : int = prim::Constant[value=0]() # test/test_jit.py:3742:22<NewLine>  %2 : int = prim::Constant[value=1]() # test/test_jit.py:3742:28<NewLine>  %3 : int = prim::Constant[value=2]() # test/test_jit.py:3745:23<NewLine>  %4 : Tensor = prim::profile(%x.1)<NewLine>  %5 : int = aten::size(%4, %1) # test/test_jit.py:3742:15<NewLine>  %6 : bool = aten::eq(%5, %2) # test/test_jit.py:3742:15<NewLine>  %7 : int = prim::If(%6) # test/test_jit.py:3742:12<NewLine>    block0():<NewLine>      -&gt; (%2)<NewLine>    block1():<NewLine>      -&gt; (%3)<NewLine>   = prim::profile()<NewLine>  return (%7)<NewLine></code></pre><NewLine><p>first graph, still profiling</p><NewLine><pre><code class=""lang-auto"">graph(%x.1 : Tensor):<NewLine>  %2 : int = prim::Constant[value=1]() # test/test_jit.py:3742:28<NewLine>  %10 : int = prim::BailoutTemplate_0()<NewLine>  %9 : Double(1:2, 2:1) = prim::BailOut[index=0](%10, %x.1)<NewLine>  return (%2)<NewLine>with prim::BailoutTemplate_0 = graph(%x.1 : Tensor):<NewLine>  %1 : Double(1:2, 2:1) = prim::BailOut[index=0](%x.1)<NewLine>  %2 : int = prim::Constant[value=0]() # test/test_jit.py:3742:22<NewLine>  %3 : int = prim::Constant[value=1]() # test/test_jit.py:3742:28<NewLine>  %4 : int = prim::Constant[value=2]() # test/test_jit.py:3745:23<NewLine>  %5 : int = aten::size(%1, %2) # test/test_jit.py:3742:15<NewLine>  %6 : bool = aten::eq(%5, %3) # test/test_jit.py:3742:15<NewLine>  %7 : int = prim::If(%6) # test/test_jit.py:3742:12<NewLine>    block0():<NewLine>      -&gt; (%3)<NewLine>    block1():<NewLine>      -&gt; (%4)<NewLine>  return (%7)<NewLine></code></pre><NewLine><p>second graph, optimized from profiles.<br/><NewLine>additionally, you can read<br/><NewLine><a href=""https://github.com/pytorch/pytorch/blob/53af9df557aff745edf24193ece784fd008c6f19/torch/csrc/jit/OVERVIEW.md#profiling-programs"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/53af9df557aff745edf24193ece784fd008c6f19/torch/csrc/jit/OVERVIEW.md#profiling-programs</a>.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello sir,<br/><NewLine>I tried to visualize the runtime performance improvement made by convolution layer which I implemented from scratch Vs torchscript version of convolution layer Vs torch.nn.conv2d() module for 100 iterations with input (128,3,28,28), out_channel =64, kernel size=3.</p><NewLine><p>Convolution layer from scratch in CUDA -&gt; 9.366 seconds<br/><NewLine>torchscript convolution layer from scratch in CUDA -&gt; 6.636 seconds<br/><NewLine>torch.nn.conv2d() -&gt; 475.614 milliseconds.</p><NewLine><p>My code</p><NewLine><pre><code class=""lang-auto"">class conv2D(nn.Module):<NewLine>  def __init__(self, in_channel, out_channel, kernel_size):<NewLine>    super(conv2D,self).__init__()<NewLine>    self.weight = torch.nn.Parameter(torch.ones(out_channel,in_channel,kernel_size, kernel_size))<NewLine>    self.bias = torch.nn.Parameter(torch.zeros(out_channel))<NewLine>    self.kernel_size = kernel_size<NewLine>    self.in_channel = in_channel<NewLine>    self.out_channel = out_channel<NewLine><NewLine>  def forward(self, image):<NewLine>    img_height = image.shape[3]<NewLine>    img_width = image.shape[2]<NewLine>    batch_size = image.shape[0]<NewLine>    out_height = img_height-self.kernel_size+1<NewLine>    out_width = img_width-self.kernel_size+1<NewLine><NewLine>    output = torch.zeros(batch_size,self.out_channel,out_width,out_height)<NewLine>    for k in range(batch_size):<NewLine>      for i in range(out_height):<NewLine>        for j in range(out_width):<NewLine>          temp = torch.sum(image[k,:,j:j+self.kernel_size,i:i+self.kernel_size]*self.weight,dim=(1,2,3))<NewLine>          output[k,:,i,j]=torch.add(temp,self.bias)<NewLine>      return output<NewLine></code></pre><NewLine><p>Scripting the model and running with a sample input to get an optimized graph</p><NewLine><pre><code class=""lang-auto"">x = torch.ones(128,3,28,28).to(""cuda"")<NewLine>c = conv2D(3,64,3).to(""cuda"")<NewLine>c_s = torch.jit.script(c).to(""cuda"")<NewLine>c_s(x)<NewLine></code></pre><NewLine><p>Profiling both the scripted and normal method.</p><NewLine><pre><code class=""lang-auto"">with torch.autograd.profiler.profile(use_cuda=True) as prof:<NewLine>  with torch.no_grad():<NewLine>    for i in range(100):<NewLine>      c(x)<NewLine>print(prof.table())<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">with torch.autograd.profiler.profile(use_cuda=True) as prof:<NewLine>  with torch.no_grad():<NewLine>    for i in range(100):<NewLine>      c_s(x)<NewLine>print(prof.table())<NewLine></code></pre><NewLine><p><strong>Is there any problem in my approach? and how to optimize even more?</strong>. I request you to help me with this problem.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Sorry for delay. Currently we only generate new optimized kernels for a series of pointwise ops on GPU. If that is not part of your use case it is unlikely you will see speedup at the moment.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sir, But loop unrolling won’t provide higher performance?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Midhilesh; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Midhilesh; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Midhilesh; <NewLine> ,"REPLY_DATE 1: July 1, 2020,  9:33pm; <NewLine> REPLY_DATE 2: July 2, 2020,  9:06am; <NewLine> REPLY_DATE 3: July 3, 2020,  2:43pm; <NewLine> REPLY_DATE 4: July 21, 2020,  7:59am; <NewLine> REPLY_DATE 5: July 23, 2020, 11:43pm; <NewLine> REPLY_DATE 6: July 24, 2020,  5:18am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
83325,Torch.jit.script error: Tensor cannot be used as a tuple,2020-05-29T06:31:52.670Z,0,142,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Here is my forward function.<br/><NewLine>def forward(self, inputs):<br/><NewLine>if self.first_time:<br/><NewLine>p3, p4, p5 = inputs</p><NewLine><pre><code>        p3_in = self.f3(p3)<NewLine>        p4_in = self.f4(p4)<NewLine>        p5_in = self.f5(p5)<NewLine><NewLine>    else:<NewLine>        p3_in, p4_in, p5_in = inputs<NewLine></code></pre><NewLine><p>error shows:<br/><NewLine>RuntimeError:<br/><NewLine>Tensor cannot be used as a tuple:<br/><NewLine>if self.first_time:<br/><NewLine>p3, p4, p5 = inputs<br/><NewLine>~~~~~~ &lt;— HERE</p><NewLine><p>is there anyway to resolve this error?</p><NewLine></div>",https://discuss.pytorch.org/u/kevpan,,kevpan,"May 29, 2020,  6:31am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I got the same problem. Did you solve it ? THX</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/111365; <NewLine> ,"REPLY_DATE 1: July 23, 2020,  3:19am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
90131,How to fix &ldquo;Unknown IValue type for pickling: Device&rdquo; in PyTorch 1.3?,2020-07-22T12:37:45.635Z,1,68,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I got a model trained with PyTorch 1.4. If I script and save this model with PyTorch 1.4, it will save successfully, but I need to script and save this model with PyTorch 1.3. When I save model with 1.3, I got error message:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Unknown IValue type for pickling: Device (pushIValueImpl at /pytorch/torch/csrc/jit/pickler.cpp:125)<NewLine></code></pre><NewLine><p>I want to know how to fix this Device issue. Thx.</p><NewLine></div>",https://discuss.pytorch.org/u/kaituoxu,(Kaituoxu),kaituoxu,"July 22, 2020, 12:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It implies that somewhere in your model, you’re storing a device type and trying to serialize it (like <code>self.foo = my_device</code>). This is supported in 1.4 but not 1.3; if you want your model to work with 1.3, you’ll have to avoid storing references to devices.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thx.<br/><NewLine>In my class, I have a</p><NewLine><pre><code class=""lang-auto"">@property <NewLine>def device():...<NewLine></code></pre><NewLine><p>After I delete this part, I can convert model successfully.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kaituoxu; <NewLine> ,"REPLY_DATE 1: July 23, 2020,  1:54am; <NewLine> REPLY_DATE 2: July 23, 2020,  1:55am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
88921,Save and load Inline and Lowered Graphs as ScriptModules,2020-07-13T01:45:34.582Z,0,108,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to save a inline’d and lowered graph as a ScriptModule and load it using torch.jit.load? My use case is to resolve the ClassType inputs into actual accessed tensor parameters.</p><NewLine></div>",https://discuss.pytorch.org/u/vdantu,(Vamshidhar Dantu (dan21c)),vdantu,"July 13, 2020,  1:45am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, this should be possible. What did you try that didn’t work ?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/eellison"">@eellison</a>:  Thanks for your response. I need some help/suggestion in how to proceed. Lowering a graph (which belongs to a scriptmodule’s method) returns a lowered graph and parameters. This pass replaces the input class object (for example <code>""self""</code>) with actual parameters belonging to <code>""self""</code> which are accessed throughout the graph. Now, creating a method with this lowered graph and adding this method to a module will seem like adding a <code>method</code> which takes original inputs along with the parameter tensors. The question I had is, how do I package these parameters and this lowered graph such that when we run the method, I would only need to provide the inputs to original ScriptModule and not these parameters? Not sure if I am explaining the question well. Please do let me know if I am not thinking straight.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could we do the following?</p><NewLine><ol><NewLine><li>Run LowerGraph pass: This pass will return a new Graph and a list of tensors.</li><NewLine><li>Add these Tensors as constants in the graph and remove the corresponding Graph input from the graph. Replace all the input’s references with the constants references.</li><NewLine><li>Create a module from this graph.</li><NewLine></ol><NewLine><p>Is this a valid approach?</p><NewLine><p>This will create a graph whose inputs are only the original model’s inputs.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vdantu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vdantu; <NewLine> ,"REPLY_DATE 1: July 13, 2020,  4:45pm; <NewLine> REPLY_DATE 2: July 13, 2020,  5:10pm; <NewLine> REPLY_DATE 3: July 22, 2020,  2:00pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
89990,Different behaivour of exported model in TorchScript,2020-07-21T07:13:24.018Z,1,64,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there,</p><NewLine><p>I am trying to export and use a model defined in pytorch by using the “torch.jit.script” method. When I try to use the exported model the result obtained is awful if you compare with the original results. The model loads pretrained weigths and after all of these I use the method “torch.jit.script” and save it to the file. I have found some related <a href=""https://github.com/pytorch/pytorch/issues/30754"" rel=""nofollow noopener"">issue in github</a>, however I have not managed to solve the problem.</p><NewLine><p>Any idea of the problem? Thank you in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/eusebioaguilera,(Eusebio),eusebioaguilera,"July 21, 2020,  7:13am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you have some code to reproduce the issue? It’s hard to say what’s going on without more information</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your response. Finally, I have managed to get similar results using the exported model. The problem was that the tensor was transposed by using a custom ToTensor transform instead of using the standard ToTensor transform.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/eusebioaguilera; <NewLine> ,"REPLY_DATE 1: July 21, 2020,  8:51pm; <NewLine> REPLY_DATE 2: July 22, 2020, 10:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
89983,Help with simple jit scripting,2020-07-21T05:11:47.840Z,0,74,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,<br/><NewLine>I’ve been trying to use jit scripting but I seem to be getting no success.  I’ve now tried to create a very simple test case as shown.  I’m running this on conda python 3.7, cuda 10.1, pytorch 1.5.1, CentOS 7.6 installed as the website says.</p><NewLine><pre><code class=""lang-auto"">import torch                                                                                                                                                                          <NewLine>                                                                                                                                                                                      <NewLine>@torch.jit.script                                                                                                                                                                     <NewLine>def simple_kernel(x1, y1, x2, y2):                                                                                                                                                    <NewLine>    xi = torch.max(x1, x2)                                                                                                                                                            <NewLine>    yi = torch.max(y1, y2)                                                                                                                                                            <NewLine>    zi = xi+yi                                                                                                                                                                        <NewLine>    return zi                                                                                                                                                                                                                                                                                                                                                               <NewLine>                                                                                                                                                                                      <NewLine>x1, y1, x2, y2 = torch.randn(4, 10_000_000, device='cuda')                                                                                                                            <NewLine>zz = simple_kernel(x1, y1, x2, y2)  <NewLine>simple_kernel.graph_for(x1, y1, x2, y2)                                                                                                                                               <NewLine>print(simple_kernel.graph)                                                                                                                                                            <NewLine></code></pre><NewLine><p>When I run this, I see the following:</p><NewLine><pre><code class=""lang-auto"">(base) [jlquinn test]$ PYTORCH_FUSION_DEBUG=1 python jittst1.py<NewLine>graph(%x1.1 : Tensor,<NewLine>      %y1.1 : Tensor,<NewLine>      %x2.1 : Tensor,<NewLine>      %y2.1 : Tensor):<NewLine>  %12 : int = prim::Constant[value=1]()<NewLine>  %xi.1 : Tensor = aten::max(%x1.1, %x2.1) # jittst1.py:6:9<NewLine>  %yi.1 : Tensor = aten::max(%y1.1, %y2.1) # jittst1.py:7:9<NewLine>  %zi.1 : Tensor = aten::add(%xi.1, %yi.1, %12) # jittst1.py:8:9<NewLine>  return (%zi.1)<NewLine></code></pre><NewLine><p>From what I’ve read, this should be a simple case for pytorch to fuse, consisting of simple pointwise operations, but it appears that there is no fusion happening.  Can anyone enlighten me as to what I’m missing?</p><NewLine><p>Thanks<br/><NewLine>Jerry</p><NewLine></div>",https://discuss.pytorch.org/u/jlquinn,,jlquinn,"July 21, 2020,  5:11am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>In pytorch 1.5 it is necessary to enable to profile guided optimization to visualize the fusion group. Thus</p><NewLine><pre><code class=""lang-auto"">torch._C._jit_set_profiling_executor(False)<NewLine>@torch.jit.script                                                                                                                                                                     <NewLine>def simple_kernel(x1, y1, x2, y2):                                                                                                                                                    <NewLine>    xi = torch.max(x1, x2)                                                                                                                                                            <NewLine>    yi = torch.max(y1, y2)                                                                                                                                                            <NewLine>    zi = xi+yi                                                                                                                                                                        <NewLine>    return zi                                                                                                                                                                                                                                                                                                                                                               <NewLine></code></pre><NewLine><pre><code class=""lang-auto"">zz = simple_kernel(x1, y1, x2, y2)<NewLine>print(simple_kernel.graph_for(x1,y1,x2,y2))<NewLine></code></pre><NewLine><p>Will give the optimal answer</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for taking the time to respond.  Is this documented anywhere yet?  And more importantly, when I run a torch.jit.script function, it will optimize after it’s been run a few times, right?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Midhilesh; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jlquinn; <NewLine> ,"REPLY_DATE 1: July 21, 2020,  8:28am; <NewLine> REPLY_DATE 2: July 21, 2020,  3:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
88254,How to create a module from graph,2020-07-07T15:04:24.205Z,1,89,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey,<br/><NewLine>I plan to export a torchscript graph represantation by using <code>trace(...)</code> command, and then using <code>trace.graph()</code>. after that, I plan to apply some optimisations for the given graph and then I want to revert the result back to <code>ScriptModule</code> class.<br/><NewLine>I am familiar with: <code>torch._C._create_function_from_graph('forward', graph)</code><br/><NewLine>but it returns <code>ScriptFunction</code> class. I would like to use it as a module (ScriptModule class).</p><NewLine><p>I need it as a module class because I need to be able to access its parameters by <code>parameters()</code> method.</p><NewLine><p>thanks, Omer.</p><NewLine></div>",https://discuss.pytorch.org/u/Omer_B,(Omer B),Omer_B,"July 7, 2020,  3:04pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>if you are mutating the graph in-place, the original ScriptModule should reflect your optimizations. Today we have no public way of creating a ScriptModule from a graph alone</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for responding,<br/><NewLine>what do you mean by “in-place” in this case ?<br/><NewLine>lets assume that I have a traced module that I got by: <code>torch.jit.trace(model, sample_inputs)</code>.<br/><NewLine>do you mean that I need to change its “graph” field or “inlined_graph” field to make the module to represent the new graph? (thats mean that all its parameters should be changed if needed)</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/michael_suo"">@Michael_Suo</a> In case <strong>we do not want to modify the graph in-place,</strong><br/><NewLine>that is, we want an <code>nn.Module</code> that, given a graph, we will have the same <code>forward</code> function as the graph,<br/><NewLine>and will have same parameters/buffers/modules as used in the graph,</p><NewLine><p>Can we automatically create such <code>nn.Module</code> given a graph by inferring everything needed?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Omer_B; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/seliad; <NewLine> ,"REPLY_DATE 1: July 7, 2020,  7:31pm; <NewLine> REPLY_DATE 2: July 10, 2020,  9:22pm; <NewLine> REPLY_DATE 3: July 19, 2020,  9:33am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
89426,Inconsistency between traced and true model outputs,2020-07-16T09:01:40.014Z,1,99,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to serialize my model to run in C++, but even before C++ I’m testing in python both the original and the trace model outputs on the same input and get different results, here is my python code:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torchvision import models<NewLine>import torch.nn as nn<NewLine><NewLine>model_file = r'D:\workspace\....\model.pt'<NewLine>model_ft = models.resnet50()<NewLine>num_ftrs = model_ft.fc.in_features<NewLine>model_ft.fc = nn.Linear(num_ftrs, 6)<NewLine>model_ft.load_state_dict(torch.load(model_file))<NewLine><NewLine># An example input you would normally provide to your model's forward() method.<NewLine>example = torch.rand(1, 3, 224, 224)<NewLine><NewLine># Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.<NewLine>traced_script_module = torch.jit.trace(model_ft, example)<NewLine><NewLine># Test both outputs of original and traced model to compare<NewLine>model_ft.eval()<NewLine>with torch.no_grad():<NewLine>    output_model = model_ft(torch.zeros(1, 3, 224, 224))<NewLine><NewLine>output_traced_model = traced_script_module(torch.zeros(1, 3, 224, 224))<NewLine><NewLine>print('output_model = ' + str(output_model))<NewLine>print('output_traced_model = ' + str(output_traced_model))<NewLine># save traced model<NewLine>traced_script_module.save(""traced_resnet_model.pt"")<NewLine></code></pre><NewLine><p>and variables 'output_model ’ and 'output_traced_model ’ are completely different:</p><NewLine><pre><code class=""lang-auto"">output_model = tensor([[-0.0805,  0.2096,  0.0873, -0.0468, -0.1598, -0.0375]])<NewLine>output_traced_model = tensor([[-0.4763,  1.5731,  0.2112,  0.3496, -1.6906,  0.0191]],<NewLine>       grad_fn=&lt;DifferentiableGraphBackward&gt;)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/SandSt0rm161,(Itai Druker),SandSt0rm161,"July 16, 2020, 10:35am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The difference is created, because you are running the eager model in <code>eval()</code>, while the traced model was in <code>train()</code> mode.<br/><NewLine>From the <a href=""https://pytorch.org/docs/master/generated/torch.jit.trace.html#torch.jit.trace"">docs</a>:</p><NewLine><blockquote><NewLine><p>In the returned <a href=""https://pytorch.org/docs/master/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule""><code>ScriptModule</code></a>, operations that have different behaviors in  <code>training</code>  and  <code>eval</code>  modes will always behave as if it is in the mode it was in during tracing, no matter which mode the ScriptModule is in.</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply.<br/><NewLine>I confirm that switching the original model to eval() mode before creating the traced model solves the issue.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/SandSt0rm161; <NewLine> ,"REPLY_DATE 1: July 17, 2020, 11:52am; <NewLine> REPLY_DATE 2: July 18, 2020,  4:25am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
89588,Torchscript back to eager,2020-07-17T11:17:52.961Z,0,79,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,</p><NewLine><p>is there an easy way to convert a torchscripted model back to an eager model ?</p><NewLine><p>Example:</p><NewLine><pre><code class=""lang-python"">eager_model = MyModel()<NewLine>scripted_model = torch.jit.script(eager_model)<NewLine>recovered_eager_model = some_function(scripted_model)  ### could not find anything about it in the docs<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/tmain,,tmain,"July 17, 2020, 11:17am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, and it is strongly advised that you keep your source code around when doing stuff with JITed models.</p><NewLine><p>That said, you can probably get a reduced model by walking the traced module, setting the children in init and using the <code>.forward</code> of each module.<br/><NewLine>This would get you something you can run and re-trace, but you loose how it was built (i.e. the inits).</p><NewLine><p>Fun fact: In building a <a href=""https://lernapparat.de/transformers-pytorch-tvm/"" rel=""nofollow noopener"">PyTorch-TVM bridge</a> I built a PyTorch autograd function (but with TVM as backend instead of calling PyTorch) corresponding to the traced model.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: July 24, 2020, 10:57am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
89557,Error when exporting ScriptFuntion to ONNX,2020-07-17T08:23:52.290Z,0,52,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone, I’m new to ONNX and I’m trying to convert a model where I need do some for-loop assignmens like the code below,</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine>@torch.jit.script<NewLine>def create_alignment_v2():<NewLine>    base_mat = torch.zeros(2, 2, 2)<NewLine><NewLine>    for i in range(base_mat.size(0)):<NewLine>        base_mat[i][0][0] = 1<NewLine><NewLine>    return base_mat<NewLine><NewLine>class ToyModule(nn.Module):<NewLine><NewLine>    def __int__(self):<NewLine>        super().__init__()<NewLine><NewLine><NewLine>    def forward(self, duration_predictor_output):<NewLine><NewLine>        alignment = create_alignment_v2()<NewLine>        # output = alignment @ x<NewLine>        return alignment<NewLine><NewLine>def test():<NewLine>    module = ToyModule()<NewLine>    module.eval()<NewLine>    x = torch.rand(2, 28, 384)<NewLine>    alignment = module(x)<NewLine>    torch.onnx.export(module, x, 'toy.onnx',<NewLine>                      export_params=True,<NewLine>                      opset_version=10,<NewLine>                      do_constant_folding=True,<NewLine>                      verbose=True,<NewLine>                      input_names=['seq'],<NewLine>                      output_names=['alignment'],<NewLine>                      dynamic_axes={'seq': {0: 'batch', 1: 'sequence'},}<NewLine>                      )<NewLine><NewLine>test()<NewLine></code></pre><NewLine><p>And the error message is:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""/mfs/fangzhiqiang/workspace/tts/FastSpeech/jit_script.py"", line 83, in &lt;module&gt;<NewLine>    test()<NewLine>  File ""/mfs/fangzhiqiang/workspace/tts/FastSpeech/jit_script.py"", line 78, in test<NewLine>    dynamic_axes={'seq': {0: 'batch', 1: 'sequence'},}<NewLine>  File ""/home/fangzhiqiang/miniconda3/envs/tts/lib/python3.6/site-packages/torch/onnx/__init__.py"", line 168, in export<NewLine>    custom_opsets, enable_onnx_checker, use_external_data_format)<NewLine>  File ""/home/fangzhiqiang/miniconda3/envs/tts/lib/python3.6/site-packages/torch/onnx/utils.py"", line 69, in export<NewLine>    use_external_data_format=use_external_data_format)<NewLine>  File ""/home/fangzhiqiang/miniconda3/envs/tts/lib/python3.6/site-packages/torch/onnx/utils.py"", line 488, in _export<NewLine>    fixed_batch_size=fixed_batch_size)<NewLine>  File ""/home/fangzhiqiang/miniconda3/envs/tts/lib/python3.6/site-packages/torch/onnx/utils.py"", line 351, in _model_to_graph<NewLine>    fixed_batch_size=fixed_batch_size, params_dict=params_dict)<NewLine>  File ""/home/fangzhiqiang/miniconda3/envs/tts/lib/python3.6/site-packages/torch/onnx/utils.py"", line 120, in _optimize_graph<NewLine>    torch._C._jit_pass_onnx_prepare_inplace_ops_for_onnx(graph)<NewLine>IndexError: vector::_M_range_check: __n (which is 2) &gt;= this-&gt;size() (which is 2)<NewLine></code></pre><NewLine><p>In fact, if I remove the assignment operation, the graph can be built succesfully. I wonder whether this is a bug and how to convert such model to ONNX. Thanks for any reply~</p><NewLine></div>",https://discuss.pytorch.org/u/Zhiqiang_Fong,(Zhiqiang Fong),Zhiqiang_Fong,"July 17, 2020,  8:23am",,,,,
84258,Self.rnn.flatten_parameters() not supported in torchscript?,2020-06-05T03:56:50.997Z,0,165,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I got the following warning when loading torchscript model in libtorch:</p><NewLine><pre><code class=""lang-auto"">Warning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (_cudnn_impl at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1269)<NewLine></code></pre><NewLine><p>But when I added <code>flatten_parameters()</code> in my code like this:</p><NewLine><pre><code class=""lang-auto"">class BidirectionalLSTM(nn.Module):<NewLine>    def __init__(self, nIn, nHidden, nOut):<NewLine>        super(BidirectionalLSTM, self).__init__()<NewLine>        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)<NewLine>        self.embedding = nn.Linear(nHidden * 2, nOut)<NewLine><NewLine>    def forward(self, input):<NewLine>        self.rnn.flatten_parameters()<NewLine>        recurrent, _ = self.rnn(input)<NewLine>        T, b, h = recurrent.size()<NewLine>        t_rec = recurrent.view(T * b, h)<NewLine>        output = self.embedding(t_rec)  # [T * b, nOut]<NewLine>        output = output.view(T, b, -1)<NewLine>        return output<NewLine></code></pre><NewLine><p>I got this error:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""to_torchscript.py"", line 252, in &lt;module&gt;<NewLine>    tt = torch.jit.script(teacher_model)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1203, in script<NewLine>    return torch.jit.torch.jit._recursive.recursive_script(obj)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 173, in recursive_script<NewLine>    return copy_to_script_module(mod, overload_stubs + stubs)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 95, in copy_to_script_module<NewLine>    torch.jit._create_methods_from_stubs(script_module, stubs)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1423, in _create_methods_from_stubs<NewLine>    self._c._create_methods(self, defs, rcbs, defaults)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 195, in make_strong_submodule<NewLine>    new_strong_submodule = recursive_script(module)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 116, in recursive_script<NewLine>    return create_constant_iterable_module(mod)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 233, in create_constant_iterable_module<NewLine>    modules[key] = recursive_script(submodule)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 173, in recursive_script<NewLine>    return copy_to_script_module(mod, overload_stubs + stubs)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 95, in copy_to_script_module<NewLine>    torch.jit._create_methods_from_stubs(script_module, stubs)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1423, in _create_methods_from_stubs<NewLine>    self._c._create_methods(self, defs, rcbs, defaults)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 181, in create_method_from_fn<NewLine>    stub = torch.jit.script_method(fn, _jit_internal.createResolutionCallbackFromClosure(fn))<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1280, in script_method<NewLine>    ast = get_jit_def(fn, self_name=""ScriptModule"")<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/frontend.py"", line 169, in get_jit_def<NewLine>    return build_def(ctx, py_ast.body[0], type_line, self_name)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/frontend.py"", line 209, in build_def<NewLine>    build_stmts(ctx, body))<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/frontend.py"", line 127, in build_stmts<NewLine>    stmts = [build_stmt(ctx, s) for s in stmts]<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/frontend.py"", line 127, in &lt;listcomp&gt;<NewLine>    stmts = [build_stmt(ctx, s) for s in stmts]<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/frontend.py"", line 185, in __call__<NewLine>    return method(ctx, node)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/frontend.py"", line 283, in build_Assign<NewLine>    rhs = build_expr(ctx, stmt.value)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/frontend.py"", line 185, in __call__<NewLine>    return method(ctx, node)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/frontend.py"", line 442, in build_Call<NewLine>    args = [build_expr(ctx, py_arg) for py_arg in expr.args]<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/frontend.py"", line 442, in &lt;listcomp&gt;<NewLine>    args = [build_expr(ctx, py_arg) for py_arg in expr.args]<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/frontend.py"", line 184, in __call__<NewLine>    raise UnsupportedNodeError(ctx, node)<NewLine>torch.jit.frontend.UnsupportedNodeError: GeneratorExp aren't supported:<NewLine>at /home/dai/py36env/lib/python3.6/site-packages/torch/nn/modules/rnn.py:105:31<NewLine>        any_param = next(self.parameters()).data<NewLine>        if not any_param.is_cuda or not torch.backends.cudnn.is_acceptable(any_param):<NewLine>            return<NewLine><NewLine>        # If any parameters alias, we fall back to the slower, copying code path. This is<NewLine>        # a sufficient check, because overlapping parameter buffers that don't completely<NewLine>        # alias would break the assumptions of the uniqueness check in<NewLine>        # Module.named_parameters().<NewLine>        all_weights = self._flat_weights<NewLine>        unique_data_ptrs = set(p.data_ptr() for p in all_weights)<NewLine>                               ~ &lt;--- HERE<NewLine>        if len(unique_data_ptrs) != len(all_weights):<NewLine>            return<NewLine><NewLine>        with torch.cuda.device_of(any_param):<NewLine>            import torch.backends.cudnn.rnn as rnn<NewLine><NewLine>            # NB: This is a temporary hack while we still don't have Tensor<NewLine>            # bindings for ATen functions<NewLine>            with torch.no_grad():<NewLine>'__torch__.BidirectionalLSTM.forward' is being compiled since it was called from '__torch__.teacher.forward'<NewLine>at to_torchscript.py:221:8<NewLine>    def forward(self, input):<NewLine>        # conv features<NewLine>        conv = self.cnn(input)<NewLine>        b, c, h, w = conv.size()<NewLine><NewLine>        assert h == 1, ""the height of conv must be 1""<NewLine>        conv = conv.squeeze(2)<NewLine>        conv = conv.permute(2, 0, 1)  # [w, b, c]<NewLine>        # rnn features<NewLine>        output = self.rnn(conv)<NewLine>        ~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        T, b, h = output.size()<NewLine>        output = output.view(T, b, -1)<NewLine></code></pre><NewLine><p>How can I call flatten_parameters() in torchscript？And are there other ways to get rid of the warning?</p><NewLine><p>Looking forward to your reply.</p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"June 5, 2020,  3:57am",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Same situation here.</p><NewLine><p>And my current workaround is just suppress the warning message.</p><NewLine><pre><code class=""lang-auto"">import warnings<NewLine>warnings.filterwarnings('ignore')<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jimlinntu; <NewLine> ,"REPLY_DATE 1: July 16, 2020,  2:18am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
89384,Error when use quantize and torch.jit.script for transformerencoder,2020-07-16T00:45:18.060Z,0,54,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I tried to use the Pytorch build-in transformerencoder for my model training. In inference, I tried to quantize it and export it for libtorch in C++. I found there was some strange error when I use both quantize and torch.jit.script. However, if I only use quantize or torch.jit.script, it works well.<br/><NewLine>I am using Pytorch 1.5.1 and cuda 10.2</p><NewLine><p>The error is as follow when it calls torch.jit.script(module)</p><NewLine><p>RuntimeError:                                                                                                                                                                                                      method cannot be used as a value:                                                                                                                                                                                    File “/usr/local/lib/python3.6/dist-packages/torch/nn/modules/activation.py”, line 831                                                                                                                                           self.in_proj_weight, self.in_proj_bias,                                                                                                                                                                            self.bias_k, self.bias_v, self.add_zero_attn,                                                                                                                                                                      self.dropout, self.out_proj.weight, self.out_proj.bias,                                                                                                                                                                          ~~~~~~~~~~~~~~~~~~~~ &lt;— HERE                                                                                                                                                                       training=self.training,                                                                                                                                                                                            key_padding_mask=key_padding_mask, need_weights=need_weights,</p><NewLine><p>You could reproduce this error easily by running the following sample code.</p><NewLine><p>import torch                                                                                                                                                                                                                                                                                                                                                                                                                          class MyModule(torch.nn.Module):<br/><NewLine>def <strong>init</strong>(self, hidden_size, nhead):<br/><NewLine>super(MyModule, self).<strong>init</strong>()<br/><NewLine>encoder_layer = torch.nn.TransformerEncoderLayer(hidden_size, nhead=nhead,<br/><NewLine>dim_feedforward=hidden_size, dropout=0.1, activation=‘gelu’)<br/><NewLine>encoder_norm = torch.nn.LayerNorm(hidden_size)<br/><NewLine>self.encoder = torch.nn.TransformerEncoder(encoder_layer, 20, encoder_norm)<br/><NewLine>def forward(self, x):<br/><NewLine>out = self.encoder(out)<br/><NewLine>return out</p><NewLine><p>my_module = MyModule(1024, 8)<br/><NewLine>my_module = torch.quantization.quantize_dynamic(my_module, dtype=torch.qint8)<br/><NewLine>sm = torch.jit.script(my_module)<br/><NewLine>torch.jit.save(sm, ‘test.jit.model’)</p><NewLine></div>",https://discuss.pytorch.org/u/chenxie95,(XIE CHEN),chenxie95,"July 16, 2020, 12:46am",,,,,
89349,What is the most effective way to migrate an existing model to a newer version of Pytorch?,2020-07-15T16:04:04.646Z,0,49,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have some models currently in production, using torch’s C++ API and traced torchscripts, ala these instructions: <a href=""https://pytorch.org/docs/master/generated/torch.jit.trace.html"" rel=""nofollow noopener"">https://pytorch.org/docs/master/generated/torch.jit.trace.html</a><br/><NewLine>I also have the model’s state dicts saved away.<br/><NewLine>Overall this solution works great.</p><NewLine><p>The models and torchscripts were generated using pytorch 1.3<br/><NewLine>I want to update the next version of the system to pytorch 1.5</p><NewLine><p>Retraining is possible, but not preferred, and certainly not a long term solution.<br/><NewLine>Is there a procedure for updating my existing models, from either the saved torchscripts or the state dicts?</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/cytokinesis,,cytokinesis,"July 15, 2020,  6:39pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The easiest way is to instantiate the model in Python, load the state dict and trace the model again.<br/><NewLine>This is something to know, even though you can run the traced model without the source, you’ll regret not keeping it around at some point.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: July 15, 2020,  6:46pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
89286,Torchvision.ops.nms in torchscript,2020-07-15T08:39:10.099Z,2,142,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, As mentioned <a href=""https://pytorch.org/docs/stable/torchvision/ops.html"" rel=""nofollow noopener"">here</a> torchvision ops is currently not supported in torchscript. I wanted to know is there a timeline on when will torchscript support these functions.</p><NewLine></div>",https://discuss.pytorch.org/u/yrath,(Yuvraj),yrath,"July 15, 2020,  8:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The documentation might be slightly out of date if you compile both PyTorch and torchvision yourself or take the nightlies, because it seems to work for me:</p><NewLine><pre><code class=""lang-python"">In [1]: import torch, torchvision                                                                                                                                                                                  <NewLine><NewLine>In [2]: @torch.jit.script <NewLine>   ...: def fn(a, b): <NewLine>   ...:     return torchvision.ops.nms(a, b, 0.3) <NewLine>   ...:                                                                                                                                                                                                            <NewLine><NewLine>In [3]: fn.graph                                                                                                                                                                                                   <NewLine>Out[3]: <NewLine>graph(%a.1 : Tensor,<NewLine>      %b.1 : Tensor):<NewLine>  %5 : Function = prim::Constant[name=""nms""]()<NewLine>  %4 : float = prim::Constant[value=0.29999999999999999]() # &lt;ipython-input-2-cfe32fda8960&gt;:3:37<NewLine>  %6 : Tensor = prim::CallFunction(%5, %a.1, %b.1, %4) # &lt;ipython-input-2-cfe32fda8960&gt;:3:11<NewLine>  return (%6)<NewLine><NewLine>In [4]: fn(torch.randn(10, 4), torch.randn(10))                                                                                                                                                                    <NewLine>Out[4]: tensor([2, 8, 6, 7, 0, 9, 4, 5, 3, 1])<NewLine></code></pre><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/tom"">@tom</a> Thanks. It is working for me now in torchscript as well. But, the NMS now fails in the TRTserver.</p><NewLine><pre><code class=""lang-auto"">Unknown builtin op: torchvision::nms.<NewLine>Could not find any similar ops to torchvision::nms. This op may not exist or may not be currently supported in TorchScript.<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well, you might need to load the custom ops for that. I know nothing about TRTserver.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/yrath; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: July 15, 2020, 11:23am; <NewLine> REPLY_DATE 2: July 15, 2020,  3:41pm; <NewLine> REPLY_DATE 3: July 15, 2020,  3:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
89203,Tensorboard `add_graph`: visualize model partially composed from torch.distributions,2020-07-14T18:43:01.349Z,0,59,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m experimenting with a model that uses a GMM at the output, implemented using functionality from <code>torch.distributions</code>. Can this be visualized via tensorboard?</p><NewLine><p>When calling <code>writer.add_graph</code> on the model, I get the following error:</p><NewLine><pre><code class=""lang-auto"">/opt/conda/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py in add_graph(self, model, input_to_model, verbose)<NewLine>    705         if hasattr(model, 'forward'):<NewLine>    706             # A valid PyTorch model should have a 'forward' method<NewLine>--&gt; 707             self._get_file_writer().add_graph(graph(model, input_to_model, verbose))<NewLine>    708         else:<NewLine>    709             # Caffe2 models do not have the 'forward' method<NewLine><NewLine>/opt/conda/lib/python3.7/site-packages/torch/utils/tensorboard/_pytorch_graph.py in graph(model, args, verbose)<NewLine>    289             print(e)<NewLine>    290             print('Error occurs, No graph saved')<NewLine>--&gt; 291             raise e<NewLine>    292 <NewLine>    293     if verbose:<NewLine><NewLine>/opt/conda/lib/python3.7/site-packages/torch/utils/tensorboard/_pytorch_graph.py in graph(model, args, verbose)<NewLine>    283     with torch.onnx.set_training(model, False):  # TODO: move outside of torch.onnx?<NewLine>    284         try:<NewLine>--&gt; 285             trace = torch.jit.trace(model, args)<NewLine>    286             graph = trace.graph<NewLine>    287             torch._C._jit_pass_inline(graph)<NewLine><NewLine>/opt/conda/lib/python3.7/site-packages/torch/jit/__init__.py in trace(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, _force_outplace, _module_class, _compilation_unit)<NewLine>    873         return trace_module(func, {'forward': example_inputs}, None,<NewLine>    874                             check_trace, wrap_check_inputs(check_inputs),<NewLine>--&gt; 875                             check_tolerance, _force_outplace, _module_class)<NewLine>    876 <NewLine>    877     if (hasattr(func, '__self__') and isinstance(func.__self__, torch.nn.Module) and<NewLine><NewLine>/opt/conda/lib/python3.7/site-packages/torch/jit/__init__.py in trace_module(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, _force_outplace, _module_class, _compilation_unit)<NewLine>   1025             func = mod if method_name == ""forward"" else getattr(mod, method_name)<NewLine>   1026             example_inputs = make_tuple(example_inputs)<NewLine>-&gt; 1027             module._c._create_method_from_trace(method_name, func, example_inputs, var_lookup_fn, _force_outplace)<NewLine>   1028             check_trace_method = module._c._get_method(method_name)<NewLine>   1029 <NewLine><NewLine>RuntimeError: Tracer cannot infer type of (MixtureSameFamily(<NewLine>  Categorical(&lt;redacted&gt;),<NewLine>  MultivariateNormal(&lt;redacted&gt;)))<NewLine>:Only tensors and (possibly nested) tuples of tensors, lists, or dictsare supported as inputs or outputs of traced functions, but instead got value of type MixtureSameFamily.<NewLine></code></pre><NewLine><p>The last line clarifies in no uncertain terms <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/> . I am curious what workarounds there are.</p><NewLine></div>",https://discuss.pytorch.org/u/sid,,sid,"July 14, 2020,  6:43pm",,,,,
89105,Jit script failing for vgg16_bn,2020-07-14T07:04:25.204Z,0,45,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using the following model which is using vgg16 as base net</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine><NewLine>from basenet.vgg16_bn import vgg16_bn, init_weights<NewLine><NewLine>class double_conv(nn.Module):<NewLine>    def __init__(self, in_ch, mid_ch, out_ch):<NewLine>        super(double_conv, self).__init__()<NewLine>        self.conv = nn.Sequential(<NewLine>            nn.Conv2d(in_ch + mid_ch, mid_ch, kernel_size=1),<NewLine>            nn.BatchNorm2d(mid_ch),<NewLine>            nn.ReLU(inplace=True),<NewLine>            nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1),<NewLine>            nn.BatchNorm2d(out_ch),<NewLine>            nn.ReLU(inplace=True)<NewLine>        )<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.conv(x)<NewLine>        return x<NewLine><NewLine><NewLine>class CRAFT(nn.Module):<NewLine>    def __init__(self, pretrained=False, freeze=False):<NewLine>        super(CRAFT, self).__init__()<NewLine><NewLine>        """""" Base network """"""<NewLine>        self.basenet = vgg16_bn(pretrained, freeze)<NewLine><NewLine>        """""" U network """"""<NewLine>        self.upconv1 = double_conv(1024, 512, 256)<NewLine>        self.upconv2 = double_conv(512, 256, 128)<NewLine>        self.upconv3 = double_conv(256, 128, 64)<NewLine>        self.upconv4 = double_conv(128, 64, 32)<NewLine><NewLine>        num_class = 2<NewLine>        self.conv_cls = nn.Sequential(<NewLine>            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True),<NewLine>            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True),<NewLine>            nn.Conv2d(32, 16, kernel_size=3, padding=1), nn.ReLU(inplace=True),<NewLine>            nn.Conv2d(16, 16, kernel_size=1), nn.ReLU(inplace=True),<NewLine>            nn.Conv2d(16, num_class, kernel_size=1),<NewLine>        )<NewLine><NewLine>        init_weights(self.upconv1.modules())<NewLine>        init_weights(self.upconv2.modules())<NewLine>        init_weights(self.upconv3.modules())<NewLine>        init_weights(self.upconv4.modules())<NewLine>        init_weights(self.conv_cls.modules())<NewLine>        <NewLine>    def forward(self, x):<NewLine>        """""" Base network """"""<NewLine>        sources = self.basenet(x)<NewLine><NewLine>        """""" U network """"""<NewLine>        y = torch.cat([sources[0], sources[1]], dim=1)<NewLine>        y = self.upconv1(y)<NewLine><NewLine>        y = F.interpolate(y, size=sources[2].size()[2:], mode='bilinear', align_corners=False)<NewLine>        y = torch.cat([y, sources[2]], dim=1)<NewLine>        y = self.upconv2(y)<NewLine><NewLine>        y = F.interpolate(y, size=sources[3].size()[2:], mode='bilinear', align_corners=False)<NewLine>        y = torch.cat([y, sources[3]], dim=1)<NewLine>        y = self.upconv3(y)<NewLine><NewLine>        y = F.interpolate(y, size=sources[4].size()[2:], mode='bilinear', align_corners=False)<NewLine>        y = torch.cat([y, sources[4]], dim=1)<NewLine>        feature = self.upconv4(y)<NewLine><NewLine>        y = self.conv_cls(feature)<NewLine></code></pre><NewLine><p>when I am using</p><NewLine><pre><code class=""lang-auto"">net_scripted = torch.jit.script(net)<NewLine></code></pre><NewLine><p>I am getting</p><NewLine><pre><code class=""lang-auto"">NotSupportedError: Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults:<NewLine>  File ""/home/ao-collab/anaconda3/envs/torch/lib/python3.8/collections/__init__.py"", line 313<NewLine>def namedtuple(typename, field_names, *, rename=False, defaults=None, module=None):<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/abhiksark,(Abhik Sarkar),abhiksark,"July 14, 2020,  7:04am",,,,,
88313,Don&rsquo;t have an op for aten::uniform but it isn&rsquo;t a special case,2020-07-08T02:47:36.750Z,1,110,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Using tensorwatch (0.9.1) to draw network:</p><NewLine><pre><code class=""lang-auto"">tw.draw_model(net, (1, 3, 512, 512))<NewLine></code></pre><NewLine><p>PyTorch(1.5.1), torchvision(0.6.1)</p><NewLine><hr/><NewLine><p>RuntimeError                              Traceback (most recent call last)<br/><NewLine> in <br/><NewLine>----&gt; 1 tw.draw_model(net, (1, 3, 512, 512))</p><NewLine><p>g:\medicine\winvenv\lib\site-packages\tensorwatch_<em>init</em>_.py in draw_model(model, input_shape, orientation, png_filename)<br/><NewLine>33 def draw_model(model, input_shape=None, orientation=‘TB’, png_filename=None): <span class=""hashtag"">#orientation</span> = ‘LR’ for landscpe<br/><NewLine>34     from .model_graph.hiddenlayer import pytorch_draw_model<br/><NewLine>—&gt; 35     g = pytorch_draw_model.draw_graph(model, input_shape)<br/><NewLine>36     return g<br/><NewLine>37</p><NewLine><p>g:\medicine\winvenv\lib\site-packages\tensorwatch\model_graph\hiddenlayer\pytorch_draw_model.py in draw_graph(model, args)<br/><NewLine>33         args = torch.ones(args)<br/><NewLine>34<br/><NewLine>—&gt; 35     dot = draw_img_classifier(model, args)<br/><NewLine>36     return DotWrapper(dot)<br/><NewLine>37</p><NewLine><p>g:\medicine\winvenv\lib\site-packages\tensorwatch\model_graph\hiddenlayer\pytorch_draw_model.py in draw_img_classifier(model, dataset, display_param_nodes, rankdir, styles, input_shape)<br/><NewLine>61     try:<br/><NewLine>62         non_para_model = distiller.make_non_parallel_copy(model)<br/><NewLine>—&gt; 63         g = SummaryGraph(non_para_model, dummy_input)<br/><NewLine>64<br/><NewLine>65         return sgraph2dot(g, display_param_nodes, rankdir, styles)</p><NewLine><p>g:\medicine\winvenv\lib\site-packages\tensorwatch\model_graph\hiddenlayer\summary_graph.py in <strong>init</strong>(self, model, dummy_input, apply_scope_name_workarounds)<br/><NewLine>94                 nodes = graph.nodes()<br/><NewLine>95             elif hasattr(jit, ‘_get_trace_graph’):<br/><NewLine>—&gt; 96                 trace, _ = jit._get_trace_graph(model_clone, dummy_input, _force_outplace=True)<br/><NewLine>97                 graph = trace<br/><NewLine>98                 nodes = graph.nodes()</p><NewLine><p>g:\medicine\winvenv\lib\site-packages\torch\jit_<em>init</em>_.py in _get_trace_graph(f, args, kwargs, _force_outplace, return_inputs, _return_inputs_states)<br/><NewLine>276     if not isinstance(args, tuple):<br/><NewLine>277         args = (args,)<br/><NewLine>–&gt; 278     outs = ONNXTracedModule(f, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)<br/><NewLine>279     return outs<br/><NewLine>280</p><NewLine><p>g:\medicine\winvenv\lib\site-packages\torch\nn\modules\module.py in <strong>call</strong>(self, *input, **kwargs)<br/><NewLine>548             result = self._slow_forward(*input, **kwargs)<br/><NewLine>549         else:<br/><NewLine>–&gt; 550             result = self.forward(*input, **kwargs)<br/><NewLine>551         for hook in self._forward_hooks.values():<br/><NewLine>552             hook_result = hook(self, input, result)</p><NewLine><p>g:\medicine\winvenv\lib\site-packages\torch\jit_<em>init</em>_.py in forward(self, *args)<br/><NewLine>359             in_vars + module_state,<br/><NewLine>360             _create_interpreter_name_lookup_fn(),<br/><NewLine>–&gt; 361             self._force_outplace,<br/><NewLine>362         )<br/><NewLine>363</p><NewLine><p>RuntimeError: 0 INTERNAL ASSERT FAILED at …\torch\csrc\jit\ir\alias_analysis.cpp:318, please report a bug to PyTorch. We don’t have an op for aten::uniform but it isn’t a special case.  Argument types: Tensor, float, float, None,</p><NewLine></div>",https://discuss.pytorch.org/u/brifuture,(jiawei),brifuture,"July 8, 2020,  2:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you create an issue on <a href=""https://github.com/pytorch/pytorch/issues"">GitHub</a> so that we could track it, please?<br/><NewLine>Would it be possible to provide a minimal code snippet to reproduce this issue?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Pardon me, I cannot replicate the errors on my project.<br/><NewLine>I changed the model structure, recreate the virtualenv envrionment, then I run the jupyter-notebook a few seconds ago and got all thing worked right, the model could be visualized, displaed on jupyter-notebook or saved as <code>.png</code> format on my disk.<br/><NewLine>I’m not sure whether there was wrong with the installation of <code>pytorch</code> or flaws existed in the model at that time.<br/><NewLine>But I’m sure the <code>torchsummary</code> could be executed to display the network channels and sizes any time.</p><NewLine><p>As the problem disappeared, shall I create an issue on GitHub?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""88313"" data-username=""brifuture""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/brifuture/40/17220_2.png"" width=""20""/> brifuture:</div><NewLine><blockquote><NewLine><p>As the problem disappeared, shall I create an issue on GitHub?</p><NewLine></blockquote><NewLine></aside><NewLine><p>I don’t think it would be helpful, if you (and thus we) cannot reproduce it.<br/><NewLine>In case you are running into this issue again and can reproduce it, please create an issue. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/brifuture; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 9, 2020,  2:43am; <NewLine> REPLY_DATE 2: July 10, 2020,  8:45am; <NewLine> REPLY_DATE 3: July 12, 2020,  2:14am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
88451,Torch.jit.is_scripting and handle_torch_function,2020-07-08T21:53:32.857Z,1,210,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I observed in PyTorch’s source code some functions that follow the pattern below (for example <a href=""https://github.com/pytorch/pytorch/blob/5e03a1e9261e86d3ee62481ef40d98ca5f09646c/torch/nn/functional.py#L3915"" rel=""nofollow noopener"">this one</a>):</p><NewLine><pre><code class=""lang-auto"">def my_function(a, b, c, d=None, e=None):<NewLine>    if not torch.jit.is_scripting():<NewLine>        tens_ops = (a, b, c)<NewLine>        if any([type(t) is not Tensor for t in tens_ops]) and has_torch_function(tens_ops):<NewLine>            return handle_torch_function(my_function , a, b, c, d=d, e=e)<NewLine>   # something else here<NewLine></code></pre><NewLine><p>Could anybody please tell me what’s going on there? In particular, what do <code>torch.jit.is_scripting</code> and <code>handle_torch_function</code> do?</p><NewLine><p>And <strong>more importantly</strong>: Where can I learn those things in detail so that I can myself write similar custom functions and stop asking stupid questions like these? <img alt="":stuck_out_tongue:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/stuck_out_tongue.png?v=9"" title="":stuck_out_tongue:""/> It seems that these are about <a href=""https://pytorch.org/docs/stable/jit.html?highlight=torch.jit#module-torch.jit"" rel=""nofollow noopener"">TorchScript</a>, but I wanted to ask just in case, before diving into that.</p><NewLine><p>Thank you very much in advance for your help!</p><NewLine></div>",https://discuss.pytorch.org/u/f10w,,f10w,"July 8, 2020,  9:53pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>torch.jit.is_scripting()</code> can be used as a guard for Python-only methods, which are not exportable.<br/><NewLine>Since the JIT cannot export all arbitrary Python objects, you could thus use different “paths” in your code for the Python execution and the scripting of the mdoel.<br/><NewLine><code>handle_torch_function</code> sounds like a custom function definition.</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""88451"" data-username=""f10w""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/f10w/40/1352_2.png"" width=""20""/> f10w:</div><NewLine><blockquote><NewLine><p>Where can I learn those things in detail so that I can myself write similar custom functions and stop asking stupid questions like these?</p><NewLine></blockquote><NewLine></aside><NewLine><p>As these features are currently being developed, the documentation and tutorials might not be up to date, so please ask these kind of questions here. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine><p><code>torch.jit.is_scripting</code> can be found in the current <a href=""https://pytorch.org/docs/master/jit_language_reference.html#torch.jit.is_scripting"">master docs</a>, but might also be updated before the next release.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot, <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>! Your answer is very helpful, as always.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/f10w; <NewLine> ,"REPLY_DATE 1: July 10, 2020,  3:40pm; <NewLine> REPLY_DATE 2: July 10, 2020,  3:40pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
88600,How to use TorchScript with Dropout for training?,2020-07-09T18:53:35.887Z,1,78,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My model is:</p><NewLine><pre><code class=""lang-auto"">import torch.nn as nn<NewLine><NewLine>class Net(nn.Module):<NewLine>    def __init__(self, num_input, num_hidden, num_classes, dropout,<NewLine>                 activation='tanh'):<NewLine>        super(Net, self).__init__()<NewLine>        self.dropout = nn.Dropout(dropout)<NewLine>        self.fc1 = nn.Linear(num_input, num_hidden)<NewLine>        self.fc2 = nn.Linear(num_hidden, num_classes)<NewLine><NewLine>        if activation == 'tanh':<NewLine>            self.activation_f = torch.tanh<NewLine>        elif activation == 'relu':<NewLine>            self.activation_f = torch.relu<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.activation_f(self.fc1(x))<NewLine>        x = self.dropout(x)<NewLine>        x = self.fc2(x)<NewLine>        return x<NewLine></code></pre><NewLine><p>I call my model for instance as:</p><NewLine><pre><code class=""lang-auto"">model = Net(14,512,2,0.2).to(device)<NewLine></code></pre><NewLine><p>However once I use <code>TorchScript</code> as:</p><NewLine><pre><code class=""lang-auto"">traced_model = torch.jit.trace(model, torch.zeros([1, 14], dtype=torch.float))<NewLine></code></pre><NewLine><p>I receive the following error:</p><NewLine><pre><code class=""lang-auto"">IndexError: The shape of the mask [2] at index 0 does not match the shape of the indexed tensor [1, 2] at index 0<NewLine></code></pre><NewLine><p>I know that if I use <code>model.eval()</code> I don’t receive any error BUT I want to use my model for training and not evaluation. Does anybody know any solution or workaround for such problem?</p><NewLine><p>PS: I am using <code>PyTorch</code> version <code>1.4</code>.</p><NewLine></div>",https://discuss.pytorch.org/u/fermat97,(Pierre),fermat97,"July 9, 2020,  6:53pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am unable to reproduce this error with <code>Python 3.8.3</code> and <code>pytorch-1.4.0</code> from <code>conda</code>.</p><NewLine><p>Based on your error message, it seems like your input shape might be the problem. Try using an input with shape <code>[14]</code> as the input during tracing:</p><NewLine><pre><code class=""lang-auto"">traced_model = torch.jit.trace(model, torch.zeros([14], dtype=torch.float))<NewLine></code></pre><NewLine><p>Stepping back a bit, I would advise you to use scripting (<code>torch.jit.script</code>) rather than tracing for this use case. The reason is that control flow is not visible to tracing and from what I remember, the backward pass for dropout predicates whether or not the incoming gradient should be backpropagated based on whether the corresponding input was passed through during the forward pass.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, but it is not possible to take input of size <code>[14]</code>, where is the batch dimension (<code>[1,14]</code>)? I don’t think the dimension is the problem since once I fix the dropout rate to <code>0.0</code> then there is no error message and it works just fine.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I still cannot reproduce your error. Can you provide more details on your setup? There is an environment details collection script in the PyTorch repository.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SplitInfinity; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/fermat97; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/SplitInfinity; <NewLine> ,"REPLY_DATE 1: July 9, 2020, 10:23pm; <NewLine> REPLY_DATE 2: July 9, 2020, 11:32pm; <NewLine> REPLY_DATE 3: July 9, 2020, 11:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
40324,Help! Tracing fails because of &ldquo;parameter sharing&rdquo;?,2019-03-19T21:46:24.946Z,1,1246,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Folks, <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=7"" title="":slight_smile:""/></p><NewLine><p>I’m trying to export a model to cpp via the tracing method (Following <a href=""https://pytorch.org/tutorials/advanced/cpp_export.html#converting-to-torch-script-via-tracing"" rel=""nofollow noopener"">this</a> but with a more complex model), but running “torch.jit.trace(model, input_x)” fails with exception “TracedModules don’t support parameter sharing between modules”.</p><NewLine><p>Unfortunately, there is no indication as to which modules or which parameters are shared. And I have no idea how to find out. How can I find out where it hangs?</p><NewLine><p>(FYI, the module I’m trying to export is this one: <a href=""https://github.com/ternaus/robot-surgery-segmentation/blob/master/models.py#L126"" rel=""nofollow noopener"">Code on Github</a>)</p><NewLine><p>I’m grateful for any pointer towards a solution (I also tried the annotation method but also failed <img alt="":confused:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=7"" title="":confused:""/> I guess some of the model’s modules don’t support that).</p><NewLine></div>",https://discuss.pytorch.org/u/torchie,,torchie,"March 19, 2019, 10:03pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not sure about the tracing - but i was able to script it successfully with the following</p><NewLine><pre><code class=""lang-auto"">  class DecoderBlockLinkNet(torch.jit.ScriptModule):<NewLine>            def __init__(self, in_channels, n_filters):<NewLine>                super().__init__()<NewLine><NewLine>                self.relu = nn.ReLU(inplace=True)<NewLine><NewLine>                # B, C, H, W -&gt; B, C/4, H, W<NewLine>                self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1)<NewLine>                self.norm1 = nn.BatchNorm2d(in_channels // 4)<NewLine><NewLine>                # B, C/4, H, W -&gt; B, C/4, 2 * H, 2 * W<NewLine>                self.deconv2 = nn.ConvTranspose2d(in_channels // 4, in_channels // 4, kernel_size=4,<NewLine>                                                  stride=2, padding=1, output_padding=0)<NewLine>                self.norm2 = nn.BatchNorm2d(in_channels // 4)<NewLine><NewLine>                # B, C/4, H, W -&gt; B, C, H, W<NewLine>                self.conv3 = nn.Conv2d(in_channels // 4, n_filters, 1)<NewLine>                self.norm3 = nn.BatchNorm2d(n_filters)<NewLine><NewLine>            @torch.jit.script_method<NewLine>            def forward(self, x):<NewLine>                x = self.conv1(x)<NewLine>                x = self.norm1(x)<NewLine>                x = self.relu(x)<NewLine>                x = self.deconv2(x)<NewLine>                x = self.norm2(x)<NewLine>                x = self.relu(x)<NewLine>                x = self.conv3(x)<NewLine>                x = self.norm3(x)<NewLine>                x = self.relu(x)<NewLine>                return x<NewLine><NewLine>        class UNet16(torch.jit.ScriptModule):<NewLine>            __constants__ = [""conv1"", ""conv2"", ""conv3"", ""conv4"", ""conv5""]<NewLine><NewLine>            def __init__(self, num_classes=1, num_filters=32, pretrained=False):<NewLine>                """"""<NewLine>                :param num_classes:<NewLine>                :param num_filters:<NewLine>                :param pretrained:<NewLine>                    False - no pre-trained network used<NewLine>                    True - encoder pre-trained with VGG11<NewLine>                """"""<NewLine>                super().__init__()<NewLine>                self.num_classes = num_classes<NewLine><NewLine>                self.pool = nn.MaxPool2d(2, 2)<NewLine><NewLine>                self.encoder = torchvision.models.vgg16(pretrained=pretrained).features<NewLine><NewLine>                self.relu = nn.ReLU(inplace=True)<NewLine><NewLine>                self.conv1 = nn.Sequential(self.encoder[0],<NewLine>                                           self.relu,<NewLine>                                           self.encoder[2],<NewLine>                                           self.relu)<NewLine><NewLine>                self.conv2 = nn.Sequential(self.encoder[5],<NewLine>                                           self.relu,<NewLine>                                           self.encoder[7],<NewLine>                                           self.relu)<NewLine><NewLine>                self.conv3 = nn.Sequential(self.encoder[10],<NewLine>                                           self.relu,<NewLine>                                           self.encoder[12],<NewLine>                                           self.relu,<NewLine>                                           self.encoder[14],<NewLine>                                           self.relu)<NewLine><NewLine>                self.conv4 = nn.Sequential(self.encoder[17],<NewLine>                                           self.relu,<NewLine>                                           self.encoder[19],<NewLine>                                           self.relu,<NewLine>                                           self.encoder[21],<NewLine>                                           self.relu)<NewLine><NewLine>                self.conv5 = nn.Sequential(self.encoder[24],<NewLine>                                           self.relu,<NewLine>                                           self.encoder[26],<NewLine>                                           self.relu,<NewLine>                                           self.encoder[28],<NewLine>                                           self.relu)<NewLine><NewLine>                self.center = DecoderBlock(512, num_filters * 8 * 2, num_filters * 8)<NewLine><NewLine>                self.dec5 = DecoderBlock(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8)<NewLine>                self.dec4 = DecoderBlock(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8)<NewLine>                self.dec3 = DecoderBlock(256 + num_filters * 8, num_filters * 4 * 2, num_filters * 2)<NewLine>                self.dec2 = DecoderBlock(128 + num_filters * 2, num_filters * 2 * 2, num_filters)<NewLine>                self.dec1 = ConvRelu(64 + num_filters, num_filters)<NewLine>                self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)<NewLine><NewLine>            @torch.jit.script_method<NewLine>            def forward(self, x):<NewLine>                conv1 = self.conv1(x)<NewLine>                conv2 = self.conv2(self.pool(conv1))<NewLine>                conv3 = self.conv3(self.pool(conv2))<NewLine>                conv4 = self.conv4(self.pool(conv3))<NewLine>                conv5 = self.conv5(self.pool(conv4))<NewLine><NewLine>                center = self.center(self.pool(conv5))<NewLine><NewLine>                dec5 = self.dec5(torch.cat([center, conv5], 1))<NewLine><NewLine>                dec4 = self.dec4(torch.cat([dec5, conv4], 1))<NewLine>                dec3 = self.dec3(torch.cat([dec4, conv3], 1))<NewLine>                dec2 = self.dec2(torch.cat([dec3, conv2], 1))<NewLine>                dec1 = self.dec1(torch.cat([dec2, conv1], 1))<NewLine><NewLine>                if self.num_classes &gt; 1:<NewLine>                    x_out = F.log_softmax(self.final(dec1), dim=1)<NewLine>                else:<NewLine>                    x_out = self.final(dec1)<NewLine><NewLine>                return x_out<NewLine><NewLine>        model = UNet16()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your answer! I think you may have posted the wrong method: The model contains DecoderBlock but you posted DecoderBlockLinkNet.</p><NewLine><p>In the meantime, I was lucky to stumble upon this trick: <a href=""https://github.com/pytorch/pytorch/issues/8392#issuecomment-431863763"" rel=""nofollow noopener"">Wrap parameters in deepcopy</a>. I tried it and wrapped all references to self.encoder with copy.deepcopy and it now it traces and exports! Seems like the shared parameter was actually self.encoder!</p><NewLine><p>I’m still verifying that the exported model is actually working as expected but am optimistic. I’ll write again soon…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The deepcopy-“trick” worked. So my problem is solved! <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>hello,in my net the deepcopy method doesn’t work,could you show me where to add deepcopy,thank you</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>i use tensorboardx to view the structure of a multi-task model. but faild with this error、、、</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/torchie; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/torchie; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/redrumshinning; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/karl7; <NewLine> ,"REPLY_DATE 1: March 20, 2019,  9:35pm; <NewLine> REPLY_DATE 2: March 20, 2019, 11:49pm; <NewLine> REPLY_DATE 3: March 29, 2019,  5:35pm; <NewLine> REPLY_DATE 4: June 21, 2019,  2:23am; <NewLine> REPLY_DATE 5: July 9, 2020,  6:32am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
42906,Pytorch jit vs tensorRT,2019-04-18T06:12:58.283Z,2,747,"<div class=""post"" itemprop=""articleBody""><NewLine><p>has anyone compared the throughput of a model optimized by both jit and tensorRT?</p><NewLine></div>",https://discuss.pytorch.org/u/0xFFFFFFFF,(Joong Kun Lee),0xFFFFFFFF,"April 18, 2019,  6:13am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems to depend on the specific network. The biggest speedup I’ve seen was close to three times as fast as PyTorch. The lowest was about one and a quarter times as fast.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for sharing. That’s not too bad. could you share which models showed such speedup? and perhaps the speedup data on some models you have tried? I would really appreciate any details.<br/><NewLine>Thanks again.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>TensorRT much faster</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>jit and trt are two different things.<br/><NewLine>our team are looking into pytorch for a long time. jit is front-end while trt is back-end.<br/><NewLine>Always, jit is from python. it optimizes pytorch codes and tries to merge some  ops before running the forward. If you dig it, you will find jit and eager call the same op set and just little diff.<br/><NewLine>However, trt is another accelerated engine, which depends on Nvidia’s GPU. trt will fuse the ops as possible as it can. fused kernel can reduce cost of discrete kernel calls.<br/><NewLine>besides, trt has other exiting features.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/alanzhai219"">@alanzhai219</a>,<br/><NewLine>Is it possible to optimize a model using torch2trt then scripting the optimized model using torch script/trace? If yes, Will I get better optimization?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>It is not recommended. torch2trt is designed to help developers deploy their script/trace model in TensorRT.<br/><NewLine>In detail, script/trace just interpreters original PyTorch into IR graph and then torch2trt maps and fuses such graph in trt.<br/><NewLine>I never try the opposite flow. If you succeed, please let me know.<br/><NewLine>Thanks,<br/><NewLine>Alan Zhai</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/KevNull; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/0xFFFFFFFF; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Rizhao_Cai; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/alanzhai219; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/baheytharwat; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/alanzhai219; <NewLine> ,"REPLY_DATE 1: April 18, 2019, 12:08pm; <NewLine> REPLY_DATE 2: May 7, 2019,  7:29am; <NewLine> REPLY_DATE 3: February 12, 2020, 11:14am; <NewLine> REPLY_DATE 4: March 1, 2020,  9:42am; <NewLine> REPLY_DATE 5: July 7, 2020,  7:27pm; <NewLine> REPLY_DATE 6: July 9, 2020,  3:36am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
88433,"LSTM, &lsquo;method&rsquo; object is not subscriptable",2020-07-08T19:23:25.178Z,0,159,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I’m implementing an LSTM to predict today’s stock price using the past 10 days’ close price.<br/><NewLine>Therefore, my input is [batch_size, sequence_len = 10, input_size = 1] since there is only one feature every day.<br/><NewLine>Then I set batch_size = 50 and implement a train_loader with TensorDataset and DataLoader.<br/><NewLine>My LSTM model is defined as</p><NewLine><pre><code class=""lang-auto"">class LSTM(nn.Module):<NewLine>    def __init__(self, input_dim, hidden_dim, batch_size, output_dim, num_layers, dropout = 0.1):<NewLine>        super(LSTM, self).__init__()<NewLine>        self.input_dim = input_dim<NewLine>        self.hidden_dim = hidden_dim<NewLine>        self.batch_size = batch_size<NewLine>        self.num_layers = num_layers<NewLine>        self.dropout = dropout<NewLine>        self.output_dim = output_dim<NewLine><NewLine>        # Define the LSTM layer<NewLine>        self.lstm = nn.LSTM(input_size = input_dim, hidden_size = hidden_dim, num_layers = num_layers, dropout = dropout, batch_first = True)<NewLine>        <NewLine>        # Define the output layer, fully connected<NewLine>        self.linear = nn.Linear(hidden_dim, output_dim)<NewLine>              <NewLine>    def init_hidden(self):<NewLine>        # initialise our hidden state<NewLine>        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),<NewLine>                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))<NewLine> <NewLine>    def forward(self, x):<NewLine>        # Forward pass through LSTM layer<NewLine>        # shape of out: [input_size, batch_size, hidden_dim]<NewLine>        # shape of self.hidden: (a, b), where a and b both have shape (num_layers, batch_size, hidden_dim).<NewLine>        <NewLine>        out, self.hidden = self.lstm(x, self.init_hidden)<NewLine>        y_pred = self.linear(out[:, -1, :]) <NewLine>        return y_pred<NewLine></code></pre><NewLine><p>And when I train the model,</p><NewLine><pre><code class=""lang-auto"">INPUT_SIZE = 1 # number of features<NewLine>HIDDEN_SIZE = 64<NewLine>NUM_LAYERS = 3<NewLine>OUTPUT_SIZE = 1<NewLine><NewLine>learning_rate = 0.001<NewLine>num_epochs = 150 <NewLine>model = LSTM(INPUT_SIZE, HIDDEN_SIZE, batch_size, OUTPUT_SIZE, NUM_LAYERS)<NewLine>loss_fn = torch.nn.MSELoss()<NewLine>optimiser = torch.optim.Adam(model.parameters(), lr = learning_rate)<NewLine>model.hidden = model.init_hidden()<NewLine>model.train()  <NewLine>for local_batch, local_labels in train_loader:<NewLine>    # Transfer to GPU<NewLine>    local_batch = local_batch.float().to(device)<NewLine>    local_labels = local_labels.float().to(device)<NewLine><NewLine>    print(local_batch.shape) # torch.Size([50, 10, 1])<NewLine>    print(local_labels.shape) # torch.Size([50, 1])<NewLine><NewLine>    # Forward pass<NewLine>    y_pred = model(local_batch)<NewLine>    loss = loss_fn(y_pred, local_labels)<NewLine><NewLine>    optimiser.zero_grad()<NewLine><NewLine>    # Backward pass<NewLine>    loss.backward()<NewLine><NewLine>    # Update parameters<NewLine>    optimiser.step()<NewLine><NewLine></code></pre><NewLine><p>I got this error <code>in y_pred = model(local_batch)</code>  at /usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py in <code>check_forward_args(self, input, hidden, batch_sizes)</code>:</p><NewLine><pre><code class=""lang-auto"">-&gt; 522         self.check_hidden_size(hidden[0], expected_hidden_size,<NewLine>    523                                'Expected hidden[0] size {}, got {}')<NewLine></code></pre><NewLine><p>TypeError: ‘method’ object is not subscriptable</p><NewLine><p>I’m new to LSTM, any hints are appreciable!</p><NewLine></div>",https://discuss.pytorch.org/u/Kieran,,Kieran,"July 8, 2020,  8:09pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""88433"" data-username=""Kieran""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/kieran/40/22786_2.png"" width=""20""/> Kieran:</div><NewLine><blockquote><NewLine><p><code>out, self.hidden = self.lstm(x, self.init_hidden)</code></p><NewLine></blockquote><NewLine></aside><NewLine><p>this passes the <code>init_hidden</code> method, you likely want <code>init_hidden()</code> instead. (Why you would need to do this when 0 initial hidden it is implicit in PyTorch, I don’t know.)</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: July 9, 2020, 12:06am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
88237,Symbolic registry for custom Python operations,2020-07-07T13:28:57.619Z,0,66,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Having a custom C++ operation it’s possible to register a symbolic function for it using</p><NewLine><p><code>register_op(name, symbolic, namespace, opset)</code></p><NewLine><p>With custom Python operation it’s possible to wrap it with an <code>autograd.Function</code> having static <code>symbolic</code> method, but it’s much less agile in term of different opsets’ support. Is there a way to register symbolic for a Python operation in the same way, as it’s done for C++ ones.</p><NewLine></div>",https://discuss.pytorch.org/u/druzhkov.paul,(Pavel),druzhkov.paul,"July 7, 2020,  1:28pm",,,,,
88162,Is it possible to back propagate in a TorchScript?,2020-07-06T22:39:38.321Z,0,104,"<div class=""post"" itemprop=""articleBody""><NewLine><p>PyTorch 1.5.0</p><NewLine><p>Hello, I am using PyTorch to minimise a cost function. No training involved. I was wondering whether using TorchScript rather than pure Python could have led to a speed up. However, I am not sure whether it is possible to back propagate in TorchScript.</p><NewLine><p>Let us consider the following toy class:</p><NewLine><pre><code class=""lang-auto"">class Example(nn.Module):<NewLine>  def forward(self, x):<NewLine>    y = torch.tensor([0], dtype=x.dtype)<NewLine>    y.requires_grad = True<NewLine>    return y<NewLine></code></pre><NewLine><p>If I call <code>example_scripted = torch.jit.script(Example())</code>, I get the following error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError:<NewLine>Tried to set an attribute: grad on a non-class: Tensor:<NewLine></code></pre><NewLine><p>The issue seems the <code>requires_grad</code>. I am wondering whether it is possible to use TorchScript to back propagate. Is it?</p><NewLine><p>Please see also <a href=""https://github.com/pytorch/pytorch/issues/40561"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/40561</a></p><NewLine><p>Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/rossimat,(Mattia),rossimat,"July 6, 2020, 10:40pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah, backprop works, but some python code won’t compile. In your (contrived) case,  tensor(…).requires_grad_(True) would compile, but you don’t usually need to be explicit about requires_grad, as it propagates with operations, and there is also .detach().</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes you can train see this book:<br/><NewLine></p><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://pytorch.org/favicon.ico?"" width=""32""/><NewLine><a href=""https://pytorch.org/deep-learning-with-pytorch"" rel=""nofollow noopener"" target=""_blank"">pytorch.org</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""500"" src=""https://pytorch.org/assets/images/pytorch-logo.png"" width=""500""/><NewLine><h3><a href=""https://pytorch.org/deep-learning-with-pytorch"" rel=""nofollow noopener"" target=""_blank"">PyTorch</a></h3><NewLine><p>An open source deep learning platform that provides a seamless path from research prototyping to production deployment.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/e1fa81909363d4c18ebcb5001f6c377570a543e5"" href=""https://discuss.pytorch.org/uploads/default/original/3X/e/1/e1fa81909363d4c18ebcb5001f6c377570a543e5.jpeg"" title=""Picture1""><img alt=""Picture1"" data-base62-sha1=""wf68xoRjYvz04iKyQ5UOSEx3lxr"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/1/e1fa81909363d4c18ebcb5001f6c377570a543e5_2_10x10.png"" height=""137"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/1/e1fa81909363d4c18ebcb5001f6c377570a543e5_2_690x137.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/1/e1fa81909363d4c18ebcb5001f6c377570a543e5_2_690x137.jpeg, https://discuss.pytorch.org/uploads/default/optimized/3X/e/1/e1fa81909363d4c18ebcb5001f6c377570a543e5_2_1035x205.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/e/1/e1fa81909363d4c18ebcb5001f6c377570a543e5_2_1380x274.jpeg 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Picture1</span><span class=""informations"">1552×309 109 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/googlebot; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/lev; <NewLine> ,"REPLY_DATE 1: July 8, 2020,  5:56pm; <NewLine> REPLY_DATE 2: July 7, 2020, 12:16pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
88160,Torch scripting not working,2020-07-06T21:12:36.043Z,0,53,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Here is my model definition:</p><NewLine><pre><code class=""lang-auto"">class ResNetTC8(nn.Module):<NewLine>    def __init__(self, n_classes, n_channels, n_mfcc):        <NewLine>        super().__init__()<NewLine>        conv_size = (9, 1)<NewLine>        self.conv0 = nn.Conv2d(n_mfcc, n_channels[0], (3, 1), padding=(1, 0), bias=False)<NewLine>        self.conv1 = nn.Conv2d(n_channels[0], n_channels[1], conv_size, padding=((101+8)//2, 0), bias=False, stride=2) <NewLine>        self.conv2 = nn.Conv2d(n_channels[1], n_channels[1], conv_size, padding=(9//2, 0), bias=False)<NewLine>        self.skip_conv1 = nn.Conv2d(n_channels[0], n_channels[1], 1, padding=((101)//2, 0), bias=False, stride=2)<NewLine>        self.bn1 = nn.BatchNorm2d(n_channels[1])<NewLine>        self.bn2 = nn.BatchNorm2d(n_channels[1])<NewLine>        self.skip_bn1 = nn.BatchNorm2d(n_channels[1])<NewLine>        self.conv3 = nn.Conv2d(n_channels[1], n_channels[2], conv_size, padding=((101+8)//2, 0), bias=False, stride=2)<NewLine>        self.conv4 = nn.Conv2d(n_channels[2], n_channels[2], conv_size, padding=(9//2, 0), bias=False)<NewLine>        self.skip_conv2 = nn.Conv2d(n_channels[1], n_channels[2], 1, padding=((101)//2, 0), bias=False, stride=2)<NewLine>        self.bn3 = nn.BatchNorm2d(n_channels[2])<NewLine>        self.bn4 = nn.BatchNorm2d(n_channels[2])<NewLine>        self.skip_bn2 = nn.BatchNorm2d(n_channels[2])<NewLine>        self.conv5 = nn.Conv2d(n_channels[2], n_channels[3], conv_size, padding=((101+8)//2, 0), bias=False, stride=2)<NewLine>        self.conv6 = nn.Conv2d(n_channels[2], n_channels[3], conv_size, padding=(9//2, 0), bias=False)<NewLine>        self.skip_conv3 = nn.Conv2d(n_channels[2], n_channels[3], 1, padding=((101)//2, 0), bias=False, stride=2)<NewLine>        self.bn5 = nn.BatchNorm2d(n_channels[3])<NewLine>        self.bn6 = nn.BatchNorm2d(n_channels[3])<NewLine>        self.skip_bn3 = nn.BatchNorm2d(n_channels[3])<NewLine>        self.avg = nn.AvgPool2d((101, 1))<NewLine>        self.dropout = nn.Dropout()<NewLine>        self.output = nn.Linear(n_channels[3], 36) <NewLine>        <NewLine>    def forward(self, x):<NewLine>        x = x.reshape([-1, x.shape[2], x.shape[1], 1])<NewLine>        x = self.conv0(x)<NewLine>        y0 = self.bn1(F.relu(self.conv1(x)))<NewLine>        x = self.bn2(F.relu(self.conv2(y0))) + self.skip_bn1(F.relu(self.skip_conv1(x)))<NewLine>        y1 =  self.bn3(F.relu(self.conv3(x)))<NewLine>        x = self.bn4(F.relu(self.conv4(y1))) + self.skip_bn2(F.relu(self.skip_conv2(x)))<NewLine>        y2 =  self.bn5(F.relu(self.conv5(x)))<NewLine>        x = self.bn6(F.relu(self.conv6(y1))) + self.skip_bn3(F.relu(self.skip_conv3(x)))<NewLine>        x = self.dropout(self.avg(x)).squeeze()<NewLine>        return self.output(x)<NewLine></code></pre><NewLine><p>Model trains without error and scripting yields no errors in Python. However, running using Libtorch 1.5 on Xcode yields the following error:</p><NewLine><pre><code class=""lang-auto"">The following operation failed in the TorchScript interpreter.<NewLine>Traceback of TorchScript, serialized code (most recent call last):<NewLine>  File ""code/__torch__/torch/nn/modules/module/___torch_mangle_67.py"", line 55, in forward<NewLine>    input = torch.reshape(x, [-1, int(_22), int(_23), 1])<NewLine>    _24 = (_21).forward(input, )<NewLine>    input0 = torch.relu((_20).forward(_24, ))<NewLine>                         ~~~~~~~~~~~~ &lt;--- HERE<NewLine>    _25 = (_18).forward((_19).forward(input0, ), )<NewLine>    input1 = torch.relu(_25)<NewLine>  File ""code/__torch__/torch/nn/modules/module/___torch_mangle_46.py"", line 8, in forward<NewLine>  def forward(self: __torch__.torch.nn.modules.module.___torch_mangle_46.Module,<NewLine>    argument_1: Tensor) -&gt; Tensor:<NewLine>    input = torch._convolution(argument_1, self.weight, None, [2, 2], [54, 0], [1, 1], False, [0, 0], 1, False, False, True)<NewLine>            ~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>    return input<NewLine></code></pre><NewLine><p>Seems like it’s getting stuck on the second convolution layer, but no idea why.</p><NewLine></div>",https://discuss.pytorch.org/u/Bryan_Wang,(Bryan Wang),Bryan_Wang,"July 6, 2020,  9:12pm",,,,,
88079,How to add meaning node names to onnx model exported by torch.onnx.export?,2020-07-06T08:41:26.041Z,0,78,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a PyTorch model of torch.jit.ScriptModule and have successfully converted it to onnx format. The problem is all the onnx nodes are named with sequential numbers. E.g., in the attached image below, the circled conv’s inputs and outputs are named with numbers (visualized with Netron), which is inconvenient for the following analysis if the network is large. How can I add more meaningful names to these intermediate onnx nodes? Thanks.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/5f34d5f7212f0f78b84063b2cdff32fdb5dd9193"" href=""https://discuss.pytorch.org/uploads/default/original/3X/5/f/5f34d5f7212f0f78b84063b2cdff32fdb5dd9193.jpeg"" title=""image""><img alt=""image"" data-base62-sha1=""dAeyIai5LgWNcOhDxmOWuvN53oL"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/5/f/5f34d5f7212f0f78b84063b2cdff32fdb5dd9193_2_10x10.png"" height=""351"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/5/f/5f34d5f7212f0f78b84063b2cdff32fdb5dd9193_2_690x351.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/5/f/5f34d5f7212f0f78b84063b2cdff32fdb5dd9193_2_690x351.jpeg, https://discuss.pytorch.org/uploads/default/optimized/3X/5/f/5f34d5f7212f0f78b84063b2cdff32fdb5dd9193_2_1035x526.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/5/f/5f34d5f7212f0f78b84063b2cdff32fdb5dd9193_2_1380x702.jpeg 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">2030×1034 168 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/Liming,(LiM),Liming,"July 6, 2020,  8:41am",,,,,
84614,Error in loading the model,2020-06-08T08:19:26.687Z,3,315,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I make a small project to check. It works fine… But as soon as I put the same thing in the Kaldi (<a href=""https://github.com/cyfer0618/kaldi-pytorch-rnnlm/blob/master/kaldi/src/pyrnnlm/pytorch-rnnlm.cc"" rel=""nofollow noopener"">Pytorch lattice rescoring</a>) and run the executable file then I receive the same error as mentioned below.</p><NewLine><p><strong>Error</strong></p><NewLine><pre><code class=""lang-auto"">data/pytorch/rnnlm/newmodel2.pt error loading the model<NewLine>_ivalue_ INTERNAL ASSERT FAILED at ""/pytorch/torch/csrc/jit/api/object.cpp"":19, please report a bug to PyTorch.  (_ivalue at /pytorch/torch/csrc/jit/api/object.cpp:19)<NewLine>frame #0: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x46 (0x7f83b939d666 in /home/rakesh/rishabh_workspace/Garbage/kaldi/tools/libtorch/lib/libc10.so)<NewLine>frame #1: torch::jit::Object::_ivalue() const + 0xab (0x7f83acdc42cb in /home/rakesh/rishabh_workspace/Garbage/kaldi/tools/libtorch/lib/libtorch_cpu.so)<NewLine>frame #2: torch::jit::Object::find_method(std::string const&amp;) const + 0x26 (0x7f83acdc43a6 in /home/rakesh/rishabh_workspace/Garbage/kaldi/tools/libtorch/lib/libtorch_cpu.so)<NewLine>frame #3: &lt;unknown function&gt; + 0x9192f (0x5592eb17892f in lattice-lmrescore-py-rnnlm)<NewLine>frame #4: &lt;unknown function&gt; + 0x86b78 (0x5592eb16db78 in lattice-lmrescore-py-rnnlm)<NewLine>frame #5: &lt;unknown function&gt; + 0x867d8 (0x5592eb16d7d8 in lattice-lmrescore-py-rnnlm)<NewLine>frame #6: &lt;unknown function&gt; + 0x23c38 (0x5592eb10ac38 in lattice-lmrescore-py-rnnlm)<NewLine>frame #7: __libc_start_main + 0xe7 (0x7f836b610b97 in /lib/x86_64-linux-gnu/libc.so.6)<NewLine>frame #8: &lt;unknown function&gt; + 0x2340a (0x5592eb10a40a in lattice-lmrescore-py-rnnlm)<NewLine></code></pre><NewLine><p>pyrnnlm.cc</p><NewLine><pre><code class=""lang-auto""><NewLine><NewLine><NewLine>#include &lt;utility&gt;<NewLine>#include &lt;fstream&gt;<NewLine><NewLine>#include ""pyrnnlm/pytorch-rnnlm.h""<NewLine>#include ""util/stl-utils.h""<NewLine>#include ""util/text-utils.h""<NewLine><NewLine>// torch::Tensorflow includes were moved after tfrnnlm/tensorflow-rnnlm.h include to<NewLine>// avoid macro redefinitions. See also the note in tfrnnlm/tensorflow-rnnlm.h.<NewLine>#include &lt;torch/torch.h&gt;<NewLine>#include &lt;torch/script.h&gt;<NewLine>#include &lt;iostream&gt;<NewLine>#include &lt;memory&gt;<NewLine>#include &lt;dirent.h&gt;<NewLine><NewLine>namespace kaldi {<NewLine>using std::ifstream;<NewLine>using py_rnnlm::KaldiPyRnnlmWrapper;<NewLine>using py_rnnlm::PyRnnlmDeterministicFst;<NewLine><NewLine><NewLine>// read a unigram count file of the OOSs and generate extra OOS costs for words<NewLine>void SetUnkPenalties(const string &amp;filename,<NewLine>                     const fst::SymbolTable&amp; fst_word_symbols,<NewLine>                     std::vector&lt;float&gt; *out) {<NewLine>  if (filename == """")<NewLine>    return;<NewLine>  out-&gt;resize(fst_word_symbols.NumSymbols(), 0);  // default is 0<NewLine>  ifstream ifile(filename.c_str());<NewLine>  string word;<NewLine>  float count, total_count = 0;<NewLine>  while (ifile &gt;&gt; word &gt;&gt; count) {<NewLine>    int id = fst_word_symbols.Find(word);<NewLine>    KALDI_ASSERT(id != -1); // fst::kNoSymbol<NewLine>    (*out)[id] = count;<NewLine>    total_count += count;<NewLine>  }<NewLine><NewLine>  for (int i = 0; i &lt; out-&gt;size(); i++) {<NewLine>    if ((*out)[i] != 0) {<NewLine>      (*out)[i] = log ((*out)[i] / total_count);<NewLine>    }<NewLine>  }<NewLine>}<NewLine><NewLine>// Read pytorch model files<NewLine>// Done ****<NewLine>void KaldiPyRnnlmWrapper::ReadPyModel(const std::string &amp;py_model_path,<NewLine>                                      int32 num_threads) {<NewLine><NewLine>  // Need to initialise it<NewLine>  // torch::jit::script::Module module;<NewLine>  try {<NewLine>    // Deserialize the ScriptModule from a file using torch::jit::load().<NewLine>    //std::cout &lt;&lt; ""Model "" &lt;&lt; py_model_path.substr(0, py_model_path.size()-1) &lt;&lt; "" /newmodel2.pt"";<NewLine>    std::string file(""/newmodel2.pt"");<NewLine>    // Load model in the module<NewLine>    std::string path = py_model_path + file;<NewLine><NewLine>    std::string::iterator st = std::remove(path.begin(), path.end(), ' ');<NewLine><NewLine>    path.erase(st, path.end());<NewLine>    std::cout &lt;&lt; path;<NewLine>    module = torch::jit::load(path);<NewLine>  }<NewLine>  catch (const c10::Error&amp; e) {<NewLine>    std::cerr &lt;&lt; "" error loading the model\n"";<NewLine>    //return -1;<NewLine>    return;<NewLine>  }<NewLine><NewLine>  std::cout &lt;&lt; ""Language Model\n\n"";<NewLine><NewLine>  // (Samrat): Think we need few of these, not all<NewLine>  word_id_tensor_name_ = ""word_id"";<NewLine>  context_tensor_name_ = ""context"";<NewLine>  log_prob_tensor_name_ = ""log_prob"";<NewLine>  rnn_out_tensor_name_ = ""rnn_out"";<NewLine>  rnn_states_tensor_name_ = ""rnn_states"";<NewLine>  initial_state_tensor_name_ = ""initial_state"";<NewLine>  <NewLine>}<NewLine><NewLine>// Done ****<NewLine>// Batch_size = 1 they have hard code it<NewLine>KaldiPyRnnlmWrapper::KaldiPyRnnlmWrapper(<NewLine>    const KaldiPyRnnlmWrapperOpts &amp;opts,<NewLine>    const std::string &amp;rnn_wordlist,<NewLine>    const std::string &amp;word_symbol_table_rxfilename,<NewLine>    const std::string &amp;unk_prob_file,<NewLine>    const std::string &amp;py_model_path): opts_(opts) {<NewLine>  ReadPyModel(py_model_path, opts.num_threads);<NewLine><NewLine>  fst::SymbolTable *fst_word_symbols = NULL;<NewLine>  if (!(fst_word_symbols =<NewLine>        fst::SymbolTable::ReadText(word_symbol_table_rxfilename))) {<NewLine>    KALDI_ERR &lt;&lt; ""Could not read symbol table from file ""<NewLine>        &lt;&lt; word_symbol_table_rxfilename;<NewLine>  }<NewLine><NewLine>  fst_label_to_word_.resize(fst_word_symbols-&gt;NumSymbols());<NewLine><NewLine>  for (int32 i = 0; i &lt; fst_label_to_word_.size(); ++i) {<NewLine>    fst_label_to_word_[i] = fst_word_symbols-&gt;Find(i);<NewLine>    if (fst_label_to_word_[i] == """") {<NewLine>      KALDI_ERR &lt;&lt; ""Could not find word for integer "" &lt;&lt; i &lt;&lt; "" in the word ""<NewLine>          &lt;&lt; ""symbol table, mismatched symbol table or you have discoutinuous ""<NewLine>          &lt;&lt; ""integers in your symbol table?"";<NewLine>    }<NewLine>  }<NewLine><NewLine>  // first put all -1's; will check later<NewLine>  fst_label_to_rnn_label_.resize(fst_word_symbols-&gt;NumSymbols(), -1);<NewLine>  num_total_words = fst_word_symbols-&gt;NumSymbols();<NewLine><NewLine>  // read rnn wordlist and then generate ngram-label-to-rnn-label map<NewLine>  oos_ = -1;<NewLine>  { // input.<NewLine>    ifstream ifile(rnn_wordlist.c_str());<NewLine>    string word;<NewLine>    int id = -1;<NewLine>    eos_ = 0;<NewLine>    while (ifile &gt;&gt; word) {<NewLine>      id++;<NewLine>      rnn_label_to_word_.push_back(word);  // vector[i] = word<NewLine><NewLine>      int fst_label = fst_word_symbols-&gt;Find(word);<NewLine>      if (fst_label == -1) { // fst::kNoSymbol<NewLine>        if (id == eos_)<NewLine>          continue;<NewLine><NewLine>        KALDI_ASSERT(word == opts_.unk_symbol &amp;&amp; oos_ == -1);<NewLine>        oos_ = id;<NewLine>        continue;<NewLine>      }<NewLine>      KALDI_ASSERT(fst_label &gt;= 0);<NewLine>      fst_label_to_rnn_label_[fst_label] = id;<NewLine>    }<NewLine>  }<NewLine>  if (fst_label_to_word_.size() &gt; rnn_label_to_word_.size()) {<NewLine>    KALDI_ASSERT(oos_ != -1);<NewLine>  }<NewLine>  num_rnn_words = rnn_label_to_word_.size();<NewLine><NewLine>  // we must have an oos symbol in the wordlist<NewLine>  if (oos_ == -1)<NewLine>    return;<NewLine><NewLine>  for (int i = 0; i &lt; fst_label_to_rnn_label_.size(); i++) {<NewLine>    if (fst_label_to_rnn_label_[i] == -1) {<NewLine>      fst_label_to_rnn_label_[i] = oos_;<NewLine>    }<NewLine>  }<NewLine><NewLine>  AcquireInitialTensors();<NewLine>  SetUnkPenalties(unk_prob_file, *fst_word_symbols, &amp;unk_costs_);<NewLine>  delete fst_word_symbols;<NewLine>}<NewLine><NewLine>KaldiPyRnnlmWrapper::~KaldiPyRnnlmWrapper() {<NewLine>}<NewLine>// Done<NewLine> <NewLine>void KaldiPyRnnlmWrapper::AcquireInitialTensors() {<NewLine>  // Status status;<NewLine>  // get the initial context; this is basically the all-0 tensor<NewLine>  /*<NewLine>  (Samrat): Have to figure out get_initial_state(batch_size) ? what should btchsz be ?<NewLine>  */<NewLine>  //auto hidden = module.get_method(""get_initial_state"")({torch::tensor({1})});<NewLine>  //initial_context_ = hidden.toTensor();<NewLine><NewLine>  initial_context_=module.get_method(""get_initial_state"")({torch::tensor({1})}).toTensor();<NewLine><NewLine><NewLine>  //changed function call name (Samrat)<NewLine>  auto bosword = torch::tensor({eos_});<NewLine><NewLine>  auto hidden = module.get_method(""single_step_rnn_out"")({initial_context_, bosword});<NewLine>  initial_cell_ = hidden.toTensor();<NewLine><NewLine><NewLine><NewLine><NewLine>  // {<NewLine>  //   std::vector&lt;torch::Tensor&gt; state;<NewLine>  //   status = bundle_.session-&gt;Run(std::vector&lt;std::pair&lt;string, torch::Tensor&gt; &gt;(),<NewLine>  //                          {initial_state_tensor_name_}, {}, &amp;state);<NewLine>  //   if (!status.ok()) {<NewLine>  //     KALDI_ERR &lt;&lt; status.ToString();<NewLine>  //   }<NewLine>  //   initial_context_ = state[0];<NewLine>  // }<NewLine><NewLine>  // get the initial pre-final-affine layer<NewLine>  // {<NewLine>  //   std::vector&lt;torch::Tensor&gt; state;<NewLine>  //   torch::Tensor bosword(tensorflow::DT_INT32, {1, 1});<NewLine>  //   bosword.scalar&lt;int32&gt;()() = eos_;  // eos_ is more like a sentence boundary<NewLine><NewLine>  //   std::vector&lt;std::pair&lt;string, torch::Tensor&gt; &gt; inputs = {<NewLine>  //     {word_id_tensor_name_, bosword},<NewLine>  //     {context_tensor_name_, initial_context_},<NewLine>  //   };<NewLine><NewLine>  //   status = bundle_.session-&gt;Run(inputs, {rnn_out_tensor_name_}, {}, &amp;state);<NewLine>  //   if (!status.ok()) {<NewLine>  //     KALDI_ERR &lt;&lt; status.ToString();<NewLine>  //   }<NewLine>  //   initial_cell_ = state[0];<NewLine>  // }<NewLine>}<NewLine><NewLine><NewLine>/*<NewLine>// Need to change *****<NewLine>BaseFloat KaldiPyRnnlmWrapper::GetLogProb(int32 word,<NewLine>                                          int32 fst_word,<NewLine>                                          const torch::Tensor &amp;context_in,<NewLine>                                          const torch::Tensor &amp;cell_in,<NewLine>                                          torch::Tensor *context_out,<NewLine>                                          torch::Tensor *new_cell) {<NewLine>  torch::Tensor thisword(torch::Tensor, {1, 1});<NewLine><NewLine>  thisword.scalar&lt;int32&gt;()() = word;<NewLine><NewLine>  std::vector&lt;torch::Tensor&gt; outputs;<NewLine><NewLine>  std::vector&lt;std::pair&lt;string, torch::Tensor&gt; &gt; inputs = {<NewLine>    {word_id_tensor_name_, thisword},<NewLine>    {context_tensor_name_, context_in},<NewLine>  };<NewLine><NewLine>  if (context_out != NULL) {<NewLine>    // The session will initialize the outputs<NewLine>    // Run the session, evaluating our ""c"" operation from the graph<NewLine>    Status status = bundle_.session-&gt;Run(inputs,<NewLine>        {log_prob_tensor_name_,<NewLine>         rnn_out_tensor_name_,<NewLine>         rnn_states_tensor_name_}, {}, &amp;outputs);<NewLine>    if (!status.ok()) {<NewLine>      KALDI_ERR &lt;&lt; status.ToString();<NewLine>    }<NewLine><NewLine>    *context_out = outputs[1];<NewLine>    *new_cell = outputs[2];<NewLine>  } else {<NewLine>    // Run the session, evaluating our ""c"" operation from the graph<NewLine>    Status status = bundle_.session-&gt;Run(inputs,<NewLine>        {log_prob_tensor_name_}, {}, &amp;outputs);<NewLine>    if (!status.ok()) {<NewLine>      KALDI_ERR &lt;&lt; status.ToString();<NewLine>    }<NewLine>  }<NewLine><NewLine>  float ans;<NewLine>  if (word != oos_) {<NewLine>    ans = outputs[0].scalar&lt;float&gt;()();<NewLine>  } else {<NewLine>    if (unk_costs_.size() == 0) {<NewLine>      ans = outputs[0].scalar&lt;float&gt;()() - log(num_total_words - num_rnn_words);<NewLine>    } else {<NewLine>      ans = outputs[0].scalar&lt;float&gt;()() + unk_costs_[fst_word];<NewLine>    }<NewLine>  }<NewLine><NewLine>  return ans;<NewLine>}<NewLine>*/<NewLine><NewLine>/*<NewLine>  Below is my(Samrat) modified version of the above function only. <NewLine>  Replace if you think something is incorrect.<NewLine>*/<NewLine><NewLine><NewLine>BaseFloat KaldiPyRnnlmWrapper::GetLogProb(int32 word,<NewLine>                                          int32 fst_word,<NewLine>                                          const torch::Tensor &amp;context_in,<NewLine>                                          const torch::Tensor &amp;cell_in,<NewLine>                                          torch::Tensor *context_out,<NewLine>                                          torch::Tensor *new_cell) {<NewLine>  //torch::Tensor thisword(torch::Tensor, {1, 1});<NewLine>  <NewLine>  //thisword.scalar&lt;int32&gt;()() = word;<NewLine>  torch::Tensor thisword = torch::tensor({word});<NewLine><NewLine><NewLine>  //std::vector&lt;torch::Tensor&gt; outputs;<NewLine><NewLine>  // std::vector&lt;std::pair&lt;string, torch::Tensor&gt; &gt; inputs = {<NewLine>  //   {word_id_tensor_name_, thisword},<NewLine>  //   {context_tensor_name_, context_in},<NewLine>  // };<NewLine><NewLine><NewLine><NewLine>  auto outputs = module.get_method(""single_step"")({context_in, thisword});<NewLine>  if (context_out != NULL) {<NewLine>    // The session will initialize the outputs<NewLine>    // Run the session, evaluating our ""c"" operation from the graph<NewLine>    // Status status = bundle_.session-&gt;Run(inputs,<NewLine>    //     {log_prob_tensor_name_,<NewLine>    //      rnn_out_tensor_name_,<NewLine>    //      rnn_states_tensor_name_}, {}, &amp;outputs);<NewLine><NewLine>    // if (!status.ok()) {<NewLine>    //   KALDI_ERR &lt;&lt; status.ToString();<NewLine>    // }<NewLine><NewLine>    *context_out = module.get_method(""single_step_rnn_out"")({context_in, thisword}).toTensor();<NewLine>    *new_cell = module.get_method(""single_step_rnn_state"")({context_in, thisword}).toTensor();<NewLine>  } //else {<NewLine>    // Run the session, evaluating our ""c"" operation from the graph<NewLine>    // Status status = bundle_.session-&gt;Run(inputs,<NewLine>    //     {log_prob_tensor_name_}, {}, &amp;outputs);<NewLine>    // if (!status.ok()) {<NewLine>    //   KALDI_ERR &lt;&lt; status.ToString();<NewLine>    // }<NewLine>  //}<NewLine><NewLine>  /*<NewLine>    (Samrat): Can through error so have to check manually in testLM<NewLine>    Hopefully expect it to return a float<NewLine>  */<NewLine> <NewLine>  float log_prob=(float)module.get_method(""single_step_log"")({context_in, thisword}).toDouble();<NewLine>  float ans;<NewLine>  if (word != oos_) {<NewLine>    //ans = outputs[0].scalar&lt;float&gt;()();<NewLine>    ans = log_prob;<NewLine>  } else {<NewLine>    if (unk_costs_.size() == 0) {<NewLine>      //ans = outputs[0].scalar&lt;float&gt;()() - log(num_total_words - num_rnn_words);<NewLine>      ans = log_prob - log(num_total_words - num_rnn_words);<NewLine>    } else {<NewLine>      //ans = outputs[0].scalar&lt;float&gt;()() + unk_costs_[fst_word];<NewLine>      ans = log_prob + unk_costs_[fst_word];<NewLine>    }<NewLine>  }<NewLine><NewLine>  return ans;<NewLine>}<NewLine><NewLine>// Done *****<NewLine>const torch::Tensor&amp; KaldiPyRnnlmWrapper::GetInitialContext() const {<NewLine>  return initial_context_;<NewLine>}<NewLine><NewLine>const torch::Tensor&amp; KaldiPyRnnlmWrapper::GetInitialCell() const {<NewLine>  return initial_cell_;<NewLine>}<NewLine><NewLine>int KaldiPyRnnlmWrapper::FstLabelToRnnLabel(int i) const {<NewLine>  KALDI_ASSERT(i &gt;= 0 &amp;&amp; i &lt; fst_label_to_rnn_label_.size());<NewLine>  return fst_label_to_rnn_label_[i];<NewLine>}<NewLine><NewLine><NewLine>// Done *****<NewLine>PyRnnlmDeterministicFst::PyRnnlmDeterministicFst(int32 max_ngram_order,<NewLine>                                             KaldiPyRnnlmWrapper *rnnlm) {<NewLine>  KALDI_ASSERT(rnnlm != NULL);<NewLine>  max_ngram_order_ = max_ngram_order;<NewLine>  rnnlm_ = rnnlm;<NewLine><NewLine>  std::vector&lt;Label&gt; bos;<NewLine>  const torch::Tensor&amp; initial_context = rnnlm_-&gt;GetInitialContext();<NewLine>  const torch::Tensor&amp; initial_cell = rnnlm_-&gt;GetInitialCell();<NewLine><NewLine>  state_to_wseq_.push_back(bos);<NewLine>  state_to_context_.push_back(new torch::Tensor(initial_context));<NewLine>  state_to_cell_.push_back(new torch::Tensor(initial_cell));<NewLine>  wseq_to_state_[bos] = 0;<NewLine>  start_state_ = 0;<NewLine>}<NewLine><NewLine>// Done *****<NewLine>PyRnnlmDeterministicFst::~PyRnnlmDeterministicFst() {<NewLine>  for (int i = 0; i &lt; state_to_context_.size(); i++) {<NewLine>    delete state_to_context_[i];<NewLine>  }<NewLine>  for (int i = 0; i &lt; state_to_cell_.size(); i++) {<NewLine>    delete state_to_cell_[i];<NewLine>  }<NewLine>}<NewLine><NewLine>// Done *****<NewLine>void PyRnnlmDeterministicFst::Clear() {<NewLine>  // similar to the destructor but we retain the 0-th entries in each map<NewLine>  // which corresponds to the &lt;bos&gt; state<NewLine>  for (int i = 1; i &lt; state_to_context_.size(); i++) {<NewLine>    delete state_to_context_[i];<NewLine>  }<NewLine>  for (int i = 1; i &lt; state_to_cell_.size(); i++) {<NewLine>    delete state_to_cell_[i];<NewLine>  }<NewLine><NewLine>  state_to_context_.resize(1);<NewLine>  state_to_cell_.resize(1);<NewLine>  state_to_wseq_.resize(1);<NewLine>  wseq_to_state_.clear();<NewLine>  wseq_to_state_[state_to_wseq_[0]] = 0;<NewLine>}<NewLine><NewLine>// Done *****<NewLine>fst::StdArc::Weight PyRnnlmDeterministicFst::Final(StateId s) {<NewLine>  // At this point, we should have created the state.<NewLine>  KALDI_ASSERT(static_cast&lt;size_t&gt;(s) &lt; state_to_wseq_.size());<NewLine><NewLine>  std::vector&lt;Label&gt; wseq = state_to_wseq_[s];<NewLine>  BaseFloat logprob = rnnlm_-&gt;GetLogProb(rnnlm_-&gt;GetEos(),<NewLine>                         -1,  // only need type; this param will not be used<NewLine>                         *state_to_context_[s],<NewLine>                         *state_to_cell_[s], NULL, NULL);<NewLine>  return Weight(-logprob);<NewLine>}<NewLine><NewLine>// Done *****<NewLine>bool PyRnnlmDeterministicFst::GetArc(StateId s, Label ilabel,<NewLine>                                     fst::StdArc *oarc) {<NewLine>  KALDI_ASSERT(static_cast&lt;size_t&gt;(s) &lt; state_to_wseq_.size());<NewLine><NewLine>  std::vector&lt;Label&gt; wseq = state_to_wseq_[s];<NewLine>  torch::Tensor *new_context = new torch::Tensor();<NewLine>  torch::Tensor *new_cell = new torch::Tensor();<NewLine><NewLine>  // look-up the rnn label from the FST label<NewLine>  int32 rnn_word = rnnlm_-&gt;FstLabelToRnnLabel(ilabel);<NewLine>  BaseFloat logprob = rnnlm_-&gt;GetLogProb(rnn_word,<NewLine>                                         ilabel,<NewLine>                                         *state_to_context_[s],<NewLine>                                         *state_to_cell_[s],<NewLine>                                         new_context,<NewLine>                                         new_cell);<NewLine><NewLine>  wseq.push_back(rnn_word);<NewLine>  if (max_ngram_order_ &gt; 0) {<NewLine>    while (wseq.size() &gt;= max_ngram_order_) {<NewLine>      // History state has at most &lt;max_ngram_order_&gt; - 1 words in the state.<NewLine>      wseq.erase(wseq.begin(), wseq.begin() + 1);<NewLine>    }<NewLine>  }<NewLine><NewLine>  std::pair&lt;const std::vector&lt;Label&gt;, StateId&gt; wseq_state_pair(<NewLine>      wseq, static_cast&lt;Label&gt;(state_to_wseq_.size()));<NewLine><NewLine>  // Attemps to insert the current &lt;lseq_state_pair&gt;. If the pair already exists<NewLine>  // then it returns false.<NewLine>  typedef MapType::iterator IterType;<NewLine>  std::pair&lt;IterType, bool&gt; result = wseq_to_state_.insert(wseq_state_pair);<NewLine><NewLine>  // If the pair was just inserted, then also add it to &lt;state_to_wseq_&gt; and<NewLine>  // &lt;state_to_context_&gt;.<NewLine>  if (result.second == true) {<NewLine>    state_to_wseq_.push_back(wseq);<NewLine>    state_to_context_.push_back(new_context);<NewLine>    state_to_cell_.push_back(new_cell);<NewLine>  } else {<NewLine>    delete new_context;<NewLine>    delete new_cell;<NewLine>  }<NewLine><NewLine>  // Creates the arc.<NewLine>  oarc-&gt;ilabel = ilabel;<NewLine>  oarc-&gt;olabel = ilabel;<NewLine>  oarc-&gt;nextstate = result.first-&gt;second;<NewLine>  oarc-&gt;weight = Weight(-logprob);<NewLine><NewLine>  return true;<NewLine>}<NewLine><NewLine>}  // namespace kaldi<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/krrishabh,(Rishabh Kumar),krrishabh,"June 8, 2020, 11:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/krrishabh"">@krrishabh</a><br/><NewLine>Can you post this in ‘jit’ category?</p><NewLine><p>And, check what pytorch/libtorch version is used to generate your .pt file and what libtorch version you were used to load it.  This might be a version mismatch as well.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have change the tag from C++ to jit<br/><NewLine>libtorch is used to load the model<br/><NewLine>libtorch Build version : 1.6.0.dev20200501+cu101</p><NewLine><p>Model is saved in Pytorch version : 1.3.1</p><NewLine><p>Moreover, I have made a small project where it is working fine… but in my Kaldi Project the model is not able to get load. (both have same Configuration)</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/krrishabh"">@krrishabh</a><br/><NewLine>I think version mismatch might be the root cause.<br/><NewLine>If you saved your .pt with 1.3.1, please use the same version of libtorch to load it.<br/><NewLine>We have a similar issue here,<br/><NewLine>‘<a href=""https://github.com/pytorch/pytorch/issues/39623"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/39623</a>’<br/><NewLine>please read the comments see how to download libtorch 1.3.1, or if possible, upgrade your pytorch to 1.6dev, or I believe 1.5 is fine.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have upgrade the Pytorch version to 1.5 but I am getting the same error. Moreover, It was not a problem of version because when I am building a small project it is working correctly.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/krrishabh"">@krrishabh</a><br/><NewLine>Since the error message says “error loading the model”, it is likely the program failed to load the model file in <code>torch::jit::load(path)</code>.</p><NewLine><p>Please make sure <code>data/pytorch/rnnlm/newmodel2.pt</code> exists and it is accessible from the binary.<br/><NewLine>If you compile the program into <code>build</code> directory, the path might be <code>../data/pytorch/rnnlm/newmodel2.pt</code>.</p><NewLine><p>And the message “<code>_ivalue_ INTERNAL ASSERT FAILED at ...</code>” is shown when an instance of <code>torch::jit::script::Module</code> is not initialized (like the situation that the exception from <code>torch::jit::load()</code> is caught and you reference the uninitialized module instance after that).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/glaringlee; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/krrishabh; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/glaringlee; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/krrishabh; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/m4saka; <NewLine> ,"REPLY_DATE 1: June 8, 2020,  6:56pm; <NewLine> REPLY_DATE 2: June 9, 2020, 12:35am; <NewLine> REPLY_DATE 3: June 9, 2020,  1:27am; <NewLine> REPLY_DATE 4: June 10, 2020, 12:56am; <NewLine> REPLY_DATE 5: July 3, 2020, 11:24am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> 
87744,spconv with jit script problem,2020-07-02T14:53:06.573Z,0,89,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, everyone.<br/><NewLine>I have to use spconv.conv.py,</p><NewLine><p>I got error in this place.</p><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “trace_smallnet.py”, line 27, in <br/><NewLine>sm = torch.jit.script(net_middle)<br/><NewLine>File “/home/liangdao/.local/lib/python3.6/site-packages/torch/jit/<strong>init</strong>.py”, line 1162, in script<br/><NewLine>return _convert_to_script_module(obj)<br/><NewLine>File “/home/liangdao/.local/lib/python3.6/site-packages/torch/jit/<strong>init</strong>.py”, line 1812, in _convert_to_script_module<br/><NewLine>return WeakScriptModuleProxy(mod, stubs)<br/><NewLine>File “/home/liangdao/.local/lib/python3.6/site-packages/torch/jit/<strong>init</strong>.py”, line 1386, in init_then_register<br/><NewLine>original_init(self, *args, **kwargs)<br/><NewLine>File “/home/liangdao/.local/lib/python3.6/site-packages/torch/jit/<strong>init</strong>.py”, line 1736, in <strong>init</strong><br/><NewLine>_create_methods_from_stubs(self, stubs)<br/><NewLine>File “/home/liangdao/.local/lib/python3.6/site-packages/torch/jit/<strong>init</strong>.py”, line 1347, in _create_methods_from_stubs<br/><NewLine>self._c._create_methods(self, defs, rcbs, defaults)<br/><NewLine>File “/home/liangdao/.local/lib/python3.6/site-packages/torch/jit/<strong>init</strong>.py”, line 982, in _make_strong_submodule<br/><NewLine>new_strong_submodule = _convert_to_script_module(module)<br/><NewLine>File “/home/liangdao/.local/lib/python3.6/site-packages/torch/jit/<strong>init</strong>.py”, line 1812, in _convert_to_script_module<br/><NewLine>return WeakScriptModuleProxy(mod, stubs)<br/><NewLine>File “/home/liangdao/.local/lib/python3.6/site-packages/torch/jit/<strong>init</strong>.py”, line 1386, in init_then_register<br/><NewLine>original_init(self, *args, **kwargs)<br/><NewLine>File “/home/liangdao/.local/lib/python3.6/site-packages/torch/jit/<strong>init</strong>.py”, line 1736, in <strong>init</strong><br/><NewLine>_create_methods_from_stubs(self, stubs)<br/><NewLine>File “/home/liangdao/.local/lib/python3.6/site-packages/torch/jit/<strong>init</strong>.py”, line 1347, in _create_methods_from_stubs<br/><NewLine>self._c._create_methods(self, defs, rcbs, defaults)<br/><NewLine>RuntimeError:<br/><NewLine>Arguments for call are not valid.<br/><NewLine>The following operator variants are available:</p><NewLine><p>aten::_set_item(Tensor<a></a> l, int idx, Tensor(b) el) -&gt; (Tensor<a></a>):<br/><NewLine>Expected a value of type ‘List[Tensor]’ for argument ‘l’ but instead found type ‘Dict[str, Tensor]’.</p><NewLine><p>aten::_set_item(int<a></a> l, int idx, int el) -&gt; (int<a></a>):<br/><NewLine>Expected a value of type ‘List[int]’ for argument ‘l’ but instead found type ‘Dict[str, Tensor]’.</p><NewLine><p>aten::_set_item(float<a></a> l, int idx, float el) -&gt; (float<a></a>):<br/><NewLine>Expected a value of type ‘List[float]’ for argument ‘l’ but instead found type ‘Dict[str, Tensor]’.</p><NewLine><p>aten::_set_item(bool<a></a> l, int idx, bool el) -&gt; (bool<a></a>):<br/><NewLine>Expected a value of type ‘List[bool]’ for argument ‘l’ but instead found type ‘Dict[str, Tensor]’.</p><NewLine><p>aten::_set_item(t<a></a> l, int idx, t(b) el) -&gt; (t<a></a>):<br/><NewLine>Could not match type Dict[str, Tensor] to List[t] in argument ‘l’: Cannot match List[t] to Dict[str, Tensor].</p><NewLine><p>aten::_set_item(Dict(str, t)(a) l, str idx, t(b) v) -&gt; ():<br/><NewLine>Could not match type Tuple[Tensor, Tensor, Tensor, Tensor, Tensor] to t in argument ‘v’: Type variable ‘t’ previously matched to type Tensor is matched to type Tuple[Tensor, Tensor, Tensor, Tensor, Tensor].</p><NewLine><p>aten::_set_item(Dict(int, t)(a) l, int idx, t(b) v) -&gt; ():<br/><NewLine>Expected a value of type ‘Dict[int, t]’ for argument ‘l’ but instead found type ‘Dict[str, Tensor]’.</p><NewLine><p>aten::_set_item(Dict(float, t)(a) l, float idx, t(b) v) -&gt; ():<br/><NewLine>Expected a value of type ‘Dict[float, t]’ for argument ‘l’ but instead found type ‘Dict[str, Tensor]’.</p><NewLine><p>The original call is:<br/><NewLine>at /home/liangdao/.local/lib/python3.6/site-packages/spconv/conv.py:320:13<br/><NewLine>self.output_padding,<br/><NewLine>self.subm,<br/><NewLine>self.transposed,<br/><NewLine>use_hash=self.use_hash)</p><NewLine><pre><code>        outids = temp__[0]<NewLine>        indice_pairs = temp__[1]<NewLine>        indice_pair_num = temp__[2]<NewLine>        <NewLine>        indice_dict[self.indice_key] = (outids, indices,<NewLine>        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...  &lt;--- HERE<NewLine>                                                indice_pairs,<NewLine>                                                indice_pair_num,<NewLine>                                                spatial_shape)<NewLine>        print(indice_dict[self.indice_key])<NewLine>                                   <NewLine>    if self.fused_bn:<NewLine>        assert self.bias is not None<NewLine>        out_features = ops.fused_indice_conv(<NewLine>            features, self.weight, self.bias, indice_pairs.to(device),'__module__.__torch__.torchplus.tools.DefaultArgLayer.forward' is being compiled since it was called from '__module__.__torch__.models.middle.SpMiddleFHD.forward'<NewLine></code></pre><NewLine><p>at /home/liangdao/second.pytorch/impl_DL/py_cxx_integration/py_scripts/sample_model/models/middle.py:119:9<br/><NewLine>def forward(self, voxel_features, coors):<br/><NewLine># coors[:, 1] += 1</p><NewLine><pre><code>    coors = coors.int()<NewLine>    ret = spconv.SparseConvTensor(voxel_features, coors,self.sparse_shape,self.batch_size)<NewLine>    #print(self.time_process)<NewLine>    ret = self.SubMConv3d_0(ret,ret.spatial_shape,ret.indice_dict,self.time_process,ret.indices)<NewLine>    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>    ret.features = F.relu(self.BatchNorm1d_16(ret.features))<NewLine>    self.time_process += 1<NewLine>    #print(self.time_process)<NewLine>    ret = self.SubMConv3d_00(ret,ret.spatial_shape,ret.indice_dict,self.time_process,ret.indices)<NewLine>    ret.features = F.relu(self.BatchNorm1d_16(ret.features))<NewLine>    self.time_process += 1<NewLine>    #print(self.time_process)<NewLine>    ret = self.SpConv3d_0(ret,ret.spatial_shape,ret.indice_dict,self.time_process,ret.indices)<NewLine>    ret.features = F.relu(self.BatchNorm1d_32(ret.features))<NewLine></code></pre><NewLine><p>can somebody help me to solve this issue?</p><NewLine></div>",https://discuss.pytorch.org/u/XiaoHan0705,(XiaoHan0705),XiaoHan0705,"July 2, 2020,  2:53pm",,,,,
87706,Why does broadcasting operation cannot be fused in torchscript?,2020-07-02T09:11:27.375Z,0,50,"<div class=""post"" itemprop=""articleBody""><NewLine><p>While reading about the optimization that can be made using torchscript, I came to know that <strong>fusing broadcasting operation increases execution time</strong>. I am unable to understand the reason behind it.</p><NewLine><p>Can any please help me understand this?</p><NewLine></div>",https://discuss.pytorch.org/u/Midhilesh,(Midhilesh),Midhilesh,"July 2, 2020,  9:11am",,,,,
87655,TorchScript compatibility between pytorch versions,2020-07-01T22:50:14.280Z,0,174,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to use libtorch in a C++ application that’s restricted to the gcc-4.8 ABI, which forces me to link against libtorch-1.2.0 (I was told newer version of libtorch aren’t compatible with that ABI).</p><NewLine><p>This application needs to load a pre-trained model. That model was trained and saved from a python script that needs to use pytorch-1.3.0 or newer.</p><NewLine><p>Trying to load the serialized TorchScript model aborts the program with this error:</p><NewLine><pre><code>terminate called after throwing an instance of 'c10::Error'<NewLine>  what():  [enforce fail at inline_container.cc:137] . PytorchStreamReader failed closing reader: file not found <NewLine></code></pre><NewLine><p>This happens both from the C++ and the python API. Is this a compatibility issue?</p><NewLine><p>This is easily reproducible: from an environment with either pytorch-1.3.1 or 1.5.0, run</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine>class TestModule(torch.nn.Module):<NewLine>    def __init__(self, N, M):<NewLine>        super().__init__()<NewLine>        self.weight = torch.nn.Parameter(torch.rand(N, M))<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.weight.mv(x)<NewLine><NewLine>my_module = TestModule(10,20)<NewLine>test_module = torch.jit.script(my_module)<NewLine><NewLine>test_module.save(""test_module.pt"")<NewLine></code></pre><NewLine><p>Then from an environment with pytorch-1.2.0:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>m = torch.jit.load('test_module.pt')<NewLine></code></pre><NewLine><p>If the file was written from 1.5.0, I get</p><NewLine><pre><code>RuntimeError: version_number &lt;= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED at ../caffe2/serialize/inline_container.cc:131, please report a bug to PyTorch. Attempted to read a PyTorch file with version 2, but the maximum supported version for reading is 1. Your PyTorch installation may be too old.<NewLine></code></pre><NewLine><p>which is clearly a compatibility issue. But if the file was written from 1.3.1, I get the “file not found” error above (note this is a process abort, not an exception, from both C++ and python API).</p><NewLine><p>I could not find documentation on TorchScript file compatibility across versions. Is that information available somewhere?</p><NewLine><p>Thanks,<br/><NewLine>A.</p><NewLine></div>",https://discuss.pytorch.org/u/antoche,(Antoine),antoche,"July 1, 2020, 10:50pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, we maintain backwards compatibility but not forwards compatiblity. New versions will work with older models, but older versions will not work with newer models.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/eellison; <NewLine> ,"REPLY_DATE 1: July 1, 2020, 11:38pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
87617,Torchscript fails with bias term?,2020-07-01T16:19:12.441Z,0,58,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I’ve run into some behavior that I can’t explain. Below is a snippet of code that was lifted from this discussion: <a href=""https://github.com/pytorch/pytorch/issues/24235"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/24235</a>.</p><NewLine><p>What I’ve found is that if bias=False in testSubMod then the compilation runs with no problem and outputs test.onnx as a file but if bias=True then the compilation fails with the stack trace shown below.</p><NewLine><p>I’m pretty new to working with ONNX and TorchScript so any insights would be really appreciated!</p><NewLine><p>Thanks,<br/><NewLine>Zach</p><NewLine><pre><code class=""lang-python"">import torch <NewLine>import onnx<NewLine><NewLine>class testSubMod(torch.nn.Module):<NewLine>    def __init__(self, rnn_dims=32):<NewLine>        super().__init__()<NewLine>        self.lin = torch.nn.Linear(32, 32, bias=True) # &lt; This, this bit here!<NewLine><NewLine>    def forward(self, out):<NewLine>        for _ in torch.arange(8):<NewLine>            out = self.lin(out)<NewLine>        return out<NewLine><NewLine><NewLine>class test(torch.nn.Module):<NewLine>    def __init__(self, rnn_dims=32):<NewLine>        super().__init__()<NewLine>        self.submod = torch.jit.script(testSubMod())<NewLine><NewLine>    def forward(self, x):<NewLine>        out = torch.ones(<NewLine>            [<NewLine>                x.size(0),<NewLine>                x.size(1)<NewLine>            ],<NewLine>            dtype=x.dtype,<NewLine>            device=x.device<NewLine>        )<NewLine><NewLine>        return self.submod(out)<NewLine><NewLine>if __name__=='__main__':<NewLine>    model = test()<NewLine>    model = torch.jit.script(model)<NewLine><NewLine>    input_data = torch.ones((32, 32, 32)).float()<NewLine>    output = model(input_data)<NewLine><NewLine>    torch.onnx.export(model, input_data,<NewLine>                      'test.onnx', example_outputs=output)<NewLine><NewLine><NewLine>    onnx_model = onnx.load(""test.onnx"")<NewLine>    print(onnx.helper.printable_graph(onnx_model.graph))<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">/home/USERNAME/anaconda3/envs/rain_man/lib/python3.7/site-packages/torch/onnx/utils.py:617: UserWarning: ONNX export failed on ATen operator dim because torch.onnx.symbolic_opset9.dim does not exist<NewLine>  .format(op_name, opset_version, op_name))<NewLine>Traceback (most recent call last):<NewLine>  File ""compile.py"", line 38, in &lt;module&gt;<NewLine>    'test.onnx', example_outputs=output)<NewLine>  File ""/home/USERNAME/anaconda3/envs/rain_man/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 143, in export<NewLine>    strip_doc_string, dynamic_axes, keep_initializers_as_inputs)<NewLine>  File ""/home/USERNAME/anaconda3/envs/rain_man/lib/python3.7/site-packages/torch/onnx/utils.py"", line 66, in export<NewLine>    dynamic_axes=dynamic_axes, keep_initializers_as_inputs=keep_initializers_as_inputs)<NewLine>  File ""/home/USERNAME/anaconda3/envs/rain_man/lib/python3.7/site-packages/torch/onnx/utils.py"", line 382, in _export<NewLine>    fixed_batch_size=fixed_batch_size)<NewLine>  File ""/home/USERNAME/anaconda3/envs/rain_man/lib/python3.7/site-packages/torch/onnx/utils.py"", line 262, in _model_to_graph<NewLine>    fixed_batch_size=fixed_batch_size)<NewLine>  File ""/home/USERNAME/anaconda3/envs/rain_man/lib/python3.7/site-packages/torch/onnx/utils.py"", line 132, in _optimize_graph<NewLine>    graph = torch._C._jit_pass_onnx(graph, operator_export_type)<NewLine>  File ""/home/USERNAME/anaconda3/envs/rain_man/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 174, in _run_symbolic_function<NewLine>    return utils._run_symbolic_function(*args, **kwargs)<NewLine>  File ""/home/USERNAME/anaconda3/envs/rain_man/lib/python3.7/site-packages/torch/onnx/utils.py"", line 644, in _run_symbolic_function<NewLine>    torch._C._jit_pass_onnx_block(b, new_block, operator_export_type, env)<NewLine>  File ""/home/USERNAME/anaconda3/envs/rain_man/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 174, in _run_symbolic_function<NewLine>    return utils._run_symbolic_function(*args, **kwargs)<NewLine>  File ""/home/USERNAME/anaconda3/envs/rain_man/lib/python3.7/site-packages/torch/onnx/utils.py"", line 618, in _run_symbolic_function<NewLine>    op_fn = sym_registry.get_registered_op(op_name, '', opset_version)<NewLine>  File ""/home/USERNAME/anaconda3/envs/rain_man/lib/python3.7/site-packages/torch/onnx/symbolic_registry.py"", line 91, in get_registered_op<NewLine>    return _registry[(domain, version)][opname]<NewLine>KeyError: 'dim'<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/zacharynew,(Zachary New),zacharynew,"July 1, 2020,  4:19pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, looks look a bug with ONNX. Could you open a git issue for it?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/eellison; <NewLine> ,"REPLY_DATE 1: July 1, 2020,  9:36pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
87383,Another weird bug of torchscript: it thinks my python bool is a tensor but it&rsquo;s not,2020-06-30T05:02:55.966Z,1,70,"<div class=""post"" itemprop=""articleBody""><NewLine><p>TorchScript Errors are usually very informative.</p><NewLine><pre><code class=""lang-auto"">EMPTY_FLOAT = torch.zeros(0).float()<NewLine><NewLine>class Bug(nn.Module):<NewLine><NewLine>    def __init__(self):<NewLine>        super(Bug, self).__init__()<NewLine><NewLine>    def forward(self,<NewLine>                my_bool: bool,<NewLine>                x=EMPTY_FLOAT):<NewLine><NewLine>        if my_bool:<NewLine>            y = x<NewLine>        else:<NewLine>            y = EMPTY_FLOAT<NewLine></code></pre><NewLine><p>Then run</p><NewLine><pre><code class=""lang-auto"">bug = Bug()<NewLine>bug_script = torch.jit.script(bug)<NewLine></code></pre><NewLine><p>Then comes the compile time error</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>python value of type 'Tensor' cannot be used as a value:<NewLine>  File ""&lt;ipython-input-310-549c5631a7a3&gt;"", line 15<NewLine>    <NewLine>        if my_bool:<NewLine>        ~~~~~~~~~~~...  &lt;--- HERE<NewLine>            <NewLine>            y = x<NewLine></code></pre><NewLine><p>The problem seems to be <code>EMPTY_FLOAT</code> if I use <code>torch.zeros(0).float()</code> instead it compiles correctly.</p><NewLine></div>",https://discuss.pytorch.org/u/Chenchao_Zhao,(Chenchao Zhao),Chenchao_Zhao,"June 30, 2020,  4:35pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Very strange, do you mind filing an issue on GitHub and we can follow up there? Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: June 30, 2020,  5:56am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
84817,ArrayRef length issue when export pytorch model to onnx,2020-06-09T22:39:14.257Z,2,96,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><p>I have some problem converting my pytorch model into onnx. My input to the model is a list(since I define my own data loader collate_fn), where length of the list is the batch size and each element in the list is a custom tuple of elements. My output is a tensor with shape: batchsize x height x width</p><NewLine><p>The code converting model to onnx:<br/><NewLine><code><br/><NewLine># Export the model<br/><NewLine>torch.onnx.export(model,               # model being run<br/><NewLine>cuda(X),                         # model input (or a tuple for multiple inputs)<br/><NewLine>“final.onnx”,   # where to save the model (can be a file or file-like object)<br/><NewLine>export_params=True,        # store the trained parameter weights inside the model file<br/><NewLine>opset_version=9,          # the ONNX version to export the model to<br/><NewLine>do_constant_folding=True,  # whether to execute constant folding for optimization<br/><NewLine>input_names = [‘input’],   # the model’s input names<br/><NewLine>output_names = [‘pred_traj_tensor’], # the model’s output names<br/><NewLine>dynamic_axes={‘input’ : {0 : ‘batch_size’},    # variable lenght axes<br/><NewLine>‘pred_traj_tensor’ : {0 : ‘batch_size’}}, verbose=True)<br/><NewLine></code></p><NewLine><p>Runtime Error:<br/><NewLine><code><br/><NewLine>File “/home/weide/anaconda3/envs/fuel-py36/lib/python3.6/site-packages/torch/onnx/utils.py”, line 144, in _optimize_graph<br/><NewLine>torch._C._jit_pass_onnx_peephole(graph, _export_onnx_opset_version, fixed_batch_size)<br/><NewLine>RuntimeError: ArrayRef: invalid index Index = 0; Length = 0 (at at /opt/conda/conda-bld/pytorch_1579022034529/work/c10/util/ArrayRef.h:197)<br/><NewLine></code></p><NewLine><p>Really don’t know how to debug further. Any help is greatly appreciated !</p><NewLine><p>Here is the detailed stack trace.</p><NewLine><code><NewLine>Traceback (most recent call last):<NewLine>  File ""learning_algorithms/prediction/datasets/apollo_vehicle_trajectory_dataset/convert_onnx.py"", line 113, in <NewLine>    'pred_traj_tensor' : {0 : 'batch_size'}})<NewLine>  File ""/home/weide/anaconda3/envs/fuel-py36/lib/python3.6/site-packages/torch/onnx/__init__.py"", line 148, in export<NewLine>    strip_doc_string, dynamic_axes, keep_initializers_as_inputs)<NewLine>  File ""/home/weide/anaconda3/envs/fuel-py36/lib/python3.6/site-packages/torch/onnx/utils.py"", line 66, in export<NewLine>    dynamic_axes=dynamic_axes, keep_initializers_as_inputs=keep_initializers_as_inputs)<NewLine>  File ""/home/weide/anaconda3/envs/fuel-py36/lib/python3.6/site-packages/torch/onnx/utils.py"", line 418, in _export<NewLine>    fixed_batch_size=fixed_batch_size)<NewLine>  File ""/home/weide/anaconda3/envs/fuel-py36/lib/python3.6/site-packages/torch/onnx/utils.py"", line 298, in _model_to_graph<NewLine>    fixed_batch_size=fixed_batch_size, params_dict=params_dict)<NewLine>  File ""/home/weide/anaconda3/envs/fuel-py36/lib/python3.6/site-packages/torch/onnx/utils.py"", line 144, in _optimize_graph<NewLine>    torch._C._jit_pass_onnx_peephole(graph, _export_onnx_opset_version, fixed_batch_size)<NewLine>RuntimeError: ArrayRef: invalid index Index = 0; Length = 0 (at at /opt/conda/conda-bld/pytorch_1579022034529/work/c10/util/ArrayRef.h:197)<NewLine>frame #0: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x47 (0x7fb2baa2c627 in /home/weide/anaconda3/envs/fuel-py36/lib/python3.6/site-packages/torch/lib/libc10.so)<NewLine>frame #1:  + 0x6f484b (0x7fb2ed63184b in /home/weide/anaconda3/envs/fuel-py36/lib/python3.6/site-packages/torch/lib/libtorch_python.so)<NewLine>frame #2:  + 0x7003ec (0x7fb2ed63d3ec in /home/weide/anaconda3/envs/fuel-py36/lib/python3.6/site-packages/torch/lib/libtorch_python.so)<NewLine>frame #3: torch::jit::PeepholeOptimizeONNX(std::shared_ptr&amp;, int, bool) + 0x125 (0x7fb2ed642985 in /home/weide/anaconda3/envs/fuel-py36/lib/python3.6/site-packages/torch/lib/libtorch_python.so)<NewLine>frame #4:  + 0x6b496c (0x7fb2ed5f196c in /home/weide/anaconda3/envs/fuel-py36/lib/python3.6/site-packages/torch/lib/libtorch_python.so)<NewLine>frame #5:  + 0x28c076 (0x7fb2ed1c9076 in /home/weide/anaconda3/envs/fuel-py36/lib/python3.6/site-packages/torch/lib/libtorch_python.so)<NewLine><NewLine>frame #36: __libc_start_main + 0xe7 (0x7fb30ac9ab97 in /lib/x86_64-linux-gnu/libc.so.6)<NewLine></code><NewLine></div>",https://discuss.pytorch.org/u/weidezhang,(Weide Zhang),weidezhang,"June 9, 2020, 11:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/weidezhang"">@weidezhang</a>, seems like an ONNX bug. I would suggest raising an issue on the pytorch repo.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>created: <a href=""https://github.com/pytorch/pytorch/issues/39762"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/39762</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/eellison"">@eellison</a>, could you help me to see who is able to look at the issue on github ? I have updated the details(model and test script) on github.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/weidezhang; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/weidezhang; <NewLine> ,"REPLY_DATE 1: June 9, 2020, 11:47pm; <NewLine> REPLY_DATE 2: June 9, 2020, 11:58pm; <NewLine> REPLY_DATE 3: June 29, 2020, 10:29pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
87332,ONNX is_in_onnx_export,2020-06-29T17:28:55.775Z,0,53,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have already created <a href=""https://github.com/pytorch/pytorch/issues/27265"" rel=""nofollow noopener"">this</a> issue.</p><NewLine><p>Many times we want to know if we are in the middle of exporting to ONNX to select a different path (say MemoryEfficientSwish vs Swish). Passing a boolean around all the time is not a great idea. So far I have been using <code>torch._C._get_tracing_state()</code> but it is private API.</p><NewLine><p>I just noticed there is  <code>torch.onnx.is_in_onnx_export()</code> . Is it equivalent to  <code>torch._C._get_tracing_state()</code> ?</p><NewLine></div>",https://discuss.pytorch.org/u/dashesy,(dashesy),dashesy,"June 29, 2020,  5:28pm",,,,,
87132,Errors when trying to run jit script,2020-06-27T18:07:32.766Z,0,68,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was trying to run this code section and with my limited understanding of jit, thought I can get a performance improvement on the function.</p><NewLine><pre><code class=""lang-auto"">@torch.jit.script<NewLine>def nms_pytorch2(dets, thresh):<NewLine>    overlap = bbox_overlap(dets, dets)<NewLine><NewLine>    treshold_matrix = torch.tril((overlap &gt; thresh), diagonal=-1)<NewLine><NewLine>    # Tensor elements indicate whether box should be kept<NewLine>    is_maximum = treshold_matrix.new_ones(dets.shape[0])<NewLine><NewLine>    # loop over all boxes with highest confidence in the scene<NewLine>    # Apply this vectorized over all boxes in the batch.<NewLine>    for box in treshold_matrix.unbind(-1):<NewLine>        # Disable all other boxes in the same scene if the current box is not<NewLine>        # disabled.<NewLine>        is_maximum = is_maximum &amp; ~box<NewLine><NewLine>        # Also disable the overlaps of boxes which getting disabled right now.<NewLine>        treshold_matrix &amp;= ~box.unsqueeze(-2)<NewLine><NewLine>    return is_maximum<NewLine></code></pre><NewLine><p>However, I got the following error and am not sure how to handle it</p><NewLine><pre><code class=""lang-auto"">torch.jit.frontend.NotSupportedError: unsupported kind of augumented assignment: BitAnd:<NewLine>         # Also disable the overlaps of boxes which getting disabled right now.<NewLine>        treshold_matrix &amp;= ~box.unsqueeze(-2)<NewLine>                        ~~ &lt;--- HERE<NewLine>    return is_maximum<NewLine></code></pre><NewLine><p>Your help would be really appreciated</p><NewLine></div>",https://discuss.pytorch.org/u/chinmay5,(Chinmay5),chinmay5,"June 27, 2020,  6:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just figured the issue out, the operations need to be non in-place. Just putting a temp variable and assigning value of this temp variable in the next line solved the issue for me.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/chinmay5; <NewLine> ,"REPLY_DATE 1: June 27, 2020,  7:26pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
86972,Typing in Pytorch,2020-06-26T02:08:03.710Z,0,85,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I use type annotation for my code.</p><NewLine><p>Is there any typing annotation guideline for pytorch?</p><NewLine><p>I want to do something like this.</p><NewLine><pre><code class=""lang-auto"">class MyModule(nn.Module):<NewLine>  def __init__(self):<NewLine>    super().__init__()<NewLine>    self.linear = nn.Linear(10, 4)<NewLine><NewLine>  def forward(self, x: torch.Tensor[torch.float, [B, 10]]) -&gt; torch.Tensor[torch.float, [B, 4]]:<NewLine>    return self.linear(x)<NewLine></code></pre><NewLine><p>I guess torchScript or some other approach, e.g. View(?), could support this one, but I couldn’t find official or near-official document.</p><NewLine><p>Is there anyone who know about this, help me <img alt="":smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smile.png?v=9"" title="":smile:""/></p><NewLine><p>Thanks,</p><NewLine></div>",https://discuss.pytorch.org/u/658130434af411741ceb,(Seonghyeon Lee),658130434af411741ceb,"June 26, 2020,  2:08am",1 Like,,,,
86939,Different behaviors: nn.Module vs ScriptModule,2020-06-25T18:44:31.105Z,0,69,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I noticed some annoying differences between nn.Module and ScriptModule</p><NewLine><p><code>torch.__version__ = 1.4.0</code></p><NewLine><p>Example:</p><NewLine><pre><code class=""lang-auto"">class DataClass(nn.Module):<NewLine>    my_data: List[int]<NewLine>    <NewLine>    def __init__(self):<NewLine>        super(DataClass, self).__init__()<NewLine>        self.my_data = []<NewLine>    <NewLine>    def forward(self, new: int):<NewLine>        self.my_data.append(new)<NewLine>        return self.my_data<NewLine></code></pre><NewLine><p>nn.Module:</p><NewLine><pre><code class=""lang-auto"">dat = DataClass()<NewLine>dat(0)<NewLine>dat.my_data.append(1)<NewLine>print(dat.my_data)<NewLine>&gt;&gt;&gt;  [0, 1]<NewLine></code></pre><NewLine><p>ScriptModule</p><NewLine><pre><code class=""lang-auto"">dat = DataClass()<NewLine>script_dat = torch.jit.script(dat)<NewLine>script_dat(0)<NewLine>script_dat.my_data.append(1)<NewLine>print(script_dat.my_data)<NewLine><NewLine>&gt;&gt;&gt;  [0]<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Chenchao_Zhao,(Chenchao Zhao),Chenchao_Zhao,"June 25, 2020,  7:26pm",,,,,
86791,Calling .eval() on a torch.jit.ScriptModule leaves paramers.requires_grad True,2020-06-24T22:29:31.324Z,0,66,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was having trouble with memory usage of my traced model in C++, and I discovered that <code>.eval()</code> doesn’t change the <code>requires_grad</code> for the parameters in my ScriptModule. Is this intended behaviour? I can say that as a user this was not expected behaviour. As a user I would like it to work as I expect, to warn, or to raise. Given that I can manually set the requires_grad behaviour, it seems like my expected behaviour is possible?</p><NewLine><p>I think the underlying cause is that <code>my_script_module.layer</code> is a <code>RecursiveScriptModule</code> and has no <code>.children()</code>.</p><NewLine><p>PyTorch 1.5</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine><NewLine>class MyScriptModule(torch.jit.ScriptModule):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.layer = torch.nn.Linear(1, 1, bias=False)<NewLine><NewLine><NewLine>my_script_module = MyScriptModule()<NewLine><NewLine># [True]<NewLine>print([p.requires_grad for p in my_script_module.parameters()])<NewLine><NewLine>my_script_module.eval()<NewLine><NewLine># [True] :(<NewLine>print([p.requires_grad for p in my_script_module.parameters()])<NewLine><NewLine>for p in my_script_module.parameters():<NewLine>    p.requires_grad = False<NewLine><NewLine># [False] :)<NewLine>print([p.requires_grad for p in my_script_module.parameters()])<NewLine></code></pre><NewLine><hr/><NewLine><p>I know this isn’t a totally normal thing to be doing. For what it’s worth, I am subclassing <code>ScrtipModule</code> in this way so that I can do the following. Maybe I should do something differently?</p><NewLine><pre><code class=""lang-auto"">class MyModule(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.inner = MyScriptModule()<NewLine><NewLine>    def forward(self, x):<NewLine>        # stuff<NewLine>        x = self.inner(x)<NewLine>        # more stuff<NewLine>        return x<NewLine><NewLine><NewLine>class MyScriptModule(torch.jit.ScriptModule):<NewLine>    """""" Psuedo-code """"""<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.layer = torch.nn.Linear(1, 1, bias=False)<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def forward(self, x):<NewLine>        out = torch.zeros_like(x)<NewLine>        for i in range(x.size()[0]):<NewLine>            out = self.layer(x)<NewLine>        return out<NewLine><NewLine><NewLine>my_module = MyModule()<NewLine>my_module.eval()<NewLine><NewLine>torch.jit.trace(my_module, sample)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/alekseynp,(Aleksey Nozdryn-Plotnicki),alekseynp,"June 24, 2020, 10:29pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""86791"" data-username=""alekseynp""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/alekseynp/40/26000_2.png"" width=""20""/> alekseynp:</div><NewLine><blockquote><NewLine><p>I discovered that <code>.eval()</code> doesn’t change the <code>requires_grad</code> for the parameters in my ScriptModule. Is this intended behaviour?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, this is expected and also won’t change the <code>requires_grad</code> attributes of an “eager” model.<br/><NewLine><code>model.eval()</code> and <code>model.train()</code> change the internal <code>self.training</code> flag of all modules recursively starting from the parent module. By doing so, the behavior of some layers will be changed.<br/><NewLine>E.g. dropout will be disabled and batchnorm layers will use their running statistics to normalize the incoming data instead of calculating the batch statistics.</p><NewLine><p>If you want to freeze the parameters, you would have to set their <code>.requires_grad</code> attribute to <code>False</code>.</p><NewLine><p>You could of course use both in combination, e.g. freeze all parameters, but leave the dropout layers enabled, or let all parameters train, but use the running stats of batchnorm layers.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: June 25, 2020,  6:35am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
86785,Understanding PyTorch operators,2020-06-24T21:54:31.804Z,0,66,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Mainly looking for information about aten and prim operators. Is there documentation to look at all of them? What is the total number of them? It seems there’s a C API to use them directly but is there a Python API as well? Basically I want to be able to build a torchscript graph node by node with specific operators.</p><NewLine></div>",https://discuss.pytorch.org/u/flynntax,,flynntax,"June 24, 2020,  9:54pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The aten and prim namespaces are considered internal details so we don’t really have user-facing documentation explaining them. The closest (for aten) is maybe <a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md</a>.</p><NewLine><p>There is are Python bindings to the PyTorch IR in (<a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/python/python_ir.cpp"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/python/python_ir.cpp</a>), but once again they are considered internal.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: June 24, 2020, 11:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
86774,Debugging JIT tracing/scripting,2020-06-24T19:02:43.695Z,0,106,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m currently trying to JIT compile <a href=""https://github.com/xingyizhou/CenterTrack"" rel=""nofollow noopener"">CenterTrack</a> with either tracing or scripting.</p><NewLine><p>CenterTrack uses <a href=""https://github.com/CharlesShang/DCNv2"" rel=""nofollow noopener"">DCNv2</a>. I’ve found some resources that mentioned I needed to fix a couple things first:</p><NewLine><ul><NewLine><li>change code that uses Python integers instead of tensors. Got that from: <a href=""https://github.com/xi11xi19/CenterNet2TorchScript"" rel=""nofollow noopener"">https://github.com/xi11xi19/CenterNet2TorchScript</a><NewLine></li><NewLine><li>possibly update to torch 1.5 by making code changes described here: <a href=""https://github.com/CharlesShang/DCNv2/pull/58"" rel=""nofollow noopener"">https://github.com/CharlesShang/DCNv2/pull/58</a> (applied these to the above DCNv2 version instead of the original version)</li><NewLine></ul><NewLine><p>Now I’ve removed some of the initial errors I got when trying to trace, but am stuck the current ones.<br/><NewLine>When I try to trace:</p><NewLine><pre><code class=""lang-auto""># Try trace<NewLine>input_var = torch.zeros([1, 3, 736, 1280], dtype=torch.float32).cuda()<NewLine>try:	<NewLine>    traced_script_module = torch.jit.trace(detector.model, input_var)<NewLine>except Exception as e:<NewLine>    print(f'Exception (trace): {e}')<NewLine></code></pre><NewLine><p>The only thing that is returned is:</p><NewLine><blockquote><NewLine><p>Exception (trace): Only tensors, lists and tuples of tensors can be output from traced functions</p><NewLine></blockquote><NewLine><p>And when I instead try torch script I have two issues, a small one being requirements of default values for tensors, but a larger one being heavy use of *args as inputs throughout the codebase.</p><NewLine><p>I suspect that the torch trace issue comes from some forward() methods returning dictionaries. Is that not supported in v1.5? Is there a recommended alternative? And also is there a way to find out where in my python code the error “Only tensors, lists and tuples of tensors can be output from traced functions” refers to?</p><NewLine><p>Any help is greatly appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/austinmw,(Austin),austinmw,"June 24, 2020,  7:07pm",,,,,
86735,List of JIT-supported operations,2020-06-24T14:14:28.263Z,2,70,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, may I know where I can find a list of JIT-supported or not supported operations in Pytorch please?</p><NewLine></div>",https://discuss.pytorch.org/u/tengerye,(Tengerye),tengerye,"June 24, 2020,  2:14pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>supported PyTorch stuff: <a href=""https://pytorch.org/docs/master/jit_builtin_functions.html#builtin-functions"" rel=""nofollow noopener"">https://pytorch.org/docs/master/jit_builtin_functions.html#builtin-functions</a><br/><NewLine>unsupported PyTorch stuff: <a href=""https://pytorch.org/docs/master/jit_unsupported.html#jit-unsupported"" rel=""nofollow noopener"">https://pytorch.org/docs/master/jit_unsupported.html#jit-unsupported</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you so much. But I can’t find <code>SyncBatchNorm</code> in either page. Is there something wrong?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tengerye; <NewLine> ,"REPLY_DATE 1: June 24, 2020,  4:46pm; <NewLine> REPLY_DATE 2: June 24, 2020,  4:56pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
86555,Unable to view optimized graph while using torch.jit.script,2020-06-23T13:09:28.542Z,0,52,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have currently started learning torchscript and tried to visualize the optimized graph but I am unsuccessful. The following is my code</p><NewLine><pre><code class=""lang-auto"">@torch.jit.script<NewLine>def cell_end(ingate, forgetgate, cellgate, outgate, cx):<NewLine>    ingate = torch.sigmoid(ingate)<NewLine>    forgetgate = torch.sigmoid(forgetgate)<NewLine>    cellgate = torch.tanh(cellgate)<NewLine>    outgate = torch.sigmoid(outgate)<NewLine><NewLine>    cy = (forgetgate * cx) + (ingate * cellgate)<NewLine>    hy = outgate * torch.tanh(cy)<NewLine><NewLine>    return hy, cy<NewLine></code></pre><NewLine><p>For the above function i got the following graph</p><NewLine><pre><code class=""lang-auto"">graph(%ingate.1 : Tensor,<NewLine>      %forgetgate.1 : Tensor,<NewLine>      %cellgate.1 : Tensor,<NewLine>      %outgate.1 : Tensor,<NewLine>      %cx.1 : Tensor):<NewLine>  %19 : int = prim::Constant[value=1]()<NewLine>  %ingate.3 : Tensor = aten::sigmoid(%ingate.1) # &lt;ipython-input-2-8da29633480c&gt;:3:13<NewLine>  %forgetgate.3 : Tensor = aten::sigmoid(%forgetgate.1) # &lt;ipython-input-2-8da29633480c&gt;:4:17<NewLine>  %cellgate.3 : Tensor = aten::tanh(%cellgate.1) # &lt;ipython-input-2-8da29633480c&gt;:5:15<NewLine>  %outgate.3 : Tensor = aten::sigmoid(%outgate.1) # &lt;ipython-input-2-8da29633480c&gt;:6:14<NewLine>  %15 : Tensor = aten::mul(%forgetgate.3, %cx.1) # &lt;ipython-input-2-8da29633480c&gt;:8:10<NewLine>  %18 : Tensor = aten::mul(%ingate.3, %cellgate.3) # &lt;ipython-input-2-8da29633480c&gt;:8:30<NewLine>  %cy.1 : Tensor = aten::add(%15, %18, %19) # &lt;ipython-input-2-8da29633480c&gt;:8:10<NewLine>  %23 : Tensor = aten::tanh(%cy.1) # &lt;ipython-input-2-8da29633480c&gt;:9:19<NewLine>  %hy.1 : Tensor = aten::mul(%outgate.3, %23) # &lt;ipython-input-2-8da29633480c&gt;:9:9<NewLine>  %27 : (Tensor, Tensor) = prim::TupleConstruct(%hy.1, %cy.1)<NewLine>  return (%27)<NewLine></code></pre><NewLine><p>Running with an input</p><NewLine><pre><code class=""lang-auto"">inp = torch.randn(5, 10, 4)<NewLine>cell_end(*inp)<NewLine></code></pre><NewLine><p>Even after running the input over the graph I got the same graph</p><NewLine><pre><code class=""lang-auto"">graph(%ingate.1 : Tensor,<NewLine>      %forgetgate.1 : Tensor,<NewLine>      %cellgate.1 : Tensor,<NewLine>      %outgate.1 : Tensor,<NewLine>      %cx.1 : Tensor):<NewLine>  %5 : int = prim::Constant[value=1]()<NewLine>  %ingate.3 : Tensor = aten::sigmoid(%ingate.1) # &lt;ipython-input-2-8da29633480c&gt;:3:13<NewLine>  %forgetgate.3 : Tensor = aten::sigmoid(%forgetgate.1) # &lt;ipython-input-2-8da29633480c&gt;:4:17<NewLine>  %cellgate.3 : Tensor = aten::tanh(%cellgate.1) # &lt;ipython-input-2-8da29633480c&gt;:5:15<NewLine>  %outgate.3 : Tensor = aten::sigmoid(%outgate.1) # &lt;ipython-input-2-8da29633480c&gt;:6:14<NewLine>  %10 : Tensor = aten::mul(%forgetgate.3, %cx.1) # &lt;ipython-input-2-8da29633480c&gt;:8:10<NewLine>  %11 : Tensor = aten::mul(%ingate.3, %cellgate.3) # &lt;ipython-input-2-8da29633480c&gt;:8:30<NewLine>  %cy.1 : Tensor = aten::add(%10, %11, %5) # &lt;ipython-input-2-8da29633480c&gt;:8:10<NewLine>  %13 : Tensor = aten::tanh(%cy.1) # &lt;ipython-input-2-8da29633480c&gt;:9:19<NewLine>  %hy.1 : Tensor = aten::mul(%outgate.3, %13) # &lt;ipython-input-2-8da29633480c&gt;:9:9<NewLine>  %15 : (Tensor, Tensor) = prim::TupleConstruct(%hy.1, %cy.1)<NewLine>  return (%15)<NewLine></code></pre><NewLine><p>I don’t know the reason behind it, Anyone please help me with this</p><NewLine></div>",https://discuss.pytorch.org/u/Midhilesh,(Midhilesh),Midhilesh,"June 23, 2020,  1:09pm",,,,,
62202,How to transform a pytorch JIT pth model to caffe prototxt/caffemodel,2019-11-26T07:56:49.665Z,1,493,"<div class=""post"" itemprop=""articleBody""><NewLine><p>For example:<br/><NewLine>I use the static quantization tutorial and generate a scripted quantized model.<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a></p><NewLine><p>Since the quantized model is different from the float model owing to quantizing (combining conv+bn into conv and so on). So I can’t obtain the .py file descripting the quantized model after quantization. Therefore I would like to know how to generate caffe prototxt which draws the whole network?</p><NewLine></div>",https://discuss.pytorch.org/u/arsen,(arsen),arsen,"November 26, 2019,  7:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you elaborate on your higher-level goals a little bit? We don’t have any direct way to produce a caffe2 model from a PyTorch model, but you can see a description of the compiled model like so</p><NewLine><pre><code class=""lang-python"">model = torch.jit.load(model_file)<NewLine>print(model.graph)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>What about exporting to caffe2 in an indirect way? Is it possible to somehow use the scale/zero_point and get the same outputs as in PyTorch?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Shani_Gamrian; <NewLine> ,"REPLY_DATE 1: December 2, 2019,  9:02pm; <NewLine> REPLY_DATE 2: June 23, 2020,  9:51am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
85977,torch::jit::script::Module::Forward failes (LoadLibraryA) on c++ static lib for windows,2020-06-18T20:17:49.923Z,7,175,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to train on linux (python) and do inference on windows with c++ static lib application.<br/><NewLine>When calling torch::jit::script::Module::Forward(), following error occurs.<br/><NewLine>The application with dll does not fail.</p><NewLine><pre><code class=""lang-auto"">The following operation failed in the TorchScript interpreter.<NewLine>Traceback of TorchScript, serialized code (most recent call last):<NewLine>  File ""code/__torch__/Model.py"", line 37, in forward<NewLine>    _19 = (_6).forward((_7).forward((_8).forward(_18, ), ), )<NewLine>    input0 = torch.cat([(_5).forward(_19, ), _15], 1)<NewLine>    _20 = (_3).forward((_4).forward(input0, ), )<NewLine>                        ~~~~~~~~~~~ &lt;--- HERE<NewLine>    _21 = (_2).forward((_14).forward2(_20, ), )<NewLine>    return (_0).forward((_1).forward(_21, ), )<NewLine>  File ""code/__torch__/CompactModel.py"", line 36, in forward<NewLine>    _18 = (_9).forward((_10).forward(_17, ), )<NewLine>    _19 = (_6).forward((_7).forward((_8).forward(_18, ), ), )<NewLine>    input0 = torch.cat([(_5).forward(_19, ), _15], 1)<NewLine>             ~~~~~~~~~ &lt;--- HERE<NewLine>    _20 = (_3).forward((_4).forward(input0, ), )<NewLine>    _21 = (_2).forward((_14).forward2(_20, ), )<NewLine><NewLine>Traceback of TorchScript, original code (most recent call last):<NewLine>/docker_share/source/Model.py(193): forward<NewLine>/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py(534): _slow_forward<NewLine>/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py(548): __call__<NewLine>/usr/local/lib/python3.8/site-packages/torch/jit/__init__.py(1027): trace_module<NewLine>/usr/local/lib/python3.8/site-packages/torch/jit/__init__.py(873): trace<NewLine>./source/jitrace.py(700): &lt;module&gt;<NewLine>RuntimeError: error in LoadLibraryA<NewLine></code></pre><NewLine><p>The error message changed from “LoadLibraryA” to “GetProcAddress” when I placed “caffe2_nvrtc.dll” next to the application.  caffe2_nvrtc.dll is created under build/bin and caffe2_nvrtc.lib is not created.<br/><NewLine>Is caffe2_nvrtc.dll related this problem ?</p><NewLine><p>Do you have any suggestion ?<br/><NewLine>Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/tkr,,tkr,"June 18, 2020,  8:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please copy all the DLLs to the directory of your application.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>There are NO DLLs in my application dir.<br/><NewLine>The cuda and cudnn libraries are in the $PATH.</p><NewLine><p>Although I do not want to use dynamic link library,<br/><NewLine>application succeeded by below.</p><NewLine><ul><NewLine><li>link official distributed “caffe2_nvrtc.lib”</li><NewLine><li>place “caffe2_nvcrt.dll” next to the application.</li><NewLine></ul><NewLine><p>So, to succeed with static library, I tried below and it didn’t work.</p><NewLine><ul><NewLine><li>change CMakeFiles.txt under “caffe2” directory as below and run build.</li><NewLine></ul><NewLine><pre><code class=""lang-auto"">565c565<NewLine>&lt;     add_library(caffe2_nvrtc SHARED ${ATen_NVRTC_STUB_SRCS})<NewLine>---<NewLine>&gt;     add_library(caffe2_nvrtc ${ATen_NVRTC_STUB_SRCS})<NewLine></code></pre><NewLine><ul><NewLine><li>caffe2_nvrtc.lib is created (and caffe2_nvrtc.dll is not).</li><NewLine><li>link it to the app.</li><NewLine></ul><NewLine><p>But it does work if “caffe2_nvrtc.dll” (the official one) is placed next to the application.</p><NewLine><p>The difference between the loaded DLLs when the application succeeds or fails are :</p><NewLine><ul><NewLine><li>caffe2_nvrtc.dll</li><NewLine><li>nvrtc64_101_0.dll</li><NewLine></ul><NewLine><p>So what should I do to succeed with static cuffe2_nvrtc.lib?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, we don’t provide static libs currently. You’ll have to build that from source.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""85977"" data-username=""tkr""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tkr/40/16838_2.png"" width=""20""/> tkr:</div><NewLine><blockquote><NewLine><p>cuffe2_nvrtc.lib</p><NewLine></blockquote><NewLine></aside><NewLine><p>Unlike the relations between <code>.so</code> and <code>.a</code> on Linux, <code>.lib</code> files don’t necessarily refer to the file name of a static lib. It can also be an import library for the DLL.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Hey, we don’t provide static libs currently. You’ll have to build that from source.</p><NewLine></blockquote><NewLine><p>Let me rephrase what I wanted to say…<br/><NewLine>What I want to do is to create successful static version of libtorch.<br/><NewLine>I have built the static  libtorch from source with “set BUILD_SHARED_LIBS=OFF” BUT</p><NewLine><ul><NewLine><li>cuffe2_nvrtc.lib is NOT created.</li><NewLine><li>caffe2_nvrtc.dll is created under torch/bin.</li><NewLine></ul><NewLine><p>My assumption is below.<br/><NewLine>“SHARED” is set in add_library and because of that, DLL is created.  At the same time,<br/><NewLine>BUILD_SHARED_LIBS=OFF and dllexport is not defined and lib is not created. Files under lib are linked to the application and error occurs.</p><NewLine><p>I now know by using official caffe2_nvrtc.lib and caffe2_nvrtc.dll, the application succeeds.<br/><NewLine>I’d like to know how to create static libtorch library.</p><NewLine><p>I’ve done rewriting CMakeList.txt and built static version of libtorch and</p><NewLine><ul><NewLine><li>caffe2_nvrtc.lib is created (and this should be static lib, right?)</li><NewLine><li>caffe2_nvrtc.dll is NOT created.</li><NewLine></ul><NewLine><p>By linking above to the app, it will end up with the error I wrote in the beginning.<br/><NewLine>By placing official caffe2_nvrtc.dll next to the app, it works. (meaning static lib is not created correctly ?)</p><NewLine><p>Any suggestion to succeed my attempt?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Maybe some of the libs are optimized away. You could try passing <code>/WHOLEARCHIVE:caffe2_nvrtc.lib</code> in your project to force the linker to stop doing that.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I thought the same way.</p><NewLine><ul><NewLine><li>/WHOLEARCHIVE:caffe2_nvrtc.lib</li><NewLine><li>link C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\lib\x64\nvrtc.lib</li><NewLine></ul><NewLine><p>I tried above and still get the same error.<br/><NewLine>The application does not load “nvrtc64_101_0.dll”</p><NewLine><p>Any other suggestion?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>use WSL2 to run ubuntu on WINDOW!</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>You may need to link other CUDA libs as well, like cudart64_xxx.lib. But some of them doesn’t have static libs so you still need some DLLs.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well, I don’t think WSL2 is a good choice for deployment.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve tried few other things and below change on caffe2/CMakeList.txt worked!</p><NewLine><pre><code class=""lang-diff"">diff --git a/caffe2/CMakeLists.txt b/caffe2/CMakeLists.txt<NewLine>index 8025a7de3c..8e94978e72 100644<NewLine>--- a/caffe2/CMakeLists.txt<NewLine>+++ b/caffe2/CMakeLists.txt<NewLine>@@ -561,7 +561,7 @@ if (NOT INTERN_BUILD_MOBILE OR NOT BUILD_CAFFE2_MOBILE)<NewLine>       ${TORCH_SRC_DIR}/csrc/cuda/comm.cpp<NewLine>       ${TORCH_SRC_DIR}/csrc/jit/tensorexpr/cuda_codegen.cpp<NewLine>     )<NewLine>-    add_library(caffe2_nvrtc SHARED ${ATen_NVRTC_STUB_SRCS})<NewLine>+    add_library(caffe2_nvrtc ${ATen_NVRTC_STUB_SRCS})<NewLine>     target_link_libraries(caffe2_nvrtc ${CUDA_NVRTC} ${CUDA_CUDA_LIB} ${CUDA_NVRTC_LIB})<NewLine>     target_include_directories(caffe2_nvrtc PRIVATE ${CUDA_INCLUDE_DIRS})<NewLine>     install(TARGETS caffe2_nvrtc DESTINATION ""${TORCH_INSTALL_LIB_DIR}"")<NewLine>@@ -703,6 +703,9 @@ ELSEIF(USE_CUDA)<NewLine>   cuda_add_library(torch_cuda ${Caffe2_GPU_SRCS})<NewLine>   set(CUDA_LINK_LIBRARIES_KEYWORD)<NewLine>   torch_compile_options(torch_cuda)  # see cmake/public/utils.cmake<NewLine>+  if (NOT BUILD_SHARED_LIBS)<NewLine>+    target_compile_definitions(torch_cuda PRIVATE USE_DIRECT_NVRTC)<NewLine>+  endif()<NewLine>   if (USE_NCCL)<NewLine>     target_link_libraries(torch_cuda PRIVATE __caffe2_nccl)<NewLine></code></pre><NewLine><p>In aten/src/ATen/cuda/detail/CUDAHooks.cpp, “<span class=""hashtag"">#ifdef</span> USE_DIRECT_NVRTC” directive is used.<br/><NewLine>But “USE_DIRECT_NVRTC” was not defined in any CMakeList.txt and because of that,<br/><NewLine>application linked with satic libtorch tries to load “caffe2_nvrtc.dll”.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""12"" data-topic=""85977"" data-username=""tkr""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tkr/40/16838_2.png"" width=""20""/> tkr:</div><NewLine><blockquote><NewLine><p>USE_DIRECT_NVRTC</p><NewLine></blockquote><NewLine></aside><NewLine><p>Interesting facts. Thanks for the finding.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/peterjc123; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tkr; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/peterjc123; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/peterjc123; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/tkr; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/peterjc123; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/tkr; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Scott_Hoang; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/peterjc123; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/peterjc123; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/tkr; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/peterjc123; <NewLine> ,"REPLY_DATE 1: June 20, 2020,  4:31am; <NewLine> REPLY_DATE 2: June 20, 2020,  4:24pm; <NewLine> REPLY_DATE 3: June 20, 2020,  4:56pm; <NewLine> REPLY_DATE 4: June 20, 2020,  4:59pm; <NewLine> REPLY_DATE 5: June 20, 2020,  6:06pm; <NewLine> REPLY_DATE 6: June 21, 2020,  8:04am; <NewLine> REPLY_DATE 7: June 21, 2020,  5:08pm; <NewLine> REPLY_DATE 8: June 21, 2020,  5:11pm; <NewLine> REPLY_DATE 9: June 22, 2020,  3:14am; <NewLine> REPLY_DATE 10: June 22, 2020,  3:16am; <NewLine> REPLY_DATE 11: June 22, 2020,  3:02pm; <NewLine> REPLY_DATE 12: June 22, 2020,  3:23pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> 
62478,Torch.jit.trace() only works on example input?,2019-11-29T03:09:35.851Z,5,1385,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve created a model with a forward function that takes “x” as input (image of size (3,416,416)). I create a trace of the model using: <code>module = torch.jit.trace(model, example_forward_input)</code>, then save that model using <code>module.save(""model.pt"")</code>. Then I load this model trace into an Android application. When I send an input to the model (from the phone) that is identical to the input used as “example_forward_input”, I get the correct result. However, when I use any other input tensor (same shape), I get poor results. Is this supposed to be the behaviour of the trace function? Is there a function that traces a model that can generalize to any inputs? Any guidance would be much appreciated.</p><NewLine><p>For some more detail: This is a YOLOv3 based model that involves detection and classification. The classification with different inputs into the traced model gives similar results to the same inputs in the model. However, the detection locations differ (in w/h especially) when running an input that was not used as an example through the traced model.</p><NewLine><p>EDIT: I’m guessing this is due to the fact that my forward module uses control-flow that is dependent on the input, as outlined <a href=""https://pytorch.org/tutorials/advanced/cpp_export.html"" rel=""nofollow noopener"">here</a>. However, when I try to convert the model to a script module, as outlined on that same page. I get the following error: <code>raise NotSupportedError(ctx_range, _vararg_kwarg_err) torch.jit.frontend.NotSupportedError: Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults:   File ""C:\Users\isaac\Anaconda3\envs\SNProject\lib\site-packages\torch\nn\modules\module.py"", line 85     def forward(self, *input):                       ~~~~~~ &lt;--- HERE         print(torch.__version__)         r""""""Defines the computation performed at every call.</code> As you can see this is coming from the torch library itself. Any suggestions on how to proceed?</p><NewLine></div>",https://discuss.pytorch.org/u/IsaacBerman,(Isaac Berman),IsaacBerman,"November 29, 2019,  4:26am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You are right about the input-dependent control flow requiring <code>torch.jit.script</code> instead of <code>torch.jit.trace</code>. Can you link the YoloV3 implementation you’re using so we can reproduce this error?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I solved the problem. I ended up using torch.jit.trace(), but then having my YOLOLayer inherit a ScriptModule: <code>class YOLOLayer(torch.jit.ScriptModule):</code>, and made my forward method:</p><NewLine><p><code>@torch.jit.script_method     def forward(self, x, targets=torch.tensor([]), img_dim=torch.tensor(416)):</code><br/><NewLine>and helper method<br/><NewLine><code>@torch.jit.script_method     def compute_grid_offsets(self, grid_size):</code> decorated with <span class=""mention"">@torch.jit.script_method</span>. After I did this I just went line-by-line fixing any errors that appeared due to incompatibility with the scripting.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Good to hear you fixed it! We changed the API to TorchScript in PyTorch 1.2 to make it easier to use (i.e. you no longer need to change your model to inherit from <code>ScriptModule</code> instead of <code>nn.Module</code> and you don’t need <code>@script_method</code>), you can read more about it <a href=""https://pytorch.org/docs/master/jit.html#migrating-to-pytorch-1-2-recursive-scripting-api"" rel=""nofollow noopener"">here</a>. But this is just sugar over the same thing you’re already doing, so if you already have it working you don’t need to change anything.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, <a class=""mention"" href=""/u/isaacberman"">@IsaacBerman</a></p><NewLine><p>I’m trying to do it but can’t reproduce it for some errors.<br/><NewLine>Could you share the fixed codes?<br/><NewLine>Is the code based on <a href=""https://github.com/eriklindernoren/PyTorch-YOLOv3"" rel=""nofollow noopener"">https://github.com/eriklindernoren/PyTorch-YOLOv3</a>?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/junjihashimoto"">@junjihashimoto</a>,</p><NewLine><p>Yes it is. The only layer you have to change is YOLOLayer. Since this was just for tracing, I commented out the loss calculations. As follows:</p><NewLine><pre><code class=""lang-auto"">@torch.jit.script<NewLine>def compare_size(size1, size2):<NewLine>    return size1 != size2<NewLine><NewLine>@torch.jit.script<NewLine>def get_input(x):<NewLine>    return x<NewLine><NewLine>@torch.jit.script<NewLine>def get_pred_boxes(x, grid):<NewLine>    return x + grid<NewLine>@torch.jit.script<NewLine>def set_grid_size(x):<NewLine>    return torch.tensor(x.size(2))<NewLine><NewLine>@torch.jit.script<NewLine>def normalize_by_stride(anchors, stride):<NewLine>    return torch.div(anchors, stride)<NewLine></code></pre><NewLine><p>`class YOLOLayer(torch.jit.ScriptModule):<br/><NewLine>“”“Detection layer”""""</p><NewLine><pre><code>def __init__(self, anchors, num_classes, img_dim=416):<NewLine>    super(YOLOLayer, self).__init__()<NewLine>    self.anchors = torch.tensor(anchors)<NewLine>    self.num_anchors = len(anchors)<NewLine>    self.num_classes = num_classes<NewLine>    self.ignore_thres = 0.5<NewLine>    self.mse_loss = nn.MSELoss()<NewLine>    self.bce_loss = nn.BCELoss()<NewLine>    self.obj_scale = 1<NewLine>    self.noobj_scale = 100<NewLine>    self.metrics = {}<NewLine>    self.img_dim = torch.tensor(img_dim)<NewLine>    self.grid_size = torch.tensor(0) # grid size<NewLine>    self.stride = torch.tensor(0)<NewLine>    self.grid_x = torch.tensor([])<NewLine>    self.grid_y = torch.tensor([])<NewLine>    self.scaled_anchors = torch.tensor([])<NewLine>    self.anchor_w = torch.tensor([])<NewLine>    self.anchor_h = torch.tensor([])<NewLine><NewLine>@torch.jit.script_method<NewLine>def compute_grid_offsets(self, grid_size):<NewLine>    self.grid_size = grid_size.float()<NewLine>    g = self.grid_size.int()<NewLine><NewLine>    self.grid_size = self.grid_size.float()<NewLine><NewLine>    self.stride = self.img_dim / self.grid_size<NewLine><NewLine>    self.stride = self.stride.float()<NewLine>    <NewLine>    self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, int(g.item()), int(g.item())])<NewLine>    self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, int(g.item()), int(g.item())])<NewLine><NewLine>    <NewLine>    self.scaled_anchors = torch.div(self.anchors, self.stride)<NewLine><NewLine>    self.anchor_w = self.scaled_anchors[:, 0:1].reshape(1, self.num_anchors, 1, 1)<NewLine>    self.anchor_h = self.scaled_anchors[:, 1:2].reshape(1, self.num_anchors, 1, 1)<NewLine><NewLine><NewLine>@torch.jit.script_method<NewLine>def forward(self, x, targets=torch.tensor([]), img_dim=torch.tensor(416)):    <NewLine>    self.img_dim = img_dim<NewLine>    num_samples = x.size(0)<NewLine><NewLine>    grid_size = set_grid_size(x)<NewLine><NewLine>    <NewLine>    self.compute_grid_offsets(grid_size)<NewLine><NewLine>    prediction = (<NewLine>        x.view(num_samples, self.num_anchors, self.num_classes + 5, grid_size, grid_size)<NewLine>        .permute(0, 1, 3, 4, 2)<NewLine>        .contiguous()<NewLine>    )<NewLine><NewLine>    # Get outputs<NewLine>    x = torch.sigmoid(prediction[..., 0])  # Center x<NewLine>    y = torch.sigmoid(prediction[..., 1])  # Center y<NewLine>    w = prediction[..., 2]  # Width<NewLine>    h = prediction[..., 3]  # Height<NewLine>    pred_conf = torch.sigmoid(prediction[..., 4])  # Conf<NewLine>    pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.<NewLine><NewLine>    #print(""YOLO_LAYER: {}"".format(x[0][0][0]))<NewLine>    # Add offset and scale with anchors<NewLine>    pred_boxes = torch.zeros(prediction[..., :4].shape)<NewLine><NewLine>    pred_boxes = torch.stack((x.data+self.grid_x,y.data+self.grid_y,torch.exp(w.data)*self.anchor_w,torch.exp(h.data)*self.anchor_h),4)<NewLine>    output = torch.cat(<NewLine>        (<NewLine>            pred_boxes.view(num_samples, -1, 4) * self.stride,<NewLine>            pred_conf.view(num_samples, -1, 1),<NewLine>            pred_cls.view(num_samples, -1, self.num_classes),<NewLine>        ),<NewLine>        -1,<NewLine>    )<NewLine>    #print(output[0][0][0])<NewLine><NewLine>    # if targets is None:<NewLine>    #     return output, 0<NewLine>    # else:<NewLine>    #     iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf = build_targets(<NewLine>    #         pred_boxes=pred_boxes,<NewLine>    #         pred_cls=pred_cls,<NewLine>    #         target=targets,<NewLine>    #         anchors=self.scaled_anchors,<NewLine>    #         ignore_thres=self.ignore_thres,<NewLine>    #     )<NewLine><NewLine>    #     # Loss : Mask outputs to ignore non-existing objects (except with conf. loss)<NewLine>    #     loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])<NewLine>    #     loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])<NewLine>    #     loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])<NewLine>    #     loss_h = self.mse_loss(h[obj_mask], th[obj_mask])<NewLine>    #     loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])<NewLine>    #     loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])<NewLine>    #     loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj<NewLine>    #     loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])<NewLine>    #     total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls<NewLine><NewLine>    #     # Metrics<NewLine>    #     cls_acc = 100 * class_mask[obj_mask].mean()<NewLine>    #     conf_obj = pred_conf[obj_mask].mean()<NewLine>    #     conf_noobj = pred_conf[noobj_mask].mean()<NewLine>    #     conf50 = (pred_conf &gt; 0.5).float()<NewLine>    #     iou50 = (iou_scores &gt; 0.5).float()<NewLine>    #     iou75 = (iou_scores &gt; 0.75).float()<NewLine>    #     detected_mask = conf50 * class_mask * tconf<NewLine>    #     precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + 1e-16)<NewLine>    #     recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + 1e-16)<NewLine>    #     recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + 1e-16)<NewLine><NewLine>    #     self.metrics = {<NewLine>    #         ""loss"": to_cpu(total_loss).item(),<NewLine>    #         ""x"": to_cpu(loss_x).item(),<NewLine>    #         ""y"": to_cpu(loss_y).item(),<NewLine>    #         ""w"": to_cpu(loss_w).item(),<NewLine>    #         ""h"": to_cpu(loss_h).item(),<NewLine>    #         ""conf"": to_cpu(loss_conf).item(),<NewLine>    #         ""cls"": to_cpu(loss_cls).item(),<NewLine>    #         ""cls_acc"": to_cpu(cls_acc).item(),<NewLine>    #         ""recall50"": to_cpu(recall50).item(),<NewLine>    #         ""recall75"": to_cpu(recall75).item(),<NewLine>    #         ""precision"": to_cpu(precision).item(),<NewLine>    #         ""conf_obj"": to_cpu(conf_obj).item(),<NewLine>    #         ""conf_noobj"": to_cpu(conf_noobj).item(),<NewLine>    #         ""grid_size"": grid_size,<NewLine>    #     }<NewLine>    #total_loss = 0<NewLine>    return output, torch.tensor(0)<NewLine></code></pre><NewLine><p>`</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your help.<br/><NewLine>I did it.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is is possible to deploy a network without classfier layer onto IOS? I just want to output the final features and compare two tensors in some way, e.g. euclidean distance. Did you deploy on IOS?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/IsaacBerman; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/junjihashimoto; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/IsaacBerman; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/junjihashimoto; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Peter_Wong; <NewLine> ,"REPLY_DATE 1: December 2, 2019,  8:56pm; <NewLine> REPLY_DATE 2: December 2, 2019, 10:29pm; <NewLine> REPLY_DATE 3: December 2, 2019, 11:39pm; <NewLine> REPLY_DATE 4: March 13, 2020,  3:25am; <NewLine> REPLY_DATE 5: March 13, 2020,  7:57pm; <NewLine> REPLY_DATE 6: March 14, 2020,  7:09am; <NewLine> REPLY_DATE 7: June 22, 2020,  9:52am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
60297,Print network architecture in cpp jit,2019-11-07T16:25:00.496Z,4,312,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>In python,  we could use print (model) to show the network architecture,  how about in cpp?</p><NewLine><p>Say i hv loaded jit model with jit load with the name model, how could i do the similar in cpp.</p><NewLine><p>Thanks,<br/><NewLine>Rgds,<br/><NewLine>CL</p><NewLine></div>",https://discuss.pytorch.org/u/tancl,,tancl,"November 7, 2019,  4:25pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We don’t have any built-in utils for this, but you can do a simple version manually:</p><NewLine><pre><code class=""lang-cpp"">#include &lt;torch/script.h&gt;<NewLine><NewLine>#include &lt;iostream&gt;<NewLine>#include &lt;memory&gt;<NewLine><NewLine>void tabs(size_t num) {<NewLine>  for (size_t i = 0; i &lt; num; i++) {<NewLine>    std::cout &lt;&lt; ""\t"";<NewLine>  }<NewLine>}<NewLine><NewLine>void print_modules(const torch::jit::script::Module&amp; module, size_t level = 0) {<NewLine>  std::cout &lt;&lt; module.name().qualifiedName() &lt;&lt; "" (\n"";<NewLine>  for (const auto&amp; module : module.get_modules()) {<NewLine>    tabs(level + 1);<NewLine>    print_modules(module.module, level + 1);<NewLine>  }<NewLine>  tabs(level);<NewLine>  std::cout &lt;&lt; "")\n"";<NewLine>}<NewLine><NewLine>int main(int argc, const char *argv[]) {<NewLine>  torch::jit::script::Module container = torch::jit::load(""m.pt"");<NewLine>  print_modules(container);<NewLine>  return 0;<NewLine>}<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Driazati,</p><NewLine><p>Thanks for the codes, it helps. just some typo i think, the</p><NewLine><p><code>print_modules(module.module, level + 1);</code></p><NewLine><p>should be</p><NewLine><pre><code class=""lang-auto"">print_modules(module, level + 1);<NewLine></code></pre><NewLine><p>Appreciate your help. The references for these seems to be not much, could you share some good reference for this ? For e.g., the method (or the var) that can return the input size of a pre-trained network etc.</p><NewLine><p>Thanks again.</p><NewLine><p>Regards,<br/><NewLine>CL</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can find the generated docs here <a href=""https://pytorch.org/cppdocs/api/namespace_torch__jit.html#namespace-torch-jit"" rel=""nofollow noopener"">https://pytorch.org/cppdocs/api/namespace_torch__jit.html#namespace-torch-jit</a>. Unfortunately they are pretty sparse, we have ongoing efforts to improve them.</p><NewLine><p>As for getting the input size a network expects, we don’t have any utilities for that since TorchScript models aren’t specialized to specific shapes.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi,</p><NewLine><p>thanks again for the reply and link. yes i’ve visited the link before, might be too sparse for non hardcore programmer. anyway, thanks for your reply again.</p><NewLine><p>rgds,<br/><NewLine>CL</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Hope you don’t mind, I created another topic to get more details from the jit model. I understand that there are no shape properties for the input, I was wondering whether we could print the each layers details as stated in :</p><NewLine><aside class=""quote quote-modified"" data-post=""1"" data-topic=""60465""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v4/letter/t/9fc348/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/model-summary-for-libtorch-jit-model/60465"">Model Summary for libTorch jit model?</a> <a class=""badge-wrapper bullet"" href=""/c/jit""><span class=""badge-category-bg"" style=""background-color: #0088CC;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""A category for torchscript and the PyTorch JIT compiler"">jit</span></a><NewLine></div><NewLine><blockquote><NewLine>    Hi, <NewLine>Is it possible to get the model summary described in this link <NewLine><NewLine> <NewLine>, but for cpp jit model? <NewLine>This is an enhancement for printing the model in this link; <NewLine><NewLine> <NewLine>Thanks. <NewLine>rgds, <NewLine>CL<NewLine>  </blockquote><NewLine></aside><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi,</p><NewLine><p>adding these lines</p><NewLine><pre><code class=""lang-auto"">  for (const auto&amp; parameter : module.get_parameters()) {<NewLine>	tabs(level + 1);<NewLine>	std::cout &lt;&lt; parameter.name() &lt;&lt; '\t';<NewLine>	std::cout &lt;&lt; parameter.value().toTensor().sizes() &lt;&lt; '\n';<NewLine>  } <NewLine><NewLine></code></pre><NewLine><p>shall give the summary for the network.</p><NewLine><p>if anyone having better way to show the network summary for jit model in C++, please feel free to share.</p><NewLine><p>Thanks.</p><NewLine><p>rgds,<br/><NewLine>CL</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>does anybody know how to do this using libtorch (iOs)? <code>module.get_parameters()</code> is not defined.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tancl; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/tancl; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/tancl; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/tancl; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Bryan_Wang; <NewLine> ,"REPLY_DATE 1: November 8, 2019,  2:19am; <NewLine> REPLY_DATE 2: November 8, 2019,  2:28am; <NewLine> REPLY_DATE 3: November 8, 2019,  4:39am; <NewLine> REPLY_DATE 4: November 8, 2019,  7:50am; <NewLine> REPLY_DATE 5: November 10, 2019,  4:12pm; <NewLine> REPLY_DATE 6: November 12, 2019,  4:24pm; <NewLine> REPLY_DATE 7: June 19, 2020,  1:34am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> 
85638,Cannot obtain same output tensor values in C++ side,2020-06-16T12:18:46.304Z,1,93,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I try to make predictions for a 4-class classification project in C++ side using libtorch 1.4. However, I cannot obtain same predictions compared to Python side. Firstly, I obtain same input tensor values just before prediction. When I compare the output tensor values, I noticed that they are different. You can find those values in that picture:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/93f9ffae93722cba195d68d3db17dae21ba670fc"" href=""https://discuss.pytorch.org/uploads/default/original/3X/9/3/93f9ffae93722cba195d68d3db17dae21ba670fc.png"" title=""Output_tensor_values_Python_and_C++""><img alt=""Output_tensor_values_Python_and_C++"" data-base62-sha1=""l73O62HRCOpZeR07Gw8N9UOYSpm"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/3/93f9ffae93722cba195d68d3db17dae21ba670fc_2_10x10.png"" height=""315"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/3/93f9ffae93722cba195d68d3db17dae21ba670fc_2_690x315.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/3/93f9ffae93722cba195d68d3db17dae21ba670fc_2_690x315.png, https://discuss.pytorch.org/uploads/default/optimized/3X/9/3/93f9ffae93722cba195d68d3db17dae21ba670fc_2_1035x472.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/9/3/93f9ffae93722cba195d68d3db17dae21ba670fc_2_1380x630.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Output_tensor_values_Python_and_C++</span><span class=""informations"">1883×861 114 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>Left side includes Python output tensor values and the prediction results for each input picture.<br/><NewLine>Right side includes C++ output tensor values and the prediction results for each input picture.</p><NewLine><p>Could you offer a solution to obtain same output tensor values and prediction results?</p><NewLine></div>",https://discuss.pytorch.org/u/sercan,(sercan),sercan,"June 16, 2020, 12:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you make sure to set the model to evaluation mode via <code>model.eval()</code> before executing the forward pass?<br/><NewLine>Also, I assume you’ve checked the inputs already or are you using any (random) preprocessing steps?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I noticed that I used opencv functions to apply normalization using this code:</p><NewLine><pre><code class=""lang-auto"">subtract(image, Scalar(0.485, 0.456, 0.406), temp);<NewLine>divide(temp, Scalar(0.229, 0.224, 0.225), image);<NewLine></code></pre><NewLine><p>This operation changes only the first channel and does not change other channels. For this reason, in fact, input tensor values are different. I applied normalization directly on tensor values writing this code:</p><NewLine><pre><code>	tensor_image = tensor_image.permute({ 2,0,1 });//chw<NewLine>	tensor_image = tensor_image.toType(torch::kFloat);<NewLine>	tensor_image = tensor_image.div(255.0);<NewLine>	//normalize<NewLine>	tensor_image[0] = tensor_image[0].sub_(0.485).div_(0.229);<NewLine>	tensor_image[1] = tensor_image[1].sub_(0.456).div_(0.224);<NewLine>	tensor_image[2] = tensor_image[2].sub_(0.406).div_(0.225);<NewLine></code></pre><NewLine><p>Thus, I obtained the same input tensor values compared to Python side. After prediction, I obtained the same output tensor values. My problem solved.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/sercan; <NewLine> ,"REPLY_DATE 1: June 17, 2020,  6:56am; <NewLine> REPLY_DATE 2: June 17, 2020, 12:38pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
85122,Low inference speed with libtorch on win10,2020-06-12T00:24:56.701Z,0,84,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to deploy pytorch YOLO model in windows c++ environment.<br/><NewLine>The  environment is libtorch1.4-debug + vs2017 + cuda10.1,<br/><NewLine>the c++ inference code can run successfully, but it takes about 0.16s for<br/><NewLine>an input image, as a comparison, the time is 0.03s in python environment.<br/><NewLine>And i also found that the GPU usage rate is rather low, any ideas on how to solve this problem?</p><NewLine></div>",https://discuss.pytorch.org/u/Wenfei_Yang,(Wenfei Yang),Wenfei_Yang,"June 12, 2020, 12:24am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>How did you profile the libtorch and Python code?<br/><NewLine>Note that CUDA operations are asynchronous, so you would need to synchronize the code before starting and stopping the timer.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Plus: why do you use the debug version for benchmarking? The optimization switches are turned off in those builds.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/peterjc123; <NewLine> ,"REPLY_DATE 1: June 12, 2020,  8:53am; <NewLine> REPLY_DATE 2: June 17, 2020,  3:24am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
85639,Cannot load trace file using libtorch 1.5 in C++,2020-06-16T12:20:58.669Z,0,59,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I created a trace file in Python side. When I try to load it in C++ side, It gave me this error message.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/55c741216ac9355424fb18a6a4f648161f3694af"" href=""https://discuss.pytorch.org/uploads/default/original/3X/5/5/55c741216ac9355424fb18a6a4f648161f3694af.png"" title=""libtorch_1_5_load_error_message""><img alt=""libtorch_1_5_load_error_message"" data-base62-sha1=""cePu6FuujsI6wE2QWGZ66bzuws7"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/5/5/55c741216ac9355424fb18a6a4f648161f3694af_2_10x10.png"" height=""260"" src=""https://discuss.pytorch.org/uploads/default/original/3X/5/5/55c741216ac9355424fb18a6a4f648161f3694af.png"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">libtorch_1_5_load_error_message</span><span class=""informations"">1032×390 30.1 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine>I couldn’t load this trace file. Could you help me?</p><NewLine></div>",https://discuss.pytorch.org/u/sercan,(sercan),sercan,"June 16, 2020, 12:20pm",,,,,
85052,Convert TSM pytorch model to onnx,2020-06-11T12:39:56.999Z,0,80,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to convert tsm model <a href=""https://github.com/mit-han-lab/temporal-shift-module"" rel=""nofollow noopener"">https://github.com/mit-han-lab/temporal-shift-module</a> to onnx. when convert it show me that</p><NewLine><pre><code class=""lang-auto"">TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<NewLine>  out[:, :-1, :fold] = x[:, 1:, :fold]  # shift left<NewLine>/Users/zhanghongxing/vscode_project/tsm_inference/infer/../ops/temporal_shift.py:37: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.<NewLine>  out[:, :-1, :fold] = x[:, 1:, :fold]  # shift left<NewLine>/Users/zhanghongxing/vscode_project/tsm_inference/infer/../ops/temporal_shift.py:38: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<NewLine>  out[:, 1:, fold: 2 * fold] = x[:, :-1, fold: 2 * fold]  # shift right<NewLine>/Users/zhanghongxing/vscode_project/tsm_inference/infer/../ops/temporal_shift.py:38: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.<NewLine>  out[:, 1:, fold: 2 * fold] = x[:, :-1, fold: 2 * fold]  # shift right<NewLine>/Users/zhanghongxing/vscode_project/tsm_inference/infer/../ops/temporal_shift.py:39: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<NewLine>  out[:, :, 2 * fold:] = x[:, :, 2 * fold:]  # not shift<NewLine>/Users/zhanghongxing/vscode_project/tsm_inference/infer/../ops/temporal_shift.py:39: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.<NewLine>  out[:, :, 2 * fold:] = x[:, :, 2 * fold:]  # not shift<NewLine></code></pre><NewLine><p><strong>this is the code snippet：</strong></p><NewLine><pre><code class=""lang-auto"">            out = torch.zeros_like(x)<NewLine>            out[:, :-1, :fold] = x[:, 1:, :fold]  # shift left<NewLine>            out[:, 1:, fold: 2 * fold] = x[:, :-1, fold: 2 * fold]  # shift right<NewLine>            out[:, :, 2 * fold:] = x[:, :, 2 * fold:]  # not shift<NewLine></code></pre><NewLine><p>i dont think this is a inplace operate.<br/><NewLine>and then I get the onnx model .but the output is different with the pytorch output.<br/><NewLine><strong>pytorch output:[[0.04369894 0.09070115 0.8193747  … 0.25200492 0.0048383  0.04694336]]</strong><br/><NewLine><strong>onnx output:[0.         0.         0.         … 0.03162321 0.         0.        ]]</strong></p><NewLine></div>",https://discuss.pytorch.org/u/Usernamezhx,(zhanghx),Usernamezhx,"June 11, 2020, 12:42pm",,,,,
53836,Custom LSTM returns nan,2019-08-20T13:44:07.638Z,1,855,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,i implemented my own custom LSTMCell based on [<a href=""https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py</a>],<br/><NewLine>but during back-propagation i get nan values (after two or three iterations).To be more specific my net is consisted of CNN (Alexnet) + CustomRNN + Log_Softmax and is trained with CTC loss.As far as my custom LSTM is concerned, it is an implementation of Differential RNN <a href=""https://arxiv.org/abs/1504.06678"" rel=""nofollow noopener"">https://arxiv.org/abs/1504.06678</a>. Below are some snippets of my code.<br/><NewLine>LSTMCell:<div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/59c2b688c7536edd26a20942a58461af6ca210ff"" href=""https://discuss.pytorch.org/uploads/default/original/2X/5/59c2b688c7536edd26a20942a58461af6ca210ff.png"" title=""Screenshot from 2019-08-20 16:33:54.png""><img alt=""Screenshot%20from%202019-08-20%2016%3A33%3A54"" data-base62-sha1=""cO3FgzodKVG82cx4k7HyQjm2dfV"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/59c2b688c7536edd26a20942a58461af6ca210ff_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/59c2b688c7536edd26a20942a58461af6ca210ff_2_646x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/59c2b688c7536edd26a20942a58461af6ca210ff_2_646x500.png, https://discuss.pytorch.org/uploads/default/optimized/2X/5/59c2b688c7536edd26a20942a58461af6ca210ff_2_969x750.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/5/59c2b688c7536edd26a20942a58461af6ca210ff.png 2x"" width=""646""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screenshot from 2019-08-20 16:33:54.png</span><span class=""informations"">1059×819 112 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><pre><code class=""lang-auto"">Model forward:<NewLine>def forward(self, x):<NewLine>        LSTMState = namedtuple('LSTMState', ['hx', 'cx', 'dc'])<NewLine><NewLine>        batch_size, timesteps, C, H, W = x.size()<NewLine>        c_in = x.view(batch_size * timesteps, C, H, W)<NewLine>        c_out = self.cnn(c_in)<NewLine>        c_out = c_out.view(-1, batch_size, 4096)<NewLine><NewLine>        h1 = torch.zeros(batch_size, self.hidden_size).cuda(0)<NewLine>        h2 = torch.zeros(batch_size, self.hidden_size).cuda(0)<NewLine>        h3 = torch.zeros(batch_size, self.hidden_size).cuda(0)<NewLine><NewLine>        states = [[LSTMState(h1, h2, h3)<NewLine>                   for _ in range(2)]<NewLine>                  for _ in range(self.num_layers)]<NewLine>        r_out, out_state = self.rnn(c_out, states)<NewLine>        custom_state = double_flatten_states(out_state)<NewLine>        r_out2 = self.last_linear(r_out)<NewLine>        return (r_out2)<NewLine></code></pre><NewLine><p>Thanks in advance</p><NewLine></div>",https://discuss.pytorch.org/u/Theocharis,,Theocharis,"August 20, 2019,  1:48pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I guess you should also include some of your training code to help troubleshoot. The code you’ve provided here looks ok.</p><NewLine><p>Given that it happens after a few epochs I guess the gradient is either vanishing or exploding. Either one could be caused by a learning rate issue. For exploding gradients you could try gradient clipping.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Jack and thanks for your reply.First of all  what i meant was that i got nan after two or three batches, not epochs(reducing learning rate wouldn’t work).What i modified to the original code is that i added one state (the derivative of cell state, dc) and the computation of gates of<br/><NewLine>course, as you can see above, and consequently i can’t understand why it should return nan (i can run the original code with my model and training code without a problem)</p><NewLine><p>Training code:</p><NewLine><pre><code class=""lang-auto"">criterion = nn.CTCLoss(blank=0, reduction='mean')<NewLine>    # with autograd.detect_anomaly():<NewLine>    for batch_idx, (data, target) in enumerate(train_loader):<NewLine>        data, target = data.to(device), target.to(device)<NewLine>        optimizer.zero_grad()<NewLine>        output = model(data)<NewLine>        input_len = torch.tensor([output.size(0)], dtype=torch.int)<NewLine>        target_len = torch.tensor([target.size(1)], dtype=torch.int)<NewLine>        log_probs = nn.functional.log_softmax(output, dim=2)<NewLine>        loss = criterion(log_probs, target, input_len, target_len)<NewLine>        train_loss += loss.item()<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine></code></pre><NewLine><p>LSTM full code:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>from torch.nn import Parameter<NewLine>import torch.jit as jit<NewLine>import warnings<NewLine>from collections import namedtuple<NewLine>from typing import List, Tuple<NewLine>from torch import Tensor<NewLine>import numbers<NewLine><NewLine>def script_lstm(input_size, hidden_size, num_layers, bias=True,<NewLine>                batch_first=False, dropout=False, bidirectional=True):<NewLine>    <NewLine>    assert bias<NewLine>    assert not batch_first<NewLine>    stack_type = StackedLSTM2<NewLine>    layer_type = BidirLSTMLayer<NewLine>    dirs = 2<NewLine>    return stack_type(num_layers, layer_type,<NewLine>                      first_layer_args=[LSTMCell, input_size, hidden_size],<NewLine>                      other_layer_args=[LSTMCell, hidden_size * dirs, hidden_size])<NewLine><NewLine>def reverse(lst):<NewLine>    # type: (List[Tensor]) -&gt; List[Tensor]<NewLine>    return lst[::-1]<NewLine><NewLine>class LSTMCell(jit.ScriptModule):<NewLine>    def __init__(self, input_size, hidden_size, order=1):<NewLine>        # __constants__ = ['order']<NewLine>        super(LSTMCell, self).__init__()<NewLine><NewLine>        self.order = order<NewLine>        self.input_size = input_size<NewLine>        self.hidden_size = hidden_size<NewLine><NewLine>        self.weight_ih = Parameter(torch.randn(4 * hidden_size, input_size))<NewLine>        self.weight_hh = Parameter(torch.randn(4 * hidden_size, hidden_size))<NewLine>        self.bias_ih = Parameter(torch.randn(4 * hidden_size))<NewLine>        self.bias_hh = Parameter(torch.randn(4 * hidden_size))<NewLine><NewLine>        ###weight-bias for st-1, eq.6,7 for N=0###<NewLine>        self.weight_ch_prev = Parameter(torch.randn(2 * hidden_size, hidden_size))<NewLine>        self.bias_ch_prev = Parameter(torch.randn(2 * hidden_size))<NewLine><NewLine>        ###weight-bias for d(st-1), eq.6,7 for N=1###<NewLine>        self.weight_ch_dc_prev = Parameter(torch.randn(2 * self.order * hidden_size,hidden_size))<NewLine>        self.bias_ch_dc_prev = Parameter(torch.randn(2 * self.order  * hidden_size))<NewLine><NewLine>        ###weight-bias for st, eq.8 for N=0###<NewLine>        self.weight_ch_cur = Parameter(torch.randn(hidden_size, hidden_size))<NewLine>        self.bias_ch_cur = Parameter(torch.randn(hidden_size))<NewLine><NewLine>        ###weight-bias for d(st-1), eq.8 for N=1<NewLine>        self.weight_ch_dc_cur = Parameter(torch.randn(self.order * hidden_size,hidden_size))<NewLine>        self.bias_ch_dc_cur = Parameter(torch.randn(self.order * hidden_size))<NewLine><NewLine>    @jit.script_method<NewLine>    def forward(self, input, state):<NewLine>        # type: (Tensor, Tuple[Tensor, Tensor, Tensor]) -&gt; Tuple[Tensor, Tuple[Tensor, Tensor, Tensor]]<NewLine>        hx, cx, dc = state<NewLine>        gates = (torch.mm(input, self.weight_ih.t()) + self.bias_ih +<NewLine>                 torch.mm(hx, self.weight_hh.t()) + self.bias_hh)<NewLine>        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)<NewLine><NewLine>        gates_2 = (torch.mm(dc, self.weight_ch_dc_prev.t()) + self.bias_ch_dc_prev + torch.mm(cx, self.weight_ch_prev.t()) + self.bias_ch_prev)<NewLine>        ingate_2, forgetgate_2 = gates_2.chunk(2, 1)<NewLine>        ingate = ingate + ingate_2<NewLine>        forgetgate = forgetgate + forgetgate_2<NewLine><NewLine>        ingate = torch.sigmoid(ingate)<NewLine>        forgetgate = torch.sigmoid(forgetgate)<NewLine>        cellgate = torch.tanh(cellgate)<NewLine><NewLine>        cy = (forgetgate * cx) + (ingate * cellgate)<NewLine>        outgate = outgate + (torch.mm(cy-cx, self.weight_ch_dc_cur.t()) + self.bias_ch_dc_cur + torch.mm(cy, self.weight_ch_cur.t()) + self.bias_ch_cur )<NewLine>        outgate = torch.sigmoid(outgate)<NewLine>        hy = outgate * torch.tanh(cy)<NewLine>        d_c = cy - cx<NewLine>        return hy, (hy, cy, d_c)<NewLine>        <NewLine>class LSTMLayer(jit.ScriptModule):<NewLine>    def __init__(self, cell, *cell_args):<NewLine>        super(LSTMLayer, self).__init__()<NewLine>        self.cell = cell(*cell_args)<NewLine><NewLine>    @jit.script_method<NewLine>    def forward(self, input, state):<NewLine>        # type: (Tensor, Tuple[Tensor, Tensor, Tensor]) -&gt; Tuple[Tensor, Tuple[Tensor, Tensor, Tensor]]<NewLine>        inputs = input.unbind(0)<NewLine>        outputs = torch.jit.annotate(List[Tensor], [])<NewLine>        for i in range(len(inputs)):<NewLine>            out, state = self.cell(inputs[i], state)<NewLine>            outputs += [out]<NewLine>        return torch.stack(outputs), state<NewLine><NewLine>class ReverseLSTMLayer(jit.ScriptModule):<NewLine>    def __init__(self, cell, *cell_args):<NewLine>        super(ReverseLSTMLayer, self).__init__()<NewLine>        self.cell = cell(*cell_args)<NewLine><NewLine>    @jit.script_method<NewLine>    def forward(self, input, state):<NewLine>        # type: (Tensor, Tuple[Tensor, Tensor, Tensor]) -&gt; Tuple[Tensor, Tuple[Tensor, Tensor, Tensor]]<NewLine>        inputs = reverse(input.unbind(0))<NewLine>        outputs = jit.annotate(List[Tensor], [])<NewLine>        for i in range(len(inputs)):<NewLine>            out, state = self.cell(inputs[i], state)<NewLine>            outputs += [out]<NewLine>        return torch.stack(reverse(outputs)), state<NewLine><NewLine>class BidirLSTMLayer(jit.ScriptModule):<NewLine>    __constants__ = ['directions']<NewLine><NewLine>    def __init__(self, cell, *cell_args):<NewLine>        super(BidirLSTMLayer, self).__init__()<NewLine>        self.directions = nn.ModuleList([<NewLine>            LSTMLayer(cell, *cell_args),<NewLine>            ReverseLSTMLayer(cell, *cell_args),<NewLine>        ])<NewLine>    @jit.script_method<NewLine>    def forward(self, input, states):<NewLine>        # type: (Tensor, List[Tuple[Tensor, Tensor, Tensor]]) -&gt; Tuple[Tensor, List[Tuple[Tensor, Tensor, Tensor]]]<NewLine>        # List[LSTMState]: [forward LSTMState, backward LSTMState]<NewLine>        outputs = jit.annotate(List[Tensor], [])<NewLine>        output_states = jit.annotate(List[Tuple[Tensor, Tensor, Tensor]], [])<NewLine>        # XXX: enumerate https://github.com/pytorch/pytorch/issues/14471<NewLine>        i = 0<NewLine>        for direction in self.directions:<NewLine>            state = states[i]<NewLine>            out, out_state = direction(input, state)<NewLine>            outputs += [out]<NewLine>            output_states += [out_state]<NewLine>            i += 1<NewLine>        return torch.cat(outputs, -1), output_states<NewLine><NewLine>def init_stacked_lstm(num_layers, layer, first_layer_args, other_layer_args):<NewLine><NewLine>    layers = [layer(*first_layer_args)] + [layer(*other_layer_args)<NewLine>                                           for _ in range(num_layers - 1)]<NewLine><NewLine>    return nn.ModuleList(layers)<NewLine><NewLine>class StackedLSTM2(jit.ScriptModule):<NewLine>    __constants__ = ['layers'] <NewLine><NewLine>    def __init__(self, num_layers, layer, first_layer_args, other_layer_args):<NewLine>        super(StackedLSTM2, self).__init__()<NewLine><NewLine>        self.layers = init_stacked_lstm(num_layers, layer, first_layer_args,<NewLine>                                        other_layer_args)<NewLine>    @jit.script_method<NewLine>    def forward(self, input, states):<NewLine>        # type: (Tensor, List[List[Tuple[Tensor, Tensor, Tensor]]]) -&gt; Tuple[Tensor, List[List[Tuple[Tensor, Tensor, Tensor]]]]<NewLine>        # List[List[LSTMState]]: The outer list is for layers,<NewLine>        #                        inner list is for directions.<NewLine>        output_states = jit.annotate(List[List[Tuple[Tensor, Tensor, Tensor]]], [])<NewLine>        output = input<NewLine>        # XXX: enumerate https://github.com/pytorch/pytorch/issues/14471<NewLine>        i = 0<NewLine>        for rnn_layer in self.layers:<NewLine>            state = states[i]<NewLine>            output, out_state = rnn_layer(output, state)<NewLine>            output_states += [out_state]<NewLine>            i += 1<NewLine>        return output, output_states<NewLine><NewLine></code></pre><NewLine><p>Thanks again.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>When i use  “with autograd.detect_anomaly()” i get the following error messages:</p><NewLine><pre><code class=""lang-auto""><NewLine>RuntimeError: Function 'Sigmoidbackward' returned  nan values in its 0th output<NewLine><NewLine>RuntimeError: Function 'DivBackward0' returned nan values in its 0th output<NewLine><NewLine>RuntimeError: Function 'CudnnConvolutionBackward' returned nan values in its 0th output<NewLine><NewLine>RuntimeError: Function torch::jit::(anonymous namespace)::DifferentiableGraphBackward returned nan values in its 2th output<NewLine></code></pre><NewLine><p>Can anyone tell me if it is caused by overflow/division by zero ,etc.?<br/><NewLine>Edit: I narrowed it down, the problem is in the following lines, but still can’t resolve it:</p><NewLine><pre><code class=""lang-auto"">ingate = ingate + ingate_2<NewLine>forgetgate = forgetgate + forgetgate_2<NewLine>outgate = outgate + (torch.mm(cy-cx, self.weight_ch_dc_cur.t()) + self.bias_ch_dc_cur + torch.mm(cy, self.weight_ch_cur.t()) + self.bias_ch_cur )<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Theocharis,</p><NewLine><p>I am having similar issue. I am trying to run the existing LSTM code from fastrnn provided in github, but when I am back-propagating I get similar error. Did you get a chance to resolve this error? The same input runs fine for native LSTM.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey Tejo,<br/><NewLine>as far as i can recall, i couldn’t resolve this error <img alt="":confused:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=9"" title="":confused:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jackparrySG; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Theocharis; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Theocharis; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/tejo; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Theocharis; <NewLine> ,"REPLY_DATE 1: August 20, 2019,  4:26pm; <NewLine> REPLY_DATE 2: August 21, 2019,  6:36am; <NewLine> REPLY_DATE 3: August 21, 2019, 12:35pm; <NewLine> REPLY_DATE 4: June 10, 2020,  9:22am; <NewLine> REPLY_DATE 5: June 10, 2020, 12:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
84230,Does torch.jit.script need to specify device?,2020-06-04T20:53:12.233Z,0,87,"<div class=""post"" itemprop=""articleBody""><NewLine><p>For using <code>torch.jit.script</code> is recommended to pass model to cpu if you are doing inference in that device?</p><NewLine></div>",https://discuss.pytorch.org/u/WaterKnight,(David Lacalle),WaterKnight,"June 4, 2020,  8:53pm",,,,,
84177,ONNX fixes input/output shape of torch.jit.script function?,2020-06-04T12:33:32.222Z,0,111,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">@torch.jit.script<NewLine>def check_init(input_data, hidden_size, prev_state):<NewLine>    # type: (torch.Tensor, int, torch.Tensor) -&gt; torch.Tensor<NewLine>    batch_size = input_data.size(0)<NewLine>    spatial_size_0 = input_data.size(2)<NewLine>    spatial_size_1 = input_data.size(3)<NewLine>    # generate empty prev_state, if None is provided<NewLine>    state_size = (2, batch_size, hidden_size ,spatial_size_0, spatial_size_1)<NewLine>    if prev_state.size(0) == 0:<NewLine>        state = torch.zeros(state_size, device=input_data.device)<NewLine>    else:<NewLine>        state = prev_state.view(state_size)<NewLine>    return state<NewLine></code></pre><NewLine><p>I am trying to export my model to ONNX and I have a function that will check if the previous state is initialized and I will initialize it based on the input size. Because I have an if statement I decorated the function with <code>@torch.jit.script</code>. <code>prev_state</code> is a tensor of dimension 5. At first I pass an empty tensor like this <code>torch.tensor([]).view(0,0,0,0,0)</code>. In all subsequent runs, I will pass back the returned tensor. This check_init function is used at 3 different places in the network and the <code>input_data</code> variable is the output of one of the stages of the neural net. For the original input of the full neural net, I have set the input and output to have <code>dynamic_axes</code>.</p><NewLine><p>The model is properly exported to a .onnx file. However, when I run it with an input, I get the following error at the second iteration (the first iteration when prev_state is of size (0,0,0,0,0) works fine):</p><NewLine><pre><code class=""lang-auto"">2020-06-04 13:32:14.289010608 [E:onnxruntime:, sequential_executor.cc:281 Execute] Non-zero status code returned while running Identity node. Name:'Identity_29' Status Message: /onnxruntime_src/onnxruntime/core/framework/execution_frame.cc:66 onnxruntime::common::Status onnxruntime::IExecutionFrame::GetOrCreateNodeOutputMLValue(int, const onnxruntime::TensorShape*, OrtValue*&amp;, size_t) shape &amp;&amp; tensor.Shape() == *shape was false. OrtValue shape verification failed. Current shape:{0,0,0,0,0} Requested shape:{2,1,64,92,120}<NewLine><NewLine>2020-06-04 13:32:14.289043204 [E:onnxruntime:, sequential_executor.cc:281 Execute] Non-zero status code returned while running If node. Name:'If_21' Status Message: Non-zero status code returned while running Identity node. Name:'Identity_29' Status Message: /onnxruntime_src/onnxruntime/core/framework/execution_frame.cc:66 onnxruntime::common::Status onnxruntime::IExecutionFrame::GetOrCreateNodeOutputMLValue(int, const onnxruntime::TensorShape*, OrtValue*&amp;, size_t) shape &amp;&amp; tensor.Shape() == *shape was false. OrtValue shape verification failed. Current shape:{0,0,0,0,0} Requested shape:{2,1,64,92,120}<NewLine><NewLine>Traceback (most recent call last):<NewLine>  File ""run_onnx.py"", line 108, in &lt;module&gt;<NewLine>    runner.update(input_tensor, last_timestamp)<NewLine>  File ""/home/test/image_onnx.py"", line 103, in update<NewLine>    onnx_out = self.onnx_session.run(None, onnx_input)<NewLine>  File ""/home/test/lib/python3.8/site-packages/onnxruntime/capi/session.py"", line 111, in run<NewLine>    return self._sess.run(output_names, input_feed, run_options)<NewLine>onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running If node. Name:'If_21' Status Message: Non-zero status code returned while running Identity node. Name:'Identity_29' Status Message: /onnxruntime_src/onnxruntime/core/framework/execution_frame.cc:66 onnxruntime::common::Status onnxruntime::IExecutionFrame::GetOrCreateNodeOutputMLValue(int, const onnxruntime::TensorShape*, OrtValue*&amp;, size_t) shape &amp;&amp; tensor.Shape() == *shape was false. OrtValue shape verification failed. Current shape:{0,0,0,0,0} Requested shape:{2,1,64,92,120}<NewLine></code></pre><NewLine><p>Shouldn’t the exporter ‘propagate’ the dynamic axis property to all subsequent stages of the network and realize that those variables will also have dynamic size? Any ideas on how to solve this problem?</p><NewLine><p>Exporting using just TorchScript works fine but it seems ONNX is less flexible</p><NewLine></div>",https://discuss.pytorch.org/u/Andreas_Georgiou,(Andreas Georgiou),Andreas_Georgiou,"June 4, 2020, 12:38pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/andreas_georgiou"">@Andreas_Georgiou</a>,</p><NewLine><p>This error occurs within ONNX Runtime, so it’s likely the case that you should report an issue there, and then work backwards up the stack. It’s not clear if the issue is within PyTorch ONNX export, or if the ONNX exporter is emitting a valid ONNX model and it’s a failed analysis within ONNX runtime.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/James_Reed; <NewLine> ,"REPLY_DATE 1: June 4, 2020,  5:22pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
80320,[ONNX] Tried to trace &lt;submodule&gt; but it is not part of the active trace,2020-05-08T11:38:12.926Z,3,340,"<div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""hashtag"" href=""/c/jit/13"">#<span>jit</span></a> <a class=""hashtag"" href=""/c/quantization/17"">#<span>quantization</span></a></p><NewLine><p>Hello!</p><NewLine><p>I am trying to convert quantized model to Caffe2. I know that it is needed to use TorchScript tracing before ONNX exporting. I am trying to convert RCNN model the following way: 1) perform quantization, 2) trace quantized backbone to torchscript, 3) swap original backbone with quantized one (other parts of the network as they were), 4) patch and export network with ONNX</p><NewLine><p>It is possible to convert original (non-quantized) network to Caffe2 without any errors, but when I do swapping of backbone and export ONNX I see the following:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""./tools/torchscript_converter.py"", line 158, in &lt;module&gt;<NewLine>    caffe2_model = export_caffe2_model(cfg, orig_model, first_batch)<NewLine>  File ""/root/some_detectron2/detectron2/export/api.py"", line 157, in export_caffe2_model<NewLine>    return Caffe2Tracer(cfg, model, inputs).export_caffe2()<NewLine>  File ""/root/some_detectron2/detectron2/export/api.py"", line 95, in export_caffe2<NewLine>    predict_net, init_net = export_caffe2_detection_model(model, inputs)<NewLine>  File ""/root/some_detectron2/detectron2/export/caffe2_export.py"", line 148, in export_caffe2_detection_model<NewLine>    onnx_model = export_onnx_model(model, (tensor_inputs,))<NewLine>  File ""/root/some_detectron2/detectron2/export/caffe2_export.py"", line 67, in export_onnx_model<NewLine>    export_params=True,<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 172, in export<NewLine>    custom_opsets, enable_onnx_checker, use_external_data_format)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 92, in export<NewLine>    use_external_data_format=use_external_data_format)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 525, in _export<NewLine>    fixed_batch_size=fixed_batch_size)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 364, in _model_to_graph<NewLine>    graph, torch_out = _trace_and_get_graph_from_model(model, args)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 317, in _trace_and_get_graph_from_model<NewLine>    torch.jit._get_trace_graph(model, args, strict=False, _force_outplace=False, _return_inputs_states=True)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py"", line 277, in _get_trace_graph<NewLine>    outs = ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 562, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py"", line 359, in forward<NewLine>    self._force_outplace,<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py"", line 345, in wrapper<NewLine>    outs.append(self.inner(*trace_inputs))<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 560, in __call__<NewLine>    result = self._slow_forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 546, in _slow_forward<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/contextlib.py"", line 74, in inner<NewLine>    return func(*args, **kwds)<NewLine>  File ""/root/some_detectron2/detectron2/export/caffe2_modeling.py"", line 326, in forward<NewLine>    features = self._wrapped_model.backbone(images.tensor)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 560, in __call__<NewLine>    result = self._slow_forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 536, in _slow_forward<NewLine>    return self.forward(*input, **kwargs)<NewLine>RuntimeError: Tried to trace &lt;__torch__.densepose.modeling.layers.bifpn.BiFPN object at 0x5570e7633550&gt; but it is not part of the active trace. Modules that are called during a trace must be registered as<NewLine> submodules of the thing being traced.<NewLine></code></pre><NewLine><p>Is this because I’ve mixed TracedModule with eager mode modules?</p><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"May 8, 2020, 11:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>yeah according to the error message, the model being tracing is calling something outside of the traced model.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think the step 2 here is probably wrong, you should not trace a quantized model before swapping it in? You can swap a quantized backbone model in first, then do onnx export, there’s no need to do explicit tracing as ONNX export will do it underlying.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <a class=""mention"" href=""/u/wanchaol"">@wanchaol</a>, <a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a>. I’ll try again</p><NewLine><p>But I’ve already opened a topic with such question and <a class=""mention"" href=""/u/supriyar"">@supriyar</a> has said that it is needed to <code>jit.trace</code> before <code>onnx.export</code> if we work with quantized model.<br/><NewLine>Here’s the refference <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/onnx-export-of-quantized-model/76884/8"">ONNX export of quantized model</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes. I’ve tried and finally it’s given other errors <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>For now it is model with quantized backbone and untouched other parts. It is possible to <code>onnx.export</code> original model, but quantized behave the following way:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""./tools/torchscript_converter.py"", line 145, in &lt;module&gt;<NewLine>    caffe2_model = export_caffe2_model(cfg, torch_model, first_batch)<NewLine>  File ""/root/some_detectron2/detectron2/export/api.py"", line 157, in export_caffe2_model<NewLine>    return Caffe2Tracer(cfg, model, inputs).export_caffe2()<NewLine>  File ""/root/some_detectron2/detectron2/export/api.py"", line 95, in export_caffe2<NewLine>    predict_net, init_net = export_caffe2_detection_model(model, inputs)<NewLine>  File ""/root/some_detectron2/detectron2/export/caffe2_export.py"", line 145, in export_caffe2_detection_model<NewLine>    onnx_model = export_onnx_model(model, (tensor_inputs,))<NewLine>  File ""/root/some_detectron2/detectron2/export/caffe2_export.py"", line 62, in export_onnx_model<NewLine>    operator_export_type=OperatorExportTypes.ONNX_ATEN_FALLBACK,<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 172, in export<NewLine>    custom_opsets, enable_onnx_checker, use_external_data_format)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 92, in export<NewLine>    use_external_data_format=use_external_data_format)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 530, in _export<NewLine>    fixed_batch_size=fixed_batch_size)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 366, in _model_to_graph<NewLine>    graph, torch_out = _trace_and_get_graph_from_model(model, args)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 319, in _trace_and_get_graph_from_model<NewLine>    torch.jit._get_trace_graph(model, args, strict=False, _force_outplace=False, _return_inputs_states=True)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py"", line 283, in _get_trace_graph<NewLine>    outs = ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 574, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py"", line 371, in forward<NewLine>    self._force_outplace,<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py"", line 357, in wrapper<NewLine>    outs.append(self.inner(*trace_inputs))<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 572, in __call__<NewLine>    result = self._slow_forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 558, in _slow_forward<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/contextlib.py"", line 74, in inner<NewLine>    return func(*args, **kwds)<NewLine>  File ""/root/some_detectron2/detectron2/export/caffe2_modeling.py"", line 326, in forward<NewLine>    features = self._wrapped_model.backbone(images.tensor)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 572, in __call__<NewLine>    result = self._slow_forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 558, in _slow_forward<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/DensePose_ADASE/densepose/modeling/quantize_caffe2.py"", line 166, in new_forward<NewLine>    p5, p4, p3, p2 = self.bottom_up(x)  # top-&gt;down<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 572, in __call__<NewLine>    result = self._slow_forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 558, in _slow_forward<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/timm/models/efficientnet.py"", line 350, in forward<NewLine>    x = self.conv_stem(x)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 572, in __call__<NewLine>    result = self._slow_forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 558, in _slow_forward<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/intrinsic/quantized/modules/conv_relu.py"", line 37, in forward<NewLine>    input, self._packed_params, self.scale, self.zero_point)<NewLine>RuntimeError: Tried to trace &lt;__torch__.torch.classes.quantized.Conv2dPackedParamsBase object at 0x5632a7b49180&gt; but it is not part of the active trace. Modules that are called during a trace must be registered as submodules of the thing being traced.<NewLine></code></pre><NewLine><p>Why this traceable on original but not on quantized? Any thoughts?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m facing the same error when I try to export pytorch’s quantized MobilenetV2 model to ONNX. Is there any updates on this?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have a look at this thread maybe you’ll find something relevant</p><NewLine><aside class=""quote quote-modified"" data-post=""10"" data-topic=""82244""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v4/letter/s/db5fbb/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/onnx-quantized-fused-conv2d-wont-trace/82244/10"">[ONNX] Quantized fused Conv2d won't trace</a> <a class=""badge-wrapper bullet"" href=""/c/quantization""><span class=""badge-category-bg"" style=""background-color: #0088CC;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""This category is for questions, discussion and issues related to PyTorch’s quantization feature."">quantization</span></a><NewLine></div><NewLine><blockquote><NewLine>    Please re-try with pytorch nightly build, we recently fixed this so you shouldn’t be seeing this error anymore. <NewLine><NewLine>Seems like the conv layer is not quantized so it produces onnx::Conv as opposed to the _caffe2::Int8Conv operator. Currently the onnx export path to caffe2 does not support partially quantized model, so it expects the entire pytorch model to be able to get quantized.<NewLine>  </blockquote><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Janani_Sundara_gandh; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/zetyquickly; <NewLine> ,"REPLY_DATE 1: May 8, 2020,  5:56pm; <NewLine> REPLY_DATE 2: May 8, 2020,  6:02pm; <NewLine> REPLY_DATE 3: May 8, 2020,  7:19pm; <NewLine> REPLY_DATE 4: May 8, 2020,  9:32pm; <NewLine> REPLY_DATE 5: June 3, 2020,  2:34pm; <NewLine> REPLY_DATE 6: June 4, 2020, 10:52am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
84079,Jit.script Tacotron2,2020-06-03T16:13:18.070Z,0,112,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, i have successfully managed to convert NVIDIA Tacotron2 using torch.jit.script. However, when i try to run it on an Android Device i get a <strong>RuntimeError: NNPACK SpatialConvolution_updateOutput failed</strong><br/><NewLine>Below is the exact error:<br/><NewLine>2020-05-29 16:25:58.166 24263-24427…/AndroidRuntime:   File “…/python3.6/site-packages/torch/nn/modules/conv.py”, line 207, in forward<br/><NewLine>self.weight, self.bias, self.stride,<br/><NewLine>_single(0), self.dilation, self.groups)<br/><NewLine>return F.conv1d(input, self.weight, self.bias, self.stride,<br/><NewLine>~~~~~~~~ &lt;— HERE<br/><NewLine>self.padding, self.dilation, self.groups)<br/><NewLine>RuntimeError: NNPACK SpatialConvolution_updateOutput failed</p><NewLine><p>Jit is created with PyTorch 1.5 and python 3.6.10<br/><NewLine>The Android version is 9.<br/><NewLine>Any idea on how to resolve this problem?</p><NewLine></div>",https://discuss.pytorch.org/u/kots,,kots,"June 3, 2020,  4:15pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>is it possible that it’s running out of memory, or something?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/smth; <NewLine> ,"REPLY_DATE 1: June 3, 2020,  7:37pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
83578,nn.ModuleList loses None objects inside it after scripting,2020-05-31T13:07:19.969Z,16,177,"<div class=""post"" itemprop=""articleBody""><NewLine><h2><NewLine><img alt="":bug:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/bug.png?v=9"" title="":bug:""/> Bug</h2><NewLine><h2>To Reproduce</h2><NewLine><p>Executing torch.jit.script over my model is working however it returns a model that fails at runtime.</p><NewLine><p>Looking deeply the nn.ModuleList is loosing None elements from the Modulelist.</p><NewLine><p>Here, above I attach a code for reproducing the error:</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import sys<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch<NewLine>from torchvision import transforms<NewLine>from PIL import Image<NewLine><NewLine><NewLine>class TestBlock(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(TestBlock, self).__init__()<NewLine>        <NewLine>        layers = []<NewLine>        layers.append(None)<NewLine>        layers.append(None)<NewLine>        layers.append(nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,<NewLine>                               bias=False))<NewLine>        self.layer = nn.ModuleList(layers)<NewLine>        <NewLine>    def forward(self,x):<NewLine>        for aux in self.layer:<NewLine>            print(""ENTER"")<NewLine>            if aux is not None:<NewLine>                x = aux(x)<NewLine>                print(""Not None"")<NewLine>        return x<NewLine></code></pre><NewLine><p>Creating model and tracing it:</p><NewLine><pre><code class=""lang-auto"">model=TestBlock()<NewLine>traced_cell=torch.jit.script(model)<NewLine></code></pre><NewLine><p>Testing model with an image:</p><NewLine><pre><code class=""lang-auto"">img = Image.open(""test.png"")<NewLine><NewLine>my_transforms = transforms.Compose([transforms.Resize((1002,1002)),<NewLine>                                    transforms.ToTensor(),<NewLine>                                    transforms.Normalize(<NewLine>                                                        [0.485, 0.456, 0.406],<NewLine>                                                        [0.229, 0.224, 0.225])])<NewLine>img_input= my_transforms(img).unsqueeze(0).cpu()<NewLine><NewLine>res=model(img_input)<NewLine></code></pre><NewLine><p>This outputs the next:</p><NewLine><pre><code class=""lang-auto"">ENTER<NewLine>ENTER<NewLine>ENTER<NewLine>Not None<NewLine></code></pre><NewLine><p>Traced version output:</p><NewLine><pre><code class=""lang-auto"">res=traced_cell(img_input)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">ENTER<NewLine>Not None<NewLine></code></pre><NewLine><h2>Expected behavior</h2><NewLine><p>Get same output as original model</p><NewLine></div>",https://discuss.pytorch.org/u/WaterKnight,(David Lacalle),WaterKnight,"June 2, 2020,  9:23am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You are currently trying to script a tensor in:</p><NewLine><pre><code class=""lang-python"">traced_cell=torch.jit.script(aux)<NewLine></code></pre><NewLine><p>so I assume you want to pass <code>model</code> instead to the method?</p><NewLine><p>Try to narrow down the issue, as your current code contains more than 700 lines of code.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, I paste the wrong code. I am scripting the model with torch.jit.script(model).</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""83578"" data-username=""ptrblck""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ptrblck/40/1823_2.png"" width=""20""/> ptrblck:</div><NewLine><blockquote><NewLine><p>Try to narrow down the issue, as your current code contains more than 700 lines of code.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I reduced the model to 15lines producing the same error. Look at it again pls <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s great. Thanks for reducing the code.</p><NewLine><p>I would assume it’s on purpose for the JIT to remove no-ops from the graph, as they won’t do anything.<br/><NewLine>What’s your use case that you need these <code>None</code> objects in an <code>nn.ModuleList</code>?<br/><NewLine>As a workaround you could probably just add <code>nn.Identity()</code> modules instead of <code>None</code>s.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""83578"" data-username=""ptrblck""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ptrblck/40/1823_2.png"" width=""20""/> ptrblck:</div><NewLine><blockquote><NewLine><p>What’s your use case that you need these <code>None</code> objects in an <code>nn.ModuleList</code> ?</p><NewLine></blockquote><NewLine></aside><NewLine><p>This is not my defined Arch, this arch is <a href=""https://github.com/HRNet/HRNet-Semantic-Segmentation/blob/pytorch-v1.1/lib/models/seg_hrnet.py"" rel=""nofollow noopener"">Microsoft HRNet</a>.</p><NewLine><p>But if you look in his code they are using it to create a new list:</p><NewLine><pre><code class=""lang-auto"">y_list = self.stage2(x_list)<NewLine><NewLine>        x_list = []<NewLine>        for i in range(self.transition2):<NewLine>            if self.transition2[i] is not None:<NewLine>                x_list.append(self.transition2[i](y_list[-1]))<NewLine>            else:<NewLine>                x_list.append(y_list[i])<NewLine></code></pre><NewLine><p><code>self.transition2</code> is the module list containing <code>None</code> objects</p><NewLine><p>With you workaround C should change <code>is not None</code> with <code>not is isinstance(layer, nn.Identity())</code><br/><NewLine>I am going to try your work-around and let you know!</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>The previous approach work.</p><NewLine><p>However, the model works fast with first image. If we pass an image again it never ends! In addition, TorchScript model is also slower at first image.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the update and the code snippet.<br/><NewLine>The <code>nn.Identity()</code> approach might work, but looks quite hacky given the new code snippet.</p><NewLine><p>However, I’m not familiar with the model, so don’t know which approach would be best to make is scriptable and would suggest to create an issue in their GitHub.</p><NewLine><aside class=""quote no-group"" data-post=""8"" data-topic=""83578"" data-username=""WaterKnight""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/w/9d8465/40.png"" width=""20""/> WaterKnight:</div><NewLine><blockquote><NewLine><p>However, the model works fast with first image. If we pass an image again it never ends! In addition, TorchScript model is also slower at first image.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I understand that the eager model is working for a single iteration and hangs in the second one?<br/><NewLine>While the scripted model is slower in the first iteration and works fine afterwards?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""9"" data-topic=""83578"" data-username=""ptrblck""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ptrblck/40/1823_2.png"" width=""20""/> ptrblck:</div><NewLine><blockquote><NewLine><p>I understand that the eager model is working for a single iteration and hangs in the second one?<br/><NewLine>While the scripted model is slower in the first iteration and works fine afterwards?</p><NewLine></blockquote><NewLine></aside><NewLine><p>The model works well in eager model. After scripting it,  the first iteration is like 6 seconds, second one around 3minutes and third one and go on like 1 second.</p><NewLine><p>I think that it is being optimized. Is there any way of having pre optimizing in a Flask API Rest or disabling optimization? Or Saving optimized one so when it gets loaded is the optimized version?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could try to use</p><NewLine><pre><code class=""lang-python"">torch._C._jit_set_profiling_executor(False)<NewLine>torch._C._jit_set_profiling_mode(False)<NewLine></code></pre><NewLine><p>at the beginning of your script to disable the optimization.<br/><NewLine>However, how if your Flask application is running longer than a couple of seconds, the startup time could probably be ignored.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""11"" data-topic=""83578"" data-username=""ptrblck""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ptrblck/40/1823_2.png"" width=""20""/> ptrblck:</div><NewLine><blockquote><NewLine><p>However, how if your Flask application is running longer than a couple of seconds, the startup time could probably be ignored.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I don’t understand it.</p><NewLine><p>My Flask application has several models loaded. It makes inference over the same image with different models, the models names are given in a list.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Since the second iteration seems to run for 3 minutes, I would ask you to create an issue <a href=""https://github.com/pytorch/pytorch/issues"">here</a>, as it doesn’t seem right.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>If the first iteration would be a warmup time (ignoring the 3 minutes, which seems to be a bug), then you would only pay the cost once. Every other time the prediction would use the optimized graph and should be fast.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah okey. Thank you very much for the info and all your help in the forums!</p><NewLine><p>What issue should I submit? Upload traced model or the model definition code?</p><NewLine><p>I have an issue open for None objects dissapearing fron inside nn.ModuleList</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>If possible, a minimal code snippet, which is executable and shows the JIT behavior, where the second iteration takes 3 minutes, while the first one finishes in 6 seconds.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t know if I will be able to paste a minimal snippet. This post was created with the part of nn.ModuleList containing Nones.</p><NewLine><p>I have tried to measure times again:</p><NewLine><p>Withouth tracing:</p><NewLine><pre><code class=""lang-auto"">CPU times: user 12.8 s, sys: 1.41 s, total: 14.2 s<NewLine>Wall time: 2.32 s<NewLine></code></pre><NewLine><p>Scripted First Iteration:</p><NewLine><pre><code class=""lang-auto"">CPU times: user 15 s, sys: 1.77 s, total: 16.8 s<NewLine>Wall time: 4.64 s<NewLine></code></pre><NewLine><p>Scripted Second Iteration:</p><NewLine><pre><code class=""lang-auto"">CPU times: user 5min 8s, sys: 1.14 s, total: 5min 9s<NewLine>Wall time: 5min<NewLine></code></pre><NewLine><p>Scripted Third Iteration:</p><NewLine><pre><code class=""lang-auto"">CPU times: user 11 s, sys: 1.18 s, total: 12.2 s<NewLine>Wall time: 2.02 s<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""13"" data-topic=""83578"" data-username=""ptrblck""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ptrblck/40/1823_2.png"" width=""20""/> ptrblck:</div><NewLine><blockquote><NewLine><p>I would ask you to create an issue <a href=""https://github.com/pytorch/pytorch/issues"" rel=""nofollow noopener"">here</a></p><NewLine></blockquote><NewLine></aside><NewLine><p>Done.<br/><NewLine></p><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/39438"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/39438"" rel=""nofollow noopener"" target=""_blank"">[JIT] Scripted model second run cost 5 mins, after that 1-2 secs</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-06-03"" data-format=""ll"" data-time=""10:44:06"" data-timezone=""UTC"">10:44AM - 03 Jun 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/WaterKnight1998"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""WaterKnight1998"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars1.githubusercontent.com/u/41203448?v=4"" width=""20""/><NewLine>          WaterKnight1998<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">🐛 Bug<NewLine>This model uses an nn.ModuleList that used to contains None objects. After scripting the model None disappear. I opened an...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">jit</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/WaterKnight; <NewLine> ,"REPLY_DATE 1: June 1, 2020,  7:35am; <NewLine> REPLY_DATE 2: June 2, 2020,  9:23am; <NewLine> REPLY_DATE 3: June 2, 2020,  9:29am; <NewLine> REPLY_DATE 4: June 2, 2020,  6:35pm; <NewLine> REPLY_DATE 5: June 2, 2020,  7:48pm; <NewLine> REPLY_DATE 6: June 2, 2020,  9:38pm; <NewLine> REPLY_DATE 7: June 3, 2020,  5:46am; <NewLine> REPLY_DATE 8: June 3, 2020,  7:23am; <NewLine> REPLY_DATE 9: June 3, 2020,  8:00am; <NewLine> REPLY_DATE 10: June 3, 2020,  8:19am; <NewLine> REPLY_DATE 11: June 3, 2020,  8:20am; <NewLine> REPLY_DATE 12: June 3, 2020,  8:22am; <NewLine> REPLY_DATE 13: June 3, 2020,  8:25am; <NewLine> REPLY_DATE 14: June 3, 2020,  8:35am; <NewLine> REPLY_DATE 15: June 3, 2020,  9:01am; <NewLine> REPLY_DATE 16: June 3, 2020, 10:46am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> 
83772,Graph_executor behavior change in 1.5,2020-06-01T23:01:40.539Z,1,92,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>It seems that <code>.graph_for</code> doesn’t seem to run optimizations anymore with PyTorch 1.5.</p><NewLine><p>For example, from the following simple code, <code>graph_for</code> dumps different graphs between PyTorch 1.4 and PyTorch 1.5.</p><NewLine><pre><code class=""lang-auto"">$ cat small.py<NewLine>import torch<NewLine><NewLine>def f(x,y): return x + y * 3<NewLine><NewLine>print(torch.jit.script(f).graph_for(torch.rand(2, 2, device='cuda'), torch.rand(2, 2, device='cuda')))<NewLine></code></pre><NewLine><p><strong>PyTorch 1.4</strong>:</p><NewLine><pre><code class=""lang-auto"">$ python small.py<NewLine>graph(%x.1 : Float(*, *),<NewLine>      %y.1 : Float(*, *)):<NewLine>  %6 : Float(*, *) = prim::FusionGroup_0(%x.1, %y.1)<NewLine>  return (%6)<NewLine>with prim::FusionGroup_0 = graph(%0 : Float(*, *),<NewLine>      %4 : Float(*, *)):<NewLine>  %2 : int = prim::Constant[value=1]()<NewLine>  %5 : int = prim::Constant[value=3]() # small.py:3:27<NewLine>  %6 : Float(*, *) = aten::mul(%4, %5) # small.py:3:23<NewLine>  %3 : Float(*, *) = aten::add(%0, %6, %2) # small.py:3:19<NewLine>  return (%3)<NewLine></code></pre><NewLine><p><strong>PyTorch 1.5</strong>:</p><NewLine><pre><code class=""lang-auto"">$ python small.py<NewLine>graph(%x.1 : Tensor,<NewLine>      %y.1 : Tensor):<NewLine>  %3 : int = prim::Constant[value=3]() # small.py:3:27<NewLine>  %2 : int = prim::Constant[value=1]()<NewLine>  %4 : Tensor = aten::mul(%y.1, %3) # small.py:3:23<NewLine>  %5 : Tensor = aten::add(%x.1, %4, %2) # small.py:3:19<NewLine>  return (%5)<NewLine></code></pre><NewLine><p>Is this expected?  With PyTorch 1.5, I don’t see any fusion group.</p><NewLine></div>",https://discuss.pytorch.org/u/seanprime7,,seanprime7,"June 1, 2020, 11:02pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Found the description of the change in the release note: <a href=""https://github.com/pytorch/pytorch/releases/tag/v1.5.0"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/releases/tag/v1.5.0</a>.</p><NewLine><p>But even with <code>torch._C._jit_set_profiling_mode(True)</code>, I don’t see any fusion group in the output.</p><NewLine><pre><code class=""lang-auto"">$ cat small.py<NewLine>import torch<NewLine>torch._C._jit_set_profiling_mode(True)<NewLine><NewLine>def f(x,y): return x + y * 3<NewLine><NewLine>print(torch.jit.script(f).graph_for(torch.rand(2, 2, device='cuda'), torch.rand(2, 2, device='cuda')))<NewLine><NewLine>$ python small.py<NewLine>graph(%x.1 : Tensor,<NewLine>      %y.1 : Tensor):<NewLine>  %2 : int = prim::Constant[value=3]() # small.py:4:27<NewLine>  %3 : int = prim::Constant[value=1]()<NewLine>  %4 : Tensor = prim::profile(%y.1)<NewLine>  %5 : Tensor = aten::mul(%4, %2) # small.py:4:23<NewLine>  %6 : Tensor = prim::profile(%x.1)<NewLine>  %7 : Tensor = prim::profile(%5)<NewLine>  %8 : Tensor = aten::add(%6, %7, %3) # small.py:4:19<NewLine>  %9 : Tensor = prim::profile(%8)<NewLine>   = prim::profile()<NewLine>  return (%9)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you try to run the code in the latest nightly build?<br/><NewLine>If you can still reproduce this issue, feel free to create an issue <a href=""https://github.com/pytorch/pytorch/issues"">here</a> including the reproducible code snippets.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/seanprime7; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: June 1, 2020, 11:24pm; <NewLine> REPLY_DATE 2: June 2, 2020,  7:34am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
69372,Torchscript support for Deformable Convolution v2,2020-02-11T14:18:00.078Z,7,777,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!<br/><NewLine>I heard PyTorch supports Deformable Convolution out of the box since 1.4 release, I just can not say by looking at code if it is version 1 or version 2 (<a href=""https://github.com/CharlesShang/DCNv2/tree/master"" rel=""nofollow noopener"">https://github.com/CharlesShang/DCNv2/tree/master</a>).<br/><NewLine>My question is does the support for Deformable Convolution mean I can jit it to torchscript?<br/><NewLine>And if I add the DCv2 layer to PyTroch from the aforementioned repository can I convert it to torchscript?</p><NewLine><p>Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/igor_davidyuk,(Igor Davidyuk),igor_davidyuk,"February 11, 2020,  2:18pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It looks like the custom kernels in the repo linked are bound using pybind (<a href=""https://github.com/CharlesShang/DCNv2/blob/master/src/vision.cpp"" rel=""nofollow noopener"">https://github.com/CharlesShang/DCNv2/blob/master/src/vision.cpp</a>). So they will not work with TorchScript as is. However, it is possible to register the operator with TorchScript following <a href=""https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html</a>, which would work with TorchScript.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot!<br/><NewLine>I will try it out.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Any progress yet ? regarding dcnv2</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>It works fine.<br/><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicon.ico"" width=""16""/><NewLine><a href=""https://github.com/xi11xi19/CenterNet2TorchScript"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars1.githubusercontent.com/u/11573107?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/xi11xi19/CenterNet2TorchScript"" rel=""nofollow noopener"" target=""_blank"">xi11xi19/CenterNet2TorchScript</a></h3><NewLine><p>centernet pytorch model to torch script model. Contribute to xi11xi19/CenterNet2TorchScript development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Which libtorch version did u use to compile dcn_v2_cuda_forward_v2? I got error when compiling</p><NewLine><pre><code class=""lang-auto"">candidate: constexpr torch::jit::RegisterOperators::RegisterOperators(torch::jit::RegisterOperators&amp;&amp;)<NewLine>/usr/lib/libtorch_abi11_14/include/torch/csrc/jit/custom_operator.h:16:18: note:   candidate expects 1 argument, 2 provided<NewLine>CMakeFiles/dcn_v2_cuda_forward_v2.dir/build.make:1509: recipe for target 'CMakeFiles/dcn_v2_cuda_forward_v2.dir/vision.cpp.o' faile<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>In newer versions operator registration api changed, you should type like this</p><NewLine><blockquote><NewLine><p>static auto registry = torch::RegisterOperators(“my_ops::warp_perspective”, &amp;warp_perspective);</p><NewLine></blockquote><NewLine><p>Change your first line.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Now it works fine (pythorch 1.4), thanks, btw do you try using  pytorch version 1.5 I got this error:<br/><NewLine>Could not export Python function call ‘_DCNv2’. Remove calls to Python functions before export</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I try to trace model for CenterTrack model, but I got this error</p><NewLine><blockquote><NewLine><p>TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:<br/><NewLine>Not within tolerance rtol=1e-05 atol=1e-05 at input[26, 79, 149] (-1.9391770362854004 vs. -1.9395387172698975) and 669 other locations (0.00%)<br/><NewLine>check_tolerance, _force_outplace, True, _module_class)</p><NewLine></blockquote><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zdevito; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/igor_davidyuk; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Kiki_Rizki_Arpiandi; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/igor_davidyuk; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Kiki_Rizki_Arpiandi; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/igor_davidyuk; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Kiki_Rizki_Arpiandi; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Kiki_Rizki_Arpiandi; <NewLine> ,"REPLY_DATE 1: February 11, 2020,  5:42pm; <NewLine> REPLY_DATE 2: February 12, 2020,  9:33am; <NewLine> REPLY_DATE 3: March 11, 2020,  3:44am; <NewLine> REPLY_DATE 4: March 11, 2020,  9:39am; <NewLine> REPLY_DATE 5: March 12, 2020,  5:46am; <NewLine> REPLY_DATE 6: March 19, 2020,  7:44am; <NewLine> REPLY_DATE 7: May 29, 2020,  6:43am; <NewLine> REPLY_DATE 8: June 2, 2020,  4:20am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
60465,Model Summary for libTorch jit model?,2019-11-09T13:38:11.100Z,3,434,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Is it possible to get the model summary described in this link<br/><NewLine><aside class=""quote quote-modified"" data-post=""5"" data-topic=""2678""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/sksq96/40/13282_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/is-there-similar-pytorch-function-as-model-summary-as-keras/2678/5"">Is there similar pytorch function as model.summary() as keras?</a><NewLine></div><NewLine><blockquote><NewLine>    Yes, you can get exact Keras representation, using <a href=""https://github.com/sksq96/pytorch-summary"" rel=""nofollow noopener"">this code</a>. <NewLine>Example for VGG16<NewLine>from torchvision import models<NewLine>from summary import summary<NewLine><NewLine>vgg = models.vgg16()<NewLine>summary(vgg, (3, 224, 224))<NewLine><NewLine>----------------------------------------------------------------<NewLine>        Layer (type)               Output Shpae         Param #<NewLine>================================================================<NewLine>            Conv2d-1         [-1, 64, 224, 224]            1792<NewLine>              ReLU-2         [-1, 64, 224, 224]    …<NewLine>  </blockquote><NewLine></aside><NewLine><br/><NewLine>, but for cpp jit model?</p><NewLine><p>This is an enhancement for printing the model in this link;<br/><NewLine><aside class=""quote quote-modified"" data-post=""2"" data-topic=""60297""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/driazati/40/15393_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/print-network-architecture-in-cpp-jit/60297/2"">Print network architecture in cpp jit</a> <a class=""badge-wrapper bullet"" href=""/c/jit""><span class=""badge-category-bg"" style=""background-color: #0088CC;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""A category for torchscript and the PyTorch JIT compiler"">jit</span></a><NewLine></div><NewLine><blockquote><NewLine>    We don’t have any built-in utils for this, but you can do a simple version manually: <NewLine>#include &lt;torch/script.h&gt;<NewLine><NewLine>#include &lt;iostream&gt;<NewLine>#include &lt;memory&gt;<NewLine><NewLine>void tabs(size_t num) {<NewLine>  for (size_t i = 0; i &lt; num; i++) {<NewLine>    std::cout &lt;&lt; ""\t"";<NewLine>  }<NewLine>}<NewLine><NewLine>void print_modules(const torch::jit::script::Module&amp; module, size_t level = 0) {<NewLine>  std::cout &lt;&lt; module.name().qualifiedName() &lt;&lt; "" (\n"";<NewLine>  for (const auto&amp; module : module.get_modules()) {<NewLine>    tabs(level + 1);<NewLine>    print_modules(module.module, level + 1);<NewLine>  …<NewLine>  </blockquote><NewLine></aside><NewLine></p><NewLine><p>Thanks.</p><NewLine><p>rgds,<br/><NewLine>CL</p><NewLine></div>",https://discuss.pytorch.org/u/tancl,,tancl,"November 10, 2019,  4:08pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just for sharing, this the one of the way of show network architecture for jit model in cpp.</p><NewLine><pre><code class=""lang-auto"">// torchex1.cpp : This file contains the 'main' function. Program execution<NewLine>// begins and ends there.<NewLine>//<NewLine><NewLine>#include &lt;torch/script.h&gt;<NewLine>#include &lt;iostream&gt;<NewLine>#include &lt;inttypes.h&gt;<NewLine><NewLine>#include &lt;iostream&gt;<NewLine>#include &lt;memory&gt;<NewLine><NewLine>void tabs(size_t num) {<NewLine>  for (size_t i = 0; i &lt; num; i++) {<NewLine>	std::cout &lt;&lt; ""\t"";<NewLine>  }<NewLine>}<NewLine><NewLine>void print_modules(const torch::jit::script::Module&amp; module, size_t level = 0) {<NewLine>  // std::cout &lt;&lt; module.name().qualifiedName() &lt;&lt; "" (\n"";<NewLine>  std::cout &lt;&lt; module.name().name() &lt;&lt; "" (\n"";	<NewLine>  <NewLine>  for (const auto&amp; parameter : module.get_parameters()) {<NewLine>	tabs(level + 1);<NewLine>	std::cout &lt;&lt; parameter.name() &lt;&lt; '\t';<NewLine>	std::cout &lt;&lt; parameter.value().toTensor().sizes() &lt;&lt; '\n';<NewLine>  } <NewLine><NewLine><NewLine>  for (const auto&amp; module : module.get_modules()) {<NewLine>	tabs(level + 1);<NewLine>	print_modules(module, level + 1);<NewLine>  }<NewLine><NewLine>  tabs(level);<NewLine>  std::cout &lt;&lt; "")\n"";<NewLine>}<NewLine><NewLine>int main(int argc, const char* argv[]) {<NewLine>  torch::jit::script::Module container = torch::jit::load(""net.pt"");<NewLine>  print_modules(container);<NewLine>  return 0;<NewLine>}<NewLine><NewLine></code></pre><NewLine><p>The output looks like:</p><NewLine><pre><code class=""lang-auto"">net (<NewLine>        conv1 (<NewLine>                weight  [10, 1, 5, 5]<NewLine>                bias    [10]<NewLine>        )<NewLine>        conv2 (<NewLine>                weight  [20, 10, 5, 5]<NewLine>                bias    [20]<NewLine>        )<NewLine>        conv2_drop (<NewLine>        )<NewLine>        fc1 (<NewLine>                weight  [50, 320]<NewLine>                bias    [50]<NewLine>        )<NewLine>        fc2 (<NewLine>                weight  [10, 50]<NewLine>                bias    [10]<NewLine>        )<NewLine>)<NewLine><NewLine></code></pre><NewLine><p>It seems like a stupid way, any smarter way please feel free to share.</p><NewLine><p>thanks.</p><NewLine><p>rgds,<br/><NewLine>CL</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It also looks like there is <code>script::Module::dump()</code> which will print something like this, you can toggle it to include the sections that are relevant to you:</p><NewLine><pre><code class=""lang-cpp"">  void dump(<NewLine>      bool print_method_bodies,  // you probably want this to be `false` for a summary<NewLine>      bool print_attr_values,<NewLine>      bool print_param_values) const;<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">module __torch__.M {<NewLine>  parameters {<NewLine>  }<NewLine>  attributes {<NewLine>    training = True<NewLine>  }<NewLine>  methods {<NewLine>    method forward {<NewLine>      graph(%self : ClassType&lt;M&gt;,<NewLine>            %x.1 : Tensor):<NewLine>        %3 : Tensor = prim::CallMethod[name=""other_fn""](%self, %x.1) # ../test.py:36:15<NewLine>        return (%3)<NewLine>  <NewLine>    }<NewLine>    method other_fn {<NewLine>      graph(%self : ClassType&lt;M&gt;,<NewLine>            %x.1 : Tensor):<NewLine>        %4 : int = prim::Constant[value=1]()<NewLine>        %3 : int = prim::Constant[value=10]() # ../test.py:33:19<NewLine>        %5 : Tensor = aten::add(%x.1, %3, %4) # ../test.py:33:15<NewLine>        return (%5)<NewLine>  <NewLine>    }<NewLine>  }<NewLine>  submodules {<NewLine>  }<NewLine>}<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks for the recommendation again.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>when i use the code ,i met the trouble “class torch::jit::script::Module have no members name and  get_parameters()”</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tancl; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tancl; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Rem_H; <NewLine> ,"REPLY_DATE 1: November 12, 2019,  4:21pm; <NewLine> REPLY_DATE 2: November 13, 2019,  9:45pm; <NewLine> REPLY_DATE 3: November 14, 2019,  1:16am; <NewLine> REPLY_DATE 4: May 29, 2020,  6:17am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
83264,Expected integer literal for index,2020-05-28T16:29:34.359Z,0,205,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I opened this issue becasue torch.jit.script is not working with hrnet</p><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/39165"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/39165"" rel=""nofollow noopener"" target=""_blank"">[JIT] Expected integer literal for index:</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-05-28"" data-format=""ll"" data-time=""16:21:54"" data-timezone=""UTC"">04:21PM - 28 May 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/WaterKnight1998"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""WaterKnight1998"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars1.githubusercontent.com/u/41203448?v=4"" width=""20""/><NewLine>          WaterKnight1998<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">🐛 Bug<NewLine>I have tried to convert this model to TorchScript using torch.jit.script. However, I am getting this issue:<NewLine>RuntimeError: <NewLine>Expected integer literal...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">jit</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>",https://discuss.pytorch.org/u/WaterKnight,(David Lacalle),WaterKnight,"May 28, 2020,  4:29pm",,,,,
81324,How to set different threads number for different modules in TorchScript,2020-05-15T06:17:37.996Z,1,106,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I read this <a href=""https://pytorch.org/docs/stable/torch.html?highlight=set_num_thread#torch.set_num_threads"" rel=""nofollow noopener"">doc</a> and found that torch.set_num_threads and torch.set_num_interop_threads can only be called before running script model.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/3050cbc1971738b04913b177a10be352225ade11"" href=""https://discuss.pytorch.org/uploads/default/original/3X/3/0/3050cbc1971738b04913b177a10be352225ade11.png"" title=""threads""><img alt=""threads"" data-base62-sha1=""6Tq1qASZRWUMF2qPYAdbErzR9lL"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/3/0/3050cbc1971738b04913b177a10be352225ade11_2_10x10.png"" height=""362"" src=""https://discuss.pytorch.org/uploads/default/original/3X/3/0/3050cbc1971738b04913b177a10be352225ade11.png"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">threads</span><span class=""informations"">870×457 29.7 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>Is there a way to control number of threads more flexibly in TorchScript that for a model’s different modules, we can set difference number of interop threads and introop threads to run?</p><NewLine><p>Thanks for any answers!</p><NewLine></div>",https://discuss.pytorch.org/u/knitvoger,(knitvoger),knitvoger,"May 15, 2020,  6:20am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, there’s no option today for controlling parallelism on a module level</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Got it, thank you for your answer!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/knitvoger; <NewLine> ,"REPLY_DATE 1: May 28, 2020,  7:55am; <NewLine> REPLY_DATE 2: May 28, 2020,  7:56am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
82939,Modify the existing Torchscript model,2020-05-26T08:57:44.898Z,0,80,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I am trying to modify an existing Torchscript model for model compression. For example, I want to change several parameters of the aten:_convolution operators. To explain the question more clearly, suppose we traced a model in the following way:</p><NewLine><pre><code class=""lang-auto"">net = torchvison.models.resnet18().cuda()<NewLine>data = torch.ones(1, 3 , 224, 224)<NewLine>traced = torch.jit.trace(net, data)<NewLine></code></pre><NewLine><p>The question is can I modify the torchscript model by directly modifying the traced.graph so that the new model can be loaded and run in a different place?   I also noticed that there is a ‘code’ variable in the ‘traced’, what is this variable for? Do I need to modify the ‘code’ variable at the same time?</p><NewLine><p>Thanks ~</p><NewLine></div>",https://discuss.pytorch.org/u/AlvinZheng,(AlvinZheng),AlvinZheng,"May 26, 2020,  8:57am",,,,,
75874,torch::jit::script::Module with hook function?,2020-04-08T14:20:33.756Z,3,132,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is there any way to compute the gradient of activation map at a certain later with respect to the one of the output components with torch::jit::script::Module?<br/><NewLine>it seems the hook function only supports the nn::Module.<br/><NewLine>Could you provide a simple example?</p><NewLine><p>Thank you very much in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/Edwinzero,(Zhao Edwin),Edwinzero,"April 8, 2020,  2:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hooks aren’t supported right now but we are working on it and it will probably be done in the next couple weeks</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much for your reply <a class=""mention"" href=""/u/driazati"">@driazati</a> !<br/><NewLine>I am looking forward to that feature but however, I am catching a due with in one week, I wonder is there any alternate way to compute the gradient of hidden activation map at certain layer with jit::script::Module?</p><NewLine><p>Thank you very much for your attention to this matter.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>We don’t have any support for hooks so anything you do is going to be pretty hacky; that said, you might be able to get something working by adding a custom operator to TorchScript that just calls a C++ <code>autograd::Function</code> that does nothing except let’s you grab the gradients in the backward pass.</p><NewLine><p>There’s an example of a custom op used in this way in torchvision, see <a href=""https://github.com/pytorch/vision/blob/master/torchvision/csrc/empty_tensor_op.h"">https://github.com/pytorch/vision/blob/master/torchvision/csrc/empty_tensor_op.h</a> and <a href=""https://github.com/pytorch/vision/blob/43e94b39bcdda519c093ca11d99dfa2568aa7258/torchvision/csrc/vision.cpp#L51"">https://github.com/pytorch/vision/blob/43e94b39bcdda519c093ca11d99dfa2568aa7258/torchvision/csrc/vision.cpp#L51</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group quote-modified"" data-post=""4"" data-topic=""75874"" data-username=""driazati""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/driazati/40/15393_2.png"" width=""20""/> driazati:</div><NewLine><blockquote><NewLine><p>Hooks aren’t supported right now but we are working on it and it will probably be done in the next couple weeks</p><NewLine></blockquote><NewLine></aside><NewLine><p>Hello <a class=""mention"" href=""/u/driazati"">@driazati</a>, does current LibTorch1.5 supports hook in the jit model?<br/><NewLine>If so, could you please give an example of using hook function?<br/><NewLine>Thank you very much for your attention to this matter.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Edwinzero; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Edwinzero; <NewLine> ,"REPLY_DATE 1: April 9, 2020,  3:22am; <NewLine> REPLY_DATE 2: April 9, 2020,  4:59am; <NewLine> REPLY_DATE 3: April 10, 2020,  3:10am; <NewLine> REPLY_DATE 4: May 23, 2020,  2:23am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> 
81873,TorchScript for the backward (autograd) graph,2020-05-18T18:41:02.552Z,2,140,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to get a sense for the level of support in TorchScript for back propagation graphs produced by using autograd. If anyone can provide a quick summary, and/or pointers to how one can experiment with this - it would be much appreciated.</p><NewLine><p>Ljubisa</p><NewLine></div>",https://discuss.pytorch.org/u/ljubisa,(Ljubisa Bajic),ljubisa,"May 18, 2020,  6:41pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>TorchScript has full support for PyTorch’s tape-based autograd. You can call <code>backward()</code> on your tensors if you are recording gradients and it should work.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Michael,</p><NewLine><p>Thanks for the prompt response. I am interested in tracing through the backward graph using TorchScript and dumping the IR for the autodiff-ed backdrop graph, for full graph optimization in a separate framework. To be precise - on the example of a backward op for a matmul, I’d expect to get the appropriately transposed matmul relevant to the backward pass in the dumped IR graph. Would you expect this to be possible?</p><NewLine><p>Ljubisa</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah, we do not have a public API for exposing a static backward graph, as PyTorch relies on dynamic autograd for doing automatic differentiation. We do have an internal API for symbolic differentiation (see <code>torch/csrc/jit/runtime/autodiff.cpp</code> which you can play with, but it is not complete and we don’t have any stability guarantees about it <img alt="":stuck_out_tongue:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/stuck_out_tongue.png?v=9"" title="":stuck_out_tongue:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ljubisa; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: May 18, 2020,  7:25pm; <NewLine> REPLY_DATE 2: May 19, 2020,  1:42pm; <NewLine> REPLY_DATE 3: May 21, 2020,  2:53am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
81412,JIT for simple linear models?,2020-05-15T16:56:08.299Z,1,135,"<div class=""post"" itemprop=""articleBody""><NewLine><p>What can be expected from JIT for simple linear models?</p><NewLine><p>It’s possible I’m using JIT trace_module wrong, but here’s my model:</p><NewLine><pre><code class=""lang-python"">model = nn.Sequential(<NewLine>        nn.Linear(784, 200),<NewLine>        nn.ReLU(),<NewLine>        nn.Linear(200, 200),<NewLine>        nn.ReLU(),<NewLine>        nn.Linear(200, 10),<NewLine>)<NewLine>model = model.to(DEVICE)<NewLine>model = torch.jit.trace_module(<NewLine>        model, {'forward': torch.randn(BATCH_SIZE, 784, device=DEVICE)})<NewLine><NewLine>for inputs, targets in data:<NewLine>  optimizer.zero_grad()<NewLine>  outputs = model(inputs)<NewLine>  loss = loss_fn(outputs, targets)<NewLine>  loss.backward()<NewLine>  optimizer.step()<NewLine></code></pre><NewLine><p>A simple linear model for MNIST.</p><NewLine><p>I have a few observations:</p><NewLine><ol><NewLine><li>From using Nsight systems, it’s quite clear that without JIT, the performance is quite bad. This is normal since the kernels are very small, so there’s nothing much for the CPU or GPU to do.</li><NewLine><li>I’d expect JIT to be able to fuse quite a few of these kernels though. However, when using <code>trace</code> as shown above, the situation is exactly the same as without JIT. How come? Am I using JIT correctly here? And what kernels can I expect to be fused for this (simple) model ?</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/mjoux,(Matt),mjoux,"May 15, 2020,  4:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for bringing this to our attention.</p><NewLine><p>Supporting matmul + relu on CPU, among other types of fusion, in a few months is on our roadmap. Then we might look into GPU. We will let you know if it is close to being ready.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the quick answer and yes, it would be good to know when this is ready <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/> !</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zheng-xq; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mjoux; <NewLine> ,"REPLY_DATE 1: May 15, 2020,  7:12pm; <NewLine> REPLY_DATE 2: May 18, 2020,  7:47am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> 
80547,Error in script::Module::get_method,2020-05-10T08:58:43.212Z,2,147,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">torch::Tensor hidden = module.get_method(""get_initial_state"")({torch::tensor({20})});<NewLine></code></pre><NewLine><p><strong>/home/rakesh/rishabh_workspace/Garbage/LMtesting/LMtesting.cpp:65:64:</strong> <strong>error:</strong> conversion from ‘ <strong>c10::IValue</strong> ’ to non-scalar type ‘ <strong>at::Tensor</strong> ’ requested</p><NewLine><p>torch::Tensor hidden = <strong>module.get_method(“get_initial_state”)({torch::tensor({20})})</strong> ;<br/><NewLine>^</p><NewLine></div>",https://discuss.pytorch.org/u/krrishabh,(Rishabh Kumar),krrishabh,"May 10, 2020,  8:59am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>get_method</code> returns an IValue, which you can sort of think of as like a PyObject. You need to explicitly cast it to the C++ type you want, so you should do <code>hidden.toTensor()</code> to get an <code>at::Tensor</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>For example :</p><NewLine><pre><code class=""lang-auto"">auto hidden_map = module.get_method(""get_initial_state"")({torch::tensor({1})});<NewLine>torch::Tensor t = hidden_map.toTensor()<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/krrishabh; <NewLine> ,"REPLY_DATE 1: May 18, 2020,  1:39am; <NewLine> REPLY_DATE 2: May 18, 2020,  1:39am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
81629,FastRNNs Benchmark - custom lstms reverse,2020-05-17T11:34:06.467Z,0,85,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to write a custom LSTM with TorchScript. Naturally, my first impulse was to take the fastRNNs benchmark <a href=""https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py"" rel=""nofollow noopener"">custom_lstms file</a> and to run it. However, it fails in both torch 1.4 and 1.5.</p><NewLine><blockquote><NewLine><p>RuntimeError: The following operation failed in the TorchScript interpreter.<br/><NewLine>Traceback of TorchScript (most recent call last):<br/><NewLine>File “C:/Users/anarc/git/Elmo-Training-PyTorch/elmo/modules/test.py”, line 298, in forward<br/><NewLine>for rnn_layer in self.layers:<br/><NewLine>state = states[i]<br/><NewLine>output, out_state = rnn_layer(output, state)<br/><NewLine>~~~~~~~~~ &lt;— HERE<br/><NewLine>output_states += [out_state]<br/><NewLine>i += 1<br/><NewLine>File “C:/Users/anarc/git/Elmo-Training-PyTorch/elmo/modules/test.py”, line 238, in forward<br/><NewLine>for direction in self.directions:<br/><NewLine>state = states[i]<br/><NewLine>out, out_state = direction(input, state)<br/><NewLine>~~~~~~~~~ &lt;— HERE<br/><NewLine>outputs += [out]<br/><NewLine>output_states += [out_state]<br/><NewLine>File “C:/Users/anarc/git/Elmo-Training-PyTorch/elmo/modules/test.py”, line 210, in forward<br/><NewLine>def forward(self, input, state):<br/><NewLine># type: (Tensor, Tuple[Tensor, Tensor]) -&gt; Tuple[Tensor, Tuple[Tensor, Tensor]]<br/><NewLine>inputs = reverse(input.unbind(0))<br/><NewLine>~~~~~~~ &lt;— HERE<br/><NewLine>outputs = jit.annotate(List[Tensor], [])<br/><NewLine>for i in range(len(inputs)):<br/><NewLine>File “C:/Users/anarc/git/Elmo-Training-PyTorch/elmo/modules/test.py”, line 90, in reverse<br/><NewLine>def reverse(lst):<br/><NewLine># type: (List[Tensor]) -&gt; List[Tensor]<br/><NewLine>return lst[::-1]<br/><NewLine>~~~~~~~~ &lt;— HERE<br/><NewLine>RuntimeError: invalid vector subscript</p><NewLine></blockquote><NewLine><p>To my understanding, negative strides are neither supported, nor on the roadmap (according to <a href=""https://github.com/pytorch/pytorch/issues/229#issuecomment-548092342"" rel=""nofollow noopener"">this</a>). So this doesn’t surprise me, but why is that reverse function there then?</p><NewLine><p><strong>Am I missing something, is there a trick to reversing a Tensor like that?</strong></p><NewLine><p>Obviously, that would make creating a backwards layer easier (and probably faster than torch.flip, since it allocates new memory apparently)</p><NewLine></div>",https://discuss.pytorch.org/u/tsteffek,(Tsteffek),tsteffek,"May 17, 2020, 11:34am",,,,,
81067,Subblocks of Node,2020-05-13T17:36:35.756Z,0,77,"<div class=""post"" itemprop=""articleBody""><NewLine><p><code>torch/csrc/jit/ir/ir.h</code> explains the concept of sub-blocks of a node and state that inputs to a block that represent control flow act as equivalent phi-nodes by defining a new Value to represent any term that has multiple definitions depending on how control flowed.</p><NewLine><p>Is there an example where I can see the creation of these new Values?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/almeetb,,almeetb,"May 13, 2020,  5:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The easiest way to see example IR is to compile a small torchscript snippet using <code>torch.jit.script()</code> and calling <code>.graph</code> on the resulting object.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: May 15, 2020,  5:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
81158,Timing in Torchscript,2020-05-14T08:41:10.929Z,1,106,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Below code is not working in torch.jit.script</p><NewLine><pre><code class=""lang-auto"">    def forward(self, x):<NewLine>        start = time.time() * 1000<NewLine>        y = self.compute_conv1d((x))<NewLine>        print(""cost %d ms"" % (time.time() * 1000 - start))<NewLine>        return y<NewLine></code></pre><NewLine><p>error</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>Python builtin &lt;built-in function time&gt; is currently not supported in Torchscript:<NewLine></code></pre><NewLine><p>Since time.time() is not supported in Torchscript, is there any way to measure each module’s cost inside script model? I’m running on a cpu machine.</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/knitvoger,(knitvoger),knitvoger,"May 14, 2020,  8:41am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>something like</p><NewLine><pre><code class=""lang-auto"">@jit.ignore<NewLine>def jtime() -&gt; float:<NewLine>  return time.time()<NewLine></code></pre><NewLine><p>autograd.profiler.profile + export_chrome_trace is another method</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/googlebot"">@googlebot</a> thank you! It works!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/googlebot; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/knitvoger; <NewLine> ,"REPLY_DATE 1: May 14, 2020,  2:42pm; <NewLine> REPLY_DATE 2: May 14, 2020,  2:43pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
81079,Does TorchScript support RPC-based models?,2020-05-13T18:33:26.269Z,1,155,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve been trying to run <code>torch.jit.script(rpc_model)</code> on the RNN parallel model described at <a href=""https://pytorch.org/tutorials/intermediate/rpc_tutorial.html#distributed-rnn-using-distributed-autograd-and-distributed-optimizer"" rel=""nofollow noopener"">Pytorch’s RPC tutorial</a>, but have been hitting issues.</p><NewLine><p>My attempt includes adding <code>script_model = torch.jit.script(model)</code> tight before training loop and comment out the loop, as I dont need to train the model.</p><NewLine><p>The first error I got is related to TorchScript not supportting <code>*args</code> and <code>**kwargs</code> from <code>_remote_method</code> and <code>_call_method</code>. In an attempt to bypass this error, I have changed RNNModel.forward to a simple <code>emb = rpc_sync(self.emb_table_rref.owner(), EmbeddingTable.forward, args=input)</code> (and commented out all other calls to local and remote layers) just to see whether this layer could be converted to torchscript. It failed with the following a similar issue, but not due to any public facing dictionary: <code>torch.jit.frontend.NotSupportedError: keyword-arg expansion is not supported:</code>. Here is the full stack</p><NewLine><p>$ mpirun -np 2 python rnn_jit.py<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “rnn_jit.py”, line 156, in <br/><NewLine>run_worker()<br/><NewLine>File “rnn_jit.py”, line 145, in run_worker<br/><NewLine>_run_trainer()<br/><NewLine>File “rnn_jit.py”, line 116, in _run_trainer<br/><NewLine>traced = torch.jit.script(model)<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/<strong>init</strong>.py”, line 1261, in script<br/><NewLine>return torch.jit._recursive.create_script_module(obj, torch.jit._recursive.infer_methods_to_compile)<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 305, in create_script_module<br/><NewLine>return create_script_module_impl(nn_module, concrete_type, stubs_fn)<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 361, in create_script_module_impl<br/><NewLine>create_methods_from_stubs(concrete_type, stubs)<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 279, in create_methods_from_stubs<br/><NewLine>concrete_type._create_methods(defs, rcbs, defaults)<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 568, in try_compile_fn<br/><NewLine>return torch.jit.script(fn, _rcb=rcb)<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/<strong>init</strong>.py”, line 1290, in script<br/><NewLine>fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 568, in try_compile_fn<br/><NewLine>return torch.jit.script(fn, _rcb=rcb)<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/<strong>init</strong>.py”, line 1287, in script<br/><NewLine>ast = get_jit_def(obj)<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/frontend.py”, line 173, in get_jit_def<br/><NewLine>return build_def(ctx, py_ast.body[0], type_line, self_name)<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/frontend.py”, line 206, in build_def<br/><NewLine>build_stmts(ctx, body))<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/frontend.py”, line 129, in build_stmts<br/><NewLine>stmts = [build_stmt(ctx, s) for s in stmts]<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/frontend.py”, line 129, in <br/><NewLine>stmts = [build_stmt(ctx, s) for s in stmts]<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/frontend.py”, line 181, in <strong>call</strong><br/><NewLine>return method(ctx, node)<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/frontend.py”, line 363, in build_If<br/><NewLine>build_stmts(ctx, stmt.body),<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/frontend.py”, line 129, in build_stmts<br/><NewLine>stmts = [build_stmt(ctx, s) for s in stmts]<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/frontend.py”, line 129, in <br/><NewLine>stmts = [build_stmt(ctx, s) for s in stmts]<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/frontend.py”, line 181, in <strong>call</strong><br/><NewLine>return method(ctx, node)<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/frontend.py”, line 288, in build_Assign<br/><NewLine>rhs = build_expr(ctx, stmt.value)<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/frontend.py”, line 181, in <strong>call</strong><br/><NewLine>return method(ctx, node)<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/frontend.py”, line 464, in build_Call<br/><NewLine>raise NotSupportedError(kw_expr.range(), ‘keyword-arg expansion is not supported’)<br/><NewLine>torch.jit.frontend.NotSupportedError: keyword-arg expansion is not supported:<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/distributed/rpc/api.py”, line 476</p><NewLine><pre><code>if qualified_name is not None:<NewLine>    fut = _invoke_rpc_builtin(dst_worker_info, qualified_name, rf, *args, **kwargs)<NewLine>                                                                            ~~~~~~ &lt;--- HERE<NewLine>elif isinstance(func, torch.jit.ScriptFunction):<NewLine>    fut = _invoke_rpc_torchscript(dst_worker_info.name, func, args, kwargs)<NewLine></code></pre><NewLine><p>‘rpc_sync’ is being compiled since it was called from ‘RNNModel.forward’<br/><NewLine>File “rnn_jit.py”, line 68<br/><NewLine># pass input to the remote embedding table and fetch emb tensor back<br/><NewLine># emb = _remote_method(EmbeddingTable.forward, self.emb_table_rref, input)<br/><NewLine>emb = rpc_sync(self.emb_table_rref.owner(), EmbeddingTable.forward, args=input)<br/><NewLine>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;— HERE<br/><NewLine># output, hidden = self.rnn(emb, hidden)<br/><NewLine># pass output to the rremote decoder and get the decoded output back</p><NewLine><hr/><NewLine><h2>Primary job  terminated normally, but 1 process returned<br/><NewLine>a non-zero exit code. Per user-direction, the job has been aborted.</h2><NewLine><hr/><NewLine><p>mpirun detected that one or more processes exited with non-zero status, thus causing<br/><NewLine>the job to be terminated. The first process to do so was:</p><NewLine><h2>Process name: [[29685,1],1]<br/><NewLine>Exit code:    1</h2><NewLine></div>",https://discuss.pytorch.org/u/Thiago.Crepaldi,(Thiago Crepaldi),Thiago.Crepaldi,"May 13, 2020,  6:33pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/thiago.crepaldi"">@Thiago.Crepaldi</a></p><NewLine><p>TorchScript integration with RPC is still experimental, and we are working on closing the gaps. Currently, in v1.5, applications can run TorchScript functions using RPC, e.g., <code>rpc_sync/rpc_async/remote(to, my_script_func, args=(...))</code>. But, within a script function only <code>rpc_async</code> can be called. See the test below:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/dad552666e1aaff174b549a38fce24b517c53d21/torch/testing/_internal/distributed/rpc/jit/rpc_test.py#L146-L154"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/dad552666e1aaff174b549a38fce24b517c53d21/torch/testing/_internal/distributed/rpc/jit/rpc_test.py#L146-L154"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/dad552666e1aaff174b549a38fce24b517c53d21/torch/testing/_internal/distributed/rpc/jit/rpc_test.py#L146-L154</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""146"" style=""counter-reset: li-counter 145 ;""><NewLine><li>@torch.jit.script</li><NewLine><li>def call_rpc_with_profiling(handle: Tensor, dst_worker_name: str) -&gt; Tensor:</li><NewLine><li>    # Call rpc_async from within ScriptFunction and ensure that we can attach</li><NewLine><li>    # profiling callbacks. Note that handle here is a Tensor representation of</li><NewLine><li>    # RecordFunction.</li><NewLine><li>    fut = rpc.rpc_async(dst_worker_name, one_arg, (torch.tensor(1),))</li><NewLine><li>    torch.ops.profiler._call_end_callbacks_on_jit_fut(handle, fut)</li><NewLine><li>    ret = fut.wait()</li><NewLine><li>    return ret</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Could you please try if using <code>rpc_async</code> works for you?</p><NewLine><p>The issue below is tracking the progress on adding native <code>RemoteModule</code> support. Please feel free to comment there.</p><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/37136"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/37136"" rel=""nofollow noopener"" target=""_blank"">[Design][RFC] RemoteModule API Design</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-04-23"" data-format=""ll"" data-time=""08:03:15"" data-timezone=""UTC"">08:03AM - 23 Apr 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/xush6528"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""xush6528"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/7608630?v=4"" width=""20""/><NewLine>          xush6528<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">[Design][RFC] RemoteModule API Design<NewLine>FB internal link, [Design] RemoteModule<NewLine>Goal<NewLine>Provide a wrapper nn.Module API that serves these purposes<NewLine>Users should feel like using remote...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">jit</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">module: rpc</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the quick reply, Shen Li. By using rpc_async, I got similar error:</p><NewLine><p>$ mpirun -np 2 python rnn_jit.py<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “rnn_jit.py”, line 156, in <br/><NewLine>run_worker()<br/><NewLine>File “rnn_jit.py”, line 145, in run_worker<br/><NewLine>_run_trainer()<br/><NewLine>File “rnn_jit.py”, line 116, in _run_trainer<br/><NewLine>script_model = torch.jit.script(model)<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/<strong>init</strong>.py”, line 1261, in script<br/><NewLine>return torch.jit._recursive.create_script_module(obj, torch.jit._recursive.infer_methods_to_compile)<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 305, in create_script_module<br/><NewLine>return create_script_module_impl(nn_module, concrete_type, stubs_fn)<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 361, in create_script_module_impl<br/><NewLine>create_methods_from_stubs(concrete_type, stubs)<br/><NewLine>File “/opt/conda/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 279, in create_methods_from_stubs<br/><NewLine>concrete_type._create_methods(defs, rcbs, defaults)<br/><NewLine>RuntimeError:<br/><NewLine>rpc_async(dst_worker_name, user_callable, args, kwargs)does not support kwargs yet:<br/><NewLine>File “rnn_jit.py”, line 68<br/><NewLine># pass input to the remote embedding table and fetch emb tensor back<br/><NewLine># emb = _remote_method(EmbeddingTable.forward, self.emb_table_rref, input)<br/><NewLine>emb = rpc_async(self.emb_table_rref.owner(), EmbeddingTable.forward, args=input)<br/><NewLine>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;— HERE<br/><NewLine># output, hidden = self.rnn(emb, hidden)<br/><NewLine># pass output to the rremote decoder and get the decoded output back</p><NewLine><p>Line 68 refers to the rpc_async call inside the forward method<br/><NewLine>emb = rpc_async(self.emb_table_rref.owner(), EmbeddingTable.forward, args=input)</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>This script is a repro for the issue</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import os<NewLine>import torch.distributed as dist<NewLine>from torch.distributed.rpc import rpc_sync<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.distributed.rpc as rpc<NewLine>from torch import optim #XXX include this<NewLine>from torch.distributed.optim import DistributedOptimizer #XXX include this<NewLine>from torch.distributed.rpc import RRef, rpc_async, remote<NewLine>import torch.distributed.autograd as dist_autograd #XXX include this<NewLine><NewLine>def get_local_rank():<NewLine>    return int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])<NewLine><NewLine>def _parameter_rrefs(module):<NewLine>    param_rrefs = []<NewLine>    for param in module.parameters():<NewLine>        param_rrefs.append(RRef(param))<NewLine>    return param_rrefs<NewLine><NewLine>class EmbeddingTable(nn.Module):<NewLine>    r""""""<NewLine>    Encoding layers of the RNNModel<NewLine>    """"""<NewLine>    def __init__(self, ntoken, ninp, dropout):<NewLine>        super(EmbeddingTable, self).__init__()<NewLine>        self.drop = nn.Dropout(dropout)<NewLine>        self.encoder = nn.Embedding(ntoken, ninp).cuda()<NewLine>        self.encoder.weight.data.uniform_(-0.1, 0.1)<NewLine><NewLine>    def forward(self, input):<NewLine>        return self.drop(self.encoder(input.cuda())).cpu() # XXX: extra ')'<NewLine><NewLine><NewLine>class Decoder(nn.Module):<NewLine>    def __init__(self, ntoken, nhid, dropout):<NewLine>        super(Decoder, self).__init__()<NewLine>        self.drop = nn.Dropout(dropout)<NewLine>        self.decoder = nn.Linear(nhid, ntoken)<NewLine>        self.decoder.bias.data.zero_()<NewLine>        self.decoder.weight.data.uniform_(-0.1, 0.1)<NewLine><NewLine>    def forward(self, output):<NewLine>        return self.decoder(self.drop(output))<NewLine><NewLine>class RNNModel(nn.Module):<NewLine>    def __init__(self, ps, ntoken, ninp, nhid, nlayers, dropout=0.5):<NewLine>        super(RNNModel, self).__init__()<NewLine><NewLine>        # setup embedding table remotely<NewLine>        self.emb_table_rref = rpc.remote(ps, EmbeddingTable, args=(ntoken, ninp, dropout))<NewLine>        # setup LSTM locally<NewLine>        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)<NewLine>        # setup decoder remotely<NewLine>        self.decoder_rref = rpc.remote(ps, Decoder, args=(ntoken, nhid, dropout))<NewLine><NewLine>    def forward(self, input, hidden):<NewLine>        # pass input to the remote embedding table and fetch emb tensor back<NewLine>        # emb = _remote_method(EmbeddingTable.forward, self.emb_table_rref, input) # Original call<NewLine>        emb = rpc_async(self.emb_table_rref.owner(), EmbeddingTable.forward, args=input) # adapted call<NewLine><NewLine>        output, hidden = self.rnn(emb, hidden)<NewLine><NewLine>        # pass output to the rremote decoder and get the decoded output back<NewLine>        # decoded = _remote_method(Decoder.forward, self.decoder_rref, output) # Original call<NewLine>        decoded = rpc_async(self.decoder_rref.owner(), Decoder.forward, args=output) # adapted call<NewLine>        return decoded, hidden<NewLine>    def parameter_rrefs(self):<NewLine>        remote_params = []<NewLine>        # get RRefs of embedding table<NewLine>        # remote_params.extend(_remote_method(_parameter_rrefs, self.emb_table_rref)) # Original call<NewLine>        remote_params.extend(rpc_async(_parameter_rrefs, self.emb_table_rref))<NewLine><NewLine>        # create RRefs for local parameters<NewLine>        remote_params.extend(_parameter_rrefs(self.rnn))<NewLine><NewLine>        # get RRefs of decoder<NewLine>        # remote_params.extend(_remote_method(_parameter_rrefs, self.decoder_rref)) # Original call<NewLine>        remote_params.extend(rpc_async(_parameter_rrefs, self.decoder_rref)) # Adapted call<NewLine>        return remote_params<NewLine><NewLine>def _run_trainer():<NewLine>    ntoken = 10<NewLine>    ninp = 2<NewLine>    nhid = 3<NewLine>    nlayers = 4<NewLine>    model = RNNModel('ps', ntoken, ninp, nhid, nlayers) # XXX: no rnn.<NewLine>    script_model = torch.jit.script(model)<NewLine>    print(script_model)<NewLine><NewLine>def run_worker():<NewLine>    world_size=2<NewLine>    rank=get_local_rank()<NewLine>    os.environ['MASTER_ADDR'] = '10.123.134.28'<NewLine>    os.environ['MASTER_PORT'] = '21234'<NewLine>    if rank == 1:<NewLine>        rpc.init_rpc(""trainer"", rank=rank, world_size=world_size)<NewLine>        _run_trainer()<NewLine>    else:<NewLine>        rpc.init_rpc(""ps"", rank=rank, world_size=world_size)<NewLine>        # parameter server do nothing<NewLine>        pass<NewLine>    # block until all rpcs finish<NewLine>    rpc.shutdown()<NewLine><NewLine>if __name__==""__main__"":<NewLine>    run_worker()<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Thiago.Crepaldi; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Thiago.Crepaldi; <NewLine> ,"REPLY_DATE 1: May 13, 2020,  6:49pm; <NewLine> REPLY_DATE 2: May 13, 2020,  7:02pm; <NewLine> REPLY_DATE 3: May 13, 2020,  7:26pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
80685,Torch.jit._fork is not working and all operators runs sequentially,2020-05-11T10:58:01.168Z,3,100,"<div class=""post"" itemprop=""articleBody""><NewLine><p>i followed this link <a href=""https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html"" rel=""nofollow noopener"">cpu_threading_torchscript_inference</a> to try to enable inter op multi threading. In my test script, i called torch.set_num_interop_threads(8) to use all my cpu cores and runs 8 conv1d operators with torch.jit._fork. The result shows that torch.jit._fork is not working and all operators runs sequentially.</p><NewLine><p>Duration the test, cpu usage is almost 100%, which means that multi threading is not actually enabled.</p><NewLine><p>I got the warning “Access to a protected member _fork of a class” for torch.jit._fork, does this matter?</p><NewLine><p>Any answers are appreciate and thank you for your help!</p><NewLine><p>My core script</p><NewLine><pre><code class=""lang-auto"">    def forward(self, x, threads, seq):<NewLine>        iterval = seq // threads<NewLine>        conv_res = []<NewLine>        conv_threads = []<NewLine>        start = time.time() * 1000<NewLine>        for i in range(threads):<NewLine>            start_inner = time.time() * 1000<NewLine>            y = torch.jit._fork(self.compute, (x[:, :, i * iterval:(i + 1) * iterval]))<NewLine>            print(""fork %d cost %d ms"" % (i, time.time() * 1000 - start_inner))<NewLine>            conv_threads.append(y)<NewLine>        print(""fork totally cost %d ms"" % (time.time() * 1000 - start))<NewLine>        start = time.time() * 1000<NewLine>        for i in range(threads):<NewLine>            conv_res.append(torch.jit._wait(conv_threads[i]))<NewLine>        print(""wait cost %d ms"" % (time.time() * 1000 - start))<NewLine>        return conv_res<NewLine></code></pre><NewLine><p>Test result</p><NewLine><pre><code class=""lang-auto"">fork 0 cost 5 ms<NewLine>fork 1 cost 5 ms<NewLine>fork 2 cost 4 ms<NewLine>fork 3 cost 5 ms<NewLine>fork 4 cost 4 ms<NewLine>fork 5 cost 5 ms<NewLine>fork 6 cost 5 ms<NewLine>fork 7 cost 5 ms<NewLine>fork totally cost 41 ms<NewLine>wait cost 0 ms<NewLine></code></pre><NewLine><p>Full script</p><NewLine><pre><code class=""lang-auto"">import time<NewLine>import torch<NewLine>import threading<NewLine>import torch.nn as nn<NewLine>from torch.nn.utils import weight_norm<NewLine><NewLine><NewLine>class MyConvParallel(nn.Module):<NewLine>    def __init__(self, *args, **kwargs):<NewLine>        super().__init__()<NewLine>        self.cell = nn.Conv1d(*args, **kwargs)<NewLine>        self.cell.weight.data.normal_(0.0, 0.02)<NewLine><NewLine>    def compute(self, x):<NewLine>        return self.cell(x)<NewLine><NewLine>    def forward(self, x, threads, seq):<NewLine>        iterval = seq // threads<NewLine>        conv_res = []<NewLine>        conv_threads = []<NewLine>        start = time.time() * 1000<NewLine>        for i in range(threads):<NewLine>            start_inner = time.time() * 1000<NewLine>            y = torch.jit._fork(self.compute, (x[:, :, i * iterval:(i + 1) * iterval]))<NewLine>            print(""fork %d cost %d ms"" % (i, time.time() * 1000 - start_inner))<NewLine>            conv_threads.append(y)<NewLine>        print(""fork totally cost %d ms"" % (time.time() * 1000 - start))<NewLine>        start = time.time() * 1000<NewLine>        for i in range(threads):<NewLine>            conv_res.append(torch.jit._wait(conv_threads[i]))<NewLine>        print(""wait cost %d ms"" % (time.time() * 1000 - start))<NewLine>        return conv_res<NewLine><NewLine><NewLine>def main():<NewLine>    #print(*torch.__config__.show().split(""\n""), sep=""\n"");exit(0)<NewLine>    intro_threads = 1<NewLine>    inter_threads = 8<NewLine>    dim = 64<NewLine>    kernels = 3<NewLine>    seq = 80000<NewLine>    torch.set_num_threads(intro_threads)<NewLine>    torch.set_num_interop_threads(inter_threads)<NewLine>    MyCell = MyConvParallel(dim, dim, kernel_size=kernels, stride=1)<NewLine>    MyCell.eval()<NewLine>    inputs = []<NewLine>    iter = 1000<NewLine>    for i in range(iter):<NewLine>        inputs.append(torch.rand(1, dim, seq))<NewLine><NewLine>    start = time.time() * 1000<NewLine>    for i in range(iter):<NewLine>        print(i)<NewLine>        y = MyCell(inputs[i], inter_threads, seq)<NewLine>        #print(y)<NewLine>    end = time.time() * 1000<NewLine>    print('cost %d ms per iter\n' % ((end - start) / iter))<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    main()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/knitvoger,(knitvoger),knitvoger,"May 11, 2020, 10:58am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>That only works in TorchScript, try <code>MyCell=torch.jit.script(MyCell)</code></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks <a class=""mention"" href=""/u/googlebot"">@googlebot</a> ! Problem resolved.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/googlebot; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/knitvoger; <NewLine> ,"REPLY_DATE 1: May 12, 2020,  3:26am; <NewLine> REPLY_DATE 2: May 12, 2020,  3:46am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
80150,Accessing TorchScript module methods implemented in Python in C++,2020-05-07T10:01:27.236Z,0,105,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’ve written a model in Python, translated it to TorchScript with torch.jit.script(model), and serialized with torch.jit.save(). In a C++ program, I can load the model with torch::jit::load() and use the inference with model.forward().</p><NewLine><p>Is there a way of accessing other (than forward) methods from the model in the C++ domain?</p><NewLine><p>A direct model.my_method() call would require that the C++ compiler knows how to access this function. If the model is loaded during the execution, it may have functions of unknown name and the compiler (i.e., I) doesn’t know how to use them. It there a way to accomplish this, e.g., with something like model.call(‘my_method’, method_args)?</p><NewLine></div>",https://discuss.pytorch.org/u/paulus,,paulus,"May 7, 2020, 10:01am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can use <code>mymodule.get_method(""name_of_your_method"")(method_args);</code></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Excellent! My guess wasn’t so far, but somehow I didn’t manage to find the correct function.</p><NewLine><p>Many thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zdevito; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/paulus; <NewLine> ,"REPLY_DATE 1: May 8, 2020,  2:23pm; <NewLine> REPLY_DATE 2: May 8, 2020,  6:49am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> 
63209,Speed of Custom RNN is SUPER SLOW,2019-12-06T07:47:20.210Z,11,993,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Based on code here</p><NewLine><blockquote><NewLine><p><a href=""https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py</a></p><NewLine></blockquote><NewLine><p>I write an example to compare the cumputation capability of native lstm and custom lstm.<br/><NewLine>But I found that the speed of custom lstm is 100 times slower than native lstm class.</p><NewLine><p>here is my test code:</p><NewLine><pre><code class=""lang-auto"">import torch.nn as nn<NewLine>import time<NewLine>from models.custom_lstm import LSTMLayer, LSTMCell, script_lstm, LSTMState<NewLine><NewLine>input_size = 1024<NewLine>cell_size =2048<NewLine>batch_size =20 <NewLine>seq_len = 200 <NewLine><NewLine>native_lstm=nn.LSTM(input_size, cell_size,1).cuda()<NewLine>custom_lstm=script_lstm(input_size, cell_size,1).cuda()<NewLine>inp = torch.randn(seq_len,batch_size,input_size).cuda()<NewLine>hx = inp.new_zeros(batch_size, cell_size, requires_grad=False)<NewLine>cx = inp.new_zeros(batch_size, cell_size, requires_grad=False)<NewLine><NewLine>t1 = time.time()<NewLine>out, hid = native_lstm(inp)<NewLine>t2 = time.time()<NewLine>out2, hid2 = custom_lstm(inp, [(hx, cx)])<NewLine>t3 = time.time()<NewLine><NewLine>print ('lstm:{}\ncustom lstm:{}\n'.format(t2-t1, t3-t2))<NewLine></code></pre><NewLine><p>And here is the result:<br/><NewLine><img alt=""image"" data-base62-sha1=""webhu2ZTMHAw2zaJf77AFtkWdHB"" height=""54"" src=""https://discuss.pytorch.org/uploads/default/original/3X/e/1/e1dff809f5f6394ed9589a23c54f714bba13b0d3.png"" width=""359""/></p><NewLine><pre><code class=""lang-auto"">lstm:0.015676498413085938<NewLine>custom lstm:1.0338680744171143<NewLine></code></pre><NewLine><p>My torch version is 1.3.1, GPU is TITANV with Cuda10 and cuDNN7.4.1<br/><NewLine>I also tried on pytorch 1.1, GPU TITAN XP with CUDA9.1, which has same ratio of speed.</p><NewLine><p>Any idea?<br/><NewLine>Thanks so much</p><NewLine></div>",https://discuss.pytorch.org/u/senmao,(senmao),senmao,"December 6, 2019,  7:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The TorchScript runtime does some optimizations on the first pass (it assumes you will be running your compiled model’s inference many times), so this is likely why it looks much slower. Could you try running <code>custom_lstm</code> a couple times before you benchmark it and comparing?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I retest both class for 1000 times, and the result seems more reasonable.</p><NewLine><blockquote><NewLine><p>lstm:49.758071184158325<br/><NewLine>custom lstm:55.80940389633179</p><NewLine></blockquote><NewLine><p>Thanks for you answer.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I met the same problem. is there any way to disable the optimization or choose the optimization level or after optimization we can save the model.because when I load the torchscript model in C++, the first pass takes about 20s while the others’ infer time is about 0.5s.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think there’s a parameter called <code>optimize</code> for scripting in scripting using touchscript.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>in the source code ""<code>optimize</code> is deprecated and has no effect.""<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/docs/stable/_modules/torch/jit.html#script"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/stable/_modules/torch/jit.html#script</a></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>The code suggested an alternative: <code>warnings.warn(""`optimize` is deprecated and has no effect. Use `with torch.jit.optimized_execution() instead"")</code></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>you could try setting <code>torch._C._jit_set_profiling_mode()</code> to <code>True</code> and <code>torch._C._jit_set_profiling_mode</code> to <code>False</code><br/><NewLine>This mode was specifically added for speeding up compilation times for inference.<br/><NewLine>You could also indeed try <code>with torch.jit.optimized_execution()</code> if compilation times are still high for you. The latter runs even fewer optimizations.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>ok, thx. I will try this method</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>great, thanks. I will try to use these methods</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>when I use python the torch.jit.optimized_execution() could solve the problem, thanks<br/><NewLine>however, how should I solve this problem in C++?<br/><NewLine>thanks in andvanve</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>you could try this.</p><NewLine><pre><code class=""lang-auto"">#include &lt;torch/csrc/jit/update_graph_executor_opt.h&gt;<NewLine>//...<NewLine>setGraphExecutorOptimize(false);<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Great, thanks.I solved the problem by setGraphExecutorOptimize(false);</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I still have some questions about the custom RNN:</p><NewLine><ol><NewLine><li><NewLine><p>I am able to reproduce senmao’s results that lstm and custom lstm have similar performance in 1000 times, but this is partly due to the original lstm becomes worse. This can be seen in senmao’s results. The first run of the original lstm is 0.015. If the performance is consistent, 1000 runs would take 15 seconds instead of 49.758 as reported (I have verified this myself).</p><NewLine></li><NewLine><li><NewLine><p>Although I have no idea why the original lstm becomes worse, I get rid of the problem by changing the hyper parameters to:<br/><NewLine>input_size = 37<br/><NewLine>cell_size =256<br/><NewLine>batch_size =128<br/><NewLine>seq_len = 60<br/><NewLine>Now the original lstm becomes stable. In this case, the custom lstm is 10 times slower than the original lstm. Here are the results of 1000 runs: lstm:1.54s, custom lstm:19.75s. Can anyone please suggest how the custom lstm can be modified to have comparable performance with the original lstm?</p><NewLine></li><NewLine></ol><NewLine><p>Thanks so much!</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi senmao! I’m writting you because I tried to get your results by my own… but, I cound’t get them. I have the same as you posted and I’m getting the time by this code:</p><NewLine><p>for i in range(1000):</p><NewLine><p>t1 = time.time()</p><NewLine><p>out, hid = native_lstm(inp)</p><NewLine><p>t2 = time.time()</p><NewLine><p>t_native += t2-t1</p><NewLine><p>t3 = time.time()</p><NewLine><p>out2, hid2 = custom_lstm(inp, [(hx, cx)])</p><NewLine><p>t4 = time.time()</p><NewLine><p>t_custom += t4-t3</p><NewLine><p>Thanks for all!</p><NewLine><p>Regards,<br/><NewLine>David.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/senmao; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/huoge; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/G.M; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/huoge; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/G.M; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/villedepommes; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/huoge; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/huoge; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/huoge; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/villedepommes; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/huoge; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/KENG; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/David_Lorca; <NewLine> ,"REPLY_DATE 1: December 9, 2019,  5:39am; <NewLine> REPLY_DATE 2: December 9, 2019,  5:38am; <NewLine> REPLY_DATE 3: December 19, 2019,  3:07am; <NewLine> REPLY_DATE 4: January 2, 2020, 11:51am; <NewLine> REPLY_DATE 5: January 8, 2020, 12:37pm; <NewLine> REPLY_DATE 6: January 13, 2020,  1:08pm; <NewLine> REPLY_DATE 7: January 14, 2020,  6:22am; <NewLine> REPLY_DATE 8: January 15, 2020,  3:35am; <NewLine> REPLY_DATE 9: January 15, 2020,  3:37am; <NewLine> REPLY_DATE 10: February 24, 2020,  8:22am; <NewLine> REPLY_DATE 11: February 25, 2020,  6:20pm; <NewLine> REPLY_DATE 12: March 5, 2020,  8:32am; <NewLine> REPLY_DATE 13: March 25, 2020,  9:43am; <NewLine> REPLY_DATE 14: May 4, 2020,  4:21pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: 2 Likes; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> 
79674,TorchScript and eager mode mix during ONNX export,2020-05-04T12:27:01.491Z,0,91,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, everyone,</p><NewLine><p><a class=""hashtag"" href=""/c/jit/13"">#<span>jit</span></a> <a class=""hashtag"" href=""/c/quantization/17"">#<span>quantization</span></a></p><NewLine><p>I am struggling with exporting quantized model from PyTorch to Caffe2. For now it is clear that previously quantized model should be traced then exported via ONNX to Caffe2. But what if it is impossible (for some reason) to trace some part of the network which might be non-quantized? Could we possibly mix TorchScript module and eager mode modules in ONNX export?</p><NewLine><p>Also I have other question but it is not related to the topic name. I’ve also tried to convert whole traced model but ONNX throws an errors, that I unable to understand. The steps are the following:</p><NewLine><ol><NewLine><li>I’ve traced quantized PyTorch model</li><NewLine><li>Called <code>torch.onnx.export</code> with <code>opset_version=11, operator_export_type=OperatorExportTypes.ONNX_ATEN_FALLBACK</code><NewLine></li><NewLine></ol><NewLine><p>That produces an error:</p><NewLine><pre><code class=""lang-auto"">  File ""./tools/torchscript_converter.py"", line 118, in &lt;module&gt;<NewLine>    onnx_model = export_onnx_model_with_torchscript(cfg, torch_model, first_batch)<NewLine>  File ""/root/some_detectron2/detectron2/export/api.py"", line 180, in export_onnx_model_with_torchscript<NewLine>    return Caffe2Tracer(cfg, model, inputs).export_onnx_with_torchscript()<NewLine>  File ""/root/some_detectron2/detectron2/export/api.py"", line 138, in export_onnx_with_torchscript<NewLine>    return export_onnx_model_impl(traced_model, (inputs,))<NewLine>  File ""/root/some_detectron2/detectron2/export/caffe2_export.py"", line 67, in export_onnx_model<NewLine>    export_params=True,<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 172, in export<NewLine>    custom_opsets, enable_onnx_checker, use_external_data_format)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 92, in export<NewLine>    use_external_data_format=use_external_data_format)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 552, in _export<NewLine>    _check_onnx_proto(proto)<NewLine>RuntimeError: Attribute 'kernel_shape' is expected to have field 'ints'<NewLine><NewLine>==&gt; Context: Bad node spec: input: ""441"" input: ""7"" output: ""442"" op_type: ""Conv"" attribute { name: ""dilations"" ints: 1 ints: 1 type: INTS } attribute { name: ""group"" i: 32 type: INT } attribute { name: ""<NewLine>kernel_shape"" type: INTS } attribute { name: ""pads"" ints: 1 ints: 1 ints: 1 ints: 1 type: INTS } attribute { name: ""strides"" ints: 1 ints: 1 type: INTS } <NewLine></code></pre><NewLine><p>And the debug logs there is:</p><NewLine><pre><code class=""lang-auto"">...<NewLine>  %442 : Tensor = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=annotate(List[int], []), pads=[1, 1, 1, 1], strides=[1, 1]](%441, %7) # /root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/$<NewLine>orch/nn/modules/conv.py:348:0<NewLine>  %443 : Tensor = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002](%442, %8, %9, %10, %11) # /root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/fu$<NewLine>ctional.py:1957:0<NewLine>  %444 : Tensor = onnx::Relu(%443) # /root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/functional.py:1061:0<NewLine>...<NewLine></code></pre><NewLine><p>If it is impossible to tell what’s wrong, could you please guide me how to localize an issue?</p><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"May 4, 2020, 12:27pm",,,,,
78959,[JIT] Tracing quantized batchnorm issue,2020-04-29T12:28:46.274Z,2,281,"<div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""hashtag"" href=""/c/jit/13"">#<span>jit</span></a> <a class=""hashtag"" href=""/c/quantization/17"">#<span>quantization</span></a> <a class=""hashtag"" href=""/c/mobile/18"">#<span>mobile</span></a></p><NewLine><p>Hello everyone,</p><NewLine><p>After I was <a href=""https://discuss.pytorch.org/t/onnx-export-of-quantized-model/76884"">guided</a> how to deploy quantized models on mobile I’ve decided to give a try to quantized TorchScript model.</p><NewLine><p>What I have is EfficientNet backbone that was quantized with QAT tools and qnnpack config. After quantization I’ve traced it with <code>torch.jit.script</code> and saved for later deploy. Also I have traced model without quantization</p><NewLine><p>I’ve tried both on mobile.<br/><NewLine>In the second case everything works well. But in the first case I’m obtaining errors:</p><NewLine><pre><code class=""lang-auto"">HWHMA:/data/local/Detectron2Mobile # ./speed_benchmark_torch --model=./ts_models/orig_backbone.pt --input_dims=""1,3,512,768"" --input_type=float --warmup=10 --iter=10                                      <NewLine>Starting benchmark.<NewLine>Running warmup runs.<NewLine>Main runs.<NewLine>Main run finished. Milliseconds per iter: 2146.44. Iters per second: 0.465887<NewLine>/speed_benchmark_torch --model=./ts_models/quant_backbone.pt --input_dims=""1,3,512,768"" --input_type=float --warmup=10 --iter=10                                                                          &lt;<NewLine>terminating with uncaught exception of type torch::jit::ErrorReport: <NewLine>Unknown builtin op: quantized::batch_norm.<NewLine>Here are some suggestions: <NewLine>	quantized::batch_norm2d<NewLine>	quantized::batch_norm3d<NewLine><NewLine>The original call is:<NewLine>...&lt;calls&gt;...<NewLine>Serialized   File ""code/__torch__/torch/nn/quantized/modules/batchnorm.py"", line 14<NewLine>    _1 = self.running_mean<NewLine>    _2 = self.bias<NewLine>    input = ops.quantized.batch_norm(argument_1, self.weight, _2, _1, _0, 1.0000000000000001e-05, 0.44537684321403503, 129)<NewLine>            ~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>    return input<NewLine><NewLine>Aborted<NewLine></code></pre><NewLine><p>There’s no op <code>quantized::batch_norm</code> but there are <code>quantized::batch_norm2d/3d</code>. In the working case traced code looks the following way:</p><NewLine><pre><code class=""lang-auto"">def forward(self,<NewLine>    argument_1: Tensor) -&gt; Tensor:<NewLine>  _0 = self.running_var<NewLine>  _1 = self.running_mean<NewLine>  _2 = self.bias<NewLine>  input = torch.batch_norm(argument_1, self.weight, _2, _1, _0, False, 0.10000000000000001, 1.0000000000000001e-05, True)<NewLine>  return input<NewLine></code></pre><NewLine><p>What is happening in the not working case? Is it wrong substitution while tracing or there’s an issue with <code>quantized::batch_norm</code>?</p><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"April 29, 2020, 12:28pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>UPD:</p><NewLine><p>I’ve tried to convert traced graph to Caffe2. And have run into same problem.</p><NewLine><p>Traced with <code>torch.jit.script</code> model cannot be converted to Caffe2 because of  <code>batchnorm</code>.</p><NewLine><pre><code class=""lang-auto"">model = torch.jit.load(buf)<NewLine>f = io.BytesIO()<NewLine>torch.onnx.export(model, x, f, example_outputs=outputs,<NewLine>                  operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)<NewLine>f.seek(0)<NewLine></code></pre><NewLine><p>Where the model is traced backbone</p><NewLine><pre><code class=""lang-auto"">~/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py in _optimize_graph(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict)<NewLine>    157             torch.onnx.symbolic_helper._quantized_ops.clear()<NewLine>    158             # Unpack quantized weights for conv and linear ops and insert into graph.<NewLine>--&gt; 159             torch._C._jit_pass_onnx_unpack_quantized_weights(graph, params_dict)<NewLine>    160 <NewLine>    161             # Insert permutes before and after each conv op to ensure correct order.<NewLine><NewLine>RuntimeError: false INTERNAL ASSERT FAILED at /opt/conda/conda-bld/pytorch_1586761698468/work/torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp:99, please report a bug to PyTorch. Unrecognized quantized operator while trying to compute q_scale for operator quantized::batch_norm<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>For the first question - are you using nightly build of pytorch? The op name was updated in <a href=""https://github.com/pytorch/pytorch/pull/36494"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/36494</a> to quantized.batch_norm2d to make it more consistent with the implementation. You might have to re-do the QAT convert with the same pytorch build to make sure you get the same op name.</p><NewLine><p>For the second question - We currently do not have the quantized pytorch to caffe2 conversion flow working for the quantized::batch_norm2d operator. Mainly due to the fact that caffe2 quantized ops currently don’t have this operator.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/supriyar"">@supriyar</a> thank you!</p><NewLine><p>There was a version right below that PR. As I can see now <code>quantized::batchnorm2d</code> appeared and the model works on mobile device.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/supriyar"">@supriyar</a> would you mind to discuss how to convert models for now. Is it possible to exclude <code>bn</code> operators and and reach same functionality only with <code>int8_conv_op_relu</code>?</p><NewLine><p>As it was done in this example model <a href=""https://github.com/caffe2/models/tree/master/resnet50_quantized"" rel=""nofollow noopener"">https://github.com/caffe2/models/tree/master/resnet50_quantized</a></p><NewLine><p><strong>UPD:</strong> I think the answer is to use fused modules from <code>intrinsic</code></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/zetyquickly; <NewLine> ,"REPLY_DATE 1: April 29, 2020,  3:32pm; <NewLine> REPLY_DATE 2: April 29, 2020,  6:55pm; <NewLine> REPLY_DATE 3: April 29, 2020,  6:56pm; <NewLine> REPLY_DATE 4: April 30, 2020,  7:16pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
79158,Converting intermediate variable to ONNX graph input,2020-04-30T18:26:22.407Z,0,73,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!</p><NewLine><p>I’m currently trying to export a model to ONNX from PyTorch. I have some layers inside a model that are not supported by the ONNX and I don’t really want them to be inside ONNX graph anyway, so I’m trying to use <code>keep_initializers_as_inputs=True</code> in order to fool tracer that my variables are actually inputs and inside custom nn.Module implementation’s <code>forward</code>, I’ve added this:</p><NewLine><pre><code class=""lang-auto"">        if onnx.is_in_onnx_export():<NewLine>            gamma = torch.zeros([int(x.shape[2]), int(x.shape[3])]).clone().detach().requires_grad_(True)<NewLine>            beta = torch.zeros([int(x.shape[2]), int(x.shape[3])]).clone().detach().requires_grad_(True)<NewLine>        else:<NewLine></code></pre><NewLine><p>but those initializers aren’t getting traced as graph inputs, but rather as constants and then being baked into a graph. I’ve tried various approach but all of them failed. Can somebody hint on how can one convert intermediate variables to graph inputs?</p><NewLine></div>",https://discuss.pytorch.org/u/s1ddok,(Andrey Volodin),s1ddok,"April 30, 2020,  6:26pm",,,,,
78059,Change the input name of traced module,2020-04-23T14:11:32.933Z,3,80,"<div class=""post"" itemprop=""articleBody""><NewLine><p>is there any ways for me to change the name of input variables of traced module? I have looked into the documentation of <a href=""https://pytorch.org/docs/stable/jit.html#torch.jit.trace"" rel=""nofollow noopener""><code>torch.jit.trace</code></a> and it could not be changed beforehand as well, e.g. with dictionary.<br/><NewLine>basically I’m trying to mimic the behavior of <code>input_names</code> argument in <a href=""https://pytorch.org/docs/stable/onnx.html#torch.onnx.export"" rel=""nofollow noopener""><code>torch.onnx.export</code></a></p><NewLine></div>",https://discuss.pytorch.org/u/triwahyuu,(Tri Wahyu Utomo),triwahyuu,"April 23, 2020,  2:17pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t believe we expose a way to do this. Names are not semantically meaningful in a JIT graph anyway, they are only for human readability</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks for the reply.<br/><NewLine>what I want to do is so that I could use keyword arguments as an input to forward.<br/><NewLine>in my traced module, there are 3 arguments, the name of the first two arguments have already been properly inferred by tracing, but the last one is not.<br/><NewLine>my question now becomes how does the tracer infer the argument name for this?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The tracer looks in the python interpreter frame state’s local variables (<code>f_local</code>) for names (see <a href=""https://github.com/pytorch/pytorch/blob/master/torch/jit/__init__.py#L299"" rel=""nofollow noopener"">here</a>. So if, for a given traced value, it finds a corresponding local, it will add that as the name in the graph.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you, I will look into that</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/triwahyuu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/triwahyuu; <NewLine> ,"REPLY_DATE 1: April 23, 2020,  6:29pm; <NewLine> REPLY_DATE 2: April 24, 2020,  8:37am; <NewLine> REPLY_DATE 3: April 29, 2020,  5:31pm; <NewLine> REPLY_DATE 4: April 30, 2020,  3:25am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
78858,Running cpp torch::jit::Graph,2020-04-28T17:53:59.371Z,1,143,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, I’m quite stuck on trying to run torch::jit::Graph with cpp API, I have already tried to search for docs or implementations but failed.<br/><NewLine>my main purpose to do so is to divide a JIT Graph into subgraphs (I would like to control which nodes go to each subgraph). My plan is to parse a graph into nodes and then create new graphs from each sequence of nodes (with my own decision) and then run each graph one by one.</p><NewLine><p>this is what I have already tried:</p><NewLine><pre><code class=""lang-cpp"">#include &lt;torch/script.h&gt;<NewLine>#include &lt;iostream&gt;<NewLine><NewLine>int main() {<NewLine>    torch::jit::script::Module my_model = torch::jit::load(""&lt;MY_PATH&gt;/model.pkl"");<NewLine><NewLine>    torch::jit::script::Method m = my_model.get_method(""forward"");<NewLine>    auto g = m.graph();<NewLine><NewLine>    auto cu = std::make_shared&lt;torch::jit::script::CompilationUnit&gt;();<NewLine>    c10::QualifiedName name(""forward"");<NewLine>    torch::Function *fn = cu-&gt;create_function(std::move(name), g);<NewLine><NewLine>    std::vector&lt;torch::jit::IValue&gt; inputs;<NewLine>    inputs.push_back(torch::rand({1, 1, 3, 3}));<NewLine><NewLine>    auto output = fn-&gt;operator()(inputs);<NewLine>    std::cout &lt;&lt; output &lt;&lt; std::endl;<NewLine>}<NewLine></code></pre><NewLine><p>based on <code>model.pkl</code> that was produced by the python code below:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine>class Net(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Net, self).__init__()<NewLine>        self.conv = nn.Conv2d(1, 1, 3)<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.conv(x)<NewLine><NewLine>scripted_foo = torch.jit.script(Net())<NewLine>scripted_foo.save('model.pkl')<NewLine></code></pre><NewLine><p>this cpp approch produces the following error:</p><NewLine><pre><code class=""lang-auto"">libc++abi.dylib: terminating with uncaught exception of type c10::Error: forward() Expected a value of type '__torch__.Net' for argument 'self' but instead found type 'Tensor'.<NewLine>Position: 0<NewLine>Declaration: forward(__torch__.Net self, Tensor x) -&gt; (Tensor) (checkArg at ../aten/src/ATen/core/function_schema_inl.h:194)<NewLine>frame #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) + 135 (0x1038ce787 in libc10.dylib)<NewLine>frame #1: c10::FunctionSchema::checkArg(c10::IValue const&amp;, c10::Argument const&amp;, c10::optional&lt;unsigned long&gt;) const + 719 (0x11285ee6f in libtorch.dylib)<NewLine>frame #2: c10::FunctionSchema::checkAndNormalizeInputs(std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;&amp;, std::__1::unordered_map&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, c10::IValue, std::__1::hash&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt;, std::__1::equal_to&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt;, std::__1::allocator&lt;std::__1::pair&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const, c10::IValue&gt; &gt; &gt; const&amp;) const + 228 (0x11285ddf4 in libtorch.dylib)<NewLine>frame #3: torch::jit::Function::operator()(std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;, std::__1::unordered_map&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, c10::IValue, std::__1::hash&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt;, std::__1::equal_to&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt;, std::__1::allocator&lt;std::__1::pair&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const, c10::IValue&gt; &gt; &gt; const&amp;) + 49 (0x11285d221 in libtorch.dylib)<NewLine>frame #4: main + 686 (0x1037eb59e in deep_project)<NewLine>frame #5: start + 1 (0x7fff700f0cc9 in libdyld.dylib)<NewLine>frame #6: 0x0 + 1 (0x1 in ???)<NewLine><NewLine>Process finished with exit code 6<NewLine></code></pre><NewLine><p>I assume that I need to push <code>self</code> of type <code>__torch__.Net</code> somehow but I dont know how, I will be happy for some help.<br/><NewLine>thanks a lot.</p><NewLine></div>",https://discuss.pytorch.org/u/Omer_B,(Omer B),Omer_B,"April 29, 2020,  8:30am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you post this under ‘jit’ category?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>For methods like, as in Python, the first argument to the graph is <code>self</code>, which represents the module object instance. The <code>Module</code> API takes care of adding <code>self</code> to the stack, see <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/api/module.cpp#L115"" rel=""nofollow noopener"">here</a>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks a lot, it works - I have another question though, do you know what is the most clean way to create a graph from a couple of nodes ?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Probably using the base Graph API for node creation and insertion is the cleanest. For an example of how to take “foreign” <code>Node*</code>s not owned by a graph and copy them into the graph, the <code>Graph::copy</code> method is a useful example: <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/ir/ir.cpp#L680"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/ir/ir.cpp#L680</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/glaringlee; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Omer_B; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: April 29, 2020,  5:52am; <NewLine> REPLY_DATE 2: April 29, 2020,  5:40pm; <NewLine> REPLY_DATE 3: April 29, 2020,  5:41pm; <NewLine> REPLY_DATE 4: April 29, 2020,  6:21pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 2 Likes; <NewLine> 
78376,TorchScript and ModuleList Type,2020-04-25T13:06:42.870Z,0,165,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to script a PyTorch model.  I am running into the following issues;</p><NewLine><p>My original code utilizes subscripts as follows:</p><NewLine><p>for i in range(self.n_layers):</p><NewLine><pre><code>            self.in_layers[i](a),<NewLine>            self.cond_layers[I](b) ...<NewLine></code></pre><NewLine><p>Both in_layers and cond_layers are of type torch.nn.ModuleList().</p><NewLine><p>When trying to compile I receive an error that these types are not subscriptable.  To work around this issue, I created a new function that receives a torch.nn.ModuleList and iterates over each item to find the right one. I  am trying to use MyPy annotations to get this to compile as follows:</p><NewLine><p>def moduleListChoice(self, ml, choice):<br/><NewLine># type: (torch.nn.ModuleList, int)<br/><NewLine>moduleNumber = 0<br/><NewLine>for module in ml:<br/><NewLine>if moduleNumber == choice:<br/><NewLine>return module<br/><NewLine>moduleNumber = moduleNumber + 1<br/><NewLine>print(“Module not found”)</p><NewLine><p>The problem is that I can’t figure out what the type should be for the first argument.  torch.nn.ModuleList doesn’t work.  Any thoughts on how get this to work?</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Jon2718,,Jon2718,"April 25, 2020,  1:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://github.com/pytorch/pytorch/pull/29236"">This PR</a> should have added the support for indexing into <code>nn.ModuleList</code>. Which PyTorch version are you using? If you are using an older one (pre <code>1.5</code>), could you update to the latest stable version and rerun your script, please?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi.  Thanks.  I will upgrade to 1.5 and report back.  I’m on 1.4 currently.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Jon2718; <NewLine> ,"REPLY_DATE 1: April 26, 2020,  6:50am; <NewLine> REPLY_DATE 2: April 26, 2020,  3:54pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
78446,ModuleList Object Not Subscriptable,2020-04-26T03:20:24.939Z,2,115,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to script some PyTorch code that utilizes ModuleList.  The code makes use of subscripting of a ModuleList object as follows:</p><NewLine><p>or i in range(self.n_layers):<br/><NewLine>acts = fused_add_tanh_sigmoid_multiply(<br/><NewLine>self.in_layers<a>i</a>,<br/><NewLine>self.cond_layers<a>i</a>,<br/><NewLine>torch.IntTensor([self.n_channels]))</p><NewLine><p>I am met with the following error:</p><NewLine><p>RuntimeError:<br/><NewLine>‘module’ object is not subscriptable:<br/><NewLine>at /workspace/tacotron2/waveglow/model.py:147:16<br/><NewLine>for i in range(self.n_layers):<br/><NewLine>acts = fused_add_tanh_sigmoid_multiply(<br/><NewLine>self.in_layers<a>i</a>,<br/><NewLine>~~~~~~~~~~~~~~~~ &lt;— HERE<br/><NewLine>self.cond_layers<a>i</a>,<br/><NewLine>torch.IntTensor([self.n_channels]))</p><NewLine><p>Any help would be greatly appreciated.</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Jon2718,,Jon2718,"April 26, 2020,  3:20am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is this a double post from <a href=""https://discuss.pytorch.org/t/torchscript-and-modulelist-type/78376/2"">here</a> or are these errors differently in some sense?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks.  I will update to 1.5 and try again.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Jon2718; <NewLine> ,"REPLY_DATE 1: April 26, 2020,  8:04am; <NewLine> REPLY_DATE 2: April 26, 2020,  3:53pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
78154,Problem in converting GRU to jit,2020-04-24T04:26:28.153Z,0,92,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have trained  a model containing GRU. When I try to convert it to jit, I found that the outputs of the original model and jit model are different. This problem can be reconstructed by the code below. It’s very strange that other operation(like fc/conv) is ok, the output of print(model(y)-jit_model(y)) is an zero matrix whhile GRU is not.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch import nn<NewLine><NewLine>class gruModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(gruModel, self).__init__()<NewLine>        self.biGRU = nn.GRU(256*5, 100,  num_layers=1, bidirectional=True, batch_first=True, bias=True)<NewLine>        # self.fc = nn.Linear(256*5, 200)<NewLine><NewLine>    def forward(self, x):<NewLine>        # x = self.fc(x)<NewLine>        x, _ = self.biGRU(x, torch.zeros(2, x.size(0), 100, device=x.device))<NewLine>        return x<NewLine><NewLine>if __name__ == '__main__':<NewLine>    y = torch.rand([1, 256, 1280]).cuda()<NewLine>    model = gruModel()<NewLine>    model = torch.nn.DataParallel(model).to(torch.device('cuda'))<NewLine>    model.eval()<NewLine><NewLine>    traced_script = torch.jit.trace(model.module, y)<NewLine>    traced_script.eval()<NewLine>    traced_script.save(""gru_jit.pt"")<NewLine>    jit_model = torch.jit.load(""gru_jit.pt"")<NewLine><NewLine>    print(model(y)-jit_model(y))<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/lzj9072,,lzj9072,"April 24, 2020,  4:26am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have found the solution…<br/><NewLine>if the model contains GRU layer, you should load the model in cpu then to the gpu.</p><NewLine><pre><code class=""lang-auto"">model = torch.jit.load(model_path, map_location=torch.device(""cpu""))<NewLine>model = model.cuda()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/lzj9072"">@lzj9072</a> glad you found the solution to your problem.  There’s an issue out for this problem here you can follow: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/problem-in-converting-gru-to-jit/78154"">Problem in converting GRU to jit</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/lzj9072; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/eellison; <NewLine> ,"REPLY_DATE 1: April 26, 2020,  6:59am; <NewLine> REPLY_DATE 2: April 24, 2020,  4:15pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
77929,JIT Compiling Extensions,2020-04-22T21:38:47.543Z,1,101,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,</p><NewLine><p>I’m trying to use the deformable convolutions cpp extensions from the <a href=""https://github.com/open-mmlab/mmdetection"" rel=""nofollow noopener"">mmdetection repo</a> without a <code>setup.py</code> but by compiling them just in time with <code>torch.utils.cpp_extension.load()</code> as suggested <a href=""https://pytorch.org/tutorials/advanced/cpp_extension.html"" rel=""nofollow noopener"">here</a>. However, I’m having some trouble giving the <code>load()</code> function the correct path.</p><NewLine><p>My folder structure is as follows:</p><NewLine><pre><code class=""lang-auto"">├── dcn<NewLine>│   ├── deform_conv.py<NewLine>│   ├── deform_pool.py<NewLine>│   ├── __init__.py<NewLine>│   └── src<NewLine>│       ├── deform_conv_cuda.cpp<NewLine>│       ├── deform_conv_cuda_kernel.cu<NewLine>│       ├── deform_pool_cuda.cpp<NewLine>│       └── deform_pool_cuda_kernel.cu<NewLine>└── test_dcn.py<NewLine></code></pre><NewLine><p>In <code>test_dcn.py</code> I import the deformable convolutions with <code>from dcn import DeformConvPack</code> and in the file <code>deform_conv.py</code> I inserted the following at the top:</p><NewLine><pre><code class=""lang-auto""># deform_conv.py<NewLine><NewLine>from torch.utils.cpp_extension import load<NewLine>deform_conv_cuda = load(name='deform_conv_cuda', sources=['src/deform_conv_cuda.cpp', 'src/deform_conv_cuda_kernel.cu'])<NewLine><NewLine># Rest of the code<NewLine># ...<NewLine></code></pre><NewLine><p>I expected this to work but the compilation fails because the <code>.cpp</code> and <code>.cu</code> files cannot be found. If I instead specify the sources as <code>'dcn/src/deform_conv_cuda.cpp'</code> and <code>'dcn/src/deform_conv_cuda_kernel.cu'</code> it works.</p><NewLine><p>Could someone explain me the logic behind this? Thank you very much =)</p><NewLine></div>",https://discuss.pytorch.org/u/MauroPfister,(Mauro Pfister),MauroPfister,"April 23, 2020, 11:19am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Basically, a working directory of process and python’s path used with module import are distinct things; as compilation is about creating child processes, and module with load() lives in entirely different directory, the former directory is used as base.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the answer! In the mean time I came to the same conclusion and used the following snippet as a workaround. That way I can import the <code>nn.Module</code> defined in <code>deform_conv.py</code> from anywhere I want without having to worry about getting the path right. I’m just a bit surprised, that the <code>torch.utils.cpp_extension.load()</code> function doesn’t take care of that.</p><NewLine><pre><code class=""lang-auto""># deform_conv.py<NewLine><NewLine>import os<NewLine>from torch.utils.cpp_extension import load<NewLine>parent_dir = os.path.dirname(os.path.abspath(__file__))<NewLine>sources = ['src/deform_conv_cuda.cpp', 'src/deform_conv_cuda_kernel.cu']  # Paths of sources relative to this file<NewLine>abs_sources = [os.path.join(parent_dir, source) for source in sources]  # Absolute paths of sources<NewLine>deform_conv_cuda = load(name='deform_conv_cuda', sources=abs_sources)  # JIT compilation of extensions<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/googlebot; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/MauroPfister; <NewLine> ,"REPLY_DATE 1: April 23, 2020,  2:45pm; <NewLine> REPLY_DATE 2: April 23, 2020,  3:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
77397,Problems of image segmentation using LibTorch,2020-04-19T07:05:04.137Z,5,226,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, guys</p><NewLine><p>I trained a image segmentation model and want to do inference using libtorch. The model predict a mask with a input image.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/bc0ad41b9cfecefd40e04a24114a93b8322afb17"" href=""https://discuss.pytorch.org/uploads/default/original/3X/b/c/bc0ad41b9cfecefd40e04a24114a93b8322afb17.png"" title=""捕获1""><img alt=""捕获1"" data-base62-sha1=""qPuXNwS3lyTIMt7PbZ7xyG0rUq3"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/c/bc0ad41b9cfecefd40e04a24114a93b8322afb17_2_10x10.png"" height=""230"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/c/bc0ad41b9cfecefd40e04a24114a93b8322afb17_2_690x230.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/c/bc0ad41b9cfecefd40e04a24114a93b8322afb17_2_690x230.png, https://discuss.pytorch.org/uploads/default/original/3X/b/c/bc0ad41b9cfecefd40e04a24114a93b8322afb17.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/b/c/bc0ad41b9cfecefd40e04a24114a93b8322afb17.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">捕获1</span><span class=""informations"">794×265 45.4 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine>I traced the model for later C++ usage.</p><NewLine><pre><code class=""lang-auto"">#define CV_8UC3 CV_MAKETYPE(CV_8U,3)<NewLine>#include &lt;torch/script.h&gt; // One-stop header.<NewLine>#include &lt;iostream&gt;<NewLine>#include &lt;memory&gt;<NewLine>#include&lt;opencv2/opencv.hpp&gt;<NewLine>#include &lt;opencv2/core/core.hpp&gt;<NewLine>#include &lt;opencv2/imgproc/imgproc.hpp&gt;<NewLine>#include &lt;opencv2/highgui/highgui.hpp&gt;<NewLine>#include &lt;opencv2/imgproc/types_c.h&gt;<NewLine>#include &lt;cuda.h&gt;<NewLine>#include &lt;cuda_runtime.h&gt;<NewLine><NewLine>using namespace cv;<NewLine>using namespace std;<NewLine><NewLine>int main() {<NewLine>    std::string model_path = ""D:/project/WDD/model_cpu.pt"";<NewLine>    std::string image_path = ""D:/data/glass_crack/converted/84/img.png"";<NewLine><NewLine>    torch::jit::script::Module model = torch::jit::load(model_path);<NewLine>    assert(module != nullptr);<NewLine>    std::cout &lt;&lt; ""load model sucessfully.\n"";<NewLine><NewLine>    //load img and normalize<NewLine>    Mat img = imread(image_path, 1); <NewLine>    cv::cvtColor(img, img, CV_BGR2RGB);<NewLine> <NewLine>    if (img.empty())<NewLine>    {<NewLine>        printf(""could not show image..."");<NewLine>        return -1;<NewLine>    }<NewLine>    <NewLine>    cv::Mat img_float;<NewLine>    img.convertTo(img_float, CV_32FC3, 1.0f / 255.0f); <NewLine><NewLine>    auto tensor_image = torch::from_blob(img_float.data, { 1, img.cols, img.rows, 3 });<NewLine>    tensor_image = tensor_image.permute({ 0, 3, 1, 2 });<NewLine>    //normalize<NewLine>    tensor_image[0][0] = tensor_image[0][0].sub(0.485).div(0.229); <NewLine>    tensor_image[0][1] = tensor_image[0][1].sub(0.456).div(0.224);<NewLine>    tensor_image[0][2] = tensor_image[0][2].sub(0.406).div(0.225);<NewLine><NewLine>    std::vector&lt;torch::jit::IValue&gt; inputs;<NewLine>    inputs.emplace_back(tensor_image);<NewLine><NewLine>    // Execute the model and turn its output into a tensor.<NewLine>    at::Tensor out_tensor = model.forward(inputs).toTensor();<NewLine><NewLine>    // convert result to CV mat and save<NewLine>    out_tensor = out_tensor.squeeze().detach(); <NewLine>    out_tensor = out_tensor.mul(255).clamp(0, 255).to(torch::kU8);<NewLine>    std::cout &lt;&lt; out_tensor.sizes() &lt;&lt; '\n';<NewLine>    cv::Mat resultImg(img.rows, img.cols, CV_8UC3);<NewLine>    std::memcpy((void*)resultImg.data, out_tensor.data_ptr(), sizeof(torch::kU8) * out_tensor.numel());<NewLine>    imwrite(""landscape_output.jpg"", resultImg);<NewLine>    std::cout &lt;&lt; ""Done!\n"";<NewLine>    while (1);<NewLine>}<NewLine></code></pre><NewLine><p>it loaded model and did forward sucessfully, however, the problem is the output. The output:<br/><NewLine><img alt=""捕获"" data-base62-sha1=""fUhZV2B5QLon8lfCLwzwMnI9tYW"" height=""316"" src=""https://discuss.pytorch.org/uploads/default/original/3X/6/f/6f7d8dfa4772e855859e3ab6048b492850d523ce.png"" width=""467""/><br/><NewLine>the output is not as expected, totally nonsense…</p><NewLine><p>can someone help?</p><NewLine></div>",https://discuss.pytorch.org/u/hktxt,(Max),hktxt,"April 19, 2020,  7:13am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you try to permute the dimensions again to <code>NHWC</code> before converting the output to an OpenCV array?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks for reply~<br/><NewLine>the output mask has a shape of [N, C, H, W], here N =1, C=1.<br/><NewLine>after</p><NewLine><pre><code class=""lang-auto"">out_tensor = out_tensor.squeeze().detach(); <NewLine></code></pre><NewLine><p>out_tensor’s shape: [H, W].</p><NewLine><p>As you recommend:</p><NewLine><pre><code class=""lang-auto"">ut_tensor = out_tensor.squeeze(0).detach().permute({ 1,2,0 });<NewLine></code></pre><NewLine><p>I use this line of code instead.  so the shape become: [H, W, 1]. the mask:<br/><NewLine><img alt=""landscape_output"" data-base62-sha1=""6iX7swSkaHWSRfZZCVp0A17Zxrv"" height=""450"" src=""https://discuss.pytorch.org/uploads/default/original/3X/2/c/2c3186f94c36aa81013bcfcfbfca341445953795.jpeg"" width=""650""/><br/><NewLine>not working…</p><NewLine><p>I find that C = 1, so I changed</p><NewLine><pre><code class=""lang-auto"">cv::Mat resultImg(img.rows, img.cols, CV_8UC3);<NewLine></code></pre><NewLine><p>to</p><NewLine><pre><code class=""lang-auto"">cv::Mat resultImg(img.rows, img.cols, CV_8UC1);<NewLine></code></pre><NewLine><p>then the predicted mask:<br/><NewLine><img alt=""landscape_output"" data-base62-sha1=""bNVPRZtNfBXYuyNFidNLocQQv1R"" height=""450"" src=""https://discuss.pytorch.org/uploads/default/original/3X/5/2/52bce28958981b2964e035b59b5e38da0463e38f.jpeg"" width=""650""/></p><NewLine><p>…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the update!<br/><NewLine>The current output still looks as if the copying of the data is running into a row/columns mismatch.<br/><NewLine>You see these diagonal “edges”, which would correspond to the desired width, but are shifted in each row.<br/><NewLine>I would recommend to check the shape of the output, the created OpenCV image and make sure that the output tensor is contiguous in memory.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>problem solved. thanks for help.</p><NewLine><p>Yes, the row/col problem. the seg model should outputs a mask which has the same shape of input image. However, a input image with 450x650 has a predicted mask of shape 448x648. python predicts the same shape but libtorch does not.<br/><NewLine>so I modified:</p><NewLine><pre><code class=""lang-auto"">cv::Mat resultImg(out_tensor.sizes()[0], out_tensor.sizes()[1], CV_8UC1);<NewLine></code></pre><NewLine><p>problem solved.<br/><NewLine><img alt=""landscape_output"" data-base62-sha1=""7AXCkB9vd3soU2sii4VE07axgls"" height=""448"" src=""https://discuss.pytorch.org/uploads/default/original/3X/3/5/353cd2a5798a6ad4ff9f75942564ba260f60082a.jpeg"" width=""648""/></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Good to hear you solved the issue!<br/><NewLine>However, why doesn’t <code>libtorch</code> output the same shape as the Python model?<br/><NewLine>Did you narrowed down this issue, as it sounds like a bug.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>caffe has a differnent implementation of convolutional layers compared with pytorch.<br/><NewLine>if the code of libtorch comes from caffe, then it explains why.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hktxt; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/hktxt; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/hktxt; <NewLine> ,"REPLY_DATE 1: April 19, 2020,  9:03am; <NewLine> REPLY_DATE 2: April 19, 2020,  1:07pm; <NewLine> REPLY_DATE 3: April 20, 2020,  6:58am; <NewLine> REPLY_DATE 4: April 20, 2020,  6:57am; <NewLine> REPLY_DATE 5: April 20, 2020,  7:13am; <NewLine> REPLY_DATE 6: April 21, 2020,  1:13am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
77184,How to investigate RuntimeError: bad optional access,2020-04-17T12:35:53.203Z,1,98,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to convert a torch model to torchscript. I have removed all non supported operations from my model (the ones raised by <code>torch.jit.script(model)</code> ) but now the conversion fails with the following error:</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-13-d068ba517b85&gt; in &lt;module&gt;<NewLine>      1 import torch<NewLine>----&gt; 2 torchscript_model = torch.jit.script(fa)<NewLine><NewLine>~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/jit/__init__.py in script(obj, optimize, _frames_up, _rcb)<NewLine>   1201 <NewLine>   1202     if isinstance(obj, torch.nn.Module):<NewLine>-&gt; 1203         return torch.jit.torch.jit._recursive.recursive_script(obj)<NewLine>   1204 <NewLine>   1205     qualified_name = _qualified_name(obj)<NewLine><NewLine>~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/jit/_recursive.py in recursive_script(mod, exclude_methods)<NewLine>    171     filtered_methods = filter(ignore_overloaded, methods)<NewLine>    172     stubs = list(map(make_stub, filtered_methods))<NewLine>--&gt; 173     return copy_to_script_module(mod, overload_stubs + stubs)<NewLine>    174 <NewLine>    175 <NewLine><NewLine>~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/jit/_recursive.py in copy_to_script_module(original, stubs)<NewLine>     93             setattr(script_module, name, item)<NewLine>     94 <NewLine>---&gt; 95     torch.jit._create_methods_from_stubs(script_module, stubs)<NewLine>     96 <NewLine>     97     # Now that methods have been compiled, take methods that have been compiled<NewLine><NewLine>~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/jit/__init__.py in _create_methods_from_stubs(self, stubs)<NewLine>   1421     rcbs = [m.resolution_callback for m in stubs]<NewLine>   1422     defaults = [get_default_args(m.original_method) for m in stubs]<NewLine>-&gt; 1423     self._c._create_methods(self, defs, rcbs, defaults)<NewLine>   1424 <NewLine>   1425 # For each user-defined class that subclasses ScriptModule this meta-class,<NewLine><NewLine>~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/jit/_recursive.py in create_method_from_fn(module, fn)<NewLine>    183         # We don't want to call the hooks here since the graph that is calling<NewLine>    184         # this function is not yet complete<NewLine>--&gt; 185         torch.jit._create_methods_from_stubs(module, (stub,))<NewLine>    186     return stub<NewLine>    187 <NewLine><NewLine>~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/jit/__init__.py in _create_methods_from_stubs(self, stubs)<NewLine>   1421     rcbs = [m.resolution_callback for m in stubs]<NewLine>   1422     defaults = [get_default_args(m.original_method) for m in stubs]<NewLine>-&gt; 1423     self._c._create_methods(self, defs, rcbs, defaults)<NewLine>   1424 <NewLine>   1425 # For each user-defined class that subclasses ScriptModule this meta-class,<NewLine><NewLine>~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/jit/_recursive.py in make_strong_submodule(field, module, parent)<NewLine>    193 <NewLine>    194     # Convert the module to a ScriptModule<NewLine>--&gt; 195     new_strong_submodule = recursive_script(module)<NewLine>    196 <NewLine>    197     # Install the ScriptModule on the python side<NewLine><NewLine>~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/jit/_recursive.py in recursive_script(mod, exclude_methods)<NewLine>    171     filtered_methods = filter(ignore_overloaded, methods)<NewLine>    172     stubs = list(map(make_stub, filtered_methods))<NewLine>--&gt; 173     return copy_to_script_module(mod, overload_stubs + stubs)<NewLine>    174 <NewLine>    175 <NewLine><NewLine>~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/jit/_recursive.py in copy_to_script_module(original, stubs)<NewLine>     93             setattr(script_module, name, item)<NewLine>     94 <NewLine>---&gt; 95     torch.jit._create_methods_from_stubs(script_module, stubs)<NewLine>     96 <NewLine>     97     # Now that methods have been compiled, take methods that have been compiled<NewLine><NewLine>~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/jit/__init__.py in _create_methods_from_stubs(self, stubs)<NewLine>   1421     rcbs = [m.resolution_callback for m in stubs]<NewLine>   1422     defaults = [get_default_args(m.original_method) for m in stubs]<NewLine>-&gt; 1423     self._c._create_methods(self, defs, rcbs, defaults)<NewLine>   1424 <NewLine>   1425 # For each user-defined class that subclasses ScriptModule this meta-class,<NewLine><NewLine>~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/jit/_recursive.py in create_method_from_fn(module, fn)<NewLine>    183         # We don't want to call the hooks here since the graph that is calling<NewLine>    184         # this function is not yet complete<NewLine>--&gt; 185         torch.jit._create_methods_from_stubs(module, (stub,))<NewLine>    186     return stub<NewLine>    187 <NewLine><NewLine>~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/jit/__init__.py in _create_methods_from_stubs(self, stubs)<NewLine>   1421     rcbs = [m.resolution_callback for m in stubs]<NewLine>   1422     defaults = [get_default_args(m.original_method) for m in stubs]<NewLine>-&gt; 1423     self._c._create_methods(self, defs, rcbs, defaults)<NewLine>   1424 <NewLine>   1425 # For each user-defined class that subclasses ScriptModule this meta-class,<NewLine><NewLine>~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/jit/_recursive.py in create_method_from_fn(module, fn)<NewLine>    183         # We don't want to call the hooks here since the graph that is calling<NewLine>    184         # this function is not yet complete<NewLine>--&gt; 185         torch.jit._create_methods_from_stubs(module, (stub,))<NewLine>    186     return stub<NewLine>    187 <NewLine><NewLine>~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/jit/__init__.py in _create_methods_from_stubs(self, stubs)<NewLine>   1421     rcbs = [m.resolution_callback for m in stubs]<NewLine>   1422     defaults = [get_default_args(m.original_method) for m in stubs]<NewLine>-&gt; 1423     self._c._create_methods(self, defs, rcbs, defaults)<NewLine>   1424 <NewLine>   1425 # For each user-defined class that subclasses ScriptModule this meta-class,<NewLine><NewLine>RuntimeError: bad optional access<NewLine></code></pre><NewLine><p>I have no idea how to investigate or solve this problem. Is there any way to get to the root of the issue?</p><NewLine><p>Thanks,</p><NewLine><p>Julien</p><NewLine></div>",https://discuss.pytorch.org/u/Julien_Despois,(Julien Despois),Julien_Despois,"April 17, 2020, 12:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This looks like a bug, would you mind filing an issue on GitHub with a repro so we can work on a fix?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your answer.<br/><NewLine>After investigating the model (commenting blocks of code until the error is gone), I have traced back to the error to creating a tensor manually from a blend of 0-dim tensor and floats.</p><NewLine><p>E.g. <code>torch.tensor([[my_tensor, my_tensor]])</code> works <img alt="":white_check_mark:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/white_check_mark.png?v=9"" title="":white_check_mark:""/><br/><NewLine>E.g. <code>torch.tensor([[my_tensor, 0.0]])</code> raises the error <img alt="":x:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/x.png?v=9"" title="":x:""/></p><NewLine><p>I have solved it in my case by using:<br/><NewLine><code>torch.cat([my_tensor.view(1), torch.tensor([0.0])]).unsqueeze</code><br/><NewLine>or something similar.</p><NewLine><p>Julien</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s great debugging! <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/><br/><NewLine>An issue would still be good, as the error message wasn’t really helpful.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Julien_Despois; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: April 18, 2020, 11:14am; <NewLine> REPLY_DATE 2: April 18, 2020, 12:09pm; <NewLine> REPLY_DATE 3: April 19, 2020, 12:12am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
76128,No graph attribute when use torch.jit._overload_method,2020-04-10T03:29:02.567Z,3,235,"<div class=""post"" itemprop=""articleBody""><NewLine><p>PyTorch version: 1.6.0.dev20200407+cu101</p><NewLine><p>When use torch.jit._overload_method to overload functions, it seems that there is no graph attribute in the output torchscript.</p><NewLine><pre><code class=""lang-auto"">class MyModule(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine><NewLine>    @torch.jit._overload_method  # noqa: F811<NewLine>    def forward(self, x: int) -&gt; int:  # noqa: F811<NewLine>        pass<NewLine><NewLine>    @torch.jit._overload_method  # noqa: F811<NewLine>    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:  # noqa: F811<NewLine>        pass<NewLine><NewLine>    def forward(self, x):  # noqa: F811<NewLine>        original = x<NewLine><NewLine>        if isinstance(original, int):<NewLine>            return original + 2<NewLine>        else:<NewLine>            return original.sum()<NewLine><NewLine>model = MyModule()<NewLine>s = torch.jit.script(model)<NewLine>torch._C._jit_pass_inline(s.graph)  # error happens here<NewLine></code></pre><NewLine><p>error message:</p><NewLine><pre><code class=""lang-auto"">torch.nn.modules.module.ModuleAttributeError: 'RecursiveScriptModule' object has no attribute 'graph'<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/BobChen,,BobChen,"April 10, 2020,  5:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, this is is because there is no single “graph”, there are two separate graphs, one for each overload. I could maybe add an api for something like <code>model.forwards.graphs</code>, or <code>model.forward.graph_for_types(1)</code> to get the graph for a specific overload.</p><NewLine><p>Generally, overloads should be usable and work as you expect when they are contained in other modules but you may run into a little bit of difficulty if you are interacting with them as a top level module. They’re still an internal feature but will be cleaned up and released probably in the next release or the one after.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""76128"" data-username=""eellison""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/eellison/40/9338_2.png"" width=""20""/> eellison:</div><NewLine><blockquote><NewLine><p>Generally, overloads should be usable and work as you expect when they are contained in other modules but you may run into a little bit of difficulty if you are interacting with them as a top level module. They’re still an internal feature but will be cleaned up and released probably in the next release or the one after.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thank you very much:)<br/><NewLine>I put overloads into sub modules and find there is one single graph for one kind of overload as you said:</p><NewLine><pre><code class=""lang-auto"">class MySubModule(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine><NewLine>    @torch.jit._overload_method  # noqa: F811<NewLine>    def forward(self, x: int) -&gt; int:  # noqa: F811<NewLine>        pass<NewLine><NewLine>    @torch.jit._overload_method  # noqa: F811<NewLine>    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:  # noqa: F811<NewLine>        pass<NewLine><NewLine>    def forward(self, x):  # noqa: F811<NewLine>        original = x<NewLine><NewLine>        if isinstance(original, int):<NewLine>            return original + 2<NewLine>        else:<NewLine>            return original.sum()<NewLine><NewLine>class MyModule(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.sub_module = MySubModule()<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.sub_module(x)<NewLine><NewLine><NewLine>model = MyModule()<NewLine>s = torch.jit.script(model)<NewLine>torch._C._jit_pass_inline(s.graph)<NewLine>print(s.graph)<NewLine></code></pre><NewLine><p>output:</p><NewLine><pre><code class=""lang-auto"">graph(%self : __torch__.MyModule,<NewLine>      %x.1 : Tensor):<NewLine>      %2 : __torch__.MySubModule = prim::GetAttr[name=""sub_module""](%self)<NewLine>      %14 : None = prim::Constant()<NewLine>      %15 : Tensor = aten::sum(%x.1, %14) <NewLine>  return (%15)<NewLine></code></pre><NewLine><p>Since default type is torch.Tensor, the corresponding graph is obtained. So is there any ways to obtain one single torchscript for all overloads for now?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The MyModule you’re using only takes a Tensor. If you want to access the int graph you could try using a different module with the submodule that takes in an int.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Got it, thank you very much:)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/BobChen; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/BobChen; <NewLine> ,"REPLY_DATE 1: April 10, 2020,  6:09pm; <NewLine> REPLY_DATE 2: April 11, 2020,  6:24am; <NewLine> REPLY_DATE 3: April 14, 2020,  5:15pm; <NewLine> REPLY_DATE 4: April 15, 2020,  3:25am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
76666,Extract constants from Torchscript module,2020-04-14T01:55:19.579Z,4,126,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am building a library that converts a torchscript module to mxnet gluon. Given the two language is very similar I have an easy time parsing the .code and recreate the model. However, I am not able to extract the constants that appear in the code (e.g. CONSTANTS.c0). Any idea how I can obtain the value of this from the jit compiled module?</p><NewLine><p>Thanks a lot!</p><NewLine></div>",https://discuss.pytorch.org/u/ifeherva,,ifeherva,"April 14, 2020,  1:55am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you trying to extract the constants from the serialized binary or in C++? If it’s the former you’ll have to parse the <code>constants.pkl</code> in the serialized zip file, which is in the same format as Python’s pickler (our reader is <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/serialization/pickler.cpp"">here</a>).</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Shouldn’t be too hard to add an endpoint that also passes back the constants table, given that we populate it with any call to <code>PythonPrint</code>. <a class=""mention"" href=""/u/ifeherva"">@ifeherva</a> can you file a feature request on github for this?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am using currently the .code attribute of the TracedModule class. For example a resnet50 backbone with a constant scaling factor looks like this:</p><NewLine><pre><code>def forward(self,<NewLine>    input: Tensor) -&gt; Tensor:<NewLine>_0 = self.model.relu<NewLine>_1 = (self.model.bn1).forward((self.model.conv1).forward(input, ), )<NewLine>_2 = self.model.layer1<NewLine>_3 = (self.model.maxpool).forward((_0).forward(_1, ), )<NewLine>_4 = self.model.layer3<NewLine>_5 = (self.model.layer2).forward((_2).forward(_3, ), )<NewLine>_6 = self._pool<NewLine>_7 = (self.model.layer4).forward((_4).forward(_5, ), )<NewLine>_8 = (_6).forward(_7, )<NewLine>_9 = ops.prim.NumToTensor(torch.size(_8, 0))<NewLine>x = torch.view(_8, [int(_9), -1])<NewLine>return torch.mul(x, CONSTANTS.c0)<NewLine></code></pre><NewLine><p>I did’t consider the zip file yet, I thought there is a way to get it ‘on-the-fly’.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Will do that, thanks!</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""76666"" data-username=""ifeherva""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/i/94ad74/40.png"" width=""20""/> ifeherva:</div><NewLine><blockquote><NewLine><p>TracedModule</p><NewLine></blockquote><NewLine></aside><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/36625"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/36625"" rel=""nofollow noopener"" target=""_blank"">Add TracedModule attribute for the constants table</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-04-14"" data-format=""ll"" data-time=""22:33:29"" data-timezone=""UTC"">10:33PM - 14 Apr 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/ifeherva"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""ifeherva"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars3.githubusercontent.com/u/3716849?v=4"" width=""20""/><NewLine>          ifeherva<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">🚀 Feature<NewLine>TracedModuel currently returns the compiled code via the .code() member function. This code can possible contain constants (e.g. CONSTANTS.c0), however...</p><NewLine></div><NewLine><div class=""labels""><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ifeherva; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ifeherva; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ifeherva; <NewLine> ,"REPLY_DATE 1: April 14, 2020,  2:17am; <NewLine> REPLY_DATE 2: April 14, 2020,  2:18am; <NewLine> REPLY_DATE 3: April 14, 2020,  2:29am; <NewLine> REPLY_DATE 4: April 14, 2020,  2:30am; <NewLine> REPLY_DATE 5: April 14, 2020, 10:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
76365,RuntimeError: Unknown IValue type for pickling: Device (pushIValueImpl at /opt/conda/conda-bld/pytorch_1579022030672/work/torch/csrc/jit/pickler.cpp:132),2020-04-11T19:39:44.789Z,1,119,"<div class=""post"" itemprop=""articleBody""><NewLine><p>i get this error when i use torch.jit.script then torch.jit.save<br/><NewLine>: frame <span class=""hashtag"">#0:</span> c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x47 (0x7f37df3cb627 in /home/kencorp/miniconda3/lib/python3.7/site-packages/torch/lib/libc10.so)<br/><NewLine>frame <span class=""hashtag"">#1:</span>  + 0x2f9d95d (0x7f37e258e95d in /home/kencorp/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch.so)<br/><NewLine>frame <span class=""hashtag"">#2:</span>  + 0x2f9dc3b (0x7f37e258ec3b in /home/kencorp/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch.so)<br/><NewLine>frame <span class=""hashtag"">#3:</span>  + 0x2f9d1ee (0x7f37e258e1ee in /home/kencorp/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch.so)<br/><NewLine>frame <span class=""hashtag"">#4:</span>  + 0x2f9dc5b (0x7f37e258ec5b in /home/kencorp/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch.so)<br/><NewLine>frame <span class=""hashtag"">#5:</span>  + 0x32bcdd9 (0x7f37e28addd9 in /home/kencorp/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch.so)<br/><NewLine>frame <span class=""hashtag"">#6:</span>  + 0x32c0bdd (0x7f37e28b1bdd in /home/kencorp/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch.so)<br/><NewLine>frame <span class=""hashtag"">#7:</span> torch::jit::ExportModule(torch::jit::script::Module const&amp;, std::string const&amp;, std::unordered_map&lt;std::string, std::string, std::hash<a>std::string</a>, std::equal_to<a>std::string</a>, std::allocator&lt;std::pair&lt;std::string const, std::string&gt; &gt; &gt; const&amp;, bool) + 0x19b (0x7f37e28aba5b in /home/kencorp/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch.so)<br/><NewLine>frame <span class=""hashtag"">#8:</span>  + 0x743dd2 (0x7f37e581cdd2 in /home/kencorp/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)<br/><NewLine>frame <span class=""hashtag"">#9:</span>  + 0x26b876 (0x7f37e5344876 in /home/kencorp/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)<br/><NewLine><br/><NewLine>frame <span class=""hashtag"">#29:</span> __libc_start_main + 0xf0 (0x7f37f0fb1830 in /lib/x86_64-linux-gnu/libc.so.6)</p><NewLine></div>",https://discuss.pytorch.org/u/Tekker_Tiger,(Tekker Tiger),Tekker_Tiger,"April 11, 2020,  7:39pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Support for <code>Device</code> was added somewhat recently, are you on the latest version of PyTorch? If so, can you open an issue on GitHub with a repro?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><blockquote><NewLine><blockquote><NewLine><p>import torch<br/><NewLine>torch.<strong>version</strong><br/><NewLine>‘1.4.0’</p><NewLine></blockquote><NewLine></blockquote><NewLine><blockquote><NewLine><blockquote></blockquote><NewLine></blockquote><NewLine></blockquote><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Tekker_Tiger; <NewLine> ,"REPLY_DATE 1: April 14, 2020,  2:18am; <NewLine> REPLY_DATE 2: April 14, 2020,  6:51pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
76323,Will torch.jit.unused support normal object class?,2020-04-11T14:18:43.023Z,1,92,"<div class=""post"" itemprop=""articleBody""><NewLine><p>It seems that torch.jit.unused does not support normal object class. Is there any plans to support it? For now, the following codes will cause an error: <strong>'torch.jit.frontend.UnsupportedNodeError: with statements aren’t supported:'</strong></p><NewLine><pre><code class=""lang-auto"">@torch.jit.script<NewLine>class MyObject(object):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine><NewLine>    @torch.jit.unused<NewLine>    def unsupported(self):<NewLine>        with torch.no_grad():<NewLine>            print('This is not supported by jit.')<NewLine><NewLine>    def supported(self, x):<NewLine>        print('This is supported by jit.')<NewLine></code></pre><NewLine><p>By the way, could anyone give some advices to handle this situation where classes with torchscript-unsupported attributes are used during torchscript inference process.</p><NewLine></div>",https://discuss.pytorch.org/u/BobChen,,BobChen,"April 12, 2020,  1:29am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Right now everything that is in a <code>@torch.jit.script</code>ed class has to be TorchScript compatible, but we’re working on fixing it so <code>@ignore</code> and <code>@unused</code> work as they should. The best way around it for now is probably to use free-standing functions (which you can <code>@ignore</code>) that take the object as the first argument</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""2"" data-topic=""76323"" data-username=""driazati""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/driazati/40/15393_2.png"" width=""20""/> driazati:</div><NewLine><blockquote><NewLine><p>Right now everything that is in a <code>@torch.jit.script</code> ed class has to be TorchScript compatible, but we’re working on fixing it so <code>@ignore</code> and <code>@unused</code> work as they should. The best way around it for now is probably to use free-standing functions (which you can <code>@ignore</code> ) that take the object as the first argument</p><NewLine></blockquote><NewLine></aside><NewLine><p>Got it, thank you very much:)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/BobChen; <NewLine> ,"REPLY_DATE 1: April 14, 2020,  2:10am; <NewLine> REPLY_DATE 2: April 14, 2020,  9:02am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
76264,JNI does not support GPU?,2020-04-11T01:11:37.150Z,0,65,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When using JNI from Java, it seems not possible to allocate tensor objects to the GPU, and pytorch JNI will always allocate the tensors to CPU.<br/><NewLine>It’s also not possible to move the model to GPU memory, or if the traced model uses a GPU, it’s not possible to move it back to CPU.</p><NewLine><p>Can someone confirm that my understanding is correct?</p><NewLine></div>",https://discuss.pytorch.org/u/ajmssc,(Jean-Marc Soumet),ajmssc,"April 11, 2020,  1:11am",,,,,
75594,Convert torch._C.Graph to torch.jit.Graph,2020-04-06T18:51:01.863Z,0,101,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>Is there any way to convert torch._C.Graph to torch.jit.Graph. Or is there a way generate a dot file from the torch._C.Graph. I would like to create a dot file from tracing like in <a href=""https://github.com/szagoruyko/pytorchviz/blob/master/torchviz/dot.py"" rel=""nofollow noopener"">make_dot_from_trace</a> function. But it takes only torch.jit.Graph as input.</p><NewLine><p>When I ran my model through <a href=""https://pytorch.org/docs/stable/jit.html"" rel=""nofollow noopener"">torch.jit.trace(model, input)</a>; I  got a object whose .graph was of type torch._C.Graph which throws error when I keep it as a parameter for. make_dot_from_trace function.</p><NewLine></div>",https://discuss.pytorch.org/u/Sharath_R,(Sharath R),Sharath_R,"April 6, 2020,  6:51pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi so we don’t have a <code>torch.jit.Graph</code> object to begin with I think, all graph objects are <code>torch._C.Graph</code>, so I think if pytorchviz is using <code>torch.jit.Graph</code> that could be already a issue, you probably should submit a issue or a fix to the pytorchviz repo instead.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wanchaol; <NewLine> ,"REPLY_DATE 1: April 6, 2020,  7:11pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
75323,Limit number of threads in Java?,2020-04-04T09:46:01.432Z,0,53,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using Java for making inferences. However, it consumes too many CPU threads, is it possible to set an upper limit like it is possible to do in Python and C++? I cannot find any reference on how to do so.</p><NewLine><p>Thank you, Jakob</p><NewLine></div>",https://discuss.pytorch.org/u/Jakkes,(Jakob),Jakkes,"April 4, 2020,  9:46am",,,,,
68873,JITed GRU too slow,2020-02-06T14:25:39.357Z,3,292,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to create some custom GRU variants (mainly Layer normalized).<br/><NewLine>I was following some blog posts and benchmarks (like <a href=""https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py</a>) and I created following small benchmark <a href=""https://gist.github.com/usamec/af21be7b83e6b1a3f38c26136af811f3"" rel=""nofollow noopener"">https://gist.github.com/usamec/af21be7b83e6b1a3f38c26136af811f3</a>, where it seems, that JITed GRU is 10 times slower than cuDNN implemention (but 3times faster than nonJITed). (Using GeForce RTX 2080 Ti).</p><NewLine><p>It is advertised, that at least JIT forward pass runs similarly than cuDNN, so what am I doing wrong?<br/><NewLine>(I tried running it multiple times, so results are not affected by cold start).</p><NewLine></div>",https://discuss.pytorch.org/u/Vlado_Boza,(Vlado Boza),Vlado_Boza,"February 6, 2020,  2:45pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>As far as I know, cuDNN is the fastest in all circumstances. And JIT optimizes the code as it runs, so later batches run faster than the first few batches.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried several runs with same inputs. So cold start is not an issue.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>can you post your benchmark script? 10 times slower is very weird as we don’t expect that much.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>There is a linked script in original post: <a href=""https://gist.github.com/usamec/af21be7b83e6b1a3f38c26136af811f3"" rel=""nofollow noopener"">https://gist.github.com/usamec/af21be7b83e6b1a3f38c26136af811f3</a></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Also when I increase size of GRU to 1024 features, then the relative difference is much smaller (32 ms JIT, 25 ms cuDNN).</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have the same problem, though my variant is closer to RNN than GRU, so I would think it would be even easier to optimize. Increasing the size has the same effect of shrinking the performance gap, but I need the feature size to be relatively small.</p><NewLine><p>Would be great to figure out how to improve performance of jit RNNs. It’s a really simple model (and inference in C++ is lightning fast), so I’m not sure why it’s so slow.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you please open an issue on github and paste in the no. here? I will be taking a look at this very shortly.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/35998"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/35998"" rel=""nofollow noopener"" target=""_blank"">JITed GRU too slow</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-04-03"" data-format=""ll"" data-time=""22:56:33"" data-timezone=""UTC"">10:56PM - 03 Apr 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/usamec"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""usamec"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars3.githubusercontent.com/u/1625559?v=4"" width=""20""/><NewLine>          usamec<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">🐛 Bug<NewLine>It is advertised, that forward pass of JITed RNNs (e.g. GRU) is as fast as cuDNN implementation.<NewLine>But it is not...</p><NewLine></div><NewLine><div class=""labels""><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/G.M; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Vlado_Boza; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Vlado_Boza; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Vlado_Boza; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Daniel_Gonzalez; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/villedepommes; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Vlado_Boza; <NewLine> ,"REPLY_DATE 1: February 6, 2020,  2:31pm; <NewLine> REPLY_DATE 2: February 6, 2020,  2:34pm; <NewLine> REPLY_DATE 3: February 7, 2020,  6:58pm; <NewLine> REPLY_DATE 4: February 7, 2020,  7:43pm; <NewLine> REPLY_DATE 5: February 8, 2020,  5:32pm; <NewLine> REPLY_DATE 6: March 9, 2020, 12:38pm; <NewLine> REPLY_DATE 7: April 3, 2020,  8:29pm; <NewLine> REPLY_DATE 8: April 3, 2020, 10:56pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
74927,Named Tensors + jit,2020-03-31T18:07:02.842Z,0,84,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>are there any plans to support jit compilation of code using named tensor operations?</p><NewLine><p>IMHO, named tensors are potentially great, but are in a weird place: rewriting old code seems not worth it, especially with that list of unsupported subsystems. But even for new code, that is big and complex enough to require dimension reorderings, one has to choose between JIT and named tensors beforehand. To me, JIT seems more valuable (and even more so for simpler code fragments, that require no more that some occasional head/tail [un]squeeze).</p><NewLine></div>",https://discuss.pytorch.org/u/googlebot,(Alex),googlebot,"March 31, 2020,  6:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/googlebot"">@googlebot</a>,</p><NewLine><p>We’ve discussed it but there are no immediate plans. However it wouldn’t be that hard to add. Maybe by the next release ?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/eellison; <NewLine> ,"REPLY_DATE 1: April 2, 2020,  8:09pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
74799,How to deploy torchscript module when original data structure class is not avaliable,2020-03-30T13:50:34.774Z,2,237,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Take detectron2 as an example. I have transformed a model from detectron2 with torch.jit.script to torchscript, the input of which is annotated with <code>detectron2.structures.box</code>. Because there is no detectron2 python package in the deployment environment of the torchscript module , I define another input data class <code>x.box</code> which has totally the same codes as <code>detectron2.structures.box</code>. But when this new input class is sent to the torchscript module, an error happened:<br/><NewLine><code>RuntimeError: classType INTERNAL ASSERT FAILED at /pytorch/torch/csrc/jit/python/pybind_utils.h:742, please report a bug to PyTorch.</code></p><NewLine><p>Is this mean that I must send the same input data class(detectron2.structures.box) to the model? Or, is there any ways to make it work?</p><NewLine><p>Thanks a lot:)</p><NewLine></div>",https://discuss.pytorch.org/u/BobChen,,BobChen,"March 30, 2020,  1:54pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have figured out the reason. It is because the data class of the output of torchscript is not supported in the deployment environment.</p><NewLine><p>So is there any way to register a new data class which consist of the same codes of the original one in torchscript in the deployment environment and construct links between them?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you able to post a repro for the internal assert failure?</p><NewLine><p>It sounds like you’re asking for <a href=""https://en.wikipedia.org/wiki/Structural_type_system"">structural typing</a> instead of <a href=""https://en.wikipedia.org/wiki/Nominal_type_system"">nominal typing</a>. TorchScript is nominally typed, so we don’t have a blessed way to do exactly what you’re asking. (It does look like some version of structural typing is supported, but that is <a href=""https://github.com/pytorch/pytorch/issues/35788"">likely an accident</a>).</p><NewLine><p><code>detectron2.structures.box</code> should be included with the serialized code of your TorchScript model, so you should be able to reconstruct it inside the JIT (though the API to do this may not exist/be pretty). Once we fix the above bug we can probably figure out some hack to make this still work.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The codes are still being modified, but I can illustrate this problem with some sample codes：</p><NewLine><p>In the development environment where torchscript is exported, we have:</p><NewLine><pre><code class=""lang-auto"">from detectron2.structures import Box<NewLine><NewLine>class MyModule(nn.Module) -&gt; Box:<NewLine>    some_of_codes<NewLine></code></pre><NewLine><p>then we export torchscript:</p><NewLine><pre><code class=""lang-auto"">model = MyModule()<NewLine>torch.jit.script(model).save('scripted_model.pt')<NewLine></code></pre><NewLine><p>In the deployment environment, we have:</p><NewLine><pre><code class=""lang-auto"">model = torch.jit.load('scripted_model.pt')<NewLine>input = some_data_of_model<NewLine>output = model(input)<NewLine></code></pre><NewLine><p>Since the data type ‘detectron2.structures.Box’ is not available here, the RuntimeError mentioned above will appear.</p><NewLine><p>I can see that the ‘detectron2.structures.Box’ class is available in the serialized code of my TorchScript, but it seems that I can not use it to construct an instance which can receive the result from the output of torchscript.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/BobChen; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/BobChen; <NewLine> ,"REPLY_DATE 1: March 31, 2020,  7:57am; <NewLine> REPLY_DATE 2: April 1, 2020, 12:25am; <NewLine> REPLY_DATE 3: April 2, 2020,  1:53am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
74659,Cannot reproduce gradients of GRU,2020-03-29T03:08:46.931Z,0,74,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I made a custom GRU following <a href=""https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py"" rel=""nofollow noopener"">this custom LSTM</a>. Compared with the Pytorch native GRU, I am able to reproduce the outputs/loss, but not the gradients.</p><NewLine><p>The custom GRU layer is the following:</p><NewLine><pre><code class=""lang-python"">class GRUCell(jit.ScriptModule):<NewLine>    __constants__ = ['ngate']<NewLine>    def __init__(self, input_size, hidden_size):<NewLine>        super(GRUCell, self).__init__()<NewLine>        self.input_size = input_size<NewLine>        self.hidden_size = hidden_size<NewLine>        self.ngate = 3<NewLine>        self.w_ih = Parameter(torch.randn(self.ngate * hidden_size, input_size))<NewLine>        self.w_hh = Parameter(torch.randn(self.ngate * hidden_size, hidden_size))<NewLine>        self.b_ih = Parameter(torch.randn(self.ngate * hidden_size))<NewLine>        self.b_hh = Parameter(torch.randn(self.ngate * hidden_size))<NewLine><NewLine>    @jit.script_method<NewLine>    def forward(self, inputs, hidden):<NewLine>        # type: (Tensor, Tensor) -&gt; Tensor<NewLine>        gi = torch.mm(inputs, self.w_ih.t()) + self.b_ih<NewLine>        gh = torch.mm(hidden, self.w_hh.t()) + self.b_hh<NewLine>        i_r, i_i, i_n = gi.chunk(self.ngate, 1)<NewLine>        h_r, h_i, h_n = gh.chunk(self.ngate, 1)<NewLine>        resetgate = torch.sigmoid(i_r + h_r)<NewLine>        inputgate = torch.sigmoid(i_i + h_i)<NewLine>        newgate = torch.tanh(i_n + resetgate * h_n)<NewLine>        hy = newgate + inputgate * (hidden - newgate)<NewLine>        return hy<NewLine><NewLine>class GRULayer(jit.ScriptModule):<NewLine>    def __init__(self, cell, *cell_args):<NewLine>        super(GRULayer, self).__init__()<NewLine>        self.cell = cell(*cell_args)<NewLine><NewLine>    @jit.script_method<NewLine>    def forward(self, inputs, out):<NewLine>        # type: (Tensor, Tensor) -&gt; Tensor<NewLine>        inputs = inputs.unbind(0)<NewLine>        outputs = torch.jit.annotate(List[Tensor], [])<NewLine>        for i in range(len(inputs)):<NewLine>            out = self.cell(inputs[i], out)<NewLine>            outputs += [out]<NewLine>        return torch.stack(outputs)<NewLine></code></pre><NewLine><p>The outputs and gradients are compared:</p><NewLine><pre><code class=""lang-python"">torch.manual_seed(10)<NewLine>seq_len = 5<NewLine>batch = 5<NewLine>input_size = 3<NewLine>num_classes = 2<NewLine>hidden_size = num_classes<NewLine>criterion = nn.CrossEntropyLoss()<NewLine>inp = torch.randn(seq_len, batch, input_size)<NewLine>label = torch.randint(low = 0, high = num_classes, size = (batch,))<NewLine>state = torch.randn(batch, hidden_size)<NewLine><NewLine>rnn = GRULayer(GRUCell, input_size, hidden_size)<NewLine>out = rnn(inp, state)<NewLine>out = out[-1]<NewLine>loss = criterion(out, label)<NewLine>loss.backward()<NewLine>gradients = [x.grad for x in rnn.parameters()]<NewLine><NewLine># Control: pytorch native GRU<NewLine>native_gru = nn.GRU(input_size, hidden_size, 1, batch_first = False)<NewLine>native_gru_state = state.unsqueeze(0)<NewLine>for native_gru_param, custom_param in zip(native_gru.all_weights[0], rnn.parameters()):<NewLine>    assert native_gru_param.shape == custom_param.shape<NewLine>    with torch.no_grad():<NewLine>        native_gru_param.copy_(custom_param)<NewLine>native_gru_out, native_gru_out_state = native_gru(inp, native_gru_state)<NewLine>native_gru_out = native_gru_out[-1, :, :]<NewLine>native_gru_loss = criterion(native_gru_out, label)<NewLine>native_gru_loss.backward()<NewLine>native_gru_gradients = [x.grad for x in native_gru.all_weights[0]]<NewLine><NewLine>print(""loss is"", loss.item())<NewLine>print(""loss difference is"", (loss - native_gru_loss).max().item())<NewLine>print(""gradient differences are"")<NewLine>for x, y in zip(gradients, native_gru_gradients):<NewLine>    # print(x.abs().max().item())<NewLine>    # print(y.abs().max().item())<NewLine>    print((x - y).abs().max().item())<NewLine></code></pre><NewLine><p>Here are the outputs:</p><NewLine><pre><code class=""lang-python"">loss is 0.5983431935310364<NewLine>loss difference is 0.0<NewLine>gradient differences are<NewLine>0.16684868931770325<NewLine>0.10169483721256256<NewLine>0.08745706081390381<NewLine>0.06843984127044678<NewLine></code></pre><NewLine><p>I think the problem lies in GRULayer, as I am able to reproduce the gradients in a single GRUcell (e.g. by setting seq_len = 1). What makes the problem more weird is I am able to reproduce the gradients of the Pytorch native LSTM layers with the original custom LSTM provided in the link.</p><NewLine><p>Your help is very much appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/KENG,,KENG,"March 29, 2020,  6:35am",,,,,
74602,How to translate pytorch IR to MLIR,2020-03-28T11:57:16.457Z,0,123,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is there any way to convert pytorch IR（JIT） to MLIR？Does IR also exist in eager mode?</p><NewLine></div>",https://discuss.pytorch.org/u/lichao,(lichao),lichao,"March 28, 2020, 11:57am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Anybody know this topic?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/lichao; <NewLine> ,"REPLY_DATE 1: March 29, 2020,  5:13am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
74593,"RuntimeError: type INTERNAL ASSERT FAILED at ../torch/csrc/jit/ir.h:1266, please report a bug to PyTorch.",2020-03-28T10:26:02.756Z,1,413,"<div class=""post"" itemprop=""articleBody""><NewLine><h3>Bug</h3><NewLine><pre><code class=""lang-python"">Traceback (most recent call last):<NewLine>  File ""/Users/simon/.pyenv/versions/3.7.4/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1034, in trace_module<NewLine>    module._c._create_method_from_trace(method_name, func, example_inputs, var_lookup_fn, _force_outplace)<NewLine>RuntimeError: type INTERNAL ASSERT FAILED at ../torch/csrc/jit/ir.h:1266, please report a bug to PyTorch.  (setType at ../torch/csrc/jit/ir.h:1266)<NewLine>frame #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) + 135 (0x11ac06787 in libc10.dylib)<NewLine>frame #1: torch::jit::Value::setType(std::__1::shared_ptr&lt;c10::Type&gt;) + 459 (0x11eee6eab in libtorch.dylib)<NewLine>frame #2: torch::jit::Graph::createGetAttr(torch::jit::Value*, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) + 259 (0x11ef6ee03 in libtorch.dylib)<NewLine>frame #3: torch::jit::(anonymous namespace)::ConvertTracedAttrReferences::convertAttrReferencesToLocalGetAttrs(torch::jit::Block*, c10::QualifiedName const&amp;, torch::jit::Value*) + 714 (0x11efdc3da in libtorch.dylib)<NewLine>frame #4: torch::jit::FixupTraceScopeBlocks(std::__1::shared_ptr&lt;torch::jit::Graph&gt;&amp;, torch::jit::script::Module*) + 316 (0x11efd935c in libtorch.dylib)<NewLine>frame #5: torch::jit::tracer::trace(std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;, std::__1::function&lt;std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt; (std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;)&gt; const&amp;, std::__1::function&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; (at::Tensor const&amp;)&gt;, bool, torch::jit::script::Module*) + 2127 (0x11f1c8fef in libtorch.dylib)<NewLine>frame #6: torch::jit::tracer::createGraphByTracing(pybind11::function const&amp;, std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;, pybind11::function const&amp;, bool, torch::jit::script::Module*) + 361 (0x11b7d3a49 in libtorch_python.dylib)<NewLine>frame #7: void pybind11::cpp_function::initialize&lt;torch::jit::script::initJitScriptBindings(_object*)::$_13, void, torch::jit::script::Module&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, pybind11::function, pybind11::tuple, pybind11::function, bool, pybind11::name, pybind11::is_method, pybind11::sibling&gt;(torch::jit::script::initJitScriptBindings(_object*)::$_13&amp;&amp;, void (*)(torch::jit::script::Module&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, pybind11::function, pybind11::tuple, pybind11::function, bool), pybind11::name const&amp;, pybind11::is_method const&amp;, pybind11::sibling const&amp;)::'lambda'(pybind11::detail::function_call&amp;)::__invoke(pybind11::detail::function_call&amp;) + 319 (0x11b814d9f in libtorch_python.dylib)<NewLine>frame #8: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 3372 (0x11b1479fc in libtorch_python.dylib)<NewLine>&lt;omitting python frames&gt;<NewLine>frame #13: __pyx_f_18_pydevd_frame_eval_35pydevd_frame_evaluator_darwin_37_64_get_bytecode_while_frame_eval + 6471 (0x11aa0cef7 in pydevd_frame_evaluator_darwin_37_64.cpython-37m-darwin.so)<NewLine>frame #18: __pyx_f_18_pydevd_frame_eval_35pydevd_frame_evaluator_darwin_37_64_get_bytecode_while_frame_eval + 6471 (0x11aa0cef7 in pydevd_frame_evaluator_darwin_37_64.cpython-37m-darwin.so)<NewLine>frame #23: __pyx_f_18_pydevd_frame_eval_35pydevd_frame_evaluator_darwin_37_64_get_bytecode_while_frame_eval + 6471 (0x11aa0cef7 in pydevd_frame_evaluator_darwin_37_64.cpython-37m-darwin.so)<NewLine>frame #27: __pyx_f_18_pydevd_frame_eval_35pydevd_frame_evaluator_darwin_37_64_get_bytecode_while_frame_eval + 6471 (0x11aa0cef7 in pydevd_frame_evaluator_darwin_37_64.cpython-37m-darwin.so)<NewLine>frame #35: __pyx_f_18_pydevd_frame_eval_35pydevd_frame_evaluator_darwin_37_64_get_bytecode_while_frame_eval + 6471 (0x11aa0cef7 in pydevd_frame_evaluator_darwin_37_64.cpython-37m-darwin.so)<NewLine>frame #40: __pyx_f_18_pydevd_frame_eval_35pydevd_frame_evaluator_darwin_37_64_get_bytecode_while_frame_eval + 6471 (0x11aa0cef7 in pydevd_frame_evaluator_darwin_37_64.cpython-37m-darwin.so)<NewLine>frame #57: start + 1 (0x7fff6bfa93d5 in libdyld.dylib)<NewLine>frame #58: 0x0 + 10 (0xa in ???)<NewLine></code></pre><NewLine><h3>Environment</h3><NewLine><pre><code class=""lang-python"">PyTorch version: 1.4.0<NewLine>Is debug build: No<NewLine>CUDA used to build PyTorch: None<NewLine><NewLine>OS: Mac OSX 10.14.6<NewLine>GCC version: Could not collect<NewLine>CMake version: Could not collect<NewLine><NewLine>Python version: 3.7<NewLine>Is CUDA available: No<NewLine>CUDA runtime version: No CUDA<NewLine>GPU models and configuration: No CUDA<NewLine>Nvidia driver version: No CUDA<NewLine>cuDNN version: No CUDA<NewLine><NewLine>Versions of relevant libraries:<NewLine>[pip3] numpy==1.18.2<NewLine>[pip3] torch==1.4.0<NewLine>[pip3] torchvision==0.5.0<NewLine>[conda] Could not collect<NewLine></code></pre><NewLine><h3>Repeat</h3><NewLine><pre><code class=""lang-python"">from collections import defaultdict<NewLine><NewLine>import torch<NewLine>import torch.jit<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>from torchvision import models<NewLine><NewLine>from utils import *<NewLine><NewLine>class MyConv2D(nn.Module):<NewLine>    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):<NewLine>        super(MyConv2D, self).__init__()<NewLine>        self.weight = torch.zeros((out_channels, in_channels, kernel_size, kernel_size))<NewLine>        self.bias = torch.zeros(out_channels)<NewLine><NewLine>        self.in_channels = in_channels<NewLine>        self.out_channels = out_channels<NewLine>        self.kernel_size = (kernel_size, kernel_size)<NewLine>        self.stride = (stride, stride)<NewLine><NewLine>    def forward(self, x):<NewLine>        return F.conv2d(x, self.weight, self.bias, self.stride)<NewLine><NewLine>    def extra_repr(self):<NewLine>        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'<NewLine>             ', stride={stride}')<NewLine>        return s.format(**self.__dict__)<NewLine><NewLine><NewLine>class ResidualBlock(nn.Module):<NewLine>    def __init__(self, channels):<NewLine>        super(ResidualBlock, self).__init__()<NewLine>        self.conv = nn.Sequential(<NewLine>            *ConvLayer(channels, channels, kernel_size=3, stride=1),<NewLine>            *ConvLayer(channels, channels, kernel_size=3, stride=1, relu=False)<NewLine>        )<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.conv(x) + x<NewLine><NewLine><NewLine>def ConvLayer(in_channels, out_channels, kernel_size=3, stride=1,<NewLine>              upsample=None, instance_norm=True, relu=True, trainable=False):<NewLine>    layers = []<NewLine>    if upsample:<NewLine>        layers.append(nn.Upsample(mode='nearest', scale_factor=upsample))<NewLine>    layers.append(nn.ReflectionPad2d(kernel_size // 2))<NewLine>    if trainable:<NewLine>        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride))<NewLine>    else:<NewLine>        layers.append(MyConv2D(in_channels, out_channels, kernel_size, stride))<NewLine>    if instance_norm:<NewLine>        layers.append(nn.InstanceNorm2d(out_channels))<NewLine>    if relu:<NewLine>        layers.append(nn.ReLU())<NewLine>    return layers<NewLine><NewLine><NewLine>class TransformNet(nn.Module):<NewLine>    def __init__(self, base=8):<NewLine>        super(TransformNet, self).__init__()<NewLine>        self.base = base<NewLine>        self.weights = []<NewLine>        self.downsampling = nn.Sequential(<NewLine>            *ConvLayer(3, base, kernel_size=9, trainable=True),<NewLine>            *ConvLayer(base, base * 2, kernel_size=3, stride=2),<NewLine>            *ConvLayer(base * 2, base * 4, kernel_size=3, stride=2),<NewLine>        )<NewLine>        self.residuals = nn.Sequential(*[ResidualBlock(base * 4) for _ in range(5)])<NewLine>        self.upsampling = nn.Sequential(<NewLine>            *ConvLayer(base * 4, base * 2, kernel_size=3, upsample=2),<NewLine>            *ConvLayer(base * 2, base, kernel_size=3, upsample=2),<NewLine>            *ConvLayer(base, 3, kernel_size=9, instance_norm=False, relu=False, trainable=True),<NewLine>        )<NewLine>        self.get_param_dict()<NewLine><NewLine>    def forward(self, X):<NewLine>        y = self.downsampling(X)<NewLine>        y = self.residuals(y)<NewLine>        y = self.upsampling(y)<NewLine>        return y<NewLine><NewLine>    def get_param_dict(self):<NewLine>        """"""找出该网络所有 MyConv2D 层，计算它们需要的权值数量""""""<NewLine>        param_dict = defaultdict(int)<NewLine>        for value in self.named_modules():<NewLine>            if isinstance(value[1], MyConv2D):<NewLine>                param_dict[value[0]] += int(np.prod(value[1].weight.shape))<NewLine>                param_dict[value[0]] += int(np.prod(value[1].bias.shape))<NewLine>        return param_dict<NewLine><NewLine>    def set_my_attr(self, name, value):<NewLine>        # 下面这个循环是一步步遍历类似 residuals.0.conv.1 的字符串，找到相应的权值<NewLine>        target = self<NewLine>        for x in name.split('.'):<NewLine>            if x.isnumeric():<NewLine>                target = target.__getitem__(int(x))<NewLine>            else:<NewLine>                target = getattr(target, x)<NewLine><NewLine>        # 设置对应的权值<NewLine>        n_weight = np.prod(target.weight.shape)<NewLine>        target.weight = value[:n_weight].view(target.weight.shape)<NewLine>        target.bias = value[n_weight:].view(target.bias.shape)<NewLine><NewLine>    def set_weights(self, weights, i=0):<NewLine>        """"""输入权值字典，对该网络所有的 MyConv2D 层设置权值""""""<NewLine>        for name, param in weights.items():<NewLine>            self.set_my_attr(name, weights[name][i])<NewLine><NewLine><NewLine>class MetaNet(nn.Module):<NewLine>    def __init__(self, param_dict):<NewLine>        super(MetaNet, self).__init__()<NewLine>        self.hidden = nn.Linear(1920, 128 * len(param_dict))<NewLine>        self.fc_list = []<NewLine>        for index, (name, params) in enumerate(param_dict.items()):<NewLine>            setattr(self, name, nn.Linear(128, params))<NewLine>            self.fc_list.append(name)<NewLine><NewLine>    def forward(self, mean_std_features):<NewLine>        hidden = F.relu(self.hidden(mean_std_features))<NewLine>        filters = []<NewLine>        labels = []<NewLine>        for index, name in enumerate(self.fc_list):<NewLine>            labels.append(torch.ones(1, dtype=torch.int8) * index)<NewLine>            filters.append(getattr(self, name)(hidden[:, index * 128:(index + 1) * 128]))<NewLine>        return tuple(filters)<NewLine><NewLine>def test_mobile():<NewLine>    transform_net = TransformNet(base=32).cpu().eval()<NewLine>    meta_net = MetaNet(transform_net.get_param_dict()).cpu().eval()<NewLine>    input_1 = torch.ones(1, 1920)<NewLine>    meta_net_mobile = torch.jit.trace(meta_net, input_1)<NewLine>    meta_net_mobile.save(""models/meta_net_mobile.pt"")<NewLine>    print(meta_net_mobile.code)<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    test_mobile()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Silbernitrat,,Silbernitrat,"March 28, 2020, 10:26am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just an additional info:<br/><NewLine>It executes fine with the torch version below:</p><NewLine><pre><code class=""lang-auto"">numpy==1.17.4<NewLine>numpydoc==0.9.1<NewLine>torch==1.3.1<NewLine>torchvision==0.4.2<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>yes, it works well in this environment.Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/InnovArul; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Silbernitrat; <NewLine> ,"REPLY_DATE 1: March 28, 2020, 11:43am; <NewLine> REPLY_DATE 2: March 28, 2020, 11:56am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
74455,Possible memory leak in torch.jit.trace,2020-03-26T14:54:19.800Z,2,155,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><p>Maybe I’m doing something wrong, but I’ve noticed a continuous increase in the memory usage when calling torch.jit.trace(model, inputs) multiple times in the same process.<br/><NewLine>I’m using the following test code:</p><NewLine><pre><code class=""lang-python""><NewLine>#Utilities <NewLine>import os<NewLine>import psutil<NewLine><NewLine>#JIT trace test<NewLine>import torch<NewLine>import torchvision.models as model_zoo<NewLine><NewLine>with torch.no_grad():<NewLine><NewLine>  #Create a simple resnet <NewLine>  model = model_zoo.resnet18()<NewLine>  model.eval<NewLine><NewLine>  #Create sample input<NewLine>  sample_input = torch.randn(size=[2, 3, 480, 640], requires_grad=False)<NewLine><NewLine>  #Get process id<NewLine>  process = psutil.Process(os.getpid())<NewLine><NewLine>  #Repeat tracing <NewLine>  for i in range(0, 1000):<NewLine>    tr_model = torch.jit.trace(model, sample_input, check_trace=False)<NewLine>    print(""Iter: {} = {}"".format(i, process.memory_full_info()))<NewLine><NewLine></code></pre><NewLine><p>I’m using PyTorch 1.4.0 installed with conda.<br/><NewLine>I get the same problem with check_trace=True and check_trace=False</p><NewLine></div>",https://discuss.pytorch.org/u/alepack,,alepack,"March 26, 2020,  3:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for reporting this issue!<br/><NewLine>How large is the leak for each iteration?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><p>Looking at the output of the script, it seems that there is not a constant increase in the memory usage.<br/><NewLine>For example, if I run the test above with check_trace=True for 80 iterations, I get the following output:</p><NewLine><pre><code class=""lang-auto"">Iter: 0 = pfullmem(rss=326701056, vms=729473024, shared=75661312, text=2265088, lib=0, data=278097920, dirty=0, uss=319164416, pss=320976896, swap=0)<NewLine>Iter: 1 = pfullmem(rss=334843904, vms=737370112, shared=76226560, text=2265088, lib=0, data=285995008, dirty=0, uss=326877184, pss=328689664, swap=0)<NewLine>Iter: 2 = pfullmem(rss=342360064, vms=744742912, shared=76226560, text=2265088, lib=0, data=293367808, dirty=0, uss=334262272, pss=336074752, swap=0)<NewLine>Iter: 3 = pfullmem(rss=379133952, vms=781746176, shared=76226560, text=2265088, lib=0, data=330371072, dirty=0, uss=371142656, pss=372955136, swap=0)<NewLine>Iter: 4 = pfullmem(rss=407904256, vms=810496000, shared=76226560, text=2265088, lib=0, data=359120896, dirty=0, uss=399900672, pss=401713152, swap=0)<NewLine>Iter: 5 = pfullmem(rss=417579008, vms=820097024, shared=76226560, text=2265088, lib=0, data=368721920, dirty=0, uss=409509888, pss=411322368, swap=0)<NewLine>Iter: 6 = pfullmem(rss=408051712, vms=810266624, shared=76226560, text=2265088, lib=0, data=358891520, dirty=0, uss=399818752, pss=401631232, swap=0)<NewLine>Iter: 7 = pfullmem(rss=427544576, vms=829927424, shared=76226560, text=2265088, lib=0, data=378552320, dirty=0, uss=419491840, pss=421304320, swap=0)<NewLine>Iter: 8 = pfullmem(rss=442355712, vms=844673024, shared=76226560, text=2265088, lib=0, data=393297920, dirty=0, uss=434245632, pss=436058112, swap=0)<NewLine>Iter: 9 = pfullmem(rss=449597440, vms=852045824, shared=76226560, text=2265088, lib=0, data=400670720, dirty=0, uss=441626624, pss=443439104, swap=0)<NewLine>Iter: 10 = pfullmem(rss=449785856, vms=852045824, shared=76226560, text=2265088, lib=0, data=400670720, dirty=0, uss=441634816, pss=443447296, swap=0)<NewLine>Iter: 11 = pfullmem(rss=457093120, vms=859418624, shared=76226560, text=2265088, lib=0, data=408043520, dirty=0, uss=449015808, pss=450828288, swap=0)<NewLine>Iter: 12 = pfullmem(rss=457015296, vms=859418624, shared=76226560, text=2265088, lib=0, data=408043520, dirty=0, uss=449028096, pss=450840576, swap=0)<NewLine>Iter: 13 = pfullmem(rss=496234496, vms=898740224, shared=76226560, text=2265088, lib=0, data=447365120, dirty=0, uss=488226816, pss=490039296, swap=0)<NewLine>Iter: 14 = pfullmem(rss=511221760, vms=913670144, shared=76226560, text=2265088, lib=0, data=462295040, dirty=0, uss=503164928, pss=504977408, swap=0)<NewLine>Iter: 15 = pfullmem(rss=511164416, vms=913670144, shared=76226560, text=2265088, lib=0, data=462295040, dirty=0, uss=503173120, pss=504985600, swap=0)<NewLine>Iter: 16 = pfullmem(rss=550715392, vms=952991744, shared=76226560, text=2265088, lib=0, data=501616640, dirty=0, uss=542507008, pss=544319488, swap=0)<NewLine>Iter: 17 = pfullmem(rss=550686720, vms=952991744, shared=76226560, text=2265088, lib=0, data=501616640, dirty=0, uss=542515200, pss=544327680, swap=0)<NewLine>Iter: 18 = pfullmem(rss=550268928, vms=952578048, shared=76226560, text=2265088, lib=0, data=501202944, dirty=0, uss=542109696, pss=543922176, swap=0)<NewLine>Iter: 19 = pfullmem(rss=559927296, vms=962408448, shared=76226560, text=2265088, lib=0, data=511033344, dirty=0, uss=551948288, pss=553760768, swap=0)<NewLine>Iter: 20 = pfullmem(rss=530780160, vms=932917248, shared=76226560, text=2265088, lib=0, data=481542144, dirty=0, uss=522661888, pss=524474368, swap=0)<NewLine>Iter: 21 = pfullmem(rss=530845696, vms=932917248, shared=76226560, text=2265088, lib=0, data=481542144, dirty=0, uss=522661888, pss=524474368, swap=0)<NewLine>Iter: 22 = pfullmem(rss=535318528, vms=937832448, shared=76226560, text=2265088, lib=0, data=486457344, dirty=0, uss=527446016, pss=529258496, swap=0)<NewLine>Iter: 23 = pfullmem(rss=550354944, vms=952578048, shared=76226560, text=2265088, lib=0, data=501202944, dirty=0, uss=542322688, pss=544135168, swap=0)<NewLine>Iter: 24 = pfullmem(rss=557764608, vms=959950848, shared=76226560, text=2265088, lib=0, data=508575744, dirty=0, uss=549695488, pss=551507968, swap=0)<NewLine>Iter: 25 = pfullmem(rss=565141504, vms=967323648, shared=76226560, text=2265088, lib=0, data=515948544, dirty=0, uss=557068288, pss=558880768, swap=0)<NewLine>Iter: 26 = pfullmem(rss=572645376, vms=974696448, shared=76226560, text=2265088, lib=0, data=523321344, dirty=0, uss=564441088, pss=566253568, swap=0)<NewLine>Iter: 27 = pfullmem(rss=579878912, vms=982069248, shared=76226560, text=2265088, lib=0, data=530694144, dirty=0, uss=571813888, pss=573626368, swap=0)<NewLine>Iter: 28 = pfullmem(rss=594636800, vms=997076992, shared=76226560, text=2265088, lib=0, data=545701888, dirty=0, uss=586567680, pss=588380160, swap=0)<NewLine>Iter: 29 = pfullmem(rss=602152960, vms=1004449792, shared=76226560, text=2265088, lib=0, data=553074688, dirty=0, uss=593948672, pss=595761152, swap=0)<NewLine>Iter: 30 = pfullmem(rss=641359872, vms=1043771392, shared=76226560, text=2265088, lib=0, data=592396288, dirty=0, uss=633151488, pss=634963968, swap=0)<NewLine>Iter: 31 = pfullmem(rss=616792064, vms=1019195392, shared=76226560, text=2265088, lib=0, data=567820288, dirty=0, uss=608706560, pss=610519040, swap=0)<NewLine>Iter: 32 = pfullmem(rss=624295936, vms=1026568192, shared=76226560, text=2265088, lib=0, data=575193088, dirty=0, uss=616083456, pss=617895936, swap=0)<NewLine>Iter: 33 = pfullmem(rss=641503232, vms=1043771392, shared=76226560, text=2265088, lib=0, data=592396288, dirty=0, uss=633294848, pss=635107328, swap=0)<NewLine>Iter: 34 = pfullmem(rss=725028864, vms=1127489536, shared=76226560, text=2265088, lib=0, data=676114432, dirty=0, uss=716894208, pss=718706688, swap=0)<NewLine>Iter: 35 = pfullmem(rss=725000192, vms=1127489536, shared=76226560, text=2265088, lib=0, data=676114432, dirty=0, uss=716902400, pss=718714880, swap=0)<NewLine>Iter: 36 = pfullmem(rss=724959232, vms=1127489536, shared=76226560, text=2265088, lib=0, data=676114432, dirty=0, uss=716910592, pss=718723072, swap=0)<NewLine>Iter: 37 = pfullmem(rss=695332864, vms=1097781248, shared=76226560, text=2265088, lib=0, data=646406144, dirty=0, uss=687341568, pss=689154048, swap=0)<NewLine>Iter: 38 = pfullmem(rss=714858496, vms=1117442048, shared=76226560, text=2265088, lib=0, data=666066944, dirty=0, uss=706879488, pss=708691968, swap=0)<NewLine>Iter: 39 = pfullmem(rss=695586816, vms=1097781248, shared=76226560, text=2265088, lib=0, data=646406144, dirty=0, uss=687362048, pss=689174528, swap=0)<NewLine>Iter: 40 = pfullmem(rss=761667584, vms=1164136448, shared=76226560, text=2265088, lib=0, data=712761344, dirty=0, uss=753594368, pss=755406848, swap=0)<NewLine>Iter: 41 = pfullmem(rss=751833088, vms=1154306048, shared=76226560, text=2265088, lib=0, data=702930944, dirty=0, uss=743772160, pss=745584640, swap=0)<NewLine>Iter: 42 = pfullmem(rss=751779840, vms=1154306048, shared=76226560, text=2265088, lib=0, data=702930944, dirty=0, uss=743780352, pss=745592832, swap=0)<NewLine>Iter: 43 = pfullmem(rss=732422144, vms=1134645248, shared=76226560, text=2265088, lib=0, data=683270144, dirty=0, uss=724258816, pss=726071296, swap=0)<NewLine>Iter: 44 = pfullmem(rss=737099776, vms=1139560448, shared=76226560, text=2265088, lib=0, data=688185344, dirty=0, uss=729055232, pss=730867712, swap=0)<NewLine>Iter: 45 = pfullmem(rss=751976448, vms=1154306048, shared=76226560, text=2265088, lib=0, data=702930944, dirty=0, uss=743940096, pss=745752576, swap=0)<NewLine>Iter: 46 = pfullmem(rss=761769984, vms=1164136448, shared=76226560, text=2265088, lib=0, data=712761344, dirty=0, uss=753778688, pss=755591168, swap=0)<NewLine>Iter: 47 = pfullmem(rss=781651968, vms=1183797248, shared=76226560, text=2265088, lib=0, data=732422144, dirty=0, uss=773447680, pss=775260160, swap=0)<NewLine>Iter: 48 = pfullmem(rss=789053440, vms=1191170048, shared=76226560, text=2265088, lib=0, data=739794944, dirty=0, uss=780832768, pss=782645248, swap=0)<NewLine>Iter: 49 = pfullmem(rss=788971520, vms=1191170048, shared=76226560, text=2265088, lib=0, data=739794944, dirty=0, uss=780840960, pss=782653440, swap=0)<NewLine>Iter: 50 = pfullmem(rss=796258304, vms=1198542848, shared=76226560, text=2265088, lib=0, data=747167744, dirty=0, uss=788221952, pss=790034432, swap=0)<NewLine>Iter: 51 = pfullmem(rss=811008000, vms=1213288448, shared=76226560, text=2265088, lib=0, data=761913344, dirty=0, uss=802975744, pss=804788224, swap=0)<NewLine>Iter: 52 = pfullmem(rss=818520064, vms=1220661248, shared=76226560, text=2265088, lib=0, data=769286144, dirty=0, uss=810356736, pss=812169216, swap=0)<NewLine>Iter: 53 = pfullmem(rss=825761792, vms=1228034048, shared=76226560, text=2265088, lib=0, data=776658944, dirty=0, uss=817741824, pss=819554304, swap=0)<NewLine>Iter: 54 = pfullmem(rss=833257472, vms=1235406848, shared=76226560, text=2265088, lib=0, data=784031744, dirty=0, uss=825122816, pss=826934272, swap=0)<NewLine>Iter: 55 = pfullmem(rss=840499200, vms=1242779648, shared=76226560, text=2265088, lib=0, data=791404544, dirty=0, uss=832503808, pss=834316288, swap=0)<NewLine>Iter: 56 = pfullmem(rss=848011264, vms=1250152448, shared=76226560, text=2265088, lib=0, data=798777344, dirty=0, uss=839884800, pss=841694208, swap=0)<NewLine>Iter: 57 = pfullmem(rss=855269376, vms=1257525248, shared=76226560, text=2265088, lib=0, data=806150144, dirty=0, uss=847265792, pss=849075200, swap=0)<NewLine>Iter: 58 = pfullmem(rss=862777344, vms=1264898048, shared=76226560, text=2265088, lib=0, data=813522944, dirty=0, uss=854642688, pss=856452096, swap=0)<NewLine>Iter: 59 = pfullmem(rss=933801984, vms=1336430592, shared=76226560, text=2265088, lib=0, data=885055488, dirty=0, uss=925798400, pss=927607808, swap=0)<NewLine>Iter: 60 = pfullmem(rss=933892096, vms=1336430592, shared=76226560, text=2265088, lib=0, data=885055488, dirty=0, uss=925806592, pss=927616000, swap=0)<NewLine>Iter: 61 = pfullmem(rss=933851136, vms=1336430592, shared=76226560, text=2265088, lib=0, data=885055488, dirty=0, uss=925814784, pss=927624192, swap=0)<NewLine>Iter: 62 = pfullmem(rss=900866048, vms=1303351296, shared=76226560, text=2265088, lib=0, data=851976192, dirty=0, uss=892878848, pss=894691328, swap=0)<NewLine>Iter: 63 = pfullmem(rss=905740288, vms=1308266496, shared=76226560, text=2265088, lib=0, data=856891392, dirty=0, uss=897671168, pss=899483648, swap=0)<NewLine>Iter: 64 = pfullmem(rss=927948800, vms=1330384896, shared=76226560, text=2265088, lib=0, data=879009792, dirty=0, uss=919920640, pss=921733120, swap=0)<NewLine>Iter: 65 = pfullmem(rss=935456768, vms=1337757696, shared=76226560, text=2265088, lib=0, data=886382592, dirty=0, uss=927301632, pss=929114112, swap=0)<NewLine>Iter: 66 = pfullmem(rss=942710784, vms=1345130496, shared=76226560, text=2265088, lib=0, data=893755392, dirty=0, uss=934686720, pss=936499200, swap=0)<NewLine>Iter: 67 = pfullmem(rss=950222848, vms=1352503296, shared=76226560, text=2265088, lib=0, data=901128192, dirty=0, uss=942067712, pss=943880192, swap=0)<NewLine>Iter: 68 = pfullmem(rss=957468672, vms=1359876096, shared=76226560, text=2265088, lib=0, data=908500992, dirty=0, uss=949448704, pss=951261184, swap=0)<NewLine>Iter: 69 = pfullmem(rss=972296192, vms=1374621696, shared=76226560, text=2265088, lib=0, data=923246592, dirty=0, uss=964202496, pss=966014976, swap=0)<NewLine>Iter: 70 = pfullmem(rss=987045888, vms=1389367296, shared=76226560, text=2265088, lib=0, data=937992192, dirty=0, uss=978956288, pss=980768768, swap=0)<NewLine>Iter: 71 = pfullmem(rss=994557952, vms=1396740096, shared=76226560, text=2265088, lib=0, data=945364992, dirty=0, uss=986341376, pss=988153856, swap=0)<NewLine>Iter: 72 = pfullmem(rss=1001803776, vms=1404112896, shared=76226560, text=2265088, lib=0, data=952737792, dirty=0, uss=993722368, pss=995534848, swap=0)<NewLine>Iter: 73 = pfullmem(rss=1033687040, vms=1436061696, shared=76226560, text=2265088, lib=0, data=984686592, dirty=0, uss=1025548288, pss=1027360768, swap=0)<NewLine>Iter: 74 = pfullmem(rss=1009147904, vms=1411485696, shared=76226560, text=2265088, lib=0, data=960110592, dirty=0, uss=1001111552, pss=1002924032, swap=0)<NewLine>Iter: 75 = pfullmem(rss=1016659968, vms=1418858496, shared=76226560, text=2265088, lib=0, data=967483392, dirty=0, uss=1008496640, pss=1010309120, swap=0)<NewLine>Iter: 76 = pfullmem(rss=1023901696, vms=1426231296, shared=76226560, text=2265088, lib=0, data=974856192, dirty=0, uss=1015877632, pss=1017690112, swap=0)<NewLine>Iter: 77 = pfullmem(rss=1038667776, vms=1440976896, shared=76226560, text=2265088, lib=0, data=989601792, dirty=0, uss=1030631424, pss=1032443904, swap=0)<NewLine>Iter: 78 = pfullmem(rss=1046183936, vms=1448349696, shared=76226560, text=2265088, lib=0, data=996974592, dirty=0, uss=1038012416, pss=1039824896, swap=0)<NewLine>Iter: 79 = pfullmem(rss=1085386752, vms=1487671296, shared=76226560, text=2265088, lib=0, data=1036296192, dirty=0, uss=1077211136, pss=1079023616, swap=0)<NewLine>Iter: 80 = pfullmem(rss=1109991424, vms=1512247296, shared=76226560, text=2265088, lib=0, data=1060872192, dirty=0, uss=1101799424, pss=1103611904, swap=0)<NewLine></code></pre><NewLine><p>As you can see it starts from ~ 300MB and reaches 1.1GB.<br/><NewLine>If it helps, I’ve noticed that, If I set check_trace=False, the memory usage increases more slowly and it takes more iterations to reach 1GB</p><NewLine><p>Thanks</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for this information.<br/><NewLine>We’ll try to reproduce it.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I can reproduce it with the latest master build and have created a bug <a href=""https://github.com/pytorch/pytorch/issues/35600"">here</a>.<br/><NewLine>Thanks again for reporting this issue here. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/alepack; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: March 27, 2020,  5:50am; <NewLine> REPLY_DATE 2: March 27, 2020,  8:14am; <NewLine> REPLY_DATE 3: March 27, 2020,  8:23am; <NewLine> REPLY_DATE 4: March 28, 2020, 10:47am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
74346,Result of trace() and script() produce the same output while they should be different,2020-03-25T13:43:20.066Z,1,106,"<div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>Traced module</li><NewLine></ol><NewLine><pre><code class=""lang-python"">torch.manual_seed(25)<NewLine>module = torch.jit.trace(net, torch.randn(1,3,5,5))<NewLine>print(module.code)<NewLine>module(torch.randn(1,3,5,5))[0, 0, 0]<NewLine></code></pre><NewLine><p>And the output</p><NewLine><pre><code class=""lang-python""># module.code<NewLine>def forward(self,<NewLine>    input: Tensor) -&gt; Tensor:<NewLine>  _0 = getattr(self.convs, ""1"")<NewLine>  _1 = (getattr(self.convs, ""0"")).forward(input, )<NewLine>  _2 = getattr(self.residuals, ""0"")<NewLine>  _3 = (getattr(self.convs, ""2"")).forward((_0).forward(_1, ), )<NewLine>  _4 = getattr(self.residuals, ""2"")<NewLine>  _5 = (getattr(self.residuals, ""1"")).forward((_2).forward(_3, ), )<NewLine>  _6 = getattr(self.upsample_convs, ""1"")<NewLine>  _7 = (getattr(self.upsample_convs, ""0"")).forward((_4).forward(_5, ), )<NewLine>  _8 = (self.final_conv).forward((_6).forward(_7, ), )<NewLine>  return torch.sigmoid(_8)<NewLine># Output<NewLine>tensor([0.3275, 0.4366, 0.2979, 0.2537, 0.4489, 0.4663, 0.4455, 0.4363],<NewLine>       grad_fn=&lt;SelectBackward&gt;)<NewLine></code></pre><NewLine><ol start=""2""><NewLine><li>Scripted module: I only changed <code>torch.jit.trace()</code> to <code>torch.jit.script()</code><NewLine></li><NewLine></ol><NewLine><pre><code class=""lang-python""># module.code<NewLine>def forward(self,<NewLine>    x: Tensor) -&gt; Tensor:<NewLine>  _0 = self.convs<NewLine>  _1 = getattr(_0, ""0"")<NewLine>  _2 = getattr(_0, ""1"")<NewLine>  _3 = getattr(_0, ""2"")<NewLine>  x0 = (_1).forward(x, )<NewLine>  x1 = (_2).forward(x0, )<NewLine>  x2 = (_3).forward(x1, )<NewLine>  _4 = self.residuals<NewLine>  _5 = getattr(_4, ""0"")<NewLine>  _6 = getattr(_4, ""1"")<NewLine>  _7 = getattr(_4, ""2"")<NewLine>  x3 = (_5).forward(x2, )<NewLine>  x4 = (_6).forward(x3, )<NewLine>  x5 = (_7).forward(x4, )<NewLine>  _8 = self.upsample_convs<NewLine>  _9 = getattr(_8, ""0"")<NewLine>  _10 = getattr(_8, ""1"")<NewLine>  x6 = (_9).forward(x5, )<NewLine>  x7 = (_10).forward(x6, )<NewLine>  x8 = (self.final_conv).forward(x7, )<NewLine>  return torch.sigmoid(x8)<NewLine># Output<NewLine>tensor([0.3275, 0.4366, 0.2979, 0.2537, 0.4489, 0.4663, 0.4455, 0.4363],<NewLine>       grad_fn=&lt;SelectBackward&gt;)<NewLine></code></pre><NewLine><p>You can see the results <code>module.code</code> is totally different, as my <code>forward()</code> contains loops, but the module’s output is exactly the same.<br/><NewLine>Why is this the case? Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/minhduc0711,,minhduc0711,"March 26, 2020,  8:22am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The two don’t necessarily produce the same IR (which is what you’re seeing, it corresponds directly to <code>module.graph</code>) since tracing only records tensor operations as they occur, while scripting will compile the code itself. Tracing will unroll loops based on your example input, so this is probably why you’re seeing the same results.</p><NewLine><p>But if your model has no data dependent control flow (meaning that tracing will generate correct results), the output from the two will be identical (and it will also match the output from eager (no jit) PyTorch)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>you’re right, my model has no data dependent control flow, as I was only looping over a <code>nn.ModuleList</code></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/minhduc0711; <NewLine> ,"REPLY_DATE 1: March 26, 2020,  9:12am; <NewLine> REPLY_DATE 2: March 26, 2020,  9:18am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
42282,TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect&hellip;This means that the trace might not generalize to other inputs!,2019-04-11T12:23:01.313Z,3,2829,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to create trace file. Related code part is here:</p><NewLine><pre><code>def center_crop(self, layer, target_size):<NewLine>    _, _, layer_height, layer_width = layer.size()<NewLine><NewLine>    diff_y = (layer_height - target_size[0]) // 2<NewLine>    diff_x = (layer_width - target_size[1]) // 2<NewLine>    return layer[:, :, diff_y:(diff_y + target_size[0]), diff_x:(diff_x + target_size[1])]<NewLine><NewLine>def forward(self, x, bridge):<NewLine><NewLine>    up = self.up(x)<NewLine>    crop1 = self.center_crop(bridge, up.shape[2:])<NewLine>    out = torch.cat([up, crop1], 1)<NewLine>    out = self.conv_block(out)<NewLine></code></pre><NewLine><p>It gives the warning at this line:</p><NewLine><blockquote><NewLine><p>return layer[:, :, diff_y:(diff_y + target_size[0]), diff_x:(diff_x + target_size[1])]</p><NewLine></blockquote><NewLine><p>Then, I loaded this trace file in C++ and made prediction for a test image. Then I compared the result with the output in python. I observed that results are very different. I guess that the reason originates from that warning.</p><NewLine></div>",https://discuss.pytorch.org/u/sercan,(sercan),sercan,"April 11, 2019, 12:23pm",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The tracer uses the example inputs you provide and records the operations. If a different input would result in a different operation, then this will not be captured by the tracer. Here it’s recording the arguments to the slice operation as constants (so it will always slice it with the values produced with the example inputs).</p><NewLine><p>To get around this, put any code with data-dependent control flow inside a <a href=""https://pytorch.org/docs/stable/jit.html#creating-torchscript-code"" rel=""nofollow noopener""><code>ScriptModule</code></a>, and then call that in your traced code.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/driazati"">@driazati</a>  I added a helper function and wrote this code:</p><NewLine><pre><code>@torch.jit.script<NewLine>def center_slice_helper(layer, diff_y, diff_x, h_end, w_end):<NewLine>   return layer[:, :, diff_y:h_end, diff_x:w_end]<NewLine><NewLine>def center_crop(self, layer, target_size):<NewLine>    #_, _, layer_height, layer_width = layer.size()<NewLine><NewLine>    diff_y = (layer.shape[2] - target_size.shape[2]) // 2<NewLine>    diff_x = (layer.shape[3] - target_size.shape[3]) // 2<NewLine><NewLine>    h_end = diff_y + target_size.shape[2]<NewLine>    w_end = diff_x + target_size.shape[3]<NewLine><NewLine>    return center_slice_helper(layer, diff_y, diff_x, h_end, w_end)<NewLine><NewLine><NewLine>def forward(self, x, bridge):<NewLine><NewLine>    up = self.up(x)<NewLine>    crop1 = self.center_crop(bridge, up)<NewLine>    out = torch.cat([up, crop1], 1)<NewLine>    out = self.conv_block(out)<NewLine><NewLine>    return out<NewLine></code></pre><NewLine><p>At this time, it gave this exception:</p><NewLine><pre><code class=""lang-auto"">torch.jit.TracingCheckError: Tracing failed sanity checks!<NewLine>Encountered an exception while running the Python function with test inputs.<NewLine>Exception:<NewLine>	center_slice_helper() expected value of type Tensor for argument 'diff_y' in position 1, but instead got value of type int.<NewLine>	Value: 0<NewLine>	Declaration: center_slice_helper(Tensor layer, Tensor diff_y, Tensor diff_x, Tensor h_end, Tensor w_end) -&gt; Tensor<NewLine></code></pre><NewLine><p>I debugged the code and observed that x parameter of forward function has this type:</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/0b5021d014ea169b8f4bab7344c8e58d542045be"" href=""https://discuss.pytorch.org/uploads/default/original/2X/0/0b5021d014ea169b8f4bab7344c8e58d542045be.png"" title=""x_parameter.PNG""><img alt=""x_parameter"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/0/0b5021d014ea169b8f4bab7344c8e58d542045be_2_10x10.png"" height=""288"" src=""https://discuss.pytorch.org/uploads/default/original/2X/0/0b5021d014ea169b8f4bab7344c8e58d542045be.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">x_parameter.PNG</span><span class=""informations"">932×390 8.66 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>How to handle this?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/driazati"">@driazati</a> I have read the related part in documentation: <a href=""https://pytorch.org/docs/stable/jit.html#mixing-tracing-and-scripting"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/jit.html#mixing-tracing-and-scripting</a></p><NewLine><p>I understood that I should use MyPy-style type annotation. What is your opinion?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Our support for non-tensor types in tracing is pretty limited (see <a href=""https://github.com/pytorch/pytorch/issues/14455"" rel=""nofollow noopener"">#14455</a>), but your model looks like using script mode would work well. See something like:</p><NewLine><pre><code class=""lang-python"">@torch.jit.script<NewLine>def center_slice_helper(layer, diff_y, diff_x, h_end, w_end):<NewLine>    # type: (Tensor, int, int, int, int) -&gt; Tensor<NewLine>    return layer[:, :, diff_y:h_end, diff_x:w_end]<NewLine><NewLine>class M(torch.jit.ScriptModule):<NewLine>    @torch.jit.script_method<NewLine>    def center_crop(self, layer, target_size):<NewLine>        #_, _, layer_height, layer_width = layer.size()<NewLine><NewLine>        diff_y = (layer.shape[2] - target_size.shape[2]) // 2<NewLine>        diff_x = (layer.shape[3] - target_size.shape[3]) // 2<NewLine><NewLine>        h_end = diff_y + target_size.shape[2]<NewLine>        w_end = diff_x + target_size.shape[3]<NewLine><NewLine>        return center_slice_helper(layer, diff_y, diff_x, h_end, w_end)<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def forward(self, x, bridge):<NewLine>        crop1 = self.center_crop(bridge, x)<NewLine>        return crop1<NewLine><NewLine>m = M()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello，I’ve been having this problem lately，How did you solve it？</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/sercan; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/sercan; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/111257; <NewLine> ,"REPLY_DATE 1: April 11, 2019,  8:25pm; <NewLine> REPLY_DATE 2: April 16, 2019,  6:58am; <NewLine> REPLY_DATE 3: April 17, 2019,  8:43am; <NewLine> REPLY_DATE 4: April 17, 2019,  6:09pm; <NewLine> REPLY_DATE 5: March 24, 2020, 12:59am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
74018,Spconv with torchscript,2020-03-22T10:13:33.571Z,0,110,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m having trouble using torchscript with my model which uses spconv. I need spconv, since I’m working with large sparse matrices. However,</p><NewLine><pre><code class=""lang-auto"">traced_script_module= torch.jit.trace(model, example)<NewLine></code></pre><NewLine><p>throws the following error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Found an unsupported argument type c10::List&lt;at::Tensor&gt; in the JIT tracer.<NewLine></code></pre><NewLine><p>I’m also getting a lot of transformation errors like:</p><NewLine><pre><code class=""lang-auto"">TracerWarning: Converting a tensor to a NumPy array might cause the trace to be incorrect.<NewLine></code></pre><NewLine><p>Any Idea what could cause this?</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Jakov_Gaspar,(Jakov Gaspar),Jakov_Gaspar,"March 22, 2020, 10:13am",,,,,
73708,Does TorchScript C++ use online JIT?,2020-03-19T04:00:58.317Z,2,108,"<div class=""post"" itemprop=""articleBody""><NewLine><p>This is just to clarify my understanding of JIT.</p><NewLine><p>When we talk about PyTorch JIT - is the translator that generates TorchScript IR a JIT itself ? And when we execute the TorchScript (using libtorch C++ library) is there an another JIT that is interpreting TorchScript IR and generating code that calls libtorch APIs and dispatches the calls ? If so, should we expect this online JIT to have a runtime overhead in terms of cpu cycles and memory ?</p><NewLine><p>Thanks,<br/><NewLine>Yetanadur</p><NewLine></div>",https://discuss.pytorch.org/u/yetanadur,(Ananth Durbha),yetanadur,"March 19, 2020,  4:00am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/yetanadur"">@yetanadur</a>,</p><NewLine><p>No, IR generation is a fully ahead-of-time process. JIT compilation is used to recover information that is potentially dynamic, such as tensor shapes, that can be used to make better optimization decisions during runtime. This all happens entirely after IR generation, though. Please let us know if you have further questions.</p><NewLine><p>James</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks James.</p><NewLine><p>Where can I find the entry point function (in source code) for JIT ? And which function has the code to jump to the JIT’d (optimized) code ? Basically want to step thru with a debugger.</p><NewLine><p>Thanks a bunch,<br/><NewLine>Yetanadur</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is true for 1.5, but isn’t true on master, and won’t be true for the upcoming release.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/James_Reed; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/yetanadur; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/eellison; <NewLine> ,"REPLY_DATE 1: March 19, 2020,  4:46am; <NewLine> REPLY_DATE 2: March 19, 2020,  5:47am; <NewLine> REPLY_DATE 3: March 20, 2020, 11:18pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
73794,Pytorch jit script error,2020-03-19T18:51:43.188Z,1,118,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi I’m trying to export my code to jit script, but come across the following error:<br/><NewLine>RuntimeError: inputs_.size() == 1 INTERNAL ASSERT FAILED at /pytorch/torch/csrc/jit/ir.h:364, please report a bug to PyTorch.  (input at /pytorch/torch/csrc/jit/ir.h:364)<br/><NewLine>frame <span class=""hashtag"">#0:</span> c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x33 (0x7f193351b813 in /job/.local/lib/python3.7/site-packages/torch/lib/libc10.so)<br/><NewLine>frame <span class=""hashtag"">#1:</span>  + 0x51af79 (0x7f19364baf79 in /job/.local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)<br/><NewLine>frame <span class=""hashtag"">#2:</span>  + 0x516db5 (0x7f19364b6db5 in /job/.local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)<br/><NewLine>frame <span class=""hashtag"">#3:</span> torch::jit::PeepholeOptimizeONNX(std::shared_ptr<a>torch::jit::Graph</a>&amp;, int, bool) + 0xcb (0x7f19364baa5b in /job/.local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)<br/><NewLine>frame <span class=""hashtag"">#4:</span>  + 0x4cfbd5 (0x7f193646fbd5 in /job/.local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)<br/><NewLine>frame <span class=""hashtag"">#5:</span>  + 0x210ba4 (0x7f19361b0ba4 in /job/.local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)<br/><NewLine><br/><NewLine>frame <span class=""hashtag"">#32:</span> __libc_start_main + 0xe7 (0x7f193a3dab97 in /lib/x86_64-linux-gnu/libc.so.6)</p><NewLine><p>Is there any clue on what problem it is? Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/shy,(shy),shy,"March 19, 2020,  6:51pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>It looks quite bad yes.<br/><NewLine>Can you share a small code sample that reproduces this issue?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://github.com/kmkurn/pytorch-crf/blob/1b7ced20c01352ccf70b7ae7bb99a58366de8e48/torchcrf/__init__.py#L259"" rel=""nofollow noopener"">https://github.com/kmkurn/pytorch-crf/blob/1b7ced20c01352ccf70b7ae7bb99a58366de8e48/torchcrf/<strong>init</strong>.py#L259</a></p><NewLine><p>It’s pretty deal with this function, i move out this function as a non-class function and add <span class=""mention"">@torch.jit.script</span></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/shy"">@shy</a>,</p><NewLine><p>This looks like an issue with some precondition in ONNX export. Do you mind <a href=""https://github.com/pytorch/pytorch/issues/new/choose"" rel=""nofollow noopener"">filing an issue</a> on github so we can route this to the right developers?</p><NewLine><p>Thanks,<br/><NewLine>James</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/shy; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/James_Reed; <NewLine> ,"REPLY_DATE 1: March 19, 2020,  6:56pm; <NewLine> REPLY_DATE 2: March 19, 2020,  7:01pm; <NewLine> REPLY_DATE 3: March 19, 2020,  7:06pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
72984,Load the model and tensor in C++ (libtorch) for inference,2020-03-12T11:37:20.112Z,3,150,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m new to C++ frontend. Here is my current code</p><NewLine><pre><code class=""lang-auto"">#include &lt;torch/script.h&gt; <NewLine><NewLine>#include &lt;iostream&gt;<NewLine>#include &lt;memory&gt;<NewLine><NewLine>int main(int argc, const char* argv[]) {<NewLine>  <NewLine>  torch::jit::script::Module module;<NewLine>  try {<NewLine>     module = torch::jit::load(argv[1]);<NewLine>  }<NewLine>  catch (const c10::Error&amp; e) {<NewLine>    std::cerr &lt;&lt; ""error loading the model\n"";<NewLine>    return -1;<NewLine>  }<NewLine>  <NewLine>    std::vector&lt;torch::Tensor&gt; sample_input;<NewLine>    torch::load(sample_input, argv[2]);<NewLine>    <NewLine>    std::cout &lt;&lt; ""Loaded Successfully\n"";<NewLine>    <NewLine>    std::vector&lt;torch::jit::IValue&gt; inputs;<NewLine>    inputs.push_back(sample_input);<NewLine><NewLine>    at::Tensor output = module.forward(inputs).toTensor();<NewLine>    std::cout &lt;&lt; output &lt;&lt; '\n';<NewLine>    <NewLine>}<NewLine></code></pre><NewLine><p>I want to run the executable as</p><NewLine><pre><code class=""lang-auto"">./example-app ../model.pt ../sample_input.pth<NewLine></code></pre><NewLine><p>The code doesn’t compile (<code>make</code>) .<br/><NewLine>In the documentation, I saw usage of <code>torch::load</code> but during <code>make</code> I noticed the error message <code> ‘load’ is not a member of ‘torch’</code>.<br/><NewLine>Also the inputs would not append the tensor, not sure what <code>IValue</code> is.</p><NewLine><p>Could you please provide the fix?</p><NewLine></div>",https://discuss.pytorch.org/u/kl_divergence,,kl_divergence,"March 12, 2020,  2:30pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You probably need to <code>#include &lt;torch/all.h&gt;</code> for it to pick up <code>torch::load</code>.</p><NewLine><p><a href=""https://github.com/pytorch/pytorch/issues/20356#issuecomment-567663701"">This comment</a> has more info about the save/load APIs.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: March 12, 2020,  5:43pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
73081,TorchScript function slower than non-JIT?,2020-03-13T06:45:50.290Z,0,77,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a simple function (LSTM layer) that I’m converting to TorchScript and executing. From some initial experiments, it looks like the JIT version runs slower than the non-JIT version on both CPU and GPU. The relevant code is listed here: <a href=""https://github.com/lmnt-com/haste/blob/master/frameworks/pytorch/lstm.py#L31-L64"" rel=""nofollow noopener"">https://github.com/lmnt-com/haste/blob/master/frameworks/pytorch/lstm.py#L31-L64</a>. Is this expected behavior?</p><NewLine></div>",https://discuss.pytorch.org/u/sharvil,(Sharvil Nanavati),sharvil,"March 13, 2020,  6:45am",,,,,
65428,Maxpool2d doesn&rsquo;t work in TorchScript after quantization?,2019-12-31T03:38:30.586Z,3,356,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I saved the quantized model to TorchScript successfully, but when I run the TorchScript model, I met the following problem:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""ocr_quant.py"", line 293, in  &lt;module&gt;<NewLine>    js_out = ts(x)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>RuntimeError: Didn't find kernel to dispatch to for operator 'aten::max_pool2d_with_indices'. Tried to look up kernel for dispatch key 'QuantizedCPUTensorId'. Registered dispatch keys are: [CUDATensorId, CPUTensorId, VariableTensorId]<NewLine>The above operation failed in interpreter, with the following stack trace:<NewLine>at &lt;string&gt;:63:30<NewLine><NewLine>            return torch.avg_pool2d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override), backward<NewLine><NewLine>        def max_pool2d(self,<NewLine>                       kernel_size: List[int],<NewLine>                       stride: List[int],<NewLine>                       padding: List[int],<NewLine>                       dilation: List[int],<NewLine>                       ceil_mode: bool):<NewLine>            output, indices = torch.max_pool2d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode)<NewLine>                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>            def backward(grad_output):<NewLine>                grad_self = torch.max_pool2d_with_indices_backward(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices)<NewLine>                return grad_self, None, None, None, None, None<NewLine>            return output, backward<NewLine><NewLine>        def max_pool2d_with_indices(self,<NewLine>                                    kernel_size: List[int],<NewLine>                                    stride: List[int],<NewLine>                                    padding: List[int],<NewLine><NewLine>The above operation failed in interpreter, with the following stack trace:<NewLine></code></pre><NewLine><p>Here’s my code for quantization:</p><NewLine><pre><code class=""lang-auto"">    crnn.cnn.fuse_model() <NewLine><NewLine>    img = Image.open(""3.png"").convert(""L"").resize((300,32))<NewLine>    x = ToTensor()(img).unsqueeze(0)<NewLine>    out = crnn(x)<NewLine>      <NewLine>    crnn.cnn.qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine>    torch.quantization.prepare(crnn.cnn,inplace=True)<NewLine><NewLine>    #  calibration<NewLine>    crnn(x)<NewLine><NewLine>    # quantize<NewLine>    torch.quantization.convert(crnn.cnn, inplace=True)<NewLine>    torch.quantization.quantize_dynamic(crnn.rnn,{nn.LSTM,nn.Linear},dtype=torch.qint8,inplace=True)<NewLine><NewLine>    quantized_out = crnn(x)<NewLine><NewLine>    ts = torch.jit.script(crnn)<NewLine>    ts_out = ts(x)<NewLine></code></pre><NewLine><p>The error occurs when the TorchScript model <code>ts</code> is called.</p><NewLine><p>And here’s my code for crnn.cnn</p><NewLine><pre><code class=""lang-auto"">class ConvBNReLU(nn.Sequential):<NewLine>    def __init__(self,i,in_channels,out_channels,kernel_size=3,stride = 1,padding = 1,groups = 1,bn = False):<NewLine>        self.bn = bn<NewLine>        if bn:<NewLine>            super(ConvBNReLU,self).__init__(OrderedDict([<NewLine>                (""conv{0}"".format(i),nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding,groups = groups,bias = True)),<NewLine>                (""batchnorm{0}"".format(i),nn.BatchNorm2d(out_channels)),<NewLine>                (""relu{0}"".format(i),nn.ReLU(inplace=False))<NewLine>            ]))<NewLine>        else:<NewLine>            super(ConvBNReLU,self).__init__(OrderedDict([<NewLine>                (""conv{0}"".format(i),nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding,groups = groups,bias = True)),<NewLine>                (""relu{0}"".format(i),nn.ReLU(inplace=False))<NewLine>            ]))<NewLine><NewLine><NewLine>class CNN(nn.Module):<NewLine><NewLine>    def __init__(self, imgH, nc):<NewLine>       <NewLine>        super(CNN, self).__init__()<NewLine>        assert imgH % 16 == 0, 'imgH has to be a multiple of 16'<NewLine><NewLine>        ks = [3, 3, 3, 3, 3, 3, 2]<NewLine>        ps = [1, 1, 1, 1, 1, 1, 0]<NewLine>        ss = [1, 1, 1, 1, 1, 1, 1]<NewLine>        nm = [64, 128, 256, 256, 512, 512, 512]<NewLine><NewLine>        self.cnn = nn.Sequential()<NewLine>        pn = 0<NewLine>        for i in range(7):<NewLine>            nIn = nc if i == 0 else nm[i - 1]<NewLine>            nOut = nm[i]<NewLine><NewLine>            if i in [0,1,3,5]:<NewLine>                self.cnn.add_module(""ConvBNReLU{}"".format(i),ConvBNReLU(i,nIn,nOut,ks[i],ss[i],ps[i],bn=False))<NewLine>                if i in [0,1]:<NewLine>                    self.cnn.add_module(""pooling{0}"".format(pn),nn.MaxPool2d(2,2))<NewLine>                else:<NewLine>                    self.cnn.add_module(""pooling{0}"".format(pn),nn.MaxPool2d((2,2),(2,1),(0,1)))<NewLine>                pn += 1 <NewLine>            else:<NewLine>                self.cnn.add_module(""ConvBNReLU{}"".format(i),ConvBNReLU(i,nIn,nOut,ks[i],ss[i],ps[i],bn=True))<NewLine><NewLine>        self.quant = QuantStub()<NewLine>        self.dequant = DeQuantStub()<NewLine><NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine>        x = self.cnn(x)<NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine></code></pre><NewLine><p>My PyTorch version is 1.3.1+cpu.</p><NewLine><p>How can I solve this problem? Looking forward to your replay.</p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"December 31, 2019,  9:20am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The input tensor of CRNN is cropped from an tensor of image, when I convert the tensor to PIL image and convert it back, the code works well.<br/><NewLine><code>img_out = ToTensor()(ToPILImage()(img_out.squeeze(0))).unsqueeze(0)</code></p><NewLine><p>and the following code also helps:</p><NewLine><pre><code class=""lang-auto"">img_out = torch.ones(img_out_.shape)<NewLine>img_out.data = img_out_.clone()<NewLine></code></pre><NewLine><p>Could anyone tell me the differences between cropped tensor and tensor converted from image?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/dalalaa"">@dalalaa</a>, that sounds like a bug. Do you mind raising an issue on github ?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply, I will try to find an minimum code to reproduce it.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I know this is late but for others visiting this with the same error I found that wrapping the <code>ts(x)</code> in a <code>torch.no_grad()</code> block solved the error</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dalalaa; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/dalalaa; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ahs8w; <NewLine> ,"REPLY_DATE 1: January 1, 2020,  6:19am; <NewLine> REPLY_DATE 2: January 3, 2020, 12:38am; <NewLine> REPLY_DATE 3: January 6, 2020,  3:35am; <NewLine> REPLY_DATE 4: March 12, 2020, 10:20pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
72663,Out of CUDA memory while tracing a model with jit.trace(),2020-03-10T04:53:18.597Z,1,97,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I got “out of memory” when I tried to trace a model with jit.<br/><NewLine>What is interesting is that when I run the model in the test mode it works.</p><NewLine><p>What is the difference between testing and tracing?</p><NewLine></div>",https://discuss.pytorch.org/u/ldw,,ldw,"March 10, 2020,  4:53am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>When you say “test mode”, do you mean calling <code>eval()</code> and then <code>forward()</code>? There are minor differences; certain <code>Module</code>s have different behavior in <code>eval</code> vs. <code>training</code> mode. In addition, if during testing you aren’t recording gradients (e.g. you set <code>requires_grad</code> to off), you don’t have to build up an autograd graph and keep the bookkeeping structures around.</p><NewLine><p>Keep in mind that tracing will actually execute your model in order to trace it. When you run the model forward on the example inputs manually, does it still oom?</p><NewLine><p>It would be helpful to have a script that we can use to reproduce the issue, otherwise there isn’t a ton of information to go off of.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>With respect to “test mode”, I meant what you said. And I feel like there seems to be a memory leak in my code then.</p><NewLine><p>Thanks for your reply.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ldw; <NewLine> ,"REPLY_DATE 1: March 10, 2020,  5:57am; <NewLine> REPLY_DATE 2: March 12, 2020,  8:05am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
67879,Prediction using traced torch model,2020-01-28T08:10:17.906Z,1,114,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How can I make a prediction? using traced torch model( the model is suppose to identify image similarity model , not image classification ) I need help please because am about to make an android app</p><NewLine></div>",https://discuss.pytorch.org/u/Terbinos_Getachew,(Terbinos Getachew),Terbinos_Getachew,"January 28, 2020,  8:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You should be able to call it just like you call the model before you trace it. If you are looking for an Android specific example, check out <a href=""https://github.com/pytorch/java-demo"">this repo</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I did implement it on the android but the tensor that I am getting after prediction on my android app is different that of on the desktop</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you post a repro that shows how you are calling it on desktop vs andriod?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Terbinos_Getachew; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: January 28, 2020,  6:56pm; <NewLine> REPLY_DATE 2: March 11, 2020, 12:34pm; <NewLine> REPLY_DATE 3: March 11, 2020,  9:02pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
72553,Error in wrapping multiple Scripted models into a New TorchScript model,2020-03-09T06:26:16.752Z,1,127,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to wrap two scripted model(from torch.jit.script) into a new torchscript model in my use case, I’ve narrow down the problems as below:</p><NewLine><pre><code class=""lang-auto"">(noted that, cls: classification model, mask: segmentation model)<NewLine>1. res34(cls) + res34(cls)  =&gt; Pass (Same model arch and same weight(same epoch))<NewLine>2. res34(cls) + res34(cls)  =&gt; Pass (Same model arch and diff weight(diff epoch))<NewLine>3. res18(cls) + res34(cls)  =&gt; Fail (Same model arch and diff weight(diff epoch))<NewLine>4. res34(cls) + res34(mask) =&gt; Fail (Diff model structure)<NewLine></code></pre><NewLine><p>Before getting into the snippet, to be mentioned is that, two failed cases (3, 4) get different error message.<br/><NewLine>Looking forward to the answer. Thanks!!</p><NewLine><p>(PyTorch Version: 1.4.0)</p><NewLine><pre><code class=""lang-auto"">class TestNet(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(TestNet, self).__init__()<NewLine>        self.model1 = torch.jit.load(SAVE_MODEL_PATH)       # res 34<NewLine>        #self.model2 = torch.jit.load(SAVE_MODEL_PATH)      # res 34<NewLine>        #self.model2 = torch.jit.load(SAVE_MODEL_PATH2)     # res 18<NewLine>        #self.model2 = torch.jit.load(SAVE_MODEL_PATH3)     # res 34 different weight<NewLine>        self.model2 = torch.jit.load(SAVE_MODEL_PATH4)      # mask<NewLine>    def forward(self, x:torch.Tensor)-&gt;torch.Tensor:<NewLine>        return self.model1(x)[0]<NewLine>def test_ensemble():<NewLine>    input640 = torch.rand(1, 3, 640, 640).cuda()<NewLine>    test_model = TestNet()<NewLine>    # build script model<NewLine>    test_model_libtorch = torch.jit.script(test_model)<NewLine>    test_model_libtorch.save(SAVE_MODEL_PATH)<NewLine>    test_model_libtorch = torch.jit.load(SAVE_MODEL_PATH).cuda()<NewLine>    output = test_model_libtorch(input640)<NewLine>    exit(0)<NewLine>res34 + res34  =&gt; OK<NewLine>res34 + res34 different weight  =&gt; OK<NewLine>res34 + res18  =&gt;  fail when forward<NewLine>RuntimeError: input.isTensor() INTERNAL ASSERT FAILED at /pytorch/torch/csrc/jit/argument_spec.h:89, please report a bug to PyTorch. Expected Tensor but found Bool (addTensor at /pytorch/torch/csrc/jit/argument_spec.h:89)<NewLine>frame #0: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x33 (0x7fa6372ff193 in /usr/local/lib/python3.7/dist-packages/torch/lib/libc10.so)<NewLine>frame #1: &lt;unknown function&gt; + 0x4059e33 (0x7fa63b9b3e33 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so)<NewLine>frame #2: torch::jit::ArgumentSpecCreator::create(bool, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt; const&amp;) const + 0x230 (0x7fa63b9ae8d0 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so)<NewLine>frame #3: &lt;unknown function&gt; + 0x40848fb (0x7fa63b9de8fb in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so)<NewLine>frame #4: &lt;unknown function&gt; + 0x407b8c1 (0x7fa63b9d58c1 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so)<NewLine>frame #5: torch::jit::Function::run(std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) + 0x60 (0x7fa63bc9a480 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so)<NewLine>frame #6: &lt;unknown function&gt; + 0x7b4a8b (0x7fa69551ca8b in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)<NewLine>frame #7: &lt;unknown function&gt; + 0x7b53bf (0x7fa69551d3bf in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)<NewLine>frame #8: &lt;unknown function&gt; + 0x774d76 (0x7fa6954dcd76 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)<NewLine>frame #9: &lt;unknown function&gt; + 0x295a74 (0x7fa694ffda74 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)<NewLine>&lt;omitting python frames&gt;<NewLine>frame #13: python() [0x530583]<NewLine>frame #18: python() [0x530547]<NewLine>frame #25: python() [0x62d082]<NewLine>frame #28: python() [0x606505]<NewLine>frame #30: __libc_start_main + 0xf0 (0x7fa6991a9830 in /lib/x86_64-linux-gnu/libc.so.6)<NewLine>res34 + mask  =&gt;  fail when jit.load<NewLine>  test_model_libtorch = torch.jit.load(SAVE_MODEL_PATH).cuda()<NewLine>  File ""/usr/local/lib/python3.7/dist-packages/torch/jit/__init__.py"", line 235, in load<NewLine>    cpp_module = torch._C.import_ir_module(cu, f, map_location, _extra_files)<NewLine>IndexError: Argument passed to at() was not in the map.<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/RebirthT,,RebirthT,"March 9, 2020,  6:26am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the report! Since this is a bug report, do you mind filing an issue on GH and we can follow up there? Over there, I will probably ask you for a script that I can run that will reproduce the problem. Thanks <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for ur prompt attention.<br/><NewLine>Will prepare the script and file an issue on GH!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/RebirthT; <NewLine> ,"REPLY_DATE 1: March 9, 2020,  4:46pm; <NewLine> REPLY_DATE 2: March 10, 2020,  5:44am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
53623,JIT GRU Implementation Optimization,2019-08-17T20:10:48.057Z,3,438,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Following <a href=""https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/"" rel=""nofollow noopener"">this guide</a> and <a href=""https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py"" rel=""nofollow noopener"">this code</a> and inspired by <a href=""https://github.com/pytorch/pytorch/issues/5261"" rel=""nofollow noopener"">this discussion</a>, I whipped up my own GRU cell implementation using JIT as follows.</p><NewLine><pre><code class=""lang-auto"">class JitGRUCell(jit.ScriptModule):<NewLine>    def __init__(self, input_size, hidden_size):<NewLine>        super(JitGRUCell, self).__init__()<NewLine>        self.input_size = input_size<NewLine>        self.hidden_size = hidden_size<NewLine>        self.weight_ih = Parameter(torch.Tensor(3 * hidden_size, input_size))<NewLine>        self.weight_hh = Parameter(torch.Tensor(3 * hidden_size, hidden_size))<NewLine>        self.bias_ih = Parameter(torch.Tensor(3 * hidden_size))<NewLine>        self.bias_hh = Parameter(torch.Tensor(3 * hidden_size))<NewLine><NewLine>        self.reset_parameters()<NewLine><NewLine>    def reset_parameters(self):<NewLine>        stdv = 1.0 / math.sqrt(self.hidden_size)<NewLine>        for weight in self.parameters():<NewLine>            weight.data.uniform_(-stdv, stdv)<NewLine><NewLine>    @jit.script_method<NewLine>    def forward(self, x, hidden):<NewLine>        # type: (Tensor, Tensor) -&gt; Tensor<NewLine>        x = x.view(-1, x.size(1))<NewLine>        x_results = torch.mm(x, self.weight_ih.t()) + self.bias_ih<NewLine>        h_results = torch.mm(hidden, self.weight_hh.t()) + self.bias_hh<NewLine><NewLine>        x_results = x_results.squeeze()<NewLine>        h_results = h_results.squeeze()<NewLine><NewLine>        i_r, i_z, i_n = x_results.chunk(3, 1)<NewLine>        h_r, h_z, h_n = h_results.chunk(3, 1)<NewLine><NewLine>        r = torch.sigmoid(i_r + h_r)<NewLine>        z = torch.sigmoid(i_z + h_z)<NewLine>        n = torch.tanh(i_n + r * h_n)<NewLine><NewLine>        return n - torch.mul(n, z) + torch.mul(z, hidden)<NewLine></code></pre><NewLine><p>The implementation itself looks very straightforward, however performance is not great.</p><NewLine><p>What are some tricks that I could use to optimize this implementation further? I’m very new to JIT script modules and I appreciate each and every suggestion.</p><NewLine></div>",https://discuss.pytorch.org/u/Maghoumi,(Mehran Maghoumi),Maghoumi,"August 17, 2019,  8:10pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi, Maghoumi. I’ve suffered form the same problem as you， do you fix it?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry for the super late response. I just randomly saw this thread again. Nope… Still using the same code. Did you solve this problem?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>What do you mean “return h”? IIRC the output of a GRU cell is the hidden state itself (you feed it back to itself as the previous hidden state)</p><NewLine><p>Also, I had some unit tests in place, and could confirm that this code returned close-enough results to PyTorch’s GRUCell implementation</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>yep. you are right. good thing i started with i moght be wrong …<img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>By the way, here’s my full implementation (including support for multiple layers, etc.): <a href=""https://github.com/Maghoumi/JitGRU"" rel=""nofollow noopener"">https://github.com/Maghoumi/JitGRU</a></p><NewLine><p>Suggestions for improvement are greatly welcome!</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you are comparing jit version to default version, there certainly will be a gap, especially during backward process. I noticed that both the implementation from <a href=""https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py"" rel=""nofollow noopener"">here</a> and your implementation used <code>+=[i]</code>, but I recommend using <code>.append(i)</code>. I don’t know how are these two methods translated to C++ but it seems that <code>.append(i)</code> is faster than simply adding, especially when you want to add many elements.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wuclddwf; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Maghoumi; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Maghoumi; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Dan_Erez; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Maghoumi; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/G.M; <NewLine> ,"REPLY_DATE 1: November 27, 2019,  7:20am; <NewLine> REPLY_DATE 2: January 15, 2020,  5:30am; <NewLine> REPLY_DATE 3: January 15, 2020,  5:32am; <NewLine> REPLY_DATE 4: January 15, 2020,  7:03am; <NewLine> REPLY_DATE 5: March 10, 2020,  1:03am; <NewLine> REPLY_DATE 6: March 10, 2020,  2:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
72405,Torchscript with spconv,2020-03-07T15:43:14.948Z,1,183,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I’m trying to convert my model via jit.trace(). I’m currently using the spconv library for my Network. Training the network works fine and with no errors, but using jit.trace() throws a size mismatch error.</p><NewLine><p>Here are the layers of my model:</p><NewLine><pre><code class=""lang-auto"">self.net = spconv.SparseSequential(<NewLine>            spconv.SparseConv2d(1, 64, 1),<NewLine>            nn.PReLU(),<NewLine>            spconv.SparseConv2d(64, 256, 2, padding=(1, 0)),<NewLine>            nn.PReLU(),<NewLine>            spconv.SparseConv2d(256, 512, 2, padding=(1, 0)),<NewLine>            nn.PReLU(),<NewLine>            spconv.SparseConv2d(512, 256, 2, padding=(0, 1)),<NewLine>            nn.PReLU(),<NewLine>            spconv.SparseConv2d(256, 64, 2, padding=(0, 1)),<NewLine>            nn.PReLU(),<NewLine>            spconv.SparseConv2d(64, 1, 1),<NewLine>            # nn.Tanh(),<NewLine>        )<NewLine></code></pre><NewLine><p>Here is the error message:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: size mismatch, m1: [375 x 375], m2: [1 x 64] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:290<NewLine></code></pre><NewLine><p>The input consists of 375x375 matrices. The network operates on CUDA.</p><NewLine><p>Maybe someone has an idea why this error comes up.</p><NewLine><p>Thank you</p><NewLine></div>",https://discuss.pytorch.org/u/Jakov_Gaspar,(Jakov Gaspar),Jakov_Gaspar,"March 7, 2020,  3:43pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>That error looks like it’s coming from the actual operation, not anything to do with tracing. Are you tracing it with the same inputs you’re using to run it successfully in normal PyTorch?</p><NewLine><p>If your model has any control flow that depends on the inputs tracing may not work correctly. Can you also post the full code you’re using to run/trace your model?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I already solved the problem, you were right. I accidentally passed a float as batch size, which caused this error.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Jakov_Gaspar; <NewLine> ,"REPLY_DATE 1: March 8, 2020,  8:09pm; <NewLine> REPLY_DATE 2: March 9, 2020, 11:17am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
71300,Optional submodule in ScriptedModule,2020-02-27T13:46:47.600Z,0,146,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! How should I declare an optional BatchNorm submodule in a valid TorchScript way?</p><NewLine><pre><code class=""lang-auto"">class Downsample(nn.Module):<NewLine>    def __init__(<NewLine>        self,<NewLine>        in_channels: int,<NewLine>        out_channels: int,<NewLine>        kernel_size: int,<NewLine>        apply_batchnorm: bool = False,<NewLine>    ):<NewLine>        super(Downsample, self).__init__()<NewLine>        self.conv = nn.Conv2d(<NewLine>            in_channels=in_channels,<NewLine>            out_channels=out_channels,<NewLine>            kernel_size=kernel_size,<NewLine>            stride=2,<NewLine>            bias=False,<NewLine>            padding=1,<NewLine>        )<NewLine>        self.apply_batchnorm = apply_batchnorm<NewLine>        if self.apply_batchnorm:<NewLine>            self.bn = nn.BatchNorm2d(out_channels)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.conv(x)<NewLine>        if self.apply_batchnorm:<NewLine>            x = self.bn(x)<NewLine>        x = F.leaky_relu(x, 0.3)<NewLine>        return x<NewLine></code></pre><NewLine><p>When trying to run <code>torch.jit.script(Downsample(1, 2, 3))</code> I get following error.</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>Module 'Downsample' has no attribute 'bn' :<NewLine>  File ""path/to/file.py"", line 100<NewLine>        x = self.conv(x)<NewLine>        if self.apply_batchnorm:<NewLine>            x = self.bn(x)<NewLine>                ~~~~~~~ &lt;--- HERE<NewLine>        x = F.leaky_relu(x, 0.3)<NewLine>        return x<NewLine></code></pre><NewLine><p>I understand that in TorchScript every variable must have single static type. Adding <code>bn: Optional[nn.BatchNorm2d]</code> to the class definition does not help.</p><NewLine></div>",https://discuss.pytorch.org/u/apisarek,(Andrzej Pisarek),apisarek,"February 28, 2020,  8:57am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is close, the issue is that when <code>self.apply_batchnorm</code> is false, there is no <code>bn</code> attribute on the module, so it cannot be accessed / checked. So</p><NewLine><pre><code class=""lang-python"">        if self.apply_batchnorm:<NewLine>            self.bn = nn.BatchNorm2d(out_channels)<NewLine></code></pre><NewLine><p>turns into</p><NewLine><pre><code class=""lang-python"">        if self.apply_batchnorm:<NewLine>            self.bn = nn.BatchNorm2d(out_channels)<NewLine>        else:<NewLine>            self.bn = None<NewLine></code></pre><NewLine><p>Another piece is that to call an optional module, the compiler must be able to figure out that it is not <code>None</code> when it is called, so instead of <code>if self.apply_batchnorm</code> you have to do <code>if self.bn is not None</code>.</p><NewLine><p>This is the full working example</p><NewLine><pre><code class=""lang-python"">class Downsample(nn.Module):<NewLine>    def __init__(<NewLine>        self,<NewLine>        in_channels: int,<NewLine>        out_channels: int,<NewLine>        kernel_size: int,<NewLine>        apply_batchnorm: bool = False,<NewLine>    ):<NewLine>        super(Downsample, self).__init__()<NewLine>        self.conv = nn.Conv2d(<NewLine>            in_channels=in_channels,<NewLine>            out_channels=out_channels,<NewLine>            kernel_size=kernel_size,<NewLine>            stride=2,<NewLine>            bias=False,<NewLine>            padding=1,<NewLine>        )<NewLine>        self.apply_batchnorm = apply_batchnorm<NewLine>        if self.apply_batchnorm:<NewLine>            self.bn = nn.BatchNorm2d(out_channels)<NewLine>        else:<NewLine>            self.bn = None<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.conv(x)<NewLine>        if self.bn is not None:<NewLine>            x = self.bn(x)<NewLine>        x = F.leaky_relu(x, 0.3)<NewLine>        return x<NewLine><NewLine><NewLine>torch.jit.script(Downsample(1, 2, 3))<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>U can also use <code>nn.Identity</code> to replace the batchnorm. For example, in <code>__init__</code>, u can write:</p><NewLine><pre><code class=""lang-auto"">if self.apply_batchnorm:<NewLine>    self.bn = nn.BatchNorm2d(out_channels)<NewLine>else:<NewLine>    self.bn = nn.Identity()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you driazati and G.M, I have also found another solution in Github issues in the meantime.</p><NewLine><aside class=""onebox githubpullrequest""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/vision/pull/1270/files/0f8a35d12d7b17776b2feb561f73e91fcedc8026#diff-bab13aad3b16866a3936568fb6fcbe70"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/vision</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Pull Request""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 12 16"" width=""60""><path d=""M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/vision/pull/1270"" rel=""nofollow noopener"" target=""_blank"">make shufflenet and resnet scriptable</a><NewLine></h4><NewLine><div class=""branches""><NewLine><code>pytorch:master</code> ← <code>eellison:shufflenet</code><NewLine></div><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-08-28"" data-format=""ll"" data-time=""16:11:54"" data-timezone=""UTC"">04:11PM - 28 Aug 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/eellison"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""eellison"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/11477974?v=4"" width=""20""/><NewLine>          eellison<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""6 commits changed 3 files with 6 additions and 2 deletions""><NewLine><a href=""https://github.com/pytorch/vision/pull/1270/files"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+6</span><NewLine><span class=""removed"">-2</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/26067"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/26067"" rel=""nofollow noopener"" target=""_blank"">TorchScript doesn't handle submodule being None</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-09-11"" data-format=""ll"" data-time=""23:37:20"" data-timezone=""UTC"">11:37PM - 11 Sep 19 UTC</span><NewLine></div><NewLine><div class=""date""><NewLine>          closed <span class=""discourse-local-date"" data-date=""2020-01-27"" data-format=""ll"" data-time=""07:05:52"" data-timezone=""UTC"">07:05AM - 27 Jan 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/dzhulgakov"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""dzhulgakov"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/17890620?v=4"" width=""20""/><NewLine>          dzhulgakov<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">🐛 Bug<NewLine>If one of the submodules is None, TorchScript doesn't recognize it as an attribute, unless explicitly specified in __constants__ lists.<NewLine>Obviously,...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">jit</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">jit-backlog</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/G.M; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/apisarek; <NewLine> ,"REPLY_DATE 1: February 28, 2020,  8:58am; <NewLine> REPLY_DATE 2: February 28, 2020,  8:58am; <NewLine> REPLY_DATE 3: February 28, 2020,  9:00am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
62263,Saving FusionGroup as part of ScriptModule,2019-11-26T23:45:29.348Z,2,216,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am able to see the FusionGroup in the graph dump as part of optimization passes. But, I am not seeing that in the saved ScriptModule. How can I be able to save the last executed optimized graph (last_executed_optimized_graph, which includes FusionGroup) in the ScriptModule?</p><NewLine><p>Thank you</p><NewLine></div>",https://discuss.pytorch.org/u/Poorna,(Poorna),Poorna,"November 26, 2019, 11:45pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The code for a <code>ScriptModule</code> is saved in its un-optimized form for a number of stability and performance related reasons. When it’s loaded, it gets re-compiled and re-optimized, so you don’t have to worry about the <code>FusionGroup</code> getting saved.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for the response <a class=""mention"" href=""/u/driazati"">@driazati</a> . While I agree that re-compilation and re-optimization could be done during load time, not all optimizations are quick enough to do for every load. If there is an opportunity to save the optimized model (on the target hardware), we wont be incurring the optimization cost. Do you recommend or suggest ways in which we can achieve this?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Bumping this question up. Is it possible to save an optimized graph along with its fusiongroups/subgraphs in a scriptmodule?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is not possible today but may be enabled in the future as we build out support for different backends (e.g. <a href=""https://github.com/pytorch/pytorch/pull/32178"">this PR to enable lowered graphs for consumption by backend passes</a>)</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/driazati"">@driazati</a>:  Awesome . Thanks a ton for this pointer. I will go through this PR. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Poorna; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vdantu; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/vdantu; <NewLine> ,"REPLY_DATE 1: November 27, 2019,  7:41pm; <NewLine> REPLY_DATE 2: November 27, 2019,  9:08pm; <NewLine> REPLY_DATE 3: February 26, 2020, 10:42pm; <NewLine> REPLY_DATE 4: February 27, 2020,  6:49pm; <NewLine> REPLY_DATE 5: February 27, 2020,  7:23pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
71317,Torch.cat throws error for tensor list when compiling with torchscript,2020-02-27T16:33:07.490Z,0,220,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Torch.cat throws error for tensor lists when used within torchscript.<br/><NewLine>Kindly let me know of a fix/workaround.</p><NewLine><p>Here is a minimal example to reproduce the bug.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine>""""""<NewLine>Smallest working bug for torch.cat torchscript<NewLine>""""""<NewLine><NewLine><NewLine>class Model(nn.Module):<NewLine>    """"""dummy model for showing error""""""<NewLine><NewLine>    def __init__(self):<NewLine>        super(Model, self).__init__()<NewLine>        pass<NewLine><NewLine>    def forward(self):<NewLine>        a = torch.rand([6, 1, 12])<NewLine>        b = torch.rand([6, 1, 12])<NewLine>        out = torch.cat([a, b], axis=2)<NewLine>        return out<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    model = Model()<NewLine>    print(model())  # works<NewLine>    torch.jit.script(model)  # throws error<NewLine><NewLine></code></pre><NewLine><pre><code class=""lang-auto"">This code throws the following error:<NewLine>File ""/home/anil/.conda/envs/rnn/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1423, in _create_methods_from_stubs<NewLine>    self._c._create_methods(self, defs, rcbs, defaults)<NewLine>RuntimeError: <NewLine>Arguments for call are not valid.<NewLine>The following operator variants are available:<NewLine>  <NewLine>  aten::cat(Tensor[] tensors, int dim=0) -&gt; (Tensor):<NewLine>  Keyword argument axis unknown.<NewLine>  <NewLine>  aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -&gt; (Tensor(a!)):<NewLine>  Argument out not provided.<NewLine><NewLine>The original call is:<NewLine>at smallest_working_bug_torch_cat_torchscript.py:19:14<NewLine>    def forward(self):<NewLine>        a = torch.rand([6, 1, 12])<NewLine>        b = torch.rand([6, 1, 12])<NewLine>        out = torch.cat([a, b], axis=2)<NewLine>              ~~~~~~~~~ &lt;--- HERE<NewLine>        return out<NewLine></code></pre><NewLine><p>Thank you for your consideration</p><NewLine></div>",https://discuss.pytorch.org/u/ParticularlyPythonic,(Anil Radhakrishnan),ParticularlyPythonic,"February 27, 2020,  4:33pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>PyTorch supports the <code>axis</code> keyword arg for numpy compatibility, but it looks like there is a bug where this isn’t translating into TorchScript. Most TorchScript ops use <code>dim</code> in place of <code>axis</code> (the meaning is the same), so if you change that in your code it should work, i.e. <code>torch.cat([a, b], axis=2)</code> becomes <code>torch.cat([a, b], dim=2)</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: February 27, 2020,  7:09pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
70408,How to add input shape dimension hint to an optimized graph?,2020-02-20T03:31:56.454Z,2,415,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m trying to get an optimized graph from graph executor via ScriptModule’s <code>graph_for(...)</code> method. A simple test case is below:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine>conv = torch.nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)<NewLine><NewLine>#  To avoid dealing with prim::Bailout stuff<NewLine>torch._C._jit_set_profiling_executor(False)<NewLine><NewLine>inp = torch.rand(1, 3, 224, 224)<NewLine>trace = torch.jit.trace(conv, inp).eval()<NewLine><NewLine>print(trace.graph_for(inp))<NewLine></code></pre><NewLine><p>But it seems graph executor erases shape information on construction, so the output of <code>graph_for</code> has shape information removed.</p><NewLine><pre><code class=""lang-auto"">graph(%self : __torch__.torch.nn.modules.module.Module,<NewLine>      %input : Float(*, *, *, *)):<NewLine>  %23 : int[] = prim::Constant[value=[0, 0]]()<NewLine>  %22 : int[] = prim::Constant[value=[1, 1]]()<NewLine>  %4 : int = prim::Constant[value=1]() # /home/masa/projects/deep/pytorch/torch/nn/modules/conv.py:345:0<NewLine>  %13 : bool = prim::Constant[value=0]() # /home/masa/projects/deep/pytorch/torch/nn/modules/conv.py:345:0<NewLine>  %20 : bool = prim::Constant[value=1]() # /home/masa/projects/deep/pytorch/torch/nn/modules/conv.py:345:0<NewLine>  %2 : Float(*) = prim::GetAttr[name=""bias""](%self)<NewLine>  %3 : Float(*, *, *, *) = prim::GetAttr[name=""weight""](%self)<NewLine>  %21 : Float(*, *, *, *) = aten::_convolution(%input, %3, %2, %22, %23, %22, %13, %23, %4, %13, %13, %20) # /home/masa/projects/deep/pytorch/torch/nn/modules/conv.py:345:0<NewLine>  return (%21)<NewLine><NewLine></code></pre><NewLine><p>If I know that my input shape is always fixed, is there a way to add explicit shape information? I know that the output of trace has shape information preserved, but I want an optimized graph available via <code>graph_for(...)</code>.</p><NewLine></div>",https://discuss.pytorch.org/u/masahi,,masahi,"February 20, 2020,  3:33am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you please elaborate on your use case? The exact static shapes were added to enable backends to generate more efficient kernels. I’m not sure they will be added to serialization or any other public interface.</p><NewLine><p>WIthout,<br/><NewLine><code>torch._C._jit_set_profiling_executor(False)</code> we don’t capture the exact shapes only tensor ranks. Even the rank information, I believe, considered internal for use by backends and optimization passes.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>ok, my use case is to translate Torchscript IR to <a href=""https://github.com/apache/incubator-tvm"" rel=""nofollow noopener"">TVM</a> compiler’s IR. I have a repo <a href=""https://github.com/masahi/torchscript-to-tvm"" rel=""nofollow noopener"">https://github.com/masahi/torchscript-to-tvm</a> which demonstrates translating torch models to TVM, compile and run it under TVM and get the identical output as torch.</p><NewLine><p>Right now I take the output of <code>torch.jit.trace(...)</code> which has shape information attached, and for each torch operator node I translate it to corresponding one in TVM. Since TVM is mostly a static compiler, shape information is required.</p><NewLine><p>TVM has its own set of optimization passes, so it is no problem to take the unoptimized input torch IR. Currently I’m applying the Torchscript inlining pass to remove <code>prim::CallMethod</code> wrapping,  but it seems rather ad hoc to me and I rather want to apply other optimization passes in torch as well.</p><NewLine><p>I know I can apply each optimization passes manually (and I do for the inline pass), but the API prefix <code>torch._C._jit_pass*</code> suggests they are not “officially” supported, so I’m not sure if I want to use them directly. Since I discovered that I can access the optimized graph via <code>graph_for(...)</code> method, I’m looking to see if this is something I can use.</p><NewLine><p>I disabled the profiling executor because it adds <code>prim::Bailout</code> and <code>prim::BailoutTemplate</code> nodes for which I have no idea how to translate to TVM. Since input shape is static in TVM, I think they are not relevant to my use case, so I don’t want to see them in the input IR.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I would suggest that you run it with the profiling executor a few times with inputs that cover the different Tensor dimensions you expect to use, and then add a pass to remove Bailout nodes. This should give you a Graph with shape information to maximum generality.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>There are also a few passes that will be landed shortly in Pytorch JIT that should help with this conversion. Freezing and a functionalization pass. I will comment here when they are landed.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, if removing bailout is possible that would definitely work for me. At a quick glance there is no pass to remove Bailout nodes in torch, but I’ll try if I can do it from “userland”.</p><NewLine><p>Another API I’m interested in is <code>_propagate_shapes</code> function:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/8527ba8b7059d8c7df6fc2c11271ceefbf91ce5a/torch/csrc/jit/script/init.cpp#L476-L485"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/8527ba8b7059d8c7df6fc2c11271ceefbf91ce5a/torch/csrc/jit/script/init.cpp#L476-L485"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/8527ba8b7059d8c7df6fc2c11271ceefbf91ce5a/torch/csrc/jit/script/init.cpp#L476-L485</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""476"" style=""counter-reset: li-counter 475 ;""><NewLine><li>static std::shared_ptr&lt;Graph&gt; _propagate_shapes(</li><NewLine><li>    Graph&amp; graph,</li><NewLine><li>    std::vector&lt;at::Tensor&gt; inputs,</li><NewLine><li>    bool with_grad = false) {</li><NewLine><li>  Stack stack(inputs.begin(), inputs.end());</li><NewLine><li>  auto retval = graph.copy();</li><NewLine><li>  setInputTensorTypes(*retval, stack, /*complete=*/false);</li><NewLine><li>  PropagateInputShapes(retval);</li><NewLine><li>  return retval;</li><NewLine><li>}</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Do you know the use case of this function and if this is something I should take a look?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>This API is only useful with the executor that isn’t the profiled executor, so I don’t think it applies. Yes, the pass doesn’t exactly exist as you need it. The logic should pretty much be the same as <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/passes/guard_elimination.cpp#L145"" rel=""nofollow noopener"">right here</a>, except you are always removing the guards.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/villedepommes; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/masahi; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/masahi; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/eellison; <NewLine> ,"REPLY_DATE 1: February 20, 2020,  6:58pm; <NewLine> REPLY_DATE 2: February 20, 2020,  8:25pm; <NewLine> REPLY_DATE 3: February 20, 2020, 10:00pm; <NewLine> REPLY_DATE 4: February 20, 2020, 10:29pm; <NewLine> REPLY_DATE 5: February 20, 2020, 10:37pm; <NewLine> REPLY_DATE 6: February 20, 2020, 10:55pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> 
70331,Why do we use __constants__ (or Final)?,2020-02-19T14:29:07.936Z,3,471,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi All,</p><NewLine><p>I’m looking at the TorchVision implementation of GoogLeNet and I see that in the inception block is used <code>__constants__</code> in the class definition. I read the documentation but I still don’t understand how it works and what happen if I remove it. In my understanding is something used by TorchScript but I don’t have yet the full picture (probably because I need to learn more about TorchScript).</p><NewLine><p>I also tried:</p><NewLine><pre><code class=""lang-python"">inception_block = Inception(192, 64, 96, 128, 16, 32, 32)<NewLine>inception_block = torch.jit.script(inception_block)<NewLine>inception_block<NewLine></code></pre><NewLine><p>And I don’t receive any error even if I remove <code>__constants__ = ['branch2', 'branch3', 'branch4']</code> in the class definition.</p><NewLine><pre><code class=""lang-python"">class Inception(nn.Module):<NewLine>    __constants__ = ['branch2', 'branch3', 'branch4']<NewLine><NewLine>    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj,<NewLine>                 conv_block=None):<NewLine>        super(Inception, self).__init__()<NewLine>        if conv_block is None:<NewLine>            conv_block = BasicConv2d<NewLine>        self.branch1 = conv_block(in_channels, ch1x1, kernel_size=1)<NewLine><NewLine>        self.branch2 = nn.Sequential(<NewLine>            conv_block(in_channels, ch3x3red, kernel_size=1),<NewLine>            conv_block(ch3x3red, ch3x3, kernel_size=3, padding=1)<NewLine>        )<NewLine><NewLine>        self.branch3 = nn.Sequential(<NewLine>            conv_block(in_channels, ch5x5red, kernel_size=1),<NewLine>            conv_block(ch5x5red, ch5x5, kernel_size=3, padding=1)<NewLine>        )<NewLine><NewLine>        self.branch4 = nn.Sequential(<NewLine>            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),<NewLine>            conv_block(in_channels, pool_proj, kernel_size=1)<NewLine>        )<NewLine><NewLine>    def _forward(self, x):<NewLine>        branch1 = self.branch1(x)<NewLine>        branch2 = self.branch2(x)<NewLine>        branch3 = self.branch3(x)<NewLine>        branch4 = self.branch4(x)<NewLine><NewLine>        outputs = [branch1, branch2, branch3, branch4]<NewLine>        return outputs<NewLine><NewLine>    def forward(self, x):<NewLine>        outputs = self._forward(x)<NewLine>        return torch.cat(outputs, 1)<NewLine></code></pre><NewLine><p>Can you explain me better what is the utility of adding <strong>constants</strong> and what happen if I don’t do it in a case like this one?</p><NewLine><p>Thanks,<br/><NewLine>Mario</p><NewLine></div>",https://discuss.pytorch.org/u/mnslarcher,(Mario Namtao Shianti Larcher),mnslarcher,"February 19, 2020,  2:29pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The relevant docs can be found in the “ How do I store attributes on a ScriptModule” question <a href=""https://pytorch.org/docs/stable/jit.html#frequently-asked-questions"">here</a>. The idea is that if the jit knows certain values are constant and unchanging, then more aggressive optimizations and re-ordering can be done with those values versus things that are just regular attributes on a model.</p><NewLine><p>So when you remove <code>__constants__</code> or a <code>Final</code> type annotation the model’s behavior shouldn’t change, but less information is available to the jit about what it can do with your code. The jit’s type system will enforce that these values are not mutated, so that can make your code cleaner as well.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks Driazati!</p><NewLine><p>I still have some confusion by the fact that here we are marking as <code>__constants__</code> blocks that have weights that will change during back-propagation, again I’m not an expert of jit so is possible that I’m missing something that doesn’t regard <code>__constants__</code> at all.</p><NewLine><p>With <code>__constants__</code> are we just saying that the shape and type will not change but we are ok if the parameter values change?</p><NewLine><p>I understand something like<br/><NewLine><code>__constants__ = ['a']</code> followed by a <code>self.a = 4</code> because I can see that 4 is a constant value that can be hard-coded but I’m still confused by how blocks with weights are treated.</p><NewLine><p>Thanks again,<br/><NewLine>Mario</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah yeah, the case you’re seeing here is a hack that we’ve been using for a while, basically some modules can have optional submodules (i.e. either the submodule can be present or <code>None</code>). When we script something we just so happen to add submodules first, then constants, skipping any names that are already present on the module, so it either gets added as a normal submodule (ignoring the entry in <code>__constants__</code>) or as a <code>None</code> constant. If the compiler sees a <code>None</code> constant in an <code>if</code>-statement, it will skip compilation of the code inside the <code>if</code>, allowing us to support uses like <a href=""https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L67-L68""><code>downsample</code> in resnet</a>.</p><NewLine><p>So if you see a <code>nn.Module</code> in <code>__constants__</code>, all it really means is <code>Optional[nn.Module]</code>, we just have this kind-of-nonsense way to specify that.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ahah great now I get it and it makes more sense ;)! Thanks for the answer!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mnslarcher; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mnslarcher; <NewLine> ,"REPLY_DATE 1: February 19, 2020,  6:21pm; <NewLine> REPLY_DATE 2: February 20, 2020,  8:46am; <NewLine> REPLY_DATE 3: February 20, 2020,  7:45pm; <NewLine> REPLY_DATE 4: February 20, 2020,  7:44pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
70122,"Trying to use jit.trace, results in &lsquo;AttributeError: &lsquo;FaceAlignment&rsquo; object has no attribute &lsquo;__name__&rsquo; error",2020-02-18T06:10:27.616Z,3,380,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone,<br/><NewLine>I’m trying to have weight caching , but on one of my models it fails . for caching I’m simply running this snippet :</p><NewLine><pre><code class=""lang-python"">fname = 'cached_weights_s3fd.pt'<NewLine>test_imgs = Detector.gen_fake_img(batch=1)<NewLine>traced = torch.jit.trace(self.detector, test_imgs, check_trace=False)<NewLine>script = traced.save(fname)<NewLine>self.detector = torch.jit.load(fname)<NewLine></code></pre><NewLine><p>and the full stacktrace is as follows:</p><NewLine><pre><code class=""lang-python"">Traceback (most recent call last):<NewLine>  File ""d:\Codes\Pytorch_Retinaface\detection_core\main_detector.py"", line 2074, in &lt;module&gt;<NewLine>    run_test()<NewLine>  File ""d:\Codes\Pytorch_Retinaface\detection_core\main_detector.py"", line 2065, in run_test<NewLine>    run_capture_sfd()<NewLine>  File ""d:\Codes\Pytorch_Retinaface\detection_core\main_detector.py"", line 2002, in run_capture_sfd<NewLine>    after_detection_fn=None)<NewLine>  File ""Pytorch\detection_core\main_detector.py"", line 1523, in __init__<NewLine>    self._init()<NewLine>  File ""Pytorch\detection_core\main_detector.py"", line 1536, in _init<NewLine>    traced = torch.jit.trace(self.detector, test_imgs, check_trace=False)<NewLine>  File ""C:\Users\User\Anaconda3\lib\site-packages\torch\jit\__init__.py"", line 911, in trace<NewLine>    name = _qualified_name(func)<NewLine>  File ""C:\Users\User\Anaconda3\lib\site-packages\torch\_jit_internal.py"", line 683, in _qualified_name<NewLine>    name = obj.__name__<NewLine>AttributeError: 'FaceAlignment' object has no attribute '__name__'<NewLine><NewLine></code></pre><NewLine><p>Whats wrong?<br/><NewLine>As far as  I know all classes has <code>__name__</code> attribute, including the mentioned class in the error.(the name is ‘FaceAlignment’ obviously)<br/><NewLine>So I’m not sure why I’m getting this. Any help is greatly appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/Shisho_Sama,(A curious guy here!),Shisho_Sama,"February 18, 2020,  5:10pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Upon further investigations, I turns out in <code>\torch\_jit_internal.py"", line 684,</code> the object that is causing the error is</p><NewLine><pre><code class=""lang-python"">Object is type: &lt;class 'function'&gt; and the object itself is : &lt;function LSTM.forward at 0x00000214C495D048&gt;<NewLine></code></pre><NewLine><p>The Irony is, I do not have any LSTM cell in my model!<br/><NewLine>What am I seeing this?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for doing some investigation! Are you able to share the binary for <code>cached_weights_s3fd.pt</code> so we can try to debug this on our end?</p><NewLine><p>I think what you’re seeing here is some initialization the jit does when you first call <code>torch.jit.script</code> or <code>torch.jit.trace</code>. We have to grab the qualified names for a few modules that have overloaded forward methods (only <code>nn.LSTM</code> and <code>nn.GRU</code>), which is what you’re seeing. You could try setting a breakpoint and skipping past these calls to get to the time it’s actually invoked with your class.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group quote-modified"" data-post=""1"" data-topic=""70122"" data-username=""Shisho_Sama""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/shisho_sama/40/6926_2.png"" width=""20""/> Shisho_Sama:</div><NewLine><blockquote><NewLine><p>fname = ‘cached_weights_s3fd.pt’ test_imgs = Detector.gen_fake_img(batch=1) traced = torch.jit.trace(self.detector, test_imgs, check_trace=False) script = traced.save(fname) self.detector = torch.jit.load(fname)</p><NewLine></blockquote><NewLine></aside><NewLine><p>The cached weights are not generated, it fails before that.<br/><NewLine>You can replicate  the issue by cloning this repo : <a href=""https://github.com/1adrianb/face-alignment"" rel=""nofollow noopener"">https://github.com/1adrianb/face-alignment</a><br/><NewLine>and running this snippet :</p><NewLine><pre><code class=""lang-python"">import face_alignment<NewLine>import torch<NewLine>import torchvision as tv<NewLine><NewLine>def gen_fake_img(samples=1000, batch=10, image_size=(3, 224, 224), num_classes=2):<NewLine>        fake_dt = tv.datasets.FakeData(samples,<NewLine>                             image_size, num_classes,<NewLine>                             tv.transforms.ToTensor())<NewLine>        dt_ldr = torch.utils.data.DataLoader(fake_dt, batch, pin_memory=True)<NewLine>        sample_imgs, _ = next(iter(dt_ldr))<NewLine>        return sample_imgs<NewLine><NewLine>detector = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, device='cpu')<NewLine><NewLine>fname = 'cached_weights_s3fd.pt'<NewLine>test_imgs = gen_fake_img(batch=1)<NewLine>traced = torch.jit.trace(detector, test_imgs, check_trace=False)<NewLine>script = traced.save(fname)<NewLine>detector = torch.jit.load(fname)<NewLine><NewLine>fa.get_landmarks('test/assets/aflw-test.jpg')<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Found the casue, I was sending the wrong model!<br/><NewLine>its all fixed and fine now!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> ,"REPLY_DATE 1: February 18, 2020,  6:48am; <NewLine> REPLY_DATE 2: February 18, 2020,  6:48pm; <NewLine> REPLY_DATE 3: February 19, 2020,  5:11am; <NewLine> REPLY_DATE 4: February 19, 2020,  5:11pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
69881,Jit does not support doesn&rsquo;t support binary operator,2020-02-16T04:12:04.575Z,0,95,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a code that looks like this</p><NewLine><pre><code class=""lang-auto"">import torch as tc<NewLine>from torch import jit<NewLine>@jit.script<NewLine>def func(inp):<NewLine>    return inp&lt;&lt;1<NewLine>a = tc.tensor([3,4,5])<NewLine>func(a)<NewLine></code></pre><NewLine><p>when I run it, I got the error:</p><NewLine><pre><code class=""lang-auto"">torch.jit.frontend.NotSupportedError: unsupported binary operator: LShift:<NewLine>  File ""example.py"", line 5<NewLine>@jit.script<NewLine>def func(inp):<NewLine>    return inp&lt;&lt;1<NewLine>              ~~ &lt;--- HERE<NewLine></code></pre><NewLine><p>However, if I change <code>inp&lt;&lt;1</code> to <code>inp.__lshift__(1)</code>, it works. This also happens in C++. Is there a reason behind this or is it just that support for binary operators are not fully implemented yet? I’m using Pytorch 1.4.0 on windows with the corresponding version of libtorch.<br/><NewLine>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/G.M,,G.M,"February 16, 2020,  4:27am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like this was just an oversight, can you <a href=""https://github.com/pytorch/pytorch/issues/new/choose"">file a bug</a> with this info so we can have it in our issue tracker? (We’re happy to accept any PRs to add it too!)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: February 19, 2020,  1:24am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
69486,"Version Problem, how to make PyTorch 1.2 and 1.3 work together",2020-02-12T13:17:36.031Z,3,310,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I have some version problems. And need your help. Let discuss it.</p><NewLine><p>I am using the <a href=""https://github.com/NervanaSystems/distiller"" rel=""nofollow noopener"">distiller</a> to prune my object detection model.<br/><NewLine>Then I need to deploy the pruned model on a Jetson TX2 with TensorRT 6.0.</p><NewLine><p>However, for the distiller, it requires the JIT module of PyTorch 1.3.1 to get the graph a module such that the distiller can do the actual pruning (not just putting a zeros mask).</p><NewLine><p>After getting the pruned model, I exported it to an ONNX model with the PyTorch 1.3.1, however, I found that the ONNX parser of TensorRT 6.0 is not compatible with PyTorch 1.3.1, but with PyTorch 1.2.<br/><NewLine>I have raised this issue on <a href=""https://github.com/NVIDIA/TensorRT/issues/376"" rel=""nofollow noopener"">github</a>.</p><NewLine><p>Now I have trouble.<br/><NewLine>How can I make them work together?</p><NewLine><p>I have tried two ways, but both of them failed.<br/><NewLine>I created two conda environments, one is with PyTorch 1.3.1 and distiller, called env1.3, another is with PyTorch 1.2.0, called env1.2</p><NewLine><ol><NewLine><li><NewLine><p>Under env1.3, I saved the whole model (not just state_dict) with torch.save and pickle.dump, but when I tried to use torch.load or pickle.load to load the model under env1.2, an error will occury, saying that there is no module called distiller.</p><NewLine></li><NewLine><li><NewLine><p>Under env1.3, I used torch.jit.trace and get a ScriptModulde, and then I used torch.jit.save to save the model to the disk. However, when I used torch.jit.load under env1.2 to load the jit model, I got</p><NewLine></li><NewLine></ol><NewLine><blockquote><NewLine><p>terminate called after throwing an instance of ‘c10::Error’<br/><NewLine>what():  [enforce fail at inline_container.cc:137] . PytorchStreamReader failed closing reader: file not found<br/><NewLine>frame <span class=""hashtag"">#0:</span> c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&amp;, void const*) + 0x47 (0x7f9f1c67ee17 in /home/rizhao/anaconda3/envs/torch/lib/python3.6/site-packages/torch/lib/libc10.so)<br/><NewLine>frame <span class=""hashtag"">#1:</span> caffe2::serialize::PyTorchStreamReader::valid(char const*) + 0x6b (0x7f9f1f60775b in /home/rizhao/anaconda3/envs/torch/lib/python3.6/site-packages/torch/lib/libtorch.so)<br/><NewLine>frame <span class=""hashtag"">#2:</span> caffe2::serialize::PyTorchStreamReader::~PyTorchStreamReader() + 0x1f (0x7f9f1f6077af in /home/rizhao/anaconda3/envs/torch/lib/python3.6/site-packages/torch/lib/libtorch.so)<br/><NewLine>frame <span class=""hashtag"">#3:</span>  + 0x3c17637 (0x7f9f206e6637 in /home/rizhao/anaconda3/envs/torch/lib/python3.6/site-packages/torch/lib/libtorch.so)<br/><NewLine>frame <span class=""hashtag"">#4:</span> torch::jit::import_ir_module(std::shared_ptr<a>torch::jit::script::CompilationUnit</a>, std::string const&amp;, c10::optional<a>c10::Device</a>, std::unordered_map&lt;std::string, std::string, std::hash<a>std::string</a>, std::equal_to<a>std::string</a>, std::allocator&lt;std::pair&lt;std::string const, std::string&gt; &gt; &gt;&amp;) + 0x1d0 (0x7f9f206ed220 in /home/rizhao/anaconda3/envs/torch/lib/python3.6/site-packages/torch/lib/libtorch.so)<br/><NewLine>frame <span class=""hashtag"">#5:</span>  + 0x4d69dc (0x7f9f66ddf9dc in /home/rizhao/anaconda3/envs/torch/lib/python3.6/site-packages/torch/lib/libtorch_python.so)<br/><NewLine>frame <span class=""hashtag"">#6:</span>  + 0x1d3ef4 (0x7f9f66adcef4 in /home/rizhao/anaconda3/envs/torch/lib/python3.6/site-packages/torch/lib/libtorch_python.so)<br/><NewLine><br/><NewLine>frame <span class=""hashtag"">#22:</span> __libc_start_main + 0xf0 (0x7f9f75830830 in /lib/x86_64-linux-gnu/libc.so.6)</p><NewLine></blockquote><NewLine><p>Aborted (core dumped)</p><NewLine><p>Now, I have no idea how to solve it. Any cues will be much appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/Rizhao_Cai,(Riz),Rizhao_Cai,"February 12, 2020,  1:47pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you able to upload the binary (maybe to a GitHub repo or something) you are trying to load so we can try to reproduce this issue?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks. Can I just upload the ONNX model exported by PyTorch 1.3 or PyTorch 1.2? Besides, I can also upload the exported JIT module.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""69486"" data-username=""Rizhao_Cai""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/rizhao_cai/40/20465_2.png"" width=""20""/> Rizhao_Cai:</div><NewLine><blockquote><NewLine><p>upload the ONNX model exported by PyTorch 1.3 or PyTorch 1.2? Besides, I can also upload the exported</p><NewLine></blockquote><NewLine></aside><NewLine><p>Upload everything you can, at the very least the model you load that causes the error to be thrown</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Rizhao_Cai; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: February 13, 2020,  6:39pm; <NewLine> REPLY_DATE 2: February 14, 2020,  9:06am; <NewLine> REPLY_DATE 3: February 18, 2020,  6:41pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
64575,Speed of First pass is very slow,2019-12-20T02:33:06.689Z,4,514,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone. I recently loading a torchscript model in C++, when i use the model to infer, the first pass takes about 20s, while the others take only about 0.5s.</p><NewLine><p>Has anyone ever done any related work or met the same problem?</p><NewLine><p>is there any way to disable the optimization or choose the optimization level or after optimization we can save the model(or the computation graph)?</p><NewLine><p>or is it an Inevitable warm-up process?</p><NewLine><p>I’d appreciate if anybody can help me! Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/huoge,,huoge,"December 20, 2019,  2:45am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello huoge,</p><NewLine><p>It’s likely that you’re correct in the assessment that optimization is what’s making the first pass slow. However, 20 seconds seems pretty high and we’d like to understand what exactly is happening here. Do you mind sharing your serialized model file so we can have a look? Also, which version of PyTorch are you using?</p><NewLine><p>We don’t currently have an optimization level option.</p><NewLine><p>James</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello  James,</p><NewLine><p>Thanks for replying me, I build the pytorch from source and the version is 1.4.0a0+93db2b8<br/><NewLine>the model is a modified transformer. I transfer the model to torchscript model by using script_model = torch.jit.script(model). (and definitely some other work for jit-compatible)</p><NewLine><p>by the way, I use the torch.quantization.quantize_dynamic to quantize the model, then the first pass<br/><NewLine>costs about 12.5s.</p><NewLine><p>this is parts of print(script_model):<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/1c86884e2c7d84b1b529fc8924a2a86abb7cadbe"" href=""https://discuss.pytorch.org/uploads/default/original/3X/1/c/1c86884e2c7d84b1b529fc8924a2a86abb7cadbe.png"" title=""image""><img alt=""image"" data-base62-sha1=""44lBqfgoOfnoAUxXDponBKetOVM"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/1/c/1c86884e2c7d84b1b529fc8924a2a86abb7cadbe_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/original/3X/1/c/1c86884e2c7d84b1b529fc8924a2a86abb7cadbe.png"" width=""320""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">505×787 9.6 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>I only use torch.jit.script(), should I mix the trace and script?</p><NewLine><p>and how can I submit the serialized model file to you? for some reason, I can’t give you the<br/><NewLine>modifiled transformer, but I can provide the orginal transformer serialized model file for you which first pass costs 32.83s while the others take about 9s(yeah, it’s about 20s again). The origin and modifiled transformer are same net architecture but have different inference functions, so  they have same problem in first pass.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can anyone help me with this issue?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you solve a “warm up” scripted model problem?<br/><NewLine>Simplest way to share this problem is:<br/><NewLine><code><br/><NewLine>import torchvision, torch, time</code></p><NewLine><p>model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)<br/><NewLine>model = torch.jit.script(model)<br/><NewLine>model.eval()<br/><NewLine>x = [torch.randn((3,224,224))]<br/><NewLine>for i in range(3):<br/><NewLine>start = time.time()<br/><NewLine>model(x)<br/><NewLine>print(‘Time elapsed: {}’.format(time.time()-start))<br/><NewLine></p><NewLine><p>Output:<br/><NewLine><i> Time elapsed: 38.297527551651<br/><NewLine>Time elapsed: 6.655704021453857<br/><NewLine>Time elapsed: 6.651334762573242</i></p><NewLine><p>So, can anybody help with explaining how I can load and run scripted model without this ""warm up’?<br/><NewLine>Thanx.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Maybe this post helps? <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/speed-of-custom-rnn-is-super-slow/63209/9"">Speed of Custom RNN is SUPER SLOW</a></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, with torch.jit.optimized_execution(False): really helped.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/James_Reed; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/huoge; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/huoge; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Pavel_Platonov; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/G.M; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Pavel_Platonov; <NewLine> ,"REPLY_DATE 1: December 20, 2019,  2:50am; <NewLine> REPLY_DATE 2: December 20, 2019,  9:34am; <NewLine> REPLY_DATE 3: January 8, 2020,  1:24pm; <NewLine> REPLY_DATE 4: February 14, 2020,  5:51pm; <NewLine> REPLY_DATE 5: March 5, 2020,  8:33am; <NewLine> REPLY_DATE 6: February 15, 2020,  4:17pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 2 Likes; <NewLine> REPLY 6 LIKES: ; <NewLine> 
69261,Loading tensors from pytorch to torch C++,2020-02-10T18:55:36.368Z,2,211,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I am trying to load tensor from pytorch to c++, following suggestions <a href=""https://github.com/pytorch/pytorch/issues/20356"" rel=""nofollow noopener"">this</a> github issue. However, it looks like <a href=""https://pytorch.org/cppdocs/api/structtorch_1_1jit_1_1script_1_1_module.html#exhale-struct-structtorch-1-1jit-1-1script-1-1-module"" rel=""nofollow noopener"">torch::jit::script::Module</a> no longer has  <code>get_attribute</code>  and I having a tough time understanding how to get named parameters using the API.</p><NewLine><p>Can you please suggest how to get named_parameters with the current API?</p><NewLine></div>",https://discuss.pytorch.org/u/veryluckyxyz,(Veryluckyxyz),veryluckyxyz,"February 10, 2020,  6:55pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey sorry for the confusion here. The TorchScript C++ API is still experimental and we made some changes to the API, we will update the doc in the next release. To answer your question, we are trying to mimic the module API in python, so you can get the named parameters via <code>named_parameters()</code> call in C++, see a list of methods <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/script/module.h#L156"" rel=""nofollow noopener"">here</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your response. i was able to get the values using  <code>attr</code>  but I still could not understand how to extract values from  <code>slot_iterator_impl</code>.<br/><NewLine>If its not too much trouble, can I please request sample code to extract for the same example (reproduced below)? It can help me better understand the codebase!</p><NewLine><p>Thank you again!</p><NewLine><pre><code class=""lang-auto"">#include &lt;torch/script.h&gt;<NewLine><NewLine>#include &lt;iostream&gt;<NewLine>#include &lt;memory&gt;<NewLine><NewLine>int main(int argc, const char *argv[]) {<NewLine>  torch::jit::script::Module container = torch::jit::load(""container.pt"");<NewLine><NewLine>  // Load values by name<NewLine>  torch::Tensor a = container.get_attribute(""a"").toTensor();<NewLine>  std::cout &lt;&lt; a &lt;&lt; ""\n"";<NewLine><NewLine>  torch::Tensor b = container.get_attribute(""b"").toTensor();<NewLine>  std::cout &lt;&lt; b &lt;&lt; ""\n"";<NewLine><NewLine>  std::string c = container.get_attribute(""c"").toStringRef();<NewLine>  std::cout &lt;&lt; c &lt;&lt; ""\n"";<NewLine><NewLine>  int64_t d = container.get_attribute(""d"").toInt();<NewLine>  std::cout &lt;&lt; d &lt;&lt; ""\n"";<NewLine><NewLine>  return 0;<NewLine>}<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you are just trying to move values between Python and C++, the API in <a href=""https://github.com/pytorch/pytorch/issues/20356#issuecomment-567663701"">this comment</a> is now the blessed way to do that. But to answer your question, for a model like</p><NewLine><pre><code class=""lang-python"">import torch<NewLine><NewLine>class Model(torch.nn.Module):<NewLine><NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.w1 = torch.nn.Parameter(torch.ones(2, 2))<NewLine>        self.w2 = torch.nn.Parameter(torch.ones(2, 2))<NewLine><NewLine>    def forward(self):<NewLine>        return self.w1 + self.w2<NewLine><NewLine>m = torch.jit.script(Model())<NewLine>torch.jit.save(m, 'model.pt')<NewLine></code></pre><NewLine><p>You can iterate over the parameters like:</p><NewLine><pre><code class=""lang-cpp"">#include &lt;torch/script.h&gt;<NewLine><NewLine>int main() {<NewLine>  auto m = torch::jit::load(""model.pt"");<NewLine>  for (torch::jit::script::Named&lt;at::Tensor&gt; p :<NewLine>       m.named_parameters(/*recurse=*/true)) {<NewLine>    std::cout &lt;&lt; p.name &lt;&lt; "": "" &lt;&lt; p.value &lt;&lt; ""\n"";<NewLine>  }<NewLine>  return 0;<NewLine>}<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/veryluckyxyz; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: February 10, 2020,  7:59pm; <NewLine> REPLY_DATE 2: February 10, 2020,  8:11pm; <NewLine> REPLY_DATE 3: February 13, 2020,  6:46pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
69452,Will torch.jit.script change the output of model?,2020-02-12T07:30:45.160Z,1,130,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have converted a model to scipt module using <code>torch.jit.scipt</code>. However, I find there is some difference between the output of the original model and the output of the scripted model like:</p><NewLine><pre><code class=""lang-auto"">model.eval()<NewLine>scripted_model = torch.jit.script(model)<NewLine><NewLine># there is some difference between output and scripted_output<NewLine>output = model(inputs)<NewLine>scripted_output = scripted_model(inputs)<NewLine></code></pre><NewLine><p>Each element has an average difference of 1e-6</p><NewLine><p>Is this phenomenon normal?</p><NewLine></div>",https://discuss.pytorch.org/u/BobChen,,BobChen,"February 12, 2020,  7:30am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, this small difference is expected due to the limited precision for float32.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thans for your fast reply:)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/BobChen; <NewLine> ,"REPLY_DATE 1: February 12, 2020,  7:53am; <NewLine> REPLY_DATE 2: February 12, 2020,  8:14am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> 
69045,torch.jit.Future type annotation,2020-02-08T07:57:11.681Z,1,191,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is there any way to give a type hint to the output of <code>torch.jit._wait</code>?</p><NewLine><p>The below example fails to compile to torchscript</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from typing import List<NewLine><NewLine>def process(i: int) -&gt; int:<NewLine>    return i + 1<NewLine><NewLine>@torch.jit.script<NewLine>def process_many(l: List[int]) -&gt; List[int]:<NewLine>    futs: List[torch.jit.Future] = []<NewLine>    out: List[int] = []<NewLine><NewLine>    for v in l:<NewLine>        futs.append(torch.jit._fork(process, v))<NewLine><NewLine>    for f in futs:<NewLine>        out.append(torch.jit._wait(f))<NewLine><NewLine>    return out<NewLine></code></pre><NewLine><p>with the error</p><NewLine><pre><code class=""lang-auto"">  File ""/usr/local/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1281, in script<NewLine>    fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))<NewLine>RuntimeError:<NewLine>Unknown type name 'torch.jit.Future':<NewLine>  File ""./demo.py"", line 10<NewLine>@torch.jit.script<NewLine>def process_many(l: List[int]) -&gt; List[int]:<NewLine>    futs: List[torch.jit.Future] = []<NewLine>               ~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>    out: List[int] = []<NewLine></code></pre><NewLine><p>However, if i change the for loop to a list comprehension, it compiles fine</p><NewLine><pre><code class=""lang-auto"">@torch.jit.script<NewLine>def process_many(l: List[int]) -&gt; List[int]:<NewLine>    futs = [torch.jit._fork(process, v) for v in l]<NewLine>    out: List[int] = []<NewLine><NewLine>    for f in futs:<NewLine>        out.append(torch.jit._wait(f))<NewLine><NewLine>    return out<NewLine><NewLine># &gt;&gt;&gt; print(process_many([0, 3, 5])<NewLine># [1, 4, 6]<NewLine></code></pre><NewLine><p>What haven’t been able to figure out is if <code>torch.jit.Future</code> is the correct type annotation for me to be using here. For now, it seems possible to work around this just using a list comprehension, but it could get ugly if the logic becomes more complex.</p><NewLine><p>I’m on 1.4.0, if it makes any difference</p><NewLine></div>",https://discuss.pytorch.org/u/cnapun,(Nikil),cnapun,"February 8, 2020,  8:31am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Our future support is really only half done (it’s still an internal API, hence the <code>_</code> at the beginning of <code>fork</code> and <code>wait</code>) so I don’t think there’s a way to do this without the list comprehension you mentioned. Could you <a href=""https://github.com/pytorch/pytorch/issues/new/choose"">file a bug report here</a>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks! just discovered that there seems to be a bug report out for this already<br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/26578"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/26578"" rel=""nofollow noopener"" target=""_blank"">[jit] Fix future type annotation in python</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-09-20"" data-format=""ll"" data-time=""23:35:55"" data-timezone=""UTC"">11:35PM - 20 Sep 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/wanchaol"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""wanchaol"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/9443650?v=4"" width=""20""/><NewLine>          wanchaol<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">We should fix the Future type annotation to make it also work in python, fork will immediately execute the function single...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">high priority</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">jit</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">jit-backlog</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/cnapun; <NewLine> ,"REPLY_DATE 1: February 10, 2020,  7:14pm; <NewLine> REPLY_DATE 2: February 10, 2020,  7:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
68951,TorchScript Model inference slow in python,2020-02-07T09:20:16.310Z,2,522,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When testing if the prediction is correct after the model is transferring to script model, I found that the inference time is twice longer than pytorch model.</p><NewLine><pre><code class=""lang-auto""># pytorch 1.4, python 3.7<NewLine>out1 = model(input) # forward time: 0.05 sec<NewLine>ScriptModel = torch.jit.script(model, input)<NewLine>out2 = ScriptModel(input) # forward time: 0.12 sec<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/RebirthT,,RebirthT,"February 7, 2020,  9:20am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The first pass does some extra work (details in the link below), if you warm up <code>ScriptModel</code> by calling it a few times first the time should improve.</p><NewLine><aside class=""quote quote-modified"" data-post=""1"" data-topic=""63209""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v4/letter/s/c89c15/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/speed-of-custom-rnn-is-super-slow/63209"">Speed of Custom RNN is SUPER SLOW</a> <a class=""badge-wrapper bullet"" href=""/c/jit""><span class=""badge-category-bg"" style=""background-color: #0088CC;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""A category for TorchScript and the PyTorch JIT compiler"">jit</span></a><NewLine></div><NewLine><blockquote><NewLine>    Hi, <NewLine>Based on code here <NewLine><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py</a> <NewLine><NewLine>I write an example to compare the cumputation capability of native lstm and custom lstm. <NewLine>But I found that the speed of custom lstm is 100 times slower than native lstm class. <NewLine>here is my test code: <NewLine>import torch.nn as nn<NewLine>import time<NewLine>from models.custom_lstm import LSTMLayer, LSTMCell, script_lstm, LSTMState<NewLine><NewLine>input_size = 1024<NewLine>cell_size =2048<NewLine>batch_size =20 <NewLine>seq_len = 200 <NewLine><NewLine>native_lstm=n…<NewLine>  </blockquote><NewLine></aside><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>As the snippet below, script model actually get slower than average time 0.12sec in the first iteration in for loop as <a class=""mention"" href=""/u/driazati"">@driazati</a> mentioned above . However, after the first iteration, the rest is ~ 0.12 sec/img and is still twice longer than pytorch model.<br/><NewLine>Put more information here: batch_size=1</p><NewLine><pre><code class=""lang-auto"">pytorch 1.4, python 3.7<NewLine># pytorch model<NewLine>for i in range(image_number):<NewLine>    out1 = model(input) # forward time: 0.05 sec<NewLine><NewLine>#torchscript model<NewLine>ScriptModel = torch.jit.script(model, input)<NewLine>for i in range(image_number):<NewLine>    out2 = ScriptModel(input) # forward time: 0.12 sec if i &gt; 0, 3 sec if i==0<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you are using the native lstm, it is normal that the native one is faster than touchscript. Also, according to <a href=""https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/#writing-custom-rnns"" rel=""nofollow noopener"">here</a>, increasing the batch size can reduce the difference in performance. And according to my own experience, increasing the hidden size can also reduce the difference.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/RebirthT; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/G.M; <NewLine> ,"REPLY_DATE 1: February 7, 2020, 10:42pm; <NewLine> REPLY_DATE 2: February 9, 2020,  2:33pm; <NewLine> REPLY_DATE 3: February 9, 2020,  3:58pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
68825,Errors when use traced model to predict,2020-02-06T06:56:40.533Z,0,112,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone, I am a new to Torch Script.<br/><NewLine>I have traced my model When I loading and predicting result based on my model had some errors.<br/><NewLine>Anyone can have me figure it out?</p><NewLine><p>Input:</p><NewLine><pre><code class=""lang-cpp"">    at::Tensor f_f = torch::tensor({269,  90,  32, 269,  65,  85,  17, 269, 104,  13,   4,  21,  13, 269, 15,  95,   5, 269,  41,  30,  21,  29, 270, 270},+);<NewLine><NewLine>    at::Tensor f_p = torch::tensor({3,  7, 13, 17, 22, 23},torch::kFloat32);<NewLine><NewLine>    at::Tensor b_f = torch::tensor({270, 270,  29,  21,  30,  41, 269,   5,  95,  15, 269,  13,  21,   4, 13, 104, 269,  17,  85,  65, 269,  32,  90, 269},torch::kFloat32);<NewLine><NewLine>    at::Tensor b_p = torch::tensor({23, 20, 16, 10,  6,  1},torch::kFloat32);<NewLine><NewLine>    at::Tensor w_f = torch::tensor({1020, 1083, 4027, 3087,  262, 8765},torch::kFloat32);<NewLine><NewLine>    f_f = at::reshape(f_f , {24, 1});<NewLine>    f_p = at::reshape(f_p , {6, 1});<NewLine>    b_f = at::reshape(b_f , {24, 1});<NewLine>    b_p = at::reshape(b_p , {6, 1});<NewLine>    w_f = at::reshape(w_f , {6, 1});<NewLine><NewLine>    inputs.push_back(f_f);<NewLine>    inputs.push_back(f_p);<NewLine>    inputs.push_back(b_f);<NewLine>    inputs.push_back(b_p);<NewLine>    inputs.push_back(w_f);<NewLine><NewLine>    at::Tensor output = module.forward(inputs).toTensor();<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">Error:<NewLine>terminate called after throwing an instance of 'std::runtime_error'<NewLine>  what():  Expected tensor for argument #1 'indices' to have scalar type Long; but got CPUFloatType instead (while checking arguments for embedding)<NewLine>The above operation failed in interpreter, with the following stack trace:<NewLine>at code/__torch__/torch/nn/modules/module.py:8:12<NewLine>op_version_set = 1<NewLine>class Module(Module):<NewLine>  __parameters__ = [""weight"", ]<NewLine>  training : bool<NewLine>  weight : Tensor<NewLine>  def forward(self: __torch__.torch.nn.modules.module.Module,<NewLine>    forw_sentence: Tensor) -&gt; Tensor:<NewLine>    input = torch.embedding(self.weight, forw_sentence, -1, False, False)<NewLine>            ~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>    return input<NewLine>  def forward1(self: __torch__.torch.nn.modules.module.Module,<NewLine>    tensor: Tensor) -&gt; Tensor:<NewLine>    input = torch.embedding(self.weight, tensor, -1, False, False)<NewLine>    return input<NewLine>Compiled from code /home/bao/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py(1484): embedding<NewLine>/home/bao/anaconda3/lib/python3.7/site-packages/torch/nn/modules/sparse.py(114): forward<NewLine>/home/bao/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py(516): _slow_forward<NewLine>/home/bao/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py(530): __call__<NewLine>/home/bao/Desktop/segment_vtcc_test/lm_lstm_crf/model/lm_lstm_crf.py(222): forward<NewLine>/home/bao/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py(516): _slow_forward<NewLine>/home/bao/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py(530): __call__<NewLine>/home/bao/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py(1034): trace_module<NewLine>/home/bao/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py(882): trace<NewLine>/home/bao/Desktop/segment_vtcc_test/convert_model_1.py(132): &lt;module&gt;<NewLine></code></pre><NewLine><p>Thank in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/baobn,(Bui Ngoc Bao),baobn,"February 7, 2020, 10:43pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, are you able to post a repro of how to produce your traced model? Either a link to a <code>.pt</code> file or (even better) the code you use to trace and save the model.</p><NewLine><p>From what I can tell the input to this module <code>forw_sentence</code> is being used as the <code>indices</code> for the embedding operation (which, as in the error, must be of type Long instead of Float), but it’s hard to tell what the actual error is without the full repro.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: February 7, 2020, 10:46pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
68988,Parameter order changes when using TorchScript,2020-02-07T16:09:41.435Z,0,115,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>After updating PyTorch 1.0.1 -&gt; 1.4.0 I encountered some unwanted behavior. When creating a ScriptModule, the order of its Parameters changes. I’ll attach an example based on the <a href=""https://github.com/pytorch/benchmark/blob/master/rnns/fastrnns/custom_lstms.py"" rel=""nofollow noopener"">https://github.com/pytorch/benchmark/blob/master/rnns/fastrnns/custom_lstms.py</a> file.</p><NewLine><p>Two classes are defined. The only difference between the two is that one is a ScriptModule and the other an nn.Module. By plotting their parameter sizes we see that they are now stored in a different order than declared in the class (they also stop matching the native cell version). It might cause some backward compatibility problems (e.g. in the case of ‘custom_lstms.py’ the ‘test_script…’ functions stopped working).</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>from torch.nn import Parameter<NewLine>import torch.jit as jit<NewLine>from typing import List, Tuple<NewLine>from torch import Tensor<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">class LSTMCell(jit.ScriptModule):<NewLine>    def __init__(self, input_size, hidden_size):<NewLine>        super(LSTMCell, self).__init__()<NewLine>        self.input_size = input_size<NewLine>        self.hidden_size = hidden_size<NewLine>        self.weight_ih = Parameter(torch.randn(4 * hidden_size, input_size))<NewLine>        self.weight_hh = Parameter(torch.randn(4 * hidden_size, hidden_size))<NewLine>        self.bias_ih = Parameter(torch.randn(4 * hidden_size))<NewLine>        self.bias_hh = Parameter(torch.randn(4 * hidden_size))<NewLine><NewLine>    @jit.script_method<NewLine>    def forward(self, input, state):<NewLine>        # type: (Tensor, Tuple[Tensor, Tensor]) -&gt; Tensor<NewLine>        return input<NewLine>    <NewLine><NewLine>class LSTMCellNoJit(nn.Module):<NewLine>    def __init__(self, input_size, hidden_size):<NewLine>        super(LSTMCellNoJit, self).__init__()<NewLine>        self.input_size = input_size<NewLine>        self.hidden_size = hidden_size<NewLine>        self.weight_ih = Parameter(torch.randn(4 * hidden_size, input_size))<NewLine>        self.weight_hh = Parameter(torch.randn(4 * hidden_size, hidden_size))<NewLine>        self.bias_ih = Parameter(torch.randn(4 * hidden_size))<NewLine>        self.bias_hh = Parameter(torch.randn(4 * hidden_size))<NewLine><NewLine>    def forward(self, input, state):<NewLine>        return input<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">net = LSTMCell(2, 10)<NewLine><NewLine>pars = [p for p in net.parameters()]<NewLine>for i in range(len(pars)):<NewLine>    print(pars[i].shape)<NewLine></code></pre><NewLine><p>Output:<br/><NewLine>torch.Size([40, 10])<br/><NewLine>torch.Size([40])<br/><NewLine>torch.Size([40])<br/><NewLine>torch.Size([40, 2])</p><NewLine><pre><code class=""lang-auto"">net = LSTMCellNoJit(2, 10)<NewLine><NewLine>pars = [p for p in net.parameters()]<NewLine>for i in range(len(pars)):<NewLine>    print(pars[i].shape)<NewLine></code></pre><NewLine><p>Output:<br/><NewLine>torch.Size([40, 2])<br/><NewLine>torch.Size([40, 10])<br/><NewLine>torch.Size([40])<br/><NewLine>torch.Size([40])</p><NewLine></div>",https://discuss.pytorch.org/u/hungry,,hungry,"February 7, 2020,  4:09pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey thanks for the report / repro! This is an issue that’s been open for a while, we’re planning to fix it soon</p><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/25047"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/25047"" target=""_blank"">ScriptModule and nn.Module parameter ordering difference</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-08-22"" data-format=""ll"" data-time=""21:27:58"" data-timezone=""UTC"">09:27PM - 22 Aug 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/wanchaol"" target=""_blank""><NewLine><img alt=""wanchaol"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/9443650?v=4"" width=""20""/><NewLine>          wanchaol<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">class eagerNet(nn.Module):<NewLine> def __init__(self):<NewLine> super(eagerNet, self).__init__()<NewLine> self.conv1 = nn.Conv2d(1, 10, kernel_size=5)<NewLine> self.conv2 = nn.Conv2d(10, 20, kernel_size=5)<NewLine> self.fc1 = nn.Linear(320, 50)<NewLine> self.fc2...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">jit</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">jit-backlog</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: February 11, 2020,  8:30am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
67772,Can&rsquo;t enumerate through a ModuleList,2020-01-27T12:48:26.002Z,5,129,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m using Pytorch 1.2.0 with Python 3.7, and I can’t enumerate through a ModuleList.<br/><NewLine>I’m using this code</p><NewLine><pre><code class=""lang-auto"">for i,direction in enumerate(self.directions):<NewLine>            state = states[i]<NewLine>            out, out_state = direction(inp, state)<NewLine>            outputs += [out]<NewLine>            output_states += [out_state]<NewLine></code></pre><NewLine><p>, which is based from <a href=""https://github.com/pytorch/pytorch/blob/5b321a098594693eca8408169f475cb334856655/benchmarks/fastrnns/custom_lstms.py#L219"" rel=""nofollow noopener"">here</a>. and I’m getting</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>'module' object is not iterable:<NewLine></code></pre><NewLine><p>from</p><NewLine><pre><code class=""lang-auto"">        for i,direction in enumerate(self.directions):<NewLine>        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...  &lt;--- HERE<NewLine></code></pre><NewLine><p>I’ve clicked the <a href=""https://github.com/pytorch/pytorch/issues/14471"" rel=""nofollow noopener"">link</a> about enumerating in the original file(the link above), it said that enumerating is already supporterd. Since I can’t use it, can someone please tell me what should I do.</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/G.M,,G.M,"January 27, 2020, 12:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, <a class=""mention"" href=""/u/g.m"">@G.M</a>, this is fixed in our most recent release (1.4). Could you try updating your pytorch version and giving it another go?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank u very much. That’s the problem. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>But enumerate doesn’t seem to be in the 1.4.0 doc. And do u know where can I find a release note or something similar to that for Pytorch?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Good point, I think that was an omission. I’ll add a section to iterators to the docs shortly and link it here.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, turns out we already had a (small) section here: <a href=""https://pytorch.org/docs/master/jit_language_reference.html#iterables"" rel=""nofollow noopener"">https://pytorch.org/docs/master/jit_language_reference.html#iterables</a></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Alright thx <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/>, I thought it was in the 1.4.0 doc.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/G.M; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/G.M; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/G.M; <NewLine> ,"REPLY_DATE 1: January 28, 2020,  2:26am; <NewLine> REPLY_DATE 2: January 28, 2020,  2:25am; <NewLine> REPLY_DATE 3: January 28, 2020,  2:26am; <NewLine> REPLY_DATE 4: January 28, 2020,  9:07pm; <NewLine> REPLY_DATE 5: February 3, 2020,  7:40pm; <NewLine> REPLY_DATE 6: February 4, 2020,  2:17am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
68090,How to convert jit::Value into Tensor?,2020-01-30T06:16:41.801Z,3,273,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Currently, I load the torch script model as jit module.<br/><NewLine>I want to print all those parameters. So I want to convert jit::Value data strcture to Tensor.</p><NewLine><pre><code class=""lang-auto"">if(value.type()-&gt;expect&lt;torch::jit::TensorType&gt;()) {<NewLine>      auto tensor = value.toTensor();<NewLine>}<NewLine></code></pre><NewLine><p>I try to use same function of IValue but it does not work. Any idea?<br/><NewLine>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/JohnZ,(John),JohnZ,"January 30, 2020,  6:16am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>IValue</code> doesn’t have a <code>.type()</code> method since it’s a generic container for values (similar to Python’s <code>PyObject</code>). You should be able to just call <code>.toTensor()</code> to convert it to a tensor (make sure to guard it with a call to <code>.isTensor()</code> to make sure the <code>IValue</code> is what you expect).</p><NewLine><p>You can find more details <a href=""https://pytorch.org/cppdocs/api/structc10_1_1_i_value.html#exhale-struct-structc10-1-1-i-value"">here</a>. We are actively working on improving the JIT’s C++ API so issues like this will be more clear in the future.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/driazati"">@driazati</a> I think it does have a <code>type</code> method ? <a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/core/ivalue.cpp#L26"" rel=""nofollow noopener"">Here</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot, <a class=""mention"" href=""/u/driazati"">@driazati</a>.<br/><NewLine>I am using torch::jit::Value, not IValue .<br/><NewLine>I know how to convert IValue to tensor by toTensor.  How about for for torch::jit::Value or torch::jit::Node?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry for the confusion, you can think of <code>torch::jit::Value</code> as a variable in the graph (i.e. it has no concrete value until the graph is run), so there’s really no way to extract its value since it doesn’t have one. You can read more about it <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/docs/OVERVIEW.md#value"">here</a>. If you are trying to extract intermediate values from the graph, there’s no way to do that currently but you can follow along at <a href=""https://github.com/pytorch/pytorch/issues/21064"">this issue</a>.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I got it. So actually jit::Value is just symbol variable.<br/><NewLine>Thanks a lot, <a class=""mention"" href=""/u/driazati"">@driazati</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/JohnZ; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/JohnZ; <NewLine> ,"REPLY_DATE 1: January 30, 2020,  6:10pm; <NewLine> REPLY_DATE 2: January 30, 2020,  7:27pm; <NewLine> REPLY_DATE 3: January 30, 2020, 10:46pm; <NewLine> REPLY_DATE 4: January 31, 2020,  7:38pm; <NewLine> REPLY_DATE 5: February 1, 2020,  8:48pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> 
68208,Regarding Conv1d,2020-01-31T08:15:36.417Z,0,107,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi I couldn’t grammatically quite understand the following example</p><NewLine><blockquote><NewLine><blockquote><NewLine><blockquote><NewLine><p>m = nn.Conv1d(16, 33, 3, stride=2)<br/><NewLine>input = torch.randn(20, 16, 50)<br/><NewLine>output = m(input)</p><NewLine></blockquote><NewLine></blockquote><NewLine></blockquote><NewLine><p>I think the type of nn.Conv1d is class, and accordingly m should be an instance of the class. Why in the third line, m(input) is used like a function?</p><NewLine><p>Actually, it seems that &gt;&gt;&gt; output = m(input)  executes the function forward(self, input) defined in the class nn.Conv1d. So, to be grammatically correct,  I think &gt;&gt;&gt; output = m(input) should be changed into  &gt;&gt;&gt; output = m.forward(input) . But &gt;&gt;&gt; output = m(input)  indeed works.</p><NewLine><p>Could anybody explain a bit about this? What grammatical rule does it follow?</p><NewLine><p>The source code for Conv1d is at the following URL<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/docs/master/_modules/torch/nn/modules/conv.html#Conv1d"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/master/_modules/torch/nn/modules/conv.html#Conv1d</a></p><NewLine><p>Thank you very much!</p><NewLine></div>",https://discuss.pytorch.org/u/renpenghit,,renpenghit,"January 31, 2020,  8:22am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>nn.Module</code> (which <code>nn.Conv1d</code> and all the <code>nn.</code> modules extend), implements the <code>__call__</code> magic method so that it can be used like a function as in your example. <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py#L530"">The implementation can be found here</a>. Using <code>__call__</code> has some extra processing for forward/backward hooks, but, as you have found, its main job is to call <code>forward()</code></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you so much for your explanation! Truly helpful!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/renpenghit; <NewLine> ,"REPLY_DATE 1: January 31, 2020,  7:41pm; <NewLine> REPLY_DATE 2: February 1, 2020,  1:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
65606,Upgrade issue with Trace and Autograd (from torch 1.0.1 to torch 1.3.1),2020-01-02T16:37:36.353Z,5,332,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’ve been trying to migrate some code from torch 1.0.1 to torch 1.3.1 but I’m struggling with an error that appears only with the latest version.</p><NewLine><p>Here is a minimal reproducible code to illustrate my problem (just a random MLP with embedding layers for first 3 categorical columns) :</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>class NN_emb_long(torch.nn.Module):<NewLine>    def __init__(self,):<NewLine>        super(NN_emb_long, self).__init__()<NewLine>        emb_layers = []<NewLine>        for i in range(3):<NewLine>            emb_layers.append(torch.nn.Embedding(5, 2))<NewLine>        self.emb_layers = torch.nn.ModuleList(emb_layers)        <NewLine>        self.lin1 = torch.nn.Linear(8, 16)<NewLine>        self.lin_out = torch.nn.Linear(16, 2)<NewLine>    <NewLine>    def forward(self, x):<NewLine>        embs_list = []<NewLine>        for i in range(3):<NewLine>            embs = self.emb_layers[i](x[:,i].long())<NewLine>            embs_list.append(embs)<NewLine>        post_embed = torch.cat([x[:, torch.Tensor([3,4]).long()]]+embs_list, dim=1)<NewLine>        res = self.lin1(post_embed)<NewLine>        res = torch.nn.ReLU()(res)<NewLine>        res = self.lin_out(res)<NewLine>        return res, post_embed<NewLine><NewLine>NN = NN_emb_long()<NewLine>input_example = torch.ones((10, 5)).requires_grad_(True)<NewLine><NewLine>probas, post_embeddings = NN(input_example)<NewLine>grad_outputs = torch.ones(input_example.shape[0],2)<NewLine>G = torch.autograd.grad(outputs=probas,<NewLine>                        inputs=post_embeddings,<NewLine>                        grad_outputs=grad_outputs,<NewLine>                        only_inputs=True,<NewLine>                        retain_graph=True<NewLine>                       )[0]<NewLine>print(G.shape)<NewLine># until this everything should work on both version<NewLine># taking the trace shows an error only with torch 1.3.1<NewLine><NewLine>basic_trace = torch.jit.trace(NN, input_example, check_trace=True)<NewLine>probas,  post_embeddings = basic_trace(input_example)<NewLine>grad_outputs = torch.ones(10,2)<NewLine>G = torch.autograd.grad(outputs=probas,<NewLine>                        inputs=post_embeddings,<NewLine>                        grad_outputs=grad_outputs,<NewLine>                        only_inputs=True,<NewLine>                        retain_graph=True,<NewLine>                       )[0]<NewLine>print(G.shape)<NewLine><NewLine></code></pre><NewLine><p>This code should run fine with torch 1.0.1 but fail with the following error with torch 1.3.1:</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-11-d55fb440d76f&gt; in &lt;module&gt;<NewLine>      6                         grad_outputs=grad_outputs,<NewLine>      7                         only_inputs=True,<NewLine>----&gt; 8                         retain_graph=True,<NewLine>      9                        )[0]<NewLine>     10 print(G.shape)<NewLine><NewLine>.cache/poetry/engine-py3.6/lib/python3.6/site-packages/torch/autograd/__init__.py in grad(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)<NewLine>    155     return Variable._execution_engine.run_backward(<NewLine>    156         outputs, grad_outputs, retain_graph, create_graph,<NewLine>--&gt; 157         inputs, allow_unused)<NewLine>    158 <NewLine>    159 <NewLine><NewLine>RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.<NewLine></code></pre><NewLine><p>Could you please help me understand what happens here?</p><NewLine><p>Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/Sebastien_Fischman,(Sébastien Fischman),Sebastien_Fischman,"January 3, 2020, 10:21am",3 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like my issue went under the radars! Anyone? Any idea?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey sorry about the delay! Not sure what the actual bug is, but this code seems to work fine again for me on master. Could you try it with the nightly version of PyTorch or try with 1.4 (which should be released very soon!)?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you, it seems that the problem is indeed solved by upgrading directly to 1.4.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/driazati"">@driazati</a></p><NewLine><p>I work  with <a class=""mention"" href=""/u/sebastien_fischman"">@Sebastien_Fischman</a> and indeed it works in 1.4.0 now.<br/><NewLine>But we still have a problem : the models we trained beforehand on version 1.0.1 and saved as a JIT does not work anymore, when trying to load them.</p><NewLine><p>I searched for a while, and I found that inside the JIT the problematic code was</p><NewLine><p><code> _12 = torch.index(_11, [annotate(Tensor, None), cont_idxs])</code></p><NewLine><p>With new version, code would be something like</p><NewLine><p><code> _12 = torch.index(_11, annotate(List[Optional[Tensor]],[None, cont_idxs]))</code></p><NewLine><p>If I changed manually the code for this new line, it works.</p><NewLine><p>Problem is we are using PyTorch models in production, and manually modifying the models will be an issue.</p><NewLine><p>Do you have a migration script to update the JIT file ? or any solution for JIT to be retrocompatible ?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you able to share the model file that was saved on 1.0.1? The JIT should always be 100% backwards compatible so this sounds like a bug.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sure, here is the JIT file : <a href=""https://srv-file9.gofile.io/download/TCKSJe/9e06eca9fd9c2e.pt"" rel=""nofollow noopener"">https://srv-file9.gofile.io/download/TCKSJe/9e06eca9fd9c2e.pt</a><br/><NewLine>I could not add it to the message.</p><NewLine><p>This JIT was also not working in 1.3 version also <a class=""mention"" href=""/u/driazati"">@driazati</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/driazati"">@driazati</a> looks like sharing a model with this kind of link is a bad idea <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p><a class=""mention"" href=""/u/hartorn"">@Hartorn</a> account has been locked for sharing this ^^ but it’s safe to open if you wish to have a look at it!</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/driazati"">@driazati</a>,<br/><NewLine>Sorry to be a bother, but did you have time to have a look at the model ?<br/><NewLine>Do you think this bug of pytorch will be fixed, or should we find a way to update/modify our JIT file to be compatible ?</p><NewLine><p>Regards</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, I wasn’t able to download the file (I get a ""You are not authorized to download this file "" error), but I think I reproduced the same issue. I think <a href=""https://github.com/pytorch/pytorch/pull/32834"">this patch</a> should fix the bug. If you are able to build PyTorch from source (see below), you can verify that it fixes the issue</p><NewLine><pre><code class=""lang-bash"">git clone https://github.com/pytorch/pytorch --depth=1<NewLine>cd pytorch<NewLine>git fetch origin driazati/fix_annotate<NewLine>git reset --hard origin/driazati/fix_annotate<NewLine># Now build PyTorch from source<NewLine># https://github.com/pytorch/pytorch/#from-source<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks <a class=""mention"" href=""/u/driazati"">@driazati</a>, I think we’ll go for a migration script on our side anyway! Thanks for the help, really appreciate it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Sebastien_Fischman; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Sebastien_Fischman; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Hartorn; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Hartorn; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Sebastien_Fischman; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Hartorn; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Sebastien_Fischman; <NewLine> ,"REPLY_DATE 1: January 15, 2020,  8:48am; <NewLine> REPLY_DATE 2: January 15, 2020, 11:08pm; <NewLine> REPLY_DATE 3: January 17, 2020,  5:38pm; <NewLine> REPLY_DATE 4: January 22, 2020,  7:33am; <NewLine> REPLY_DATE 5: January 22, 2020,  6:36pm; <NewLine> REPLY_DATE 6: January 27, 2020,  5:03pm; <NewLine> REPLY_DATE 7: January 27, 2020,  8:45am; <NewLine> REPLY_DATE 8: January 30, 2020,  6:11pm; <NewLine> REPLY_DATE 9: January 30, 2020, 10:57pm; <NewLine> REPLY_DATE 10: January 31, 2020,  8:43am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> 
68078,Tracing custom autograd.Function,2020-01-30T03:52:51.948Z,1,229,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a custom autograd.Function:</p><NewLine><pre><code class=""lang-auto"">class raw(autograd.Function):<NewLine>    @staticmethod<NewLine>    def forward(ctx, inp):<NewLine>        ctx.a = (inp * inp + 1).reciprocal()<NewLine>        ctx.b = ctx.a.sqrt()<NewLine>        return inp * ctx.b<NewLine><NewLine>    @staticmethod<NewLine>    def backward(ctx, grad_output):<NewLine>        return grad_output * ctx.a * ctx.b<NewLine></code></pre><NewLine><p>and I want to trace this function, but when I do:</p><NewLine><pre><code class=""lang-auto"">jit.trace(raw.apply, example_inputs=tc.randn(1))<NewLine></code></pre><NewLine><p>, I get the error from this line:</p><NewLine><pre><code class=""lang-auto"">......................<NewLine>    jit.trace(raw.apply, example_inputs=tc.randn(1))<NewLine>  File ""...\Python37\lib\site-packages\torch\jit\__init__.py"", line 903, in trace<NewLine>    name = _qualified_name(func)<NewLine>  File ""...\Python37\lib\site-packages\torch\_jit_internal.py"", line 696, in _qualified_name<NewLine>    ""__module__ can't be None."".format(name))<NewLine>RuntimeError: Could not get qualified name for class 'apply': __module__ can't be None.<NewLine></code></pre><NewLine><p>This code used to work for pytorch 1.1.0, but after I updated it to 1.4.0 recently, I got this error. I’m using Python3.7.3 on windows10.</p><NewLine></div>",https://discuss.pytorch.org/u/G.M,,G.M,"January 30, 2020,  3:52am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Reproduced. Could you please open a new issue at <a href=""https://github.com/pytorch/pytorch/issues"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues</a>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Opened it: <a href=""https://github.com/pytorch/pytorch/issues/32822"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/32822</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/peterjc123; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/G.M; <NewLine> ,"REPLY_DATE 1: January 30, 2020,  5:32am; <NewLine> REPLY_DATE 2: January 30, 2020,  7:54am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
67739,Build speed_benchmark_torch for x86 instead of android?,2020-01-27T02:22:10.596Z,0,118,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In <a href=""https://discuss.pytorch.org/t/speed-benchmarking-on-android/67340/5"">this thread</a>, I learned how to compile for Android the file <code>pytorch/binaries/speed_benchmark_torch.cc</code>, which lets you benchmark the speed of torchscript models. However, for debugging purposes, I’d like to build this file for my desktop computer’s Intel CPU, so that I can better debug some failures I am getting.</p><NewLine><p>Does anyone know how to compile  <code>speed_benchmark_torch.cc</code> for desktop CPU?</p><NewLine></div>",https://discuss.pytorch.org/u/solvingPuzzles,,solvingPuzzles,"January 27, 2020,  2:22am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I figured out a way to do it. I do a fresh build of pytorch as follows:</p><NewLine><pre><code class=""lang-auto"">cd pytorch<NewLine>export BUILD_BINARY=1<NewLine>python setup.py build_ext <NewLine></code></pre><NewLine><p>An x86 CPU compatible (rather than Android or iOS compatible) binary appears here:<br/><NewLine><code>pytorch/build/bin/speed_benchmark_torch</code></p><NewLine><p>And, I was able to run it successfully on my desktop computer’s CPU.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/solvingPuzzles; <NewLine> ,"REPLY_DATE 1: January 29, 2020,  6:34am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
62634,Can i convert torch script module to nn module,2019-12-01T09:51:28.774Z,2,646,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>i have a problem when converting torch script module in to ONNX module,<br/><NewLine>but it’s works just fine on “normal” module,</p><NewLine><p>i open a bug request about that at <a href=""https://github.com/pytorch/pytorch/issues/30512"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/30512</a>,</p><NewLine><p>but if i could convert the torch script module to nn module it’s will solve my issue,</p><NewLine><p>Thanks,<br/><NewLine>Liron</p><NewLine><p>code example:(you can ignore the test_normal )</p><NewLine><pre><code class=""lang-python"">    def _test_normal(self, num_classes, dummy_input):<NewLine>        model = torchvision.models.resnet18(num_classes=num_classes)<NewLine>        model_state_fixed = {}<NewLine>        for k, v in self._model_state.items():<NewLine>            k_fixed = k[3:len(k)]<NewLine>            model_state_fixed[k_fixed] = v<NewLine>        model.load_state_dict(model_state_fixed)<NewLine>        torch.onnx.export(model, dummy_input, ""/app_data/test_torch_script/torch_script_test_normal.onnx"")<NewLine><NewLine>    def convert(self):<NewLine>        loaded = torch.jit.load(self._torch_script_path)<NewLine>        # loaded.load_state_dict(self._model_state)<NewLine>        dummy_input = torch.randn(1, 3, 224, 224)<NewLine>        target = loaded(dummy_input)<NewLine>        self._test_normal(num_classes=len(target[0]), dummy_input=dummy_input)<NewLine>        torch.onnx.export(loaded, dummy_input, self._out_onnx_path, verbose=True,<NewLine>                          operator_export_type=torch.onnx.OperatorExportTypes.ONNX,<NewLine>                          example_outputs=target)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Liron_Mor_Yosef,(Liron Mor Yosef),Liron_Mor_Yosef,"December 4, 2019,  9:53pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There isn’t a way to extract an <code>nn.Module</code> from a compiled <code>ScriptModule</code>. Is it possible for you to instead export your original module instead of a <code>ScriptModule</code>?</p><NewLine><p>For some background, <code>torch.onnx.export</code> will use <code>torch.jit.trace</code> to get an exportable graph from an <code>nn.Module</code>. The ONNX exporter does not support all the features of TorchScript (e.g. if you used <code>torch.jit.script</code> to compile your model, it may not be possible to export that compiled module to ONNX), but relying on <code>torch.jit.trace</code> enforces that only supported features are used.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks for your replay,<br/><NewLine>how can i exported model to torch script and than load it back and export to onnx?</p><NewLine><p>to save as torch script i am using the following code:</p><NewLine><pre><code class=""lang-python"">model.eval()<NewLine>dummy_input = torch.randn(1, 3, 224, 224)<NewLine>traced = torch.jit.trace(model, dummy_input)<NewLine>traced.save(""/app_data/test_torch_script/torch_script_test.zip"")<NewLine></code></pre><NewLine><p>can i change the above code in order to use it’s output for exporting to onnx?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>fixed the bug at <a href=""https://github.com/pytorch/pytorch/issues/30512"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/30512</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Liron_Mor_Yosef; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Liron_Mor_Yosef; <NewLine> ,"REPLY_DATE 1: December 2, 2019,  8:39pm; <NewLine> REPLY_DATE 2: January 23, 2020,  2:05pm; <NewLine> REPLY_DATE 3: January 23, 2020,  2:05pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
67300,Get result after each node?,2020-01-21T18:47:21.436Z,0,80,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Correct me if this isn’t a JIT problem but I would like to see the resulting output after each node in a graph (traced graph specifically). Is there any way to do this easily? My current approach (which would only look at layers) is to modify the model to save each layer’s output and return that in the forward function but this isn’t ideal as I would like to look at each node and ideally not modify the model.</p><NewLine></div>",https://discuss.pytorch.org/u/flynntax,,flynntax,"January 21, 2020,  6:47pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There is no way to do this today, we’ve had <a href=""https://github.com/pytorch/pytorch/issues/21064"">this issue</a> open for a while which describes what you want. Until we implement something like that storing the results manually is the only way to go. A cleaner way than adding a giant return to your <code>forward</code> may be to store intermediate results as attributes on the module, something like</p><NewLine><pre><code class=""lang-python"">class X(nn.Module):<NewLine>  layer1: torch.Tensor<NewLine>  layer2: torch.Tensor<NewLine><NewLine>  def __init__(self):<NewLine>    self.layer1 = None<NewLine>    self.layer2 = None<NewLine>    self.fc1 = nn.Linear(10, 10)<NewLine>  <NewLine>  def forward(self, x):<NewLine>    self.layer1 = self.fc1(x)<NewLine>    self.layer2 = self.fc1(self.layer1)<NewLine><NewLine>   return self.layer2<NewLine><NewLine>torch.jit.script(X())<NewLine></code></pre><NewLine><p><code>torchvision</code> has a similar problem, they use <a href=""https://github.com/pytorch/vision/blob/master/torchvision/models/_utils.py"">this class</a> as a workaround.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: January 21, 2020, 10:58pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
66847,How to use TensorRT backend to accelerate TorchScript,2020-01-16T11:43:29.766Z,1,218,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Can we directly use TensorRT to accelerate TorchScript without ONNX?</p><NewLine></div>",https://discuss.pytorch.org/u/BobChen,,BobChen,"January 16, 2020, 11:43am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could try to use <a href=""https://github.com/NVIDIA-AI-IOT/torch2trt"">tensor2trt</a> to directly use TensorRT with the Python API. However, as far as I know, the supported methods might be limited, so you would have to check, if your current model uses only supported layers.</p><NewLine><p>A bit unrelated to your question, but might also be interesting: you could have a look ar <a href=""https://devblogs.nvidia.com/how-to-deploy-real-time-text-to-speech-applications-on-gpus-using-tensorrt/"">this blogpost</a> to see how to deploy Tacotron2 and Waveglow to TensorRT7.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I will check it, thank you very much:)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/BobChen; <NewLine> ,"REPLY_DATE 1: January 19, 2020,  1:09am; <NewLine> REPLY_DATE 2: January 19, 2020,  3:05am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
66967,JIT does not autodiff conv &amp; batchnorm,2020-01-17T11:51:28.470Z,0,107,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>recently I found that JIT does not create DifferentiableGraph nodes corresponding to Conv and BN operations.</p><NewLine><p>Both ops are not registered in symbolic_script.h and considered to be “non-differentiable” in the sense that their gradients are always computed by Autograd engine, which loses the opportunity of making bigger DifferentiableGraphs.</p><NewLine><p>What’s the reason behind such non-optimal behavior of JIT?</p><NewLine></div>",https://discuss.pytorch.org/u/ililiilliilill,,ililiilliilill,"January 17, 2020, 11:53am",,,,,
66855,Jit enabling requires grad,2020-01-16T13:31:23.170Z,0,106,"<div class=""post"" itemprop=""articleBody""><NewLine><p>using traced model after jit compilation to do forward pass, <code>torch.clamp()</code> seems to introduce <code>requires_grad</code> to be True to a forward. In normal mode, it is fine but when I use the traced model this happens. To confirm <code>torch.clamp()</code> is introducing this I print <code>x.requires_grad</code> before and after the clamp operation.<br/><NewLine>This bug seems weird. Any fix?</p><NewLine></div>",https://discuss.pytorch.org/u/vishalagarwal,(Vishal Agarwal),vishalagarwal,"January 16, 2020,  1:31pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you have a minimal code snippet, as I cannot reproduce this issue?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: January 17, 2020,  1:30am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
66475,Jit trace issue,2020-01-13T05:00:12.575Z,0,300,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My model is Object detection model and it contains interpolate layer. Model is converting to quantized model perfectly but when I am doing torch.jit.script it is giving issues.<br/><NewLine>I tried changing torch and torchvision to currently available nightly version but it is still giving issues.</p><NewLine><p>Error while in stable version of torch and torchvision -</p><NewLine><pre><code class=""lang-auto"">Arguments for call are not valid.<NewLine>The following variants are available:<NewLine>aten::__interpolate(Tensor input, int? size=None, float[]? scale_factor=None, str mode='\156\145\141\162\145\163\164', bool? align_corners=None) -&gt; (Tensor):<NewLine>  Expected a value of type 'Optional[List[float]]' for argument 'scale_factor' but instead found type 'int'.<NewLine><NewLine>  aten::__interpolate(Tensor input, int[]? size=None, float[]? scale_factor=None, str mode='\156\145\141\162\145\163\164', bool? align_corners=None) -&gt; (Tensor):<NewLine>  Expected a value of type 'Optional[List[float]]' for argument 'scale_factor' but instead found type 'int'.<NewLine><NewLine>  aten::__interpolate(Tensor input, int? size=None, float? scale_factor=None, str mode='\156\145\141\162\145\163\164', bool? align_corners=None) -&gt; (Tensor):<NewLine>  Expected a value of type 'Optional[float]' for argument 'scale_factor' but instead found type 'int'.<NewLine><NewLine>  aten::__interpolate(Tensor input, int[]? size=None, float? scale_factor=None, str mode='\156\145\141\162\145\163\164', bool? align_corners=None) -&gt; (Tensor):<NewLine>  Expected a value of type 'Optional[float]' for argument 'scale_factor' but instead found type 'int'.<NewLine></code></pre><NewLine><p>Error after moving to nighly version of torch and torchvision -</p><NewLine><pre><code class=""lang-auto"">Arguments for call are not valid.<NewLine>The following variants are available:<NewLine><NewLine>  aten::__interpolate(Tensor input, int? size=None, float[]? scale_factor=None, str mode=""nearest"", bool? align_corners=None, bool? recompute_scale_factor=None) -&gt; (Tensor):<NewLine>  Expected a value of type 'Optional[List[float]]' for argument 'scale_factor' but instead found type 'int'.<NewLine><NewLine>  aten::__interpolate(Tensor input, int[]? size=None, float[]? scale_factor=None, str mode=""nearest"", bool? align_corners=None, bool? recompute_scale_factor=None) -&gt; (Tensor):<NewLine>  Expected a value of type 'Optional[List[float]]' for argument 'scale_factor' but instead found type 'int'.<NewLine><NewLine>  aten::__interpolate(Tensor input, int? size=None, float? scale_factor=None, str mode=""nearest"", bool? align_corners=None, bool? recompute_scale_factor=None) -&gt; (Tensor):<NewLine>  Expected a value of type 'Optional[float]' for argument 'scale_factor' but instead found type 'int'.<NewLine><NewLine>  aten::__interpolate(Tensor input, int[]? size=None, float? scale_factor=None, str mode=""nearest"", bool? align_corners=None, bool? recompute_scale_factor=None) -&gt; (Tensor):<NewLine>  Expected a value of type 'Optional[float]' for argument 'scale_factor' but instead found type 'int'.<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/mohit7,(Mohit Ranawat),mohit7,"January 13, 2020,  5:00am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s hard to tell without more context, are you able to show the code that is causing the error so we can reproduce it on our end?</p><NewLine><p>For some background, TorchScript does not do coercion from <code>int</code> to <code>float</code> like Python does, so when you’re calling interpolate you might have to do something like <code>float(my_scale_factor)</code> where <code>my_scale_factor</code> is an <code>int</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: January 17, 2020,  3:50am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
44532,Does JIT makes model faster?,2019-05-06T16:44:54.585Z,3,2187,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is there any JIT performance measurements? Does it makes a model any faster or the only benefit of involving JIT is ability to save model and perform inference in any other environment except python?</p><NewLine></div>",https://discuss.pytorch.org/u/analvikingur,(Daniil Gavrilov),analvikingur,"May 6, 2019,  4:44pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, we do monitor the performance of certain bits. For example the recent <a href=""https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/"" rel=""nofollow noopener"">PyTorch blog on RNN speedups</a> discusses benchmarks we’ve been monitoring quite closely and continue to work against. ResNet performance it also regularly checked.</p><NewLine><p>That said, whether any given model sees significant speedups, depends.</p><NewLine><ul><NewLine><li>I always give the ballpark figure of 10% speedup for moving from Python to C++ - I got this number from a couple of specific models, e.g. when you do a “1-1” translation into C++ of the LLTM model used in the C+±Extension tutorial. Your model will see different numbers. A similar speedup probably is there for the JIT.</li><NewLine><li>Where the JIT really get large speedups is when one of the optimizations can fully come into play. E.g. if you have chains of elementwise operations, they will be fused into a single kernel. As those are typically memory-bound, fusing two elementwise ops will be ~2x as fast as doing them separately.</li><NewLine></ul><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I traced the <a href=""https://huggingface.co/pytorch-transformers/examples.html"" rel=""nofollow noopener"">BERT</a> model from HuggingFace PyTorchTransformers library and getting following results for 10 iterations.<br/><NewLine>a) Using Python runtime for running the forward: 979292 µs</p><NewLine><pre><code class=""lang-auto"">import time<NewLine>model = torch.jit.load('models_backup/2_2.pt')<NewLine>x = torch.randint(2000, (1, 14), dtype=torch.long, device='cpu')<NewLine>start = time.time()<NewLine>for i in range(10):<NewLine>    model(x)<NewLine>end = time.time()<NewLine>print((end - start)*1000000, ""µs"")<NewLine></code></pre><NewLine><p>b) Using C++ runtime for running the forward: 3333758 µs which is almost 3x of what Python</p><NewLine><pre><code class=""lang-auto"">  torch::Tensor x = torch::randint(index_max, {1, inputsize}, torch::dtype(torch::kInt64).device(torch::kCPU));<NewLine>  input.push_back(x);<NewLine>  #endif<NewLine>  // Execute the model and turn its output into a tensor.<NewLine>  auto outputs = module-&gt;forward(input).toTuple();<NewLine>  auto start = chrono::steady_clock::now();<NewLine>  for (int16_t i = 0; i&lt;10; ++i)<NewLine>  {<NewLine>    outputs = module-&gt;forward(input).toTuple();<NewLine>  }<NewLine>  auto end = chrono::steady_clock::now();<NewLine>  cout &lt;&lt; ""Elapsed time in microseconds : "" <NewLine>		&lt;&lt; chrono::duration_cast&lt;chrono::microseconds&gt;(end - start).count()<NewLine>		&lt;&lt; "" µs"" &lt;&lt; endl;<NewLine></code></pre><NewLine><p><a class=""mention"" href=""/u/tom"">@tom</a> any suggestions on what am I missing ?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You are not even doing the comparison I had in mind. - If the C++/uses the JIT, you compare JIT called from Python vs JIT called from C++, and that should really have the same speed modulo constant overhead (which is not 6s).<br/><NewLine>Are you using the same inputs, libtorch, environment,…?</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, Tom<br/><NewLine>Which layers can be seen as chains of elementwise ops? Any detailed benchmarks? Thank you very much in advance.</p><NewLine><p>Best regards,</p><NewLine><p>Edward</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I teach that in my PyTorch internals training, if you’re near Munich and want to book a seat… <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/><br/><NewLine>But so the theory answer is any sequence of elementwise ops and the practical answer is anything that you see merged into fusion groups in <code>myfn.graph_for(*my_inputs) </code>. (Only is done on GPU by default.)<br/><NewLine>In addition to the <a href=""https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/"" rel=""nofollow noopener"">blog post linked above</a>, there is <a href=""https://lernapparat.de/more-jit-optimizations/"" rel=""nofollow noopener"">my blog post on optimizing LSTM backwards</a> and an old talk from me <a href=""https://lernapparat.de/pytorch-jit-android/"" rel=""nofollow noopener"">using this on a simple example IoU in detail</a>.<br/><NewLine>Obviously a lot more is to be had from extending what can be fused <a href=""https://github.com/pytorch/tvm/"" rel=""nofollow noopener"">TorchTVM</a> is a (highly experimental) approach to hook into TVM which is a (also experimental in my experience) framework that can optimize also reductions, which the JIT cannot .<br/><NewLine>Personally, I think a lot more could be had, but I’m not sure who makes that a top priority at the moment.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/sukuya; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Edwardmark; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: May 7, 2019,  9:38am; <NewLine> REPLY_DATE 2: July 25, 2019,  4:29am; <NewLine> REPLY_DATE 3: July 25, 2019,  5:00am; <NewLine> REPLY_DATE 4: January 10, 2020,  3:16am; <NewLine> REPLY_DATE 5: January 12, 2020,  9:20am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
66229,RuntimeError: “add_cpu/sub_cpu” not implemented for ‘Half’ when using Float16/Half,2020-01-09T21:41:58.965Z,2,1283,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I am testing out different types in PyTorch and noticed when calling torch.jit.trace where the input is float16/half, I am getting a runtime error (RuntimeError: “add_cpu/sub_cpu” not implemented for ‘Half’). Does tracing not work for float16? I believe I also tested with int32 inputs which work.</p><NewLine></div>",https://discuss.pytorch.org/u/flynntax,,flynntax,"January 9, 2020, 10:57pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think this might be more about operations that PyTorch supports on GPU than the types.<br/><NewLine>Does the same code run in plain PyTorch?</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>You are right in that this doesn’t seem to work for eager mode (guessing this is plain PyTorch) as just calling model(input) causes the same runtime error. But I am not running on a GPU right now (just a macbook). I guess I can probably change the category and rename the question. I guess Half is just not supported for CPU?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Indeed. (That’d be a fun weekend project for you! <img alt="":stuck_out_tongue:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/stuck_out_tongue.png?v=9"" title="":stuck_out_tongue:""/> )</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/flynntax; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: January 9, 2020, 10:30pm; <NewLine> REPLY_DATE 2: January 10, 2020,  6:32pm; <NewLine> REPLY_DATE 3: January 12, 2020,  9:09am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
66332,Torch jit script method bugs: expected a value of type Tensor for argument &lsquo;self&rsquo; but found Tensor[],2020-01-10T21:28:28.089Z,0,181,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to code up a module from jit.ScriptModule that looks like this:</p><NewLine><pre><code>class Model(jit.ScriptModule):<NewLine>    def __init__(self, size):<NewLine>        super().__init__()<NewLine>        self.rnn = nn.GRUCell(size, size)<NewLine><NewLine>    @jit.script_method<NewLine>    def forward(self, hidden, inputs):<NewLine>        something = [torch.empty(0)] * 10<NewLine>        return self.rnn(inputs, hidden)<NewLine></code></pre><NewLine><p>however, it triggers the following errors:<br/><NewLine>RuntimeError:<br/><NewLine>arguments for call are not valid:</p><NewLine><p>for operator aten::mul(Tensor self, Tensor other) -&gt; Tensor:<br/><NewLine>expected a value of type Tensor for argument ‘self’ but found Tensor[]</p><NewLine><p>I found the reason is that list comp is not available it jit, is there any fix?</p><NewLine></div>",https://discuss.pytorch.org/u/Peter_Ham,(Peter Ham),Peter_Ham,"January 10, 2020,  9:48pm",,,,,
66327,torch.jit.frontend.UnsupportedNodeError: Yield aren&rsquo;t supported:,2020-01-10T20:26:00.057Z,0,289,"<div class=""post"" itemprop=""articleBody""><NewLine><p>What does that mean?<br/><NewLine>This are the commands I ran<br/><NewLine><img alt=""torch_1"" data-base62-sha1=""cSYZCds01baJRZJzd6af36ZWtmY"" height=""129"" src=""https://discuss.pytorch.org/uploads/default/original/3X/5/a/5a513953a1ca15ec16136b21591784dd86f63b84.png"" width=""560""/></p><NewLine><p>Heres the full error:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1203, in script<NewLine>    return torch.jit.torch.jit._recursive.recursive_script(obj)<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 173, in recursive_script<NewLine>    return copy_to_script_module(mod, overload_stubs + stubs)<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 95, in copy_to_script_module<NewLine>    torch.jit._create_methods_from_stubs(script_module, stubs)<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1423, in _create_methods_from_stubs<NewLine>    self._c._create_methods(self, defs, rcbs, defaults)<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 195, in make_strong_submodule<NewLine>    new_strong_submodule = recursive_script(module)<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 116, in recursive_script<NewLine>    return create_constant_iterable_module(mod)<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 233, in create_constant_iterable_module<NewLine>    modules[key] = recursive_script(submodule)<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 173, in recursive_script<NewLine>    return copy_to_script_module(mod, overload_stubs + stubs)<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 95, in copy_to_script_module<NewLine>    torch.jit._create_methods_from_stubs(script_module, stubs)<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1423, in _create_methods_from_stubs<NewLine>    self._c._create_methods(self, defs, rcbs, defaults)<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 181, in create_method_from_fn<NewLine>    stub = torch.jit.script_method(fn, _jit_internal.createResolutionCallbackFromClosure(fn))<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1280, in script_method<NewLine>    ast = get_jit_def(fn, self_name=""ScriptModule"")<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/frontend.py"", line 169, in get_jit_def<NewLine>    return build_def(ctx, py_ast.body[0], type_line, self_name)<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/frontend.py"", line 209, in build_def<NewLine>    build_stmts(ctx, body))<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/frontend.py"", line 127, in build_stmts<NewLine>    stmts = [build_stmt(ctx, s) for s in stmts]<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/frontend.py"", line 127, in &lt;listcomp&gt;<NewLine>    stmts = [build_stmt(ctx, s) for s in stmts]<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/frontend.py"", line 185, in __call__<NewLine>    return method(ctx, node)<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/frontend.py"", line 346, in build_For<NewLine>    [build_expr(ctx, stmt.iter)], build_stmts(ctx, stmt.body))<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/frontend.py"", line 127, in build_stmts<NewLine>    stmts = [build_stmt(ctx, s) for s in stmts]<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/frontend.py"", line 127, in &lt;listcomp&gt;<NewLine>    stmts = [build_stmt(ctx, s) for s in stmts]<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/frontend.py"", line 185, in __call__<NewLine>    return method(ctx, node)<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/frontend.py"", line 352, in build_If<NewLine>    build_stmts(ctx, stmt.body),<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/frontend.py"", line 127, in build_stmts<NewLine>    stmts = [build_stmt(ctx, s) for s in stmts]<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/frontend.py"", line 127, in &lt;listcomp&gt;<NewLine>    stmts = [build_stmt(ctx, s) for s in stmts]<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/frontend.py"", line 185, in __call__<NewLine>    return method(ctx, node)<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/frontend.py"", line 279, in build_Expr<NewLine>    return ExprStmt(build_expr(ctx, value))<NewLine>  File ""/home/oywa/.local/lib/python3.6/site-packages/torch/jit/frontend.py"", line 184, in __call__<NewLine>    raise UnsupportedNodeError(ctx, node)<NewLine>torch.jit.frontend.UnsupportedNodeError: Yield aren't supported:<NewLine>at /home/oywa/.local/lib/python3.6/site-packages/torch/nn/modules/module.py:980:16<NewLine>            &gt;&gt;&gt; for name, module in model.named_children():<NewLine>            &gt;&gt;&gt;     if name in ['conv4', 'conv5']:<NewLine>            &gt;&gt;&gt;         print(module)<NewLine><NewLine>        """"""<NewLine>        memo = set()<NewLine>        for name, module in self._modules.items():<NewLine>            if module is not None and module not in memo:<NewLine>                memo.add(module)<NewLine>                yield name, module<NewLine>                ~ &lt;--- HERE<NewLine>'__torch__.torchvision.models.densenet.___torch_mangle_79._DenseBlock.forward' is being compiled since it was called from '__torch__.torchvision.models.densenet.___torch_mangle_67.DenseNet.forward'<NewLine>at /home/oywa/.local/lib/python3.6/site-packages/torchvision/models/densenet.py:155:8<NewLine>    def forward(self, x):<NewLine>        features = self.features(x)<NewLine>        ~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        out = F.relu(features, inplace=True)<NewLine>        out = F.adaptive_avg_pool2d(out, (1, 1))<NewLine>        out = torch.flatten(out, 1)<NewLine>        out = self.classifier(out)<NewLine>        return out<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/martinoywa,(Martin Oywa),martinoywa,"January 10, 2020,  8:26pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>torchvision was recently made 100% TorchScript compatible, but only in the most recent version. Can you check that your torchvision and PyTorch version?</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>print(torch.__version__)<NewLine><NewLine>import torchvision<NewLine>print(torchvision.__version__)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: January 10, 2020,  8:55pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
66281,JIT Trace model that computes gradients,2020-01-10T11:56:31.072Z,0,121,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I just wanted to ask if it’s possible to compute gradients inside a traced (torch.jit.trace) model.</p><NewLine><p>We want to use torchscript to manually train models in mobile devices by computing gradients and recomputing new parameters. I tried to create simple model like this.</p><NewLine><pre><code class=""lang-auto"">class ReturnGradientsModel(torch.nn.Module):<NewLine>    <NewLine>    def forward(self, input, w, b):<NewLine>        result = input * w + b<NewLine>        L = (10 - result).sum()<NewLine>        L.backward()<NewLine>        <NewLine>        w = w - w.grad * 1e-2<NewLine>        b = b - b.grad * 1e-2<NewLine>        <NewLine>        return w, b<NewLine><NewLine>torch.jit.trace(ReturnGradientsModel(), (torch.rand(2,2, requires_grad=True), <NewLine>                                        torch.rand(2,2, requires_grad=True), <NewLine>                                         torch.rand(2,2, requires_grad=True)))<NewLine></code></pre><NewLine><p>but it just returns the error</p><NewLine><pre><code class=""lang-auto"">RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn<NewLine></code></pre><NewLine><p>even though I explicitly passed tensors with grad enabled.</p><NewLine><p>I found this issue : <a href=""https://github.com/pytorch/pytorch/issues/15644"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/15644</a> . But it’s for onnx, not sure if it applies to torch.jit.trace as well.</p><NewLine><p>I’m grateful for any help.</p><NewLine></div>",https://discuss.pytorch.org/u/mark_jimenez,,mark_jimenez,"January 10, 2020, 11:56am",,,,,
65700,How to change the last layer(s) of a traced torchscript model,2020-01-03T18:50:33.104Z,1,411,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I am wondering if it is possible to change the last layer (or more) of a loaded torchscript model?</p><NewLine><p>This would be useful for changing the number of categories a torchscript model could predict after training again.</p><NewLine><p>Right now I get an odd error when trying to overwrite the last layer manually.</p><NewLine><pre><code class=""lang-python"">&gt;&gt; model = torch.jit.load(""model_cpu.pth"")<NewLine>&gt;&gt; model.last_layer<NewLine><NewLine>RecursiveScriptModule(original_name=Linear)<NewLine><NewLine>&gt;&gt; new_last_layer = torch.jit.script(torch.nn.Linear(a, b))<NewLine>&gt;&gt; new_last_layer<NewLine><NewLine>RecursiveScriptModule(original_name=Linear)<NewLine><NewLine>&gt;&gt; model.last_layer = new_last_layer<NewLine>&gt;&gt; a.forward(torch.rand([x, y, z ... ]))<NewLine><NewLine>Traceback (most recent call last):<NewLine>  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;<NewLine>RuntimeError: isTensor() INTERNAL ASSERT FAILED at ../aten/src/ATen/core/ivalue_inl.h:86, please report a bug to PyTorch. Expected Tensor but got Bool<NewLine>The above operation failed in interpreter.<NewLine>Traceback (most recent call last):<NewLine>/Users/****/venv/lib/python3.7/site-packages/torch/nn/functional.py(1370): linear<NewLine>/Users/****/venv/lib/python3.7/site-packages/torch/nn/modules/linear.py(87): forward<NewLine>/Users/****/venv/lib/python3.7/site-packages/torch/nn/modules/module.py(525): _slow_forward<NewLine>/Users/****/venv/lib/python3.7/site-packages/torch/nn/modules/module.py(539): __call__<NewLine>/Users/****/*****/ops/models.py(277): forward<NewLine>/Users/****/venv/lib/python3.7/site-packages/torch/nn/modules/module.py(525): _slow_forward<NewLine>/Users/****/venv/lib/python3.7/site-packages/torch/nn/modules/module.py(539): __call__<NewLine>/Users/****/venv/lib/python3.7/site-packages/torch/jit/__init__.py(997): trace_module<NewLine>/Users/****/venv/lib/python3.7/site-packages/torch/jit/__init__.py(858): trace<NewLine>convert.py(91): main<NewLine>convert.py(110): &lt;module&gt;<NewLine>Serialized   File ""code/__torch__/ops/models.py"", line 1052<NewLine>    input153 = torch.flatten(x31, 1, -1)<NewLine>    input154 = torch.dropout(input153, 0.5, False)<NewLine>    base_out = torch.addmm(bias52, input154, torch.t(weight105), beta=1, alpha=1)<NewLine>                                             ~~~~~~~ &lt;--- HERE<NewLine>    _512 = ops.prim.NumToTensor(torch.size(base_out, 1))<NewLine>    input_tensor = torch.view(base_out, [-1, 8, int(_512)])<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/jhhurwitz,(Jordan Hurwitz),jhhurwitz,"January 3, 2020,  6:51pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jhhurwitz"">@jhhurwitz</a> I don’t think there’s an easy way of doing that currently. See this issue for more context: <a href=""https://github.com/pytorch/pytorch/issues/21064"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/21064</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/eellison"">@eellison</a> Is it on the roadmap to add an easy way to do this? Looks like the issue has stalled out a little bit.</p><NewLine><p>In the meantime, even an example of the hard way to do this would be useful if you happened to know of any.</p><NewLine><p>Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jhhurwitz; <NewLine> ,"REPLY_DATE 1: January 3, 2020,  8:47pm; <NewLine> REPLY_DATE 2: January 7, 2020,  6:51pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
65939,Tensor Type for torch._C.Node,2020-01-06T21:31:27.740Z,2,238,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am taking in a trace of a model and would like to check what type the tensor is supposed to be. Is there a way to grab this info from torch._C.Node (ie: get float, double, from <a href=""https://pytorch.org/docs/stable/tensors.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/tensors.html</a>)? Or in general any way to grab that info starting from the trace?</p><NewLine></div>",https://discuss.pytorch.org/u/flynntax,,flynntax,"January 6, 2020,  9:31pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can grab it in this way:</p><NewLine><pre><code class=""lang-python"">def some_fn(x):<NewLine>	return x + x<NewLine><NewLine>some_input = torch.randn(2, 2)<NewLine>my_traced_function = torch.jit.trace(some_fn, [some_input])<NewLine><NewLine>for input in my_traced_function.graph.inputs():<NewLine>	traced_tensor_type = input.type()<NewLine>	# Prints ""Float""<NewLine>	print(traced_tensor_type.scalarType())<NewLine><NewLine># However, note that the interpreter will still run with differently typed<NewLine># tensors<NewLine>my_traced_function(torch.ones(2, 2, dtype=torch.long))<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see that this is getting it for inputs but what about intermediate nodes in the graph? It seems inputs are of type torch._C.Value which has a decorated type field but torch._C.Node doesn’t.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>Node</code> represents the whole operation (some inputs, an operation, and some outputs). The inputs and outputs are represented as <code>Value</code>s, you can get them like so:</p><NewLine><pre><code class=""lang-python"">def some_fn(x):<NewLine>	return x + x<NewLine><NewLine>some_input = torch.randn(2, 2)<NewLine>my_traced_function = torch.jit.trace(some_fn, [some_input])<NewLine>print(my_traced_function.graph)<NewLine><NewLine>for node in my_traced_function.graph.nodes():<NewLine>    for node_input in node.inputs():<NewLine>        if isinstance(node_input.type(), torch._C.TensorType):<NewLine>            print(node_input.type().scalarType())<NewLine></code></pre><NewLine><p>You can read more about the internal representations <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/docs/OVERVIEW.md#node"">here</a>.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the link, really helpful, somehow didn’t find it in my initial search. And got it, this makes a lot of sense now.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/flynntax; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/flynntax; <NewLine> ,"REPLY_DATE 1: January 6, 2020,  9:52pm; <NewLine> REPLY_DATE 2: January 6, 2020, 11:54pm; <NewLine> REPLY_DATE 3: January 7, 2020,  1:00am; <NewLine> REPLY_DATE 4: January 7, 2020,  1:23am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 2 Likes; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
65759,Does torch.jit.script support custom operators?,2020-01-04T18:17:19.874Z,2,488,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I tried to compile a module that contains a custom op defined by <code>torch.autograd.Function</code>:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>from torch.autograd import Function<NewLine><NewLine>class mul2(Function):<NewLine>    @staticmethod<NewLine>    def forward(ctx, x):<NewLine>        return x * 2<NewLine><NewLine>    @staticmethod<NewLine>    def backward(ctx, dx):<NewLine>        return dx * 2<NewLine><NewLine>def f(a, b):<NewLine>    c = a + b<NewLine>    d = mul2.apply(c)<NewLine>    e = torch.tanh(d * c)<NewLine>    return d + (e + e)<NewLine><NewLine>print(torch.jit.script(f).code)<NewLine></code></pre><NewLine><p>and I received</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""revisble.py"", line 21, in &lt;module&gt;<NewLine>    print(torch.jit.script(f).code)<NewLine>  File ""/Users/***/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1226, in script<NewLine>    fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))<NewLine>  File ""/Users/***/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1075, in _compile_and_register_class<NewLine>    ast = get_jit_class_def(obj, obj.__name__)<NewLine>  File ""/Users/***/anaconda3/lib/python3.7/site-packages/torch/jit/frontend.py"", line 148, in get_jit_class_def<NewLine>    self_name=self_name) for method in methods]<NewLine>  File ""/Users/***/anaconda3/lib/python3.7/site-packages/torch/jit/frontend.py"", line 148, in &lt;listcomp&gt;<NewLine>    self_name=self_name) for method in methods]<NewLine>  File ""/Users/***/anaconda3/lib/python3.7/site-packages/torch/jit/frontend.py"", line 169, in get_jit_def<NewLine>    return build_def(ctx, py_ast.body[0], type_line, self_name)<NewLine>  File ""/Users/***/anaconda3/lib/python3.7/site-packages/torch/jit/frontend.py"", line 198, in build_def<NewLine>    param_list = build_param_list(ctx, py_def.args, self_name)<NewLine>  File ""/Users/***/anaconda3/lib/python3.7/site-packages/torch/jit/frontend.py"", line 224, in build_param_list<NewLine>    raise NotSupportedError(ctx_range, _vararg_kwarg_err)<NewLine>torch.jit.frontend.NotSupportedError: Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults:<NewLine>at /Users/***/anaconda3/lib/python3.7/site-packages/torch/autograd/function.py:26:25<NewLine>    def mark_dirty(self, *args):<NewLine>                         ~~~~~ &lt;--- HERE<NewLine>        r""""""Marks given tensors as modified in an in-place operation.<NewLine><NewLine>        **This should be called at most once, only from inside the**<NewLine>        :func:`forward` **method, and all arguments should be inputs.**<NewLine><NewLine>        Every tensor that's been modified in-place in a call to :func:`forward`<NewLine>        should be given to this function, to ensure correctness of our checks.<NewLine>        It doesn't matter whether the function is called before or after<NewLine>        modification.<NewLine>'mul2' is being compiled since it was called from 'f'<NewLine>at revisble.py:17:4<NewLine>def f(a, b):<NewLine>    c = a + b<NewLine>    d = mul2.apply(c)<NewLine>    ~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>    e = torch.tanh(d * c)<NewLine>    return d + (e + e)<NewLine></code></pre><NewLine><p><code>torch.jit.trace</code> does work either. May I know do script mode support custom ops? If so, what is the correct way to handle custom op?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/yzh119,(Zihao Ye),yzh119,"January 4, 2020,  6:17pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We currently don’t support <code>autograd.Function</code> in Python. Right now the workaround is to define the function in C++ and bind it to TorchScript as a <a href=""https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html"">custom op</a>. This was done in <code>pytorch/vision</code> to support the Mask R-CNN model, you can see the specific implementation <a href=""https://github.com/pytorch/vision/pull/1407/files#diff-f78bab9f6ceace5815eccc4f84465144R3-R37"">here</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/driazati"">@driazati</a> thank you for your timely answer.<br/><NewLine>It seems I can only write forward function in current torchscript custom ops, right? What if users would like to customize backward functions like what we did in <code>torch.autograd.Function</code>?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If your C++ op calls a C++ autograd op (i.e. a class that extends <code>public torch::autograd::Function</code>), it will act the same as <code>torch.autograd.Function</code>. For the code below, if you bind <code>my_cool_op</code> and call it from TorchScript, it will use the backward you defined in <code>MyCoolOp</code></p><NewLine><pre><code class=""lang-cpp"">#include &lt;torch/all.h&gt;<NewLine>#include &lt;torch/python.h&gt;<NewLine><NewLine>class MyCoolOp : public torch::autograd::Function&lt;MyCoolOp&gt; {<NewLine> public:<NewLine>  static torch::autograd::variable_list forward(<NewLine>      torch::autograd::AutogradContext* ctx,<NewLine>      torch::autograd::Variable input) {<NewLine>    // forward calculation<NewLine>  }<NewLine><NewLine>  static torch::autograd::variable_list backward(<NewLine>      torch::autograd::AutogradContext* ctx,<NewLine>      torch::autograd::variable_list grad_output) {<NewLine>    // backward calculation<NewLine>  }<NewLine>};<NewLine><NewLine>torch::Tensor my_cool_op(const torch::Tensor&amp; input) {<NewLine>  return MyCoolOp::apply(input);<NewLine>}<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/yzh119; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: January 5, 2020,  2:27am; <NewLine> REPLY_DATE 2: January 5, 2020,  5:53pm; <NewLine> REPLY_DATE 3: January 6, 2020,  7:25pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 4 Likes; <NewLine> 
65719,Is it possible to plugin extra logic when serving a serialized model,2020-01-04T01:52:28.706Z,0,75,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am just wondering: it is possible to plugin extra logic between layers of the neural network model (serialized) during serving?</p><NewLine><p>Any help or idea would be greatly appreciated.</p><NewLine><p>Best,</p><NewLine></div>",https://discuss.pytorch.org/u/sleepy,(Mike),sleepy,"January 4, 2020,  1:52am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you explain your use case a bit more?<br/><NewLine>I.e. what does serving mean in the context and what would you like to add?<br/><NewLine>Are you referring to some deployment software / platform or just a plain PyTorch model, you would like to change?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply.</p><NewLine><p>Yeah, sure. I am referring to the prediction process once the serialized model (say, a DNN model) is loaded.</p><NewLine><p>I am not sure whether pytorch serving supports extra logic injection between each of the neural network layer (during the prediction process). I am suspecting that JIT does not provide such API. Does it require fundamental changes to the source code to enable such a feature?</p><NewLine><p>Best,</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/sleepy; <NewLine> ,"REPLY_DATE 1: January 4, 2020,  5:16am; <NewLine> REPLY_DATE 2: January 4, 2020,  7:44pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
65694,Result different between mobile and PC,2020-01-03T16:30:26.730Z,2,110,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I convert fastai to torchscript then run same converted model on both Android and PC(Macbook).<br/><NewLine>Result is different.</p><NewLine><p>Code convert:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>example = torch.rand(1, 3, 64, 64)<NewLine>traced_script_module = torch.jit.trace(torch_model, example)<NewLine>traced_script_module.save(""model.pt"")<NewLine></code></pre><NewLine><p>Run on PC:</p><NewLine><pre><code class=""lang-auto"">img = cv2.imread('aaanx_7.jpg')<NewLine>img = img/255.0<NewLine>img = np.moveaxis(img, -1, 0)<NewLine>img = np.array(np.expand_dims(img, axis=0))<NewLine>traced_script_module.forward(torch.from_numpy(img).float())<NewLine></code></pre><NewLine><p>Run on Android:</p><NewLine><pre><code class=""lang-auto"">bitmap = BitmapFactory.decodeStream(getAssets().open(""aaanx_7.jpg""));<NewLine>Tensor inputTensor = TensorImageUtils.bitmapToFloat32Tensor(bitmap,<NewLine>              new float[]{0.0f, 0.0f, 0.0f}, new float[]{1.0f, 1.0f, 1.0f});<NewLine>Tensor outputTensor = module.forward(IValue.from(inputTensor)).toTensor();<NewLine></code></pre><NewLine><p>Tensor output is different.<br/><NewLine>Please help me understand that.<br/><NewLine>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/Ten_Canmotcai,(Ten Canmotcai),Ten_Canmotcai,"January 3, 2020,  4:30pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is the output significantly different or does it just have small numerical differences? If it is significantly different please open an issue.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It is significantly different</p><NewLine><p>I train resnet50 to predict extended mnist and result is different number with all images</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I checking with input image and confuse with TensorImageUtils.bitmapToFloat32Tensor.<br/><NewLine>It is little-endian and it seem to be not same with PC.<br/><NewLine>I am not sure. Do you have any suggestion?</p><NewLine><p>Thanks</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Ten_Canmotcai; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Ten_Canmotcai; <NewLine> ,"REPLY_DATE 1: January 3, 2020,  8:48pm; <NewLine> REPLY_DATE 2: January 4, 2020, 12:45am; <NewLine> REPLY_DATE 3: January 4, 2020, 12:50am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
65549,Fastai problem/Input tensor issue,2020-01-02T02:40:08.483Z,0,170,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I created a model using fastai and save into torchscript.<br/><NewLine>When I run torchscript model it wrong input<br/><NewLine>My model: resnet50<br/><NewLine>then error:<br/><NewLine>Expected 4-dimensional input for 4-dimensional weight 64 3 7 7, but got 3-dimensional input of size [39, 28, 3] instead</p><NewLine><p>code:<br/><NewLine>import torch<br/><NewLine>example = torch.rand(1, 3, 64, 64)<br/><NewLine>traced_script_module = torch.jit.trace(torch_model, example)<br/><NewLine>traced_script_module.save(“model.pt”)<br/><NewLine>img = cv2.imread(‘image.jpg’)<br/><NewLine>traced_script_module.forward(torch.from_numpy(img))</p><NewLine><p>Have anybody help me?</p><NewLine></div>",https://discuss.pytorch.org/u/Ten_Canmotcai,(Ten Canmotcai),Ten_Canmotcai,"January 2, 2020,  2:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is the result of <code>torch.from_numpy(img)</code> the same dimensions as your example inputs? When using tracing, you must make sure that the example inputs given are representative.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: January 2, 2020,  3:55am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
65454,Questions about Jit RNNs,2019-12-31T11:28:46.649Z,0,151,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m recently interested in Touchscript, so I went to look at the custom LSTMs using Touchscript. In particular, I’m looking at this <a href=""https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py"" rel=""nofollow noopener"">file</a>. As a result, I have a few questions.<br/><NewLine><strong>1.</strong> I understand that combining the weights of a linear transformation are for speeding up the code. But why compute it separately for inputs and hiddens, as it is <a href=""https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py#L108"" rel=""nofollow noopener"">here</a>:<br/><NewLine><code>        hx, cx = state         gates = (torch.mm(input, self.weight_ih.t()) + self.bias_ih +                  torch.mm(hx, self.weight_hh.t()) + self.bias_hh)         ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)</code><br/><NewLine>Can someone explain the benefit in comparison to first concatenating the tensors and compute together?<br/><NewLine>**2.**Following the previous question, according to my understanding, the code mentioned above is trying to apply linear transformation separately to inputs and hiddens and add them up at last. However, 2 biases are added together, as:<br/><NewLine><code>        gates = (torch.mm(input, self.weight_ih.t()) + self.bias_ih +                  torch.mm(hx, self.weight_hh.t()) + self.bias_hh)</code><br/><NewLine>I think the 2 biases can be replaced by a single one with the exact same effect, so I want to ask what’s the difference between adding 1 bias and 2 biases.<br/><NewLine>**3.**If combing several operations can speed up the code, then instead of using: `        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)</p><NewLine><pre><code>    ingate = torch.sigmoid(ingate)<NewLine>    forgetgate = torch.sigmoid(forgetgate)<NewLine>    cellgate = torch.tanh(cellgate)<NewLine>    outgate = torch.sigmoid(outgate)`<NewLine></code></pre><NewLine><p>, why not just:<br/><NewLine><code>                 gates[:, 0:3*self.hidden_size].sigmoid_()  # Doesn't have to be in-place         ingate, forgetgate, outgate, cellgate = gates.chunk(4, 1)         cellgate=cellgate.tanh()</code><br/><NewLine>Aren’t the operations in this way more “combined together”, or is it that combing operations doesn’t effect element-wise operations much?</p><NewLine><p>Thanks for your time <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/G.M,,G.M,"December 31, 2019, 11:32am",,,,,
65416,Function type in TorchScript,2019-12-31T00:19:22.185Z,1,209,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’d like to parametrize my <code>torch.jit.script</code>'ed function with a function argument, i.e. whether a Forward-Backward algorithm should use <code>lambda x: torch.max(x, dim = dim).values</code> or <code>torch.logsumexp(x, dim = dim)</code>. When I pass it as lambda, TorchScript complains that I’m calling a tensor-typed value which happens because it types the argument as Tensor (despite the fact that it’s initialized to a lambda by default).</p><NewLine><p>Is there any way to tell TorchScript to type a value as a Callable?</p><NewLine><p>Is it possible to template a TorchScript function by an external callable values via some other mechanism?</p><NewLine><p>Should I create an issue about this on GitHub?</p><NewLine></div>",https://discuss.pytorch.org/u/vadimkantorov,(Vadim Kantorov),vadimkantorov,"December 31, 2019, 12:20am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>TorchScript currently doesn’t support callables as values (we’re working on supporting it in the coming months). The compiler doesn’t look at the default values at all since it’s not always possible to recover a full type from the default, so if types aren’t specified it just assumes everything is a Tensor. To specify otherwise, you can use Python 3 type hints or mypy style type comments (<a href=""https://pytorch.org/docs/master/jit_language_reference.html#supported-type"">details</a>).</p><NewLine><p>Function attributes on modules is the closest you can get at the moment, so something like</p><NewLine><pre><code class=""lang-python"">def my_fn(x, some_callable):<NewLine>    return some_callable(x + 10)<NewLine></code></pre><NewLine><p>would have to be changed something like to</p><NewLine><pre><code class=""lang-python"">class M(nn.Module):<NewLine>    def __init__(self, some_callable):<NewLine>        self.some_callable = some_callable<NewLine>    <NewLine>    def forward(self, x):<NewLine>        return self.some_callable(x + 10)<NewLine><NewLine>torch.jit.script(M(the_function))<NewLine>torch.jit.script(M(some_other_function))<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This still has some big unfortunate limitations.</p><NewLine><p>You can’t store some callables in a <code>ModuleList</code> or a <code>ModuleDict</code> and index that, because TorchScript sees them as modules and cannot subscript them.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Enamex; <NewLine> ,"REPLY_DATE 1: December 31, 2019,  4:54pm; <NewLine> REPLY_DATE 2: December 31, 2019, 11:13am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
65451,How to type a dynamically assigned TorchScript module to be consumed by a TorchScript function/method?,2019-12-31T10:45:03.087Z,0,63,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Example:</p><NewLine><pre><code class=""lang-auto"">@jit.script<NewLine>class Foo:<NewLine>    ...<NewLine>@jit.script<NewLine>class Quux:<NewLine>    ...<NewLine>@jit.script<NewLine>class Bar:<NewLine>    def __init__(self, cell):<NewLine>        # cell may be either a Foo or a Quux creator<NewLine>        self.cell1 = cell()<NewLine>        self.cell2 = cell()<NewLine>    def forward(self, x):<NewLine>        self.sub(x, self.cell1)<NewLine>        self.sub(x, self.cell2)<NewLine>    def sub(self, x: Tensor, cell: ?????):<NewLine>        ...<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Enamex,,Enamex,"December 31, 2019, 10:46am",,,,,
64713,Pytorch can not trace my onnx model (ignored a lot of layers),2019-12-21T14:56:13.501Z,1,210,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Very interesting, I have a model like this:</p><NewLine><pre><code class=""lang-auto"">class YOLOv3(nn.Module):<NewLine><NewLine>    def __init__(self, num_classes=80, ignore_thre=0.7, label_smooth=False, rfb=False, vis=False, asff=False):<NewLine>        super(YOLOv3, self).__init__()<NewLine>        self.module_list = build_yolov3_modules(<NewLine>            num_classes, ignore_thre, label_smooth, rfb)<NewLine><NewLine>        self.level_0_fusion = ASFF(level=0, rfb=rfb, vis=vis)<NewLine>        self.level_0_header = YOLOv3Head(anch_mask=[6, 7, 8], n_classes=num_classes, stride=32, in_ch=1024,<NewLine>                                         ignore_thre=ignore_thre, label_smooth=label_smooth, rfb=rfb)<NewLine>        self.level_1_fusion = ASFF(level=1, rfb=rfb, vis=vis)<NewLine>        self.level_1_header = YOLOv3Head(anch_mask=[3, 4, 5], n_classes=num_classes, stride=16, in_ch=512,<NewLine>                                         ignore_thre=ignore_thre, label_smooth=label_smooth, rfb=rfb)<NewLine>        self.level_2_fusion = ASFF(level=2, rfb=rfb, vis=vis)<NewLine>        self.level_2_header = YOLOv3Head(anch_mask=[0, 1, 2], n_classes=num_classes, stride=8, in_ch=256,<NewLine>                                         ignore_thre=ignore_thre, label_smooth=label_smooth, rfb=rfb)<NewLine><NewLine>    def forward(self, x, targets=None, epoch=0):<NewLine>        output = []<NewLine>        route_layers = []<NewLine><NewLine>        for i, module in enumerate(self.module_list):<NewLine>            x = module(x)<NewLine>            if i in [6, 8, 17, 24, 32]:<NewLine>                route_layers.append(x)<NewLine>            if i == 19:<NewLine>                x = torch.cat((x, route_layers[1]), 1)<NewLine>            if i == 26:<NewLine>                x = torch.cat((x, route_layers[0]), 1)<NewLine>        print(len(route_layers))<NewLine>        fused_0 = self.level_0_fusion(route_layers[2], route_layers[3], route_layers[4])<NewLine>        x = self.level_0_header(fused_0)<NewLine>        output.append(x)<NewLine><NewLine>        fused_1 = self.level_1_fusion(route_layers[2], route_layers[3], route_layers[4])<NewLine>        x = self.level_1_header(fused_1)<NewLine>        output.append(x)<NewLine><NewLine>        fused_2 = self.level_2_fusion(route_layers[2], route_layers[3], route_layers[4])<NewLine>        x = self.level_2_header(fused_2)<NewLine>        output.append(x)<NewLine>        return torch.cat(output, 1)<NewLine></code></pre><NewLine><p>This model when using <code>torch.onnx.export</code> it generate a ONNX model contains only an input and a Constant, log like this:</p><NewLine><pre><code class=""lang-auto"">      %level_2_header.Feature_adaption.dconv.weight : Float(256, 256, 3, 3),<NewLine>      %level_2_header.Feature_adaption.dconv.bias : Float(256),<NewLine>      %level_2_header.conv.weight : Float(340, 256, 1, 1),<NewLine>      %level_2_header.conv.bias : Float(340)):<NewLine>  %559 : Float(1, 52500, 85) = onnx::Constant[value=&lt;Tensor&gt;]()<NewLine>  return (%559)<NewLine></code></pre><NewLine><p>interesting thing is that, somehow I comment out these lines:</p><NewLine><pre><code class=""lang-auto""> def forward(self, x, targets=None, epoch=0):<NewLine>        output = []<NewLine>        route_layers = []<NewLine><NewLine>        for i, module in enumerate(self.module_list):<NewLine>            x = module(x)<NewLine>            if i in [6, 8, 17, 24, 32]:<NewLine>                route_layers.append(x)<NewLine>            if i == 19:<NewLine>                x = torch.cat((x, route_layers[1]), 1)<NewLine>            if i == 26:<NewLine>                x = torch.cat((x, route_layers[0]), 1)<NewLine>        # print(len(route_layers))<NewLine>        # fused_0 = self.level_0_fusion(route_layers[2], route_layers[3], route_layers[4])<NewLine>        # x = self.level_0_header(fused_0)<NewLine>        # output.append(x)<NewLine><NewLine>        # fused_1 = self.level_1_fusion(route_layers[2], route_layers[3], route_layers[4])<NewLine>        # x = self.level_1_header(fused_1)<NewLine>        # output.append(x)<NewLine><NewLine>        # fused_2 = self.level_2_fusion(route_layers[2], route_layers[3], route_layers[4])<NewLine>        # x = self.level_2_header(fused_2)<NewLine>        # output.append(x)<NewLine>        # return torch.cat(output, 1)<NewLine>        return x<NewLine></code></pre><NewLine><p>It can trace normal ONNX model.</p><NewLine><p>Does anybody can help me debug this weird issue?</p><NewLine></div>",https://discuss.pytorch.org/u/jinfagang,(Jin Tian),jinfagang,"December 21, 2019,  2:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Does anybody could help me debug it?<br/><NewLine>One bitcoin is for you if you can found the root reason.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you post a full repro to a gist or something ?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually,. this model comes from this repo: <a href=""https://github.com/ruinmessi/ASFF"" rel=""nofollow noopener"">https://github.com/ruinmessi/ASFF</a></p><NewLine><p>I am adding onnx export for it</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jinfagang; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jinfagang; <NewLine> ,"REPLY_DATE 1: December 23, 2019,  8:24am; <NewLine> REPLY_DATE 2: December 24, 2019,  6:08pm; <NewLine> REPLY_DATE 3: December 26, 2019,  8:39am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
64860,KeyError: &lsquo;prim_Uninitialized&rsquo;,2019-12-24T02:46:02.213Z,0,260,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I got this error when exporting model to ONNX:</p><NewLine><pre><code class=""lang-auto"">/home/dai/py36env/lib/python3.6/site-packages/torch/onnx/utils.py:651: UserWarning: ONNX export failed on primitive operator Uninitialized; please report a bug<NewLine>  warnings.warn(""ONNX export failed on primitive operator {}; please report a bug"".format(op_name))<NewLine>Traceback (most recent call last):<NewLine>  File ""onnx_demo.py"", line 280, in &lt;module&gt;<NewLine>    do_constant_folding=True,input_names=[""input""],output_names=[""boxes""]<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/onnx/__init__.py"", line 143, in export<NewLine>    strip_doc_string, dynamic_axes, keep_initializers_as_inputs)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/onnx/utils.py"", line 66, in export<NewLine>    dynamic_axes=dynamic_axes, keep_initializers_as_inputs=keep_initializers_as_inputs)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/onnx/utils.py"", line 382, in _export<NewLine>    fixed_batch_size=fixed_batch_size)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/onnx/utils.py"", line 262, in _model_to_graph<NewLine>    fixed_batch_size=fixed_batch_size)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/onnx/utils.py"", line 132, in _optimize_graph<NewLine>    graph = torch._C._jit_pass_onnx(graph, operator_export_type)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/onnx/__init__.py"", line 174, in _run_symbolic_function<NewLine>    return utils._run_symbolic_function(*args, **kwargs)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/onnx/utils.py"", line 652, in _run_symbolic_function<NewLine>    symbolic_fn = sym_registry.get_registered_op(symbolic_name, '', opset_version)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/onnx/symbolic_registry.py"", line 91, in get_registered_op<NewLine>    return _registry[(domain, version)][opname]<NewLine>KeyError: 'prim_Uninitialized'<NewLine></code></pre><NewLine><p>This error could come from these two lines:</p><NewLine><pre><code class=""lang-auto"">if xy_text.size(0) == 0:<NewLine>        return torch.tensor(0)<NewLine></code></pre><NewLine><p>What’s the function of prim_Uninitialized? How can I define it? Looking forward to your replay.</p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"December 24, 2019,  3:00am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>prim::Uninitialized is used to represent values which the compiler can prove will never be used. As you correctly guessed, it’s introduced by the early return statement there. You can work around this by removing early returns, breaks, and continues from your scripted function.</p><NewLine><p>Generally, you should expect ONNX to work with tracing and with anything scripted you will likely run into some issues.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/eellison; <NewLine> ,"REPLY_DATE 1: December 25, 2019, 12:35am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
64626,Tensor.shape[0] is not supported in torchscript?,2019-12-20T14:20:02.001Z,4,566,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I got the following error while converting model to torchscript.</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""ocr.py"", line 206, in &lt;module&gt;<NewLine>    ts = torch.jit.script(ocr)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1203, in script<NewLine>    return torch.jit.torch.jit._recursive.recursive_script(obj)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 173, in recursive_script<NewLine>    return copy_to_script_module(mod, overload_stubs + stubs)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 95, in copy_to_script_module<NewLine>    torch.jit._create_methods_from_stubs(script_module, stubs)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1423, in _create_methods_from_stubs<NewLine>    self._c._create_methods(self, defs, rcbs, defaults)<NewLine>RuntimeError:<NewLine>builtin cannot be used as a value:<NewLine>at ocr.py:80:23<NewLine>        img_tensor = resize_img(img_tensor)<NewLine>        # self.show(img_tensor)<NewLine><NewLine>        img_tensor_ = (img_tensor - 0.5) / 0.5<NewLine>        img_tensor_ = img_tensor_.permute(2,0,1).unsqueeze(0) # --&gt; n x c x h x w<NewLine>        boxes = self.east(img_tensor_)<NewLine><NewLine>        new_boxes = []<NewLine>        pass_list:List[int] = []<NewLine>        for i in range(boxes.shape[0]):<NewLine>                       ~~~~~~~~~~~ &lt;--- HERE<NewLine>            for j in range(i+1,boxes.shape[0]):<NewLine>                if i in pass_list or j in pass_list:<NewLine>                    continue<NewLine>                cx1,cy1,angle1 = box_analyse(boxes[i])<NewLine>                cx2,cy2,angle2 = box_analyse(boxes[j])<NewLine>                if torch.atan(torch.abs(torch.abs(cy2 - cy1) / torch.abs(cx2 - cx1))) - (90 - (angle1 + angle2) / 2) * 3.141592653 / 180 &lt; 0.1:<NewLine>                    b = torch.cat([boxes[i][:8].reshape(4, 2), boxes[j][:8].reshape(4, 2)], dim=0)<NewLine>                    new_box = torch.ops.my_ops.min_rect(b)<NewLine>                    new_boxes.append(new_box.reshape(-1))<NewLine></code></pre><NewLine><p>The variable <code>boxes</code> is a tensor returned from model <code>east</code>, is tensor.shape a builtin function in torchscript?</p><NewLine><p>By the way, how can I debug in torchscript?</p><NewLine><p>I met many problems in torchscript, it will be great if there’s a way to debug torchscript.</p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"December 20, 2019,  3:31pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Firstly, you should use boxes.size(). Secondly, <a href=""https://pytorch.org/docs/stable/jit.html#disable-jit-for-debugging"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/jit.html#disable-jit-for-debugging</a> will give you some help and my tip is using <span class=""mention"">@torch.jit.ignore</span> to check the functionality.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you, but boxes.size(0) and boxes.size()[0] doesn’t work.</p><NewLine><p>I met the same error using boxes.size(0) and boxes.size()[0]:</p><NewLine><pre><code class=""lang-auto"">Arguments for call are not valid.<NewLine>The following operator variants are available:<NewLine><NewLine>  aten::size.int(Tensor self, int dim) -&gt; (int):<NewLine>  Expected a value of type 'Tensor' for argument 'self' but instead found type 'Optional[Tensor]'.<NewLine><NewLine>  aten::size(Tensor self) -&gt; (int[]):<NewLine>  Expected a value of type 'Tensor' for argument 'self' but instead found type 'Optional[Tensor]'.<NewLine><NewLine>The original call is:<NewLine>at ocr.py:80:23<NewLine>        img_tensor = resize_img(img_tensor)<NewLine>        # self.show(img_tensor)<NewLine><NewLine>        img_tensor_ = (img_tensor - 0.5) / 0.5<NewLine>        img_tensor_ = img_tensor_.permute(2,0,1).unsqueeze(0) # --&gt; n x c x h x w<NewLine>        boxes = self.east(img_tensor_)<NewLine>        # self.show(img_tensor,boxes)<NewLine>        new_boxes = []<NewLine>        pass_list:List[int] = []<NewLine>        for i in range(boxes.size(0)):<NewLine>                       ~~~~~~~~~~ &lt;--- HERE<NewLine></code></pre><NewLine><p>And my code works well with environment variable  <code>PYTORCH_JIT=0</code>.</p><NewLine><p>What’s the difference between boxes.size()/boxes.shape in python and torchscript?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>torch.Tensor has no attribute named shape while numpy has. I thought boxes is a tensor, so I suggest you to use size(). If it is a numpy array, torchscript doesn’t support it.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m sure boxes is a tensor, its value is as following:</p><NewLine><pre><code class=""lang-auto"">tensor([[230.1149, 237.2053, 456.3304, 232.7983, 456.9069, 262.5309, 230.6913,<NewLine>         266.9381, 202.8954],<NewLine>        [331.7273, 324.1699, 679.4318, 317.2317, 680.0470, 348.1467, 332.3426,<NewLine>         355.0850,  88.9529],<NewLine>        [231.7705, 204.8157, 500.3940, 200.7935, 500.8643, 232.1562, 232.2409,<NewLine>         236.1783,  60.0000],<NewLine>        [231.7874, 157.3486, 441.9472, 153.3222, 442.4890, 181.6230, 232.3291,<NewLine>         185.6494,  47.9981],<NewLine>        [230.4137,  63.0557, 326.3553,  59.4965, 327.4994,  90.2845, 231.5578,<NewLine>          93.8438,  40.9682],<NewLine>        [152.8475, 326.7796, 311.2224, 324.2658, 311.6898, 353.6737, 153.3150,<NewLine>         356.1875,  28.9999],<NewLine>        [154.1073, 113.9810, 260.5620, 112.2871, 260.9745, 138.2966, 154.5198,<NewLine>         139.9904,  27.9924],<NewLine>        [293.3543, 111.8462, 390.4526, 108.3323, 391.4826, 136.8042, 294.3844,<NewLine>         140.3180,  13.9621],<NewLine>        [152.4046, 159.5459, 219.9668, 157.7393, 220.6146, 181.9012, 153.0524,<NewLine>         183.7079,  12.9934],<NewLine>        [153.1970, 206.8427, 218.1942, 205.0301, 218.8990, 230.2958, 153.9019,<NewLine>         232.1084,   7.0000],<NewLine>        [153.0565,  68.3798, 217.7016,  67.7295, 217.9489,  92.0420, 153.3038,<NewLine>          92.6922,   6.9623]], grad_fn=&lt;IndexBackward&gt;)<NewLine></code></pre><NewLine><p>Its type and shape is:</p><NewLine><pre><code class=""lang-auto"">torch.FloatTensor torch.Size([11, 9])<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found boxes is not tensor, it’s Optional[Tensor]. It’s inferred as Optional[Tensor] because of the control flow in the east model. The forward method of model is like this:</p><NewLine><pre><code class=""lang-auto"">def forward(self,x):<NewLine>    if condition1:<NewLine>        return None<NewLine>    else:<NewLine>        return boxes<NewLine></code></pre><NewLine><p>It’s inferred as Optional[Tensor] because the return value of forward can be None or tensor.</p><NewLine><p>And size() method if not supported by Optional[Tensor].</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/huoge; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dalalaa; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/huoge; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/dalalaa; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/dalalaa; <NewLine> ,"REPLY_DATE 1: December 21, 2019,  1:06am; <NewLine> REPLY_DATE 2: December 21, 2019,  2:29am; <NewLine> REPLY_DATE 3: December 21, 2019,  3:01am; <NewLine> REPLY_DATE 4: December 21, 2019, 12:32pm; <NewLine> REPLY_DATE 5: December 23, 2019, 12:55am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
64714,What are the difference between JIT and tensor_comprehensions for custom kernels?,2019-12-21T15:31:28.503Z,0,100,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Specifically, when writing TC-like loops in JIT-ed functions.</p><NewLine><p>My issues are:</p><NewLine><ol><NewLine><li>I haven’t been able to get good performance out of <code>jit.script</code>. My use case might be a little too dynamic?</li><NewLine><li>When JIT-ing, I have no control on any kinds of optimizations. I can’t nudge the jitter to fuse a particular sequence of operations, for example. So I can’t make use of the jitter to eliminate OOM errors.</li><NewLine><li>Trying to do it with loops is generally slower than using existing maps and reductions with (memory-hungry) intermediates.</li><NewLine><li><NewLine><code>tensor_comprehensions</code> is not available on Windows, as far as I can see <img alt="":confused:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=9"" title="":confused:""/><NewLine></li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/Enamex,,Enamex,"December 21, 2019,  3:31pm",,,,,
64710,What&rsquo;s Optional[Tensor]?,2019-12-21T12:46:12.786Z,0,462,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I got this error while calling tensor.size(0) in my project:</p><NewLine><pre><code class=""lang-auto"">RuntimeError:<NewLine>Arguments for call are not valid.<NewLine>The following operator variants are available:<NewLine><NewLine>  aten::size.int(Tensor self, int dim) -&gt; (int):<NewLine>  Expected a value of type 'Tensor' for argument 'self' but instead found type 'Optional[Tensor]'.<NewLine><NewLine>  aten::size(Tensor self) -&gt; (int[]):<NewLine>  Expected a value of type 'Tensor' for argument 'self' but instead found type 'Optional[Tensor]'.<NewLine><NewLine>The original call is:<NewLine>at ocr.py:78:23<NewLine>        img_tensor = resize_img(img_tensor)<NewLine>        # self.show(img_tensor)<NewLine><NewLine>        img_tensor_ = (img_tensor - 0.5) / 0.5<NewLine>        img_tensor_ = img_tensor_.permute(2,0,1).unsqueeze(0) # --&gt; n x c x h x w<NewLine>        boxes = self.east(img_tensor_)<NewLine>        # self.show(img_tensor,boxes)<NewLine>        new_boxes = []<NewLine>        pass_list:List[int] = []<NewLine>        for i in range(boxes.size(0)):<NewLine>                       ~~~~~~~~~~ &lt;--- HERE<NewLine></code></pre><NewLine><p>How can I get the first dimension size of a tensor in torchscript?</p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"December 21, 2019, 12:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""onebox"" href=""https://pytorch.org/docs/stable/jit.html#optional-type-refinement"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/stable/jit.html#optional-type-refinement</a><br/><NewLine>here you can get information about Optional</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/huoge; <NewLine> ,"REPLY_DATE 1: December 22, 2019,  5:55am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
64480,What&rsquo;s the difference between torch.nn.Module and torch.jit.ScriptModule?,2019-12-19T01:33:10.126Z,0,237,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The model inherited from <code>torch.nn.Module</code> also can be converted to TorchScript, in which case should we use <code>torch.jit.ScriptModule</code> instead of <code>torch.nn.Module</code>?</p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"December 19, 2019,  1:33am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You shouldn’t inherit from <code>torch.jit.ScriptModule</code>, that was an old API that we replaced in v.1.2.0</p><NewLine><p>See <a href=""https://pytorch.org/docs/master/jit.html#migrating-to-pytorch-1-2-recursive-scripting-api"">here</a> for details</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: December 20, 2019, 12:39am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
64407,How to convert decoder with attribute initialization with dictionary,2019-12-18T08:25:47.257Z,0,147,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to convert the decoder using torch.jit.script  but I am facing some error as below.</p><NewLine><h2><strong>My module</strong></h2><NewLine><pre><code class=""lang-python"">import numpy as np<NewLine>import torch<NewLine>import torch.nn as nn<NewLine><NewLine>from collections import OrderedDict<NewLine>from layers import *<NewLine><NewLine>class DepthDecoder(nn.Module):<NewLine>    def __init__(self, num_ch_enc, scales=range(4), num_output_channels=1, use_skips=True):<NewLine>        super(DepthDecoder, self).__init__()<NewLine><NewLine>        self.num_output_channels = num_output_channels<NewLine>        self.use_skips = use_skips<NewLine>        self.upsample_mode = 'nearest'<NewLine>        self.scales = scales<NewLine><NewLine>        self.num_ch_enc = num_ch_enc<NewLine>        self.num_ch_dec = np.array([16, 32, 64, 128, 256])<NewLine><NewLine>        # decoder<NewLine>        self.convs = OrderedDict()<NewLine>        for i in range(4, -1, -1):<NewLine>            # upconv_0<NewLine>            num_ch_in = self.num_ch_enc[-1] if i == 4 else self.num_ch_dec[i + 1]<NewLine>            num_ch_out = self.num_ch_dec[i]<NewLine>            self.convs[(""upconv"", i, 0)] = ConvBlock(num_ch_in, num_ch_out)<NewLine><NewLine>            # upconv_1<NewLine>            num_ch_in = self.num_ch_dec[i]<NewLine>            if self.use_skips and i &gt; 0:<NewLine>                num_ch_in += self.num_ch_enc[i - 1]<NewLine>            num_ch_out = self.num_ch_dec[i]<NewLine>            self.convs[(""upconv"", i, 1)] = ConvBlock(num_ch_in, num_ch_out)<NewLine><NewLine>        for s in self.scales:<NewLine>            self.convs[(""dispconv"", s)] = Conv3x3(self.num_ch_dec[s], self.num_output_channels)<NewLine><NewLine>        self.decoder = nn.ModuleList(list(self.convs.values()))<NewLine>        self.sigmoid = nn.Sigmoid()<NewLine><NewLine>    def forward(self, input_features):<NewLine>        outputs = {}<NewLine><NewLine>        # decoder<NewLine>        x = input_features[-1]<NewLine>        for i in range(4, -1, -1):<NewLine>            x = self.convs[(""upconv"", i, 0)](x)<NewLine>            x = [upsample(x)]<NewLine>            if self.use_skips and i &gt; 0:<NewLine>                x += [input_features[i - 1]]<NewLine>            x = torch.cat(x, 1)<NewLine>            x = self.convs[(""upconv"", i, 1)](x)<NewLine>            if i in self.scales:<NewLine>                outputs[(""disp"", i)] = self.sigmoid(self.convs[(""dispconv"", i)](x))<NewLine><NewLine>        return outputs<NewLine><NewLine>num_enc_channels = np.array([ 64,  64, 128, 256, 512])<NewLine>depth_decoder = DepthDecoder( num_ch_enc= num_enc_channels , scales=range(4))<NewLine>traced_script_module_decoder = torch.jit.script(depth_decoder)<NewLine>traced_script_module_decoder.save('new-decoder.pt')<NewLine></code></pre><NewLine><p><strong>Error :</strong></p><NewLine><pre><code class=""lang-auto"">  File ""C:\Users\lib\site-packages\torch\jit\_recursive.py"", line 259, in create_methods_from_stubs<NewLine>    concrete_type._create_methods(defs, rcbs, defaults)<NewLine>RuntimeError: <NewLine>Module 'DepthDecoder' has no attribute 'convs' (**This attribute exists on the Python module, but we failed to convert Python type: 'OrderedDict' to a TorchScript type**.):<NewLine>  File ""C:\Users\networks\depth_decoder.py"", line 55<NewLine>        x = input_features[-1]<NewLine>        for i in range(4, -1, -1):<NewLine>            x = self.convs[(""upconv"", i, 0)](x)<NewLine>                ~~~~~~~~~~ &lt;--- HERE<NewLine>            x = [upsample(x)]<NewLine>            if self.use_skips and i &gt; 0:<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Wajahat_Akhtar,(Wajahat Akhtar),Wajahat_Akhtar,"December 18, 2019,  7:01pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You’ll have to make a couple changes that may be tricky. For dictionaries the only keys that are supported are <code>str</code>, <code>int</code>, and <code>float</code>, so using a tuple as the key won’t work in TorchScript. Since the <code>range(4, -1, -1)</code> you’re using to create <code>self.convs</code> is not going to change no matter how you initialize <code>DepthDecoder</code>, I’d recommend unrolling that loop and adding them as submodules to <code>DepthDecoder</code>. So something like:</p><NewLine><pre><code class=""lang-python"">self.upconv_0_0 = ConvBlock(...)<NewLine>self.upconv_0_1 = ConvBlock(...)<NewLine></code></pre><NewLine><p>Unfortunately TorchScript is pretty picky about <code>nn.Module</code>s inside of containers, so things like <code>self.convs[""upconv1""]</code> currently aren’t supported (but this is something we’re working on fixing).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: December 18, 2019,  8:38pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
64268,"Try to use torchscript on Dict[str, List[float]]",2019-12-17T03:01:47.232Z,1,586,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone. I attempt to use</p><NewLine><p>torch.jit.script on the Dict[str,List[float]], but it doesn’t work。<br/><NewLine>Has anyone ever done any related work?</p><NewLine><p>by the way, I build the pytorch from source and the torch version is 1.4.0a0+93db2b8</p><NewLine><p>I’d appreciate if anybody can help me! Thanks in advance!</p><NewLine><p>here is the code:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from typing import Dict, List<NewLine>class dictlist(torch.nn.Module):<NewLine>    item:Dict[str, List[float]]<NewLine>    hyps:List[Dict[str, List[float]]]<NewLine>    def __init__(self):<NewLine>        torch.nn.Module.__init__(self)<NewLine>        self.item = {'score': [0.0], 'ys': [0, 1]}<NewLine>        self.hyps = [self.item]<NewLine>    def forward(self):<NewLine>        new_item:Dict[str, List[float]] = {'score':[1.0], 'ys': [1,2,3]}<NewLine>        self.hyps.append(new_item)<NewLine>model = dictlist()<NewLine>script_model = torch.jit.script(model)<NewLine></code></pre><NewLine><p>and here is the log:</p><NewLine><p><code> RuntimeError: values[i]-&gt;type()-&gt;isSubtypeOf(value_type) INTERNAL ASSERT FAILED at /pytorch/torch/csrc/jit/ir.cpp:1527, please report a bug to PyTorch.  (createDict at pytorch/torch/csrc/jit/ir.cpp:1527)</code></p><NewLine></div>",https://discuss.pytorch.org/u/huoge,,huoge,"December 17, 2019,  3:44am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The problem here is that [1,2,3] is a list of integers not a list of floats. If it is changed to [1.0, 2.0, 3.0], it should work. I submitted <a href=""https://github.com/pytorch/pytorch/pull/31375"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/31375</a> to make the error message report correctly.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>oh, thanks. I thought it would be converted automatically.<br/><NewLine>when I change it to a list of floats, it works.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually, I am also facing the same issue as above : I am not able to use dictionary. I am using the following code to convert my decoder to Torch script.</p><NewLine><pre><code class=""lang-python"">import numpy as np<NewLine>import torch<NewLine>import torch.nn as nn<NewLine><NewLine>from collections import OrderedDict<NewLine>from layers import *<NewLine><NewLine>class DepthDecoder(nn.Module):<NewLine><NewLine>def  **init** (self, num_ch_enc, scales=range(4), num_output_channels=1, use_skips=True):<NewLine>super(DepthDecoder, self). **init** ()<NewLine><NewLine>    self.num_output_channels = num_output_channels<NewLine>    self.use_skips = use_skips<NewLine>    self.upsample_mode = 'nearest'<NewLine>    self.scales = scales<NewLine><NewLine>    self.num_ch_enc = num_ch_enc<NewLine>    self.num_ch_dec = np.array([16, 32, 64, 128, 256])<NewLine><NewLine>    # decoder<NewLine>    self.convs = OrderedDict()<NewLine>    for i in range(4, -1, -1):<NewLine>        # upconv_0<NewLine>        num_ch_in = self.num_ch_enc[-1] if i == 4 else self.num_ch_dec[i + 1]<NewLine>        num_ch_out = self.num_ch_dec[i]<NewLine>        self.convs[(""upconv"", i, 0)] = ConvBlock(num_ch_in, num_ch_out)<NewLine><NewLine>        # upconv_1<NewLine>        num_ch_in = self.num_ch_dec[i]<NewLine>        if self.use_skips and i &gt; 0:<NewLine>            num_ch_in += self.num_ch_enc[i - 1]<NewLine>        num_ch_out = self.num_ch_dec[i]<NewLine>        self.convs[(""upconv"", i, 1)] = ConvBlock(num_ch_in, num_ch_out)<NewLine><NewLine>    for s in self.scales:<NewLine>        self.convs[(""dispconv"", s)] = Conv3x3(self.num_ch_dec[s], self.num_output_channels)<NewLine><NewLine>    self.decoder = nn.ModuleList(list(self.convs.values()))<NewLine>    self.sigmoid = nn.Sigmoid()<NewLine><NewLine>def forward(self, input_features):<NewLine>    outputs = {}<NewLine><NewLine>    # decoder<NewLine>    x = input_features[-1]<NewLine>    for i in range(4, -1, -1):<NewLine>        x = self.convs[(""upconv"", i, 0)](x)<NewLine>        x = [upsample(x)]<NewLine>        if self.use_skips and i &gt; 0:<NewLine>            x += [input_features[i - 1]]<NewLine>        x = torch.cat(x, 1)<NewLine>        x = self.convs[(""upconv"", i, 1)](x)<NewLine>        if i in self.scales:<NewLine>            outputs[(""disp"", i)] = self.sigmoid(self.convs[(""dispconv"", i)](x))<NewLine><NewLine>    return outputs<NewLine><NewLine>num_enc_channels = np.array([ 64, 64, 128, 256, 512])<NewLine>depth_decoder = DepthDecoder( num_ch_enc= num_enc_channels , scales=range(4))<NewLine>traced_script_module_decoder = torch.jit.script(depth_decoder)<NewLine>traced_script_module_decoder.save(‘new-decoder.pt’)<NewLine></code></pre><NewLine><p><strong>Error :</strong></p><NewLine><pre><code class=""lang-auto"">File “C:\Users\lib\site-packages\torch\jit_recursive.py”, line 259, in create_methods_from_stubs<NewLine>concrete_type._create_methods(defs, rcbs, defaults)<NewLine>RuntimeError:<NewLine>Module ‘DepthDecoder’ has no attribute ‘convs’ ( **This attribute exists on the Python module, but we failed to convert Python type: ‘OrderedDict’ to a TorchScript type** .):<NewLine>File “C:\Users\networks\depth_decoder.py”, line 55<NewLine>x = input_features[-1]<NewLine>for i in range(4, -1, -1):<NewLine>x = self.convs(“upconv”, i, 0)<NewLine>~~~~~~~~~~ &lt;— HERE<NewLine>x = [upsample(x)]<NewLine>if self.use_skips and i &gt; 0:<NewLine></code></pre><NewLine><p><strong>More Specifically I am trying to convert this nn.Module using TorchScript</strong></p><NewLine><p>Link : <a href=""https://github.com/nianticlabs/monodepth2/blob/master/networks/depth_decoder.py"" rel=""nofollow noopener"">https://github.com/nianticlabs/monodepth2/blob/master/networks/depth_decoder.py</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zdevito; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/huoge; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Wajahat_Akhtar; <NewLine> ,"REPLY_DATE 1: December 19, 2019,  2:53am; <NewLine> REPLY_DATE 2: December 18, 2019,  1:28am; <NewLine> REPLY_DATE 3: December 18, 2019,  7:00pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
64295,_register_state_dict_hook is not supported on ScriptModules,2019-12-17T08:56:00.102Z,0,183,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m quite new to torch.jit and Apex. I’m getting this error. What does it mean ?</p><NewLine><pre><code class=""lang-auto"">RuntimeError: _register_state_dict_hook is not supported on ScriptModules<NewLine></code></pre><NewLine><p>Here’s my code</p><NewLine><pre><code class=""lang-auto"">model, optimizer = amp.initialize(model, optimizer,opt_level='O2',<NewLine>                                          keep_batchnorm_fp32=False,<NewLine>                                          loss_scale=config.TRAIN.FP16_LOSS_SCALE,<NewLine>                                          min_loss_scale=128.0)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/kl_divergence,,kl_divergence,"December 17, 2019,  8:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>state_dict_hook is an internal implementation that allows some modules to edit their state_dict before they get returned. torch.jit doesn’t generally support any hooks, including state_dict hooks, so it is possible that Apex is installing these hooks on a jit module and then it is failing. However, I do not know the details of how Apex works, so I am not sure that is precisely what is happening.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Scripting is currently not supported in <code>apex</code>, but tracing should work.<br/><NewLine>We are currently working on upstreaming mixed-precision training, so that you won’t need to build <code>apex</code> soon. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zdevito; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: December 17, 2019,  6:49pm; <NewLine> REPLY_DATE 2: December 18, 2019,  3:09am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
64239,How to convert the encoder to torch script module,2019-12-16T17:09:04.454Z,0,284,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-python"">class ResnetEncoder(nn.Module):<NewLine>    """"""Pytorch module for a resnet encoder<NewLine>    """"""<NewLine>    def __init__(self, num_layers, pretrained, num_input_images=1):<NewLine>        super(ResnetEncoder, self).__init__()<NewLine><NewLine>        self.num_ch_enc = np.array([64, 64, 128, 256, 512])<NewLine><NewLine>        resnets = {18: models.resnet18,<NewLine>                   34: models.resnet34,<NewLine>                   50: models.resnet50,<NewLine>                   101: models.resnet101,<NewLine>                   152: models.resnet152}<NewLine><NewLine>        if num_layers not in resnets:<NewLine>            raise ValueError(""{} is not a valid number of resnet layers"".format(num_layers))<NewLine><NewLine>        if num_input_images &gt; 1:<NewLine>            self.encoder = resnet_multiimage_input(num_layers, pretrained, num_input_images)<NewLine>        else:<NewLine>            self.encoder = resnets[num_layers](pretrained)<NewLine><NewLine>        if num_layers &gt; 34:<NewLine>            self.num_ch_enc[1:] *= 4<NewLine><NewLine>    def forward(self, input_image):<NewLine>        self.features = []<NewLine>        x = (input_image - 0.45) / 0.225<NewLine>        x = self.encoder.conv1(x)<NewLine>        x = self.encoder.bn1(x)<NewLine>        self.features.append(self.encoder.relu(x))<NewLine>        self.features.append(self.encoder.layer1(self.encoder.maxpool(self.features[-1])))<NewLine>        self.features.append(self.encoder.layer2(self.features[-1]))<NewLine>        self.features.append(self.encoder.layer3(self.features[-1]))<NewLine>        self.features.append(self.encoder.layer4(self.features[-1]))<NewLine><NewLine>        return self.features<NewLine><NewLine>encoder = ResnetEncoder(18, True )<NewLine>example = torch.rand(1, 3, 640, 192)<NewLine>traced_script_module_encoder = torch.jit.trace(encoder.__getattr__('encoder'), example )<NewLine>traced_script_module.save('encoder_new.pt')<NewLine>torch.jit.load('encoder_new.pt')<NewLine></code></pre><NewLine><p>I have tried to convert the model via trace and loaded it back but it returns different shape features as also suggested by the community trace will not work (Tracing doesn’t understand dynamic control flow, so sometimes it will “constant-ify” shapes in your model. Try turning your model in to a ScriptModule and using TorchScript;)</p><NewLine><p>But in order to convert via torch.jit.script I get the following error<br/><NewLine>TypeError: module, class, method, function, traceback, frame, or code object was expected, got ResnetEncoder</p><NewLine><p>while using below example :</p><NewLine><pre><code class=""lang-python"">encoder = ResnetEncoder(18, True )<NewLine>traced_script_module_encoder = torch.jit.script(encoder)<NewLine>traced_script_module_encoder.save('new-encoder.pt')<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Wajahat_Akhtar,(Wajahat Akhtar),Wajahat_Akhtar,"December 16, 2019,  9:15pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You may be on an old version of <code>torchvision</code> or <code>torch</code>, can you make sure you’re on the latest of both? (or use the nightly if there are still errors)</p><NewLine><p>Your code works fine for me after applying the following diff:</p><NewLine><pre><code class=""lang-diff"">diff --git a/test.py b/test.py<NewLine>index e305823abd..a96eb93829 100644<NewLine>--- a/test.py<NewLine>+++ b/test.py<NewLine>@@ -208,21 +208,20 @@ class ResnetEncoder(nn.Module):<NewLine>             self.num_ch_enc[1:] *= 4<NewLine> <NewLine>     def forward(self, input_image):<NewLine>-        self.features = []<NewLine>+        features = []<NewLine>         x = (input_image - 0.45) / 0.225<NewLine>         x = self.encoder.conv1(x)<NewLine>         x = self.encoder.bn1(x)<NewLine>-        self.features.append(self.encoder.relu(x))<NewLine>-        self.features.append(self.encoder.layer1(self.encoder.maxpool(self.features[-1])))<NewLine>-        self.features.append(self.encoder.layer2(self.features[-1]))<NewLine>-        self.features.append(self.encoder.layer3(self.features[-1]))<NewLine>-        self.features.append(self.encoder.layer4(self.features[-1]))<NewLine>+        features.append(self.encoder.relu(x))<NewLine>+        features.append(self.encoder.layer1(self.encoder.maxpool(features[-1])))<NewLine>+        features.append(self.encoder.layer2(features[-1]))<NewLine>+        features.append(self.encoder.layer3(features[-1]))<NewLine>+        features.append(self.encoder.layer4(features[-1]))<NewLine> <NewLine>-        return self.features<NewLine>+        return features<NewLine> <NewLine> encoder = ResnetEncoder(18, True )<NewLine>-example = torch.rand(1, 3, 640, 192)<NewLine>-traced_script_module_encoder = torch.jit.trace(encoder.__getattr__('encoder'), example )<NewLine>+traced_script_module = torch.jit.script(encoder)<NewLine> traced_script_module.save('encoder_new.pt')<NewLine> torch.jit.load('encoder_new.pt')<NewLine></code></pre><NewLine><p>This is necessary since in TorchScript you can’t add new attributes to <code>self</code> outside of <code>__init__</code> (you can only mutate existing attributes), but here it looks like <code>features</code> wasn’t being used outside of <code>forward</code> anyways.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thankyou <a class=""mention"" href=""/u/driazati"">@driazati</a> for the quick reply, actually I am still facing the same issue after performing the changes( This is necessary since in TorchScript you can’t add new attributes to  <code>self</code>  outside of  <code>__init__</code>  (you can only mutate existing attributes)) suggested by you. I am using pytorch 1.0.1, torchvision 0.2.2. (Might be i upgrade it to pytorch 1.2)</p><NewLine><p>Also, do you get the same feature output before saving and after loading back the saved encoder model again ? and which version of Pytorch and torchvision are you using.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Now, I have updated pytorch to 1.3.1(Stable Version) and torchvision 0.4.<br/><NewLine>While running the same code now the error is changed to :</p><NewLine><p>File “C:\Users\anaconda3\envs\pytorch_converter\lib\site-packages\torch\jit_<em>init</em>_.py”, line 1423, in _create_methods_from_stubs<br/><NewLine>self._c._create_methods(self, defs, rcbs, defaults)</p><NewLine><p>RuntimeError:<br/><NewLine>module has no attribute ‘downsample’:</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>People have suggested to use nightly version : <a href=""https://github.com/pytorch/pytorch/issues/28351"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/28351</a></p><NewLine><p>I will try and will update if I fix the issue.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thankyou so much@driazati , It worked with torch nightly and the conversion is completed.</p><NewLine><p><strong>Now I move to a next step and their I have a similar error</strong> <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>Now I am trying to convert the decoder using torch.jit.script  but I am facing some error as below.</p><NewLine><h2><strong>My module</strong></h2><NewLine><p>import numpy as np<br/><NewLine>import torch<br/><NewLine>import torch.nn as nn</p><NewLine><p>from collections import OrderedDict<br/><NewLine>from layers import *</p><NewLine><p>class DepthDecoder(nn.Module):<br/><NewLine>def <strong>init</strong>(self, num_ch_enc, scales=range(4), num_output_channels=1, use_skips=True):<br/><NewLine>super(DepthDecoder, self).<strong>init</strong>()</p><NewLine><pre><code>    self.num_output_channels = num_output_channels<NewLine>    self.use_skips = use_skips<NewLine>    self.upsample_mode = 'nearest'<NewLine>    self.scales = scales<NewLine><NewLine>    self.num_ch_enc = num_ch_enc<NewLine>    self.num_ch_dec = np.array([16, 32, 64, 128, 256])<NewLine><NewLine>    # decoder<NewLine>    self.convs = OrderedDict()<NewLine>    for i in range(4, -1, -1):<NewLine>        # upconv_0<NewLine>        num_ch_in = self.num_ch_enc[-1] if i == 4 else self.num_ch_dec[i + 1]<NewLine>        num_ch_out = self.num_ch_dec[i]<NewLine>        self.convs[(""upconv"", i, 0)] = ConvBlock(num_ch_in, num_ch_out)<NewLine><NewLine>        # upconv_1<NewLine>        num_ch_in = self.num_ch_dec[i]<NewLine>        if self.use_skips and i &gt; 0:<NewLine>            num_ch_in += self.num_ch_enc[i - 1]<NewLine>        num_ch_out = self.num_ch_dec[i]<NewLine>        self.convs[(""upconv"", i, 1)] = ConvBlock(num_ch_in, num_ch_out)<NewLine><NewLine>    for s in self.scales:<NewLine>        self.convs[(""dispconv"", s)] = Conv3x3(self.num_ch_dec[s], self.num_output_channels)<NewLine><NewLine>    self.decoder = nn.ModuleList(list(self.convs.values()))<NewLine>    self.sigmoid = nn.Sigmoid()<NewLine><NewLine>def forward(self, input_features):<NewLine>    outputs = {}<NewLine><NewLine>    # decoder<NewLine>    x = input_features[-1]<NewLine>    for i in range(4, -1, -1):<NewLine>        x = self.convs[(""upconv"", i, 0)](x)<NewLine>        x = [upsample(x)]<NewLine>        if self.use_skips and i &gt; 0:<NewLine>            x += [input_features[i - 1]]<NewLine>        x = torch.cat(x, 1)<NewLine>        x = self.convs[(""upconv"", i, 1)](x)<NewLine>        if i in self.scales:<NewLine>            outputs[(""disp"", i)] = self.sigmoid(self.convs[(""dispconv"", i)](x))<NewLine><NewLine>    return outputs<NewLine></code></pre><NewLine><p>num_enc_channels = np.array([ 64,  64, 128, 256, 512])<br/><NewLine>depth_decoder = DepthDecoder( num_ch_enc= num_enc_channels , scales=range(4))<br/><NewLine>traced_script_module_decoder = torch.jit.script(depth_decoder)<br/><NewLine>traced_script_module_decoder.save(‘new-decoder.pt’)</p><NewLine><p><strong>Error :</strong></p><NewLine><p>File “C:\Users\lib\site-packages\torch\jit_recursive.py”, line 259, in create_methods_from_stubs<br/><NewLine>concrete_type._create_methods(defs, rcbs, defaults)<br/><NewLine>RuntimeError:<br/><NewLine>Module ‘DepthDecoder’ has no attribute ‘convs’ (This attribute exists on the Python module, but we failed to convert Python type: ‘OrderedDict’ to a TorchScript type.):<br/><NewLine>File “C:\Users\networks\depth_decoder.py”, line 55<br/><NewLine>x = input_features[-1]<br/><NewLine>for i in range(4, -1, -1):<br/><NewLine>x = self.convs<a>(“upconv”, i, 0)</a><br/><NewLine>~~~~~~~~~~ &lt;— HERE<br/><NewLine>x = [upsample(x)]<br/><NewLine>if self.use_skips and i &gt; 0:</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I feel the error is the same as explained here in the link : <a href=""https://github.com/pytorch/pytorch/issues/23905"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/23905</a></p><NewLine><p>failed to convert Python type: ‘dict’ to a TorchScript type.</p><NewLine><p>Is there any fix ?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Wajahat_Akhtar; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Wajahat_Akhtar; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Wajahat_Akhtar; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Wajahat_Akhtar; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Wajahat_Akhtar; <NewLine> ,"REPLY_DATE 1: December 18, 2019,  1:36pm; <NewLine> REPLY_DATE 2: December 17, 2019,  9:04am; <NewLine> REPLY_DATE 3: December 17, 2019,  9:36am; <NewLine> REPLY_DATE 4: December 18, 2019,  1:36pm; <NewLine> REPLY_DATE 5: December 17, 2019, 10:59am; <NewLine> REPLY_DATE 6: December 17, 2019,  1:19pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
64090,How can I use &ldquo;__cuda_array_interface__&rdquo; and tensor.requires_grad_() together?,2019-12-15T05:39:12.024Z,2,247,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I use package Numba to write the forward and backward cuda operation, and use it as a module. When I use it, the input tensors, with grad required, go through the modules. The example can be described as:</p><NewLine><pre><code class=""lang-auto"">def forward_cudakernel():<NewLine>        ...   # write by NUMBA<NewLine><NewLine>def backward_cudakernel():<NewLine>        ...  # write by NUMBA<NewLine><NewLine>def forward_cuda(x)<NewLine>    with torch.no_grad():<NewLine>        x = numba.cuda.as_cuda_array(x)<NewLine>        forward_cudakernel[blocks, threads](x)<NewLine><NewLine>def backward_cuda(x)<NewLine>    with torch.no_grad():<NewLine>        x = numba.cuda.as_cuda_array(x)<NewLine>        backward_cudakernel[blocks, threads](x)<NewLine><NewLine>class BaseModule(Function):<NewLine>    @staticmethod<NewLine>    def forward():<NewLine>        ...<NewLine>        return forward_cuda()<NewLine>    @staticmethod<NewLine>    def backward():<NewLine>        ...<NewLine>        return grads<NewLine><NewLine>class Module(nn.Module):<NewLine>    def forward(feature):<NewLine>        fun = BaseModule.apply<NewLine>        return fun(feature)<NewLine><NewLine>&gt;&gt;&gt; features = torch.rand(2, 256, 20, 20).cuda()<NewLine>&gt;&gt;&gt; features.requires_grad_()<NewLine>&gt;&gt;&gt; net = Module()<NewLine>&gt;&gt;&gt; out = net(features)<NewLine>&gt;&gt;&gt; out.mean.backward()<NewLine></code></pre><NewLine><p>Where “out” and “feature” needs grads so that the module can run. However, the attr “<strong>cuda_array_interface</strong>”（cuda.as_cuda_array related） is not available when a tensor is with gradient. It seems incompatible. So, How can I use “numba” and “pytorch” together? Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/JTiC,,JTiC,"December 15, 2019,  5:39am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Now, I add a line <code>x = x.data</code> in forward_cuda(), discarding the gradient before using “as_cuda_array”. I find it worked, but I don’t know if there are some potential bugs remained.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>As mentioned in the error message that you get if you try to use it <a href=""https://github.com/pytorch/pytorch/blob/717274c001509ffbc14189edef0f7d2ee2e5b777/torch/tensor.py#L538-L542"">here</a> you should use <code>.detach()</code> and not <code>.data</code>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much! <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/JTiC; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/JTiC; <NewLine> ,"REPLY_DATE 1: December 16, 2019,  2:15am; <NewLine> REPLY_DATE 2: December 16, 2019, 10:55am; <NewLine> REPLY_DATE 3: December 17, 2019, 11:40am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
64067,Guiding the JITer to properly fusing non-contiguous map-reduce operations,2019-12-14T19:44:06.619Z,0,80,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Example case:</p><NewLine><pre><code class=""lang-auto"">ONES = ones(1).to(DEVICE)<NewLine>@jit.script<NewLine>def foo(feats, mask):<NewLine>    # mask : [batch features]<NewLine>    # feats : [features features]<NewLine>    return where( # ((1))<NewLine>        mask,<NewLine>        feats.unsqueeze(0),<NewLine>        # shape: [_ features features]<NewLine>        ONES<NewLine>    # shape before prod: [batch features features]<NewLine>    ).prod(1) # ((2))<NewLine>    # shape after prod: [batch features]<NewLine></code></pre><NewLine><p>((1)) and ((2)) should be fusible by directly storing the reduction in the out tensor while iterating over the mask, the data, and the reduction operation.</p><NewLine><p>This… would probably be hell for the vectorizer. But at this point I’d take the hit in exchange for being able to run the thing in memory.</p><NewLine><p>BUT… if any guru around here can show me the logical way I’ve been missing for doing this memory efficiently with existing primitives, please do! <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/></p><NewLine></div>",https://discuss.pytorch.org/u/Enamex,,Enamex,"December 14, 2019,  7:46pm",,,,,
63911,"Iterator expression is expected to be a list, iterable, or range, found value of type &lsquo;Tensor&rsquo;",2019-12-13T02:04:10.949Z,0,198,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I met this error when converting my model to torchscript.</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>iterator expression is expected to be a list, iterable, or range, found value of type 'Tensor':<NewLine>at /home/dai/scripts/card_ocr_cpu/detector/model_torchscript.py:39:28<NewLine>def standard_nms(S, thres):<NewLine>    <NewLine>    order = torch.argsort(S[:, 8]).flip(0)<NewLine>    keep:List[int] = []<NewLine>    while order.size(0) &gt; 0:<NewLine>        i = order[0]<NewLine>        keep.append(i.long().item())<NewLine>        ovr = torch.tensor([intersection(S[i], S[t]) for t in order[1:]])<NewLine>                            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        inds = torch.where(ovr &lt;= thres)[0]<NewLine>        order = order[inds + 1]<NewLine>    return S[keep]<NewLine>'standard_nms' is being compiled since it was called from 'nms_locality'<NewLine>at /home/dai/scripts/card_ocr_cpu/detector/model_torchscript.py:63:11<NewLine>            p = weighted_merge(g, p)<NewLine>        else:  <NewLine>            if p.size(0) &gt; 1:<NewLine>                S.append(p)<NewLine>            p = g<NewLine>    if p is not None:<NewLine>        S.append(p)<NewLine>    if len(S) == 0:<NewLine>        return torch.tensor([0])<NewLine>    return standard_nms(torch.stack(S), thres)<NewLine>           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>'nms_locality' is being compiled since it was called from 'get_boxes_torch'<NewLine>at /home/dai/scripts/card_ocr_cpu/detector/model_torchscript.py:247:4<NewLine>        return None<NewLine>    boxes = torch.zeros((polys_restored.shape[0],9)).float()<NewLine>    boxes[:,:8] = polys_restored<NewLine>    xs = xy_text[index,0]<NewLine>    ys = xy_text[index,1]<NewLine>    boxes[:,8] = score[xs,ys]<NewLine>    <NewLine>    boxes = nms_locality(boxes,nms_thresh)<NewLine>    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>    return boxes<NewLine>'get_boxes_torch' is being compiled since it was called from '__torch__.EAST.forward'<NewLine>at /home/dai/scripts/card_ocr_cpu/detector/model_torchscript.py:216:12<NewLine>    def forward(self, x, train:bool=False):<NewLine>        x1,x2,x3,x4 = self.extractor(x)<NewLine>        x = self.merge(x1,x2,x3,x4)<NewLine><NewLine>        # del x1,x2,x3,x4<NewLine>        # collect()<NewLine><NewLine>        score,geo = self.output(x)<NewLine>        if not train:<NewLine>            boxes = get_boxes_torch(score,geo,score_thresh=0.95,nms_thresh=0.2)<NewLine>            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine><NewLine>            return boxes<NewLine>        return score,geo<NewLine></code></pre><NewLine><p>How can I  iterate through the tensor like a list. Looking forward for your replay.</p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"December 13, 2019,  2:04am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This should already get supported. it might not be in v1.3 yet, can you try update your pytorch version to nightly build to see if it works?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wanchaol; <NewLine> ,"REPLY_DATE 1: December 13, 2019,  2:54am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
63817,What&rsquo;s the difference between scriptmodule.graph() vs. scriptmodule.graph_for(),2019-12-12T04:06:24.505Z,1,201,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I noticed that <a href=""https://pytorch.org/docs/stable/jit.html?highlight=scriptmodule#torch.jit.ScriptModule"" rel=""nofollow noopener""><code>scriptmodule</code> have graph() and graph_for() methods</a>, but I am confused about those two methods. Does anyone can tell me the difference? really thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/Jonson,(叶俊贤),Jonson,"December 12, 2019,  6:17am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>graph_for</code> should try to optimize or fuse the graph for the provided inputs, while <code>graph</code> should print the IR directly.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Great! now I see, and I have another question, will <code>torch.jit.script()</code> try to optimize or fuse the graph when exporting a pytorch native model to a scriptmodule?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It will automatically optimize/fuse for you when you run it with some inputs, <code>graph_for</code> is showing you a preview of what will be executed for a certain set of inputs.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Jonson; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: December 12, 2019,  6:18am; <NewLine> REPLY_DATE 2: December 12, 2019,  6:18am; <NewLine> REPLY_DATE 3: December 12, 2019,  5:56pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
63482,ResNet50 Multiple Output Nodes,2019-12-09T13:03:00.030Z,0,120,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi All,</p><NewLine><p>I am trying to build two CNN’s on top of ResNet50, one as a regression node and one as a classification node.</p><NewLine><pre><code class=""lang-auto"">class resnet50(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(resnet50, self).__init__()<NewLine>        self.left = nn.Sequential(<NewLine>                                  nn.AdaptiveAvgPool2d(1024),<NewLine>                                  nn.AdaptiveMaxPool2d(512),<NewLine>                                  nn.Flatten(),<NewLine>                                  nn.BatchNorm1d(512),<NewLine>                                  nn.Dropout(0.25),<NewLine>                                  nn.LeakyReLU(),<NewLine>                                  nn.Linear(256, 64),<NewLine>                                  nn.Dropout(0.5),<NewLine>                                  nn.LeakyReLU(),<NewLine>                                  nn.Linear(64, 1)<NewLine>                                  )<NewLine>        self.right = nn.Sequential(<NewLine>                                  nn.AdaptiveAvgPool2d(1024),<NewLine>                                  nn.AdaptiveMaxPool2d(512),<NewLine>                                  nn.Flatten(),<NewLine>                                  nn.BatchNorm1d(512),<NewLine>                                  nn.Dropout(0.25),<NewLine>                                  nn.LeakyReLU(),<NewLine>                                  nn.Linear(256, 64),<NewLine>                                  nn.Dropout(0.5),<NewLine>                                  nn.LeakyReLU(),<NewLine>                                  nn.Linear(64, 7)<NewLine>                                  )<NewLine>        self.model = models.resnet50(pretrained=True)<NewLine>        self.model.fc = nn.Identity()<NewLine>        <NewLine>    def forward(self, x):<NewLine>        x = self.model(x)<NewLine>        print(x.shape)<NewLine>        count_out = self.left(x)<NewLine>        class_out = self.right(x)<NewLine>        return count_out, class_out<NewLine></code></pre><NewLine><p>I tried it in a way as given in this previous problem but i get the following error when i attempt a forward pass.</p><NewLine><pre><code class=""lang-auto"">o1, o2 = model(x)<NewLine>torch.Size([1, 2048])<NewLine>Traceback (most recent call last):<NewLine><NewLine>  File ""&lt;ipython-input-9-d7dc74ba0de2&gt;"", line 1, in &lt;module&gt;<NewLine>    o1, o2 = model(x)<NewLine><NewLine>  File ""/home/siddhesh/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine><NewLine>  File ""/home/siddhesh/Work/Projects/LYSTO/Scripts/utils/new_models.py"", line 55, in forward<NewLine>    count_out = self.left(x)<NewLine><NewLine>  File ""/home/siddhesh/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine><NewLine>  File ""/home/siddhesh/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 92, in forward<NewLine>    input = module(input)<NewLine><NewLine>  File ""/home/siddhesh/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine><NewLine>  File ""/home/siddhesh/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/pooling.py"", line 1031, in forward<NewLine>    return F.adaptive_avg_pool2d(input, self.output_size)<NewLine><NewLine>  File ""/home/siddhesh/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py"", line 768, in adaptive_avg_pool2d<NewLine>    return torch._C._nn.adaptive_avg_pool2d(input, _output_size)<NewLine><NewLine>RuntimeError: non-empty 3D or 4D (batch mode) tensor expected for input<NewLine></code></pre><NewLine><p>Can someone help me as to where i might be ruining my forward pass with this?</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/Geeks_Sid,(Siddhesh Thakur),Geeks_Sid,"December 9, 2019,  1:03pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>torch.nn.AdaptiveAvgPool2d</code>'s input is a 3D or 4D tensor.<br/><NewLine><code>x = self.model(x)</code> return a 2D tensor.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Eta_C; <NewLine> ,"REPLY_DATE 1: December 9, 2019,  1:13pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
63073,Windows LibTorch C++ Load CUDA Module with std runtime error message &ldquo;shape [4] is invalid for input if size 40&rdquo;,2019-12-05T03:32:28.674Z,3,312,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi~</p><NewLine><p>I use torch.jit.trace to create the model for LibTorch C++.<br/><NewLine>Python code show as follows:</p><NewLine><pre><code class=""lang-auto"">    class ResBlock_W512H15(nn.Module):<NewLine>        def __init__(self, idx=0, filter_num=16, kernelSize=(3, 9), dropValue=0.2, poolSize=4):<NewLine>            super(ResBlock_W512H15, self).__init__()<NewLine>            self.in_ch = (2 ** idx) * filter_num<NewLine>            self.out_ch = (2 ** (idx + 1)) * filter_num<NewLine>            self.poolSize = poolSize<NewLine>            self.bn = nn.BatchNorm2d(self.in_ch)<NewLine>            self.conv = nn.Conv2d(in_channels=self.in_ch,<NewLine>                                  out_channels=self.out_ch,<NewLine>                                  kernel_size=kernelSize,<NewLine>                                  padding=(kernelSize[0]//2, 4))<NewLine>            self.drop = nn.Dropout2d(dropValue)<NewLine>            self.mp = nn.MaxPool2d(kernel_size=(1, poolSize),<NewLine>                                   stride=None)<NewLine>    <NewLine>        def forward(self, x):<NewLine>            shortcut = self.mp(x)<NewLine>            P_top = (self.out_ch - self.in_ch) // 2<NewLine>            P_buttum = (self.out_ch - self.in_ch) - P_top<NewLine>            shortcut = F.pad(shortcut, (0, 0, 0, 0, P_top, P_buttum))<NewLine>            out = x<NewLine>            out = F.relu(self.bn(out))<NewLine>            out = self.drop(out)<NewLine>            out = self.conv(out)<NewLine>            out = self.mp(out)<NewLine>            out += shortcut<NewLine>            return out<NewLine>    <NewLine>    <NewLine>    class Model_W512H15(nn.Module):<NewLine>        def __init__(self, inChannel=1, filter_num=16, kernelSize=(3, 9), num_out=15, num_categories=4):<NewLine>            super(Model_W512H15, self).__init__()<NewLine>            self.filter_num = filter_num<NewLine>            self.kernelSize = kernelSize<NewLine>            self.num_out = num_out<NewLine>            self.num_categories = num_categories<NewLine>            self.conv1 = nn.Conv2d(in_channels=inChannel,<NewLine>                                   out_channels=self.filter_num,<NewLine>                                   kernel_size=self.kernelSize,<NewLine>                                   padding=(kernelSize[0]//2, 4))<NewLine>            # --- Resblocks<NewLine>            self.ConvBlock0 = ResBlock_W512H15(0, filter_num=self.filter_num, kernelSize=self.kernelSize, poolSize=4)<NewLine>            self.ConvBlock1 = ResBlock_W512H15(1, filter_num=self.filter_num, kernelSize=self.kernelSize, poolSize=4)<NewLine>            self.ConvBlock2 = ResBlock_W512H15(2, filter_num=self.filter_num, kernelSize=self.kernelSize, poolSize=4)<NewLine>            self.ConvBlock3 = ResBlock_W512H15(3, filter_num=self.filter_num, kernelSize=self.kernelSize, poolSize=2)<NewLine>            self.ConvBlock4 = ResBlock_W512H15(4, filter_num=self.filter_num, kernelSize=self.kernelSize, poolSize=2)<NewLine>            self.ConvBlock5 = ResBlock_W512H15(5, filter_num=self.filter_num, kernelSize=self.kernelSize, poolSize=2)<NewLine>            # --- Final<NewLine>            self.final_ch = (2 ** 6) * self.filter_num<NewLine>            self.bn = nn.BatchNorm2d(self.final_ch)<NewLine>            self.m = nn.Softmax(dim=2)<NewLine>            self.fc = nn.ModuleList()<NewLine>            for i in range(self.num_out):<NewLine>                self.fc.append(nn.Linear(self.final_ch, self.num_categories))<NewLine>    <NewLine>    <NewLine>        def forward(self, x):<NewLine>            out = x  # (1,15,512)<NewLine>            out = self.conv1(out)  # (16,15,512)<NewLine>            out = self.ConvBlock0(out)  # (32,15,128)<NewLine>            out = self.ConvBlock1(out)  # (64,15,32)<NewLine>            out = self.ConvBlock2(out)  # (128,15,8)<NewLine>            out = self.ConvBlock3(out)  # (256,15,4)<NewLine>            out = self.ConvBlock4(out)  # (512,15,2)<NewLine>            out = self.ConvBlock5(out)  # (1024,15,1)<NewLine>            out = F.relu(self.bn(out))<NewLine>            out = out.permute(0, 3, 2, 1)<NewLine>            out_final = torch.zeros([out.size()[0], self.num_out, self.num_categories]).cuda()<NewLine>            for i in range(self.num_out):<NewLine>                x1 = self.fc[i](out[:, :, i, :])<NewLine>                out_final[:, i, :] = x1[:, 0, :]<NewLine>    <NewLine>            out_final = self.m(out_final)<NewLine>            return out_final<NewLine>    <NewLine>    <NewLine>    device = torch.device('cuda')<NewLine>    model = Model_W512H15(kernelSize=(7, 9)).to(device)<NewLine>    model.eval()<NewLine>    input = torch.ones(1, 1, 15, 512).cuda()<NewLine>    <NewLine>    trace_net = torch.jit.trace(model, input)<NewLine>    trace_net.eval()<NewLine>    trace_net.save(""CppModel.pt"")<NewLine></code></pre><NewLine><p>And I run the following C++ code:</p><NewLine><pre><code class=""lang-auto"">	//--- Load model<NewLine>	string ModulePath = ""CppModel.pt"";<NewLine>	torch::jit::script::Module module;<NewLine>	module = torch::jit::load(ModulePath);<NewLine>	module.to(at::kCUDA);<NewLine>	module.eval();<NewLine><NewLine>	//--- Test input<NewLine>	at::Tensor example = torch::ones({ 10, 1, 15, 512 });<NewLine>	vector&lt;torch::jit::IValue&gt; example_i;<NewLine>	example_i.push_back(example.to(at::kCUDA));<NewLine>	try {<NewLine>		auto output = module.forward(example_i).toTensor();<NewLine>	}<NewLine>	catch (std::runtime_error &amp; e) {<NewLine>		std::cerr &lt;&lt; e.what() &lt;&lt; std::endl;<NewLine>	}<NewLine></code></pre><NewLine><p>I got the error message as follows:</p><NewLine><p><a href=""https://raw.githubusercontent.com/BrianHan777/Test/master/errormessage.png"" rel=""nofollow noopener"" target=""_blank""><img alt=""image"" height=""345"" src=""https://raw.githubusercontent.com/BrianHan777/Test/master/errormessage.png"" width=""690""/></a></p><NewLine><p>Could someone help me?</p><NewLine><h2>Environment</h2><NewLine><ul><NewLine><li>PyTorch Version (e.g., 1.0): Pytorch 1.3</li><NewLine><li>Libtorch Version: Nightly version</li><NewLine><li>OS (e.g., Linux): Window 10</li><NewLine><li>Visual studio 2019</li><NewLine><li>How you installed PyTorch (<code>conda</code>, <code>pip</code>, source): pip</li><NewLine><li>Build command you used (if compiling from source):</li><NewLine><li>Python version: Python 3.6</li><NewLine><li>CUDA/cuDNN version: CUDA 9.2</li><NewLine><li>GPU models and configuration:</li><NewLine><li>Any other relevant information:</li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/111158,(Brian Han),111158,"December 5, 2019,  3:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It looks like the shape of the input to the model in Python is <code>(1, 1, 15, 512)</code> but in C++ it was <code>(10, 1, 15, 512)</code>, is that the source of the error?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, I think it was not the source of the error.<br/><NewLine>In this c++ code, “10” is batch size.<br/><NewLine>I have tested the jit model from this source (<a href=""https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py"" rel=""nofollow noopener"">https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py</a>).<br/><NewLine>In C++, the shape of the input is (10,1,32,32).<br/><NewLine>It run successfully.</p><NewLine><p>According to the error message, I think the error source may from this line</p><NewLine><pre><code>            out_final[:, i, :] = x1[:, 0, :]</code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>torch.jit.trace</code> doesn’t record data flowing over Python values, they instead just get recorded as constants with respect to the inputs provided to <code>torch.jit.trace</code>. So it looks like for your model the batch size is being recorded into the graph and will be fixed so no other batch sizes work:</p><NewLine><p>Consider a batch size of 10 and the resulting graph</p><NewLine><p><code>trace_net = torch.jit.trace(model, [torch.ones(10, 1, 15, 512)])</code></p><NewLine><pre><code class=""lang-auto"">  _30 = torch.copy_(_29, torch.view(_27, [10, 4]), False)                                                                                                                                                                          <NewLine></code></pre><NewLine><p>versus a batch size of 3</p><NewLine><p><code>trace_net = torch.jit.trace(model, [torch.ones(3, 1, 15, 512)])</code></p><NewLine><pre><code class=""lang-auto"">  _30 = torch.copy_(_29, torch.view(_27, [3, 4]), False)                                                                                            <NewLine></code></pre><NewLine><p>The only way around this is to use <code>torch.jit.script</code> instead of <code>torch.jit.trace</code> which will <a href=""https://pytorch.org/docs/master/jit.html#torchscript-language-reference"">compile your code</a> instead of tracing its execution. For example</p><NewLine><pre><code class=""lang-python"">        ...<NewLine>        out = F.relu(self.bn(out))<NewLine>        out = out.permute(0, 3, 2, 1)<NewLine>        out_final = torch.zeros([out.size()[0], self.num_out, self.num_categories])<NewLine>        i = 0<NewLine>        # ModuleLists cannot be indexed in TorchScript, so the loop here must<NewLine>        # be changed<NewLine>        for fc in self.fc:<NewLine>            x1 = fc(out[:, :, i, :])<NewLine>            out_final[:, i, :] = x1[:, 0, :]<NewLine>            i += 1<NewLine><NewLine>        out_final = self.m(out_final)<NewLine>        return out_final<NewLine><NewLine><NewLine>model = Model_W512H15(kernelSize=(7, 9))<NewLine>model.eval()<NewLine>trace_net = torch.jit.script(model)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>It works!!<br/><NewLine>Thank you so much.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/111158; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/111158; <NewLine> ,"REPLY_DATE 1: December 5, 2019,  4:26am; <NewLine> REPLY_DATE 2: December 5, 2019,  5:39am; <NewLine> REPLY_DATE 3: December 6, 2019,  7:08am; <NewLine> REPLY_DATE 4: December 6, 2019,  7:08am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
63198,What is the version of pytorch and libtorch used in pytorch/extension_script ？,2019-12-06T03:55:09.787Z,1,170,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I compiled the code following the instruction in docker. but got the following error while running <code>python script.py</code>:</p><NewLine><pre><code class=""lang-auto"">OSError: /home/huang/vsopencv/extension-script/example_app/build/warp_perspective/libwarp_perspective.so: undefined symbol: _ZN3c1017RegisterOperators25checkSchemaAndRegisterOp_ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEONS0_7OptionsE<NewLine></code></pre><NewLine><p>I opened an issue in the repository <a href=""https://github.com/pytorch/extension-script/issues/6"" rel=""nofollow noopener"">issue#6</a>, but got no answer.</p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"December 6, 2019,  3:55am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you importing <code>torch</code> before your extension?<br/><NewLine>This error is often seen, if you try to import the extension directly.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes，I imported torch before loading extension，my code：</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine>torch.ops.load_library(""/home/example_app/build/warp_perspective/libwarp_perspective.so"")<NewLine><NewLine><NewLine>@torch.jit.script<NewLine>def compute(x, y):<NewLine>    if bool(x[0][0] == 42):<NewLine>        z = 5<NewLine>    else:<NewLine>        z = 10<NewLine>    x = torch.ops.my_ops.warp_perspective(x, torch.eye(3))<NewLine>    return x.matmul(y) + z<NewLine><NewLine><NewLine>print(compute.graph)<NewLine>print(compute(torch.randn(4, 8), torch.randn(8, 5)))<NewLine><NewLine>compute.save(""example.pt"")<NewLine></code></pre><NewLine><p>I got this error when running <code>python script.py</code>：</p><NewLine><pre><code class=""lang-auto"">(base) root@1b05a3dbfb17:/home# python script.py<NewLine>Traceback (most recent call last):<NewLine>  File ""script.py"", line 3, in &lt;module&gt;<NewLine>    torch.ops.load_library(""/home/example_app/build/warp_perspective/libwarp_perspective.so"")<NewLine>  File ""/root/local/miniconda/lib/python3.7/site-packages/torch/_ops.py"", line 106, in load_library<NewLine>    ctypes.CDLL(path)<NewLine>  File ""/root/local/miniconda/lib/python3.7/ctypes/__init__.py"", line 364, in __init__<NewLine>    self._handle = _dlopen(self._name, mode)<NewLine>OSError: /home/example_app/build/warp_perspective/libwarp_perspective.so: undefined symbol: _ZN3c104impl24ExcludeTensorTypeIdGuardD1Ev<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dalalaa; <NewLine> ,"REPLY_DATE 1: December 6, 2019,  5:50am; <NewLine> REPLY_DATE 2: December 6, 2019,  6:27am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
62929,Call custom functions by jit in C++,2019-12-03T22:46:01.690Z,2,327,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have created the following class and saved <code>scripted_module</code> and loaded in <code>C++</code> API by:</p><NewLine><pre><code class=""lang-auto"">    torch::jit::script::Module module;<NewLine>    module = torch::jit::load(""scriptmodule.pt"");<NewLine></code></pre><NewLine><p>Now, the question is how can I call <code>func</code> from C++?</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine>class MyModule(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(MyModule, self).__init__()<NewLine><NewLine>    @torch.jit.ignore<NewLine>    def get_rand(self):<NewLine>        return torch.randint(0, 2, size=[1])<NewLine><NewLine>    def forward(self):<NewLine>        pass <NewLine>    <NewLine>    @torch.jit.export<NewLine>    def func(self):<NewLine>        done = self.get_rand()<NewLine>        print (done)<NewLine><NewLine>scripted_module = torch.jit.script(MyModule())<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/afshin67,(Afshin Oroojlooy),afshin67,"December 3, 2019, 10:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Does this answer help?</p><NewLine><aside class=""quote quote-modified"" data-post=""4"" data-topic=""60926""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/driazati/40/15393_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/jit-for-tolist/60926/4"">Jit for tolist()</a> <a class=""badge-wrapper bullet"" href=""/c/jit""><span class=""badge-category-bg"" style=""background-color: #0088CC;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""A category for torchscript and the PyTorch JIT compiler"">jit</span></a><NewLine></div><NewLine><blockquote><NewLine>    For methods other than forward you have to explicitly get the method and run it. For this Module <NewLine>class M(nn.Module):<NewLine>    @torch.jit.export<NewLine>    def infer(self, x):<NewLine>        return x + 10<NewLine><NewLine>torch.jit.script(M()).save(""m.pt"")<NewLine><NewLine>You can run it in C++ with script::Module::get_method <NewLine>int main() {<NewLine>  auto module = torch::jit::load(""m.pt"");<NewLine>  auto result = module.get_method(""infer"")({torch::ones({2, 2})});<NewLine>  std::cout &lt;&lt; result &lt;&lt; ""\n"";<NewLine>}<NewLine><NewLine>We have an open issue to improve our C++ documentation to make thi…<NewLine>  </blockquote><NewLine></aside><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Also just FYI <code>@torch.jit.ignore</code> functions cannot be exported, if you do <code>scripted_module.save(""out.pt"")</code> you’ll get</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""../test.py"", line 196, in &lt;module&gt;<NewLine>    scripted_module.save('s.pt')<NewLine>  File ""/home/pytorch/torch/jit/__init__.py"", line 1621, in save<NewLine>    return self._c.save(*args, **kwargs)<NewLine>RuntimeError: <NewLine>Could not export Python function call 'get_rand'. Remove calls to Python functions before export. Did you forget add @script or @script_method annotation? If this is a nn.ModuleList, add it to __constants__:<NewLine>  File ""../test.py"", line 192<NewLine>    @torch.jit.export<NewLine>    def func(self):<NewLine>        done = self.get_rand()<NewLine>               ~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        print (done)<NewLine></code></pre><NewLine><p>Which is expected behavior (since saved models are expected to run without a Python runtime attached)</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, this worked.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, I faced a similar example and as you mentioned it makes sense.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/afshin67; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/afshin67; <NewLine> ,"REPLY_DATE 1: December 3, 2019, 11:36pm; <NewLine> REPLY_DATE 2: June 26, 2020,  6:12pm; <NewLine> REPLY_DATE 3: December 6, 2019,  3:07am; <NewLine> REPLY_DATE 4: December 6, 2019,  3:08am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
46561,Cannot insert a Tensor that requires grad as a constant,2019-05-29T15:17:52.549Z,5,2648,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I implemented my own version of CaffeNet in PyTorch and I am trying to save the model in an onnx file. But I have the error described in the title when I run this code:</p><NewLine><p>""<br/><NewLine>import torch.nn as nn<br/><NewLine>import torch</p><NewLine><p>class Flatten(nn.Module):<br/><NewLine>def forward(self,x):<br/><NewLine>return x.view(x.size(0),-1)</p><NewLine><p>class CaffeNet(nn.Module):<br/><NewLine>def <strong>init</strong>(self):<br/><NewLine>super(CaffeNet,self).<strong>init</strong>()</p><NewLine><pre><code>    self.layers=[]<NewLine><NewLine>    self.layers.append(nn.Conv2d(in_channels=3,out_channels=3,kernel_size=11))<NewLine>    self.layers.append(nn.ZeroPad2d(padding=1))<NewLine><NewLine>    self.layers.append(nn.Conv2d(3,96,55))<NewLine>    self.layers.append(nn.MaxPool2d(kernel_size=2,stride=2))<NewLine>    self.layers.append(nn.ZeroPad2d(padding=1))<NewLine><NewLine>    self.layers.append(nn.Conv2d(96,192,27))<NewLine>    self.layers.append(nn.MaxPool2d(kernel_size=2, stride=2))<NewLine>    self.layers.append(nn.ZeroPad2d(padding=1))<NewLine><NewLine>    self.layers.append(nn.Conv2d(192, 288, 13))<NewLine>    self.layers.append(nn.ZeroPad2d(padding=1))<NewLine><NewLine>    self.layers.append(nn.Conv2d(288, 288, 13))<NewLine>    self.layers.append(nn.ZeroPad2d(padding=1))<NewLine><NewLine>    self.layers.append(nn.Conv2d(288, 256, 11)) #12-th layer, normally kernel_size=13<NewLine>    self.layers.append(nn.MaxPool2d(kernel_size=2, stride=2))<NewLine>    self.layers.append(nn.ZeroPad2d(padding=1))<NewLine><NewLine>    self.layers.append(nn.Conv2d(256, 256, 3)) #15-th layer, normally kernel_size=13<NewLine>    self.layers.append(nn.MaxPool2d(kernel_size=2, stride=2))<NewLine>    self.layers.append(nn.ZeroPad2d(padding=1))<NewLine><NewLine>    self.layers.append(Flatten())<NewLine><NewLine>    self.layers.append(nn.Linear(2304,2304)) #19-th layer, normally 4096 neurons<NewLine>    self.layers.append(nn.ReLU())<NewLine>    self.layers.append(nn.Dropout())<NewLine><NewLine>    self.layers.append(nn.Linear(2304,2304))<NewLine>    self.layers.append(nn.ReLU())<NewLine>    self.layers.append(nn.Dropout())<NewLine><NewLine>    self.layers.append(nn.Linear(2304,1000))<NewLine>    self.layers.append(nn.Softmax())<NewLine><NewLine><NewLine>def forward(self, x):<NewLine>    for layer in self.layers:<NewLine>        x=layer(x)<NewLine>    return x<NewLine></code></pre><NewLine><p>net=CaffeNet()<br/><NewLine>input = torch.randn(20, 3, 227, 227,requires_grad=False)<br/><NewLine>input_names = [ “input” ]<br/><NewLine>output_names = [ “output” ]<br/><NewLine>torch.onnx.export(net,input,“caffenet.onnx”,verbose=True, input_names=input_names, output_names=output_names)<br/><NewLine>""</p><NewLine><p>I am using python2.7. The last line of the stack trace is:<br/><NewLine>“RuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient”</p><NewLine></div>",https://discuss.pytorch.org/u/mloft,,mloft,"May 29, 2019,  3:17pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The modules you’re adding have some Parameters, so they need to be submodules of <code>CaffeNet</code>. It should work if you use a <code>torch.nn.Sequential</code> instead of a regular Python list for <code>self.layers</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok Thanks you. I’m gonna try this on monday when I’m going back to work. I hope it will work <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks you very much driazati, it works <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi，I have the same problem as you. How did you solve it，Look forward to your reply<br/><NewLine>thanks</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi, the subLayers of your module should be its submodules. So:<br/><NewLine>1/ I made my CaffeNet module derived from nn.sequential<br/><NewLine>2/Then I defined a <strong>add</strong> method which adds layer to the subModule dictionary (self._modules) of a nn.sequential:<br/><NewLine>def <strong>add</strong>(self, layer):<br/><NewLine>self._modules[str(self.index)]=layer<br/><NewLine>self.index=self.index+1<br/><NewLine>3/Then I call this method with each layer of CaffeNet</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply, you are a good man, but I don’t know the specific operation, can you provide the code to solve it?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mloft; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mloft; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/111199; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mloft; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/111199; <NewLine> ,"REPLY_DATE 1: May 30, 2019, 11:14pm; <NewLine> REPLY_DATE 2: May 31, 2019,  8:31pm; <NewLine> REPLY_DATE 3: June 3, 2019,  1:03pm; <NewLine> REPLY_DATE 4: December 4, 2019,  3:12pm; <NewLine> REPLY_DATE 5: December 4, 2019,  3:31pm; <NewLine> REPLY_DATE 6: December 5, 2019,  3:06am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 2 Likes; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> 
62859,Fail to convert PyTorch module to TorchScript,2019-12-03T10:01:08.697Z,1,343,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve created a model with a forward function like this:</p><NewLine><pre><code class=""lang-auto"">class Net(nn.Module):<NewLine>...<NewLine>  def forward(self, num_nodes, num_feats, nodes):<NewLine>          features = nn.Embedding(num_nodes, num_feats)<NewLine>          features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)<NewLine></code></pre><NewLine><p>then save that model using</p><NewLine><pre><code class=""lang-auto"">traced_script_module = torch.jit.script(net)<NewLine>traced_script_module.save(model_path1)<NewLine></code></pre><NewLine><p>I have train model successfully, but get this error when save the model.</p><NewLine><pre><code class=""lang-auto"">NotSupportedError: Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults:...<NewLine>'Embedding' is being compiled since it was called from '__torch__.___torch_mangle_0.Net.forward'<NewLine>at &lt;ipython-input-5-501dbaacc7a5&gt;:42:8<NewLine>    def forward(self, num_nodes, num_feats, nodes):<NewLine>        <NewLine>        features = nn.Embedding(num_nodes, num_feats)<NewLine>        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)<NewLine><NewLine></code></pre><NewLine><p>And pytorch’s version is 1.3<br/><NewLine>What is the best way to handle this?<br/><NewLine>Any help appreciated!!!</p><NewLine></div>",https://discuss.pytorch.org/u/herbertguoqi,(Herbertguoqi),herbertguoqi,"December 3, 2019, 10:01am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You should move the initialization of submodules out into <code>__init__</code> rather than in your model’s <code>forward</code>. This will also likely improve performance since costly parameter allocations only need to happen once instead of on each run of your model’s <code>forward</code>.</p><NewLine><pre><code class=""lang-python"">class M(nn.Module):<NewLine>    def __init__(self, num_nodes, num_feats):<NewLine>        self.features = nn.Embedding(num_nodes, num_feats)<NewLine>        self.features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)<NewLine>    <NewLine>    def forward(self, nodes):<NewLine>        result = self.features(...)<NewLine>        ...<NewLine><NewLine>model = M()<NewLine>script_model = torch.jit.script(model)<NewLine>script_model.save(""script_model.pt"")<NewLine></code></pre><NewLine><p>When classes are instantiated in TorchScript, the entire class must be compatible with the TorchScript compiler (<a href=""https://pytorch.org/docs/master/jit.html#id3"">details</a>), which is not the case for most <code>nn.Module</code>s. However, if <code>nn.Module</code>s are saved on <code>self</code> in <code>__init__</code>, only the methods that are actually used in the <code>forward</code> of your model <code>M</code> need to be compatible with the compiler (which should work for any module in <code>nn</code> except for <a href=""https://pytorch.org/docs/master/jit.html#builtin-functions"">these 3</a>).</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Got it！<br/><NewLine>Thank you for the answers.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/herbertguoqi; <NewLine> ,"REPLY_DATE 1: December 4, 2019,  5:48am; <NewLine> REPLY_DATE 2: December 4, 2019,  2:49am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
62908,"Int?, Tensor?, types? Operator implementations",2019-12-03T18:49:28.585Z,1,271,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am printing out the graph for a model and am seeing int? and Tensor? types. Here is the graph.</p><NewLine><pre><code class=""lang-auto"">graph(%self : ClassType&lt;Conv2D2&gt;,<NewLine>      %input.1 : Float(1, 3, 224, 224)):<NewLine>  %1 : ClassType&lt;Conv2d&gt; = prim::GetAttr[name=""conv""](%self)<NewLine>  %weight : Tensor = prim::GetAttr[name=""weight""](%1)<NewLine>  %5 : Tensor? = prim::Constant(), scope: Conv2D2/Conv2d[conv]<NewLine>  %6 : int = prim::Constant[value=1](), scope: Conv2D2/Conv2d[conv] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0<NewLine>  %7 : int = prim::Constant[value=1](), scope: Conv2D2/Conv2d[conv] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0<NewLine>  %8 : int[] = prim::ListConstruct(%6, %7), scope: Conv2D2/Conv2d[conv]<NewLine>  %9 : int = prim::Constant[value=0](), scope: Conv2D2/Conv2d[conv] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0<NewLine>  %10 : int = prim::Constant[value=0](), scope: Conv2D2/Conv2d[conv] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0<NewLine>  %11 : int[] = prim::ListConstruct(%9, %10), scope: Conv2D2/Conv2d[conv]<NewLine>  %12 : int = prim::Constant[value=1](), scope: Conv2D2/Conv2d[conv] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0<NewLine>  %13 : int = prim::Constant[value=1](), scope: Conv2D2/Conv2d[conv] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0<NewLine>  %14 : int[] = prim::ListConstruct(%12, %13), scope: Conv2D2/Conv2d[conv]<NewLine>  %15 : bool = prim::Constant[value=0](), scope: Conv2D2/Conv2d[conv] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0<NewLine>  %16 : int = prim::Constant[value=0](), scope: Conv2D2/Conv2d[conv] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0<NewLine>  %17 : int = prim::Constant[value=0](), scope: Conv2D2/Conv2d[conv] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0<NewLine>  %18 : int[] = prim::ListConstruct(%16, %17), scope: Conv2D2/Conv2d[conv]<NewLine>  %19 : int = prim::Constant[value=1](), scope: Conv2D2/Conv2d[conv] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0<NewLine>  %20 : bool = prim::Constant[value=0](), scope: Conv2D2/Conv2d[conv] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0<NewLine>  %21 : bool = prim::Constant[value=0](), scope: Conv2D2/Conv2d[conv] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0<NewLine>  %22 : bool = prim::Constant[value=1](), scope: Conv2D2/Conv2d[conv] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0<NewLine>  %input : Float(1, 64, 218, 218) = aten::_convolution(%input.1, %weight, %5, %8, %11, %14, %15, %18, %19, %20, %21, %22), scope: Conv2D2/Conv2d[conv] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0<NewLine>  %24 : int = prim::Constant[value=1](), scope: Conv2D2/Softmax[softmax] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1230:0<NewLine>  %25 : int? = prim::Constant(), scope: Conv2D2/Softmax[softmax]<NewLine>  %26 : Float(1, 64, 218, 218) = aten::softmax(%input, %24, %25), scope: Conv2D2/Softmax[softmax] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1230:0<NewLine>  return (%26)<NewLine></code></pre><NewLine><p>What are these empty constant operators w/ weird typings supposed to represent? Where can I find source code or documentation for these? Also off-shoot question, but where can I find the actual implementation for operators (<a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/core/interned_strings.h"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/core/interned_strings.h</a>)?</p><NewLine></div>",https://discuss.pytorch.org/u/flynntax,,flynntax,"December 3, 2019,  8:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>For sad legacy reasons we have two ways of presenting types, one that matches Python’s <code>typing</code> and our own internal one (which is what you’re seeing), even though the underlying types are the same. The <code>?</code> here means optional, so <code>int?</code> is equivalent to <code>Optional[int]</code>, and the empty <code>prim::Constant()</code> means <code>None</code>.</p><NewLine><p><code>prim::Constant</code> is a <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/interpreter.cpp#L565"">special case</a> in the TorchScript interpreter. Other operators that don’t directly call the underlying torch operators live in <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/register_prim_ops.cpp"">register_prim_ops.cpp</a>. However, <a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml"">most operators</a> are generated at build time since they just call PyTorch tensor ops and are placed in <code>torch/csrc/jit/generated</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah, makes sense now. Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/flynntax; <NewLine> ,"REPLY_DATE 1: December 4, 2019,  1:13am; <NewLine> REPLY_DATE 2: December 3, 2019, 11:49pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
62351,Use numpy in script class (torch.jit.script),2019-11-27T22:38:18.128Z,1,542,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was wondering if I can use numpy APIs in a function which is going to be scripted by <code>torch.jit.script</code>. I have this simple function which does not work:</p><NewLine><p>import torch<br/><NewLine>import torch.nn as nn</p><NewLine><p>class MyModule(nn.Module):<br/><NewLine>def <strong>init</strong>(self):<br/><NewLine>super(MyModule, self).<strong>init</strong>()</p><NewLine><pre><code>@torch.jit.ignore<NewLine>def call_np():<NewLine>    return torch.jit.export(np.random.choice(2, p=[.95,.05]))<NewLine><NewLine>def forward(self):<NewLine>    pass <NewLine><NewLine>@torch.jit.export<NewLine>def func(self):<NewLine>    done = self.call_np()<NewLine>    print (done)<NewLine></code></pre><NewLine><blockquote><NewLine><p>scripted_module = torch.jit.script(MyModule())<br/><NewLine>scripted_module.func()</p><NewLine></blockquote><NewLine><p>which results in:</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-133-ab1ce37d6edc&gt; in &lt;module&gt;()<NewLine>     18         print (done)<NewLine>     19 <NewLine>---&gt; 20 scripted_module = torch.jit.script(MyModule())<NewLine>     21 scripted_module.func()<NewLine><NewLine>C:\ProgramData\Anaconda3\lib\site-packages\torch\jit\__init__.py in script(obj, optimize, _frames_up, _rcb)<NewLine>   1201 <NewLine>   1202     if isinstance(obj, torch.nn.Module):<NewLine>-&gt; 1203         return torch.jit.torch.jit._recursive.recursive_script(obj)<NewLine>   1204 <NewLine>   1205     qualified_name = _qualified_name(obj)<NewLine><NewLine>C:\ProgramData\Anaconda3\lib\site-packages\torch\jit\_recursive.py in recursive_script(mod, exclude_methods)<NewLine>    171     filtered_methods = filter(ignore_overloaded, methods)<NewLine>    172     stubs = list(map(make_stub, filtered_methods))<NewLine>--&gt; 173     return copy_to_script_module(mod, overload_stubs + stubs)<NewLine>    174 <NewLine>    175 <NewLine><NewLine>C:\ProgramData\Anaconda3\lib\site-packages\torch\jit\_recursive.py in copy_to_script_module(original, stubs)<NewLine>     93             setattr(script_module, name, item)<NewLine>     94 <NewLine>---&gt; 95     torch.jit._create_methods_from_stubs(script_module, stubs)<NewLine>     96 <NewLine>     97     # Now that methods have been compiled, take methods that have been compiled<NewLine><NewLine>C:\ProgramData\Anaconda3\lib\site-packages\torch\jit\__init__.py in _create_methods_from_stubs(self, stubs)<NewLine>   1421     rcbs = [m.resolution_callback for m in stubs]<NewLine>   1422     defaults = [get_default_args(m.original_method) for m in stubs]<NewLine>-&gt; 1423     self._c._create_methods(self, defs, rcbs, defaults)<NewLine>   1424 <NewLine>   1425 # For each user-defined class that subclasses ScriptModule this meta-class,<NewLine><NewLine>RuntimeError: Unable to cast Python instance of type &lt;class 'int'&gt; to C++ type 'unsigned __int64'<NewLine></code></pre><NewLine><p>I appreciate any help or comment.</p><NewLine></div>",https://discuss.pytorch.org/u/afshin67,(Afshin Oroojlooy),afshin67,"November 27, 2019, 10:42pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>TorchScript only supports PyTorch and the <code>math</code> module, so <code>numpy</code> functions won’t work natively and can’t be exported. You can use <code>torch.jit.ignore</code> as you have done to leave a call to the Python interpreter. Modifying your example slightly and running with the latest version of PyTorch:</p><NewLine><pre><code class=""lang-python"">@torch.jit.ignore<NewLine>def call_np() -&gt; int:<NewLine>    return np.random.choice(2, p=[.95,.05])<NewLine><NewLine>class MyModule(nn.Module):<NewLine>    def forward(self):<NewLine>        pass<NewLine><NewLine>    @torch.jit.export<NewLine>    def func(self):<NewLine>        done = call_np()<NewLine>        print (done)<NewLine><NewLine>scripted_module = torch.jit.script(MyModule())<NewLine>print(scripted_module.func.graph)<NewLine>scripted_module.func()<NewLine></code></pre><NewLine><p>prints</p><NewLine><pre><code class=""lang-auto"">graph(%self : __torch__.MyModule):<NewLine>  %3 : None = prim::Constant() # ../test.py:184:4<NewLine>  %done.1 : int = ^call_np()() # ../test.py:185:15<NewLine>   = prim::Print(%done.1) # ../test.py:186:8<NewLine>  return (%3)<NewLine><NewLine>0<NewLine></code></pre><NewLine><p>In which we can see <code>^call_np()</code> which is an upcall to Python. You must also type annotate <code>call_np</code> since all arguments / returns are assumed to be <code>Tensor</code>s unless otherwise specified. <code>torch.jit.export</code> is also intended to be used only as a decorator on a function (<a href=""https://pytorch.org/docs/master/jit.html#torch.jit.export"" rel=""nofollow noopener"">see here for details</a>).</p><NewLine><p>I couldn’t repro your exact error, could you <a href=""https://github.com/pytorch/pytorch/issues/new?template=bug-report.md"" rel=""nofollow noopener"">file an issue on GitHub</a> with a full repro to produce your issue? Even if what’s happening should throw an error the message could be made more clear. I also ran into <a href=""https://github.com/pytorch/pytorch/issues/30625"" rel=""nofollow noopener"">this issue</a> while trying to repro it which seems like another bug.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the answer. I’ll fine the the bug on github page.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/afshin67; <NewLine> ,"REPLY_DATE 1: December 2, 2019,  8:54pm; <NewLine> REPLY_DATE 2: December 3, 2019,  3:49pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
62746,Unknown builtin op: aten::bool,2019-12-02T12:48:08.099Z,1,150,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Which function can be used to replace <code>tensor.bool()</code>?</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""/home/dai/scripts/mobileocr/detector/mobilenet_east_deploy_v2.py"", line 311, in &lt;module&gt;<NewLine>    script_module = torch.jit.script(net)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1203, in script<NewLine>    return torch.jit.torch.jit._recursive.recursive_script(obj)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 173, in recursive_script<NewLine>    return copy_to_script_module(mod, overload_stubs + stubs)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 95, in copy_to_script_module<NewLine>    torch.jit._create_methods_from_stubs(script_module, stubs)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1423, in _create_methods_from_stubs<NewLine>    self._c._create_methods(self, defs, rcbs, defaults)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 222, in try_compile_fn<NewLine>    return torch.jit.script(fn, _rcb=rcb)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1226, in script<NewLine>    fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 222, in try_compile_fn<NewLine>    return torch.jit.script(fn, _rcb=rcb)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1226, in script<NewLine>    fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 222, in try_compile_fn<NewLine>    return torch.jit.script(fn, _rcb=rcb)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1226, in script<NewLine>    fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))<NewLine>RuntimeError: <NewLine>Unknown builtin op: aten::bool.<NewLine>Here are some suggestions: <NewLine>	aten::Bool<NewLine>	aten::roll<NewLine><NewLine>The original call is:<NewLine>at /home/dai/scripts/mobileocr/detector/mobilenet_east_deploy_v2.py:291:88<NewLine>def is_valid_poly_torch(res,score_shape,scale:int):<NewLine>    # cnt = 0<NewLine>    # for i in range(res.shape[1]):<NewLine>    #     if res[0,i] &lt; 0 or res[0,i] &gt;= score_shape[1] * 4 or \<NewLine>    #         res[1,i] &lt; 0 or res[1,i] &gt;= score_shape[0] * 4:<NewLine>    #         cnt += 1<NewLine>    # return True if cnt &lt;= 1 else False<NewLine>    print(""res shape"",res.shape)<NewLine>    cnt = torch.sum(<NewLine>        (res[:,0,:] &lt; 0) + (res[:,0,:] &gt;= score_shape[1] * scale) + (res[:,1,:] &lt; 0) + (res[:,1,:] &gt;= score_shape[0] * scale).bool(),<NewLine>                                                                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        dim = 1<NewLine>    )<NewLine>    print(""cnt shape"",cnt.shape)<NewLine>    return cnt &lt;= 1<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"December 2, 2019, 12:49pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is likely a bug, can you <a href=""https://github.com/pytorch/pytorch/issues/new?template=bug-report.md"" rel=""nofollow noopener"">open an issue on GitHub</a>? Thanks!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your replay, I opened an issue on Github:</p><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/30638"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/30638"" rel=""nofollow noopener"" target=""_blank"">Unknown builtin op: aten::bool</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-12-03"" data-format=""ll"" data-time=""00:55:24"" data-timezone=""UTC"">12:55AM - 03 Dec 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/Arctanxy"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""Arctanxy"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars3.githubusercontent.com/u/19771342?v=4"" width=""20""/><NewLine>          Arctanxy<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">🐛 Bug<NewLine>tensor.bool() doesn't work in torch.jit.script：<NewLine>Code sample to reproduce this error:<NewLine>class net(torch.nn.Module):<NewLine> def __init__(self):<NewLine> super(net,self).__init__()<NewLine> def forward(self,x):<NewLine> return x.bool()<NewLine>model = net()<NewLine>ts...</p><NewLine></div><NewLine><div class=""labels""><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dalalaa; <NewLine> ,"REPLY_DATE 1: December 2, 2019,  8:33pm; <NewLine> REPLY_DATE 2: December 3, 2019, 12:56am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
62692,How to convert nms to torchscript?,2019-12-02T00:51:57.382Z,1,193,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to convert a detection model to torchscript, but there are some control/loop in nms code, how can I replace them with operations supported in torchscript?</p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"December 2, 2019, 12:51am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can use normal Python control flow in <a href=""https://pytorch.org/docs/stable/jit.html#torch.jit.script"" rel=""nofollow noopener"">script mode</a> (in contrast to tracing which it sounds like you’re talking about). Are you running into any specific issues?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you, script mode solved my problem.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dalalaa; <NewLine> ,"REPLY_DATE 1: December 3, 2019, 12:31am; <NewLine> REPLY_DATE 2: December 3, 2019, 12:31am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
57990,Weird Error when using TorchScript,2019-10-11T21:21:02.499Z,1,211,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am implementing a variant of LSTMs using TorchScript by modifying the code in the fastrnn benchmark written by <a class=""mention"" href=""/u/tom"">@tom</a> but I am getting a weird error:</p><NewLine><pre><code class=""lang-bash"">RuntimeError: <NewLine>Return value was annotated as having type Tuple[Tensor, List[__torch__.model.subLSTM.nn.GRNState]] but is actually of type Tuple[Tensor, List[__torch__.model.subLSTM.nn.GRNState]]:<NewLine>at ../../src/model/subLSTM/nn.py:214:9<NewLine>            if i &lt; self.num_layers - 1:<NewLine>                output = self.dropout_layer(output)<NewLine><NewLine>            output_states.append(out_state)<NewLine>            i += 1<NewLine><NewLine>        if self.batch_first:<NewLine>            output = output.transpose(0, 1)<NewLine><NewLine>        return output, output_states<NewLine>        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine></code></pre><NewLine><p>Which does not make sense since it is the same type. The code for the mode is the following:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.nn as nn<NewLine>from torch.nn import Parameter<NewLine>import torch.jit as jit<NewLine>import warnings<NewLine>from collections import namedtuple<NewLine>from typing import List, Tuple<NewLine>from torch import Tensor<NewLine><NewLine><NewLine>GRNState = namedtuple('GRNState', ['hx', 'cx'])<NewLine><NewLine><NewLine>def reverse(lst):<NewLine>    # type: (List[Tensor]) -&gt; List[Tensor]<NewLine>    return lst[::-1]<NewLine><NewLine><NewLine>class SubLSTMCell(jit.ScriptModule):<NewLine>    def __init__(self, input_size, hidden_size):<NewLine>        super(SubLSTMCell, self).__init__()<NewLine>        self.input_size = input_size<NewLine>        self.hidden_size = hidden_size<NewLine>        self.weight_ih = Parameter(torch.randn(4 * hidden_size, input_size))<NewLine>        self.weight_hh = Parameter(torch.randn(4 * hidden_size, hidden_size))<NewLine>        self.bias_ih = Parameter(torch.randn(4 * hidden_size))<NewLine>        self.bias_hh = Parameter(torch.randn(4 * hidden_size))<NewLine><NewLine>    @jit.script_method<NewLine>    def forward(self, input: Tensor, state: GRNState) -&gt; Tuple[Tensor, GRNState]:<NewLine>        hx, cx = state<NewLine>        gates = (torch.mm(input, self.weight_ih.t()) + self.bias_ih +<NewLine>                 torch.mm(hx, self.weight_hh.t()) + self.bias_hh).sigmoid()<NewLine>        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)<NewLine><NewLine>        cy = (forgetgate * cx) - (ingate - cellgate)<NewLine>        hy = outgate - torch.tanh(cy)<NewLine><NewLine>        return hy, GRNState(hy, cy)<NewLine><NewLine><NewLine>class LayerNormSubLSTMCell(jit.ScriptModule):<NewLine>    def __init__(self, input_size, hidden_size):<NewLine>        super(LayerNormSubLSTMCell, self).__init__()<NewLine>        self.input_size = input_size<NewLine>        self.hidden_size = hidden_size<NewLine>        self.weight_ih = Parameter(torch.randn(4 * hidden_size, input_size))<NewLine>        self.weight_hh = Parameter(torch.randn(4 * hidden_size, hidden_size))<NewLine>        # The layernorms provide learnable biases<NewLine><NewLine>        self.layernorm_i = nn.LayerNorm(4 * hidden_size)<NewLine>        self.layernorm_h = nn.LayerNorm(4 * hidden_size)<NewLine>        self.layernorm_c = nn.LayerNorm(hidden_size)<NewLine><NewLine>    @jit.script_method<NewLine>    def forward(self, input: Tensor, state: GRNState) -&gt; Tuple[Tensor, GRNState]:<NewLine>        hx, cx = state<NewLine>        igates = self.layernorm_i(torch.mm(input, self.weight_ih.t()))<NewLine>        hgates = self.layernorm_h(torch.mm(hx, self.weight_hh.t()))<NewLine>        gates = (igates + hgates).sigmoid()<NewLine>        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)<NewLine><NewLine>        cy = self.layernorm_c((forgetgate * cx) + (ingate - cellgate))<NewLine>        hy = outgate - torch.tanh(cy)<NewLine><NewLine>        return hy, GRNState(hy, cy)<NewLine><NewLine><NewLine>class GRNLayer(jit.ScriptModule):<NewLine>    def __init__(self, cell, *cell_args):<NewLine>        super(GRNLayer, self).__init__()<NewLine>        self.cell = cell(*cell_args)<NewLine><NewLine>    @jit.script_method<NewLine>    def forward(self, input: Tensor, state: GRNState) -&gt; Tuple[Tensor, GRNState]:<NewLine>        inputs = input.unbind(0)<NewLine>        outputs: List[Tensor] = []<NewLine>        for i in range(len(inputs)):<NewLine>            out, state = self.cell(inputs[i], state)<NewLine>            outputs += [out]<NewLine>        return torch.stack(outputs), state<NewLine><NewLine><NewLine>class ReverseGRNLayer(jit.ScriptModule):<NewLine>    def __init__(self, cell, *cell_args):<NewLine>        super(ReverseGRNLayer, self).__init__()<NewLine>        self.cell = cell(*cell_args)<NewLine><NewLine>    @jit.script_method<NewLine>    def forward(self, input:Tensor, state:GRNState) -&gt; Tuple[Tensor, GRNState]:<NewLine>        inputs = reverse(input.unbind(0))<NewLine>        outputs = jit.annotate(List[Tensor], [])<NewLine>        for i in range(len(inputs)):<NewLine>            out, state = self.cell(inputs[i], state)<NewLine>            outputs += [out]<NewLine>        return torch.stack(reverse(outputs)), state<NewLine><NewLine><NewLine>class BidirLayer(jit.ScriptModule):<NewLine>    __constants__ = ['directions']<NewLine><NewLine>    def __init__(self, cell, *cell_args):<NewLine>        super(BidirLayer, self).__init__()<NewLine>        self.directions = nn.ModuleList([<NewLine>            GRNLayer(cell, *cell_args),<NewLine>            ReverseGRNLayer(cell, *cell_args),<NewLine>        ])<NewLine><NewLine>    @jit.script_method<NewLine>    def forward(self, input: Tensor, states: List[GRNState]) -&gt; Tuple[Tensor, List[GRNState]]:<NewLine>        outputs: List[Tensor] = []<NewLine>        output_states: List[GRNState] = []<NewLine><NewLine>        i = 0<NewLine>        for direction in self.directions:<NewLine>            state = states[i]<NewLine>            out, out_state = direction(input, state)<NewLine>            outputs += [out]<NewLine>            output_states += [out_state]<NewLine>            i += 1<NewLine>        return torch.cat(outputs, -1), output_states<NewLine><NewLine><NewLine>def init_stacked_lstm(num_layers, layer, cell, input_size, hidden_size):<NewLine>    layers = [layer(cell, input_size, hidden_size)] + \<NewLine>             [layer(cell, hidden_size, hidden_size) for _ in range(num_layers - 1)]<NewLine>    return nn.ModuleList(layers)<NewLine><NewLine><NewLine>def init_states(num_layers, batch_size, hidden_size, device):<NewLine>    states: List[GRNState] = []<NewLine>    temp = torch.randn(num_layers, batch_size, hidden_size,<NewLine>                       2, device=device).unbind(0)<NewLine><NewLine>    for s in temp:<NewLine>        hx, cx = s.unbind(2)<NewLine>        states.append(GRNState(hx, cx))<NewLine><NewLine>    return states<NewLine><NewLine><NewLine>class SubLSTM(jit.ScriptModule):<NewLine>    # Necessary for iterating through self.layers and dropout support<NewLine>    __constants__ = ['layers', 'num_layers', 'batch_first', 'hidden_size']<NewLine><NewLine>    def __init__(self, input_size, hidden_size, num_layers, bias=True,<NewLine>                batch_first=False, dropout=0.0, bidirectional=False,<NewLine>                layer_norm=False):<NewLine>        super(SubLSTM, self).__init__()<NewLine><NewLine>        layer = BidirLayer if bidirectional else GRNLayer<NewLine>        cell = LayerNormSubLSTMCell if layer_norm else SubLSTMCell<NewLine><NewLine>        self.layers = init_stacked_lstm(<NewLine>            num_layers, layer, cell, input_size, hidden_size)<NewLine><NewLine>        if dropout &gt; 0 and num_layers == 1:<NewLine>            warnings.warn(""dropout lstm adds dropout layers after all but last ""<NewLine>                          ""recurrent layer, it expects num_layers greater than ""<NewLine>                          ""1, but got num_layers = 1"")<NewLine><NewLine>        self.dropout_layer = nn.Dropout(dropout)<NewLine><NewLine>        self.num_layers = num_layers<NewLine>        self.batch_first = batch_first<NewLine>        self.hidden_size = hidden_size<NewLine><NewLine><NewLine>    @jit.script_method<NewLine>    def forward(self, input: Tensor, states: List[GRNState]=None) -&gt; Tuple[Tensor, List[GRNState]]:<NewLine>        output = input if not self.batch_first else input.transpose(0, 1)<NewLine>        output_states: List[GRNState] = []<NewLine><NewLine>        if states is None:<NewLine>            states = init_states(self.num_layers, output.size(1),<NewLine>                                 self.hidden_size, output.device)<NewLine><NewLine>        i = 0<NewLine>        for rnn_layer in self.layers:<NewLine>            state = states[i]<NewLine>            output, out_state = rnn_layer(output, state)<NewLine><NewLine>            # Apply the dropout layer except the last layer<NewLine>            if i &lt; self.num_layers - 1:<NewLine>                output = self.dropout_layer(output)<NewLine><NewLine>            output_states.append(out_state)<NewLine>            i += 1<NewLine><NewLine>        if self.batch_first:<NewLine>            output = output.transpose(0, 1)<NewLine><NewLine>        return output, output_states<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/mllera,(Milton Llera),mllera,"October 11, 2019,  9:21pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I didn’t write that code, I just used it for benchmarking.</p><NewLine><p>So are you using PyTorch 1.3? Maybe making those a plain Tensor tuple instead of a Namedtuple helps.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>What version are you using ? I tried this on master and it worked.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Thanks, it was the named tuple. A bit annoying since they can make the code a lot cleaner. By the way I don’t get the same level of performance as in the examples, which is a bit weird.</p><NewLine><p>PS: Sorry for the very delayed reply</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/eellison; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mllera; <NewLine> ,"REPLY_DATE 1: November 28, 2019, 10:59am; <NewLine> REPLY_DATE 2: October 15, 2019,  4:16pm; <NewLine> REPLY_DATE 3: November 28, 2019, 10:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
62389,Unknown builtin op: aten::Tensor,2019-11-28T07:57:02.576Z,1,493,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I got this error while trying to convert nn.module to torchscript.</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""/home/dai/scripts/mobileocr/detector/mobilenet_east_deploy.py"", line 240, in &lt;module&gt;<NewLine>    script_module = torch.jit.script(net)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1203, in script<NewLine>    return torch.jit.torch.jit._recursive.recursive_script(obj)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 173, in recursive_script<NewLine>    return copy_to_script_module(mod, overload_stubs + stubs)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 95, in copy_to_script_module<NewLine>    torch.jit._create_methods_from_stubs(script_module, stubs)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1423, in _create_methods_from_stubs<NewLine>    self._c._create_methods(self, defs, rcbs, defaults)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 222, in try_compile_fn<NewLine>    return torch.jit.script(fn, _rcb=rcb)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1226, in script<NewLine>    fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/_recursive.py"", line 222, in try_compile_fn<NewLine>    return torch.jit.script(fn, _rcb=rcb)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1226, in script<NewLine>    fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))<NewLine>RuntimeError: <NewLine>Unknown builtin op: aten::Tensor.<NewLine>Here are some suggestions: <NewLine>	aten::tensor<NewLine><NewLine>The original call is:<NewLine>at /home/dai/scripts/mobileocr/detector/mobilenet_east_deploy.py:193:17<NewLine>    for i in range(valid_pos.shape[0]):<NewLine>        x = valid_pos[i,0]<NewLine>        y = valid_pos[i,1]<NewLine>        y_min = y - d[0,i]<NewLine>        y_max = y + d[1,i]<NewLine>        x_min = x - d[2,i]<NewLine>        x_max = x + d[3,i]<NewLine>        rotate_mat = get_rotate_mat_torch(-angle[i])<NewLine><NewLine>        temp_x = torch.Tensor([[x_min,x_max,x_max,x_min]]) - x<NewLine>                 ~~~~~~~~~~~~ &lt;--- HERE<NewLine>        temp_y = torch.Tensor([[y_min,y_min,y_max,y_max]]) - y<NewLine>        coordinates = torch.cat([temp_x,temp_y],dim = 0)<NewLine>        # res = torch.dot(rotate_mat,coordinates)<NewLine>        res = torch.matmul(rotate_mat,coordinates)<NewLine>        res[0,:] += x<NewLine>        res[1,:] += y<NewLine><NewLine>        if is_valid_poly_torch(res,score_shape,scale):<NewLine>            index.append(i)<NewLine><NewLine></code></pre><NewLine><p>My pytorch version is 1.3.1 + cu92</p><NewLine><p>How can I solve this problem or what function can be used to replace torch.Tensor()?</p><NewLine><p>Please remind me if more information is needed.</p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"November 28, 2019,  7:58am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Try to use the suggested <code>torch.tensor</code> instead (lowercase t).<br/><NewLine>I would generally not recommend to use <code>torch.Tensor</code>, as this might give you uninitialized code, e.g. by calling <code>torch.Tensor(10)</code>.<br/><NewLine><code>torch.tensor</code> is the factory method to pass values as a list etc.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much, problem solved.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dalalaa; <NewLine> ,"REPLY_DATE 1: November 28, 2019,  8:08am; <NewLine> REPLY_DATE 2: November 28, 2019,  8:08am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
61941,Is there any way to parallelize code over modules that don’t depend on each other?,2019-11-23T08:46:35.716Z,0,102,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In TF, independent sections of the graph are automatically executed in parallel if called together in one session.run(…) call. A simple case is this:</p><NewLine><pre><code class=""lang-auto"">xs = [ ... ] # list of torch.Tensors<NewLine>models = [ ... ]  # list of nn.Modules<NewLine>out = [m(x) for m, x in zip(models, xu)]<NewLine></code></pre><NewLine><p>It seems to me if this code is dynamically executed, it is not possible to parallelize the calls of m1(x1), m2(x2), … mn(xn) for some obvious performance benefits.</p><NewLine><p>What is the best way in PyTorch to achieve this effect? Is it to use the jit?</p><NewLine></div>",https://discuss.pytorch.org/u/amatsukawa,(Akihiro Matsukawa),amatsukawa,"November 23, 2019,  8:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The JIT execution won’t run anything in parallel, right now it mostly does code transformations and fusions to optimize performance.</p><NewLine><p>If you have multiple GPUs, <a href=""https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"" rel=""nofollow noopener"">DataParallel</a> might be helpful.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: November 27, 2019,  7:36pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
61911,Multi-gpu inference with torchscript model is slow,2019-11-23T01:25:27.886Z,0,331,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I traced some classification models from torchvision and then wanted to apply  <code>DataParallel</code> for multi-gpu inference. While no error is thrown, for more than 1 gpu passed as <code>device_ids</code> to <code>DataParallel</code>, inference on the torchscript model is much slower than the regular torch model (5x-10x slower), even if batch_size=1, which results in effectively only 1 gpu being used.</p><NewLine></div>",https://discuss.pytorch.org/u/Arvind_Vepa,(Arvind Vepa),Arvind_Vepa,"November 23, 2019,  1:25am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If the batch size is set to 1, only a single GPU will be used, as the data cannot be chunked and send to each device.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>so i know that. I was just giving an example of the issues I was encountering with <code>DataParallel</code>. The issue is the torchscript model inferences are significantly slower than the standard torch model inferences. I tested a few different parameters, like <code>batch_size</code> and different number of gpu device ids with <code>DataParallel</code> (as well as a few different architectures). I found with 1 device_id or when not using <code>DataParallel</code> at all, the model inference speed was similar between the torchscript and torch model. However, when there was more than 1 device_id passed to <code>DataParallel</code>, the inference speed for the torchscript model reduced dramatically in comparison to the standard torch model with the same parameters (5x-10x slower). I brought up the example because it was curious that even when only 1 gpu was effectively being used when batch_size=1 with 2 device_ids passed, the torchscript model was still significantly slower than the regular torch model. However, when 1 device_id was passed, the torchscript model was similar in speed to the torch model.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Arvind_Vepa; <NewLine> ,"REPLY_DATE 1: November 23, 2019,  1:31am; <NewLine> REPLY_DATE 2: November 23, 2019,  2:37am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
61766,Why isn&rsquo;t torch::jit::script::Module::forward const?,2019-11-21T17:30:54.598Z,5,191,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Wondering why the forward method isn’t const?  And since it’s not, what’s the proper way to get back to the loaded state of the module without having to actually reload?</p><NewLine><p>Thanks,<br/><NewLine>Marc</p><NewLine></div>",https://discuss.pytorch.org/u/kb1ooo,(Marc Vaillant),kb1ooo,"November 21, 2019,  7:41pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/kb1ooo"">@kb1ooo</a> Would you like to post this question in <a href=""https://discuss.pytorch.org/c/jit"">https://discuss.pytorch.org/c/jit</a>? The JIT team monitors that channel more closely and we would be able to have a response there.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/yf225"">@yf225</a> ok, thanks.  Is there a way to just move it there or do I repost?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/kb1ooo"">@kb1ooo</a> Reposting might be the easiest, thanks and sorry for the bad experience <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/yf225"">@yf225</a> oops, ok I saw that I could change the category so that’s what I did.  Maybe not as effective as a repost.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>Module</code>s can have stateful data attached to themselves as parameters or attributes. These can be mutated during <code>forward</code> (or any other method that is run on the <code>Module</code>), so it doesn’t make sense to have it be <code>const</code>.</p><NewLine><p>We don’t have any way of backing out changes to <code>Module</code> state, so re-loading is the only way to get back a known state. We may in the future have a user-accessible way of stating that some method is pure and has no mutable operations, but that’s still something we’re discussing.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok, understood, thank you.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/yf225; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kb1ooo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/yf225; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/kb1ooo; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/kb1ooo; <NewLine> ,"REPLY_DATE 1: November 21, 2019,  7:11pm; <NewLine> REPLY_DATE 2: November 21, 2019,  7:14pm; <NewLine> REPLY_DATE 3: November 21, 2019,  7:48pm; <NewLine> REPLY_DATE 4: November 21, 2019,  7:51pm; <NewLine> REPLY_DATE 5: November 21, 2019,  8:02pm; <NewLine> REPLY_DATE 6: November 21, 2019,  8:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
61660,Failed JIT Trace for Torch Mobile iOS (Size not specified in nn.interpolate?) (Segmentation),2019-11-20T21:11:39.574Z,0,194,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m attempting to JIT trace a semantic segmentation model based on a Resnet50 dilated encoder and an accompanying PPMDeepsup decoder to use in iOS/torch mobile.</p><NewLine><p>When running the following to JIT trace my segmentation module with a random image input of 224x224:</p><NewLine><pre><code class=""lang-auto"">input_tensor = torch.rand(1, 3, 224, 224)<NewLine>model.eval()<NewLine>script_model = torch.jit.trace(model, input_tensor)<NewLine></code></pre><NewLine><p>I get the  the following trace. Apparently when my Forward function calls nn.interpolate() it claims that the size variable (final image segmentation size of 224x224) is not provided (found None) even though as seen in the trace below I’ve even tried hard coding the necessary tuple of (224, 224). Why am I still receiving this exception?</p><NewLine><p><strong>Trace</strong></p><NewLine><pre><code class=""lang-auto"">~/thd-visual-ai/segmentation/semantic-segmentation-pytorch/models/models.py in forward(self, conv_out, segSize)<NewLine>    481         print(segSize)<NewLine>    482         if self.use_softmax:  # is True during inference<NewLine>--&gt; 483             x = nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)<NewLine>    484             x = nn.functional.softmax(x, dim=1)<NewLine>    485             return x<NewLine><NewLine>~/Desktop/Projects/thdEnv/lib/python3.6/site-packages/torch/nn/functional.py in interpolate(input, size, scale_factor, mode, align_corners)<NewLine>   2516         raise NotImplementedError(""Got 4D input, but linear mode needs 3D input"")<NewLine>   2517     elif input.dim() == 4 and mode == 'bilinear':<NewLine>-&gt; 2518         return torch._C._nn.upsample_bilinear2d(input, _output_size(2), align_corners)<NewLine>   2519     elif input.dim() == 4 and mode == 'trilinear':<NewLine>   2520         raise NotImplementedError(""Got 4D input, but trilinear mode needs 5D input"")<NewLine><NewLine>~/Desktop/Projects/thdEnv/lib/python3.6/site-packages/torch/nn/functional.py in _output_size(dim)<NewLine>   2470 <NewLine>   2471     def _output_size(dim):<NewLine>-&gt; 2472         _check_size_scale_factor(dim)<NewLine>   2473         if size is not None:<NewLine>   2474             return size<NewLine><NewLine>~/Desktop/Projects/thdEnv/lib/python3.6/site-packages/torch/nn/functional.py in _check_size_scale_factor(dim)<NewLine>   2461     def _check_size_scale_factor(dim):<NewLine>   2462         if size is None and scale_factor is None:<NewLine>-&gt; 2463             raise ValueError('either size or scale_factor should be defined')<NewLine>   2464         if size is not None and scale_factor is not None:<NewLine>   2465             raise ValueError('only one of size or scale_factor should be defined')<NewLine><NewLine>ValueError: either size or scale_factor should be defined<NewLine><NewLine></code></pre><NewLine><p>I’ve defined size at line <strong>483</strong> hardcoded to try to fix this and it’s still giving none. I will attach the model.summary() here of my semantic segmentation model to help.</p><NewLine><pre><code class=""lang-auto"">Sequential(<NewLine>  (0): ResnetDilated(<NewLine>    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<NewLine>    (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>    (relu1): ReLU(inplace=True)<NewLine>    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>    (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>    (relu2): ReLU(inplace=True)<NewLine>    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>    (bn3): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>    (relu3): ReLU(inplace=True)<NewLine>    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)<NewLine>    (layer1): Sequential(<NewLine>      (0): Bottleneck(<NewLine>        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>        (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn3): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (relu): ReLU(inplace=True)<NewLine>        (downsample): Sequential(<NewLine>          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>          (1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        )<NewLine>      )<NewLine>      (1): Bottleneck(<NewLine>        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>        (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn3): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (relu): ReLU(inplace=True)<NewLine>      )<NewLine>      (2): Bottleneck(<NewLine>        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>        (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn3): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (relu): ReLU(inplace=True)<NewLine>      )<NewLine>    )<NewLine>    (layer2): Sequential(<NewLine>      (0): Bottleneck(<NewLine>        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<NewLine>        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (relu): ReLU(inplace=True)<NewLine>        (downsample): Sequential(<NewLine>          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)<NewLine>          (1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        )<NewLine>      )<NewLine>      (1): Bottleneck(<NewLine>        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (relu): ReLU(inplace=True)<NewLine>      )<NewLine>      (2): Bottleneck(<NewLine>        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (relu): ReLU(inplace=True)<NewLine>      )<NewLine>      (3): Bottleneck(<NewLine>        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (relu): ReLU(inplace=True)<NewLine>      )<NewLine>    )<NewLine>    (layer3): Sequential(<NewLine>      (0): Bottleneck(<NewLine>        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (relu): ReLU(inplace=True)<NewLine>        (downsample): Sequential(<NewLine>          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>          (1): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        )<NewLine>      )<NewLine>      (1): Bottleneck(<NewLine>        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)<NewLine>        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (relu): ReLU(inplace=True)<NewLine>      )<NewLine>      (2): Bottleneck(<NewLine>        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)<NewLine>        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (relu): ReLU(inplace=True)<NewLine>      )<NewLine>      (3): Bottleneck(<NewLine>        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)<NewLine>        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (relu): ReLU(inplace=True)<NewLine>      )<NewLine>      (4): Bottleneck(<NewLine>        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)<NewLine>        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (relu): ReLU(inplace=True)<NewLine>      )<NewLine>      (5): Bottleneck(<NewLine>        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)<NewLine>        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (relu): ReLU(inplace=True)<NewLine>      )<NewLine>    )<NewLine>    (layer4): Sequential(<NewLine>      (0): Bottleneck(<NewLine>        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)<NewLine>        (bn2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn3): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (relu): ReLU(inplace=True)<NewLine>        (downsample): Sequential(<NewLine>          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>          (1): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        )<NewLine>      )<NewLine>      (1): Bottleneck(<NewLine>        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)<NewLine>        (bn2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn3): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (relu): ReLU(inplace=True)<NewLine>      )<NewLine>      (2): Bottleneck(<NewLine>        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)<NewLine>        (bn2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn3): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (relu): ReLU(inplace=True)<NewLine>      )<NewLine>    )<NewLine>  )<NewLine>  (1): PPMDeepsup(<NewLine>    (ppm): ModuleList(<NewLine>      (0): Sequential(<NewLine>        (0): AdaptiveAvgPool2d(output_size=1)<NewLine>        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (3): ReLU(inplace=True)<NewLine>      )<NewLine>      (1): Sequential(<NewLine>        (0): AdaptiveAvgPool2d(output_size=2)<NewLine>        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (3): ReLU(inplace=True)<NewLine>      )<NewLine>      (2): Sequential(<NewLine>        (0): AdaptiveAvgPool2d(output_size=3)<NewLine>        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (3): ReLU(inplace=True)<NewLine>      )<NewLine>      (3): Sequential(<NewLine>        (0): AdaptiveAvgPool2d(output_size=6)<NewLine>        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>        (3): ReLU(inplace=True)<NewLine>      )<NewLine>    )<NewLine>    (cbr_deepsup): Sequential(<NewLine>      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>      (2): ReLU(inplace=True)<NewLine>    )<NewLine>    (conv_last): Sequential(<NewLine>      (0): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)<NewLine>      (2): ReLU(inplace=True)<NewLine>      (3): Dropout2d(p=0.1, inplace=False)<NewLine>      (4): Conv2d(512, 150, kernel_size=(1, 1), stride=(1, 1))<NewLine>    )<NewLine>    (conv_last_deepsup): Conv2d(512, 150, kernel_size=(1, 1), stride=(1, 1))<NewLine>    (dropout_deepsup): Dropout2d(p=0.1, inplace=False)<NewLine>  )<NewLine>  (2): NLLLoss()<NewLine>)<NewLine></code></pre><NewLine><p>I appreciate any help!</p><NewLine></div>",https://discuss.pytorch.org/u/HussainHaris,(Haris),HussainHaris,"November 20, 2019,  9:11pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you share the <code>model</code> itself? A simple test around interpolate (using the nightly version of PyTorch) seems to indicate that it works</p><NewLine><pre><code class=""lang-python"">def test_interpolate(x):<NewLine>    return nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)<NewLine><NewLine># this is successful<NewLine>x = torch.jit.trace(test_interpolate, (torch.randn(1, 3, 224, 224)))<NewLine>print(x.graph)<NewLine></code></pre><NewLine><p>If the bug persists, you might have better luck using <code>torch.jit.script</code> around the area that calls interpolate.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: November 21, 2019,  8:06pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
61720,Will torch script support typing.Union?,2019-11-21T10:41:24.809Z,0,302,"<div class=""post"" itemprop=""articleBody""><NewLine><p><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/23420"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/23420"" rel=""nofollow noopener"" target=""_blank"">Will the Jit support typehint OPTIONAL or UNION?</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-07-26"" data-format=""ll"" data-time=""06:59:13"" data-timezone=""UTC"">06:59AM - 26 Jul 19 UTC</span><NewLine></div><NewLine><div class=""date""><NewLine>          closed <span class=""discourse-local-date"" data-date=""2019-07-26"" data-format=""ll"" data-time=""11:40:37"" data-timezone=""UTC"">11:40AM - 26 Jul 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/NeilWangziyu"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""NeilWangziyu"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/16078718?v=4"" width=""20""/><NewLine>          NeilWangziyu<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">❓ Questions and Help<NewLine>Will the Jit support typehint OPTIONAL or UNION?<NewLine>As the title indicates, will jit support we use UNION and...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">jit</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine><aside class=""quote"" data-post=""7"" data-topic=""60845""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v4/letter/z/3da27b/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/jit-mobile-is-isinstance-supposed-to-work-with-torchscript/60845/7"">[JIT] [Mobile] Is isinstance() supposed to work with TorchScript</a> <a class=""badge-wrapper bullet"" href=""/c/jit""><span class=""badge-category-bg"" style=""background-color: #0088CC;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""A category for torchscript and the PyTorch JIT compiler"">jit</span></a><NewLine></div><NewLine><blockquote><NewLine>    This returns None but anyway I haven’t looked for that functionality <img alt=""stuck_out_tongue"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/stuck_out_tongue.png?v=9"" title=""stuck_out_tongue""/><NewLine></blockquote><NewLine></aside><NewLine><br/><NewLine>any workaround?</p><NewLine></div>",https://discuss.pytorch.org/u/songzw,,songzw,"November 21, 2019, 10:41am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think torchscript supports Optional: <a href=""https://pytorch.org/docs/master/jit.html#optional-type-refinement"">https://pytorch.org/docs/master/jit.html#optional-type-refinement</a>.</p><NewLine><p>TorchScript doesn’t support Union. If you have a use case, opening a feature request on the repository (instead of a question) might be the way to go.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/richard; <NewLine> ,"REPLY_DATE 1: November 21, 2019,  3:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
61147,How can I get atttr like stride or padding of Conv2d in cpp ScriptModule,2019-11-16T03:23:28.133Z,0,131,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I instantiated a model and transfrom it into ScriptMoudle using this:</p><NewLine><pre><code class=""lang-auto"">alex_model = torchvision.models.AlexNet(num_classes=10)<NewLine>f = torch.jit.script(alex_model)<NewLine>f.save(""./models/alexnet.pt"")<NewLine></code></pre><NewLine><p>I cannot get the attr like stride of Conv2d when I <code>dump()</code> the model in c++<br/><NewLine>only a constant name like this:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/50b7501a238ae343e1737fc3787cb26cebbfcf5a"" href=""https://discuss.pytorch.org/uploads/default/original/3X/5/0/50b7501a238ae343e1737fc3787cb26cebbfcf5a.png"" title=""12""><img alt=""12"" data-base62-sha1=""bw2WvCBDE1UyVnPdVDzbZOJURoC"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/5/0/50b7501a238ae343e1737fc3787cb26cebbfcf5a_2_10x10.png"" height=""120"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/5/0/50b7501a238ae343e1737fc3787cb26cebbfcf5a_2_690x120.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/5/0/50b7501a238ae343e1737fc3787cb26cebbfcf5a_2_690x120.png, https://discuss.pytorch.org/uploads/default/optimized/3X/5/0/50b7501a238ae343e1737fc3787cb26cebbfcf5a_2_1035x180.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/5/0/50b7501a238ae343e1737fc3787cb26cebbfcf5a_2_1380x240.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">12</span><span class=""informations"">1922×336 15.3 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>How can I get the value like stride or padding of <code>Conv2d</code> when loading the model in c++?<br/><NewLine>THANKS VERY MUCH!</p><NewLine></div>",https://discuss.pytorch.org/u/yy738686337,(Yy738686337),yy738686337,"November 16, 2019,  3:23am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Constant values are inlined directly into the code of the module, so we currently have no way of accessing them once compilation has run, and the modules in <code>nn.Module</code>s store all their values as constants. I opened <a href=""https://github.com/pytorch/pytorch/issues/30019"" rel=""nofollow noopener"">this issue</a> so you can track that for fixes.</p><NewLine><p>As a temporary workaround, you could grab the values you care about in Python and stash them as <a href=""https://pytorch.org/docs/master/jit.html#module-attributes"" rel=""nofollow noopener"">attributes</a> instead of <a href=""https://pytorch.org/docs/master/jit.html#python-defined-constants"" rel=""nofollow noopener"">constants</a>:</p><NewLine><pre><code class=""lang-python"">class M(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.alex_model = torchvision.models.AlexNet(num_classes=10)<NewLine>        self.first_conv_stride = getattr(self.alex_model.features, '0').stride<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.alex_model(x)<NewLine><NewLine><NewLine>sm = torch.jit.script(M())<NewLine>print(sm.first_conv_stride)<NewLine></code></pre><NewLine><p>and in C++</p><NewLine><pre><code class=""lang-cpp"">auto module = torch::jit::load(""alexnet.pt"");<NewLine>std::cout &lt;&lt; module.attr(""first_conv_stride"") &lt;&lt; ""\n"";<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: November 18, 2019,  6:49pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
61060,The size of mobilenetv2 become much bigger when convert to torchscript model,2019-11-15T07:34:41.337Z,1,171,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I convert mobilenet_v2 network to torchscript model, the size increased from 13.5MB to 43.1MB. It’s too big to deploy in Android.</p><NewLine><p>How can I compress the torchscript model ?</p><NewLine><p>My code:</p><NewLine><pre><code class=""lang-auto"">from torchvision.models import mobilenet_v2<NewLine>net = mobilenet_v2()<NewLine>torch.save(net,""net.pth"")<NewLine>script_module = torch.jit.script(net)<NewLine>torch.jit.save(script_module,""net.pt"")<NewLine></code></pre><NewLine><p>The size of models:<br/><NewLine><img alt=""%5D%7D%60OQ1C_(8%40U_2%409%24JSCNWO"" data-base62-sha1=""bwa9VnE0WndtSFVvICHCibR7d1N"" height=""45"" src=""https://discuss.pytorch.org/uploads/default/original/3X/5/0/50baae5dd0d83acf4bb1d329cab187910f14a6db.png"" width=""297""/></p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"November 15, 2019,  7:34am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you on the latest version of PyTorch (1.3)? We made some changes to significantly decrease the size of TorchScript binaries. For <code>mobilenet_v2</code> on the nightly version of PyTorch, I get very similar sizes between eager PyTorch and TorchScript:</p><NewLine><pre><code class=""lang-bash"">$ ls -la --block-size=M<NewLine>-rw-------.  1 user user 15M Nov 15 13:06 net.pt<NewLine>-rw-------.  1 user user 14M Nov 15 13:06 net.pth<NewLine></code></pre><NewLine><p>If you are on PyTorch 1.3 and still are getting this problem, could you try using the nightly PyTorch package and see if that fixes it?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you, the problem solved when I upgraded torch from 1.2 to 1.3.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dalalaa; <NewLine> ,"REPLY_DATE 1: November 18, 2019,  2:44am; <NewLine> REPLY_DATE 2: November 18, 2019,  2:44am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
60926,Jit for tolist(),2019-11-14T03:37:26.164Z,3,373,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone. I attempt to use torch.jit.script on the something like hyp_ctc_state_prev.tolist(), but it doesn’t work。I build the pytorch from source and the torch version is 1.4.0a0+2e7dd54。<br/><NewLine>I’d appreciate if anybody can help me! Or if there is a workable implementation, please let me know! Thanks in advance!<br/><NewLine>here is the log:</p><NewLine><details><NewLine><summary><NewLine>logs</summary><NewLine><p>RuntimeError:                                                                                                                   │                    # will be (2 x beam) hyps at most<br/><NewLine>Tried to access nonexistent attribute or method ‘tolist’ of type ‘Tensor’.:<br/><NewLine>hyps_score = [hyp_score]<br/><NewLine>hyps_ctc_state_prev = [hyp_ctc_state_prev.tolist()]<br/><NewLine>~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;— HERE<br/><NewLine>hyps_ctc_score_prev = [hyp_ctc_score_prev]<br/><NewLine>ended_hyps_score = []</p><NewLine></details><NewLine></div>",https://discuss.pytorch.org/u/huoge,,huoge,"November 14, 2019,  3:37am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is a known bug <a href=""https://github.com/pytorch/pytorch/issues/26752"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/26752</a></p><NewLine><p>You can workaround by implementing it yourself, but it can be cumbersome due to the reasons explained in the bug report (plus the fact that TorchScript does not support generic functions or recursion):</p><NewLine><pre><code class=""lang-python"">def my_1d_tolist(x):<NewLine>    result: List[float] = []<NewLine>    for i in x:<NewLine>        result.append(i.item())<NewLine>    return result<NewLine><NewLine>@torch.jit.script<NewLine>def my_2d_tolist(x):<NewLine>    result: List[List[float]] = []<NewLine>    for i in x:<NewLine>        result.append(my_1d_tolist(i))<NewLine>    return result<NewLine><NewLine>x = torch.ones(2, 2)<NewLine><NewLine># These calls should be the same<NewLine>print(x.tolist())<NewLine>print(my_2d_tolist(x))<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot! I implemented it by myself yesterday and I find that use c++ to load torchscript model only support the function name ‘forward’. If I change the function name ‘forward’ to ‘infer’，and use the decorator <span class=""mention"">@torch.jit.export</span> , transfer the model to torchscript.however, when i use c++ to load the model like  torch::jit::script::Module module = torch::jit::load(‘model.pt’);  and then use module.infer(inputs), the cpp file won’t be built. Do you know why it happen?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>For methods other than <code>forward</code> you have to explicitly get the method and run it. For this <code>Module</code></p><NewLine><pre><code class=""lang-python"">class M(nn.Module):<NewLine>    @torch.jit.export<NewLine>    def infer(self, x):<NewLine>        return x + 10<NewLine><NewLine>torch.jit.script(M()).save(""m.pt"")<NewLine></code></pre><NewLine><p>You can run it in C++ with <code>script::Module::get_method</code></p><NewLine><pre><code class=""lang-cpp"">int main() {<NewLine>  auto module = torch::jit::load(""m.pt"");<NewLine>  auto result = module.get_method(""infer"")({torch::ones({2, 2})});<NewLine>  std::cout &lt;&lt; result &lt;&lt; ""\n"";<NewLine>}<NewLine></code></pre><NewLine><p>We have an open issue to improve our C++ documentation to make things like this more clear in the future.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot! it’s very helpful to me!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/huoge; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/huoge; <NewLine> ,"REPLY_DATE 1: December 19, 2019,  2:54am; <NewLine> REPLY_DATE 2: November 15, 2019,  2:44am; <NewLine> REPLY_DATE 3: December 19, 2019,  2:54am; <NewLine> REPLY_DATE 4: November 18, 2019,  1:49am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 2 Likes; <NewLine> REPLY 4 LIKES: ; <NewLine> 
53567,Quantization as pytorch.script graph manipulation,2019-08-16T22:36:52.442Z,5,766,"<div class=""post"" itemprop=""articleBody""><NewLine><p><strong>Gentle Introduction</strong><br/><NewLine>Quantization is often tackled as a rewrite of the original model. We can overload Convolution (i.e, the <em>module</em> convolution), we can add quantization layer before and after (Glow style plus above), but if we use the convolution as functional we may want to add different quantization for the different slots (input, weights, and bias).</p><NewLine><p>When I talk to HW people, they would like to break a convolution into a correlation and a bias addition. Thus, reorganize the convolution operation into two distinct operations. Quantization can be different for the weights, the correlation, the bias and their sums.</p><NewLine><p>Then Quantization affects the forward computation. It affects the backward and the range of the quantization can be used as parameter and the gradient computation could/should use it for training.</p><NewLine><p><strong>Quantization as pass</strong><br/><NewLine>In this forum, there is a nice tutorial how to introduce an optimization pass. This pass uses CustomFuseGraph. The Good:  it boils down to an import. The Bad: FuseGraph optimizations are based on same input same output operations (convolution does not belong here)[ PLEASE CORRECT ME]. This pass will change the forward computation, thus the pass should be done before any AUTOGRAD. With this example, we do not have much control when the optimization and it seems too late.</p><NewLine><p><strong>Trainable and Automatic quantization for TF/Caffe</strong><br/><NewLine>What automatic tools do in TF, Caffe, they modify the computation graph by pattern recognition and HW requirements, they train the network, then they remove those layers for inference. After that a dedicated compiler will take the computation graph and write code for a specific architecture.</p><NewLine><p><strong>Quantization as jit pass</strong><br/><NewLine>The way I see it, it will be nice to register a jit pass. This pass must be before  gradient computation. This pass will be basically an IR graph manipulation where a few targeted operation will be at first a sub graph but the inputs are completely qualified so that the “rewrite” of the graph can be local, complete and without side effects (nice functional).</p><NewLine><p><strong>Question for the masters</strong><br/><NewLine>Would you like to let me know how to get started in a practical way ?</p><NewLine><p>Please, hit me with questions, pointer, github links … whatever you consider important.</p><NewLine></div>",https://discuss.pytorch.org/u/paolo.dalberto,(Paolo D'Alberto),paolo.dalberto,"August 16, 2019, 10:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Check out: <a href=""https://github.com/pytorch/pytorch/issues?utf8=%E2%9C%93&amp;q=is%3Aissue+is%3Aopen+quantization"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues?utf8=✓&amp;q=is%3Aissue+is%3Aopen+quantization</a>. We are actively working on something very similar to your proposal, soon to be released!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Let me know what I can do to contribute (time,resource, coding).</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just trying to throw more context here, this is the earlier issue tracking the proposal: <a href=""https://github.com/pytorch/pytorch/issues/18318"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/18318</a> and the proposal itself: <a href=""https://github.com/pytorch/pytorch/wiki/torch_quantization_design_proposal"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/wiki/torch_quantization_design_proposal</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>From the proposals and implementations, I will try to learn any simple way to add a pass that  will target any layer and create a sub-graph (At graph layer) before gradient computation.</p><NewLine><p>Such an addition, will help third parties to describe the computation differently and closer to a dedicated engine (we are interested in FPGA kernels): the computation is similar to to CPU, possibly using the same basic operation and changing  the order.</p><NewLine><p>The introduction of fake quantization nodes is  one way we pursue to modify the subgraph. But our fake nodes will affect the forward and the backward.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Paolo! I’m currently working on graph mode quantization (along with other people, of course), so I’ll try to shed some light on what we have in our plans.</p><NewLine><p>As you noticed, quantization can be implemented as a jit pass. But we consider this as one of two modes of how quantization could be done.</p><NewLine><p>The first mode is called eager mode quantization, where the user is expected to structure their model into submodules and insert quantization stubs manually. This way they completely control the quantization process and can fine-tune the results (and also it can be applied to non-scriptable models). The drawback of this approach is that the user needs to edit their model.</p><NewLine><p>The second mode is called graph mode and it is based on jit. It’s more than one pass, but the idea is the same as you described: a user scripts/traces their model, passes it to some black-box quantizer and gets a quantized version of their model as a result.</p><NewLine><p>In graph mode we roughly expect the following passes:</p><NewLine><ol><NewLine><li>inserting instrumentation nodes for collecting runtime statistics (aka observers)</li><NewLine><li>replacing quantization/dequantization nodes into the model</li><NewLine><li>fusing dequantize-some_op-quantize patterns to quantize_op</li><NewLine></ol><NewLine><p>There are actually more more-specific pass than these three, however these should give you an idea of what functionality would be there. As we work on those passes, we’re trying to build them on top of generic features that could be useful for other purposes as well (for instance, several months back we implemented subgraph-matcher and subgraph-rewriter to facilitate fusion of arbitrary graph patterns). We probably will add some features to facilitate transformations on module level (rather than on a graph, or function, level), but the specific details are still TBD.</p><NewLine><p>I think you might help us trying out API as soon as it’s ready and letting us know if the the workflow makes sense for you and covers your use cases.</p><NewLine><p>For now please stay tuned, we’re actively working on it and expect to show something soon!</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>We = I + Xilinx (HW+SW developers)</p><NewLine><p>We are interested in the graph mode.</p><NewLine><p>The most common scenario we can imagine it is a model that has been designed and trained; however,  part of it will be executed on a fixed point precision device. This opens a lot of interesting questions.</p><NewLine><p>Please, consider to use us not only as Guinea Piggys but we can help you in giving you test cases, suggestions, and also “me” the worst coder in the west world I will need to figure this out for the purpose to then plug in a specialized partitioner and compiler.</p><NewLine><p>Please, consider to come down (if you are in the Bay Area) and visit Xilinx and see what we can do for you as well.</p><NewLine><p>thank you</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Paolo!</p><NewLine><p>Sorry for the delayed reply. Right now we’re on the final stretch before we should have graph mode working end-to-end (see e.g. this stack of PRs: <a href=""https://github.com/pytorch/pytorch/pull/24426"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/24426</a>), so I don’t see an easy way to offload some of the current work to you right now. This is a critical path, so it can’t be parallelized that much I think. However, once it lands I expect we would discover many places where it can be improved or bugs which need to be fixed - and at that point your help in both finding these spots and helping with fixes would be much appreciated!</p><NewLine><p>Until then I suggest you to get familiar with JIT IR in case you haven’t worked with it much before. A good overview of it can be found here: <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/docs/OVERVIEW.md"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/docs/OVERVIEW.md</a> . I could try to provide more pointers if you need them - let me know if that’s the case.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you Michael. This week I will try to interact with the IR.<br/><NewLine>Let me know if you guys are willing to come down and introduce you work when you are ready.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Let me address a few questions so you can help me and I can help you.</p><NewLine><p><strong>Q</strong> Can we (will be able to) register a JIT pass that may/will affect the “gradient computation” ?</p><NewLine><p>Registering a pass, as for code optimization, is elegant and you and I can work independently. The HW people can change their minds to suite different HW configurations; I can create versions; and our packages are imported afterwords.  The main concern is that you will need to give us partial control when to call the pass.  This means that I can break a perfectly fine JIT.</p><NewLine><p>In your proposal, you will give us the opportunity to activate “the pass”, which is great. Next question.</p><NewLine><p><strong>Q</strong> Can we target layers that affect “the gradient computation” ?</p><NewLine><p>I know quantization of convolutions is HW dependent. This means that I will need space to wiggle in choosing the convolution layer and  what do do with it. In practice, the HW will dictate the computation shape and format: in turn, the computation of the convolution. The computation is still based on basic operations such as aten::conv and addition of the bias, I know the HW people want them separated.</p><NewLine><p><strong>Q</strong> is it possible to have a tutorial I can build upon so I can practice on the master code for:</p><NewLine><ul><NewLine><li><NewLine><p>(Any) layer selection into a subgraph.</p><NewLine></li><NewLine><li><NewLine><p>Where to locate such a phase so that I do the least damage to the JIT optimization process</p><NewLine></li><NewLine><li><NewLine><p>The documentation is great but I learned more from a post like this: <a href=""https://zasdfgbnm.github.io/2018/09/20/PyTorch-JIT-Source-Code-Read-Note/'"" rel=""nofollow noopener"">https://zasdfgbnm.github.io/2018/09/20/PyTorch-JIT-Source-Code-Read-Note/</a></p><NewLine></li><NewLine></ul><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>I work at Xilinx as well. We released <a href=""https://github.com/Xilinx/graffitist"" rel=""nofollow noopener"">Graffitist</a> earlier this year, a TensorFlow package for quantization-aware training. I’d like to point out that there is a critical issue in the way FakeQuant’s gradients (wrt min/max vars) are implemented in TensorFlow. We shed more light on it in <a href=""https://arxiv.org/pdf/1903.08066.pdf"" rel=""nofollow noopener"">our paper</a>, but it has to do with the correct straight-through-estimator (STE) for training thresholds or scale-factors. It’d be nice to address this in PyTorch early on (I’m happy to be of help and can contribute as well). This is essential for quantizing traditionally difficult networks such as mobilenets, with almost no loss in accuracy (refer <a href=""https://arxiv.org/pdf/1903.08066.pdf"" rel=""nofollow noopener"">Table 4 in paper</a>).</p><NewLine><p>I agree with Paolo, in that having control over the JIT IR pass is essential for target-specific transforms on the graph. I did go over the documentation (JIT README), but some things are not clear to me yet. For instance, how to (i) pattern-match and manipulate sub-graphs, (ii) insert custom ops, (iii) invoke custom passes on the IR.</p><NewLine><p>Please let us know<br/><NewLine>(i) if you can visit Xilinx (San Jose) for a JIT deep-dive, and<br/><NewLine>(ii) how we can be of help/contribute.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please, let me know what we can do to help?<br/><NewLine>Would you like to come down to Xilinx and give a talk ?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/paolo.dalberto; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jjsjann123; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/paolo.dalberto; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Michael_Zolotukhin; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/paolo.dalberto; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Michael_Zolotukhin; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/paolo.dalberto; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/paolo.dalberto; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/sambhav_jain; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/paolo.dalberto; <NewLine> ,"REPLY_DATE 1: August 17, 2019,  3:02am; <NewLine> REPLY_DATE 2: August 19, 2019,  5:24pm; <NewLine> REPLY_DATE 3: August 19, 2019, 10:34pm; <NewLine> REPLY_DATE 4: August 20, 2019, 12:07am; <NewLine> REPLY_DATE 5: August 20, 2019, 10:55pm; <NewLine> REPLY_DATE 6: August 21, 2019,  5:46pm; <NewLine> REPLY_DATE 7: August 23, 2019, 10:47pm; <NewLine> REPLY_DATE 8: August 26, 2019,  5:25pm; <NewLine> REPLY_DATE 9: August 26, 2019,  7:10pm; <NewLine> REPLY_DATE 10: September 4, 2019,  2:11am; <NewLine> REPLY_DATE 11: September 3, 2019,  8:15pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> 
60679,TorchScript register backward C++ functions,2019-11-12T01:25:16.856Z,0,284,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I just read this tutorial and it does not touch the backward part.<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html</a></p><NewLine><p>My understanding is that to get autograd work, we will also need to register the corresponding backward function. However, I could not find how this works. I checked PyTorch source code (torch/csrc/autograd/generated/VariableTypeEverything.cpp) and could not find where backward functions are registered either.</p><NewLine><p>Help will be appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/ralphmao,,ralphmao,"November 12, 2019,  1:25am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You implement it in C++ similarly to in Python via <code>autograd.Function</code>. You then have to register an op that uses your autograd function. See this example</p><NewLine><pre><code class=""lang-cpp"">#include &lt;torch/script.h&gt;<NewLine>#include &lt;torch/all.h&gt;<NewLine><NewLine>#include &lt;iostream&gt;<NewLine>#include &lt;memory&gt;<NewLine><NewLine>using namespace at;<NewLine>using torch::Tensor;<NewLine>using torch::autograd::AutogradContext;<NewLine>using torch::autograd::Variable;<NewLine>using torch::autograd::variable_list;<NewLine><NewLine>// computes f(x) = 2x<NewLine>class MyDouble : public torch::autograd::Function&lt;MyDouble&gt; {<NewLine> public:<NewLine>  static variable_list forward(<NewLine>      AutogradContext* ctx,<NewLine>      Variable input) {<NewLine>    return {input + input};<NewLine>  }<NewLine><NewLine>  static variable_list backward(<NewLine>      AutogradContext* ctx,<NewLine>      variable_list grad_output) {<NewLine>    return {torch::ones({2, 2}) + 1};<NewLine>  }<NewLine>};<NewLine><NewLine>Tensor double_op(const Tensor&amp; input) {<NewLine>  return MyDouble::apply(input)[0];<NewLine>}<NewLine><NewLine>static auto registry =<NewLine>  torch::RegisterOperators(""my_ops::double_op"", &amp;double_op);<NewLine></code></pre><NewLine><p>and in Python</p><NewLine><pre><code class=""lang-python"">import torch<NewLine><NewLine>torch.ops.load_library(""build/libmy_custom_op.so"")<NewLine><NewLine>@torch.jit.script<NewLine>def fn(x):<NewLine>    return torch.ops.my_ops.double_op(x)<NewLine><NewLine>x = torch.randn(2, 2, requires_grad=True)<NewLine>print(fn(x))<NewLine>x.backward(torch.ones(2, 2))<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: November 15, 2019, 12:19am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> 
60957,Jit.scripting nn.Module with function attributes,2019-11-14T08:47:08.708Z,1,291,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Suppose I want to instantiate a Module with a function:</p><NewLine><pre><code class=""lang-auto"">class Foo(nn.Module):<NewLine>    def __init__(self, fn):<NewLine>        super(Foo, self).__init__()<NewLine>        self.fn = fn<NewLine>        <NewLine>    def forward(self, x):<NewLine>        return self.fn(x)<NewLine></code></pre><NewLine><p>This works fine</p><NewLine><pre><code class=""lang-auto"">def square(x):<NewLine>    return x**2<NewLine><NewLine>foonet = Foo(square)<NewLine></code></pre><NewLine><p>But attempting to script the module fails</p><NewLine><pre><code class=""lang-auto"">foonet_script = torch.jit.script(foonet)<NewLine></code></pre><NewLine><p>giving</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>module has no attribute 'fn':<NewLine>at &lt;ipython-input-25-24af853ed724&gt;:7:15<NewLine>    def forward(self, x):<NewLine>        return self.fn(x)<NewLine>               ~~~~~~~ &lt;--- HERE<NewLine></code></pre><NewLine><p>It doesn’t help if I first script the function</p><NewLine><pre><code class=""lang-auto"">square_script = torch.jit.script(square)<NewLine></code></pre><NewLine><p>and then instantiate with the resulting <code>torch._C.Function</code>. On the other hand, if I pass in another <code>nn.Module</code>, everything is fine. Setting up a whole new Module when all I want is the function call seems overkill though.</p><NewLine></div>",https://discuss.pytorch.org/u/AustenLamacraft,(Austen Lamacraft),AustenLamacraft,"November 14, 2019,  8:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><strong>Update:</strong> It’s a known bug see <a href=""https://github.com/pytorch/pytorch/issues/27495"" rel=""nofollow noopener"">this issue</a> for function attributes and <a href=""https://discuss.pytorch.org/t/jit-scripted-attributes-inside-module/60645"">this post</a> for class attributes.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This bug should be fixed in <a href=""https://github.com/pytorch/pytorch/pull/28569"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/28569</a>, you should be able to try out the fix if you build master from source or use the nightly PyTorch pip package</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/AustenLamacraft; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: November 14, 2019,  2:33pm; <NewLine> REPLY_DATE 2: November 14, 2019,  9:53pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
60845,[JIT] [Mobile] Is isinstance() supposed to work with TorchScript,2019-11-13T10:49:36.136Z,5,531,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello there,</p><NewLine><p>I’ve faced similar issues during deploying scripted model on Android.</p><NewLine><ol><NewLine><li>The first question is “How to check whole grap or code of scripted model?”.<br/><NewLine><code>ScriptedModule.code</code> provides only upper level code, but inside stacktrace on mobile I see more information about how the code looks like.</li><NewLine><li>Second is about isinstance(). My model has plenty of <code>isinstance(tensor, torch.Tensor)</code> or such which are converted to in <code>tensor1 = unchecked_cast(Tensor, tensor)</code> script code. Here comes an error.</li><NewLine><li>Why there’s no error while exporting/scripting of module?</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"November 13, 2019, 10:49am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>isinstance</code> is supported but its result is static. It is useful for <code>Module</code>s that have different attribute types passed in, e.g.</p><NewLine><pre><code class=""lang-python"">class M(torch.nn.Module):<NewLine>    def __init__(self, x):<NewLine>        super().__init__()<NewLine>        self.x = x<NewLine><NewLine>    def forward(self):<NewLine>        if isinstance(self.x, List[str]):<NewLine>            return self.x[2]<NewLine>        else:<NewLine>            return self.x + 2<NewLine><NewLine>print(torch.jit.script(M(['bye'])).graph)<NewLine>print(torch.jit.script(M(2)).graph)<NewLine></code></pre><NewLine><p>The compiler is able to see the <code>isinstance</code> check and evaluate it at compile time and remove the unused branch. The graphs show this:</p><NewLine><pre><code class=""lang-auto"">graph(%self : ClassType&lt;M&gt;):<NewLine>  %3 : str = prim::Constant[value=""hi""]() # ../test.py:22:19<NewLine>  return (%3)<NewLine><NewLine>graph(%self : ClassType&lt;M&gt;):<NewLine>  %4 : int = prim::Constant[value=2]() # ../test.py:24:28<NewLine>  %3 : int = prim::GetAttr[name=""x""](%self)<NewLine>  %5 : int = aten::add(%3, %4) # ../test.py:24:19<NewLine>  return (%5)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>As for 1), we recently changed the behavior so that functions in <code>.code</code> and <code>.graph</code> appear as function calls (previously we were inlining the function bodies). So we’re still missing the functionality to show the called functions. For now you can re-enable inlining to see the entire graph:</p><NewLine><pre><code class=""lang-python"">def other_fn(x):<NewLine>    return x + 10<NewLine><NewLine># Change the inlining mode before you compile<NewLine>torch._C._jit_set_inline_everything_mode(True)<NewLine><NewLine>@torch.jit.script<NewLine>def fn(x):<NewLine>    return other_fn(x)<NewLine><NewLine>print(fn.code)<NewLine>print(fn.graph)<NewLine></code></pre><NewLine><p>You can track this bug in <a href=""https://github.com/pytorch/pytorch/issues/29750"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/29750</a>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""60845""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/driazati/40/15393_2.png"" width=""20""/> driazati:</div><NewLine><blockquote><NewLine><p>def other_fn(x): return x + 10 # Change the inlining mode before you compile torch._C._jit_set_inline_everything_mode(True) <span class=""mention"">@torch.jit.script</span> def fn(x): return other_fn(x) print(fn.code) print(fn.graph)</p><NewLine></blockquote><NewLine></aside><NewLine><p>You can also print out a model directly with</p><NewLine><pre><code class=""lang-python"">class M(nn.Module):<NewLine>    def other_fn(self, x):<NewLine>        return x + 10<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.other_fn(x)<NewLine><NewLine>m = torch.jit.script(M())<NewLine>print(m._c.dump())<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>This one is unclear to me.</p><NewLine><p>How could we annotate input for <code>__init__()</code> and output of <code>forward()</code> functions? The <code>Union[int, List[str]]</code> typing is unsupported</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""60845""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/driazati/40/15393_2.png"" width=""20""/> driazati:</div><NewLine><blockquote><NewLine><p>torch._C._jit_set_inline_everything_mode(True)</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thanks a lot! That clarified the way how to see the whole code</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>This returns <code>None</code> but anyway I haven’t looked for that functionality <img alt="":stuck_out_tongue:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/stuck_out_tongue.png?v=9"" title="":stuck_out_tongue:""/></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""5"" data-topic=""60845""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/z/3da27b/40.png"" width=""20""/> zetyquickly:</div><NewLine><blockquote><NewLine><p>This one is unclear to me.</p><NewLine><p>How could we annotate input for <code>__init__()</code> and output of <code>forward()</code> functions? The <code>Union[int, List[str]]</code> typing is unsupported</p><NewLine></blockquote><NewLine></aside><NewLine><p><code>__init__</code> on <code>nn.Module</code>s runs in Python (<code>torch.jit.script</code> only sees the module after it has been initialized), so you can annotate that with whatever Python annotations you want (but they won’t be enforced by the compiler at all, for that you should use something like mypy).</p><NewLine><p>For methods that are compiled (e.g. <code>forward</code> and anything it calls), the return types can be deduced from the code. If you want to explicitly write it out, you can use any of the type annotations listed <a href=""https://pytorch.org/docs/master/jit.html#supported-type"" rel=""nofollow noopener"">here</a>.</p><NewLine><p><code>Union</code>s aren’t supported so they won’t work in TorchScript. As a workaround you could do something like <code>Tuple[Optional[int], Optional[List[str]]]</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: November 13, 2019,  9:23pm; <NewLine> REPLY_DATE 2: November 14, 2019,  8:40am; <NewLine> REPLY_DATE 3: November 13, 2019,  9:43pm; <NewLine> REPLY_DATE 4: November 14, 2019,  8:20am; <NewLine> REPLY_DATE 5: November 14, 2019,  8:42am; <NewLine> REPLY_DATE 6: November 14, 2019,  8:45am; <NewLine> REPLY_DATE 7: November 14, 2019,  6:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
60645,[JIT] Scripted attributes inside module,2019-11-11T14:53:32.545Z,1,587,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello there,</p><NewLine><p>I’ve faced some unexpected behaviour of <code>torch.jit.script()</code>. The issue may be reproduced with the following code.<br/><NewLine>As I learned from docs ( <a href=""https://pytorch.org/docs/stable/jit.html#id3"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/jit.html#id3</a> ) one can use custom TorchScript classe if it’s properly written. But…</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from typing import Tuple<NewLine><NewLine>@torch.jit.script<NewLine>class MyClass(object):<NewLine>    def __init__(self, weights = (1.0, 1.0, 1.0, 1.0,)):<NewLine>        # type: (Tuple[float, float, float, float])<NewLine>        self.weights = weights<NewLine>    def apply(self):<NewLine>        # type: () -&gt; Tuple[float, float, float, float]<NewLine>        return self.weights<NewLine><NewLine>class MyModule(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.class_field = torch.jit.export(MyClass(weights = (1.0, 1.0, 1.0, 1.0,)))<NewLine>                                        <NewLine>    def forward(self, x):<NewLine>        self.class_field.apply()<NewLine>        return x + 10<NewLine><NewLine>m = torch.jit.script(MyModule())<NewLine></code></pre><NewLine><p>Produces such error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>Module 'MyModule' has no attribute 'class_field' (This attribute exists on the Python module, but we failed to convert Python type: 'MyClass' to a TorchScript type.):<NewLine>at script_test.py:20:8<NewLine>    def forward(self, x):<NewLine>        self.class_field.apply()<NewLine>        ~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        return x + 10<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"November 11, 2019,  2:53pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems that I’ve solved the problem. Found out that attributes introduced in <code>__init__(self,...)</code> should be explicitly annotated the following way:</p><NewLine><pre><code class=""lang-auto"">class MyModule(torch.nn.Module):<NewLine>    class_field : MyClass<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.class_field = torch.jit.export(MyClass(weights = (1.0, 1.0, 1.0, 1.0,)))                      <NewLine></code></pre><NewLine><p>Ofk, if they are custom scripted classes.</p><NewLine><p>P.S. The proper way to initialize custom class field is still unclear to me, I mean:</p><NewLine><pre><code class=""lang-auto"">self.class_field = torch.jit.export(MyClass(weights = (1.0, 1.0, 1.0, 1.0,)))<NewLine></code></pre><NewLine><p>or just</p><NewLine><pre><code class=""lang-auto"">self.class_field = MyClass(weights = (1.0, 1.0, 1.0, 1.0,))<NewLine></code></pre><NewLine><p>Behaviour is the same</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is a bug, I’ve filed <a href=""https://github.com/pytorch/pytorch/issues/29597"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/29597</a> since this should be something that we can do automatically.</p><NewLine><p><code>export</code> is meant to be used as a decorator on functions that need to be compiled but are not called from <code>forward</code> or anything <code>forward</code> calls. So for your code you can just get rid of the call to <code>torch.jit.export</code>. See <a href=""https://pytorch.org/docs/stable/jit.html#torch.jit.export"" rel=""nofollow noopener"">these docs</a> for details.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: November 11, 2019,  3:43pm; <NewLine> REPLY_DATE 2: November 11, 2019, 10:57pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
60289,Unable to import &lsquo;weak_module&rsquo;,2019-11-07T15:18:40.270Z,6,557,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Everyone,<br/><NewLine>I’m unable to import ‘weak_module’ from ‘torch._jit_internal’. It says 'ImportError: cannot import name ‘weak_module’. I’m using Pytorch 1.3.0a0+24ae9b5 version. Please help me.</p><NewLine></div>",https://discuss.pytorch.org/u/Soniya,(Soniya),Soniya,"November 7, 2019,  3:23pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is some internal module, it might have been removed/renamed. Why do you want to use it?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to run EfficientNet which has used the module in the following manner.<br/><NewLine>‘from torch._jit_internal import weak_module, weak_script_method’</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""60289""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/5fc32e/40.png"" width=""20""/> Soniya:</div><NewLine><blockquote><NewLine><p>EfficientNet</p><NewLine></blockquote><NewLine></aside><NewLine><p>Which EfficientNet implementation are you using? The decorators its pulling from <code>_jit_internal</code> were deleted in v1.2. Their functionality is now automatic, so they can just be deleted and everything should work the same.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m using the implementation which is given here <a href=""https://github.com/katsura-jp/efficientnet-pytorch"" rel=""nofollow noopener"">https://github.com/katsura-jp/efficientnet-pytorch</a></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>It should work if you apply this patch</p><NewLine><pre><code class=""lang-diff"">diff --git a/model/swish.py b/model/swish.py<NewLine>index 66adfa5..3f68678 100644<NewLine>--- a/model/swish.py<NewLine>+++ b/model/swish.py<NewLine>@@ -1,10 +1,8 @@<NewLine> import torch<NewLine> import torch.nn as nn<NewLine> from torch.nn.parameter import Parameter<NewLine>-from torch._jit_internal import weak_module, weak_script_method<NewLine> <NewLine> <NewLine>-@weak_module<NewLine> class Swish(nn.Module):<NewLine>     def __init__(self, train_beta=False):<NewLine>         super(Swish, self).__init__()<NewLine>@@ -13,7 +11,6 @@ class Swish(nn.Module):<NewLine>         else:<NewLine>             self.weight = 1.0<NewLine> <NewLine>-    @weak_script_method<NewLine>     def forward(self, input):<NewLine>         return input * torch.sigmoid(self.weight * input)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Where to apply this patch? I’m unable to find the corresponding file from where i have used it as ‘from torch._jit_internal’.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>He refers to <a href=""https://github.com/katsura-jp/efficientnet-pytorch/blob/master/model/swish.py"" rel=""nofollow noopener"">this file</a> in the repo you linked above.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Soniya; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Soniya; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Soniya; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: November 7, 2019,  3:23pm; <NewLine> REPLY_DATE 2: November 7, 2019,  3:28pm; <NewLine> REPLY_DATE 3: November 7, 2019,  5:06pm; <NewLine> REPLY_DATE 4: November 7, 2019,  5:26pm; <NewLine> REPLY_DATE 5: November 7, 2019,  5:32pm; <NewLine> REPLY_DATE 6: November 8, 2019, 12:22pm; <NewLine> REPLY_DATE 7: November 8, 2019,  3:39pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
60312,[JIT] NamedTuple&rsquo;s _fields attribute,2019-11-07T20:28:09.153Z,0,132,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Don’t know is it a bug or just partial coverage but the _fields attribute of NamedTuple unavailable while scripting.</p><NewLine><pre><code class=""lang-auto"">class _Contents(NamedTuple):<NewLine>    a : Optional[torch.Tensor]<NewLine>    b : Optional[torch.Tensor]<NewLine>    c : Optional[torch.Tensor]<NewLine>    d : Optional[torch.Tensor]<NewLine><NewLine>@torch.jit.script<NewLine>class Container:<NewLine>   def __init__(self, fields):<NewLine>      # type: (Tuple[int, int], _Contents)<NewLine>      self.fields = fields<NewLine>  def has(self, name: str) -&gt; bool:<NewLine>        """"""<NewLine>        Returns:<NewLine>            bool: whether the field called `name` exists.<NewLine>        """"""<NewLine>        return name in self.fields._fields<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>Unknown attribute to named tuple:<NewLine>at container.py:91:23<NewLine>    def has(self, name: str) -&gt; bool:<NewLine>        """"""<NewLine>        Returns:<NewLine>            bool: whether the field called `name` exists.<NewLine>        """"""<NewLine>        return name in self.fields._fields<NewLine>                       ~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"November 7, 2019,  9:41pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We have not bound <code>_fields</code> into NamedTuples. If you file a feature request on GitHub we can take a look at implementing it <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: November 7, 2019,  9:49pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
50707,JIT: Tried to access to nonexistent attribute,2019-07-16T07:39:29.902Z,1,1292,"<div class=""post"" itemprop=""articleBody""><NewLine><p>PyTorch version: 1.1</p><NewLine><p>I’m optimizing Decoder part of model.py at <a href=""https://github.com/nvidia/tacotron2"" rel=""nofollow noopener"">https://github.com/nvidia/tacotron2</a> using JIT.</p><NewLine><pre><code class=""lang-auto"">@torch.jit.script<NewLine>class DecoderOptions:<NewLine>    @torch.jit.script_method<NewLine>    def __init__(self, decoder, memory, mask):<NewLine>        B = memory.size(0)<NewLine>        MAX_TIME = memory.size(1)<NewLine><NewLine>        self.attention_hidden = Variable(memory.data.new(<NewLine>            B, decoder.attention_rnn_dim).zero_())<NewLine>        self.attention_cell = Variable(memory.data.new(<NewLine>            B, decoder.attention_rnn_dim).zero_())<NewLine><NewLine>        self.decoder_hidden = Variable(memory.data.new(<NewLine>            B, decoder.decoder_rnn_dim).zero_())<NewLine>        self.decoder_cell = Variable(memory.data.new(<NewLine>            B, decoder.decoder_rnn_dim).zero_())<NewLine><NewLine>        self.attention_weights = Variable(memory.data.new(<NewLine>            B, MAX_TIME).zero_())<NewLine>        self.attention_weights_cum = Variable(memory.data.new(<NewLine>            B, MAX_TIME).zero_())<NewLine>        self.attention_context = Variable(memory.data.new(<NewLine>            B, decoder.encoder_embedding_dim).zero_())<NewLine><NewLine>        self.memory = memory<NewLine>        self.processed_memory = decoder.attention_layer.memory_layer(memory)<NewLine>        self.mask = mask<NewLine><NewLine>class Decoder(torch.jit.ScriptModule):<NewLine>    def __init__(self, hparams):<NewLine>        # same to nvidia version<NewLine><NewLine>    def get_go_frame(self, memory):<NewLine>        # same to nvidia version<NewLine><NewLine>    def parse_decoder_inputs(self, decoder_inputs):<NewLine>        # same to nvidia version<NewLine><NewLine>    def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):<NewLine>        # same to nvidia version<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def decode(self, decoder_input, options: DecoderOptions):<NewLine>        # type: (Tensor, DecoderOptions) -&gt; (Tuple[Tensor, Tensor, Tensor, DecoderOptions])<NewLine>        """""" Decoder step using stored states, attention and memory<NewLine>        PARAMS<NewLine>        ------<NewLine>        decoder_input: previous mel output<NewLine><NewLine>        RETURNS<NewLine>        -------<NewLine>        mel_output:<NewLine>        gate_output: gate output energies<NewLine>        attention_weights:<NewLine>        """"""<NewLine>        cell_input = torch.cat((decoder_input, options.attention_context), -1)<NewLine>        options.attention_hidden, options.attention_cell = self.attention_rnn(<NewLine>            cell_input, (options.attention_hidden, options.attention_cell))<NewLine>        options.attention_hidden = F.dropout(<NewLine>            options.attention_hidden, self.p_attention_dropout, self.training)<NewLine><NewLine>        attention_weights_cat = torch.cat(<NewLine>            (options.attention_weights.unsqueeze(1),<NewLine>             options.attention_weights_cum.unsqueeze(1)), dim=1)<NewLine>        options.attention_context, options.attention_weights = self.attention_layer(<NewLine>            options.attention_hidden, options.memory, options.processed_memory,<NewLine>            attention_weights_cat, options.mask)<NewLine><NewLine>        options.attention_weights_cum += options.attention_weights<NewLine>        decoder_input = torch.cat(<NewLine>            (options.attention_hidden, options.attention_context), -1)<NewLine>        options.decoder_hidden, options.decoder_cell = self.decoder_rnn(<NewLine>            decoder_input, (options.decoder_hidden, options.decoder_cell))<NewLine>        options.decoder_hidden = F.dropout(<NewLine>            options.decoder_hidden, self.p_decoder_dropout, self.training)<NewLine><NewLine>        decoder_hidden_attention_context = torch.cat(<NewLine>            (options.decoder_hidden, options.attention_context), dim=1)<NewLine>        decoder_output = self.linear_projection(<NewLine>            decoder_hidden_attention_context)<NewLine><NewLine>        gate_prediction = self.gate_layer(decoder_hidden_attention_context)<NewLine>        return decoder_output, gate_prediction, options.attention_weights, options<NewLine><NewLine>    def forward(self, memory, decoder_inputs, memory_lengths):<NewLine>        """""" Decoder forward pass for training<NewLine>        PARAMS<NewLine>        ------<NewLine>        memory: Encoder outputs<NewLine>        decoder_inputs: Decoder inputs for teacher forcing. i.e. mel-specs<NewLine>        memory_lengths: Encoder output lengths for attention masking.<NewLine><NewLine>        RETURNS<NewLine>        -------<NewLine>        mel_outputs: mel outputs from the decoder<NewLine>        gate_outputs: gate outputs from the decoder<NewLine>        alignments: sequence of attention weights from the decoder<NewLine>        """"""<NewLine><NewLine>        decoder_input = self.get_go_frame(memory).unsqueeze(0)<NewLine>        decoder_inputs = self.parse_decoder_inputs(decoder_inputs)<NewLine>        decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)<NewLine>        decoder_inputs = self.prenet(decoder_inputs)<NewLine><NewLine>        options = DecoderOptions(self, memory, ~get_mask_from_lengths(memory_lengths))<NewLine><NewLine>        mel_outputs, gate_outputs, alignments = [], [], []<NewLine>        while len(mel_outputs) &lt; decoder_inputs.size(0) - 1:<NewLine>            decoder_input = decoder_inputs[len(mel_outputs)]<NewLine>            mel_output, gate_output, attention_weights, options = self.decode(<NewLine>                decoder_input, options)<NewLine>            mel_outputs += [mel_output.squeeze(1)]<NewLine>            gate_outputs += [gate_output.squeeze()]<NewLine>            alignments += [attention_weights]<NewLine><NewLine>        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(<NewLine>            mel_outputs, gate_outputs, alignments)<NewLine><NewLine>        return mel_outputs, gate_outputs, alignments<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def inference(self, memory, options: DecoderOptions):<NewLine>        """""" Decoder inference<NewLine>        PARAMS<NewLine>        ------<NewLine>        memory: Encoder outputs<NewLine><NewLine>        RETURNS<NewLine>        -------<NewLine>        mel_outputs: mel outputs from the decoder<NewLine>        gate_outputs: gate outputs from the decoder<NewLine>        alignments: sequence of attention weights from the decoder<NewLine>        """"""<NewLine>        decoder_input = self.get_go_frame(memory)<NewLine><NewLine>        mel_outputs, gate_outputs, alignments = [], [], []<NewLine>        run_more = True<NewLine>        while run_more:<NewLine>            decoder_input = self.prenet(decoder_input)<NewLine>            mel_output, gate_output, alignment, options = self.decode(decoder_input, options)<NewLine><NewLine>            mel_outputs += [mel_output.squeeze(1)]<NewLine>            gate_outputs += [gate_output]<NewLine>            alignments += [alignment]<NewLine><NewLine>            if torch.sigmoid(gate_output.data) &gt; self.gate_threshold:<NewLine>                run_more = False<NewLine>            elif len(mel_outputs) == self.max_decoder_steps:<NewLine>                print(""Warning! Reached max decoder steps"")<NewLine>                run_more = False<NewLine><NewLine>            decoder_input = mel_output<NewLine><NewLine>        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(<NewLine>            mel_outputs, gate_outputs, alignments)<NewLine><NewLine>        return mel_outputs, gate_outputs, alignments<NewLine></code></pre><NewLine><p>and I call the decoder in Tacotron2.inference with this line:</p><NewLine><pre><code class=""lang-auto"">mel_outputs, gate_outputs, alignments = self.decoder.inference(<NewLine>            encoder_outputs, DecoderOptions(self.decoder, encoder_outputs))<NewLine></code></pre><NewLine><p>But the compiler says:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>Tried to access to nonexistent attribute attention_context. Did you forget to initialize it in __init__()?:<NewLine>    ------<NewLine>    decoder_input: previous mel output<NewLine><NewLine>    RETURNS<NewLine>    -------<NewLine>    mel_output:<NewLine>    gate_output: gate output energies<NewLine>    attention_weights:<NewLine>    """"""<NewLine>    cell_input = torch.cat((decoder_input, options.attention_context), -1)<NewLine>                                           ~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>    options.attention_hidden, options.attention_cell = self.attention_rnn(<NewLine>        cell_input, (options.attention_hidden, options.attention_cell))<NewLine>    options.attention_hidden = F.dropout(<NewLine>        options.attention_hidden, self.p_attention_dropout, self.training)<NewLine><NewLine>    attention_weights_cat = torch.cat(<NewLine>        (options.attention_weights.unsqueeze(1),<NewLine>         options.attention_weights_cum.unsqueeze(1)), dim=1)<NewLine>    options.attention_context, options.attention_weights = self.attention_layer(<NewLine></code></pre><NewLine><p>I exactly initialized options.attention_context at DecoderOptions.<strong>init</strong>(). Why this error occurs?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/minty99,(Muhwan Kim),minty99,"July 16, 2019,  7:39am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the report! Do you mind posting the exact script you’re using to generate that error so that I can run it to see what’s happening?</p><NewLine><p>In the meantime, you could try removing <code>@torch.jit.script_method</code> from the <code>__init__</code> method of <code>DecoderOptions</code>—for class types, the single annotation at the top will result in all methods getting compiled so it may have unexpected effects.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/michael_suo"">@Michael_Suo</a>  Problem is still present:</p><NewLine><pre><code class=""lang-auto"">class SomeClass(object):<NewLine>    def __init__(self, x):<NewLine>        # type: (int)<NewLine>        self.x = x<NewLine>    def some_method(self, a, b):<NewLine>        # type: (Tensor, Tensor)<NewLine>        assert torch.isfinite(a).all().item(), ""Some of elements are infinite or NaN!""<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>Tried to access nonexistent attribute or method 'all' of type 'bool'.:```</code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>One could overcome this issue by creating extra variable<br/><NewLine><code>bool_tensor : torch.Tensor = (deltas == deltas) &amp; ~(torch.eq(deltas.abs(), torch._six.inf))</code><br/><NewLine>This is how <code>torch.isfinite()</code> was calculated in previous version <a href=""https://s0pytorch0org.icopy.site/docs/0.4.1/_modules/torch/functional.html"" rel=""nofollow noopener"">https://s0pytorch0org.icopy.site/docs/0.4.1/_modules/torch/functional.html</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>This looks like a bug with <code>isfinite</code>, see <a href=""https://github.com/pytorch/pytorch/issues/29340"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/29340</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: July 16, 2019,  4:42pm; <NewLine> REPLY_DATE 2: November 6, 2019, 11:53pm; <NewLine> REPLY_DATE 3: November 7, 2019, 12:32am; <NewLine> REPLY_DATE 4: November 7, 2019, 12:49am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
60062,"When I training the car evaluation, the training loss, validation loss and accuracy doesn’t change ,the above is my code, i am a beginner, please give me advice, thank you very much",2019-11-05T12:30:05.603Z,0,178,"<div class=""post"" itemprop=""articleBody""><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/cc17f376b69f0038efcd8fafaa5b6c153d65f18b"" href=""https://discuss.pytorch.org/uploads/default/original/3X/c/c/cc17f376b69f0038efcd8fafaa5b6c153d65f18b.png"" title=""image.png""><img alt=""image"" data-base62-sha1=""t7uIz8ePz0eS2H3Q5tvKzXZs8JR"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/c/cc17f376b69f0038efcd8fafaa5b6c153d65f18b_2_10x10.png"" height=""211"" src=""https://discuss.pytorch.org/uploads/default/original/3X/c/c/cc17f376b69f0038efcd8fafaa5b6c153d65f18b.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">944×290 6.68 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>When I training the car evaluation, the training loss, validation loss and accuracy doesn’t change ,the above is my code,  i am a beginner, please give me advice, thank you very much.</p><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""1"" data-topic=""60062""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/111179/40/17324_2.png"" width=""20""/> 111179:</div><NewLine><blockquote><NewLine><pre><code> import numpy as np<NewLine> from collections import Counter<NewLine> from sklearn import datasets<NewLine> import torch.nn.functional as F<NewLine> from torch.autograd import Variable<NewLine> import matplotlib.pyplot as plt<NewLine> import torch<NewLine> import  torch.utils.data as Data<NewLine> import pandas as pd<NewLine> import torch.nn as nn<NewLine> from sklearn.model_selection import cross_val_predict<NewLine> from sklearn.model_selection import train_test_split<NewLine> import torch.utils.data as utils<NewLine> import torch.utils.data as td<NewLine> dataframe= pd.read_csv('iris dataset classification/car.csv')<NewLine> dataframe.columns = ['buying','maint','doors','persons','lug_boot','safety','classes']<NewLine> dataframe.buying.replace(('vhigh','high','med','low'),(1,2,3,4), inplace=True)<NewLine> dataframe.maint.replace(('vhigh','high','med','low'),(1,2,3,4), inplace=True)<NewLine> dataframe.doors.replace(('2','3','4','5more'),(1,2,3,4), inplace=True)<NewLine> dataframe.persons.replace(('2','4','more'),(1,2,3), inplace=True)<NewLine>dataframe.lug_boot.replace(('small','med','big'),(1,2,3), inplace=True)<NewLine>dataframe.safety.replace(('low','med','high'),(1,2,3), inplace=True)<NewLine>dataframe.classes.replace(('unacc','acc','good','vgood'),(0,1,2,3), inplace=True)<NewLine>array = dataframe.values<NewLine>x = array[:,:6]<NewLine>y = array[:,6]<NewLine>x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)<NewLine> train_x = torch.Tensor(x_train).float()<NewLine> train_y = torch.Tensor(y_train).long()<NewLine> train_ds = utils.TensorDataset(train_x,train_y)<NewLine> train_loader = td.DataLoader(train_ds, batch_size=10,shuffle=True, num_workers=1)<NewLine> test_x = torch.Tensor(x_test).float()<NewLine> test_y = torch.Tensor(y_test).long()<NewLine>test_ds = utils.TensorDataset(test_x,test_y)<NewLine>test_loader = td.DataLoader(test_ds, batch_size=10, shuffle=True, num_workers=1)<NewLine>class IrisNet(nn.Module):<NewLine>        def __init__(self):<NewLine>        super(IrisNet, self).__init__()<NewLine>       self.fc1 = nn.Linear(6, 50)<NewLine>        self.fc3 = nn.Linear(50, 4)<NewLine><NewLine>    def forward(self, x):<NewLine>         x = F.relu(self.fc1(x))<NewLine>         x = torch.softmax(self.fc3(x),dim=1)<NewLine>         return x<NewLine><NewLine>    model = IrisNet()<NewLine>  def train(model, data_loader, optimizer):<NewLine>  model.train()<NewLine> train_loss = 0<NewLine><NewLine>for batch, tensor in enumerate(data_loader):<NewLine>    data, target = tensor<NewLine>    optimizer.zero_grad()<NewLine>    out = model(data)<NewLine>    loss = loss_criteria(out, target)<NewLine>    train_loss += loss.item()<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine>avg_loss = train_loss / len(data_loader.dataset)<NewLine>return avg_loss<NewLine>def test(model, data_loader):<NewLine>    model.eval()<NewLine>    test_loss = 0<NewLine>    correct = 0<NewLine><NewLine>with torch.no_grad():<NewLine>    for batch, tensor in enumerate(data_loader):<NewLine>        data, target = tensor<NewLine>        out = model(data)<NewLine>        test_loss += loss_criteria(out, target).item()<NewLine>        _, predicted = torch.max(out.data, 1)<NewLine>        correct += torch.sum(target==predicted).item()<NewLine>        <NewLine>avg_accuracy = correct / len(data_loader.dataset)<NewLine>avg_loss = test_loss / len(data_loader.dataset)<NewLine>return avg_loss, avg_accuracy<NewLine><NewLine>loss_criteria = nn.CrossEntropyLoss()<NewLine>learning_rate = 0.01<NewLine>optimizer= torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99))<NewLine>epoch_nums = []<NewLine>training_loss = []<NewLine>validation_loss = []<NewLine><NewLine>epochs = 600<NewLine>for epoch in range(1, epochs + 1):<NewLine>      train_loss = train(model, train_loader, optimizer)<NewLine><NewLine>      test_loss, accuracy = test(model, test_loader)<NewLine>     epoch_nums.append(epoch)<NewLine>     training_loss.append(train_loss)<NewLine>     validation_loss.append(test_loss)   <NewLine>if (epoch) % 10 == 0:<NewLine>    print('Epoch {:d}: Training loss= {:.4f}, Validation loss= {:.4f}, Accuracy={:.4%}'.format(epoch, train_loss, test_loss, accuracy))<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine></div>",https://discuss.pytorch.org/u/111179,(mmmm),111179,"November 6, 2019,  1:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Based on your screenshot, it seems the training and validation losses do change.<br/><NewLine>However, <code>nn.CrossEntropyLoss</code> expects raw logits, so you should remove the last <code>softmax</code> layer from your model.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes，I have solve it!! Thank you very much!!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/111179; <NewLine> ,"REPLY_DATE 1: November 6, 2019,  6:22am; <NewLine> REPLY_DATE 2: November 6, 2019,  6:11am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
60136,a error about the model of pytorch transform to libtorch,2019-11-06T03:50:52.896Z,0,159,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">    # load net<NewLine>    num_classes = len(labelmap) + 1                      # +1 for background<NewLine>    net = build_refinedet('test', int(args.input_size), num_classes)            # initialize SSD<NewLine>    net.load_state_dict(torch.load(args.trained_model,map_location='cuda:0'))    # model = torch.load(model_path, map_location='cuda:0')<NewLine>    net.eval()<NewLine>    print('Finished loading model!')<NewLine><NewLine>    # 向模型中输入数据以得到模型参数<NewLine>    example = torch.rand(1,3,320,320).cuda()<NewLine>    traced_script_module = torch.jit.trace(net,example)<NewLine><NewLine>    # 保存模型<NewLine>    traced_script_module.save(""torch_script_eval.pt"")<NewLine></code></pre><NewLine><p>i want to make the refinedet model transform to libtorch,but it have a error about “RuntimeError: Attempted to trace Detect_RefineDet, but tracing of legacy functions is not supported”</p><NewLine><p>Thank you for helping me！</p><NewLine></div>",https://discuss.pytorch.org/u/chen1234520,(MG_Chen),chen1234520,"November 6, 2019,  3:50am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>As the error message says, the model is using PyTorch 0.1.2-style Functions. (Newer autograd.Functions cannot be traced either, but that is another story…)<br/><NewLine>You probably want to convert the model to use the nms functions provided by TorchVision, those have tracing support.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: November 6, 2019,  5:13am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
59661,Is torch.Graph executable?,2019-10-31T09:39:28.209Z,1,133,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m interested in the optimization of a PyTorch JIT graph. For this purpose, I want to find a way to execute an existing <code>torch.Graph</code> directly. Is there any official or unofficial way for this purpose?</p><NewLine></div>",https://discuss.pytorch.org/u/sublee,(Heungsub Lee),sublee,"October 31, 2019,  9:39am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found a private API <code>torch._C._create_function_from_graph(name, graph)</code> which reconstructs a <code>torch.Function</code> from an existing JIT graph.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The usual thing to do is to transform the graph. This is what the various passes in the JIT do and you can also register custom passes. For example, the difference between <code>fn.graph</code> and <code>fn.graph_for(*inputs)</code> comes from the passes executed after specialization of the input arguments.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/sublee; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: October 31, 2019,  4:37pm; <NewLine> REPLY_DATE 2: October 31, 2019,  9:56pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
59539,RuntimeError no grad accumulator for a saved leaf error,2019-10-30T05:24:56.662Z,3,252,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey guys, these lines incur the error in jit mode. Do you have any idea?</p><NewLine><pre><code class=""lang-auto""> matching_scores = torch.matmul(bike_key_out.permute(0, 3, 4, 2, 1), taxi_key_out.permute(0, 3, 4, 1, 2))<NewLine><NewLine>bt_t_x = torch.matmul(matching_scores, taxi_x.permute(0, 3, 4, 2, 1)).permute(0, 4, 3, 1, 2)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""multi_expt.py"", line 329, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""multi_expt.py"", line 298, in main<NewLine>    (bike_loss + taxi_loss).backward()<NewLine>  File ""/home/jindeng/anaconda3/envs/myenv/lib/python3.7/site-packages/torch/tensor.py"", line 118, in backward<NewLine>    torch.autograd.backward(self, gradient, retain_graph, create_graph)<NewLine>  File ""/home/jindeng/anaconda3/envs/myenv/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 93, in backward<NewLine>    allow_unreachable=True)  # allow_unreachable flag<NewLine>RuntimeError: No grad accumulator for a saved leaf!<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/111175,(Jinliang),111175,"October 30, 2019,  5:24am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think those are the leaves the error talks about. Could you try to reduce your code to an reproducing example?</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>It might be related to <a href=""https://github.com/pytorch/pytorch/issues/19769"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/19769</a><br/><NewLine>We would need a small code sample to reproduce this to be sure.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think I find out where the problem is. I unbind a tensor to a list of subtensors, and iteratively retrieve each subtensor to conduct further operation. If I substitue it by direct indexing on the original tensor, the error is gone.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your help, I think I find out the reason of my issue.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Could you share what you changed to fix this?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/111175; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/111175; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: October 30, 2019,  1:08pm; <NewLine> REPLY_DATE 2: October 30, 2019,  3:02pm; <NewLine> REPLY_DATE 3: October 31, 2019, 12:24am; <NewLine> REPLY_DATE 4: October 31, 2019, 12:28am; <NewLine> REPLY_DATE 5: October 31, 2019,  2:40pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
58627,"Question on CustomFuseGraph (graph_fuser). pytorch/tvm , pytorch/glow",2019-10-19T03:16:35.229Z,2,637,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,<br/><NewLine>I have some basic questions on CustomFuseGraph and its relation to other projects. I see that <strong><a href=""https://github.com/pytorch/tvm/blob/master/torch_tvm/fusion_pass.cpp#L119"" rel=""nofollow noopener"">pytorch/tvm</a></strong> and <strong><a href=""https://github.com/pytorch/glow/blob/master/torch_glow/src/GlowFuser.cpp#L131"" rel=""nofollow noopener"">pytorch/glow</a></strong> are using different custom fuse passes and not using the <strong>CustomFuseGraph</strong>. Could someone explain why they do not use the graph_fuser provided by PyTorch? Sorry if this is a naive question.</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/vdantu,(Vamshidhar Dantu (dan21c)),vdantu,"October 20, 2019,  1:37am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Traditionally, the graph fuser in PyTorch has been operating on a per-<code>Block</code> basis. This is inherited (at least it used to a few weeks ago) by the CustomFuseGraph.<br/><NewLine>PyTorch TVM (and probably also glow) are interested in fusing across blocks, e.g. to also fuse control flow nodes. This is why <a href=""https://github.com/pytorch/tvm/pull/72"" rel=""nofollow noopener"">PyTorch TVM has switched away from CustomFuseGraph</a>. Personally, I would expect that custom fusion mechanism will be folded back into PyTorch when the picture of what users need there becomes clearer (just like CustomFuseGraph was introduced when one thought that one could also make good use of that, I think I recall that originally we thought that the CustomFuseGraph might already work for PyTorch TVM).</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/tom"">@tom</a>: Awesome. Thanks for taking the time and responding. I think that answers my question to a great extent. So, is the plan to upstream these changes to the CustomFuseGraph? Also, the torch/tvm’s custom fusion seems to ignore the control flow ops for now <a href=""https://github.com/pytorch/tvm/blob/master/torch_tvm/fusion_pass.cpp#L30"" rel=""nofollow noopener"">seen in fusion_pass.cpp</a>.</p><NewLine><p>I was also experimenting a bit and see that fusing blocks of control flow ops is not currently handled in <strong>subgraph_utils</strong>, is this understanding right ?</p><NewLine><p>Lets take this graph for example</p><NewLine><pre><code class=""lang-auto"">graph(%x.1 : Float(*)):<NewLine>  %18 : Float(1) = prim::Constant[value={1}]()<NewLine>  %16 : int[] = prim::Constant[value=[1]]()<NewLine>  %1 : None = prim::Constant()<NewLine>  %2 : int = prim::Constant[value=1]() # script_module.py:64:22<NewLine>  %3 : int = prim::Constant[value=2]() # script_module.py:68:20<NewLine>  %4 : int = prim::Constant[value=3]() # script_module.py:71:18<NewLine>  %ret.1 : Double(*) = aten::zeros(%16, %1, %1, %1, %1) # script_module.py:64:10<NewLine>  %7 : Bool(*) = aten::eq(%x.1, %2) # script_module.py:65:7<NewLine>  %8 : bool = aten::Bool(%7) # script_module.py:65:7<NewLine>  %ret : Tensor(*) = prim::If(%8) # script_module.py:65:4<NewLine>    block0():<NewLine>      %12 : Tensor = aten::add_(%ret.1, %18, %2) # script_module.py:67:8<NewLine>      %ret.4 : Double(*) = aten::mul(%ret.1, %3) # script_module.py:68:14<NewLine>      %ret.7 : Double(*) = aten::add(%ret.4, %18, %2) # script_module.py:69:14<NewLine>      -&gt; (%ret.7)<NewLine>    block1():<NewLine>      %ret.9 : Float(*) = aten::add(%x.1, %4, %2) # script_module.py:71:14<NewLine>      -&gt; (%ret.9)<NewLine>  return (%ret)<NewLine></code></pre><NewLine><p>Here, the first node <code>aten::add_()</code> inside  <code>block0()</code> of  <code>prim::If</code> has one of its inputs <code>%ret.1</code> which is coming from outside the <code>prim::If</code>. But when cloning this node, during <code>mergeNodeIntoSubgraph</code> its not able to find the metadata of this input node. I think its because when cloning, the <code>value_map</code> only has the inputs in block scope and not the graph scope. I am not very familiar with the PT graph manipulation to understand the reason why only block’s inputs are added to value_map in <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/ir.cpp#L632"" rel=""nofollow noopener"">ir.cpp</a> and only the prim::If’s inputs are added to the value map in <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/passes/utils/subgraph_utils.cpp#L70"" rel=""nofollow noopener"">subgraph_utils</a> . The error seems to come because the <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/ir.cpp#L1586"" rel=""nofollow noopener"">value_map(i) in ir.cpp</a> returns a NULL as the <code>ret.1</code> is not in the <code>value_map</code>.</p><NewLine><p>Is there an example in code  where fusion of control flow ops is handled? Also, please correct me if my understanding is wrong.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>So I don’t know how to answer most of your questions and I haven’t looked in a while relative to the rate of change. Apparently, something is not quite there yet for the control flow in torch TVM.<br/><NewLine>For the fusion in the fusion pass, blocks is not currently handled, and I am ignorant of whether the obstacle torch TVM hit was with the graph rearrangement itself or something else.<br/><NewLine>to the best of my knowledge we don’t currently fuse inplace ops, so I would expect adventure when you try that. Maybe that is part of the problem you are seeing.</p><NewLine><p>I don’t know any examples beyond the obvious users. In theory, I would expect the optimizations removing ifs with constant true/false result to give some idea of what it would take to move ops with blocks into a subgraph.</p><NewLine><p>I’ll be very interested to hear from your progress.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yep, your understanding is correct. Control flow is currently unhandled.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/bwasti"">@bwasti</a>:  Thanks for your response. Could you let me know if my understanding <a href=""https://discuss.pytorch.org/t/question-on-customfusegraph-graph-fuser-pytorch-tvm-pytorch-glow/58627/3"">above</a> is correct? I just want to understand if fusion of control flow ops into the custom op node is even allowed by PT.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vdantu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/bwasti; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/vdantu; <NewLine> ,"REPLY_DATE 1: October 20, 2019, 12:01pm; <NewLine> REPLY_DATE 2: October 20, 2019,  9:41pm; <NewLine> REPLY_DATE 3: October 20, 2019,  9:42pm; <NewLine> REPLY_DATE 4: October 30, 2019,  3:33am; <NewLine> REPLY_DATE 5: October 30, 2019,  3:53am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
59360,Can&rsquo;t gdb in fused_kernel.cpp,2019-10-28T08:38:18.251Z,0,99,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am looking into PyTorch-jit-fusion now. I try to set a breakpoint in fuser_kernel.cpp at FusedKernelCPU::FusedKernelCPU(…) when I run a jit-trace stript. Anyone can tell me when and how such function will be invoked? Any reply will be appreicated. thanks</p><NewLine></div>",https://discuss.pytorch.org/u/alanzhai219,(Alan Zhai),alanzhai219,"October 28, 2019,  8:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Last I looked, CPU fusion was disabled by default, so you need to enable it with<br/><NewLine><code>torch._C._jit_override_can_fuse_on_cpu(True)</code>.</p><NewLine><p>Printing <code>myfn.graph_for(inp)</code> will show if you have fusion for your inputs.<br/><NewLine>Note that as optimized graphs are cached, you need to re-define the traced/scripted function in order to clear the cache between after enabling CPU fusion if you previously ran it with CPU fusion disabled.</p><NewLine><p>Note that CPU fusion is disabled by default due to performance &amp; flakiness issues. In turn AVX &amp; co are disabled in the CPU fuser because they sometimes cause problems (see the commentary in <code>fused_kernel.cpp</code>).</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: December 5, 2019,  6:45am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> 
59268,GPU utilization is low while using CUDA extension on multi-gpu,2019-10-26T08:16:28.234Z,3,152,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I built my CUDA extension module following <a href=""https://pytorch.org/tutorials/advanced/cpp_extension.html"" rel=""nofollow noopener"">this</a> link. Everything works well when I only use 1 GPU. And its utilization is high (&gt;90%). However, when I integrated it into my neural work and trained with 2 GPU, the utilization of each gpu is pretty low (≈50%).<br/><NewLine>Any ideas? thanks!<br/><NewLine>My environments:<br/><NewLine>Pytorch:1.1<br/><NewLine>CUDA:10.1<br/><NewLine>OS:Ubuntu 18<br/><NewLine>GPU: RTX2080ti</p><NewLine></div>",https://discuss.pytorch.org/u/jhd,(JINHAO DUAN),jhd,"October 26, 2019,  8:17am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Maybe you can analysis the running time of each part, such as data loading, model forwarding, output processing. This may not be caused by forwarding.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, <a class=""mention"" href=""/u/mendel123"">@Mendel123</a>  thanks for your reply.<br/><NewLine>After analysis the running time of  each part, Forward has taken the largest part.</p><NewLine><div class=""md-table""><NewLine><table><NewLine><thead><NewLine><tr><NewLine><th style=""text-align:left""></th><NewLine><th style=""text-align:left"">Forward</th><NewLine><th style=""text-align:left"">backward</th><NewLine><th style=""text-align:center"">Data</th><NewLine></tr><NewLine></thead><NewLine><tbody><NewLine><tr><NewLine><td style=""text-align:left"">2 GPU(32 batchsize)</td><NewLine><td style=""text-align:left"">1.1 sec</td><NewLine><td style=""text-align:left"">0.27 sec</td><NewLine><td style=""text-align:center"">0.001</td><NewLine></tr><NewLine><tr><NewLine><td style=""text-align:left"">1 GPU(16 batchsize)</td><NewLine><td style=""text-align:left"">0.4 sec</td><NewLine><td style=""text-align:left"">0.27 sec</td><NewLine><td style=""text-align:center"">0.001</td><NewLine></tr><NewLine></tbody><NewLine></table><NewLine></div><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I used APEX mix-precision training, will it be related to apex? Besides, I didn’t implement <code>half()</code> operation in my module , so I just convert inputs to<code> float()</code> before my invoking and convert it to <code>half()</code> after my invoking.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Checking line by line, I finally found that declaring variables and allocating CUDA memory in CUDA extension of Pyotrch will greatly reduce GPU efficiency. By removing the following statements,</p><NewLine><pre><code class=""lang-auto"">float *dist_Dev; <NewLine>gpuErrchk(cudaMalloc((void**)&amp;dist_Dev, myParameter.obj_num * myParameter.cluster_num * sizeof(float)));<NewLine>int *obj_num_Dev; <NewLine>gpuErrchk(cudaMalloc((void**)&amp;obj_um_Dev, myParameter.cluster_num * sizeof(int)));<NewLine>int *num_per_classt;<NewLine>gpuErrchk(cudaMalloc((void**)&amp;num_per_classt, myParameter.t * myParameter.cluster_num * sizeof(int)));<NewLine></code></pre><NewLine><p>, the utilization of each gpu is high. And each variable will be created in pytorch and then passed to CUDA module.<br/><NewLine>However, it still can’t expalin why single GPU works. I also compiled C++/CUDA into a <code>.so</code> file and invoked by <code>ctypes</code>.The result proves that declearing variables like the method above is fine.<br/><NewLine>So I guess there might be a problem with the way CUDA extension works.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Mendel123; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jhd; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jhd; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jhd; <NewLine> ,"REPLY_DATE 1: October 26, 2019,  9:11am; <NewLine> REPLY_DATE 2: October 26, 2019,  9:34am; <NewLine> REPLY_DATE 3: October 26, 2019,  9:40am; <NewLine> REPLY_DATE 4: October 26, 2019,  1:21pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
58967,Try to use torchscript on torch.nn.transformer,2019-10-23T03:11:22.168Z,3,358,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone. I attempt to use torch.jit.script on the torch.nn.transformer, but it doesn’t work。<br/><NewLine>Has anyone ever done any related work?</p><NewLine><p>I build the pytorch from source and the torch version is 1.4.0a0+2e7dd54</p><NewLine><p>I’d appreciate if anybody can help me! Or if there is a workable implementation, please let me know! Thanks in advance!</p><NewLine><p>here is the code:</p><NewLine><blockquote><NewLine><p>import torch<br/><NewLine>import torch.nn as nn<br/><NewLine>torch.manual_seed(2)<br/><NewLine>transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)<br/><NewLine>trans_model = torch.jit.script(transformer_model)<br/><NewLine>trans_model.save(‘test.pt’)</p><NewLine></blockquote><NewLine><p>and here is the log:</p><NewLine><blockquote><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “transformer_demo.py”, line 13, in <br/><NewLine>trans_model = torch.jit.script(transformer_model)<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/<strong>init</strong>.py”, line 1239, in script<br/><NewLine>return torch.jit.torch.jit._recursive.recursive_script(obj)<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 508, in recursive_script<br/><NewLine>return create_script_module(nn_module, infer_methods_to_compile(nn_module))<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 305, in create_script_module<br/><NewLine>concrete_type = concrete_type_store.get_or_create_concrete_type(nn_module)<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 243, in get_or_create_concrete_type<br/><NewLine>raw_concrete_type = infer_raw_concrete_type(nn_module)<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 91, in infer_raw_concrete_type<br/><NewLine>sub_concrete_type = concrete_type_store.get_or_create_concrete_type(item)<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 243, in get_or_create_concrete_type<br/><NewLine>raw_concrete_type = infer_raw_concrete_type(nn_module)<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 91, in infer_raw_concrete_type<br/><NewLine>sub_concrete_type = concrete_type_store.get_or_create_concrete_type(item)<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 240, in get_or_create_concrete_type<br/><NewLine>scripted = create_constant_iterable_module(nn_module)<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 539, in create_constant_iterable_module<br/><NewLine>modules[key] = recursive_script(submodule)<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 508, in recursive_script<br/><NewLine>return create_script_module(nn_module, infer_methods_to_compile(nn_module))<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 308, in create_script_module<br/><NewLine>return create_script_module_impl(nn_module, concrete_type, cpp_module, stubs)<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 358, in create_script_module_impl<br/><NewLine>script_module = torch.jit.RecursiveScriptModule._construct(cpp_module, init_fn)<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/<strong>init</strong>.py”, line 1612, in _construct<br/><NewLine>init_fn(script_module)<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 340, in init_fn<br/><NewLine>scripted = recursive_script(orig_value)<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 508, in recursive_script<br/><NewLine>return create_script_module(nn_module, infer_methods_to_compile(nn_module))<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 308, in create_script_module<br/><NewLine>return create_script_module_impl(nn_module, concrete_type, cpp_module, stubs)<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 362, in create_script_module_impl<br/><NewLine>create_methods_from_stubs(concrete_type, stubs)<br/><NewLine>File “/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 268, in create_methods_from_stubs<br/><NewLine>concrete_type._create_methods(defs, rcbs, defaults)<br/><NewLine>RuntimeError:<br/><NewLine>Module ‘MultiheadAttention’ has no attribute ‘q_proj_weight’ :<br/><NewLine>at /home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/activation.py:771:30<br/><NewLine>key_padding_mask=key_padding_mask, need_weights=need_weights,<br/><NewLine>attn_mask=attn_mask, use_separate_proj_weight=True,<br/><NewLine>q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,<br/><NewLine>~~~~~~~~~~~~~~~~~~ &lt;— HERE<br/><NewLine>v_proj_weight=self.v_proj_weight)<br/><NewLine>else:</p><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/huoge,,huoge,"October 23, 2019,  5:22pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This was brought up <a href=""https://github.com/pytorch/pytorch/issues/24173"" rel=""nofollow noopener"">here</a> but the fix <a href=""https://github.com/pytorch/pytorch/pull/24204"" rel=""nofollow noopener"">here</a> never landed due to backwards-compatibility issues, I can try to get it through soon.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>ok, thanks! I tried the statement  ‘self.register_parameter(‘q_proj_weight’, None)’，but it seems doesn’t work。</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It turns out there are some other fixes needed, see these two PRs for details (<a href=""https://github.com/pytorch/pytorch/pull/28555"" rel=""nofollow noopener"">#28555</a> and <a href=""https://github.com/pytorch/pytorch/pull/28561"" rel=""nofollow noopener"">#28561</a>). If you need it now you can check out <span class=""hashtag"">#28561</span> and <a href=""https://github.com/pytorch/pytorch#from-source"" rel=""nofollow noopener"">build from source</a>, otherwise it should be in the nightly package in a few days.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thx a lot. it’s really helpful to me.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/huoge; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/huoge; <NewLine> ,"REPLY_DATE 1: October 23, 2019,  5:59pm; <NewLine> REPLY_DATE 2: October 24, 2019,  6:57am; <NewLine> REPLY_DATE 3: October 24, 2019,  5:52pm; <NewLine> REPLY_DATE 4: October 25, 2019,  3:16am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
58230,Downsample got error when converting model to .pt using script method,2019-10-15T01:27:04.951Z,3,316,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The error comes from the torchvision itself:</p><NewLine><pre><code class=""lang-auto"">attribute 'downsample' of type 'NoneType' is not usable in a script method (did you forget to add it __constants__?):<NewLine>at /nfs/engine/rajagopalar/anaconda3/envs/torchExp/lib/python3.6/site-packages/torchvision/models/resnet.py:109:12<NewLine>        out = self.relu(out)<NewLine></code></pre><NewLine><p>Original post is here --&gt; <a href=""https://stackoverflow.com/questions/57602574/pytorch-torchscript-error-attribute-downsample-of-type-nonetype-is-not-u"" rel=""nofollow noopener"">original post</a></p><NewLine><p>Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/RebirthT,,RebirthT,"October 15, 2019,  1:27am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This was a bug in torchvision, it’s fixed with <a href=""https://github.com/pytorch/vision/pull/1343"" rel=""nofollow noopener"">this PR</a>.</p><NewLine><p>We’re working on pushing a new release soon that will include this fix (among many others!). To get the fix locally today, you can <a href=""https://github.com/pytorch/vision#installation"" rel=""nofollow noopener"">install torchvision master from source</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/driazati"">@driazati</a> Thanks for your prompt reply<br/><NewLine>After adding <code>__constant__ = ['downsample']</code> I got the error below:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: attempting to re-assign constant 'downsample' in WeakScriptModuleProxy<NewLine></code></pre><NewLine><p>Thanks</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you on the latest version of PyTorch (v.1.3.0)? You can check with <code>print(torch.__version__)</code>. <code>WeakScriptModuleProxy</code> was removed prior to the 1.3 release, so you may need to update to get the right fixes.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/driazati"">@driazati</a>  I’ll check my libtorch version,<br/><NewLine>Thanks for sharing !!!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/RebirthT; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/RebirthT; <NewLine> ,"REPLY_DATE 1: October 15, 2019,  9:38pm; <NewLine> REPLY_DATE 2: October 18, 2019,  2:31am; <NewLine> REPLY_DATE 3: October 23, 2019,  5:25pm; <NewLine> REPLY_DATE 4: October 24, 2019,  5:07am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
59062,Practical use case for _extra_files?,2019-10-23T22:50:51.764Z,0,131,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Can someone help me understand when and how to use the _extra_files parameter is <code>torch.jit.save</code> and <code>torch.jit.load</code>? The code example in the documents (with <code>extra_files['foo.txt'] = 'bar'</code>) didn’t help me understand what its practical use would be.</p><NewLine></div>",https://discuss.pytorch.org/u/wnnaone,,wnnaone,"October 23, 2019, 10:50pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It lets you wrap up any extra data you want into the output of <code>torch.save</code> and have it be decoded for you by <code>torch.load</code>. Maybe you want to ship some documentation or versioning metadata with your saved model to help with deployment, <code>extra_files</code> lets you do that without having to ship files separately alongside your <code>.pt</code> file.</p><NewLine><p>The <code>.pt</code> file from <code>torch.jit.save</code> is a zip file, so any <code>extra_files</code> just get added to the zip archive.</p><NewLine><p>To dig into the docs example a little more, for this code:</p><NewLine><pre><code class=""lang-auto"">class M(nn.Module):<NewLine>    def forward(self):<NewLine>        pass<NewLine><NewLine>m = torch.jit.script(M())<NewLine>extra_files = torch._C.ExtraFilesMap()<NewLine>extra_files['foo.txt'] = 'bar'<NewLine>torch.jit.save(m, 'scriptmodule.pt', _extra_files=extra_files)<NewLine></code></pre><NewLine><p>Then in a shell</p><NewLine><pre><code class=""lang-auto"">$ unzip scriptmodule.pt<NewLine>Archive:  scriptmodule.pt<NewLine> extracting: scriptmodule/version    <NewLine> extracting: scriptmodule/extra/foo.txt  <NewLine> extracting: scriptmodule/data.pkl   <NewLine> extracting: scriptmodule/code/__torch__.py  <NewLine> extracting: scriptmodule/code/__torch__.py.debug_pkl  <NewLine> extracting: scriptmodule/constants.pkl  <NewLine>$ cat scriptmodule/extra/foo.txt<NewLine>bar<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: October 24, 2019, 12:46am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
58512,Training a torch::jit::script::Module,2019-10-17T18:34:44.075Z,1,262,"<div class=""post"" itemprop=""articleBody""><NewLine><p>There is an example for training a <code>torch::nn::Module</code> <a href=""https://pytorch.org/cppdocs/frontend.html#end-to-end-example"" rel=""nofollow noopener"">https://pytorch.org/cppdocs/frontend.html#end-to-end-example</a> but when I try to use a <code>torch::jit::script::Module</code>, I cannot call <code>.parameters()</code> like in the example.</p><NewLine><p>Is there a way to train a  <code>torch::jit::script::Module</code> in C++?</p><NewLine></div>",https://discuss.pytorch.org/u/markl,,markl,"October 17, 2019,  9:25pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Weirdly unlike in Python, <code>jit::script::Module</code> doesn’t have the same API as <code>nn::Module</code>.</p><NewLine><p>You should be able to access a <code>script::Module</code>'s parameters via something like:</p><NewLine><pre><code class=""lang-cpp"">for (auto param_slot : my_module.get_parameters()) {<NewLine>    auto my_tensor = param_slot.value().toTensor();<NewLine>}<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The <code>nn::Module::parameters()</code> returns a thing called a <code>slot_list_impl&lt;&gt;</code>.</p><NewLine><p>Do you have advice on how I can make one of these from the tensors returned by the <code>script::Module::get_parameters()</code>?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>slot_list_impl&lt;NameValue&gt;</code> is an iterator over the one of a <code>script::Module</code>'s internal lists of Parameters/Attributes. Can you elaborate on your use case for constructing one yourself? You should be able to mess with a <code>script::Module</code> by using <code>script::Module::register_parameter</code> to change any parameter value.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I was trying to adapt the training loop to work for <code>torch::jit::script::Module</code> so I needed the same interface that the optimizer expects. However, I realized that it accepts <code>std::vector&lt;at::Tensor&gt;</code>, so I actually don’t have to create <code>slot_list_impl</code> after all.</p><NewLine><p>Maybe you can take a look at what I was trying to do here <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/jit-module-parameters-are-not-updating-when-training/58945/8"">jit::script::Module parameters are not updating when training</a></p><NewLine><p>I think for the code to actually work, I need to use <code>register_parameter</code> or <code>set_parameter</code> to push the updated tensor back into the <code>script::Module</code> after the optimizer has acted on it. Does that sound right? Or could there be a way that the optimizer can update the tensor values in place?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/markl; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/markl; <NewLine> ,"REPLY_DATE 1: October 22, 2019, 12:42am; <NewLine> REPLY_DATE 2: October 18, 2019,  1:29am; <NewLine> REPLY_DATE 3: October 23, 2019,  5:51pm; <NewLine> REPLY_DATE 4: October 23, 2019,  7:32pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
58540,Does torchscript support custom autograd function?,2019-10-18T02:34:55.020Z,0,320,"<div class=""post"" itemprop=""articleBody""><NewLine><p>does torchscript support custom autograd function?</p><NewLine><p>I implemented a custom function followed this link:<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html</a></p><NewLine><p>Does torchsceipt support this kind of function? I followed this torchscript tutorial:<a href=""https://pytorch.org/docs/stable/jit.html#creating-torchscript-code"" rel=""nofollow noopener"">TorchScript</a> and an <a href=""https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py"" rel=""nofollow noopener"">example</a></p><NewLine><p>However, I got error “ValueError: Compiled functions can’t take variable number of arguments or use keyword-only arguments with defaults”, and it’s my custom autograd function caused this error. I assumed that the compiler doesn’t know about the argument type of the custom autograd function. I cannot find how to make custom autograd work with torchscript.</p><NewLine></div>",https://discuss.pytorch.org/u/acdc,(acdc),acdc,"October 18, 2019,  8:00pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We do not currently support custom autograd functions, but it is something on our radar that we would like to do in the future. You can find more context in <a href=""https://github.com/pytorch/pytorch/issues/22329"" rel=""nofollow noopener"">this issue</a>.</p><NewLine><p>It is also possible to replicate most of the behavior in custom autograd functions now via custom C++ operators.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: October 23, 2019,  5:27pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
58615,Pytorch IR with Line Numbers,2019-10-18T22:53:35.341Z,0,275,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I was wondering how to generate the PyTorch IR for a Torch Script module with line numbers, like the one shown in the example code:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/docs/source/jit.rst#interpreting-graphs"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/master/docs/source/jit.rst#interpreting-graphs"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/master/docs/source/jit.rst#interpreting-graphs</a></h4><NewLine><pre><code class=""lang-rst"">TorchScript<NewLine>===========<NewLine><NewLine>.. toctree::<NewLine>   :maxdepth: 1<NewLine>   :caption: Builtin Functions<NewLine>   :hidden:<NewLine><NewLine>   torch.jit.supported_ops &lt;jit_builtin_functions&gt;<NewLine><NewLine>.. contents:: :local:<NewLine><NewLine>.. automodule:: torch.jit<NewLine>.. currentmodule:: torch.jit<NewLine><NewLine>TorchScript is a way to create serializable and optimizable models from PyTorch code.<NewLine>Any TorchScript program can be saved from a Python<NewLine>process and loaded in a process where there is no Python dependency.<NewLine><NewLine>We provide tools to incrementally transition a model from a pure Python program<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/master/docs/source/jit.rst#interpreting-graphs"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Currently, <code>foo.graph</code> does not get you the IR with line numbers, so I was wondering if it was possible to get the IR with the line numbers included.</p><NewLine><p>Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/esqu1,,esqu1,"October 18, 2019, 10:53pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you on the latest version of PyTorch (v1.3.0)? You can check with <code>print(torch.__version__)</code>. This feature was recently added so if you’re on an older version (e.g. v1.0) it might not show up.</p><NewLine><p>For a simple example like:</p><NewLine><pre><code class=""lang-python"">@torch.jit.script<NewLine>def x(e):<NewLine>    return e + 10<NewLine><NewLine>print(x.graph)<NewLine></code></pre><NewLine><p>You should get something like</p><NewLine><pre><code class=""lang-auto"">graph(%e.1 : Tensor):<NewLine>  %3 : int = prim::Constant[value=1]()<NewLine>  %2 : int = prim::Constant[value=10]() # ../test.py:33:15<NewLine>  %4 : Tensor = aten::add(%e.1, %2, %3) # ../test.py:33:11<NewLine>  return (%4)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: October 23, 2019,  5:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
58839,How to get the device of a torch::jit::script::Module?,2019-10-21T23:43:08.742Z,0,171,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to know if <code>to(at::kCUDA)</code> has been called on the module</p><NewLine></div>",https://discuss.pytorch.org/u/markl,,markl,"October 21, 2019, 11:43pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>script::Module</code>s don’t track their device since the device only affects the Tensor parameters stored in the module. The only way to check the device would be to check one of the Tensor parameters on the module and view its device.</p><NewLine><p>We definitely need to have a more accessible documentation page, but you can see the definition of <code>script::Module</code> <a href=""https://github.com/pytorch/pytorch/blob/0aa694ebe5615c1de01c8075a5084bc30c79e04d/torch/csrc/jit/script/module.h"" rel=""nofollow noopener"">here</a> for a complete list of its methods.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: October 23, 2019,  7:26pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
58860,TorchScript formal language spec,2019-10-22T04:21:39.816Z,1,196,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><p>Is there somewhere formal TorchScript grammar spec available? Or it’s all hardcoded in JIT complier? Having that would help implementing some decent TorchScript tooling and add support in popular editors like vs.code or emacs and would greatly improve experience. Don’t know exactly how others are doing this, but my current development flow is little bit frustrating: write model in python, run jit, and plumb all places which are not supported by jit right now. And there is a lot little details which I’m learning by actually converting my code not covered anywhere else (recent dicovery for example: jit does not support yield, which makes perfect sense if You think about it, especially in context of optimization, but it’s not documented). I would rather like to write my models in TorchScript subset from the beginning.</p><NewLine><p>Artur</p><NewLine></div>",https://discuss.pytorch.org/u/akarazniewicz,(Artur Karazniewicz),akarazniewicz,"October 22, 2019,  4:21am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, thanks for the feedback! We don’t have a specification for what parts of Python we cover and don’t. The closest we have is the <a href=""https://pytorch.org/docs/master/jit.html#torchscript-language-reference"" rel=""nofollow noopener"">language reference</a>.</p><NewLine><p>I think you’re right that it would be really useful to at least have a comprehensive inventory of what keywords, builtins, etc. are supported—I’ll take a look at writing something up and try to have it in the docs by the next release.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is linked on the language reference page but maybe hidden, we list all the <code>Tensor</code> and <code>torch.nn.functional</code> methods we support <a href=""https://pytorch.org/docs/master/jit_builtin_functions.html#builtin-functions"" rel=""nofollow noopener"">here</a>, but that is missing a few things (including many Python globals, <code>nn.Module</code>s, and some other TorchScript builtins)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: October 22, 2019,  7:07pm; <NewLine> REPLY_DATE 2: October 23, 2019,  5:14pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
58649,RuntimeError: cuda runtime error (78) : a PTX JIT compilation failed at /pytorch/torch/csrc/generic/serialization.cpp:137,2019-10-19T09:34:18.823Z,0,190,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,<br/><NewLine>I have an error, the information is as follows</p><NewLine><pre><code class=""lang-auto"">THCudaCheck FAIL file=/pytorch/torch/csrc/generic/serialization.cpp line=137 error=78 : a PTX JIT compilation failed<NewLine>Traceback (most recent call last):<NewLine>  File ""test_video.py"", line 221, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""test_video.py"", line 110, in main<NewLine>    net.load_state_dict(torch.load(trained_model))<NewLine>  File ""/home/sean/anaconda3/envs/pt40/lib/python3.6/site-packages/torch/serialization.py"", line 303, in load<NewLine>    return _load(f, map_location, pickle_module)<NewLine>  File ""/home/sean/anaconda3/envs/pt40/lib/python3.6/site-packages/torch/serialization.py"", line 476, in _load<NewLine>    deserialized_objects[key]._set_from_file(f, offset, f_is_real_file)<NewLine>RuntimeError: cuda runtime error (78) : a PTX JIT compilation failed at /pytorch/torch/csrc/generic/serialization.cpp:137<NewLine></code></pre><NewLine><p>Environment information are:</p><NewLine><ol><NewLine><li>Pytorch0.40</li><NewLine><li>cuda-8.0</li><NewLine><li>Nvidia Titan X, Nvidia Titan Xp</li><NewLine></ol><NewLine><p>Could you help me solve it. Thank you in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/SeanChenxy,(Sean Chen),SeanChenxy,"October 19, 2019,  9:34am",,,,,
58560,Memory leaks when a model is transferred from gpu to cpu and save,2019-10-18T08:44:44.170Z,0,104,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-python"">def save_model:<NewLine>        net = self.my_net.module<NewLine>        dummy_input = torch.randn(1,3,112,112).cpu()<NewLine>        net.eval()<NewLine>        net.cpu() # switch to cpu to save the model to avoid some issues on Pytorch 1.1.0<NewLine>        script_module = torch.jit.trace(net,dummy_input)<NewLine>        script_module.save('my_model.pt')<NewLine>        net.cuda() # switch to cuda to continue training on GPU<NewLine>        net.train()<NewLine></code></pre><NewLine><p>The memory usage keeps increasing after each call to save_model, in other words, memory is being leaked.</p><NewLine><p>May anyone help to me to understand this issue? Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/Tan_Dat_Nguyen,(Tan Dat Nguyen),Tan_Dat_Nguyen,"October 18, 2019,  8:45am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Memory on the cpu or gpu side?<br/><NewLine>How do you measure this?<br/><NewLine>Could you provide a small script to reproduce this?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: October 18, 2019,  7:59pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
58312,Cycle detection in computation graph during fusion,2019-10-15T18:36:44.308Z,1,181,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When nodes are merged (graph fusion pass for example), how are cycles detected/avoided? Is it taken care by moveAfterTopologicallyValid in alias analysis? What algorithm is used here? Any pointers would be highly appreciated <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/vdantu,(Vamshidhar Dantu (dan21c)),vdantu,"October 15, 2019,  6:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What do you mean by cycles?<br/><NewLine>The graphs are acyclic and we loop until there is no more nodes to add to a fusion group…</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Lets consider a graph, like the following,<br/><NewLine><img alt=""image"" data-base62-sha1=""xW8FQd0FivT7IyOx9V8xFWIadk2"" height=""162"" src=""https://discuss.pytorch.org/uploads/default/original/3X/e/d/edda35f3b8f40cd46091976ba2e7c932bbf854fe.png"" width=""284""/><br/><NewLine>(Copied from <a href=""https://cs.stackexchange.com/questions/71369/edge-contraction-in-dag"" rel=""nofollow noopener"">here</a>)</p><NewLine><p>If we combine 1, 3, 4 red nodes above, we form a loop right? Considering 1, 3, 4 are can be grouped, what prevents the fusion from happening? If my question doesn’t make sense, please feel free to correct me <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/> .</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, <code>moveAfterTopologicallyValid</code> takes care of that. It does dependence analysis to make sure that this merging is disallowed.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""58312""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/7993a0/40.png"" width=""20""/> Michael_Suo:</div><NewLine><blockquote><NewLine><p>takes care of that. It does dependence analysis to make sure that this merging is disallowed.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Cool. Thanks for the response <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vdantu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/vdantu; <NewLine> ,"REPLY_DATE 1: October 15, 2019,  7:03pm; <NewLine> REPLY_DATE 2: October 15, 2019, 10:27pm; <NewLine> REPLY_DATE 3: October 18, 2019, 12:08am; <NewLine> REPLY_DATE 4: October 15, 2019, 11:04pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 2 Likes; <NewLine> REPLY 4 LIKES: ; <NewLine> 
55851,Serialize jit optimized graphs and visualize them,2019-09-13T05:39:07.132Z,1,238,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was curious to know if anyone know how I could serialize a jit graph into something that we can visualize. I am currently using graph-&gt;dump() and node-&gt;dump()'s to debug. I wanted to know how we could visualize the graph.</p><NewLine></div>",https://discuss.pytorch.org/u/vdantu,(Vamshidhar Dantu (dan21c)),vdantu,"September 13, 2019,  5:39am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can print out the graph as valid Python code with <code>my_module.code</code>. We used to have a <a href=""https://github.com/pytorch/pytorch/blob/5949bb27b59f4f209313c42e6d28a07bb0af8e7e/torch/contrib/_graph_vis.py"" rel=""nofollow noopener"">helper</a> to turn a <code>Graph</code> into a visualization via GraphViz, but it wasn’t maintained and was deleted.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks. I used .graph and .code to look at the graph. But wanted to get something more visual :).  I hadn’t seen the helper. I also have a interesting case. I want to use this from the C++ side <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/> . Is there a similar dump function that can be invoked from C++ side or do we need to write one.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>We don’t have anything like that for C++ and probably won’t (it’s outside of our scope), but you should have all the tools / APIs for <code>Graph</code> and <code>Node</code> to make something for your own purposes.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>A little late, but thanks for the response <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vdantu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/vdantu; <NewLine> ,"REPLY_DATE 1: September 13, 2019, 11:29pm; <NewLine> REPLY_DATE 2: September 14, 2019,  1:33am; <NewLine> REPLY_DATE 3: September 16, 2019,  6:29pm; <NewLine> REPLY_DATE 4: October 15, 2019,  6:37pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
57009,Register_forward_hook is not supported on ScriptModules,2019-09-27T23:31:57.016Z,0,548,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi torch experts,</p><NewLine><p>I am trying to add a forward hook to my model. But I got the error message indicating the hook won’t work with jit module.</p><NewLine><p>Is there a particular reason why hook won’t work with jit modules? How can I work around this while still keep my module as a script module?</p><NewLine><p>Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/xzhu1900,,xzhu1900,"September 27, 2019, 11:31pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>register_forward_hooks</code> (and <code>register_backward_hooks</code>) currently aren’t supported in TorchScript. If you’d like to see them added, please file a <a href=""https://github.com/pytorch/pytorch/issues/new?template=feature-request.md"" rel=""nofollow noopener"">feature request on GitHub</a>.</p><NewLine><p>In your particular case it sounds like you’re inheriting from <code>ScriptModule</code> as the way to access the TorchScript compiler. An API change in v1.2.0 lets you compile <code>nn.Module</code>s without making them inherit from <code>ScriptModule</code> directly, see <a href=""https://pytorch.org/docs/master/jit.html#migrating-to-pytorch-1-2-recursive-scripting-api"" rel=""nofollow noopener"">these docs</a> for details:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine>class M(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.conv = nn.Conv2d(5, 5, 2)<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.conv(x) + 10<NewLine><NewLine><NewLine>def my_hook(self, *args):<NewLine>    print(""Hello from my_hook"")<NewLine><NewLine><NewLine>m = M()<NewLine>m.conv.register_forward_hook(my_hook)<NewLine><NewLine># `my_hook` will be called<NewLine>m(torch.randn(5, 5, 2, 2))<NewLine><NewLine>a_scripted_module = torch.jit.script(m)<NewLine><NewLine># `my_hook` will NOT be called, forward hooks are lost<NewLine># when an `nn.Module` is compiled<NewLine>a_scripted_module(torch.randn(5, 5, 2, 2))<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: October 9, 2019,  6:15pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
57355,Export from TorchScript to ONNX,2019-10-02T20:12:21.113Z,1,688,"<div class=""post"" itemprop=""articleBody""><NewLine><p>TorchScript -&gt; ONNX conversion of this simple module fails (<a href=""https://pastebin.com/DH8RgyQT"" rel=""nofollow noopener"">pastebin</a>). Am I doing something wrong?</p><NewLine><p>If one doesn’t jit-compile the model, everything works.</p><NewLine><pre><code class=""lang-auto"">from tempfile import TemporaryFile<NewLine><NewLine>import torch<NewLine>import torch.onnx<NewLine>import torch.jit<NewLine>from torch import nn, Tensor<NewLine><NewLine>print(f""PyTorch version is {torch.__version__}"")<NewLine><NewLine>class Model(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.module = nn.Linear(<NewLine>            in_features=8, out_features=4)<NewLine>        self.module2 = nn.Linear(<NewLine>            in_features=4, out_features=2)<NewLine>       <NewLine>    def forward(self, x: Tensor) -&gt; Tensor:<NewLine>        preout = self.module(x)<NewLine>        out = self.module2(preout)<NewLine>        return out<NewLine><NewLine><NewLine>model = Model()<NewLine>model = torch.jit.script(model)<NewLine><NewLine>dummy_input = torch.randn(3, 8)<NewLine>dummy_output = model(dummy_input)<NewLine><NewLine>with TemporaryFile() as temp:<NewLine>    torch.onnx.export(model=model, <NewLine>                      args=dummy_input, <NewLine>                      example_outputs=dummy_output,<NewLine>                      f=temp, <NewLine>                      verbose=True)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/racoiaws,(Roman Korostik),racoiaws,"October 2, 2019,  8:13pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>When the ONNX exporter sees an <code>nn.Module</code>, it uses the TorchScript tracer to graph a graph, then converts that graph to an ONNX graph. The TorchScript compiler (<code>torch.jit.script</code>) should be functionally equivalent, so it sound like this is a bug. Could you file an <a href=""https://github.com/pytorch/pytorch/issues/new?template=bug-report.md"" rel=""nofollow noopener"">issue on GitHub</a> so we can track this? Thanks!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sure, I’ll file an issue on GitHub!</p><NewLine><p>Edit: <a href=""https://github.com/pytorch/pytorch/issues/27569"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/27569</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/racoiaws; <NewLine> ,"REPLY_DATE 1: October 8, 2019,  8:37pm; <NewLine> REPLY_DATE 2: October 8, 2019,  9:06pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
57275,Python functions to create nodes / modify graph,2019-10-01T21:24:17.297Z,0,118,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’d like to modify the graph by inserting new inputs (hopefully by using addInput in the graph). Is there a way to do this from Python after getting a torch._C.Graph object? Couldn’t seem to find an constructor/create function when doing dir(torch._C.Node).</p><NewLine></div>",https://discuss.pytorch.org/u/flynntax,,flynntax,"October 1, 2019,  9:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is purposefully not exposed, the only way to get a graph is by compiling code with <code>torch.jit.script</code> or tracing with <code>torch.jit.trace</code>. Potentially you could extract the code you are compiling initially with the Python <code>inspect</code> module and edit it that way, so it would be changed in the way you want before TorchScript sees it.</p><NewLine><p>You could also drop down to C++ (and potentially use PyBind to make it callable from Python) and write a custom pass that operates on a <code>Graph</code> object, see <a href=""https://jott.live/markdown/Writing%20a%20Toy%20Backend%20Compiler%20for%20PyTorch"" rel=""nofollow noopener"">this tutorial</a> for details.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: October 8, 2019,  8:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
57577,Jit.script and custom usage of PackedSequence,2019-10-06T19:06:36.593Z,0,376,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I would like to use PackedSequence directly in a custom module for nlp task. Specifically, given a padded batch, i want to convert it to a packed sequence, perform some operations on the <code>data</code> and convert back to padded sequence.</p><NewLine><p>An example recipe is below (modifed the example from <a href=""https://pytorch.org/docs/stable/jit.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/jit.html</a>)</p><NewLine><p>my recipe works in python but when using <code>jit.script</code>, it fails with</p><NewLine><p><code>ValueError: Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults</code></p><NewLine><p>I wanted to learn why PackedSequence fails with torch script when used here but works fine when called using wrapper methods in torch.nn.utils.rnn. And how I can fix it.</p><NewLine><p>I will be grateful for any help in understanding this behavior.</p><NewLine><p>Thank you!</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch.nn.utils.rnn import PackedSequence<NewLine>import torch.nn._VF as torch_varfuncs<NewLine>from torch._jit_internal import Optional<NewLine><NewLine>class MyModule(torch.nn.Module):<NewLine>    def __init__(self, N, M):<NewLine>        super(MyModule, self).__init__()<NewLine>        # This parameter will be copied to the new ScriptModule<NewLine>        self.weight = torch.nn.Parameter(torch.rand(N, M))<NewLine><NewLine>        # When this submodule is used, it will be compiled<NewLine>        self.linear = torch.nn.Linear(N, M)<NewLine><NewLine>    def _pad(self, data, batch_first, batch_sizes, pad_value, sorted_indices, unsorted_indices):<NewLine>        packed_seq = torch.nn.utils.rnn.PackedSequence(data, batch_sizes, sorted_indices, unsorted_indices)<NewLine>        return torch.nn.utils.rnn.pad_packed_sequence(packed_seq, batch_first, pad_value)<NewLine><NewLine>    def forward(self, input, data_lengths):<NewLine>        batch_first = True<NewLine>        packed_input = torch.nn.utils.rnn.pack_padded_sequence(input, batch_first=batch_first,lengths=data_lengths, enforce_sorted=False)<NewLine>        output = self.weight.mv(packed_input.data)<NewLine>        # This calls the `forward` method of the `nn.Linear` module, which will<NewLine>        # cause the `self.linear` submodule to be compiled to a `ScriptModule` here<NewLine>        output = self.linear(output)<NewLine>        return self._pad(output, batch_first, packed_input.batch_sizes,-1.0, packed_input.sorted_indices, packed_input.unsorted_indices)<NewLine><NewLine>class MyModuleVF(MyModule):<NewLine><NewLine>    def _pad(self, data, batch_first1: bool, batch_sizes, pad_value: float, sorted_indices: Optional[torch.Tensor], unsorted_indices: Optional[torch.Tensor]):<NewLine><NewLine>        max_length = batch_sizes.size(0)<NewLine>        padded_output, lengths = torch_varfuncs._pad_packed_sequence(data, batch_sizes, batch_first1, -1.0, max_length)<NewLine>        if sorted_indices is not None:<NewLine>           # had to invert permute specifically as pytorch method was giving errors in jit (arange is returning float type and not long, as expected)<NewLine>            output = torch.empty_like(sorted_indices)<NewLine>            output.scatter_(0, sorted_indices,torch.arange(0, sorted_indices.numel(), device=sorted_indices.device).long())<NewLine>            batch_dim = 0 if batch_first1 else 1<NewLine>            return padded_output.index_select(batch_dim, output), lengths[output]<NewLine>        return padded_output, lengths<NewLine><NewLine>test_input = torch.tensor([[1., 2., 3., 4.], [5., 6., -1.0, -1.0],[8, 9, 10, -1.0]], dtype=torch.float)<NewLine>data_lengths = torch.tensor([4,2,3])<NewLine>size_ = (test_input &gt; 0).sum()<NewLine># works<NewLine>mm = MyModule(20,size_.item())<NewLine>result = mm(test_input, data_lengths)<NewLine><NewLine><NewLine># works<NewLine>mmvf = MyModuleVF(20,size_.item())<NewLine>result_vf = mmvf(test_input, data_lengths)<NewLine><NewLine><NewLine># works<NewLine>mmvf_s = torch.jit.script(MyModuleVF(20,size_.item()))<NewLine>result_vf_s = mmvf_s(test_input, data_lengths)<NewLine><NewLine># does not work<NewLine>mm_s = torch.jit.script(MyModule(20,size_.item()))<NewLine>result_s = mm(test_input, data_lengths)<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/veryluckyxyz,(Veryluckyxyz),veryluckyxyz,"October 6, 2019,  7:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the repro! There is a bug somewhere here, would you mind filing an <a href=""https://github.com/pytorch/pytorch/issues/new?template=bug-report.md"" rel=""nofollow noopener"">issue on GitHub</a>?</p><NewLine><p>For some reason it works for me on master if you use <code>PackedSequence</code> directly instead of the qualified version, so that could be a workaround until we get this fixed:</p><NewLine><pre><code class=""lang-auto"">    def _pad(self, data, batch_first: bool, batch_sizes, pad_value: float, sorted_indices: Optional[torch.Tensor], unsorted_indices: Optional[torch.Tensor]):<NewLine>        packed_seq = PackedSequence(data, batch_sizes, sorted_indices, unsorted_indices)<NewLine>        return torch.nn.utils.rnn.pad_packed_sequence(packed_seq, batch_first, pad_value)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: October 8, 2019,  8:32pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
57439,Some of CPU cores don&rsquo;t work when I increase training epochs,2019-10-04T07:59:49.363Z,0,106,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m using profiling tool VTune Amplifier. What I’m interested in is parallel programming, both in thread level and instruction levels. The number of cores in my server is 16, and it supports AVX instructions. (not support AVX2, AVX512)</p><NewLine><p>lscpu gives:</p><NewLine><p>Architecture:          x86_64<br/><NewLine>CPU op-mode(s):        32-bit, 64-bit<br/><NewLine>Byte Order:            Little Endian<br/><NewLine>CPU(s):                16<br/><NewLine>On-line CPU(s) list:   0-15<br/><NewLine>Thread(s) per core:    1<br/><NewLine>Core(s) per socket:    8<br/><NewLine>Socket(s):             2<br/><NewLine>NUMA node(s):          2<br/><NewLine>Vendor ID:             GenuineIntel<br/><NewLine>CPU family:            6<br/><NewLine>Model:                 62<br/><NewLine>Model name:            Intel® Xeon® CPU E5-2650 v2 @ 2.60GHz<br/><NewLine>Stepping:              4<br/><NewLine>CPU MHz:               1200.433<br/><NewLine>CPU max MHz:           3400.0000<br/><NewLine>CPU min MHz:           1200.0000<br/><NewLine>BogoMIPS:              5201.92<br/><NewLine>Virtualization:        VT-x<br/><NewLine>L1d cache:             32K<br/><NewLine>L1i cache:             32K<br/><NewLine>L2 cache:              256K<br/><NewLine>L3 cache:              20480K<br/><NewLine>NUMA node0 CPU(s):     0,2,4,6,8,10,12,14<br/><NewLine>NUMA node1 CPU(s):     1,3,5,7,9,11,13,15<br/><NewLine>Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts md_clear flush_l1d</p><NewLine><p>I’m profiling resnet18 training code below. I don’t copy the code of printing loss and accuracy.</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine>import torchvision<NewLine>import torchvision.transforms as transforms<NewLine>import torchvision.models as models<NewLine><NewLine>transform_train = transforms.Compose([<NewLine>    transforms.RandomCrop(32, padding=4),<NewLine>    transforms.RandomHorizontalFlip(),<NewLine>    transforms.ToTensor(),<NewLine>    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),<NewLine>])<NewLine><NewLine>#transform_test = transforms.Compose([<NewLine>#    transforms.ToTensor(),<NewLine>#    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),<NewLine>#])<NewLine><NewLine>trainset = torchvision.datasets.CIFAR10(root='./data', train=True,<NewLine>                                        download=True, transform=transform_train)<NewLine>trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,<NewLine>                                          shuffle=True, num_workers=0)<NewLine>#testset = torchvision.datasets.CIFAR10(root='./data', train=False,<NewLine>#                                       download=True, transform=transform_test)<NewLine>#testloader = torch.utils.data.DataLoader(testset, batch_size=100,<NewLine>#                                         shuffle=False, num_workers=2)<NewLine><NewLine># get some random training images<NewLine>dataiter = iter(trainloader)<NewLine>images, labels = dataiter.next()<NewLine><NewLine># define network<NewLine>net = models.resnet18(pretrained=False)<NewLine><NewLine>criterion = nn.CrossEntropyLoss()<NewLine>optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)<NewLine><NewLine>for epoch in range(15):  # loop over the dataset multiple times<NewLine>    <NewLine>    running_loss = 0.0<NewLine><NewLine>    for i, data in enumerate(trainloader, 0):<NewLine><NewLine>        # get the inputs; data is a list of [inputs, labels]<NewLine>        inputs, labels = data<NewLine>        <NewLine>        # zero the parameter gradients<NewLine>        optimizer.zero_grad()<NewLine><NewLine>        # forward + backward + optimize<NewLine>        outputs = net(inputs)<NewLine>        loss = criterion(outputs, labels)<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine><NewLine>        # calculate loss<NewLine>        running_loss += loss.item()<NewLine></code></pre><NewLine><p>In my profiling result, I found that AVX dynamic codes (which are hotspots in my code) are mostly executed by 16 threads. (Total 48~49 threads are running, but 16 of them are terminated before training, and the other 16 of them are executing other codes) I have some interesting results. As I increase the number of training loops, some of CPU doesn’t work. I attached result images below with google drive link. Files numbered 1~4 are for epoch 5, 15, 25, and 50, respectively.</p><NewLine><p><a href=""https://drive.google.com/open?id=1dwy_DA6e6M9f9ruvOaR7-yFzOVvBD7jP"" rel=""nofollow noopener"">VTune Results</a></p><NewLine><p>The CPU Utilization metrics are 58.3%, 62.1%, 53%, and 49.4%, respectively. I think I have to mention some note. For epoch 50, I’ve profiled it twice because of the extremely low metric at the first time. It was 31.1%. The result image of this is in the link above, with the file name numbered 5.</p><NewLine><p>Is there anyone who could give me some insight about these results?</p><NewLine></div>",https://discuss.pytorch.org/u/jiseongg,(Jiseongg),jiseongg,"October 4, 2019,  8:01am",,,,,
56869,How can I get conv stride info in tracedModel,2019-09-26T07:45:21.951Z,1,170,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Friends,</p><NewLine><p>I use traced_model._modules[‘conv1’] to access conv module.<br/><NewLine>But how can I find ‘stride’ info in it?</p><NewLine><p>Thanks,<br/><NewLine>8086</p><NewLine></div>",https://discuss.pytorch.org/u/joe8086,(Joe8086),joe8086,"September 26, 2019,  7:45am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Would <code>print(traced_model._modules['conv1'].stride)</code> work?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks ptrblck, that’s a good guide.<br/><NewLine>But I did try still fail.<br/><NewLine>I want to able to access these fields after load traced file.<br/><NewLine>My test environment is at Windows pytorch 1.1.0</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torchvision<NewLine>m = torchvision.models.resnet18()<NewLine>t_m = torch.jit.trace(m, torch.rand(1, 3, 224, 224))<NewLine>m._modules['conv1'].stride<NewLine>#(2, 2)<NewLine>t_m._modules['conv1']<NewLine>#TracedModule[Conv2d]()<NewLine>t_m._modules['conv1'].stride<NewLine>---------------------------------------------------------------------------<NewLine># AttributeError                            Traceback (most recent call last)<NewLine># &lt;ipython-input-26-22862c1bf06c&gt; in &lt;module&gt;()<NewLine># ----&gt; 1 t_m._modules['conv1'].stride<NewLine><NewLine># c:\program files\python35\lib\site-packages\torch\jit\__init__.py in __getattr__(self, attr)<NewLine>   # 1230             if self._c._has_attribute(attr):<NewLine>   # 1231                 return self._c._get_attribute(attr)<NewLine># -&gt; 1232             return Module.__getattr__(self, attr)<NewLine>   # 1233 <NewLine>   # 1234         def __setattr__(self, attr, value):<NewLine><NewLine># c:\program files\python35\lib\site-packages\torch\nn\modules\module.py in __getattr__(self, name)<NewLine>    # 537                 return modules[name]<NewLine>    # 538         raise AttributeError(""'{}' object has no attribute '{}'"".format(<NewLine># --&gt; 539             type(self).__name__, name))<NewLine>    # 540 <NewLine>    # 541     def __setattr__(self, name, value):<NewLine><NewLine># AttributeError: 'TracedModule' object has no attribute 'stride'<NewLine><NewLine>torch.jit.save(t_m, 'traced_resnet18.pt')<NewLine>reload_t_m = torch.jit.load('traced_resnet18.pt')<NewLine>reload_t_m._modules['conv1']<NewLine>#ScriptModule()<NewLine>reload_t_m._modules['conv1'].stride<NewLine>---------------------------------------------------------------------------<NewLine># AttributeError                            Traceback (most recent call last)<NewLine># &lt;ipython-input-31-eae0d5d2cc24&gt; in &lt;module&gt;()<NewLine># ----&gt; 1 reload_t_m._modules['conv1'].stride<NewLine><NewLine># c:\program files\python35\lib\site-packages\torch\jit\__init__.py in __getattr__(self, attr)<NewLine>   # 1230             if self._c._has_attribute(attr):<NewLine>   # 1231                 return self._c._get_attribute(attr)<NewLine># -&gt; 1232             return Module.__getattr__(self, attr)<NewLine>   # 1233 <NewLine>   # 1234         def __setattr__(self, attr, value):<NewLine><NewLine># c:\program files\python35\lib\site-packages\torch\nn\modules\module.py in __getattr__(self, name)<NewLine>    # 537                 return modules[name]<NewLine>    # 538         raise AttributeError(""'{}' object has no attribute '{}'"".format(<NewLine># --&gt; 539             type(self).__name__, name))<NewLine>    # 540 <NewLine>    # 541     def __setattr__(self, name, value):<NewLine><NewLine># AttributeError: 'ScriptModule' object has no attribute 'stride'<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>And does ScriptModel have api to query module’s input list?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the code.<br/><NewLine>The attributes seems to be indeed hidden and I’m not sure if this is a bug or a feature. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""4"" data-topic=""56869""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/j/b5e925/40.png"" width=""20""/> joe8086:</div><NewLine><blockquote><NewLine><p>And does ScriptModel have api to query module’s input list?</p><NewLine></blockquote><NewLine></aside><NewLine><p>I’m not sure to understand the question completely. Could you give an example?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/joe8086; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/joe8086; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 26, 2019, 11:57am; <NewLine> REPLY_DATE 2: September 27, 2019,  2:37am; <NewLine> REPLY_DATE 3: September 27, 2019,  6:02am; <NewLine> REPLY_DATE 4: September 30, 2019,  8:34am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
57000,Trace Graph changes after saving/loading,2019-09-27T20:07:41.480Z,0,89,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m doing a simple add operation where the graph looks like:</p><NewLine><p>graph(%self : ClassType,<br/><NewLine>%1 : Float(1, 3, 224, 224)):<br/><NewLine>%2 : Long() = prim::Constant[value={1}]()<br/><NewLine>%3 : int = prim::Constant[value=1]()<br/><NewLine>%4 : Float(1, 3, 224, 224) = aten::add(%1, %2, %3)<br/><NewLine>return (%4)</p><NewLine><p>Which is correct but am seeing that after saving and loading the trace, it changes to:</p><NewLine><p>graph(%self : ClassType,<br/><NewLine>%argument_1.1 : Tensor):<br/><NewLine>%3 : Tensor = prim::Constant[value={1}]()<br/><NewLine>%4 : int = prim::Constant[value=1]()<br/><NewLine>%5 : Tensor = aten::add(%argument_1.1, %3, %4)<br/><NewLine>return (%5)</p><NewLine><p>Is there a reason it doesn’t preserve types and names? It seems like a statically allocated tensor is being converted to a dynamic one w/o any size/shape present? I am using torch.jit.save on the trace object and torch.jit.load. Am used to operating on IntType, etc but am not sure how to use TensorType.</p><NewLine></div>",https://discuss.pytorch.org/u/flynntax,,flynntax,"September 27, 2019,  8:44pm",,,,,
56675,Inconsistent results when load the weights from JIT ScriptModule to nn.Module,2019-09-24T01:55:14.019Z,0,264,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><pre><code class=""lang-auto"">jit_net = torch.jit.load(saved_path)   # load the pre-trained network defined as a ScriptModule<NewLine>nn_net = TheSameNet()    # this is the same network as jit_net but defined as a nn.Module<NewLine>nn_net.load_state_dict(jit_net.state_dict())<NewLine></code></pre><NewLine><p>I have a pre-trained ScriptModule and now I want to change some forward-pass function of it, therefore I define an exactly same nn.Module class and want to load the weights into this new network. I do this by the above code.<br/><NewLine>However, when I successfully load the pre-trained weights in the ScriptModule model to the nn.Module model, the “nn_net” outputs different results as the “jit_net” for the same inputs. I would like to know if there is a proper way to transfer the weights between jit-scriptmodule and nn.module, or do I miss something, or if it is potentially a bug for the inconsistent output, or is it just not recommened to do so (transfering weights between ScriptModule and nn.Module).</p><NewLine></div>",https://discuss.pytorch.org/u/Bingchen_Liu,(Bingchen Liu),Bingchen_Liu,"September 24, 2019,  1:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I also meet this kind inconsistent before.<br/><NewLine>So I give up use ScriptModel, instead I build a parser to parse ScriptModel.graph IR.<br/><NewLine>Then use the IR label to access state_dict<br/><NewLine>But the IR is not public release, it still not a good idea to handle this issue (because IR may change between different version)<br/><NewLine>I think this is a pytorch bug, need pytorch to fix it.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is not expected, no, but could happen if there is a source of non-determinism in the model. If you could file a Github issue with a repro, that would be very helpful. Ideally a simple model that we can run to produce an inconsistency.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/joe8086; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: September 27, 2019,  6:08am; <NewLine> REPLY_DATE 2: September 27, 2019,  6:40am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
56838,Export a LSTM with an embedding (unknow size input) layer,2019-09-25T21:14:40.037Z,0,90,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I’m trying to export in ONNX a model which received a list of index of word for a embedding layer in my model. The problem is I need a dummy input for the export and my input size will vary.</p><NewLine><p>How can I solved my problem ?</p><NewLine><p>tks</p><NewLine></div>",https://discuss.pytorch.org/u/davebulaval,(David Beauchemin),davebulaval,"September 25, 2019,  9:14pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found a work around by padding all my sentence to a fixed length.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/davebulaval; <NewLine> ,"REPLY_DATE 1: September 26, 2019,  4:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
56345, no viable conversion from &lsquo;script::Module&rsquo; to &lsquo;std::shared_ptrtorch::jit::script::Module&rsquo; std::shared_ptrtorch::jit::script::Module module = torch::jit::load(argv[1]);,2019-09-19T09:14:05.650Z,0,185,"<div class=""post"" itemprop=""articleBody""><NewLine><p>int main(int argc, const char* argv[]) {<br/><NewLine>if (argc != 2) {<br/><NewLine>std::cerr &lt;&lt; “usage: example-app \n”;<br/><NewLine>return -1;<br/><NewLine>}</p><NewLine><p>cv::VideoCapture stream(0);<br/><NewLine>cv::namedWindow(“Gesture Detect”, cv::WINDOW_AUTOSIZE);</p><NewLine><p>std::shared_ptrtorch::jit::script::Module module = torch::jit::load(argv[1]);</p><NewLine><p>module-&gt;to(at::kCUDA);<br/><NewLine>…</p><NewLine><p>it error fellow :</p><NewLine><p>/Applications/CLion.app/Contents/bin/cmake/mac/bin/cmake --build /Users/yuchao/CLionProjects/untitled/cmake-build-debug --target untitled – -j 2<br/><NewLine>Scanning dependencies of target untitled<br/><NewLine>[ 50%] Building CXX object CMakeFiles/untitled.dir/main.cpp.o<br/><NewLine>/Users/yuchao/CLionProjects/untitled/main.cpp:53:49: error: no viable conversion from ‘script::Module’ to ‘std::shared_ptrtorch::jit::script::Module’<br/><NewLine>std::shared_ptrtorch::jit::script::Module module = torch::jit::load(argv[1]);<br/><NewLine>^ ~~~~~~~~~~~~~~~~~~~~~~~~~<br/><NewLine>/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3753:23: note: candidate constructor not viable: no known conversion from ‘script::Module’ to ‘std::nullptr_t’ (aka ‘nullptr_t’) for 1st argument<br/><NewLine>_LIBCPP_CONSTEXPR shared_ptr(nullptr_t) _NOEXCEPT;<br/><NewLine>^<br/><NewLine>/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3767:5: note: candidate constructor not viable: no known conversion from ‘script::Module’ to ‘const std::__1::shared_ptrtorch::jit::script::Module &amp;’ for 1st argument<br/><NewLine>shared_ptr(const shared_ptr&amp; __r) _NOEXCEPT;<br/><NewLine>^<br/><NewLine>/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3775:5: note: candidate constructor not viable: no known conversion from ‘script::Module’ to ‘std::__1::shared_ptrtorch::jit::script::Module &amp;&amp;’ for 1st argument<br/><NewLine>shared_ptr(shared_ptr&amp;&amp; __r) _NOEXCEPT;<br/><NewLine>^<br/><NewLine>/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3770:9: note: candidate template ignored: could not match ‘shared_ptr’ against ‘torch::jit::script::Module’<br/><NewLine>shared_ptr(const shared_ptr&lt;_Yp&gt;&amp; __r,<br/><NewLine>^<br/><NewLine>/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3776:52: note: candidate template ignored: could not match ‘shared_ptr’ against ‘torch::jit::script::Module’<br/><NewLine>template _LIBCPP_INLINE_VISIBILITY shared_ptr(shared_ptr&lt;_Yp&gt;&amp;&amp; __r,<br/><NewLine>^<br/><NewLine>/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3785:9: note: candidate template ignored: could not match ‘auto_ptr’ against ‘torch::jit::script::Module’<br/><NewLine>shared_ptr(auto_ptr&lt;_Yp&gt;&amp;&amp; __r,<br/><NewLine>^<br/><NewLine>/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3795:9: note: candidate template ignored: could not match ‘unique_ptr&lt;type-parameter-0-0, type-parameter-0-1&gt;’ against ‘torch::jit::script::Module’<br/><NewLine>shared_ptr(unique_ptr&lt;_Yp, _Dp&gt;&amp;&amp;,<br/><NewLine>^<br/><NewLine>/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3804:9: note: candidate template ignored: could not match ‘unique_ptr&lt;type-parameter-0-0, type-parameter-0-1&gt;’ against ‘torch::jit::script::Module’<br/><NewLine>shared_ptr(unique_ptr&lt;_Yp, _Dp&gt;&amp;&amp;,<br/><NewLine>^<br/><NewLine>1 error generated.<br/><NewLine>make[3]: *** [CMakeFiles/untitled.dir/main.cpp.o] Error 1<br/><NewLine>make[2]: *** [CMakeFiles/untitled.dir/all] Error 2<br/><NewLine>make[1]: *** [CMakeFiles/untitled.dir/rule] Error 2<br/><NewLine>make: *** [untitled] Error 2</p><NewLine></div>",https://discuss.pytorch.org/u/yuchao86,(余超),yuchao86,"September 19, 2019,  9:14am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Why don’t you use <code>auto module = torch::jit::load(argv[1])</code> ?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>IIRC torch::jit::load now returns a Module rather than a shared_ptr in C++ frontend, starting from PyTorch 1.2 release,<br/><NewLine><code>script::Module</code>  is now a reference type, please see the release notes here <a href=""https://github.com/pytorch/pytorch/releases"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/releases</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/wanchaol; <NewLine> ,"REPLY_DATE 1: September 19, 2019,  1:52pm; <NewLine> REPLY_DATE 2: September 19, 2019,  8:11pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
56229,Is it recommended to convert a model into a traced or scripted one and then train it?,2019-09-18T04:27:14.656Z,0,104,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My question is as the title. From what I read, there is some speed-up benefit when converting a model to static graph so I suppose training a traced or scripted model is faster. Do you have any idea about this?</p><NewLine></div>",https://discuss.pytorch.org/u/justanhduc,(Duc Nguyen),justanhduc,"September 18, 2019,  4:27am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The JIT can bring speed benefits for some patterns (especially if your code has lots of unfused pointwise operations). See <a href=""https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/"" rel=""nofollow noopener"">this post</a> as an example.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: September 19, 2019,  5:17pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
56180,Find if a module is being traced,2019-09-17T17:45:28.702Z,1,220,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I would like to call a different function when tracing. I have registered it through<br/><NewLine><code>torch::jit::RegisterOperators()</code> and had to change it a little for that, so would like to call it only if it is tracing.<br/><NewLine>Is there a way to find if the module is being traced?<br/><NewLine>I currently rely on the fact that <code>tenssor.shape[0]</code> is a tensor while tracing, but an <code>int</code> otherwise. That is not the best way.</p><NewLine><p>Something like <code>torch.jit.is_running</code></p><NewLine></div>",https://discuss.pytorch.org/u/dashesy,(dashesy),dashesy,"September 17, 2019,  5:45pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi thanks for posting the question. We do have a method do know if it’s in tracing state or not using <code>torch._C._get_tracing_state()</code>, it will return true if you are in tracing otherwise false. But note that this is a internal API and it might break in the future (also unlikely).</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks! that would be very useful, a public facing API would be most very welcome. It will be better than relying on <code>if isinstance(bbs.shape[0], torch.Tensor)</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dashesy; <NewLine> ,"REPLY_DATE 1: September 17, 2019,  7:22pm; <NewLine> REPLY_DATE 2: September 17, 2019,  7:23pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
56078,Parallel execution of computation graph,2019-09-16T17:26:56.077Z,1,163,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry for some basic questions, but interpreter in JIT seems to run all the operators in a sequence. Can it be assumed that JIT based execution of computation-graphs are sequential unless we use “torch.script._fork” and “future.wait()” primitives? Or am I missing something basic <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/vdantu,(Vamshidhar Dantu (dan21c)),vdantu,"September 16, 2019,  5:26pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, that is correct</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the response <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vdantu; <NewLine> ,"REPLY_DATE 1: October 18, 2019, 12:09am; <NewLine> REPLY_DATE 2: September 17, 2019,  2:21am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
55898,Why doesn&rsquo;t script module serialization support returning multiple outputs?,2019-09-13T22:37:30.097Z,1,167,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi All,<br/><NewLine>I am curious to understand the reasoning behind <a href=""https://github.com/pytorch/pytorch/blob/v1.2.0/torch/csrc/jit/passes/python_print.cpp#L788"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/v1.2.0/torch/csrc/jit/passes/python_print.cpp#L788</a>? Why would jit module export not support multiple values?  I also see that the python-c++ interface is only returning at the last element in the stack vector (stack.back()) when we invoke a scriptmodule from python, but why this limitation?  Why not return all elements in the stack vector? I am surely missing more technical details and would love to understand this further.</p><NewLine><p>Thanks,</p><NewLine></div>",https://discuss.pytorch.org/u/vdantu,(Vamshidhar Dantu (dan21c)),vdantu,"September 13, 2019, 10:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We follow the same conventions as Python here, if a function has multiple return values they all get wrapped up into a tuple, so it’s actually only 1 return value.</p><NewLine><p>You can have as many returns as you want, they will just be wrapped into a tuple that you will have to unpack.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks. I think I understand now. I will dig into the code to look at where this creation of tuples happens on the C++ side <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>We have an open issue to document it better and make it a nicer experience: <a href=""https://github.com/pytorch/pytorch/issues/17165"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/17165</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vdantu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: September 13, 2019, 11:16pm; <NewLine> REPLY_DATE 2: September 14, 2019,  1:24am; <NewLine> REPLY_DATE 3: September 16, 2019,  6:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
55265,Why is GraphFuser a NonDiffOptimization?,2019-09-05T14:52:59.693Z,0,90,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In <a href=""https://github.com/pytorch/pytorch/blob/8554416a199c4cec01c60c7015d8301d2bb39b64/torch/csrc/jit/graph_executor.cpp#L548"" rel=""nofollow noopener"">graph_executor.cpp</a>,<br/><NewLine>If the graph to be optimized needs Gradient, <code>runNondiffOptimization(gradient.f)</code> will be called.</p><NewLine><p>runNondiffOptimization() runs some optimizations including BatchMM and FuseGraph.<br/><NewLine>I wonder why are these optimization non-differentiable?</p><NewLine><p>Take the <code>FuseGraph</code> as an example. It fuses continuous point-wise ops.<br/><NewLine>If it can fuse <code>f(g(x))</code>, why can’t it fuse <code>df(g(x)) * dg(x)</code>.</p><NewLine><p>Or it’s simply because the current implementation of FuseGraph does not support this?</p><NewLine></div>",https://discuss.pytorch.org/u/X.Guo,,X.Guo,"September 5, 2019,  2:52pm",,,,,
55152,"Torch.jit.trace unexpected error with `torch.cat(&hellip;, dim=-1)`",2019-09-04T14:59:18.315Z,1,349,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Find below a Minimum Reproducible Example that crashes both in Pytorch 1.1 and Pytorch 1.2 with CUDA (it works with CPU).</p><NewLine><pre><code class=""lang-auto""><NewLine><NewLine>import torch <NewLine>from torch import nn<NewLine><NewLine><NewLine>device = torch.device('cuda') # crashes with cuda, works with cpu<NewLine><NewLine><NewLine>class Model(nn.Module):<NewLine><NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.linear1 = nn.Linear(2, 16)<NewLine>        self.linear2 = nn.Linear(2, 16)<NewLine><NewLine>    def forward(self, x, y):<NewLine>        x = self.linear1(x)<NewLine>        y = self.linear2(y)<NewLine>        return torch.cat([x, y], dim=-1) # if we replace -1 with 1 works fine<NewLine><NewLine><NewLine>model = Model().to(device)<NewLine><NewLine>data = [torch.randn(1, 2).to(device), torch.randn(1, 2).to(device)] <NewLine><NewLine>traced = torch.jit.trace(model, data)<NewLine><NewLine>print(traced)<NewLine></code></pre><NewLine><p>Surprisingly the above works with CPU backend but not with CUDA backend. It also works when <code>torch.cat(..., dim=1)</code> but crashes with a negative dimension refering to the same one <code>torch.cat(..., dim=-1)</code>.</p><NewLine><p>Find the jit.trace error below (not very explanatory):</p><NewLine><pre><code class=""lang-auto"">torch.jit.TracingCheckError: Tracing failed sanity checks!<NewLine>Encountered an exception while running the trace with test inputs.<NewLine>Exception:<NewLine>        vector::_M_range_check: __n (which is 18446744073709551615) &gt;= this-&gt;size() (which is 2)<NewLine>        The above operation failed in interpreter, with the following stack trace:<NewLine><NewLine>                               <NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/imaluengo,(Imanol Luengo),imaluengo,"September 4, 2019,  3:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is a bug, you can track it in the corresponding <a href=""https://github.com/pytorch/pytorch/issues/25648"" rel=""nofollow noopener"">GitHub issue</a>, any updates / fixes that go in will get posted there.</p><NewLine><p>As a workaround you can wrap the negative index around manually with something like <code>dim=len(x.shape) + (-1)</code></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yup sorry! Realised that later and filled an issue with it. <code>x.ndim - 1</code> should also work.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/imaluengo; <NewLine> ,"REPLY_DATE 1: September 5, 2019,  7:27am; <NewLine> REPLY_DATE 2: September 5, 2019,  7:28am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
55111,How to change dynamic model to onnx?,2019-09-04T06:08:06.445Z,0,116,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to change <a href=""https://github.com/xingyizhou/ExtremeNet"" rel=""nofollow noopener"">Extremenet</a> to onnx, which programmed a dynamic model, I searched<br/><NewLine>1 <a href=""https://pytorch.org/docs/stable/onnx.html#tracing-vs-scripting"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/onnx.html#tracing-vs-scripting</a><br/><NewLine>2 <a href=""https://github.com/onnx/tutorials"" rel=""nofollow noopener"">https://github.com/onnx/tutorials</a><br/><NewLine>It seems not told how to change a dynamic pytorch model to onnx， where is the example of change dynamic model to onnx?<br/><NewLine>below is core dynamic code patch:</p><NewLine><pre><code class=""lang-auto"">class exkp(nn.Module):<NewLine>def __init__(<NewLine>    self, n, nstack, dims, modules, out_dim, pre=None, cnv_dim=256, <NewLine>    make_tl_layer=None, make_br_layer=None,<NewLine>    make_cnv_layer=make_cnv_layer, make_heat_layer=make_kp_layer,<NewLine>    make_tag_layer=make_kp_layer, make_regr_layer=make_kp_layer,<NewLine>    make_up_layer=make_layer, make_low_layer=make_layer, <NewLine>    make_hg_layer=make_layer, make_hg_layer_revr=make_layer_revr,<NewLine>    make_pool_layer=make_pool_layer, make_unpool_layer=make_unpool_layer,<NewLine>    make_merge_layer=make_merge_layer, make_inter_layer=make_inter_layer, <NewLine>    kp_layer=residual<NewLine>):<NewLine>    super(exkp, self).__init__()<NewLine>    self.nstack    = nstack<NewLine>    self._decode   = _exct_decode<NewLine><NewLine>    curr_dim = dims[0]<NewLine><NewLine>    self.pre = nn.Sequential(<NewLine>        convolution(7, 3, 128, stride=2),<NewLine>        residual(3, 128, 256, stride=2)<NewLine>    ) if pre is None else pre<NewLine><NewLine>    self.kps  = nn.ModuleList([<NewLine>        kp_module(<NewLine>            n, dims, modules, layer=kp_layer,<NewLine>            make_up_layer=make_up_layer,<NewLine>            make_low_layer=make_low_layer,<NewLine>            make_hg_layer=make_hg_layer,<NewLine>            make_hg_layer_revr=make_hg_layer_revr,<NewLine>            make_pool_layer=make_pool_layer,<NewLine>            make_unpool_layer=make_unpool_layer,<NewLine>            make_merge_layer=make_merge_layer<NewLine>        ) for _ in range(nstack)<NewLine>    ])<NewLine>    self.cnvs = nn.ModuleList([<NewLine>        make_cnv_layer(curr_dim, cnv_dim) for _ in range(nstack)<NewLine>    ])<NewLine><NewLine>    ## keypoint heatmaps<NewLine>    self.t_heats = nn.ModuleList([<NewLine>        make_heat_layer(cnv_dim, curr_dim, out_dim) for _ in range(nstack)<NewLine>    ])<NewLine><NewLine>    self.l_heats = nn.ModuleList([<NewLine>        make_heat_layer(cnv_dim, curr_dim, out_dim) for _ in range(nstack)<NewLine>    ])<NewLine><NewLine>    self.b_heats = nn.ModuleList([<NewLine>        make_heat_layer(cnv_dim, curr_dim, out_dim) for _ in range(nstack)<NewLine>    ])<NewLine><NewLine>    self.r_heats = nn.ModuleList([<NewLine>        make_heat_layer(cnv_dim, curr_dim, out_dim) for _ in range(nstack)<NewLine>    ])<NewLine><NewLine>    self.ct_heats = nn.ModuleList([<NewLine>        make_heat_layer(cnv_dim, curr_dim, out_dim) for _ in range(nstack)<NewLine>    ])<NewLine><NewLine>    for t_heat, l_heat, b_heat, r_heat, ct_heat in \<NewLine>      zip(self.t_heats, self.l_heats, self.b_heats, \<NewLine>          self.r_heats, self.ct_heats):<NewLine>        t_heat[-1].bias.data.fill_(-2.19)<NewLine>        l_heat[-1].bias.data.fill_(-2.19)<NewLine>        b_heat[-1].bias.data.fill_(-2.19)<NewLine>        r_heat[-1].bias.data.fill_(-2.19)<NewLine>        ct_heat[-1].bias.data.fill_(-2.19)<NewLine><NewLine>    self.inters = nn.ModuleList([<NewLine>        make_inter_layer(curr_dim) for _ in range(nstack - 1)<NewLine>    ])<NewLine><NewLine>    self.inters_ = nn.ModuleList([<NewLine>        nn.Sequential(<NewLine>            nn.Conv2d(curr_dim, curr_dim, (1, 1), bias=False),<NewLine>            nn.BatchNorm2d(curr_dim)<NewLine>        ) for _ in range(nstack - 1)<NewLine>    ])<NewLine>    self.cnvs_   = nn.ModuleList([<NewLine>        nn.Sequential(<NewLine>            nn.Conv2d(cnv_dim, curr_dim, (1, 1), bias=False),<NewLine>            nn.BatchNorm2d(curr_dim)<NewLine>        ) for _ in range(nstack - 1)<NewLine>    ])<NewLine><NewLine>    self.t_regrs = nn.ModuleList([<NewLine>        make_regr_layer(cnv_dim, curr_dim, 2) for _ in range(nstack)<NewLine>    ])<NewLine>    self.l_regrs = nn.ModuleList([<NewLine>        make_regr_layer(cnv_dim, curr_dim, 2) for _ in range(nstack)<NewLine>    ])<NewLine>    self.b_regrs = nn.ModuleList([<NewLine>        make_regr_layer(cnv_dim, curr_dim, 2) for _ in range(nstack)<NewLine>    ])<NewLine>    self.r_regrs = nn.ModuleList([<NewLine>        make_regr_layer(cnv_dim, curr_dim, 2) for _ in range(nstack)<NewLine>    ])<NewLine><NewLine>    self.relu = nn.ReLU(inplace=True)<NewLine><NewLine>def _train(self, *xs):<NewLine>    image  = xs[0]<NewLine>    t_inds = xs[1]<NewLine>    l_inds = xs[2]<NewLine>    b_inds = xs[3]<NewLine>    r_inds = xs[4]<NewLine><NewLine>    inter = self.pre(image)<NewLine>    outs  = []<NewLine><NewLine>    layers = zip(<NewLine>        self.kps, self.cnvs,<NewLine>        self.t_heats, self.l_heats, self.b_heats, self.r_heats,<NewLine>        self.ct_heats,<NewLine>        self.t_regrs, self.l_regrs, self.b_regrs, self.r_regrs,<NewLine>    )<NewLine>    for ind, layer in enumerate(layers):<NewLine>        kp_, cnv_          = layer[0:2]<NewLine>        t_heat_, l_heat_, b_heat_, r_heat_ = layer[2:6]<NewLine>        ct_heat_                           = layer[6]<NewLine>        t_regr_, l_regr_, b_regr_, r_regr_ = layer[7:11]<NewLine><NewLine>        kp  = kp_(inter)<NewLine>        cnv = cnv_(kp)<NewLine><NewLine>        t_heat, l_heat = t_heat_(cnv), l_heat_(cnv)<NewLine>        b_heat, r_heat = b_heat_(cnv), r_heat_(cnv)<NewLine>        ct_heat        = ct_heat_(cnv)<NewLine><NewLine>        t_regr, l_regr = t_regr_(cnv), l_regr_(cnv)<NewLine>        b_regr, r_regr = b_regr_(cnv), r_regr_(cnv)<NewLine><NewLine>        t_regr = _tranpose_and_gather_feat(t_regr, t_inds)<NewLine>        l_regr = _tranpose_and_gather_feat(l_regr, l_inds)<NewLine>        b_regr = _tranpose_and_gather_feat(b_regr, b_inds)<NewLine>        r_regr = _tranpose_and_gather_feat(r_regr, r_inds)<NewLine><NewLine>        outs += [t_heat, l_heat, b_heat, r_heat, ct_heat, \<NewLine>                 t_regr, l_regr, b_regr, r_regr]<NewLine><NewLine>        if ind &lt; self.nstack - 1:<NewLine>            inter = self.inters_[ind](inter) + self.cnvs_[ind](cnv)<NewLine>            inter = self.relu(inter)<NewLine>            inter = self.inters[ind](inter)<NewLine>    # print(""+++++++++++++++outs shape:"", outs[0].shape, outs[1].shape, outs[2].shape,outs[3].shape,outs[4].shape,outs[5].shape,outs[6].shape,outs[7].shape,outs[8].shape,)<NewLine>    # print(""+++++++++++++++outs shape:"", outs[9].shape, outs[10].shape, outs[11].shape,outs[12].shape,outs[13].shape,outs[14].shape,outs[15].shape,outs[16].shape,outs[17].shape,)<NewLine>    return outs<NewLine><NewLine># @torch.jit.script<NewLine>def _test(self, *xs, **kwargs):<NewLine>    image = xs[0]<NewLine><NewLine>    inter = self.pre(image)<NewLine>    outs  = []<NewLine><NewLine>    layers = zip(<NewLine>        self.kps, self.cnvs,<NewLine>        self.t_heats, self.l_heats, self.b_heats, self.r_heats,<NewLine>        self.ct_heats,<NewLine>        self.t_regrs, self.l_regrs, self.b_regrs, self.r_regrs,<NewLine>    )<NewLine>    for ind, layer in enumerate(layers):<NewLine>        kp_, cnv_                          = layer[0:2]<NewLine>        t_heat_, l_heat_, b_heat_, r_heat_ = layer[2:6]<NewLine>        ct_heat_                           = layer[6]<NewLine>        t_regr_, l_regr_, b_regr_, r_regr_ = layer[7:11]<NewLine><NewLine>        kp  = kp_(inter)<NewLine>        cnv = cnv_(kp)<NewLine><NewLine>        if ind == self.nstack - 1:<NewLine>            t_heat, l_heat = t_heat_(cnv), l_heat_(cnv)<NewLine>            b_heat, r_heat = b_heat_(cnv), r_heat_(cnv)<NewLine>            ct_heat        = ct_heat_(cnv)<NewLine><NewLine>            t_regr, l_regr = t_regr_(cnv), l_regr_(cnv)<NewLine>            b_regr, r_regr = b_regr_(cnv), r_regr_(cnv)<NewLine><NewLine>            outs += [t_heat, l_heat, b_heat, r_heat, ct_heat,<NewLine>                     t_regr, l_regr, b_regr, r_regr]<NewLine><NewLine>        if ind &lt; self.nstack - 1:<NewLine>            inter = self.inters_[ind](inter) + self.cnvs_[ind](cnv)<NewLine>            inter = self.relu(inter)<NewLine>            inter = self.inters[ind](inter)<NewLine>    # if kwargs['debug']:<NewLine>    #     _debug(image, t_heat, l_heat, b_heat, r_heat, ct_heat)<NewLine>    # del kwargs['debug']<NewLine>    # print(""output shape: "", self._decode(*outs[-9:], **kwargs).shape)<NewLine>    return self._decode(*outs[-9:], **kwargs)<NewLine>    # return outs[-9:]<NewLine><NewLine>def forward(self, *xs, **kwargs):<NewLine>    if len(xs) &gt; 1:<NewLine>        return self._train(*xs, **kwargs)<NewLine>    return self._test(*xs, **kwargs)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/111144,(轱辘 ),111144,"September 4, 2019,  6:08am",,,,,
54953,Tor.jit.trace to work on mask RCNN,2019-09-02T11:50:09.966Z,0,300,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is there any chance to have the torch.jit.trace instrunction work on a maskrcnn_resnet50_fpn model since it doesn’t seem to work yet, see <a href=""https://github.com/pytorch/vision/issues/1002"" rel=""nofollow noopener"">https://github.com/pytorch/vision/issues/1002</a> ?</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/TidelyPom,(Thierry),TidelyPom,"September 2, 2019, 11:50am",,,,,
54641,How do I pass a keyword argument to the forward used by a pre-forward hook?,2019-08-29T05:57:36.253Z,1,676,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Given a torch’s <code>nn.Module</code> with a pre-forward hook, e.g.</p><NewLine><pre><code>import torch<NewLine>import torch.nn as nn<NewLine><NewLine>class NeoEmbeddings(nn.Embedding):<NewLine>    def __init__(self, num_embeddings:int, embedding_dim:int, padding_idx=-1):<NewLine>        super().__init__(num_embeddings, embedding_dim, padding_idx)<NewLine>        self.register_forward_pre_hook(self.neo_genesis)<NewLine>    <NewLine>    @staticmethod<NewLine>    def neo_genesis(self, input, higgs_bosson=0):<NewLine>        if higgs_bosson:<NewLine>            input = input + higgs_bosson<NewLine>        return input<NewLine></code></pre><NewLine><p>It’s possible to let an input tensor go through some manipulation before going to the actual <code>forward()</code> function, e.g.</p><NewLine><pre><code>&gt;&gt;&gt; x = NeoEmbeddings(10, 5, 1)<NewLine>&gt;&gt;&gt; x.forward(torch.tensor([0,2,5,8]))<NewLine>tensor([[-1.6449,  0.5832, -0.0165, -1.3329,  0.6878],<NewLine>        [-0.3262,  0.5844,  0.6917,  0.1268,  2.1363],<NewLine>        [ 1.0772,  0.1748, -0.7131,  0.7405,  1.5733],<NewLine>        [ 0.7651,  0.4619,  0.4388, -0.2752, -0.3018]],<NewLine>       grad_fn=&lt;EmbeddingBackward&gt;)<NewLine><NewLine>&gt;&gt;&gt; print(x._forward_pre_hooks)<NewLine>OrderedDict([(25, &lt;function NeoEmbeddings.neo_genesis at 0x1208d10d0&gt;)])<NewLine></code></pre><NewLine><p><strong>How could we pass the arguments (<code>*args</code> or <code>**kwargs</code>) that the pre-forward hook needs but not accepted by the default <code>forward()</code> function?</strong></p><NewLine><p>Without modification/overriding the <code>forward()</code> function, this is not possible:</p><NewLine><pre><code>&gt;&gt;&gt; x = NeoEmbeddings(10, 5, 1)<NewLine>&gt;&gt;&gt; x.forward(torch.tensor([0,2,5,8]), higgs_bosson=2)<NewLine><NewLine>----------------------------------------------------<NewLine>TypeError                                 Traceback (most recent call last)<NewLine>&lt;ipython-input-102-8705a40a3cc2&gt; in &lt;module&gt;<NewLine>      1 x = NeoEmbeddings(10, 5, 1)<NewLine>----&gt; 2 x.forward(torch.tensor([0,2,5,8]), higgs_bosson=2)<NewLine><NewLine>TypeError: forward() got an unexpected keyword argument 'higgs_bosson'</code></pre><NewLine></div>",https://discuss.pytorch.org/u/alvations,,alvations,"August 29, 2019,  5:57am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Also on <a href=""https://stackoverflow.com/questions/57703808/how-do-i-pass-a-keyword-argument-to-the-forward-used-by-a-pre-forward-hook"" rel=""nofollow noopener"">https://stackoverflow.com/questions/57703808/how-do-i-pass-a-keyword-argument-to-the-forward-used-by-a-pre-forward-hook</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Why is the forward pre-hook necessary here? Why not include it at the beginning of the forward?<br/><NewLine>Or if you want to inherit the forward from the parent class, create a new forward, do the preprocessing and then call the parent forward with <code>super().forward(args)</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alvations; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: August 29, 2019,  5:58am; <NewLine> REPLY_DATE 2: August 30, 2019,  9:08pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
54504,Python2 TorchScript type annotation tuple vs Tuple,2019-08-28T00:19:05.009Z,1,187,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a torch script that accepts a tuple[Tensor],</p><NewLine><pre><code class=""lang-python"">@torch.jit.script<NewLine>def smt_pred(confs, child, child_sizes, threshold, obj,<NewLine>             height, width, a):<NewLine>    # type: (Tuple[Tensor],Tuple[Tensor],Tuple[Tensor],Tensor,Tensor,Tensor,Tensor,Tensor) -&gt; Tuple[Tensor,Tensor]<NewLine><NewLine></code></pre><NewLine><p>When I call the code in Python2 I get the error</p><NewLine><blockquote><NewLine><p>RuntimeError: smt_pred() Expected a value of type ‘Tuple[Tensor]’ for argument ‘confs’ but instead found type ‘tuple’.</p><NewLine></blockquote><NewLine><p><code>tuple</code> is same as <code>Tuple</code> and I could not find how to annotate confs to be tuple of tensors. Is it possible in Python2?</p><NewLine><hr/><NewLine><p>Changing <code>Tuple</code> to <code>List</code> works in Python2, so this is only about <code>Tuple</code></p><NewLine></div>",https://discuss.pytorch.org/u/dashesy,(dashesy),dashesy,"August 28, 2019, 12:42am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Added an issue <a href=""https://github.com/pytorch/pytorch/issues/25329"" rel=""nofollow noopener"">here</a> because <code>List</code> works (and I just have to pass <code>list(confs)</code> as a workaround)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Copied from the issue:</p><NewLine><p>I’m guessing that you’re passing in a tuple that doesn’t look like <code>(torch.ones(2, 2),)</code>. For a <code>Tuple</code> the type needs to be fixed and completely specified and the elements can be different types. In TorchScript, <code>a</code>, <code>b</code>, and <code>c</code> below are all different types</p><NewLine><pre><code class=""lang-python""># type is `Tuple[int, int, int]<NewLine>a = (1, 2, 3)<NewLine><NewLine># type is `Tuple[str, int, int]<NewLine>b = ('hi', 2, 3)<NewLine><NewLine># type is `Tuple[int]`<NewLine>c = (2,)<NewLine></code></pre><NewLine><p>For lists, the element types must all be the same, which is why it can be any length but you only need to specify <code>List[Tensor]</code> or <code>List[int]</code>. You can read more about it <a href=""https://pytorch.org/docs/master/jit.html#supported-type"" rel=""nofollow noopener"">here</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dashesy; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: August 28, 2019,  5:58pm; <NewLine> REPLY_DATE 2: August 28, 2019,  9:05pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
54373,Problem with mixing torch.jit.script and torch.jit.trace,2019-08-26T16:38:52.594Z,1,412,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I am using torch.jit.script to export my models but I encountered a problem with Resnet. This is my model definition:</p><NewLine><pre><code class=""lang-auto"">class Model(nn.Module):<NewLine>    def __init__(self, num_cats):<NewLine>        super(Model, self).__init__()    <NewLine>        self.model = torchvision.models.resnet18(pretrained=True)<NewLine>        self.model.fc = nn.Linear(self.model.fc.in_features, num_cats)<NewLine>        self.model = torch.jit.trace(self.model, torch.rand(1,3,224,224))<NewLine>    def forward(self, x):<NewLine>        x = self.model(x)<NewLine>        return x<NewLine></code></pre><NewLine><p>Training works fine and validation accuracy goes as expected. Finally I export my model like this</p><NewLine><pre><code class=""lang-auto"">scripted = torch.jit.script(model)<NewLine>torch.jit.save(scripted, 'scripted_model.pth')<NewLine></code></pre><NewLine><p>The thing is that when I load my model</p><NewLine><pre><code class=""lang-auto"">model = torch.jit.load('scripted_model.pth')<NewLine></code></pre><NewLine><p>and use it to perform inference on the entire validation set with the same batch size than the one used for training I get the same predictions. But if I try to perform inference on a single image (or in the validation set with a small batch size), I get totally wrong predictions.</p><NewLine><p>I’ve tried the same example with a custom CNN and I get good results. Also I tried exporting the resnet model with torch.jit.trace and torch.jit.trace_module (instead of torch.jit.script) and also get good results.</p><NewLine><p>I would appreciate any help on the issue, since I find the functionality of torch.jit.script very powerful in production.</p><NewLine></div>",https://discuss.pytorch.org/u/Juan_Sensio,(Juan Sensio),Juan_Sensio,"August 27, 2019, 11:20am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the minimal repro! This sounds like a bug, would you mind filing an <a href=""https://github.com/pytorch/pytorch"" rel=""nofollow noopener"">issue on GitHub</a> and adding some info about your environment so we can track it better?</p><NewLine><p>From a first look I’m not getting any difference between eager mode, the original scripted model, and the loaded scripted model (<a href=""https://gist.github.com/driazati/a4c26bf6660e97c066b2cd1faba295cd"" rel=""nofollow noopener"">script here</a>). Since <code>self.model</code> here is a traced graph the compilation should also be very simple since all it would really compile is the <code>return self.model(x)</code> statement, so I’m not sure why the results would be different.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, you are right there is no difference between the models. The thing is that they all are wrong. Consider the following models:</p><NewLine><pre><code class=""lang-auto"">class Model1(nn.Module):<NewLine>    def __init__(self, num_cats):<NewLine>        super(Model1, self).__init__()    <NewLine>        self.model = torchvision.models.resnet18(pretrained=True)<NewLine>        self.model.fc = nn.Linear(self.model.fc.in_features, num_cats)<NewLine>        self.model = torch.jit.trace(self.model, torch.rand(1,3,224,224))<NewLine>    def forward(self, x):<NewLine>        x = self.model(x)<NewLine>        return x<NewLine><NewLine>class Model2(nn.Module):<NewLine>    def __init__(self, num_cats):<NewLine>        super(Model2, self).__init__()    <NewLine>        self.model = torchvision.models.resnet18(pretrained=True)<NewLine>        self.model.fc = nn.Linear(self.model.fc.in_features, num_cats)<NewLine>    def forward(self, x):<NewLine>        x = self.model(x)<NewLine>        return x<NewLine></code></pre><NewLine><p>I can train both models with the same dataset and hyperparameters and achieve similar expected results (good results). I can then export them as follows:</p><NewLine><pre><code class=""lang-auto"">scripted1 = torch.jit.script(model1)<NewLine>torch.jit.save(scripted1, 'scripted_model1.pth')<NewLine><NewLine>scripted2 = torch.jit.trace(model2)<NewLine>torch.jit.save(scripted2, 'scripted_model2.pth')<NewLine></code></pre><NewLine><p>And load them</p><NewLine><pre><code class=""lang-auto"">loaded1 = torch.jit.load('scripted_model1.pth')<NewLine>loaded2 = torch.jit.load('scripted_model2.pth')<NewLine></code></pre><NewLine><p>In all cases the 3 versions of each model give the same results, the difference is that using the first version only works fine when performing inference with a large batch size (and failing in single image inference). The second model, however, gives good predictions in all cases.</p><NewLine><p>PS: I tried the same experiment than the one proposed here but with a custom CNN in place of resnet18 and I observe the same problem. Hence it seems to be something related with the interaction between torch.jit.script and torch.jit.trace.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Juan_Sensio; <NewLine> ,"REPLY_DATE 1: August 26, 2019,  5:46pm; <NewLine> REPLY_DATE 2: August 27, 2019, 11:05am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
53904,Type Error while Scripting Learned Positional Embedding,2019-08-21T07:58:24.937Z,1,170,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to script LearnedPositionalEmbedding from Fairseq:<br/><NewLine><code>CODE</code></p><NewLine><pre><code class=""lang-auto"">import torch.nn as nn<NewLine>import torch<NewLine>import torch.jit<NewLine>from fairseq import utils<NewLine><NewLine>class LearnedPositionalEmbedding(nn.Embedding):<NewLine>    def __init__(self, num_embeddings, embedding_dim, padding_idx, left_pad):<NewLine>        super().__init__(num_embeddings, embedding_dim, padding_idx)<NewLine>        self.left_pad = left_pad<NewLine>        # self.register_buffer('padding_idx_', padding_idx)<NewLine><NewLine>    def forward(self, input_, incremental_state=None):<NewLine>        """"""Input is expected to be of size [bsz x seqlen].""""""<NewLine>        if incremental_state is not None:<NewLine>            positions = input_.data.new(1, 1).fill_(self.padding_idx_ + input_.size(1))<NewLine>        else:<NewLine>            positions = utils.make_positions(input_.data, self.padding_idx_, self.left_pad)<NewLine>        return super().forward(positions)<NewLine><NewLine>    def max_positions(self):<NewLine>        """"""Maximum number of supported positions.""""""<NewLine>        return self.num_embeddings - self.padding_idx_ - 1<NewLine><NewLine>padding_idx = torch.tensor([[1]], dtype=torch.long)<NewLine>model = LearnedPositionalEmbedding(4,5, padding_idx, False)<NewLine>model_scripted = torch.jit.script(model)<NewLine></code></pre><NewLine><p>And I get following error:</p><NewLine><pre><code class=""lang-auto"">TypeError: <NewLine>'Tensor' object for attribute 'padding_idx' is not a valid constant.<NewLine>Valid constants are:<NewLine>  1. a nn.ModuleList<NewLine>  2. a value of type {bool, float, int, str, NoneType, function, device, layout, dtype}<NewLine>  3. a list or tuple of (2)<NewLine></code></pre><NewLine><p>Even issue <a href=""https://github.com/pytorch/pytorch/issues/16284"" rel=""nofollow noopener"">#16284</a> is not of help.</p><NewLine></div>",https://discuss.pytorch.org/u/sukuya,(Sukuya),sukuya,"August 21, 2019,  7:58am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>In the <code>nn.Embedding</code> provided by PyTorch, <code>padding_idx</code> is an <code>int</code> that doesn’t change, for that reason we have added it to <code>__constants__</code> in <code>nn.Embedding</code> (<a href=""https://github.com/pytorch/pytorch/blob/10c4b98ade8349d841518d22f19a653a939e260c/torch/nn/modules/sparse.py#L77-L78"" rel=""nofollow noopener"">code here</a>). Since <code>LearnedPositionalEmbedding</code> does not override <code>__constants__</code>, it gets the one from <code>nn.Embedding</code>.</p><NewLine><p>If you want <code>padding_idx</code> to be a Tensor (<a href=""https://pytorch.org/docs/master/jit.html#python-defined-constants"" rel=""nofollow noopener"">which is not a supported constant type</a>), you’ll have to provide your own <code>__constants__</code> that removes <code>padding_idx</code> but keeps the other stuff around, something like:</p><NewLine><pre><code class=""lang-python"">from fairseq import utils<NewLine><NewLine>class LearnedPositionalEmbedding(nn.Embedding):<NewLine>    __constants__ = ['num_embeddings', 'embedding_dim', 'max_norm',<NewLine>                     'norm_type', 'scale_grad_by_freq', 'sparse']<NewLine>    def __init__(self, num_embeddings, embedding_dim, padding_idx, left_pad):<NewLine>        super().__init__(num_embeddings, embedding_dim, padding_idx)<NewLine>        self.left_pad = left_pad</code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: August 23, 2019,  9:31pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
51884,Self Attention Layer Export using Torch Script,2019-07-29T06:11:55.122Z,1,436,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I get the below error when trying to trace the following code.</p><NewLine><pre><code class=""lang-auto"">frame #0: std::function&lt;std::string ()&gt;::operator()() const + 0x11 (0x7f3e4b04d441 in /bigdisk2/sunil/pytorchnightly/lib/python3.6/site-packages/torch/lib/libc10.so)<NewLine>frame #1: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x2a (0x7f3e4b04cd7a in /bigdisk2/sunil/pytorchnightly/lib/python3.6/site-packages/torch/lib/libc10.so)<NewLine></code></pre><NewLine><p>The code I am trying is this:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.jit<NewLine>import copy<NewLine>from torch.nn import functional as F<NewLine>from torch.nn import Module<NewLine>from torch.nn import MultiheadAttention<NewLine>from torch.nn import ModuleList<NewLine>from torch.nn.init import xavier_uniform_<NewLine>from torch.nn import Dropout<NewLine>from torch.nn import Linear<NewLine>from torch.nn import LayerNorm<NewLine><NewLine>class TransformerEncoderLayer(Module):<NewLine>    r""""""TransformerEncoderLayer is made up of self-attn and feedforward network.<NewLine>    This standard encoder layer is based on the paper ""Attention Is All You Need"".<NewLine>    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,<NewLine>    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in<NewLine>    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement<NewLine>    in a different way during application.<NewLine><NewLine>    Args:<NewLine>        d_model: the number of expected features in the input (required).<NewLine>        nhead: the number of heads in the multiheadattention models (required).<NewLine>        dim_feedforward: the dimension of the feedforward network model (default=2048).<NewLine>        dropout: the dropout value (default=0.1).<NewLine><NewLine>    Examples::<NewLine>        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)<NewLine>    """"""<NewLine><NewLine>    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):<NewLine>        super(TransformerEncoderLayer, self).__init__()<NewLine>        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)<NewLine>        # Implementation of Feedforward model<NewLine>        self.linear1 = Linear(d_model, dim_feedforward)<NewLine>        self.dropout = Dropout(dropout)<NewLine>        self.linear2 = Linear(dim_feedforward, d_model)<NewLine><NewLine>        self.norm1 = LayerNorm(d_model)<NewLine>        self.norm2 = LayerNorm(d_model)<NewLine>        self.dropout1 = Dropout(dropout)<NewLine>        self.dropout2 = Dropout(dropout)<NewLine><NewLine>    def forward(self, src, src_mask=None, src_key_padding_mask=None):<NewLine>        r""""""Pass the input through the endocder layer.<NewLine><NewLine>        Args:<NewLine>            src: the sequnce to the encoder layer (required).<NewLine>            src_mask: the mask for the src sequence (optional).<NewLine>            src_key_padding_mask: the mask for the src keys per batch (optional).<NewLine><NewLine>        Shape:<NewLine>            see the docs in Transformer class.<NewLine>        """"""<NewLine>        # src2 = self.self_attn(src, src, src, attn_mask=src_mask,<NewLine>                            #   key_padding_mask=src_key_padding_mask)[0]<NewLine>        # src = src + self.dropout1(src2)<NewLine>        src = self.norm1(src)<NewLine>        # print(src)<NewLine>        # src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))<NewLine>        # src = src + self.dropout2(src2)<NewLine>        src = self.norm2(src)<NewLine>        # print(src)<NewLine>        return src<NewLine><NewLine><NewLine>layerencoder = TransformerEncoderLayer(512,1).to('cuda')<NewLine>input = torch.rand(1,1,512).to('cuda')<NewLine>print(layerencoder(input).shape)<NewLine>layerencoder.eval()<NewLine>traced_model  = torch.jit.trace(layerencoder,input)<NewLine></code></pre><NewLine><p>If I remove the second layer normalization, I am able to trace. I am using PyTorch nightly build.</p><NewLine></div>",https://discuss.pytorch.org/u/sukuya,(Sukuya),sukuya,"July 31, 2019,  7:15am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the report! Can you file an issue on github and we can track it from there? cc <a class=""mention"" href=""/u/wanchaol"">@wanchaol</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/michael_suo"">@Michael_Suo</a> I updated my cuda_driver to 10 and things are working fine now for me. But I am getting issues with dropout layers. I will create an issue with that on weekend.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just an update: Drop out also works fine with PyTorch1.2.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/sukuya; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/sukuya; <NewLine> ,"REPLY_DATE 1: August 1, 2019,  7:33am; <NewLine> REPLY_DATE 2: August 1, 2019,  7:33am; <NewLine> REPLY_DATE 3: August 21, 2019,  8:30am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
53487,Alias specs in signatures in PyTorch 1.2,2019-08-15T20:57:52.629Z,2,328,"<div class=""post"" itemprop=""articleBody""><NewLine><p>With the PyTorch 1.2, I have noticed that any alias specification when registering an operator would cause a crash (tried on Win64).<br/><NewLine>So I had to change</p><NewLine><pre><code class=""lang-auto"">""some_op(Tensor(a) t) -&gt; Tensor(a)""<NewLine></code></pre><NewLine><p>to</p><NewLine><pre><code class=""lang-auto"">""some_op(Tensor t) -&gt; Tensor""<NewLine></code></pre><NewLine><p>to make it work again.</p><NewLine><p>Is there an alternative?</p><NewLine></div>",https://discuss.pytorch.org/u/leowalkling,,leowalkling,"August 15, 2019,  8:57pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you have a minimal code example reproducing the crash? And what is the error message you’re seeing exactly?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The error message is simply:<br/><NewLine>OSError: [WinError 1114] A dynamic link library (DLL) initialization routine failed<br/><NewLine>I have created a repo for it:<br/><NewLine><a href=""https://github.com/leowalkling/pytorch_register_op_minimal"" rel=""nofollow noopener"">https://github.com/leowalkling/pytorch_register_op_minimal</a><br/><NewLine>I’m using scikit-build to easily generate a solution file but the compilation flags are taken from <code>torch.utils.cpp_extension</code>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found out, that in PyTorch 1.2, the default behavior is not to allow alias specifications, leading to a segfault.<br/><NewLine>This can be changed, but alias specifications are not allowed for third-party extensions as stated in the error message:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: node-&gt;kind().is_prim() || node-&gt;kind().is_aten() INTERNAL ASSERT FAILED at ..\torch\csrc\jit\passes\alias_analysis.cpp:392, please report a bug to PyTorch. The current code base should only have AliasAnalysisKind::FROM_SCHEMA for aten:: and prim:: ops but we found it for tp::broadcast_tensors. We want to open this up though. (analyzeImpl at ..\torch\csrc\jit\passes\alias_analysis.cpp:392)<NewLine>(no backtrace available)<NewLine></code></pre><NewLine><p>To get this error message, I passed an Options struct as a third (optional) argument to RegisterOperators.op(), i.e.:</p><NewLine><pre><code class=""lang-auto"">torch::RegisterOperators()<NewLine>.op(<NewLine>""some_op(Tensor(a) t) -&gt; Tensor(a)"",<NewLine>&amp;some_op,<NewLine>torch::RegisterOperators::options().aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Sebastian_Messmer; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/leowalkling; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/leowalkling; <NewLine> ,"REPLY_DATE 1: August 15, 2019, 10:09pm; <NewLine> REPLY_DATE 2: August 16, 2019, 10:24pm; <NewLine> REPLY_DATE 3: August 19, 2019,  1:09pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
53263,Torch.jit isuue with submodules(Resnet),2019-08-13T09:07:16.940Z,0,335,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I created a model that uses two Resnets(pre-trained) as feature extractors<br/><NewLine>In my model I used a pre-trained Resnet as a sub module.</p><NewLine><p>Training and testing work well in pytorch, but<br/><NewLine>If you use simple torch.jit.trace while converting torch, the following error occurs.</p><NewLine><p><strong>could not export python function call Scatter. Remove calls to Python functions before export. Did you forget add <span class=""mention"">@script</span> or <span class=""mention"">@script_method</span> annotation? If this is a nn.ModuleList, add it to  <strong>constants</strong> .:</strong></p><NewLine><p>if i want to use sub_module, it looks like i need to use trace and script together, as shown in the code below from the official pytorch homepage.<br/><NewLine>##################################</p><NewLine><p>import torch<br/><NewLine>import torchvision<br/><NewLine>class MyScriptModule(torch.nn.Module):<br/><NewLine>def <strong>init</strong>(self):<br/><NewLine>super(MyScriptModule, self).<strong>init</strong>()<br/><NewLine>self.means = torch.nn.Parameter(torch.tensor([103.939, 116.779, 123.68])<br/><NewLine>.resize_(1, 3, 1, 1))<br/><NewLine>self.resnet = torch.jit.trace(torchvision.models.resnet18(),<br/><NewLine>torch.rand(1, 3, 224, 224))<br/><NewLine>def forward(self, input):<br/><NewLine>return self.resnet(input - self.means)<br/><NewLine>my_script_module = torch.jit.script(MyScriptModule())</p><NewLine><p>########################################################</p><NewLine><p>but, This also causes an error.</p><NewLine><p>Please advise if this approach is correct or if you can make a simple trace.</p><NewLine><p>plz…</p><NewLine></div>",https://discuss.pytorch.org/u/kcw4875,(chanwoong Kwak),kcw4875,"August 13, 2019,  9:08am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The code snippet from the website you posted works fine for me (on PyTorch 1.2, are you up to date?), can you post a full code example that results in the <code>could not export Python function..</code> error?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: August 16, 2019, 11:05pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
52650,Why does the same python model code behave differently in scripting and not scripting?,2019-08-06T08:54:34.428Z,0,144,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I want to script a model, it comes a warning message:  <strong>RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().</strong> But if I don’t script it and just run it the warning message won’t appear. I wonder whether it is a BUG? And Could it be solved?</p><NewLine></div>",https://discuss.pytorch.org/u/kevinhaoliu,(kevin),kevinhaoliu,"August 6, 2019,  8:57am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have the same problem with this <a href=""https://github.com/pytorch/pytorch/issues/10234"" rel=""nofollow noopener"">Github issue</a> when run the model  in C++ on GPU. If I don’t run it on GPU it will work well.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>My pytorch version is 1.1. CUDA version is 9.0, CUDNN is 7.3.0. Can anyone give me a help?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/kevinhaoliu; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kevinhaoliu; <NewLine> ,"REPLY_DATE 1: August 6, 2019,  9:55am; <NewLine> REPLY_DATE 2: August 14, 2019,  7:10am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
53004,How to upgrade fastrnn&rsquo;s LayerNormLSTM to support PackedSequence?,2019-08-09T15:22:41.168Z,0,221,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In <a href=""https://github.com/pytorch/benchmark/blob/master/rnns/fastrnns/custom_lstms.py"" rel=""nofollow noopener"">fastrnn’s custom LSTM</a>, we could conveniently define JIT LayerNormLSTM. I am wondering that how could we also make it support PackedSequence as input data for varied lengths of tensors ?</p><NewLine></div>",https://discuss.pytorch.org/u/Xingdong_Zuo,(Xingdong Zuo),Xingdong_Zuo,"August 12, 2019,  1:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This should be supported:</p><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/21282"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://github.com/julioasotodv"" rel=""nofollow noopener""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars1.githubusercontent.com/u/20630819?v=2&amp;s=96"" width=""60""/><NewLine></a><NewLine><h4><a href=""https://github.com/pytorch/pytorch/issues/21282"" rel=""nofollow noopener"" target=""_blank"">Issue: TorchScript support for torch.nn.utils.rnn.pack_padded_sequence() and torch.nn.utils.rnn.pad_packed_sequence()</a></h4><NewLine><div class=""date"" style=""margin-top:10px;""><NewLine><div class=""user"" style=""margin-top:10px;""><NewLine>	opened by <a href=""https://github.com/julioasotodv"" rel=""nofollow noopener"" target=""_blank"">julioasotodv</a><NewLine>	on <a href=""https://github.com/pytorch/pytorch/issues/21282"" rel=""nofollow noopener"" target=""_blank"">2019-06-03</a><NewLine></div><NewLine><div class=""user""><NewLine>	closed by <a href=""https://github.com/driazati"" rel=""nofollow noopener"" target=""_blank"">driazati</a><NewLine>	on <a href=""https://github.com/pytorch/pytorch/issues/21282"" rel=""nofollow noopener"" target=""_blank"">2019-08-07</a><NewLine></div><NewLine></div><NewLine><pre class=""content"" style=""white-space: pre-wrap;"">🚀 Feature<NewLine>Add TorchScript support for torch.nn.utils.rnn.pack_padded_sequence() and torch.nn.utils.rnn.pad_packed_sequence().<NewLine>Motivation<NewLine>Motivated by https://discuss.pytorch.org/t/torch-jit-script-for-torch-nn-utils-rnn-pack-padded-sequence/30668, this appears to be a desired feature given that tracing is...</pre><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">jit</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/eellison; <NewLine> ,"REPLY_DATE 1: August 17, 2019,  8:01am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
53149,"Pytorch Error occur, when converting my custom model to torch",2019-08-12T08:13:17.683Z,0,143,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I trained the model for the experiment, I did a test on pytorch well.<br/><NewLine>but ,The following error occurs when converting and saving through torch.jit.trace.</p><NewLine><p>Full error phrase :</p><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “D:/Landmark/pytorch/convert2C_merge_model.py”, line 190, in <br/><NewLine>traced_script_module.save(""./model/output.pt"")<br/><NewLine>File “C:\Users\kcw\Anaconda3\envs\pytorch\lib\site-packages\torch\jit_<em>init</em>_.py”, line 1205, in save<br/><NewLine>return self._c.save(*args, **kwargs)<br/><NewLine>RuntimeError:<br/><NewLine>could not export python function call Scatter. Remove calls to Python functions before export. Did you forget add <span class=""mention"">@script</span> or <span class=""mention"">@script_method</span> annotation? If this is a nn.ModuleList, add it to <strong>constants</strong>.:<br/><NewLine>C:\Users\kcw\Anaconda3\envs\pytorch\lib\site-packages\torch\nn\parallel\scatter_gather.py(13): scatter_map<br/><NewLine>C:\Users\kcw\Anaconda3\envs\pytorch\lib\site-packages\torch\nn\parallel\scatter_gather.py(15): scatter_map<br/><NewLine>C:\Users\kcw\Anaconda3\envs\pytorch\lib\site-packages\torch\nn\parallel\scatter_gather.py(28): scatter<br/><NewLine>C:\Users\kcw\Anaconda3\envs\pytorch\lib\site-packages\torch\nn\parallel\scatter_gather.py(35): scatter_kwargs<br/><NewLine>C:\Users\kcw\Anaconda3\envs\pytorch\lib\site-packages\torch\nn\parallel\data_parallel.py(159): scatter<br/><NewLine>C:\Users\kcw\Anaconda3\envs\pytorch\lib\site-packages\torch\nn\parallel\data_parallel.py(148): forward<br/><NewLine>C:\Users\kcw\Anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py(481): _slow_forward<br/><NewLine>C:\Users\kcw\Anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py(491): <strong>call</strong><br/><NewLine>D:/Landmark/pytorch/convert2C_merge_model.py(74): forward<br/><NewLine>C:\Users\kcw\Anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py(481): <em>slow_forward<br/><NewLine>C:\Users\kcw\Anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py(491): <strong>call</strong><br/><NewLine>C:\Users\kcw\Anaconda3\envs\pytorch\lib\site-packages\torch\jit_<em>init</em></em>.py(688): trace<br/><NewLine>D:/Landmark/pytorch/convert2C_merge_model.py(187): </p><NewLine><p>help me… plz…</p><NewLine></div>",https://discuss.pytorch.org/u/kcw4875,(chanwoong Kwak),kcw4875,"August 12, 2019,  8:31am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you pleas post a repro and/or file a github issue?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/eellison; <NewLine> ,"REPLY_DATE 1: August 12, 2019,  5:07pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
53100,TorchScript: indexing question / filling NaNs,2019-08-11T11:45:26.200Z,1,474,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!</p><NewLine><p>I’m trying to <code>jit.script</code>-compile a model which uses the <code>t[t != t] = ...</code> trick to fill the nans of a tensor with a default value. However, torchscript does not seem to appreciate this kind of indexing. Does anyone know a solution/workaround?</p><NewLine><p>Thank you!</p><NewLine><p>Here’s a small example plus error message:</p><NewLine><pre><code class=""lang-python""> @torch.jit.script <NewLine> def f(): <NewLine>     t = torch.tensor(0.) <NewLine>     t[t!=t] = 7 <NewLine></code></pre><NewLine><pre><code class=""lang-nohighlight"">RuntimeError: <NewLine>Arguments for call are not valid.<NewLine>The following operator variants are available:<NewLine>  <NewLine>  aten::index_put_(Tensor(a) self, Tensor?[] indices, Tensor values, bool accumulate=False) -&gt; (Tensor(a)):<NewLine>  Expected a value of type 'Tensor' for argument 'values' but instead found type 'int'.<NewLine>  <NewLine>  aten::index_put_(Tensor(a) self, Tensor[] indices, Tensor values, bool accumulate=False) -&gt; (Tensor(a)):<NewLine>  Expected a value of type 'List[Tensor]' for argument 'indices' but instead found type 'List[Optional[Tensor]]'.<NewLine><NewLine>The original call is:<NewLine>at &lt;ipython-input-21-45d08c8a21f7&gt;:4:5<NewLine>@torch.jit.script<NewLine>def f():<NewLine>    t = torch.tensor(0.)<NewLine>    t[t!=t] = 7<NewLine>    ~~~~~~~~~~~ &lt;--- HERE<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/achim,(Achim Passen),achim,"August 11, 2019, 11:49am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not sure if it’s the indexing or rather the rhs of the assignment, since the error says:</p><NewLine><blockquote><NewLine><p>Expected a value of type ‘Tensor’ for argument ‘values’ but instead found type ‘int’.</p><NewLine></blockquote><NewLine><p>This seems to work:</p><NewLine><pre><code class=""lang-python"">@torch.jit.script <NewLine>def f(): <NewLine>    t = torch.tensor([0.])<NewLine>    t[t!=t] = torch.tensor(7.)<NewLine>    return t<NewLine><NewLine>f()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Your interpretation of the error msg makes a lot more sense. <img alt="":smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smile.png?v=9"" title="":smile:""/><br/><NewLine>Thank you!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/achim; <NewLine> ,"REPLY_DATE 1: August 17, 2019,  8:01am; <NewLine> REPLY_DATE 2: August 12, 2019, 11:19am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
53165,Pytorch cuda python,2019-08-12T11:00:20.517Z,0,87,"<div class=""post"" itemprop=""articleBody""><NewLine><p>On the Mixed C++/CUDA extension of pytorch.</p><NewLine></div>",https://discuss.pytorch.org/u/huang,(lin),huang,"August 12, 2019, 11:00am",,,,,
52451,Recursive @torch.jit.script export to ONNX,2019-08-02T23:09:03.734Z,0,165,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a <a href=""https://github.com/microsoft/pytorch_od/blob/master/mictorch/softmaxtree.py"" rel=""nofollow noopener"">SoftmaxTree implementation in C++ extension</a>, that I cannot convert to ONNX. I can change the implementation to use recursive jit script, but before that I wanted to see if then it can be exported to ONNX.</p><NewLine><p>I have tried simple <code>@torch.jit.script</code> with an if-statement can be exported and results in an <a href=""https://github.com/onnx/onnx/blob/master/docs/Operators.md#If"" rel=""nofollow noopener"">if node</a> in hte graph. But anything more complicated I am not sure.</p><NewLine></div>",https://discuss.pytorch.org/u/dashesy,(dashesy),dashesy,"August 2, 2019, 11:09pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It depends on your code, the TorchScript compiler always makes the same type of graph, and there are various limitations on what can be exported to ONNX. Looking at the link you posted, TorchScript currently doesn’t support custom autograd <code>Functions</code>, but this is something we are working on supporting.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: August 7, 2019, 12:56am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
51069,Sharing parameters of multiple traces,2019-07-19T13:17:36.773Z,0,129,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to share parameters of multiple JIT traces? I have a module that has two methods that I’d like to call from C++. Both methods touch the same hierarchy of submodules of the module, but in different ways (e.g. different order). I also need to optimize the parameters in C++, so the parameters will change gradually and both methods need to see the latest values.</p><NewLine><p>My understanding is that if I trace each method (in 1.1), I will obtain two ScriptModule objects, each having its own copy of the parameters. It is not clear to me if it is possible to share the parameters somehow.</p><NewLine><p>Looking at the docs of 1.2 (1.2.0a0+84c2c89), it seems one can trace multiple methods of one module. Will the parameters be shared correctly?</p><NewLine><p>Finally, since the submodules are hierarchical, will parameters() on the module return all parameters touched by the traces?</p><NewLine></div>",https://discuss.pytorch.org/u/trewick,,trewick,"July 19, 2019,  1:17pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Will the parameters be shared correctly?</p><NewLine></blockquote><NewLine><p>Yes, parameters should be shared across module’s methods.</p><NewLine><p>In this <a href=""https://github.com/pytorch/pytorch/blob/master/test/test_jit.py#L3159"" rel=""nofollow noopener"">test</a>, for example, <code>conv.weight</code> is shared by both <code>forward</code> and <code>weighted_kernel_sum</code>.</p><NewLine><blockquote><NewLine><p>Finally, since the submodules are hierarchical, will parameters() on the module return all parameters touched by the traces</p><NewLine></blockquote><NewLine><p>Yes, that should work as well, unless I’ve misunderstood the question.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/villedepommes; <NewLine> ,"REPLY_DATE 1: August 2, 2019,  5:06am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
49475,Cross-platform PyTorch stand-alone executables,2019-07-02T11:45:32.015Z,6,998,"<div class=""post"" itemprop=""articleBody""><NewLine><p>At this stage of my project, I need to compile my code to binaries that work under Windows and / or Ubuntu. I know that <code>jit</code> is a great mile stone in <code>PyTorch</code>, but, I am not sure if this is the correct approach that I can use to achieve my objectives; not to mention the complexity and effort needed to use <code>jit</code>; or <strong>PyTorch v1.0</strong>  C++ Front-End. That said, there are some tools to convert Python to binaries, for example, <a href=""https://www.pyinstaller.org/"" rel=""nofollow noopener"">PyInstaller</a> , which are not compatible with <code>PyTorch</code>. Are there any other alternatives to do <code>PyTorch</code> code compilation?</p><NewLine><p>Thanks in advance</p><NewLine></div>",https://discuss.pytorch.org/u/Deeply,(Deeply),Deeply,"July 2, 2019,  3:26pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What difficulties are you seeing with TorchScript (aka jit)? We are trying to make it as easy to use as possible for models defined in Python with PyTorch, so if you’re having issues please let us know. It appears that PyInstaller includes a Python runtime inside the binary, whereas TorchScript and the C++ frontend both have no dependency on Python and can be run in multithreaded environments with no GIL.</p><NewLine><p>As for <a href=""https://pytorch.org/tutorials/advanced/cpp_export.html"" rel=""nofollow noopener"">TorchScript</a> vs. the <a href=""https://pytorch.org/tutorials/advanced/cpp_frontend.html"" rel=""nofollow noopener"">C++ frontend</a>, that’s a more personal choice as they both rely on <code>libtorch</code> and the difference mostly comes down to how you want to define your model (in Python + TorchScript vs in C++), so it’s hard to say without more information on your use case.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your answer, which saved me putting lots of sparse effort.</p><NewLine><p>To be honest, I have not tried <code>jit</code> yet; but I need to know which path to take before going forward with it.</p><NewLine><p>From your answer I can say, as I am using PyTorch, then, it’s better to use <code>jit</code>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/driazati"">@driazati</a>, but if one needs to (1) train C++ and then (2) serialize model and then (3) load in C++ on the production, do you have to go through TorchScript in (2)? (Ok, I know that there’s ONYX)</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""49475""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/soldierofhell/40/14074_2.png"" width=""20""/> soldierofhell:</div><NewLine><blockquote><NewLine><p>one needs to (1) train C++ and then (2) serialize model and then (3) load in C++ on the production, do you have to</p><NewLine></blockquote><NewLine></aside><NewLine><p>The C++ frontend is a high level API that provides a similar experience as using PyTorch’s Python API, as such it has its own serialization mechanisms, so you can iterate on your model all in C++, <a href=""https://pytorch.org/cppdocs/frontend.html#end-to-end-example"" rel=""nofollow noopener"">this example shows how</a>.</p><NewLine><p>TorchScript lets you go from PyTorch models coded in Python to something that you can load in C++, so it’s not necessary for that case.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <a class=""mention"" href=""/u/driazati"">@driazati</a>, but it’s not clear what  torch::save do. From API doc seems like full serialization of Module or Tensor, but the comment in example states “checkpoint” (state_dict)? What is the method of serialization, is it something similar to Tensorflow ProtoBuf? Are there aby limitations?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>We have many serialization formats and they’re all different and easy to mix up. We’re working on ways to fix the UX here but for now:</p><NewLine><ul><NewLine><li><NewLine><code>torch.save()</code> in eager mode Python lets you save models so they can be loaded in Python</li><NewLine><li><NewLine><code>torch::save()</code> in the C++ API lets you save models so they can be loaded in C++ with <code>torch::load()</code><NewLine></li><NewLine><li><NewLine><code>torch.jit.save()</code> in eager mode Python lets you save models that have been compiled with TorchScript so they can be loaded in Python with <code>torch.jit.load()</code> and in C++ with <code>torch::jit::load()</code><NewLine></li><NewLine></ul><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/driazati"">@driazati</a>, thanks, but from your answer it’s still unclear what torch::save() saves: only “weights” or whole model (Module).<br/><NewLine>Definitely at least doc should be more precise. And to be honest this jit thing is also a little confusing, because this is kind of another world. E.g. in Tensorflow if you load .pb you end with normal tf.Graph not something like tf.pb.Graph <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Deeply; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/soldierofhell; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/soldierofhell; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/soldierofhell; <NewLine> ,"REPLY_DATE 1: July 2, 2019,  8:00pm; <NewLine> REPLY_DATE 2: July 2, 2019,  9:24pm; <NewLine> REPLY_DATE 3: July 14, 2019,  1:24pm; <NewLine> REPLY_DATE 4: July 16, 2019,  5:34pm; <NewLine> REPLY_DATE 5: July 27, 2019,  8:37am; <NewLine> REPLY_DATE 6: July 31, 2019,  7:49pm; <NewLine> REPLY_DATE 7: August 1, 2019,  8:03pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
52145,List of Arbitrary types not supported,2019-07-31T06:23:34.412Z,0,127,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><p>I was adding some code to allow accepting list inputs &amp; got this error when running the script</p><NewLine><blockquote><NewLine><p>RuntimeError: Tracing a list of arbitrary type is currently not supported!</p><NewLine></blockquote><NewLine><p>To which it leads me to <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/tracer.h"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/tracer.h</a></p><NewLine><p>Apparently there’s also a similar error for dicts of arbitrary type.</p><NewLine><p>Why is this error coming up in the first place, if I may ask?</p><NewLine></div>",https://discuss.pytorch.org/u/lawrencekiba,(macman),lawrencekiba,"July 31, 2019,  8:59am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Tracing can only record what happens to tensors, so a trace that involves lists of arbitrary type is high likely to produce incorrect results. Therefore we throw rather than silently change your output.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: July 31, 2019,  4:22pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
51979,Jit.trace with optimize=True gives wrong output,2019-07-29T22:17:17.712Z,0,109,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I trace a model with jit.trace and the <code>optimize=True</code> flag, I get a tracer warning saying that the outputs don’t match and the error is also significant. But when I trace with the flag set to False, everything is fine. Any idea why this might be happening?</p><NewLine></div>",https://discuss.pytorch.org/u/mpry,,mpry,"July 29, 2019, 10:17pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you post a reproducible code example so that we could have a look?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 30, 2019,  9:34pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
51045,Does the torch script support torch.nn.Dropout?,2019-07-19T09:33:52.415Z,6,382,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I want to script a method which calls torch.nn.dropout, it comes up a error message:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>unknown builtin op:<NewLine>    """"""<NewLine>    Chunking.<NewLine><NewLine>    Parameters<NewLine>    ----------<NewLine>    z_in : ``torch.LongTensor``, required.<NewLine>       The output of the character-level lstms.<NewLine>    """"""<NewLine><NewLine>    z_in = self.drop_(z_in)<NewLine>           ~~~~~~~~~ &lt;--- HERE<NewLine><NewLine>    out = self.chunk_layer(z_in).squeeze(1)<NewLine><NewLine>    return out<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">self.drop = torch.nn.Dropout(p=droprate)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">@torch.jit.script<NewLine>     def chunking(self, z_in):<NewLine>         """"""<NewLine>         Chunking.<NewLine> <NewLine>         Parameters<NewLine>         ----------<NewLine>         z_in : ``torch.LongTensor``, required.<NewLine>            The output of the character-level lstms.<NewLine>         """"""<NewLine> <NewLine>         z_in = self.drop(z_in)<NewLine> <NewLine>         out = self.chunk_layer(z_in).squeeze(1)<NewLine> <NewLine>         return out<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/kevinhaoliu,(kevin),kevinhaoliu,"July 19, 2019,  9:33am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, dropout should be supported:</p><NewLine><pre><code class=""lang-python"">class MyModel(torch.jit.ScriptModule):<NewLine>    def __init__(self):<NewLine>        super(MyModel, self).__init__()<NewLine>        self.fc1 = nn.Linear(10, 10)<NewLine>        self.drop = nn.Dropout()<NewLine>        <NewLine>    @torch.jit.script_method<NewLine>    def forward(self, x):<NewLine>        x = self.fc1(x)<NewLine>        x = self.drop(x)<NewLine>        return x<NewLine><NewLine>model = MyModel()<NewLine>x = torch.randn(1, 10)<NewLine>output = model(x)<NewLine><NewLine>traced_model = torch.jit.trace(model, x)<NewLine>traced_model(x)<NewLine></code></pre><NewLine><p>Could you post a code snippet to reproduce this issue?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply. I have solved it because I found it should be <em><strong><span class=""mention"">@torch.jit.script_method</span></strong></em> rather than <em><strong><span class=""mention"">@torch.jit.script</span></strong></em> . Thank you again!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have another question: when I use the sequential container, it will tell me a confusing error message with this:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/13f1d6e4f056c3a49b7686e8ba74843d23500cb7"" href=""https://discuss.pytorch.org/uploads/default/original/2X/1/13f1d6e4f056c3a49b7686e8ba74843d23500cb7.png"" title=""image.png""><img alt=""image"" data-base62-sha1=""2Qrd3h3WYWcl5e8lqW9moRopDFl"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/1/13f1d6e4f056c3a49b7686e8ba74843d23500cb7_2_10x10.png"" height=""117"" src=""https://discuss.pytorch.org/uploads/default/original/2X/1/13f1d6e4f056c3a49b7686e8ba74843d23500cb7.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">723×123 2.75 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine>But the <strong>forward</strong> function doesn’t appear in my code. My PyTorch version is 1.0.0.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think <a href=""https://github.com/pytorch/pytorch/issues/16400#issuecomment-457983682"" rel=""nofollow noopener"">this answer</a> might help.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>But the point is that I don’t write this <strong>forward</strong> function. It seems like the function is in the sequential container.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>How are you scripting the module?<br/><NewLine>As you can see in the linked example, the <code>nn.Sequential</code> module is wrapped inside a <code>torch.jit.ScriptModule</code>.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am using the <code>torch.jit.ScriptModule</code> too. Mine is like that:</p><NewLine><pre><code class=""lang-auto"">class NER(torch.jit.ScriptModule):<NewLine> <NewLine>      __constants__ = ['rnn_outdim', 'one_direction_dim', 'add_proj']<NewLine>      def __init__(self, rnn,<NewLine>                  w_num: int,<NewLine>                  w_dim: int,<NewLine>                  c_num: int,<NewLine>                  c_dim: int,<NewLine>                  y_dim: int,<NewLine>                  y_num: int,<NewLine>                  droprate: float):<NewLine>  <NewLine>          super(NER, self).__init__()<NewLine>  <NewLine>          ...<NewLine>  <NewLine>          if self.add_proj:<NewLine>          ...<NewLine>              self.chunk_layer = nn.Sequential(self.to_chunk, self.drop, self.to_chunk_proj, self.drop, self.chunk_weight)<NewLine>              self.type_layer = nn.Sequential(self.to_type, self.drop, self.to_type_proj, self.drop, self.type_weight)<NewLine>          else:<NewLine>              ...<NewLine>              self.chunk_layer = nn.Sequential(self.to_chunk, self.drop, self.chunk_weight)<NewLine>              self.type_layer = nn.Sequential(self.to_type, self.drop, self.type_weight)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kevinhaoliu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/kevinhaoliu; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/kevinhaoliu; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/kevinhaoliu; <NewLine> ,"REPLY_DATE 1: July 20, 2019, 12:10am; <NewLine> REPLY_DATE 2: July 22, 2019,  2:05am; <NewLine> REPLY_DATE 3: July 22, 2019,  7:06am; <NewLine> REPLY_DATE 4: July 22, 2019,  8:30am; <NewLine> REPLY_DATE 5: July 22, 2019,  8:34am; <NewLine> REPLY_DATE 6: July 22, 2019,  8:38am; <NewLine> REPLY_DATE 7: July 22, 2019,  8:46am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
50716,Problem converting a model via Annotation,2019-07-16T09:05:38.769Z,1,367,"<div class=""post"" itemprop=""articleBody""><NewLine><p><strong>I want to convert the model but I meet some problems:</strong></p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>could not export python function call &lt;python_value&gt;. Remove calls to python functions before export.:<NewLine>    mask : ``torch.ByteTensor`` , required.<NewLine>        The mask for character-level input.<NewLine>    """"""<NewLine>    w_emb = self.word_embed(w_in)<NewLine><NewLine>    c_emb = self.char_embed(c_in)<NewLine><NewLine>    emb = self.drop( torch.cat([w_emb, c_emb], 2) )<NewLine><NewLine>    out = self.rnn(emb)<NewLine>          ~~~~~~~~ &lt;--- HERE<NewLine><NewLine>    mask = mask.unsqueeze(2).expand_as(out)<NewLine><NewLine>    out = out.masked_select(mask).view(-1, self.rnn_outdim)<NewLine><NewLine>    return out<NewLine></code></pre><NewLine><p><strong>Here is the code：</strong></p><NewLine><pre><code class=""lang-auto"">def __init__(self, rnn, <NewLine>                w_num: int, <NewLine>                w_dim: int, <NewLine>                c_num: int, <NewLine>                c_dim: int, <NewLine>                y_dim: int, <NewLine>                y_num: int, <NewLine>                droprate: float):<NewLine><NewLine>        super(NER, self).__init__()<NewLine><NewLine>        self.rnn = rnn<NewLine>        self.rnn_outdim = self.rnn.output_dim<NewLine>        self.one_direction_dim = self.rnn_outdim // 2<NewLine>        self.word_embed = nn.Embedding(w_num, w_dim)<NewLine>        self.char_embed = nn.Embedding(c_num, c_dim)<NewLine>        self.drop = nn.Dropout(p=droprate)<NewLine>        self.add_proj = y_dim &gt; 0<NewLine>        self.to_chunk = highway(self.rnn_outdim)<NewLine>        self.to_type = highway(self.rnn_outdim)<NewLine><NewLine>        if self.add_proj:<NewLine>            self.to_chunk_proj = nn.Linear(self.rnn_outdim, y_dim)<NewLine>            self.to_type_proj = nn.Linear(self.rnn_outdim, y_dim)<NewLine>            self.chunk_weight = nn.Linear(y_dim, 1)<NewLine>            self.type_weight = nn.Linear(y_dim, y_num)<NewLine>            self.chunk_layer = nn.Sequential(self.to_chunk, self.drop, self.to_chunk_proj, self.drop, self.chunk_weight)<NewLine>            self.type_layer = nn.Sequential(self.to_type, self.drop, self.to_type_proj, self.drop, self.type_weight)<NewLine>        else:<NewLine>            self.chunk_weight = nn.Linear(self.rnn_outdim, 1)<NewLine>            self.type_weight = nn.Linear(self.rnn_outdim, y_num)<NewLine>            self.chunk_layer = nn.Sequential(self.to_chunk, self.drop, self.chunk_weight)<NewLine>            self.type_layer = nn.Sequential(self.to_type, self.drop, self.type_weight)<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def forward(self, w_in, c_in, mask):<NewLine>        """"""<NewLine>        Sequence labeling model.<NewLine><NewLine>        Parameters<NewLine>        ----------<NewLine>        w_in : ``torch.LongTensor``, required.<NewLine>            The RNN unit.<NewLine>        c_in : ``torch.LongTensor`` , required.<NewLine>            The number of characters.<NewLine>        mask : ``torch.ByteTensor`` , required.<NewLine>            The mask for character-level input.<NewLine>        """"""<NewLine>        w_emb = self.word_embed(w_in)<NewLine><NewLine>        c_emb = self.char_embed(c_in)<NewLine><NewLine>        emb = self.drop( torch.cat([w_emb, c_emb], 2) )<NewLine><NewLine>        out = self.rnn(emb)<NewLine><NewLine>        mask = mask.unsqueeze(2).expand_as(out)<NewLine><NewLine>        out = out.masked_select(mask).view(-1, self.rnn_outdim)<NewLine><NewLine>        return out<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">    rnn_map = {'Basic': BasicRNN}<NewLine>    rnn_layer = rnn_map[args.rnn_layer](args.layer_num, args.rnn_unit, args.word_dim + args.char_dim, args.hid_dim, args.droprate, args.batch_norm)<NewLine>    ner_model = NER(rnn_layer, len(w_map), args.word_dim, len(c_map), args.char_dim, args.label_dim, len(tl_map), args.droprate)<NewLine>    ner_model.load_state_dict(model)<NewLine>    ner_model.to(device)<NewLine>    ner_model.eval()<NewLine></code></pre><NewLine><pre><code class=""lang-auto""> def __init__(self, unit, input_dim, hid_dim, droprate, batch_norm):<NewLine>          super(BasicUnit, self).__init__(  <NewLine>          self.unit_type = unit<NewLine>          rnnunit_map = {'rnn': nn.RNN, 'lstm': nn.LSTM, 'gru': nn.GRU}<NewLine>          self.layer = torch.jit.trace(nn.LSTM(input_dim, hid_dim//2, 1, batch_first=True, bidirectional=True), torch.randn(500, 1, input_dim))<NewLine>             --&gt;I trace the lstm here<NewLine>          self.droprate = droprate<NewLine>          self.batch_norm = batch_norm<NewLine>          if self.batch_norm:<NewLine>              self.bn = nn.BatchNorm1d(hid_dim)<NewLine>          self.output_dim = hid_dim<NewLine>  <NewLine>          self.init_hidden()<NewLine></code></pre><NewLine><p><strong>How can I convert it? My pytorch version is 1.0.0.</strong></p><NewLine></div>",https://discuss.pytorch.org/u/kevinhaoliu,(kevin),kevinhaoliu,"July 16, 2019,  9:14am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, the error is saying if you want to export(serialize) your model to disk, you will need to convert all your models to TorchScript (either via tracing or scripting). Right now your model only have partially converted to TorchScript. For how to convert, here is our doc <a href=""https://pytorch.org/docs/stable/jit.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/jit.html</a>, I also recommend you to try our <a href=""https://github.com/pytorch/pytorch/issues/20939"" rel=""nofollow noopener"">new API</a> if you stay on our nightly builds.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply. I am a freshman, and I have read the doc, but I think I have done the conversion. So, could you please tell me what’ wrong with my conversion. Thank you again!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><strong>Another question:</strong></p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>cannot call a value:<NewLine>    Returns<NewLine>    ----------<NewLine>    output: ``torch.FloatTensor``.   <NewLine>        The output of RNNs.<NewLine>    """"""<NewLine>    out, _ = self.layer(x)<NewLine><NewLine>    if self.batch_norm:<NewLine>        output_size = out.size()<NewLine>        out = self.bn(out.view(-1, self.output_dim)).view(output_size)<NewLine>              ~~~~~~~ &lt;--- HERE<NewLine><NewLine>    if self.droprate &gt; 0:<NewLine>        out = F.dropout(out, p=self.droprate, training=self.training)<NewLine><NewLine>    return out<NewLine></code></pre><NewLine><p><strong>The code is:</strong></p><NewLine><pre><code class=""lang-auto"">@torch.jit.script_method<NewLine>      def forward(self, x):<NewLine>          """"""<NewLine>          Calculate the output.<NewLine>  <NewLine>          Parameters<NewLine>          ----------<NewLine>          x : ``torch.LongTensor``, required.<NewLine>              the input tensor, of shape (seq_len, batch_size, input_dim).<NewLine> <NewLine>          Returns<NewLine>          ----------<NewLine>          output: ``torch.FloatTensor``.   <NewLine>             The output of RNNs.<NewLine>         """"""<NewLine>          out, _ = self.layer(x)<NewLine>  <NewLine>          if self.batch_norm:<NewLine>              output_size = out.size()<NewLine>              out = self.bn(out.view(-1, self.output_dim)).view(output_size)<NewLine>  <NewLine>          if self.droprate &gt; 0:<NewLine>              out = F.dropout(out, p=self.droprate, training=self.training)<NewLine>  <NewLine>          return out<NewLine></code></pre><NewLine><p><strong>The init function is the fourth one in the first post.</strong></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>One more question, I get an error message with:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>could not export python function call &lt;python_value&gt;. Remove calls to python functions before export.:<NewLine><NewLine>        def forward(self, input):<NewLine>            for m in self:<NewLine>                input = m(input)<NewLine>                        ~ &lt;--- HERE<NewLine>            return input<NewLine></code></pre><NewLine><p>But I can’t find this <strong>forward</strong> function in my code. Why does this happen? Please help me!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kevinhaoliu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/kevinhaoliu; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/kevinhaoliu; <NewLine> ,"REPLY_DATE 1: July 16, 2019,  5:57pm; <NewLine> REPLY_DATE 2: July 17, 2019,  1:55am; <NewLine> REPLY_DATE 3: July 17, 2019,  7:20am; <NewLine> REPLY_DATE 4: July 18, 2019,  7:32am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
50830,No scopes name and type info when loading the trace model,2019-07-17T09:50:00.791Z,0,173,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-python"">class LinearInLinear(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(LinearInLinear, self).__init__()<NewLine>        self.l = nn.Linear(3, 5)<NewLine>        self.l1 = nn.Linear(5, 5)<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.l1(self.l(x + x))<NewLine><NewLine><NewLine>class MyModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(MyModel, self).__init__()<NewLine>        self.l1 = LinearInLinear()<NewLine>        self.l = LinearInLinear()<NewLine><NewLine>    def forward(self, input):<NewLine>        x1 = self.l1(input)<NewLine>        x2 = self.l(input)<NewLine>        return x1 + x2 + x2<NewLine>if __name__ == ""__main__"":<NewLine>    dummy_input = (torch.rand(5, 3),)<NewLine>    # with torch.onnx.set_training(model, False):<NewLine>    trace = torch.jit.trace(model, dummy_input)<NewLine>    # _optimize_trace(trace, torch._C._onnx.OperatorExportTypes.ONNX)<NewLine>    trace.save(""b.pt"")<NewLine>    print(trace.graph)<NewLine>    for node in trace.graph.nodes():<NewLine>        if node.kind() == ""prim::Constant"":<NewLine>            continue<NewLine>        print(list(node.outputs())[0].type().scalarType())<NewLine>    print(type(trace))<NewLine>    k = torch.jit.load(""b.pt"")<NewLine>    print(type(k))<NewLine>    print(k.graph)<NewLine>    for node in k.graph.nodes():<NewLine>        if node.kind() == ""prim::Constant"":<NewLine>            continue<NewLine>        print(node)<NewLine>        output = list(node.outputs())[0]<NewLine>        print(output.type().scalarType())<NewLine>   the last<NewLine></code></pre><NewLine><p>the last sentence produce the error below<br/><NewLine>RuntimeError: r ASSERT FAILED at /pytorch/aten/src/ATen/core/jit_type.h:142, please report a bug to PyTorch.</p><NewLine></div>",https://discuss.pytorch.org/u/jackmsye,,jackmsye,"July 17, 2019,  9:50am",,,,,
50180,Unsupported comparison operator: Transformer Model Export (MultiHeadAttention Layer),2019-07-10T02:13:57.009Z,0,276,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to export the Transformer Model to Torch Script. In creating a module list from MultiHeadAttention Layer, the following error is generated. Accompanying code is also attached.</p><NewLine><pre><code class=""lang-auto"">File ""/anaconda3/lib/python3.7/site-packages/torch/jit/frontend.py"", line 505, in build_Compare<NewLine>    raise NotSupportedError(err_range, ""unsupported comparison operator: "" + op.__name__)<NewLine>torch.jit.frontend.NotSupportedError: unsupported comparison operator: In<NewLine>    kv_same = key.data_ptr() == value.data_ptr()<NewLine><NewLine>    tgt_len, bsz, embed_dim = query.size()<NewLine>    assert embed_dim == self.embed_dim<NewLine>    assert list(query.size()) == [tgt_len, bsz, embed_dim]<NewLine>    assert key.size() == value.size()<NewLine><NewLine>    if incremental_state is not None:<NewLine>        saved_state = self._get_input_buffer(incremental_state)<NewLine>        if 'prev_key' in saved_state:<NewLine>            ~~~~~~~~~~~~~ &lt;--- HERE<NewLine></code></pre><NewLine><p>CODE:</p><NewLine><pre><code class=""lang-auto"">__constants__ = ['attentions', 'causal', 'layers_module']<NewLine><NewLine>def __init__(self, &lt;parameters&gt;)<NewLine>        att_modules = []<NewLine>        for _ in range(num_layers):<NewLine>            att_modules.append(nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout))<NewLine>        self.attentions = nn.ModuleList(att_modules)<NewLine></code></pre><NewLine><p>If I create an empty ModuleList and then append the MultiHeadAttention Layer it gives no error in no script mode, but  ModuleList has to be Const in Script Mode, that route is blocked as well. Error is generated in constructor itself as confirmed while debugging, not in forward method.</p><NewLine></div>",https://discuss.pytorch.org/u/sukuya,(Sukuya),sukuya,"July 10, 2019,  2:15am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Support for <code>in</code> was recently added, could you try using <code>pytorch-nightly</code> and see if that fixes this issue?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/driazati"">@driazati</a>  Yup, that issue is resolved but it fails now at this stage:</p><NewLine><pre><code class=""lang-auto"">if hasattr(self, '_qkv_same_embed_dim') and self._qkv_same_embed_dim is False:<NewLine>           ~~~~~~~ &lt;--- HERE<NewLine>            return F.multi_head_attention_forward(<NewLine>                query, key, value, self.embed_dim, self.num_heads,<NewLine>                self.in_proj_weight, self.in_proj_bias,<NewLine>                self.bias_k, self.bias_v, self.add_zero_attn,<NewLine>                self.dropout, self.out_proj.weight, self.out_proj.bias, <NewLine>                training=self.training,<NewLine>                key_padding_mask=key_padding_mask, need_weights=need_weights, <NewLine>                attn_mask=attn_mask, use_separate_proj_weight=True,<NewLine>                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,<NewLine></code></pre><NewLine><p>Is it because <code>hasattr</code>  or <code>and</code> is not supported yet ?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/sukuya; <NewLine> ,"REPLY_DATE 1: July 10, 2019,  6:40pm; <NewLine> REPLY_DATE 2: July 11, 2019,  8:10am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
49072,Cant save models when having some custom layers using torch.jit.trace,2019-06-27T02:46:06.474Z,1,1490,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m a freshman to pytorch, recently I got some troubles when trying to convert a python models to c++. HELPPPPPP！！</p><NewLine><p>enviroment:<br/><NewLine>pytorch1.1</p><NewLine><p>When i trying to trace some custom layers which implemented by c++, i got an error like this:</p><NewLine><pre><code class=""lang-auto"">could not export python function call &lt;python_value&gt;. Remove calls to Python functions before export. Did you forget add @script or @script_method annotation? If this is a nn.ModuleList, add it to __constants__.:<NewLine>@torch.jit.script_method<NewLine>def forward(self, x):<NewLine>    p1_conv1 = self.p1_conv1(x)<NewLine>    pool1    = self.pool1(p1_conv1) # Troubles is here<NewLine>               ~~~~~~~~~~ &lt;--- HERE<NewLine><NewLine><NewLine></code></pre><NewLine><p>the code is here:</p><NewLine><pre><code class=""lang-auto"">class corner_pool(torch.jit.ScriptModule):<NewLine>    def _init_layers(self, dim, pool1, pool2):<NewLine>        self.p1_conv1 = torch.jit.trace(convolution(3, dim, 128), torch.rand(1, 256, 64, 64))<NewLine>        self.pool1 = TopPool()  #custom layers implemented in cpp<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def forward(self, x):<NewLine>        p1_conv1 = self.p1_conv1(x)<NewLine>        pool1    = self.pool1(p1_conv1) # Troubles is here<NewLine></code></pre><NewLine><p>the toppool code is:</p><NewLine><pre><code class=""lang-auto"">class TopPoolFunction(Function):<NewLine>    @staticmethod<NewLine>    def forward(ctx, input):<NewLine>        output = TopPool.forward(input)[0]<NewLine>        ctx.save_for_backward(input)<NewLine>        return output<NewLine><NewLine>    @staticmethod<NewLine>    def backward(ctx, grad_output):<NewLine>        input  = ctx.saved_variables[0]<NewLine>        output = TopPool.backward(input, grad_output)[0]<NewLine>        return output<NewLine><NewLine>class TopPool(nn.Module):<NewLine>    def forward(self, x):<NewLine>        result = TopPoolFunction.apply(x)<NewLine>        return result<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/patrickpjiang,(Pingjiang),patrickpjiang,"June 27, 2019,  2:46am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group quote-modified"" data-post=""1"" data-topic=""49072""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/p/9dc877/40.png"" width=""20""/> patrickpjiang:</div><NewLine><blockquote><NewLine><p>could not export python function call &lt;python_value&gt;. Remove calls to Python functions before export. Did you forget add <span class=""mention"">@script</span> or <span class=""mention"">@script_method</span> annotation? If this is a nn.ModuleList, add it to <strong>constants</strong>.:</p><NewLine></blockquote><NewLine></aside><NewLine><p>The error message says that you have a function that you did not provide script annotation. In your case you will need to script <code>TopPoolFunction</code> and <code>TopPool</code> to make the model to be exported.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply.<br/><NewLine>Yes, you are right, but when i apply script on <code>TopPoolFunction</code>, i got an error:</p><NewLine><pre><code class=""lang-auto"">attribute lookup is not defined on python value of type 'FunctionMeta':<NewLine>@torch.jit.script_method<NewLine>def forward(self, x):<NewLine>    result = TopPoolFunction.apply(x)<NewLine>             ~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>    return result<NewLine><NewLine><NewLine></code></pre><NewLine><p>Here is how i script TopPool:</p><NewLine><pre><code class=""lang-auto"">class TopPool(torch.jit.ScriptModule):<NewLine>     # here is what i added, is there any problem?<NewLine>    @torch.jit.script_method        <NewLine>    def forward(self, x):<NewLine>        result = TopPoolFunction.apply(x)<NewLine>        return result<NewLine></code></pre><NewLine><p>For <code>TopPoolFunction</code>, i actually have no idea how to script…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>seems like <code>TopPoolFunction</code> is a autograd function, we don’t support script autograd Functions, so it is not scriptable.</p><NewLine><p>If you want to script it, you will need to write your TopPool function in python or TorchScript in order for TorchScript to compile it(TorchScript can only compile python, not C++),</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/patrickpjiang; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/wanchaol; <NewLine> ,"REPLY_DATE 1: June 27, 2019, 10:26pm; <NewLine> REPLY_DATE 2: June 28, 2019,  1:01am; <NewLine> REPLY_DATE 3: June 28, 2019,  3:01am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
48613,"Stateless constant tensor w/ DataParallel, JIT",2019-06-21T11:00:47.690Z,0,125,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m currently writing a fairly complex library that exposes various Modules and I have the following requirement in multiple places that I have a hard time satisfying.<br/><NewLine>I need to be able to define a constant tensor in a Module that is initialized with some user-defined value and shape. I want the tensor to move to the correct target hardware when doing model.to(some_device), but I don’t want the value to be part of the state_dict when the model is saved/loaded, so that if the user pass a different value at init time, that value is taken and not overwritten when a state_dict is loaded.<br/><NewLine>There are possible ways around this, e.g. define a Buffer, manually exclude it from the state_dict when the model is saved, and the reload with strict=False. My issue with this is that I would basically pass on the problem to the library’s user, and having a requirement on strict=False would also expose me to all sort of other weaknesses. Instead, I want to be able to manage the problem from within the library, i.e. from the Module definition.<br/><NewLine>My only idea at the moment is to use a Buffer and override the load_state APIs of the Module, but I have never seen it done anywhere. Any sort of pointer would be highly appreciated.<br/><NewLine>The implementation should work with the JIT compiler in a DataParallel context.</p><NewLine><p>Alessandro</p><NewLine></div>",https://discuss.pytorch.org/u/volcacius,(Alessandro),volcacius,"June 21, 2019, 11:00am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>To answer myself, I partially solved the problem for by splitting the value into a buffer with value 0 and a python constant with the user-defined value, and then summing them in the forward pass. Hopefully the JIT is smart enough to see that they are both constant, and it propagates them when required, but I haven’t verified it yet.<br/><NewLine>There is still has a requirement on strict=False though, since loading a pretrained model to which any of these layers have been added will complain about the missing buffer.<br/><NewLine>It would be nice to see a proper API for this use case, especially give the fact that constant values could be easily optimized by the JIT.</p><NewLine><p>Alessandro</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/volcacius; <NewLine> ,"REPLY_DATE 1: June 26, 2019,  2:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
48498,How does backward work with JIT module?,2019-06-20T08:54:19.374Z,0,294,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve read README.md about JIT at (<a href=""https://github.com/WyldeCat/pytorch/blob/master/torch/csrc/jit/README.md"" rel=""nofollow noopener"">https://github.com/WyldeCat/pytorch/blob/master/torch/csrc/jit/README.md</a>).</p><NewLine><p>There is a great explanation about how JIT handles a module’s forward method.</p><NewLine><p>But I can’t find any explanation about how backward pass works.</p><NewLine><p>So, I want to ask whether JIT involves in backward pass or not and is there any way to print backward pass’s graph like JIT module’s forward graph.</p><NewLine><p>Thanks in advanced.</p><NewLine></div>",https://discuss.pytorch.org/u/eee4447b4b8a2cbdfbbd,(wyldecat),eee4447b4b8a2cbdfbbd,"June 20, 2019,  8:54am",2 Likes,,,,
47221,Jit.trace() question about SiamMask network,2019-06-06T08:05:56.207Z,0,194,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My Siammask network is used for object tracking. There are two input arguments in my network, one is target object and the other one is search domain. I hope to use libtorch, but i dont’t know how can I use jit.trace() funciton in Pytorch to save it . I referred some blog and docs but could not find a method for double input args about jit.trace() function. Cound anyone meet the similar situation?</p><NewLine></div>",https://discuss.pytorch.org/u/Erissonleo,(Erissonleo),Erissonleo,"June 6, 2019,  8:05am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>torch.jit.trace()</code> takes in the function to trace and a tuple of example inputs, so you can pass as many inputs as your model needs.</p><NewLine><p>See <a href=""https://pytorch.org/docs/stable/jit.html#torch.jit.ScriptModule"" rel=""nofollow noopener"">Tracing</a> for more details, does this fix your issue?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: June 18, 2019,  8:56pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
47418,Is there support of recursive custom classes like Tree in Tree-LSTM?,2019-06-08T11:39:06.557Z,3,502,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there!<br/><NewLine>I am currently trying to make JIT optimizations work on the source code of Tree-LSTM model. The Tree class in the model is a crucial part of it, so I need to make it a custom class type so it can be used by the core methods of the model. That’s when I find a problem:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from typing import List<NewLine><NewLine>@torch.jit.script<NewLine>class Tree(object):<NewLine>    def __init__(self):<NewLine>        self.parent = None<NewLine>        self.num_children = 0<NewLine>        self.children = torch.jit.annotate(List[Tree], [])<NewLine><NewLine>    # further definitions omitted<NewLine></code></pre><NewLine><p>When I try to run the code, here is the error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>Unknown type name Tree:<NewLine>def __init__(self):<NewLine>    self.parent = None<NewLine>    self.num_children = 0<NewLine>    self.children = torch.jit.annotate(List[Tree], [])<NewLine>                                            ~~~~ &lt;--- HERE<NewLine></code></pre><NewLine><p>So the question is: Tree is basically a recursive structure, the children of a tree node is a list of tree nodes. Therefore for the children variable of a Tree class, I need to define an empty list of Tree class type. But since the Tree class type definition is still halfway, the interpreter cannot recognize Tree type.</p><NewLine><p>I am wondering is there any method that I can solve this problem, or there is actually no support for custom classes like the Tree above in current version of PyTorch and I should try other ways.</p><NewLine><p>It would be so nice if someone can give me a hand. Thanks a lot!</p><NewLine></div>",https://discuss.pytorch.org/u/FereX98,,FereX98,"June 8, 2019,  2:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I believe this is fixed in master (your class compiles for me). Could you try your code on our nightly release and see if it works?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The code successfully runs after I’ve switched to nightly release, cool! Thank you!</p><NewLine><p>But there is bad news that new errors arise when the interpreter tries to compile other methods. For example:</p><NewLine><pre><code class=""lang-auto"">@torch.jit.script<NewLine>class Tree(object):<NewLine>    def __init__(self):<NewLine>        self.parent = torch.jit.annotate(Optional[Tree], None)<NewLine><NewLine>    def add_child(self, child):<NewLine>        # type: (Tree) -&gt; None<NewLine>        child.parent = torch.jit.annotate(Optional[Tree], self)<NewLine></code></pre><NewLine><p>When run to the last line, an error arises:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>expected an expression of type Optional[__torch__.Tree] but found __torch__.Tree:<NewLine>at /Users/fere/repos/treelstm.pytorch/treelstm/tree.py:25:43<NewLine>    def add_child(self, child):<NewLine>        # type: (Tree) -&gt; None<NewLine>        child.parent = torch.jit.annotate(Optional[Tree], self)<NewLine>                                          ~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine></code></pre><NewLine><p>The interpreter regards <code>self</code> as <code>Tree</code> instead of <code>Optional[Tree]</code>, which makes sense but I thought that <code>torch.jit.annotate</code> should be able to do some casting work. The reason why I think so is that in code above, the interpreter takes <code>torch.jit.annotate(Optional[Tree], None)</code> as an <code>Optional[Tree]</code> type value instead of <code>None</code>. I’ve gone through the TorchScript documentation and done some searching, but the only method I’ve found that is able to specify types for a value other than function parameters and return values is by using <code>torch.jit.annotate</code>. Also, I haven’t find any ways which can cast a <code>T</code> typed value to an <code>Optional[T]</code> value. So I am curious that is there anything I miss?</p><NewLine><p>Another problem is below:</p><NewLine><pre><code class=""lang-auto"">@torch.jit.script<NewLine>class Tree(object):<NewLine>    def __init__(self):<NewLine>        self.num_children = torch.jit.annotate(int, 0)<NewLine><NewLine>    def add_child(self):<NewLine>        self.num_children += 1<NewLine></code></pre><NewLine><p>Error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>left-hand side of augmented assignment to module parameters/buffers can only be tensor types:<NewLine>at /Users/fere/repos/treelstm.pytorch/treelstm/tree.py:15:9<NewLine>    def add_child(self):<NewLine>        self.num_children += 1<NewLine>        ~~~~~~~~ &lt;--- HERE<NewLine></code></pre><NewLine><p>Does the error message means that only Tensor-typed values can be updated in a custom TorchScript class?</p><NewLine><p>Thankful and grateful for your help!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>First of all—thank you for the feedback! Support for classes is in its early days and reports from intrepid users are super valuable.</p><NewLine><p>To summarize, it seems that you are running into two problems:</p><NewLine><ol><NewLine><li><NewLine><p><code>torch.jit.annotate()</code> is not doing implicit type promotion from <code>T</code> to <code>Optional[T]</code> for class types.<br/><NewLine>This is a bug on our side, and we’ll look into fixing it. Can you file a Github task and we can track it there?</p><NewLine></li><NewLine><li><NewLine><p>Augmented assignment doesn’t work on non-tensors. This is a known issue, and we have a fix coming this week for it. In the meantime, just re-assigning is an easy workaround:</p><NewLine></li><NewLine></ol><NewLine><pre><code class=""lang-auto"">def add_child(self):<NewLine>    self.num_children = self.num_children + 1<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>actually no need to file an issue, <a href=""https://github.com/pytorch/pytorch/pull/21593"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/21593</a> fixes <span class=""hashtag"">#1</span>.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Update:<br/><NewLine>We do not actually currently support recursive class definitions. I didn’t remember when I first replied. <a href=""https://github.com/pytorch/pytorch/pull/21842"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/21842</a> improves the error message in this case.</p><NewLine><p>We will likely support it soon (before the next release) but for now it will not work.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot for the update! Guess now I need to wait or look for a temporal workaround. <img alt="":grinning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/grinning.png?v=9"" title="":grinning:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/FereX98; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/FereX98; <NewLine> ,"REPLY_DATE 1: June 8, 2019,  8:43pm; <NewLine> REPLY_DATE 2: June 9, 2019,  8:05am; <NewLine> REPLY_DATE 3: June 10, 2019,  4:41pm; <NewLine> REPLY_DATE 4: June 10, 2019,  5:22pm; <NewLine> REPLY_DATE 5: June 16, 2019,  9:19am; <NewLine> REPLY_DATE 6: June 17, 2019,  7:46am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
48048,TorchScript segfault when using recursive custom classes,2019-06-15T18:28:38.397Z,0,276,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Following the comments in <a href=""https://discuss.pytorch.org/t/is-there-support-of-recursive-custom-classes-like-tree-in-tree-lstm/47418"">this question</a>, I was trying to create a class that has attributes of the same class type as shown below:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from typing import Dict, List, Optional<NewLine><NewLine><NewLine>@torch.jit.script<NewLine>class JSONValue(object):<NewLine>    def __init__(self):<NewLine>        self.value = torch.jit.annotate(Optional[JSONValue], None)<NewLine><NewLine>class TestModule(torch.jit.ScriptModule):<NewLine>    @torch.jit.script_method<NewLine>    def forward(self, input):<NewLine>        # type: (JSONValue) -&gt; JSONValue<NewLine>        return input<NewLine><NewLine>m = TestModule()<NewLine><NewLine>m.save(""test.pt"")<NewLine></code></pre><NewLine><p>But this code results in a segfault with message <code>Segmentation fault: 11</code>. Any thoughts on what could be wrong?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/sidms,(Sid MS),sidms,"June 15, 2019,  6:29pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We do not actually currently support recursive class definitions. I didn’t remember that in the previous thread (will comment there as well). <a href=""https://github.com/pytorch/pytorch/pull/21842"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/21842</a> improves the error message in this case.</p><NewLine><p>We will likely support it soon (before the next release) but for now it will not work.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: June 17, 2019,  5:48am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
47744,Editing ScriptModule parameters,2019-06-12T13:36:56.995Z,0,244,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi.</p><NewLine><p>I know this seems like an anti-pattern question to ask, but I was wondering if there’s a way to edit a ScriptModule’s parameters and delete references to the old object at the same time?</p><NewLine><p>Currently it is not allowed to delete children of <code>ScriptModule._parameters</code>, which I assume is because the management of this happens in the lower C++ levels of the code.</p><NewLine><p>Are there any thoughts on supporting this in the future?</p><NewLine><p>For now it is at least possible to replace a value in the same <code>ScriptModule._parameters</code> as long as it’s of lesser size than the current value.</p><NewLine><p>Example of something that will still execute (after the model is traced):</p><NewLine><pre><code class=""lang-auto"">class Net(nn.Module):<NewLine>    def __init__(self, num_emb):<NewLine>        super(Net, self).__init__()<NewLine>        self.emb = nn.Embedding(num_emb, 100)<NewLine>        <NewLine>    def forward(self, x):<NewLine>        return self.emb(x)<NewLine>m = Net(10000)<NewLine>*trace model*<NewLine>subset = torch.randn_like(m.emb.weight[:80])<NewLine>m.emb.weight.data = subset<NewLine></code></pre><NewLine><p>Are there any huge drawbacks to doing this, other than being anti-pattern?<br/><NewLine>My usecase is training a model, shipping it to production, then trimming it down to a subset of our most recent items before accepting queries.</p><NewLine><p>Bonus question: Are there any good ways to convert a traced model back to a “normal” model after doing <code>torch.jit.load()</code>?</p><NewLine></div>",https://discuss.pytorch.org/u/NegatioN,(Joakim),NegatioN,"June 12, 2019,  2:13pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Modifying a traced model, then doing a <code>copy()</code> forces a “proper” re-allocate at least. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/><br/><NewLine>Any assistance with some of the other questions would still be appreciated! But my immediate needs are at least met.</p><NewLine><pre><code class=""lang-auto"">lm = torch.jit.load('m.pt')<NewLine>subset = torch.randn_like(lm.emb.weight[:80])<NewLine>lm.emb.weight.data = subset<NewLine>tmp = lm.copy()<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/NegatioN; <NewLine> ,"REPLY_DATE 1: June 13, 2019,  2:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
44212,Asking for more than 128G of memory,2019-05-02T19:07:51.952Z,3,292,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>So I trained a 3D-UNet with 16 base filters and 5 layers deep. Now I am trying to infer it on a 240x240x155 on a CPU. I have allocated 128GB of ram, it still pops out with an error.</p><NewLine><p>RuntimeError: $ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at /opt/conda/conda-bld/pytorch</p><NewLine><p>I do not have more money to buy new ram, The model should require at the most 32GB of ram for that image.</p><NewLine><p>Can I know where I may be going wrong?</p><NewLine><p>Thanks,<br/><NewLine>Siddhesh</p><NewLine></div>",https://discuss.pytorch.org/u/Geeks_Sid,(Siddhesh Thakur),Geeks_Sid,"May 2, 2019,  7:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you post the code for the model? Also did this occur when executing a TorchScript function/module or a normal <code>nn.Module</code>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I cannot post the model since it is an ongoing work. But I can confirm that I trained this model on a 16GB GPU.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>As far as I understand your issue, the training script takes 16GB at most running on the GPU and more than 128GB on the CPU?<br/><NewLine>If that’s correct, do you see an increasing memory usage during training or does your script run out of memory during the first iteration?<br/><NewLine>Did you change something in your data loading pipeline, e.g. are you loading the complete dataset into RAM?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh I was not even talking about training, it is the cost of inference on a single example.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Geeks_Sid; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Geeks_Sid; <NewLine> ,"REPLY_DATE 1: May 2, 2019,  9:37pm; <NewLine> REPLY_DATE 2: May 2, 2019,  9:41pm; <NewLine> REPLY_DATE 3: June 10, 2019,  7:32am; <NewLine> REPLY_DATE 4: June 10, 2019,  4:10pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
47379,[tutorial] Adding New Compiler Backends to PyTorch JIT,2019-06-07T20:57:59.969Z,0,310,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey all,</p><NewLine><p>I wrote up a tutorial on how to integrate a new backend into PyTorch JIT. The process is relatively simple and only requires around 2-3 PyTorch specific API calls. It can be found <a href=""https://jott.live/markdown/Writing%20a%20Toy%20Backend%20Compiler%20for%20PyTorch"" rel=""nofollow noopener"">here</a>.</p><NewLine></div>",https://discuss.pytorch.org/u/bwasti,(Bram Wasti),bwasti,"June 7, 2019,  8:58pm",3 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>That looks like an awesome tutorial, Bram!<br/><NewLine>Thanks for sharing. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: June 9, 2019,  2:19pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
47082,Use of Dataloader and TensorDataset,2019-06-05T00:28:49.543Z,0,603,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>So I was wondering the following two things.</p><NewLine><ol><NewLine><li>What is the use of DataLoader?</li><NewLine><li>What is the use of TensorDataset?</li><NewLine></ol><NewLine><p>So the scenario begins where I have my current machine with 128GB of RAM.<br/><NewLine>Is there a particular reason why I should use TensorDataset or DataLoader instead of converting my complete Dataset into a Tensor?<br/><NewLine>Wouldn’t this be a considerably faster operation and also cause less usage of I/O inturn giving me speedups?</p><NewLine><p>Am I missing something?<br/><NewLine>Sounds like a good discussing point of pytorch to me.</p><NewLine></div>",https://discuss.pytorch.org/u/Geeks_Sid,(Siddhesh Thakur),Geeks_Sid,"June 5, 2019, 12:28am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Loading the complete dataset into memory might work for you, but won’t certainly work for a lot of use cases, e.g. dealing with 10,000,000 images.</p><NewLine><p>Also, the initial loading of the whole dataset might slow down your iteration speed.<br/><NewLine>I.e. if you are still experimenting with your code and would like to iterate quickly, waiting several minutes for the data loading just to see a code error might be annoying. Of course this can be avoided by loading only a subset of the data, but would need additional code.</p><NewLine><p>Other benefits, e.g. shuffling and batching, will also be missing and you again would have to implement it manually.</p><NewLine><p>The idea behind the <code>DataLoader</code> is to load your data using multiprocessing (and pinned memory) to asynchronously push your data batch onto the GPU during training so that you can basically hide the data loading time. This is of course the optimal use case and if you are working with a slow HDD, you will most likely notice the data loading time.</p><NewLine><p>Anyway, the choice is of course yours, as PyTorch does not depend on a <code>Dataset</code> or <code>DataLoader</code> usage to work properly.</p><NewLine><p>The <code>TensorDataset</code> is a convenient method to wrap already loaded tensors into a <code>Dataset</code> and e.g. to use a <code>Subset</code> or wrap it in a <code>DataLoader</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, but let’s say my training set is of 7GB and I iterate over it a for 200 epochs, this means I have traversed through my harddisk or SSD for 1.4TB of data. Instead, if I just load this into the ram for 7~10GB(Considering overheads), would I not save myself from that hassle? Also, batchsize and shuffling can also be manually manipulated.<br/><NewLine>My final question would then become,<br/><NewLine>'Is this more energy efficient and will it give me a speedup? ’</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Geeks_Sid; <NewLine> ,"REPLY_DATE 1: June 5, 2019, 10:30am; <NewLine> REPLY_DATE 2: June 5, 2019, 12:36pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
47123,JIT tracing and parameter sharing,2019-06-05T10:49:49.356Z,0,145,"<div class=""post"" itemprop=""articleBody""><NewLine><p>torch.jit.trace doesn’t allow us to trace a module that has shared parameters.<br/><NewLine>Does someone know the reason?</p><NewLine><p>I guess it aims to prevent the shared parameters from being concurrently updated (and destroyed).<br/><NewLine>If so, I think parameter sharing is safe when we know the modules that access the parameters are NOT concurrently run.</p><NewLine><p>For example, modules that run in a sequence never update their parameters concurrently.<br/><NewLine>Could someone tell me if I can use tracing in that case (by removing the detection of shared parameters).</p><NewLine></div>",https://discuss.pytorch.org/u/mtnk,(Masahiro),mtnk,"June 5, 2019, 10:49am",,,,,
46909,TypeError: &lsquo;TopLevelTracedModule&rsquo; object is not iterable,2019-06-03T01:31:47.735Z,0,898,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Im new to pytorch programming. I have a model in python there checkpoint file is saved in.ckpt format. I want to save that model in the .pt format ( use in c++ ) .  The model uses two images as input. Then i sent two images using  jit:: trace into model. I get following errors any help is appreciated. Thank you.</p><NewLine><p><strong>The model in python</strong></p><NewLine><pre><code>left = cv2.imread(args.left)<NewLine>right = cv2.imread(args.right)<NewLine><NewLine>pairs = {'left': left, 'right': right}<NewLine><NewLine>transform = T.Compose([Normalize(mean, std), ToTensor(), Pad(384, 1248)])<NewLine>pairs = transform(pairs)<NewLine>left = pairs['left'].to(device).unsqueeze(0)<NewLine>right = pairs['right'].to(device).unsqueeze(0)<NewLine><NewLine>model = PSMNet(args.maxdisp).to(device)<NewLine>if len(device_ids) &gt; 1:<NewLine>    model = nn.DataParallel(model, device_ids=device_ids)<NewLine><NewLine>state = torch.load(args.model_path)<NewLine>if len(device_ids) == 1:<NewLine>    from collections import OrderedDict<NewLine>    new_state_dict = OrderedDict()<NewLine>    for k, v in state['state_dict'].items():<NewLine>        namekey = k[7:] # remove `module.`<NewLine>        new_state_dict[namekey] = v<NewLine>    state['state_dict'] = new_state_dict<NewLine><NewLine>model.load_state_dict(state['state_dict'])<NewLine>print('load model from {}'.format(args.model_path))<NewLine>print('epoch: {}'.format(state['epoch']))<NewLine>print('3px-error: {}%'.format(state['error']))<NewLine><NewLine>model.eval()<NewLine><NewLine>with torch.no_grad():<NewLine>    _, _, disp = model(left, right)]<NewLine></code></pre><NewLine><p><strong>The Trace program that i tried to save model file in .pt format</strong></p><NewLine><blockquote><NewLine><pre><code>leftTest = torch.randn(3, 384, 1248).to(device).unsqueeze(0)<NewLine>rightTest = torch.randn(3, 384, 1248).to(device).unsqueeze(0)<NewLine><NewLine>with torch.no_grad():<NewLine># error line <NewLine>    _, _, dispTest = torch.jit.trace(model, (leftTest, rightTest)) <NewLine></code></pre><NewLine></blockquote><NewLine><p><strong>where the problem is</strong></p><NewLine><blockquote><NewLine><p>def forward(self, left_img, right_img):</p><NewLine><pre><code>    original_size = [self.D, left_img.size(2), left_img.size(3)]<NewLine>    left_cost = self.cost_net(left_img)  # [B, 32, 1/4H, 1/4W]<NewLine>    right_cost = self.cost_net(right_img)  # [B, 32, 1/4H, 1/4W]<NewLine>    # cost = torch.cat([left_cost, right_cost], dim=1)  # [B, 64, 1/4H, 1/4W]<NewLine>    # B, C, H, W = cost.size()<NewLine><NewLine>    # print('left_cost')<NewLine>    # print(left_cost[0, 0, :3, :3])<NewLine><NewLine>    B, C, H, W = left_cost.size()<NewLine><NewLine>    cost_volume = torch.zeros(B, C * 2, self.D // 4, H, W).type_as(left_cost)  # [B, 64, D, 1/4H, 1/4W]<NewLine><NewLine>    # for i in range(self.D // 4):<NewLine>    #     cost_volume[:, :, i, :, i:] = cost[:, :, :, i:]<NewLine><NewLine>    for i in range(self.D // 4):<NewLine>        if i &gt; 0:<NewLine>            cost_volume[:, :C, i, :, i:] = left_cost[:, :, :, i:] # use 32 <NewLine>            cost_volume[:, C:, i, :, i:] = right_cost[:, :, :, :-i] # use 32<NewLine>        else:<NewLine>            # come at first<NewLine>            cost_volume[:, :C, i, :, :] = left_cost # use 32<NewLine>            cost_volume[:, C:, i, :, :] = right_cost<NewLine><NewLine>    disp1, disp2, disp3 = self.stackedhourglass(cost_volume, out_size=original_size)<NewLine><NewLine>    return disp1, disp2, disp3<NewLine></code></pre><NewLine></blockquote><NewLine><p><strong>Error log that i get</strong></p><NewLine><pre><code class=""lang-auto"">/home/ven/.local/lib/python3.5/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.<NewLine>  ""See the documentation of nn.Upsample for details."".format(mode))<NewLine>save diparity map in /home/ven/Downloads/PSMNet/depth.png<NewLine>shape left ----&gt; torch.Size([1, 3, 384, 1248])<NewLine>shape right ----&gt; torch.Size([1, 3, 384, 1248])<NewLine>/home/ven/Downloads/PSMNet/models/PSMnet.py:42: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<NewLine>  cost_volume[:, :C, i, :, :] = left_cost<NewLine>/home/ven/Downloads/PSMNet/models/PSMnet.py:43: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<NewLine>  cost_volume[:, C:, i, :, :] = right_cost<NewLine>/home/ven/Downloads/PSMNet/models/PSMnet.py:39: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<NewLine>  cost_volume[:, :C, i, :, i:] = left_cost[:, :, :, i:]<NewLine>/home/ven/Downloads/PSMNet/models/PSMnet.py:40: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<NewLine>  cost_volume[:, C:, i, :, i:] = right_cost[:, :, :, :-i]<NewLine>/home/ven/.local/lib/python3.5/site-packages/torch/jit/__init__.py:702: TracerWarning: Output nr 3. of the traced function does not match the corresponding output of the Python function. Detailed error:<NewLine>Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 282, 783] (68.55914306640625 vs. 68.55826568603516) and 42 other locations (0.00%)<NewLine>  _check_trace([example_inputs], func, executor_options, traced, check_tolerance, _force_outplace)<NewLine>Traceback (most recent call last):<NewLine>  File ""/home/ven/Downloads/PSMNet/inference.py"", line 118, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""/home/ven/Downloads/PSMNet/inference.py"", line 85, in main<NewLine>    _, _, disp = torch.jit.trace(model, (leftTest, rightTest))<NewLine>TypeError: 'TopLevelTracedModule' object is not iterable<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Venushka,(Venushka),Venushka,"June 3, 2019,  8:29am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>solved. By changing<br/><NewLine>dispTest = torch.jit.trace(model, (leftTest, rightTest))<br/><NewLine>to B, C, H, W assigned corresponding numerical values</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Venushka; <NewLine> ,"REPLY_DATE 1: June 5, 2019,  4:23am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
46355,PTX JIT compilation failed when running FasterRCNN,2019-05-27T20:25:12.047Z,1,934,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I try to train FasterRCNN in newly released torchvision 0.3, I run into a PTX JIT compilation failed error.</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""train_rcnn.py"", line 124, in &lt;module&gt;<NewLine>    loss_dict = model(images, targets)<NewLine>  File ""/opt/anaconda3/envs/nuscenes/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/danielkang92/nuscenes/code/FasterRCNN/model.py"", line 19, in forward<NewLine>    return self.rcnn(images, targets)<NewLine>  File ""/opt/anaconda3/envs/nuscenes/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/opt/anaconda3/envs/nuscenes/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py"", line 48, in forward<NewLine>    features = self.backbone(images.tensors)<NewLine>  File ""/opt/anaconda3/envs/nuscenes/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/opt/anaconda3/envs/nuscenes/lib/python3.7/site-packages/torch/nn/modules/container.py"", line 92, in forward<NewLine>    input = module(input)<NewLine>  File ""/opt/anaconda3/envs/nuscenes/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/opt/anaconda3/envs/nuscenes/lib/python3.7/site-packages/torchvision/models/_utils.py"", line 58, in forward<NewLine>    x = module(x)<NewLine>  File ""/opt/anaconda3/envs/nuscenes/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>RuntimeError: /pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp:202: a PTX JIT compilation failed<NewLine></code></pre><NewLine><p>My environment is as follows :</p><NewLine><pre><code class=""lang-auto"">Collecting environment information...<NewLine>PyTorch version: 1.1.0<NewLine>Is debug build: No<NewLine>CUDA used to build PyTorch: 9.0.176<NewLine><NewLine>OS: Debian GNU/Linux 9.8 (stretch)<NewLine>GCC version: (Debian 6.3.0-18+deb9u1) 6.3.0 20170516<NewLine>CMake version: Could not collect<NewLine><NewLine>Python version: 3.7<NewLine>Is CUDA available: Yes<NewLine>CUDA runtime version: 10.0.130<NewLine>GPU models and configuration: GPU 0: Tesla P100-PCIE-16GB<NewLine>Nvidia driver version: 410.72<NewLine>cuDNN version: Could not collect<NewLine><NewLine>Versions of relevant libraries:<NewLine>[pip3] intel-numpy==1.15.1<NewLine>[pip3] numpy==1.16.3<NewLine>[pip3] torch==1.1.0<NewLine>[pip3] torchvision==0.3.0<NewLine>[conda] torch                     1.1.0                    pypi_0    pypi<NewLine>[conda] torchvision               0.3.0                    pypi_0    pypi<NewLine></code></pre><NewLine><p>Would appreciate any help on this!</p><NewLine></div>",https://discuss.pytorch.org/u/danthekang,(Dan),danthekang,"May 27, 2019,  8:25pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the report! Can you file an issue on github with the same info and we can track it from there? Thanks</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just filled it out here: <a href=""https://github.com/pytorch/pytorch/issues/21004"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/21004</a><br/><NewLine>Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/danthekang; <NewLine> ,"REPLY_DATE 1: May 28, 2019,  3:55am; <NewLine> REPLY_DATE 2: May 28, 2019,  5:29am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
45950,Torchscript module/function -&gt; Python,2019-05-22T20:14:07.767Z,0,209,"<div class=""post"" itemprop=""articleBody""><NewLine><p>After using <span class=""mention"">@torch.jit.script</span> or torch.jit.trace on a Module/function is there a way to convert the traced module/function to Python again?</p><NewLine></div>",https://discuss.pytorch.org/u/mari-linhares,(Marianne Linhares Monteiro),mari-linhares,"May 22, 2019,  8:14pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can call it from Python but if you want to debug into it, you would have to use a switch in your code to make it use the original Python function.<br/><NewLine>You can also use the environment variable PYTORCH_JIT to disable it for the entire run of your program.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>We don’t have a way to convert it back to the original python. The <code>.code</code> attribute for a method will give you valid python syntax that corresponds to that method implementation though.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/leowalkling; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: May 23, 2019,  2:10pm; <NewLine> REPLY_DATE 2: July 16, 2019,  6:21pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
45788,How to Eliminate Warnings While Creating VGG_11 Trace File,2019-05-21T09:02:29.589Z,0,106,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I try to create trace file of a model without any warning. I use the model VGG_11 at this <a href=""https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py"" rel=""nofollow noopener"">link</a>. I trained the model with my custom dataset then saved the model as a .pth file. Then, I loaded this .pth file and created trace file as a .pt file. Then, it gave those warnings:<div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/45bf0125016bcf078c1c4cd621ae7b71204acac5"" href=""https://discuss.pytorch.org/uploads/default/original/2X/4/45bf0125016bcf078c1c4cd621ae7b71204acac5.png"" title=""VGG_Trace_Warnings.PNG""><img alt=""VGG_Trace_Warnings"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/4/45bf0125016bcf078c1c4cd621ae7b71204acac5_2_10x10.png"" height=""116"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/4/45bf0125016bcf078c1c4cd621ae7b71204acac5_2_690x116.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/4/45bf0125016bcf078c1c4cd621ae7b71204acac5_2_690x116.png, https://discuss.pytorch.org/uploads/default/optimized/2X/4/45bf0125016bcf078c1c4cd621ae7b71204acac5_2_1035x174.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/4/45bf0125016bcf078c1c4cd621ae7b71204acac5_2_1380x232.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">VGG_Trace_Warnings.PNG</span><span class=""informations"">1853×313 29.7 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine>Could you offer a solution about how to eliminate those warnings?</p><NewLine></div>",https://discuss.pytorch.org/u/sercan,(sercan),sercan,"May 21, 2019,  9:02am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The warning is telling you that there are non-deterministic nodes in your model, specifically dropouts. If you’re tracing a model that you already trained, you may have forgotten to set <code>eval()</code> on the module.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: May 22, 2019,  7:27am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
45824,Realtime capabilities of TorchScript / memory allocations,2019-05-21T15:14:55.993Z,0,144,"<div class=""post"" itemprop=""articleBody""><NewLine><p>At the moment I am exploring the possibility of writing controllers for a real robot hardware using TorchScript. One of the requirements is to have low latency and constant execution times for the script. That is as the robot controller is running in a realtime environment with hard 1 kHz (=1 ms) time limits.</p><NewLine><p>Does someone of you know how the JIT compiler of TorchScript handles memory allocations? Is there something like a “realtime” mode switch to avoid memory allocations or does this all depends on the implementation of the TorchScript C++ operators?</p><NewLine><p>Best,<br/><NewLine>julian</p><NewLine></div>",https://discuss.pytorch.org/u/jviereck,(Julian Viereck),jviereck,"May 21, 2019,  3:14pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The JIT has no realtime mode. Allocation is controlled by operator implementations, which are generally (for CPU ops) tuned for server workloads.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: May 21, 2019,  4:32pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
35206,Unclear about relation between TorchScript and backprop,2019-01-21T12:08:19.088Z,1,408,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I am trying to write a TorchScript operator to replace a custom torch.autograd.Function to influence the gradient computation, since TorchScript cannot serialize calls to custom Functions.<br/><NewLine>Is there an equivalent of torch.autograd.Function in ATen or Torch?</p><NewLine><p>Best regards<br/><NewLine>Leopold</p><NewLine></div>",https://discuss.pytorch.org/u/leowalkling,,leowalkling,"January 21, 2019, 12:08pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/leowalkling"">@leowalkling</a>,  could you elaborate more on what you want to replace here with a example? In case it’s helpful, in JIT we currently put ops to 2 categories:</p><NewLine><ol><NewLine><li>Ops have autodiff formulas in autodiff.cpp: their backward graph is replaced by <a href=""https://github.com/pytorch/pytorch/blob/0cb24098c74f8ebed81ec08b83bf6cb5ab3903f5/torch/csrc/jit/graph_executor.cpp#L76"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/0cb24098c74f8ebed81ec08b83bf6cb5ab3903f5/torch/csrc/jit/graph_executor.cpp#L76</a><NewLine></li><NewLine><li>Ops don’t have autodiff formulas: their backwards are handled by eager mode autograd directly.<br/><NewLine>Please let us know the context of problem so that we can help more on this.</li><NewLine></ol><NewLine><p>Thanks,<br/><NewLine>Ailing</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>For reference, I’ve come up with a solution by imitating the generated kernels and the code generation<br/><NewLine>from /tools/autograd.<br/><NewLine>But it seems to me that it is a very intricate task, given that there is little documentation, so I made sure to follow the original code of builtin operations closely.</p><NewLine><p>The purpose I had in mind was to perform in-place modification of a variable without incrementing invalidating its former gradient, but instead creating a new variable depending on the old one.<br/><NewLine>I’m using this in my implementations of IAF, MADE, etc. to save RAM (by using fewer clones of the same data).</p><NewLine><p>The following is what wasn’t obvious from the docs (to me):</p><NewLine><ol><NewLine><li>Use ta::make_variable or ta::make_variable_view to create the output Variable_s</li><NewLine><li>Implement a new subclass of ta::TraceableFunction in your module.<br/><NewLine>With Pytorch 1.0 from Conda, the headers lack generated code, such as the xxxBackward classes, so there was no good way to use even the existing subclasses of ta::TraceableFunction.</li><NewLine><li>Instantiate your custom xxxBackward and register it in the graph using the methods set_next_edges and add_input_metadata of TraceableFunction and set_gradient_edge of Variable. Helper functions available in “torch/csrc/autograd/functions/utils.h”</li><NewLine><li>TorchScript-compatibility requires registering the op as described in the tutorials on extending TorchScript.</li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ailzhang; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/leowalkling; <NewLine> ,"REPLY_DATE 1: February 5, 2019,  8:07pm; <NewLine> REPLY_DATE 2: May 19, 2019,  8:26pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
44802,Tracing backward pass in v1.1.0,2019-05-09T09:29:41.229Z,1,491,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello All,</p><NewLine><p>My team at Microsoft Research India has built an adaptive compiler framework for deep learning jobs and implemented it on PyTorch (<a href=""https://dl.acm.org/citation.cfm?id=3304072&amp;preflayout=tabs"" rel=""nofollow noopener"">ASPLOS '19 paper</a>). We used the tracing API in v0.3 to capture the compute graph representation of the network and its gradient computation, whose execution was then optimized by our framework.</p><NewLine><p>We would like to port the implementation to v1.1.0, which entails capturing the compute graph through the tracing API. Since the backward pass doesn’t return any outputs, it’s not possible to use v1.1.0’s tracing API to build the computational graph for the backward pass. I tried the graph visualization package to build the computational graph for the backward pass, but the nodes were named “SelectBackward” or “ViewBackward”.</p><NewLine><p>Could someone please help me with understanding the tracing API so that I can capture the graph for the backward pass as well ? Or please suggest a different methodology ?</p><NewLine><p>Thank You,<br/><NewLine>Sanjay</p><NewLine></div>",https://discuss.pytorch.org/u/singam-sanjay,,singam-sanjay,"May 9, 2019,  9:29am",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/singam-sanjay"">@singam-sanjay</a>, could you give an example about how it worked in 0.3 and what you need from 1.1.0?<br/><NewLine>“SelectBackward” are the autograd nodes and I’m not sure if they’re expected to show up in your backward pass. (Given that you capture the graph representation and do optimization through your compiler framework.)<br/><NewLine>Happy to help if you can give more context. Thanks!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ailzhang"">@ailzhang</a>,</p><NewLine><p>Thanks for responding !</p><NewLine><p>We used the tracing API in 0.3 in the following way,</p><NewLine><ol><NewLine><li>Tracing the forward pass,</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">trace, fwd_outputs = torch.jit.trace(model, fwd_args, nderivs = 1)<NewLine></code></pre><NewLine><ol start=""2""><NewLine><li>Tracing the backward pass,</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">torch.autograd.backward(fwd_outputs, bwd_args)<NewLine></code></pre><NewLine><p>After Step 2, the <code>trace.graph()</code> is updated with nodes from the backward pass.</p><NewLine><p>Could you please suggest an approach to generate equivalent results in the v1.1.0 API ?</p><NewLine><p>Thanks,<br/><NewLine>Sanjay</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ailzhang; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/singam-sanjay; <NewLine> ,"REPLY_DATE 1: May 9, 2019,  4:07pm; <NewLine> REPLY_DATE 2: May 14, 2019,  7:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
44811,How to get parameter tensor of corresponding param node in a traced jit graph?,2019-05-09T10:24:33.896Z,1,290,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am working on a torch to tensorrt project. currently the major problem is impossible to get correct weight of an op. A traced resnet18 model produces following inputs:<br/><NewLine><code>node: %input.1 : Float(1, 3, 224, 224), %702 : Tensor, %703 : Tensor, %704 : Tensor, %705 : Tensor, %706 : Tensor, %707 : Tensor, %708 : Tensor, %709 : Tensor, %710 : Tensor, %711 : Tensor, %712 : Tensor, %713 : Tensor, %714 : Tensor, %715 : Tensor, %716 : Tensor, %717 : Tensor, %718 : Tensor, %719 : Tensor, %720 : Tensor, %721 : Tensor, %722 : Tensor, %723 : Tensor, %724 : Tensor, %725 : Tensor, %726 : Tensor, %727 : Tensor, %728 : Tensor, %729 : Tensor, %730 : Tensor, %731 : Tensor, %732 : Tensor, %733 : Tensor, %734 : Tensor, %735 : Tensor, %736 : Tensor, %737 : Tensor, %738 : Tensor, %739 : Tensor, %740 : Tensor, %741 : Tensor, %742 : Tensor, %743 : Tensor, %744 : Tensor, %745 : Tensor, %746 : Tensor, %747 : Tensor, %748 : Tensor, %749 : Tensor, %750 : Tensor, %751 : Tensor, %752 : Tensor, %753 : Tensor, %754 : Tensor, %755 : Tensor, %756 : Tensor, %757 : Tensor, %758 : Tensor, %759 : Tensor, %760 : Tensor, %761 : Tensor, %762 : Tensor, %763 : Tensor, %764 : Tensor, %765 : Tensor, %766 : Tensor, %767 : Tensor, %768 : Tensor, %769 : Tensor, %770 : Tensor, %771 : Tensor, %772 : Tensor, %773 : Tensor, %774 : Tensor, %775 : Tensor, %776 : Tensor, %777 : Tensor, %778 : Tensor, %779 : Tensor, %780 : Tensor, %781 : Tensor, %782 : Tensor, %783 : Tensor, %784 : Tensor, %785 : Tensor, %786 : Tensor, %787 : Tensor, %788 : Tensor, %789 : Tensor, %790 : Tensor, %791 : Tensor, %792 : Tensor, %793 : Tensor, %794 : Tensor, %795 : Tensor, %796 : Tensor, %797 : Tensor, %798 : Tensor, %799 : Tensor, %800 : Tensor, %801 : Tensor, %802 : Tensor, %803 : Tensor = prim::Param()</code><br/><NewLine>It’s possible to get correct input nodes, but for parameter nodes, the only information I can get is “index” of slot, I don’t know how to get corresponding weight of a parameter node.<br/><NewLine>A workaround is use torch.jit._unique_state_dict and remove all untracked variables to get a list of params, then assign them to param node in reversed order. but this isn’t work for models with unused modules such as torchvision.models.inception_v3 (it has a aux output).<br/><NewLine>Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/traveller59,,traveller59,"May 9, 2019, 10:31am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry I don’t quite get what you are asking for. what do you exactly mean <code>the weight of an op</code>? and <code>the correct input node</code>? If you can provide more context that will be good for us to answer your exact question</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>torch.jit.trace create a graph, graph.inputs() return input nodes in net.forward and parameter nodes, the problem is there is no way to get corresponding weight tensor for a parameter node.<br/><NewLine>I currently use this <a href=""https://github.com/traveller59/torch2trt/blob/master/torch2trt/core.py#L395"" rel=""nofollow noopener"">code</a> to get weight tensor to parameter node mapping, but this isn’t guaranteed by pytorch doc.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/traveller59; <NewLine> ,"REPLY_DATE 1: May 10, 2019,  9:33pm; <NewLine> REPLY_DATE 2: May 11, 2019,  2:44am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
44455,Torch.as_tensor not available in TorchScript,2019-05-05T13:49:44.249Z,0,150,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to convert a list of tuples into a tensor using <code>torch.as_tensor</code> which throws the following error:</p><NewLine><blockquote><NewLine><p>unknown builtin op: aten::as_tensor<br/><NewLine>Could not find any similar ops to aten::as_tensor. This op may not exist or may not be currently supported in TorchScript</p><NewLine></blockquote><NewLine><p>Is there any other way to accomplish this? Using <code>torch.tensor</code> also doesn’t work.<br/><NewLine>I’m using PyTorch version 1.1.0</p><NewLine></div>",https://discuss.pytorch.org/u/TheCodez,(Michael Kösel),TheCodez,"May 5, 2019,  1:50pm",,,,,
44192,Making Jit work,2019-05-02T15:52:30.914Z,6,571,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I have the following code:</p><NewLine><pre><code class=""lang-auto"">def normalize(data: torch.Tensor, mean: torch.Tensor,<NewLine>              std: torch.Tensor) -&gt; torch.Tensor:<NewLine><NewLine>    """"""Normalise the image with channel-wise mean and standard deviation.<NewLine><NewLine>    Args:<NewLine>        data (torch.Tensor): The image tensor to be normalised.<NewLine>        mean (torch.Tensor): Mean for each channel.<NewLine>        std (torch.Tensor): Standard deviations for each channel.<NewLine><NewLine>        Returns:<NewLine>        Tensor: The normalised image tensor.<NewLine>    """"""<NewLine><NewLine>    if not torch.is_tensor(data):<NewLine>        raise TypeError('data should be a tensor. Got {}'.format(type(data)))<NewLine><NewLine>    if not torch.is_tensor(mean):<NewLine>        raise TypeError('mean should be a tensor. Got {}'.format(type(mean)))<NewLine><NewLine>    if not torch.is_tensor(std):<NewLine>        raise TypeError('std should be a tensor. Got {}'.format(type(std)))<NewLine><NewLine>    if len(mean) != data.shape[-3] and mean.shape[:2] != data.shape[:2]:<NewLine>        raise ValueError('mean lenght and number of channels do not match')<NewLine><NewLine>    if len(std) != data.shape[-3] and std.shape[:2] != data.shape[:2]:<NewLine>        raise ValueError('std lenght and number of channels do not match')<NewLine><NewLine>    if std.shape != mean.shape:<NewLine>        raise ValueError('std and mean must have the same shape')<NewLine><NewLine>    mean = mean[..., :, None, None].to(data.device)<NewLine>    std = std[..., :, None, None].to(data.device)<NewLine><NewLine>    out = data.sub(mean).div(std)<NewLine><NewLine>    return out<NewLine></code></pre><NewLine><p>I would like for this function to be able to be executed <strong>with</strong> and <strong>without</strong> JIT.  The problem I am currently facing is that, since this function has control flow statements (IFs) when I run:</p><NewLine><pre><code class=""lang-auto"">        f = image.Normalize(mean, std)<NewLine>        jit_trace = torch.jit.trace(f, data)<NewLine>        jit_trace(data2)<NewLine></code></pre><NewLine><p>It will run correctly, but if the input changes in such a way that the control flow takes a different branch then it will not raise the desired exception or give the desired output.</p><NewLine><p>How can I make this work?  Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/Diego,(Diego),Diego,"May 2, 2019,  3:55pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you want to use data dependent control flow in TorchScript, you need to use script mode, see the docs <a href=""https://pytorch.org/docs/stable/jit.html#creating-torchscript-code"" rel=""nofollow noopener"">here</a> under “Scripting:” for details/examples.</p><NewLine><p>For your model in particular it looks like it would work to add <code>@torch.jit.script</code> to <code>normalize()</code> and remove the call to <code>torch.jit.trace</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, If I do that is there a way to call normalize() without jit. I would like to have to choice of running it with and without jit, just in case I would like to add a not jittable operation in the future</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>There is a global environment variable <code>PYTORCH_JIT=0</code> that you can use to switch off JIT entirely, if you want to run the jitted parts with regular eager pytorch.</p><NewLine><p><a class=""onebox"" href=""https://pytorch.org/docs/stable/jit.html?highlight=pytorch_jit#envvar-PYTORCH_JIT=1"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/stable/jit.html?highlight=pytorch_jit#envvar-PYTORCH_JIT=1</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the replies. However I cannot find the environment variable PYTORCH_JIT in my system. To search for environment variables I use the following code:</p><NewLine><pre><code class=""lang-auto"">for key in os.environ.keys():<NewLine>      if ""py"" in key.lower():<NewLine>          print(key)<NewLine></code></pre><NewLine><p>Which yields:<br/><NewLine>CONDA_PYTHON_EXE<br/><NewLine>PYTHONPATH</p><NewLine><p>Nothing Pytorch related is shown.</p><NewLine><p>On a second note. The <span class=""mention"">@torch.script.jit</span> runs the jitted code and throws the following error.</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;<NewLine>  File ""/home/diego/Projects/torchgeometry/.dev_env/lib/python3.7/site-packages/torch/nn/mo<NewLine>dules/module.py"", line 493, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/diego/Projects/torchgeometry/torchgeometry/image/normalization.py"", line 28,<NewLine>in forward<NewLine>    return normalize(input, self.mean, self.std)<NewLine>RuntimeError:<NewLine>Dimension out of range (expected to be in range of [-1, 0], but got -3) (maybe_wrap_dim at<NewLine>/opt/conda/conda-bld/pytorch_1556653215914/work/c10/core/WrapDimMinimal.h:20)<NewLine>frame #0: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x45 (0x7ff3515d7dc5<NewLine> in /home/diego/Projects/torchgeometry/.dev_env/lib/python3.7/site-packages/torch/lib/libc1<NewLine>0.so)<NewLine>frame #1: &lt;unknown function&gt; + 0x7d4a40 (0x7ff351fc5a40 in /home/diego/Projects/torchgeomet<NewLine>ry/.dev_env/lib/python3.7/site-packages/torch/lib/libcaffe2.so)<NewLine>frame #2: at::native::slice(at::Tensor const&amp;, long, long, long, long) + 0x4e (0x7ff351fc63<NewLine>8e in /home/diego/Projects/torchgeometry/.dev_env/lib/python3.7/site-packages/torch/lib/lib<NewLine>caffe2.so)<NewLine>frame #3: at::TypeDefault::slice(at::Tensor const&amp;, long, long, long, long) const + 0x1a (0<NewLine>x7ff3522255fa in /home/diego/Projects/torchgeometry/.dev_env/lib/python3.7/site-packages/to<NewLine>rch/lib/libcaffe2.so)<NewLine>frame #4: torch::autograd::VariableType::slice(at::Tensor const&amp;, long, long, long, long) c<NewLine>onst + 0x6d3 (0x7ff34a32ea23 in /home/diego/Projects/torchgeometry/.dev_env/lib/python3.7/s<NewLine>ite-packages/torch/lib/libtorch.so.1)<NewLine>frame #5: &lt;unknown function&gt; + 0x985117 (0x7ff34a4ec117 in /home/diego/Projects/torchgeomet<NewLine>ry/.dev_env/lib/python3.7/site-packages/torch/lib/libtorch.so.1)<NewLine>frame #6: &lt;unknown function&gt; + 0xa73df8 (0x7ff34a5dadf8 in /home/diego/Projects/torchgeomet<NewLine>ry/.dev_env/lib/python3.7/site-packages/torch/lib/libtorch.so.1)<NewLine>frame #7: torch::jit::InterpreterState::run(std::vector&lt;c10::IValue, std::allocator&lt;c10::IV<NewLine>alue&gt; &gt;&amp;) + 0x22 (0x7ff34a5d6372 in /home/diego/Projects/torchgeometry/.dev_env/lib/python3<NewLine>.7/site-packages/torch/lib/libtorch.so.1)<NewLine>frame #8: &lt;unknown function&gt; + 0xa5b2d9 (0x7ff34a5c22d9 in /home/diego/Projects/torchgeomet<NewLine>ry/.dev_env/lib/python3.7/site-packages/torch/lib/libtorch.so.1)<NewLine>frame #9: &lt;unknown function&gt; + 0x458bf3 (0x7ff3776d0bf3 in /home/diego/Projects/torchgeomet<NewLine>ry/.dev_env/lib/python3.7/site-packages/torch/lib/libtorch_python.so)<NewLine>frame #10: &lt;unknown function&gt; + 0x12d07a (0x7ff3773a507a in /home/diego/Projects/torchgeome<NewLine>try/.dev_env/lib/python3.7/site-packages/torch/lib/libtorch_python.so)<NewLine>&lt;omitting python frames&gt;<NewLine>frame #37: __libc_start_main + 0xe7 (0x7ff384563b97 in /lib/x86_64-linux-gnu/libc.so.6)<NewLine>:<NewLine>operation failed in interpreter:<NewLine>        raise ValueError('mean lenght and number of channels do not match')<NewLine><NewLine>    if std.shape[0] != data.shape[-3] and std.shape[:2] != data.shape[:2]:<NewLine>        raise ValueError('std lenght and number of channels do not match')<NewLine><NewLine>    if std.shape != mean.shape:<NewLine>        raise ValueError('std and mean must have the same shape')<NewLine><NewLine>    '''<NewLine>    mean = mean[..., :, None, None].to(data.device)<NewLine>           ~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>    std = std[..., :, None, None].to(data.device)<NewLine><NewLine>    out = (data - mean) / std<NewLine><NewLine>    return out<NewLine></code></pre><NewLine><p>What am I doing wrong?</p><NewLine><p>I am using Pytorch 1.1 on an Anaconda envrionment</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>You have to set it yourself, so if your code is in <code>model.py</code>, running</p><NewLine><pre><code class=""lang-auto"">$ PYTORCH_JIT=0 python model.py<NewLine></code></pre><NewLine><p>would set the variable (assuming you’re using bash).</p><NewLine><p>As for your code in script, I think it may have something to do with your input shapes, this snippet below works for me (TorchScript is statically typed so the <code>is_tensor</code> checks would also evaluate to true, so they’re removed). It’s slicing <code>data.shape[-3]</code> implies that the inputs need to be at least 4 dimensions.</p><NewLine><pre><code class=""lang-python"">@torch.jit.script<NewLine>def normalize(data: torch.Tensor, mean: torch.Tensor,<NewLine>              std: torch.Tensor) -&gt; torch.Tensor:<NewLine>    if len(mean) != data.shape[-3] and mean.shape[:2] != data.shape[:2]:<NewLine>        raise ValueError('mean lenght and number of channels do not match')<NewLine><NewLine>    if len(std) != data.shape[-3] and std.shape[:2] != data.shape[:2]:<NewLine>        raise ValueError('std lenght and number of channels do not match')<NewLine><NewLine>    if std.shape != mean.shape:<NewLine>        raise ValueError('std and mean must have the same shape')<NewLine><NewLine>    mean = mean[..., :, None, None].to(data.device)<NewLine>    std = std[..., :, None, None].to(data.device)<NewLine><NewLine>    out = data.sub(mean).div(std)<NewLine><NewLine>    return out<NewLine><NewLine>print(normalize(torch.ones(2, 2, 2, 2), torch.ones(2, 2, 2, 2), torch.ones(2, 2, 2, 2)))<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried executing the script with $PYTORCH_JIT=0 as you said and got the following error:</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/69bfbe461f51a685cc9335b99f07dd5d65f37258"" href=""https://discuss.pytorch.org/uploads/default/original/2X/6/69bfbe461f51a685cc9335b99f07dd5d65f37258.png"" title=""image.png""><img alt=""image"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/6/69bfbe461f51a685cc9335b99f07dd5d65f37258_2_10x10.png"" height=""381"" src=""https://discuss.pytorch.org/uploads/default/original/2X/6/69bfbe461f51a685cc9335b99f07dd5d65f37258.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">899×497 72.7 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>As for the runtimeerror. I don’t think there is anything wrong with the code, since it will run in native Pytorch. I also had to comment the istensor lines since they raise a different exception.<br/><NewLine>I dont know why its not commented in the snippet I pasted but I did comment those lines, so even if the data.shape[-3] is wrong those lines should not run at all.</p><NewLine><p>P.S: -3 requires at least 3 dimensions since the last one is -1.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like we had a bug with <code>PYTORCH_JIT=0</code>, it’s fixed in <a href=""https://github.com/pytorch/pytorch/pull/20120"" rel=""nofollow noopener"">#20120</a>.</p><NewLine><p>Can you post the full code you are using to run this that generates the error?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sure thing. Its just a simple test:</p><NewLine><pre><code class=""lang-auto"">    def test_normalize(self):<NewLine><NewLine>        # prepare input data<NewLine>        data = torch.ones(1, 2, 2)<NewLine>        mean = torch.tensor([0.5])<NewLine>        std = torch.tensor([2.0])<NewLine><NewLine>        # expected output<NewLine>        expected = torch.tensor([0.25]).repeat(1, 2, 2).view_as(data)<NewLine><NewLine>        f = image.Normalize(mean, std)<NewLine>        assert_allclose(f(data), expected)<NewLine></code></pre><NewLine><p>And this is the normalize class definition</p><NewLine><pre><code class=""lang-auto"">class Normalize(nn.Module):<NewLine><NewLine>    """"""<NewLine>    Normalize a tensor image or a batch of tensor images<NewLine>    with mean and standard deviation. Input must be a tensor of shape (C, H, W)<NewLine>    or a batch of tensors (*, C, H, W).<NewLine>    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels,<NewLine>    this transform will normalize each channel of the input ``torch.*Tensor``<NewLine>    i.e. ``input[channel] = (input[channel] - mean[channel]) / std[channel]``<NewLine><NewLine>    Args:<NewLine>        mean (torch.Tensor): Mean for each channel.<NewLine>        std (torch.Tensor): Standard deviation for each channel.<NewLine>    """"""<NewLine><NewLine>    def __init__(self, mean: torch.Tensor, std: torch.Tensor) -&gt; None:<NewLine><NewLine>        super(Normalize, self).__init__()<NewLine><NewLine>        self.mean = mean<NewLine>        self.std = std<NewLine><NewLine>    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:  # type: ignore<NewLine>        return normalize(input, self.mean, self.std)<NewLine><NewLine>    def __repr__(self):<NewLine>        repr = '(mean={0}, std={1})'.format(self.mean, self.std)<NewLine>        return self.__class__.__name__ + repr<NewLine></code></pre><NewLine><p>P.S: This and every other test pass perfectly without jit</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Diego; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Diego; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Diego; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Diego; <NewLine> ,"REPLY_DATE 1: May 2, 2019,  5:41pm; <NewLine> REPLY_DATE 2: May 2, 2019,  7:56pm; <NewLine> REPLY_DATE 3: May 2, 2019,  8:25pm; <NewLine> REPLY_DATE 4: May 3, 2019,  7:10pm; <NewLine> REPLY_DATE 5: May 3, 2019,  9:27pm; <NewLine> REPLY_DATE 6: May 3, 2019,  9:55pm; <NewLine> REPLY_DATE 7: May 4, 2019, 12:19am; <NewLine> REPLY_DATE 8: May 4, 2019, 12:25am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
43924,Can&rsquo;t save the LSTM model with torch.jit.script_method,2019-04-29T11:56:22.446Z,2,521,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">class BidirectionalLSTM(torch.jit.ScriptModule):<NewLine>    # Inputs hidden units Out<NewLine>    def __init__(self, nIn, nHidden, nOut):<NewLine>        super(BidirectionalLSTM, self).__init__()<NewLine><NewLine>        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)<NewLine>        self.embedding = nn.Linear(nHidden * 2, nOut)<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def forward(self, input):<NewLine>        recurrent, _ = self.rnn(input)<NewLine>        T, b, h = recurrent.size()<NewLine>        t_rec = recurrent.view(T * b, h)<NewLine><NewLine>        output = self.embedding(t_rec)  # [T * b, nOut]<NewLine>        output = output.view(T, b, -1)<NewLine><NewLine>        return output<NewLine><NewLine>net = BidirectionalLSTM(256,256, 512)<NewLine>net.save('model.pt')<NewLine></code></pre><NewLine><p>got the error:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""/home/xxh/Desktop/crnn/models/test.py"", line 24, in &lt;module&gt;<NewLine>    net.save('model.pt')<NewLine>RuntimeError: <NewLine>could not export python function call &lt;python_value&gt;. Remove calls to python functions before export.:<NewLine>@torch.jit.script_method<NewLine>def forward(self, input):<NewLine>    recurrent, _ = self.rnn(input)<NewLine>                   ~~~~~~~~ &lt;--- HERE<NewLine>    T, b, h = recurrent.size()<NewLine>    t_rec = recurrent.view(T * b, h)<NewLine><NewLine>    output = self.embedding(t_rec)  # [T * b, nOut]<NewLine>    output = output.view(T, b, -1)<NewLine><NewLine>    return output<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/XiaXuehai,(Xia Xuehai),XiaXuehai,"April 29, 2019, 11:59am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think that maybe the problem is that <code>nn.LSTM</code> and <code>nn.Linear</code> are not traced.</p><NewLine><p>From the docs:</p><NewLine><blockquote><NewLine><p>To be able to save a module, it must not make any calls to native python functions. This means that all submodules must be subclasses of ScriptModules as well.</p><NewLine></blockquote><NewLine><p>So you have to trace the inner modules as done in the example documentation for Conv2D</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>from torch.jit import ScriptModule, script_method, trace<NewLine><NewLine>class MyScriptModule(ScriptModule):<NewLine>    def __init__(self):<NewLine>        super(MyScriptModule, self).__init__()<NewLine>        # trace produces a ScriptModule's conv1 and conv2<NewLine>        self.conv1 = trace(nn.Conv2d(1, 20, 5), torch.rand(1, 1, 16, 16))<NewLine>        self.conv2 = trace(nn.Conv2d(20, 20, 5), torch.rand(1, 20, 16, 16))<NewLine><NewLine>    @script_method<NewLine>    def forward(self, input):<NewLine>      input = F.relu(self.conv1(input))<NewLine>      input = F.relu(self.conv2(input))<NewLine>      return input<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/acobobby"">@acobobby</a> You are correct but we have some special infrastructure to support <code>torch.nn</code> Modules without needing to trace them (see <a href=""https://pytorch.org/docs/master/jit.html#builtin-functions"" rel=""nofollow noopener"">builtin functions</a> in the master docs for details), so just assigning them as submodules without tracing them is fine (e.g.<br/><NewLine><code>self.conv1 = nn.Conv2d(1, 20, 5)</code>).</p><NewLine><p>I don’t remember if these changes were put in an official release yet though, <a class=""mention"" href=""/u/xiaxuehai"">@XiaXuehai</a> could you try to reproduce your issue on the PyTorch nightly build? Your code snippet runs fine for me on it.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/driazati"">@driazati</a> Thanks, PyTorch nightly is fine.</p><NewLine><p>Another question. How to load the static_dict in<code>torch.jit.script_method</code>, the net name is changed, loaded by <code>strict=False</code> is not correct!</p><NewLine><p>Solved. I changed the pretrained model’s layer name one by one.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/acobobby; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/XiaXuehai; <NewLine> ,"REPLY_DATE 1: April 29, 2019, 12:20pm; <NewLine> REPLY_DATE 2: April 30, 2019,  9:36am; <NewLine> REPLY_DATE 3: April 30, 2019,  9:37am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
43985,It is possible to iterate torch._C.graph?,2019-04-30T05:44:41.970Z,0,474,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi folks,</p><NewLine><p>I’m following this <a href=""https://pytorch.org/docs/stable/onnx.html"" rel=""nofollow noopener"">Official PyTorch ONNX tutorial</a> and I would like to iterate through the <code>torch._C.graph</code> that generated so I can obtain the “layer indexes” from the trace (i.e. for the AlexNet tutorial, it is <code>%17</code>, <code>%18</code> and so on).</p><NewLine><p>Is that possible? The reason why I want the indexes because I want to do a one-to-one layer comparison between PyTorch and Core ML layers generated by my model. Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/alwc,(Alex),alwc,"April 30, 2019,  5:44am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think I figured it out. For those who interested in getting the “layer indexes”, or to be specific, the “output indexes”, check out this file <a href=""https://github.com/pytorch/pytorch/blob/98e312cf96f6a5e23933cd8794097063ee3cbc8c/torch/utils/tensorboard/_pytorch_graph.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/98e312cf96f6a5e23933cd8794097063ee3cbc8c/torch/utils/tensorboard/_pytorch_graph.py</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alwc; <NewLine> ,"REPLY_DATE 1: April 30, 2019,  7:09am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
43827,How to convert the model with no fixed size input to Torch Script?,2019-04-28T09:42:59.970Z,1,500,"<div class=""post"" itemprop=""articleBody""><NewLine><p>If the size of input image is not fixed, how to convert the model to torch script?</p><NewLine></div>",https://discuss.pytorch.org/u/XiaXuehai,(Xia Xuehai),XiaXuehai,"April 28, 2019,  9:43am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You should use the <span class=""mention"">@torch.jit.script</span> annotation, which will recover the full semantics of your model (including any control flow that depends on input size). See the tutorial for more info: <a href=""https://pytorch.org/tutorials/advanced/cpp_export.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/cpp_export.html</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks. But I got an error.</p><NewLine><p>my code is like this:</p><NewLine><pre><code class=""lang-auto"">class BidirectionalLSTM(torch.jit.ScriptModule):<NewLine>    __constants__ = ['rnn']<NewLine>    # Inputs hidden units Out<NewLine>    def __init__(self, nIn, nHidden, nOut):<NewLine>        super(BidirectionalLSTM, self).__init__()<NewLine><NewLine>        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)<NewLine>        self.embedding = nn.Linear(nHidden * 2, nOut)<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def forward(self, input):<NewLine>        recurrent, _ = self.rnn(input)<NewLine>        T, b, h = recurrent.size()<NewLine>        t_rec = recurrent.view(T * b, h)<NewLine><NewLine>        output = self.embedding(t_rec)  # [T * b, nOut]<NewLine>        output = output.view(T, b, -1)<NewLine><NewLine>        return output<NewLine><NewLine>xx = BidirectionalLSTM(256, 512, 512)<NewLine>xx.save(""xxh.pt"")<NewLine></code></pre><NewLine><p>got an error</p><NewLine><pre><code class=""lang-auto"">could not export python function call &lt;python_value&gt;. Remove calls to python functions before export.:<NewLine>@torch.jit.script_method<NewLine>def forward(self, input):<NewLine>    recurrent, _ = self.rnn(input)<NewLine>                   ~~~~~~~~ &lt;--- HERE<NewLine></code></pre><NewLine><p>I add one line of code:<br/><NewLine><code>__constants__ = [ 'rnn']</code></p><NewLine><p>and got this:</p><NewLine><pre><code class=""lang-auto"">TypeError: 'LSTM' object for attribute 'rnn' is not a valid constant.<NewLine>Valid constants are:<NewLine>  1. a nn.ModuleList<NewLine>  2. a value of type {bool, float, int, str, NoneType, function, device, layout, dtype}<NewLine>  3. a list or tuple of (2)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/XiaXuehai; <NewLine> ,"REPLY_DATE 1: April 28, 2019,  7:51pm; <NewLine> REPLY_DATE 2: April 29, 2019,  8:08am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
43066,Maintaining dropout layer for deployment,2019-04-19T17:27:01.282Z,0,650,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I am trying to deploy a pytorch model with dropout layers using torchscript into the c++ api. However, I run into an issue where the parameters which can be loaded using the regular python script and the torchscript model are different. When setting up the model by inheriting from torch.jit.ScriptModule (as is necessary to export the annotated model) I observe that for every dropout layer, a parameter named *.training which is empty is created and causes a problem when loading weights from a file stream, since the model weights exported from the model inherited from nn.Module don’t have a parameter associated with the dropout layers (see the example outputs below).</p><NewLine><p>I realize that it is uncommon to preserve dropout layers outside of training, however in this case, the model is supposed to retain the dropout layers to exhibit stochastic behavior.<br/><NewLine>Below is 2 versions of the code i run for the model, one which uses pytorch and the other which i try to create a torchscript module, along with the code I have attached the output comparison between the state dictionary of the torchscript and the pytorch modules.</p><NewLine><p>Below is the output from printing the first few parameters in the model state dictionary when the model is created using torch.jit.ScriptModule (notice the empty tensor parameter for layer 2, “fc.2.training”)</p><NewLine><pre><code class=""lang-auto"">Model's state_dict:                                                                                                  fc.0.bias        torch.Size([1280])<NewLine>fc.0.weight      torch.Size([1280, 74])<NewLine>fc.1.weight      torch.Size([1])<NewLine>fc.2.training    torch.Size([])<NewLine>fc.3.bias        torch.Size([896]) <NewLine>fc.3.weight      torch.Size([896, 1280]) <NewLine></code></pre><NewLine><p>Below is the output from printing the first few parameters in the model state dictionary when the model is created using nn.Module (here notice there is no parameter associated with layer 2)</p><NewLine><pre><code class=""lang-auto"">Model's state_dict:                                                                                                  fc.0.weight      torch.Size([1280, 74])<NewLine>fc.0.bias        torch.Size([1280])<NewLine>fc.1.weight      torch.Size([1])<NewLine>fc.3.weight      torch.Size([896, 1280])<NewLine>fc.3.bias        torch.Size([896]) <NewLine></code></pre><NewLine><p>Below is a photo of the 2 python programs used to produce the results</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/bcc104f0f30c62cac8bbee6e1ec89b0993c941fa"" href=""https://discuss.pytorch.org/uploads/default/original/2X/b/bcc104f0f30c62cac8bbee6e1ec89b0993c941fa.png"" title=""Dropout_code.PNG""><img alt=""Dropout_code"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/b/bcc104f0f30c62cac8bbee6e1ec89b0993c941fa_2_10x10.png"" height=""350"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/b/bcc104f0f30c62cac8bbee6e1ec89b0993c941fa_2_690x350.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/b/bcc104f0f30c62cac8bbee6e1ec89b0993c941fa_2_690x350.png, https://discuss.pytorch.org/uploads/default/optimized/2X/b/bcc104f0f30c62cac8bbee6e1ec89b0993c941fa_2_1035x525.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/b/bcc104f0f30c62cac8bbee6e1ec89b0993c941fa_2_1380x700.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Dropout_code.PNG</span><span class=""informations"">1795×912 90.5 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>If anyone can help me figure out how to export my trained model so that I can load it into a C++ program with the dropout preserved, that would be much appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/Christopher_Johnson,(Christopher Johnson),Christopher_Johnson,"April 20, 2019,  6:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The <code>training</code> parameter was a hack that is not needed anymore, <a href=""https://github.com/pytorch/pytorch/pull/19587"" rel=""nofollow noopener"">this PR</a> fixes it so it won’t show up in the state dict anymore. Could you post your model as code so we can run it and repro your issue to make sure it’s fixed? Thanks!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>(post withdrawn by author, will be automatically deleted in 24 hours unless flagged)</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import pickle<NewLine>from torch.autograd import Variable<NewLine>import numpy as np<NewLine>import pypcd<NewLine><NewLine><NewLine>class Encoder_End2End_Annotated(torch.jit.ScriptModule):<NewLine>    __constants__ = ['encoder']<NewLine><NewLine>    def __init__(self):<NewLine>        super(Encoder_End2End_Annotated, self).__init__()<NewLine>        self.encoder = nn.Sequential(nn.Linear(16053, 256), nn.PReLU(), #adds dropouts are not expected<NewLine>                                    nn.Linear(256, 256), nn.PReLU(),<NewLine>                                 	nn.Linear(256, 60))<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def forward(self, x):<NewLine>        x = self.encoder(x)<NewLine>        return x<NewLine><NewLine><NewLine>class MLP_NN_Annotated(torch.jit.ScriptModule):<NewLine>    __constants__ = ['fc']<NewLine>    def __init__(self):<NewLine>        super(MLP_NN_Annotated, self).__init__()<NewLine>        self.fc = nn.Sequential(<NewLine>                	nn.Linear(74, 1280), nn.PReLU(), nn.Dropout(),<NewLine>                	nn.Linear(1280, 896), nn.PReLU(), nn.Dropout(),<NewLine>                	nn.Linear(896, 512), nn.PReLU(), nn.Dropout(),<NewLine>                	nn.Linear(512, 384), nn.PReLU(), nn.Dropout(),<NewLine>                	nn.Linear(384, 256), nn.PReLU(), nn.Dropout(),<NewLine>                	nn.Linear(256, 128), nn.PReLU(), nn.Dropout(),<NewLine>                	nn.Linear(128, 64), nn.PReLU(), nn.Dropout(),<NewLine>                	nn.Linear(64, 32), nn.PReLU(),<NewLine>                	nn.Linear(32, 7))<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def forward(self, x):<NewLine>        out = self.fc(x)<NewLine>        return out<NewLine><NewLine><NewLine><NewLine><NewLine># Creates the script<NewLine>encoder = Encoder_End2End_Annotated()<NewLine>MLP = MLP_NN_Annotated()<NewLine><NewLine>#modified to load weights<NewLine>device = torch.device('cpu')<NewLine>cae_filename = 'cae_encoder_140.pkl'<NewLine>mlp_filename = 'mlp_PReLU_ae_dd140.pkl'<NewLine>encoder.load_state_dict(torch.load(cae_filename, map_location=device))<NewLine><NewLine># Print model's state_dict<NewLine>print(""Model's state_dict:"")<NewLine>for param_tensor in MLP.state_dict():<NewLine>    print(param_tensor, ""\t"", MLP.state_dict()[param_tensor].size())<NewLine><NewLine><NewLine>#print(MLP.state_dict())<NewLine><NewLine><NewLine>MLP.load_state_dict(torch.load(mlp_filename, map_location=device))<NewLine><NewLine><NewLine>encoder.save(""encoder_annotated.pt"")<NewLine>encoder.save(""mlp_annotated.pt"")<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/asimeonov; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Christopher_Johnson; <NewLine> ,"REPLY_DATE 1: April 22, 2019, 11:02pm; <NewLine> REPLY_DATE 2: April 26, 2019,  3:45pm; <NewLine> REPLY_DATE 3: April 25, 2019,  8:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
42833,Cannot access nn.Linear.in_features in ScriptModule,2019-04-17T11:23:05.692Z,0,333,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-python"">import torch.nn as nn<NewLine>import torch.jit as jit<NewLine><NewLine>class TestModule(jit.ScriptModule):<NewLine>  def __init__(self):<NewLine>    super().__init__()<NewLine>    self.linear = nn.Linear(16, 16)<NewLine><NewLine><NewLine>m = TestModule()<NewLine>print(m.linear.in_features)<NewLine></code></pre><NewLine><p>The code above throws AttributeError</p><NewLine><pre><code class=""lang-python"">&gt;&gt;&gt; m.linear.in_features<NewLine>Traceback (most recent call last):<NewLine>  File ""/home/qbx2/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1197, in __getattr__<NewLine>    return ScriptModule.__getattr__(self, attr)<NewLine>  File ""/home/qbx2/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1102, in __getattr__<NewLine>    return Module.__getattr__(self, attr)<NewLine>  File ""/home/qbx2/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 535, in __getattr__<NewLine>    type(self).__name__, name))<NewLine>AttributeError: 'WeakScriptModuleProxy' object has no attribute 'in_features'<NewLine><NewLine>During handling of the above exception, another exception occurred:<NewLine><NewLine>Traceback (most recent call last):<NewLine>  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;<NewLine>  File ""/home/qbx2/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1200, in __getattr__<NewLine>    return getattr(self.__dict__[""_original""](), attr)<NewLine>AttributeError: 'NoneType' object has no attribute 'in_features'<NewLine></code></pre><NewLine><p>Looking at the code (<a href=""https://github.com/pytorch/pytorch/blob/master/torch/jit/"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/jit/</a><strong>init</strong>.py#L1226), I think WeakScriptModuleProxy should have copied in_features and other fields to self here.</p><NewLine><p>I’m just posting this issue here to check if any workaround for this already exists.</p><NewLine><p>Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/qbx2,(SunYeop Lee),qbx2,"April 17, 2019, 11:23am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>When <code>nn</code> modules are used in a <code>ScriptModule</code>, they get wrapped in an internal class that only copies the <code>nn</code> module’s buffers, submodules, and parameters. So as a workaround you could do something like <code>self.linear.weight.shape[1]</code> to get the <code>in_features</code>.</p><NewLine><p>Really we should be copying everything though, could you file an <a href=""https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3Ajit"" rel=""nofollow noopener"">issue on GitHub</a> with the same example code you posted here?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, opened an issue: <a href=""https://github.com/pytorch/pytorch/issues/19363"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/19363</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/qbx2; <NewLine> ,"REPLY_DATE 1: April 17, 2019,  5:58pm; <NewLine> REPLY_DATE 2: April 17, 2019,  6:12pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
42709,"How to create a variable of optional type(e.g., Optional[Tuple[int,int]]), with C++ libtorch jit api?",2019-04-16T07:27:22.412Z,1,313,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a pytorch script which accepts Optional[Tuple[int, int]] as the input. The script is exported to “script.pb”.<br/><NewLine>Then in C++ api, the script.pb is loaded. But how can I create C++ obj of type Optional[Tuple[int, int]]? Is there any documentation?</p><NewLine></div>",https://discuss.pytorch.org/u/eff317bb3b850d350ea9,(露 叶),eff317bb3b850d350ea9,"April 16, 2019,  7:28am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like there was a bug exposed by this use case…, the following will apply once <a href=""https://github.com/pytorch/pytorch/pull/19311"" rel=""nofollow noopener"">this fix</a> is merged.</p><NewLine><p>Our C++ value type (<code>IValue</code>) doesn’t have any concept of optional/not, it is either some concrete value (e.g. <code>Tuple[int, int]</code>) or <code>None</code>. Only the JIT’s type system knows what things can be optional. You can see a small example of how to construct one <a href=""https://github.com/driazati/torchscript-examples/blob/master/values/ivalues.cpp#L31-L38"" rel=""nofollow noopener"">here</a>.</p><NewLine><p>The <a href=""https://github.com/pytorch/pytorch/blob/master/test/cpp/api/jit.cpp"" rel=""nofollow noopener"">C++ API tests</a> may also be helpful.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/driazati"">@driazati</a> let’s make an issue tracking improved C++ API documentation, especially of IValues, etc… I’m seeing a lot of questions on it, and what we have now is not great.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just curious, in which case you need to create C++ obj of optional type? I think if you have Optional[Tuple[int, int]], you can always pass concrete objs into.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>There is already an issue open, <a href=""https://github.com/pytorch/pytorch/issues/17165"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/17165</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/eellison; <NewLine> ,"REPLY_DATE 1: April 16, 2019,  5:27pm; <NewLine> REPLY_DATE 2: April 16, 2019,  5:43pm; <NewLine> REPLY_DATE 3: April 16, 2019,  5:56pm; <NewLine> REPLY_DATE 4: April 17, 2019, 12:06am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
42664,Transforming a model based on graph information,2019-04-15T21:14:01.983Z,0,197,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am writing a graph transform that needs to make modifications on a model based on graph information. My question is, how do I keep references between module names in the model and node names in the graph?</p><NewLine><p>In other words, how do I reference parameters in the parent module when I am in a child module in the model? As far as I can tell, there is no graph information until the model has been compiled.</p><NewLine><p>For example, let’s look at a snippet from Resnet18</p><NewLine><pre><code class=""lang-auto"">ResNet(<NewLine>  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)<NewLine>  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>  (relu): ReLU(inplace)<NewLine>  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)<NewLine>  ...<NewLine>  (layer2): Sequential(<NewLine>    (0): BasicBlock(<NewLine>      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<NewLine>      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu): ReLU(inplace)<NewLine>      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (downsample): Sequential(<NewLine>        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)<NewLine>        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      )<NewLine>    )<NewLine>    ...<NewLine></code></pre><NewLine><p>The node <code>layer2.0.downsample.0</code> gets its input from the node <code>bn1</code>, but there is no way to tell this until the graph is compiled. In my case, when I traverse the resnet model and I get to <code>layer2.0.downsample.0</code>, I want to know the number of <code>out_channels</code> from the parent node.</p><NewLine><p>Stealing some ideas from the <code>hiddenlayer</code> project, I know I can use the jit compiler to get the graph</p><NewLine><pre><code class=""lang-auto"">trace, out = torch.jit.get_trace_graph(model, torch.onnx._optimize_trace(trace, torch.onnx.OperatorExportTypes.ONNX)<NewLine>torch_graph = trace.graph()<NewLine></code></pre><NewLine><p>which can then be traversed in trace order. However, all the model names and such are lost at this point. I could write my transformer by compiling a lookup table by traversing through the graph in trace order, and then applying the changes from the lookup table back to the model. But how would I keep references to the original modules while traversing the graph?</p><NewLine><p>Any help is welcome, thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/mateja,(Mateja Putic),mateja,"April 16, 2019, 12:48am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You don’t know what your parent is. And that’s a design decision rather than a missing feature. For one thing you can use a single model several times.</p><NewLine><p>If you want to pass runtime information to modules, pass them in as arguments (or use a functional interface in the first place).</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: April 16, 2019,  5:32am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
42418,BERT torch graph tensor shape issues,2019-04-12T17:29:28.024Z,0,206,"<div class=""post"" itemprop=""articleBody""><NewLine><p>All!</p><NewLine><p>First of all, I’m new to this forum so excuse me if I’m not addressing the right audience.</p><NewLine><p>I’m working on getting BERT forward and backprop traces from PyTorch jit and I ran into a problem: I seem to have encountered a case where the tensor dimensions don’t line up with my expectations. I’ll attach the full graph dump, but it’s unwieldy, so let me summarize:</p><NewLine><p>The forward graph has the following relevant lines:</p><NewLine><pre><code class=""lang-auto"">  %x.8 : Float(1, 3, 10) = aten::add(%output.6, %bias, %28)<NewLine>  %self_size.3 : int[] = aten::size(%x.8)<NewLine>  %57 : int = prim::Constant[value=-1](), scope: BERT/SublayerConnection<NewLine>  %114 : int[] = prim::ListConstruct(%57), scope: BERT/SublayerConnection/LayerNorm[norm]<NewLine>  %mean : Float(1, 3, 1) = aten::mean(%x.8, %114, %44), scope: BERT/SublayerConnection/LayerNorm[norm]<NewLine>  %307 : int[] = aten::size(%mean)<NewLine></code></pre><NewLine><p>The backprop graph has three inputs that are important here, with the following connections:</p><NewLine><pre><code class=""lang-auto"">   %76 : int[],           &lt;== %114          (input[76]  connected to output[40])<NewLine>   %self_size.2 : int[],  &lt;== %self_size.3  (input[106] connected to output[70]) <NewLine>   %108 : int[],          &lt;== %307          (input[108] connected to output[72])<NewLine></code></pre><NewLine><p>And these values propagate through the backprop graph as follows:</p><NewLine><pre><code class=""lang-auto"">  %130 : int = prim::Constant[value=0](), scope: BERT/BERTEmbedding[embedding]/TokenEmbedding[token]<NewLine>  %300 : int = aten::select(%76, %130)<NewLine>  %225 : Tensor, %226 : Tensor = prim::GradOf[name=""aten::sub""](%224)<NewLine>    block0():<NewLine>      %227 : Tensor = aten::_grad_sum_to_size(%224, %self_size.2)<NewLine>      %228 : Tensor = aten::neg(%224)<NewLine>      %229 : Tensor = aten::mul(%228, %134), scope: BERT/SublayerConnection/LayerNorm[norm]<NewLine>      %230 : Tensor = aten::_grad_sum_to_size(%229, %108), scope: BERT/SublayerConnection/LayerNorm[norm]<NewLine>      -&gt; (%227, %230)<NewLine>  %301 : Tensor = aten::unsqueeze(%226, %300)<NewLine>  %128 : bool = prim::Constant[value=0](), scope: BERT/BERTEmbedding[embedding]/TokenEmbedding[token]<NewLine>  %302 : Tensor = aten::expand(%301, %self_size.2, %128)<NewLine></code></pre><NewLine><p>So, if I read this right:</p><NewLine><ul><NewLine><li>%128 is just a constant ‘false’, saying that the expand is an explicit one.</li><NewLine><li>The second parameter (%self_size.2) is the list [1,3,10]</li><NewLine><li>The first parameter (%301) is the result of an unsqueeze(%226, -1), where %226 is _grad_sum_to_size(%229, %108). %108 in turn is [1,3,1].</li><NewLine></ul><NewLine><p>Winding it all forward again:</p><NewLine><ul><NewLine><li>%226 will have the shape of %108, that is [1,3,1]</li><NewLine><li>%301 will have one extra dimension added to it to the end by the unsqueeze, so it becomes [1,3,1,1]</li><NewLine><li>Finally in the last line, it seems we try to expand a Tensor of the shape <strong>[1,3,1,1]</strong> by an expansion list of <strong>[1,3,10]</strong>.</li><NewLine></ul><NewLine><p>This doesn’t seem right, in fact it blows up if I try to do this in Python.</p><NewLine><p>So, can you help me figure out where my logic is incorrect? What should I look at, what do I misunderstand about these operations?</p><NewLine><p>The full graphs and their connectivity vectors are here: <a href=""http://www.modularcircuits.com/download/bert.log"" rel=""nofollow noopener"">http://www.modularcircuits.com/download/bert.log</a></p><NewLine><p>Thanks,<br/><NewLine>Andras</p><NewLine></div>",https://discuss.pytorch.org/u/andras_tantos,(Andras Tantos),andras_tantos,"April 12, 2019,  5:33pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Andras,</p><NewLine><p>Thanks for asking here. I don’t know how exactly the program you have is wrong. But if it blows up in Python already, you can use pdb to debug on your python (non-jit) program, and see what’s wrong in the intermediate steps. Put something in your program <code>__import__('pdb').set_trace()</code> and you can see where it becomes wrong, so that we might have more context.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply.</p><NewLine><p>What I meant by ‘blowing up’ is that if I manually call ‘expand’ with a tensor of dimensions [1,3,1,1] and an expansion list of [1,3,10], I get a runtime error stating that the expansion list is shorter than the number of dimensions of the input tensor. Which of course makes complete sense, but for the life of me I can’t figure out how that wouldn’t be the case in this particular graph.</p><NewLine><p>My original goal is not to execute the graph, merely to extract type information from it for each of the values. and that’s where of course I run into trouble with this particular subsection of it.</p><NewLine><p>Thanks again,<br/><NewLine>Andras</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/andras_tantos; <NewLine> ,"REPLY_DATE 1: April 12, 2019,  6:01pm; <NewLine> REPLY_DATE 2: April 12, 2019,  7:02pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
42299,Namedtuple in C++ interface,2019-04-11T16:05:27.649Z,0,319,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I have traced and saved a model that gives as output a python namedtuple. How do I go about accessing the fields of this output in the C++ api?</p><NewLine><p>For eg -<br/><NewLine>from collections import named_tuple<br/><NewLine>NT = namedtuple(‘NT’, [‘output1’, ‘output2’])<br/><NewLine>def f(x):<br/><NewLine>output_tuple = NT(x * 2, x / 2)<br/><NewLine>return output_tuple</p><NewLine><p>I am able to trace and save the above function using torch.jit.trace and would like to index into this output using the keyword arguments for NT in C++.</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/Siddhesh_Mhatre,(Siddhesh Mhatre),Siddhesh_Mhatre,"April 11, 2019,  4:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>namedtuple</code> does not work out of the box yet (though it is on our roadmap). In your example it is de-sugared to a regular tuple. You can see what’s going on under the hood with the <code>.code</code> property of the traced code. For example</p><NewLine><pre><code class=""lang-python"">from collections import namedtuple<NewLine>MyTuple = namedtuple('MyTuple', ['output1', 'output2'])<NewLine>def f(x):<NewLine>    output_tuple = MyTuple(x * 2, x / 2)<NewLine>    return output_tuple<NewLine>traced = torch.jit.trace(f, (torch.ones(2, 2)))<NewLine>print(traced.code)<NewLine></code></pre><NewLine><p>Outputs</p><NewLine><pre><code class=""lang-auto"">def forward(self,<NewLine>    x: Tensor) -&gt; Tuple[Tensor, Tensor]:<NewLine>  _0 = (torch.mul(x, CONSTANTS.c0), torch.div(x, CONSTANTS.c0))<NewLine>  return _0<NewLine></code></pre><NewLine><p>In C++ you can access the tuple like this</p><NewLine><pre><code class=""lang-cpp"">torch::IValue output = my_module-&gt;forward(...);<NewLine>std::vector&lt;torch::IValue&gt;&amp; tuple_elements = output-&gt;toTuple().elements();<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: April 12, 2019,  7:35am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
41812,Cannot Obtain Similar DL Prediction Result in Pytorch C++ API Compared to Python,2019-04-06T07:54:04.597Z,0,141,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have trained a deep learning model using unet architecture in order to segment the nuclei in python and pytorch. I would like to load this pretrained model and make prediction in C++. For this reason, I obtained trace file(with pt extension). Then, I have run this code:</p><NewLine><pre><code class=""lang-auto"">    int main(int argc, const char* argv[]) {<NewLine><NewLine><NewLine>        Mat image;<NewLine>        image = imread(""C:/Users/Sercan/PycharmProjects/samplepyTorch/test_2.png"", CV_LOAD_IMAGE_COLOR);<NewLine><NewLine>        std::shared_ptr&lt;torch::jit::script::Module&gt; module = torch::jit::load(""C:/Users/Sercan/PycharmProjects/samplepyTorch/epistroma_unet_best_model_trace.pt"");<NewLine>        module-&gt;to(torch::kCUDA);<NewLine><NewLine><NewLine>        std::vector&lt;int64_t&gt; sizes = { 1, 3, image.rows, image.cols };<NewLine>        torch::TensorOptions options(torch::ScalarType::Byte);<NewLine>        torch::Tensor tensor_image = torch::from_blob(image.data, torch::IntList(sizes), options);<NewLine>        tensor_image = tensor_image.toType(torch::kFloat);<NewLine>        auto result = module-&gt;forward({ tensor_image.to(at::kCUDA) }).toTensor();<NewLine><NewLine><NewLine>        result = result.squeeze().cpu();<NewLine>        result = at::sigmoid(result);<NewLine><NewLine>        cv::Mat img_out(image.rows, image.cols, CV_32F, result.data&lt;float&gt;());<NewLine><NewLine>        cv::imwrite(""img_out.png"", img_out);<NewLine><NewLine>    }<NewLine></code></pre><NewLine><p>Image outputs ( First image: test image, Second image: Python prediction result, Third image: C++ prediction result):<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/e5b838682ba6d698981d329d2f46e80e02988d87"" href=""https://discuss.pytorch.org/uploads/default/original/2X/e/e5b838682ba6d698981d329d2f46e80e02988d87.png"" title=""concatenated.png""><img alt=""concatenated"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/e/e5b838682ba6d698981d329d2f46e80e02988d87_2_10x10.png"" height=""230"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/e/e5b838682ba6d698981d329d2f46e80e02988d87_2_690x230.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/e/e5b838682ba6d698981d329d2f46e80e02988d87_2_690x230.png, https://discuss.pytorch.org/uploads/default/original/2X/e/e5b838682ba6d698981d329d2f46e80e02988d87.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/e/e5b838682ba6d698981d329d2f46e80e02988d87.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">concatenated.png</span><span class=""informations"">768×256 114 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>As you see, C++ prediction output is not similar to python prediction output. Could you offer a solution to fix this problem?</p><NewLine></div>",https://discuss.pytorch.org/u/sercan,(sercan),sercan,"April 6, 2019,  7:54am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This seems like a bug (maybe related to <a href=""https://github.com/pytorch/pytorch/issues/18617"" rel=""nofollow noopener"">#18617</a>), could you <a href=""https://github.com/pytorch/pytorch/issues"" rel=""nofollow noopener"">file a report</a> on GitHub? Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: April 11, 2019,  8:15pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
41795,How to make jit script works with a custom RNN module (renamed),2019-04-06T00:55:13.999Z,0,854,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I created an issue at:<br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch.github.io/issues/181"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch.github.io</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://github.com/liqunfu"" rel=""nofollow noopener""><NewLine><img class=""thumbnail onebox-avatar"" height=""96"" src=""https://avatars1.githubusercontent.com/u/3318051?v=2&amp;s=96"" width=""96""/><NewLine></a><NewLine><h4><a href=""https://github.com/pytorch/pytorch.github.io/issues/181"" rel=""nofollow noopener"" target=""_blank"">Issue: pytorch blog the-road-to-1_0 has example code that failed to work</a></h4><NewLine><div class=""date"" style=""margin-top:10px;""><NewLine><div class=""user"" style=""margin-top:10px;""><NewLine>	opened by <a href=""https://github.com/liqunfu"" rel=""nofollow noopener"" target=""_blank"">liqunfu</a><NewLine>	on <a href=""https://github.com/pytorch/pytorch.github.io/issues/181"" rel=""nofollow noopener"" target=""_blank"">2019-04-06</a><NewLine></div><NewLine><div class=""user""><NewLine></div><NewLine></div><NewLine><pre class=""content"" style=""white-space: pre-wrap;"">This is regarding sample at:<NewLine>https://pytorch.org/blog/the-road-to-1_0/:<NewLine>from torch.jit import script<NewLine>@script<NewLine>def rnn_loop(x):<NewLine>hidden = None<NewLine>for x_t in x.split(1):<NewLine>x, hidden = model(x, hidden)<NewLine>return x<NewLine>I cannot make...</pre><NewLine><div class=""labels""><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>However, I am thinking maybe here is the right place to ask for help. Thus I copied my ask here:</p><NewLine><p>This is regarding sample at:<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/blog/the-road-to-1_0/:"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/blog/the-road-to-1_0/:</a></p><NewLine><pre><code class=""lang-python"">from torch.jit import script<NewLine><NewLine>@script<NewLine>def rnn_loop(x):<NewLine>hidden = None<NewLine>for x_t in x.split(1):<NewLine>x, hidden = model(x, hidden)<NewLine>return x<NewLine></code></pre><NewLine><p>I cannot make it to work. Here is my code:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.nn as nn<NewLine>from torch.autograd import Variable<NewLine>import numpy as np<NewLine><NewLine>def test_ScriptModelRNN():<NewLine>class SimpleRNNCell(nn.Module):<NewLine>def  **init** (self, input_size, hidden_size):<NewLine>super(SimpleRNNCell, self). **init** ()<NewLine>self.linear_h = nn.Linear(input_size, hidden_size)<NewLine><NewLine>    def forward(self, inp, h_0):<NewLine>        h = self.linear_h(inp)<NewLine>        return h + h_0, h<NewLine><NewLine>with torch.no_grad():<NewLine>    sequence_len, input_size, hidden_size = 4, 3, 2<NewLine>    model = SimpleRNNCell(input_size, hidden_size)<NewLine>    hidden = torch.zeros(1, hidden_size)<NewLine><NewLine>    # # test cell<NewLine>    # cell_input = torch.randn(input_size)<NewLine>    # cell_output, hidden = model(cell_input, hidden)<NewLine>    # import pdb; pdb.set_trace()<NewLine>    # #<NewLine><NewLine>    @torch.jit.script<NewLine>    def rnn_loop(x):<NewLine>        hidden = None<NewLine>        for x_t in x.split(1):<NewLine>            x, hidden = model(x_t, hidden)<NewLine>        return x<NewLine>        <NewLine>    input = torch.randn(sequence_len, input_size)<NewLine>    output = rnn_loop(input)<NewLine></code></pre><NewLine><p>I am getting:</p><NewLine><pre><code class=""lang-auto"">Exception has occurred: RuntimeError<NewLine><NewLine>for operator (Tensor 0, Tensor 1) -&amp;gt; (Tensor, Tensor):<NewLine>expected a value of type Tensor for argument '1' but found Tensor?<NewLine>@torch.jit.script<NewLine>def rnn_loop(x):<NewLine>hidden = None<NewLine>for x_t in x.split(1):<NewLine>x, hidden = model(x_t, hidden)<NewLine>~~~~~~ &amp;lt;--- HERE<NewLine>return x<NewLine>:<NewLine>@torch.jit.script<NewLine>def rnn_loop(x):<NewLine>hidden = None<NewLine>for x_t in x.split(1):<NewLine>x, hidden = model(x_t, hidden)<NewLine>~~~~~ &amp;lt;--- HERE<NewLine>return x<NewLine>File ""/home/liqun/pytorch/torch/jit/ **init** .py"", line 751, in script<NewLine>_jit_script_compile(mod, ast, _rcb, get_default_args(obj))<NewLine>File ""/home/liqun/Untitled Folder/test_onnx_export.py"", line 218, in test_ScriptModelRNN<NewLine>@torch.jit.script<NewLine>File ""/home/liqun/Untitled Folder/test_onnx_export.py"", line 282, in <NewLine>test_ScriptModelRNN()<NewLine>File ""/home/liqun/.conda/envs/py36/lib/python3.6/runpy.py"", line 85, in _run_code<NewLine>exec(code, run_globals)<NewLine>File ""/home/liqun/.conda/envs/py36/lib/python3.6/runpy.py"", line 96, in _run_module_code<NewLine>mod_name, mod_spec, pkg_name, script_name)<NewLine>File ""/home/liqun/.conda/envs/py36/lib/python3.6/runpy.py"", line 263, in run_path<NewLine>pkg_name=pkg_name, script_name=fname)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/liqun,(liqun fu),liqun,"April 8, 2019,  4:03pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the question, <a class=""mention"" href=""/u/liqun"">@liqun</a>.</p><NewLine><p>The problem looks like you’re passing None as the hidden:</p><NewLine><pre><code class=""lang-auto"">def rnn_loop(x):<NewLine>hidden = None<NewLine>**for** x_t **in** x.split(1):<NewLine>x, hidden = model(x_t, hidden)<NewLine></code></pre><NewLine><p>However, you’re not handling <code>None</code> hidden values in your model.</p><NewLine><p>Please see the following file <a href=""https://github.com/pytorch/benchmark/blob/master/rnns/fastrnns/custom_lstms.py"" rel=""nofollow noopener"">https://github.com/pytorch/benchmark/blob/master/rnns/fastrnns/custom_lstms.py</a> for some samples of custom RNNs written with TorchScript.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks Richard!<br/><NewLine>The link is very helpful. I will take a close look.<br/><NewLine>BTW, if I assign a tensor value instead of None to the hidden, like this:<br/><NewLine>hidden = torch.zeros(1, hidden_size)<br/><NewLine>I am getting:<br/><NewLine>python value of type ‘int’ cannot be used as a value:<br/><NewLine><span class=""mention"">@torch.jit.script</span><br/><NewLine>def rnn_loop(x):<br/><NewLine>hidden = torch.zeros(1, hidden_size)<br/><NewLine>~~~~~~~~~~~ &lt;— HERE<br/><NewLine>for x_t in x.split(1):<br/><NewLine>x, hidden = model(x_t, hidden)<br/><NewLine>return x<br/><NewLine>File “/home/liqun/pytorch/torch/jit/<strong>init</strong>.py”, line 751, in script<br/><NewLine>_jit_script_compile(mod, ast, _rcb, get_default_args(obj))</p><NewLine><p>Actually this was the error I have been getting. The None assignment was just to workaround this error to see how far I can go.</p><NewLine><p>And if I assign a tensor value to the hidden outside of the loop, I am getting:<br/><NewLine>python value of type ‘Tensor’ cannot be used as a value:<br/><NewLine><span class=""mention"">@torch.jit.script</span><br/><NewLine>def rnn_loop(x):<br/><NewLine><span class=""hashtag"">#hidden</span> = torch.zeros(1, hidden_size)<br/><NewLine>for x_t in x.split(1):<br/><NewLine>x, hidden = model(x_t, hidden)<br/><NewLine>~~~~~~ &lt;— HERE</p><NewLine><p>Thanks again,<br/><NewLine>Liqun</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/richard; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/liqun; <NewLine> ,"REPLY_DATE 1: April 8, 2019,  6:50pm; <NewLine> REPLY_DATE 2: April 8, 2019,  4:19pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
41526,Saving validation time inferences,2019-04-02T21:27:35.932Z,5,152,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to save images at validation time after almost every epoch to see how my network is learning but I am getting this error.<br/><NewLine>Here is my code to save images.</p><NewLine><pre><code class=""lang-auto"">def test(epoch, test_loss_list):<NewLine>		model.eval()<NewLine>		for batch_idx, (subject) in enumerate(validation_loader):<NewLine>			image = subject['image_data']<NewLine>			mask = subject['gt_data']<NewLine>			if params['cuda']:<NewLine>				image, mask = image.cuda(), mask.cuda()		  #Loading images into the GPU and ignore the affine.<NewLine>			with torch.no_grad():<NewLine>				output = model(image)<NewLine>#				loss = criterion(output, mask)<NewLine>				loss = dice_loss(output, mask)<NewLine>				test_loss_list.append(loss.data.item())<NewLine>				<NewLine>#			Saving the image and its mask in its own folder<NewLine>			save_image(subject['image_name'], image.cpu().detach().numpy(), np.array(list(subject['affine'])), epoch, params['epoch_dir'])<NewLine>			save_image(subject['image_name'], mask.cpu().detach().numpy(), np.array(list(subject['affine'])), epoch, params['epoch_dir'], mask = True)<NewLine><NewLine>			if batch_idx % int(params['log_interval']) == 0:<NewLine>				print('Test Epoch: {} [{}/{} ({:.0f}%)]\tAverage DICE Loss: {:.6f}'.format(<NewLine>					epoch, batch_idx * len(image), len(validation_loader.dataset),<NewLine>					100. * batch_idx / len(validation_loader), loss.data.item()))<NewLine>		for param_group in optimizer.param_groups:<NewLine>			print(""Learning rate: "", param_group['lr'])<NewLine>		sys.stdout.flush()<NewLine><NewLine>	def save_image(image_name, image, affine, epoch, folder, mask = False):<NewLine>		""""""<NewLine>		parameters:<NewLine>		image_name : takes in a string of the image name<NewLine>		image : expecting a numpy array<NewLine>		affine : expecting a numpy affine<NewLine>		epoch : the epoch count whichever you're running from<NewLine>		folder : the epoch where stuff is gonna be stored in<NewLine>		""""""<NewLine>		c = nib.Nifti1Image(image, affine)<NewLine>		os.mkdir(os.path.join(folder, epoch, image_name))<NewLine>		if mask == True:<NewLine>			c = np.array(c, dtype = np.int8)<NewLine>			nib.save(c, os.path.join(folder, epoch, image_name, image_name + '_mask.nii.gz'))<NewLine>		else:<NewLine>			nib.save(c, os.path.join(folder, epoch, image_name, image_name + '.nii.gz'))<NewLine><NewLine>		return<NewLine></code></pre><NewLine><p>This is the error generated</p><NewLine><pre><code class=""lang-auto"">save_image(subject['image_name'], image.cpu().detach().numpy(), np.array(list(subject['affine'])), epoch, params['epoch_dir'])<NewLine><NewLine>ValueError: only one element tensors can be converted to Python scalars<NewLine></code></pre><NewLine><p>Can anyone tell me where I may be going wrong?</p><NewLine></div>",https://discuss.pytorch.org/u/Geeks_Sid,(Siddhesh Thakur),Geeks_Sid,"April 2, 2019,  9:27pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you post the whole stack trace so that we can have a look?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The stack trace</p><NewLine><pre><code class=""lang-auto"">    runfile('/somestuff/pytorch_projects/trainer.py', wdir='/somestuff/pytorch_projects')<NewLine><NewLine>  File ""/home/someone/anaconda3/envs/pytorch/lib/python3.6/site-packages/spyder_kernels/customize/spydercustomize.py"", line 786, in runfile<NewLine>    execfile(filename, namespace)<NewLine><NewLine>  File ""/home/someone/anaconda3/envs/pytorch/lib/python3.6/site-packages/spyder_kernels/customize/spydercustomize.py"", line 110, in execfile<NewLine>    exec(compile(f.read(), filename, 'exec'), namespace)<NewLine><NewLine>  File ""/somestuff/pytorch_projects/trainer.py"", line 318, in &lt;module&gt;<NewLine>    main()<NewLine><NewLine>  File ""/somestuff/pytorch_projects/trainer.py"", line 298, in main<NewLine>    test(i, test_loss_list)<NewLine><NewLine>  File ""/somestuff/pytorch_projects/trainer.py"", line 252, in test<NewLine>    save_image(subject['image_name'], image.cpu().detach().numpy(), np.array(list(subject['affine'])), epoch, params['epoch_dir'])<NewLine><NewLine>ValueError: only one element tensors can be converted to Python scalars<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the information!<br/><NewLine>That doesn’t really clear things up, so could you please print the shapes and values of each argument you are passing?</p><NewLine><pre><code class=""lang-python"">print(subject['image_name'])<NewLine>print(image.cpu().detach().numpy().shape)<NewLine>...<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">subject['image_name']  #this is a string, at least that's what my dataloader is returning<NewLine>image                  #this is a tensor object of shape [1, 128, 128, 128]<NewLine></code></pre><NewLine><p>I am new to Pytorch so it is hard to figure this out on how to detach and save it.<br/><NewLine>Here is my dataloader object</p><NewLine><pre><code class=""lang-auto"">class SomethingDataset(Dataset):<NewLine>    def __init__(self, csv_file, root_dir):<NewLine>        self.df = pd.read_csv(csv_file, header = None)<NewLine>        self.root = root_dir<NewLine>        <NewLine>    def __len__(self):<NewLine>        return len(self.df)<NewLine>    <NewLine>    def __getitem__(self, subejct_id):<NewLine>#        folder_path = self.df[subject_id, 0]<NewLine>        image_name = self.df.iloc[subject_id, 0]<NewLine>        image_path = os.path.join(self.df.iloc[subject_id, 1])<NewLine>        gt_path = os.path.join(self.df.iloc[subject_id, 2])<NewLine>        image = nib.load(image_path)<NewLine>        gt = nib.load(gt_path)<NewLine>        image_data = np.reshape(image.get_fdata().astype(np.float32), (1, 128, 128, 128))<NewLine>        gt_data = np.reshape(gt.get_data().astype(np.float32), (1, 128, 128, 128))<NewLine>        affine = image.affine<NewLine>        sample = {'image_name':image_name, 'image_data':image_data, 'gt_data':gt_data, 'affine':affine}<NewLine>        return sample<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>What do the other arguments print out ( <code>np.array(list(subject['affine'])), epoch, params['epoch_dir']</code>)?<br/><NewLine>Although your image dimension look strange, I assume <code>nib.Nifti1Image</code> should handle it.<br/><NewLine>Could you try to debug your code line by line? E.g. I’m not sure if <code>nib.Nifti1Image</code> takes images in this shape or without the batch dimension, so you could try to call <code>image = image.squeeze()</code> before casting it.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>The Batch Dimension is such a big problem while saving the images. Thank you for the hint. I will try to run this and update soon.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Geeks_Sid; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Geeks_Sid; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Geeks_Sid; <NewLine> ,"REPLY_DATE 1: April 2, 2019,  9:30pm; <NewLine> REPLY_DATE 2: April 2, 2019,  9:32pm; <NewLine> REPLY_DATE 3: April 2, 2019,  9:33pm; <NewLine> REPLY_DATE 4: April 2, 2019,  9:53pm; <NewLine> REPLY_DATE 5: April 2, 2019, 10:06pm; <NewLine> REPLY_DATE 6: April 2, 2019, 11:10pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> 
41456,Pytorch model production,2019-04-02T04:26:06.534Z,0,167,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it good time to use jit in production level?<br/><NewLine>or tensorflow serving is better?</p><NewLine></div>",https://discuss.pytorch.org/u/juhyung,(손주형),juhyung,"April 2, 2019,  4:26am",,,,,
41342,Difference between writing in a single line,2019-03-31T21:16:33.725Z,2,143,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi.</p><NewLine><p>is there a difference between</p><NewLine><pre><code class=""lang-auto"">x = F.leaky_relu(self.in_2(self.conv1(x)), inplace=True)<NewLine></code></pre><NewLine><p>and</p><NewLine><pre><code class=""lang-auto"">x = self.conv1(x)<NewLine>x = self.in_2(x)<NewLine>x = F.leaky_relu(x, inplace=True)<NewLine></code></pre><NewLine><p>Does writing in a single line mean multiple feature maps won’t be created?<br/><NewLine>What are the pros and cons?</p><NewLine></div>",https://discuss.pytorch.org/u/Geeks_Sid,(Siddhesh Thakur),Geeks_Sid,"March 31, 2019,  9:16pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Both code snippets will create the same output.<br/><NewLine>It’s basically a question of your coding style.</p><NewLine><p>In the second example, you could add some debugging print statement slightly easier, e.g. in case you would like to see the shape of the intermediate activation.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, I thought that the first one would use less memory as compared to the second one. It’s not the case, right?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>That shouldn’t be the case, since <code>x</code> is reused and thus will be overwritten while Autograd will take care of creating the same computation graph.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Geeks_Sid; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: March 31, 2019, 10:57pm; <NewLine> REPLY_DATE 2: March 31, 2019, 10:58pm; <NewLine> REPLY_DATE 3: March 31, 2019, 10:59pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
40895,Should I use @weak_module for my modules?,2019-03-26T05:28:27.214Z,2,438,"<div class=""post"" itemprop=""articleBody""><NewLine><p>If I make a new module to use in a NN, should I always be adding the <span class=""mention"">@weak_module</span> annotation, or in general, when would I want to use this for my own modules?</p><NewLine><p>Specifically in my case, I was considering subclassing torch.nn.conv2d so that I could manipulate the weights before sending it through the normal conv2d module, so should I add <span class=""mention"">@weak_module</span> to my new subclass?<br/><NewLine>The <a href=""https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#Conv2d"" rel=""nofollow noopener"">original conv2d source</a> has it, so wasn’t sure if my modules also need it.</p><NewLine></div>",https://discuss.pytorch.org/u/himat,(Hima),himat,"March 26, 2019,  5:28am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You shouldn’t need to worry about the <span class=""mention"">@weak_module</span> annotation, that is internal to JIT to make the nn library compatible and low overhead with JIT, it is not a public API so it means use it at your own risk.</p><NewLine><p>In practice, you don’t need to care about that annotation in most cases. If you want to extend a nn class and use it in TorchScript, I encourage you to make a ScriptModule as indicated by the JIT documentation (<a href=""https://pytorch.org/docs/stable/jit.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/jit.html</a>)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>But would using <span class=""mention"">@weak_module</span> and <span class=""mention"">@weak_script</span> make my custom nn changes faster, or no?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>@weak_module</code> and <code>@weak_script</code> only delay compilation until the module they’re attached to is used, so there would be no performance change with it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/himat; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: March 26, 2019,  5:51am; <NewLine> REPLY_DATE 2: March 26, 2019,  3:53pm; <NewLine> REPLY_DATE 3: March 28, 2019, 10:46pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
41126,Disabling gradient calculation in jit.script_method,2019-03-28T15:21:17.006Z,0,216,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using scripting to convert a model to TorchScript module. How can I disable the grad-calculation in predict method of the module?<br/><NewLine>I tried the standard <code>no_grad()</code> and <code>set_grad_enabled()</code> options. But it seems both are not supported in a <code>jit.script_method</code>.</p><NewLine></div>",https://discuss.pytorch.org/u/vicki,(Vicki Anand),vicki,"March 28, 2019,  3:21pm",1 Like,,,,
40652,JIT with torch.nn.functional.interpolate,2019-03-22T18:29:41.089Z,2,420,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I found a strange behavior (maybe it’s normal, idk) during a JIT conversion of one of my model.</p><NewLine><p>When I use the jit capabilities to export my model with <code>torch.jit.trace(model, torch.randn(1, 2, 10, 10, 10))</code>, if I have a <code>torch.nn.functional.interpolate(x, scale_factor=2, model=""trilinear"", align_corners=True)</code> inside the forward pass, the jit model seems to be working with an input of size <code>(1, 2, 10, 10, 10)</code> strictly.</p><NewLine><p>Here is a small script to reproduce the behavior:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine><NewLine>class Model(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Model, self).__init__()<NewLine>        self.conv = torch.nn.Conv3d(5, 1, 3, padding=1, bias=False)<NewLine><NewLine>    def forward(self, x):<NewLine>        new_x = self.conv(x)<NewLine>        up_x = torch.nn.functional.interpolate(<NewLine>            new_x, scale_factor=2, mode=""trilinear"", align_corners=True)<NewLine>        return up_x<NewLine><NewLine><NewLine>inp_5 = torch.randn(1, 5, 5, 5, 5)<NewLine>inp_10 = torch.randn(1, 5, 10, 10, 10)<NewLine>inp_15 = torch.randn(1, 5, 15, 15, 15)<NewLine><NewLine>model = Model()<NewLine>model.eval()<NewLine>trace = torch.jit.trace(model, inp_10)<NewLine>trace.save(""trace.pth"")<NewLine><NewLine>result_model_5 = model(inp_5)<NewLine>result_model_10 = model(inp_10)<NewLine>result_model_15 = model(inp_15)<NewLine><NewLine>t_model = torch.jit.load(""trace.pth"")<NewLine>result_t_model_5 = t_model(inp_5)<NewLine>result_t_model_10 = t_model(inp_10)<NewLine>result_t_model_15 = t_model(inp_15)<NewLine><NewLine>print(""Shape  5, {} ||| {}"".format(result_model_5.shape, result_t_model_5.shape))<NewLine>print(""Shape 10, {} ||| {}"".format(result_model_10.shape, result_t_model_10.shape))<NewLine>print(""Shape 15, {} ||| {}"".format(result_model_15.shape, result_t_model_15.shape))<NewLine>torch.allclose(result_model_5, result_t_model_5)<NewLine>torch.allclose(result_model_10, result_t_model_10)<NewLine>torch.allclose(result_model_15, result_t_model_15)<NewLine></code></pre><NewLine><p>Outputs:</p><NewLine><pre><code class=""lang-auto"">Shape  5, torch.Size([1, 1, 10, 10, 10]) ||| torch.Size([1, 1, 20, 20, 20])<NewLine>Shape 10, torch.Size([1, 1, 20, 20, 20]) ||| torch.Size([1, 1, 20, 20, 20])<NewLine>Shape 15, torch.Size([1, 1, 30, 30, 30]) ||| torch.Size([1, 1, 20, 20, 20])<NewLine>Traceback (most recent call last):<NewLine>  File ""main.py"", line 37, in &lt;module&gt;<NewLine>    torch.allclose(result_model_5, result_t_model_5)<NewLine>RuntimeError: The size of tensor a (10) must match the size of tensor b (20) at non-singleton dimension 4<NewLine></code></pre><NewLine><p>Is it normal behavior?</p><NewLine></div>",https://discuss.pytorch.org/u/czotti,(Clément Zotti),czotti,"March 23, 2019, 11:44am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Seems Odd,</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine><NewLine>class Model(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Model, self).__init__()<NewLine>        self.conv = torch.nn.Conv3d(5, 1, 3, padding=1, bias=False)<NewLine><NewLine>    def forward(self, x):<NewLine>        new_x = self.conv(x)<NewLine>        up_x = torch.nn.functional.interpolate(<NewLine>            new_x, scale_factor=2, mode=""trilinear"", align_corners=True)<NewLine>        return up_x<NewLine><NewLine><NewLine>inp_5 = torch.randn(1, 5, 5, 5, 5)<NewLine>inp_10 = torch.randn(1, 5, 10, 10, 10)<NewLine>inp_15 = torch.randn(1, 5, 15, 15, 15)<NewLine><NewLine>model = Model()<NewLine>model.eval()<NewLine>trace = torch.jit.trace(model, inp_10)<NewLine>trace.save(""trace.pth"")<NewLine><NewLine>result_model_5 = model(inp_5)<NewLine>result_model_10 = model(inp_10)<NewLine>result_model_15 = model(inp_15)<NewLine>print(""Shape  5, {} ||| {}"".format(result_model_5.shape, result_model_5.shape))<NewLine>print(""Shape 10, {} ||| {}"".format(result_model_10.shape, result_model_10.shape))<NewLine>print(""Shape 15, {} ||| {}"".format(result_model_15.shape, result_model_15.shape))<NewLine>t_model = torch.jit.load(""trace.pth"")<NewLine>result_t_model_5 = t_model(inp_5)<NewLine>result_t_model_10 = t_model(inp_10)<NewLine>result_t_model_15 = t_model(inp_15)<NewLine><NewLine>print(""Shape  5, {} ||| {}"".format(result_model_5.shape, result_t_model_5.shape))<NewLine>print(""Shape 10, {} ||| {}"".format(result_model_10.shape, result_t_model_10.shape))<NewLine>print(""Shape 15, {} ||| {}"".format(result_model_15.shape, result_t_model_15.shape))<NewLine>torch.allclose(result_model_5, result_t_model_5)<NewLine>torch.allclose(result_model_10, result_t_model_10)<NewLine>torch.allclose(result_model_15, result_t_model_15<NewLine></code></pre><NewLine><p>Execute this and revert, please.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t see any difference with your proposal:</p><NewLine><pre><code class=""lang-auto"">Shape  5, torch.Size([1, 1, 10, 10, 10]) ||| torch.Size([1, 1, 10, 10, 10])<NewLine>Shape 10, torch.Size([1, 1, 20, 20, 20]) ||| torch.Size([1, 1, 20, 20, 20])<NewLine>Shape 15, torch.Size([1, 1, 30, 30, 30]) ||| torch.Size([1, 1, 30, 30, 30])<NewLine><NewLine>Shape  5, torch.Size([1, 1, 10, 10, 10]) ||| torch.Size([1, 1, 20, 20, 20])<NewLine>Shape 10, torch.Size([1, 1, 20, 20, 20]) ||| torch.Size([1, 1, 20, 20, 20])<NewLine>Shape 15, torch.Size([1, 1, 30, 30, 30]) ||| torch.Size([1, 1, 20, 20, 20])<NewLine>Traceback (most recent call last):<NewLine>  File ""test_ans.py"", line 39, in &lt;module&gt;<NewLine>    torch.allclose(result_model_5, result_t_model_5)<NewLine>RuntimeError: The size of tensor a (10) must match the size of tensor b (20) at non-singleton dimension 4<NewLine></code></pre><NewLine><p>There is still an issue(?) with the model generated with the JIT. When you say revert you mean retest?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Tracing doesn’t understand dynamic control flow, so sometimes it will “constant-ify” shapes in your model. Try turning your model in to a ScriptModule and using TorchScript; it should fix this problem.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your time!</p><NewLine><p>I made it works with something like this:</p><NewLine><pre><code class=""lang-python"">class Interpolate(torch.jit.ScriptModule):<NewLine>    __constants__ = [""scale_factor"", ""mode"", ""align_corners""]<NewLine><NewLine>    def __init__(self, scale_factor=2.0, mode=""nearest"", align_corners=None):<NewLine>        super(Interpolate, self).__init__()<NewLine>        self.scale_factor = scale_factor<NewLine>        self.mode = mode<NewLine>        self.align_corners = align_corners<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def forward(self, X):<NewLine>        return nn.functional.interpolate(X, scale_factor=self.scale_factor,<NewLine>                                         mode=self.mode, align_corners=self.align_corners)<NewLine></code></pre><NewLine><p>I finally get the desired output:</p><NewLine><pre><code class=""lang-auto"">Shape  5, torch.Size([1, 1, 10, 10, 10]) ||| torch.Size([1, 1, 10, 10, 10])<NewLine>Shape 10, torch.Size([1, 1, 20, 20, 20]) ||| torch.Size([1, 1, 20, 20, 20])<NewLine>Shape 15, torch.Size([1, 1, 30, 30, 30]) ||| torch.Size([1, 1, 30, 30, 30])<NewLine>True<NewLine>True<NewLine>True<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Geeks_Sid; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/czotti; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/czotti; <NewLine> ,"REPLY_DATE 1: March 22, 2019,  7:16pm; <NewLine> REPLY_DATE 2: March 23, 2019, 11:56am; <NewLine> REPLY_DATE 3: March 25, 2019,  7:52pm; <NewLine> REPLY_DATE 4: March 25, 2019,  7:52pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> 
40677,Error in @torch.jit.script,2019-03-23T04:56:58.083Z,0,312,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to learn more about the JIT compiler and was implementing the examples from the documentation, particularly this one (from <a href=""https://pytorch.org/docs/stable/jit.html#torch.jit.ScriptModule"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/jit.html#torch.jit.ScriptModule</a>):</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>@torch.jit.script<NewLine>def foo(x, y):<NewLine>    if x.max() &gt; y.max():<NewLine>        r = x<NewLine>    else:<NewLine>        r = y<NewLine>    return r<NewLine></code></pre><NewLine><p>The execution of the code resulted in this error:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/a68fd5b52d078a270e66a9eb85d0c3de0f743c7e"" href=""https://discuss.pytorch.org/uploads/default/original/2X/a/a68fd5b52d078a270e66a9eb85d0c3de0f743c7e.png"" title=""image.png""><img alt=""image"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/a/a68fd5b52d078a270e66a9eb85d0c3de0f743c7e_2_10x10.png"" height=""323"" src=""https://discuss.pytorch.org/uploads/default/original/2X/a/a68fd5b52d078a270e66a9eb85d0c3de0f743c7e.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">963×451 12.4 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine>Do you guys have any ideas on what is wrong?</p><NewLine></div>",https://discuss.pytorch.org/u/halahup,(Stepan Ulyanin),halahup,"March 23, 2019,  4:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We made a change to make the bool casting more strict, try: <code>if bool(x.max() &gt; y.max()):</code> instead. We need to update the docs to this effect, I’ll file a GH issue.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/18381"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://github.com/suo"" rel=""nofollow noopener""><NewLine><img class=""thumbnail onebox-avatar"" height=""96"" src=""https://avatars1.githubusercontent.com/u/1617424?v=2&amp;s=96"" width=""96""/><NewLine></a><NewLine><h4><a href=""https://github.com/pytorch/pytorch/issues/18381"" rel=""nofollow noopener"" target=""_blank"">Issue: [jit] be more permissive with bool casting.</a></h4><NewLine><div class=""date"" style=""margin-top:10px;""><NewLine><div class=""user"" style=""margin-top:10px;""><NewLine>	opened by <a href=""https://github.com/suo"" rel=""nofollow noopener"" target=""_blank"">suo</a><NewLine>	on <a href=""https://github.com/pytorch/pytorch/issues/18381"" rel=""nofollow noopener"" target=""_blank"">2019-03-23</a><NewLine></div><NewLine><div class=""user""><NewLine></div><NewLine></div><NewLine><pre class=""content"" style=""white-space: pre-wrap;"">In the TorchScript docs there is an example that doesn't compile:<NewLine>import torch<NewLine>@torch.jit.script<NewLine>def foo(x, y):<NewLine> if x.max() &gt; y.max():<NewLine> r = x<NewLine>...</pre><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">jit</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: March 23, 2019,  5:35am; <NewLine> REPLY_DATE 2: March 23, 2019,  6:13am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
40661,TorchScript: Using tensor populated with a different module,2019-03-22T19:56:52.242Z,0,444,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>What’s the best way to write TorchScript code which does this:</p><NewLine><pre><code class=""lang-auto"">class S(torch.jit.ScriptModule):<NewLine>    def __init__(self):<NewLine>        self.tensor_constant = torch.ones(2)<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def forward(self):<NewLine>        return self.tensor_constant + 2<NewLine><NewLine>S()<NewLine></code></pre><NewLine><p>It fails with<br/><NewLine><code>attribute 'tensor_constant' of type 'Tensor' is not usable in a script method (Tensors must be added to a module as a buffer or parameter):</code></p><NewLine><p>In other words, in TorchScript how can I use a tensor populated using a different module?</p><NewLine><p>Thanks,<br/><NewLine>Omkar</p><NewLine></div>",https://discuss.pytorch.org/u/Omkar,,Omkar,"March 22, 2019,  7:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There are 2 things wrong with the code</p><NewLine><ol><NewLine><li>You created a new class ‘S’ which subclassed torch.jit.ScriptModule. Now after creating the class S, you have to call it’s super-constructor i.e. you have to run the <code>__init__()</code> function of the class you are subclassing from (which in your case is <strong>init</strong>() function of torch.jit.ScriptModule). It is done with this code <code>super().__init__()</code><NewLine></li><NewLine><li>When creating torch scripts you have to use register buffers (as JIT compiles the module for you, so it must have info about everything)</li><NewLine></ol><NewLine><pre><code class=""lang-python"">class S(torch.jit.ScriptModule):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.register_buffer('tensor_constant', torch.ones(2, dtype=torch.float))<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def forward(self):<NewLine>        return self.tensor_constant + 2<NewLine><NewLine>S()<NewLine></code></pre><NewLine><p>Ask for clasrifications.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks Kushaj. I think I can use <code>register_buffer</code> in my use case.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Kushaj; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Omkar; <NewLine> ,"REPLY_DATE 1: March 22, 2019,  8:16pm; <NewLine> REPLY_DATE 2: March 22, 2019,  8:52pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
40548,"Using Dict[str, Tensor] type as a argument for a jit script_method",2019-03-21T18:55:45.312Z,0,186,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a model whose predict method takes an argument of type <code>Dict[str, Tensor]</code>.<br/><NewLine>I have been able to successfully serialize and save the model as a <code>ScriptModule</code>.<br/><NewLine>Now I want to test the model from c++ front-end, but I am not able to figure out how to create a test-input using the c++ front-end having the required type of <code>Dict[str, Tensor]</code>.</p><NewLine><p>Another related question: How can I save an input of this type from python so that I will be able to later load it the form c++ frontend in order to test the ScriptModule with this input?</p><NewLine></div>",https://discuss.pytorch.org/u/vicki,(Vicki Anand),vicki,"March 21, 2019,  6:58pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I put up an <a href=""https://github.com/driazati/torchscript-examples/tree/master/values"" rel=""nofollow noopener"">end-to-end example</a> that should help. The dict construction in C++ can be found <a href=""https://github.com/driazati/torchscript-examples/blob/5ca0a02a835dcf75c66336fbc2280351b13fdc40/values/ivalues.cpp#L15-L21"" rel=""nofollow noopener"">here</a>. Please follow up if anything is still unclear.</p><NewLine><p>For your other question, we don’t support that yet but it shouldn’t take too long to add, you can track it here: <a href=""https://github.com/pytorch/pytorch/issues/18286"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/18286</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/driazati; <NewLine> ,"REPLY_DATE 1: March 21, 2019,  8:16pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
36640,Do items in lists have to consist of only a single type?,2019-02-07T21:23:44.915Z,0,438,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to trace nvidia’s tacotron 2 model and interface with it via. the C++ frontend.</p><NewLine><p>Running the traced function via. the Python frontend works just fine, and reports results as expected.</p><NewLine><p>Through the C++ frontend however, it complains of a list of weights being fed into tacotron’s decoder LSTM not being of equal size (as each weight parameter is of different size/type).</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/53481e64c26e7f57b549e9f857f05ad45d2b1a05"" href=""https://discuss.pytorch.org/uploads/default/original/2X/5/53481e64c26e7f57b549e9f857f05ad45d2b1a05.png"" title=""image.png""><img alt=""image"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/53481e64c26e7f57b549e9f857f05ad45d2b1a05_2_10x10.png"" height=""286"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/53481e64c26e7f57b549e9f857f05ad45d2b1a05_2_690x286.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/53481e64c26e7f57b549e9f857f05ad45d2b1a05_2_690x286.png, https://discuss.pytorch.org/uploads/default/optimized/2X/5/53481e64c26e7f57b549e9f857f05ad45d2b1a05_2_1035x429.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/5/53481e64c26e7f57b549e9f857f05ad45d2b1a05.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">1272×529 83.5 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>The code used to trace and export out the model is as follows (nvidia’s implementation is linked here: <a href=""https://github.com/NVIDIA/tacotron2"" rel=""nofollow noopener"">https://github.com/NVIDIA/tacotron2</a>):</p><NewLine><pre><code class=""lang-python"">import numpy as np<NewLine>import torch<NewLine><NewLine>from hparams import create_hparams<NewLine>from text import text_to_sequence<NewLine>from train import load_model<NewLine><NewLine>hparams = create_hparams()<NewLine>hparams.sampling_rate = 22050<NewLine><NewLine>tacotron = load_model(hparams)<NewLine>tacotron.load_state_dict(torch.load(""tacotron2_statedict.pt"", map_location='cpu')['state_dict'])<NewLine>tacotron.eval()<NewLine><NewLine>print(tacotron)<NewLine><NewLine>text = ""This is some random text.""<NewLine>sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]<NewLine>sequence = torch.autograd.Variable(torch.from_numpy(sequence)).long()<NewLine><NewLine>traced_tacotron = torch.jit.trace(tacotron.inference, sequence, optimize=False, check_trace=False)<NewLine>traced_tacotron.save(""tacotronzzz.pt"")<NewLine><NewLine>print(tacotron.inference(sequence))<NewLine></code></pre><NewLine><p>Here is the C++ frontend code:</p><NewLine><pre><code class=""lang-cpp"">#include &lt;iostream&gt;<NewLine>#include &lt;torch/script.h&gt;<NewLine>#include &lt;torch/torch.h&gt;<NewLine><NewLine>using namespace std;<NewLine><NewLine>int main() {<NewLine>    shared_ptr&lt;torch::jit::script::Module&gt; tacotron = torch::jit::load(""tacotronzzz.pt"");<NewLine><NewLine>    assert(tacotron != nullptr);<NewLine><NewLine>    return 0;<NewLine>}<NewLine></code></pre><NewLine><p>If it helps, I can also provide a download to <code>tacotronzzz.pt</code>.</p><NewLine><p>Any help is much appreciated; if this is actually a bug, I’m happy to dig into the internals of <code>libtorch</code> and see if this could be fixed in any way.</p><NewLine></div>",https://discuss.pytorch.org/u/Dranithix,(Kenta Iwasaki),Dranithix,"February 7, 2019,  9:25pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just to add some reference links regarding how Lists in the IR are interpreted:</p><NewLine><p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/ac00e85e36c236f141d0621bfbbbbe8c9ffeefd1/torch/csrc/jit/script/compiler.cpp#L2224"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/ac00e85e36c236f141d0621bfbbbbe8c9ffeefd1/torch/csrc/jit/script/compiler.cpp#L2224"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/ac00e85e36c236f141d0621bfbbbbe8c9ffeefd1/torch/csrc/jit/script/compiler.cpp#L2224</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""2214"" style=""counter-reset: li-counter 2213 ;""><NewLine><li>  // if the list is non-empty use type_of(list[0])</li><NewLine><li>  // otherwise assume it is List[Tensor]</li><NewLine><li>  TypePtr elem_type = TensorType::get();</li><NewLine><li>  if (type_hint &amp;&amp; type_hint-&gt;kind() == TypeKind::ListType) {</li><NewLine><li>    elem_type = type_hint-&gt;expect&lt;ListType&gt;()-&gt;getElementType();</li><NewLine><li>  } else if (!values.empty()) {</li><NewLine><li>    elem_type = values.at(0)-&gt;type();</li><NewLine><li>  }</li><NewLine><li>  for (auto v : values) {</li><NewLine><li>    if (!v-&gt;type()-&gt;isSubtypeOf(elem_type)) {</li><NewLine><li class=""selected"">      throw ErrorReport(tree)</li><NewLine><li>          &lt;&lt; ""Lists must contain only a single type, expected: ""</li><NewLine><li>          &lt;&lt; *elem_type &lt;&lt; "" but found "" &lt;&lt; *v-&gt;type() &lt;&lt; "" instead"";</li><NewLine><li>    }</li><NewLine><li>  }</li><NewLine><li>  Value* result =</li><NewLine><li>      graph-&gt;insertNode(graph-&gt;createList(elem_type, values))-&gt;output();</li><NewLine><li>  return result;</li><NewLine><li>} break;</li><NewLine><li>case TK_TUPLE_LITERAL: {</li><NewLine><li>  auto ll = TupleLiteral(tree);</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/ac00e85e36c236f141d0621bfbbbbe8c9ffeefd1/torch/csrc/jit/ir.cpp#L1237"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/ac00e85e36c236f141d0621bfbbbbe8c9ffeefd1/torch/csrc/jit/ir.cpp#L1237"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/ac00e85e36c236f141d0621bfbbbbe8c9ffeefd1/torch/csrc/jit/ir.cpp#L1237</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""1227"" style=""counter-reset: li-counter 1226 ;""><NewLine><li>n-&gt;i_(attr::end, end);</li><NewLine><li>std::vector&lt;TypePtr&gt; output_types;</li><NewLine><li>for (auto i = beg; i &lt; end; ++i) {</li><NewLine><li>  output_types.push_back(tuple_type-&gt;elements().at(i));</li><NewLine><li>}</li><NewLine><li>auto tt = TupleType::create(std::move(output_types));</li><NewLine><li>n-&gt;output()-&gt;setType(tt);</li><NewLine><li>return n;</li><NewLine><li>}</li><NewLine><li><NewLine></li><NewLine><li class=""selected"">Node* Graph::createList(const TypePtr&amp; elem_type, at::ArrayRef&lt;Value*&gt; values) {</li><NewLine><li>auto n = create(prim::ListConstruct, values);</li><NewLine><li>for (const auto&amp; v : values) {</li><NewLine><li>  AT_ASSERT(v-&gt;type()-&gt;isSubtypeOf(elem_type));</li><NewLine><li>}</li><NewLine><li>n-&gt;output()-&gt;setType(ListType::create(elem_type));</li><NewLine><li>return n;</li><NewLine><li>}</li><NewLine><li>Node* Graph::createListUnpack(Value* v, size_t size) {</li><NewLine><li>ListTypePtr list_type = v-&gt;type()-&gt;expect&lt;ListType&gt;();</li><NewLine><li>TypePtr elem_type = list_type-&gt;getElementType();</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This in general seems to be a problem with tracing LSTM.</p><NewLine><p>Here is the class traced:</p><NewLine><pre><code class=""lang-python"">class Encoder(nn.Module):<NewLine>    """"""Encoder module:<NewLine>        - Three 1-d convolution banks<NewLine>        - Bidirectional LSTM<NewLine>    """"""<NewLine>    def __init__(self, hparams):<NewLine>        super(Encoder, self).__init__()<NewLine><NewLine>        convolutions = []<NewLine>        for _ in range(hparams.encoder_n_convolutions):<NewLine>            conv_layer = nn.Sequential(<NewLine>                ConvNorm(hparams.encoder_embedding_dim,<NewLine>                         hparams.encoder_embedding_dim,<NewLine>                         kernel_size=hparams.encoder_kernel_size, stride=1,<NewLine>                         padding=int((hparams.encoder_kernel_size - 1) / 2),<NewLine>                         dilation=1, w_init_gain='relu'),<NewLine>                nn.BatchNorm1d(hparams.encoder_embedding_dim))<NewLine>            convolutions.append(conv_layer)<NewLine>        self.convolutions = nn.ModuleList(convolutions)<NewLine><NewLine>        self.lstm = nn.LSTM(hparams.encoder_embedding_dim,<NewLine>                            int(hparams.encoder_embedding_dim / 2), 1,<NewLine>                            batch_first=True, bidirectional=True)<NewLine><NewLine>    def forward(self, x, input_lengths):<NewLine>        for conv in self.convolutions:<NewLine>            x = F.dropout(F.relu(conv(x)), 0.5, self.training)<NewLine><NewLine>        x = x.transpose(1, 2)<NewLine><NewLine>        # pytorch tensor are not reversible, hence the conversion<NewLine>        input_lengths = input_lengths.cpu().numpy()<NewLine>        x = nn.utils.rnn.pack_padded_sequence(<NewLine>            x, input_lengths, batch_first=True)<NewLine><NewLine>        self.lstm.flatten_parameters()<NewLine>        outputs, _ = self.lstm(x)<NewLine><NewLine>        outputs, _ = nn.utils.rnn.pad_packed_sequence(<NewLine>            outputs, batch_first=True)<NewLine><NewLine>        return outputs<NewLine><NewLine>    def inference(self, x):<NewLine>        for conv in self.convolutions:<NewLine>            x = F.dropout(F.relu(conv(x)), 0.5, self.training)<NewLine><NewLine>        x = x.transpose(1, 2)<NewLine><NewLine>        self.lstm.flatten_parameters()<NewLine>        outputs, _ = self.lstm(x)<NewLine><NewLine>        return outputs<NewLine></code></pre><NewLine><p>This yields the IR which makes use of lists whose elements are of multiple types (each element is a Float tensor, though of different shape/size).</p><NewLine><pre><code class=""lang-auto"">%hx.1 : Float(2, 1, 256) = aten::zeros(%105, %106, %107, %108), scope: LSTM<NewLine>  %146 : Tensor[] = prim::ListConstruct(%hx.1, %hx.1), scope: LSTM<NewLine>  %147 : Float(1024, 512) = prim::Constant[value=&lt;Tensor&gt;](), scope: LSTM<NewLine>  %148 : Float(1024, 256) = prim::Constant[value=&lt;Tensor&gt;](), scope: LSTM<NewLine>  %149 : Float(1024) = prim::Constant[value=&lt;Tensor&gt;](), scope: LSTM<NewLine>  %150 : Float(1024) = prim::Constant[value=&lt;Tensor&gt;](), scope: LSTM<NewLine>  %151 : Float(1024, 512) = prim::Constant[value=&lt;Tensor&gt;](), scope: LSTM<NewLine>  %152 : Float(1024, 256) = prim::Constant[value=&lt;Tensor&gt;](), scope: LSTM<NewLine>  %153 : Float(1024) = prim::Constant[value=&lt;Tensor&gt;](), scope: LSTM<NewLine>  %154 : Float(1024) = prim::Constant[value=&lt;Tensor&gt;](), scope: LSTM<NewLine>  %155 : Tensor[] = prim::ListConstruct(%147, %148, %149, %150, %151, %152, %153, %154), scope: LSTM<NewLine>  %156 : bool = prim::Constant[value=1](), scope: LSTM<NewLine>  %157 : int = prim::Constant[value=1](), scope: LSTM<NewLine>  %158 : float = prim::Constant[value=0](), scope: LSTM<NewLine>  %159 : bool = prim::Constant[value=0](), scope: LSTM<NewLine>  %160 : bool = prim::Constant[value=1](), scope: LSTM<NewLine>  %161 : bool = prim::Constant[value=1](), scope: LSTM<NewLine>  %memory : Float(1!, 41, 512), %163 : Float(2, 1, 256), %164 : Float(2, 1, 256) = aten::lstm(%input.14, %146, %155, %156, %157, %158, %159, %160, %161), scope: LSTM<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>whoops, missed this somehow, sorry for the late reply. We have made a change in master that allows lists to hold tensors of different shapes/sizes. If you try a nightly build it should work for you</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Dranithix; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Dranithix; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: February 7, 2019,  9:48pm; <NewLine> REPLY_DATE 2: February 8, 2019,  3:28am; <NewLine> REPLY_DATE 3: March 21, 2019,  7:42pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
39062,Getting input and output tensors for each node in trace graph,2019-03-06T06:04:39.189Z,0,490,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I wish to do forward and backward execution in ScriptModule and then retrieve input and output tensors for each node in execution graph. Is this possible?</p><NewLine><p>I’ve code like below:</p><NewLine><pre><code class=""lang-auto"">    trace, out = torch.jit.get_trace_graph(model, args)<NewLine>    out.backward(gradient=torch.ones(out.size()))<NewLine>    torch.onnx._optimize_trace(trace, torch.onnx.OperatorExportTypes.ONNX)<NewLine>    torch_graph = trace.graph()<NewLine><NewLine>    for node in torch_graph.nodes():<NewLine>      # How to get input and output tensor for this node?<NewLine>      # Additionally is it possible to get parameters (weight/bias) for this node?<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/sytelus,(Shital Shah),sytelus,"March 6, 2019,  6:11am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you explain a bit about what you’re trying to accomplish? Generally we don’t recommend that people use the python IR bindings as the API is not stable and you can get into some C+±y trouble. Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: March 21, 2019,  7:41pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
40258,Where is source code of Tensor.bmm?,2019-03-19T06:55:58.152Z,0,387,"<div class=""post"" itemprop=""articleBody""><NewLine><p>hi, i can’t find source defination of Tensor.bmm which i think is a matrix operation method? can kindly tell which file has it  even written in c++ I am still interesting. thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/suniuin,(suniuin),suniuin,"March 19, 2019,  6:55am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Look in <a href=""https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native</a> for the relevant implementations</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: March 21, 2019,  7:40pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
39908,Dropout in RNN for each layer,2019-03-15T04:13:54.709Z,0,134,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m currently reading c++ implementation of RNN in ATen (<a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/RNN.cpp"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/RNN.cpp</a>). Based on the following implementation, is my understanding correct that in every layer there will be dropout applied with the same mask for every steps?</p><NewLine><pre><code class=""lang-auto"">template&lt;typename io_type, typename hidden_type, typename weight_type&gt;<NewLine>LayerOutput&lt;io_type, std::vector&lt;hidden_type&gt;&gt;<NewLine>apply_layer_stack(const Layer&lt;io_type, hidden_type, weight_type&gt;&amp; layer, const io_type&amp; input,<NewLine>                  const std::vector&lt;hidden_type&gt;&amp; hiddens, const std::vector&lt;weight_type&gt;&amp; weights,<NewLine>                  int64_t num_layers, double dropout_p, bool train) {<NewLine>  AT_CHECK(num_layers == hiddens.size(), ""Expected more hidden states in stacked_rnn"");<NewLine>  AT_CHECK(num_layers == weights.size(), ""Expected more weights in stacked_rnn"");<NewLine><NewLine>  auto layer_input = input;<NewLine>  auto hidden_it = hiddens.begin();<NewLine>  auto weight_it = weights.begin();<NewLine>  std::vector&lt;hidden_type&gt; final_hiddens;<NewLine>  for (int64_t l = 0; l &lt; num_layers; ++l) {<NewLine>    auto layer_output = layer(layer_input, *(hidden_it++), *(weight_it++));<NewLine>    final_hiddens.push_back(layer_output.final_hidden);<NewLine>    layer_input = layer_output.outputs;<NewLine><NewLine>    if (dropout_p != 0 &amp;&amp; train &amp;&amp; l &lt; num_layers - 1) {<NewLine>      layer_input = dropout(layer_input, dropout_p);<NewLine>    }<NewLine>  }<NewLine><NewLine>  return {layer_input, final_hiddens};<NewLine>}<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/akurniawan,(Aditya Kurniawan),akurniawan,"March 15, 2019,  4:13am",,,,,
39118,Tracing custom RNN model,2019-03-06T15:15:35.546Z,0,686,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi !</p><NewLine><p>I am currently trying to trace a custom RNN model to be able to execute it faster (I think it would make sense given how it uses for loops, but feel free to correct me if I’m wrong), for which a minimal example would be as follows :</p><NewLine><pre><code>import torch<NewLine>from torch.nn import Module, Parameter<NewLine>from torch.autograd import Variable<NewLine><NewLine>class minimal_ex(Module):<NewLine>    def __init__(self, input_size=1, recurrent_size=1):<NewLine>        super(minimal_ex, self).__init__()<NewLine>        n_h, n_i = recurrent_size, input_size<NewLine>        self.input_size = input_size<NewLine>        self.recurrent_size = recurrent_size<NewLine>        self.activation = lambda x: x<NewLine><NewLine>        self.mask_rec = Parameter(torch.ones(n_h, n_h), requires_grad=False)<NewLine>        self.mask_in = Parameter(torch.ones(n_h, n_i), requires_grad=False)<NewLine>        self.neuron_signs = Parameter((torch.ones(n_h)).float(), requires_grad=False)<NewLine><NewLine>        self.w_i = Parameter(torch.ones(recurrent_size, input_size))<NewLine>        self.w_h = Parameter(torch.zeros(recurrent_size, recurrent_size))<NewLine>        self.bias = Parameter(torch.zeros(recurrent_size))<NewLine>        self.cuda()<NewLine><NewLine>    def forward(self, inp):<NewLine>        h_0 = Variable(torch.zeros(1, self.recurrent_size)).cuda()<NewLine>        batch_size, seq_len, dim = inp.shape<NewLine>        w_i = (self.w_i * self.mask_in).transpose(0,1)<NewLine>        w_h = (self.w_h * self.mask_rec).transpose(0,1)<NewLine><NewLine>        w_h_with_signs = torch.mm(torch.diag(self.neuron_signs), w_h)<NewLine><NewLine>        h = FloatTensor(batch_size, seq_len, self.recurrent_size)<NewLine>        h[:, -1, :] = h_0<NewLine><NewLine>        for t in torch.arange(seq_len):<NewLine>            h[:, t, :] = self.activation(torch.mm(inp[:, t, :].clone(), w_i) +<NewLine>                         torch.mm(h[:, t-1, :].clone(), w_h_with_signs) - self.bias)<NewLine>        return h<NewLine><NewLine>bs = 64<NewLine>seq_len = 500<NewLine>in_size = 2<NewLine>inp = torch.FloatTensor(bs, seq_len, in_size).uniform_().cuda()<NewLine><NewLine>model = minimal_ex(input_size=in_size, recurrent_size=128)<NewLine>traced_forward = torch.jit.trace(model, inp)<NewLine></code></pre><NewLine><p>When I do so, the output contains several warnings :</p><NewLine><ul><NewLine><li>TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can’t record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<br/><NewLine>h = FloatTensor(batch_size, seq_len, self.recurrent_size)</li><NewLine><li>RuntimeWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won’t change the number of iterations executed (and might lead to errors or silently give incorrect results).<br/><NewLine>‘incorrect results).’, category=RuntimeWarning)</li><NewLine><li>TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can’t record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<br/><NewLine>torch.mm(h[:, t-1, :].clone(), w_h_with_signs) - self.bias)</li><NewLine><li>TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the repeated trace. Detailed error:<br/><NewLine>Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 26, 0] (0.48441100120544434 vs. 3.6893488147419103e+19) and 4095999 other locations (100.00%)<br/><NewLine>_check_trace([example_inputs], func, executor_options, module, check_tolerance, _force_outplace)</li><NewLine></ul><NewLine><p>I was expecting some warnings (in particular, if I change the seq_len I would not expect it to work, but this is fine), but I don’t understand why the output of the traced model blows up while the Python one does not.</p><NewLine><p>Is there something wrong with the way I use trace?</p><NewLine><p>Thank you in advance for your help</p><NewLine></div>",https://discuss.pytorch.org/u/AFanthomme,(Arnaud Fanthomme),AFanthomme,"March 6, 2019,  3:15pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, are you sure that repro is exactly what you are running? When I try to run it on our master branch, I get a few runtime errors. (some are trivial, like an unqualified call to FloatTensor).</p><NewLine><p>Just want to make sure we are running exactly the same code so I can look into your issue</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry about that, I had removed a local import that I thought was irrelevant but defined FloatTensor = torch.cuda.FloatTensor</p><NewLine><p>I edited the code to directly use torch.FloatTensor, and surprisingly it makes a difference and the model does not blow up anymore (but still gives wrong results) :</p><NewLine><p>New code :</p><NewLine><pre><code>import torch<NewLine>from torch.nn import Module, Parameter<NewLine>from torch.autograd import Variable<NewLine><NewLine><NewLine>class minimal_ex(Module):<NewLine>    def __init__(self, input_size=1, recurrent_size=1):<NewLine>        super(minimal_ex, self).__init__()<NewLine>        n_h, n_i = recurrent_size, input_size<NewLine>        self.input_size = input_size<NewLine>        self.recurrent_size = recurrent_size<NewLine>        self.activation = lambda x: x<NewLine><NewLine>        self.mask_rec = Parameter(torch.ones(n_h, n_h), requires_grad=False)<NewLine>        self.mask_in = Parameter(torch.ones(n_h, n_i), requires_grad=False)<NewLine><NewLine>        self.neuron_signs = Parameter((torch.ones(n_h)).float(), requires_grad=False)<NewLine><NewLine>        self.w_i = Parameter(torch.ones(recurrent_size, input_size))<NewLine>        self.w_h = Parameter(torch.zeros(recurrent_size, recurrent_size))<NewLine>        self.bias = Parameter(torch.zeros(recurrent_size))<NewLine><NewLine>        self.cuda()<NewLine><NewLine>    def forward(self, inp):<NewLine>        h_0 = Variable(torch.zeros(1, self.recurrent_size)).cuda()<NewLine>        batch_size, seq_len, dim = inp.shape<NewLine>        w_i = (self.w_i * self.mask_in).transpose(0,1)<NewLine>        w_h = (self.w_h * self.mask_rec).transpose(0,1)<NewLine><NewLine>        w_h_with_signs = torch.mm(torch.diag(self.neuron_signs), w_h)<NewLine><NewLine>        h = torch.FloatTensor(batch_size, seq_len, self.recurrent_size).cuda()<NewLine>        h[:, -1, :] = h_0<NewLine><NewLine>        for t in torch.arange(seq_len):<NewLine>            h[:, t, :] = self.activation(torch.mm(inp[:, t, :].clone(), w_i) +<NewLine>                         torch.mm(h[:, t-1, :].clone(), w_h_with_signs) - self.bias)<NewLine>        return h<NewLine><NewLine>if __name__ == '__main__':<NewLine>    print(torch.__version__)<NewLine>    print(torch.version.cuda)<NewLine><NewLine>    bs = 64<NewLine>    seq_len = 500<NewLine>    in_size = 2<NewLine>    inp = torch.FloatTensor(bs, seq_len, in_size).uniform_().cuda()<NewLine><NewLine>    model = minimal_ex(input_size=in_size, recurrent_size=128)<NewLine>    traced_forward = torch.jit.trace(model, inp)<NewLine></code></pre><NewLine><p>New output :<br/><NewLine>1.0.0<br/><NewLine>9.0.176</p><NewLine><p>rep.py:37: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can’t record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<br/><NewLine>h = torch.FloatTensor(batch_size, seq_len, self.recurrent_size).cuda()<br/><NewLine>/users/fanthomme/miniconda3/envs/env_these/lib/python3.6/site-packages/torch/tensor.py:427: RuntimeWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won’t change the number of iterations executed (and might lead to errors or silently give incorrect results).<br/><NewLine>‘incorrect results).’, category=RuntimeWarning)<br/><NewLine>rep.py:42: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can’t record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<br/><NewLine>torch.mm(h[:, t-1, :].clone(), w_h_with_signs) - self.bias)<br/><NewLine>/users/fanthomme/miniconda3/envs/env_these/lib/python3.6/site-packages/torch/jit/<strong>init</strong>.py:642: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:<br/><NewLine>Not within tolerance rtol=1e-05 atol=1e-05 at input[34, 459, 0] (0.0 vs. 1.991980791091919) and 4095999 other locations (100.00%)<br/><NewLine>_check_trace([example_inputs], func, executor_options, module, check_tolerance, _force_outplace)</p><NewLine><p>I might also add that running that script several times always gives 100% discrepancy, but the exact position and value slightly differ. And the model is running on a Tesla K40c</p><NewLine><p>Please do not hesitate if you have any other question, and thanks a lot</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/AFanthomme; <NewLine> ,"REPLY_DATE 1: March 6, 2019,  9:28pm; <NewLine> REPLY_DATE 2: March 7, 2019,  9:05am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
39014,JIT doesn&rsquo;t support &lsquo;to&rsquo; keyword,2019-03-05T18:05:45.469Z,1,213,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Everyone,<br/><NewLine>I have a problem regarding JIT of Pytorch.</p><NewLine><aside class=""onebox githubcommit""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/15340/commits/10fc5b3a20bcc40fbc39534a8f47cfc2c898a88b"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://github.com/vfdev-5"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""vfdev-5"" class=""thumbnail onebox-avatar"" height=""90"" src=""https://avatars0.githubusercontent.com/u/2459423?v=4"" width=""90""/><NewLine></a><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/commit/10fc5b3a20bcc40fbc39534a8f47cfc2c898a88b"" rel=""nofollow noopener"" target=""_blank"">Implement 'to' on ScriptModules</a><NewLine></h4><NewLine><div class=""date""><NewLine>  by <a href=""https://github.com/vfdev-5"" rel=""nofollow noopener"" target=""_blank"">vfdev-5</a><NewLine>  on <a href=""https://github.com/pytorch/pytorch/commit/10fc5b3a20bcc40fbc39534a8f47cfc2c898a88b"" rel=""nofollow noopener"" target=""_blank"">01:13PM - 18 Dec 18 UTC</a><NewLine></div><NewLine><div class=""github-commit-stats""><NewLine>  changed <strong>2 files</strong><NewLine>  with <strong>6 additions</strong><NewLine>  and <strong>1 deletions</strong>.<NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>This link says that <strong><code>to</code></strong> is implemented in the Pytorch and ready for use but as I try to use it, it doesn’t function, I even have uninstalled and reinstalled the whole thing 3 time and I have made sure that I have latest version of pytorch which is 1.0.1 which is also shown in the picture below<img alt=""Screenshot%20from%202019-03-05%2018-59-27"" height=""138"" src=""https://discuss.pytorch.org/uploads/default/original/2X/8/8ea20ce69cb6ad803027e0e63d50a9b7745f03a8.png"" width=""664""/></p><NewLine><p>The thing is, changes mentioned in the above github link doesn’t appear in the pytroch I install with command <code>conda install pytorch torchvision cudatoolkit=9.0 -c pytorch</code> which are in the file <code>test/test_jit.py</code> and <code>torch/jit/__init__.py</code>.<br/><NewLine>I also tried to install from source but it gives weird errors and doesn’t install.</p><NewLine><p>Any idea what’s happening here?</p><NewLine></div>",https://discuss.pytorch.org/u/arslansadiq,(Muhammad Arslan Ansari),arslansadiq,"March 5, 2019,  6:05pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not sure, if it’ll help, but could you try to install the nightly builds and check it again?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Already did that man, doesn’t help.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/arslansadiq; <NewLine> ,"REPLY_DATE 1: March 5, 2019,  8:58pm; <NewLine> REPLY_DATE 2: March 6, 2019,  1:15pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
37976,Get intermediate layer outputs from traced graph,2019-02-22T16:22:36.520Z,1,520,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><p>I am trying to visualize intermediate layer outputs generate by one input image during inference of a PyTorch model. Preferably I would like to do this from a traced graph, for example one from the torchvision modelzoo.</p><NewLine><p>I.e. if I have a model file created like this</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torchvision<NewLine><NewLine>org_model = torchvision.models.resnet18(pretrained=True)<NewLine>traced_net = torch.jit.trace(org_model, torch.rand(1, 3, 224, 224))<NewLine><NewLine>torch.jit.save(traced_net, ""resnet.pth"") <NewLine></code></pre><NewLine><p>Then I want to be able to load that model and output the activations of for example “layer1”</p><NewLine><pre><code class=""lang-auto"">traced_model_loaded = torch.jit.load(""resnet.pth"")<NewLine><NewLine>input_ = torch.rand(1, 3, 224, 224)<NewLine>layer1_act = traced_model_loaded.layer1(input_)<NewLine></code></pre><NewLine><p>Is this possible? If not, can I in some way modify the original PyTorch model so that an arbitrary number of layer activations becomes accessible?  Using forward hooks does not seem to be supported.</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/karin.stacke,(Karin Stacke),karin.stacke,"February 22, 2019,  4:22pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think you can always output the activations using the org_model, or debug in python as you want, and if you think it’s good, then do the tracing and serialization.</p><NewLine><p>If you want to see the intermediates in the traced model, you can still modify the original model and add print stmts etc to debug it.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, it would however require me to have to original model. What I wanted to do was to create a generic way to visualize the layer activations of an arbitrary layer (or channel) at inference. However, have realized that it is not possible, that you need the original model to be able to do this. The easiest way of doing this when having the model was to register forward hooks, which then outputs the resulting activations during a forward pass. So I got the functionality I wanted, but not with the traced graph.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/karin.stacke; <NewLine> ,"REPLY_DATE 1: March 5, 2019,  8:08pm; <NewLine> REPLY_DATE 2: March 6, 2019,  9:35am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
35939,Cannot move ScriptModule to GPU with to(),2019-01-30T03:49:15.976Z,6,918,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My code is</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn.functional as F<NewLine><NewLine>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<NewLine><NewLine>class ScriptNet(torch.jit.ScriptModule):<NewLine>    def __init__(self, n_features, out_size):<NewLine>        super().__init__()<NewLine>        self.fc1 = torch.jit.trace(torch.nn.Linear(n_features, 50), torch.rand(1, n_features))<NewLine>        self.fc2 = torch.jit.trace(torch.nn.Linear(50, out_size), torch.rand(1, 50))<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def forward(self, x):<NewLine>        x = F.dropout(F.relu(self.fc1(x)), 0.5)<NewLine>        return F.softmax(self.fc2(x))<NewLine><NewLine>net = ScriptNet(10,2).to(device)<NewLine></code></pre><NewLine><p>I got this error:</p><NewLine><blockquote><NewLine><p>RuntimeError: to is not supported on TracedModules</p><NewLine></blockquote><NewLine><p>I know I can save the ScriptModule and use torch.jit.load(modulefile, map_location=‘cuda’) to load the module into GPU, but I wonder if I can directly move the module to GPU without saving and loading.</p><NewLine></div>",https://discuss.pytorch.org/u/Hong,(关鸿),Hong,"January 30, 2019,  3:52am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What version of PyTorch are you using? This should be available in 1.0 or the nightlies</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can’t do this with <code>.to</code> but <code>net.cuda()</code> should work.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>net.cuda() works. Thank you. Do you think Pytorch should add .to() to ScriptModules?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>To answer your last question: <a class=""mention"" href=""/u/tom"">@tom</a> told me, that this is enabled on github master and nightlies. If you want that feature, you should probably switch to one of these.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have <code>1.0.1</code> version and it is still giving this error:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine><NewLine>  File ""&lt;ipython-input-2-0cb739d0d1fb&gt;"", line 1, in &lt;module&gt;<NewLine>    runfile('/home/hiwi/Desktop/HIWI_Data/Combustion_NN_Model/train.py', wdir='/home/hiwi/Desktop/HIWI_Data/Combustion_NN_Model')<NewLine><NewLine>  File ""/home/hiwi/anaconda3/lib/python3.6/site-packages/spyder_kernels/customize/spydercustomize.py"", line 668, in runfile<NewLine>    execfile(filename, namespace)<NewLine><NewLine>  File ""/home/hiwi/anaconda3/lib/python3.6/site-packages/spyder_kernels/customize/spydercustomize.py"", line 108, in execfile<NewLine>    exec(compile(f.read(), filename, 'exec'), namespace)<NewLine><NewLine>  File ""/home/hiwi/Desktop/HIWI_Data/Combustion_NN_Model/train.py"", line 70, in &lt;module&gt;<NewLine>    main(config, args.resume)<NewLine><NewLine>  File ""/home/hiwi/Desktop/HIWI_Data/Combustion_NN_Model/train.py"", line 42, in main<NewLine>    train_logger=train_logger)<NewLine><NewLine>  File ""/home/hiwi/Desktop/HIWI_Data/Combustion_NN_Model/trainer/trainer.py"", line 16, in __init__<NewLine>    super(Trainer, self).__init__(model, loss, metrics, optimizer, resume, config, train_logger)<NewLine><NewLine>  File ""/home/hiwi/Desktop/HIWI_Data/Combustion_NN_Model/base/base_trainer.py"", line 21, in __init__<NewLine>    self.model = model.to(self.device)<NewLine><NewLine>  File ""/home/hiwi/anaconda3/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1280, in fail<NewLine>    raise RuntimeError(name + "" is not supported on TracedModules"")<NewLine><NewLine>RuntimeError: to is not supported on TracedModules<NewLine></code></pre><NewLine><p>this is how it is being implemented:</p><NewLine><pre><code class=""lang-auto"">class BaseTrainer:<NewLine>    """"""<NewLine>    Base class for all trainers<NewLine>    """"""<NewLine>    def __init__(self, model, loss, metrics, optimizer, resume, config, train_logger=None):<NewLine>        self.config = config<NewLine>        self.logger = logging.getLogger(self.__class__.__name__)<NewLine><NewLine>        # setup GPU device if available, move model into configured device<NewLine>        self.device, device_ids = self._prepare_device(config['n_gpu'])<NewLine>        self.model = model.to(self.device)<NewLine>        if len(device_ids) &gt; 1:<NewLine>            self.model = torch.nn.DataParallel(model, device_ids=device_ids)<NewLine><NewLine>        self.loss = loss<NewLine>        self.metrics = metrics<NewLine>        self.optimizer = optimizer<NewLine><NewLine>        self.epochs = config['trainer']['epochs']<NewLine>        self.save_freq = config['trainer']['save_freq']<NewLine>        self.verbosity = config['trainer']['verbosity']<NewLine><NewLine>        self.train_logger = train_logger<NewLine><NewLine>        # configuration to monitor model performance and save best<NewLine>        self.monitor = config['trainer']['monitor']<NewLine>        self.monitor_mode = config['trainer']['monitor_mode']<NewLine>        assert self.monitor_mode in ['min', 'max', 'off']<NewLine>        self.monitor_best = math.inf if self.monitor_mode == 'min' else -math.inf<NewLine>        self.start_epoch = 1<NewLine><NewLine>        # setup directory for checkpoint saving<NewLine>        start_time = datetime.datetime.now().strftime('%m%d_%H%M%S')<NewLine>        self.checkpoint_dir = os.path.join(config['trainer']['save_dir'], config['name'], start_time)<NewLine>        # setup visualization writer instance<NewLine>        writer_dir = os.path.join(config['visualization']['log_dir'], config['name'], start_time)<NewLine>        self.writer = WriterTensorboardX(writer_dir, self.logger, config['visualization']['tensorboardX'])<NewLine><NewLine>        # Save configuration file into checkpoint directory:<NewLine>        ensure_dir(self.checkpoint_dir)<NewLine>        config_save_path = os.path.join(self.checkpoint_dir, 'config.json')<NewLine>        with open(config_save_path, 'w') as handle:<NewLine>            json.dump(config, handle, indent=4, sort_keys=False)<NewLine><NewLine>        if resume:<NewLine>            self._resume_checkpoint(resume)<NewLine></code></pre><NewLine><p>and I am using the traced modules like this:</p><NewLine><p><strong>Model File</strong></p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>from base import BaseModel<NewLine>import json<NewLine>import argparse<NewLine><NewLine><NewLine>class CombustionModel(BaseModel):<NewLine>    def __init__(self, num_features=7):<NewLine>        super(CombustionModel, self).__init__()<NewLine>        sizes = self.get_botleneck_size()              #sizes for bottlenecks<NewLine>        self.Fc1 = nn.Linear(in_features = 2, out_features = 500, bias=True)<NewLine>        self.Fc2 = nn.Linear(in_features = 500, out_features = 500, bias=True)<NewLine>        self.Fc3_bottleneck = nn.Linear(in_features = 500, out_features = sizes[0], bias=True)<NewLine>        self.Fc4 = nn.Linear(in_features = sizes[0], out_features = 500, bias=True)<NewLine>        self.Fc5_bottleneck = nn.Linear(in_features = 500, out_features = sizes[1], bias=True)<NewLine>        self.Fc6 = nn.Linear(in_features = sizes[1], out_features = 500, bias=True)<NewLine>        self.Fc7_bottleneck = nn.Linear(in_features = 500, out_features = sizes[2], bias=True)<NewLine>        self.Fc8 = nn.Linear(in_features = sizes[2], out_features = 500, bias=True)<NewLine>        self.Fc9_bottleneck = nn.Linear(in_features = 500, out_features = sizes[3], bias=True)<NewLine>        self.Fc10 = nn.Linear(in_features = sizes[3], out_features = 500, bias=True)<NewLine>        self.Fc11_bottleneck = nn.Linear(in_features = 500, out_features = sizes[4], bias=True)<NewLine>        self.Fc12 = nn.Linear(in_features = sizes[4], out_features = num_features, bias=True)<NewLine>    <NewLine>    def get_botleneck_size(self):<NewLine>         parser = argparse.ArgumentParser(description='BottleNeck')<NewLine>         parser.add_argument('-c', '--config', default='config.json', type=str,<NewLine>                           help='config file path (default: None)')<NewLine>         args = parser.parse_args()<NewLine>         config = json.load(open(args.config))<NewLine>         bottleneck_size = config['arch']['bottleneck_size']<NewLine>         if type(bottleneck_size) is list:<NewLine>             if len(bottleneck_size) == 5:    #comparing it to 5 because we have 5 bottlenecks in the model<NewLine>                 pass<NewLine>             else:<NewLine>                 raise Exception(""bottleneck's list length in config.json file is not equal to number of bottnecks in model's structure"")<NewLine>             return bottleneck_size<NewLine>         elif type(bottleneck_size) is int:<NewLine>             list_tmp = []<NewLine>             for i in range(5):<NewLine>                 list_tmp.append(bottleneck_size)<NewLine>             bottleneck_size = list_tmp<NewLine>             del(list_tmp)<NewLine>             return bottleneck_size<NewLine>    <NewLine>    @torch.jit.script_method     <NewLine>    def forward(self, x):<NewLine>        '''<NewLine>        This function computes the network computations based on input x <NewLine>        built in the constructor of the the CombustionModel<NewLine>        '''<NewLine>        <NewLine>        '''First Layer'''<NewLine>        x = self.Fc1(x)<NewLine>        x = F.relu(x)<NewLine>        <NewLine>        '''First ResNet Block'''<NewLine>        res_calc = self.Fc2(x)<NewLine>        res_calc = F.relu(res_calc)<NewLine>        res_calc = self.Fc3_bottleneck(res_calc)<NewLine>        x = F.relu(torch.add(x, res_calc))<NewLine>        <NewLine>        '''Second ResNet Block'''<NewLine>        res_calc = self.Fc4(x)<NewLine>        res_calc = F.relu(res_calc)<NewLine>        res_calc = self.Fc5_bottleneck(res_calc)<NewLine>        x = F.relu(torch.add(x, res_calc))<NewLine>        <NewLine>        '''Third ResNet Block'''<NewLine>        res_calc = self.Fc6(x)<NewLine>        res_calc = F.relu(res_calc)<NewLine>        res_calc = self.Fc7_bottleneck(res_calc)<NewLine>        x = F.relu(torch.add(x, res_calc))<NewLine>        <NewLine>        '''Fourth ResNet Block'''<NewLine>        res_calc = self.Fc8(x)<NewLine>        res_calc = F.relu(res_calc)<NewLine>        res_calc = self.Fc9_bottleneck(res_calc)<NewLine>        x = F.relu(torch.add(x, res_calc))<NewLine>        <NewLine>        '''Fifth ResNet Block'''<NewLine>        res_calc = self.Fc10(x)<NewLine>        res_calc = F.relu(res_calc)<NewLine>        res_calc = self.Fc11_bottleneck(res_calc)<NewLine>        x = F.relu(torch.add(x, res_calc))<NewLine>        <NewLine>        '''Regression layer'''<NewLine>        return self.Fc12(x)<NewLine>        <NewLine><NewLine></code></pre><NewLine><p><strong>this is base model file</strong></p><NewLine><pre><code class=""lang-auto"">import logging<NewLine>import torch<NewLine>import numpy as np<NewLine><NewLine><NewLine>class BaseModel(torch.jit.ScriptModule):<NewLine>    """"""<NewLine>    Base class for all models<NewLine>    """"""<NewLine>    def __init__(self):<NewLine>        super(BaseModel, self).__init__()<NewLine>        self.logger = logging.getLogger(self.__class__.__name__)<NewLine><NewLine>    def forward(self, *input):<NewLine>        """"""<NewLine>        Forward pass logic<NewLine><NewLine>        :return: Model output<NewLine>        """"""<NewLine>        raise NotImplementedError<NewLine><NewLine>    def summary(self):<NewLine>        """"""<NewLine>        Model summary<NewLine>        """"""<NewLine>        model_parameters = filter(lambda p: p.requires_grad, self.parameters())<NewLine>        params = sum([np.prod(p.size()) for p in model_parameters])<NewLine>        self.logger.info('Trainable parameters: {}'.format(params))<NewLine>        self.logger.info(self)<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you sure you’re using 1.0.1? Try putting a <code>print(torch.__version__)</code> in your model somewhere. <a href=""https://github.com/pytorch/pytorch/pull/15340"" rel=""nofollow noopener"">This change</a> should have fixed the issue and is available in the latest release.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><img alt=""Screenshot%20from%202019-03-04%2013-13-22"" height=""141"" src=""https://discuss.pytorch.org/uploads/default/original/2X/c/c6b58795e2143d1e3752ebe44c8cb5dac02206e4.png"" width=""663""/></p><NewLine><p>The version is the same as you mentioned.<br/><NewLine>Now I am trying to incorporate what you suggested</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I uninstalled the pytorch and redid it again but the changes you suggested doesn’t appear in the installing directory.<br/><NewLine>I also tried to clone it form source like this:</p><NewLine><pre><code class=""lang-auto"">git clone --recursive https://github.com/pytorch/pytorch<NewLine>cd pytorch<NewLine></code></pre><NewLine><p>and then the changes appeared in the <code>pytorch/test/test_jit.py</code> and <code>torch/jit/__init__.py</code>  but as I tried to install it via running this command:</p><NewLine><pre><code class=""lang-auto"">export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}<NewLine>python setup.py install<NewLine></code></pre><NewLine><p>I got this kind of error:</p><NewLine><pre><code class=""lang-auto"">caffe2/CMakeFiles/caffe2_gpu.dir/build.make:210147: recipe for target 'caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp.o' failed<NewLine>make[2]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp.o] Error 1<NewLine>make[2]: *** Waiting for unfinished jobs....<NewLine>CMakeFiles/Makefile2:6469: recipe for target 'caffe2/CMakeFiles/caffe2_gpu.dir/all' failed<NewLine>make[1]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/all] Error 2<NewLine>Makefile:140: recipe for target 'all' failed<NewLine>make: *** [all] Error 2<NewLine>Traceback (most recent call last):<NewLine>  File ""setup.py"", line 710, in &lt;module&gt;<NewLine>    build_deps()<NewLine>  File ""setup.py"", line 282, in build_deps<NewLine>    build_dir='build')<NewLine>  File ""/home/hiwi/pytorch/tools/build_pytorch_libs.py"", line 255, in build_caffe2<NewLine>    check_call(['make', '-j', str(max_jobs), 'install'], cwd=build_dir, env=my_env)<NewLine>  File ""/home/hiwi/anaconda3/lib/python3.6/subprocess.py"", line 311, in check_call<NewLine>    raise CalledProcessError(retcode, cmd)<NewLine>subprocess.CalledProcessError: Command '['make', '-j', '4', 'install']' returned non-zero exit status 2.<NewLine></code></pre><NewLine><p>Any idea what’s happening here?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/justusschock; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Hong; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/justusschock; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/arslansadiq; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/arslansadiq; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/arslansadiq; <NewLine> ,"REPLY_DATE 1: February 5, 2019,  7:49pm; <NewLine> REPLY_DATE 2: February 8, 2019,  5:14pm; <NewLine> REPLY_DATE 3: February 6, 2019,  5:33pm; <NewLine> REPLY_DATE 4: February 11, 2019,  7:34am; <NewLine> REPLY_DATE 5: February 28, 2019,  5:40pm; <NewLine> REPLY_DATE 6: February 28, 2019,  7:15pm; <NewLine> REPLY_DATE 7: March 4, 2019, 12:15pm; <NewLine> REPLY_DATE 8: March 4, 2019,  6:35pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
38869,Wrong results when JIT exporting an LSTM module,2019-03-04T12:43:10.997Z,1,318,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Find below a self contained example:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch import nn<NewLine><NewLine>T = 10<NewLine>F = 20<NewLine>device = torch.device('cuda')<NewLine><NewLine>print('Generating data')<NewLine>data = (torch.rand(1, T, F) * 0.1).to(device)<NewLine><NewLine>print('Loading model')<NewLine>model = nn.LSTM(F, F, num_layers=1, batch_first=True, bidirectional=True, dropout=0)<NewLine>model = model.eval().to(device)<NewLine><NewLine>print('Tracing model')<NewLine>tmodel = torch.jit.trace(model, (data,))<NewLine>tmodel.save('/tmp/test.pt')<NewLine><NewLine>print('Productionazing model')<NewLine>pmodel = torch.jit.load('/tmp/test.pt', map_location=device)<NewLine><NewLine>print('Forwarding data')<NewLine>with torch.no_grad():<NewLine>    o1 = model(data)[0]<NewLine>    o2 = tmodel(data)[0]<NewLine>    o3 = pmodel(data)[0]<NewLine><NewLine>assert (o1 == o2).all()   # WORKS<NewLine>assert (o2 == o3).all()   # FAILS<NewLine></code></pre><NewLine><p>The above example shows 3 different versions of the same model:</p><NewLine><ul><NewLine><li><NewLine><code>model</code>: raw LSTM</li><NewLine><li><NewLine><code>tmodel</code>: traced LSTM</li><NewLine><li><NewLine><code>pmodel</code>: dumped and loaded tmodel</li><NewLine></ul><NewLine><p><code>model</code> and <code>tmodel</code> calculate the same output. On the contrary. <code>pmodel</code> outputs wrong values + NaNs.</p><NewLine><p>Reproducible in both versions: (a) 1.0.0 and (b) 1.0.1</p><NewLine><p>Am I doing something wrong?</p><NewLine></div>",https://discuss.pytorch.org/u/imaluengo,(Imanol Luengo),imaluengo,"March 4, 2019,  2:28pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just updated the code by replacing <code>.cuda()</code> with <code>.to(device)</code>. Bug is still reproducible when using <code>device = torch.device('cuda')</code> but seems to the code seems to work with <code>device = torch.device('cpu')</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Issue no longer reproducible with <code>torch-nightly-1.0.0.dev20190304</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/imaluengo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/imaluengo; <NewLine> ,"REPLY_DATE 1: March 4, 2019,  2:29pm; <NewLine> REPLY_DATE 2: March 12, 2019,  7:44am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
38262,The output tensor&rsquo;s shape of a scripted model is fixed by example&rsquo;s shape?,2019-02-26T06:56:52.106Z,1,487,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In a pytorch network model, if I use <code>nn.ConvTranspose2d</code> and do <code>traced_script_module = torch.jit.trace(model, example)</code> after constructing the model. Whatever the shape size of example tensor is, I can get the output tensor, of which the shape is same as the input image, from <code>traced_script_module</code>.<br/><NewLine>But if I use <code>nn.Upsample</code> instead of <code>nn.ConvTranspose2d</code> in the model. Strange things happen. I only can get the output tensor which the shape of is the same as example tensor that I used in <code>torch.jit.trace(model, example)</code>.<br/><NewLine>Can anyone explain this strange thing. Is that related to <code>weak_script_method</code> decorator or something?</p><NewLine></div>",https://discuss.pytorch.org/u/cfanyyx,(cfanyyx),cfanyyx,"February 26, 2019,  7:08am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/cfanyyx"">@cfanyyx</a>, thanks for raising the issue. The problem on this is because <code>nn.Upsample</code> is a module that contains data dependent control flow (which means the traced model totally depends on the input you provide during tracing and will not generalize to the new input). To make <code>nn.Upsample</code> working in JIT, you will need to use TorchScript scripting instead of tracing. You can refer more to the doc <a href=""https://pytorch.org/docs/master/jit.html"" rel=""nofollow noopener"">here</a></p><NewLine><p>Let me know if that’s clear or not.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply. It helps me a lot. I will have a try. Actually, I tried to use TorchScript scripting before, but I didn’t make it so I changed to use tracing instead. I will try TorchScript scripting again. And I will come back if I meet any new question.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/cfanyyx; <NewLine> ,"REPLY_DATE 1: February 28, 2019,  7:31pm; <NewLine> REPLY_DATE 2: March 1, 2019,  7:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
38501,"When I converted my model into Torch Script using torch. jit. trace, a warning `TracedModules don&rsquo;t support parameter sharing between modules&rsquo;appeared. What does that mean?",2019-02-28T04:44:04.555Z,1,341,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Or how to use torch. jit. ScriptModule to generate model.pt</p><NewLine></div>",https://discuss.pytorch.org/u/redrumshinning,(Redrumshinning),redrumshinning,"February 28, 2019,  4:44am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It means you cannot share parameters (like weights) between modules and trace the forward() successfully. Consider decomposing your model into pieces that don’t share parameters.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply.But I still don’t understand how to share parameters between modules, and how can I decompose it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/redrumshinning; <NewLine> ,"REPLY_DATE 1: February 28, 2019,  7:08pm; <NewLine> REPLY_DATE 2: March 1, 2019,  1:14am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
37552,Loading BatchNorm1d with JIT,2019-02-18T14:41:15.013Z,3,610,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!</p><NewLine><p>I’m trying to load a PyTorch model in C++, using JIT. The model is defined as follows:</p><NewLine><pre><code class=""lang-auto"">class JitModel(torch.jit.ScriptModule):<NewLine>    def __init__(self):<NewLine>        super(JitModel, self).__init__()<NewLine><NewLine>        self.n_layers = 5<NewLine>        self.n_features = 14<NewLine>        self.fc1 = torch.nn.Linear(14, 14)<NewLine>        self.fc2 = torch.nn.Linear(14, 14)<NewLine>        self.fc3 = torch.nn.Linear(14, 14)<NewLine>        self.fc4 = torch.nn.Linear(14, 14)<NewLine>        self.fc5 = torch.nn.Linear(14, 14)<NewLine>        self.out = torch.nn.Linear(14, 1)<NewLine>        self.normalise = torch.nn.BatchNorm1d(14)<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def forward(self, x):<NewLine>        _x = x<NewLine>        # _x = self.normalise(_x)<NewLine>        _x = F.relu(self.fc1(_x))<NewLine>        _x = F.relu(self.fc2(_x))<NewLine>        _x = F.relu(self.fc3(_x))<NewLine>        _x = F.relu(self.fc4(_x))<NewLine>        _x = F.relu(self.fc5(_x))<NewLine>        _x = torch.sigmoid(self.out(_x))<NewLine>        return _x<NewLine></code></pre><NewLine><p>where</p><NewLine><pre><code class=""lang-auto"">F = torch.nn.functional<NewLine></code></pre><NewLine><p>This model is used for training in a Python3 script and then saved using the JIT save() function.<br/><NewLine>When loaded with</p><NewLine><pre><code class=""lang-auto"">torch.jit.load(""model.pt"") (in Python)<NewLine></code></pre><NewLine><p>the model is loaded correctly.<br/><NewLine>When loaded with</p><NewLine><pre><code class=""lang-auto"">torch::jit::load(""model.pt"") (in C++)<NewLine></code></pre><NewLine><p>this happens:</p><NewLine><pre><code class=""lang-auto"">terminate called after throwing an instance of 'torch::jit::script::ErrorReport'<NewLine>  what():<NewLine>Return value was annotated as having type Tuple[] but is actually of type Optional[Tuple[]]:<NewLine>op_version_set = 0<NewLine>def _check_input_dim(self,<NewLine>    input: Tensor) -&gt; Tuple[]:<NewLine>  _0 = torch.ne(torch.dim(input), 2)<NewLine>  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...  &lt;--- HERE<NewLine>  if _0:<NewLine>    _1 = torch.ne(torch.dim(input), 3)<NewLine>  else:<NewLine>    _1 = _0<NewLine>  if _1:<NewLine>    ops.prim.RaiseException(""Exception"")<NewLine>  else:<NewLine>    pass<NewLine>def forward(self,<NewLine>Aborted (core dumped)<NewLine></code></pre><NewLine><p>If I comment the line:</p><NewLine><pre><code class=""lang-auto"">        self.normalise = torch.nn.BatchNorm1d(14)<NewLine></code></pre><NewLine><p>in the definition, the C++ script loads the model correctly.<br/><NewLine>Really no idea of what’s wrong with this implementation.</p><NewLine></div>",https://discuss.pytorch.org/u/Anthair,(Anthair),Anthair,"February 18, 2019,  2:51pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This looks like a bug in our side. I’ve failed a <a href=""https://github.com/pytorch/pytorch/issues/17327"" rel=""nofollow noopener"">Github issue</a>, feel free to follow along there.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the answer <a class=""mention"" href=""/u/michael_suo"">@Michael_Suo</a>, I’ll follow.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/anthair"">@Anthair</a>,</p><NewLine><p>Thanks for raising this issue, i tried to reproduce it, but it works all fine on my side (even if with self.normalise uncommented), c++ frontend loads the model correctly. Can you verify if this is still the case in our latest nightly? If it is still the case, can you share your environment? it might be a environment only issue.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/wanchaol"">@wanchaol</a>,</p><NewLine><p>thank you for the help.<br/><NewLine>Using the latest (20190222) nightly, the model is loaded in C++ (or at least, it doesn’t crash when calling the load function). However, it still crashes with the stable 1.0.0.<br/><NewLine>I’ll cleanup the environment and try again with the stable.</p><NewLine><p>Thanks again for helping with the issue.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/anthair"">@Anthair</a> 1.0.1 is our latest stable version as it contains bunch of bug fixes from 1.0.0, please feel free to try out 1.0.1 and see if the error still bumps out or not. If you want to try out our latest feature, you can stick to our nightlies or build the master on your own <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/wanchaol"">@wanchaol</a>, thanks for the link. I downloaded  1.0.1 and I can confirm that the ::Load function does not crash. However, using either the nightly 20190222 or the stable 1.0.1, using the model for inference results in a crash:</p><NewLine><pre><code class=""lang-auto"">Model Loaded<NewLine>0x12321b0<NewLine>terminate called after throwing an instance of 'torch::jit::JITException'<NewLine>  what():<NewLine>Exception:<NewLine>operation failed in interpreter:<NewLine>op_version_set = 0<NewLine>def forward(self,<NewLine>    x: Tensor) -&gt; Tensor:<NewLine>  _0 = torch.ne(torch.dim(x), 2)<NewLine>  if _0:<NewLine>    _1 = torch.ne(torch.dim(x), 3)<NewLine>  else:<NewLine>    _1 = _0<NewLine>  if _1:<NewLine>    ops.prim.RaiseException(""Exception"")<NewLine>    ~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>  else:<NewLine>    pass<NewLine>  _2 = bool(self.normalise.training)<NewLine>  if _2:<NewLine>    _3 = True<NewLine>  else:<NewLine>    _3 = _2<NewLine>  if _3:<NewLine>    _4 = torch.add_(self.normalise.num_batches_tracked, 1, 1)<NewLine>Aborted (core dumped)<NewLine></code></pre><NewLine><p>The loading and inference code is:</p><NewLine><pre><code class=""lang-auto"">#include &lt;torch/script.h&gt; // One-stop header.<NewLine>#include &lt;ATen/ATen.h&gt;<NewLine><NewLine>#include &lt;iostream&gt;<NewLine>#include &lt;memory&gt;<NewLine><NewLine>int main(int argc, const char* argv[]) {<NewLine>  if (argc != 2) {<NewLine>    std::cerr &lt;&lt; ""usage: example-app &lt;path-to-exported-script-module&gt;\n"";<NewLine>    return -1;<NewLine>  }<NewLine><NewLine>  auto module = torch::jit::load(argv[1]);<NewLine><NewLine>  assert(module != nullptr);<NewLine>  std::cout &lt;&lt; ""Model Loaded\n"";<NewLine><NewLine>  std::vector&lt;torch::jit::IValue&gt; inputs;<NewLine>  inputs.push_back(torch::rand({14,}));<NewLine><NewLine>  std::cout &lt;&lt; module &lt;&lt; std::endl;<NewLine><NewLine>  auto output = module-&gt;forward(inputs);<NewLine>}<NewLine></code></pre><NewLine><p>I ran it on two different, totally independent setups, with the same results.<br/><NewLine>Here is the training file, for testing:<br/><NewLine><a class=""onebox"" href=""https://drive.google.com/file/d/1jEgTXMRFkUa1pynOK_rP0Lir39nzhtt8/view?usp=sharing"" rel=""nofollow noopener"" target=""_blank"">https://drive.google.com/file/d/1jEgTXMRFkUa1pynOK_rP0Lir39nzhtt8/view?usp=sharing</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/anthair"">@Anthair</a> thanks a lot for the follow up, I will try to reproduce it and get back to you with more details</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/anthair"">@Anthair</a> OK I looked into your code, When you have batchnorm1d, your input must be 2d or 3d, refer to the doc <a href=""https://pytorch.org/docs/stable/nn.html#batchnorm1d"" rel=""nofollow noopener"">here</a>, changing the input to something like torch::rand(2, 14) works well.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/wanchaol"">@wanchaol</a>, ok it was a stupid error on my side. Thanks for the help, now the inference works fine, I’d say we can close.</p><NewLine><p>PS: I really would like to check the weights and the general state of the model, is there something similar to load_state_dict() in C++? That would make eveything easier.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah for the load_state_dict thing, I think it should be on our roadmap for c++ frontend. Feel free to create a issue to get on track <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Anthair; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Anthair; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Anthair; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/wanchaol; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Anthair; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/wanchaol; <NewLine> ,"REPLY_DATE 1: February 21, 2019, 12:53am; <NewLine> REPLY_DATE 2: February 21, 2019,  8:58am; <NewLine> REPLY_DATE 3: February 21, 2019,  9:36pm; <NewLine> REPLY_DATE 4: February 22, 2019,  9:55am; <NewLine> REPLY_DATE 5: February 25, 2019,  6:41pm; <NewLine> REPLY_DATE 6: February 26, 2019,  3:08pm; <NewLine> REPLY_DATE 7: February 26, 2019,  6:49pm; <NewLine> REPLY_DATE 8: February 28, 2019,  8:11am; <NewLine> REPLY_DATE 9: February 28, 2019,  2:27pm; <NewLine> REPLY_DATE 10: February 28, 2019,  6:48pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> 
36346,Trouble with reshape and jit/trace,2019-02-04T09:02:44.989Z,4,477,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a model that has a reshape operation inside it (essentially to do something like group normalisation, but different). I reshape such that the channel dimension becomes two channels, sum over one of them, divide by it and then reshape it back.</p><NewLine><p>This works fine while training and testing, but when I jit.trace the model I get a malformed model, where the ‘self’ gets overwritten (see the ‘self=…’ line). As seen here in part of the code.py:</p><NewLine><blockquote><NewLine><p>x_70 = torch.add_(x_69, input_65, alpha=1)<br/><NewLine>_288 = ops.prim.NumToTensor(torch.size(x_70, 0))<br/><NewLine>_289 = int(_288)<br/><NewLine>_290 = int(_288)<br/><NewLine>self = ops.prim.NumToTensor(torch.size(x_70, 1))<br/><NewLine>_291 = int(self)<br/><NewLine>_292 = ops.prim.NumToTensor(torch.size(x_70, 2))<br/><NewLine>_293 = int(_292)<br/><NewLine>_294 = int(_292)<br/><NewLine>_295 = ops.prim.NumToTensor(torch.size(x_70, 3))<br/><NewLine>_296 = int(_295)<br/><NewLine>_297 = int(_295)<br/><NewLine>_298 = ops.prim.NumToTensor(torch.size(x_70, 4))<br/><NewLine>_299 = int(_298)<br/><NewLine>_300 = int(_298)<br/><NewLine>_301 = [_290, int(torch.div(self, CONSTANTS.c0)), 4, _294, _297, _300]<br/><NewLine>x_71 = torch.reshape(x_70, _301)</p><NewLine></blockquote><NewLine><p>When I replace ‘self’ with ‘self_19’ it’s allright, and I can load the model.<br/><NewLine>However I also have issues exporting in ‘onnx’ which complains about the reshape operation.<br/><NewLine>And I have troubles then running the model in the C++ API, the model does not work on GPU on linux (but works on CPU on LINUX, and both GPU and CPU on Windows).</p><NewLine><p>I have a feeling all these problems are related, is there something known about the reshape operation that causes this?</p><NewLine></div>",https://discuss.pytorch.org/u/marijnfs,(Marijn Stollenga),marijnfs,"February 4, 2019,  9:02am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the report! Seems like it may be a problem with our serialization code. Could you provide a small module/script that reproduces the problem so that we can investigate?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok to reproduce it I have a reshaping operation. It is essential that ‘view’ gets a shape that is calculated partly from another shape, as that seems to cause the trouble. I added a linear layer after that to make sure ‘self’ is used again and it fails:</p><NewLine><blockquote><NewLine><p>#!/usr/bin/ipython3</p><NewLine><p>import torch</p><NewLine><p>class Example(torch.nn.Module):<br/><NewLine>def <strong>init</strong>(self):<br/><NewLine>super(Example, self).<strong>init</strong>()</p><NewLine><pre><code>def forward(self, x):<NewLine>    s = x.shape<NewLine>    b = x.view(x.shape[0],x.shape[1]//2,2)<NewLine>    accum = b.sum(1, keepdim=True)<NewLine>    b = b * accum<NewLine>    return b.view(*s)<NewLine></code></pre><NewLine><p>class ExampleNested(torch.nn.Module):<br/><NewLine>def <strong>init</strong>(self):<br/><NewLine>super(ExampleNested, self).<strong>init</strong>()<br/><NewLine>self.ex = Example()<br/><NewLine>self.lin = torch.nn.Linear(4,4)</p><NewLine><pre><code>def forward(self, x):<NewLine>    x = self.ex(x)<NewLine>    x = self.lin(x)<NewLine>    return x<NewLine></code></pre><NewLine><p>a = torch.randn(4,4)</p><NewLine><p>example = ExampleNested()<br/><NewLine>traced = torch.jit.trace(example, a)<br/><NewLine>traced.save(“trace.tmp”)</p><NewLine></blockquote><NewLine><p>This gives me:</p><NewLine><blockquote><NewLine><p>op_version_set = 0<br/><NewLine>def forward(self,<br/><NewLine>x: Tensor) -&gt; Tensor:<br/><NewLine>_0 = ops.prim.NumToTensor(torch.size(x, 0))<br/><NewLine>_1 = int(_0)<br/><NewLine>_2 = ops.prim.NumToTensor(torch.size(x, 1))<br/><NewLine>_3 = int(_2)<br/><NewLine>_4 = ops.prim.NumToTensor(torch.size(x, 0))<br/><NewLine>_5 = int(_4)<br/><NewLine>self = ops.prim.NumToTensor(torch.size(x, 1))<br/><NewLine>_6 = [_5, int(torch.div(self, CONSTANTS.c0)), 2]<br/><NewLine>b_1 = torch.view(x, _6)<br/><NewLine>accum = torch.sum(b_1, [1], True)<br/><NewLine>b = torch.mul(b_1, accum)<br/><NewLine>input = torch.view(b, [_1, _3])<br/><NewLine>_7 = torch.addmm(self.lin.bias, input, torch.t(self.lin.weight), beta=1, alpha=1)<br/><NewLine>return _7</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Btw to follow up, I’ll add that I only see this problem happening with the <em>last</em> operation being done. So it doesn’t matter how many of such layers I connect together, only the last operation gets the wrong ‘self’ naming without the number at the end.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for report. I’ve filed a GH issue <a href=""https://github.com/pytorch/pytorch/issues/17147"" rel=""nofollow noopener"">here</a> and we will update you there.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see from the issue that the bugfix is in 1.0.1! I’ll try it now, since it seems the pip package is updated.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/marijnfs; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/marijnfs; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/marijnfs; <NewLine> ,"REPLY_DATE 1: February 4, 2019,  5:38pm; <NewLine> REPLY_DATE 2: February 5, 2019,  9:16am; <NewLine> REPLY_DATE 3: February 11, 2019, 10:41am; <NewLine> REPLY_DATE 4: February 15, 2019,  2:05am; <NewLine> REPLY_DATE 5: February 19, 2019,  8:53am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> 
37075,Low GPU utilization when training an ensemble,2019-02-13T10:06:03.405Z,1,965,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I would like to train an ensemble of same network architectures. Currently, I’m defining it this way.</p><NewLine><pre><code class=""lang-auto"">def make_net(network_specs):<NewLine>    # return nn.Module<NewLine><NewLine>class Ensemble(nn.Module):<NewLine>    def __init__(self, network_specs, ensemble_size):<NewLine>        super().__init__()<NewLine>        self.model = nn.ModuleList([make_net(network_specs) for _ in range(ensemble_size)])<NewLine>        <NewLine>    def forward(self, x):<NewLine>        return torch.stack([self.model[i](x[i]) for i in range(ensemble_size)])<NewLine></code></pre><NewLine><p>However, backprop of the <code>stack</code> operator doesn’t seem to be parallelized (in the same GPU). The GPU utilization is very low, around 15%.</p><NewLine><p>Thinking that dynamic graph might be the cause (<a href=""https://discuss.pytorch.org/t/is-there-a-way-to-parallelize-independent-sequential-steps/3360"">similar thread here</a>), I’ve recently tried using <code>@torch.jit</code>. The performance still doesn’t improve.</p><NewLine><p>What am I doing wrong here? How can I improve the performance of my ensemble model?</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/zer0ne,,zer0ne,"February 13, 2019, 10:06am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>All the opreations that run on the GPU are asynchronous. So if the GPU usage is very low, it’s most likely because your networks are not big enough to use all the GPU.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>If your network is big enough you can try making bigger batches and check CPU and Disk usage maybe you are doing some data manipulation inside of train loop so its bottleneck.<br/><NewLine>And if you dont move big enough piece of data into .cuda() before training it also can be bottleneck and cpu usage should be pretty high.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>If the network is small, we would expect the GPU utilization to get higher when we increase the ensemble size. However, the GPU utilization doesn’t increase when we scale the ensemble size. The total run time increases almost linearly with respect to the ensemble size.</p><NewLine><p>Note that this is a reinforcement learning task (on simple environments), so data processing/transfer is not a bottleneck.</p><NewLine><p>Here is the visualization of my network when the ensemble size is 4.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/35f7e6d2dac25e74e4ab0afbca11e4091d9b41c6"" href=""https://discuss.pytorch.org/uploads/default/original/2X/3/35f7e6d2dac25e74e4ab0afbca11e4091d9b41c6.png"" title=""bootstrap_size=4.png""><img alt=""bootstrap_size%3D4"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/3/35f7e6d2dac25e74e4ab0afbca11e4091d9b41c6_2_10x10.png"" height=""274"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/3/35f7e6d2dac25e74e4ab0afbca11e4091d9b41c6_2_690x274.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/3/35f7e6d2dac25e74e4ab0afbca11e4091d9b41c6_2_690x274.png, https://discuss.pytorch.org/uploads/default/optimized/2X/3/35f7e6d2dac25e74e4ab0afbca11e4091d9b41c6_2_1035x411.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/3/35f7e6d2dac25e74e4ab0afbca11e4091d9b41c6_2_1380x548.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">bootstrap_size=4.png</span><span class=""informations"">2310×920 303 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>And here’s our profiling result for different ensemble sizes.</p><NewLine><pre><code class=""lang-auto"">time_forward, time_backward: run 50 times<NewLine>Backward, Forward: time_B128/time_b4 ~ 28-30<NewLine><NewLine>Bootstrap_size: 4:<NewLine>-----------------------------------------<NewLine>time_backward 0.07882976531982422<NewLine>mean_time_backward 0.0015765953063964844<NewLine>time_forward 0.05576205253601074<NewLine>mean_time_forward 0.0011152410507202148<NewLine>time_backward 0.07231521606445312<NewLine>mean_time_backward 0.0014463043212890624<NewLine>time_forward 0.05587363243103027<NewLine>mean_time_forward 0.0011174726486206056<NewLine>time_backward 0.07005977630615234<NewLine>mean_time_backward 0.0014011955261230469<NewLine>time_forward 0.05555391311645508<NewLine>mean_time_forward 0.0011110782623291015<NewLine>time_backward 0.07131695747375488<NewLine>mean_time_backward 0.0014263391494750977<NewLine>time_forward 0.055143117904663086<NewLine>mean_time_forward 0.0011028623580932617<NewLine>time_backward 0.06970882415771484<NewLine>mean_time_backward 0.001394176483154297<NewLine>time_forward 0.05509185791015625<NewLine>mean_time_forward 0.001101837158203125<NewLine>time_backward 0.0810239315032959<NewLine>mean_time_backward 0.001620478630065918<NewLine>time_forward 0.05518746376037598<NewLine>mean_time_forward 0.0011037492752075195<NewLine>time_backward 0.07718276977539062<NewLine>mean_time_backward 0.0015436553955078126<NewLine>time_forward 0.05403590202331543<NewLine>mean_time_forward 0.0010807180404663085<NewLine><NewLine>Bootstrap_size: 32:<NewLine>-----------------------------------------<NewLine>time_backward 0.48969507217407227<NewLine>mean_time_backward 0.009793901443481445<NewLine>time_forward 0.4311997890472412<NewLine>mean_time_forward 0.008623995780944825<NewLine>time_backward 0.4772953987121582<NewLine>mean_time_backward 0.009545907974243165<NewLine>time_forward 0.516700029373169<NewLine>mean_time_forward 0.01033400058746338<NewLine>time_backward 0.4743640422821045<NewLine>mean_time_backward 0.00948728084564209<NewLine>time_forward 0.5470066070556641<NewLine>mean_time_forward 0.01094013214111328<NewLine>time_backward 0.5156633853912354<NewLine>mean_time_backward 0.010313267707824708<NewLine>time_forward 0.5515599250793457<NewLine>mean_time_forward 0.011031198501586913<NewLine>time_backward 0.48656153678894043<NewLine>mean_time_backward 0.009731230735778808<NewLine>time_forward 0.5587642192840576<NewLine>mean_time_forward 0.011175284385681153<NewLine>time_backward 0.48267650604248047<NewLine>mean_time_backward 0.009653530120849609<NewLine>time_forward 0.549140214920044<NewLine>mean_time_forward 0.01098280429840088<NewLine>time_backward 0.493422269821167<NewLine>mean_time_backward 0.00986844539642334<NewLine>time_forward 0.546377420425415<NewLine>mean_time_forward 0.0109275484085083<NewLine><NewLine>Bootstrap_size: 128:<NewLine>-----------------------------------------<NewLine>time_backward 2.0336191654205322<NewLine>mean_time_backward 0.040672383308410644<NewLine>time_forward 2.0258209705352783<NewLine>mean_time_forward 0.040516419410705565<NewLine>time_backward 2.0157926082611084<NewLine>mean_time_backward 0.04031585216522217<NewLine>time_forward 1.716789960861206<NewLine>mean_time_forward 0.03433579921722412<NewLine>time_backward 1.9942104816436768<NewLine>mean_time_backward 0.039884209632873535<NewLine>time_forward 1.6753108501434326<NewLine>mean_time_forward 0.033506217002868655<NewLine>time_backward 2.0784974098205566<NewLine>mean_time_backward 0.04156994819641113<NewLine>time_forward 1.6769888401031494<NewLine>mean_time_forward 0.033539776802062986<NewLine>time_backward 1.9966001510620117<NewLine>mean_time_backward 0.03993200302124023<NewLine>time_forward 1.6629443168640137<NewLine>mean_time_forward 0.033258886337280275<NewLine>time_backward 1.9680683612823486<NewLine>mean_time_backward 0.039361367225646975<NewLine>time_forward 1.679962158203125<NewLine>mean_time_forward 0.0335992431640625<NewLine>time_backward 2.00929856300354<NewLine>mean_time_backward 0.0401859712600708<NewLine>time_forward 1.664689302444458<NewLine>mean_time_forward 0.03329378604888916<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>The thing with GPUs is that they are very good at doing one very parrallel task but not many small parrallel tasks <img alt="":confused:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=6"" title="":confused:""/><br/><NewLine>You can use nvidia visual profiler nvvp if you want to look more in details how your code runs on the gpu. But you’re most certainly going to have “low core usage” if you have small tasks.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dawwdd; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/zer0ne; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: February 13, 2019, 10:24am; <NewLine> REPLY_DATE 2: February 13, 2019, 10:50am; <NewLine> REPLY_DATE 3: February 14, 2019,  6:33pm; <NewLine> REPLY_DATE 4: February 15, 2019, 10:01am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
37163,"TypeError in torch.jit.trace(model, example)",2019-02-14T09:07:34.955Z,2,316,"<div class=""post"" itemprop=""articleBody""><NewLine><p>ubuntu 16LTS,torch version:1.0.1.post2,-cpu version,Python 2.7.15 |Anaconda</p><NewLine><p>here’s my codes:<br/><NewLine>import torch<br/><NewLine>import torchvision<br/><NewLine>import numpy as np<br/><NewLine>from importlib import import_module<br/><NewLine>from torch.nn import DataParallel<br/><NewLine>model = import_module(‘net_detector’)<br/><NewLine>config1, nod_net, loss, get_pbb = model.get_model()<br/><NewLine>checkpoint = torch.load(‘130.ckpt’)<br/><NewLine>model.load_state_dict(checkpoint[‘state_dict’])</p><NewLine><p>example = np.load(’./236350_clean.npy’)<br/><NewLine>example=torch.from_numpy(example)</p><NewLine><p>traced_script_module = torch.jit.trace(model, example)</p><NewLine><p>I got error as follow:</p><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “convert2pt.py”, line 18, in <br/><NewLine>traced_script_module = torch.jit.trace(nod_net, example)<br/><NewLine>File “/home/qrf/anaconda2/lib/python2.7/site-packages/torch/jit/<strong>init</strong>.py”, line 636, in trace<br/><NewLine>var_lookup_fn, _force_outplace)<br/><NewLine>File “/home/qrf/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py”, line 487, in <strong>call</strong><br/><NewLine>result = self._slow_forward(*input, **kwargs)<br/><NewLine>File “/home/qrf/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py”, line 477, in _slow_forward<br/><NewLine>result = self.forward(*input, **kwargs)<br/><NewLine>TypeError: forward() takes exactly 3 arguments (2 given)</p><NewLine></div>",https://discuss.pytorch.org/u/qrf,,qrf,"February 14, 2019,  9:14am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>the presaved model was saved in pytorch ‘0.3.1’</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you have a version of the model that is compatible with the current version of PyTorch? We do not guarantee that tracing works on models written for PyTorch 0.3</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>yes,thank you.maybe i should retrain my model using the stable version to figure out if it’s a “version” thing.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/qrf; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/qrf; <NewLine> ,"REPLY_DATE 1: February 14, 2019,  9:09am; <NewLine> REPLY_DATE 2: February 15, 2019,  7:27am; <NewLine> REPLY_DATE 3: February 15, 2019,  7:42am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
36945,Does JIT plan to support the differentiation of most operators?And when？,2019-02-12T02:30:29.516Z,0,292,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I saw that jit supports the differential ability of some operators (about 80), but the number of ATen’s operators (about 700) is much larger than the current support.<br/><NewLine>When does the plan support the differential ability of all operators?</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/autodiff.cpp#L30-L146"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/autodiff.cpp#L30-L146"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/master/torch/csrc/jit/autodiff.cpp#L30-L146</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""30"" style=""counter-reset: li-counter 29 ;""><NewLine><li>bool isDifferentiable(Node* n) {</li><NewLine><li>// TODO: scalar-tensor ops should be canonicalized</li><NewLine><li>static OperatorSet differentiable_ops = {</li><NewLine><li>    ""aten::add(Tensor self, Tensor other, *, Scalar alpha) -&gt; Tensor"",</li><NewLine><li>    ""aten::add(Tensor self, Scalar other, Scalar alpha) -&gt; Tensor"",</li><NewLine><li>    ""aten::sub(Tensor self, Tensor other, *, Scalar alpha) -&gt; Tensor"",</li><NewLine><li>    ""aten::sub(Tensor self, Scalar other, Scalar alpha) -&gt; Tensor"",</li><NewLine><li>    ""aten::mul(Tensor self, Tensor other) -&gt; Tensor"",</li><NewLine><li>    ""aten::mul(Tensor self, Scalar other) -&gt; Tensor"",</li><NewLine><li>    ""aten::div(Tensor self, Tensor other) -&gt; Tensor"",</li><NewLine><li>    ""aten::div(Tensor self, Scalar other) -&gt; Tensor"",</li><NewLine><li>    ""aten::max(Tensor self, Tensor other) -&gt; Tensor"",</li><NewLine><li>    ""aten::min(Tensor self, Tensor other) -&gt; Tensor"",</li><NewLine><li>    ""aten::sigmoid(Tensor self) -&gt; Tensor"",</li><NewLine><li>    ""aten::tanh(Tensor self) -&gt; Tensor"",</li><NewLine><li>    ""aten::relu(Tensor self) -&gt; Tensor"",</li><NewLine><li>    ""aten::threshold(Tensor self, Scalar threshold, Scalar value) -&gt; Tensor"",</li><NewLine><li>    ""aten::erf(Tensor self) -&gt; Tensor"",</li><NewLine><li>    ""aten::erfc(Tensor self) -&gt; Tensor"",</li><NewLine><li>    ""aten::exp(Tensor self) -&gt; Tensor"",</li><NewLine></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/autodiff.cpp#L30-L146"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>",https://discuss.pytorch.org/u/Alex_Zhang,,Alex_Zhang,"February 12, 2019,  2:30am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We plan to increase the number of symbolic derivatives that we support, but we don’t have a specific timeline for when we’ll reach full coverage. In the meantime, autograd still works for JIT modules, so as long as you set <code>requires_grad</code> on your input tensors you will be able call <code>backward()</code> like in Python.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: February 15, 2019,  7:24am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
37051,How to convert multi-label-classifier to torch script in pytorch 1.0,2019-02-13T03:11:41.133Z,1,362,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Everyone,</p><NewLine><p>I’m trying to convert multilabel classification model (pytorch 1.0) to torch script via annotation.</p><NewLine><p>Here is my model:</p><NewLine><pre><code class=""lang-auto"">class MultiLabelModel(nn.Module):<NewLine>    def __init__(self, basemodel, basemodel_output, num_classes):<NewLine>        super(MultiLabelModel, self).__init__()<NewLine>        self.basemodel = basemodel<NewLine>        self.num_classes = num_classes<NewLine>        for index, num_class in enumerate(num_classes):<NewLine>            setattr(self, ""FullyConnectedLayer_"" + str(index), nn.Linear(basemodel_output, num_class))<NewLine>    <NewLine>    def forward(self, x):<NewLine>        x = self.basemodel.forward(x)<NewLine>        outs = list()<NewLine>        dir(self)<NewLine>        for index, num_class in enumerate(self.num_classes):<NewLine>            fun = eval(""self.FullyConnectedLayer_"" + str(index))<NewLine>            out = fun(x)<NewLine>            outs.append(out)<NewLine>        return outs<NewLine></code></pre><NewLine><p>The basemodel is predefined backbone model.</p><NewLine><p>I tried this:</p><NewLine><pre><code class=""lang-auto"">class MultiLabelModel(torch.jit.ScriptModule):<NewLine>    __constants__ = ['num_classes']<NewLine>    __constants__ = ['basemodel']<NewLine>    __constants__ = ['basemodel_output']<NewLine>    def __init__(self, basemodel, basemodel_output, num_classes):<NewLine>        super(MultiLabelModel, self).__init__()<NewLine>        self.basemodel = basemodel<NewLine>        self.num_classes = num_classes<NewLine>        self.basemodel_output = basemodel_output<NewLine>        for index, num_class in enumerate(num_classes):<NewLine>            setattr(self, ""FullyConnectedLayer_"" + str(index), nn.Linear(basemodel_output, num_class))<NewLine>    @torch.jit.script_method<NewLine>    def forward(self, x):<NewLine>        x = self.basemodel.forward(x)<NewLine>        outs = list()<NewLine>        for index, num_class in enumerate(self.num_classes):<NewLine>            fun = eval(""self.FullyConnectedLayer_"" + str(index))<NewLine>            out = fun(x)<NewLine>            outs.append(out)<NewLine>        return outs<NewLine></code></pre><NewLine><p>and get error:</p><NewLine><blockquote><NewLine><p>expected a value of type Tensor for argument ‘0’ but found int</p><NewLine></blockquote><NewLine><p>seems like I can’t define the multilabel classifier like this:</p><NewLine><pre><code class=""lang-auto"">        for index, num_class in enumerate(num_classes):<NewLine>            setattr(self, ""FullyConnectedLayer_"" + str(index), nn.Linear(basemodel_output, num_class))<NewLine></code></pre><NewLine><p>Hope somebody can help me out.</p><NewLine></div>",https://discuss.pytorch.org/u/Wyn_Mew,(Wyn Mew),Wyn_Mew,"February 13, 2019,  7:45am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you share a complete example which includes driver code for the script?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Complete example:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torchvision<NewLine>import torch.nn as nn<NewLine><NewLine>MyClassNum = 10<NewLine><NewLine><NewLine>class FeatureExtraction(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super(FeatureExtraction, self).__init__()<NewLine>        self.resnet = torchvision.models.resnet18(pretrained=True)<NewLine>        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])<NewLine>        self.resnet.cuda()<NewLine>    def forward(self, image_batch):<NewLine>        return self.resnet(image_batch)<NewLine><NewLine>class MultiLabelModel(torch.jit.ScriptModule):<NewLine>    __constants__ = ['num_classes']<NewLine>    def __init__(self, num_classes):<NewLine>        super(MultiLabelModel, self).__init__()<NewLine>        self.num_classes = MyClassNum<NewLine>        for index in enumerate(range(0,self.num_classes)):<NewLine>            setattr(self, ""FullyConnectedLayer_"" + str(index[0]), nn.Linear(512, 1))<NewLine>    @torch.jit.script_method<NewLine>    def forward(self, x):<NewLine>        x = x.view(x.size(0), -1) # flatten<NewLine>        outs = list()<NewLine>        for index in enumerate(range(0,self.num_classes)):<NewLine>            fun = eval(""self.FullyConnectedLayer_"" + str(index[0]))<NewLine>            out = fun(x)<NewLine>            outs.append(out)<NewLine>        return outs<NewLine><NewLine>class MultiLabelClassifier(torch.jit.ScriptModule):<NewLine>    def __init__(self, num_classes):<NewLine>        super(MultiLabelClassifier, self).__init__()<NewLine>        self.FeatureExtraction = FeatureExtraction()<NewLine>        self.classifier = MultiLabelModel(num_classes)<NewLine>    @torch.jit.script_method<NewLine>    def forward(self, img):<NewLine>        feature = self.FeatureExtraction(img)<NewLine>        preLabels= self.classifier(feature)<NewLine>        return preLabels<NewLine><NewLine>my_script_module = MultiLabelClassifier(MyClassNum)<NewLine>my_script_module.save(""model.pt"")<NewLine></code></pre><NewLine><p>Thanks for your quick reply.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pramod.srinivasan; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Wyn_Mew; <NewLine> ,"REPLY_DATE 1: February 13, 2019,  7:09am; <NewLine> REPLY_DATE 2: February 13, 2019,  7:57am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
36681,TorchScript model&rsquo;s output is wrong when loads from CPP,2019-02-08T09:46:22.362Z,4,522,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Team,<br/><NewLine>I have a very simple fizbuz model made using pytorch python APIs which I have exported as ScriptModule. I am loading the same module from python and CPP and passing same input but getting wrong output in CPP. <strong>In fact, regardless of what ever input I pass, I get the exactly same values in the output from CPP</strong><br/><NewLine>Here is my python and CPP code for the same</p><NewLine><p>PS: I am a CPP noob</p><NewLine><pre><code class=""lang-auto""># ~/myp/HOD/8.P/FizBuzTorchScript&gt; python fizbuz.py fizbuz_model.pt 2<NewLine><NewLine>import sys<NewLine>import torch<NewLine><NewLine><NewLine>def main():<NewLine>    net = torch.jit.load(sys.argv[1])<NewLine>    temp = [int(i) for i in '{0:b}'.format(int(sys.argv[2]))]<NewLine>    array = [0] * (10 - len(temp)) + temp<NewLine>    inputs = torch.Tensor([array])<NewLine>    print(inputs)  # tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])<NewLine>    output = net(inputs)<NewLine>    print(output)  # tensor([[ -1.8873, -17.1001,  -3.7774,   3.7985]], ...<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    main()<NewLine><NewLine></code></pre><NewLine><pre><code class=""lang-auto"">// ~/myp/HOD/8.P/FizBuzTorchScript/build&gt; ./fizbuz ../fizbuz_model.pt 2<NewLine><NewLine>#include &lt;torch/script.h&gt;<NewLine><NewLine>#include &lt;iostream&gt;<NewLine>#include &lt;memory&gt;<NewLine>#include &lt;string&gt;<NewLine><NewLine>int main(int argc, const char* argv[]) {<NewLine>	if (argc != 3) {<NewLine>		std::cerr &lt;&lt; ""usage: &lt;appname&gt; &lt;path&gt; &lt;int&gt;\n"";<NewLine>		return -1;<NewLine>	}<NewLine>	std::string arg = argv[2];<NewLine>	int x = std::stoi(arg);<NewLine>	int array[10];<NewLine><NewLine>	int i;<NewLine>	int j = 9;<NewLine>	for (i = 0; i &lt; 10; ++i) {<NewLine>	    array[j] = (x &gt;&gt; i) &amp; 1;<NewLine>	    j--;<NewLine>	}<NewLine>	std::shared_ptr&lt;torch::jit::script::Module&gt; module = torch::jit::load(argv[1]);<NewLine>	torch::Tensor tensor_in = torch::from_blob(array, {1, 10});<NewLine>	std::vector&lt;torch::jit::IValue&gt; inputs;<NewLine>	inputs.push_back(tensor_in);<NewLine>	std::cout &lt;&lt; inputs &lt;&lt; '\n';<NewLine>	/*<NewLine>		1e-45 *<NewLine>			 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  1.4013  0.0000<NewLine>			[ Variable[CPUFloatType]{1,10} ]<NewLine>	*/<NewLine><NewLine><NewLine>	at::Tensor output = module-&gt;forward(inputs).toTensor();<NewLine>	std::cout &lt;&lt; output &lt;&lt; '\n';<NewLine><NewLine>	/*<NewLine>		 3.7295 -23.8977 -8.2652 -1.3901<NewLine>			[ Variable[CPUFloatType]{1,4} ]<NewLine>	*/<NewLine>}<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/hhsecond,(Sherin Thomas),hhsecond,"February 9, 2019,  4:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here is the model, if it helps</p><NewLine><pre><code class=""lang-auto"">input_size = 10<NewLine>output_size = 4<NewLine>hidden_size = 100<NewLine><NewLine><NewLine>class FizBuzNet(nn.Module):<NewLine>    """"""<NewLine>    2 layer network for predicting fiz or buz<NewLine>    param: input_size -&gt; int<NewLine>    param: output_size -&gt; int<NewLine>    """"""<NewLine><NewLine>    def __init__(self, input_size, hidden_size, output_size):<NewLine>        super(FizBuzNet, self).__init__()<NewLine>        self.hidden = nn.Linear(input_size, hidden_size)<NewLine>        self.out = nn.Linear(hidden_size, output_size)<NewLine><NewLine>    def forward(self, batch):<NewLine>        hidden = self.hidden(batch)<NewLine>        activated = torch.sigmoid(hidden)<NewLine>        out = self.out(activated)<NewLine>        return out<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group quote-modified"" data-post=""1"" data-topic=""36681""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/hhsecond/40/553_2.png"" width=""20""/> hhsecond:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">int array[10]; <NewLine>...<NewLine>torch::Tensor tensor_in = torch::from_blob(array, {1, 10});<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>From the docs, from_blob takes a <code>void*</code> and the (optional) TensorOptions specify the type, probably defaulting to float. So maybe declaring <code>array</code> to be a float array works better.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""36681""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tom/40/3162_2.png"" width=""20""/> tom:</div><NewLine><blockquote><NewLine><p>TensorOptions</p><NewLine></blockquote><NewLine></aside><NewLine><p>That did not help <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=6"" title="":frowning:""/><br/><NewLine>Does it look like a bug in the JIT module (I know, highly unlikely) or am I doing something wrong?<br/><NewLine>Also for what ever input I pass, I get this exactly same output</p><NewLine><pre><code class=""lang-auto""> 3.7295 -23.8977 -8.2652 -1.3901<NewLine>[ Variable[CPUFloatType]{1,4} ]<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>As Thomas said, you probably have to make <code>array</code> a float, you have it as <code>int array[10]</code>.<br/><NewLine>If you have <code>int array[10]</code> and re-interpret it as a float, it’s probably going to have weird floats come out on the other side.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a ton <a class=""mention"" href=""/u/smth"">@smth</a> <a class=""mention"" href=""/u/tom"">@tom</a>. That worked. In fact, I got the answer two minutes ago from <a class=""mention"" href=""/u/lantiga"">@lantiga</a> and was about to post here.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>ahah, yes, I was posting here when I saw <a class=""mention"" href=""/u/smth"">@smth</a> reply live <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/><br/><NewLine>I confirm that using <code>float array[10]</code> fixes it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hhsecond; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/hhsecond; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/hhsecond; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/lantiga; <NewLine> ,"REPLY_DATE 1: February 8, 2019, 11:56am; <NewLine> REPLY_DATE 2: February 9, 2019, 12:49pm; <NewLine> REPLY_DATE 3: February 9, 2019,  1:12pm; <NewLine> REPLY_DATE 4: February 9, 2019,  2:28pm; <NewLine> REPLY_DATE 5: February 9, 2019,  2:21pm; <NewLine> REPLY_DATE 6: February 9, 2019,  2:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> 
36624,ModuleList of Sequentials doesn&rsquo;t work,2019-02-07T16:45:02.072Z,2,800,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m trying to export a ModuleList of Sequentials. It’s failing with the following error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>could not export python function call &lt;python_value&gt;. Remove calls to python functions before export.:<NewLine>@torch.jit.script_method<NewLine>def forward(self, x):<NewLine>    # type: (List[Tensor]) -&gt; List[Tensor]<NewLine>    outputs = []<NewLine>    for cur_xc in self.xc:<NewLine>        out = cur_xc(torch.tensor(0))<NewLine>              ~~~~~~ &lt;--- HERE<NewLine>        outputs.append(out)<NewLine>    return outputs<NewLine><NewLine></code></pre><NewLine><p>Example:</p><NewLine><pre><code class=""lang-auto"">from fractions import gcd<NewLine><NewLine>import torch<NewLine>from torch import nn<NewLine><NewLine><NewLine>class CrossScale(torch.jit.ScriptModule):<NewLine>    __constants__ = ['xc']<NewLine><NewLine>    def __init__(self, n, ng=32):<NewLine>        super(CrossScale, self).__init__()<NewLine><NewLine>        xc = []<NewLine>        for i in range(len(n)):<NewLine>            m = nn.Sequential(<NewLine>                nn.Conv2d(n[i], n[i], 1, bias=False),<NewLine>                nn.GroupNorm(gcd(ng, n[i]), n[i]))<NewLine>            xc.append(m)<NewLine>        self.xc = nn.ModuleList(xc)<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def forward(self, x):<NewLine>        # type: (List[Tensor]) -&gt; List[Tensor]<NewLine>        outputs = []<NewLine>        for cur_xc in self.xc:<NewLine>            out = cur_xc(torch.tensor(0))<NewLine>            outputs.append(out)<NewLine>        return outputs<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    n = [32, 64, 96]<NewLine>    cs = CrossScale(n)<NewLine><NewLine>    cs.save(""cs.pt"")<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/lsm,(LSM),lsm,"February 7, 2019,  4:45pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, try including the ModuleList in the <code>__constants__</code> list. We have an issue up to improve the error message in this case: <a href=""https://github.com/pytorch/pytorch/issues/16400"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/16400</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, you can see in the snippet I included that the module list is already in the constants list.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>lol you’re right, I wasn’t reading carefully enough! Thanks for filing the GH issue, will follow up there</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/lsm; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: February 7, 2019,  5:25pm; <NewLine> REPLY_DATE 2: February 7, 2019,  5:37pm; <NewLine> REPLY_DATE 3: February 7, 2019,  5:46pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
36616,Why torch.jit is so slow?,2019-02-07T14:53:20.474Z,2,1202,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello! Today I compared numba.jit and torch.jit and was very surprised. What am I doing wrong?</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/3aa92186da57d6566e8d5ce9d9b1c3989098a5d7"" href=""https://discuss.pytorch.org/uploads/default/original/2X/3/3aa92186da57d6566e8d5ce9d9b1c3989098a5d7.png"" title=""image.png""><img alt=""image"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/3/3aa92186da57d6566e8d5ce9d9b1c3989098a5d7_2_10x10.png"" height=""312"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/3/3aa92186da57d6566e8d5ce9d9b1c3989098a5d7_2_690x312.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/3/3aa92186da57d6566e8d5ce9d9b1c3989098a5d7_2_690x312.png, https://discuss.pytorch.org/uploads/default/original/2X/3/3aa92186da57d6566e8d5ce9d9b1c3989098a5d7.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/3/3aa92186da57d6566e8d5ce9d9b1c3989098a5d7.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">721×327 25.8 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from numba import jit<NewLine><NewLine>@torch.jit.script<NewLine>def torch_jit_sum(x : torch.Tensor):<NewLine>    res = torch.zeros_like(x[0, 0])<NewLine>    for i in range(x.shape[0]):<NewLine>        for j in range(x.shape[1]):<NewLine>            res += x[i, j]<NewLine>    return res<NewLine><NewLine>def blablabla(x):<NewLine>    with torch.no_grad():<NewLine>        return torch_jit_sum(x)<NewLine><NewLine>def loop_sum(x):<NewLine>    res = torch.zeros_like(x[0, 0])<NewLine>    for i in range(x.shape[0]):<NewLine>        for j in range(x.shape[1]):<NewLine>            res += x[i, j]<NewLine>    return res<NewLine><NewLine>@jit<NewLine>def numba_sum(x):<NewLine>    res = 0.0<NewLine>    for i in range(x.shape[0]):<NewLine>        for j in range(x.shape[1]):<NewLine>            res += x[i, j]<NewLine>    return res<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Daulbaev,(Талгат),Daulbaev,"February 7, 2019,  2:53pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/daulbaev"">@Daulbaev</a>! The reason torch.jit is slow in your example, is because it’s not designed for this particular use-case <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/> The kinds of speed ups you will see with torch.jit are situations, e.g., when you have a number of pointwise operations, and torch.jit will be able to fuse them together and eliminate overhead and memory traffic. torch.jit, at this point in time, is not designed to take pointwise loops as you’ve written here, and compile them into machine code directly.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for a quick response. Do you have an example of a function where torch.jit on CPU is faster than numba.jit?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Given that numba jit compiles single cuda kernels, it’s going to be at leas as fast in execution.<br/><NewLine>However, for many things, the expressive power of PyTorch is much greater and the JIT will take those ops and optimize them.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ezyang; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Daulbaev; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: February 7, 2019,  3:00pm; <NewLine> REPLY_DATE 2: February 7, 2019,  3:04pm; <NewLine> REPLY_DATE 3: February 7, 2019,  4:19pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
36445,[Updated] problem enumerating nn.ModuleList with torch jit,2019-02-05T16:12:52.932Z,1,872,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I successfully traced the model, but looks like I cannot save it.<br/><NewLine>I thought that if I could do something like  model = trace(model,  (params) ) then it is ready so saving? Am I wrong?<br/><NewLine>Torch version is ‘1.0.0.dev20190130’</p><NewLine><p>Here is the trace:</p><NewLine><pre><code>---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-23-4014fec4f00b&gt; in &lt;module&gt;<NewLine>----&gt; 1 ctc_model.save(""ctc_test.ph"")<NewLine><NewLine>RuntimeError: <NewLine>could not export python function call &lt;python_value&gt;. Remove calls to python functions before     export.:<NewLine>@torch.jit.script_method<NewLine>def forward(self, x, x_length):<NewLine>    h_t, x_length = self.rnn(x, x_length)<NewLine>                    ~~~~~~~~ &lt;--- HERE</code></pre><NewLine></div>",https://discuss.pytorch.org/u/octopusyo,,octopusyo,"February 6, 2019, 10:13am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You cannot export a model if in contains a Python function call. Is <code>self.rnn()</code> a Python function?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think if self.rnn() is subclassed from torch.jit.ScriptModule(), then it should be possible to trace &amp; save it.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you provide a script/model we can use the reproduce the problem? It’s hard to say what’s going on here without more information. Thanks!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much for your replies!<br/><NewLine>Actually I think I fixed my original question: self.rnn was a nn.Module and now I also made in ScriptModule, but now I have a new problem. Looks like I cannot loop over nn.ModuleList. I tried to index it in the loop but did not work as well. Is it even possible to use jit for nn.ModuleList?</p><NewLine><p>I omitted some parts for brevity:</p><NewLine><pre><code>class PyramidalRNNENcoder(ScriptModule):<NewLine><NewLine>__constants__ = ['num_layers']<NewLine><NewLine>def __init__(self, num_mels, encoder_size, num_layers, downsampling=None, dropout=0.0):<NewLine>    super(PyramidalRNNENcoder, self).__init__()<NewLine>    ...<NewLine>    self.rnns =nn.ModuleList()<NewLine>    for i in range(num_layers):<NewLine>        input_size = num_mels*2 if i == 0 else encoder_size*2<NewLine>        lstm_i = nn.LSTM(input_size,<NewLine>                                 hidden_size=encoder_size, bidirectional=True)<NewLine>        initialize_lstm(lstm_i)           <NewLine>        self.rnns.append(lstm_i)<NewLine>    self.num_layers = num_layers<NewLine>    ...<NewLine>    <NewLine>@torch.jit.script_method<NewLine>def forward(self, x, x_length):<NewLine>    batch_size = x.size(0)<NewLine>    ...<NewLine>    idx = 0<NewLine>    <NewLine>    for rnn in self.rnns:<NewLine>      ~~~~~~~~~~~~~~~~~~  RuntimeError:  python value of type 'ModuleList' cannot be used as a tuple:<NewLine>      rnn_result = rnn(data)</code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Try putting <code>""rnns""</code> in the <code>__constants__</code> attribute. We should work on having a better error msg here</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Ananth; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/octopusyo; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: February 5, 2019,  6:01pm; <NewLine> REPLY_DATE 2: February 5, 2019,  7:45pm; <NewLine> REPLY_DATE 3: February 5, 2019,  7:46pm; <NewLine> REPLY_DATE 4: February 6, 2019, 10:11am; <NewLine> REPLY_DATE 5: February 6, 2019,  8:01pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 2 Likes; <NewLine> 
34859,[jit] Shouldn&rsquo;t I use torch.randn_like when using torch.jit.tracing?,2019-01-17T02:46:31.309Z,1,763,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I implement VAE in pytorch, I need to use torch.randn_like() like this:</p><NewLine><pre><code class=""lang-auto"">""""""<NewLine>:param mean: torch.Tensor<NewLine>""""""<NewLine>eps = torch.randn_like(mean)<NewLine></code></pre><NewLine><p>However, when I use torch.jit.tracing to convert my VAE model into the graph representation, I face this warning message:</p><NewLine><pre><code class=""lang-auto"">/*/lib/python3.6/site-packages/torch/jit/__init__.py:644: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:<NewLine>Not within tolerance rtol=1e-05 atol=1e-05 at input[8, 15] (-0.3760705292224884 vs. 5.01010274887085) and 199 other locations (100.00%)<NewLine>  _check_trace([example_inputs], func, executor_options, module, check_tolerance, _force_outplace)<NewLine>/*/lib/python3.6/site-packages/torch/jit/__init__.py:644: TracerWarning: Trace had nondeterministic nodes. Nodes:<NewLine>	%eps : Float(10, 13) = aten::randn_like(%mean, %20, %21, %22), scope: Decoder<NewLine>This may cause errors in trace checking. To disable trace checking, pass check_trace=False to torch.jit.trace()<NewLine>  _check_trace([example_inputs], func, executor_options, module, check_tolerance, _force_outplace)<NewLine>/*/lib/python3.6/site-packages/torch/jit/__init__.py:644: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:<NewLine>Not within tolerance rtol=1e-05 atol=1e-05 at input[3, 8] (-1.5253040790557861 vs. 3.154987096786499) and 129 other locations (100.00%)<NewLine>  _check_trace([example_inputs], func, executor_options, module, check_tolerance, _force_outplace)<NewLine></code></pre><NewLine><p>What should I do?<br/><NewLine>What is trace checking?<br/><NewLine>What will passing check_trace=False to torch.jit.tracing() cause?</p><NewLine></div>",https://discuss.pytorch.org/u/Kazutoshi_Shinoda,(Kazutoshi Shinoda),Kazutoshi_Shinoda,"January 17, 2019,  2:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Trace checking compares the results of the traced function to the actual function to make sure they are the same. It’s just a sanity check, but non-deterministic ops like <code>randn_like()</code> will cause trace checking to fail. This failure is expected, and if you really want to use non-determinism in your model then you can disable the trace checking as the warning says.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/michael_suo"">@Michael_Suo</a><br/><NewLine>Thank you for your reply!<br/><NewLine>I understand what happened.<br/><NewLine>I’ll try it again with check_trace=False.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Kazutoshi_Shinoda; <NewLine> ,"REPLY_DATE 1: February 5, 2019,  8:01pm; <NewLine> REPLY_DATE 2: February 6, 2019,  6:16am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
36234,Am I using torch.jit correctly?,2019-02-02T15:53:12.523Z,1,476,"<div class=""post"" itemprop=""articleBody""><NewLine><p>hi. I am using PyTorch1.0 for the first time since PyTorch 0.2.X.<br/><NewLine>I have a simple question about <code>torch.jit</code>.</p><NewLine><p>Please check [this code] (<a href=""https://colab.research.google.com/gist/hellocybernetics/013f7d6fb007df1d8c70161872acce72/pytorch_jit_test.ipynb?authuser=1"" rel=""nofollow noopener"">https://colab.research.google.com/gist/hellocybernetics/013f7d6fb007df1d8c70161872acce72/pytorch_jit_test.ipynb?authuser=1</a>) at gist.</p><NewLine><p>JIT did NOT improve the speed. Am I using <code>torch.jit</code> correctly?</p><NewLine></div>",https://discuss.pytorch.org/u/hellocybernetics,(Hellocyber),hellocybernetics,"February 2, 2019,  3:53pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Your code looks reasonable to me. Keep in mind that using the JIT may not necessarily yield big performance increases. Right now, the primary use case for the JIT is running PyTorch models in production without a dependency on Python.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you to reply.</p><NewLine><p>Before I came PyTorch1.0, I was using TensorFlow 1.X. Lately, TF have “eager execution mode” which is define by run. In TF2.0, eager execution mode is default, so I think the code will become PYTHONIC like PyTorch therefore the difference in usage feeling will disappear.</p><NewLine><p>Then TF have a great JIT function which translator eager code into a function which works as TF graph internally named <code>tf.function</code> at TF2.0.This JIT function makes eager code which much slower than PyTorch into faster than PyTorch. So, I tried to make pytorch faster with <code>torch.jit</code>. (I think that Pyro uses <code>torch.jit</code> for speed at Variational Inference API.)</p><NewLine><p>If <code>torch.jit</code> is not for speed, don’t we need <code>torch.jit</code> at prototyping in research? If so, when considering production, what is difference between using caffe2 and <code>torch.jit</code>?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hellocybernetics; <NewLine> ,"REPLY_DATE 1: February 4, 2019,  5:35pm; <NewLine> REPLY_DATE 2: February 6, 2019,  3:19am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
35333,JIT runtime error with nested lists,2019-01-22T21:08:59.083Z,0,558,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m getting an error when running a dummy jit script. It seems it cannot infer types on a list of lists.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine><NewLine>class PreProcessor(torch.jit.ScriptModule):<NewLine><NewLine>    def __init__(self):<NewLine>        super(PreProcessor, self).__init__()<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def forward(self, frames):<NewLine>        # type: (List[Tensor]) -&gt; List[Tensor]<NewLine>        lidars = []<NewLine>        for i in range(len(frames)):<NewLine>            frame = frames[i]<NewLine>            lidars.append(frame)<NewLine>        return lidars<NewLine><NewLine><NewLine>class Inference(torch.jit.ScriptModule):<NewLine><NewLine>    def __init__(self):<NewLine>        super(Inference, self).__init__()<NewLine>        self.preprocessor = PreProcessor()<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def forward(self, batched_frames):<NewLine>        # type: (List[List[Tensor]]) -&gt; List[List[Tensor]]<NewLine>        data = []<NewLine>        for i in range(len(batched_frames)):<NewLine>            frames = batched_frames[i]<NewLine>            preprocessed_data = self.preprocessor(frames)<NewLine>            data.append(preprocessed_data)<NewLine>        return data<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    p = Inference()<NewLine><NewLine>    print(p)<NewLine>    p.save(""p.pt"")<NewLine></code></pre><NewLine><p>The relevant stack trace output:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: <NewLine>arguments for call are not valid:<NewLine>  for operator aten::append(t[] self, t el) -&gt; t[]:<NewLine>  could not match type Tensor[] to t in argument 'el': type variable 't' previously matched to type Tensor is matched to type Tensor[]<NewLine>  @torch.jit.script_method<NewLine>  def forward(self, batched_frames):<NewLine>      # type: (List[List[Tensor]]) -&gt; List[List[Tensor]]<NewLine>      data = []<NewLine>      for i in range(len(batched_frames)):<NewLine>          frames = batched_frames[i]<NewLine>          preprocessed_data = self.preprocessor(frames)<NewLine>          data.append(preprocessed_data)<NewLine>                      ~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>      return data<NewLine></code></pre><NewLine><p>Is this a known issue?</p><NewLine></div>",https://discuss.pytorch.org/u/lsm,(LSM),lsm,"January 22, 2019, 11:08pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! Lists are statically typed in TorchScript. The default type for a list declared like <code>foo = []</code> is <code>List[Tensor]</code></p><NewLine><p>So in <code>data.append(preprocessed_data)</code> you are trying to append a list of tensors to a list of tensors.</p><NewLine><p>You can fix this by annotating <code>data</code> properly (search “Variable Type Annotation” <a href=""https://pytorch.org/docs/master/jit.html"" rel=""nofollow noopener"">here</a>).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: February 5, 2019,  7:53pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
36269,Using jit to deploy Encoder Decoder model,2019-02-03T04:51:06.834Z,0,392,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I currently have a trained model for a RNN Encoder Decoder model and I’m trying to follow the tutorial for deploying code using jit and C++ here <a href=""https://pytorch.org/tutorials/advanced/cpp_export.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/cpp_export.html</a> . For the example inputs, I’m supposed to pass in what one normally puts for the forward pass from the model; however, I am getting the following error:</p><NewLine><pre><code class=""lang-auto"">SyntaxError: invalid syntax<NewLine>&gt;&gt;&gt; traced_script_module = torch.jit.trace(model, (example1, example2))<NewLine>Traceback (most recent call last):<NewLine>  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;<NewLine>  File ""/usr/local/lib/python3.7/site-packages/torch/jit/__init__.py"", line 634, in trace<NewLine>    module = TopLevelTracedModule(func, **executor_options)<NewLine>  File ""/usr/local/lib/python3.7/site-packages/torch/jit/__init__.py"", line 963, in init_then_register<NewLine>    original_init(self, *args, **kwargs)<NewLine>  File ""/usr/local/lib/python3.7/site-packages/torch/jit/__init__.py"", line 963, in init_then_register<NewLine>    original_init(self, *args, **kwargs)<NewLine>  File ""/usr/local/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1316, in __init__<NewLine>    self._name = orig.__name__<NewLine>AttributeError: 'dict' object has no attribute '__name__'<NewLine></code></pre><NewLine><p>Here is my code from the forward pass of the model</p><NewLine><pre><code class=""lang-auto"">def forward(self, input, hidden, return_hiddens=False, noise=False):<NewLine>        # input = (torch.randn(1,2))<NewLine>        # hidden = (torch.randn(2,1,32), torch.randn(2,1,32))<NewLine>        print('Input tuple length', len(input))<NewLine>        print('Hidden tuple length', len(hidden))<NewLine>        #print(input[0])<NewLine>        #print(hidden[0].size())<NewLine>        #print(hidden)<NewLine>        #print(hidden[1].size())<NewLine><NewLine>        emb = self.drop(self.encoder(input.contiguous().view(-1, self.enc_input_size)))<NewLine>        emb = emb.view(-1, input.size(1), self.rnn_hid_size)  # [ seq_len * batch_size * feature_size]<NewLine>        if noise:<NewLine>            hidden = (F.dropout(hidden[0], training=True, p=0.9), F.dropout(hidden[1], training=True, p=0.9))<NewLine><NewLine>        output, hidden = self.rnn(emb, hidden)<NewLine>        output = self.drop(output)<NewLine>        # [(seq_len * batch_size) * feature_size]<NewLine>        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))<NewLine>        decoded = decoded.view(output.size(0), output.size(1), decoded.size(1)) # [seq_len * batch_size * feature_size]<NewLine>        if self.res_connection:<NewLine>            decoded = decoded + input<NewLine>        if return_hiddens:<NewLine>            return decoded, hidden, output<NewLine><NewLine>        return decoded, hidden<NewLine></code></pre><NewLine><p>Here is the code I used when trying to use <code>torch.jit.trace</code></p><NewLine><pre><code class=""lang-auto"">model = torch.load('./gcloud-gpu-results/save/ecg/model_best/chfdb_chf13_45590.pth', map_location='cpu')<NewLine>example1 = (torch.randn(1,2))<NewLine>example2 = (torch.randn(2,1,32), torch.randn(2,1,32))<NewLine>traced_script_module = torch.jit.trace(model, example1, example2))<NewLine></code></pre><NewLine><p>Let me know, if I should provide any more information or be more clear. Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/sonamghosh,(Sam),sonamghosh,"February 3, 2019,  4:51am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you check what the type object you are <code>torch.load()</code>ing is? It seems that the tracing path thinks it’s a <code>dict</code> for some reason.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Michael_Suo; <NewLine> ,"REPLY_DATE 1: February 4, 2019,  5:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
35316,What is USE_TENSORRT flag used for?,2019-01-22T16:09:30.926Z,0,189,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve built Pytorch from source for Jetson Xavier successfully, and I am now interested in accelerating inference. What does the USE_TENSORRT flag accomplish? Seems like without this flag I could convert my model to ONNX, build the engine and call the NVinfer API directly?</p><NewLine><p>Thanks,<br/><NewLine>Rich</p><NewLine></div>",https://discuss.pytorch.org/u/Rich_Tarquini,(Rich Tarquini),Rich_Tarquini,"January 22, 2019,  4:09pm",,,,,
34817,TorchScript + Pythran,2019-01-16T14:57:11.945Z,0,218,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there,</p><NewLine><p>It seems to me that TorchScript is a subset of <a href=""https://github.com/serge-sans-paille/pythran"" rel=""nofollow noopener"">Pythran</a>, or at least that we could enlarge Pythran to make it a superset of PyTorch. If that’s the case, it would be interesting to have both compiler share the same AST optimization engine.</p><NewLine><p>Pythran optimizations have been designed to be relatively independent from  the target, there’s probably room for cooperation there.</p><NewLine><p>Does that look like a good idea?</p><NewLine></div>",https://discuss.pytorch.org/u/serge_sans_paille,,serge_sans_paille,"January 16, 2019,  3:11pm",,,,,
