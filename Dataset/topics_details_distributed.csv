id,title,created_at,reply_count,views,description,creator_link,creator_name,creator_alias,post_date,post_likes,replies,repliers_links,reply_dates,reply_likes
33465,About the distributed category,2018-12-31T07:01:04.087Z,0,448,"<div class=""post"" itemprop=""articleBody""><NewLine><p>(Replace this first paragraph with a brief description of your new category. This guidance will appear in the category selection area, so try to keep it below 200 characters. <strong>Until you edit this description or create topics, this category won’t appear on the categories page.</strong>)</p><NewLine><p>Use the following paragraphs for a longer description, or to establish category guidelines or rules:</p><NewLine><ul><NewLine><li><NewLine><p>Why should people use this category? What is it for?</p><NewLine></li><NewLine><li><NewLine><p>How exactly is this different than the other categories we already have?</p><NewLine></li><NewLine><li><NewLine><p>What should topics in this category generally contain?</p><NewLine></li><NewLine><li><NewLine><p>Do we need this category? Can we merge with another category, or subcategory?</p><NewLine></li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/smth,,smth,"December 31, 2018,  7:01am",,,,,
92589,Using IterableDataset with DistributedDataParallel,2020-08-12T16:37:12.689Z,3,140,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m building an NLP application that with a dataloader that builds batches out of sequential blocks of text in a file.  I have been using an IterableDataset since my text file won’t fit into memory.  However, when I use with with DistributedDataParallel, the dataloader is replicated across processes and each GPU ends up with the same batch of data.  How can I give each GPU a different batch of data to take advantage of distributed training?</p><NewLine><p>Note:  My dataloader can load ~300 batches/second on a single and each GPU takes ~2 seconds to process a batch, so dataloader speed should not be a significant limiting factor even if batches were sent in serial to different GPUs</p><NewLine></div>",https://discuss.pytorch.org/u/kartch,,kartch,"August 12, 2020,  4:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>How did you verify that all the gpus are using the same batch. Thats not how DDP works, it will take different chunks automatically.</p><NewLine><p><a class=""onebox"" href=""https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html</a></p><NewLine><p>This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine and each device, and <strong>each such replica handles a portion of the input</strong>. During the backwards pass, gradients from each node are averaged.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I met a similar problem recently, and I think the batches should be the same across different GPUs according to the source code.<br/><NewLine>If you look at the function DistributedSampler which we use in DDP,  the chunking function is done by this class. However, if you look at the source code of Dataloader, sampler will not affect the behavior of data fetching of iterable datasets.<br/><NewLine>see line 34 in <a href=""https://github.com/pytorch/pytorch/blob/8fd9fe93be08c9f0ab81507081fac1387a4aca56/torch/utils/data/_utils/fetch.py#L18"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/8fd9fe93be08c9f0ab81507081fac1387a4aca56/torch/utils/data/_utils/fetch.py#L18</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/bsridatta"">@bsridatta</a> I verified that my data was being replicated across batches by actually printing them out.  I’m not sure, but this problem may be a product of using pytorch-lightning, which makes a copy of the dataloader for each GPU.</p><NewLine><p>In any case, I was able to fix the problem by creating an array of pointers to the start of each training example in my file using an approach similar to the one used <a href=""https://github.com/jegesh/python-random-access-file-reader"" rel=""nofollow noopener"">here</a>.  This allowed me to quickly sample random training contexts from my text file without needing to read it into memory.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/kartch"">@kartch</a>, thanks a lot for the explaining the workaround. You maybe right about pytorch-lightning, had few crazy issues, some of the backdraws of abstraction I guess.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/vitalyfedyunin"">@VitalyFedyunin</a> <a class=""mention"" href=""/u/simonw"">@SimonW</a> I’m wondering if we officially support using DistributedSampler with IterableDataset?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>In your dataset class, you can take in a shard ID to shard the dataset properly. Then using the distributed training rank as the shard ID should work.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/bsridatta; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/rbslibingnan; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/kartch; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/bsridatta; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/SimonW; <NewLine> ,"REPLY_DATE 1: August 12, 2020,  4:43pm; <NewLine> REPLY_DATE 2: September 2, 2020,  3:11am; <NewLine> REPLY_DATE 3: September 15, 2020,  6:51pm; <NewLine> REPLY_DATE 4: September 20, 2020, 12:50pm; <NewLine> REPLY_DATE 5: September 23, 2020,  2:55am; <NewLine> REPLY_DATE 6: September 27, 2020, 12:48am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
47152,Training performance degrades with DistributedDataParallel,2019-06-05T17:11:55.780Z,13,4241,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m training a conv model using DataParallel (DP) and DistributedDataParallel (DDP) modes. For DDP, I only use it on a single node and each process is one GPU.<br/><NewLine>My model has many BatchNorm2d layers. Given all other things the same, I observe that DP trains better than DDP (in classification accuracy). Even if I add <code>SyncBN</code> from pytorch 1.1, I still observe that DP &gt; DDP+SyncBN &gt; DDP without SyncBN in test accuracy.</p><NewLine><p>I’m aware of the difference between DP and DDP’s handling of averaging/sum: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/is-average-the-correct-way-for-the-gradient-in-distributeddataparallel-with-multi-nodes/34260/16"">Is average the correct way for the gradient in DistributedDataParallel with multi nodes?</a><br/><NewLine>The LR and total batch size are the same for both DP, DDP+SyncBN, and DDP.</p><NewLine><p>If I understand correctly, DP doesn’t do SyncBN, so DP should in theory achieve the same test accuracy as DDP (given small batch size per GPU)? If we assume larger <em>effective</em> batch size leads to better result, I should expect the following test performance ranking:</p><NewLine><p>DDP+SyncBN &gt; DP == DDP</p><NewLine><p>but in practice, I observe: DP &gt; DDP+SyncBN &gt; DDP</p><NewLine><p>Because DDP+SyncBN is 30% faster than DP, I really hope to solve the training gap so that I can take advantage of DDP’s superior speed. Thanks for any help!</p><NewLine></div>",https://discuss.pytorch.org/u/JimFan,(Jim Fan),JimFan,"June 5, 2019,  5:53pm",4 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Jim,<br/><NewLine>From <a href=""https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed.html"" rel=""nofollow noopener"">docs</a><br/><NewLine><code>DistributedDataParallel</code> can be used in the following two ways:<br/><NewLine>(1) Single-Process Multi-GPU<br/><NewLine>(2) Multi-Process Single-GPU<br/><NewLine>Second method the highly recommended way to use <code>DistributedDataParallel</code>, with<br/><NewLine>multiple processes, each of which operates on a single GPU. This is<br/><NewLine>currently the fastest approach to do data parallel training using PyTorch<br/><NewLine>and applies to both single-node(multi-GPU) and multi-node data<br/><NewLine>parallel training.</p><NewLine><p>Which one are you using ?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m using (2). More specifically, I have one node with 8 GPUs. I launch DDP with 8 separate processes, each one owns a single GPU.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I do not have a good guess then <img alt="":confused:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=9"" title="":confused:""/></p><NewLine><p>Do you have any CPU heavy pre-processing, do you report only GPU performance ?<br/><NewLine>Which back-end are you using, NCCL ?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>By “performance”, I mean the classification accuracy. Somehow DDP+SyncBN achieves worse test accuracy than DP, so there must be some problematic differences in the numeric algorithm. The speed isn’t the issue here. Thanks!</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>My mistake, I got it wrong. Thanks for clarification.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I can only comment on the differences between DP and DDP w.r.t. batch normalization. With DP your module is replicated before each call to <code>forward</code>, which means that only the BN stats from the first replica are kept around. With DDP each process keeps their own version of BN. And with SyncBN you’ll end up with stats that are “more averaged” than the stats kept when using DP, because they only include stats for a batch in a single replica, instead of all replicas.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I also encountered this performance issue with DistributedDataParallel, hope someone could give a solution. <img alt="":slightly_frowning_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slightly_frowning_face.png?v=9"" title="":slightly_frowning_face:""/></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found the problem in my code, it’s because of the cudnn batch norm. According to <a href=""https://github.com/Microsoft/human-pose-estimation.pytorch/issues/8"" rel=""nofollow noopener"">this github issue</a>, the solution is to edit the batchnorm part in torch/nn/functional.py or set torch.backends.cudnn.enabled = False.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could edit the batchnorm part in torch/nn/functional.py work for sync bn?<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/91d28026f8e8386d45dfb7ddab84c906479dead1/torch/nn/modules/batchnorm.py#L450"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/91d28026f8e8386d45dfb7ddab84c906479dead1/torch/nn/modules/batchnorm.py#L450"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/91d28026f8e8386d45dfb7ddab84c906479dead1/torch/nn/modules/batchnorm.py#L450</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""440"" style=""counter-reset: li-counter 439 ;""><NewLine><li>        else:  # use exponential moving average</li><NewLine><li>            exponential_average_factor = self.momentum</li><NewLine><li><NewLine></li><NewLine><li>    world_size = 1</li><NewLine><li>    process_group = torch.distributed.group.WORLD</li><NewLine><li>    if self.process_group:</li><NewLine><li>        process_group = self.process_group</li><NewLine><li>    world_size = torch.distributed.get_world_size(process_group)</li><NewLine><li><NewLine></li><NewLine><li>    # fallback to framework BN when synchronization is not necessary</li><NewLine><li class=""selected"">    if world_size == 1 or (not self.training and self.track_running_stats):</li><NewLine><li>        return F.batch_norm(</li><NewLine><li>            input, self.running_mean, self.running_var, self.weight, self.bias,</li><NewLine><li>            self.training or not self.track_running_stats,</li><NewLine><li>            exponential_average_factor, self.eps)</li><NewLine><li>    else:</li><NewLine><li>        return sync_batch_norm.apply(</li><NewLine><li>            input, self.weight, self.bias, self.running_mean, self.running_var,</li><NewLine><li>            self.eps, exponential_average_factor, process_group, world_size)</li><NewLine><li><NewLine></li><NewLine><li>@classmethod</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>The batch norm in torch.nn.functional is used just for evaluation. I think editing this would do nothing to sync batch norm. How do you edit the file to make sync bn work normally?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>You are right, although the performance improves after disable cudnn, the gap still remains. I can’t figure out the problem and now I have to use nn.DataParallel <img alt="":slightly_frowning_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slightly_frowning_face.png?v=9"" title="":slightly_frowning_face:""/> .</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mr.z"">@Mr.Z</a> Do you find the  problem? I also get a very worse accuracy when use SyncBN + DDP for batchsize=16( 4 GPUs on one node, 4 images for each GPU), and when I use DataParallel + SyncBN, evrything is OK.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Same here. Performance of DDP model is weaker than one trained on a single GPU. Playing with lr/bs does not help. As number of GPUs in DPP training grows - performance degrades.</p><NewLine><p>Has anyone found the solution ?</p><NewLine><p>UPDT: the reason was found for my case. When training DDP model we need to use <code>DistributedSampler</code> which is passed to <code>Dataloader</code>. We need to <code>train_dataloader.sampler.set_epoch(epoch)</code> on every epoch start.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>having the same issue (DP much better validation metrics than DDP). setting</p><NewLine><p><code>torch.backends.cudnn.enabled = False</code></p><NewLine><p>slows my runtime down by 3x.</p><NewLine><p>monkey patching <code>torch.nn.functional.batch_norm</code></p><NewLine><pre><code class=""lang-python"">def monkey_patch_bn():<NewLine>    # print(inspect.getsource(torch.nn.functional.batch_norm))<NewLine>    def batch_norm(input, running_mean, running_var, weight=None, bias=None,<NewLine>                   training=False, momentum=0.1, eps=1e-5):<NewLine>        if training:<NewLine>            size = input.size()<NewLine>            size_prods = size[0]<NewLine>            for i in range(len(size) - 2):<NewLine>                size_prods *= size[i + 2]<NewLine>            if size_prods == 1:<NewLine>                raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))<NewLine><NewLine>        return torch.batch_norm(<NewLine>            input, weight, bias, running_mean, running_var,<NewLine>            training, momentum, eps, False<NewLine>        )<NewLine>    torch.nn.functional.batch_norm = batch_norm<NewLine></code></pre><NewLine><p>doesn’t seem to help.</p><NewLine><p><code>train_dataloader.sampler.set_epoch(epoch)</code> doesn’t seem to help either.</p><NewLine><p>EDIT:</p><NewLine><p>what does seem to work is dividing my <code>lr</code> by my <code>world_size</code>, although i’m not sure why.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/yueshanggu"">@YueshangGu</a> Hi, How do you use  DataParallel + SyncBN at the same time? I though SyncBN only works with DistributedDataParallel.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Another issue spotted in my case: a model has to be transferred to proper device before wrapping into DDP.</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Our experience with Kinetics 400 using PyTorch 1.3 on a node with two GPUs as follows:</p><NewLine><blockquote><NewLine><p>Single GPU &gt; DP (-0.2%) &gt; DP w/ sync BN (-0.3%)</p><NewLine></blockquote><NewLine><p>Single GPU serves as the baseline for DP and DP w/ sync BN.<br/><NewLine>The tradeoff with distributed training is understandable but sync BN causing worse accuracy is not trivial to ignore.</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>My setting is same with you, just testing in HMDB51. I also get the results as follows:<br/><NewLine>DP&gt;DP w/sync BN. Now, Do you find the solution to deal with this issue?</p><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi guys, where can i find the code of SyncBN?</p><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>Maybe the learning rate is the problem?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/enisberk; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/JimFan; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/enisberk; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/JimFan; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/enisberk; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Mr.Z; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Mr.Z; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/JingLi; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Mr.Z; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/YueshangGu; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/Sergii_Makarevych; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/makslevental; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/Beinan_Wang; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/Sergii_Makarevych; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/farleylai; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/jinyuanfeng; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/zhi_lim; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/ginobilinie; <NewLine> ,"REPLY_DATE 1: June 5, 2019,  5:38pm; <NewLine> REPLY_DATE 2: June 5, 2019,  5:39pm; <NewLine> REPLY_DATE 3: June 5, 2019,  5:46pm; <NewLine> REPLY_DATE 4: June 5, 2019,  5:53pm; <NewLine> REPLY_DATE 5: June 5, 2019,  5:54pm; <NewLine> REPLY_DATE 6: June 24, 2019,  6:08am; <NewLine> REPLY_DATE 7: July 1, 2019,  6:19am; <NewLine> REPLY_DATE 8: July 15, 2019,  1:42am; <NewLine> REPLY_DATE 9: July 29, 2019,  7:52am; <NewLine> REPLY_DATE 10: August 1, 2019, 12:14pm; <NewLine> REPLY_DATE 11: August 8, 2019, 12:32pm; <NewLine> REPLY_DATE 12: August 27, 2019,  9:57am; <NewLine> REPLY_DATE 13: September 3, 2019,  5:22pm; <NewLine> REPLY_DATE 14: September 13, 2019,  4:50pm; <NewLine> REPLY_DATE 15: October 22, 2019,  5:45am; <NewLine> REPLY_DATE 16: October 22, 2019,  3:09pm; <NewLine> REPLY_DATE 17: December 5, 2019, 12:42pm; <NewLine> REPLY_DATE 18: December 20, 2019,  7:06am; <NewLine> REPLY_DATE 19: December 29, 2019,  5:49am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 3 Likes; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: 3 Likes; <NewLine> REPLY 8 LIKES: 2 Likes; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: 3 Likes; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> REPLY 18 LIKES: ; <NewLine> REPLY 19 LIKES: ; <NewLine> 
97040,How can I use torch.nn.DataParallel while I&rsquo;m doing transfer learning?,2020-09-21T18:38:24.234Z,2,68,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to do my training on multiple GPUs by using the following code (the latest pytorch version):</p><NewLine><pre><code class=""lang-auto"">from torchvision import models<NewLine>model = model.vgg16(pretrained=True)<NewLine>model.classifier._modules['6'] = torch.nn.Linear(4096, 10)<NewLine>self.model = torch.nn.DataParallel(model, device_ids=[0,1,2]).cuda()<NewLine>self.model = model.to(f'cuda:0')<NewLine>...<NewLine>def forward(self, input_data):<NewLine>    output = self.model.forward(input_data)<NewLine></code></pre><NewLine><p>I get this error when I call <strong>self.model.forward(input_data)</strong> :</p><NewLine><pre><code class=""lang-auto"">  File ""/home/poahmadvand/py3env/lib/python3.7/site-packages/torchvision/models/vgg.py"", line 43, in forward<NewLine>    x = self.features(x)<NewLine>  File ""/home/poahmadvand/py3env/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/poahmadvand/py3env/lib/python3.7/site-packages/torch/nn/modules/container.py"", line 100, in forward<NewLine>    input = module(input)<NewLine>  File ""/home/poahmadvand/py3env/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/poahmadvand/py3env/lib/python3.7/site-packages/torch/nn/modules/conv.py"", line 345, in forward<NewLine>    return self.conv2d_forward(input, self.weight)<NewLine>  File ""/home/poahmadvand/py3env/lib/python3.7/site-packages/torch/nn/modules/conv.py"", line 342, in conv2d_forward<NewLine>    self.padding, self.dilation, self.groups)<NewLine>RuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 0 does not equal 1 (while checking arguments for cudnn_convolution)<NewLine></code></pre><NewLine><p>How can I fix this error? thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/pouya.ahmadvand,(Pouya Ahmadvand),pouya.ahmadvand,"September 21, 2020,  6:38pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I can’t seem to reproduce the issue that you’re seeing. This code works fine on PyTorch 1.6:</p><NewLine><pre><code class=""lang-auto"">from torchvision import models<NewLine>import torch<NewLine>model = models.vgg16(pretrained=True)<NewLine>model.classifier._modules['6'] = torch.nn.Linear(4096, 10)<NewLine>model = torch.nn.DataParallel(model, device_ids=[0,1,2]).cuda()<NewLine>model = model.to(f'cuda:0')<NewLine>input_data = torch.rand(10, 3, 225, 225)<NewLine>model(input_data)<NewLine></code></pre><NewLine><p>Am I missing something here?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, Do you have multiple GPUs on your machine?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, I’m trying this on an 8 GPU machine.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pouya.ahmadvand"">@pouya.ahmadvand</a> Do you run into the same error even if you run the code I pasted above?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, I’ll try it tomorrow and let you know.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pritamdamania87"">@pritamdamania87</a> Thanks, it now works. There problem was that I used a function to fetch the number of GPUs with the highest free memory available, and this function returns an array like [5, 6, 7]. Then:</p><NewLine><pre><code class=""lang-auto"">selected_gpus = [5, 6, 7]<NewLine>model = torch.nn.DataParallel(model, device_ids=selected_gpus).cuda()   <NewLine>model = model.to(f'cuda:{selected_gpus[0]}')<NewLine></code></pre><NewLine><p>which gives me that error. now I’m using the return array of gpu selector and set the system variable CUDA_VISIBLE_DEVICES by it and then:</p><NewLine><pre><code class=""lang-auto"">selected_gpus = [5, 6, 7]<NewLine>os.environ[""CUDA_VISIBLE_DEVICES""] = ','.join(str(x) for x in selected_gpus)<NewLine>model = torch.nn.DataParallel(model, device_ids=range(0,len(selected_gpus)).cuda()<NewLine>model = model.to(f'cuda:0')<NewLine></code></pre><NewLine><p>It works fine now.<br/><NewLine>Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pouya.ahmadvand; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/pouya.ahmadvand; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/pouya.ahmadvand; <NewLine> ,"REPLY_DATE 1: September 23, 2020,  3:23am; <NewLine> REPLY_DATE 2: September 23, 2020,  5:33pm; <NewLine> REPLY_DATE 3: September 23, 2020,  9:01pm; <NewLine> REPLY_DATE 4: September 24, 2020,  1:42am; <NewLine> REPLY_DATE 5: September 24, 2020,  3:43am; <NewLine> REPLY_DATE 6: September 25, 2020,  7:07pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
97378,The acc in multi gpus is lower than one gpus,2020-09-24T14:36:48.291Z,0,18,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, my code is in <a href=""https://github.com/sangyx/dgl/tree/master/examples/pytorch/GATNE-T/src"" rel=""nofollow noopener"">https://github.com/sangyx/dgl/tree/master/examples/pytorch/GATNE-T/src</a>.</p><NewLine><p>I run the <a href=""https://github.com/sangyx/dgl/blob/master/examples/pytorch/GATNE-T/src/main_sparse.py"" rel=""nofollow noopener"">main_sparse.py</a> will get acc 0.94. But the acc will not higher than 85% with <a href=""https://github.com/sangyx/dgl/blob/master/examples/pytorch/GATNE-T/src/main_sparse_multi_gpu.py"" rel=""nofollow noopener"">main_sparse_multi_gpu.py</a> even I set the gpu=0.<br/><NewLine>Is there any error in my code?</p><NewLine><p>My environment is Pytorch 1.6 and dgl-cu10.2 0.52. You can get the test data example in <a href=""https://github.com/sangyx/dgl/tree/master/examples/pytorch/GATNE-T"" rel=""nofollow noopener"">https://github.com/sangyx/dgl/tree/master/examples/pytorch/GATNE-T</a></p><NewLine></div>",https://discuss.pytorch.org/u/sangyx,(Yunxin Sang),sangyx,"September 24, 2020,  3:54pm",,,,,
97332,PyTorch RPC multiple threading training,2020-09-24T06:06:38.086Z,0,21,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!<br/><NewLine>I try to build a distributed RL with PyTorch RPC, but I got a problem that I can’t use my GPU power fully. Here is my code structure. They are two parts, learner and actor.</p><NewLine><pre><code class=""lang-auto"">class Learner:<NewLine>    <NewLine>    def __init__(self):<NewLine>        self.policy = Policy()<NewLine>        self.buffer = container()<NewLine>        run_multiple_actor()<NewLine>    <NewLine>    def learning_loop(self):<NewLine>        ""backward &amp; optimize""<NewLine>        while True:<NewLine>            get_trans_from_buffer()<NewLine>            computing_loss()<NewLine>            backward()<NewLine>            optimizer.step()<NewLine>    <NewLine>    def get_action(self, state):<NewLine>        action = self.policy(state)<NewLine>        insert_to_buffer(state,action)<NewLine>        return action<NewLine><NewLine>class Actor:<NewLine>    <NewLine>    def __init__(self):<NewLine>        self.env = env<NewLine>        self.learner_rref = rref<NewLine>    <NewLine>    def act_loop(self):<NewLine>        while True:<NewLine>            state = self.env.step(action)<NewLine>            action = self.learner_rref.run_sync().get_action(state)<NewLine></code></pre><NewLine><p>For learner, after initating, it runs in <code>learning_loop</code>. For actor, after initating, it runs in <code>act_loop</code> and call Learner’s <code>get_action</code> from remote.</p><NewLine><p>The question is that could <code>get_action</code> threads run simultaneously on GPU? If they can, tt seems that as long as I run enough actors, I fully use my GPU power. However, after adding serveal actors, the volatile of GPU stop inscreasing and stays in a low level(eg. 20% or 30%). And I don’t think it’s a problem of my CPU cores. I have enough CPU to make all actors run simultaneously.</p><NewLine><p>Could anyone point out what’s the problem of my code. I am new to PyTorch PRC, help me please.</p><NewLine></div>",https://discuss.pytorch.org/u/LW-Ricarido,(Lw Ricarido),LW-Ricarido,"September 24, 2020,  6:06am",,,,,
97124,Why does each GPU occupy different memory using DDP?,2020-09-22T11:49:27.046Z,2,54,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I found a strange thing. When I use two GPUs, the memory occupied in the two GPUs is the same. But when I use 4 or 6 GPUs, the memory occupied in GPUs is not exactly the same.</p><NewLine><p>Has anyone ever been in this situation?</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/c12788a40dabf26eacdf5cf8e75614f5a598aefa"" href=""https://discuss.pytorch.org/uploads/default/original/3X/c/1/c12788a40dabf26eacdf5cf8e75614f5a598aefa.png"" title=""image""><img alt=""image"" data-base62-sha1=""ryIQQU2mTPgdtOzP3LLk6y6dbaG"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/1/c12788a40dabf26eacdf5cf8e75614f5a598aefa_2_10x10.png"" height=""399"" src=""https://discuss.pytorch.org/uploads/default/original/3X/c/1/c12788a40dabf26eacdf5cf8e75614f5a598aefa.png"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">859×497 47 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/Shanyan_Guan,(Shanyan Guan),Shanyan_Guan,"September 22, 2020, 11:49am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>A device might run a bit ahead of the others and could thus create another memory footprint.<br/><NewLine>Also, if you didn’t set e.g. cudnn to the deterministic mode, the kernel selection might slightly vary between the devices.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>, Thanks for your quick reply! I have set cudnn to the deterministic mode. Maybe this is due to the different speeds between each GPU. Will this case affect the training process?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>DDP synchronizes the devices if necessary and communicates the gradients etc. between the GPUs.<br/><NewLine>If one device is a ms faster, it would have to wait, but besides that you shouldn’t see any effects.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Shanyan_Guan; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 23, 2020,  8:55am; <NewLine> REPLY_DATE 2: September 23, 2020,  4:43pm; <NewLine> REPLY_DATE 3: September 24, 2020, 12:22am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
97194,Do DataParallel and DistributedDataParallel affect the batch size and GPU memory consumption?,2020-09-23T03:23:55.922Z,0,28,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Do DataParallel and DistributedDataParallel affect the batch size and GPU memory consumption?<br/><NewLine>(I use NCCL backend).</p><NewLine><ol><NewLine><li><NewLine><p>If I set batch_size=4 and train with nn.DataParallel or nn.DistributedDataParallel on 8 GPUs, then what will be the batch-size and mini_batch_size: 4, 8, or 32?</p><NewLine></li><NewLine><li><NewLine><p>Can I use batch_size lower than number of GPUs, batch_size=4 for 8xGPUs (will it lead to error, or will be used only 4 GPUs or will be batch_size increased to 8 or 32)?</p><NewLine></li><NewLine><li><NewLine><p>I tried to train EfficientNet-L2 by using each of nn.DataParallel and nn.DistributedDataParallel, but with nn.DataParallel I can use batch_size 2x higher than with nn.DistributedDataParallel without CUDA Out of memory. Does nn.DistributedDataParallel spend 2x time more GPU memory than nn.DataParallel?</p><NewLine></li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/mdc,(MDC),mdc,"September 23, 2020, 12:09pm",,,,,
96724,Sharing model between processes automatically allocates new memory,2020-09-18T11:23:50.703Z,0,42,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve encounter a problem when sharing a model between processes, and it is critical to me (for memory resources).</p><NewLine><p>I’ve been sharing a model between several processes (on linux, ubuntu). The model is used only for a forward pass, since it performs some sort of pre-processing for the samples (before fed to a different network). I’ve done everything I can to ensure that  - the model is on eval mode, each parameter has ‘grad’ flag False, and forward is under ‘with torch.no_grad():’.</p><NewLine><p>The problem is that after the new process is spawned, for some reason it allocates new memory on the GPU. At first I thought this memory is intermediate values of the computational graph, but then I noticed each process still allocates new memory on GPU, even when sleep is invoked (i.e. before even running data through the model),  the memory is still allocated. Further more, it is a lot of memory relative to the model! The model is about 4GB (lets say 2GB of weights and 2GB of optimizer), the memory allocated is 1GB (!), which may also indicate that the network is not completely replicated, only a part of it.</p><NewLine><p>Here is an example code, it think it contains the must critical parts of what I’m doing</p><NewLine><pre><code class=""lang-python"">def inferrerFunc(neuralNetwork):<NewLine>	#If we use sleep here, memory is still allocated on GPU<NewLine>	#time.sleep(1000)<NewLine>	#Imagine theres a dataset here,,,<NewLine>	for x in dataset:<NewLine>		y_t = neuralNetwork(x)<NewLine><NewLine>class mainProc():<NewLine>	def __init__(self):<NewLine>		<NewLine>		self.neuralNetwork = neuralNetwork()<NewLine><NewLine>		torch.multiprocessing.set_start_method('spawn', force=True)<NewLine>		self.neuralNetwork.share_memory()<NewLine><NewLine>		self.neuralNetwork.eval()<NewLine>	<NewLine>	def startInferrer(self):<NewLine><NewLine>		self.inferrer = torch.multiprocessing.Process(target = inferrerFunc, args = (self.neuralNetwork,))<NewLine><NewLine>		self.inferrer.start()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/radim_shark,(radim shark),radim_shark,"September 18, 2020, 10:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>When you pass <code>self.neuralNetwork</code> as a parameter, I believe it is pickled and then unpickled in the child process. The unpickling process most re allocating some memory. Note that <code>share_memory</code> only applies to CPU tensors and not GPU tensors. The pickling usually happens in <code>spawn</code> mode, you can try and use <code>fork</code> to see if that resolves the issue.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> ,"REPLY_DATE 1: September 23, 2020,  3:45am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
96829,Process stuck when training on multiple nodes using DistributedDataParallel,2020-09-19T11:04:33.672Z,0,36,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to run the script <code>mnist-distributed.py</code> from <a href=""https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html"" rel=""nofollow noopener"">Distributed data parallel training in Pytorch</a>. I have also pasted the same code here. (I have replaced my actual <code>MASTER_ADDR</code> with <code>a.b.c.d</code> for posting here).</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import argparse<NewLine>import torch.multiprocessing as mp<NewLine>import torchvision<NewLine>import torchvision.transforms as transforms<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.distributed as dist<NewLine><NewLine>class ConvNet(nn.Module):<NewLine>    def __init__(self, num_classes=10):<NewLine>        super(ConvNet, self).__init__()<NewLine>        self.layer1 = nn.Sequential(<NewLine>            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),<NewLine>            nn.BatchNorm2d(16),<NewLine>            nn.ReLU(),<NewLine>            nn.MaxPool2d(kernel_size=2, stride=2))<NewLine>        self.layer2 = nn.Sequential(<NewLine>            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),<NewLine>            nn.BatchNorm2d(32),<NewLine>            nn.ReLU(),<NewLine>            nn.MaxPool2d(kernel_size=2, stride=2))<NewLine>        self.fc = nn.Linear(7*7*32, num_classes)<NewLine><NewLine>    def forward(self, x):<NewLine>        out = self.layer1(x)<NewLine>        out = self.layer2(out)<NewLine>        out = out.reshape(out.size(0), -1)<NewLine>        out = self.fc(out)<NewLine>        return out<NewLine><NewLine>def main():<NewLine>    parser = argparse.ArgumentParser()<NewLine>    parser.add_argument('-n', '--nodes', default=1, type=int, metavar='N')<NewLine>    parser.add_argument('-g', '--gpus', default=1, type=int,<NewLine>                        help='number of gpus per node')<NewLine>    parser.add_argument('-nr', '--nr', default=0, type=int,<NewLine>                        help='ranking within the nodes')<NewLine>    parser.add_argument('--epochs', default=2, type=int, metavar='N',<NewLine>                        help='number of total epochs to run')<NewLine>    args = parser.parse_args()<NewLine>    args.world_size = args.gpus * args.nodes               <NewLine>    os.environ['MASTER_ADDR'] = 'a.b.c.d'              <NewLine>    os.environ['MASTER_PORT'] = '8890'                    <NewLine>    mp.spawn(train, nprocs=args.gpus, args=(args,))       <NewLine><NewLine>def train(gpu, args):<NewLine>    rank = args.nr * args.gpus + gpu	                          <NewLine>    dist.init_process_group(                                   <NewLine>    	backend='nccl',                                         <NewLine>   		init_method='env://',                                   <NewLine>    	world_size=args.world_size,                              <NewLine>    	rank=rank                                               <NewLine>    )                                                          <NewLine>    <NewLine>    torch.manual_seed(0)<NewLine>    model = ConvNet()<NewLine>    torch.cuda.set_device(gpu)<NewLine>    model.cuda(gpu)<NewLine>    batch_size = 100<NewLine>    # define loss function (criterion) and optimizer<NewLine>    criterion = nn.CrossEntropyLoss().cuda(gpu)<NewLine>    optimizer = torch.optim.SGD(model.parameters(), 1e-4)<NewLine>    <NewLine>    # Wrap the model<NewLine>    model = nn.parallel.DistributedDataParallel(model,<NewLine>                                                device_ids=[gpu])<NewLine><NewLine>    # Data loading code<NewLine>    train_dataset = torchvision.datasets.MNIST(<NewLine>        root='./data',<NewLine>        train=True,<NewLine>        transform=transforms.ToTensor(),<NewLine>        download=True<NewLine>    )                                               <NewLine>    train_sampler = torch.utils.data.distributed.DistributedSampler(<NewLine>    	train_dataset,<NewLine>    	num_replicas=args.world_size,<NewLine>    	rank=rank<NewLine>    )<NewLine><NewLine>    train_loader = torch.utils.data.DataLoader(<NewLine>    	dataset=train_dataset,<NewLine>       batch_size=batch_size,<NewLine>       shuffle=False,            <NewLine>       num_workers=0,<NewLine>       pin_memory=True,<NewLine>      sampler=train_sampler)     <NewLine><NewLine>    total_step = len(train_loader)<NewLine>    for epoch in range(args.epochs):<NewLine>        for i, (images, labels) in enumerate(train_loader):<NewLine>            images = images.cuda(non_blocking=True)<NewLine>            labels = labels.cuda(non_blocking=True)<NewLine>            # Forward pass<NewLine>            outputs = model(images)<NewLine>            loss = criterion(outputs, labels)<NewLine><NewLine>            # Backward and optimize<NewLine>            optimizer.zero_grad()<NewLine>            loss.backward()<NewLine>            optimizer.step()<NewLine>            if (i + 1) % 100 == 0 and gpu == 0:<NewLine>                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(<NewLine>                    epoch + 1, <NewLine>                    args.epochs, <NewLine>                    i + 1, <NewLine>                    total_step,<NewLine>                    loss.item())<NewLine>                   )<NewLine><NewLine>if __name__ == '__main__':<NewLine>    main()<NewLine></code></pre><NewLine><p>There are 2 nodes with 2 GPUs each. I run this command from the terminal of the master node-</p><NewLine><blockquote><NewLine><p>python mnist-distributed.py -n 2 -g 2 -nr 0</p><NewLine></blockquote><NewLine><p>, and then this from the terminal of the other node-</p><NewLine><blockquote><NewLine><p>python mnist-distributed.py -n 2 -g 2 -nr 1</p><NewLine></blockquote><NewLine><p>But then my process gets stuck with no output on either terminal.</p><NewLine><p>Running the same code on a single node using the following command works perfectly fine-</p><NewLine><blockquote><NewLine><p>python mnist-distributed.py -n 1 -g 2 -nr 0</p><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/motor_junkie,,motor_junkie,"September 19, 2020, 11:16am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you verify that a.b.c.d is reachable on port 8890 from the other node? Also, it would help to understand where exactly the process gets stuck. You could add a few print statements around <code>init_process_group</code> to check if the initialization is getting stuck.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> ,"REPLY_DATE 1: September 23, 2020,  3:27am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
96630,How to implement distributed model parallel using torch.distributed,2020-09-17T14:29:28.843Z,2,59,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to implement model parallel using torch.distributed(DistributedDataParallel), and I am wondering is there a tutorial for that (single node multiple GPUs)? I know nn.DataParallel is easier to use, however, I use another package that only support torch.distributed. Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/xdwang0726,,xdwang0726,"September 17, 2020,  2:29pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yep, here is the tutorial: <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#combine-ddp-with-model-parallelism"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#combine-ddp-with-model-parallelism</a></p><NewLine><p>It should work if you place different layers of the model on different GPUs and pass it to DDP. DDP should be able to detect it is a multi-GPU model. One caveat is that, please make sure no GPUs are shared across processes.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I followed the tutorial online, however I got the error message <code>RuntimeError: Model replicas must have an equal number of parameters.</code> in <code>model = torch.nn.parallel.DistributedDataParallel(model)</code><br/><NewLine>Any idea what might cause this issue? Thanks!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/xdwang0726"">@xdwang0726</a> Do you see this error when using the code in the tutorial? Or is it some custom code based on the tutorial? If it is the latter, could you provide a minimal repro of the issue?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xdwang0726; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> ,"REPLY_DATE 1: September 17, 2020,  2:59pm; <NewLine> REPLY_DATE 2: September 20, 2020, 11:29am; <NewLine> REPLY_DATE 3: September 23, 2020,  2:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
97027,SyncBatchNorm across multiple batches of the SAME gpu - is it possible?,2020-09-21T17:20:07.575Z,0,29,"<div class=""post"" itemprop=""articleBody""><NewLine><h3>Fake Distributed on 1 GPU</h3><NewLine><p>I have big samples, so I can’t use a big batch size. I virtually increase the batch size simply by calling the <code>optimizer.step()</code> every N batches. However, that of course doesn’t help the statistics of BatchNorm that are calculated per batch, and suffer from that. There is only so much I can do with the batchnorm momentum… I would like to simulate a distributed system on 1 GPU and sync the BN layers across multiple fake-parallel batches.<br/><NewLine>Is that possible?</p><NewLine></div>",https://discuss.pytorch.org/u/Alon,(Alon),Alon,"September 21, 2020,  5:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is using multiple GPUs to increase the effective batch size not an option for you?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> ,"REPLY_DATE 1: September 23, 2020,  2:43am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
96965,"Can we split a large, pytorch-built-in `nn.Module` to multiple-gpu?",2020-09-21T05:50:04.435Z,0,41,"<div class=""post"" itemprop=""articleBody""><NewLine><p>eg. I have and <code>nn.Linear</code> whose <code>in_features = 500e4</code> and <code>out_features = 3000</code>, so number of trainable parameters consume 55 GB (500e4 * 3000 * 4 / 1024 ** 2) memory. My single GPU has 12 GB and I have 8 GPU on one machine. How can I parallel this “atomic” built-in module on multiple gpu?</p><NewLine></div>",https://discuss.pytorch.org/u/JiaRu2016,(JIA Ru),JiaRu2016,"September 21, 2020,  5:50am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m afraid we don’t provide any construct to do this automatically.<br/><NewLine>But you can simply create 8 different Linear that each take a subset of the input and split the input yourself and call each of these Linears and then add all the results (assuming your split on the input size here given that it is the biggest).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: September 22, 2020,  1:24am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
96050,Lazy generation of large random matrix,2020-09-12T17:46:00.940Z,0,54,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to lazy-generate a larger-than-memory random matrix with pytorch? Ideally I am looking to generate a 1e9 x 5000 matrix and compute the scalar product with another 5k x 5k matrix.<br/><NewLine>I had a look at keops, but I didn’t manage to get too far with that.</p><NewLine><p>Is it be possible to do this in a distributed manner, for example by interfacing somehow with pyspark?</p><NewLine></div>",https://discuss.pytorch.org/u/Luca_Mingarelli,(Luca Mingarelli),Luca_Mingarelli,"September 12, 2020,  5:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/luca_mingarelli"">@Luca_Mingarelli</a></p><NewLine><p><code>torch.distributed</code> package can help to distribute tensors across multiple nodes, but, as of today, you still need to implement the distribute matrix multiplication on your own.  If you are looking for features like Mesh-TensorFlow. It is not yet available.</p><NewLine><p>This is a very interesting request. Do you mind share some some details of the application?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>Say I want to perform N monte carlo simulations each consisting in a dot product of a vector of length M (M=5k) times a square matrix M x M. One way is a very lengthy and never ending loop. A matrix multiplication is however much faster. This is particularly useful in credit risk, to estimate distributions of losses. With numpy you can only use an in-between solution by making batches, but being able to lazily perform such operation gives massive advantages. Currently I solved the problem by a combination of xarray and dask, but would be great to see this in pytorch as well.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Luca_Mingarelli; <NewLine> ,"REPLY_DATE 1: September 15, 2020,  2:58am; <NewLine> REPLY_DATE 2: September 20, 2020,  1:01pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
96338,How to use `torch.nn.parallel.DistributedDataParallel` and `torch.utils.checkpoint` together,2020-09-15T09:30:54.863Z,5,75,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I have some problems using <code>torch.nn.parallel.DistributedDataParallel</code> (DDP) and <code>torch.utils.checkpoint</code> together.</p><NewLine><p>It is Ok, if I set <code>find_unused_parameters=False</code> in DDP. The dilemma is that my network is a dynamic CNN, which will not forward the whole model during training, which means I have to set the <code>find_unused_parameters=True</code>… And, if I don’t use the <code>torch.utils.checkpoint</code>, my network is too large to run, leading to the OOM problem…</p><NewLine><p>Therefore, what should I do to meet my demands?</p><NewLine><p>There are some links to this question, but they not solve my problem.</p><NewLine><ol><NewLine><li><a href=""https://github.com/pytorch/pytorch/issues/43135"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/43135</a></li><NewLine><li><a href=""https://github.com/pytorch/pytorch/issues/24005"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/24005</a></li><NewLine></ol><NewLine><p>Part of error report:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/9f3a15a8ad35a17798ca4b6bbe3dfcfe69488f44"" href=""https://discuss.pytorch.org/uploads/default/original/3X/9/f/9f3a15a8ad35a17798ca4b6bbe3dfcfe69488f44.png"" title=""image""><img alt=""image"" data-base62-sha1=""mIAmxgKgSPlR8aAocTvBFnwlqrq"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/f/9f3a15a8ad35a17798ca4b6bbe3dfcfe69488f44_2_10x10.png"" height=""132"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/f/9f3a15a8ad35a17798ca4b6bbe3dfcfe69488f44_2_690x132.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/f/9f3a15a8ad35a17798ca4b6bbe3dfcfe69488f44_2_690x132.png, https://discuss.pytorch.org/uploads/default/optimized/3X/9/f/9f3a15a8ad35a17798ca4b6bbe3dfcfe69488f44_2_1035x198.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/9/f/9f3a15a8ad35a17798ca4b6bbe3dfcfe69488f44_2_1380x264.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1875×360 114 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>Thanks for all the suggestions!!!</p><NewLine></div>",https://discuss.pytorch.org/u/LicharYuan,(Lichar Yuan),LicharYuan,"September 16, 2020,  3:30am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>DDP does not work with <code>torch.utils.checkpoint</code> yet. One work around is to run forward-backward on the local model, and then manually run <code>all_reduce</code> to synchronize gradients after the backward pass.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s ok in <code>find_unused_parameters=False</code>, so I guess maybe manually define <code>checkpoint</code> can solve my problem?</p><NewLine><p>Thus, I modify the checkpoint to save the variable(indicating which part to forward), and get output again in backward with this variable.</p><NewLine><p>But it makes new problems (Not Enough Values to unpack in backward)…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""96338"" data-username=""LicharYuan""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/licharyuan/40/28857_2.png"" width=""20""/> LicharYuan:</div><NewLine><blockquote><NewLine><p>It’s ok in <code>find_unused_parameters=False</code> , so I guess maybe manually define <code>checkpoint</code> can solve my problem?</p><NewLine></blockquote><NewLine></aside><NewLine><p>The reason <code>find_unused_parameters=True</code> does not work is because, DDP will try to traverse the autograd graph from output at the end of the forward pass when <code>find_unused_parameters</code> is set to True. However, with checkpoint, the autograd graphs are reconstructed during the backward pass, and hence it is not available when DDP tries to traverse it, which will make DDP think those unreachable parameters are not used in the forward pass (although they are just hidden by checkpoint). When setting <code>find_unused_parameters=False</code>, DDP will skip the traverse, and expect that all parameters are used and autograd engine will compute grad for each parameter <strong>exactly once</strong>.</p><NewLine><blockquote><NewLine><p>But it makes new problems (Not Enough Values to unpack in backward)…</p><NewLine></blockquote><NewLine><p>Could you please elaborate more on this problem?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""96338"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mrshenli/40/12220_2.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>The reason <code>find_unused_parameters=True</code> does not work is because, DDP will try to traverse the autograd graph from output at the end of the forward pass when <code>find_unused_parameters</code> is set to True. However, with checkpoint, the autograd graphs are reconstructed during the backward pass, and hence it is not available when DDP tries to traverse it, which will make DDP think those unreachable parameters are not used in the forward pass (although they are just hidden by checkpoint). When setting <code>find_unused_parameters=False</code> , DDP will skip the traverse, and expect that all parameters are used and autograd engine will compute grad for each parameter <strong>exactly once</strong> .</p><NewLine></blockquote><NewLine></aside><NewLine><p>If so, then the new problem is caused by my codes. I will try to figure it out .</p><NewLine><p>I understand the effect of <code>find_unused_paramerts</code>  now. Therefore, even my network has some unused parameters, but in each forward, each GPU updates the same unused parameters. In such a situation, I also can set  <code>find_unused_paramerts=False</code> all right ?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I fix my problem like the FP16Optimizer in <a href=""https://github.com/open-mmlab/mmdetection/blob/927d71a98ce2446193c626d26268df0c29922fb0/mmdet/core/fp16/hooks.py#L11"" rel=""nofollow noopener"">mmdetection</a>, which is similar to the Apex <code>delay all reduce</code>.</p><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""96338"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mrshenli/40/12220_2.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>One work around is to run forward-backward on the local model, and then manually run <code>all_reduce</code> to synchronize gradients after the backward pass.</p><NewLine></blockquote><NewLine></aside><NewLine><p>And I realize the solution is just you said before… Thank you very much!!!</p><NewLine><p>But it seems work no matter what value of <code>find_unused_parameters</code>. I wonder why it work when <code>find_unused_parameters=True</code>, the traverse should failed logically.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>But it seems work no matter what value of  <code>find_unused_parameters</code> . I wonder why it work when  <code>find_unused_parameters=True</code> , the traverse should failed logically.</p><NewLine></blockquote><NewLine><p>IDK, I would assume this would lead to autograd hook fire on a ready parameter, which should trigger an error in DDP. BTW, when you manually run allreduce, DDP is no longer necessary. Any reason for still wrapping the model with DDP?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>You are right. Actually, there is no need to use the ddp anymore. Thx~</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/LicharYuan; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/LicharYuan; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/LicharYuan; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/LicharYuan; <NewLine> ,"REPLY_DATE 1: September 17, 2020, 10:00am; <NewLine> REPLY_DATE 2: September 16, 2020,  3:33am; <NewLine> REPLY_DATE 3: September 16, 2020,  2:55pm; <NewLine> REPLY_DATE 4: September 17, 2020,  6:49am; <NewLine> REPLY_DATE 5: September 17, 2020, 10:08am; <NewLine> REPLY_DATE 6: September 18, 2020,  3:01pm; <NewLine> REPLY_DATE 7: September 28, 2020,  8:53am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
95642,"When I use 1024 nodes in rpc, I meet RuntimeError &ldquo;listen: Address already in use&rdquo;",2020-09-09T09:49:55.031Z,6,133,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">rpc.init_rpc('env_{}'.format(rank), rank=rank, world_size=opt.world_size, rpc_backend_options=rpc.ProcessGroupRpcBackendOptions(rpc_timeout=100))<NewLine></code></pre><NewLine><p>if I use 128 nodes, it works.<br/><NewLine>but when I use 1024 nodes (32 servers * 32 processes or 16 servers * 64 processes), I meet</p><NewLine><pre><code class=""lang-auto"">...<NewLine>RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:184] listen: Address already in use<NewLine></code></pre><NewLine><p>my environment is  pytorch 1.6.0, python 3.7, cuda 10.1.<br/><NewLine>is there anyone have met it before ?</p><NewLine></div>",https://discuss.pytorch.org/u/yueyilia,(hzy),yueyilia,"September 10, 2020,  8:26am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/yueyilia"">@yueyilia</a>, I haven’t seen this error before. Which version of PyTorch are you using?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>my environment is  pytorch 1.6.0, python 3.7, cuda 10.1.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Does <a href=""https://pytorch.org/docs/master/rpc.html#tensorpipe-backend"" rel=""nofollow noopener"">tensorpipe RPC backend</a> work in this case? It is still experimental, but we plan to make it the default backend and focus on it in future releases.</p><NewLine><p>Regarding the error, I suspect this is a limitation in Gloo. I don’t have enough resource to verify this locally. If possible, can you try if <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group"" rel=""nofollow noopener""><code>init_process_group</code></a> and <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_reduce"" rel=""nofollow noopener""><code>all_reduce</code></a> work with 1024 nodes?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I replace process group with tensorpipe, I also meet</p><NewLine><pre><code class=""lang-auto"">...<NewLine>RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:184] listen: Address already in use<NewLine></code></pre><NewLine><p>It seems that tensorpipe still depends on gloo.<br/><NewLine>I run init_process_group and all_reduce, and the error is the same.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems that process group needs too many ports. It will establish a connection between every two nodes, so that each server needs 32*1024 ports (32 servers * 32 processes) to establish a tcp connection. Do you have any plans to optimize it?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>It seems that tensorpipe still depends on gloo.</p><NewLine></blockquote><NewLine><p>It’s true for now, but only for initialization and shutdown. We will remove its dependency on gloo in future releases. (hopefully in v1.8)</p><NewLine><blockquote><NewLine><p>It seems that process group needs too many ports. It will establish a connection between every two nodes, so that each server needs 32*1024 ports (32 servers * 32 processes) to establish a tcp connection. Do you have any plans to optimize it?</p><NewLine></blockquote><NewLine><p>Good catch! I am not aware of any plan to improve gloo for this. cc <a class=""mention"" href=""/u/pbelevich"">@pbelevich</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>For the RPC Framework it seems like this is happening since Gloo creates a tcp connection for all combination of processes in the group.</p><NewLine><p>I’m wondering if this can be avoid in TensorPipe where the TCP connections are created on demand and kept in a pool for reuse. Typically in a RPC environment, we’re not talking to all the nodes in the group at the same time.</p><NewLine><p><a class=""mention"" href=""/u/yueyilia"">@yueyilia</a> Could you add some details about your use case for RPC here? Are all the nodes (1024) communicating with all the other nodes in your application at the same time? Is it possible to run 1 process per server in your application to get around this in the short term? If GIL is  currently the bottleneck, there is some amount of TorchScript support in the RPC framework that might help getting round GIL.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc: <a class=""mention"" href=""/u/lcw"">@lcw</a> re creating TCP connections on demand in TensorPipe</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>TensorPipe would fare better if indeed your topology (i.e., the links you actually use) are a significantly smaller subset than the complete graph. (For example, if you use a server-client pattern). In other words, if your graph is sparse. For dense (near-complete) graphs TensorPipe will perform even worse than Gloo because each pipe will internally use multiple TCP connections, whereas Gloo uses only one.</p><NewLine><p>The reason you’re currently unable to use TensorPipe is because, indeed, it uses Gloo internally for the join step. We’ve been wanting to get rid of this for a while but it’s hard, because the RPC agent’s join must do a barrier, and it’s easier to do it through a library that already does collectives (namely Gloo) rather than re-implement it. We could use the c10d::Store instead of the ProcessGroup for that, but currently the Store isn’t powerful enough. <a class=""mention"" href=""/u/osalpekar"">@osalpekar</a> was thinking of refactoring it though so maybe then we could do this change. See <a href=""https://github.com/pytorch/pytorch/issues/42879"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/42879</a> and <a href=""https://github.com/pytorch/pytorch/issues/41614"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/41614</a> for more context.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/yueyilia; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/yueyilia; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/yueyilia; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/osalpekar; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/lcw; <NewLine> ,"REPLY_DATE 1: September 9, 2020,  5:50pm; <NewLine> REPLY_DATE 2: September 10, 2020,  1:56am; <NewLine> REPLY_DATE 3: September 10, 2020,  2:45am; <NewLine> REPLY_DATE 4: September 10, 2020,  9:57am; <NewLine> REPLY_DATE 5: September 10, 2020,  8:24am; <NewLine> REPLY_DATE 6: September 10, 2020,  2:10pm; <NewLine> REPLY_DATE 7: September 17, 2020,  9:29pm; <NewLine> REPLY_DATE 8: September 17, 2020, 11:10pm; <NewLine> REPLY_DATE 9: September 18, 2020, 11:10am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> 
96429,How to make rpc work with WSL,2020-09-16T03:00:49.612Z,4,72,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying below sample on multiple machines:<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/examples/blob/master/distributed/rpc/parameter_server/rpc_parameter_server.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/examples/blob/master/distributed/rpc/parameter_server/rpc_parameter_server.py"" rel=""nofollow noopener"" target=""_blank"">pytorch/examples/blob/master/distributed/rpc/parameter_server/rpc_parameter_server.py</a></h4><NewLine><pre><code class=""lang-py"">import argparse<NewLine>import os<NewLine>import time<NewLine>from threading import Lock<NewLine><NewLine>import torch<NewLine>import torch.distributed.autograd as dist_autograd<NewLine>import torch.distributed.rpc as rpc<NewLine>import torch.multiprocessing as mp<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>from torch import optim<NewLine>from torch.distributed.optim import DistributedOptimizer<NewLine>from torchvision import datasets, transforms<NewLine><NewLine># --------- MNIST Network to train, from pytorch/examples -----<NewLine><NewLine><NewLine>class Net(nn.Module):<NewLine>    def __init__(self, num_gpus=0):<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/examples/blob/master/distributed/rpc/parameter_server/rpc_parameter_server.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>When I tried on 2 linux machine in same domain, this sample works fine. But when I try on 2 WSL machine, I can’t make these 2 WSL machine connect with each other, both machines blocking waiting on rpc init.</p><NewLine><p>Tried the "" <strong>WSL 2 TPC NETWORK FORWARDING</strong>"" workaround from below post, rpc still can’t connect with each other:<br/><NewLine></p><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/microsoft/WSL/issues/4150"" rel=""nofollow noopener"" target=""_blank"">github.com/microsoft/WSL</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/microsoft/WSL/issues/4150"" rel=""nofollow noopener"" target=""_blank"">[WSL 2] NIC Bridge mode 🖧 (Has TCP Workaround🔨)</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-06-16"" data-format=""ll"" data-time=""01:35:15"" data-timezone=""UTC"">01:35AM - 16 Jun 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/jkasten2"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""jkasten2"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/645861?v=4"" width=""20""/><NewLine>          jkasten2<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">Issue<NewLine>WSL 2 seems to NAT it's virtual network, instead of making it bridged to the host NIC. My goal is for...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">network</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>So I’m wondering if there is any working sample/setting for WSL? Any suggestions on how to trouble the issue is also highly appreciated, thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/frankdong,,frankdong,"September 16, 2020,  3:00am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We haven’t tests WSL (I assume you mean Windows Subsystem for Linux), and are not sure what are the gaps if there are any.</p><NewLine><p>cc <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> do you know who is familiar with WSL?</p><NewLine><p>Not sure if this can help, I would try to verify if the following command resolves to the correct interface. If not, set <a href=""https://pytorch.org/docs/stable/distributed.html#choosing-the-network-interface-to-use"" rel=""nofollow noopener""><code>GLOO_SOCKET_IFNAME</code></a> explicitly.</p><NewLine><pre><code class=""lang-auto"">getent hosts `hostname`<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> Thanks for your reply.<br/><NewLine>Yes, we are using Windows Subsystem for Linux as Windows not support distributed yet. Although I see an open PR for this (but we don’t know if rpc will be support in this PR): <a href=""https://github.com/pytorch/pytorch/pull/42897"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/42897</a><br/><NewLine>And yes, hostname can resolve correctly. One thing that different on linux machine and WSL machine we are aware of is WSL machine share ip with host windows system but need port forwarding from windows to WSL. We already tried this solution that add port forwarding but seems rpc still can’t connect.<br/><NewLine><a href=""https://github.com/microsoft/WSL/issues/4150"" rel=""nofollow noopener"">https://github.com/microsoft/WSL/issues/4150</a></p><NewLine><p>So question is say rank0 is listening on one port 12560 for example, is this the only port will be used on rank0 machine during rpc initialization and connection?<br/><NewLine>Also, is there any way to output detailed information from rpc during initialization so we can find more details here. Ideally we will be able to know what address/port rank0 is listening on and where other ranks are connecting to, whether there is some mismatch on the address or other things cause the rpc initialization hangs.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Unfortunately, I’m not familiar with it and don’t know a specific user who might be.<br/><NewLine>Maybe <a class=""mention"" href=""/u/maxiluk"">@maxiluk</a> or <a class=""mention"" href=""/u/peterjc123"">@peterjc123</a> would know more.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""96429"" data-username=""frankdong""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/frankdong/40/28780_2.png"" width=""20""/> frankdong:</div><NewLine><blockquote><NewLine><p>Although I see an open PR for this (but we don’t know if rpc will be support in this PR):</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yep, MSFT experts are helping us adding Windows support to PT Distributed. The first step focuses on DDP only, but we do plan to cover all features in the distributed package in future releases.</p><NewLine><blockquote><NewLine><p>So question is say rank0 is listening on one port 12560 for example, is this the only port will be used on rank0 machine during rpc initialization and connection?</p><NewLine></blockquote><NewLine><p>I see. No, that port is only used for rendezvous during initialization. RPC backend will grab a port for each pair of workers, which are not visible to users.</p><NewLine><p>cc <a class=""mention"" href=""/u/lcw"">@lcw</a> any suggestion on how RPC can work with port forwarding?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""96429"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mrshenli/40/12220_2.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>RPC backend will grab a port for each pair of workers, which are not visible to users.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Do we know the port range so we can have a test</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not super familiar with port forwarding on WSL, but I assume it does some sort of NAT, right? In that case, I suspect it would be enough if only the listening socket(s) on each server had a fixed port which is correctly forwarded? (as then all other sockets that are accepted will have a random port but the NAT would be aware of them and would handle them)</p><NewLine><p>Unfortunately, at this moment, also the port of the listening socket(s) is picked at random (well, not at random, but we’re letting the kernel give us back an arbitrary available port). We don’t support a way for the user to specify a port. Which I think means there’s no real way for you to set up port forwarding before launching the application.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/frankdong; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/frankdong; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/lcw; <NewLine> ,"REPLY_DATE 1: September 16, 2020,  6:28pm; <NewLine> REPLY_DATE 2: September 17, 2020,  3:25am; <NewLine> REPLY_DATE 3: September 17, 2020,  5:00am; <NewLine> REPLY_DATE 4: September 17, 2020,  3:04pm; <NewLine> REPLY_DATE 5: September 17, 2020,  7:04pm; <NewLine> REPLY_DATE 6: September 18, 2020,  9:13am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> 
96463,Program freezes when doing distributed training,2020-09-16T09:08:26.784Z,4,66,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to add distributed training on my program as my model is relatively large. The program runs well without distributed training, however, when I add distributed training, the program freezes right at <code>model = torch.nn.parallel.DistributedDataParallel(model)</code> without returning any error messages. I am wondering have anyone else faced this situation before? Are there any possible solutions? Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/xdwang0726,,xdwang0726,"September 16, 2020,  9:08am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""96463"" data-username=""xdwang0726""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/xdwang0726/40/17441_2.png"" width=""20""/> xdwang0726:</div><NewLine><blockquote><NewLine><p>model = torch.nn.parallel.DistributedDataParallel(model)</p><NewLine></blockquote><NewLine></aside><NewLine><p>When doing the above without specifying a <code>device_id</code>, it will try to replicate the model to all visible devices in each process (unless the model is on CPU). Is this intentional? The recommended use of DDP is to let each process exclusively operate on one GPU.</p><NewLine><p>Beside, before v1.7 DDP will create communication buckets. The total size of those buckets will be the same as model size. So the GPU memory size needs to be at least 3X of the model size.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply! I have specified the cuda ids when training. When I trained in a single GPU, it returns the error message that ‘CUDA requires ~6G more memory’. I add one more 16G GPU to do the training. According to your suggestion, it seems like I need at least 4 GPUs in total? Thanks!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""96463"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mrshenli/40/12220_2.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>the GPU memory size needs to be at least 3X of the model size.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thanks for your reply! I have specified the cuda ids when training. When I trained in a single GPU, it returns the error message that ‘CUDA requires ~6G more memory’. I add one more 16G GPU to do the training. According to your suggestion, it seems like I need at least 4 GPUs in total? Thanks!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I got it work, never mind, thanks</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/xdwang0726"">@xdwang0726</a>, do you mind sharing what was the cause of the problem, and how it was resolved? In case future users hit the same issue.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Before I was using <code> local_rank = torch.distributed.get_rank()</code> and the program freezes. I manually set <code>local_rank=-1</code> that solves the problem.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xdwang0726; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/xdwang0726; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/xdwang0726; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/xdwang0726; <NewLine> ,"REPLY_DATE 1: September 16, 2020,  2:59pm; <NewLine> REPLY_DATE 2: September 17, 2020,  2:00am; <NewLine> REPLY_DATE 3: September 17, 2020,  2:00am; <NewLine> REPLY_DATE 4: September 17, 2020,  1:20pm; <NewLine> REPLY_DATE 5: September 17, 2020,  2:56pm; <NewLine> REPLY_DATE 6: September 18, 2020,  1:58am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
96312,Training model across multiple remote servers- each having multiple GPUs,2020-09-15T06:37:53.815Z,2,51,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to train a model across multiple remote servers in my department? These servers are not connected to each other. I want to use GPUs of both the servers (with different IP addresses) so that I can train with larger batch size.</p><NewLine><p>I have seen <a href=""https://pytorch.org/docs/stable/nn.html#distributeddataparallel"" rel=""nofollow noopener""> <code>nn.DistributedDataParallel</code> </a> but how do I mention the IP address of multiple servers?</p><NewLine></div>",https://discuss.pytorch.org/u/motor_junkie,,motor_junkie,"September 15, 2020,  9:51am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""96312"" data-username=""motor_junkie""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/motor_junkie/40/12946_2.png"" width=""20""/> motor_junkie:</div><NewLine><blockquote><NewLine><p>These servers are not connected to each other.</p><NewLine></blockquote><NewLine></aside><NewLine><p>What does this mean? Their IPs are not reachable from each other?</p><NewLine><blockquote><NewLine><p>I have seen <a href=""https://pytorch.org/docs/stable/nn.html#distributeddataparallel"" rel=""nofollow noopener""> <code>nn.DistributedDataParallel</code> </a>but how do I mention the IP address of multiple servers?</p><NewLine></blockquote><NewLine><p>If they can reach each other through network, yes, <code>DistributedDataParallel</code> can work across multiple machines. You need to provide the master address and master IP for all peers to do rendezvous. See <a href=""https://github.com/pytorch/examples/blob/master/distributed/ddp/README.md"" rel=""nofollow noopener"">this example</a>.</p><NewLine><p>If you want to choose a specific network interface, you can configure the following two env vars (<a href=""https://pytorch.org/docs/stable/distributed.html#common-environment-variables"" rel=""nofollow noopener"">more details</a>). You only need one of them depending on which backend you are using.</p><NewLine><pre><code class=""lang-auto"">NCCL_SOCKET_IFNAME, for example export NCCL_SOCKET_IFNAME=eth0<NewLine>GLOO_SOCKET_IFNAME, for example export GLOO_SOCKET_IFNAME=eth0<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think I didn’t describe my situtation correctly. What I meant is that they are different systems. One has IP: a.b.c.<strong>d</strong>, and the other has IP: a.b.c.<strong>e</strong>.</p><NewLine><p>Okay, thanks! I’ll try it out</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I think I didn’t describe my situtation correctly. What I meant is that they are different systems. One has IP: a.b.c. <strong>d</strong> , and the other has IP: a.b.c. <strong>e</strong> .</p><NewLine></blockquote><NewLine><p>I see. This should be fine. You only need to specify one of them as the master, and set <code>MASTER_ADDR</code> and <code>MASTER_PORT</code> for all peers to point to that master. This will allow all peers to do rendezvous, and the rendezvous process will create connections between pairs.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/motor_junkie; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: September 19, 2020, 10:53am; <NewLine> REPLY_DATE 2: September 17, 2020,  6:44am; <NewLine> REPLY_DATE 3: September 17, 2020,  3:08pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
62443,DDP taking up too much memory on rank 0,2019-11-28T16:28:28.902Z,1,287,"<div class=""post"" itemprop=""articleBody""><NewLine><p><strong>Edit: Mistaken!</strong><br/><NewLine>This was my issue:<br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/23138"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/23138"" rel=""nofollow noopener"" target=""_blank"">DistributedDataParallel: resume training from a checkpoint results in additional processes on GPU 0</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-07-21"" data-format=""ll"" data-time=""01:23:03"" data-timezone=""UTC"">01:23AM - 21 Jul 19 UTC</span><NewLine></div><NewLine><div class=""date""><NewLine>          closed <span class=""discourse-local-date"" data-date=""2019-07-21"" data-format=""ll"" data-time=""03:00:34"" data-timezone=""UTC"">03:00AM - 21 Jul 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/qchenclaire"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""qchenclaire"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars1.githubusercontent.com/u/24658370?v=4"" width=""20""/><NewLine>          qchenclaire<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">Hi,<NewLine>When I was trying the imagenet example with DistributedDataParallel, using single node with 4 gpus, I found that when I add...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">module: distributed</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/cb63a779ce29d8a7ddb168539aad24eaf9810f45"" href=""https://discuss.pytorch.org/uploads/default/original/3X/c/b/cb63a779ce29d8a7ddb168539aad24eaf9810f45.png"" title=""13%20PM""><img alt=""13%20PM"" data-base62-sha1=""t1gqWM4FyQM3kkfYlQXJEylr0vb"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/b/cb63a779ce29d8a7ddb168539aad24eaf9810f45_2_10x10.png"" height=""223"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/b/cb63a779ce29d8a7ddb168539aad24eaf9810f45_2_690x223.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/b/cb63a779ce29d8a7ddb168539aad24eaf9810f45_2_690x223.png, https://discuss.pytorch.org/uploads/default/optimized/3X/c/b/cb63a779ce29d8a7ddb168539aad24eaf9810f45_2_1035x334.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/c/b/cb63a779ce29d8a7ddb168539aad24eaf9810f45_2_1380x446.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">13%20PM</span><span class=""informations"">1416×458 32.4 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine>Here’s a screenshot of distributed training in Pytorch when I call the train function like:<br/><NewLine><code>CUDA_VISIBLE_DEVICES=1,2,3,4 python -m torch.distributed.launch --nproc_per_node=4 train_new.py</code>. You can see that the first rank has also initted 3 separate processes for each other GPU. When I use 10 GPUs on a box this severely limits the batch size, since the 0th dimension node has so much less capacity. What is it storing? I thought gradients in DDP were all-reduced. I’ve also tried turning broadcast_buffers to False to no avail.<br/><NewLine>Model is stacked modules of 1D-conv, relu, batch norm, LSTM, followed by a large softmax layer and CTC loss. Backend is NCCL<br/><NewLine>Pytorch 1.3.0, Cuda 10.1, Titan RTX, Ubuntu 18.04. Can provide more code upon request.</p><NewLine></div>",https://discuss.pytorch.org/u/PCerles,(P Cerles),PCerles,"November 28, 2019,  5:18pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>any solution?<br/><NewLine>github issue solution does not work for me</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Discussion <a href=""https://discuss.pytorch.org/t/torch-not-able-to-utilize-gpu-ram-properly/83245/8"">here</a> might be helpful.</p><NewLine><p>This is likely due to some tensors/context is unintentionally created on the 1st GPU, e.g., when calling <code>torch.cuda.empty_cache()</code> without a device guard. Solutions would be either 1) carefully walking though libs/codes to make sure no states leaks to <code>cuda:0</code>, or 2) set <code>CUDA_VISIBLE_DEVICES</code> to let each process only see one GPU.The second approach might be easier.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pcerles"">@PCerles</a> I’m having a similar issue. Were you able to resolve your problem? Thanks.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pcerles"">@PCerles</a> <a class=""mention"" href=""/u/felix_kreuk"">@Felix_Kreuk</a></p><NewLine><p>What <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> mentioned could seamlessly happen when you load saved parameters without specifying <code>map_location</code>.<br/><NewLine><code>torch.load</code> by default loads parameters to the device where they were, usually the rank 0 device.<br/><NewLine><code>load_state_dict</code> then copies the loaded value from that device to the target device.<br/><NewLine>After the intermediate use, torch still occupies the GPU memory as cached memory.<br/><NewLine>I had a similar issue and solved it by directly loading parameters to the target device.</p><NewLine><p>For example:</p><NewLine><pre><code class=""lang-python"">state_dict = torch.load(model_name, map_location=self.args.device)<NewLine>self.load_state_dict(state_dict)<NewLine></code></pre><NewLine><p>Full code <a href=""https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/model/__init__.py#L102"" rel=""nofollow noopener"">here</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/songzw; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Felix_Kreuk; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/seungjun; <NewLine> ,"REPLY_DATE 1: June 20, 2020,  5:13am; <NewLine> REPLY_DATE 2: June 22, 2020,  3:07pm; <NewLine> REPLY_DATE 3: September 15, 2020, 12:58pm; <NewLine> REPLY_DATE 4: September 17, 2020,  1:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
95529,Access data of a GPU process from main,2020-09-08T08:40:15.956Z,4,76,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi.</p><NewLine><p>I have a program using distributed data parallel of Pytorch.<br/><NewLine>It’s working well, but I do not know how to access data in the GPU process from main().</p><NewLine><p>In particular, a GPU process, train(), produces a list of loss, and I want to plot it in the main() after returning from spawn(). However, I do not know how to access the list in train() on GPU from main() on CPU.</p><NewLine><p>If I use a global variable, it should work, but it does not seem to be the best answer. I understand that printing loss can be done by gpu[0], and maybe even plotting the graph too. But, I want to do many tasks to analyze the results in main().</p><NewLine><p>I appreciate any information or examples. Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/TT_YY,(TT YY),TT_YY,"September 8, 2020,  8:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have checked communication using args and global variables.<br/><NewLine>Non of them works for transmitting data from a  GPU process to main(), when I use DDP.</p><NewLine><p>Usually, the arguments hold pointers to variables, and main() and called functions can share the same variables. But, when the processes are spawned in DDP, it seems that args are deep-copied and there is no common variables having the same ids between main() and the processes.</p><NewLine><p>A global variable can be declared in the process, but it’s not shared with main().</p><NewLine><p>Does everybody using DDP plot the loss charts in a spawned process on GPU?<br/><NewLine>I have no idea how to send the loss list from the process to main().</p><NewLine><p>Please advise.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""95529"" data-username=""TT_YY""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tt_yy/40/25673_2.png"" width=""20""/> TT_YY:</div><NewLine><blockquote><NewLine><p>I have checked communication using args and global variables.<br/><NewLine>Non of them works for transmitting data from a GPU process to main(), when I use DDP.</p><NewLine></blockquote><NewLine></aside><NewLine><p>This is true, because Python global vars are per-process concept.</p><NewLine><blockquote><NewLine><p>Does everybody using DDP plot the loss charts in a spawned process on GPU?</p><NewLine></blockquote><NewLine><p>This can be done using <code>torch.multiprocessing.SimpleQueue</code>. E.g., let the main process create the queue, pass it to the child process, and then let the child process put the loss object to the queue. Then, the main process should be able to see that.</p><NewLine><p>The test below can serve as an example:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/2c4b4aa81bc8dba8272e9c7190edcaa3e114ec15/test/test_multiprocessing.py#L580-L600"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/2c4b4aa81bc8dba8272e9c7190edcaa3e114ec15/test/test_multiprocessing.py#L580-L600"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/2c4b4aa81bc8dba8272e9c7190edcaa3e114ec15/test/test_multiprocessing.py#L580-L600</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""580"" style=""counter-reset: li-counter 579 ;""><NewLine><li>def test_event_multiprocess(self):</li><NewLine><li>    event = torch.cuda.Event(enable_timing=False, interprocess=True)</li><NewLine><li>    self.assertTrue(event.query())</li><NewLine><li><NewLine></li><li>    ctx = mp.get_context('spawn')</li><NewLine><li>    p2c = ctx.SimpleQueue()</li><NewLine><li>    c2p = ctx.SimpleQueue()</li><NewLine><li>    p = ctx.Process(</li><NewLine><li>        target=TestMultiprocessing._test_event_multiprocess_child,</li><NewLine><li>        args=(event, p2c, c2p))</li><NewLine><li>    p.start()</li><NewLine><li><NewLine></li><li>    c2p.get()  # wait for until child process is ready</li><NewLine><li>    torch.cuda._sleep(50000000)  # spin for about 50 ms</li><NewLine><li>    event.record()</li><NewLine><li>    p2c.put(0)  # notify child event is recorded</li><NewLine><li><NewLine></li><li>    self.assertFalse(event.query())</li><NewLine><li>    c2p.get()  # wait for synchronization in child</li><NewLine><li>    self.assertTrue(event.query())</li><NewLine><li>    p.join()</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not sure if my case is what you want but I use GPU communication functions to plot loss graph during training.</p><NewLine><p>I use a custom Loss class that inherits nn.modules.loss._Loss.<br/><NewLine>It calculates the loss and stores the record and plots the loss graph.<br/><NewLine>The loss values are synchronized inside the Loss class<br/><NewLine>Thus, I don’t have to send values to main() scope.</p><NewLine><p>Here’s my public <a href=""https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/loss/__init__.py#L259"" rel=""nofollow noopener"">github code</a>.</p><NewLine><p>all_reduce function performs sync and it is called at the end of an epoch from trainer.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you, Shen Li.</p><NewLine><p>This is be what I was looking for. I will try the sample code and try to use SimpleQueue() in my program. I hope it works!</p><NewLine><p>Thank you.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you, <a href=""https://discuss.pytorch.org/u/seungjun"">seungjun</a></p><NewLine><p>I appreciate your code.<br/><NewLine>I understand that all_reduce function transmits loss between GPUs.</p><NewLine><p>I am trying to evaluate performance of optimizers by changing many factors such as batch size and learning rate. Therefore, I have multiple loops to change the variables outside the optimization loop. Also, I have to collect many kinds of data along with loss and accuracy to analyze the details of the optimizers and plot them.</p><NewLine><p>I felt a kind of odd about performing such non-multiplication looping tasks and all plotting using expensive GPU and its memory. So, I tried to implement all the outer loops, analysis, and plotting tasks in main(), which requires getting the information from the spawned GPU processes.</p><NewLine><p>Thank you.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/TT_YY; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/seungjun; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/TT_YY; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/TT_YY; <NewLine> ,"REPLY_DATE 1: September 10, 2020,  4:46am; <NewLine> REPLY_DATE 2: September 15, 2020,  3:18am; <NewLine> REPLY_DATE 3: September 15, 2020,  8:10am; <NewLine> REPLY_DATE 4: September 16, 2020,  8:25am; <NewLine> REPLY_DATE 5: September 16, 2020,  8:46am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
96360,Distributed Data Parallel allreduce,2020-09-15T12:21:37.272Z,1,35,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is there a way to verify if allreduce operation is getting called in a multinode DDP training with nccl backend ? In my training the results of single node and distributed training appear similar.  <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> <a class=""mention"" href=""/u/apaszke"">@apaszke</a></p><NewLine></div>",https://discuss.pytorch.org/u/Sourabh_Daptardar,(Sourabh Daptardar),Sourabh_Daptardar,"September 15, 2020, 12:21pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>One option is to use <a href=""https://www.telesens.co/2019/04/04/distributed-data-parallel-training-using-pytorch-on-aws/"" rel=""nofollow noopener""><code>nvprof</code></a>.</p><NewLine><blockquote><NewLine><p>In my training the results of single node and distributed training appear similar.</p><NewLine></blockquote><NewLine><p>You mean speed is similar? What is the batch size fed into each DDP instance? When using DDP, the <code>batch_size</code> should be updated to <code>original_batch_size/world_size</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>No. I have divided the batch size by the world size.<br/><NewLine>Will check out nvprof and also create minimal working example as I can not share code.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Sourabh_Daptardar; <NewLine> ,"REPLY_DATE 1: September 15, 2020,  2:28pm; <NewLine> REPLY_DATE 2: September 15, 2020,  6:53pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
96274,how distributed.rpc package manages nodes,2020-09-14T21:29:33.950Z,0,52,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have several questions about distributed.rpc package, hope someone could shed a light on them, thanks:</p><NewLine><ol><NewLine><li>Looks like during rpc init(init_rpc), all nodes need to call init_rpc then init will be finished, otherwise all nodes will block waiting. Is there any reason for that behavior? In some scenario, when main node get initialized and some (not all) workers got initialized, the main node should able to send some work to workers and no need to wait for all workers to be initialized.</li><NewLine><li>Does rpc support adds/removes nodes after all nodes got initialized?</li><NewLine><li>What will happen if some node got disconnected after all nodes got initialized? Will the rest of nodes blocking waiting or other nodes will work as normal?</li><NewLine><li>Does rpc have any fail recover built in or plan to introduce in future?</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/frankdong,,frankdong,"September 14, 2020,  9:29pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/frankdong"">@frankdong</a>, thanks for the questions. Could you please share some more info about your use case? We would love to learn your requirements and use that as input for elastic RPC design.</p><NewLine><blockquote><NewLine><p>Looks like during rpc init(init_rpc), all nodes need to call init_rpc then init will be finished, otherwise all nodes will block waiting. Is there any reason for that behavior? In some scenario, when main node get initialized and some (not all) workers got initialized, the main node should able to send some work to workers and no need to wait for all workers to be initialized.</p><NewLine></blockquote><NewLine><p>This is a limitation of the current RPC implementation. The main reason is because early versions of RPC uses <code>ProcessGroupGloo</code> to do communication, which requires a rendezvous to initialize. We are working on adding elastic support (nodes can join/leave dynamically), and hopeful can address this problem. TensorPipe RPC backend is one step towards that, but the current version still has some dependency on <code>ProcessGroupGloo</code>.</p><NewLine><p>cc <a class=""mention"" href=""/u/agolynski"">@agolynski</a> for elasticity<br/><NewLine>cc <a class=""mention"" href=""/u/lcw"">@lcw</a> for TensorPipe</p><NewLine><blockquote><NewLine><p>Does rpc support adds/removes nodes after all nodes got initialized?</p><NewLine></blockquote><NewLine><p>Not yet, but this is in our roadmap.</p><NewLine><blockquote><NewLine><p>What will happen if some node got disconnected after all nodes got initialized? Will the rest of nodes blocking waiting or other nodes will work as normal?<br/><NewLine>Does rpc have any fail recover built in or plan to introduce in future?</p><NewLine></blockquote><NewLine><p>If the network failure is transient, it should be fine. It will retry internal control messages automatically. If the message contains user function and got lost during the network failure, the sender should see an timeout error.</p><NewLine><p>If it’s permanent network failure or node failure, the current version cannot handle those. We do have plan to cover this gap. There are some relevant discussion here: <a href=""https://github.com/pytorch/pytorch/issues/41425"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/41425</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> Thanks for your detailed answer.<br/><NewLine>Do we have rough timeline for elastic rpc?<br/><NewLine>Regarding our scenario: We are using rpc package as the distribute infrastructure. We have 1 scheduler node that is state-full and will split jobs and send out to workers. Workers are state-less and will receive job from scheduler and report result back to scheduler as soon as it finishes execution. Then scheduler will decide whether we have more jobs to send out and check if there is any idle workers and coordinate the whole execution process. So we will need our distribute infrastructure to be flexible enough to manage all distributed workers and handle node failure properly.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/frankdong; <NewLine> ,"REPLY_DATE 1: September 14, 2020,  9:43pm; <NewLine> REPLY_DATE 2: September 15, 2020,  6:52pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
95976,Parallel training All_Reduce on single GPU,2020-09-11T21:56:26.987Z,1,44,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I want to use data parallel to train my model on single GPU. I followed the example of Pytorch DISTRIBUTED DATA PARALLEL, and pass the same device_id to 4 processes. With all-reduce sync method, it runs even slower than using a single process. The interesting thing is by disabling all_reduce sync-up for gradients, there is a great speed up of training itself. So I think the GPU has extra compute capacity for multiple training process, but the bottleneck is all-reduce method. Any one know the reason for this bottleneck? Is there any other way to sync param gradients without using all_reduce? Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Yi_Zhang,(Yi Zhang),Yi_Zhang,"September 11, 2020,  9:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I want to use data parallel to train my model on single GPU. I followed the example of Pytorch DISTRIBUTED DATA PARALLEL, and pass the same device_id to 4 processes. With all-reduce sync method, it runs even slower than using a single process.</p><NewLine></blockquote><NewLine><p>The <code>all_reduce</code> op expects each process to exclusively work on a different GPU. If the same GPU is shared across processes, it does not guarantee to work.</p><NewLine><blockquote><NewLine><p>The interesting thing is by disabling all_reduce sync-up for gradients, there is a great speed up of training itself.</p><NewLine></blockquote><NewLine><p>How did you disable that in DDP?</p><NewLine><blockquote><NewLine><p>So I think the GPU has extra compute capacity for multiple training process, but the bottleneck is all-reduce method.</p><NewLine></blockquote><NewLine><p>Looks like your model is small enough such that each op will just occupy a subset of resources in the GPU.</p><NewLine><blockquote><NewLine><p>Is there any other way to sync param gradients without using all_reduce?</p><NewLine></blockquote><NewLine><p>Since you just use one GPU, you can try using <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html"" rel=""nofollow noopener"">multi-processing</a>. Say launch a main process and use <code>torch.multiprocessing</code> to spawn 4 subprocesses. Use <code>torch.multiprocessing.SimpleQueue</code> to pass grad tensors from sub-processes back to the main process, let the main process accumulate them, and then pass the result back to all subprocesses.</p><NewLine><p>The test below can serve as a example for <code>SimpleQueue</code>:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/2c4b4aa81bc8dba8272e9c7190edcaa3e114ec15/test/test_multiprocessing.py#L580-L600"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/2c4b4aa81bc8dba8272e9c7190edcaa3e114ec15/test/test_multiprocessing.py#L580-L600"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/2c4b4aa81bc8dba8272e9c7190edcaa3e114ec15/test/test_multiprocessing.py#L580-L600</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""580"" style=""counter-reset: li-counter 579 ;""><NewLine><li>def test_event_multiprocess(self):</li><NewLine><li>    event = torch.cuda.Event(enable_timing=False, interprocess=True)</li><NewLine><li>    self.assertTrue(event.query())</li><NewLine><li><NewLine></li><li>    ctx = mp.get_context('spawn')</li><NewLine><li>    p2c = ctx.SimpleQueue()</li><NewLine><li>    c2p = ctx.SimpleQueue()</li><NewLine><li>    p = ctx.Process(</li><NewLine><li>        target=TestMultiprocessing._test_event_multiprocess_child,</li><NewLine><li>        args=(event, p2c, c2p))</li><NewLine><li>    p.start()</li><NewLine><li><NewLine></li><li>    c2p.get()  # wait for until child process is ready</li><NewLine><li>    torch.cuda._sleep(50000000)  # spin for about 50 ms</li><NewLine><li>    event.record()</li><NewLine><li>    p2c.put(0)  # notify child event is recorded</li><NewLine><li><NewLine></li><li>    self.assertFalse(event.query())</li><NewLine><li>    c2p.get()  # wait for synchronization in child</li><NewLine><li>    self.assertTrue(event.query())</li><NewLine><li>    p.join()</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>, thanks for reply. I was following this example to use a average_gradients function, which calls all_reduce:<br/><NewLine><img alt=""截屏2020-09-15 下午1.49.16"" data-base62-sha1=""97pkweD1rFzjN7PZDZfA7JtduTM"" height=""132"" src=""https://discuss.pytorch.org/uploads/default/original/3X/3/f/3fe9f4f6d83102462e3ab9f1332d8e4dca502782.png"" width=""546""/><br/><NewLine>So to disable all_reduce, I just didn’t call this function when doing the training. And I notice there are two different examples, the other one uses DDP model and the sync-up step is done in backward computation, but I want to have a customized sync-up method, that’s why I choose the way with a separate average_gradients function.</p><NewLine><p>I have tried using SimpleQueue to pass some large data, but it seems slow when calling .get(), I’ll try to use it for passing grad tensors and see how it performs.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Yi_Zhang; <NewLine> ,"REPLY_DATE 1: September 15, 2020,  3:14am; <NewLine> REPLY_DATE 2: September 15, 2020,  5:54pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
95921,CUDA OOM while using torch.distributed.launch and no OOM training without it,2020-09-11T12:37:41.758Z,2,51,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello folks!</p><NewLine><p>I’m stuck with one very strange problem: I work with recently released <a href=""https://github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch"" rel=""nofollow noopener"">Scene Graph Benchmark</a>  and made it train on GQA, but I have one issue with that. It trains as expected when I use the following command: <code>CUDA_VISIBLE_DEVICES=0,1,2,3 python tools/relation_train_net.py --config-file ""configs/e2e_relation_X_101_32_8_FPN_1x.yaml"" </code> (it uses <code>batch_size = 2</code>). But when I decide to use another command with torch.distributed.launch (<code>CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --master_port 10025 --nproc_per_node=1 tools/relation_train_net.py --config-file ""configs/e2e_relation_X_101_32_8_FPN_1x.yaml""</code>) I’m getting <code>RuntimeError: CUDA out of memory</code>( it still has <code>batch_size = 2</code>). Initially I wanted to train it on  4 GPUs with <code> batch_size = 8</code>, but figured out about this problem. What can be the problem? And what should I do in order to properly  train it on 4 GPUs?</p><NewLine><p>My set up includes 4 2080ti so It has a plenty of memory.</p><NewLine></div>",https://discuss.pytorch.org/u/nullkatar,(Leon Kochiev),nullkatar,"September 11, 2020, 12:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""95921"" data-username=""nullkatar""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/nullkatar/40/25106_2.png"" width=""20""/> nullkatar:</div><NewLine><blockquote><NewLine><p>I’m stuck with one very strange problem: I work with recently released <a href=""https://github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch"" rel=""nofollow noopener"">Scene Graph Benchmark</a> and made it train on GQA, but I have one issue with that. It trains as expected when I use the following command: <code>CUDA_VISIBLE_DEVICES=0,1,2,3 python tools/relation_train_net.py --config-file ""configs/e2e_relation_X_101_32_8_FPN_1x.yaml"" </code> (it uses <code>batch_size = 2</code> ). But when I decide to use another command with torch.distributed.launch ( <code>CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --master_port 10025 --nproc_per_node=1 tools/relation_train_net.py --config-file ""configs/e2e_relation_X_101_32_8_FPN_1x.yaml""</code> ) I’m getting <code>RuntimeError: CUDA out of memory</code> ( it still has <code>batch_size = 2</code> ). Initially I wanted to train it on 4 GPUs with <code> batch_size = 8</code> , but figured out about this problem. What can be the problem? And what should I do in order to properly train it on 4 GPUs?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Hi Leon,</p><NewLine><p>What version of pytorch do you use?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello Alexander,</p><NewLine><p>1.6.0,  one which installs with <code>conda install torch</code>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you try to run the DDP command on a single node and GPU and check the memory usage?<br/><NewLine>I guess the code might create unnecessary CUDA contexts on other devices, but since the repository contains a lot of files I haven’t looked through all of them.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/agolynski; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/nullkatar; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 11, 2020,  5:07pm; <NewLine> REPLY_DATE 2: September 11, 2020, 10:00pm; <NewLine> REPLY_DATE 3: September 15, 2020,  7:58am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
96229,Distributed training failed without errors,2020-09-14T12:55:00.397Z,2,55,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I just change</p><NewLine><blockquote><NewLine><p><a href=""https://github.com/ZPdesu/SEAN"" rel=""nofollow noopener"">https://github.com/ZPdesu/SEAN</a> to distributed training.</p><NewLine></blockquote><NewLine><p>And I get a weird error.<br/><NewLine>It is no error，when I use one gpu.<br/><NewLine>But, It will stop without printing errors when I use multi gpus.<br/><NewLine>It will stop after Every 13 epoches by two gpus.</p><NewLine><p>What could be the cause of such a problem？</p><NewLine><p>my environments:<br/><NewLine>ubuntu: 16.04<br/><NewLine>gpu：nvidia-2080ti<br/><NewLine>cuda: 10.1 / 10.2<br/><NewLine>pytorch: 1.6.0 / 1.7.0<br/><NewLine>nccl: 2.4.8 / 2.7.6<br/><NewLine>python:3.6 / 3.7</p><NewLine></div>",https://discuss.pytorch.org/u/Feywell,(Feywell),Feywell,"September 14, 2020, 12:55pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/feywell"">@Feywell</a></p><NewLine><p>By “change <a href=""https://github.com/ZPdesu/SEAN"" rel=""nofollow noopener"">https://github.com/ZPdesu/SEAN</a> distributed training”, which distributed training API are you referring to (e.g., DistributedDataParallel, c10d, RPC)?</p><NewLine><p>Could you please share the code that uses distributed APIs?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just use DistributedDataParallel like this:</p><NewLine><pre><code class=""lang-auto"">    if opt.distributed:<NewLine>        cudnn.benchmark = True<NewLine>        opt.device = ""cuda""<NewLine><NewLine>        torch.cuda.set_device(opt.local_rank)<NewLine>        torch.distributed.init_process_group(backend=""nccl"",<NewLine>                                             init_method=""env://"") <NewLine> <NewLine>        synchronize()<NewLine></code></pre><NewLine><p>And model :</p><NewLine><pre><code class=""lang-auto"">         if opt.distributed:<NewLine>            self.pix2pix_model = torch.nn.parallel.DistributedDataParallel(self.pix2pix_model,<NewLine>                                                              device_ids=[opt.local_rank],<NewLine>                                                              output_device=opt.local_rank,<NewLine>                                                              find_unused_parameters=True)<NewLine>            self.pix2pix_model_on_one_gpu = self.pix2pix_model.module<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The initialization looks correct to me.</p><NewLine><blockquote><NewLine><p><code>self.pix2pix_model_on_one_gpu = self.pix2pix_model.module</code></p><NewLine></blockquote><NewLine><p>Question: why retrieving the local model from DDP model?</p><NewLine><blockquote><NewLine><p>It will stop after Every 13 epoches by two gpus.</p><NewLine></blockquote><NewLine><p>You mean the program crashes without any error message? How did you launch the two DDP processes?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>This line just be used to save model:<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/ZPdesu/SEAN/blob/04c7536ff3fecd2d1a09c9ae046a1144636033a5/trainers/pix2pix_trainer.py#L23"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/ZPdesu/SEAN/blob/04c7536ff3fecd2d1a09c9ae046a1144636033a5/trainers/pix2pix_trainer.py#L23"" rel=""nofollow noopener"" target=""_blank"">ZPdesu/SEAN/blob/04c7536ff3fecd2d1a09c9ae046a1144636033a5/trainers/pix2pix_trainer.py#L23</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""13"" style=""counter-reset: li-counter 12 ;""><NewLine><li>updates the weights of the network while reporting losses</li><NewLine><li>and the latest visuals to visualize the progress in training.</li><NewLine><li>""""""</li><NewLine><li><NewLine></li><li>def __init__(self, opt):</li><NewLine><li>    self.opt = opt</li><NewLine><li>    self.pix2pix_model = Pix2PixModel(opt)</li><NewLine><li>    if len(opt.gpu_ids) &gt; 0:</li><NewLine><li>        self.pix2pix_model = DataParallelWithCallback(self.pix2pix_model,</li><NewLine><li>                                                      device_ids=opt.gpu_ids)</li><NewLine><li class=""selected"">        self.pix2pix_model_on_one_gpu = self.pix2pix_model.module</li><NewLine><li>    else:</li><NewLine><li>        self.pix2pix_model_on_one_gpu = self.pix2pix_model</li><NewLine><li><NewLine></li><li>    self.generated = None</li><NewLine><li>    if opt.isTrain:</li><NewLine><li>        self.optimizer_G, self.optimizer_D = \</li><NewLine><li>            self.pix2pix_model_on_one_gpu.create_optimizers(opt)</li><NewLine><li>        self.old_lr = opt.lr</li><NewLine><li><NewLine></li><li>def run_generator_one_step(self, data):</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>The program will crash in differenct epoches by different number’s gpus without error message.<br/><NewLine>But It is ok in one gpu.<br/><NewLine>I use pytorch launch function:<br/><NewLine><code>python -m torch.distributed.launch --nproc_per_node=$NGPUS train.py</code></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Feywell; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Feywell; <NewLine> ,"REPLY_DATE 1: September 14, 2020,  6:54pm; <NewLine> REPLY_DATE 2: September 15, 2020,  2:14am; <NewLine> REPLY_DATE 3: September 15, 2020,  3:02am; <NewLine> REPLY_DATE 4: September 15, 2020,  4:23am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
95070,Why running multiprocessing is slower than one process on CPU,2020-09-03T12:59:07.240Z,0,50,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have an inference function <code>func</code> that takes on average 9 sec to run. But when I try to use multiprocessing to parallelize it (even using torch.multiprocessing) each inference takes on average 20 sec why is that ?</p><NewLine><p><strong>For info:</strong><br/><NewLine><code>func</code> is an inference function which takes in a <code>patient_name</code> and runs a torch model in inference on that patient’s data.</p><NewLine><pre><code class=""lang-auto"">device = torch.device(torch.device('cpu'))<NewLine><NewLine>def func(patient_name):<NewLine>    <NewLine>    data = np.load(my_dict[system_name]['data_path'])<NewLine>    model_state = torch.load(my_dict[system_name]['model_state_path'],map_location='cpu')<NewLine>    <NewLine>    model = my_net(my_dict[system_name]['HPs'])<NewLine>    model = model.to(device)<NewLine><NewLine>    model.load_state_dict(model_state)<NewLine>    model.eval()<NewLine>    <NewLine>    result = model(torch.FloatTensor(data).to(device))<NewLine>    return result<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">from torch.multiprocessing import pool<NewLine><NewLine>core_cnt = 10<NewLine><NewLine>pool = Pool(core_cnt)<NewLine>out = pool.starmap(func, pool_args)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/anrki,,anrki,"September 3, 2020,  1:01pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you add some profiling to see which part is the bottleneck?</p><NewLine><p>One reason might be PyTorch ops already launches many threads. So it possible that one process already saturates CPU resources.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: September 15, 2020,  3:26am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
95144,Using convert_sync_batchnorm let my code be deadlock,2020-09-04T06:51:14.096Z,0,34,"<div class=""post"" itemprop=""articleBody""><NewLine><p>when i use the code “net = torch.nn.SyncBatchNorm.convert_sync_batchnorm(net)” to replace BN with SyncBatchNorm, the code would be deadlock like this:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/358b80317d8478c933cee1c9d01cb23d8532eed2"" href=""https://discuss.pytorch.org/uploads/default/original/3X/3/5/358b80317d8478c933cee1c9d01cb23d8532eed2.png"" title=""image""><img alt=""image"" data-base62-sha1=""7DGbsQp6mNe9LirfyjD9LoWOEAG"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/3/5/358b80317d8478c933cee1c9d01cb23d8532eed2_2_10x10.png"" height=""229"" src=""https://discuss.pytorch.org/uploads/default/original/3X/3/5/358b80317d8478c933cee1c9d01cb23d8532eed2.png"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">880×293 30.8 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>it seems to be a problem with dataloader. And the relevant code is as follows<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/c072e4e054593df5765baa7da8b59ebffbe0eafb"" href=""https://discuss.pytorch.org/uploads/default/original/3X/c/0/c072e4e054593df5765baa7da8b59ebffbe0eafb.png"" title=""image""><img alt=""image"" data-base62-sha1=""rstPGHnsS8sQaHxuasxbgka4067"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/0/c072e4e054593df5765baa7da8b59ebffbe0eafb_2_10x10.png"" height=""346"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/0/c072e4e054593df5765baa7da8b59ebffbe0eafb_2_690x346.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/0/c072e4e054593df5765baa7da8b59ebffbe0eafb_2_690x346.png, https://discuss.pytorch.org/uploads/default/optimized/3X/c/0/c072e4e054593df5765baa7da8b59ebffbe0eafb_2_1035x519.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/c/0/c072e4e054593df5765baa7da8b59ebffbe0eafb.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1114×559 69.8 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/8d04d004987395dd45ab135fd9024c5fd100d7f0"" href=""https://discuss.pytorch.org/uploads/default/original/3X/8/d/8d04d004987395dd45ab135fd9024c5fd100d7f0.png"" title=""image""><img alt=""image"" data-base62-sha1=""k7vDjVJaSPtJA2Ngf65HSvhbCXS"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/d/8d04d004987395dd45ab135fd9024c5fd100d7f0_2_10x10.png"" height=""310"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/d/8d04d004987395dd45ab135fd9024c5fd100d7f0_2_690x310.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/d/8d04d004987395dd45ab135fd9024c5fd100d7f0_2_690x310.png, https://discuss.pytorch.org/uploads/default/optimized/3X/8/d/8d04d004987395dd45ab135fd9024c5fd100d7f0_2_1035x465.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/8/d/8d04d004987395dd45ab135fd9024c5fd100d7f0.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1066×480 45.7 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>Is there any kind person to help me？Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/mercer_Alex,(mercer Alex),mercer_Alex,"September 4, 2020,  6:51am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The difference between <code>BatchNorm</code> and <code>SyncBatchNorm</code> is that <code>SyncBatchNorm</code> uses <code>torch.distributed.all_reduce</code> in the backward pass.</p><NewLine><p>Two questions:</p><NewLine><ol><NewLine><li>What args and env vars did you pass to <code>init_process_group</code>?</li><NewLine><li>If you program, is there any other code that launches communication ops?</li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: September 15, 2020,  3:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
95244,Consistent result in distributed environment,2020-09-05T04:21:16.027Z,1,46,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi All,<br/><NewLine>I am a new user of torch. I would like to develop a dataloader module to feed data to torch C++ API. My focus is the C++ API.<br/><NewLine>My dataset is distributed among multiple compute nodes. Each node owns a disjoint portion of data WRT any other nodes. Let’s say I have 5 nodes. They own 200,200,200,300,100 examples of data respectively. Is there a way to achieve the same training result from training on a single node (the node owns all 1000 rows of data) or on a 3-node cluster (the data distribution is different from 5-node cluster case)?</p><NewLine><p>Is there a way to control/know how torch generates the indices when calling customDataset::get(index) in training and in scoring? Are the indices sequential (1,2,3,…) or random?</p><NewLine><p>If I deploy multiple workers to load data in a node, is there a way to know the caller’s thread id from customDataset::get()?</p><NewLine><pre><code>auto data_loader = torch::data::make_data_loader(std::move(dataset),<NewLine>    torch::data::DataLoaderOptions().batch_size(kBatchSize).workers(2));<NewLine></code></pre><NewLine><p>Thank you very much.</p><NewLine></div>",https://discuss.pytorch.org/u/lampadephoria,,lampadephoria,"September 5, 2020,  4:34am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>Can someone offer any insight please?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/vitalyfedyunin"">@VitalyFedyunin</a> <a class=""mention"" href=""/u/simonw"">@SimonW</a>  for dataloader<br/><NewLine>cc <a class=""mention"" href=""/u/glaringlee"">@glaringlee</a> for C++ API</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/lampadephoria; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: September 14, 2020,  6:54pm; <NewLine> REPLY_DATE 2: September 14, 2020,  8:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
94290,Issue model parallelism,2020-08-27T10:54:26.633Z,0,60,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am quite a pytorch newby, I hope this is the right place to post my issue. I am trying to train a transformer model with model parallelism following closely the megatron example from fairseq (just using complete transformer model instead gpt, same options including --fp16).<br/><NewLine></p><aside class=""onebox allowlistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""32""/><NewLine><a href=""https://github.com/pytorch/fairseq/tree/master/examples/megatron_11b"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""400"" src=""https://avatars2.githubusercontent.com/u/21003710?s=400&amp;v=4"" width=""400""/><NewLine><h3><a href=""https://github.com/pytorch/fairseq/tree/master/examples/megatron_11b"" rel=""nofollow noopener"" target=""_blank"">pytorch/fairseq</a></h3><NewLine><p>Facebook AI Research Sequence-to-Sequence Toolkit written in Python. - pytorch/fairseq</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>My setup is: two nodes with 6 GPUs (Titan RTX) each.<br/><NewLine>Pytorch 1.6<br/><NewLine>Cuda 10.1.243<br/><NewLine>Ubuntu 18.04 LTS</p><NewLine><p>The model trains, however only with 4 GPUs per node. When switching to 6 GPUs per node (plus tweaking the model/dictionary to ensure divisibility by number of GPUs) I get the following error on the second node (right when training should start):</p><NewLine><blockquote><NewLine><p>terminate called after throwing an instance of ‘c10::Error’<br/><NewLine>what(): CUDA error: misaligned address<br/><NewLine>Exception raised from create_event_internal at /pytorch/c10/cuda/CUDACachingAllocator.cpp:687 (most recent call first):<br/><NewLine>frame <span class=""hashtag"">#0:</span> c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7fc06b9c91e2 in /secondary/thies/.virtualenvs/pytorch-1.6/lib/python3.6/site-packages/torch/lib/libc10.so)<br/><NewLine>frame [<span class=""hashtag"">#1</span>]: c10::cuda::CUDACachingAllocator::raw_delete(void*) + 0xad2 (0x7fc06bc17f92 in /secondary/thies/.virtualenvs/pytorch-1.6/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)<br/><NewLine>frame <a href=""https://github.com/pytorch/fairseq/issues/2"" rel=""nofollow noopener"">#2</a>: c10::TensorImpl::release_resources() + 0x4d (0x7fc06b9b79cd in /secondary/thies/.virtualenvs/pytorch-1.6/lib/python3.6/site-packages/torch/lib/libc10.so)<br/><NewLine>frame [<span class=""hashtag"">#3</span>]: std::vector&lt;at::Tensor, std::allocatorat::Tensor &gt;::~vector() + 0x5c (0x7fc0b3262d1c in /secondary/thies/.virtualenvs/pytorch-1.6/lib/python3.6/site-packages/torch/lib/libtorch_python.so)<br/><NewLine>frame [<span class=""hashtag"">#4</span>]: torch::autograd::Engine::evaluate_function(std::shared_ptrtorch::autograd::GraphTask&amp;, torch::autograd::Node*, torch::autograd::InputBuffer&amp;, std::shared_ptrtorch::autograd::ReadyQueue const&amp;) + 0x16b2 (0x7fc0a5d8f6b2 in /secondary/thies/.virtualenvs/pytorch-1.6/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)<br/><NewLine>frame [<span class=""hashtag"">#5</span>]: torch::autograd::Engine::thread_main(std::shared_ptrtorch::autograd::GraphTask const&amp;) + 0x451 (0x7fc0a5d8ffa1 in /secondary/thies/.virtualenvs/pytorch-1.6/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)<br/><NewLine>frame [<span class=""hashtag"">#6</span>]: torch::autograd::Engine::thread_init(int, std::shared_ptrtorch::autograd::ReadyQueue const&amp;, bool) + 0x89 (0x7fc0a5d88119 in /secondary/thies/.virtualenvs/pytorch-1.6/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)<br/><NewLine>frame [<span class=""hashtag"">#7</span>]: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptrtorch::autograd::ReadyQueue const&amp;, bool) + 0x4a (0x7fc0b352834a in /secondary/thies/.virtualenvs/pytorch-1.6/lib/python3.6/site-packages/torch/lib/libtorch_python.so)<br/><NewLine>frame [<span class=""hashtag"">#8</span>]: + 0xbd6ef (0x7fc0b46826ef in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)<br/><NewLine>frame [<span class=""hashtag"">#9</span>]: + 0x76db (0x7fc0b85746db in /lib/x86_64-linux-gnu/libpthread.so.0)<br/><NewLine>frame [<span class=""hashtag"">#10</span>]: clone + 0x3f (0x7fc0b88ad88f in /lib/x86_64-linux-gnu/libc.so.6)</p><NewLine></blockquote><NewLine><p>It seems there is problem with the c10 library, however I get exactly the same error when adding the --ddp-backend=no_c10d option.</p><NewLine><p>When removing the fp16 option the model trains fine on the c10 backend.</p><NewLine></div>",https://discuss.pytorch.org/u/Thies1006,(Thies),Thies1006,"August 27, 2020, 11:07am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/thies1006"">@Thies1006</a>, as developers watching this channel have limited knowledge about fairseq models, it’s hard to tell what went wrong. Have you tried posting this as an issue in fairseq repo?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! Sorry for the delay. Yes, I posted this as well in the fairseq repo. It seems that when compiling pytorch from sources, this error disappears.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Thies1006; <NewLine> ,"REPLY_DATE 1: August 27, 2020,  2:38pm; <NewLine> REPLY_DATE 2: September 14, 2020,  3:16pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
95457,"TypeError in dist_autograd.backward(cid, [loss])",2020-09-07T14:46:29.702Z,1,56,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am using the code from <a href=""https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html"" rel=""nofollow noopener"">IMPLEMENTING A PARAMETER SERVER USING DISTRIBUTED RPC FRAMEWORK</a> tutorial, which should be straightforward to implement. However, I am receiving the TypeError (see following screenshot) while executing <strong>dist_autograd.backward(cid, [list])</strong> and can not get rid of it after trying a lot. Is this because the tensors need to be scalar?<br/><NewLine>Your help will be much appreciated.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/00a79573dad7c22b8eb7663dd2f0e0359249e05a"" href=""https://discuss.pytorch.org/uploads/default/original/3X/0/0/00a79573dad7c22b8eb7663dd2f0e0359249e05a.png"" title=""Screenshot from 2020-09-07 15-48-06""><img alt=""Screenshot from 2020-09-07 15-48-06"" data-base62-sha1=""5N2Tg5iavnAWzy2rlk87X0uChQ"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/0/00a79573dad7c22b8eb7663dd2f0e0359249e05a_2_10x10.png"" height=""126"" src=""https://discuss.pytorch.org/uploads/default/original/3X/0/0/00a79573dad7c22b8eb7663dd2f0e0359249e05a.png"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screenshot from 2020-09-07 15-48-06</span><span class=""informations"">815×149 17.9 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/Khairul_Mottakin,(Khairul Mottakin),Khairul_Mottakin,"September 7, 2020,  2:49pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/khairul_mottakin"">@Khairul_Mottakin</a>, looks like you are using PyTorch v1.4? The cid arg is added in v1.5 IIRC. Could you please try upgrade to the latest release v1.6?</p><NewLine><p>cc <a class=""mention"" href=""/u/rvarm1"">@rvarm1</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yah, it is working locally after updating pytorch version. Many many thanks <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> and <a class=""mention"" href=""/u/rvarm1"">@rvarm1</a> for your contribution and helping us to implement distributed system in our own ways.<br/><NewLine>While started training from remote worker, it says “RuntimeError: […/third_party/gloo/gloo/transport/tcp/pair.cc:769] connect [127.0.1.1]:14769: Connection refused”. The same issue had been raised by <a class=""mention"" href=""/u/oleg_ivanov"">@Oleg_Ivanov</a> in <a href=""https://discuss.pytorch.org/t/strange-behaviour-of-gloo-tcp-transport/66651"">here</a>. I am not sure whether it has been solved.  Should I use ""os.environ[‘GLOO_SOCKET_IFNAME’]=‘nonexist’ "" ?</p><NewLine><p>Can you suggest any tutorial for building such smaller cluster (2-3 remote workers with 1 master PS) to implement the Parameter Server using RPC of PyTorch?</p><NewLine><p>Thank you very much once again.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/khairul_mottakin"">@Khairul_Mottakin</a>, can you try printing out <code>GLOO_SOCKET_IFNAME</code>, <code>MASTER_ADDR</code>, and <code>MASTER_PORT</code> immediately before where <code>init_rpc</code> is called on all processes? And args did you pass to <code>init_rpc</code>?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Khairul_Mottakin; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: September 10, 2020,  3:06am; <NewLine> REPLY_DATE 2: September 13, 2020,  3:01pm; <NewLine> REPLY_DATE 3: September 14, 2020,  2:19pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
83679,Error: unrecognized arguments: &ndash;local_rank=1,2020-06-01T09:34:35.453Z,2,594,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have single machine with two GPUs.This errors occurred when I used this command ‘CUDA_VISIBLE_DEVICES=1,0 python -m torch.distributed.launch --nproc_per_node=2 train.py’ train my model parallelly.<br/><NewLine>Here’s my code, could anyone help me?</p><NewLine><pre><code class=""lang-auto"">os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'<NewLine>torch.distributed.init_process_group(backend='nccl')<NewLine><NewLine>parser = argparse.ArgumentParser(description='param')<NewLine>parser.add_argument('--iters', default=10,type=str)<NewLine>parser.add_argument('--data_size', default=2048,type=int)<NewLine>parser.add_argument('--batch_size', default=256,type=int)<NewLine>parser.add_argument('--loss_name', default='KL',type=str)<NewLine>parser.add_argument('--lr', default=0.01,type=int)<NewLine>parser.add_argument('--reg_param', default=0.1,type=int)<NewLine>parser.add_argument('--save_loss_path', default='./',type=str)<NewLine>parser.add_argument('--use_gpu', type=bool, default=False)<NewLine><NewLine><NewLine>def cleanup():<NewLine>    dist.destroy_process_group()<NewLine><NewLine><NewLine>def train(iters,<NewLine>          data_size,<NewLine>          batch_size,<NewLine>          loss_name,<NewLine>          lr,<NewLine>          reg_param,<NewLine>          save_loss_path,<NewLine>          use_gpu):<NewLine><NewLine>    save_loss_csv = save_loss_path + loss_name + '.csv'<NewLine>    create_csv_4_KL(path=save_loss_csv)<NewLine>    atlas = np.load(atlas_file)<NewLine><NewLine>    if use_gpu:<NewLine>        model = Model().to(device)<NewLine>        model = torch.nn.parallel.DistributedDataParallel(model)<NewLine>    else:<NewLine>        model = Model()<NewLine><NewLine>    opt = Adam(model.parameters(), lr=lr)<NewLine><NewLine>    if loss_name == 'KL':<NewLine>        from losses import KL_Divergence<NewLine><NewLine>        loss_fun = KL_Divergence<NewLine><NewLine>    elif loss_name == 'MSE':<NewLine>        from losses import mse_loss<NewLine><NewLine>        loss_fun = mse_loss<NewLine><NewLine>    elif loss_name == 'NCC':<NewLine>        from losses import ncc_loss<NewLine><NewLine>        loss_fun = ncc_loss<NewLine><NewLine>    else:<NewLine>        print(""There's no such a loss fuction {}"".format(loss_name))<NewLine><NewLine>    import losses<NewLine><NewLine>    Grad_loss = losses.gradient_loss<NewLine><NewLine>    train_generator = DataGenerater(json_path=json_path, data_size=data_size)<NewLine>    train_set = DataLoader(train_generator, batch_size=batch_size, shuffle=True, num_workers=16,<NewLine>                           sampler=DistributedSampler(train_generator))<NewLine><NewLine>    reg_param = reg_param<NewLine><NewLine>    fixed = torch.Tensor(atlas)<NewLine>    fixed.unsqueeze_(0)<NewLine>    fixed.unsqueeze_(0)<NewLine>    if use_gpu:<NewLine>        fixed = fixed.expand(batch_size, 1, 128, 128, 128).cuda()<NewLine>        fixed = fixed.expand(batch_size, 1, 128, 128, 128)<NewLine><NewLine>    fixed_norm = fixed / 255<NewLine>    if use_gpu:<NewLine>        fixed_norm = fixed_norm.to(device)<NewLine><NewLine>    for epoch in range(iters):<NewLine>        start_time = time.time()<NewLine>        loss_epoch = 0.0<NewLine>        for i, batch_moving in enumerate(train_set):<NewLine>            if use_gpu:<NewLine>                batch_moving_cuda = batch_moving.cuda()<NewLine>            else:<NewLine>                batch_moving_cuda = batch_moving<NewLine><NewLine>            batch_moving_cuda_norm = batch_moving_cuda / 255<NewLine><NewLine>            wrap, flow = model(batch_moving_cuda_norm, fixed_norm)<NewLine><NewLine>            loss = loss_fun(wrap, fixed_norm) + reg_param * Grad_loss(flow)<NewLine><NewLine>            loss_epoch += loss.item()<NewLine><NewLine>            opt.zero_grad()<NewLine>            loss.backward()<NewLine>            opt.step()<NewLine><NewLine>        append_csv(save_loss_csv,<NewLine>                   zip([[epoch + 1]], [loss_epoch]))<NewLine>        end_time = time.time()<NewLine>        loop_cost = end_time - start_time<NewLine>        print(""After [ {} ] seconds and {} epoches, selected the {} loss to train, the loss is [ {} ].""<NewLine>              .format(loop_cost, epoch + 1, loss_name, loss_epoch / (2048 / batch_size)))<NewLine><NewLine>    para_save_file = save_loss_path + 'res/' + 'MyModel-slice-{}-{}-{}-{}.pth'.format(loss_name, iters, reg_param, now)<NewLine>    if os.path.exists(para_save_file):<NewLine>        os.remove(para_save_file)<NewLine><NewLine>    torch.save(model.state_dict(), para_save_file)<NewLine>    print(""The model saved in {}"".format(para_save_file))<NewLine><NewLine>if __name__ == ""__main__"":<NewLine><NewLine>    args = parser.parse_args()<NewLine><NewLine>    now = datetime.now().date()<NewLine>    json_path = '/home/mamingrui/code/MyModel/brain.json'<NewLine>    atlas_file = '/home/mamingrui/data/atlas/atlas.npy',<NewLine><NewLine>    # initialize the process group<NewLine>    dist.init_process_group(""nccl"")<NewLine>    local_rank = torch.distributed.get_rank()<NewLine>    torch.cuda.set_device(local_rank)<NewLine>    device = torch.device(""cuda"", local_rank)<NewLine><NewLine>    train(iters=args.iters,<NewLine>          data_size=args.data_size,<NewLine>          batch_size=args.batch_size,<NewLine>          loss_name=args.loss_name,<NewLine>          lr=args.lr,<NewLine>          reg_param=args.reg_param,<NewLine>          save_loss_path=args.save_loss_path,<NewLine>          use_gpu=args.use_gpu)<NewLine>    cleanup()<NewLine></code></pre><NewLine><p>The errro report below</p><NewLine><pre><code class=""lang-auto"">*****************************************<NewLine>Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.<NewLine>*****************************************<NewLine>usage: train.py [-h] [--iters ITERS] [--data_size DATA_SIZE]<NewLine>                [--batch_size BATCH_SIZE] [--loss_name LOSS_NAME] [--lr LR]<NewLine>                [--reg_param REG_PARAM] [--save_loss_path SAVE_LOSS_PATH]<NewLine>                [--use_gpu USE_GPU]<NewLine>train.py: error: unrecognized arguments: --local_rank=0<NewLine>usage: train.py [-h] [--iters ITERS] [--data_size DATA_SIZE]<NewLine>                [--batch_size BATCH_SIZE] [--loss_name LOSS_NAME] [--lr LR]<NewLine>                [--reg_param REG_PARAM] [--save_loss_path SAVE_LOSS_PATH]<NewLine>                [--use_gpu USE_GPU]<NewLine>train.py: error: unrecognized arguments: --local_rank=1<NewLine>Traceback (most recent call last):<NewLine>  File ""/home/mamingrui/anaconda3/lib/python3.7/runpy.py"", line 193, in _run_module_as_main<NewLine>    ""__main__"", mod_spec)<NewLine>  File ""/home/mamingrui/anaconda3/lib/python3.7/runpy.py"", line 85, in _run_code<NewLine>    exec(code, run_globals)<NewLine>  File ""/home/mamingrui/anaconda3/lib/python3.7/site-packages/torch/distributed/launch.py"", line 253, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""/home/mamingrui/anaconda3/lib/python3.7/site-packages/torch/distributed/launch.py"", line 249, in main<NewLine>    cmd=cmd)<NewLine>subprocess.CalledProcessError: Command '['/home/mamingrui/anaconda3/bin/python', '-u', 'train.py', '--local_rank=1']' returned non-zero exit status 2.<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/mamemoryyy111,(Mamemoryyy111),mamemoryyy111,"June 1, 2020,  9:46am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The launcher will pass a <code>--local_rank</code> arg to your <code>train.py</code> script, so you need to add that to the <code>ArgumentParser</code>.</p><NewLine><p>Besides. you need to pass that <code>rank</code>, and <code>world_size</code>, and <code>init_method</code> (which basically contains MASTER_ADDR and MASTER_PORT) to <code>dist.init_process_group</code> either through arguments or env vars.</p><NewLine><p>This example might be helpful: <a href=""https://github.com/pytorch/examples/pull/743"" rel=""nofollow noopener"">https://github.com/pytorch/examples/pull/743</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I also met this problem, but I didn’t understand the answer upstairs. How did you solve this problem?<br/><NewLine>thank you</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The the error mentioned in the original post basically means that the launcher script tries to pass <code>--local_rank=1</code> as an argument to your script (i.e., <code>train.py</code> in this case). However, <code>train.py</code> is not configured to accept that argument.</p><NewLine><pre><code class=""lang-auto"">train.py: error: unrecognized arguments: --local_rank=1<NewLine></code></pre><NewLine><p>To solve this issue, you can add the following to your <code>ArgumentParser</code>.</p><NewLine><p><code>parser.add_argument(""--local_rank"", type=int, default=0)</code></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks.but after i add parser.add_argument(""–local_rank"", type=int, default=0),this errors also occurred.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xinj96; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Herb2333; <NewLine> ,"REPLY_DATE 1: June 2, 2020,  2:48am; <NewLine> REPLY_DATE 2: August 27, 2020,  1:38pm; <NewLine> REPLY_DATE 3: August 27, 2020,  2:45pm; <NewLine> REPLY_DATE 4: September 14, 2020,  2:14pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
62143,How to use my own sampler when I already use DistributedSampler?,2019-11-25T18:51:02.863Z,16,2799,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to use my custom sampler (for example, I need oversampling and I want to use this repo: <a href=""https://github.com/ufoym/imbalanced-dataset-sampler"" rel=""nofollow noopener"">https://github.com/ufoym/imbalanced-dataset-sampler</a>), but I already use <code>DistributedSampler</code> for <code>DataLoader</code>, because I use multi-gpu training. How can I pass to <code>DataLoader</code> one more sampler or maybe I can do it using <code>Dataset</code>? Currently, I use pretty simple <code>ImageFolder</code> dataset and it would be cool if I didn’t need to rewrite it.</p><NewLine></div>",https://discuss.pytorch.org/u/Oktai15,(Oktai Tatanov),Oktai15,"November 25, 2019,  6:51pm",3 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can implement a Wrapper class for your dataset and do the sampling there. For example, if you were to combine DistributedSampler with SubsetRandomSampler, you can implement a dataset wrapper like this:</p><NewLine><pre><code class=""lang-auto"">class DistributedIndicesWrapper(torch.utils.data.Dataset):<NewLine>    """"""<NewLine>    Utility wrapper so that torch.utils.data.distributed.DistributedSampler can work with train test splits<NewLine>    """"""<NewLine>    def __init__(self, dataset: torch.utils.data.Dataset, indices: torch.Tensor):<NewLine>        self.dataset = dataset<NewLine>        self.indices = indices<NewLine><NewLine>    def __len__(self):<NewLine>        return self.indices.size(0)<NewLine><NewLine>    def __getitem__(self, item):<NewLine>        # TODO: do the sampling here ?<NewLine>        idx = self.indices[item]<NewLine>        return self.dataset[idx]<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for idea, <a href=""https://discuss.pytorch.org/u/danielhavir"">danielhavir</a>!</p><NewLine><p>For everyone who is looking for oversampling wrapper under <code>FolderDataset</code>, you can look at this:</p><NewLine><pre><code class=""lang-python"">class OversamplingWrapper(torch.utils.data.Dataset):<NewLine>    def __init__(self, folder_dataset, oversampling_size=1000):<NewLine>        self.folder_dataset = folder_dataset<NewLine>        self.oversampling_size = oversampling_size<NewLine>        self.num_classes = len(folder_dataset.classes)<NewLine><NewLine>        self.class_idx_to_sample_ids = {i: [] for i in range(self.num_classes)}<NewLine>        for idx, (_, class_id) in enumerate(folder_dataset.samples):<NewLine>            self.class_idx_to_sample_ids[class_id].append(idx)<NewLine><NewLine>    def __len__(self):<NewLine>        return self.num_classes * self.oversampling_size<NewLine><NewLine>    def __getitem__(self, index):<NewLine>        class_id = index % self.num_classes<NewLine>        sample_idx = random.sample(self.class_idx_to_sample_ids[class_id], 1)<NewLine>        return self.folder_dataset[sample_idx[0]]<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’ve got a similar goal for distributed training only with WeightedRandomSampler and a custom torch.utils.data.Dataset .<br/><NewLine>I have 2 classes, positive (say 100) and negative (say 1000).<br/><NewLine>Each epoch, I want all positive examples, and an equal number of random negative samples.</p><NewLine><pre><code class=""lang-auto"">ds = custom_dataset(args)<NewLine> <NewLine>weights = 1. /torch.tensor([ds.n_positive, ds.n_negative], dtype=torch.float)<NewLine>samples_weights = weights[ds.all_targets]<NewLine>WRsampler = WeightedRandomSampler(<NewLine>        weights=samples_weights,<NewLine>        num_samples=len(samples_weights),<NewLine>        replacement=True<NewLine>        )<NewLine></code></pre><NewLine><p>But I can’t figure this out.<br/><NewLine>If I want random negative samples regardless of batch_size, wouldn’t I need a dataloader wrapper? How would I go about a dataloader wrapper?<br/><NewLine>Any hints or suggestions, much appreciated.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Help us <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>, you’re our only hope. (and have you considered running for president 2020?)</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you using <code>nn.DistributedDataParallel</code> as shown in <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"">this tutorial</a>?<br/><NewLine>If so, I assume you are using a <code>DistributedSampler</code> to only use a valid subset of your dataset in each process?</p><NewLine><p>In that case we should be able to add weighted sampling into the sampler, but let me know, if my assumptions are correct before diving into it. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""62143"" data-username=""James_Condon""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/james_condon/40/10595_2.png"" width=""20""/> James_Condon:</div><NewLine><blockquote><NewLine><p>and have you considered running for president 2020?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Hahaha, president of discuss-land? <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve been using pytorch lightning with the ‘ddp’ distributed data parallel backend and <code>torch.utils.data.distributed.DistributedSampler(ds)</code> as the DataLoader sampler argument. To be honest, I’m unsure of the subsetting that this represents, despite having a look at the source code, but happy to learn. Also happy to refactor for a clean, robust solution. Cheers</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not familiar with lightning, but I assume it’s just using the <code>torch.utils.data.DistributedSampler</code>.<br/><NewLine>Based on the implementation of <code>DistributedSampler</code> and <code>WeightedRandomSampler</code>, this code might work:</p><NewLine><pre><code class=""lang-python"">class DistributedWeightedSampler(Sampler):<NewLine>    def __init__(self, dataset, num_replicas=None, rank=None, replacement=True):<NewLine>        if num_replicas is None:<NewLine>            if not dist.is_available():<NewLine>                raise RuntimeError(""Requires distributed package to be available"")<NewLine>            num_replicas = dist.get_world_size()<NewLine>        if rank is None:<NewLine>            if not dist.is_available():<NewLine>                raise RuntimeError(""Requires distributed package to be available"")<NewLine>            rank = dist.get_rank()<NewLine>        self.dataset = dataset<NewLine>        self.num_replicas = num_replicas<NewLine>        self.rank = rank<NewLine>        self.epoch = 0<NewLine>        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))<NewLine>        self.total_size = self.num_samples * self.num_replicas<NewLine>        self.replacement = replacement<NewLine><NewLine><NewLine>    def calculate_weights(self, targets):<NewLine>        class_sample_count = torch.tensor(<NewLine>            [(targets == t).sum() for t in torch.unique(targets, sorted=True)])<NewLine>        weight = 1. / class_sample_count.double()<NewLine>        samples_weight = torch.tensor([weight[t] for t in targets])<NewLine>        return samples_weight<NewLine><NewLine>    def __iter__(self):<NewLine>        # deterministically shuffle based on epoch<NewLine>        g = torch.Generator()<NewLine>        g.manual_seed(self.epoch)<NewLine>        if self.shuffle:<NewLine>            indices = torch.randperm(len(self.dataset), generator=g).tolist()<NewLine>        else:<NewLine>            indices = list(range(len(self.dataset)))<NewLine><NewLine>        # add extra samples to make it evenly divisible<NewLine>        indices += indices[:(self.total_size - len(indices))]<NewLine>        assert len(indices) == self.total_size<NewLine><NewLine>        # subsample<NewLine>        indices = indices[self.rank:self.total_size:self.num_replicas]<NewLine>        assert len(indices) == self.num_samples<NewLine><NewLine>        # get targets (you can alternatively pass them in __init__, if this op is expensive)<NewLine>        targets = self.dataset.targets<NewLine>        targets = targets[self.rank:self.total_size:self.num_replicas]<NewLine>        assert len(targets) == self.num_samples<NewLine>        weights = self.calculate_weights(targets)<NewLine><NewLine>        return iter(torch.multinomial(weights, self.num_samples, self.replacement).tollist())<NewLine><NewLine>    def __len__(self):<NewLine>        return self.num_samples<NewLine><NewLine>    def set_epoch(self, epoch):<NewLine>        self.epoch = epoch<NewLine></code></pre><NewLine><p>This <code>DistributedWeightedSampler</code> will get the <code>targets</code> of your dataset, create the weights for the current split, and use <code>torch.multinomial</code> to sample from these samples as is done in the <code>WeightedRandomSampler</code>.<br/><NewLine>This code is untested and I just hacked it together, so please let me know, if this would work at all or if you are seeing any issues.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Above and beyond, as usual. Just need to verify whats doing through but pretty sure this has done the trick. Thanks a million!</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have a slightly different but related question here. Is it possible to have a SequentialSampler followed by a DistributedSampler? I am not sure if this would work when using <strong>multi GPUs</strong> as data could have been split randomly already.</p><NewLine><p>The reason I am asking this question is that I would like to create a single dataloader from multiple data sources, and I would like each mini-batch of the dataloader to contain only one kind of data. This can easily be done if I create one dataloader for every single data source (and when training go through each of them one by one), but for my purpose here I am wondering if something similar can be achieved by only using one dataloader for all data sources.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks again. This is working great but it seems to be responsible for processes hanging / getting stuck on GPU when main script is terminated or ‘early-stopped’.</p><NewLine><pre><code class=""lang-auto"">~python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 18 leaked semaphores to clean up at shutdown<NewLine></code></pre><NewLine><p>Any hints as to how this can be cleaned up?<br/><NewLine>Cheers</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Which code hangs or yields the semaphore warning?<br/><NewLine>Is is the <code>DistributedWeightedSampler</code>?</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>If the main training script gets early stopped or I keyboard interrupt it, my nvidia-smi memory usage on one of my (two) gpus stays almost full and VGPU stays at 100%. semaphore warning is after <code>pkill python</code>. Doesn’t seem to happen if I’m using any other sampler.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the notice. As I haven’t tested the code, it might yield these side effects.<br/><NewLine>Could you take a look at <a href=""https://github.com/pytorch/pytorch/issues/23430"">this issue</a> and see, it this approach would better fit your needs?</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>Cheers. Pretty sure this was a rookie error, forgot a torch.no_grad() for my val loop. Hasn’t been an issue since adding that in. Thanks.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Superb, thanks very much for this <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>.<br/><NewLine>Maybe I’m missing something here, but in the __iter__ function, shouldn’t<br/><NewLine>targets = targets[indices]<br/><NewLine>or similar, rather than what we currently have:<br/><NewLine>targets = targets[self.rank:self.total_size:self.num_replicas]<br/><NewLine>otherwise don’t we just leave indicies hanging in the breeze and not doing anything?  Just realised we still need a way to map the selected targets back to the original dataset indicies.  I will post when I have something…<br/><NewLine>thanks again for your awesomeness</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, you are probably right. <code>indices</code> has already the extra samples and also is already subsampled, so that it should be used instead of indexing the <code>targets</code> directly.</p><NewLine><p>I’ll observe the linked issue on GitHub, as it should provide a cleaner way of implementing this behavior. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks, I just looked at linked issue, and agree that it is exactly what I was looking for.  It looks like it is not ready yet.  so here is what I will use in the meantime …</p><NewLine><pre><code>def __iter__(self):<NewLine>    # deterministically shuffle based on epoch<NewLine>    g = torch.Generator()<NewLine>    g.manual_seed(self.epoch)<NewLine>    if self.shuffle:<NewLine>        indices = torch.randperm(len(self.dataset), generator=g).tolist()<NewLine>    else:<NewLine>        indices = list(range(len(self.dataset)))<NewLine><NewLine>    # add extra samples to make it evenly divisible<NewLine>    indices += indices[:(self.total_size - len(indices))]<NewLine>    assert len(indices) == self.total_size<NewLine><NewLine>    # subsample<NewLine>    indices = indices[self.rank:self.total_size:self.num_replicas]<NewLine>    assert len(indices) == self.num_samples<NewLine><NewLine>    # get targets (you can alternatively pass them in __init__, if this op is expensive)<NewLine>    targets = self.dataset.targets<NewLine>    # select only the wanted targets for this subsample<NewLine>    targets = torch.tensor(targets)[indices]<NewLine>    assert len(targets) == self.num_samples<NewLine>    # randomly sample this subset, producing balanced classes<NewLine>    weights = self.calculate_weights(targets)<NewLine>    subsample_balanced_indicies = torch.multinomial(weights, self.num_samples, self.replacement)<NewLine>    # now map these target indicies back to the original dataset index...<NewLine>    dataset_indices = torch.tensor(indices)[subsample_balanced_indicies]<NewLine><NewLine>    return iter(dataset_indices.tolist())</code></pre><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p>Interesting thread.<br/><NewLine>If we want to do a simple random sampler, wouldn’t something like this work? We just take a random sample of our whole dataset on point of dataset creation, than wrap our Dataset class in DistributedSampler, which would take care of splitting it among processes?<br/><NewLine>We make reload  of the Dataset every epoch so a new sample is drawn.</p><NewLine><pre><code class=""lang-auto"">class Dataset():<NewLine>    def __init__(self, seed, sample_size, *args, **kwargs):<NewLine>        random.seed(seed)<NewLine>        random.shuffle(self.ids)<NewLine>        self.ids = self.ids[:sample_size]<NewLine><NewLine>dataset = Dataset()<NewLine>dataset = torch.utils.data.distributed.DistributedSampler(dataset)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>This <code>Dataset</code> would sample the same data points in each process wouldn’t it?<br/><NewLine>The original <code>DistributedSampler</code> will split the indices such that each process would draw its own samples.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/danielhavir; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Oktai15; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/James_Condon; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/James_Condon; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/James_Condon; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/James_Condon; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Eddie_Wu; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/James_Condon; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/James_Condon; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/James_Condon; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/glyn; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/glyn; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/Westerby; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: November 25, 2019,  7:23pm; <NewLine> REPLY_DATE 2: November 26, 2019, 11:43am; <NewLine> REPLY_DATE 3: February 15, 2020,  7:12am; <NewLine> REPLY_DATE 4: February 16, 2020,  5:08am; <NewLine> REPLY_DATE 5: February 16, 2020,  5:44am; <NewLine> REPLY_DATE 6: February 16, 2020,  6:09am; <NewLine> REPLY_DATE 7: February 16, 2020,  8:42am; <NewLine> REPLY_DATE 8: February 16, 2020, 10:37am; <NewLine> REPLY_DATE 9: February 21, 2020,  1:13am; <NewLine> REPLY_DATE 10: March 14, 2020, 12:05am; <NewLine> REPLY_DATE 11: March 14, 2020,  8:49pm; <NewLine> REPLY_DATE 12: March 15, 2020,  1:38pm; <NewLine> REPLY_DATE 13: March 15, 2020, 11:05pm; <NewLine> REPLY_DATE 14: March 18, 2020,  1:33pm; <NewLine> REPLY_DATE 15: March 24, 2020,  3:53am; <NewLine> REPLY_DATE 16: March 24, 2020,  4:22am; <NewLine> REPLY_DATE 17: March 24, 2020,  4:50am; <NewLine> REPLY_DATE 18: April 15, 2020,  4:59pm; <NewLine> REPLY_DATE 19: April 16, 2020,  4:27am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: 3 Likes; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 2 Likes; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 3 Likes; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: 2 Likes; <NewLine> REPLY 18 LIKES: ; <NewLine> REPLY 19 LIKES: ; <NewLine> 
42791,Trouble with multiple GPU setup - thread lock,2019-04-17T03:06:09.563Z,3,603,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I have spent the past day trying to figure out how to use multiple GPUs. In theory, parallelizing models across multiple GPUs is supposed to be as as easy as simply wrapping models with <code>nn.DataParallel</code>. However, I have found that this does not work for me. To use the most simple and canonical thing I could find for proof of this, I ran the code in the <a href=""https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"" rel=""nofollow noopener"">Data Parallelism tutorial</a>, line for line. The output is as follows - it is the same output that I get every time I try to run Pytorch with multiple GPUs:</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>KeyboardInterrupt                         Traceback (most recent call last)<NewLine>&lt;ipython-input-3-0f0d83e9ef13&gt; in &lt;module&gt;<NewLine>      1 for data in rand_loader:<NewLine>      2     input = data.to(device)<NewLine>----&gt; 3     output = model(input)<NewLine>      4     print(""Outside: input size"", input.size(),<NewLine>      5           ""output_size"", output.size())<NewLine><NewLine>/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)<NewLine>    487             result = self._slow_forward(*input, **kwargs)<NewLine>    488         else:<NewLine>--&gt; 489             result = self.forward(*input, **kwargs)<NewLine>    490         for hook in self._forward_hooks.values():<NewLine>    491             hook_result = hook(self, input, result)<NewLine><NewLine>/usr/local/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py in forward(self, *inputs, **kwargs)<NewLine>    141             return self.module(*inputs[0], **kwargs[0])<NewLine>    142         replicas = self.replicate(self.module, self.device_ids[:len(inputs)])<NewLine>--&gt; 143         outputs = self.parallel_apply(replicas, inputs, kwargs)<NewLine>    144         return self.gather(outputs, self.output_device)<NewLine>    145 <NewLine><NewLine>/usr/local/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py in parallel_apply(self, replicas, inputs, kwargs)<NewLine>    151 <NewLine>    152     def parallel_apply(self, replicas, inputs, kwargs):<NewLine>--&gt; 153         return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])<NewLine>    154 <NewLine>    155     def gather(self, outputs, output_device):<NewLine><NewLine>/usr/local/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py in parallel_apply(modules, inputs, kwargs_tup, devices)<NewLine>     73             thread.start()<NewLine>     74         for thread in threads:<NewLine>---&gt; 75             thread.join()<NewLine>     76     else:<NewLine>     77         _worker(0, modules[0], inputs[0], kwargs_tup[0], devices[0])<NewLine><NewLine>/usr/local/lib/python3.6/threading.py in join(self, timeout)<NewLine>   1054 <NewLine>   1055         if timeout is None:<NewLine>-&gt; 1056             self._wait_for_tstate_lock()<NewLine>   1057         else:<NewLine>   1058             # the behavior of a negative timeout isn't documented, but<NewLine><NewLine>/usr/local/lib/python3.6/threading.py in _wait_for_tstate_lock(self, block, timeout)<NewLine>   1070         if lock is None:  # already determined that the C code is done<NewLine>   1071             assert self._is_stopped<NewLine>-&gt; 1072         elif lock.acquire(block, timeout):<NewLine>   1073             lock.release()<NewLine>   1074             self._stop()<NewLine><NewLine>KeyboardInterrupt: <NewLine></code></pre><NewLine><p>Note that it hangs - I have to keyboard interrupt to stop. And the error is the same every time - some sort of deadlock is entered into, although I do not understand how or why.</p><NewLine><p>Some information about my system:<br/><NewLine>Operating System: Ubuntu 16.04<br/><NewLine>GPUS: 4 1080tis<br/><NewLine>Pytorch version: 1.01<br/><NewLine>CUDA version: 10.0<br/><NewLine>NVIDIA Driver: 415</p><NewLine><p>I have tried everything from only having a specific permutation of my GPUs be visible to CUDA to reinstalling everything related to CUDA but can’t figure out why I cannot run with multiple GPUs. If anyone could point me in the right direction, it would be greatly appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/Jeffrey_Wang,(Jeffrey Wang),Jeffrey_Wang,"April 17, 2019,  3:07am",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Was this ever resolved? I’m facing the same problem.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Which version of PyTorch are you using? Can you share a self-contained repro?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> was this issue ever solved. I have seen a lot of threads on pytorch forums regarding NCCL deadlock but I didn’t find any solution.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I was not able to solve this issue, and my rig is currently disassembled and across the country so no way I can be of much help unfortunately.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems this issue was not solved and we don’t have a code snippet to reproduce this issue.<br/><NewLine>I would generally recommend to use the latest stable version (and try out the nightly, if possible) using the latest CUDA, NCCL etc. versions. If the error is still observable, an executable code snippet to reproduce this issue is very helpful.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried the pytorch-nightly which uses NCCL 2.7.6. I have not faced the deadlock again yet. Thanks <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>.</p><NewLine><p><a class=""mention"" href=""/u/jeffrey_wang"">@Jeffrey_Wang</a> you might want to try that out.</p><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> what could be issue with the previous version?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""42791"" data-username=""iamshnik""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/iamshnik/40/28740_2.png"" width=""20""/> iamshnik:</div><NewLine><blockquote><NewLine><p>what could be issue with the previous version?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Nothing we are aware of, i.e. we haven’t seen deadlocks in NCCL 2.4 before.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jtyo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iamshnik; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Jeffrey_Wang; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/iamshnik; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 10, 2020,  9:00pm; <NewLine> REPLY_DATE 2: September 11, 2020,  1:12am; <NewLine> REPLY_DATE 3: September 13, 2020,  3:30pm; <NewLine> REPLY_DATE 4: September 13, 2020,  5:04pm; <NewLine> REPLY_DATE 5: September 13, 2020, 10:04pm; <NewLine> REPLY_DATE 6: September 14, 2020,  5:15am; <NewLine> REPLY_DATE 7: September 14, 2020,  7:44am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
95575,Memory leak of rank 0 in distributed training,2020-09-08T16:10:46.908Z,4,92,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I come across a problem when I try to train my model in a distributed way.</p><NewLine><p>In summary, the problem is how to solve the memory leak in rank 0.</p><NewLine><p>When I run the code in single gpu, it works well and occupies 10.3G GPU memory. My GPU is 2080ti 11GB. But when I run the code in the distributed way, OOM occurred.<br/><NewLine>I build a class named <code>Trainer</code>, then initiate dataset and model inside. The rough code is showed below. The process contains several backward operations.</p><NewLine><pre><code class=""lang-auto"">class Trainer():<NewLine>    def __init__(self):<NewLine>        self.data_initial()<NewLine>        self.model_initial()<NewLine>        self.train()<NewLine><NewLine>   def data_initial(self):<NewLine>        source_data = DataSet(.....)<NewLine>        source_sampler = data.distributed.DistributedSampler(source_data, seed=1234)<NewLine>        source_dataloader = data.DataLoader(source_data, batch_size=..., num_workers=4,<NewLine>            pin_memory=False, sampler=source_sampler)<NewLine>        self.source_data = enumerate(source_dataloader)<NewLine><NewLine>        target_data = DataSet(.....)<NewLine>        target_sampler = data.distributed.DistributedSampler(target_data, seed=1234)<NewLine>        target_dataloader = data.DataLoader(source_data, batch_size=..., num_workers=4,<NewLine>            pin_memory=False, sampler=target_sampler)<NewLine>        self.target_data = enumerate(target_dataloader)<NewLine><NewLine>    def model_initial(self):<NewLine>        # build backbone<NewLine>        rank = dist.get_rank()<NewLine>        self.backbone = ResNet().cuda(rank)<NewLine>        self.backbone = DDP(self.backbone, device_ids=[rank])<NewLine>        <NewLine>        # restore_part<NewLine>        the parameter succeeds restore. I have checked it.<NewLine>        self.backbone.train()<NewLine><NewLine>        # classifier<NewLine>        self.classifier = Classifier(...).cuda(rank)<NewLine>        self.classifier = DDP(self.classifier, device_ids=[rank])<NewLine>        self.classifier.train()<NewLine><NewLine>        # optimizer<NewLine>        self.backbone_optimizer = optim.SGD(self.backbone.parameters(), ...)<NewLine>        self.backbone_optimizer.zero_grad()<NewLine><NewLine>        self.classifier_optimizer = optim.Adam(self.classifier.parameters(), ...)<NewLine>        self.classifier_optimizer.zero_grad()<NewLine><NewLine>   def train(self):<NewLine>        # pytorch prework<NewLine>        rank = dist.get_rank()<NewLine>        self.criterion = torch.nn.BCEWithLogitsLoss().cuda(rank)<NewLine>        <NewLine>        for i in range(1, self.config.num_steps):<NewLine>            # get data<NewLine>            c, batch = self.source_data.__next__()<NewLine>            image, label, _, _ = batch<NewLine>            _, batch = self.target_data.__next__()<NewLine>            image_t, label_t, _, _ = batch<NewLine><NewLine>            self.step(i, image, label, image_t, label_t, loss_dic)<NewLine><NewLine>            gc.collect()<NewLine><NewLine>def step(self, i, image, label, image_t, label_t, loss_dic):<NewLine>        rank = dist.get_rank()<NewLine><NewLine>        self.backbone_optimizer.zero_grad()<NewLine>        self.classifier_optimizer.zero_grad()<NewLine><NewLine>        # supervised learning for source<NewLine>        image = Variable(image).cuda(rank)<NewLine>        x = self.backbone(image)<NewLine>        y1, _ = self.classifier(x)<NewLine>        loss = self.criterion(y, label.long().cuda(rank))<NewLine>        loss.backward()<NewLine>      <NewLine><NewLine>        for para in self.classifier.parameters():<NewLine>            para.requires_grad = False<NewLine>        image_t = Variable(image_t).cuda(rank)<NewLine>        x = self.backbone(image_t)<NewLine>        _, y2 = self.classifier(x)<NewLine><NewLine>        label2 = Variable(...).cuda(rank)<NewLine>        loss = self.criterion(y2, label2)<NewLine>        loss.backward()<NewLine><NewLine>        ###<NewLine>        # optimize the parameter<NewLine>        self.backbone_optimizer.step()<NewLine>        self.classifier_optimizer.step()<NewLine><NewLine>        ###<NewLine>        # recycle variable<NewLine>        #delete some variable in intermedia process<NewLine>        del x .....<NewLine>        torch.cuda.empty_cache()<NewLine></code></pre><NewLine><p>My main function to call distributed training is showed below.</p><NewLine><pre><code class=""lang-auto"">def main():<NewLine>    world_size = torch.cuda.device_count()<NewLine>    mp.spawn(sub_process, args=(world_size), nprocs=world_size, join=True)<NewLine><NewLine>def sub_process(rank, world_size):<NewLine>    set_up(rank, world_size)<NewLine>    trainer = Trainer()<NewLine>    cleanup()<NewLine><NewLine>def set_up(rank, world_size):<NewLine>    os.environ['MASTER_ADDR'] = '127.0.0.113'<NewLine>    os.environ['MASTER_PORT'] = '12355'<NewLine>    dist.init_process_group(""nccl"", rank=rank, world_size=world_size) <NewLine><NewLine>def cleanup():<NewLine>    dist.destroy_process_group()<NewLine></code></pre><NewLine><p>So in process 0(rank 0), the first step training is ok, but in the second step. OOM occurred.</p><NewLine><pre><code class=""lang-auto"">-- Process 0 terminated with the following error:<NewLine>Traceback (most recent call last):<NewLine>  File ""/home/xx/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 20, in _wrap<NewLine>    fn(i, *args)<NewLine>.......<NewLine>File ""/home/xx/code/xxx.py"", line 225, in step<NewLine>    loss_aux = self.seg_criterion(out_aux, label.long().cuda(rank))<NewLine>  File ""/home/xx/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/xx/code/multitask/utils/utils.py"", line 44, in forward<NewLine>    predict = predict[target_mask.view(n, h, w, 1).repeat(1, 1, 1, c)].view(-1, c)<NewLine>RuntimeError: CUDA out of memory. Tried to allocate 536.00 MiB (GPU 0; 10.73 GiB total capacity; 8.30 GiB already allocated; 198.56 MiB free; 9.01 GiB reserved in total by PyTorch)<NewLine></code></pre><NewLine><p>I know that maybe the memory leak occurred. But I have no experience about distributed one. How should I solve this problem</p><NewLine></div>",https://discuss.pytorch.org/u/JIE_LIU,(JIE LIU),JIE_LIU,"September 9, 2020,  1:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Does anyone know how to solve it? It’s a little emergent. In summary, how to solve the memory leak in rank 0.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/jie_liu"">@JIE_LIU</a>, which version of PyTorch are you using? If it is v1.6, I suspect it is  due to the DDP comm bucket reconstruction algorithm temporarily boost memory consumption, and then hit the OOM problem.</p><NewLine><p>cc <a class=""mention"" href=""/u/yanli_zhao"">@Yanli_Zhao</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>sure, it’s 1.6.0. How can I avoid this situation?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think there is a decent way to get rid of it in v1.6. One hacky solution might be introducing a tiny unused parameter in the model (e.g., <code>self.unused = nn.Linear(1, 1)</code>), and then set <code>find_unused_parameters=True</code> in DDP ctor, which would disable bucket rebuilt.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/v1.6.0/torch/csrc/distributed/c10d/reducer.cpp#L381"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/v1.6.0/torch/csrc/distributed/c10d/reducer.cpp#L381"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/v1.6.0/torch/csrc/distributed/c10d/reducer.cpp#L381</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""371"" style=""counter-reset: li-counter 370 ;""><NewLine><li><NewLine></li><li>// Rebuild bucket only if 1) it is the first time to rebuild bucket 2)</li><NewLine><li>// unused_parameters_ is empty, currently it does not support when there are</li><NewLine><li>// unused parameters 3) this backward pass needs to run allreduce. Here, we</li><NewLine><li>// just dump tensors and their parameter indices into rebuilt_params_ and</li><NewLine><li>// rebuilt_param_indices_ based on gradient arriving order, and then at the</li><NewLine><li>// end of finalize_backward(), buckets will be rebuilt based on</li><NewLine><li>// rebuilt_params_ and rebuilt_param_indices_, and then will be broadcasted</li><NewLine><li>// and intialized. Also we only need to dump tensors and parameter indcies of</li><NewLine><li>// one replica.</li><NewLine><li class=""selected"">if (!has_rebuilt_bucket_ &amp;&amp; unused_parameters_.empty() &amp;&amp;</li><NewLine><li>    index.replica_index == 0) {</li><NewLine><li>  rebuilt_params_.push_back(</li><NewLine><li>      replicas_[index.replica_index][index.variable_index]);</li><NewLine><li>  rebuilt_param_indices_.push_back(index.variable_index);</li><NewLine><li>}</li><NewLine><li><NewLine></li><li>// If there are model parameters that went unused when computing the model</li><NewLine><li>// output, they won't be part of the autograd graph, and won't receive</li><NewLine><li>// gradients. These parameters are discovered in the `prepare_for_backward`</li><NewLine><li>// function and their indexes stored in the `unused_parameters_` vector.</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>This looks like a regression to me. <a class=""mention"" href=""/u/yanli_zhao"">@Yanli_Zhao</a> has a some recent work to reduce DDP memory footprint and hopefully that can help.</p><NewLine><p>BTW, if possible, can you try the same script with PyTorch v1.5, it will help to confirm if bucket reconstruction is indeed the culprit.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your solution. I’ll try this later and give you feedback</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/JIE_LIU; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/JIE_LIU; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/JIE_LIU; <NewLine> ,"REPLY_DATE 1: September 9, 2020,  1:39am; <NewLine> REPLY_DATE 2: September 10, 2020,  2:54am; <NewLine> REPLY_DATE 3: September 10, 2020,  3:05am; <NewLine> REPLY_DATE 4: September 10, 2020,  3:54pm; <NewLine> REPLY_DATE 5: September 13, 2020,  9:39am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
95900,"When using distribute, how to write log?",2020-09-11T09:14:50.460Z,0,29,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I implement my code modified from <a href=""https://github.com/pytorch/examples/blob/master/imagenet/main.py"" rel=""nofollow noopener"">https://github.com/pytorch/examples/blob/master/imagenet/main.py</a> ,and now I have 2 problems:<br/><NewLine>1.Dose this code is now the best way to do distribution?<br/><NewLine>2.How to to write logs? In my opinion, We should find the major process and wrtie, but could the mp.spawn can do this?<br/><NewLine>Many thanks to your replies!</p><NewLine></div>",https://discuss.pytorch.org/u/wangyilan,,wangyilan,"September 11, 2020,  9:14am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>1.Dose this code is now the best way to do distribution?</p><NewLine></blockquote><NewLine><p>This depends on the application requirements. For available tools, see this: <a href=""https://pytorch.org/tutorials/beginner/dist_overview.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/beginner/dist_overview.html</a></p><NewLine><blockquote><NewLine><p>2.How to to write logs? In my opinion, We should find the major process and wrtie, but could the mp.spawn can do this?</p><NewLine></blockquote><NewLine><p>You can use the rank to control which process does the log, e.g.:</p><NewLine><pre><code class=""lang-python"">import torch.distributed as dist<NewLine><NewLine>if dist.get_rank() == 0:<NewLine>    # do log<NewLine></code></pre><NewLine><p>If you are using RPC, the counterpart API is <a href=""https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.get_worker_info"" rel=""nofollow noopener""><code>get_worker_info</code></a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: September 11, 2020,  7:42pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
95547,DistributedDataParallel: cuda init; forced parameter sync; pinning?,2020-09-08T11:53:02.746Z,3,79,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to figure out DistributedDataParallel (on a single machine; single GPU / process mode). I’ve got a few questions:</p><NewLine><ol><NewLine><li><NewLine><p>Will all launched processes do CUDA init? Is it safe? Should we explicitly set CUDA_VISIBLE_DEVICES per launched process to ensure that it can see only one device? ( think fork/spawn global state issues with CUDA / OpenMP / pthreads etc…) Would it prevent it from rpc using nccl?</p><NewLine></li><NewLine><li><NewLine><p>Is it sensible to do a forced parameter sync once in a while? Inherent GPU parallelism non-determinism can cause parameter divergence that could cause replica parameter divergence for some sensitive models (even if gradients are sychronized)? How does one do it?</p><NewLine></li><NewLine><li><NewLine><p>When does it make sense to do NUMA node pinning? CPU affinity pinning?</p><NewLine></li><NewLine></ol><NewLine><p>Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/vadimkantorov,(Vadim Kantorov),vadimkantorov,"September 8, 2020, 12:09pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""95547"" data-username=""vadimkantorov""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/vadimkantorov/40/365_2.png"" width=""20""/> vadimkantorov:</div><NewLine><blockquote><NewLine><p>Will all launched processes do CUDA init?</p><NewLine></blockquote><NewLine></aside><NewLine><p>CUDA is lazily initialized. So if one process is not touching a specific device, corresponding CUDA context shouldn’t be created on that device.</p><NewLine><blockquote><NewLine><p>Is it safe?</p><NewLine></blockquote><NewLine><p>Even if multiple processes create context on the same device, it won’t crash, but each context consumes about 500MB CUDA memory, which is not desired.</p><NewLine><blockquote><NewLine><p>Should we explicitly set CUDA_VISIBLE_DEVICES per launched process to ensure that it can see only one device?</p><NewLine></blockquote><NewLine><p>This is the recommended way, as it also rules out the possibility that some third-party library accidentally create tensors on other devices.</p><NewLine><blockquote><NewLine><p>Would it prevent it from rpc using nccl?</p><NewLine></blockquote><NewLine><p>Which rpc are you referring to? Are you using DDP in conjunction with <code>torch.distributed.rpc</code>?</p><NewLine><blockquote><NewLine><p>Is it sensible to do a forced parameter sync once in a while? Inherent GPU parallelism non-determinism can cause parameter divergence that could cause replica parameter divergence for some sensitive models (even if gradients are sychronized)?</p><NewLine></blockquote><NewLine><p>If there are drifts, then, yes, manually sync once a while would help.</p><NewLine><blockquote><NewLine><p>How does one do it?</p><NewLine></blockquote><NewLine><p>You can use broadcast, see the code linked below. If you would like to calculate average, you can also use <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_reduce"" rel=""nofollow noopener""><code>all_reduce</code></a> and then divide by <code>world_size</code>.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/3806c939bda0df4c0f38b9d356c004f384535ac1/torch/nn/parallel/distributed.py#L404"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/3806c939bda0df4c0f38b9d356c004f384535ac1/torch/nn/parallel/distributed.py#L404"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/3806c939bda0df4c0f38b9d356c004f384535ac1/torch/nn/parallel/distributed.py#L404</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""394"" style=""counter-reset: li-counter 393 ;""><NewLine><li>    self.bucket_bytes_cap = int(bucket_cap_mb * 1024 * 1024)</li><NewLine><li><NewLine></li><li>    # Sync params and buffers</li><NewLine><li>    self._sync_params_and_buffers(authoritative_rank=0)</li><NewLine><li><NewLine></li><li>    self._ddp_init_helper()</li><NewLine><li><NewLine></li><li>def _sync_params_and_buffers(self, authoritative_rank=0):</li><NewLine><li>    module_states = list(self.module.state_dict().values())</li><NewLine><li>    if len(module_states) &gt; 0:</li><NewLine><li class=""selected"">        self._distributed_broadcast_coalesced(</li><NewLine><li>            module_states,</li><NewLine><li>            self.broadcast_bucket_size,</li><NewLine><li>            authoritative_rank)</li><NewLine><li><NewLine></li><li>def _ddp_init_helper(self):</li><NewLine><li>    """"""</li><NewLine><li>    Initialization helper function that does the following:</li><NewLine><li><NewLine></li><li>    (1) replicating the module from device[0] to the other devices</li><NewLine><li>    (2) bucketing the parameters for reductions</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><blockquote><NewLine><p>When does it make sense to do NUMA node pinning? CPU affinity pinning?</p><NewLine></blockquote><NewLine><p>Hey <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>, do you know what’s the best practice here?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> for these responses! Look like it should be in some official guide <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> I’m having troubles launching the most basic two-node distributed configuration (I checked, TCP connection with nc works ok). It doesn’t seem to respect the passed port. If you could take a look, it would be awesome!</p><NewLine><p><strong>UPD:</strong> I created an issue to discuss this: <a href=""https://github.com/pytorch/pytorch/issues/44544"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/44544</a></p><NewLine><pre><code class=""lang-python"">import os<NewLine>import torch<NewLine>import argparse<NewLine>import torch.distributed as dist<NewLine><NewLine>if __name__ == '__main__':<NewLine>    parser = argparse.ArgumentParser()<NewLine>    parser.add_argument('--backend', default='gloo')<NewLine>    parser.add_argument('--rank', type=int, default=0)<NewLine>    parser.add_argument('--world-size', type=int, default=1)<NewLine>    args = parser.parse_args()<NewLine>    dist.init_process_group(args.backend, init_method=""env://"", rank=args.rank, world_size=args.world_size)<NewLine>    print(f""Master node {os.environ['MASTER_ADDR']}:{os.environ['MASTER_PORT']}. Rank {args.rank}. World size: {args.world_size}"")<NewLine>    test_tensor = torch.tensor(args.rank+1)<NewLine>    if args.backend == 'nccl':<NewLine>        test_tensor = test_tensor.cuda()<NewLine>    dist.all_reduce(test_tensor, op=dist.ReduceOp.SUM)<NewLine>    print(f""Test value: {test_tensor.item()}, expected: {sum(range(args.world_size+1))}"")<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">Test standalone<NewLine><NewLine>Node 1<NewLine>Input: <NewLine>GLOO_SOCKET_IFNAME=team0 MASTER_ADDR=10.81.13.54 MASTER_PORT=12345 python distributed_example.py<NewLine>Output: <NewLine>Master node 10.81.13.54:12345. Rank 0. World size: 1<NewLine>Test value: 1, expected: 1<NewLine><NewLine>Node 2<NewLine>Input: <NewLine>GLOO_SOCKET_IFNAME=team0 MASTER_ADDR=10.81.13.51 MASTER_PORT=12345 python distributed_example.py <NewLine>Output: <NewLine>Master node 10.81.13.51:12345. Rank 0. World size: 1<NewLine>Test value: 1, expected: 1<NewLine><NewLine>Test disctibuted Gloo<NewLine><NewLine>Node 1<NewLine>Input: <NewLine>GLOO_SOCKET_IFNAME=team0 MASTER_ADDR=10.81.13.54 MASTER_PORT=12345 python distributed_example.py --rank 0 --world-size 2<NewLine>Output: <NewLine>Traceback (most recent call last):<NewLine>  File ""distributed_example.py"", line 11, in &lt;module&gt;<NewLine>    dist.init_process_group('gloo', init_method=""env://"", rank=args.rank, world_size=args.world_size)<NewLine>  File ""/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 425, in init_process_group<NewLine>    _default_pg = _new_process_group_helper(<NewLine>  File ""/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 499, in _new_process_group_helper<NewLine>    pg = ProcessGroupGloo(<NewLine>RuntimeError: [/opt/conda/conda-bld/pytorch_1595629411241/work/third_party/gloo/gloo/transport/tcp/pair.cc:769] connect [10.81.13.51]:11169: No route to host<NewLine><NewLine>Node 2<NewLine>Input:<NewLine>GLOO_SOCKET_IFNAME=team0 MASTER_ADDR=10.81.13.54 MASTER_PORT=12345 python distributed_example.py --rank 1 --world-size 2<NewLine>Output:<NewLine><NewLine>Test distributed NCCL<NewLine><NewLine>Node 1<NewLine>Input:<NewLine>NCCL_SOCKET_IFNAME=team0 NCCL_DEBUG=INFO MASTER_ADDR=10.81.13.54 MASTER_PORT=12345 python distributed_example.py --rank 0 --world-size 2 --backend nccl<NewLine>Output:<NewLine>Master node 10.81.13.54:12345. Rank 0. World size: 2<NewLine>srs-ds11:64570:64570 [0] NCCL INFO Bootstrap : Using [0]team0:10.81.13.54&lt;0&gt;<NewLine>srs-ds11:64570:64570 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine><NewLine>srs-ds11:64570:64570 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]<NewLine>srs-ds11:64570:64570 [0] NCCL INFO NET/Socket : Using [0]team0:10.81.13.54&lt;0&gt;<NewLine>NCCL version 2.4.8+cuda10.1<NewLine>srs-ds11:64570:65266 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555<NewLine><NewLine>Node 2<NewLine>Input:<NewLine>NCCL_SOCKET_IFNAME=team0 NCCL_DEBUG=INFO MASTER_ADDR=10.81.13.54 MASTER_PORT=12345 python distributed_example.py --rank 1 --world-size 2 --backend nccl<NewLine>Output:<NewLine>Master node 10.81.13.54:12345. Rank 1. World size: 2<NewLine>srs-ds8:192240:192240 [0] NCCL INFO Bootstrap : Using [0]team0:10.81.13.51&lt;0&gt;<NewLine>srs-ds8:192240:192240 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine><NewLine>srs-ds8:192240:192240 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]<NewLine>srs-ds8:192240:192240 [0] NCCL INFO NET/Socket : Using [0]team0:10.81.13.51&lt;0&gt;<NewLine>srs-ds8:192240:192316 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555<NewLine><NewLine>srs-ds8:192240:192316 [0] include/socket.h:390 NCCL WARN Connect to 10.81.13.54&lt;34419&gt; failed : No route to host<NewLine>srs-ds8:192240:192316 [0] NCCL INFO bootstrap.cc:100 -&gt; 2<NewLine>srs-ds8:192240:192316 [0] NCCL INFO bootstrap.cc:326 -&gt; 2<NewLine>srs-ds8:192240:192316 [0] NCCL INFO init.cc:695 -&gt; 2<NewLine>srs-ds8:192240:192316 [0] NCCL INFO init.cc:951 -&gt; 2<NewLine>srs-ds8:192240:192316 [0] NCCL INFO misc/group.cc:69 -&gt; 2 [Async thread]<NewLine>Traceback (most recent call last):<NewLine>  File ""distributed_example.py"", line 17, in &lt;module&gt;<NewLine>    dist.all_reduce(test_tensor, op=dist.ReduceOp.SUM)<NewLine>  File ""/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 936, in all_reduce<NewLine>    work = _default_pg.allreduce([tensor], opts)<NewLine>RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1595629411241/work/torch/lib/c10d/ProcessGroupNCCL.cpp:518, unhandled system error, NCCL version 2.4.8<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>For the glooo case, I noticed that the master address for two nodes are different, is this a typo?</p><NewLine><pre><code class=""lang-auto"">node 1: MASTER_ADDR=10.81.13.54<NewLine>node 2: MASTER_ADDR=10.81.13.51<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>No it doesn’t. Addresses are different because it is standalone test (one node/one worker) for sanity check. Theare are three cases: single node test, multi node test with gloo backend and with nccl backend.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vadimkantorov; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/aabugaliev; <NewLine> ,"REPLY_DATE 1: September 10, 2020,  3:04am; <NewLine> REPLY_DATE 2: September 11, 2020,  9:36am; <NewLine> REPLY_DATE 3: September 11, 2020,  2:17pm; <NewLine> REPLY_DATE 4: September 11, 2020,  7:19pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
52107,Specifying ports to be used in Pytorch multi-node distributed training,2019-07-30T23:19:34.425Z,0,128,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am facing problems with using torch distributed training in a multi-node setup. Apart from the specified master port, looks like Pytorch tries to open random ports for inter-node communication. In my setup I get a limited number of specified open ports. Is there some way I can force Pytorch to use only the given ports for internode communication?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/sarthak_garg,(sarthak garg),sarthak_garg,"July 30, 2019, 11:19pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We got the same problem…</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vadimkantorov; <NewLine> ,"REPLY_DATE 1: September 11, 2020,  7:53am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
95725,Is there any approach in pytorch to achieve distributed network connection between CPU prediction and GPU training?,2020-09-10T01:18:49.486Z,1,38,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Out team is planning to use CPUs from multiple computers to do network prediction and data production, and then use a single GPU server to do network training. Is there any method we can use in torch.distributed package that can help us with this situation?</p><NewLine></div>",https://discuss.pytorch.org/u/bqdqj,,bqdqj,"September 10, 2020,  1:18am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://pytorch.org/docs/master/rpc.html"" rel=""nofollow noopener""><code>torch.distributed.rpc</code></a> should be able to help. <a href=""https://pytorch.org/tutorials/beginner/dist_overview.html#general-distributed-training"" rel=""nofollow noopener"">Here</a> is a list of tutorials.</p><NewLine><p>The use case looks similar to the following two examples:</p><NewLine><ol><NewLine><li><a href=""https://pytorch.org/tutorials/intermediate/rpc_tutorial.html#distributed-reinforcement-learning-using-rpc-and-rref"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/rpc_tutorial.html#distributed-reinforcement-learning-using-rpc-and-rref</a></li><NewLine><li><a href=""https://pytorch.org/tutorials/intermediate/rpc_async_execution.html#batch-processing-cartpole-solver"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/rpc_async_execution.html#batch-processing-cartpole-solver</a></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much, I’ll look into them carefully!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/bqdqj; <NewLine> ,"REPLY_DATE 1: September 10, 2020,  2:49am; <NewLine> REPLY_DATE 2: September 11, 2020,  7:31am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
94992,Question about pipeline parallelism,2020-09-03T01:08:32.204Z,0,52,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I notice that in  <a>https://github.com/pytorch/examples/tree/master/distributed/rpc/pipeline</a>, every input batch is divided into micro-batches. This sorta likes GPipe method. But the backward pass is not parallelized, which contradicts GPipe. I wonder if there is any papers or references that can explain what pipeline algorithm are being used.</p><NewLine><p>I wonder if  it works like this gantt chart.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/8c34fc239831ad84eab4a5d7261daee35d8dcde2"" href=""https://discuss.pytorch.org/uploads/default/original/3X/8/c/8c34fc239831ad84eab4a5d7261daee35d8dcde2.png"" title=""PastedGraphic-4""><img alt=""PastedGraphic-4"" data-base62-sha1=""k0kmF5b4UoYRe7XHmSCuiPBCYzU"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/c/8c34fc239831ad84eab4a5d7261daee35d8dcde2_2_10x10.png"" height=""142"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/c/8c34fc239831ad84eab4a5d7261daee35d8dcde2_2_690x142.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/c/8c34fc239831ad84eab4a5d7261daee35d8dcde2_2_690x142.png, https://discuss.pytorch.org/uploads/default/optimized/3X/8/c/8c34fc239831ad84eab4a5d7261daee35d8dcde2_2_1035x213.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/8/c/8c34fc239831ad84eab4a5d7261daee35d8dcde2.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">PastedGraphic-4</span><span class=""informations"">1370×282 28.9 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/ConnollyLeon,,ConnollyLeon,"September 3, 2020,  1:12am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If so, is there any chances to enhance it? Makes it more like this chart below.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/e1b740c0a1f7073d7c930b6932125c7528346625"" href=""https://discuss.pytorch.org/uploads/default/original/3X/e/1/e1b740c0a1f7073d7c930b6932125c7528346625.jpeg"" title=""WeChat03357ce5fb12f7ef959d4a752be604f1""><img alt=""WeChat03357ce5fb12f7ef959d4a752be604f1"" data-base62-sha1=""wcM30i3fov6DuHIGZVOdWaOSfYN"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/1/e1b740c0a1f7073d7c930b6932125c7528346625_2_10x10.png"" height=""142"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/1/e1b740c0a1f7073d7c930b6932125c7528346625_2_690x142.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/1/e1b740c0a1f7073d7c930b6932125c7528346625_2_690x142.jpeg, https://discuss.pytorch.org/uploads/default/optimized/3X/e/1/e1b740c0a1f7073d7c930b6932125c7528346625_2_1035x213.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/e/1/e1b740c0a1f7073d7c930b6932125c7528346625_2_1380x284.jpeg 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">WeChat03357ce5fb12f7ef959d4a752be604f1</span><span class=""informations"">1504×310 104 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yep, the first chart looks correct to me.</p><NewLine><blockquote><NewLine><p>If so, is there any chances to enhance it? Makes it more like this chart below.</p><NewLine></blockquote><NewLine><p>Do you need single-machine multi-GPU pipeline parallel or multi-machine pipeline parallel?</p><NewLine><ul><NewLine><li><NewLine><p>If it is within a single machine, it is possible to parallelize backward as well. Check this project <a href=""https://github.com/kakaobrain/torchgpipe"" rel=""nofollow noopener"">torchgpipe</a>. It inserts phony dependencies between stages of different micro batches.</p><NewLine></li><NewLine><li><NewLine><p>If it’s multi-machine pipeline parallel, then you will need RPC. As of today distributed autograd cannot parallelize backward, because the <a href=""https://github.com/pytorch/pytorch/issues/23110"" rel=""nofollow noopener"">smart mode</a> has not been implemented yet. To get around this, you can still use RPC and RRef, but cannot use distributed autograd and will need to manually stitch together local autograd.</p><NewLine><p>Another possibility (not 100% sure if this would work) is to create one distributed autograd context per micro batch, and manually call <code>__enter__</code> and <a href=""https://github.com/pytorch/pytorch/blob/356aa54694cc4f1c9599fdf8320a4759aa63a12f/torch/distributed/autograd/__init__.py#L36"" rel=""nofollow noopener""><code>__exit__</code></a> on distributed autograd context. As a result, the gradients for different microbatches will be stored in different contexts, and hence you will need to call <code>dist_optimizer.step(ctx_id)</code> multiple times to apply the gradients.</p><NewLine><p>Things will become a lot easier when we add smart mode distributed autograd.</p><NewLine></li><NewLine></ul><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your replying! This help me a lot.<br/><NewLine>I will keep trying to enhance the training speed by utilizing pipeline.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ConnollyLeon; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ConnollyLeon; <NewLine> ,"REPLY_DATE 1: September 3, 2020,  1:12am; <NewLine> REPLY_DATE 2: September 28, 2020,  2:50am; <NewLine> REPLY_DATE 3: September 28, 2020,  2:52am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
95738,Torch.distributed.launch vs torch.multiprocessing.spawn,2020-09-10T04:53:56.095Z,0,33,"<div class=""post"" itemprop=""articleBody""><NewLine><p>What is the implementation and performance differences between <code>torch.distributed.launch</code> and <code>torch.multiprocessing.spawn</code>?</p><NewLine></div>",https://discuss.pytorch.org/u/briankosw,(Brian Ko),briankosw,"September 10, 2020,  4:53am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>torch.distributed.launch</code> uses <code>subprocess.Popen</code>. The perf differences between these two are typical <a href=""https://stackoverflow.com/questions/47689297/speed-comparison-using-multiprocessing-process-versus-subprocess-popen."" rel=""nofollow noopener""><code>multiprocessing</code> vs <code>subprocess</code></a></p><NewLine><p>Besides that, <code>torch.distributed.launch</code> also tries to configure several env vars and pass command line arguments for distributed training script, e.g., <code>RANK</code>, <code>LOCAL_RANK</code>, WORLD_SIZE etc. On the other hand, <code>torch.multiprocessing.spawn</code> is general multi-processing, not specifically tailored for <code>torch.distributed</code>.</p><NewLine><p>If you need multi-server distributed data parallel training, it might be more convenient to use <code>torch.distributed.launch</code> as it automatically calculates ranks for you, through <code>--nnode</code>, <code>--node_rank</code>, and <code>--nproc_per_node</code>.If you need single-server multi-gpu data parallel training, both should work the same.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: September 10, 2020,  2:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
95462,DataParallel for Mixed GPUs,2020-09-07T15:29:54.861Z,0,36,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Could pytorch effectively use setup in server with different GPUs, for example 2x 2080 Ti + 2x 3080 Ti for the one distributed learning process? Or some drawbacks may occur?</p><NewLine></div>",https://discuss.pytorch.org/u/Marat,(Закиров Марат),Marat,"September 7, 2020,  3:29pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We had similar posts in this forum, which you could search for, and the main drawback would be that the slower GPUs would potentially create the bottleneck in your setup.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 10, 2020,  9:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
95744,Distributed Data Parallel with two different model GPUs possible?,2020-09-10T06:35:20.812Z,0,22,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I have a machine with a 2080ti and 1080ti. is it possible to do distributed data-parallel on two different types of GPUs?</p><NewLine></div>",https://discuss.pytorch.org/u/miken,(Michael Nguyen),miken,"September 10, 2020,  6:35am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sure, pytroch don’t care about it. But please take care the gpu memory allocation.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/JIE_LIU; <NewLine> ,"REPLY_DATE 1: September 10, 2020,  7:44am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
95632,"Using NVIDIA apex for training, i cannot get same accuracy after training",2020-09-09T08:59:12.793Z,0,33,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I have a questions about NVIDIA apex<br/><NewLine>I know NVIDIA apex package creates each process per gpu, like this</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/d87451234bed9f147a02a8b7876e9e48a43bd48d"" href=""https://discuss.pytorch.org/uploads/default/original/3X/d/8/d87451234bed9f147a02a8b7876e9e48a43bd48d.png"" title=""image""><img alt=""image"" data-base62-sha1=""uSQlaOfk6rntfVemxQcG9PeLDf7"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/8/d87451234bed9f147a02a8b7876e9e48a43bd48d_2_10x10.png"" height=""187"" src=""https://discuss.pytorch.org/uploads/default/original/3X/d/8/d87451234bed9f147a02a8b7876e9e48a43bd48d.png"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">718×195 4.14 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>so, each process are referred as local_rank variable in my code<br/><NewLine>I want to save best accuracy from each process and i coding like below</p><NewLine><p>When i Using 2 gpus</p><NewLine><pre><code class=""lang-auto"">for epochs in range(0, args.epoch):<NewLine>   train()<NewLine>   test()<NewLine>    ...<NewLine>   save_best()<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">def save_best():<NewLine>   # 1'th gpu<NewLine>   if args.local_rank == 0:<NewLine>      is_best = test_acc &gt; best_acc<NewLine>      best_acc = max(test_acc, best_acc)<NewLine>      if is_best:<NewLine>         torch.save(...)<NewLine>   # 2'th gpu<NewLine>   if args.local_rank == 1:<NewLine>      is_best = test_acc &gt; best_acc<NewLine>      best_acc = max(test_acc, best_acc)<NewLine>      if is_best:<NewLine>         torch.save(...)<NewLine><NewLine></code></pre><NewLine><p>After 1 epoch I can verify each accuracy<br/><NewLine><strong>0’th gpu’s accuracy is 19.906, It is saved 0’th weight file</strong><br/><NewLine><strong>1’th gpu’s accuracy is 19.269, It is saved 1’th weight file</strong></p><NewLine><p>But, When i loading weight file and adapt to network, test accuracy is not equal to each result<br/><NewLine>I got <strong>19.572(0’th file)</strong>,  <strong>19.561(1’th file)</strong></p><NewLine><p>Surprisingly, When i using 1 gpu for training, the situation that i mentioned above is not happened(test accuracy while training is equal to accuracy which is loading from weight file)</p><NewLine><p>I can’t understand why this situation is happened.<br/><NewLine>Any body can help?</p><NewLine></div>",https://discuss.pytorch.org/u/Gwon,,Gwon,"September 9, 2020,  9:02am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We recommend to use the native mixed-precision training utility via <a href=""https://pytorch.org/docs/stable/amp.html""><code>torch.cuda.amp</code></a> instead of <code>apex</code>, as it should cover more tested use cases.<br/><NewLine>More information can be found <a href=""https://discuss.pytorch.org/t/torch-cuda-amp-vs-nvidia-apex/74994/3"">here</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 9, 2020,  9:02am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
94835,Embedding layer: arguments located on different gpus,2020-09-01T16:44:37.157Z,2,77,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using nn.DataParallel and I have an error inside the embedding layer that said “RuntimeError: arguments are located on different GPUs  at /pytorch/aten/src/THC/generic/THCTensorIndex.cu:403”. My network architecture is the following</p><NewLine><pre><code class=""lang-auto"">class WordEmbeddingNetwork(nn.Module):<NewLine><NewLine>    def __init__(self, word_embeddings_path, word2id, pad_token, unk_token, freeze=False):<NewLine><NewLine>        super(WordEmbeddingNetwork, self).__init__()<NewLine>        self.pad_token = pad_token<NewLine>        self.unk_token = unk_token<NewLine>        self.word2id = word2id<NewLine>        self.embedding_file = word_embeddings_path.split('/')[-1]<NewLine>        self.load_embeddings_from_file(word_embeddings_path)<NewLine><NewLine>        embedding_weights = self.get_embeddings_weights(OOV_corrections)<NewLine><NewLine>        num_embeddings, self.embedding_dim = embedding_weights.shape<NewLine>        self.embedding_layer = nn.Embedding(num_embeddings, self.embedding_dim)<NewLine>        self.embedding_layer.load_state_dict({'weight': embedding_weights})<NewLine>        if freeze:<NewLine>            for p in self.embedding_layer.parameters():<NewLine>                p.requires_grad = False<NewLine><NewLine><NewLine>    def forward(self, batch):<NewLine>        print(batch.device)<NewLine>        print(self.embedding_layer.weight.device)<NewLine>        emb = self.embedding_layer(batch)<NewLine>        return emb<NewLine><NewLine><NewLine>class MyNet(nn.Module):<NewLine><NewLine>    _HIDDEN_SIZE = 300<NewLine><NewLine>    def __init__(self, word_embeddings_path, word2id, pad_token, unk_token, seed, device='cpu'):<NewLine>        torch.manual_seed(seed)<NewLine>        super(MyNet, self).__init__()<NewLine><NewLine>        self.device = device<NewLine>        self.word_embeddings_layer = WordEmbeddingNetwork(word_embeddings_path=word_embeddings_path, <NewLine>                                                        word2id=word2id, <NewLine>                                                        pad_token=pad_token, <NewLine>                                                        unk_token=unk_token)<NewLine><NewLine>    def __init__(self, utterances, ...):<NewLine>        self.word_embedding_layer(utterances)<NewLine>        ....<NewLine></code></pre><NewLine><p>I don’t understand why embedding layer and the given input are on different gpus. Can you help me?</p><NewLine></div>",https://discuss.pytorch.org/u/Seo,,Seo,"September 1, 2020,  5:00pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried to remove the embeddings and put them on cpu. Now I have the same error on LSTM, it seems to me that nn.DataParallel moves things wrongly from one gpu to another</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Input and parameter tensors are not at the same device, found input tensor at cuda:0 and parameter tensor at cuda:1<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Finally I have solved. nn.DataParallel moves to the correct gpu only tensors, if you have list of tensors as input of your model <code>forward()</code> method, you need to move one by one tensors in the list on the correct gpu. The correct gpu can be retrieved by accessing the <code>.device</code> attribute of a tensor automatically moved by the nn.DataParallel on the correct gpu. Never force a <code>.to(device)</code> with the wrong device!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/seo"">@Seo</a>, IIUC, the DataParallel should be able to automatically scatter tensors in the input list to the correct device along the batch dimension. It uses the following code. Is your use case different from this assumption?</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/10dd25dcd18457a53e69eb319f48749a49a48430/torch/nn/parallel/scatter_gather.py#L5-L31"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/10dd25dcd18457a53e69eb319f48749a49a48430/torch/nn/parallel/scatter_gather.py#L5-L31"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/10dd25dcd18457a53e69eb319f48749a49a48430/torch/nn/parallel/scatter_gather.py#L5-L31</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""5"" style=""counter-reset: li-counter 4 ;""><NewLine><li>def scatter(inputs, target_gpus, dim=0):</li><NewLine><li>    r""""""</li><NewLine><li>    Slices tensors into approximately equal chunks and</li><NewLine><li>    distributes them across given GPUs. Duplicates</li><NewLine><li>    references to objects that are not tensors.</li><NewLine><li>    """"""</li><NewLine><li>    def scatter_map(obj):</li><NewLine><li>        if isinstance(obj, torch.Tensor):</li><NewLine><li>            return Scatter.apply(target_gpus, None, dim, obj)</li><NewLine><li>        if isinstance(obj, tuple) and len(obj) &gt; 0:</li><NewLine><li>            return list(zip(*map(scatter_map, obj)))</li><NewLine><li>        if isinstance(obj, list) and len(obj) &gt; 0:</li><NewLine><li>            return list(map(list, zip(*map(scatter_map, obj))))</li><NewLine><li>        if isinstance(obj, dict) and len(obj) &gt; 0:</li><NewLine><li>            return list(map(type(obj), zip(*map(scatter_map, obj.items()))))</li><NewLine><li>        return [obj for targets in target_gpus]</li><NewLine><li><NewLine></li><li>    # After scatter_map is called, a scatter_map cell will exist. This cell</li><NewLine><li>    # has a reference to the actual function scatter_map, which has references</li><NewLine><li>    # to a closure that has a reference to the scatter_map cell (because the</li><NewLine></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/10dd25dcd18457a53e69eb319f48749a49a48430/torch/nn/parallel/scatter_gather.py#L5-L31"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>, thank you for your response. Actually my case is different, I have a list of tensors and I want to chunk the list along its length. I have solved by implementing my own scatter method like this:</p><NewLine><pre><code class=""lang-auto"">def scatter(inputs, target_gpus, dim=0):<NewLine>    r""""""<NewLine>    Slices tensors into approximately equal chunks and<NewLine>    distributes them across given GPUs. Duplicates<NewLine>    references to objects that are not tensors.<NewLine>    """"""<NewLine>    def scatter_map(obj):<NewLine>        if isinstance(obj, torch.Tensor):<NewLine>            return Scatter.apply(target_gpus, None, dim, obj)<NewLine>        if isinstance(obj, tuple) and len(obj) &gt; 0:<NewLine>            return list(zip(*map(scatter_map, obj)))<NewLine>        if isinstance(obj, list) and len(obj) &gt; 0:<NewLine>            #on the last gpu the torch scatter always put the remaining samples to fit the batch<NewLine>            # (e.g., batch=256, n_gpus=3 ==&gt; chunks=[86, 86, 84])<NewLine>            size = math.ceil(len(obj)/len(target_gpus))<NewLine>            chunk = [obj[i * size:(i + 1) * size] for i in range(len(target_gpus)-1)]<NewLine>            diff = len(obj) - size*(len(target_gpus)-1)<NewLine>            chunk.append(obj[-diff:])<NewLine>            return chunk<NewLine>        if isinstance(obj, dict) and len(obj) &gt; 0:<NewLine>            return list(map(type(obj), zip(*map(scatter_map, obj.items()))))<NewLine>        return [obj for targets in target_gpus]<NewLine><NewLine>    # After scatter_map is called, a scatter_map cell will exist. This cell<NewLine>    # has a reference to the actual function scatter_map, which has references<NewLine>    # to a closure that has a reference to the scatter_map cell (because the<NewLine>    # fn is recursive). To avoid this reference cycle, we set the function to<NewLine>    # None, clearing the cell<NewLine>    try:<NewLine>        return scatter_map(inputs)<NewLine>    finally:<NewLine>        scatter_map = None<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Seo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Seo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Seo; <NewLine> ,"REPLY_DATE 1: September 1, 2020,  7:47pm; <NewLine> REPLY_DATE 2: September 1, 2020,  9:35pm; <NewLine> REPLY_DATE 3: September 7, 2020,  9:06pm; <NewLine> REPLY_DATE 4: September 8, 2020, 12:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
90636,`Exception: process 0 terminated with exit code 1` error when using `torch.multiprocessing.spawn` to parallelize over multiple GPUs,2020-07-27T05:53:36.397Z,20,307,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have the following code below using <code>torch.multiprocessing.spawn</code> to parallelize over multiple GPUs:</p><NewLine><pre><code class=""lang-auto"">import numpy as np<NewLine>import torch<NewLine>from torch.multiprocessing import Pool, set_start_method, spawn<NewLine><NewLine>X = np.array([[1, 3, 2, 3], [2, 3, 5, 6], [1, 2, 3, 4]])<NewLine>X = torch.DoubleTensor(X)<NewLine><NewLine>def X_power_func(j):<NewLine>    X_power = X.cuda()**j<NewLine>    return X_power<NewLine><NewLine>if __name__ == '__main__':<NewLine>    results = spawn(X_power_func, range(4), nprocs=1)<NewLine><NewLine>results<NewLine></code></pre><NewLine><p>But I am getting this error below when I run the code:</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>Exception                                 Traceback (most recent call last)<NewLine>&lt;ipython-input-9-97eb990d7396&gt; in &lt;module&gt;()<NewLine>     12 <NewLine>     13 if __name__ == '__main__':<NewLine>---&gt; 14     results = spawn(X_power_func, range(4), nprocs=1)<NewLine>     15 <NewLine>     16 results<NewLine><NewLine>2 frames<NewLine>/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py in join(self, timeout)<NewLine>    111                 raise Exception(<NewLine>    112                     ""process %d terminated with exit code %d"" %<NewLine>--&gt; 113                     (error_index, exitcode)<NewLine>    114                 )<NewLine>    115 <NewLine><NewLine>Exception: process 0 terminated with exit code 1<NewLine></code></pre><NewLine><p>What I have done wrong in my code?</p><NewLine></div>",https://discuss.pytorch.org/u/Leockl,(Leo Chow),Leockl,"July 27, 2020,  4:22pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>First, please read API document <strong>carefully</strong>:</p><NewLine><blockquote><NewLine><p>torch.multiprocessing.spawn(fn, args=(), nprocs=1, join=True, daemon=False, start_method=‘spawn’)</p><NewLine></blockquote><NewLine><blockquote><NewLine><p>Parameters</p><NewLine><ol><NewLine><li>fn (function) – …The function is called as fn(i, *args), where i is the process index and args is the passed through tuple of arguments.</li><NewLine><li>args (tuple) – Arguments passed to fn.</li><NewLine><li>nprocs (int) – Number of processes to spawn.</li><NewLine></ol><NewLine></blockquote><NewLine><blockquote><NewLine><p>Returns<br/><NewLine>None if join is True, ProcessContext if join is False</p><NewLine></blockquote><NewLine><p>First, you should call your function as:</p><NewLine><pre><code class=""lang-auto"">if __name__ == '__main__':<NewLine>    # start with 4 processes<NewLine>    # your original method will invoke your function as X_power_func(0, 0, 1, 2, 3)<NewLine>    spawn(X_power_func, nprocs=4)<NewLine></code></pre><NewLine><p>Secondly, do not put result at:</p><NewLine><pre><code class=""lang-auto"">if __name__ == '__main__':<NewLine>    results = spawn(X_power_func, range(4), nprocs=1)<NewLine><NewLine>results<NewLine></code></pre><NewLine><p>Because in your subprocesses, they will try to access “results”, but since they are not “<strong>main</strong>”, “results” is not defined,</p><NewLine><p>Thirdly, spawn will not return results.</p><NewLine><blockquote><NewLine><p>Returns<br/><NewLine>None if join is True, ProcessContext if join is False</p><NewLine></blockquote><NewLine><p>In summary, please <strong>re-read documents</strong>, they take <strong>a lot of time to write</strong>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Many thanks <a class=""mention"" href=""/u/iffix"">@iffiX</a> for input on this. I am new to <code>torch.multiprocessing.spawn</code> and PyTorch in general, so I guess I just got confused when I read the documentation on it.</p><NewLine><ol><NewLine><li>With your first point above, are you missing <code>args=()</code>?, ie.</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">if __name__ == '__main__':<NewLine>    # start with 4 processes<NewLine>    # your original method will invoke your function as X_power_func(0, 0, 1, 2, 3)<NewLine>    spawn(X_power_func, args=(0, 1, 2, 3), nprocs=4)<NewLine></code></pre><NewLine><p>I don’t understand why must we start with 4 processes?</p><NewLine><ol start=""2""><NewLine><li><NewLine><p>With your second point, if the <code>results</code> are kept within <code>main</code>, how would I then output the <code>results</code> outside of <code>main</code>?</p><NewLine></li><NewLine><li><NewLine><p>With your third point, can I ask what is <code>ProcessContext</code>? If I want to return <code>results</code>, does that mean I need to set <code>join=False</code>?</p><NewLine></li><NewLine></ol><NewLine><p>I am sorry for having so many questions. Would really be grateful if you could help.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well, being new is not an excuse! <img alt="":rofl:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/rofl.png?v=9"" title="":rofl:""/></p><NewLine><ol><NewLine><li>Yes, I am missing <code>args</code>, because torch.multiprocessing will invoke your function <code>X_power_func(rank)</code>, the default argument is the rank of the started process.</li><NewLine><li>If you want to print results outside of main, you main print it in the invoked function:<pre><code class=""lang-auto"">def X_power_func(j):<NewLine>     X_power = X.cuda()**j<NewLine>     print(X_power)<NewLine>     return X_power<NewLine></code></pre><NewLine></li><NewLine><li>No, in order to properly return results, you should either use <code>torch.multiprocessing.pool</code> or passing a pipe object or anything that can be used to perform inter-process communication.<br/><NewLine>Torch multiprocessing module is a very thin wrapper of the original multiprocessing module, it basically just registers some customized serializers.</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Or you could post your detailed requirements so that we can workout a proper solution for you.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah you’re right <a class=""mention"" href=""/u/iffix"">@iffiX</a>, I should mention here what I actually want to do.</p><NewLine><p>I am trying to figure out a way to parallelize over multiple GPUs on non-neural net computations in PyTorch. More specifically, I have developed an estimator (non-neural net) in Scikit-learn and the speed performance is slow when a certain hyperparameter is increased. To solve this, I am re-writing the estimator in PyTorch so that I can make use of GPU processing and hopefully multiple GPUs as well.</p><NewLine><p>I have posted a question here in PyTorch Forums (1-2 days ago), Stackoverflow, PyTorch Github and multiple channels in Reddit and I haven’t gotten a reply yet or no one seems to know the full solution. In fact, what I got is the opposite where a lot of folks over at Reddit wanted to know how this is done too. So far, I feel like you’re the only one who seems to know <img alt="":sweat_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/sweat_smile.png?v=9"" title="":sweat_smile:""/></p><NewLine><p>Anyhow, I did initially tried using <code>torch.multiprocessing.pool</code>. The MRE code is as below:</p><NewLine><pre><code class=""lang-auto"">import numpy as np<NewLine>import torch<NewLine>from torch.multiprocessing import Pool, set_start_method<NewLine><NewLine>X = np.array([[1, 3, 2, 3], [2, 3, 5, 6], [1, 2, 3, 4]])<NewLine>X = torch.DoubleTensor(X)<NewLine><NewLine>def X_power_func(j):<NewLine>    X_power = X.cuda()**j<NewLine>    return X_power<NewLine><NewLine>if __name__ == '__main__':<NewLine>  set_start_method('spawn', force=True)<NewLine>  with Pool(processes = 2) as p:   # Parallelizing over 2 GPUs<NewLine>    results = p.map(X_power_func, range(4))<NewLine></code></pre><NewLine><p>However when I run this code, it hangs or keeps running forever without any errors.</p><NewLine><p>When I removed <code>set_start_method('spawn', force=True)</code>, the code ran properly and gave me the results, but this only works for when I run the code once. When I ran the code again in subsequent runs, I get the error <code>RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method</code>.</p><NewLine><p>Someone in Reddit suggested that I should use <code>torch.multiprocessing.spawn</code> which lead me here back to PyTorch Forum with this post.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>This error is here because:</p><NewLine><pre><code class=""lang-auto"">X = torch.DoubleTensor(X)<NewLine><NewLine>def X_power_func(j):<NewLine>    X_power = X.cuda()**j<NewLine>    return X_power<NewLine></code></pre><NewLine><p>You are referencing a global variable here, since “fork” will map the memory of forker to forkee, you can<br/><NewLine>pass this function to subprocesses correctly, however, “fork” is not compatible with cuda, and therefore the error is thrown.</p><NewLine><p>Could you please show the full code of your estimator? Writing parallel programs in python could be a real pain, with your full code we can choose the most efficient and simplest solution.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok, the source code for the estimator is here:<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/leockl/helstrom-quantum-centroid-classifier/blob/master/hqc/hqc.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/leockl/helstrom-quantum-centroid-classifier/blob/master/hqc/hqc.py"" rel=""nofollow noopener"" target=""_blank"">leockl/helstrom-quantum-centroid-classifier/blob/master/hqc/hqc.py</a></h4><NewLine><pre><code class=""lang-py"">import numpy as np<NewLine>from sklearn.base import BaseEstimator, ClassifierMixin<NewLine>from sklearn.utils.validation import check_X_y, check_array, check_is_fitted<NewLine>from sklearn.utils.multiclass import check_classification_targets<NewLine>from sklearn.preprocessing import normalize<NewLine>from joblib import Parallel, delayed<NewLine><NewLine>class HQC(BaseEstimator, ClassifierMixin):<NewLine>    """"""The Helstrom Quantum Centroid (HQC) classifier is a quantum-inspired supervised <NewLine>    classification approach for data with binary classes (ie. data with 2 classes only).<NewLine>                         <NewLine>    Parameters<NewLine>    ----------<NewLine>    rescale : int or float, default = 1<NewLine>        The dataset rescaling factor. A parameter used for rescaling the dataset. <NewLine>    encoding : str, default = 'amplit'<NewLine>        The encoding method used to encode vectors into quantum densities. Possible values:<NewLine>        'amplit', 'stereo'. 'amplit' means using the amplitude encoding method. 'stereo' means <NewLine>        using the inverse of the standard stereographic projection encoding method. Default set <NewLine>        to 'amplit'.<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/leockl/helstrom-quantum-centroid-classifier/blob/master/hqc/hqc.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>It has already been parallelized for multiple CPUs but it’s still slow when the hyperparameter <code>n_copies</code> is increased. This hyperparameter controls the number of times a Kronecker tensor product is performed, which will result in multiplication of very large matrices when it is increased, therefore slowing down the speed performance when this hyperparameter is large.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can use <a href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor.share_memory_"" rel=""nofollow noopener""><code>share_momery_()</code></a> and <code>torch.multiprocessing.SimpleQueue</code> to implement IPC. E.g.:</p><NewLine><pre><code class=""lang-python"">import numpy as np<NewLine>import torch<NewLine>import torch.multiprocessing as mp<NewLine><NewLine><NewLine>def func(rank, x, p2c, c2p):<NewLine>    x_power = x.to(rank) ** rank<NewLine>    c2p.put(x_power)<NewLine>    # citing multiprocessing doc: Unlike CPU tensors, the <NewLine>    # sending process is required to keep the original tensor <NewLine>    # as long as the receiving process retains a copy of <NewLine>    # the tensor. The refcounting is implemented under the <NewLine>    # hood but requires users to follow the next best practices.<NewLine>    p2c.get()<NewLine>    print(f""child-{rank} done"")<NewLine><NewLine>if __name__ == '__main__':<NewLine>    nprocs = 2<NewLine>    x = torch.ones(2, 2)<NewLine>    x.share_memory_()<NewLine>    ctx = mp.get_context('spawn')<NewLine>    c2p, p2c = ctx.SimpleQueue(), ctx.SimpleQueue()<NewLine>    ps = [ctx.Process(target=func, args=(rank, x, p2c, c2p)) for rank in range(nprocs)]<NewLine>    [p.start() for p in ps]<NewLine>    tensors = [c2p.get() for _ in range(nprocs)]<NewLine>    print(tensors)<NewLine>    del tensors<NewLine>    for p in ps:<NewLine>        p2c.put(0)<NewLine>        p.join()<NewLine>    print(""parent done"")<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have read your code:</p><NewLine><ol><NewLine><li>So the first outerloop(function) is:<pre><code class=""lang-auto"">L148:             def X_prime_class_split_func(j):<NewLine></code></pre><NewLine>What’s the shape and data type of <code>X_prime_class_split[j]</code>? Maybe we could represent it as a tensor</li><NewLine><li>from L162-L166:<pre><code class=""lang-auto""> for k in range(m_class_split):<NewLine>       # Encode vectors into quantum densities<NewLine>       X_prime_class_split_each_row = X_prime_class_split_jth[k, :]<NewLine>       density_each_row = np.dot(X_prime_class_split_each_row.reshape(-1, 1),<NewLine>                                                   X_prime_class_split_each_row.reshape(1, -1))<NewLine></code></pre><NewLine>You could definetly vectorize this inner loop.</li><NewLine><li>from L171-L174<pre><code class=""lang-auto"">else:<NewLine>      density_each_row_copy = density_each_row<NewLine>      for u in range(self.n_copies - 1):<NewLine>            density_each_row = np.kron(density_each_row, density_each_row_copy)<NewLine></code></pre><NewLine>There is no efficient way to parallelize kronecker product over <code>n_copies</code> since these code are iterative and strongly serial. But you could use the <code>einsum</code> function of pytorch to calculate:<pre><code class=""lang-auto"">def kronecker(A, B):<NewLine>    return torch.einsum(""ab,cd-&gt;acbd"", A, B).view(A.size(0)*B.size(0),  A.size(1)*B.size(1))<NewLine></code></pre><NewLine></li><NewLine><li>The scale(computation intensity) of your code does not suit process based parallelism, thread based parallelism is not good in your case as well, I suggest you ""vectorize’ your code, use tensor operations insead of for loops, it would be at least 10 times more efficient.</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/iffix"">@iffiX</a>,</p><NewLine><ol><NewLine><li><NewLine></li></ol><NewLine><p><em>What’s the shape and data type of  <code>X_prime_class_split[j]</code> ?</em><br/><NewLine><code>X_prime_class_split[j]</code> is a 2d numpy array. How I have CPU parallelize my code is two-fold. First, it parallelizes over the 2 binary classes. Secondly, for each binary class, the dataset is split into batches and parallelization is performed over the batches. <code>X_prime_class_split[j]</code> is just a dataset batch, therefore it is a 2d numpy array.</p><NewLine><p><em>Maybe we could represent it as a tensor.</em><br/><NewLine>I have actually already converted all of the numpy functions in my code into PyTorch functions. It is <a href=""https://github.com/leockl/HQC/blob/master/HQC%20-%20PyTorch/HQC%20-%20PyTorch.ipynb"" rel=""nofollow noopener"">here</a> in my Github. Of course, this code is not fully working properly yet because of the issue with <code>torch.multiprocessing.Pool</code>. So in this code, <code>X_prime_class_split[j]</code> is a PyTorch tensor.</p><NewLine><ol start=""2""><NewLine><li><NewLine></li></ol><NewLine><p><em>You could definetly vectorize this inner loop.</em><br/><NewLine>Thanks for the tip! I didn’t realize chunk of codes could also be vectorized (using <code>numpy.vectorize</code>). I googled around but couldn’t find the PyTorch equivalent of <code>numpy.vectorize</code>. If I were to vectorize this part of the code, do you know how I could do it using PyTorch tensors?</p><NewLine><ol start=""3""><NewLine><li><NewLine></li></ol><NewLine><p><em>There is no efficient way to parallelize kronecker product over  <code>n_copies</code>  since these code are iterative and strongly serial.</em><br/><NewLine>Agree, there is no way to efficient parallelize kronecker product because of it’s iterative and serial nature. As mentioned above, my code is actually parallelized over the 2 binary classes and batches of the dataset, so I am not looking to parallelize the kronecker product.</p><NewLine><p><em>But you could use the  <code>einsum</code>  function of pytorch to calculate…</em><br/><NewLine>I have actually already done this in my Github link above! This gives me some comfort knowing that I am on the same page as you <img alt="":smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smile.png?v=9"" title="":smile:""/></p><NewLine><ol start=""4""><NewLine><li><NewLine></li></ol><NewLine><p>I guess if I can’t parallelize my code over multiple GPUs, plan B would be to rewrite my code to just use 1 GPU processing.</p><NewLine><p>Many many thanks again for having a look <a class=""mention"" href=""/u/iffix"">@iffiX</a>. Really appreciate it heaps!</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Many thanks <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>.</p><NewLine><p>Your code looks interesting and I am new to PyTorch, so I will have to investigate it line by line what is it doing to see if it helps in what I want to do.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I am optimizing your code, what’s the usual size of <code>n_samples</code> and <code>n_features</code>?</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>If n_samples * n_features &lt; 1e8, and you have a medium to good GPU (&gt;=GTX1080), then there is no need to split by class, below is the “vectorized” implementation, not tested.</p><NewLine><pre><code class=""lang-auto""># new implementation of kronecker<NewLine>def kronecker(A, B):<NewLine>    return torch.einsum('nab,ncd-&gt;nacbd', A, B)\<NewLine>        .view(A.size(0), A.size(1)*B.size(1), A.size(2)*B.size(2))<NewLine></code></pre><NewLine><p>Main code, no need to use pools or whatever:</p><NewLine><pre><code class=""lang-auto""># according to your code, the shape of `X_prime` should be (n_samples, n_features + 1)<NewLine># whether encoding = ""amplit"" or ""stereo""<NewLine>rows = n_samples<NewLine>cols = n_features + 1<NewLine><NewLine><NewLine># you may keep this if ""n_samples * n_features"" &gt; 1e8<NewLine># X_prime_class = X_prime[y_class_index == i]<NewLine><NewLine># Number of rows/columns in density matrix<NewLine>density_nrow_ncol = (n + 1)**self.n_copies<NewLine><NewLine># Encode vectors into quantum densities<NewLine># density: [rows, cols, cols], each density[k] is the original `density_each_row`<NewLine>density = torch.matmul(X_prime.view(rows, cols, 1),<NewLine>                       X_prime.view(rows, 1, cols))<NewLine><NewLine># Calculate n-fold Kronecker tensor product<NewLine>if self.n_copies == 1:<NewLine>    density = density<NewLine>else:<NewLine>    density_copy = density<NewLine>    for u in range(self.n_copies - 1):<NewLine>        density = kronecker(density, density_copy)<NewLine><NewLine># Calculate sum of quantum densities belonging to either class, per subset split<NewLine>density_sum = density.sum(dim = 0)<NewLine><NewLine># calculate centroid and q_hels_obs_terms<NewLine>centroid = (1 / (m_class + 1e-6))*density_sum_class<NewLine><NewLine>if self.class_wgt == 'equi':<NewLine>    q_hels_obs_terms = 0.5*centroid_class<NewLine>elif self.class_wgt == 'weighted':<NewLine>    q_hels_obs_terms = (m_class / m)*centroid_class<NewLine>else:<NewLine>    raise ValueError('class_wgt should be ""equi"" or ""weighted""')<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/iffix"">@iffiX</a>,</p><NewLine><p>The usual size of <code>n_samples</code> and <code>n_features</code> can be anything really, since I have written a classifier that can be used for any general datasets. But I will take note of your rule of thumb about if “n_samples * n_features &lt; 1e8, then no need to split by class”.</p><NewLine><p>Yes you are correct, <code>X_prime</code> is always (n_samples, n_features + 1) regardless of encoding.</p><NewLine><p>I see what you mean now by “vectorize”. First, `torch.matmul()’ itself has the “vectorization” feature where it can multiple vectors inside a matrix in a “vectorization” manner. Second, rather than just using 2d tensors, I can use a 3d (or higher) dimension tensors to incorporate the 2 classes into one tensor object and then vectors inside this one higher dimension tensor object can be “vectorized”.</p><NewLine><p>Questions:</p><NewLine><ol><NewLine><li>Can I ask how did you determine the value <code>1e8</code> in <code>n_samples * n_features &lt; 1e8</code>?</li><NewLine><li>Can I ask why is there a need to add <code>1e-6</code> in <code>(m_class + 1e-6)</code>?</li><NewLine><li>So for my code, it is difficult to parallelize over multiple GPUs at all? It would be nice if at most I could perhaps just parallelize over the 2 classes on 2 GPUs.</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>a general estimation from experience, its very rough, for your algorithm it could be anywhere between 1e6 to 1e9 depending on the platform and <code>n_features</code>.</li><NewLine><li>prevent zero devision, if denominator &gt; 0</li><NewLine><li>For multiple gpus, split by class, coarser ganularity splits fits gpus better.</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry what did you mean by “coarser ganularity splits fits gpus better”?</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>Simply: give larger chunks of data to GPU, they are beasts.</p><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok many thanks once again <a class=""mention"" href=""/u/iffix"">@iffiX</a></p><NewLine><p>With parallelizing over the 2 GPUs, I am thinking I guess I can use this:</p><NewLine><pre><code class=""lang-auto"">device = torch.device(""cuda:0"")<NewLine>X1 = X1.to(device)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">device = torch.device(""cuda:1"")<NewLine>X2 = X2.to(device)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/iffix"">@iffiX</a>, really sorry to bother you but would be great if I could get your input on one more question.</p><NewLine><p>If I were to follow the code that you had suggested, where every row vector (of the dataset, or more specifically of  <code>X_prime</code> ) is converted to a matrix (ie. a density) and then these matrices are all put into the one tensor object, I think this will run into memory blow-out issues. For eg., if my dataset has say 200,000 rows, then I will have 200,000 matrices that needs to be stored at one time, and this could cause a memory blow-out.</p><NewLine><p>In comparison, the code that I have calculates the running sum of the matrices/densities at every step when a new row vector is converted to a matrix, therefore I do not need to store all the matrices in one tensor object (and then only sum them all up after).</p><NewLine><p>Just wanted to get your thoughts on this. It’s ok too if you’re unsure.</p><NewLine><p>Many thanks once again.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Leockl; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Leockl; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Leockl; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Leockl; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/Leockl; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/Leockl; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/Leockl; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/Leockl; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/Leockl; <NewLine> ,"REPLY_DATE 1: July 27, 2020, 12:54pm; <NewLine> REPLY_DATE 2: July 27, 2020,  4:42pm; <NewLine> REPLY_DATE 3: July 27, 2020,  4:47pm; <NewLine> REPLY_DATE 4: July 27, 2020,  4:59pm; <NewLine> REPLY_DATE 5: July 27, 2020,  6:01pm; <NewLine> REPLY_DATE 6: July 27, 2020,  5:51pm; <NewLine> REPLY_DATE 7: July 27, 2020,  6:00pm; <NewLine> REPLY_DATE 8: July 27, 2020,  7:33pm; <NewLine> REPLY_DATE 9: July 28, 2020,  2:42am; <NewLine> REPLY_DATE 10: July 28, 2020,  8:27am; <NewLine> REPLY_DATE 11: July 28, 2020,  7:29am; <NewLine> REPLY_DATE 12: July 28, 2020,  1:24pm; <NewLine> REPLY_DATE 13: July 28, 2020,  1:49pm; <NewLine> REPLY_DATE 14: July 28, 2020,  4:51pm; <NewLine> REPLY_DATE 15: July 28, 2020,  5:20pm; <NewLine> REPLY_DATE 16: July 28, 2020,  5:21pm; <NewLine> REPLY_DATE 17: July 28, 2020,  5:22pm; <NewLine> REPLY_DATE 18: July 28, 2020,  6:05pm; <NewLine> REPLY_DATE 19: July 29, 2020,  4:44pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 2 Likes; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: 1 Like; <NewLine> REPLY 14 LIKES: 1 Like; <NewLine> REPLY 15 LIKES: 1 Like; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: 1 Like; <NewLine> REPLY 18 LIKES: ; <NewLine> REPLY 19 LIKES: ; <NewLine> 
81539,Distributed Training slower than DataParallel,2020-05-16T18:09:25.601Z,7,502,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,<br/><NewLine>I have been using DataParallel so far to train on single-node multiple machines. As i have seen on the forum here that DistributedDataParallel is preferred even for single node and multiple GPUs. So i switched to Distributed training.<br/><NewLine>My network is kind of large with numerous 3D convolutions so i can only fit a batch size of 1 (stereo image pair) on a single GPU.<br/><NewLine>I have noticed that the time taken by BackwardPass increases from 0.7 secs to 1.3 secs.<br/><NewLine>I am have setup the distributed setup as following.<br/><NewLine>Also GPU utilization is low. Can you kindly suggest what shall be done to increase GPU utilization and reduce backward pass time.<br/><NewLine>p.s DataLoading does not seem to be the bottleneck as it currently takes 0.08 secs.</p><NewLine><pre><code class=""lang-auto"">if config.distributed_training.enable:<NewLine>     logging.info(f""spawning multiprocesses with {config.distributed_training.num_gpus} gpus"")<NewLine>     multiprocessing.spawn(  # type: ignore<NewLine>           _train_model,<NewLine>           nprocs=config.distributed_training.num_gpus,<NewLine>           args=(pretrained_model, config, train_dataset, output_dir),<NewLine>      )<NewLine>def _train_model(<NewLine>    gpu_index: int, pretrained_model: str, config: CfgNode, train_dataset: Dataset, output_dir: Path<NewLine>) -&gt; None:<NewLine>    train_sampler = None<NewLine>    world_size = _get_world_size(config)<NewLine>    local_rank = gpu_index<NewLine><NewLine>    if config.distributed_training.enable:<NewLine>        local_rank = _setup_distributed_process(gpu_index, world_size, config)<NewLine><NewLine>    train_sampler = torch.utils.data.DistributedSampler(<NewLine>        train_dataset, num_replicas=world_size, rank=local_rank<NewLine>    )<NewLine><NewLine>    model = MyModel(config) <NewLine>    torch.cuda.set_device(local_rank)<NewLine>    _transfer_model_to_device(model, local_rank, gpu_index, config)<NewLine>    .........<NewLine><NewLine>def _setup_distributed_process(gpu_index: int, world_size: int, config: CfgNode) -&gt; int:<NewLine>    logging.info(""Setting Distributed DataParallel ...."")<NewLine>    num_gpus = config.distributed_training.num_gpus<NewLine>    local_rank = config.distributed_training.ranking_within_nodes * num_gpus + gpu_index<NewLine>    torch.cuda.set_device(local_rank)<NewLine>    _init_process(rank=local_rank, world_size=world_size, backend=""nccl"")<NewLine>    logging.info(f""Done..."")<NewLine>    return local_rank<NewLine><NewLine>def _init_process(rank: int, world_size: int, backend=""gloo""):<NewLine>    """""" Initialize the distributed environment. """"""<NewLine>    os.environ[""MASTER_ADDR""] = ""localhost""<NewLine>    os.environ[""MASTER_PORT""] = ""29500""<NewLine>    dist.init_process_group(  # type:ignore<NewLine>        backend=backend, init_method=""env://"", world_size=world_size, rank=rank<NewLine>    )<NewLine><NewLine>def _transfer_model_to_device(model: nn.Module, local_rank: int, gpu_index: int, config: CfgNode) -&gt; None:<NewLine>    if config.distributed_training.enable:<NewLine>        model.cuda(local_rank)<NewLine>        model = torch.nn.parallel.DistributedDataParallel(<NewLine>            model, device_ids=[local_rank], output_device=[local_rank]  # type:ignore<NewLine>        )<NewLine>    elif torch.cuda.device_count() &gt; 1:<NewLine>        model = torch.nn.DataParallel(model).cuda()<NewLine>    else:<NewLine>        torch.cuda.set_device(gpu_index)<NewLine>        model = model.cuda(gpu_index)<NewLine><NewLine></code></pre><NewLine><p>GPU utilization is the following<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/ecd3d490f76b400af27da179145169cac79fb182"" href=""https://discuss.pytorch.org/uploads/default/original/3X/e/c/ecd3d490f76b400af27da179145169cac79fb182.jpeg"" title=""Screen Shot 2020-05-16 at 7.07.38 PM""><img alt=""Screen Shot 2020-05-16 at 7.07.38 PM"" data-base62-sha1=""xN4wHrjHaDyUR9Q1KGNaIqcNlOq"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/c/ecd3d490f76b400af27da179145169cac79fb182_2_10x10.png"" height=""448"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/c/ecd3d490f76b400af27da179145169cac79fb182_2_690x448.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/c/ecd3d490f76b400af27da179145169cac79fb182_2_690x448.jpeg, https://discuss.pytorch.org/uploads/default/optimized/3X/e/c/ecd3d490f76b400af27da179145169cac79fb182_2_1035x672.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/e/c/ecd3d490f76b400af27da179145169cac79fb182.jpeg 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screen Shot 2020-05-16 at 7.07.38 PM</span><span class=""informations"">1316×856 286 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/tyb_10,,tyb_10,"May 16, 2020,  6:21pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""81539"" data-username=""tyb_10""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/t/8baadc/40.png"" width=""20""/> tyb_10:</div><NewLine><blockquote><NewLine><p>I have noticed that the time taken by BackwardPass increases from 0.7 secs to 1.3 secs.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Have you tried using the NCCL backend? It should be considerably faster than Gloo.</p><NewLine><p>And have measured the time spent on the entire iteration? Most overhead (replicating model, scatter input, gather output) of DataParallel is incurred during the forward pass.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> thank you for your reply.<br/><NewLine>I am using the <code>nccl</code> backend actually.<br/><NewLine>Following line from the code above.</p><NewLine><pre><code class=""lang-auto"">_init_process(rank=local_rank, world_size=world_size, backend=""nccl"")<NewLine></code></pre><NewLine><p>Yes, I have measured the time taken over the entire iteration for both Distributed and DataParallel.<br/><NewLine>The forward pass takes similar time in both or is a bit faster in DistributedDataParallel (0.75 secs vs 0.8secs in DataParallel).<br/><NewLine>The overall iteration time in DataParallel is 1.75 secs vs 2.4 secs DistributedDataParallel, where similar time is spend in Dataloading (~0.09 secs).</p><NewLine><p>p.s just saw a typo in the first line of my post. My scenario is Single-node multiple GPUs (not machines).</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/tyb_10"">@tyb_10</a>, I tried a toy model locally, but cannot reproduce this behavior. With 2 GPUs, the code below shows DP is about 9X slower than DDP. Can you try this code in your environment, or can you share a min repro of your code that I can try locally?</p><NewLine><pre><code class=""lang-auto"">DP execution time (ms) by CUDA event: 2938.427490234375<NewLine>DP execution time (s) by Python time: 2.9386751651763916 <NewLine>DDP rank-1 execution time (ms) by CUDA event 326.289306640625<NewLine>DDP rank-0 execution time (ms) by CUDA event 326.19061279296875<NewLine>DDP rank-1 execution time (s) by Python time 0.3264338970184326 <NewLine>DDP rank-0 execution time (s) by Python time 0.32636237144470215 <NewLine></code></pre><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.distributed as dist<NewLine>import torch.multiprocessing as mp<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine>from torch.nn.parallel import DistributedDataParallel as DDP<NewLine>from torch.nn.parallel import DataParallel as DP<NewLine><NewLine>import time<NewLine><NewLine>X = 100<NewLine>B = 200<NewLine><NewLine>def ddp_example(rank, world_size):<NewLine>    # create default process group<NewLine>    dist.init_process_group(""gloo"", rank=rank, world_size=world_size)<NewLine>    b = B // world_size<NewLine>    # create local model<NewLine>    model = nn.Linear(X, X).to(rank)<NewLine>    # construct DDP model<NewLine>    ddp_model = DDP(model, device_ids=[rank])<NewLine>    # define loss function and optimizer<NewLine>    loss_fn = nn.MSELoss()<NewLine>    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)<NewLine><NewLine>    with torch.cuda.device(rank):<NewLine>        tik = time.time()<NewLine>        start = torch.cuda.Event(enable_timing=True)<NewLine>        end = torch.cuda.Event(enable_timing=True)<NewLine>        start.record()<NewLine>        for _ in range(20):<NewLine>            # forward pass<NewLine>            outputs = ddp_model(torch.randn(b, X).to(rank))<NewLine>            labels = torch.randn(b, X).to(rank)<NewLine>            # backward pass<NewLine>            loss_fn(outputs, labels).backward()<NewLine>            # update parameters<NewLine>            optimizer.step()<NewLine>        end.record()<NewLine>        print(f""DDP rank-{rank} execution time (ms) by CUDA event {start.elapsed_time(end)}"")<NewLine>        torch.cuda.synchronize()<NewLine>        tok = time.time()<NewLine>        print(f""DDP rank-{rank} execution time (s) by Python time {tok - tik} "")<NewLine><NewLine><NewLine>def dp_example():<NewLine>    b = B  # don't need to divide by 2 here as DataParallel will scatter inputs<NewLine>    model = nn.Linear(X, X).to(0)<NewLine>    # construct DDP model<NewLine>    dp_model = DP(model, device_ids=[0, 1])<NewLine>    # define loss function and optimizer<NewLine>    loss_fn = nn.MSELoss()<NewLine>    optimizer = optim.SGD(dp_model.parameters(), lr=0.001)<NewLine><NewLine>    tik = time.time()<NewLine>    start = torch.cuda.Event(enable_timing=True)<NewLine>    end = torch.cuda.Event(enable_timing=True)<NewLine>    start.record()<NewLine>    for _ in range(20):<NewLine>        # forward pass<NewLine>        outputs = dp_model(torch.randn(b, X).to(0))<NewLine>        labels = torch.randn(b, X).to(0)<NewLine>        # backward pass<NewLine>        loss_fn(outputs, labels).backward()<NewLine>        # update parameters<NewLine>        optimizer.step()<NewLine>    end.record()<NewLine>    print(f""DP execution time (ms) by CUDA event: {start.elapsed_time(end)}"")<NewLine>    torch.cuda.synchronize()<NewLine>    tok = time.time()<NewLine>    print(f""DP execution time (s) by Python time: {tok - tik} "")<NewLine><NewLine><NewLine>def main():<NewLine>    dp_example()<NewLine><NewLine>    world_size = 2<NewLine>    mp.spawn(ddp_example,<NewLine>        args=(world_size,),<NewLine>        nprocs=world_size,<NewLine>        join=True)<NewLine><NewLine>if __name__==""__main__"":<NewLine>    main()<NewLine><NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> thanks a lot for the toy example.<br/><NewLine>I can reproduce your results and DDP is indeed faster than DP in this case.<br/><NewLine>I will debug my code shortly and post here the outcome if the problem still persists or otherwise the solution that fixed the issue.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> so i debugged the code and found one bug that i was not returning the wrapped model to my parent function.<br/><NewLine>so after these changes DDP and DP take similar time ~ 1.6 seconds per iteration but still DDP is not faster than DP in my case.</p><NewLine><pre><code class=""lang-auto"">.....<NewLine>if config.distributed_training.enable:<NewLine>     logging.info(f""spawning multiprocesses with {config.distributed_training.num_gpus} gpus"")<NewLine>     multiprocessing.spawn(  # type: ignore<NewLine>           _train_model,<NewLine>           nprocs=config.distributed_training.num_gpus,<NewLine>           args=(pretrained_model, config, train_dataset, output_dir),<NewLine>      )<NewLine><NewLine>def _train_model(<NewLine>    gpu_index: int, pretrained_model: str, config: CfgNode, train_dataset: Dataset, output_dir: Path<NewLine>) -&gt; nn.Module:  **# this was returning None before**<NewLine>    train_sampler = None<NewLine>    world_size = _get_world_size(config)<NewLine>    local_rank = gpu_index<NewLine>    device = None<NewLine>    if config.distributed_training.enable:<NewLine>        local_rank = _setup_distributed_process(gpu_index, world_size, config)<NewLine><NewLine>        train_sampler = torch.utils.data.DistributedSampler(<NewLine>            train_dataset, num_replicas=world_size, rank=local_rank<NewLine>        )<NewLine>        device = torch.device(local_rank)<NewLine>    else:<NewLine>        device = torch.device(""cuda"")<NewLine><NewLine>    model = MyModel(config) <NewLine>    model = _transfer_model_to_device(model, local_rank, gpu_index, config) # **now i assing to the model** <NewLine>    dataloader = _initialize_data_loader(train_dataset, config, train_sampler)<NewLine>    _execute_training(config, device, model, train_sampler, dataloader, output_dir)<NewLine><NewLine>def _execute_training(<NewLine>    config: CfgNode,<NewLine>    device: torch.device,<NewLine>    model: nn.Module,<NewLine>    train_sampler: Optional[torch.utils.data.DistributedSampler],<NewLine>    dataloader: DataLoader,<NewLine>    output_dir: Path,<NewLine>) -&gt; None:<NewLine>    network_config = config.network_config<NewLine>    loss_combiner = MultiTaskLoss(num_tasks=2).to(device)<NewLine>    trainable_params_model = list(filter(lambda p: p.requires_grad, model.parameters()))<NewLine>    trainable_params = trainable_params_model + list(loss_combiner.parameters())<NewLine>    optimizer = optim.Adam(<NewLine>        trainable_params, lr=network_config.learning_rate, betas=(0.9, 0.99), weight_decay=0.001<NewLine>    )<NewLine>    logging.info(f""logging to tensorboard at {DEFAULT_TENSORBOARD_LOG_LOCATION}"")<NewLine>    with SummaryWriter(DEFAULT_TENSORBOARD_LOG_LOCATION) as summary_writer:  # type: ignore<NewLine>        for epoch_idx in range(config.network_config.epochs):<NewLine>            if config.distributed_training.enable and train_sampler is not None:<NewLine>                train_sampler.set_epoch(epoch_idx)<NewLine>            logging.info(f""starting epoch {epoch_idx}"")<NewLine>            _adjust_learning_rate(optimizer, epoch_idx, network_config.learning_rate, network_config.lrepochs)<NewLine>             #**This is the entry point to the rest of the agnostic code that is same for both DP and DDP**<NewLine>            _train_batches(<NewLine>                epoch_idx, dataloader, model, optimizer, device, config, loss_combiner, summary_writer<NewLine>            )<NewLine><NewLine><NewLine>def _initialize_data_loader(<NewLine>    train_dataset: Dataset, config: CfgNode, train_sampler: Optional[torch.utils.data.DistributedSampler]<NewLine>) -&gt; DataLoader:<NewLine>    network_config = config.network_config<NewLine>    dataloader = DataLoader(<NewLine>        train_dataset,<NewLine>        network_config.batch_size,<NewLine>        collate_fn=custom_collate,<NewLine>        shuffle=network_config.training_data_shuffle,<NewLine>        num_workers=network_config.data_loader_num_workers,<NewLine>        drop_last=network_config.drop_last_training_sample,<NewLine>        pin_memory=network_config.data_loader_pin_memory,<NewLine>        sampler=train_sampler,<NewLine>    )<NewLine>    return dataloader<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Does your model use any buffers? You can check that by running <code>list(mode.buffers())</code>.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> doesn’t look like it , it prints empty list <img alt="":neutral_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/neutral_face.png?v=9"" title="":neutral_face:""/></p><NewLine><pre><code class=""lang-auto"">print(list(model.buffers()))<NewLine>[]```</code></pre><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""81539"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mrshenli/40/12220_2.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">    print(f""DP execution time (ms) by CUDA event: {start.elapsed_time(end)}"")<NewLine>    torch.cuda.synchronize()<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>Shouldn’t it be</p><NewLine><pre><code class=""lang-auto"">torch.cuda.synchronize()<NewLine>print(f""DP execution time (ms) by CUDA event: {start.elapsed_time(end)}"")<NewLine></code></pre><NewLine><p>Coz I got <code>RuntimeError: CUDA error: device not ready</code> when execute your code as it is.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tyb_10; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/tyb_10; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/tyb_10; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/tyb_10; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/hilman-dayo; <NewLine> ,"REPLY_DATE 1: May 17, 2020,  6:51pm; <NewLine> REPLY_DATE 2: May 17, 2020,  8:43pm; <NewLine> REPLY_DATE 3: May 18, 2020,  2:24pm; <NewLine> REPLY_DATE 4: May 18, 2020,  5:53pm; <NewLine> REPLY_DATE 5: May 19, 2020, 10:44am; <NewLine> REPLY_DATE 6: May 19, 2020,  2:39pm; <NewLine> REPLY_DATE 7: May 19, 2020,  6:21pm; <NewLine> REPLY_DATE 8: September 5, 2020,  1:58pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> 
92977,Why does Pytorch only find one physical CPU?,2020-08-16T07:53:17.391Z,5,201,"<div class=""post"" itemprop=""articleBody""><NewLine><h4>Problem description：</h4><NewLine><p>I compile the pytorch source code in arm machine.And I want to use DDP interface for<br/><NewLine>distributed training.However, I found that pytorch could only find one physical CPU, which means that my CPU usage cannot exceed 50%.(The machine has two sockets)</p><NewLine><p>My machine contains two physical Cpus, each with 64 cores.</p><NewLine><p>I use OpenBLAS as the BLAS and I compile it with openmp.In the script, I set the environment variable</p><NewLine><pre><code class=""lang-auto"">export OMP_NUM_THREADS=128<NewLine>export GOMP_CPU_AFFINITY=0-127<NewLine>export OMP_DISPLAY_ENV=true<NewLine></code></pre><NewLine><p>Then when I execute my script</p><NewLine><pre><code class=""lang-auto"">python3 -m torch.distributed.launch \<NewLine>--nproc_per_node=$NPROC_PER_NODE \<NewLine>script.py 2&gt;&amp;1<NewLine></code></pre><NewLine><p>Then I found out that all the threads were running on the same CPU core and it ouput:</p><NewLine><pre><code class=""lang-auto"">OPENMP DISPLAY ENVIRONMENT BEGIN<NewLine>  _OPENMP = '201511'<NewLine>  OMP_DYNAMIC = 'FALSE'<NewLine>  OMP_NESTED = 'FALSE'<NewLine>  OMP_NUM_THREADS = '64'<NewLine>  OMP_SCHEDULE = 'DYNAMIC'<NewLine>  OMP_PROC_BIND = 'TRUE'<NewLine>  OMP_PLACES = '{0},{1},{2},{3},{4},{5},{6},{7},{8},{9},{10},{11},{12},{13},{14},{15},{16},{17},{18},{19},{20},{21},{22},{23},{24},{25},{26},{27},{28},{29},{30},{31},{32},{33},{34},{35},{36},{37},{38},{39},{40},{41},{42},{43},{44},{45},{46},{47},{48},{49},{50},{51},{52},{53},{54},{55},{56},{57},{58},{59},{60},{61},{62},{63},{64},{65},{66},{67},{68},{69},{70},{71},{72},{73},{74},{75},{76},{77},{78},{79},{80},{81},{82},{83},{84},{85},{86},{87},{88},{89},{90},{91},{92},{93},{94},{95},{96},{97},{98},{99},{100},{101},{102},{103},{104},{105},{106},{107},{108},{109},{110},{111},{112},{113},{114},{115},{116},{117},{118},{119},{120},{121},{122},{123},{124},{125},{126},{127}'<NewLine>  OMP_STACKSIZE = '0'<NewLine>  OMP_WAIT_POLICY = 'PASSIVE'<NewLine>  OMP_THREAD_LIMIT = '4294967295'<NewLine>  OMP_MAX_ACTIVE_LEVELS = '2147483647'<NewLine>  OMP_CANCELLATION = 'FALSE'<NewLine>  OMP_DEFAULT_DEVICE = '0'<NewLine>  OMP_MAX_TASK_PRIORITY = '0'<NewLine>  OMP_DISPLAY_AFFINITY = 'FALSE'<NewLine>  OMP_AFFINITY_FORMAT = 'level %L thread %i affinity %A'<NewLine>OPENMP DISPLAY ENVIRONMENT END<NewLine><NewLine>OPENMP DISPLAY ENVIRONMENT BEGIN<NewLine>  _OPENMP = '201511'<NewLine>  OMP_DYNAMIC = 'FALSE'<NewLine>  OMP_NESTED = 'FALSE'<NewLine>  OMP_NUM_THREADS = '64'<NewLine>  OMP_SCHEDULE = 'DYNAMIC'<NewLine>  OMP_PROC_BIND = 'TRUE'<NewLine>  OMP_PLACES = '{0},{1},{2},{3},{4},{5},{6},{7},{8},{9},{10},{11},{12},{13},{14},{15},{16},{17},{18},{19},{20},{21},{22},{23},{24},{25},{26},{27},{28},{29},{30},{31},{32},{33},{34},{35},{36},{37},{38},{39},{40},{41},{42},{43},{44},{45},{46},{47},{48},{49},{50},{51},{52},{53},{54},{55},{56},{57},{58},{59},{60},{61},{62},{63}'<NewLine>  OMP_STACKSIZE = '0'<NewLine>  OMP_WAIT_POLICY = 'PASSIVE'<NewLine>  OMP_THREAD_LIMIT = '4294967295'<NewLine>  OMP_MAX_ACTIVE_LEVELS = '2147483647'<NewLine>  OMP_CANCELLATION = 'FALSE'<NewLine>  OMP_DEFAULT_DEVICE = '0'<NewLine>  OMP_MAX_TASK_PRIORITY = '0'<NewLine>  OMP_DISPLAY_AFFINITY = 'FALSE'<NewLine>  OMP_AFFINITY_FORMAT = 'level %L thread %i affinity %A'<NewLine>OPENMP DISPLAY ENVIRONMENT END<NewLine></code></pre><NewLine><p>Could someone tell me what is the reason? Thanks!</p><NewLine><h4>environment</h4><NewLine><pre><code class=""lang-auto"">Collecting environment information...<NewLine>PyTorch version: 1.6.0a0+b31f58d<NewLine>Is debug build: No<NewLine>CUDA used to build PyTorch: None<NewLine><NewLine>OS: CentOS Linux release 7.6.1810 (AltArch)<NewLine>GCC version: (GCC) 9.2.0<NewLine>CMake version: version 3.16.5<NewLine><NewLine>Python version: 3.7<NewLine>Is CUDA available: No<NewLine>CUDA runtime version: No CUDA<NewLine>GPU models and configuration: No CUDA<NewLine>Nvidia driver version: No CUDA<NewLine>cuDNN version: No CUDA<NewLine><NewLine>Versions of relevant libraries:<NewLine>[pip3] numpy==1.19.1<NewLine>[pip3] torch==1.6.0a0+b31f58d<NewLine>[conda] Could not collect<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/khalil,(khalil li),khalil,"August 16, 2020,  7:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>From the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel"" rel=""nofollow noopener"">DDP Docs</a>, you must do the following when initializing DDP:</p><NewLine><blockquote><NewLine><p>For multi-device modules and CPU modules, device_ids must be None or an empty list, and input data for the forward pass must be placed on the correct device.</p><NewLine></blockquote><NewLine><p>While you cannot specify which cores to run each process on from PyTorch, you should still be able to specify CPU affinity in general.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply!<br/><NewLine>Yeah, I did what they said:</p><NewLine><pre><code class=""lang-auto"">model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[], output_device=[])<NewLine></code></pre><NewLine><p>I also think I am able to specify CPU affinity,however I am failed.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/vitalyfedyunin"">@VitalyFedyunin</a> <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> is it possible to specify CPU affinity when using PyTorch?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>It should be possible to set the CPU affinity using <a href=""https://docs.nvidia.com/deploy/nvml-api/nvml-api-reference.html#nvml-api-reference"">NVML</a> and Tesla GPUs for DDP.<br/><NewLine>You could probably use <code>pynvml</code> as a convenient Python API to create the affinity list and set it via <code>os.sched_setaffinity</code>.<br/><NewLine>However, I haven’t played around with it a lot.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply, I have tried it but it did nothing. I implement it just like this:</p><NewLine><pre><code class=""lang-auto"">#set cpu affinity<NewLine>    pid = 0<NewLine>    affinity_mask = {i for i in range(128)}<NewLine>    os.sched_setaffinity(0, affinity_mask)<NewLine>    print(""Number of CPUs:"", os.cpu_count())<NewLine>    affinity = os.sched_getaffinity(pid)<NewLine>    real_pid = os.getpid()<NewLine>    print(""Now, process {} is eligibl to run on:{}"".format(real_pid,affinity))<NewLine></code></pre><NewLine><p>In fact, It did print the process is eligible to run on CPU:0~127</p><NewLine><pre><code class=""lang-auto"">Now, process 34094  is eligibl to run on:{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}<NewLine></code></pre><NewLine><p>However,when I use <code>ps -eLF</code> to check the threads of this process, it still uses the half of the CPU cores.<br/><NewLine>I set this environment <code>export OMP_DISPLAY_ENV=true</code> and then the script print the OpenMP message twice.Do you know what are the OpenMP calls in there two places?<br/><NewLine>As shown above,the first time <code>OMP_PLACES</code> is 0-127,but the second time it becomes 0-63.</p><NewLine><p>If I do not use DDP and just execute the py script,then I found there is just one OpenMP message.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/khalil"">@khalil</a> Could you describe your DDP setup? Are you running DDP on a single machine with multiple processes, if so how many processes per host? Or are you running DDP across multiple machines here?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am sorry for taking so long time to reply you. I found the problem is not caused by DDP. It is caused by the __init__.py file in the torch directory.</p><NewLine><p>I try to avoid load this __init__.py and the problem is disappeared. I think there are some library problem in my machine. Thanks for your reply.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/osalpekar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/khalil; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/khalil; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/khalil; <NewLine> ,"REPLY_DATE 1: August 17, 2020, 11:24pm; <NewLine> REPLY_DATE 2: August 18, 2020,  3:07am; <NewLine> REPLY_DATE 3: August 18, 2020,  8:10pm; <NewLine> REPLY_DATE 4: August 19, 2020,  4:01am; <NewLine> REPLY_DATE 5: August 19, 2020,  5:57am; <NewLine> REPLY_DATE 6: August 24, 2020, 10:06pm; <NewLine> REPLY_DATE 7: September 4, 2020,  6:07am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> 
46263,Distributed training hangs,2019-05-26T21:29:51.651Z,0,1332,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The training hangs without printing any logs. Observations/configurations:</p><NewLine><ul><NewLine><li>4 nodes. 4 GPU/node.</li><NewLine><li>distributed training with each process taking 1 GPU.</li><NewLine><li>pytorch version = 1.1; cuda version = 9.0; gpu driver version: 410.78</li><NewLine><li>use the code base of facebook/maskrcnn-benchmark, but i thought it is just normal pytorch code.</li><NewLine><li>GPU utility is close to 100%, but there is no more log.</li><NewLine><li>it has finished 278K iterations, and then hangs there without any progress (no more snapshot, no more logs)</li><NewLine><li>gdb attached to one of the process (sudo gdb -p process_id) and it seems like it hangs at cuMemcpyHtoDAsync_v2.</li><NewLine></ul><NewLine><pre><code class=""lang-auto"">(gdb) where<NewLine>#0  0x00007ffe309e1b6d in clock_gettime ()<NewLine>#1  0x00007f8cc536f876 in __GI___clock_gettime (clock_id=4, tp=0x7ffe30898660) at ../sysdeps/unix/clock_gettime.c:115<NewLine>#2  0x00007f8c6c7ecc4e in ?? () from /usr/local/nvidia/lib64/libcuda.so.1<NewLine>#3  0x00007f8c6c87b8d3 in ?? () from /usr/local/nvidia/lib64/libcuda.so.1<NewLine>#4  0x00007f8c6c89b81f in ?? () from /usr/local/nvidia/lib64/libcuda.so.1<NewLine>#5  0x00007f8c6c7c8737 in ?? () from /usr/local/nvidia/lib64/libcuda.so.1<NewLine>#6  0x00007f8c6c6d9e4e in ?? () from /usr/local/nvidia/lib64/libcuda.so.1<NewLine>#7  0x00007f8c6c6dbfc3 in ?? () from /usr/local/nvidia/lib64/libcuda.so.1<NewLine>#8  0x00007f8c6c829c82 in cuMemcpyHtoDAsync_v2 () from /usr/local/nvidia/lib64/libcuda.so.1<NewLine>#9  0x00007f8cbe7ad49c in ?? () from /opt/conda/lib/python3.6/site-packages/torch/lib/libcudart-f7fdd8d7.so.9.0<NewLine>#10 0x00007f8cbe78a573 in ?? () from /opt/conda/lib/python3.6/site-packages/torch/lib/libcudart-f7fdd8d7.so.9.0<NewLine>#11 0x00007f8cbe7c3d86 in cudaMemcpyAsync () from /opt/conda/lib/python3.6/site-packages/torch/lib/libcudart-f7fdd8d7.so.9.0<NewLine>#12 0x00007f8c836a9f4b in (anonymous namespace)::copy_from_cpu(at::Tensor&amp;, at::Tensor const&amp;) () from /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so<NewLine>#13 0x00007f8c8374a875 in void (anonymous namespace)::_copy__cuda&lt;float&gt;(at::Tensor&amp;, at::Tensor const&amp;, bool) () from /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so<NewLine>#14 0x00007f8c836aafb8 in at::native::_s_copy__cuda(at::Tensor&amp;, at::Tensor const&amp;, bool) () from /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so<NewLine>#15 0x00007f8c826d47ef in at::CUDAType::s_copy_(at::Tensor&amp;, at::Tensor const&amp;, bool) const () from /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so<NewLine>#16 0x00007f8c7764033d in at::native::copy_(at::Tensor&amp;, at::Tensor const&amp;, bool) () from /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2.so<NewLine>#17 0x00007f8cbf546dc9 in torch::autograd::VariableType::copy_(at::Tensor&amp;, at::Tensor const&amp;, bool) const () from /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so.1<NewLine>#18 0x00007f8c777829cc in at::native::to(at::Tensor const&amp;, c10::TensorOptions const&amp;, bool, bool) () from /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2.so<NewLine>#19 0x00007f8c77a01857 in at::TypeDefault::to(at::Tensor const&amp;, c10::TensorOptions const&amp;, bool, bool) const () from /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2.so<NewLine>#20 0x00007f8cbf31cb52 in torch::autograd::VariableType::to(at::Tensor const&amp;, c10::TensorOptions const&amp;, bool, bool) const ()<NewLine>   from /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so.1<NewLine>#21 0x00007f8cc0bb8eb3 in torch::autograd::dispatch_to(at::Tensor const&amp;, c10::Device, bool, bool) () from /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so<NewLine>#22 0x00007f8cc0bb9598 in torch::autograd::THPVariable_to(_object*, _object*, _object*) () from /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so<NewLine>#23 0x0000556d518096a6 in PyCFunction_Call () at /tmp/build/80754af9/python_1546130271559/work/Objects/methodobject.c:98<NewLine>#24 0x0000556d518b74ad in do_call_core (kwdict=0x7f8b9b42b168, callargs=0x7f8bb1deec18, func=0x7f8b9b42b510) at /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:5116<NewLine>#25 _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1546130271559/work/Python/ceval.c:3404<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/amsword,(Jianfeng Wang),amsword,"May 26, 2019,  9:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>found the issue.</p><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/20630"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://github.com/prafullasd"" rel=""nofollow noopener""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars1.githubusercontent.com/u/5523595?v=2&amp;s=96"" width=""60""/><NewLine></a><NewLine><h4><a href=""https://github.com/pytorch/pytorch/issues/20630"" rel=""nofollow noopener"" target=""_blank"">Issue: distributed all_reduce deadlocks in v1.1</a></h4><NewLine><div class=""date"" style=""margin-top:10px;""><NewLine><div class=""user"" style=""margin-top:10px;""><NewLine>	opened by <a href=""https://github.com/prafullasd"" rel=""nofollow noopener"" target=""_blank"">prafullasd</a><NewLine>	on <a href=""https://github.com/pytorch/pytorch/issues/20630"" rel=""nofollow noopener"" target=""_blank"">2019-05-17</a><NewLine></div><NewLine><div class=""user""><NewLine>	closed by <a href=""https://github.com/facebook-github-bot"" rel=""nofollow noopener"" target=""_blank"">facebook-github-bot</a><NewLine>	on <a href=""https://github.com/pytorch/pytorch/issues/20630"" rel=""nofollow noopener"" target=""_blank"">2019-05-26</a><NewLine></div><NewLine></div><NewLine><pre class=""content"" style=""white-space: pre-wrap;"">🐛 Bug<NewLine>I'm doing multi-node training (8 nodes, 8 gpu's each, NCCL backend) and am using DistributedDataParallel for syncing grads and distributed.all_reduce()...</pre><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">module: cuda</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">module: distributed</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote quote-modified"" data-post=""9"" data-topic=""8009""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/gemfield/40/27799_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/distributed-data-parallel-freezes-without-error-message/8009/9"">Distributed data parallel freezes without error message</a><NewLine></div><NewLine><blockquote><NewLine>    Env: <NewLine><NewLine>Ubuntu 18.04<NewLine>Pytorch 1.6.0<NewLine>CUDA 10.1<NewLine><NewLine>Actually, I am using Docker image gemfield/pytorch:1.6.0-devel which stated in <a href=""https://github.com/DeepVAC/deepvac"" rel=""nofollow noopener"">https://github.com/DeepVAC/deepvac</a> (same with above env), and use PyTorch DDP (by use the class DeepvacDDP in <a href=""https://github.com/DeepVAC/deepvac/blob/master/deepvac/syszux_deepvac.py"" rel=""nofollow noopener"">https://github.com/DeepVAC/deepvac/blob/master/deepvac/syszux_deepvac.py</a>) to train my model, which the code worked perfect yesterday. But today when I launch the train program again, the DDP is stucked in loss.backward()， with cpu 100% and GPU 100%。 <NewLine>There has no co…<NewLine>  </blockquote><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/amsword; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/gemfield; <NewLine> ,"REPLY_DATE 1: May 26, 2019, 10:18pm; <NewLine> REPLY_DATE 2: September 3, 2020,  1:04pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
93865,Distributed Data Parallel slower than Data Parallel,2020-08-24T06:52:22.221Z,9,243,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, there.</p><NewLine><p>I have implemented a Cifar10 classifier using the Data Parallel of Pytorch, and then I changed the program to use the Distributed Data Parallel. I was surprised at that the program has become very slow. Using 8 GPUs (K80) with a batch size of 4096, the Distributed Data Parallel program spends 47 seconds to train a Resnet 34 model for one epoch, while the Data Parallel program took only 32 seconds.</p><NewLine><p>I run the program on a cloud environment with 8 vCPU with 52GBytes of memory, and it does not seem to be a data transfer problem. So, I roughly measured the time spent for each task within the DP and DDP processes. The results are shown below.</p><NewLine><p>DP<br/><NewLine><img alt=""image"" data-base62-sha1=""bILdqQsOD1YfV0cuUeD8Sz4Xw30"" height=""87"" src=""https://discuss.pytorch.org/uploads/default/original/3X/5/2/52273c938d7829272f3634ef6d66750a00ed6cc2.png"" width=""632""/></p><NewLine><p>DDP<br/><NewLine><img alt=""image"" data-base62-sha1=""y4Wea03xcsSdc1KkKP92z9Ngk4M"" height=""92"" src=""https://discuss.pytorch.org/uploads/default/original/3X/e/e/eed8d894575d8749f09365a77b29db6f65031da8.png"" width=""632""/></p><NewLine><p>In the above screen shot, the left most number is value of loss and other numbers are execution time of each task in seconds. The “out” represents the forward path and “back” represents the backward. As you can see, DDP takes more than twice of the computation time compared to DP for both the forward and backward path. I do not understand why this happens.</p><NewLine><p>I suppose that this <a href=""https://github.com/pytorch/pytorch/issues/29939"" rel=""nofollow noopener"">post</a> discusses the same issue, and it seems that the issue has been addressed.</p><NewLine><p>However, it still happens in my program. The torch version of the program is 1.4.0. Should I update the version to solve the problem? Or, should I use Apex Distributed Data Parallel?</p><NewLine></div>",https://discuss.pytorch.org/u/TT_YY,(TT YY),TT_YY,"August 24, 2020,  6:57am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""93865"" data-username=""TT_YY""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tt_yy/40/25673_2.png"" width=""20""/> TT_YY:</div><NewLine><blockquote><NewLine><p>Using 8 GPUs (K80) with a batch size of 4096, the Distributed Data Parallel program spends 47 seconds to train a Resnet 34 model for one epoch, while the Data Parallel program took only 32 seconds.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Does this mean each DDP process is consuming 4096 samples per iteration and the DP process is consuming 4096 * 8 = 32768 samples?</p><NewLine><blockquote><NewLine><p>I suppose that this <a href=""https://github.com/pytorch/pytorch/issues/29939"" rel=""nofollow noopener"">post</a> discusses the same issue, and it seems that the issue has been addressed.</p><NewLine></blockquote><NewLine><p>For the post you mentioned, it is only true for BERT models and it has been addressed in PyTorch v1.6.</p><NewLine><p>BTW, how did you initialize DDP? Could you please share a repro?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply.</p><NewLine><blockquote><NewLine><p>Does this mean each DDP process is consuming 4096 samples per iteration and the DP process is consuming 4096 * 8 = 32768 samples?</p><NewLine></blockquote><NewLine><p>No, I’m talking about the global batch size, which means a DDP process consumes 512 samples per iteration.</p><NewLine><blockquote><NewLine><p>BTW, how did you initialize DDP? Could you please share a repro?</p><NewLine></blockquote><NewLine><p>OK, I will try to provide a short version that can reproduce the performance shortly, since the original program is very long because of automation and visualization. Thank you.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I share here two lists of codes to reproduce the issue. The first one is a program with DP, which is provided for comparison. The second one is with DDP, which takes longer for forward and backward path than DP.</p><NewLine><p>DP</p><NewLine><pre><code class=""lang-auto"">""""""  Training Resnet34 for Cifar10 by Data Parallel """"""<NewLine><NewLine>from __future__ import print_function<NewLine><NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine>import torch.backends.cudnn as cudnn<NewLine>import torchvision<NewLine>import torchvision.transforms as transforms<NewLine><NewLine>import sys<NewLine>import time<NewLine>import argparse<NewLine><NewLine>from models import *<NewLine><NewLine>from sync_batchnorm import convert_model, DataParallelWithCallback<NewLine><NewLine>def main() :<NewLine>    <NewLine>    parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')<NewLine>    parser.add_argument('--net', default='res34')<NewLine>    parser.add_argument('--batch_size', default=4096)<NewLine>    parser.add_argument('--optimizer', default=""Adam"")<NewLine>    parser.add_argument('--epochs', default=2)<NewLine>    parser.add_argument('--n_nodes', default=1) <NewLine>    parser.add_argument('--nr', default=0)<NewLine>    args = parser.parse_args()<NewLine><NewLine>    if torch.cuda.is_available() :<NewLine>        args.n_gpus = torch.cuda.device_count()<NewLine>        print(args.n_gpus, "" GPU(s) available"")<NewLine>        print(torch.cuda.get_device_name(0))<NewLine>        <NewLine>    else :<NewLine>        print(""GPU is NOT available."")   <NewLine>        sys.exit()<NewLine>        <NewLine>    print(""Total batch size = "", args.batch_size)    <NewLine>    print(""Batch size = "", int(args.batch_size / args.n_gpus), ""/ GPU"")    <NewLine>    print(""Optimizer = "", args.optimizer)<NewLine>    <NewLine>    train(args)<NewLine><NewLine>    print()<NewLine><NewLine>       <NewLine># Training<NewLine>def train(args):<NewLine>    <NewLine>    epochs = args.epochs<NewLine>    batch_size = args.batch_size    # total batch_size.<NewLine>    n_gpus = args.n_gpus<NewLine>    <NewLine>    worker = 8<NewLine>      <NewLine>    if args.net=='res18':<NewLine>        net = ResNet18()<NewLine>    elif args.net=='res34':<NewLine>        net = ResNet34()<NewLine>    elif args.net=='res50':<NewLine>        net = ResNet50()<NewLine>    elif args.net=='res101':<NewLine>        net = ResNet101()<NewLine>    <NewLine>    print(""Model = "", net.__class__.__name__)<NewLine>    print()<NewLine>    <NewLine>    d_list = list(range(n_gpus))        <NewLine>    net = convert_model(net).cuda() # Convert BatchNorm into SyncBatchNorm<NewLine>    net = DataParallelWithCallback(net, device_ids = d_list) # Data Parallel<NewLine>      <NewLine>    cudnn.benchmark = True  <NewLine>    <NewLine>    criterion = nn.CrossEntropyLoss()<NewLine>    <NewLine>    if args.optimizer == ""Adam"" :<NewLine>        optimizer = optim.Adam(net.parameters())<NewLine>        <NewLine>    elif args.optimizer == ""SGD"" :<NewLine>        optimizer = optim.SGD(net.parameters(), lr = 0.1)<NewLine>       <NewLine>    transform_list = [<NewLine>                  transforms.RandomChoice([<NewLine>                  transforms.RandomCrop(32, padding=4),<NewLine>                  transforms.RandomResizedCrop(32, scale=(0.7, 1.0), ratio = (1.0, 1.0)),<NewLine>                  ]),<NewLine>                  transforms.RandomHorizontalFlip(),<NewLine>                  transforms.RandomRotation(degrees = 20), <NewLine>                  transforms.ToTensor(),<NewLine>                  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),<NewLine>                  ]<NewLine>                  <NewLine>    transform_train = transforms.Compose(transform_list)<NewLine>    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)<NewLine>    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=worker)<NewLine><NewLine>    for epoch in range(epochs):<NewLine>        <NewLine>        print()<NewLine>        print(""epoch : "",epoch + 1, "" / "", epochs)<NewLine><NewLine>        net.train()        <NewLine>        <NewLine>        """"""   ------- Training loop  -------- """"""<NewLine>   <NewLine>        for batch_idx, (inputs, targets) in enumerate(trainloader):<NewLine>          <NewLine>            inputs, targets = inputs.to('cuda'), targets.to('cuda')    <NewLine>            <NewLine>            message = """"<NewLine>            t0 = time.time() <NewLine>            <NewLine>            optimizer.zero_grad()      <NewLine>            <NewLine>            t1 = time.time() <NewLine>            message += ""  zero grad: {0:.5f}"".format(t1 - t0)<NewLine>            <NewLine>            outputs = net(inputs)<NewLine>            <NewLine>            t2 = time.time() <NewLine>            message += ""  out: {0:.5f}"".format(t2 - t1)<NewLine>            <NewLine>            loss = criterion(outputs, targets)<NewLine>            <NewLine>            t3 = time.time() <NewLine>            message += ""  loss: {0:.5f}"".format(t3 - t2)<NewLine>            <NewLine>            loss.backward()<NewLine>            <NewLine>            t4 = time.time() <NewLine>            message += ""  back: {0:.5f}"".format(t4 - t3)<NewLine>            <NewLine>            loss_val = optimizer.step(loss.item)  # loss value is given through optimizer.<NewLine><NewLine>            t5 = time.time() <NewLine>            message += ""  step: {0:.5f}"".format(t5 - t4)<NewLine>                 <NewLine>            print(""{0:.6f}"".format(loss_val) + message)                    <NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    main()<NewLine>    <NewLine></code></pre><NewLine><p>DDP</p><NewLine><pre><code class=""lang-auto"">""""""  Training Resnet34 for Cifar10 by Distributed Data Parallel """"""<NewLine><NewLine>from __future__ import print_function<NewLine><NewLine>import torch.multiprocessing as mp<NewLine>import torch.distributed as dist<NewLine>from torch.nn.parallel import DistributedDataParallel as DDP<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine>import torch.nn.functional as F<NewLine>import torch.backends.cudnn as cudnn<NewLine>import torchvision<NewLine>import torchvision.transforms as transforms<NewLine><NewLine>import sys<NewLine>import os<NewLine>import time<NewLine>import argparse<NewLine><NewLine>from models import *<NewLine><NewLine>from sync_batchnorm import convert_model, DataParallelWithCallback<NewLine><NewLine>def main() :<NewLine>    <NewLine>    parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')<NewLine>    parser.add_argument('--net', default='res34')<NewLine>    parser.add_argument('--batch_size', default=4096)<NewLine>    parser.add_argument('--optimizer', default=""Adam"")<NewLine>    parser.add_argument('--epochs', default=1)<NewLine>    parser.add_argument('--n_nodes', default=1) <NewLine>    parser.add_argument('--nr', default=0)<NewLine>    args = parser.parse_args()<NewLine><NewLine>    if torch.cuda.is_available() :<NewLine>        args.n_gpus = torch.cuda.device_count()<NewLine>        print(args.n_gpus, "" GPU(s) available"")<NewLine>        print(torch.cuda.get_device_name(0))<NewLine>        <NewLine>        # for DDP<NewLine>        args.world_size = args.n_gpus * args.n_nodes<NewLine>        os.environ['MASTER_ADDR'] = 'localhost'<NewLine>        os.environ['MASTER_PORT'] = '8888' <NewLine>    <NewLine>    else :<NewLine>        print(""GPU is NOT available."")   <NewLine>        sys.exit()<NewLine>        <NewLine>    print(""Total batch size = "", args.batch_size)<NewLine>    <NewLine>    args.batch_size = int(args.batch_size / args.world_size) # for DDP<NewLine>    print(""Batch size = "", args.batch_size, ""/ GPU"")<NewLine>    <NewLine>    print(""Optimizer = "", args.optimizer)<NewLine>    <NewLine>    """""" Distributed Data Parallel (DDP)""""""<NewLine>    mp.spawn(train, nprocs=args.n_gpus, args=(args,)) <NewLine><NewLine>    print()<NewLine><NewLine>       <NewLine># Training<NewLine>def train(gpu, args):<NewLine>    <NewLine>    rank = args.nr * args.n_gpus + gpu	                          <NewLine>    dist.init_process_group(                                   <NewLine>    	backend='nccl',                                         <NewLine>   		init_method='env://',                                   <NewLine>    	world_size=args.world_size,                              <NewLine>    	rank=rank                                               <NewLine>    )                                                          <NewLine><NewLine>    epochs = args.epochs<NewLine>    batch_size = args.batch_size    # batch_size is per GPU size.<NewLine>  <NewLine>    torch.manual_seed(0)<NewLine>    <NewLine>    if args.net=='res18':<NewLine>        net = ResNet18()<NewLine>    elif args.net=='res34':<NewLine>        net = ResNet34()<NewLine>    elif args.net=='res50':<NewLine>        net = ResNet50()<NewLine>    elif args.net=='res101':<NewLine>        net = ResNet101()<NewLine>    <NewLine>    if rank == 0 :         <NewLine>        print(""Model = "", net.__class__.__name__)<NewLine>        print()<NewLine><NewLine>    <NewLine>    torch.cuda.set_device(gpu)    <NewLine>    <NewLine>    net = torch.nn.SyncBatchNorm.convert_sync_batchnorm(net)<NewLine>    net = net.cuda(gpu)<NewLine>    <NewLine>    criterion = nn.CrossEntropyLoss().cuda(gpu)<NewLine><NewLine>    if args.optimizer == ""Adam"" :<NewLine>        optimizer = optim.Adam(net.parameters())<NewLine>        <NewLine>    elif args.optimizer == ""SGD"" :<NewLine>        optimizer = optim.SGD(net.parameters(), lr = 0.1)<NewLine> <NewLine>    net = nn.parallel.DistributedDataParallel(net, device_ids=[gpu])<NewLine><NewLine>    transform_list = [<NewLine>                  transforms.RandomChoice([<NewLine>                  transforms.RandomCrop(32, padding=4),<NewLine>                  transforms.RandomResizedCrop(32, scale=(0.7, 1.0), ratio = (1.0, 1.0)),<NewLine>                  ]),<NewLine>                  transforms.RandomHorizontalFlip(),<NewLine>                  transforms.RandomRotation(degrees = 20), <NewLine>                  transforms.ToTensor(),<NewLine>                  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),<NewLine>                  ]<NewLine>                  <NewLine>    transform_train = transforms.Compose(transform_list)<NewLine>     <NewLine>    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)<NewLine><NewLine>    train_sampler = torch.utils.data.distributed.DistributedSampler(<NewLine>    	trainset,<NewLine>    	num_replicas = args.world_size,<NewLine>    	rank = rank<NewLine>    )<NewLine><NewLine>    trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, <NewLine>                                              shuffle=False, num_workers=0,<NewLine>                                              pin_memory = False, sampler=train_sampler)<NewLine><NewLine>    for epoch in range(epochs):<NewLine>        <NewLine>        if rank == 0 :<NewLine>            print()<NewLine>            print(""epoch : "",epoch + 1, "" / "", epochs)<NewLine><NewLine>        net.train()        <NewLine>        <NewLine>        """"""   ------- Training loop  -------- """"""<NewLine>   <NewLine>        for batch_idx, (inputs, targets) in enumerate(trainloader):<NewLine>                      <NewLine>            inputs = inputs.cuda(non_blocking=True)<NewLine>            targets = targets.cuda(non_blocking=True)<NewLine>            <NewLine>            message = """"<NewLine>            t0 = time.time() <NewLine>            <NewLine>            optimizer.zero_grad()      <NewLine>            <NewLine>            t1 = time.time() <NewLine>            message += ""  zero grad: {0:.5f}"".format(t1 - t0)<NewLine>            <NewLine>            outputs = net(inputs)<NewLine>            <NewLine>            t2 = time.time() <NewLine>            message += ""  out: {0:.5f}"".format(t2 - t1)<NewLine>            <NewLine>            loss = criterion(outputs, targets)<NewLine>            <NewLine>            t3 = time.time() <NewLine>            message += ""  loss: {0:.5f}"".format(t3 - t2)<NewLine>            <NewLine>            loss.backward()<NewLine>            <NewLine>            t4 = time.time() <NewLine>            message += ""  back: {0:.5f}"".format(t4 - t3)<NewLine>            <NewLine>            loss_val = optimizer.step(loss.item)  # loss value is given through optimizer.<NewLine><NewLine>            t5 = time.time() <NewLine>            message += ""  step: {0:.5f}"".format(t5 - t4)<NewLine>                 <NewLine>            if rank == 0 :<NewLine>                print(""{0:.6f}"".format(loss_val) + message)                    <NewLine><NewLine>        dist.destroy_process_group()<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    main()<NewLine>    <NewLine></code></pre><NewLine><p>Please let me know if something is wrong. Thank you.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/tt_yy"">@TT_YY</a>,  at a quick glance, I noticed that you are using <code>time.time()</code> to measure the time consumption. This does not work for CUDA ops, as they return immediately after the op inserted into the CUDA stream before they are actually done. You will need to create CUDA events and then use the <a href=""https://pytorch.org/docs/stable/cuda.html#torch.cuda.Event.elapsed_time"" rel=""nofollow noopener"">elapsed_time</a> API.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>The code snippet in <a href=""https://discuss.pytorch.org/t/distributed-training-slower-than-dataparallel/81539/4"">this comment</a> can serve as an example. Search for <code>torch.cuda.Event</code>.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>This does not work for CUDA ops, as they return immediately after the op inserted into the CUDA stream before they are actually done.</p><NewLine></blockquote><NewLine><p>Ok, I didn’t know the detail that you explained, but I meant to do a rough estimate so I thought that’s good enough. Nevertheless, the result matches the feeling or actual time spent for one epoch of training, as you can see if you run it. The DDP program takes 47 sec. while DP takes 32 sec. in my environment.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/tt_yy"">@TT_YY</a>, I took a closer look at the code and noticed that you converted <code>BatchNorm</code> to <code>SyncBatchNorm</code> for DDP, which might be the source of the slowness. If you look at <code>SyncBatchNorm</code>'s implementation (see below), it launches its own communication, which is not handled by DDP. This additional comm leads to ~10% slowdown in your program when running on 2 GPUs. When I use <code>BatchNorm</code> instead of <code>SyncBatchNorm</code>, DDP is faster than DP. In general, when comparing DDP and DP speed, we need to make sure that they run the same model.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/573940f8d71de45356b1e6c851f876a32cb8a0ac/torch/nn/modules/_functions.py#L79"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/573940f8d71de45356b1e6c851f876a32cb8a0ac/torch/nn/modules/_functions.py#L79"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/573940f8d71de45356b1e6c851f876a32cb8a0ac/torch/nn/modules/_functions.py#L79</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""69"" style=""counter-reset: li-counter 68 ;""><NewLine><li>    self.needs_input_grad[0],</li><NewLine><li>    self.needs_input_grad[1],</li><NewLine><li>    self.needs_input_grad[2]</li><NewLine><li>)</li><NewLine><li><NewLine></li><li>if self.needs_input_grad[0]:</li><NewLine><li>    # synchronizing stats used to calculate input gradient.</li><NewLine><li>    # TODO: move div_ into batch_norm_backward_elemt kernel</li><NewLine><li>    num_channels = sum_dy.shape[0]</li><NewLine><li>    combined = torch.cat([sum_dy, sum_dy_xmu], dim=0)</li><NewLine><li class=""selected"">    torch.distributed.all_reduce(</li><NewLine><li>        combined, torch.distributed.ReduceOp.SUM, process_group, async_op=False)</li><NewLine><li>    sum_dy, sum_dy_xmu = torch.split(combined, num_channels)</li><NewLine><li><NewLine></li><li>    divisor = count_tensor.sum()</li><NewLine><li>    mean_dy = sum_dy / divisor</li><NewLine><li>    mean_dy_xmu = sum_dy_xmu / divisor</li><NewLine><li>    # backward pass for gradient calculation</li><NewLine><li>    grad_input = torch.batch_norm_backward_elemt(</li><NewLine><li>        grad_output,</li><NewLine><li>        saved_input,</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>This is how I measure the latency.</p><NewLine><pre><code class=""lang-python""># run one iteration to warm up<NewLine>optimizer.zero_grad()<NewLine>outputs = net(inputs)<NewLine>loss = criterion(outputs, targets)<NewLine>loss.backward()<NewLine>loss_val = optimizer.step(loss.item) <NewLine><NewLine># measure latency of the second iteration<NewLine>start = torch.cuda.Event(enable_timing=True)<NewLine>end = torch.cuda.Event(enable_timing=True)<NewLine>start.record()<NewLine>optimizer.zero_grad()<NewLine>outputs = net(inputs)<NewLine>loss = criterion(outputs, targets)<NewLine>loss.backward()<NewLine>loss_val = optimizer.step(loss.item)<NewLine>end.record()<NewLine>torch.cuda.synchronize()<NewLine><NewLine>print(f""world size = {args.world_size}, batch size = {batch_size}, latency = {start.elapsed_time(end)}"")<NewLine></code></pre><NewLine><p>I tried to run the DDP script with the following configs on two GPUs:</p><NewLine><ol><NewLine><li><NewLine><p>Run as is</p><NewLine><pre><code class=""lang-auto"">world size = 2, batch size = 2048, latency = 506.9587707519531<NewLine>world size = 2, batch size = 2048, latency = 506.40606689453125<NewLine></code></pre><NewLine></li><NewLine><li><NewLine><p>Comment out the following line, as <code>SyncBatchNorm</code> has its own way to communicate buffers, which can e slower.</p><NewLine><pre><code class=""lang-python"">#net = torch.nn.SyncBatchNorm.convert_sync_batchnorm(net)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">world size = 2, batch size = 2048, latency = 456.42352294921875<NewLine>world size = 2, batch size = 2048, latency = 457.8104248046875<NewLine></code></pre><NewLine></li><NewLine><li><NewLine><p>Made the following edits and set <code>args.n_gpus = 1</code>. So the program runs <code>DataParallel</code>.</p><NewLine><pre><code class=""lang-python"">#net = torch.nn.SyncBatchNorm.convert_sync_batchnorm(net)<NewLine>...<NewLine>#net = nn.parallel.DistributedDataParallel(net, device_ids=[gpu])<NewLine>net = nn.parallel.DataParallel(net)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">world size = 1, batch size = 4096, latency = 496.3483581542969<NewLine></code></pre><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your detailed analysis.</p><NewLine><blockquote><NewLine><p>In general, when comparing DDP and DP speed, we need to make sure that they run the same model.</p><NewLine></blockquote><NewLine><p>I have converted BatchNorm into SyncBatchNorm in DP too, as you can find “convert_model()” in the above code list of DP.</p><NewLine><p>As you pointed out, removing convert_model() from the DP program significantly improves the performance. (2500msec. / itr  -&gt;  1500), However, I could not see such a difference of latency for the DDP program. The improvement observed in my experiment is less than 4%, with a batch size of 4096 and 8 GPUs. I used the same time measurement method as you did.</p><NewLine><p>I can tolerate the 4% difference if I can make my DDP program faster than the DP.</p><NewLine><p>DP with convert_model()                     --------------------------  DP without convert_model()<br/><NewLine><img alt=""image"" data-base62-sha1=""gnGrguX9EjYUch7SmSqSOCyyuKh"" height=""225"" src=""https://discuss.pytorch.org/uploads/default/original/3X/7/2/72d02c3bf1891b1e0404e8770d209fdf8ba5af7d.png"" width=""576""/></p><NewLine><p>DDP with convert_sync_batchnorm()        --------- DDP without convert_sync_batchnorm()<br/><NewLine><img alt=""image"" data-base62-sha1=""oY5gcMihL6AavDyvRniKmzQeWRm"" height=""206"" src=""https://discuss.pytorch.org/uploads/default/original/3X/a/e/aefeedaa2d319e0b4d1433e212c1900ad359e6c4.png"" width=""567""/></p><NewLine><p>I use convert_model(), which converts BatchNorm into a SyncBatchNorm for DP, but it is different from the torch version for DDP. The torch.nn.SyncBatchNorm.convert_sync_batchnorm() supports only DDP.</p><NewLine><p>By the way, I wonder why the latency in your experiment is one digit lower than mine. Are you using the same model (resnet34) and CIfar10?</p><NewLine><p>If there is an example program for image classification using DDP, I will be curious to see the latency. I am trying to test my original optimizer with a practical settings. So far, the test comparing it with Adam has been successful in terms of the number of steps to reach a target accuracy. Now I have to improve the wall clock time and trying to find a way to scale the speed with the batch size, like the experiment you have shown. However, DDP is still not working in my program  for that purpose now.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>I have been trying to reproduce your results, but, for some reason,  my experiment of DDP with two GPUs end up with about  4000 msec, unlike your results of about 500 msec.</p><NewLine><p>I also tried DP with the same settings and the time was 1100 msec. The experiment indicates that DP is faster than DDP for a batch size of 2048 / GPU.</p><NewLine><p>I used VGG11 model for the experiment, because K80 cannot accept 2048 data of Cifar10 with a large model like Resnet34 in its memory. I tried even a smaller model, but still DP is faster than DDP.</p><NewLine><p>Would you please specify the model and data that you used to produce the results in your former post?<br/><NewLine>Are the GPUs that you used utilize NVLink?</p><NewLine><p>Thank you for your cooperation.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""9"" data-topic=""93865"" data-username=""TT_YY""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tt_yy/40/25673_2.png"" width=""20""/> TT_YY:</div><NewLine><blockquote><NewLine><p>By the way, I wonder why the latency in your experiment is one digit lower than mine. Are you using the same model (resnet34) and CIfar10?</p><NewLine></blockquote><NewLine></aside><NewLine><p>As I don’t have a models package locally, I replaced that with <code>torchvision.models</code>, and then replaced all <code>ResNet*</code> with <code>reset*</code>. I am not sure if that’s also what you use.</p><NewLine><blockquote><NewLine><p>Would you please specify the model and data that you used to produce the results in your former post?</p><NewLine></blockquote><NewLine><p>I used the default model type in your code, so it should be <code>torchvision.models.resnet34()</code> after my replacement, and was using the same data loader.</p><NewLine><blockquote><NewLine><p>Are the GPUs that you used utilize NVLink?</p><NewLine></blockquote><NewLine><p>Yes, the comm should go through nvlink</p><NewLine><pre><code class=""lang-auto"">        GPU0    GPU1    CPU Affinity<NewLine>GPU0     X      NV4     12-23<NewLine>GPU1    NV4      X      12-23<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">+-----------------------------------------------------------------------------+<NewLine>| NVIDIA-SMI 418.116.00   Driver Version: 418.116.00   CUDA Version: 10.1     |<NewLine>|-------------------------------+----------------------+----------------------+<NewLine>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<NewLine>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<NewLine>|===============================+======================+======================|<NewLine>|   0  Quadro GP100        Off  | 00000000:81:00.0 Off |                    0 |<NewLine>| 26%   31C    P0    30W / 235W |     10MiB / 16278MiB |      0%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   1  Quadro GP100        Off  | 00000000:82:00.0 Off |                    0 |<NewLine>| 26%   31C    P0    30W / 235W |     10MiB / 16278MiB |      0%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>Thank you for sharing your GPU spec. That’s what I should have asked in the first place. GP100 far exceeds K80 in almost all the aspects of the performance. In addition, in my platform, K80 does not use NVLink. (Maybe it originally does not support it.) I suppose that the source of difference is the spec difference in the performance of DDP.</p><NewLine><p>On the other hand, I still don’t understand why DP is faster than DDP with the same GPUs in my environment. My guess is that the DP directly performing all-reduce by a high spec CPU can be faster than the DDP performing all-reduce by gpu0, which communicates through PCI bus and memory on board. But, I 'm not sure.</p><NewLine><p>Thank you very much.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""12"" data-topic=""93865"" data-username=""TT_YY""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tt_yy/40/25673_2.png"" width=""20""/> TT_YY:</div><NewLine><blockquote><NewLine><p>My guess is that the DP directly performing all-reduce by a high spec CPU can be faster than the DDP performing all-reduce by gpu0, which communicates through PCI bus and memory on board.</p><NewLine></blockquote><NewLine></aside><NewLine><p>DP uses replicate, scatter, and gather operations, which are basically <code>cudaMemcpy</code> under the hood. I suspect <code>cudaMemcpy</code> can be faster than allreduce operations used by DDP on some hardware.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Good insight. I have learned a lot. Thank you.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/TT_YY; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/TT_YY; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/TT_YY; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/TT_YY; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/TT_YY; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/TT_YY; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/TT_YY; <NewLine> ,"REPLY_DATE 1: August 24, 2020,  2:36pm; <NewLine> REPLY_DATE 2: August 25, 2020,  4:20am; <NewLine> REPLY_DATE 3: August 25, 2020,  8:52pm; <NewLine> REPLY_DATE 4: August 25, 2020,  9:14pm; <NewLine> REPLY_DATE 5: August 25, 2020,  9:16pm; <NewLine> REPLY_DATE 6: August 26, 2020,  2:56am; <NewLine> REPLY_DATE 7: August 26, 2020,  6:53pm; <NewLine> REPLY_DATE 8: August 28, 2020,  2:36am; <NewLine> REPLY_DATE 9: September 1, 2020,  8:16pm; <NewLine> REPLY_DATE 10: September 1, 2020,  8:30pm; <NewLine> REPLY_DATE 11: September 2, 2020,  3:47am; <NewLine> REPLY_DATE 12: September 2, 2020,  2:25pm; <NewLine> REPLY_DATE 13: September 3, 2020,  6:25am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: 1 Like; <NewLine> REPLY 13 LIKES: ; <NewLine> 
94998,How to implement work pool in GPU cluster,2020-09-03T02:04:29.149Z,0,34,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>What is the most efficient way to implement a work pool in GPU cluster? I have tried with JoinableQueue, but it takes long time to get large item (e.g. a train batch) from the queue. Is there a better way to implement it? Is it possible to store data on GPU and shared by different processes?<br/><NewLine>And I read this documentation for shared memory: <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/notes/multiprocessing.html</a><br/><NewLine>Is this “shared memory” in CPU or GPU? What is the structure of it?<br/><NewLine>Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/Yi_Zhang,(Yi Zhang),Yi_Zhang,"September 3, 2020,  2:04am",,,,,
94234,Synchronization mechanism with different iteration counts of for-loop,2020-08-27T01:26:00.804Z,3,92,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I’m working on modifying my model (including my custom data loader) to fit the structure of DDP. I haven’t given my code a try but I’d like to know more about the synchronization process.</p><NewLine><p>According to the many great threads on this forum, DDP takes care of the synchronization during loss.backward(). But what if the number of data in each data loader leads to different for-loop counts, would the processes with n+1 loops be blocked because the processes with n loops never reach the point?</p><NewLine><p>Say, I have 401 images, distributed to 4 data loaders with 101, 100, 100, 100 images respectively. Batch size is 4 so process 0 gets 26 iterations while others get 25. Would my process group get stuck at 26th iteration?</p><NewLine><p>Here is a simplified version of part of my code:</p><NewLine><pre><code>#......(some init process including moving self.model to DDP)......<NewLine>for phase in ['train', 'eval']:<NewLine>    dist.barrier()<NewLine>    if phase=='train':<NewLine>        self.model.train()<NewLine>        self.data_loader.train()<NewLine>    else:<NewLine>        self.model.eval()<NewLine>        self.data_loader.eval()<NewLine>    running_loss = 0<NewLine>    for inputs, labels in self.data_loader:<NewLine>        self.optimizer.zero_grad()<NewLine>        with torch.set_grad_enabled(phase=='train'):<NewLine>            outputs = self.model(inputs)<NewLine>            loss = self.loss(outputs, labels)<NewLine>            if phase == 'train':<NewLine>                loss.backward()   ### Could this or the following line get stuck during the extra loop by process 0?<NewLine>                self.optimizer.step()<NewLine>                running_loss += loss.item()*inputs.shape[0]<NewLine>        torch.cuda.empty_cache()<NewLine>    epoch_loss = running_loss/len(self.data_loader)</code></pre><NewLine><p>Thanks for any helpful hint!</p><NewLine></div>",https://discuss.pytorch.org/u/annisat,(MMLi),annisat,"August 27, 2020,  1:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""94234"" data-username=""annisat""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/a/d26b3c/40.png"" width=""20""/> annisat:</div><NewLine><blockquote><NewLine><p>According to the many great threads on this forum, DDP takes care of the synchronization during loss.backward(). But what if the number of data in each data loader leads to different for-loop counts, would the processes with n+1 loops be blocked because the processes with n loops never reach the point?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yep, the one with n+1 loops will block when using &lt;= PyTorch v1.6. There are ways to get around in user code, e.g. by collecting a signal in each iteration to see if any process has already exited. If yes, break.</p><NewLine><p><a class=""mention"" href=""/u/rvarm1"">@rvarm1</a> is working on a much better solution, which will be included in v1.7. With that solution, the process that exits early will use dummy comm ops to unblock remaining active ones. Please see the following issue and PR.</p><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/38174"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/38174"" rel=""nofollow noopener"" target=""_blank"">[RFC] Join-based API to support uneven inputs in DDP</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-05-09"" data-format=""ll"" data-time=""02:00:25"" data-timezone=""UTC"">02:00AM - 09 May 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/rohan-varma"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""rohan-varma"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/8039770?v=4"" width=""20""/><NewLine>          rohan-varma<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">🚀 Feature<NewLine>with @pritamdamania87 @mrshenli @zhaojuanmao<NewLine>This RFC is to summarize the current proposal for supporting uneven inputs across different DDP processes. Related...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">feature</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">module: distributed</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine><aside class=""onebox githubpullrequest""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/42577"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Pull Request""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 12 16"" width=""60""><path d=""M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/pull/42577"" rel=""nofollow noopener"" target=""_blank"">Join-based API to support DDP uneven inputs</a><NewLine></h4><NewLine><div class=""branches""><NewLine><code>pytorch:gh/rohan-varma/152/base</code> ← <code>pytorch:gh/rohan-varma/152/head</code><NewLine></div><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-08-05"" data-format=""ll"" data-time=""03:07:50"" data-timezone=""UTC"">03:07AM - 05 Aug 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/rohan-varma"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""rohan-varma"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/8039770?v=4"" width=""20""/><NewLine>          rohan-varma<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""37 commits changed 9 files with 894 additions and 59 deletions""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/42577/files"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+894</span><NewLine><span class=""removed"">-59</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thousand thanks for the explanation! I modified my code following your suggestion and I provide my provisional solution here for comments.</p><NewLine><pre><code><NewLine>running_loss = 0<NewLine>running_len = 0<NewLine>for inputs, labels in self.data_loader:<NewLine>    self.optimizer.zero_grad()<NewLine>    with torch.set_grad_enabled(phase=='train'):<NewLine>        outputs = self.model(inputs)<NewLine>        loss = self.loss(outputs, labels)<NewLine>        if phase == 'train':<NewLine>            loss.backward()<NewLine>            self.optimizer.step()<NewLine>            iteration_count+=1<NewLine>    running_loss += loss.item()<NewLine>    running_len += inputs.shape[0]<NewLine>    torch.cuda.empty_cache()<NewLine>    ##########<NewLine>    is_next = torch.Tensor([self.data_loader.peek()])<NewLine>    # is_next==True if the iterator has not reached the end, i.e., next loop is expected<NewLine>    dist.all_reduce_multigpu(is_next, op=dist.ReduceOp.BAND)<NewLine>    if not is_next: break<NewLine>    ##########</code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/annisat"">@annisat</a>, that looks good to me. One way to speed it up a bit is to run the <code>dist.all_reduce</code> at the beginning of the loop and set <code>async_op=True</code>. Then only wait for it when you need the result. In this way, the comm and the forward/backward/opt.step computation can overlap. Please see the code in the following thread:</p><NewLine><aside class=""quote quote-modified"" data-post=""24"" data-topic=""80345""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/mrshenli/40/12220_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/multiprocessing-barrier-blocks-all-processes/80345/24"">Multiprocessing - Barrier Blocks all Processes?</a><NewLine></div><NewLine><blockquote><NewLine>    The self-contained code below works for me. <NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>import torch.multiprocessing as mp<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine>from torch.nn.parallel import DistributedDataParallel as DDP<NewLine><NewLine><NewLine>def example(rank, world_size):<NewLine>    # create default process group<NewLine>    dist.init_process_group(""gloo"", rank=rank, world_size=world_size)<NewLine>    # create local model<NewLine>    model = nn.Linear(10, 10).to(rank)<NewLine>    # construct DDP model<NewLine>    ddp_model = DDP(model, device_i…<NewLine>  </blockquote><NewLine></aside><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the tips! It took me some while to understand and implement async_op.</p><NewLine><p>I would like to point out a problem when I ran my own code above.</p><NewLine><p>I changed my code to</p><NewLine><pre><code>is_next = torch.Tensor([self.data_loader.peek()]).cuda(self.gpu)<NewLine>col_handle = dist.all_reduce(is_next, op=dist.ReduceOp.BAND, async_op)<NewLine>...<NewLine>col_handle.wait()<NewLine>if not is_next: break</code></pre><NewLine><p>and tried it with SPSG with 2 processes. The final value of <code>is_next</code> is <code>[2]</code> rather than <code>[True]</code> or <code>[1]</code>. It seems that <code>dist.ReduceOp.BAND</code> adds up input tensors rather than doing a regular AND. Therefore I changed the first line into:</p><NewLine><pre><code>is_next = torch.Tensor([self.data_loader.peek()]).bool().cuda(self.gpu)</code></pre><NewLine><p>The Error Message says all_reduce does not support this Tensor type for now. In order to achieve my goal, I use <code>dist.ReduceOp.MIN</code> instead. Here’s my final code that actually runs smoothly without imbalanced for-loop counts blocking the synchornization process.</p><NewLine><pre><code>for inputs, labels in self.data_loader:<NewLine>    is_next = torch.Tensor([self.data_loader.peek()]).cuda(self.gpu)<NewLine>    col_handle = dist.all_reduce(is_next, op=dist.ReduceOp.MIN, async_op=True)<NewLine>    # forward and backward and step and stuff                    <NewLine>    col_handle.wait()<NewLine>    if not is_next: break</code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/annisat; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/annisat; <NewLine> ,"REPLY_DATE 1: August 27, 2020,  2:48am; <NewLine> REPLY_DATE 2: August 27, 2020,  4:58am; <NewLine> REPLY_DATE 3: August 27, 2020,  2:17pm; <NewLine> REPLY_DATE 4: September 2, 2020,  3:31am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
94775,Data Distributed Parallel runs faster when all the cards are occupied,2020-09-01T06:21:34.976Z,1,52,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi I’m rather new to the DDP, and I found a rather bizarre behavior of my training with DDP.</p><NewLine><p>Say I have 4 GPUs in total. If I run my code on the GPU:0 and GPU:1 and leave the remaining two unoccupied. Then during training the percentage of both GPU would be at maximum 50%. Now the GPU occupation is:</p><NewLine><p>gpu0: process1(50% or lower)<br/><NewLine>gpu1: process1(50% or lower)<br/><NewLine>gpu2: empty<br/><NewLine>gpu3: empty</p><NewLine><p>But when I run another training code on the remaining two GPUs, all 4 cards would hit 100% usage and both of the processes run faster than the previous situation. Now the gpu occupation is:</p><NewLine><p>gpu0: process1 (99%)<br/><NewLine>gpu1: process1 (99%)<br/><NewLine>gpu2: process2 (99%)<br/><NewLine>gpu3: process2 (99%)</p><NewLine><p>I experience this on multiple servers and it’s really confusing me. Can anyone help with this?</p><NewLine></div>",https://discuss.pytorch.org/u/Haizhou_Shi,(Haizhou Shi),Haizhou_Shi,"September 1, 2020,  6:21am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>How about the per iteration latency? If you feed the same batch size to each DDP instance (i.e., different global <code>batch_size</code>), is using 4 GPU still faster? Please use the <a href=""https://pytorch.org/docs/stable/cuda.html#torch.cuda.Event.elapsed_time"" rel=""nofollow noopener""><code>elapsed_time</code></a> API to measure that.</p><NewLine><p>As DDP uses <code>all_reduce</code> to communicate gradients, GPU utilization cannot faithfully represent how busy a GPU is. CUDA would report 100% GPU utilization even if one GPU is block waiting for another peer to join and doing nothing.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply!</p><NewLine><p>I didn’t use <code>elasped_time</code> to measure how long it takes for one iteration but I did use <code>time.time()</code> to calculate the iteration latency. As long as there is one GPU that remains unused, the process runs slower (sometimes 50% slower and sometimes 100% slower).</p><NewLine><p>The weird thing is that it seems my DDP program is influenced by other programs (they are not necessarily DDPs) running on other GPUs, in a counterintuitive way: normally we expect that programs are competing for the computational resources, but here more programs bring better performance…</p><NewLine><p>(This thing didn’t happen to my colleague’s non-DDP program so I guess it must have something to do with the DDP).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Haizhou_Shi; <NewLine> ,"REPLY_DATE 1: September 1, 2020,  2:23pm; <NewLine> REPLY_DATE 2: September 2, 2020,  2:52am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
90462,Cuda initialisation stuck in a100 machine,2020-07-24T21:18:44.497Z,4,173,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using a A100 server from GCP with the latest NGC container from nvidia. However for the support of DCNV2 i have to downgrade my pytorch version to 1.4.0. Whenever i initialise a tensor in gpu like torch.randn(3).cuda() the interpreter gets stuck and never finishes that command. Any help??</p><NewLine></div>",https://discuss.pytorch.org/u/gouthamvgk,(Goutham Kumar V),gouthamvgk,"July 24, 2020,  9:18pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>hey <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>, do you know if anyone can answer questions regarding using PyTorch cuda features on GCP/NGC?</p><NewLine><p>cc <a class=""mention"" href=""/u/ngimel"">@ngimel</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Solved!. After close to 10 mins the tensor gets initialised in gpu and from thereon no problem</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The long startup time is most likely create due to a JIT compilation of the CUDA code, if your installed PyTorch version wasn’t built for compute capability 8.0 (A100).<br/><NewLine>This would be the case, if you’ve installed the <code>1.4</code> binary instead of building it from source.<br/><NewLine>We are working towards building the PyTorch nightly binaries with the latest library stack and cc8.0.<br/><NewLine>For now, you could either build from source or let the JIT compiler run in the first CUDA call.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>What is the issue with dcnv2 support in the latest ngc container?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply. But I wonder that is it possible to build from source for old version PyTorch (for example version 1.4) with cuda11? Or is there any plan to support old version PyTorch for A100?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""90462"" data-username=""Shenggan""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/shenggan/40/28294_2.png"" width=""20""/> Shenggan:</div><NewLine><blockquote><NewLine><p>Or is there any plan to support old version PyTorch for A100?</p><NewLine></blockquote><NewLine></aside><NewLine><p>There is no plan on changing older PyTorch versions to enable CUDA11 and thus new GPU architectures, so you would have to use the latest PyTorch version.</p><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""90462"" data-username=""Shenggan""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/shenggan/40/28294_2.png"" width=""20""/> Shenggan:</div><NewLine><blockquote><NewLine><p>is it possible to build from source for old version PyTorch (for example version 1.4) with cuda11?</p><NewLine></blockquote><NewLine></aside><NewLine><p>You could try to cherry-pick all commits mentioning CUDA11 in an older version and try to build it.<br/><NewLine>However, while it might work, what’s your use case that you need to use an old PyTorch version?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply. I think porting to the latest version PyTorch is the best choice.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/gouthamvgk; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ngimel; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Shenggan; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Shenggan; <NewLine> ,"REPLY_DATE 1: July 24, 2020,  9:37pm; <NewLine> REPLY_DATE 2: July 24, 2020,  9:51pm; <NewLine> REPLY_DATE 3: July 25, 2020,  2:30am; <NewLine> REPLY_DATE 4: July 26, 2020,  7:22pm; <NewLine> REPLY_DATE 5: August 31, 2020,  4:45am; <NewLine> REPLY_DATE 6: August 31, 2020,  4:17pm; <NewLine> REPLY_DATE 7: September 2, 2020,  1:11am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
94274,Why time.time() in python is inaccurte?,2020-08-27T08:59:53.275Z,9,165,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I think it is an elementary question about programming with GPU.</p><NewLine><p>First, i tried to use time.time() in python module, to measure the operation time of some modules in NNs.<br/><NewLine>such as</p><NewLine><pre><code class=""lang-auto"">def forward(self, x):<NewLine>  end = time.time()<NewLine>  output1 = self.layer1(x)<NewLine>  time_output1 = time.time()<NewLine>  output2 = self.layer2(output1)<NewLine>  time_output2 = time.time()<NewLine>  print(time_output1 - end, time_output2-end)<NewLine></code></pre><NewLine><p>However, i found that the time information is inaccurate, and i have to use below link:<br/><NewLine><aside class=""quote"" data-post=""1"" data-topic=""26964""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/daulbaev/40/1824_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/how-to-measure-time-in-pytorch/26964"">How to measure time in PyTorch</a><NewLine></div><NewLine><blockquote><NewLine>    I have seen lots of ways to measure time in PyTorch. But what is the most proper way to do it now (both for cpu and cuda)? <NewLine>Should I clear the memory cache if I use timeit? <NewLine>And is it possible to get accurate results if I’m computing on a cluster? And is it a way to make this results reproducible? <NewLine>And what is better: timeit or profiler?<NewLine>  </blockquote><NewLine></aside><NewLine></p><NewLine><p>Although i measure the correct time by the methods in the link,<br/><NewLine>i want to know why profiling with time.time() gives inaccurate results.</p><NewLine><p>an example: <a href=""https://github.com/facebookresearch/moco/issues/66"" rel=""nofollow noopener"">https://github.com/facebookresearch/moco/issues/66</a></p><NewLine><p>Thank you !</p><NewLine></div>",https://discuss.pytorch.org/u/LeeDoYup,(Doyup Lee),LeeDoYup,"August 27, 2020,  8:59am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It isn’t. While there are more refined measures, there isn’t anything wrong with plain timing. Apparently there first are lots of people doing it wrong (both beginners and people with considerable experience) and then inaccurate representations of what exactly is wrong (“can’t use time.time” <em>Edit:</em> actually it is true you should not use it but time.perf_counter(!)): The main things to get right is warm-up and synchronization.</p><NewLine><p>The thing is that if you use the GPU, unless you call <code>torch.cuda.synchronize()</code> before taking the time (for both start and finish), you don’t know what has been executed before and after the time taking.<br/><NewLine>I invariably use the following pattern:</p><NewLine><pre><code class=""lang-auto"">def do_stuff():<NewLine>   for _ in range(100): # or 1000 or whatever, depending on how long it takes<NewLine>      do_my_computation()<NewLine>   torch.cuda.synchronize()<NewLine><NewLine>do_stuff()<NewLine>%timeit do_it()<NewLine></code></pre><NewLine><p>Of course, you need to divide the time by whatever size of the loop you have. I usually aim to have something in the msec range or so.</p><NewLine><p>What this does:<br/><NewLine>This does run the operator (<code>do_my_computation</code>) multiple times between syncs, this would reduce the influence of the synchronization (which takes time) on the measurement.</p><NewLine><p>Calling <code>do_stuff()</code> before the timing does:</p><NewLine><ul><NewLine><li>Warm-up (e.g. some things compile kernels on the fly when called for the first time etc.)</li><NewLine><li>Synchronize before starting the timing</li><NewLine></ul><NewLine><p>Timing with <code>do_stuff()</code> ensures that synchronization happens after each run (and thus implicitly before the next).</p><NewLine><p>You can do essentially the same thing with <s>time.time</s> time.perf_counter() before and after what is <code>%timeit</code> here, except that timeit will actually call <code>do_stuff</code> several times and do some stats to help you along. There also is the <code>timeit</code> module which is similar but you need to adjust the number of runs manually to the duration of your computation.</p><NewLine><p>That said, the profiler gives you more detailed information with very little effort.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello tom. first of all, thank you for fast and detailed reply !</p><NewLine><p>After i read, i tried to use time.time() and torch.cuda.synchronize() to profile the elapsed times.<br/><NewLine>However, the results are still inaccurate.</p><NewLine><p>For example of <a href=""https://github.com/facebookresearch/moco/issues/66"" rel=""nofollow noopener"">https://github.com/facebookresearch/moco/issues/66</a>,</p><NewLine><p>i) <code>time.time()</code> w/o <code>torch.cuda.synchronize()</code></p><NewLine><ul><NewLine><li>shuffle time: 0.5993 s</li><NewLine><li>inf_time: 0.1185 s</li><NewLine></ul><NewLine><p>ii) use <code>torch.cuda.Event</code> &amp; <code>torch.cuda.synchronize()</code></p><NewLine><ul><NewLine><li>shuffle time: 2.72 ms</li><NewLine><li>inf_time: 59.88 ms</li><NewLine></ul><NewLine><p>iii) use <code>time.time()</code> w/ <code>torch.cuda.synchronize()</code></p><NewLine><ul><NewLine><li>shuffle time: 0.0649 s</li><NewLine><li>inf_time: 0.0587 s</li><NewLine></ul><NewLine><p>Although i use torch.cuda.synchronize(),<br/><NewLine>the shuffle time is still over-estimated.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah, wait. The other thing you should do is use <code>time.perf_counter()</code> instead of <code>time.time()</code>. This is because <code>time.time()</code> isn’t guaranteed to actually give valid differences, but you need to use a monotonic clock for that.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks Tom.<br/><NewLine>I checked both <code>time.perf_counter()</code> and <code>time.process_time()</code> with <code>torch.cuda.synchronize()</code>, and got similar results to time.time()</p><NewLine><p>iv) use <code>time.perf_counter()</code> w/ <code>torch.cuda.synchronize()</code></p><NewLine><ul><NewLine><li>shuffle time:	 0.0650 s</li><NewLine><li>inf time:	 0.0587 s</li><NewLine></ul><NewLine><p>v) use <code>time.process_time()</code> w/ <code>torch.cuda.synchronize()</code></p><NewLine><ul><NewLine><li>shuffle time:	 0.0879 s</li><NewLine><li>inf time:	 0.0584 s</li><NewLine></ul><NewLine><p>When comparing all the results, the inference time is consistent,<br/><NewLine>but the shuffle time is inconsistent by the profiling method.</p><NewLine><p>the shuffle time is shuffleBN is Moco<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/facebookresearch/moco/blob/78b69cafae80bc74cd1a89ac3fb365dc20d157d3/moco/builder.py#L133"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/facebookresearch/moco/blob/78b69cafae80bc74cd1a89ac3fb365dc20d157d3/moco/builder.py#L133"" rel=""nofollow noopener"" target=""_blank"">facebookresearch/moco/blob/78b69cafae80bc74cd1a89ac3fb365dc20d157d3/moco/builder.py#L133</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""123"" style=""counter-reset: li-counter 122 ;""><NewLine><li><NewLine></li><li># compute query features</li><NewLine><li>q = self.encoder_q(im_q)  # queries: NxC</li><NewLine><li>q = nn.functional.normalize(q, dim=1)</li><NewLine><li><NewLine></li><li># compute key features</li><NewLine><li>with torch.no_grad():  # no gradient to keys</li><NewLine><li>    self._momentum_update_key_encoder()  # update the key encoder</li><NewLine><li><NewLine></li><li>    # shuffle for making use of BN</li><NewLine><li class=""selected"">    im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)</li><NewLine><li><NewLine></li><li>    k = self.encoder_k(im_k)  # keys: NxC</li><NewLine><li>    k = nn.functional.normalize(k, dim=1)</li><NewLine><li><NewLine></li><li>    # undo shuffle</li><NewLine><li>    k = self._batch_unshuffle_ddp(k, idx_unshuffle)</li><NewLine><li><NewLine></li><li># compute logits</li><NewLine><li># Einstein sum is more intuitive</li><NewLine><li># positive logits: Nx1</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>which gathers all samples, shuffles the index, and reallocates mini-batch to each GPU.<NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/facebookresearch/moco/blob/78b69cafae80bc74cd1a89ac3fb365dc20d157d3/moco/builder.py#L69-L94"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/facebookresearch/moco/blob/78b69cafae80bc74cd1a89ac3fb365dc20d157d3/moco/builder.py#L69-L94"" rel=""nofollow noopener"" target=""_blank"">facebookresearch/moco/blob/78b69cafae80bc74cd1a89ac3fb365dc20d157d3/moco/builder.py#L69-L94</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""69"" style=""counter-reset: li-counter 68 ;""><NewLine><li>def _batch_shuffle_ddp(self, x):</li><NewLine><li>    """"""</li><NewLine><li>    Batch shuffle, for making use of BatchNorm.</li><NewLine><li>    *** Only support DistributedDataParallel (DDP) model. ***</li><NewLine><li>    """"""</li><NewLine><li>    # gather from all gpus</li><NewLine><li>    batch_size_this = x.shape[0]</li><NewLine><li>    x_gather = concat_all_gather(x)</li><NewLine><li>    batch_size_all = x_gather.shape[0]</li><NewLine><li><NewLine></li><li>    num_gpus = batch_size_all // batch_size_this</li><NewLine><li><NewLine></li><li>    # random shuffle index</li><NewLine><li>    idx_shuffle = torch.randperm(batch_size_all).cuda()</li><NewLine><li><NewLine></li><li>    # broadcast to all gpus</li><NewLine><li>    torch.distributed.broadcast(idx_shuffle, src=0)</li><NewLine><li><NewLine></li><li>    # index for restoring</li><NewLine><li>    idx_unshuffle = torch.argsort(idx_shuffle)</li><NewLine></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/facebookresearch/moco/blob/78b69cafae80bc74cd1a89ac3fb365dc20d157d3/moco/builder.py#L69-L94"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>I cannot infer a reason why only the record time of this shuffle operation has varied by measuring method.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>That might be a lot clearer if you</p><NewLine><ul><NewLine><li>specify what exactly you want to measure,</li><NewLine><li>time these bits in isolation,</li><NewLine><li>break down the entire thing into the bits you want to measure (i.e. do your parts reconcile to the total? if not, where are overlaps or gaps in the parts).</li><NewLine></ul><NewLine><p>The links don’t seem to show the actual measurement you inserted.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>In the forward pass of moco, i am measuring the shuffling time, that is</p><NewLine><pre><code class=""lang-auto"">    def forward(self, im_q, im_k):<NewLine>        (....)<NewLine>        # compute key features<NewLine>        with torch.no_grad():  # no gradient to keys<NewLine>            self._momentum_update_key_encoder()  # update the key encoder<NewLine>            <NewLine>            start_time = time.time()<NewLine>            # shuffle for making use of BN<NewLine>            im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)<NewLine>            torch.cuda.synchronize()<NewLine>            end_time = time.time()<NewLine>            shuffle_time = end_time - start_time<NewLine>            (...)<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>So the immediate takeaway from the above discussion is</p><NewLine><ol><NewLine><li>replace time.time with time.perf_counter()</li><NewLine><li>have a torch.cuda.synchronize() before taking the the start_time,</li><NewLine><li>maybe don’t take the first batch (of a given size)</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><h3>time.time()</h3><NewLine><p>i)  <code>time.time()</code>  w/o  <code>torch.cuda.synchronize()</code></p><NewLine><ul><NewLine><li>shuffle time: 0.5993 s</li><NewLine><li>inf_time: 0.1185 s</li><NewLine></ul><NewLine><p>ii)  <code>time.time()</code>  w/o  <code>torch.cuda.synchronize()</code> both before and after an operation</p><NewLine><ul><NewLine><li>shuffle time: 0.0018 s</li><NewLine><li>inf_time: 0.031 s</li><NewLine></ul><NewLine><h2>time.perf_counter()</h2><NewLine><p>i) use  <code>time.perf_counter()</code>  w/  <code>torch.cuda.synchronize()</code></p><NewLine><ul><NewLine><li>shuffle time: 0.0650 s</li><NewLine><li>inf time: 0.0587 s</li><NewLine></ul><NewLine><p>ii) use  <code>time.perf_counter()</code>  w/  <code>torch.cuda.synchronize()</code> both before and after an operation</p><NewLine><ul><NewLine><li>shuffle time: 0.0021 s</li><NewLine><li>inf time: 0.0309 s</li><NewLine></ul><NewLine><h2>time.process_time()</h2><NewLine><p>i) use  <code>time.process_time()</code>  w/  <code>torch.cuda.synchronize()</code></p><NewLine><ul><NewLine><li>shuffle time: 0.0879 s</li><NewLine><li>inf time: 0.0584 s</li><NewLine></ul><NewLine><p>ii) use  <code>time.process_time()</code>  w/ <code>torch.cuda.synchronize()</code> both before and after an operation</p><NewLine><ul><NewLine><li>shuffle time: 0.001879 s</li><NewLine><li>inf time: 0.03107 s</li><NewLine></ul><NewLine><h2>torch.cuda.Event</h2><NewLine><p>i) use  <code>torch.cuda.Event</code>  &amp;  <code>torch.cuda.synchronize()</code></p><NewLine><ul><NewLine><li>shuffle time: 2.72 ms</li><NewLine><li>inf_time: 59.88 ms</li><NewLine></ul><NewLine><h2>Conclusions</h2><NewLine><ul><NewLine><li>When time profiling, we can use both <code>time</code> module in python and <code>torch.cuda.Event</code>.</li><NewLine><li>What we remember is, we must use <code>torch.cuda.synchronize()</code> right before and after the operations to  be profiled.</li><NewLine><li>Measurements of <code>time</code> and <code>cuda.Event</code> is some what different, but the ratio is consistent.</li><NewLine></ul><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>In the last,<br/><NewLine>can i ask some reasons why the <code>time</code> module and cuda.Event measure different elapsed times?<br/><NewLine>i am using V100x4 and DistributedDataParallel.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>I must admit that I don’t know. One thing to exclude would be stochastic variation (e.g. %timeit gives you a standard deviation, so you can imagine error bars for the measurement), but I would not see 30ms-&gt;60ms doing that.<br/><NewLine>The other part is that you need a really stable environment to get reliable benchmarking, maybe something we did here changed something w.r.t. other things going on.<br/><NewLine>(But maybe <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> knows something.)</p><NewLine><p>Fun anecdote: A long time ago, I briefly enabled remote access to the GPU I used for benchmarking for one of fellow PyTorch devs because I somehow had a much more stable timing environment then them.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/LeeDoYup; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/LeeDoYup; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/LeeDoYup; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/LeeDoYup; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/LeeDoYup; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: August 28, 2020,  4:32am; <NewLine> REPLY_DATE 2: August 27, 2020, 11:53pm; <NewLine> REPLY_DATE 3: August 28, 2020,  3:14am; <NewLine> REPLY_DATE 4: August 28, 2020,  6:48am; <NewLine> REPLY_DATE 5: August 28, 2020,  6:59am; <NewLine> REPLY_DATE 6: August 28, 2020,  8:18am; <NewLine> REPLY_DATE 7: September 1, 2020,  6:55pm; <NewLine> REPLY_DATE 8: September 1, 2020,  6:55pm; <NewLine> REPLY_DATE 9: September 1, 2020,  6:56pm; <NewLine> REPLY_DATE 10: September 1, 2020,  7:18pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: 2 Likes; <NewLine> 
94837,"Please can you guys check this code for me ,because is not training",2020-09-01T16:55:57.029Z,1,42,"<div class=""post"" itemprop=""articleBody""><NewLine><p>#%% LSTM architecture<br/><NewLine>class LSTM(nn.Module):</p><NewLine><pre><code>def __init__(self, input_dim, hidden_dim, batch_size,num_layers,output_dim):<NewLine>    super(LSTM, self).__init__()<NewLine>    self.input_dim = input_dim<NewLine>    self.hidden_dim = hidden_dim<NewLine>    self.batch_size = batch_size<NewLine>    self.num_layers = num_layers<NewLine>    self.output_dim=output_dim<NewLine><NewLine>    # Define the LSTM layer<NewLine>    self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)<NewLine><NewLine>    # Define the output layer<NewLine>    self.linear = nn.Linear(self.hidden_dim, output_dim)<NewLine>def init_hidden(self):<NewLine>    return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),<NewLine>            torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))<NewLine>def forward(self, input):<NewLine>    lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))<NewLine>    <NewLine>    # Only take the output from the final timestep<NewLine>    y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))<NewLine>    return y_pred.view(-1)<NewLine></code></pre><NewLine><p>#%% Train the Model</p><NewLine><p>loss_epoch_train = []<br/><NewLine>loss_epoch_val = []<br/><NewLine>net = net.double()<br/><NewLine>for epoch in range(num_epochs):<br/><NewLine>loss_seq_train = []<br/><NewLine>loss_seq_val = []<br/><NewLine># train loop<br/><NewLine>for seq, labels in train_loader:<br/><NewLine>seq, labels = seq.to(device), labels.to(device)<br/><NewLine># init hidden cell<br/><NewLine>net.hidden = net.init_hidden()<br/><NewLine># Clear stored gradient<br/><NewLine>optimizer.zero_grad()</p><NewLine><pre><code>    y_pred_train = net(seq.double())                        <NewLine>    <NewLine>    # loss computation and backpropagation<NewLine>    seq_loss = loss_function(y_pred_train, labels)    <NewLine>    loss_seq_train.append(seq_loss.data.cpu().numpy()) <NewLine>    seq_loss.backward()                                 <NewLine>    optimizer.step()                                   <NewLine>    print('Epoch: ' + str(epoch+1) + ', Loss: ' + str(seq_loss.item()))     <NewLine># val loop    <NewLine>for seq, labels in val_loader:                        <NewLine>    seq, labels = seq.to(device), labels.to(device)    <NewLine>    # current model prediction<NewLine>    y_pred_val = net(seq.double())                              <NewLine>    <NewLine>    # loss computation<NewLine>    seq_loss = loss_function(y_pred_val, labels)        <NewLine>    loss_seq_val.append(seq_loss.data.cpu().numpy())   <NewLine>                                                           <NewLine>loss_epoch_train.append(np.mean(loss_seq_train))        <NewLine>loss_epoch_val.append(np.mean(loss_seq_val))            <NewLine># print loss of validation and training data for each epoch<NewLine>print('Epoch '+str(epoch)+'/'+str(num_epochs)+': Train-Loss: '+str(np.round(loss_epoch_train[-1],4))+'; Val-Loss: '+str(np.round(loss_epoch_val[-1],4)))</code></pre><NewLine></div>",https://discuss.pytorch.org/u/johnago,(john),johnago,"September 1, 2020,  4:55pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/john"">@john</a>, is any part of this model using <code>DataParallel</code>, <code>DistributedDataParallel</code>, or <code>torch.distributed.rpc</code>? Any reason for tagging this question with “distributed-rpc”?</p><NewLine><p>The format of the code looks distorted, and will be hard to debug. Could you please share a properly-formatted self-contained example?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>please can i share my complete code with you because i really want to get it done correctly</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/johnago; <NewLine> ,"REPLY_DATE 1: September 1, 2020,  6:16pm; <NewLine> REPLY_DATE 2: September 1, 2020,  6:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
90863,Shared variable in GPU,2020-07-28T18:10:08.942Z,1,81,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I want to have two parallel processes in one GPU, one for training (calculating) and the other for communicating parameter updates with other GPUs. And both processes can modify a shared variable, (like a buffer to store the most updated parameters). Anyone knows is it possible to do this?<br/><NewLine>I checked this documentation: <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/notes/multiprocessing.html</a>, and it mentions “multiprocessing.Queue”, not sure is it suitable in my case? Or any good examples?</p><NewLine></div>",https://discuss.pytorch.org/u/Yi_Zhang,(Yi Zhang),Yi_Zhang,"July 28, 2020,  6:10pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This should be doable, see the example code in this post:</p><NewLine><aside class=""quote quote-modified"" data-post=""9"" data-topic=""90636""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/mrshenli/40/12220_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/exception-process-0-terminated-with-exit-code-1-error-when-using-torch-multiprocessing-spawn-to-parallelize-over-multiple-gpus/90636/9"">`Exception: process 0 terminated with exit code 1` error when using `torch.multiprocessing.spawn` to parallelize over multiple GPUs</a> <a class=""badge-wrapper bullet"" href=""/c/distributed/12""><span class=""badge-category-bg"" style=""background-color: #0088CC;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""">distributed</span></a><NewLine></div><NewLine><blockquote><NewLine>    You can use <a href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor.share_memory_"" rel=""nofollow noopener"">share_momery_()</a> and torch.multiprocessing.SimpleQueue to implement IPC. E.g.: <NewLine>import numpy as np<NewLine>import torch<NewLine>import torch.multiprocessing as mp<NewLine><NewLine><NewLine>def func(rank, x, p2c, c2p):<NewLine>    x_power = x.to(rank) ** rank<NewLine>    c2p.put(x_power)<NewLine>    # citing multiprocessing doc: Unlike CPU tensors, the <NewLine>    # sending process is required to keep the original tensor <NewLine>    # as long as the receiving process retains a copy of <NewLine>    # the tensor. The refcounting is implemented under the <NewLine>    # hood but re…<NewLine>  </blockquote><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: July 28, 2020,  7:22pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
85357,Logging with Distributed Data Parallel with PyTorch,2020-06-13T18:17:48.356Z,2,289,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to setup a training workflow with PyTorch DistributedDataParallel (DDP). Generally when I train I pass a logger through to track outputs and record useful information. However, I am having trouble using the logger I have with the DDP method. Right now my code is as follows:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.multiprocessing as mp<NewLine><NewLine><NewLine>class BaseModel:<NewLine>    def __init__(self, *args, **kwargs):<NewLine>        ...<NewLine>        ""does things""<NewLine><NewLine>    def fit(self, *args, **kwargs):<NewLine>        ...<NewLine>        'set up stuff'<NewLine>        mp.spawn(self.distributed_training, nprocs=self.num_gpus, args=(self.params, training_input, self.logger))<NewLine>        <NewLine>    def distributed_training(params, training_input, logger):<NewLine>        ...<NewLine>        for e in epochs:<NewLine>            'trains for an epoch'<NewLine>            logger.info(print_line)<NewLine></code></pre><NewLine><p>I know I am supposed to use the <code>QueueHandler</code> and <code>QueueListener</code> tools from <code>logging</code> with the import, but I have been scouring the internet and still do not have a clear understanding as to how. Any help would be greatly appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/kleingeo,(Geoff Klein),kleingeo,"June 13, 2020, 11:15pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What sort of issues do you encounter when running this code?</p><NewLine><p>You could also consider  creating a per-spawned process logger and no longer passing in the same logger into the spawned processes. This would also allow you to configure your logging on a per-DDP process basis, for example, write the logs to different files depending on the process.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>With the way the code is set-up, things passed to the logger in the spawned process just don’t go through (ie, wont print or save). I’m not sure how a spawned process logger would help, as I need to capture things in the training. Mainly, I’m just trying to record loss and metrics per-epoch, nothing I would consider overly special for a training process. I think the issue is more that the logger is initiated on the main process so sending it to some type of other process is causing issues (whether it would be on its own process or in the distributed process).</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Having the same issue here.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/rvarm1; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kleingeo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tengerye; <NewLine> ,"REPLY_DATE 1: June 23, 2020,  6:16am; <NewLine> REPLY_DATE 2: June 24, 2020,  1:09pm; <NewLine> REPLY_DATE 3: August 31, 2020,  7:53am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
90993,Profiling Distributed Data Parallel Applications,2020-07-29T17:28:57.444Z,1,69,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to profile an application using DistributedDataParallel Module.</p><NewLine><p>Is there a specific set of guidelines to measure the communication overheads (allreduce time, broadcast time, etc)?</p><NewLine><p>I used the <code>with torch.autograd.profiler.profile(use_cuda=True)</code>. But I didn’t get information about these calls. This may only track basic calls not functions like allreduce or broadcast happening in ProcessGroups (NCCL) layer.</p><NewLine><p>Please correct me if I am wrong.</p><NewLine><p>Thank You,<br/><NewLine>Vibhatha.</p><NewLine></div>",https://discuss.pytorch.org/u/Vibhatha_Abeykoon,(Vibhatha Abeykoon),Vibhatha_Abeykoon,"July 29, 2020,  5:28pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/vibhatha_abeykoon"">@Vibhatha_Abeykoon</a> DDP does not work with autograd profiler yet, but this is in our roadmap. Before that, will <a href=""https://developer.nvidia.com/blog/cuda-pro-tip-nvprof-your-handy-universal-gpu-profiler/"" rel=""nofollow noopener"">nvprof</a> able to serve your use case?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> Sorry for the late response. Yes, it could also be useful. I will check.<br/><NewLine>Thank You.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Vibhatha_Abeykoon; <NewLine> ,"REPLY_DATE 1: July 29, 2020,  6:13pm; <NewLine> REPLY_DATE 2: August 31, 2020,  4:34am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
94513,Synchronize some information between processes at the end of each epoch (use case: setting time limit for training),2020-08-29T16:31:44.017Z,2,53,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a training script which I launch using <code>torch.distributed.launch</code> on multiple GPUs. I would like to set a time limit, so that my training will early stop without surpassing this limit. Something like this:</p><NewLine><pre><code class=""lang-python""># for storing the running times of the last 3 epochs<NewLine>epoch_time_queue = deque(maxlen=3)<NewLine><NewLine>start_time = time.time()<NewLine>for epoch in range(start_epoch, args.epochs):<NewLine>    start_epoch_time = time.time()<NewLine>    # training<NewLine>    train_epoch(...)<NewLine>    # validation<NewLine>    eval_epoch(...)<NewLine>    # epoch time in minutes<NewLine>    epoch_time = (time.time() - start_epoch_time)/60<NewLine>    # average duration of the last 3 epochs<NewLine>    epoch_time_queue.append(epoch_time)<NewLine>    avg_time = sum(epoch_time_queue)/len(epoch_time_queue)<NewLine>    # if the next epoch will likely surpass the time limit, then stop here<NewLine>    estimated_next_total_time = (time.time() - start_time)/60 + avg_time<NewLine>    if args.time_limit &gt; 0 and estimated_next_total_time &gt; args.time_limit:<NewLine>        break<NewLine></code></pre><NewLine><p>The issue is that the elapsed time may be different between processes. For example, at the end of the 5th epoch, the process on GPU1 may think that it will surpass the time limit at the next (6th) epoch by a few seconds, so it stops; while GPU2 thinks that it will be able to finish the 6th epoch a few seconds before the limit, so it will continue, which is not good.</p><NewLine><p>I would like to know if there is a way for the processes to communicate about this. Ideally, a process should wait for all the other processes to finish the current epoch to decide whether to go for the next epoch or not.</p><NewLine></div>",https://discuss.pytorch.org/u/f10w,,f10w,"August 29, 2020,  4:31pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/f10w"">@f10w</a> yep, this is possible, you can use the <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather"" rel=""nofollow noopener""><code>all_gather</code></a> API to let every process to collect elapsed time from all processes.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply!<br/><NewLine>When we do <code>torch.distributed.all_gather()</code>, does it create some kind of “barrier” between the processes?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, it does, all collective communications (e.g., <code>boradcast</code>, <code>all_reduce</code>, <code>all_gather</code>, etc.) can be considered as a barrier.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/f10w; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 30, 2020, 11:24pm; <NewLine> REPLY_DATE 2: August 29, 2020,  9:09pm; <NewLine> REPLY_DATE 3: August 31, 2020, 12:56am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
93384,Pytorch suddenyl stops recognising GPU,2020-08-19T16:00:11.823Z,11,207,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am using PyTorch through Anaconda Environment and something weird happens. While working or if I leave the machine for some time and come back, PyTorch stops recognizing the GPU. And the only way it starts recognizing the GPU is after rebooting the machine.</p><NewLine><p>Why does this happen?</p><NewLine></div>",https://discuss.pytorch.org/u/Flock1,(Flock Anizak),Flock1,"August 19, 2020,  4:00pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You mean <code>torch.cuda.device_count()</code> returns 0? Can you confirm <code>nvidia-smi</code> still works correctly when that happens? And can you also check what is the value for <code>CUDA_VISIBLE_DEVICES</code> env var?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Yeah. <code>torch.cuda.device_count()</code> returns 0 and <code>torch.cuda.current_device()</code> returns the following:</p><NewLine><pre><code class=""lang-auto"">THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1591914886554/work/aten/src/THC/THCGeneral.cpp line=47 error=999 : unknown error<NewLine>Traceback (most recent call last):<NewLine>  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;<NewLine>  File ""/home/user/anaconda3/envs/work/lib/python3.8/site-packages/torch/cuda/__init__.py"", line 330, in current_device<NewLine>    _lazy_init()<NewLine>  File ""/home/user/anaconda3/envs/work/lib/python3.8/site-packages/torch/cuda/__init__.py"", line 153, in _lazy_init<NewLine>    torch._C._cuda_init()<NewLine>RuntimeError: cuda runtime error (999) : unknown error at /opt/conda/conda-bld/pytorch_1591914886554/work/aten/src/THC/THCGeneral.cpp:47<NewLine></code></pre><NewLine><p><code>nvidia-smi</code> works. For <code>CUDA_VISIBLE_DEVICES</code>, I get nothing.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>This happens to me sometimes and to fix without rebooting I reload gpu using</p><NewLine><pre><code class=""lang-auto"">$ sudo rmmod nvidia_uvm<NewLine>$ sudo modprobe nvidia_uvm<NewLine></code></pre><NewLine><p>No idea why it happens though</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>This works. How did you find out this solution? It’s so weird right. Suddenly it stops working. I think there’s some internal functioning of pytorch that changes something</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hmmm…since it worked on rebooting my laptop, I guessed it should work by just reloading the gpu. So, searched online on how to reboot nvidia gpu.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the solution. This has been bothering me for quite some time now. I’m sure they’ll fix this in later versions.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""93384"" data-username=""Flock1""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/flock1/40/12220_2.png"" width=""20""/> Flock1:</div><NewLine><blockquote><NewLine><p>I think there’s some internal functioning of pytorch that changes something</p><NewLine></blockquote><NewLine></aside><NewLine><p>Are any other CUDA applications running fine, i.e. are you able to run some CUDA examples etc.?<br/><NewLine>I’m not sure, if this is a PyTorch-related issue or rather a CUDA/NVIDIA driver issue.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I didn’t check that unfortunately. I can try checking with Keras if that lobrary is unable to recognize the GPU.</p><NewLine><p>I’ll also try running CUDA examples from within the environment and outside it.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello Flock!</p><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""1"" data-topic=""93384"" data-username=""Flock1""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/flock1/40/12220_2.png"" width=""20""/> Flock1:</div><NewLine><blockquote><NewLine><p>While working or if I leave the machine for some time and come back, PyTorch stops recognizing the GPU. And the only way it starts recognizing the GPU is after rebooting the machine.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Just to share my experience (with an old version of pytorch and an<br/><NewLine>old gpu):</p><NewLine><p>I see something similar to this.  If I launch a fresh python session<br/><NewLine>and run a pytorch script that uses cuda, then if I don’t use cuda<br/><NewLine>(or maybe just the python session) for a short-ish amount of time,<br/><NewLine>future use of cuda in that python session fails.</p><NewLine><p>But I don’t have to reboot my machine or “reload the gpu” to get<br/><NewLine>it working again; I only have to exit and restart python.</p><NewLine><p>I haven’t found any fix for it – I just live with it, restarting python as<br/><NewLine>necessary.</p><NewLine><p>Here’s a post of mine with some related observations:</p><NewLine><aside class=""quote quote-modified"" data-post=""1"" data-topic=""52696""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v4/letter/k/ecb155/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/some-observations-on-cuda-runtime-error-30/52696"">Some observations on ""cuda runtime error (30)""</a><NewLine></div><NewLine><blockquote><NewLine>    Hello Forum! <NewLine>I have some information about the behavior of “cuda runtime <NewLine>error (30)” (probably somewhat specific to my particular <NewLine>configuration). <NewLine>This is a follow-on to a number of threads about “error 30,” <NewLine>and, in particular, to this post: <NewLine><NewLine><NewLine>Clued in by Andrei’s observation that torch.cuda.is_available() <NewLine>“breaks” cuda, I find (for me) that if torch.cuda.is_available() <NewLine>is the first cuda call, subsequent cuda calls will throw “error 30” <NewLine>unless the first subsequent call is called promptly…<NewLine>  </blockquote><NewLine></aside><NewLine><p>Best.</p><NewLine><p>K. Frank</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you please share the versions of PyTorch and CUDA you are using (and perhaps a GPU type)?<br/><NewLine>Also, are there any messages printed to the kernel log (can be checked by running <code>dmesg</code>) when this happens?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey Frank,</p><NewLine><p>Thank you for sharing your experience.</p><NewLine><blockquote><NewLine><p>I see something similar to this. If I launch a fresh python session<br/><NewLine>and run a pytorch script that uses cuda, then if I don’t use cuda<br/><NewLine>(or maybe just the python session) for a short-ish amount of time,<br/><NewLine>future use of cuda in that python session fails.</p><NewLine></blockquote><NewLine><p>This is what happens but unfortunately for me, I have either have to restart the machine or reload the GPU. Just restarting python didn’t help. I even tried reloading the conda environment. It’s as if a switch went off and I have to physically switch it on again.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>I use PyTorch through conda environment.<br/><NewLine>PyTorch: 1.5.1<br/><NewLine>Cuda tool kit: 10.1.243</p><NewLine><p>On my machine, I have CUDA 11 for RTX 2070 Super GPU</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Does working from an anaconda environment affect this? Because the environment won’t use the CUDA innstalled on the machine but the one dowanloded by anaconda itself.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>As you said, the <code>cudatoolkit</code> from the conda binaries will be used and your local CUDA11 installation will thus not be used.<br/><NewLine>What do you mean by “affect this”?</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>I was referring to pytorch suddenly stops recognising thr GPU by ‘affect this’. So what I wanted to ask if is how to check which CUDA is causing the problem. The one that was installed with anaconda (<code>cudatoolkit</code>) or the one that’s locally installed (Cuda 11).</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the explanation. As said, the <code>cudatoolkit</code> (shipped via the binaries) will be used.<br/><NewLine>However, I doubt that CUDA is responsible for this behavior and would first look into potential hardware, driver, PSU issues.<br/><NewLine>You could check <code>dmesg</code> for any <code>XID</code> errors.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Flock1; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/user_123454321; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Flock1; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/user_123454321; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Flock1; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Flock1; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/KFrank; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/malfet; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/Flock1; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/Flock1; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/Flock1; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/Flock1; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: August 19, 2020,  4:15pm; <NewLine> REPLY_DATE 2: August 19, 2020,  4:45pm; <NewLine> REPLY_DATE 3: August 19, 2020,  6:29pm; <NewLine> REPLY_DATE 4: August 19, 2020,  5:30pm; <NewLine> REPLY_DATE 5: August 19, 2020,  6:23pm; <NewLine> REPLY_DATE 6: August 19, 2020,  6:30pm; <NewLine> REPLY_DATE 7: August 20, 2020,  8:39am; <NewLine> REPLY_DATE 8: August 20, 2020,  9:55am; <NewLine> REPLY_DATE 9: August 20, 2020,  1:52pm; <NewLine> REPLY_DATE 10: August 20, 2020,  5:25pm; <NewLine> REPLY_DATE 11: August 21, 2020,  5:03am; <NewLine> REPLY_DATE 12: August 21, 2020,  5:05am; <NewLine> REPLY_DATE 13: August 28, 2020,  3:49pm; <NewLine> REPLY_DATE 14: August 28, 2020, 11:54pm; <NewLine> REPLY_DATE 15: August 30, 2020,  8:21pm; <NewLine> REPLY_DATE 16: August 30, 2020, 11:26pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 2 Likes; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> 
88326,CUDA error: an illegal instruction was encountered,2020-07-08T04:53:03.090Z,6,476,"<div class=""post"" itemprop=""articleBody""><NewLine><p>a distributed training crashes with the following errors. Normally it works well, but sometimes it crashes with the following errors.</p><NewLine><p>Any idea to resolve it?</p><NewLine><p>pytorch 1.5, sync-bn is used, each GPU’s input has different dimensions.</p><NewLine><pre><code class=""lang-auto"">  File ""/opt/conda/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/apex/amp/_initialize.py"", line 197, in new_fwd<NewLine>    **applier(kwargs, input_caster))<NewLine>  File ""/tmp/code/quickdetection/src/FCOS/fcos_core/modeling/detector/generalized_rcnn.py"", line 49, in forward<NewLine>    features = self.backbone(images.tensors)<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/tmp/code/quickdetection/src/qd/layers/efficient_det.py"", line 1221, in forward<NewLine>    _, p3, p4, p5 = self.backbone_net(inputs)<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/tmp/code/quickdetection/src/qd/layers/efficient_det.py"", line 1067, in forward<NewLine>    x = self.model._bn0(x)<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py"", line 472, in forward<NewLine>    self.eps, exponential_average_factor, process_group, world_size)<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/_functions.py"", line 46, in forward<NewLine>    count_all.view(-1).long().tolist()<NewLine>RuntimeError: CUDA error: an illegal instruction was encountered<NewLine>terminate called after throwing an instance of 'c10::Error'<NewLine>  what():  CUDA error: an illegal instruction was encountered (insert_events at /opt/conda/conda-bld/pytorch_1591914742272/work/c10/cuda/CUDACachingAllocator.cpp:771)<NewLine>frame #0: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x4e (0x7fe430441b5e in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so)<NewLine>frame #1: c10::cuda::CUDACachingAllocator::raw_delete(void*) + 0x6d0 (0x7fe430686e30 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)<NewLine>frame #2: c10::TensorImpl::release_resources() + 0x4d (0x7fe43042f6ed in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so)<NewLine>frame #3: &lt;unknown function&gt; + 0x51e58a (0x7fe45da9358a in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so)<NewLine>&lt;omitting python frames&gt;<NewLine>frame #31: __libc_start_main + 0xf0 (0x7fe4783c6830 in /lib/x86_64-linux-gnu/libc.so.6)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/amsword,(Jianfeng Wang),amsword,"July 8, 2020,  4:53am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you update to PyTorch <code>1.5.1</code>, as <code>1.5.0</code> had a bug where internal assert statements were ignored?<br/><NewLine>This should hopefully yield a better error message than the illegal memory access.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>With Pytorch 1.6 / CUDA 10.2 /CUDNN7, I got following error occasionally:</p><NewLine><pre><code class=""lang-bash"">Traceback (most recent call last):<NewLine>  File ""train.py"", line 212, in &lt;module&gt;<NewLine>    train(None)<NewLine>  File ""/gemfield/hostpv/gemfield/deepvac/lib/syszux_deepvac.py"", line 335, in __call__<NewLine>    self.process()<NewLine>  File ""train.py"", line 163, in process<NewLine>    self.processTrain()<NewLine>  File ""/gemfield/hostpv/gemfield/deepvac/lib/syszux_deepvac.py"", line 294, in processTrain<NewLine>    self.doBackward()<NewLine>  File ""train.py"", line 139, in doBackward<NewLine>    self.loss.backward()<NewLine>  File ""/opt/conda/lib/python3.7/site-packages/torch/tensor.py"", line 185, in backward<NewLine>    torch.autograd.backward(self, gradient, retain_graph, create_graph)<NewLine>  File ""/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 127, in backward<NewLine>    allow_unreachable=True)  # allow_unreachable flag<NewLine>RuntimeError: transform: failed to synchronize: cudaErrorIllegalAddress: an illegal memory access was encountered<NewLine>terminate called after throwing an instance of 'c10::Error'<NewLine>  what():  CUDA error: an illegal memory access was encountered<NewLine>Exception raised from create_event_internal at /opt/conda/conda-bld/pytorch_1595629403081/work/c10/cuda/CUDACachingAllocator.cpp:687 (most recent call first):<NewLine>frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x4d (0x7fb3e291677d in /opt/conda/lib/python3.7/site-packages/torch/lib/libc10.so)<NewLine>frame #1: c10::cuda::CUDACachingAllocator::raw_delete(void*) + 0xb5d (0x7fb3e2b66d9d in /opt/conda/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)<NewLine>frame #2: c10::TensorImpl::release_resources() + 0x4d (0x7fb3e2902b1d in /opt/conda/lib/python3.7/site-packages/torch/lib/libc10.so)<NewLine>frame #3: &lt;unknown function&gt; + 0x53f0ea (0x7fb41c1990ea in /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so)<NewLine>&lt;omitting python frames&gt;<NewLine>frame #17: __libc_start_main + 0xe7 (0x7fb442bdfb97 in /lib/x86_64-linux-gnu/libc.so.6)<NewLine><NewLine>Aborted (core dumped)<NewLine></code></pre><NewLine><p>Don’t know it is a hardward issue，driver issue or pytorch issue？</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you post a minimal code snippet to reproduce this issue as well as your currently installed NVIDIA driver and the GPU you are using?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I got the same error with Pytorch1.6 for CUDA10.2 on Ubuntu 18.04</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you post a minimal code snippet as given in my previous post, so that we could have a look at this issue?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I also encountered similar issues with pytorch 1.6 with ubuntu 18 or ubuntu 16; cuda 10.1 or cuda 10.2. it works fine with pytorch 1.5.1, but this issues occurs occasionally with pytorch 1.6</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you rerun the code with <code>CUDA_LAUNCH_BLOCKING=1 python script.py args</code> and post the stack trace here?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks for your reply. however, this is random. roughly, 10% of the times, it will happen. Recently, i find pytorch 1.5.1 also has this issue. Note, in the following trace, CUDA_LAUNCH_BLOCKING is not set as 1. Paste it here and hopefully it can have some information.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/708175a3a549d3431bd077365eb99a920bd7036d"" href=""https://discuss.pytorch.org/uploads/default/original/3X/7/0/708175a3a549d3431bd077365eb99a920bd7036d.png"" title=""image""><img alt=""image"" data-base62-sha1=""g3gQenwRkHRxGJUos0WUlrUWG4Z"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/7/0/708175a3a549d3431bd077365eb99a920bd7036d_2_10x10.png"" height=""209"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/7/0/708175a3a549d3431bd077365eb99a920bd7036d_2_690x209.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/7/0/708175a3a549d3431bd077365eb99a920bd7036d_2_690x209.png, https://discuss.pytorch.org/uploads/default/optimized/3X/7/0/708175a3a549d3431bd077365eb99a920bd7036d_2_1035x313.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/7/0/708175a3a549d3431bd077365eb99a920bd7036d_2_1380x418.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">3084×938 265 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>another case. It seems like the error message is also random.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/dcf5a1d5e294ee564c50c35e80ed0c72c294bf86"" href=""https://discuss.pytorch.org/uploads/default/original/3X/d/c/dcf5a1d5e294ee564c50c35e80ed0c72c294bf86.png"" title=""image""><img alt=""image"" data-base62-sha1=""vwHj5VZbn2ycsOtyL8BbIFt7rpk"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/c/dcf5a1d5e294ee564c50c35e80ed0c72c294bf86_2_10x10.png"" height=""378"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/c/dcf5a1d5e294ee564c50c35e80ed0c72c294bf86_2_690x378.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/c/dcf5a1d5e294ee564c50c35e80ed0c72c294bf86_2_690x378.png, https://discuss.pytorch.org/uploads/default/optimized/3X/d/c/dcf5a1d5e294ee564c50c35e80ed0c72c294bf86_2_1035x567.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/d/c/dcf5a1d5e294ee564c50c35e80ed0c72c294bf86_2_1380x756.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">3312×1816 598 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you post the stack traces by wrapping them into three backticks ``` please?<br/><NewLine>If you don’t set <code>CUDA_LAUNCH_BLOCKING=1</code>, the stack trace might point to random lines of code.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/gemfield; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Xfan1025; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/amsword; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/amsword; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/amsword; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 8, 2020, 10:16am; <NewLine> REPLY_DATE 2: August 14, 2020,  2:54am; <NewLine> REPLY_DATE 3: August 14, 2020,  3:49am; <NewLine> REPLY_DATE 4: August 15, 2020,  8:56am; <NewLine> REPLY_DATE 5: August 18, 2020,  6:38am; <NewLine> REPLY_DATE 6: August 29, 2020,  4:13am; <NewLine> REPLY_DATE 7: August 29, 2020,  4:58am; <NewLine> REPLY_DATE 8: August 30, 2020, 12:53am; <NewLine> REPLY_DATE 9: August 30, 2020, 12:59am; <NewLine> REPLY_DATE 10: August 30, 2020,  2:28am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> 
94448,Why is it a bad idea to use python&rsquo;s `concurrent.futures` with pytorch and how can I paralellize batch loading in RL?,2020-08-28T18:07:03.898Z,0,86,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to load a batch from a replay buffer with pytorch asyncronously while optimizing the model parameters and thereby hide the batch loading latency. The program I run is as follows:</p><NewLine><pre><code class=""lang-python"">for _ in range(100):<NewLine>    begin = time.time()<NewLine>    batch = sample_batch()<NewLine>    batch_load += time.time() - begin<NewLine>    begin = time.time()<NewLine>    optimize(batch)<NewLine>    optimize_time += time.time() - begin<NewLine></code></pre><NewLine><p>When running this script, <code>batch_load</code> takes about 0.001 seconds and <code>optimize_time</code> about 0.009 seconds. To hide the latency of the batch_load (although it doesn’t take long in this program, it takes more time in another program which I would actually like to optimize), I thought I can use python<code>s </code>concurrent.futures<code>module to acquire a</code>future<code>from</code>sample_batch<code>and load it whilst</code>optimize` is running. This program instead looks as follows:</p><NewLine><pre><code class=""lang-auto"">with concurrent.futures.ProcessPoolExecutor(max_workers=12) as executor:<NewLine>    for _ in range(100):<NewLine>        begin = time.time()<NewLine>        future = executor.submit(sample_batch)<NewLine>        batch_load += time.time() - begin<NewLine>        begin = time.time()<NewLine>        optimize(batch)<NewLine>        optimize_time += time.time() - begin<NewLine>        batch = future.result()<NewLine></code></pre><NewLine><p>This turned out to be a pretty bad idea. The data loading time increases to 0.085 seconds and the optimization time increases to 0.13 seconds.</p><NewLine><p>Can somebody kindly educate me on why the second program is so much slower than the first? Furthermore, does somebody have any ideas on how to hide data loading latency? I appreciate any answers and suggestions very much!</p><NewLine></div>",https://discuss.pytorch.org/u/FabianSchuetze,(Fabian Schuetze),FabianSchuetze,"August 28, 2020,  6:19pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>As <code>batch_load</code> measures the latency of <code>executor.submit</code>, I assume that’s the overhead of  <code>ProcessPoolExecutor</code>?</p><NewLine><p>But it is still weird that the <code>optimize()</code> also increased a lot. Does <code>optimize()</code> run ops on GPU? If yes, you will need to either <code>torch.cuda.synchronize()</code> on that GPU, or use <a href=""https://pytorch.org/docs/stable/cuda.html#torch.cuda.Event.elapsed_time"" rel=""nofollow noopener""><code>elapsed_time</code></a> to measure the latency. Because, CUDA ops returns when the op is inserted to the stream instead of when the op is done.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> for your answer!</p><NewLine><p>Indeed, the slower run time was caused entirely by the overhead of the <code>ProcessPoolExecutor</code>.  It is interesting that this context has implications also for non-asynchronous procedure calls. I measured the entire program again with longer-running tasks and the overhead of the  <code>ProcessPoolExecutor</code> seemed to be constant but the latency of data loading could be hid below the <code>optimize</code> call.</p><NewLine><p>Again, thank you for your reply - It helped me a lot!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/FabianSchuetze; <NewLine> ,"REPLY_DATE 1: August 29, 2020,  9:16am; <NewLine> REPLY_DATE 2: August 29, 2020,  9:16am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
43369,Distributed Data Parallel vs Data Parallel. Data loading too slow for Distributed setting in the first batch of every epoch,2019-04-23T21:35:14.685Z,2,532,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to train a video classification model. I wrote a custom video dataset which essentially reads pre-extracted video frames from SSD. I want to train on a cluster of GPU machines with 4 GPU per node.</p><NewLine><p>While training on 1 machine with 4 GPUs, I have following observations under two settings</p><NewLine><p>Case 1. DistributedDataParallel:  with 4 threads for a machine (1 thread per GPU) the data loading time for the first batch of every epoch is a lot (~110 seconds)<br/><NewLine>Case 2. DataParallel: with 4 threads for the machine, the data loading time is significantly lower (for first batch of every epoch) than Case 1 (~1.5 seconds)</p><NewLine><p>I still want to use DistributedDataParallel as I want to train on multiple machines. But the extra 110 seconds every epoch is too much. How should I improve Distributed setting?</p><NewLine><p>Logs for reference.<br/><NewLine>Dataparallel 4 threads<br/><NewLine>Epoch: [0][0/7508]      Time 13.270 (13.270)    Data 1.521 (1.521)      Loss 6.2721 (6.2721)    Acc@1 0.000 (0.000)     Acc@5 0.000 (0.000)<br/><NewLine>Epoch: [0][10/7508]     Time 0.265 (1.459)      Data 0.000 (0.138)      Loss 17.9221 (17.1892)  Acc@1 0.000 (0.284)     Acc@5 0.000 (2.273)<br/><NewLine>Epoch: [0][20/7508]     Time 0.265 (0.890)      Data 0.000 (0.077)      Loss 20.7100 (14.7189)  Acc@1 0.000 (0.149)     Acc@5 0.000 (1.786)</p><NewLine><p>DistributedDataparallel 4 threads 1 thread each gpu<br/><NewLine>Epoch: [0][0/7508]      Time 117.339 (117.339)  Data 114.749 (114.749)  Loss 6.3962 (6.3962)    Acc@1 0.000 (0.000)     Acc@5 0.000 (0.000)<br/><NewLine>Epoch: [0][0/7508]      Time 117.070 (117.070)  Data 110.291 (110.291)  Loss 6.3759 (6.3759)    Acc@1 0.000 (0.000)     Acc@5 0.000 (0.000)<br/><NewLine>Epoch: [0][0/7508]      Time 117.479 (117.479)  Data 114.120 (114.120)  Loss 6.3918 (6.3918)    Acc@1 0.000 (0.000)     Acc@5 0.000 (0.000)<br/><NewLine>Epoch: [0][0/7508]      Time 116.495 (116.495)  Data 112.885 (112.885)  Loss 6.0654 (6.0654)    Acc@1 0.000 (0.000)     Acc@5 0.000 (0.000)<br/><NewLine>Epoch: [0][10/7508]     Time 0.248 (10.814)     Data 0.000 (10.262)     Loss 13.6280 (14.8321)  Acc@1 0.000 (0.000)     Acc@5 0.000 (0.000)<br/><NewLine>Epoch: [0][10/7508]     Time 0.248 (10.870)     Data 0.000 (10.030)     Loss 12.6716 (16.3162)  Acc@1 12.500 (1.136)    Acc@5 12.500 (2.273)<br/><NewLine>Epoch: [0][10/7508]     Time 0.252 (10.904)     Data 0.000 (10.375)     Loss 6.9328 (14.4093)   Acc@1 0.000 (1.136)     Acc@5 25.000 (3.409)<br/><NewLine>Epoch: [0][10/7508]     Time 0.251 (10.891)     Data 0.000 (10.432)     Loss 12.2168 (13.2482)  Acc@1 0.000 (0.000)     Acc@5 0.000 (0.000)<br/><NewLine>Epoch: [0][20/7508]     Time 0.252 (5.813)      Data 0.000 (5.260)      Loss 6.3584 (13.0522)   Acc@1 0.000 (0.595)     Acc@5 0.000 (1.190)<br/><NewLine>Epoch: [0][20/7508]     Time 0.254 (5.831)      Data 0.000 (5.440)      Loss 7.1645 (12.1273)   Acc@1 0.000 (0.595)     Acc@5 0.000 (1.786)<br/><NewLine>Epoch: [0][20/7508]     Time 0.250 (5.825)      Data 0.000 (5.470)      Loss 6.9019 (12.8164)   Acc@1 0.000 (0.595)     Acc@5 0.000 (0.595)<br/><NewLine>Epoch: [0][20/7508]     Time 0.252 (5.784)      Data 0.000 (5.381)      Loss 6.9181 (11.9140)   Acc@1 0.000 (0.000)     Acc@5 0.000 (0.000)</p><NewLine><p>For training script I am using a modified version of <a href=""https://github.com/pytorch/examples/blob/master/imagenet/main.py"" rel=""nofollow noopener"">https://github.com/pytorch/examples/blob/master/imagenet/main.py</a></p><NewLine></div>",https://discuss.pytorch.org/u/ruppesh,(Ruppesh Nalwaya),ruppesh,"April 23, 2019,  9:35pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, I am having the exactly same issue. Was your problem solved?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/c_ashraf"">@C_Ashraf</a> Is the data loading time for your case really slow or the time to actually execute DistributedDataParallel? If you have a small self contained example demonstrating the problem, it would be easier to narrow down the issue.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>My workflow is kind of complex and I do not have a self contained example. But, I will try to explain it in as much detail as possible.I have a very large dataset that I can not load into memory. So I wrote a custom dataset class</p><NewLine><pre><code class=""lang-auto"">class BigDataset(torch.utils.data.Dataset):<NewLine>    #def __init__(self, data_paths, target_paths):<NewLine>    def __init__(self, data_paths):<NewLine>        self.data_memmaps = [np.load(path, mmap_mode='r') for path in data_paths]<NewLine>        #self.target_memmaps = [np.load(path, mmap_mode='r') for path in target_paths]<NewLine>        self.start_indices = [0] * len(data_paths)<NewLine>        self.data_count = 0<NewLine>        for index, memmap in enumerate(self.data_memmaps):<NewLine>            self.start_indices[index] = self.data_count<NewLine>            self.data_count += memmap.shape[0]<NewLine><NewLine>    def __len__(self):<NewLine>        return self.data_count<NewLine><NewLine>    def __getitem__(self, index):<NewLine>        memmap_index = bisect(self.start_indices, index) - 1<NewLine>        index_in_memmap = index - self.start_indices[memmap_index]<NewLine>        data = self.data_memmaps[memmap_index][index_in_memmap]<NewLine>        return index, torch.from_numpy(data)<NewLine></code></pre><NewLine><p>Next, I read the locations of all the files (my data is separated over multiple files)</p><NewLine><pre><code class=""lang-auto"">    data_paths = [os.path.join(file_path, f'data/feature{index}.npy')<NewLine>                  for index in range(2)]<NewLine><NewLine>    dataset = BigDataset(data_paths)<NewLine></code></pre><NewLine><p>Since this dataset has both the train and validation data, I need to split it. Thus, I generate <code>train</code> and <code>val</code> indices and use the following code for train and val dataloader</p><NewLine><pre><code class=""lang-auto"">    if args.distributed:<NewLine>        #train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)<NewLine>        train_sampler = torch.utils.data.distributed.DistributedSampler(torch.utils.data.Subset(dataset, train_indices))<NewLine>        val_sampler = torch.utils.data.distributed.DistributedSampler(torch.utils.data.Subset(dataset, val_indices))<NewLine>    else:<NewLine>        train_sampler = SubsetRandomSampler(train_indices)<NewLine>        val_sampler = SubsetRandomSampler(val_indices)<NewLine><NewLine>    train_loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size,<NewLine>                                               num_workers=args.workers, sampler=train_sampler,<NewLine>                                               pin_memory=True)<NewLine>    val_loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size,<NewLine>                                             num_workers=args.worker, sampler=val_sampler,<NewLine>                                             pin_memory=True)<NewLine></code></pre><NewLine><p>I am pretty sure that the dataloader is causing the issue</p><NewLine><pre><code class=""lang-auto"">Epoch: [0][ 0/51]       Time 220.202 (220.202)  Data 205.658 (205.658)  Loss 39.61639 (39.61639)        Accuracy   0.00 (  0.00)<NewLine>Epoch: [0][ 0/51]       Time 220.181 (220.181)  Data 205.639 (205.639)  Loss 43.61139 (43.61139)        Accuracy   0.00 (  0.00)<NewLine>Epoch: [0][ 0/51]       Time 220.229 (220.229)  Data 205.687 (205.687)  Loss 35.34707 (35.34707)        Accuracy   0.00 (  0.00)<NewLine>Epoch: [0][ 0/51]       Time 220.228 (220.228)  Data 205.683 (205.683)  Loss 56.56057 (56.56057)        Accuracy   0.00 (  0.00)<NewLine><NewLine>Epoch: [0][ 1/51]       Time  0.917 (110.549)   Data  0.000 (102.820)   Loss 20.94585 (32.27862)        Accuracy   0.00 (  0.00)<NewLine>Epoch: [0][ 1/51]       Time  0.917 (110.560)   Data  0.000 (102.829)   Loss 63.88563 (51.75101)        Accuracy   0.00 (  0.00)<NewLine>Epoch: [0][ 1/51]       Time  0.917 (110.573)   Data  0.000 (102.844)   Loss 23.30010 (29.32359)        Accuracy   0.00 (  0.00)<NewLine>Epoch: [0][ 1/51]       Time  0.917 (110.572)   Data  0.000 (102.842)   Loss 33.03528 (44.79793)        Accuracy   0.00 (  0.00)<NewLine></code></pre><NewLine><p>I followed the same procedure described [here].(<a href=""https://github.com/pytorch/examples/blob/master/imagenet/main.py"" rel=""nofollow noopener"">https://github.com/pytorch/examples/blob/master/imagenet/main.py</a>)</p><NewLine><p>Is the way I am loading my data (not directly in the memory) causing this issue?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am able to reproduce the same behavior using imagenet example with <a href=""https://tiny-imagenet.herokuapp.com/"" rel=""nofollow noopener"">tiny_image_dataset</a>. Using a batch size of 256 in two gpus I get<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/9cc95c59937f8750e7ec8b85e49239d350afab19"" href=""https://discuss.pytorch.org/uploads/default/original/3X/9/c/9cc95c59937f8750e7ec8b85e49239d350afab19.png"" title=""image""><img alt=""image"" data-base62-sha1=""mmZTIWLnkc4HWXMYM6PEXtePLZL"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/c/9cc95c59937f8750e7ec8b85e49239d350afab19_2_10x10.png"" height=""130"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/c/9cc95c59937f8750e7ec8b85e49239d350afab19_2_690x130.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/c/9cc95c59937f8750e7ec8b85e49239d350afab19_2_690x130.png, https://discuss.pytorch.org/uploads/default/optimized/3X/9/c/9cc95c59937f8750e7ec8b85e49239d350afab19_2_1035x195.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/9/c/9cc95c59937f8750e7ec8b85e49239d350afab19_2_1380x260.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1390×262 39.8 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>While using a batch size of 512 in two gpus, it gives<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/865c3a0b89ce09450b652398db24bdcefbaacf76"" href=""https://discuss.pytorch.org/uploads/default/original/3X/8/6/865c3a0b89ce09450b652398db24bdcefbaacf76.png"" title=""image""><img alt=""image"" data-base62-sha1=""jaBzHOXSZmtBPStZRi7Yi70uLgG"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/6/865c3a0b89ce09450b652398db24bdcefbaacf76_2_10x10.png"" height=""122"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/6/865c3a0b89ce09450b652398db24bdcefbaacf76_2_690x122.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/6/865c3a0b89ce09450b652398db24bdcefbaacf76_2_690x122.png, https://discuss.pytorch.org/uploads/default/optimized/3X/8/6/865c3a0b89ce09450b652398db24bdcefbaacf76_2_1035x183.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/8/6/865c3a0b89ce09450b652398db24bdcefbaacf76_2_1380x244.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1384×246 39.8 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>Also, I assume this could be due to dataloader memory leak. If I use my entire dataset (120GB), I see out of memory (oom) kill before any batch is trained. I looked at pytorch discussion forum and looks like it is a very open issue. Any help solving this issue will be appreciated. Thanks.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/vitalyfedyunin"">@VitalyFedyunin</a> I was wondering if you could help out here since this seems like a dataloader issue?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! There is no (known) leak, but more like problem with misunderstanding how memory works in python+forking world, we are aware of this issue and planning to fix it this year (or sooner).</p><NewLine><p>Some work around discussed here: <a href=""https://github.com/pytorch/pytorch/issues/13246#issuecomment-612396143"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/13246#issuecomment-612396143</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/C_Ashraf; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/C_Ashraf; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/C_Ashraf; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/VitalyFedyunin; <NewLine> ,"REPLY_DATE 1: July 30, 2020,  1:46am; <NewLine> REPLY_DATE 2: July 30, 2020,  2:13am; <NewLine> REPLY_DATE 3: July 30, 2020,  3:21am; <NewLine> REPLY_DATE 4: July 30, 2020,  7:14pm; <NewLine> REPLY_DATE 5: July 30, 2020,  7:46pm; <NewLine> REPLY_DATE 6: August 28, 2020, 10:41pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
94081,DataParallel and DistributedDataParallel with fastai on AWS SageMaker performance,2020-08-25T17:56:46.456Z,3,88,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to make use of either distributed or parallel training using fastai and SageMaker notebooks or training jobs (somewhat fixed on using this service based on my team). I am running code on a <code>ml.p3.8xlarge</code> with 4x V100, but I cannot get any speed ups with any of the approaches I have taken.</p><NewLine><p>After spinning up the <code>ml.p3.8xlarge</code> notebook instance, here is the set up in my notebook using the pytorch env:</p><NewLine><pre><code class=""lang-auto"">%%bash<NewLine>pip install fastai==2.0.0 fastcore==1.0.0<NewLine>sudo mkdir -p /opt/ml/input/data/collab<NewLine>sudo chmod 777 /opt/ml/input/data/collab<NewLine></code></pre><NewLine><p>Here is the code I am testing:</p><NewLine><pre><code class=""lang-python"">import fastai, fastcore, torch<NewLine>print(f'fastai {fastai.__version__}')<NewLine>print(f'fastcore {fastcore.__version__}')<NewLine>print(f'torch {torch.__version__}')<NewLine><NewLine>from fastai.collab import *<NewLine>from fastai.tabular.all import *<NewLine>from fastai.distributed import *<NewLine><NewLine>path = untar_data(URLs.ML_100k, dest=""/opt/ml/input/data/collab"")<NewLine><NewLine>ratings = pd.read_csv(<NewLine>    path/'u.data',<NewLine>    delimiter='\t',<NewLine>    header=None,<NewLine>    names=['user','movie','rating','timestamp']<NewLine>)<NewLine><NewLine>movies = pd.read_csv(<NewLine>    path/'u.item',<NewLine>    delimiter='|',<NewLine>    encoding='latin-1',<NewLine>    usecols=(0,1),<NewLine>    names=['movie','title'],<NewLine>    header=None,<NewLine>)<NewLine><NewLine>ratings = ratings.merge(movies)<NewLine><NewLine>dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)<NewLine><NewLine>n_users = len(dls.classes['user'])<NewLine>n_movies = len(dls.classes['title'])<NewLine>n_factors = 64<NewLine><NewLine>model = EmbeddingDotBias(n_factors, n_users, n_movies)<NewLine><NewLine>learn = Learner(dls, model, loss_func=MSELossFlat())<NewLine><NewLine>print(learn.model)<NewLine><NewLine>print(""rank_distrib():"", rank_distrib())<NewLine>print(""num_distrib():"", num_distrib())<NewLine>print(""torch.cuda.device_count():"", torch.cuda.device_count())<NewLine><NewLine>epochs, lr = 5, 5e-3<NewLine><NewLine>print('learn.fit_one_cycle')<NewLine>learn.fit_one_cycle(epochs, lr)<NewLine><NewLine>print('with learn.distrib_ctx():')<NewLine>with learn.distrib_ctx():<NewLine>    learn.fit_one_cycle(epochs, lr)<NewLine><NewLine>print('with learn.distrib_ctx(torch.cuda.device_count()-1):')<NewLine>with learn.distrib_ctx(torch.cuda.device_count()-1):<NewLine>    learn.fit_one_cycle(epochs, lr)<NewLine><NewLine>print('with learn.parallel_ctx():')<NewLine>with learn.parallel_ctx():<NewLine>    learn.fit_one_cycle(epochs, lr)<NewLine><NewLine>print('nn.DataParallel(learn.model)')<NewLine>if torch.cuda.device_count() &gt; 1:<NewLine>    learn.model = nn.DataParallel(learn.model)<NewLine>learn.fit_one_cycle(epochs, lr)<NewLine></code></pre><NewLine><p>Here is the output from running code as a script:</p><NewLine><pre><code class=""lang-bash"">sh-4.2$ /home/ec2-user/anaconda3/envs/pytorch_p36/bin/python /home/ec2-user/SageMaker/cf.py<NewLine>fastai 2.0.0<NewLine>fastcore 0.1.39<NewLine>torch 1.6.0<NewLine>EmbeddingDotBias(<NewLine>  (u_weight): Embedding(944, 64)<NewLine>  (i_weight): Embedding(1665, 64)<NewLine>  (u_bias): Embedding(944, 1)<NewLine>  (i_bias): Embedding(1665, 1)<NewLine>)<NewLine>rank_distrib(): 0<NewLine>num_distrib(): 0<NewLine>torch.cuda.device_count(): 4<NewLine>learn.fit_one_cycle<NewLine>epoch     train_loss  valid_loss  time<NewLine>0         1.153435    1.154428    00:11<NewLine>1         0.957201    0.954827    00:11<NewLine>2         0.816548    0.878350    00:11<NewLine>with learn.distrib_ctx():<NewLine>epoch     train_loss  valid_loss  time<NewLine>0         0.999254    1.040871    00:11<NewLine>1         0.821853    0.914921    00:11<NewLine>2         0.658059    0.845227    00:11<NewLine>with learn.distrib_ctx(torch.cuda.device_count()-1):<NewLine>epoch     train_loss  valid_loss  time<NewLine>0         0.749317    0.997568    00:11<NewLine>1         0.580846    0.912386    00:11<NewLine>2         0.381058    0.878295    00:11<NewLine>with learn.parallel_ctx():<NewLine>epoch     train_loss  valid_loss  time<NewLine>0         0.514148    1.025872    00:25<NewLine>1         0.383893    0.996381    00:18<NewLine>2         0.204836    0.970403    00:18<NewLine>nn.DataParallel(learn.model)<NewLine>epoch     train_loss  valid_loss  time<NewLine>0         0.341708    1.103849    00:16<NewLine>1         0.272570    1.067705    00:16<NewLine>2         0.134262    1.055507    00:16<NewLine></code></pre><NewLine><p>Using the command <code>nvidia-smi dmon -s u</code> to watch GPU usage, I can see that only the training with <code>DataParallel</code> (using <code>with learn.parallel_ctx():</code> and <code>nn.DataParallel(learn.model)</code>) show GPU ids 1,2,3 being used. The problem is the data parallel is slower, even when I have tried increasing batch size or embedding size.</p><NewLine><p>Any help with this would be appreciated. I have a much larger collaborative filtering model I would like to use that is experiencing the same issues as this movie example and I need to reduce the training time hopefully with the use of parallel/distributed training.</p><NewLine></div>",https://discuss.pytorch.org/u/pl3,(Phil Lynch),pl3,"August 25, 2020,  5:57pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/pl3"">@pl3</a>, sorry about the delay.</p><NewLine><p>For <code>DataParallel</code> (DP), it can become slow when the model is large, as DP needs to replicate the model in every forward pass.</p><NewLine><p>For <code>DistributedDataParallel</code> (DDP), I would expect it is faster than local training. Which of the numbers shown above are <code>DistributedDataParallel</code>? And how did you initialize DDP module? When using DDP, did you reduce the per-process <code>batch_size</code> to <code>batch_size / world_size</code>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>For  <code>DataParallel</code>  (DP), it can become slow when the model is large, as DP needs to replicate the model in every forward pass.</p><NewLine></blockquote><NewLine><p>Ahh that makes sense why that is slower, especially for the models that I have a couple large embeddings.</p><NewLine><p>The lines after <code>with learn.distrib_ctx():</code> use DDP under the hood as a context manager that handles setting up and tearing down the distributed model. You can find a link to the code <a href=""https://github.com/fastai/fastai/blob/master/fastai/distributed.py#L145"" rel=""nofollow noopener"">here</a>, though it is a bit abstracted and a little difficult to understand (at least for me) depending on familiar with the fastai library.</p><NewLine><p>I am guessing there might be an issue with fastai functions/defaults for how it reads number of distributed GPUs available in sagemaker environments.</p><NewLine><blockquote><NewLine><p>When using DDP, did you reduce the per-process  <code>batch_size</code>  to  <code>batch_size / world_size</code> ?</p><NewLine></blockquote><NewLine><p>Slightly unclear what you mean here. I had the same batch size for each training loop which meant that each GPU in the DP would have been receiving 1/4th the batch size which I was assuming should have been faster.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""94081"" data-username=""pl3""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/p/8edcca/40.png"" width=""20""/> pl3:</div><NewLine><blockquote><NewLine><p>Slightly unclear what you mean here. I had the same batch size for each training loop which meant that each GPU in the DP would have been receiving 1/4th the batch size which I was assuming should have been faster.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I am not familiar with fastai’s DDP wrapper. When using the raw DDP API, applications need to spawn one process per GPU and then create one DDP instance and one dataloader in each process. With this setting, the per-process dataloader should use <code>batch_size/world_size</code> as the new batch size.</p><NewLine><p>Given the linked code, looks like it does not spawn subprocess for you. And it only calls <code>init_process_group</code> when <code>num_distrib() &gt; 1</code>. So, if you didn’t spawn subprocesses explicitly in application code, it might fall back to local training?</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/fastai/fastai/blob/3c6dca627c1f3812d58b0447bc9a45dd866c601f/fastai/distributed.py#L143-L159"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/fastai/fastai/blob/3c6dca627c1f3812d58b0447bc9a45dd866c601f/fastai/distributed.py#L143-L159"" rel=""nofollow noopener"" target=""_blank"">fastai/fastai/blob/3c6dca627c1f3812d58b0447bc9a45dd866c601f/fastai/distributed.py#L143-L159</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""143"" style=""counter-reset: li-counter 142 ;""><NewLine><li>@patch</li><NewLine><li>@contextmanager</li><NewLine><li>def distrib_ctx(self: Learner, cuda_id=None,sync_bn=True):</li><NewLine><li>    ""A context manager to adapt a learner to train in distributed data parallel mode.""</li><NewLine><li>    # Figure out the GPU to use from rank.  Create a dpg if none exists yet.</li><NewLine><li>    if cuda_id is None: cuda_id = rank_distrib()</li><NewLine><li>    if not torch.distributed.is_initialized():</li><NewLine><li>        setup_distrib(cuda_id)</li><NewLine><li>        cleanup_dpg =   torch.distributed.is_initialized()</li><NewLine><li>    else: cleanup_dpg = False</li><NewLine><li>    # Adapt self to DistributedDataParallel, yield, and cleanup afterwards.</li><NewLine><li>    try:</li><NewLine><li>        if num_distrib() &gt; 1: self.to_distributed(cuda_id,sync_bn)</li><NewLine><li>        yield self</li><NewLine><li>    finally:</li><NewLine><li>        self.detach_distributed()</li><NewLine><li>        if cleanup_dpg: teardown_distrib()</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>In case this is helpful, <a href=""https://pytorch.org/docs/stable/notes/ddp.html"" rel=""nofollow noopener"">here</a> is a quick example with a brief explanation of how DDP works. And <a href=""https://pytorch.org/tutorials/beginner/dist_overview.html#data-parallel-training"" rel=""nofollow noopener"">this section</a> tries to explain the differences between DP and DDP.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah you are correct, I just found out I was not implementing DDP correctly. I knew there was extra steps I needed to do to get DDP working, so was hoping that DP would speed things up, but with the large model that doesn’t seem to be the case.</p><NewLine><p>I found <a href=""https://github.com/fastai/fastai/blob/master/nbs/examples/train_imagenette.py"" rel=""nofollow noopener"">this example</a> in fastai which uses the distributed context, so I am working on my script to add in the correct functionality. I will review the links your provided as well, it seems I need to get into the docs a little more. I appreciate your help!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pl3; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/pl3; <NewLine> ,"REPLY_DATE 1: August 28, 2020,  3:42pm; <NewLine> REPLY_DATE 2: August 28, 2020,  7:51pm; <NewLine> REPLY_DATE 3: August 28, 2020,  9:22pm; <NewLine> REPLY_DATE 4: August 28, 2020,  9:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
94441,Numba cuda and pytorch distribution may conflict,2020-08-28T16:38:51.288Z,1,53,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to use numba and pytorch distribution simultaneously. When I create new tensor on GPU, I got cuda initialization error:<br/><NewLine>Here is my code:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import numpy as np<NewLine>import torch.distributed as dist<NewLine>from torch.multiprocessing import Process<NewLine>from numba import cuda<NewLine>import os<NewLine>def func(idx):<NewLine>    a = torch.randn([10, 10, 10], device=‘cuda’)<NewLine>return<NewLine>def init_process(idx, size, fn, backend=‘NCCL’):<NewLine>    os.environ[‘MASTER_ADDR’] = ‘127.0.0.1’<NewLine>    os.environ[‘MASTER_PORT’] = ‘29500’<NewLine>    dist.init_process_group(backend, rank=idx, world_size=size)<NewLine>    fn(idx)<NewLine>    return<NewLine>if  **name**  == “ **main** ”:<NewLine>    processes = []<NewLine>    for idx in range(2):<NewLine>        p = Process(target=init_process, args=(idx, 2, func))<NewLine>        p.start()<NewLine>        processes.append(p)<NewLine>    for p in processes:<NewLine>        p.join()<NewLine></code></pre><NewLine><p>Here is error report:</p><NewLine><pre><code class=""lang-auto"">THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=47 error=3 : initialization error<NewLine>Process Process-2:<NewLine>Traceback (most recent call last):<NewLine>File “/home/sss/anaconda3/envs/torch_new/lib/python3.8/multiprocessing/process.py”, line 315, in _bootstrap<NewLine>self.run()<NewLine>File “/home/sss/anaconda3/envs/torch_new/lib/python3.8/multiprocessing/process.py”, line 108, in run<NewLine>self._target(*self._args, **self._kwargs)<NewLine>File “/home/sss/Desktop/Experiment/test.py”, line 14, in init_process<NewLine>fn(idx)<NewLine>File “/home/sss/Desktop/Experiment/test.py”, line 8, in func<NewLine>a = torch.randn([10, 10, 10], device=‘cuda’)<NewLine>File “/home/sss/anaconda3/envs/torch_new/lib/python3.8/site-packages/torch/cuda/ **init** .py”, line 190, in _lazy_init<NewLine>torch._C._cuda_init()<NewLine>RuntimeError: cuda runtime error (3) : initialization error at /pytorch/aten/src/THC/THCGeneral.cpp:47<NewLine>THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=47 error=3 : initialization error<NewLine>Process Process-1:<NewLine>Traceback (most recent call last):<NewLine>File “/home/sss/anaconda3/envs/torch_new/lib/python3.8/multiprocessing/process.py”, line 315, in _bootstrap<NewLine>self.run()<NewLine>File “/home/sss/anaconda3/envs/torch_new/lib/python3.8/multiprocessing/process.py”, line 108, in run<NewLine>self._target(*self._args, **self._kwargs)<NewLine>File “/home/sss/Desktop/Experiment/test.py”, line 14, in init_process<NewLine>fn(idx)<NewLine>File “/home/sss/Desktop/Experiment/test.py”, line 8, in func<NewLine>a = torch.randn([10, 10, 10], device=‘cuda’)<NewLine>File “/home/sss/anaconda3/envs/torch_new/lib/python3.8/site-packages/torch/cuda/ **init** .py”, line 190, in _lazy_init<NewLine>torch._C._cuda_init()<NewLine>RuntimeError: cuda runtime error (3) : initialization error at /pytorch/aten/src/THC/THCGeneral.cpp:47<NewLine></code></pre><NewLine><p>When I comment “from numba import cuda”, there is no error reported. Since I really need numba and cuda, I can’t comment them. Can anyone solve my problems?</p><NewLine></div>",https://discuss.pytorch.org/u/torch_torch,(torch_torch),torch_torch,"August 28, 2020,  4:44pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could either see which version of cuda numba uses see if PyTorch offers that, too (or vice versa) or you could self-compile one or the other or both.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for the reply.<br/><NewLine>I compiled a new numba for pytorch. It worked !<br/><NewLine>Thank you !</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/torch_torch; <NewLine> ,"REPLY_DATE 1: August 28, 2020,  5:20pm; <NewLine> REPLY_DATE 2: August 28, 2020,  5:20pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
94405,How to store embeddings from different ranks in DistributedDataParallel mode?,2020-08-28T11:32:05.831Z,3,39,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to run my model on dataset and store all embeddings using DistributedDataParallel. I created dataloader with DistributedSampler and now want to store all embeddings in the form:<br/><NewLine>(image_name, embedding)</p><NewLine><p>And after that I want to save them as csv or pickle file.</p><NewLine><p>Will it be correct to create a global list and store data there or will there be conflicts with writing to the list?</p><NewLine></div>",https://discuss.pytorch.org/u/RocketFlash,(Rauf),RocketFlash,"August 28, 2020, 11:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""94405"" data-username=""RocketFlash""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/r/3da27b/40.png"" width=""20""/> RocketFlash:</div><NewLine><blockquote><NewLine><p>Will it be correct to create a global list and store data there or will there be conflicts with writing to the list?</p><NewLine></blockquote><NewLine></aside><NewLine><p>By “global list”, you mean Python global variable? And this will create a global list per process? Who will be writing to the global list? BTW, any reason for not using <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding"" rel=""nofollow noopener""><code>nn.Embedding</code></a>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, by “global list” I mean global python variable. I am using mp.spawn to start distributed training, so I thought that the variables inside the executable file in this case are visible to all ranks. But after executing the code, nothing was written into the dict. What are the benefits of using nn.Embedding? I want to store image_name and embeddings.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""94405"" data-username=""RocketFlash""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/r/3da27b/40.png"" width=""20""/> RocketFlash:</div><NewLine><blockquote><NewLine><p>But after executing the code, nothing was written into the dict.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Right, global vars are per-process, so each spawned child process will have a different global var.</p><NewLine><blockquote><NewLine><p>What are the benefits of using nn.Embedding?</p><NewLine></blockquote><NewLine><p>One benefit is that you can then run lookup ops on GPU. And if you need to let the training process to update the embedding as well, using <code>nn.Embedding</code> will also make it easier.</p><NewLine><blockquote><NewLine><p>I want to store image_name and embeddings.</p><NewLine></blockquote><NewLine><p>If you would like to pass those data back to the main process, one option is to use the multiprocessing <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html#reuse-buffers-passed-through-a-queue"" rel=""nofollow noopener"">SimpleQueue</a>. See the example below.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/cb26661fe4faf26386703180a9045e6ac6d157df/test/test_multiprocessing.py#L580-L600"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/cb26661fe4faf26386703180a9045e6ac6d157df/test/test_multiprocessing.py#L580-L600"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/cb26661fe4faf26386703180a9045e6ac6d157df/test/test_multiprocessing.py#L580-L600</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""580"" style=""counter-reset: li-counter 579 ;""><NewLine><li>def test_event_multiprocess(self):</li><NewLine><li>    event = torch.cuda.Event(enable_timing=False, interprocess=True)</li><NewLine><li>    self.assertTrue(event.query())</li><NewLine><li><NewLine></li><li>    ctx = mp.get_context('spawn')</li><NewLine><li>    p2c = ctx.SimpleQueue()</li><NewLine><li>    c2p = ctx.SimpleQueue()</li><NewLine><li>    p = ctx.Process(</li><NewLine><li>        target=TestMultiprocessing._test_event_multiprocess_child,</li><NewLine><li>        args=(event, p2c, c2p))</li><NewLine><li>    p.start()</li><NewLine><li><NewLine></li><li>    c2p.get()  # wait for until child process is ready</li><NewLine><li>    torch.cuda._sleep(50000000)  # spin for about 50 ms</li><NewLine><li>    event.record()</li><NewLine><li>    p2c.put(0)  # notify child event is recorded</li><NewLine><li><NewLine></li><li>    self.assertFalse(event.query())</li><NewLine><li>    c2p.get()  # wait for synchronization in child</li><NewLine><li>    self.assertTrue(event.query())</li><NewLine><li>    p.join()</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>I am trying to understand this requirement. In your application, is it like each subprocess will produce some image embedding independently and concurrently, and then you wanna save those?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""94405"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mrshenli/40/12220_2.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>In your application, is it like each subprocess will produce some image embedding independently and concurrently, and then you wanna save those?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, each subprocess generate embeddings from dataloader batches. I want to process all my data (generate embeddings) as fast as possible, this is why I want to use DistributedDataParallel. Process and after that save everything in one file.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/RocketFlash; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/RocketFlash; <NewLine> ,"REPLY_DATE 1: August 28, 2020,  3:19pm; <NewLine> REPLY_DATE 2: August 28, 2020,  3:42pm; <NewLine> REPLY_DATE 3: August 28, 2020,  3:53pm; <NewLine> REPLY_DATE 4: August 28, 2020,  4:05pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
94267,How to validate in DistributedDataParallel correctly?,2020-08-27T08:03:57.735Z,7,95,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to train and validate model using DistributedDataParallel. Everything is fine during training, but when the model starts validate, the code works several iterations and after crashes due to errors with threads. I do validation only in rank=0. Do I need to put dist.barrier() somewhere? Or do I need to validate in all ranks?</p><NewLine></div>",https://discuss.pytorch.org/u/RocketFlash,(Rauf),RocketFlash,"August 27, 2020,  8:03am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>When rank0 validates the model, what do other ranks do? Exit or proceed to the next training phase? And what error did you see?</p><NewLine><p>I am assuming other ranks proceed to the next training phase and then timeout during DDP backward pass. If this is the case, yep, you can try use a <code>dist.barrier()</code> to sync, like:</p><NewLine><pre><code class=""lang-python"">for _ in range(...):<NewLine>    train()<NewLine>    if rank == 0:<NewLine>        validate()<NewLine>    dist.barrier()<NewLine></code></pre><NewLine><p>If you still hit timeout at barrier, you can try setting the timeout arg in <code>init_process_group</code> to a larger value.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems I solved the problem. I used DistributedSampler in validation dataloader, now I changed sampler to None and now code works. Other ranks do nothing. I don’t know how to share validation loss values across ranks, so I do validation only in rank=0. Is it a good practice to do so? Also I am using wandb to monitor training metrics, but after several epochs I got Timeout error at barrier. Is it correct to increase the value of  timeout limit in this situation?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Take a look <a href=""https://pytorch.org/docs/stable/distributed.html#collective-functions"" rel=""nofollow noopener"">here</a> on how to share information between your ranks.</p><NewLine><p>Since you’re using wandb, I’m assuming that you’re also only logging with rank 0 as well? I wouldn’t say it’s bad practice to share the validation loss with all your ranks if the other ranks aren’t doing anything with the information. It’s important when you’re doing early stopping or learning rate scheduling based off of the validation loss.</p><NewLine><p>Personally, I had all my ranks participate in computing the validation loss. In this scenario, you wouldn’t have to deal with barriers and instead just share the tensors containing your losses. But I don’t think it’s necessarily wrong to increase the timeout limit.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you, <a class=""mention"" href=""/u/ayalaa2"">@ayalaa2</a> ! I understand that if I used all the ranks for validation it would be faster. I will try to share tensor values across ranks.</p><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""94267"" data-username=""ayalaa2""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ayalaa2/40/21195_2.png"" width=""20""/> ayalaa2:</div><NewLine><blockquote><NewLine><p>ssuming that you’re also only logging with rank 0 as well?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, I am using wandb only with rank 0.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ayalaa2"">@ayalaa2</a> could you give some example code how to use sharing data between ranks, please?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""94267"" data-username=""RocketFlash""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/r/3da27b/40.png"" width=""20""/> RocketFlash:</div><NewLine><blockquote><NewLine><p>could you give some example code how to use sharing data between ranks, please?</p><NewLine></blockquote><NewLine></aside><NewLine><p>One option is to use the collective communication APIs, e.g., <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather"" rel=""nofollow noopener""><code>all_gather</code></a> (NCCL and Gloo) or <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.gather"" rel=""nofollow noopener""><code>gather</code></a> (Gloo). Sth like:</p><NewLine><pre><code class=""lang-python""># on all ranks<NewLine><NewLine>out_tensors = [torch.zeros(2, 2), torch.zeros(2, 2)]<NewLine>inp_tensor = [torch.ones(2, 2)]<NewLine><NewLine>torch.distributed.all_gather(out_tensors, inp_tensor)<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much, <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>!</p><NewLine><p>Is it possible to store not only tensors, but also list of strings for example?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Is it possible to store not only tensors, but also list of strings for example?</p><NewLine></blockquote><NewLine><p>Yep. But the feature is only available on master (will be released in v1.7). See the following code:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/cb26661fe4faf26386703180a9045e6ac6d157df/test/test_multiprocessing.py#L580-L600"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/cb26661fe4faf26386703180a9045e6ac6d157df/test/test_multiprocessing.py#L580-L600"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/cb26661fe4faf26386703180a9045e6ac6d157df/test/test_multiprocessing.py#L580-L600</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""580"" style=""counter-reset: li-counter 579 ;""><NewLine><li>def test_event_multiprocess(self):</li><NewLine><li>    event = torch.cuda.Event(enable_timing=False, interprocess=True)</li><NewLine><li>    self.assertTrue(event.query())</li><NewLine><li><NewLine></li><li>    ctx = mp.get_context('spawn')</li><NewLine><li>    p2c = ctx.SimpleQueue()</li><NewLine><li>    c2p = ctx.SimpleQueue()</li><NewLine><li>    p = ctx.Process(</li><NewLine><li>        target=TestMultiprocessing._test_event_multiprocess_child,</li><NewLine><li>        args=(event, p2c, c2p))</li><NewLine><li>    p.start()</li><NewLine><li><NewLine></li><li>    c2p.get()  # wait for until child process is ready</li><NewLine><li>    torch.cuda._sleep(50000000)  # spin for about 50 ms</li><NewLine><li>    event.record()</li><NewLine><li>    p2c.put(0)  # notify child event is recorded</li><NewLine><li><NewLine></li><li>    self.assertFalse(event.query())</li><NewLine><li>    c2p.get()  # wait for synchronization in child</li><NewLine><li>    self.assertTrue(event.query())</li><NewLine><li>    p.join()</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>If you need it now, you can copy the code change in <code>distributed_c10d.py</code> from <a href=""https://github.com/pytorch/pytorch/pull/42189/files"" rel=""nofollow noopener"">this PR</a>. It basically is a wrapper that converts strings to tensors.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/RocketFlash; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ayalaa2; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/RocketFlash; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/RocketFlash; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/RocketFlash; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 27, 2020,  2:49pm; <NewLine> REPLY_DATE 2: August 27, 2020,  5:08pm; <NewLine> REPLY_DATE 3: August 27, 2020,  5:26pm; <NewLine> REPLY_DATE 4: August 28, 2020,  8:00am; <NewLine> REPLY_DATE 5: August 28, 2020,  1:14pm; <NewLine> REPLY_DATE 6: August 28, 2020,  3:04pm; <NewLine> REPLY_DATE 7: August 28, 2020,  3:43pm; <NewLine> REPLY_DATE 8: August 28, 2020,  3:59pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
94337,Using apex AMP (Automatic Mixed Precision) with model parallelism,2020-08-27T17:58:40.296Z,3,76,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My model has a few LSTMs which run out of Cuda memory when run on large sequences with one GPU. So I shifted a few components of the model to another GPU. I tried 2 things with Apex AMP:</p><NewLine><ol><NewLine><li>Move the model components to another GPU before invoking  <code>amp.initialize</code> . In this case, I get NaNs soon after first backpropagation.</li><NewLine><li>First invoke  <code>amp.initialize</code> , and then move the model components to another GPU. In this case, its like the model backpropagation runs on a single GPU. It runs out of Cuda memory.</li><NewLine></ol><NewLine><p>The model training runs fine without Apex, so I suppose I am missing some step where the loss is backpropagated on both GPUs. I looked through the <a href=""https://nvidia.github.io/apex/parallel.html"" rel=""nofollow noopener"">documentations</a> of Apex, however, it only talks about DataParallelism, and not ModelParallelism.</p><NewLine><p>Any ideas?</p><NewLine></div>",https://discuss.pytorch.org/u/Caesar,(Parul Aggarwal),Caesar,"August 27, 2020,  5:59pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/caesar"">@Caesar</a>, have you tried the native AMP in PyTorch? I haven’t tried that with model parallel yet, but if it does not work, that will be a bug that we need to fix.</p><NewLine><p><a class=""onebox"" href=""https://pytorch.org/docs/stable/amp.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/stable/amp.html</a><br/><NewLine><a class=""onebox"" href=""https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your response. I am constrained to use an older version of PyTorch which does not support AMP natively. So I am usingNVIDIA apex. <a href=""https://github.com/NVIDIA/apex"" rel=""nofollow noopener"">https://github.com/NVIDIA/apex</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see.</p><NewLine><p>cc AMP author <a class=""mention"" href=""/u/mcarilli"">@mcarilli</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think the apex amp API supports this without complex/undocumented hacks.  Apex amp is in maintenance mode now, no new features will be added.</p><NewLine><p>However, <code>torch.cuda.amp</code> is designed to support model parallelism (ie different layers on different device) out of the box.  Please consider upgrading if at all possible.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your response.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>There’s a <a href=""https://github.com/pytorch/pytorch/blob/1a21c92364ea0527d7e8d3b0c6728e8e768e727e/test/test_cuda.py#L2127"" rel=""nofollow noopener"">multi-GPU <code>torch.cuda.amp.GradScaler</code> test case</a> that ensures ordinary GradScaler usage supports networks with layers on different devices.</p><NewLine><p><code>torch.cuda.amp.autocast</code> locally enables/disables autocast for all devices used by the invoking thread.  (However, the autocast state is thread local, so if you spawn a thread to control each device, you must re-invoke autocast in the side thread(s).  This affects usage with <a href=""https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-multiple-gpus"" rel=""nofollow noopener""><code>torch.nn.DataParallel</code> and <code>torch.nn.parallel.DistributedDataParallel</code> with multiple GPUs per process</a>.)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Caesar; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mcarilli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Caesar; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mcarilli; <NewLine> ,"REPLY_DATE 1: August 27, 2020,  6:04pm; <NewLine> REPLY_DATE 2: August 27, 2020,  7:02pm; <NewLine> REPLY_DATE 3: August 27, 2020,  7:31pm; <NewLine> REPLY_DATE 4: August 29, 2020, 11:10pm; <NewLine> REPLY_DATE 5: August 27, 2020, 11:59pm; <NewLine> REPLY_DATE 6: August 28, 2020, 12:32am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
93959,Memory management in PyTorch implementation of multi-processing queues,2020-08-24T19:23:14.774Z,0,65,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a question for the PyTorch development team.</p><NewLine><p>How is the memory consumed by queues in PyTorch implementation of multi-processing libraries  managed?</p><NewLine><p>If you can point me to the relevant piece of code (if available) and/or provide a textual description, I would appreciate it.</p><NewLine></div>",https://discuss.pytorch.org/u/mortazavi,(Masood S. Mortazavi),mortazavi,"August 24, 2020,  7:23pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/vitalyfedyunin"">@VitalyFedyunin</a> Could you help out here since its a torch.multiprocessing question?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please check</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/e75fb4356b752097d093c7013ba85c9eb82961ef/torch/multiprocessing/reductions.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/e75fb4356b752097d093c7013ba85c9eb82961ef/torch/multiprocessing/reductions.py"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/e75fb4356b752097d093c7013ba85c9eb82961ef/torch/multiprocessing/reductions.py</a></h4><NewLine><pre><code class=""lang-py"">import torch<NewLine>import torch.utils.hooks<NewLine>from torch._namedtensor_internals import check_serializing_named_tensor<NewLine>import os<NewLine>import threading<NewLine>import multiprocessing<NewLine>from multiprocessing.util import register_after_fork<NewLine>from multiprocessing.reduction import ForkingPickler<NewLine>try:<NewLine>    # Early load resource_sharer to prevent a partially initialized instance<NewLine>    # from being inherited in a forked child process. The reduce_storage method<NewLine>    # requires this module indirectly through DupFd(). The built-in mp.Queue<NewLine>    # class pickles arguments in a background thread which may overlap with the<NewLine>    # fork.<NewLine>    import multiprocessing.resource_sharer<NewLine>except ImportError:<NewLine>    pass<NewLine><NewLine><NewLine>class StorageWeakRef(object):<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/e75fb4356b752097d093c7013ba85c9eb82961ef/torch/multiprocessing/reductions.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>and</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/cca247635c6edb323176eeac7a18d3e9ab71c558/torch/multiprocessing/queue.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/cca247635c6edb323176eeac7a18d3e9ab71c558/torch/multiprocessing/queue.py"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/cca247635c6edb323176eeac7a18d3e9ab71c558/torch/multiprocessing/queue.py</a></h4><NewLine><pre><code class=""lang-py"">import io<NewLine>import multiprocessing<NewLine>import multiprocessing.queues<NewLine>from multiprocessing.reduction import ForkingPickler<NewLine>import pickle<NewLine><NewLine><NewLine>class ConnectionWrapper(object):<NewLine>    """"""Proxy class for _multiprocessing.Connection which uses ForkingPickler to<NewLine>    serialize objects""""""<NewLine><NewLine>    def __init__(self, conn):<NewLine>        self.conn = conn<NewLine><NewLine>    def send(self, obj):<NewLine>        buf = io.BytesIO()<NewLine>        ForkingPickler(buf, pickle.HIGHEST_PROTOCOL).dump(obj)<NewLine>        self.send_bytes(buf.getvalue())<NewLine><NewLine>    def recv(self):<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/cca247635c6edb323176eeac7a18d3e9ab71c558/torch/multiprocessing/queue.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>as methods are different for CPU/GPU</p><NewLine><p>generally speaking we are passing storage descriptors and do usage ref counting.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/VitalyFedyunin; <NewLine> ,"REPLY_DATE 1: August 25, 2020,  2:25am; <NewLine> REPLY_DATE 2: August 27, 2020, 10:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
94335,How to understand GPU status and training speed,2020-08-27T17:38:03.381Z,2,47,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I have done some experiments on multi-gpu training, and I feel a bit confused about the relationship between gpu status and training speed.<br/><NewLine>My original expectation was the slowest GPU would be the bottleneck for training speed, since local parameters need to sync in each step. But my experiment result proves I was wrong. Anyone can explain why the busiest GPU doesn’t slow down training speed as expected?</p><NewLine><p>I’m using all_reduce to sync parameters:</p><NewLine><pre><code class=""lang-auto"">for param in model.parameters():<NewLine>     if param.requires_grad and param.grad is not None:<NewLine>             torch.distributed.all_reduce(param.grad.data, <NewLine>                                          op=torch.distributed.ReduceOp.SUM)<NewLine></code></pre><NewLine><p>I measured GPU-Util, and my guess is higher GPU-Util means the GPU is busier and should be slower for training same size of batches. More experiment result for training the same dataset:</p><NewLine><p>test 1: 4 GPU with about 95% GPU-Util - training time is 35 sec<br/><NewLine>test 2: 2 GPU with 0% GPU-Util, 2 GPU with 90% GPU-Util - training time is 18 sec<br/><NewLine>test 3: 3 GPU with 0% GPU-Util, 1 GPU with 97% GPU-Util - training time is 15 sec<br/><NewLine>test 4: 4 GPU with about 0% GPU-Util - training time is 10 sec</p><NewLine><p>If the slowest GPU was the bottleneck, then training time of test 2 and test 3 should be similar as test 1. But how to understand this result?<br/><NewLine>Please also let me know if you notice any mistake in my experiment.</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Yi_Zhang,(Yi Zhang),Yi_Zhang,"August 27, 2020,  5:38pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>One reason might be that CUDA GPU shows 100% utilization when running NCCL collective communications, even if it is actually block waiting for other peers to join and doing nothing. So the GPU utilization number cannot faithfully represent how busy a GPU is.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>, thanks for reply. Then do you know what could be a better way to check a GPU’s status? For example, to compare training speed on each GPU when using multi-gpu training?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>One option might be using nvprof and then visualize the result. It will show time consumed by different comp and comm ops. See the following links:</p><NewLine><ol><NewLine><li><a href=""https://www.telesens.co/2019/04/04/distributed-data-parallel-training-using-pytorch-on-aws/"" rel=""nofollow noopener"">https://www.telesens.co/2019/04/04/distributed-data-parallel-training-using-pytorch-on-aws/</a></li><NewLine><li><a href=""https://developer.nvidia.com/blog/cuda-pro-tip-nvprof-your-handy-universal-gpu-profiler/"" rel=""nofollow noopener"">https://developer.nvidia.com/blog/cuda-pro-tip-nvprof-your-handy-universal-gpu-profiler/</a></li><NewLine></ol><NewLine><p>We are also working on extending autograd profiler to work with DDP, but we don’t have a target date for it yet.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Yi_Zhang; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 27, 2020,  6:00pm; <NewLine> REPLY_DATE 2: August 27, 2020,  7:59pm; <NewLine> REPLY_DATE 3: August 27, 2020,  9:03pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
94302,Return from mp.spawn(),2020-08-27T12:24:22.489Z,0,54,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!<br/><NewLine>I am using a <code>nn.parallel.DistributedDataParallel</code> model for both training and inference on multiple gpu.<br/><NewLine>To achieve that I use<br/><NewLine><code>mp.spawn(evaluate, nprocs=n_gpu, args=(args, eval_dataset))</code><br/><NewLine>To evaluate I actually need to first run the dev dataset examples through a model and then to aggregate the results. Therefore I need to be able to return my predictions to the main process (possibly in a dict, but some other data structure should work as well). I’ve tried providing an extra dict argument <code>mp.spawn(evaluate, nprocs=n_gpu, args=(args, eval_dataset, out_dict))</code> and modifying it in the function but apparently spawn copies it, so the dict in the main process is not modified.<br/><NewLine>I guess, I could write the results to the file and then read in the main process but it doesn’t seem like the most elegant solution. Is there a better way to return values from spawned functions?<br/><NewLine>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/arinaruck,(Arina Rak),arinaruck,"August 27, 2020, 12:25pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Is there a better way to return values from spawned functions?</p><NewLine></blockquote><NewLine><p>If you want to pass the result from spawned processes back to the parent process, you can let the parent process create <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html#reuse-buffers-passed-through-a-queue"" rel=""nofollow noopener"">multiprocessing queues</a>, pass it to children processes, and let children processes send result back through the queue. See the following code:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/cb26661fe4faf26386703180a9045e6ac6d157df/test/test_multiprocessing.py#L577-L600"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/cb26661fe4faf26386703180a9045e6ac6d157df/test/test_multiprocessing.py#L577-L600"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/cb26661fe4faf26386703180a9045e6ac6d157df/test/test_multiprocessing.py#L577-L600</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""577"" style=""counter-reset: li-counter 576 ;""><NewLine><li>@unittest.skipIf(NO_MULTIPROCESSING_SPAWN, ""Disabled for environments that \</li><NewLine><li>                 don't support multiprocessing with spawn start method"")</li><NewLine><li>@unittest.skipIf(not TEST_CUDA_IPC, 'CUDA IPC not available')</li><NewLine><li>def test_event_multiprocess(self):</li><NewLine><li>    event = torch.cuda.Event(enable_timing=False, interprocess=True)</li><NewLine><li>    self.assertTrue(event.query())</li><NewLine><li><NewLine></li><li>    ctx = mp.get_context('spawn')</li><NewLine><li>    p2c = ctx.SimpleQueue()</li><NewLine><li>    c2p = ctx.SimpleQueue()</li><NewLine><li>    p = ctx.Process(</li><NewLine><li>        target=TestMultiprocessing._test_event_multiprocess_child,</li><NewLine><li>        args=(event, p2c, c2p))</li><NewLine><li>    p.start()</li><NewLine><li><NewLine></li><li>    c2p.get()  # wait for until child process is ready</li><NewLine><li>    torch.cuda._sleep(50000000)  # spin for about 50 ms</li><NewLine><li>    event.record()</li><NewLine><li>    p2c.put(0)  # notify child event is recorded</li><NewLine><li><NewLine></li></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/cb26661fe4faf26386703180a9045e6ac6d157df/test/test_multiprocessing.py#L577-L600"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>If the result does not have to go back to the parent process, you can use <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.gather"" rel=""nofollow noopener"">gather</a> or <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather"" rel=""nofollow noopener"">allgather</a> to communicate the result across children processes.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 27, 2020,  2:28pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
93648,Distributed data parallel behavior,2020-08-21T15:25:15.083Z,0,68,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>What is the behavior of using DistributedDataParallel without running the training dataset using the DistributedSampler?. Will it mean that the models are deployed on multiple GPUs , but they end up working on the same data?. I am sort of confused about the behavior. Would be good to have some clarification. Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/Trinayan_Baruah,(Trinayan Baruah),Trinayan_Baruah,"August 21, 2020,  3:25pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/trinayan_baruah"">@Trinayan_Baruah</a></p><NewLine><p>Quote a recent discussion: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/comparison-data-parallel-distributed-data-parallel/93271"">Comparison Data Parallel Distributed data parallel</a></p><NewLine><p>Please also see <a href=""https://pytorch.org/docs/stable/notes/ddp.html"" rel=""nofollow noopener"">this brief note</a> and <a href=""https://arxiv.org/pdf/2006.15704.pdf"" rel=""nofollow noopener"">this full paper</a></p><NewLine><blockquote><NewLine><p>What is the behavior of using DistributedDataParallel without running the training dataset using the DistributedSampler? Will it mean that the models are deployed on multiple GPUs , but they end up working on the same data.</p><NewLine></blockquote><NewLine><p>Yep, if you don’t use DistributedSampler or manually shard input data for each process, they will be working on the same data. In this case, every DDP instance in each process will end up with the same gradient in every iteration. As a result local gradients and synchronized global gradients will be the same, making DDP useless.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok . thank you. it makes sense</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Trinayan_Baruah; <NewLine> ,"REPLY_DATE 1: August 26, 2020,  4:37pm; <NewLine> REPLY_DATE 2: August 26, 2020,  4:37pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
93273,Dynamic number of Branches,2020-08-18T19:38:38.773Z,0,55,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How to generate dynamic number of branches?<br/><NewLine>My code use a list to store the branches, but got</p><NewLine><blockquote><NewLine><p>RuntimeError: Caught RuntimeError in replica 1 on device 1.</p><NewLine></blockquote><NewLine><blockquote><NewLine><p>RuntimeError: Expected tensor for argument <span class=""hashtag"">#1</span> ‘input’ to have the same device as tensor for argument <span class=""hashtag"">#2</span> ‘weight’; but device 1 does not equal 0 (while checking arguments for cudnn_convolution)</p><NewLine></blockquote><NewLine><p>I think it is because the weights for branches are not copied to other GPUs.</p><NewLine><pre><code class=""lang-auto"">class ResNetONE(nn.Module):<NewLine>    def __init__(self, depth, num_classes=1000, num_branches=3, block_name='BasicBlock'):<NewLine>        super(ResNetONE, self).__init__()<NewLine>        # Model type specifies number of layers for CIFAR-10 model<NewLine>        if block_name.lower() == 'basicblock':<NewLine>            assert(depth - 2) % 6 == 0, 'When use basicblock, depth should be 6n+2, e.g. 20, 32, 44, 56, 110, 1202'<NewLine>            n = (depth - 2) // 6<NewLine>            block = BasicBlock<NewLine>        elif block_name.lower() == 'bottleneck':<NewLine>            assert (depth == 2) % 9 == 0, 'When use bottleneck, depth should be 9n + 2, e.g. 20, 29, 47, 56, 110, 1199'<NewLine>            n = (depth - 2) // 9<NewLine>            block = Bottleneck<NewLine>        else:<NewLine>            raise ValueError('block_name shinterval_sumould be Basicblock or Bottleneck')<NewLine><NewLine>        self.inplanes = 16<NewLine>        self.num_branches = num_branches<NewLine>        self.num_classes = num_classes<NewLine><NewLine>        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1, bias=False)<NewLine>        self.bn1 = nn.BatchNorm2d(16)<NewLine>        self.relu = nn.ReLU(inplace=True)<NewLine>        self.layer1 = self._make_layer(block, 16, n)<NewLine>        self.layer2 = self._make_layer(block, 32, n, stride=2)<NewLine>        self.layer3 = self._make_layer(block, 64, n, stride=2)<NewLine>        self.avgpool = nn.AvgPool2d(8)<NewLine>        self.fc = nn.Linear(64 * block.expansion, self.num_classes)<NewLine>        self.branches = self._make_branches(self.layer3, self.avgpool)<NewLine>        self.gate = self._make_gate()<NewLine><NewLine>        for m in self.modules():<NewLine>            if isinstance(m, nn.Conv2d):<NewLine>                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels<NewLine>                m.weight.data.normal_(0, math.sqrt(2./n))<NewLine>                # ""normal_"" is defined by torch._C._TensorBase<NewLine>                # https://pytorch.org/docs/stable/tensors.html#torch.Tensor.normal_<NewLine>                # https://zhuanlan.zhihu.com/p/100937718<NewLine>            elif isinstance(m, nn.BatchNorm2d):<NewLine>                m.weight.data.fill_(1)<NewLine>                m.bias.data.zero_()<NewLine><NewLine>    def _make_layer(self, block, planes, blocks, stride=1):<NewLine>        downsample = None<NewLine>        if stride != 1 or self.inplanes != planes * block.expansion:<NewLine>            downsample = nn.Sequential(<NewLine>                nn.Conv2d(self.inplanes, planes * block.expansion,<NewLine>                          kernel_size=1, stride=stride, bias=False),<NewLine>                nn.BatchNorm2d(planes * block.expansion)<NewLine>            )<NewLine>        layers = []<NewLine>        layers.append(block(self.inplanes, planes, stride, downsample))<NewLine>        self.inplanes = planes * block.expansion<NewLine>        for i in range(1, blocks):<NewLine>            layers.append(block(self.inplanes, planes))<NewLine>        return nn.Sequential(*layers)<NewLine><NewLine>    def _make_branches(self, *layers):<NewLine>        branches = []<NewLine>        for i in range(self.num_branches):<NewLine>            branch = nn.Sequential(*layers)<NewLine>            branches.append(branch)<NewLine>        return branches<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Erica_Zheng,(Erica Zheng),Erica_Zheng,"August 18, 2020,  7:38pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/erica_zheng"">@Erica_Zheng</a>, are you using <code>DataParallel</code>, <code>DistributedDataParallel</code> or <code>torch.distributed</code>? And can you include a min repro? Looks like the above example does not include the <code>forward()</code> function or how the model forward pass was launched?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>  Thank you, Shen!<br/><NewLine>No Runtime Error now by using<br/><NewLine><code>branches = nn.ModuleList(branches)</code><br/><NewLine>instead of python list.<br/><NewLine>Corresponding file located in ‘models/resnet.py --&gt; ResNetONE()’.</p><NewLine><p>More details are in <a href=""https://github.com/zzzheng/ONE-implementation-mini"" rel=""nofollow noopener"">mini repo</a>.<br/><NewLine>However, the all branches produce the same results.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Erica_Zheng; <NewLine> ,"REPLY_DATE 1: August 18, 2020,  7:57pm; <NewLine> REPLY_DATE 2: August 24, 2020,  7:21pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
92694,Multiple DataLoaders with DistributedDataParallel can&rsquo;t find default process group,2020-08-13T14:24:29.735Z,3,81,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m currently trying to run an NLP model using DistributedDataParallel and I’ve been receiving the following error if I use more than one worker for DataLoader (this error appears for each worker process):</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""&lt;string&gt;"", line 1 in &lt;module&gt;<NewLine>  File ""/opt/conda/lib/python3.6/multiprocessing/spawn.py"", line 105, in spawn_main<NewLine>    exitcode = _main(df)<NewLine>  File ""/opt/conda/lib/python3.6/multiprocessing/spawn.py"", line 115, in _main<NewLine>    self = reduction.pickle.load(from_parent)<NewLine>  File ""/opt/conda/lib/site-packages/torch/nn/parallel/distributed.py"", line 396, in __setstate__<NewLine>    self.process_group = _get_default_group()<NewLine>  File ""/opt/conda/lib/site-packages/torch/distributed/distributed_c10d.py"", line 286, in _get_default_group<NewLine>    raise RuntimeError(""Default process group has not been initialized, ""<NewLine>Default process group has not been initialized, please make sure to call init_process_group.<NewLine></code></pre><NewLine><p>In main() that I call after torch.multiprocessing.spawn(), I use the following call:</p><NewLine><pre><code class=""lang-auto"">dist.init_process_group(""nccl"", rank=rank, world_size=args.gpu, init_method=""file:///app/tmp/sharedfile"")<NewLine></code></pre><NewLine><p>I don’t receive this error if I set the number of workers to 0. I still receive this error if I change init_method to env:// (and I have the port and address variables set). I would like this to work in file mode though, since I can’t change the size of /dev/shm.</p><NewLine><p>The error itself seems to trigger when I start iterating through dataloader for my epoch (which means I don’t begin a single training loop before the error).</p><NewLine><p>I’m using 4 GPUs on a single node centos docker image with pytorch 1.4.0 and python 3.6.9. Let me know if you need further info, appreciate any tips!</p><NewLine></div>",https://discuss.pytorch.org/u/claracurrier,(Clara Currier),claracurrier,"August 13, 2020,  2:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/claracurrier"">@claracurrier</a>, could you please share a code snippet?</p><NewLine><p>Did you do sth like the following? If so, the default process group is a per-process state, so you will need to call <code>init_process_group</code> in the beginning of the spawn target function (i.e., <code>target_fn</code>), not after spawn in main function.</p><NewLine><pre><code class=""lang-python"">def main():<NewLine>    spawn(target_fn, args=(...))<NewLine>    init_process_group(....)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, here’s more code, I’m following guides from tutorials and the documentation.</p><NewLine><pre><code class=""lang-auto"">def main(rank, args):<NewLine>  dist.init_process_group(...)<NewLine>  # ... load data ...<NewLine>  train_sampler = torch.utils.data.distributed.DistributedSampler(<NewLine>    train_dataset.lengths(), <NewLine>    num_replicas = args.gpus, <NewLine>    rank = rank, <NewLine>    shuffle = True<NewLine>  )<NewLine>  train_loader = torch.utils.data.DataLoader(<NewLine>    train_dataset,<NewLine>    batch_size = args.batch_size,<NewLine>    num_workers = args.data_workers,<NewLine>    collate_fn = mybatchifyfunc,<NewLine>    pin_memory = True,<NewLine>    drop_last = True,<NewLine>    shuffle = False<NewLine>  )<NewLine><NewLine>  # ... machine learning ...<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>  # ... set up logging, parse args...<NewLine>  torch.multiprocessing.spawn(main, args=(args,), nprocs=args.gpus, join=True)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> Sorry to bump, but I’m still not able to figure out the error - I’ve been cross-checking with examples but I’m following them as far as I can see. If the cause of the error is outside of distributed, I’m not able to tell because the error is thrown on spawn.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/claracurrier"">@claracurrier</a></p><NewLine><p>The above code looks correct to me. Is it possible to share a repro so that I can help debug locally?</p><NewLine><p>Regarding the original error message you posted, looks like the program is trying to pass a <code>DistributedDataParallel</code> object through the spawn args, and hence the unpickle triggered the error. What’s in the <code>args=(args,)</code> when you call <code>spawn</code>?</p><NewLine><pre><code class=""lang-auto"">  File ""/opt/conda/lib/python3.6/multiprocessing/spawn.py"", line 115, in _main<NewLine>    self = reduction.pickle.load(from_parent)<NewLine>  File ""/opt/conda/lib/site-packages/torch/nn/parallel/distributed.py"", line 396, in __setstate__<NewLine>    self.process_group = _get_default_group()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the quick reply <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a></p><NewLine><p>For <code>args</code>, it’s a Namespace object from argparse that contains all my ML parameters. It is fairly long, but it only contains ints, floats, str, lists, and bools. The DistributedDataParallel object is not passed.</p><NewLine><p>Unfortunately I’m working on a remote instance that makes copying and pasting difficult so it’ll take a little while to get a minimal reproduction. I’ll post here when I have it.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> in the course of building a barebones repro, I discovered the source of the error: I was passing something unpicklable into my Dataset instance (I was passing my model instead of the args by accident) such that it looked like this:</p><NewLine><pre><code class=""lang-auto"">def main(rank, args):<NewLine>  dist.init_process_group(...)<NewLine>  dummy_train = []<NewLine>  model = dummyModel(args)<NewLine>  model.parallelize(rank)<NewLine><NewLine>  train_dataset = MyNLPDataset(dummy_train, args)  <NewLine>  # replace args with something unpicklable to trigger error<NewLine><NewLine>  train_sampler = torch.utils.data.distributed.DistributedSampler(...)<NewLine>  train_loader = torch.utils.data.DataLoader(train_dataset ...)<NewLine>  <NewLine>  for epoch in range(args.num_epochs):<NewLine>    for training_ex in train_loader:<NewLine>      output = model.update(training_ex)<NewLine></code></pre><NewLine><p>Where the dataset class was:</p><NewLine><pre><code class=""lang-auto"">class MyNLPDataset(torch.utils.data.Dataset):<NewLine>  def __init__(self, examples, args):<NewLine>    self.examples = examples<NewLine>    self.args = args # &lt;-- previously this was saving a copy of the model object<NewLine><NewLine># ... overrided methods ...<NewLine></code></pre><NewLine><p>I think this could’ve been easier if I had a better error message - your comment about something being unpicklable helped me narrow my search. The error ultimately didn’t involve the process group.</p><NewLine><p>I’m now getting a new NCCL backend error that doesn’t affect GLOO, so I’m back to hunting for new issues. Thanks so much for your help!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/claracurrier; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/claracurrier; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/claracurrier; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/claracurrier; <NewLine> ,"REPLY_DATE 1: August 13, 2020,  2:52pm; <NewLine> REPLY_DATE 2: August 13, 2020,  3:17pm; <NewLine> REPLY_DATE 3: August 24, 2020,  2:38pm; <NewLine> REPLY_DATE 4: August 24, 2020,  2:44pm; <NewLine> REPLY_DATE 5: August 24, 2020,  2:56pm; <NewLine> REPLY_DATE 6: August 24, 2020,  6:07pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> 
93306,Average loss in DP and DDP,2020-08-19T02:55:17.558Z,5,183,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi. I have a question regarding data parallel (DP) and distributed data parallel (DDP).</p><NewLine><p>I have read many articles about DP and understand that gradient is reduced automatically. However, I could not find an article explaining whether or not loss is also reduced. For example, I believe that the following codes appear typical main routine of a DP program.</p><NewLine><pre><code class=""lang-auto"">outputs = model(inputs)<NewLine>loss = criterion(outputs, targets)<NewLine>loss.backward()<NewLine></code></pre><NewLine><p>I understand that the split inputs and the model are copied on each GPU and a forward path is concurrently computed to yield the loss, then a backward path is also concurrently computed and finally all gradients are reduced to one.</p><NewLine><p>Is the loss obtained by above code averaged over all the GPUs, which is exactly same as a loss computed by a serial program? Or, is the loss a value from just one GPU (gpu0)? I need to plot a loss chart, so I wonder if the loss is averaged over the GPUs.</p><NewLine><p>The same question applies to outputs. I also need to compute training accuracy using outputs in above code. Does it hold the results of all the GPUs? If so, in what structure of a tensor are they stored?</p><NewLine><p>Regarding DDP, above codes are written in each process running on respective GPU. In this case, how can I access the values on all the GPUs to plot the averaged loss and total accuracy?</p><NewLine><p>I appreciate any sources of information. Thank you in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/TT_YY,(TT YY),TT_YY,"August 20, 2020,  1:06am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""93306"" data-username=""TT_YY""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tt_yy/40/25673_2.png"" width=""20""/> TT_YY:</div><NewLine><blockquote><NewLine><p>However, I could not find an article explaining whether or not loss is also reduced.</p><NewLine></blockquote><NewLine></aside><NewLine><p>No, loss is not reduced because there is only one loss tensor with DP. Besides, the gradients are actually accumulated automatically by the autograd engine. As DP is single-process-multi-thread, all threads share the same autograd engine, and hence ops on different threads will be added to the same autograd graph.</p><NewLine><blockquote><NewLine><p>Is the loss obtained by above code averaged over all the GPUs, which is exactly same as a loss computed by a serial program? Or, is the loss a value from just one GPU (gpu0)? I need to plot a loss chart, so I wonder if the loss is averaged over the GPUs.</p><NewLine></blockquote><NewLine><p>DP’s forward function will gather all outputs to <code>cuda:0</code> (by default) and then return the gathered result. So, in the code above <code>outputs</code> is on one GPU and hence loss is also on one GPU.</p><NewLine><blockquote><NewLine><p>The same question applies to outputs. I also need to compute training accuracy using outputs in above code. Does it hold the results of all the GPUs? If so, in what structure of a tensor are they stored?</p><NewLine></blockquote><NewLine><p>Below is DP’s forward function. The <code>outputs</code> var on line 161 holds the output on different GPUs, but the <code>gather</code> function on line 162 copied them to one GPU.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/fa6b34b54c731938327c8e30e08b287a10b86b0a/torch/nn/parallel/data_parallel.py#L147-L162"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/fa6b34b54c731938327c8e30e08b287a10b86b0a/torch/nn/parallel/data_parallel.py#L147-L162"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/fa6b34b54c731938327c8e30e08b287a10b86b0a/torch/nn/parallel/data_parallel.py#L147-L162</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""147"" style=""counter-reset: li-counter 146 ;""><NewLine><li>def forward(self, *inputs, **kwargs):</li><NewLine><li>    if not self.device_ids:</li><NewLine><li>        return self.module(*inputs, **kwargs)</li><NewLine><li><NewLine></li><li>    for t in chain(self.module.parameters(), self.module.buffers()):</li><NewLine><li>        if t.device != self.src_device_obj:</li><NewLine><li>            raise RuntimeError(""module must have its parameters and buffers ""</li><NewLine><li>                               ""on device {} (device_ids[0]) but found one of ""</li><NewLine><li>                               ""them on device: {}"".format(self.src_device_obj, t.device))</li><NewLine><li><NewLine></li><li>    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)</li><NewLine><li>    if len(self.device_ids) == 1:</li><NewLine><li>        return self.module(*inputs[0], **kwargs[0])</li><NewLine><li>    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])</li><NewLine><li>    outputs = self.parallel_apply(replicas, inputs, kwargs)</li><NewLine><li>    return self.gather(outputs, self.output_device)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>If you want to access individual output on different GPUs, you can do so in the forward function of your model (the one you passed to DP ctor). E.g.,</p><NewLine><pre><code class=""lang-auto"">class MyModel(nn.Module):<NewLine>  def __init__(self):<NewLine>    self.fc = nnLinear(10, 10)<NewLine>  def forward(self, input):<NewLine>    output = self.fc(input)<NewLine>    print(""per-GPU output "", output)<NewLine>    return output<NewLine><NewLine><NewLine>dp = DataParallel(MyModel())<NewLine>outputs = dp(inputs) # this outputs is on one GPU<NewLine></code></pre><NewLine><blockquote><NewLine><p>Regarding DDP, above codes are written in each process running on respective GPU. In this case, how can I access the values on all the GPUs to plot the averaged loss and total accuracy?</p><NewLine></blockquote><NewLine><p>You can use <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.gather"" rel=""nofollow noopener"">gather</a> or <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather"" rel=""nofollow noopener"">all_gather</a> or <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_reduce"" rel=""nofollow noopener"">all_reduce</a> to communicate the loss to one process and print it.</p><NewLine><p>BTW, could you please add a “distributed” tag to distributed training related questions? People working on distributed training monitor that tag and can get back to you promptly.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you Shen Li for your detail explanation. It is very helpful, and now I understand what’s going on in DP and DDP. I modified your codes to see the order of data so that I can make sure that the output is correctly compared to corresponding labels in loss function.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine>device = ""cuda:0""<NewLine><NewLine>class Model(nn.Module):<NewLine><NewLine>    def __init__(self):<NewLine>        super(Model, self).__init__()<NewLine>    <NewLine>    # forward() outputs the input as it is. <NewLine>    def forward(self, input):<NewLine>        output = input<NewLine>        print(""per-GPU output "", output)<NewLine>        return output<NewLine><NewLine>model = Model()<NewLine>model = nn.DataParallel(model)<NewLine>model.to(device)<NewLine><NewLine># input is a sequence of integer in 2D shape.<NewLine>input = torch.arange(20 * 5).reshape(20, 5)<NewLine>input = input.to(device)<NewLine>print(""total input "", input)<NewLine>output = model(input)<NewLine>print(""total output "", output)<NewLine></code></pre><NewLine><p>I was not sure about the “tag” that you pointed out, but I added “distributed” to “Categories”.</p><NewLine><p>I still have a related question about DDP.<br/><NewLine>In my understanding, the gradient is a vector that points a direction where the loss increases the most. I learned from your explanation that we don’t have the “total” loss until we “gather”, “all_gather”, or “all_reduce” the loss computed in each GPU. If we use a loss in each process instead of total loss to compute each gradient and average all the gradients, will it be a correct “total” gradient of the total loss?</p><NewLine><p>In other words, I wonder if it is mathematically correct that averaging all gradients that increase each of respective loss produces a total gradient that increases the averaged loss.</p><NewLine><p>If it is not correct, I think it means that we need to do all_reduce of the loss before we do loss.backward in order to hand total loss information to each process for computing correct gradients. Is my thinking correct?</p><NewLine><p>Thank you again for your kind assistance.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""93306"" data-username=""TT_YY""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tt_yy/40/25673_2.png"" width=""20""/> TT_YY:</div><NewLine><blockquote><NewLine><p>In my understanding, the gradient is a vector that points a direction where the loss increases the most. I learned from your explanation that we don’t have the “total” loss until we “gather”, “all_gather”, or “all_reduce” the loss computed in each GPU. If we use a loss in each process instead of total loss to compute each gradient and average all the gradients, will it be a correct “total” gradient of the total loss?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Good question. Instead of communicating loss, DDP communicates gradients. So the loss is local to every process, but after the backward pass, the gradient is globally averaged, so that all processes will see the same gradient. <a href=""https://pytorch.org/docs/stable/notes/ddp.html"" rel=""nofollow noopener"">This</a> is brief explanation, and <a href=""https://arxiv.org/pdf/2006.15704.pdf"" rel=""nofollow noopener"">this</a> is a full paper describing the algorithm.</p><NewLine><blockquote><NewLine><p>If it is not correct, I think it means that we need to do all_reduce of the loss before we do loss.backward in order to hand total loss information to each process for computing correct gradients. Is my thinking correct?</p><NewLine></blockquote><NewLine><p>The reason we didn’t communicating loss is because that’s not sufficient. When computing gradients, we need both loss and activation, and the activation depends on local inputs. So we need to either communicate loss + activation or gradients. DDP does the later.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you again.</p><NewLine><p>Maybe you have fully answered my question, but I still feel that my point is missing. As I understand, a gradient is computed by the back propagation using the chain rule and first derivative of functions in a model network. Also, as you mentioned, we need the function vales within the network, as well as the loss.</p><NewLine><p>Since the method existed far before the parallelism era, the back-prop naturally started from a single “total” or “global” loss in the single processor platform. Therefore, in that case, we use a loss readily averaged over a batch of input. On the other hand, in the multi-GPU platform, a batch input is farther divided into smaller batches each of which is used to produce a “local” loss by a GPU. In that case, when computing the local gradient, the functions, inputs, and function values are exactly same as the case of the single processor platform. Only difference is using the local loss instead of the global loss.</p><NewLine><p>My question is; does averaging the local gradients computed from the local losses produce exactly the same one as the global gradient computed from the global loss?</p><NewLine><p>If the answer is no, I think that we need to average the local losses to produce a global loss and hand it to all the GPUs to compute correct local gradients that are averaged to produce a correct global gradient. This might be achieved by performing all_reduce() over the local losses <em>before</em> doing loss.backward() on each GPU.</p><NewLine><p>The answer could be yes, but I don’t know the mathematical explanation for it.</p><NewLine><p>That is my point.<br/><NewLine>If I misunderstand something, please point it out. Thank you.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>In that case, when computing the local gradient, the functions, inputs, and function values are exactly same as the case of the single processor platform.</p><NewLine></blockquote><NewLine><p>This is actually not true. Say we have a function <code>f(x) = w * x</code>, where w is the weight. Then when you compute gradient (i.e., <code>dw</code>), you will need both <code>df</code> (from loss, which depends on local input) and <code>x</code> (from local input or intermediate local output, which also depend on local input). So, if not communicating gradients, we need to communicate both the final loss and the intermediate outputs of all layers.</p><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""93306"" data-username=""TT_YY""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tt_yy/40/25673_2.png"" width=""20""/> TT_YY:</div><NewLine><blockquote><NewLine><p>does averaging the local gradients computed from the local losses produce exactly the same one as the global gradient computed from the global loss?</p><NewLine></blockquote><NewLine></aside><NewLine><p>No, this is not guaranteed to be the same, but due to a different reason. If 1) the loss function satisfies the condition <code>loss_fn([x1, x2]) == (loss_fn(x1) + loss_fn(x2)) / 2</code> and 2) batch size on all processes are the same, then average gradients should be correct. Otherwise, average won’t produce the same result. One example would be, if we use <code>.sum()</code> as the loss function, we should just sum instead of averaging the gradient.</p><NewLine><blockquote><NewLine><p>If the answer is no, I think that we need to average the local losses to produce a global loss and hand it to all the GPUs to compute correct local gradients that are averaged to produce a correct global gradient. This might be achieved by performing all_reduce() over the local losses  <em>before</em>  doing loss.backward() on each GPU.</p><NewLine></blockquote><NewLine><p>I might miss sth. If we do the above, it means we compute the gradients using global loss and <strong>local</strong> activation (i.e., global <code>df</code> and local <code>x</code> in the <code>f(x)=w*x</code> example above). In this case, what does this gradient mean?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your further explanation.</p><NewLine><blockquote><NewLine><p>So, if not communicating gradients, we need to communicate both the final loss and the intermediate outputs of all layers.</p><NewLine></blockquote><NewLine><p>Yes, I agree that we must communicate gradients to have a global gradient. My question is about relationship between the global loss and the local gradients, not about communicating losses instead of gradients.</p><NewLine><blockquote><NewLine><p>If 1) the loss function satisfies the condition  <code>loss_fn([x1, x2]) == (loss_fn(x1) + loss_fn(x2)) / 2</code>  and 2) batch size on all processes are the same, then average gradients should be correct.</p><NewLine></blockquote><NewLine><p>I understand that, in a parallel process, the losses are locally averaged on a GPU, and the resulting losses can be globally averaged. That is the reason why the condition you explained must hold to have the “average of average” being equal to the global average.</p><NewLine><p>My point is based on that a parallel process just does the same thing in parallel as a serial process does, and both of them are supposed to produce identical results.</p><NewLine><p>What I am wondering about is that the backward path of the computational graph in a DDP process starts from a local loss, while it starts from a global loss in the serial process, and they are supposed to produce the same result.</p><NewLine><p>From your former explanation, I learned that the backward path starts from the global loss in DP, but not DDP. So, I believe that DP will produce the same results as the serial process does, but I wonder about DDP.</p><NewLine><p>One thing I have come across is that, if the global loss is computed by sum() / batch_size, the backward path might start from 1 and dividing it by batch_size. If this is true, the only difference between starting from the global loss and the local loss should be difference between dividing by the global batch size and the local per-GPU batch size.</p><NewLine><p>So, I suspect that the gradients in those cases have the same direction but different sizes. In particular, the gradient from DDP might be n_gpu times larger than DP, where n_gpu is the number of GPUs. Even if this is true, that will not be a big problem, but DDP may require a different learning rate from DP. I just thought that way, but it needs a confirmation.</p><NewLine><p>Is this correct? I appreciate your assistance. Thank you.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""93306"" data-username=""TT_YY""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tt_yy/40/25673_2.png"" width=""20""/> TT_YY:</div><NewLine><blockquote><NewLine><p>So, I suspect that the gradients in those cases have the same direction but different sizes.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yep, this is true for the <code>sum() / batch_size</code> case you mentioned, on the condition that all processes are using the same batch size. Here is the test to verify that:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/97d594b9f72e7c7baf877f2394d8a5aaeda3140d/test/distributed/test_distributed.py#L2033-L2072"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/97d594b9f72e7c7baf877f2394d8a5aaeda3140d/test/distributed/test_distributed.py#L2033-L2072"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/97d594b9f72e7c7baf877f2394d8a5aaeda3140d/test/distributed/test_distributed.py#L2033-L2072</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""2033"" style=""counter-reset: li-counter 2032 ;""><NewLine><li>def _test_DistributedDataParallel(self, gpu_subset, rank, output_device=None):</li><NewLine><li>    # Run a simple end to end DDP model, use result of single node model</li><NewLine><li>    # as baseline</li><NewLine><li><NewLine></li><li>    # cpu training setup</li><NewLine><li>    model = DDP_NET</li><NewLine><li><NewLine></li><li>    # single gpu training setup</li><NewLine><li>    model_gpu = copy.deepcopy(model)</li><NewLine><li>    model_gpu.cuda(gpu_subset[0])</li><NewLine><li><NewLine></li><li>    # DDP training setup</li><NewLine><li>    model_DDP = copy.deepcopy(model)</li><NewLine><li>    model_DDP.cuda(gpu_subset[0])</li><NewLine><li>    model_DDP = nn.parallel.DistributedDataParallel(</li><NewLine><li>        model_DDP, device_ids=gpu_subset</li><NewLine><li>    )</li><NewLine><li><NewLine></li><li>    # test serializable/unserializable</li><NewLine><li>    with tempfile.NamedTemporaryFile() as tmp:</li><NewLine></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/97d594b9f72e7c7baf877f2394d8a5aaeda3140d/test/distributed/test_distributed.py#L2033-L2072"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><blockquote><NewLine><p>In particular, the gradient from DDP might be n_gpu times larger than DP, where n_gpu is the number of GPUs. Even if this is true, that will not be a big problem, but DDP may require a different learning rate from DP. I just thought that way, but it needs a confirmation.</p><NewLine></blockquote><NewLine><p>DDP computes the average of all gradients from all processes, so the gradient should be the same value as local training for the <code>sum() / batch_size</code> case. What might affect the learning rate is the batch size you configured for each DDP process. If each process is using the same <code>batch_size</code> as local training, it means that in each iteration the DDP gang collective process <code>world_size * batch_size</code> input data, so you might be more confident on the result gradient compared to local training and might need to set the learning rate to a larger value. But this is not guaranteed. See this discussion: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769"">Should we split batch_size according to ngpu_per_node when DistributedDataparallel</a></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you, Shen Li.</p><NewLine><blockquote><NewLine><p>DDP computes the average of all gradients from all processes, so the gradient should be the same value as local training for the  <code>sum() / batch_size</code>  case.</p><NewLine></blockquote><NewLine><p>I interpret it as that the difference is taken care of when computing the global gradient from the local gradients, and we will see no difference from the serial cases.</p><NewLine><blockquote><NewLine><p>What might affect the learning rate is the batch size you configured for each DDP process.</p><NewLine></blockquote><NewLine><p>I think that whether or not we expand the global batch size is a choice between computation speed per iteration and algorithmic efficiency of total convergence, with a larger learning rate that you mentioned. Besides, we can make use of the GPU memories if we choose a large batch size. I feel that a larger batch brings about faster convergence even in the wall clock time bases, if we can efficiently utilize the multiple GPUs. That’s what I’m trying to do.</p><NewLine><p>Thank you very much. I appreciate your time for this long discussion.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/TT_YY; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/TT_YY; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/TT_YY; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/TT_YY; <NewLine> ,"REPLY_DATE 1: August 19, 2020,  2:42pm; <NewLine> REPLY_DATE 2: August 20, 2020,  7:31am; <NewLine> REPLY_DATE 3: August 20, 2020,  2:15pm; <NewLine> REPLY_DATE 4: August 21, 2020,  3:30am; <NewLine> REPLY_DATE 5: August 21, 2020,  3:28am; <NewLine> REPLY_DATE 6: August 22, 2020,  5:31am; <NewLine> REPLY_DATE 7: August 23, 2020,  4:11pm; <NewLine> REPLY_DATE 8: August 24, 2020,  2:27am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> 
93414,Varying iteration time in when using pytorch distributed,2020-08-19T21:54:31.230Z,0,45,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m making some modifications to <a href=""https://github.com/facebookresearch/moco"" rel=""nofollow noopener"">MoCo</a>, which runs pytorch multiprocessing. Running the default code leads to very consistent iteration times (between 0.1s and 0.13s). After my modification (essentially adding some optimizable conditional normalization layers as input to the ResNet) the runtime has become stochastic, a bit more than half the iterations take ~0.5-0.6s (I was expecting this increase), but some take 1.5-2s.</p><NewLine><p>The dist-backend is nccl and I’m using 7 GPUs, although the same issue appears when using the default 8 GPUs.</p><NewLine><p>I’m wondering whether this stochasticity implies some bug in my implementation and, if so, what’s the best way of debugging distributed models.</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/alet,,alet,"August 19, 2020,  9:54pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/alet"">@alet</a>, can you share the implementation of modified model? Especially the “optimizable conditional normalization layers”. Would I be correct if I assume the program is using DDP both before and after the modification?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 19, 2020, 10:22pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
93403,Data Parallelization / nn.DataParallel / multi-processing for general tensor operations on dataset,2020-08-19T18:34:22.724Z,0,63,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Just like pytorch provides option like nn.DataParallel() to efficiently make use of multiple-GPUs, is there any such option for general purpose tensor operations that might not require mutli-GPUs but rather multi-processing ?</p><NewLine><p>Like if i have a function like:</p><NewLine><pre><code class=""lang-auto"">&gt; def generate_dataset(dataloader, val=False):<NewLine>&gt;     f = open('new_dataset.txt', 'w')<NewLine>&gt;         for x,y in tqdm(dataloader, total=len(dataloader)):<NewLine>&gt;             pred = []<NewLine>&gt;             for x_d in x:<NewLine>&gt;                  pred.append(general_tensor_operations(x_d))        #THIS IS'NT NECESSARILY A nn.Module()<NewLine>&gt;             data = format_to_string_data([pred, y.item()])<NewLine>&gt;             f.write(data)<NewLine>&gt;     f.close()<NewLine></code></pre><NewLine><p>Note that,</p><NewLine><ol><NewLine><li>I am making use of a torch dataloader object</li><NewLine><li>the tensor operation on x is NOT performed by a nn.Module() model</li><NewLine><li>the tensor operation doesn’t work on batch data, rather on each slice of the mini-batch</li><NewLine><li>and i write the obtained (pred,y) to a new txt file</li><NewLine></ol><NewLine><p>Now the processing bottle neck occurs at the general_tensor_operations(). So if I am to parallelize such a function (generate_dataset()), how do i do it ? Note that, if i am using multiple processes, then they should use non-overlapping subsets of the original dataloader (otherwise there will be duplicates in the new_dataset.txt). A code snippet shall help a ton.</p><NewLine><p>Thank you in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/Dipayan_Das,(Dipayan Das),Dipayan_Das,"August 19, 2020,  7:01pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If this is on the same machine, will <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html"" rel=""nofollow noopener""><code>torch.multiprocessing.queues.SimpleQueue</code></a> work for you?</p><NewLine><p>If you need across-machine communication, you can use <a href=""https://pytorch.org/docs/master/rpc.html"" rel=""nofollow noopener""><code>torch.distributed.rpc</code></a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 19, 2020,  7:11pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
93271,Comparison Data Parallel Distributed data parallel,2020-08-18T19:37:34.018Z,7,163,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello. I hope you are very well.<br/><NewLine>I am finalizing my experiment with pytorch. When I finish my paper, I hope I can share my paper in here.<br/><NewLine>Anyway, is there any detailed documentation about data parallel(dp) and distributed data parallel(ddp)<br/><NewLine>During my experiment, DP and DDP have big accuracy difference with same dataset, network, learning rate, and loss function. I hope I can put this experiment results in my paper but my professor asks the detailed explanation of why it happens. My dataset is a very unique image dataset and it is not a normal object such as imagenet or city scape stuff, so it can be a very different result than usual computer science paper. In this reason, I look around and read some articles.<br/><NewLine><a class=""onebox"" href=""https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html"" rel=""nofollow noopener"" target=""_blank"">https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html</a><br/><NewLine><a class=""onebox"" href=""https://www.telesens.co/2019/04/04/distributed-data-parallel-training-using-pytorch-on-aws/"" rel=""nofollow noopener"" target=""_blank"">https://www.telesens.co/2019/04/04/distributed-data-parallel-training-using-pytorch-on-aws/</a></p><NewLine><p>However, I am still confused about this two different multi gpu training strategies.</p><NewLine><ol><NewLine><li>What is the “reduce” mean. The “reduce” is the weight update or loss reduction.</li><NewLine><li>What is the major difference between DP and DDP in the weight update strategy? I think this is important.</li><NewLine><li>DDP affects the batch normalization (BN) or DDP still needs the synchronized BN.<br/><NewLine>Thank you for reading my question.</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/henry_Kang,(henry Kang),henry_Kang,"August 18, 2020,  8:51pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There are some comparison between DP and DDP here: <a href=""https://pytorch.org/tutorials/beginner/dist_overview.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/beginner/dist_overview.html</a></p><NewLine><blockquote><NewLine><ol><NewLine><li>What is the “reduce” mean. The “reduce” is the weight update or loss reduction.</li><NewLine></ol><NewLine></blockquote><NewLine><p>What’s the context here? If you mean <code>all_reduce</code>, it is a collective communication operation. DDP uses it to synchronize gradients. see <a href=""https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allreduce"" rel=""nofollow noopener"">https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allreduce</a></p><NewLine><blockquote><NewLine><ol start=""2""><NewLine><li>What is the major difference between DP and DDP in the weight update strategy? I think this is important.</li><NewLine></ol><NewLine></blockquote><NewLine><p>Weight update is done by the optimizer, so if you are using the same optimizer the weight update strategy should be the same. The difference between DP and DDP is how they handle gradients. DP accumulates gradients to the same <code>.grad</code> field, while DDP first use <code>all_reduce</code> to calculate the gradient sum across all processes and divide that by <code>world_size</code> to compute the mean. More details can be found in <a href=""https://arxiv.org/pdf/2006.15704.pdf"" rel=""nofollow noopener"">this paper</a>.</p><NewLine><p>The above difference has impact on how lr should be configured. See this discussion: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769"">Should we split batch_size according to ngpu_per_node when DistributedDataparallel</a></p><NewLine><blockquote><NewLine><ol start=""3""><NewLine><li>DDP affects the batch normalization (BN) or DDP still needs the synchronized BN.<br/><NewLine>Thank you for reading my question.</li><NewLine></ol><NewLine></blockquote><NewLine><p>By default, DDP will broadcast buffers from rank 0 to all other ranks, so yes, it does affect BN.</p><NewLine><p>BTW, for distributed training related questions, could you please add a “distributed” tag to the post? There is a oncall team monitoring that tag.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""93271"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mrshenli/40/12220_2.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>Weight update is done by the optimizer, so if you are using the same optimizer the weight update strategy should be the same. The</p><NewLine></blockquote><NewLine></aside><NewLine><ol><NewLine><li>Well, many people talk about “reduce” but in the context “reduce” does not seem like the literary “reduce”. That is why I ask the question. Because it keeps coming but no one defines this term first when they use it.</li><NewLine><li>what is different between reducing gradients and weight update. Do you mean, DP and DDP exactly update the same weight and same updated each layer right? It is also confusing to me.<br/><NewLine>Do you mean Batch size or LR size?  You link the batch size about it.</li><NewLine><li>I face that there is no improvement when I use the DDP with synchronized BN. That is why I am asking third question.</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>what is different between reducing gradients and weight update.</p><NewLine></blockquote><NewLine><p>There are many weight updating algorithms, e.g., Adam, SGD, Adagrad, etc. (see more <a href=""https://pytorch.org/docs/stable/optim.html"" rel=""nofollow noopener"">here</a>). And they are all independent from DP or DDP. So even if the gradient is the same, different optimizers can update the weight to a different value.</p><NewLine><p>Reducing gradients in DDP basically means communicating gradients across processes.</p><NewLine><blockquote><NewLine><p>Do you mean, DP and DDP exactly update the same weight and same updated each layer right?</p><NewLine></blockquote><NewLine><p>Neither DP nor DDP touches model weight. In the following code, it is the <code>optimzer.step()</code> that updates model weights. What DP and DDP do are preparing the <code>.grad</code> field for all parameters.</p><NewLine><pre><code class=""lang-auto"">output = model(input)<NewLine>output.sum().backward()<NewLine># DP and DDP not involved in the below this point.<NewLine>opt.step()<NewLine></code></pre><NewLine><blockquote><NewLine><p>It is also confusing to me. Do you mean Batch size or LR size? You link the batch size about it.</p><NewLine></blockquote><NewLine><p>Quoting some discussion from that link. If you search for “lr”, you will find almost all comments in that thread discusses how to configure LR and batch size.</p><NewLine><aside class=""quote quote-modified"" data-post=""3"" data-topic=""72769""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/hhxx/40/21482_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769/3"">Should we split batch_size according to ngpu_per_node when DistributedDataparallel</a> <a class=""badge-wrapper bullet"" href=""/c/distributed/12""><span class=""badge-category-bg"" style=""background-color: #0088CC;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""">distributed</span></a><NewLine></div><NewLine><blockquote><NewLine>    Another question is if we do not divide batch-size by 8, the total images processed in one epoch will be the same as usual or eight times? <NewLine>As for learning rate, if we have 8-gpus in total, there wiil be 8 DDP instances. If the batch-size in each DDP distances is 64 (has been divides manually), then one iteration will process 64×4=256 images per node. Taking all gpu into account (2 nodes, 4gpus per node), then one iteration will process 64×8=512 images.  Assuming in one-gpu-one-node scenario, we…<NewLine>  </blockquote><NewLine></aside><NewLine><aside class=""quote quote-modified"" data-post=""6"" data-topic=""72769""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/mrshenli/40/12220_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769/6"">Should we split batch_size according to ngpu_per_node when DistributedDataparallel</a> <a class=""badge-wrapper bullet"" href=""/c/distributed/12""><span class=""badge-category-bg"" style=""background-color: #0088CC;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""">distributed</span></a><NewLine></div><NewLine><blockquote><NewLine>    I agree with all your analysis on the magnitude of the gradients, and I agree that it depends on the loss function. But even with MSE loss fn, it can lead to different conclusions: <NewLine><NewLine>If the fw-bw has processed 8X data, we should set lr to 8X, meaning that the model should take a larger step if it has processed more data as the gradient is more accurate. (IIUC, this is what you advocate for)<NewLine>If the gradient is of the same magnitude, we should use 1X lr, especially when approaching convergence. Ot…<NewLine>  </blockquote><NewLine></aside><NewLine><blockquote><NewLine><p>I face that there is no improvement when I use the DDP with synchronized BN. That is why I am asking third question.</p><NewLine></blockquote><NewLine><p>Right, <code>SyncBatchNorm</code> has its own way for communication, which is out of control of DDP. Using DDP won’t change how <code>SyncBatchNorm</code> behaves.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/f64d24c941a00bc81b3017008ae212cca761d393/torch/nn/modules/_functions.py#L79-L81"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/f64d24c941a00bc81b3017008ae212cca761d393/torch/nn/modules/_functions.py#L79-L81"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/f64d24c941a00bc81b3017008ae212cca761d393/torch/nn/modules/_functions.py#L79-L81</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""79"" style=""counter-reset: li-counter 78 ;""><NewLine><li>torch.distributed.all_reduce(</li><NewLine><li>    combined, torch.distributed.ReduceOp.SUM, process_group, async_op=False)</li><NewLine><li>sum_dy, sum_dy_xmu = torch.split(combined, num_channels)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for detail explanation.<br/><NewLine>Also, DDP and LR relationship are interesting. I used to find the LR with trial and error manner…</p><NewLine><p>I got some understand about reduce and DDP. Please check my understanding.</p><NewLine><p>So Basically DP and DDP do not directly change the weight “but it is a different way to calculate the gradient in multi GPU conditions”. If this is incorrect please let me know.<br/><NewLine>The input data goes through the network, and loss calculate based on output and ground truth.<br/><NewLine>During this loss calculation, DP or DDP work differently.<br/><NewLine>However I thought that gradient is basically calculated from loss.<br/><NewLine>Each loss in the GPU has the different loss result.<br/><NewLine>DP used mean value because DP send every output result to main GPU and calculate the loss.<br/><NewLine>If my understanding is incorrect please point out.<br/><NewLine>However DDP used the different. I still do not get it this parts. In the paper they also use the average value.<br/><NewLine>What is different between mean calculation and syncronized calculation?<br/><NewLine>For update the weight in network, the optimizer updates the network using by gradient value.</p><NewLine><p>The update part is the optimizer part no DP or DDP related with it.<br/><NewLine>So the performance difference might come from LR difference? Because the bath size become different.  weight = previous weight - (gradient*learning_rate)</p><NewLine><p>Really thank you for helping me.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""93271"" data-username=""henry_Kang""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/henry_kang/40/23282_2.png"" width=""20""/> henry_Kang:</div><NewLine><blockquote><NewLine><p>So Basically DP and DDP do not directly change the weight “but it is a different way to calculate the gradient in multi GPU conditions”.</p><NewLine></blockquote><NewLine></aside><NewLine><p>correct.</p><NewLine><blockquote><NewLine><p>The input data goes through the network, and loss calculate based on output and ground truth.<br/><NewLine>During this loss calculation, DP or DDP work differently.</p><NewLine></blockquote><NewLine><p>correct.</p><NewLine><blockquote><NewLine><p>Each loss in the GPU has the different loss result.<br/><NewLine>DP used mean value because DP send every output result to main GPU and calculate the loss.</p><NewLine></blockquote><NewLine><p>This is incorrect. DP’s forward pass 1) create a model replica on every GPU, 2) scatters input to every GPU 3) feed one input shard to a different model replica 4) use one thread per model replica to create output on each GPU 5) gather all outputs from different GPUs to one GPU and return. The loss with DP is calculated based on that gathered output, and hence there is only one loss with DP.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/d06f1818ada6405a30943f58548af958c2b83ff6/torch/nn/parallel/data_parallel.py#L147-L162"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/d06f1818ada6405a30943f58548af958c2b83ff6/torch/nn/parallel/data_parallel.py#L147-L162"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/d06f1818ada6405a30943f58548af958c2b83ff6/torch/nn/parallel/data_parallel.py#L147-L162</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""147"" style=""counter-reset: li-counter 146 ;""><NewLine><li>def forward(self, *inputs, **kwargs):</li><NewLine><li>    if not self.device_ids:</li><NewLine><li>        return self.module(*inputs, **kwargs)</li><NewLine><li><NewLine></li><li>    for t in chain(self.module.parameters(), self.module.buffers()):</li><NewLine><li>        if t.device != self.src_device_obj:</li><NewLine><li>            raise RuntimeError(""module must have its parameters and buffers ""</li><NewLine><li>                               ""on device {} (device_ids[0]) but found one of ""</li><NewLine><li>                               ""them on device: {}"".format(self.src_device_obj, t.device))</li><NewLine><li><NewLine></li><li>    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)</li><NewLine><li>    if len(self.device_ids) == 1:</li><NewLine><li>        return self.module(*inputs[0], **kwargs[0])</li><NewLine><li>    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])</li><NewLine><li>    outputs = self.parallel_apply(replicas, inputs, kwargs)</li><NewLine><li>    return self.gather(outputs, self.output_device)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>DDP is multi-processing parallel, and hence it can scale across multiple machines. In this case, every process has its own loss and so there are multiple different losses. Gradients are synchronized during the backward pass using autograd hook and allreduce. For more details, I recommend reading the paper I linked above.</p><NewLine><blockquote><NewLine><p>What is different between mean calculation and syncronized calculation?</p><NewLine></blockquote><NewLine><p>Because DP is single-process-multi-thread, the scatter, parallel_apply, gather ops used in the forward pass are automatically recorded by the autograd graph. So during the backward pass, the gradients will be accumulated to the <code>.grad</code> feld. There is no grad synchronization in DP, because autograd engine does all grad accumulation already.</p><NewLine><p>As DP is multi-process and every process has its own autograd engine, we need additional code to synchronize grad.</p><NewLine><blockquote><NewLine><p>So the performance difference might come from LR difference?</p><NewLine></blockquote><NewLine><p>Yep, that’s one possible source. It also relates to what loss function you are using. If the loss function cannot guarantee <code>f([x, y]) == (f(x) + f(y)) /2</code>, then the result can also be different, as it is not compatible with gradient averaging used in DDP.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well, what I facing is that, DDP has better result than DP.<br/><NewLine>However many question in here actually said that DP give better result than DDP.<br/><NewLine>Second, our task is class imbalance and binary semantic segmentation. The task is real world image with very complex background. In this cases, DP gives us the 82 % mIoU and DDP achieves the 88% in the same loss function and same learning rate.<br/><NewLine>The Loss function is the IoU Loss.<br/><NewLine>What grad syncronization and accumulation is another new question. I will read your paper first and ask question again. Thank you. It is really difficult but I hope I can make it.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""93271"" data-username=""henry_Kang""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/henry_kang/40/23282_2.png"" width=""20""/> henry_Kang:</div><NewLine><blockquote><NewLine><p>However many question in here actually said that DP give better result than DDP.</p><NewLine></blockquote><NewLine></aside><NewLine><p>No, this is not guaranteed. The only conclusion we can draw is that DP should be able to produce the same result model as non-parallel training, and DDP cannot guarantee this. But regarding which one is better, it needs to quantitatively measured, as it is affected by a lot of factors, e.g. batch size, lr, loss function, etc.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""8"" data-topic=""93271"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mrshenli/40/12220_2.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>it needs to quantitatively measured, as it is affected by a lot of factors, e.g. batch size, lr, loss function, etc.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Ok I see. So It can be really dangerous to say that DDP is better or DP is better. I will just keep it and do not put into my the paper. Anyway I will cite your paper since I am using DDP.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/henry_Kang; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/henry_Kang; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/henry_Kang; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/henry_Kang; <NewLine> ,"REPLY_DATE 1: August 18, 2020,  8:19pm; <NewLine> REPLY_DATE 2: August 18, 2020,  9:10pm; <NewLine> REPLY_DATE 3: August 19, 2020,  4:28am; <NewLine> REPLY_DATE 4: August 18, 2020, 11:03pm; <NewLine> REPLY_DATE 5: August 19, 2020,  4:28am; <NewLine> REPLY_DATE 6: August 19, 2020,  4:41am; <NewLine> REPLY_DATE 7: August 19, 2020,  2:14pm; <NewLine> REPLY_DATE 8: August 19, 2020,  4:51pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> 
78100,Distributed: other ranks not waiting rank_0&rsquo;s evaluation,2020-04-23T17:33:51.395Z,1,93,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!<br/><NewLine>I am using <strong>DistributedDataParallel</strong> on one node with 4 GPUs. Also using <strong>DistributedSampler</strong> for training.</p><NewLine><pre><code class=""lang-python"">self.model = torch.nn.parallel.DistributedDataParallel(<NewLine>    self.model,<NewLine>    device_ids=[self.local_rank],<NewLine>    output_device=self.local_rank,<NewLine>    find_unused_parameters=True<NewLine>)<NewLine></code></pre><NewLine><p>Doing <strong>evaluation after every train epoch only on rank_0</strong>.<br/><NewLine>During evaluation I observed (through <code>nvidia-smi</code>) that other (1, 2, 3) ranks/gpus continue to be processing something with 100% load.<br/><NewLine>My questions:</p><NewLine><ol><NewLine><li>Is it possible that other ranks continuing training next epoch without waiting for rank_0 to finish evaluation?</li><NewLine><li>In case (1) is true, is it ok to leave it like this (will <strong>rank_0</strong> process it’s part of the next epoch after it finishes evaluation)? Or is it better to set a <strong>barrier</strong> so that other ranks will wait for rank_0 to do evaluation?</li><NewLine></ol><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/George_Fedoseev,(George Fedoseev),George_Fedoseev,"April 23, 2020,  5:35pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><ol><NewLine><li>Is it possible that other ranks continuing training next epoch without waiting for rank_0 to finish evaluation?</li><NewLine></ol><NewLine></blockquote><NewLine><p>Yes, it is possible. Because all communication/synchronization happen in the backward pass. So other ranks will proceed with their next forward pass and local backward pass, and then block on the AllReduce operation in the backward pass.</p><NewLine><blockquote><NewLine><ol start=""2""><NewLine><li>In case (1) is true, is it ok to leave it like this (will  <strong>rank_0</strong>  process it’s part of the next epoch after it finishes evaluation)?</li><NewLine></ol><NewLine></blockquote><NewLine><p>Yes, it should be OK to leave it this way. Other ranks is just block waiting on AllReduce until rank_0 finishes evaluations and then runs the subsequent backward pass. It shouldn’t affect the correctness.</p><NewLine><blockquote><NewLine><p>During evaluation I observed (through  <code>nvidia-smi</code> ) that other (1, 2, 3) ranks/gpus continue to be processing something with 100% load.</p><NewLine></blockquote><NewLine><p>Yes, when block waiting for AllReduce, CUDA would show busy, although there might be no real computation running.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>great answer, thanks!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for such a great explanation. Really help me figure out how to do evaluation.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/George_Fedoseev; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ParthTrehan; <NewLine> ,"REPLY_DATE 1: April 24, 2020,  7:33am; <NewLine> REPLY_DATE 2: April 24, 2020,  7:37am; <NewLine> REPLY_DATE 3: August 19, 2020,  4:32pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
93230,Setting visible devices with Distributed Data Parallel,2020-08-18T13:21:05.697Z,1,77,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible make CUDA_VISIBLE_DEVICES and DDP work together?</p><NewLine><p>I am trying to run a script on an 8 GPU server like so:</p><NewLine><pre><code class=""lang-auto"">CUDA_VISIBLE_DEVICES=0,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=7 --use_env main.py<NewLine></code></pre><NewLine><p>but I always run into:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: CUDA error: invalid device ordinal<NewLine></code></pre><NewLine><p>Here is the output of <code>nvidiia-smi</code>:</p><NewLine><pre><code class=""lang-auto"">ue Aug 18 15:21:16 2020<NewLine>+-----------------------------------------------------------------------------+<NewLine>| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |<NewLine>|-------------------------------+----------------------+----------------------+<NewLine>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<NewLine>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<NewLine>|===============================+======================+======================|<NewLine>|   0  GeForce GTX 108...  On   | 00000000:04:00.0 Off |                  N/A |<NewLine>| 20%   13C    P8     7W / 235W |      0MiB / 11178MiB |      0%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   1  GeForce GTX 108...  On   | 00000000:05:00.0 Off |                  N/A |<NewLine>| 23%   18C    P8     8W / 235W |      0MiB / 11178MiB |      0%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   2  GeForce GTX 108...  On   | 00000000:08:00.0 Off |                  N/A |<NewLine>| 23%   20C    P8     8W / 235W |      0MiB / 11178MiB |      0%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   3  GeForce GTX 108...  On   | 00000000:09:00.0 Off |                  N/A |<NewLine>| 23%   23C    P8     8W / 235W |      0MiB / 11178MiB |      0%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   4  GeForce GTX 108...  On   | 00000000:84:00.0 Off |                  N/A |<NewLine>| 23%   18C    P8    11W / 235W |      0MiB / 11178MiB |      0%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   5  GeForce GTX 108...  On   | 00000000:85:00.0 Off |                  N/A |<NewLine>| 20%   16C    P8     7W / 235W |      0MiB / 11178MiB |      0%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   6  GeForce GTX 108...  On   | 00000000:88:00.0 Off |                  N/A |<NewLine>| 20%   15C    P8     7W / 235W |      0MiB / 11178MiB |      0%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   7  GeForce GTX 108...  On   | 00000000:89:00.0 Off |                  N/A |<NewLine>| 23%   25C    P8     7W / 235W |      0MiB / 11178MiB |      0%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine><NewLine>+-----------------------------------------------------------------------------+<NewLine>| Processes:                                                       GPU Memory |<NewLine>|  GPU       PID   Type   Process name                             Usage      |<NewLine>|=============================================================================|<NewLine>|  No running processes found                                                 |<NewLine>+-----------------------------------------------------------------------------+<NewLine><NewLine></code></pre><NewLine><p>What am I missing?</p><NewLine></div>",https://discuss.pytorch.org/u/Diego,(Diego),Diego,"August 18, 2020,  2:25pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/diego"">@Diego</a>, the launching script will launch multiple sub-processes, which might be inherit the  <code>CUDA_VISIBLE_DEVICES</code> value you passed to the command line. A work around would be setting <code>CUDA_VISIBLE_DEVICES</code> in <code>main.py</code> before loading any cuda-related packages. Note that the recommended way to use DDP is one-process-per-device, i.e., each process should exclusively run on one GPU. If you want this, you need to set <code>CUDA_VISIBLE_DEVICES</code> to a different value for each subprocess.</p><NewLine><p>BTW, what’s the default <code>CUDA_VISIBLE_DEVICES</code> value in your machine? I would assume the script should be able to see all devices by default if <code>CUDA_VISIBLE_DEVICES</code> wasn’t set. And when the program throws <code>RuntimeError: CUDA error: invalid device ordinal</code>, do you know which device it tries to access?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>sorry this was a mistake by me. I had set the <code>device_ids</code> variable in the DDP constructor besides using the CUDA_VISIBLE_DEVICES variable, once I removed the former the script runs as expected.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Diego; <NewLine> ,"REPLY_DATE 1: August 18, 2020,  6:14pm; <NewLine> REPLY_DATE 2: August 19, 2020,  8:36am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
93281,"Passing model, dataset, optimizer etc. into DDP",2020-08-18T21:33:59.397Z,1,66,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, currently I have the following train function:</p><NewLine><pre><code class=""lang-python"">def train(model, dataset, sampler, criterion, optimizer, scheduler, cfg):<NewLine>    model = DataParallel(model.cuda())<NewLine><NewLine>    loader = DataLoader(dataset, bs=cfg.BS, num_workers=4, sampler=sampler)<NewLine><NewLine>    for epoch_idx in range(cfg.EPOCHS):<NewLine>        for batch, targets in loader:<NewLine>            preds = model(batch)<NewLine>            loss = criterion(preds, targets)<NewLine>            optimizer.zero_grad()<NewLine>            loss.backward()<NewLine>            optimizer.step()<NewLine>            scheduler.step()<NewLine></code></pre><NewLine><p>How can I convert this code to use <code>DistributeDataParallel</code>?</p><NewLine><p>I looked in the tutorials, but they initialise model, dataset, etc. inside the train function. I can’t do that since I want the signature to remain the same and have the flexibility of defining model, dataset, etc. outside the train function.</p><NewLine><p>Can I just pass all that using arguments in <code>mp.spawn</code>?</p><NewLine></div>",https://discuss.pytorch.org/u/Rizhiy,(Artem Vasenin),Rizhiy,"August 18, 2020,  9:33pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/rizhiy"">@Rizhiy</a></p><NewLine><p>You might not be able to pass the optimizer as that, because every subprocess needs its own dedicated optimizer. And not sure about how dateset/criterion/scheduler in the code behave in multiprocessing use cases. If it is just the model, the following might work.</p><NewLine><pre><code class=""lang-python"">def train(rank, model):<NewLine>    model = DistributedDataParallel(model.to(rank), device_ids=[rank], output_device=rank)<NewLine>    ...<NewLine><NewLine>def main():<NewLine>    model.share_memory() <NewLine>    mp.spawn(<NewLine>        train, <NewLine>        args=(model, dataset, ...),<NewLine>        nprocs=world_size,<NewLine>        join=True)<NewLine><NewLine>if __name__==""__main__"":<NewLine>    main()<NewLine></code></pre><NewLine><p>Rank is the subprocess id, which is provided my <code>mp.spawn</code> as the first argument to the target function.</p><NewLine><p>If the reason for this is to keep the <code>train()</code> signature intact, is it possible to create another <code>wrapper</code> function to wrap <code>train()</code>, configure everything in <code>wrapper</code>, call <code>train</code> in <code>wrapper</code>, and use <code>wrapper</code> as the spawn target?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>, the reason is that this function is part of the internal framework and other programmers should be able to setup the arguments how they require and <code>train()</code> is only responsible for the loop.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I guess my main question is: what can be passed as arguments in <code>mp.spawn()</code>?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>IIUC, it can accept shared memory tensor and anything that’s pickable with Python multiprocessing</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Rizhiy; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Rizhiy; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 18, 2020,  9:58pm; <NewLine> REPLY_DATE 2: August 19, 2020,  1:17am; <NewLine> REPLY_DATE 3: August 19, 2020,  1:48am; <NewLine> REPLY_DATE 4: August 19, 2020,  2:19am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
93198,How to use the system nccl library instead of the internal one of pytorch when using ddp?,2020-08-18T09:33:57.558Z,2,62,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have installed pytorch by using conda and I can directly use nccl backend tro do distributed training.  However, the internal nccl library of pytorch is 2.4.8. If I want use another manually installed nccl library such as 2.7.8 version, how can I do it? Is there any way without compiling pytroch from souce?</p><NewLine></div>",https://discuss.pytorch.org/u/ayl,(Yulong Ao),ayl,"August 18, 2020,  9:34am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ayl"">@ayl</a></p><NewLine><p>You can <code>export USE_SYSTEM_NCCL=1</code>, and then compile PyTorch from source.</p><NewLine><p>See this discussion <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/torch-distributed-not-working-on-two-machines-nccl-backend/87659"">Torch distributed not working on two machines [nccl backend]</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""93198"" data-username=""ayl""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/a/b3f665/40.png"" width=""20""/> ayl:</div><NewLine><blockquote><NewLine><p>Is there any way without compiling pytroch from souce?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thank you. Is there any way without compiling pytorch from source?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think there is an easy/safe way to do so, as the NCCL API also changes from release to release. Even if you can dynamically link libnccl, it might not be compatible with the built libtorch.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ayl; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 18, 2020,  6:16pm; <NewLine> REPLY_DATE 2: August 19, 2020, 12:32am; <NewLine> REPLY_DATE 3: August 19, 2020,  2:17am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
91600,How to use 2gpus to train in single machine (node)?,2020-08-04T11:06:27.474Z,8,146,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, everyone!<br/><NewLine>Recently I want to train a network using 2 gpus on 1 machine(node).But I really get confused cause I can not find one which descrip train ,validiation ,saving checkpoints and load checkpoints concretely?<br/><NewLine>So is there any great example could help?</p><NewLine></div>",https://discuss.pytorch.org/u/Hao_Meng,(Hao Meng),Hao_Meng,"August 4, 2020, 11:06am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here is the overview for the distributed training tools offered by PyTorch: <a href=""https://pytorch.org/tutorials/beginner/dist_overview.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/beginner/dist_overview.html</a></p><NewLine><p>If you are looking for data parallel training, you might want to start from <code>DataParallel</code>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks， I found tutorial w.r.t. DataParallel is much easier to understand and implement.<br/><NewLine>However, using <code>torch.nn.parallel.DistributedDataParallel</code> is much more difficult.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yep, <code>DataParallel</code> indeed is an easier entry point, but is not the most efficient solution. If you are looking for faster training speed or scaling to more machines later, DDP would still be the way to go.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Tanks, I find the example code of DDP on Imagenet is not easy to imitate to fit my code. Is there any more detailed  example?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here are some more general DDP examples/tutorials:</p><NewLine><aside class=""onebox allowlistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""16""/><NewLine><a href=""https://github.com/pytorch/examples/tree/master/distributed/ddp"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars2.githubusercontent.com/u/21003710?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/pytorch/examples/tree/master/distributed/ddp"" rel=""nofollow noopener"" target=""_blank"">pytorch/examples</a></h3><NewLine><p>A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc. - pytorch/examples</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p><a class=""onebox"" href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html</a></p><NewLine><p>There are also several example projects of varying complexity on GitHub that use DistributedDataParallel. They would be a great reference for pytorch code across a variety of domains.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Besides the link <a class=""mention"" href=""/u/osalpekar"">@osalpekar</a> posted above, here is a summary of all DDP docs we have currently: <a href=""https://pytorch.org/tutorials/beginner/dist_overview.html#torch-nn-parallel-distributeddataparallel"" rel=""nofollow noopener"">https://pytorch.org/tutorials/beginner/dist_overview.html#torch-nn-parallel-distributeddataparallel</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot. I will have a try.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks. I will have a try.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Hao_Meng; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Hao_Meng; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/osalpekar; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Hao_Meng; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Hao_Meng; <NewLine> ,"REPLY_DATE 1: August 4, 2020,  2:29pm; <NewLine> REPLY_DATE 2: August 10, 2020,  8:36am; <NewLine> REPLY_DATE 3: August 10, 2020,  2:54pm; <NewLine> REPLY_DATE 4: August 17, 2020,  9:42am; <NewLine> REPLY_DATE 5: August 17, 2020,  9:33pm; <NewLine> REPLY_DATE 6: August 18, 2020,  6:08pm; <NewLine> REPLY_DATE 7: August 18, 2020, 11:09pm; <NewLine> REPLY_DATE 8: August 18, 2020, 11:09pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 2 Likes; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
92967,Loss becomes higher after resuming from DDP model checkpoint,2020-08-16T02:22:38.270Z,0,59,"<div class=""post"" itemprop=""articleBody""><NewLine><h1>Problem</h1><NewLine><p>Recently, I have tried to use DDP. I just followed the tutorial to change my original code for DP.<br/><NewLine>I accidentally stopped my DDP training, so I planned to resume model as what I did before. However, loss becomes much higher after resuming, I think it might be some error in my code.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/d05f13fe78adc5f895c4d02cd3e22ba073165b57"" href=""https://discuss.pytorch.org/uploads/default/original/3X/d/0/d05f13fe78adc5f895c4d02cd3e22ba073165b57.png"" title=""image""><img alt=""image"" data-base62-sha1=""tJl16dyeYFTnBIVxYHT2G12Cu6r"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/0/d05f13fe78adc5f895c4d02cd3e22ba073165b57_2_10x10.png"" height=""190"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/0/d05f13fe78adc5f895c4d02cd3e22ba073165b57_2_690x190.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/0/d05f13fe78adc5f895c4d02cd3e22ba073165b57_2_690x190.png, https://discuss.pytorch.org/uploads/default/optimized/3X/d/0/d05f13fe78adc5f895c4d02cd3e22ba073165b57_2_1035x285.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/d/0/d05f13fe78adc5f895c4d02cd3e22ba073165b57_2_1380x380.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">2167×598 36.4 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/4f9f1e9322b6b8923a6153162dcdb8a16183cc44"" href=""https://discuss.pytorch.org/uploads/default/original/3X/4/f/4f9f1e9322b6b8923a6153162dcdb8a16183cc44.png"" title=""image""><img alt=""image"" data-base62-sha1=""bmmDbjX0uOkMGOxUFF3pc6oEdW4"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/f/4f9f1e9322b6b8923a6153162dcdb8a16183cc44_2_10x10.png"" height=""175"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/f/4f9f1e9322b6b8923a6153162dcdb8a16183cc44_2_690x175.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/f/4f9f1e9322b6b8923a6153162dcdb8a16183cc44_2_690x175.png, https://discuss.pytorch.org/uploads/default/optimized/3X/4/f/4f9f1e9322b6b8923a6153162dcdb8a16183cc44_2_1035x262.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/4/f/4f9f1e9322b6b8923a6153162dcdb8a16183cc44_2_1380x350.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">2447×622 40.1 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><hr/><NewLine><h1>Here are my codes</h1><NewLine><h2>save model</h2><NewLine><pre><code class=""lang-auto""># code for resuming after validation<NewLine>if args.local_rank == 0:<NewLine>    tf_writer.add_scalar('acc/test_top1_best', best_prec1, epoch)<NewLine><NewLine>    output_best = 'Best Prec@1: %.3f\n' % (best_prec1)<NewLine>    print(output_best)<NewLine>    log_training.write(output_best + '\n')<NewLine>    log_training.flush()<NewLine><NewLine>    save_checkpoint({<NewLine>        'epoch': epoch + 1,<NewLine>        'arch': args.arch,<NewLine>        'state_dict': model.state_dict(),<NewLine>        'optimizer': optimizer.state_dict(),<NewLine>        'best_prec1': best_prec1,<NewLine>    }, is_best)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">def save_checkpoint(state, is_best):<NewLine>    filename = '%s/%s/ckpt.pth.tar' % (args.root_model, args.store_name)<NewLine>    torch.save(state, filename)<NewLine>    if is_best:<NewLine>        shutil.copyfile(filename, filename.replace('pth.tar', 'best.pth.tar'))<NewLine></code></pre><NewLine><p>I don’t save scheduler because I manually set the learning rate for the optimizer. And I will set learning rate before training.</p><NewLine><pre><code class=""lang-auto"">adjust_learning_rate(optimizer, epoch, args.lr_type, args.lr_steps)<NewLine></code></pre><NewLine><h2>resume model</h2><NewLine><pre><code class=""lang-auto"">if args.resume:<NewLine>    if os.path.isfile(args.resume):<NewLine>        checkpoint = torch.load(args.resume, map_location=torch.device('cpu'))<NewLine>        pretrained_dict = checkpoint['state_dict']<NewLine>        new_state_dict = OrderedDict()<NewLine>        for k, v in pretrained_dict.items():<NewLine>            if '.total' not in k:<NewLine>                name = k[7:]  # remove 'module.'<NewLine>                # name = name.replace('.net', '')<NewLine>                new_state_dict[name] = v<NewLine>        model.load_state_dict(new_state_dict)<NewLine><NewLine>        model=torch.nn.parallel.DistributedDataParallel(model,<NewLine>                                                device_ids=[local_rank],<NewLine>                                                output_device=local_rank)<NewLine><NewLine>        if 'epoch' in checkpoint.keys():<NewLine>            args.start_epoch = checkpoint['epoch']<NewLine>            best_prec1 = checkpoint['best_prec1']<NewLine>            <NewLine>            # get_optim_policies is a fuction to set optimizer policies<NewLine>            optimizer = torch.optim.SGD(get_optim_policies(model), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)<NewLine><NewLine>            optimizer.load_state_dict(checkpoint['optimizer'])<NewLine>            print((""=&gt; loaded checkpoint '{}' (epoch {})""<NewLine>                .format(args.evaluate, checkpoint['epoch'])))<NewLine>            print((""=&gt; best top1 '{}'"".format(best_prec1)))<NewLine></code></pre><NewLine><p>The above codes work well when I use DP. Are there anything I miss while using DDP?</p><NewLine></div>",https://discuss.pytorch.org/u/111248,(Kunchang Li),111248,"August 16, 2020,  2:22am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>When you call <code>save_checkpoint</code>, is the <code>model</code> var a DDP instance? If yes, you might need to save <code>model.module</code> instead? But I don’t think that’s the reason for the error jump. When you load the module, if you do not use DDP or DP (just a local model), is the loss after recovery as expected?</p><NewLine><p>I might miss sth, looks like in the “resume model” part, the model state is loaded to CPU and not moved to <code>local_rank</code> before passing to DDP ctors? Or is the model already on the correct device before loading state dict?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 18, 2020,  8:08pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
93170,How to balance GPU memories in DDP?,2020-08-18T05:21:35.899Z,0,46,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When training a model with DDP, GPU for rank 0 consumes much higher memory than others.<br/><NewLine>Because of that GPU, I cannot increase batch-size for training.<br/><NewLine>Is there a good way to deal it with?</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/87e30529ae70d263e4680eac8b99629a49d912cb"" href=""https://discuss.pytorch.org/uploads/default/original/3X/8/7/87e30529ae70d263e4680eac8b99629a49d912cb.png"" title=""image""><img alt=""image"" data-base62-sha1=""jo6QvM2B6em3uaenyZyfEAWIJbJ"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/7/87e30529ae70d263e4680eac8b99629a49d912cb_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/7/87e30529ae70d263e4680eac8b99629a49d912cb_2_575x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/7/87e30529ae70d263e4680eac8b99629a49d912cb_2_575x500.png, https://discuss.pytorch.org/uploads/default/original/3X/8/7/87e30529ae70d263e4680eac8b99629a49d912cb.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/8/7/87e30529ae70d263e4680eac8b99629a49d912cb.png 2x"" width=""575""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">794×690 88.9 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/jaehyung.ca,(Jaehyung Choi),jaehyung.ca,"August 18, 2020,  5:21am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/jaehyung.ca"">@jaehyung.ca</a> did you intentionally create any tensor on <code>cuda:0</code> from every process? If not, it might be some lib/code accidentally create states on <code>cuda:0</code>. To avoid this, you can set <code>CUDA_VISIBLE_DEVICES</code> env var to make sure that all processes only sees one GPU.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank for the reply <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a></p><NewLine><p>I haven’t set explicitly device cuda:0 at any point.<br/><NewLine>And even in the <a href=""https://github.com/pytorch/examples/blob/master/distributed/ddp/main.py"" rel=""nofollow noopener"">official DDP example code</a> shows the same unbalanced GPU memory consumption.</p><NewLine><p>I solved the issue by setting torch.cuda.set_device(args.local_rank) which works the same as setting CUDA_VISIBLE_DEVICES.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jaehyung.ca; <NewLine> ,"REPLY_DATE 1: August 18, 2020,  6:26pm; <NewLine> REPLY_DATE 2: August 18, 2020,  7:29pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
92997,All reducing tensors,2020-08-16T13:32:19.338Z,2,110,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Suppose I have a vector of type <code>torch.int32</code>. During all reduce operation, do all 32bits for each coordinate gets transmitted, irrespective of the value (at the coordinate)?</p><NewLine><p>More specifically, I am interested in how we achieve higher speeds in reducing sparse tensors. (By sparse tensors I mean tensors wil large number of zeroes).</p><NewLine></div>",https://discuss.pytorch.org/u/vineeths,,vineeths,"August 17, 2020,  4:22am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We currently support <code>all_reduce</code> on sparse tensors with the Gloo backend (for both CPU and CUDA tensors), but this is not yet supported with the NCCL backend.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Tensorflow <code>all_reduce</code>s sparse tensors (tf.IndexedSlices), by <code>all_gather</code>ing followed by tensor reduction.</p><NewLine><p>Does PyTorch do the same (with Gloo backend) or it does something different under the hood?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s pretty similar - we <code>all_gather</code> the metadata, indices, and values, and then each node does a local sum of the sparse tensors. Here’s the implementation:  <a href=""https://github.com/pytorch/pytorch/blob/65bd38127a34d428915c88507878b4735edf005f/torch/lib/c10d/ProcessGroupGloo.cpp#L939"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/65bd38127a34d428915c88507878b4735edf005f/torch/lib/c10d/ProcessGroupGloo.cpp#L939</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/osalpekar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vineeths; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/osalpekar; <NewLine> ,"REPLY_DATE 1: August 17, 2020, 10:18pm; <NewLine> REPLY_DATE 2: August 18, 2020,  1:39am; <NewLine> REPLY_DATE 3: August 19, 2020, 11:14pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
93063,Is this a correct way to combine mpi and nccl in distributed training,2020-08-17T08:37:11.108Z,0,59,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone. I used the mpi to run multiprocess and use the nccl backend with DDP, Is this a correct way that I use the mpi and nccl? I’d appreciate if anybody can help me! Thanks in advance!<br/><NewLine>here is my sample code:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.distributed as dist<NewLine>from torch.nn.parallel import DistributedDataParallel as DDP<NewLine><NewLine>def dist_train(rank, size):<NewLine>   <NewLine>    local_rank = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])<NewLine>    if args.gpu:<NewLine>        torch.cuda.set_device(local_rank)<NewLine><NewLine>    # set torch device<NewLine>    device = torch.device(""cuda"" if args.gpu and torch.cuda.is_available() else ""cpu"")<NewLine>    model = model.to(device)<NewLine>    model = DDP(model, device_ids=[local_rank])<NewLine><NewLine>    '''training code......'''<NewLine><NewLine><NewLine>def init_process(rank, size, fn, backend='gloo'):<NewLine>    dist.init_process_group(backend, init_method='tcp://master_ip:port', rank=rank, world_size=size)<NewLine><NewLine>    fn(rank, size)<NewLine><NewLine>world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])<NewLine>world_rank = int(os.environ['OMPI_COMM_WORLD_RANK'])<NewLine>init_process(world_rank, world_size, dist_train, backend='nccl')<NewLine></code></pre><NewLine><p>My running command is : mpirun -np ${totals} -H ${slots} ${COMMON_MPI_PARAMETERS} python demo.py</p><NewLine></div>",https://discuss.pytorch.org/u/huoge,,huoge,"August 17, 2020,  9:19am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This should be fine, did you see any error?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 18, 2020,  6:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
93175,"In DDP training, how to do gradient synchronization among part of DDP nodes?",2020-08-18T06:41:08.794Z,0,61,"<div class=""post"" itemprop=""articleBody""><NewLine><p>As far as I know, when in DDP(DistributedDataParallel), <code>loss.backward()</code>will synchronize gradient for <strong>all nodes</strong> in the group automatically through <code>Reducer</code>. However, If I do want to synchronize and update model parameters among <strong>part of nodes</strong> in some epochs, how can I manage to do that?</p><NewLine><p>I would appreciate you for any hints or concrete code sample <img alt="":pray:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/pray.png?v=9"" title="":pray:""/></p><NewLine></div>",https://discuss.pytorch.org/u/Asta,,Asta,"August 18, 2020,  8:29am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/asta"">@Asta</a></p><NewLine><p>I see two options:</p><NewLine><p>Option 1: create two DDP instances on each process and construct then using different <code>ProcessGroup</code> instances. One DDP instance can use the global <code>ProcessGroup</code> which will synchronize across all nodes, and another DDP instance can use a different <code>ProcessGroup</code> of a sub-group which is created using the <code>new_group</code> API.</p><NewLine><p>Option 2: Use the <a href=""https://github.com/pytorch/pytorch/issues/39272"" rel=""nofollow noopener"">DDP comm hook</a> [<a href=""https://github.com/pytorch/pytorch/blob/5d608d45cfd5c42b374c9ac6d2f8bb6ddd1c6e7f/torch/nn/parallel/distributed.py#L615"" rel=""nofollow noopener"">code and example</a>]. This is still a prototype feature and might change in the future.</p><NewLine><p>One thing to mention is that, when you do this (sync gradients in subgroup), it will create gradient inconsistency across processes (as some process didn’t participate in some iteration). This would then lead to inconsistency in model replicas on different processes. DDP only broadcasts model in its ctor. To keep all model replicas consistent, it relies on the assumption that all processes see the same gradient in all iterations. So, if you do partial sync, you might also need to manually broadcast model to bring all processes back to sync, otherwise the result might be numerically incorrect.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 18, 2020,  6:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
89625,Slow NCCL gradient synchronization in distributed training,2020-07-17T16:27:33.997Z,3,156,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am training VGG11 over 16 nodes with data parallellism and NCCL backend. However, I found that the training time for one iteration is too long. I breakdown the time spent at IO, forward, backward, and optimization. It turns out that the I/O, forward, and optimization phases have similar time durations when compared with 8 nodes. The major time is increased by the gradient synchronization during the backward phase.</p><NewLine><p>I profile the code. It turns out the NCCL allreduce takes the majority of the time (see figure below, the timeline for 1 iteration over 16 nodes). I think the most of time is spent on the classifier layers. However, the Pytorch NCCL allreduce time of these layers is much longer than the expected original NCCL allreduce performance on the same amount data. In addition, I also measured PyTorch NCCL allreduce performance over the model parameters (see code below). It turns out the classifier layers take 280.682 ms (total size: 471MB). However, if I directly use NCCL allreduce bechmark to report the performance of the same amount of data the time is about 60ms. I wonder if anyone might know the reason.</p><NewLine><p>I am using Pytorch 1.4.<br/><NewLine>[0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ;</p><NewLine><p><img alt=""Screen Shot 2020-07-17 at 11.54.53 AM"" data-base62-sha1=""8Dxpo5oe9nOumY1zJdGdfRNWXOM"" height=""206"" src=""https://discuss.pytorch.org/uploads/default/original/3X/3/c/3c89942da02877606dbc1d1e788700f3c43d6018.png"" width=""264""/></p><NewLine><pre><code class=""lang-auto"">  for param in model.parameters():<NewLine>    event_start[i].record()<NewLine>    dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)<NewLine>    event_end[i].record()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/yzz,,yzz,"July 17, 2020,  4:27pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/yzz"">@yzz</a>, thanks for posting and compare dist.allreduce with NCCL allreduce benchmark.</p><NewLine><p>May I have your complete code for the comparison? also what kind of GPU are you using? and what is the network type (GPUDirect or ethernet and etc)? I can try to repro and see what it is going on here.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sure. The code is attached below. The GPU is NVIDIA V100, and GPUDirect is used.<br/><NewLine>To run the code,</p><NewLine><ol><NewLine><li>a dummy fixed-size sample dataset has to be generated. Sample size is 3 * 224 * 224 * 4 Byte. The attached script can be used to generate this dataset.</li><NewLine></ol><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">#! /bin/bash<NewLine>base=""base.file""<NewLine>dataset_base=""your_dir_path""<NewLine><NewLine>truncate -s 602112 $base<NewLine><NewLine>for class in {0..9}<NewLine>do<NewLine>  dir=""$dataset_base/${class}""<NewLine>  /bin/rm -rf $dir<NewLine>  mkdir -p $dir<NewLine>  echo $dir created<NewLine>  for img_id in {0..1300}<NewLine>  do<NewLine>    fpath=""${dataset_base}/${class}/${img_id}.fake""<NewLine>    cp $base $fpath<NewLine>  done<NewLine>done<NewLine></code></pre><NewLine></blockquote><NewLine><ol start=""2""><NewLine><li><NewLine><p>You have to set master addr and port as env variable, and change the root path in vgg11.py to the created dir path</p><NewLine></li><NewLine><li><NewLine><p>we have two files (one vgg, one data_loader).</p><NewLine></li><NewLine><li><NewLine><p>My test case: 1 GPU per node, 16 nodes, 128 samples per GPU</p><NewLine></li><NewLine><li><NewLine><p>python vgg11.py [batch_size] [rank] [rank_sizes]</p><NewLine></li><NewLine><li><NewLine><p>The print out  at the end of the output is the allreduce time spent for applying allreduce directly on each parameter.<br/><NewLine>vgg11.py</p><NewLine></li><NewLine></ol><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine>import torch.distributed as dist<NewLine><NewLine>import os<NewLine>import sys<NewLine>import time<NewLine><NewLine>import data_loader<NewLine><NewLine><NewLine>cfg = {<NewLine>    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],<NewLine>    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],<NewLine>    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],<NewLine>    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],<NewLine>}<NewLine><NewLine>class VGG(nn.Module):<NewLine>    def __init__(self, vgg_name):<NewLine>        super(VGG, self).__init__()<NewLine>        self.features = self.large_make_layers(cfg[vgg_name])<NewLine>        self.classifier = nn.Sequential(<NewLine>            nn.Linear(512 * 7 * 7, 4096),<NewLine>            nn.ReLU(),<NewLine>            nn.Dropout(),<NewLine>            nn.Linear(4096, 4096),<NewLine>            nn.ReLU(),<NewLine>            nn.Dropout(),<NewLine>            nn.Linear(4096, 1000),<NewLine>        )<NewLine><NewLine>    def forward(self, x):<NewLine>        out = self.features(x)<NewLine>        out = out.view(out.size(0), -1)<NewLine>        out = self.classifier(out)<NewLine>        return out<NewLine>    def large_make_layers(self, cfg, batch_norm=False):<NewLine>      layers = []<NewLine>      in_channels = 3<NewLine>      for v in cfg:<NewLine>          if v == 'M':<NewLine>              layers += [nn.MaxPool2d(kernel_size=2, stride=2)]<NewLine>          else:<NewLine>              conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)<NewLine>              if batch_norm:<NewLine>                  layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU()]<NewLine>              else:<NewLine>                  layers += [conv2d, nn.ReLU()]<NewLine>              in_channels = v<NewLine>      return nn.Sequential(*layers)<NewLine><NewLine><NewLine>def sync_gradients(model, batch_idx, timer):<NewLine>  """""" Gradient averaging. """"""<NewLine>  global record_event_cnt<NewLine><NewLine>  for param in model.parameters():<NewLine>    print(param.grad.data.shape)<NewLine>    print(""record_event_cnt: %d, batch_idx: %d"" % (record_event_cnt, batch_idx))<NewLine><NewLine>    if batch_idx &gt; 0:<NewLine>      put_timer(record_event_cnt, 1, timer)<NewLine>    dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)<NewLine><NewLine>    if batch_idx &gt; 0:<NewLine>      put_timer(record_event_cnt, 0, timer)<NewLine>      record_event_cnt += 1<NewLine>def cal_single_time(count, timer):<NewLine>  tot_time = 0.0<NewLine>  global para_cnt<NewLine>  for i in range(count):<NewLine>    print(""i: %d"" % (i))<NewLine>    time = timer[i].elapsed_time(timer[i + para_cnt])<NewLine>    print(""%d time: %lf"" % (i, time))<NewLine><NewLine>def put_timer(i, start, timer):<NewLine>  global para_cnt<NewLine>  if i &gt;= 0:<NewLine>    if start == 1:<NewLine>      timer[i].record()<NewLine>      print(""put start for %d "" % (i))<NewLine>    elif start == 0:<NewLine>      #print(""put timer for iteration: "" + str(i))<NewLine>      timer[para_cnt + i].record()<NewLine>      print(""put end for %d"" % (para_cnt + i))<NewLine><NewLine><NewLine><NewLine>N = int(sys.argv[1])<NewLine>rank = int(sys.argv[2])<NewLine>world_size = int(sys.argv[3])<NewLine><NewLine>record_event_cnt = 0<NewLine><NewLine>device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") # PyTorch device<NewLine>dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)<NewLine><NewLine>net = VGG('VGG11')<NewLine>net = net.cuda()<NewLine>#net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[0])<NewLine><NewLine>para_cnt = 22<NewLine>sync_timer=[]<NewLine>for i in range(para_cnt * 2):<NewLine>  sync_timer.append(torch.cuda.Event(enable_timing=True))<NewLine><NewLine><NewLine>start_event = torch.cuda.Event(enable_timing=True)<NewLine>end_event = torch.cuda.Event(enable_timing=True)<NewLine><NewLine>optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)<NewLine>criterion = nn.CrossEntropyLoss()<NewLine><NewLine>#inputs = torch.ones([N, 3, 224, 224], device=device)<NewLine>#labels = torch.empty(N, dtype=torch.long, device=device).random_(1000)<NewLine>root_path='your_dir_path'<NewLine>res_size=224<NewLine>trainset = data_loader.DatasetFolder(root=root_path, loader=data_loader.raw_data_loader, \<NewLine>    img_size=res_size, extensions=data_loader.IMG_EXTENSIONS, transform=None)<NewLine>trainloader = torch.utils.data.DataLoader(trainset, batch_size=N, shuffle=True, num_workers=1, pin_memory=True)<NewLine>torch.cuda.synchronize()<NewLine><NewLine><NewLine>for batch_idx, (inputs, labels) in enumerate(trainloader):<NewLine>  inputs, labels = inputs.to(device), labels.to(device)<NewLine><NewLine>  if batch_idx == 1:<NewLine>    torch.cuda.synchronize()<NewLine>    start = time.time()<NewLine>    start_event.record()<NewLine><NewLine>  out = net(inputs)<NewLine>  loss = criterion(out, labels)<NewLine><NewLine>  loss.backward()<NewLine>  sync_gradients(net, batch_idx, sync_timer)<NewLine>  print(""================"")<NewLine><NewLine>  optimizer.step()<NewLine>  optimizer.zero_grad()<NewLine><NewLine>  if batch_idx == 2:<NewLine>    break<NewLine><NewLine>  record_event_cnt = 0<NewLine><NewLine>end_event.record()<NewLine>torch.cuda.synchronize()<NewLine>end = time.time()<NewLine>print(end-start)<NewLine>print(""iter:%d, %d: %lf, cuda time: %lf""% (batch_idx, N, (end - start), start_event.elapsed_time(end_event)))<NewLine>print(""end record_event_cnt: %d""% (record_event_cnt))<NewLine>cal_single_time(record_event_cnt, sync_timer)<NewLine><NewLine></code></pre><NewLine><p>data_loader.py (some codes are borrowed from original torchvision data loader)</p><NewLine><pre><code class=""lang-auto"">from torchvision import datasets, transforms<NewLine>import torch<NewLine>import torchvision<NewLine>import os<NewLine>import os.path<NewLine>import time<NewLine>device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") # PyTorch device<NewLine><NewLine><NewLine>def raw_data_loader(path, size, d):<NewLine>  file_content = torch.from_file(path, dtype=torch.float, size=size)<NewLine>  #file_content = file_content.to(torch.float)<NewLine>  file_content.resize_((3, d, d))<NewLine>  return file_content<NewLine><NewLine><NewLine>IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp', '.fake')<NewLine><NewLine>def has_file_allowed_extension(filename, extensions):<NewLine>    """"""Checks if a file is an allowed extension.<NewLine>    Args:<NewLine>        filename (string): path to a file<NewLine>        extensions (tuple of strings): extensions to consider (lowercase)<NewLine><NewLine>    Returns:<NewLine>        bool: True if the filename ends with one of given extensions<NewLine>    """"""<NewLine>    return filename.lower().endswith(extensions)<NewLine>def make_dataset(directory, class_to_idx, extensions=None, is_valid_file=None):<NewLine>    instances = []<NewLine>    directory = os.path.expanduser(directory)<NewLine>    both_none = extensions is None and is_valid_file is None<NewLine>    both_something = extensions is not None and is_valid_file is not None<NewLine>    if both_none or both_something:<NewLine>        raise ValueError(""Both extensions and is_valid_file cannot be None or not None at the same time"")<NewLine>    if extensions is not None:<NewLine>        def is_valid_file(x):<NewLine>            return has_file_allowed_extension(x, extensions)<NewLine>    for target_class in sorted(class_to_idx.keys()):<NewLine>        class_index = class_to_idx[target_class]<NewLine>        target_dir = os.path.join(directory, target_class)<NewLine>        if not os.path.isdir(target_dir):<NewLine>            continue<NewLine>        for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):<NewLine>            for fname in sorted(fnames):<NewLine>                path = os.path.join(root, fname)<NewLine>                if is_valid_file(path):<NewLine>                    item = path, class_index<NewLine>                    instances.append(item)<NewLine>    return instances<NewLine><NewLine>class DatasetFolder(datasets.VisionDataset):<NewLine>  def __init__(self, root, loader, img_size, extensions=None, transform=None,<NewLine>                 target_transform=None, is_valid_file=None):<NewLine>        super(DatasetFolder, self).__init__(root, transform=transform,<NewLine>                                            target_transform=target_transform)<NewLine>        classes, class_to_idx = self._find_classes(self.root)<NewLine>        samples = make_dataset(self.root, class_to_idx, extensions, is_valid_file)<NewLine>        if len(samples) == 0:<NewLine>            msg = ""Found 0 files in subfolders of: {}\n"".format(self.root)<NewLine>            if extensions is not None:<NewLine>                msg += ""Supported extensions are: {}"".format("","".join(extensions))<NewLine>            raise RuntimeError(msg)<NewLine><NewLine>        self.loader = loader<NewLine>        self.extensions = extensions<NewLine><NewLine>        self.img_size = img_size * img_size * 3<NewLine>        self.img_res = img_size<NewLine><NewLine>        self.classes = classes<NewLine>        self.class_to_idx = class_to_idx<NewLine>        self.samples = samples<NewLine>        self.targets = [s[1] for s in samples]<NewLine>  def _find_classes(self, dir):<NewLine>        """"""<NewLine>        Finds the class folders in a dataset.<NewLine><NewLine>        Args:<NewLine>            dir (string): Root directory path.<NewLine><NewLine>        Returns:<NewLine>            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.<NewLine><NewLine>        Ensures:<NewLine>            No class is a subdirectory of another.<NewLine>        """"""<NewLine>        classes = [d.name for d in os.scandir(dir) if d.is_dir()]<NewLine>        classes.sort()<NewLine>        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}<NewLine>        return classes, class_to_idx<NewLine><NewLine>  def __getitem__(self, index):<NewLine>        """"""<NewLine>        Args:<NewLine>            index (int): Index<NewLine><NewLine>        Returns:<NewLine>            tuple: (sample, target) where target is class_index of the target class.<NewLine>        """"""<NewLine>        path, target = self.samples[index]<NewLine>        sample = self.loader(path, self.img_size, self.img_res)<NewLine>        if self.transform is not None:<NewLine>            sample = self.transform(sample)<NewLine>        if self.target_transform is not None:<NewLine>            target = self.target_transform(target)<NewLine><NewLine>        return sample, target<NewLine><NewLine><NewLine>  def __len__(self):<NewLine>        return len(self.samples)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If someone face similar issue, the problem may be caused by cudastreamsync() when transfer minibatch generated by dataloader from CPU to GPU. Since the tensor transfer is on the default cuda stream, this forces an addition synchronization in every iteration. The issue can be solved by put the tensor transfer on a separate cuda stream.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/yzz"">@yzz</a>, I am facing a similar issue there, I wonder can you show your code about how you solve this probelm.<br/><NewLine>Thanks</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Yanli_Zhao; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/yzz; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/yzz; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/RZ44; <NewLine> ,"REPLY_DATE 1: July 17, 2020, 10:14pm; <NewLine> REPLY_DATE 2: July 18, 2020,  4:01am; <NewLine> REPLY_DATE 3: July 24, 2020,  5:25pm; <NewLine> REPLY_DATE 4: August 18, 2020, 12:10pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> 
93067,Training fails mid-run when code is changed for distributed training,2020-08-17T08:50:32.874Z,0,91,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I had a few models training with distributed pytorch (DistributedSampler + DistributedDataParallel + multiprocessing). While the models were training for a few days, I changed a part of the data transformation code, where I renamed a file and changed all the necessary imports.</p><NewLine><p>After I changed this part of the code, the models that were training all suddenly crashed when initializing the next epoch. They all crashed with error messages along the lines of “No module named __”.</p><NewLine><p>What’s weird is that this module is only loaded when initializing each process, and the training loop is confined within each spawned process. Thus, I’m not sure why changing the name of this module caused my code to crash mid-training. Is this a common issue in multiprocessing? Am I misunderstanding something here?</p><NewLine><p>PS. In case it helps to know which module it was…<br/><NewLine>The module I changed was a file named <code>transformations.transforms</code>. I changed it to <code>transformations.single_transforms</code> since it seemed to be interfering with <code>torchvision.transforms</code>. As usual, loading transformations only occurs once in the code just before loading the dataset.<br/><NewLine>Also, it wasn’t like the training crashed as soon as I made this change - it crashed after finishing 1 epoch of training which is also weird…</p><NewLine><p>Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/numpee,,numpee,"August 17, 2020,  8:50am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you have some code that we can examine? I put together a very simple program with multiprocessing and tried changing one of the module names during execution, but there was no crash. Are you potentially loading the module within the <code>Dataset</code> or <code>Dataloader</code> classes? Given that the program code is compiled into Python bytecode, and only then interpreted, I’m not sure why changing the Python code mid-execution will affect the program since it’s just the bytcode being interpreted. In fact, even deleting the bytecode during execution shouldn’t make a difference since the program has been loaded into memory. Any thoughts on what could cause this behavior <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>?</p><NewLine><p>As an aside, I would recommend checkpointing (and potentially torchelastic) so you don’t lose training progress for long-running jobs.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>So my entire code base is actually quite large, but here are some necessary details.</p><NewLine><p>As I said, the module import error occurred after changing <code>transformations.transforms</code> to <code>transformations.single_transforms</code>. This module is only directly imported in <code>transformations.__init__</code> where my <code>transform_factory</code> code resides.</p><NewLine><p>The multiprocessing code is structured as follows. As usual, I have some relevant setup in the <code>main</code> function, where I spawn the <code>main_process</code> function.</p><NewLine><p>In the <code>main_process</code> function, the code is structured as:</p><NewLine><ol><NewLine><li>Init process group</li><NewLine><li>Obtain transformations (through <code>transform_factory</code> - probably the module in question)</li><NewLine><li>Create Datasets, Distributed samplers, and Dataloaders</li><NewLine><li>Create distributed model, loss fns, optimizers, etc.</li><NewLine><li>Initialize the trainer class</li><NewLine><li>Run training</li><NewLine></ol><NewLine><p>The last step - <em>run training</em> - is basically just a nested for loop where I run one round of training followed by one round of evaluation. Simplified example:</p><NewLine><pre><code class=""lang-auto"">    def run(self):<NewLine>        self.model.train()<NewLine>        for epoch in range(0, self.num_epochs):<NewLine>            if self.train_sampler is not None:<NewLine>                self.train_sampler.set_epoch(epoch)<NewLine>            for phase in ['train', 'val']:<NewLine>                if phase == 'train':<NewLine>                    self.model.train()<NewLine>                    train_results = self.train_one_epoch(epoch)<NewLine>                    print(train_results)<NewLine>                else:<NewLine>                    self.model.eval()<NewLine>                    val_results = self.validate(epoch)<NewLine>                    print(val_results)<NewLine></code></pre><NewLine><p>In no part of the training do I reference or try to import from <code>transformations.single_transforms</code>. And as you said, even if I did, <strong>it shouldn’t matter because of the  python Bytecode</strong>.</p><NewLine><p>Some other details that may help…</p><NewLine><ol><NewLine><li>I’ve been using PyTorch for around 3 years now, mostly using <code>DataParallel</code>, and I’ve never encountered this issue before. I only switched to Distributed training a few weeks ago, and it’s my first time seeing this problem.</li><NewLine><li>The training doesn’t bug out as soon as the change is made. In fact, it will finish its current phase of training (until the dataloader is finished iterating), then die during the transition from <code>train</code> --&gt; <code>evaluation</code> or vice versa.</li><NewLine></ol><NewLine><p>Finally, thanks for the suggestion. I do in fact checkpoint every epoch, so I can resume training. I just wanted to get to the bottom of this because I just can’t understand why the code would crash mid-training. I talked to one of my colleagues about this, and he said that he’s experienced something similar in distributed training. He also noted that changing the model structure will also cause the code to crash, but I haven’t checked this for myself.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/osalpekar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/numpee; <NewLine> ,"REPLY_DATE 1: August 18, 2020, 12:53am; <NewLine> REPLY_DATE 2: August 18, 2020,  1:49am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
93052,Distributed Training with Nvidia Apex library is exiting without Error Log,2020-08-17T06:52:43.871Z,2,98,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Team,</p><NewLine><p>As part of distributed training, we are trying out Nvidia Apex library and we took care of <em>Set OMP_NUM_THREADS in torch.distributed.launch</em> issue. We are running standard EN-DE (English to German) NMT example given on this documentation.</p><NewLine><p>We have noticed that without Apex library we can run the distributed training for EN-DE (English to German) NMT example but with Apex library we could not and surprising we haven’t had any error log there. Training starts and ends in a few seconds. NCCL debug level logs we are capturing but could not see any error trace there.</p><NewLine><p>On the master node, we are getting the following logs</p><NewLine><pre><code class=""lang-auto"">2020-08-12 13:52:16 | INFO | fairseq.distributed_utils | distributed init (rank 1): env://<NewLine>2020-08-12 13:52:16 | INFO | fairseq.distributed_utils | distributed init (rank 6): env://<NewLine>2020-08-12 13:52:16 | INFO | fairseq.distributed_utils | distributed init (rank 3): env://<NewLine>2020-08-12 13:52:16 | INFO | fairseq.distributed_utils | distributed init (rank 5): env://<NewLine>2020-08-12 13:52:16 | INFO | fairseq.distributed_utils | distributed init (rank 0): env://<NewLine>2020-08-12 13:52:16 | INFO | fairseq.distributed_utils | distributed init (rank 7): env://<NewLine>2020-08-12 13:52:16 | INFO | fairseq.distributed_utils | distributed init (rank 4): env://<NewLine>2020-08-12 13:52:16 | INFO | fairseq.distributed_utils | initialized host 10-7-6-170.cactuslabs.io as rank 7<NewLine>2020-08-12 13:52:16 | INFO | fairseq.distributed_utils | distributed init (rank 2): env://<NewLine>2020-08-12 13:52:16 | INFO | fairseq.distributed_utils | initialized host 10-7-6-170.cactuslabs.io as rank 4<NewLine>2020-08-12 13:52:16 | INFO | fairseq.distributed_utils | initialized host 10-7-6-170.cactuslabs.io as rank 2<NewLine>2020-08-12 13:52:17 | INFO | fairseq.distributed_utils | initialized host 10-7-6-170.cactuslabs.io as rank 1<NewLine>2020-08-12 13:52:17 | INFO | fairseq.distributed_utils | initialized host 10-7-6-170.cactuslabs.io as rank 6<NewLine>2020-08-12 13:52:17 | INFO | fairseq.distributed_utils | initialized host 10-7-6-170.cactuslabs.io as rank 3<NewLine>2020-08-12 13:52:17 | INFO | fairseq.distributed_utils | initialized host 10-7-6-170.cactuslabs.io as rank 5<NewLine>2020-08-12 13:52:20 | INFO | fairseq.distributed_utils | initialized host 10-7-6-170.cactuslabs.io as rank 0<NewLine>10-7-6-170:2171:2171 [0] NCCL INFO Bootstrap : Using [0]ens3:10.7.6.170&lt;0&gt;<NewLine>10-7-6-170:2171:2171 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>10-7-6-170:2171:2171 [0] NCCL INFO NET/IB : No device found.<NewLine>10-7-6-170:2171:2171 [0] NCCL INFO NET/Socket : Using [0]ens3:10.7.6.170&lt;0&gt;<NewLine>NCCL version 2.4.8+cuda9.2<NewLine>10-7-6-170:2178:2178 [7] NCCL INFO Bootstrap : Using [0]ens3:10.7.6.170&lt;0&gt;<NewLine>10-7-6-170:2175:2175 [4] NCCL INFO Bootstrap : Using [0]ens3:10.7.6.170&lt;0&gt;<NewLine>10-7-6-170:2173:2173 [2] NCCL INFO Bootstrap : Using [0]ens3:10.7.6.170&lt;0&gt;<NewLine>10-7-6-170:2172:2172 [1] NCCL INFO Bootstrap : Using [0]ens3:10.7.6.170&lt;0&gt;<NewLine>10-7-6-170:2174:2174 [3] NCCL INFO Bootstrap : Using [0]ens3:10.7.6.170&lt;0&gt;<NewLine>10-7-6-170:2177:2177 [6] NCCL INFO Bootstrap : Using [0]ens3:10.7.6.170&lt;0&gt;<NewLine>10-7-6-170:2176:2176 [5] NCCL INFO Bootstrap : Using [0]ens3:10.7.6.170&lt;0&gt;<NewLine>10-7-6-170:2178:2178 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>10-7-6-170:2175:2175 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>10-7-6-170:2173:2173 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>10-7-6-170:2172:2172 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>10-7-6-170:2174:2174 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>10-7-6-170:2177:2177 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>10-7-6-170:2176:2176 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>10-7-6-170:2172:2172 [1] NCCL INFO NET/IB : No device found.<NewLine>10-7-6-170:2174:2174 [3] NCCL INFO NET/IB : No device found.<NewLine>10-7-6-170:2173:2173 [2] NCCL INFO NET/IB : No device found.<NewLine>10-7-6-170:2176:2176 [5] NCCL INFO NET/IB : No device found.<NewLine>10-7-6-170:2178:2178 [7] NCCL INFO NET/IB : No device found.<NewLine>10-7-6-170:2175:2175 [4] NCCL INFO NET/IB : No device found.<NewLine>10-7-6-170:2177:2177 [6] NCCL INFO NET/IB : No device found.<NewLine>10-7-6-170:2174:2174 [3] NCCL INFO NET/Socket : Using [0]ens3:10.7.6.170&lt;0&gt;<NewLine>10-7-6-170:2172:2172 [1] NCCL INFO NET/Socket : Using [0]ens3:10.7.6.170&lt;0&gt;<NewLine>10-7-6-170:2173:2173 [2] NCCL INFO NET/Socket : Using [0]ens3:10.7.6.170&lt;0&gt;<NewLine>10-7-6-170:2177:2177 [6] NCCL INFO NET/Socket : Using [0]ens3:10.7.6.170&lt;0&gt;<NewLine>10-7-6-170:2176:2176 [5] NCCL INFO NET/Socket : Using [0]ens3:10.7.6.170&lt;0&gt;<NewLine>10-7-6-170:2178:2178 [7] NCCL INFO NET/Socket : Using [0]ens3:10.7.6.170&lt;0&gt;<NewLine>10-7-6-170:2175:2175 [4] NCCL INFO NET/Socket : Using [0]ens3:10.7.6.170&lt;0&gt;<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff<NewLine>10-7-6-170:2174:2231 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff<NewLine>10-7-6-170:2178:2235 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff<NewLine>10-7-6-170:2173:2233 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff<NewLine>10-7-6-170:2175:2237 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff<NewLine>10-7-6-170:2177:2236 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff<NewLine>10-7-6-170:2172:2232 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff<NewLine>10-7-6-170:2176:2234 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO CUDA Dev 0[0], Socket NIC distance :  PHB<NewLine>10-7-6-170:2172:2232 [1] NCCL INFO CUDA Dev 1[1], Socket NIC distance :  PHB<NewLine>10-7-6-170:2173:2233 [2] NCCL INFO CUDA Dev 2[2], Socket NIC distance :  PHB<NewLine>10-7-6-170:2174:2231 [3] NCCL INFO CUDA Dev 3[3], Socket NIC distance :  PHB<NewLine>10-7-6-170:2175:2237 [4] NCCL INFO CUDA Dev 4[4], Socket NIC distance :  PHB<NewLine>10-7-6-170:2176:2234 [5] NCCL INFO CUDA Dev 5[5], Socket NIC distance :  PHB<NewLine>10-7-6-170:2177:2236 [6] NCCL INFO CUDA Dev 6[6], Socket NIC distance :  PHB<NewLine>10-7-6-170:2178:2235 [7] NCCL INFO CUDA Dev 7[7], Socket NIC distance :  PHB<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO Channel 00 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO Channel 01 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15<NewLine>10-7-6-170:2176:2234 [5] NCCL INFO Ring 00 : 5[5] -&gt; 6[6] via P2P/IPC<NewLine>10-7-6-170:2174:2231 [3] NCCL INFO Ring 00 : 3[3] -&gt; 4[4] via P2P/IPC<NewLine>10-7-6-170:2177:2236 [6] NCCL INFO Ring 00 : 6[6] -&gt; 7[7] via P2P/IPC<NewLine>10-7-6-170:2175:2237 [4] NCCL INFO Ring 00 : 4[4] -&gt; 5[5] via P2P/IPC<NewLine>10-7-6-170:2172:2232 [1] NCCL INFO Ring 00 : 1[1] -&gt; 2[2] via P2P/IPC<NewLine>10-7-6-170:2173:2233 [2] NCCL INFO Ring 00 : 2[2] -&gt; 3[3] via P2P/IPC<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO Ring 00 : 15 -&gt; 0 [receive] via NET/Socket/0<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO Ring 00 : 0[0] -&gt; 1[1] via P2P/IPC<NewLine>10-7-6-170:2178:2235 [7] NCCL INFO Ring 00 : 7 -&gt; 8 [send] via NET/Socket/0<NewLine>10-7-6-170:2176:2234 [5] NCCL INFO Ring 00 : 5[5] -&gt; 4[4] via P2P/IPC<NewLine>10-7-6-170:2174:2231 [3] NCCL INFO Ring 00 : 3[3] -&gt; 2[2] via P2P/IPC<NewLine>10-7-6-170:2177:2236 [6] NCCL INFO Ring 00 : 6[6] -&gt; 5[5] via P2P/IPC<NewLine>10-7-6-170:2175:2237 [4] NCCL INFO Ring 00 : 4[4] -&gt; 3[3] via P2P/IPC<NewLine>10-7-6-170:2172:2232 [1] NCCL INFO Ring 00 : 1[1] -&gt; 0[0] via P2P/IPC<NewLine>10-7-6-170:2178:2235 [7] NCCL INFO Ring 00 : 7[7] -&gt; 6[6] via P2P/IPC<NewLine>10-7-6-170:2173:2233 [2] NCCL INFO Ring 00 : 2[2] -&gt; 1[1] via P2P/IPC<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO Ring 00 : 8 -&gt; 0 [receive] via NET/Socket/0<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread<NewLine>10-7-6-170:2177:2236 [6] NCCL INFO Ring 01 : 6[6] -&gt; 7[7] via P2P/IPC<NewLine>10-7-6-170:2176:2234 [5] NCCL INFO Ring 01 : 5[5] -&gt; 6[6] via P2P/IPC<NewLine>10-7-6-170:2174:2231 [3] NCCL INFO Ring 01 : 3[3] -&gt; 4[4] via P2P/IPC<NewLine>10-7-6-170:2175:2237 [4] NCCL INFO Ring 01 : 4[4] -&gt; 5[5] via P2P/IPC<NewLine>10-7-6-170:2172:2232 [1] NCCL INFO Ring 01 : 1[1] -&gt; 2[2] via P2P/IPC<NewLine>10-7-6-170:2173:2233 [2] NCCL INFO Ring 01 : 2[2] -&gt; 3[3] via P2P/IPC<NewLine>10-7-6-170:2178:2235 [7] NCCL INFO Ring 01 : 7 -&gt; 8 [send] via NET/Socket/0<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO Ring 00 : 0 -&gt; 8 [send] via NET/Socket/0<NewLine>10-7-6-170:2177:2236 [6] NCCL INFO Ring 01 : 6[6] -&gt; 5[5] via P2P/IPC<NewLine>10-7-6-170:2176:2234 [5] NCCL INFO Ring 01 : 5[5] -&gt; 4[4] via P2P/IPC<NewLine>10-7-6-170:2174:2231 [3] NCCL INFO Ring 01 : 3[3] -&gt; 2[2] via P2P/IPC<NewLine>10-7-6-170:2175:2237 [4] NCCL INFO Ring 01 : 4[4] -&gt; 3[3] via P2P/IPC<NewLine>10-7-6-170:2173:2233 [2] NCCL INFO Ring 01 : 2[2] -&gt; 1[1] via P2P/IPC<NewLine>10-7-6-170:2176:2234 [5] NCCL INFO Trees [0] 4-&gt;5-&gt;6/-1/-1 [1] 4-&gt;5-&gt;6/-1/-1<NewLine>10-7-6-170:2174:2231 [3] NCCL INFO Trees [0] 2-&gt;3-&gt;4/-1/-1 [1] 2-&gt;3-&gt;4/-1/-1<NewLine>10-7-6-170:2175:2237 [4] NCCL INFO Trees [0] 3-&gt;4-&gt;5/-1/-1 [1] 3-&gt;4-&gt;5/-1/-1<NewLine>10-7-6-170:2176:2234 [5] NCCL INFO comm 0x7fdcf4002540 rank 5 nranks 16 cudaDev 5 nvmlDev 5 - Init COMPLETE<NewLine>10-7-6-170:2174:2231 [3] NCCL INFO comm 0x7f8280002540 rank 3 nranks 16 cudaDev 3 nvmlDev 3 - Init COMPLETE<NewLine>10-7-6-170:2175:2237 [4] NCCL INFO comm 0x7f432c002540 rank 4 nranks 16 cudaDev 4 nvmlDev 4 - Init COMPLETE<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO Ring 01 : 15 -&gt; 0 [receive] via NET/Socket/0<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO Ring 01 : 0[0] -&gt; 1[1] via P2P/IPC<NewLine>10-7-6-170:2172:2232 [1] NCCL INFO Ring 01 : 1[1] -&gt; 0[0] via P2P/IPC<NewLine>10-7-6-170:2178:2235 [7] NCCL INFO Ring 01 : 7[7] -&gt; 6[6] via P2P/IPC<NewLine>10-7-6-170:2177:2236 [6] NCCL INFO Trees [0] 5-&gt;6-&gt;7/-1/-1 [1] 5-&gt;6-&gt;7/-1/-1<NewLine>10-7-6-170:2173:2233 [2] NCCL INFO Trees [0] 1-&gt;2-&gt;3/-1/-1 [1] 1-&gt;2-&gt;3/-1/-1<NewLine>10-7-6-170:2178:2235 [7] NCCL INFO Trees [0] 6-&gt;7-&gt;-1/-1/-1 [1] 6-&gt;7-&gt;-1/-1/-1<NewLine>10-7-6-170:2172:2232 [1] NCCL INFO Trees [0] 0-&gt;1-&gt;2/-1/-1 [1] 0-&gt;1-&gt;2/-1/-1<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO Ring 01 : 0 -&gt; 8 [send] via NET/Socket/0<NewLine>10-7-6-170:2177:2236 [6] NCCL INFO comm 0x7f166c002540 rank 6 nranks 16 cudaDev 6 nvmlDev 6 - Init COMPLETE<NewLine>10-7-6-170:2173:2233 [2] NCCL INFO comm 0x7f934c002540 rank 2 nranks 16 cudaDev 2 nvmlDev 2 - Init COMPLETE<NewLine>10-7-6-170:2178:2235 [7] NCCL INFO comm 0x7f7abc002540 rank 7 nranks 16 cudaDev 7 nvmlDev 7 - Init COMPLETE<NewLine>10-7-6-170:2172:2232 [1] NCCL INFO comm 0x7f9d88002540 rank 1 nranks 16 cudaDev 1 nvmlDev 1 - Init COMPLETE<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO Ring 01 : 8 -&gt; 0 [receive] via NET/Socket/0<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO Trees [0] -1-&gt;0-&gt;1/8/-1 [1] 8-&gt;0-&gt;1/-1/-1<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO Using 128 threads, Min Comp Cap 3, Trees enabled up to size 469999<NewLine>10-7-6-170:2171:2230 [0] NCCL INFO comm 0x7fd6d8002540 rank 0 nranks 16 cudaDev 0 nvmlDev 0 - Init COMPLETE<NewLine>10-7-6-170:2171:2171 [0] NCCL INFO Launch mode Parallel<NewLine>2020-08-12 13:52:23 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='env://', distributed_no_spawn=True, distributed_port=-1, distributed_rank=0, distributed_world_size=16, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=8000, max_tokens_valid=8000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, nprocs_per_node=8, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0)<NewLine>2020-08-12 13:52:23 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types<NewLine>2020-08-12 13:52:23 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types<NewLine>2020-08-12 13:52:23 | INFO | fairseq.data.data_utils | loaded 7283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.de<NewLine>2020-08-12 13:52:23 | INFO | fairseq.data.data_utils | loaded 7283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.en<NewLine>2020-08-12 13:52:23 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples<NewLine>2020-08-12 13:52:24 | INFO | fairseq_cli.train | model transformer_iwslt_de_en, criterion LabelSmoothedCrossEntropyCriterion<NewLine>2020-08-12 13:52:24 | INFO | fairseq_cli.train | num. model params: 42864640 (num. trained: 42864640)<NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | ***********************CUDA enviroments for all 16 workers***********************<NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | rank   0: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               <NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | rank   1: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               <NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | rank   2: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               <NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | rank   3: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               <NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | rank   4: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               <NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | rank   5: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               <NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | rank   6: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               <NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | rank   7: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               <NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | rank   8: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               <NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | rank   9: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               <NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | rank  10: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               <NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | rank  11: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               <NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | rank  12: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               <NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | rank  13: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               <NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | rank  14: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               <NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | rank  15: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               <NewLine>2020-08-12 13:52:24 | INFO | fairseq.utils | ***********************CUDA enviroments for all 16 workers***********************<NewLine>2020-08-12 13:52:24 | INFO | fairseq_cli.train | training on 16 devices (GPUs/TPUs)<NewLine>2020-08-12 13:52:24 | INFO | fairseq_cli.train | max tokens per GPU = 8000 and max sentences per GPU = None<NewLine>2020-08-12 13:52:24 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt<NewLine>2020-08-12 13:52:24 | INFO | fairseq.trainer | loading train data for epoch 1<NewLine>2020-08-12 13:52:24 | INFO | fairseq.data.data_utils | loaded 160239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.de<NewLine>2020-08-12 13:52:24 | INFO | fairseq.data.data_utils | loaded 160239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.en<NewLine>2020-08-12 13:52:24 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en train de-en 160239 examples<NewLine>2020-08-12 13:52:25 | INFO | fairseq.optim.adam | using FusedAdam<NewLine>2020-08-12 13:52:25 | INFO | fairseq_cli.train | done training in 0.0 seconds<NewLine>*****************************************<NewLine>Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. <NewLine>*****************************************<NewLine><NewLine></code></pre><NewLine><p>On the slave node, we are getting the following logs</p><NewLine><pre><code class=""lang-auto"">2020-08-12 13:52:20 | INFO | fairseq.distributed_utils | distributed init (rank 9): env://<NewLine>2020-08-12 13:52:20 | INFO | fairseq.distributed_utils | initialized host 10-7-6-166.cactuslabs.io as rank 9<NewLine>2020-08-12 13:52:20 | INFO | fairseq.distributed_utils | distributed init (rank 15): env://<NewLine>2020-08-12 13:52:20 | INFO | fairseq.distributed_utils | initialized host 10-7-6-166.cactuslabs.io as rank 15<NewLine>2020-08-12 13:52:20 | INFO | fairseq.distributed_utils | distributed init (rank 14): env://<NewLine>2020-08-12 13:52:20 | INFO | fairseq.distributed_utils | distributed init (rank 8): env://<NewLine>2020-08-12 13:52:20 | INFO | fairseq.distributed_utils | initialized host 10-7-6-166.cactuslabs.io as rank 14<NewLine>2020-08-12 13:52:20 | INFO | fairseq.distributed_utils | distributed init (rank 13): env://<NewLine>2020-08-12 13:52:20 | INFO | fairseq.distributed_utils | initialized host 10-7-6-166.cactuslabs.io as rank 8<NewLine>2020-08-12 13:52:20 | INFO | fairseq.distributed_utils | distributed init (rank 10): env://<NewLine>2020-08-12 13:52:20 | INFO | fairseq.distributed_utils | initialized host 10-7-6-166.cactuslabs.io as rank 13<NewLine>2020-08-12 13:52:20 | INFO | fairseq.distributed_utils | distributed init (rank 12): env://<NewLine>2020-08-12 13:52:20 | INFO | fairseq.distributed_utils | initialized host 10-7-6-166.cactuslabs.io as rank 10<NewLine>2020-08-12 13:52:20 | INFO | fairseq.distributed_utils | distributed init (rank 11): env://<NewLine>2020-08-12 13:52:20 | INFO | fairseq.distributed_utils | initialized host 10-7-6-166.cactuslabs.io as rank 12<NewLine>2020-08-12 13:52:20 | INFO | fairseq.distributed_utils | initialized host 10-7-6-166.cactuslabs.io as rank 11<NewLine>10-7-6-166:2407:2407 [4] NCCL INFO Bootstrap : Using [0]ens3:10.7.6.166&lt;0&gt;<NewLine>10-7-6-166:2407:2407 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>10-7-6-166:2407:2407 [4] NCCL INFO NET/IB : No device found.<NewLine>10-7-6-166:2407:2407 [4] NCCL INFO NET/Socket : Using [0]ens3:10.7.6.166&lt;0&gt;<NewLine>10-7-6-166:2404:2404 [1] NCCL INFO Bootstrap : Using [0]ens3:10.7.6.166&lt;0&gt;<NewLine>10-7-6-166:2404:2404 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>10-7-6-166:2409:2409 [6] NCCL INFO Bootstrap : Using [0]ens3:10.7.6.166&lt;0&gt;<NewLine>10-7-6-166:2409:2409 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>10-7-6-166:2409:2409 [6] NCCL INFO NET/IB : No device found.<NewLine>10-7-6-166:2404:2404 [1] NCCL INFO NET/IB : No device found.<NewLine>10-7-6-166:2409:2409 [6] NCCL INFO NET/Socket : Using [0]ens3:10.7.6.166&lt;0&gt;<NewLine>10-7-6-166:2404:2404 [1] NCCL INFO NET/Socket : Using [0]ens3:10.7.6.166&lt;0&gt;<NewLine>10-7-6-166:2410:2410 [7] NCCL INFO Bootstrap : Using [0]ens3:10.7.6.166&lt;0&gt;<NewLine>10-7-6-166:2410:2410 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>10-7-6-166:2410:2410 [7] NCCL INFO NET/IB : No device found.<NewLine>10-7-6-166:2410:2410 [7] NCCL INFO NET/Socket : Using [0]ens3:10.7.6.166&lt;0&gt;<NewLine>10-7-6-166:2406:2406 [3] NCCL INFO Bootstrap : Using [0]ens3:10.7.6.166&lt;0&gt;<NewLine>10-7-6-166:2406:2406 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>10-7-6-166:2406:2406 [3] NCCL INFO NET/IB : No device found.<NewLine>10-7-6-166:2406:2406 [3] NCCL INFO NET/Socket : Using [0]ens3:10.7.6.166&lt;0&gt;<NewLine>10-7-6-166:2405:2405 [2] NCCL INFO Bootstrap : Using [0]ens3:10.7.6.166&lt;0&gt;<NewLine>10-7-6-166:2405:2405 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>10-7-6-166:2405:2405 [2] NCCL INFO NET/IB : No device found.<NewLine>10-7-6-166:2405:2405 [2] NCCL INFO NET/Socket : Using [0]ens3:10.7.6.166&lt;0&gt;<NewLine>10-7-6-166:2408:2408 [5] NCCL INFO Bootstrap : Using [0]ens3:10.7.6.166&lt;0&gt;<NewLine>10-7-6-166:2408:2408 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>10-7-6-166:2408:2408 [5] NCCL INFO NET/IB : No device found.<NewLine>10-7-6-166:2408:2408 [5] NCCL INFO NET/Socket : Using [0]ens3:10.7.6.166&lt;0&gt;<NewLine>10-7-6-166:2403:2403 [0] NCCL INFO Bootstrap : Using [0]ens3:10.7.6.166&lt;0&gt;<NewLine>10-7-6-166:2403:2403 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>10-7-6-166:2403:2403 [0] NCCL INFO NET/IB : No device found.<NewLine>10-7-6-166:2403:2403 [0] NCCL INFO NET/Socket : Using [0]ens3:10.7.6.166&lt;0&gt;<NewLine>10-7-6-166:2404:2463 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff<NewLine>10-7-6-166:2410:2465 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff<NewLine>10-7-6-166:2407:2462 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff<NewLine>10-7-6-166:2409:2464 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff<NewLine>10-7-6-166:2406:2466 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff<NewLine>10-7-6-166:2405:2467 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff<NewLine>10-7-6-166:2408:2468 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff<NewLine>10-7-6-166:2403:2469 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff<NewLine>10-7-6-166:2410:2465 [7] NCCL INFO CUDA Dev 7[7], Socket NIC distance :  PHB<NewLine>10-7-6-166:2403:2469 [0] NCCL INFO CUDA Dev 0[0], Socket NIC distance :  PHB<NewLine>10-7-6-166:2404:2463 [1] NCCL INFO CUDA Dev 1[1], Socket NIC distance :  PHB<NewLine>10-7-6-166:2405:2467 [2] NCCL INFO CUDA Dev 2[2], Socket NIC distance :  PHB<NewLine>10-7-6-166:2408:2468 [5] NCCL INFO CUDA Dev 5[5], Socket NIC distance :  PHB<NewLine>10-7-6-166:2406:2466 [3] NCCL INFO CUDA Dev 3[3], Socket NIC distance :  PHB<NewLine>10-7-6-166:2407:2462 [4] NCCL INFO CUDA Dev 4[4], Socket NIC distance :  PHB<NewLine>10-7-6-166:2409:2464 [6] NCCL INFO CUDA Dev 6[6], Socket NIC distance :  PHB<NewLine>10-7-6-166:2408:2468 [5] NCCL INFO Ring 00 : 13[5] -&gt; 14[6] via P2P/IPC<NewLine>10-7-6-166:2407:2462 [4] NCCL INFO Ring 00 : 12[4] -&gt; 13[5] via P2P/IPC<NewLine>10-7-6-166:2406:2466 [3] NCCL INFO Ring 00 : 11[3] -&gt; 12[4] via P2P/IPC<NewLine>10-7-6-166:2405:2467 [2] NCCL INFO Ring 00 : 10[2] -&gt; 11[3] via P2P/IPC<NewLine>10-7-6-166:2409:2464 [6] NCCL INFO Ring 00 : 14[6] -&gt; 15[7] via P2P/IPC<NewLine>10-7-6-166:2404:2463 [1] NCCL INFO Ring 00 : 9[1] -&gt; 10[2] via P2P/IPC<NewLine>10-7-6-166:2403:2469 [0] NCCL INFO Ring 00 : 7 -&gt; 8 [receive] via NET/Socket/0<NewLine>10-7-6-166:2403:2469 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread<NewLine>10-7-6-166:2410:2465 [7] NCCL INFO Ring 00 : 15 -&gt; 0 [send] via NET/Socket/0<NewLine>10-7-6-166:2403:2469 [0] NCCL INFO Ring 00 : 8[0] -&gt; 9[1] via P2P/IPC<NewLine>10-7-6-166:2407:2462 [4] NCCL INFO Ring 00 : 12[4] -&gt; 11[3] via P2P/IPC<NewLine>10-7-6-166:2410:2465 [7] NCCL INFO Ring 00 : 15[7] -&gt; 14[6] via P2P/IPC<NewLine>10-7-6-166:2408:2468 [5] NCCL INFO Ring 00 : 13[5] -&gt; 12[4] via P2P/IPC<NewLine>10-7-6-166:2406:2466 [3] NCCL INFO Ring 00 : 11[3] -&gt; 10[2] via P2P/IPC<NewLine>10-7-6-166:2405:2467 [2] NCCL INFO Ring 00 : 10[2] -&gt; 9[1] via P2P/IPC<NewLine>10-7-6-166:2409:2464 [6] NCCL INFO Ring 00 : 14[6] -&gt; 13[5] via P2P/IPC<NewLine>10-7-6-166:2404:2463 [1] NCCL INFO Ring 00 : 9[1] -&gt; 8[0] via P2P/IPC<NewLine>10-7-6-166:2403:2469 [0] NCCL INFO Ring 00 : 8 -&gt; 0 [send] via NET/Socket/0<NewLine>10-7-6-166:2407:2462 [4] NCCL INFO Ring 01 : 12[4] -&gt; 13[5] via P2P/IPC<NewLine>10-7-6-166:2408:2468 [5] NCCL INFO Ring 01 : 13[5] -&gt; 14[6] via P2P/IPC<NewLine>10-7-6-166:2406:2466 [3] NCCL INFO Ring 01 : 11[3] -&gt; 12[4] via P2P/IPC<NewLine>10-7-6-166:2405:2467 [2] NCCL INFO Ring 01 : 10[2] -&gt; 11[3] via P2P/IPC<NewLine>10-7-6-166:2409:2464 [6] NCCL INFO Ring 01 : 14[6] -&gt; 15[7] via P2P/IPC<NewLine>10-7-6-166:2404:2463 [1] NCCL INFO Ring 01 : 9[1] -&gt; 10[2] via P2P/IPC<NewLine>10-7-6-166:2410:2465 [7] NCCL INFO Ring 01 : 15 -&gt; 0 [send] via NET/Socket/0<NewLine>10-7-6-166:2407:2462 [4] NCCL INFO Ring 01 : 12[4] -&gt; 11[3] via P2P/IPC<NewLine>10-7-6-166:2406:2466 [3] NCCL INFO Ring 01 : 11[3] -&gt; 10[2] via P2P/IPC<NewLine>10-7-6-166:2405:2467 [2] NCCL INFO Ring 01 : 10[2] -&gt; 9[1] via P2P/IPC<NewLine>10-7-6-166:2408:2468 [5] NCCL INFO Ring 01 : 13[5] -&gt; 12[4] via P2P/IPC<NewLine>10-7-6-166:2407:2462 [4] NCCL INFO Trees [0] 11-&gt;12-&gt;13/-1/-1 [1] 11-&gt;12-&gt;13/-1/-1<NewLine>10-7-6-166:2403:2469 [0] NCCL INFO Ring 00 : 0 -&gt; 8 [receive] via NET/Socket/0<NewLine>10-7-6-166:2409:2464 [6] NCCL INFO Ring 01 : 14[6] -&gt; 13[5] via P2P/IPC<NewLine>10-7-6-166:2403:2469 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread<NewLine>10-7-6-166:2406:2466 [3] NCCL INFO Trees [0] 10-&gt;11-&gt;12/-1/-1 [1] 10-&gt;11-&gt;12/-1/-1<NewLine>10-7-6-166:2408:2468 [5] NCCL INFO Trees [0] 12-&gt;13-&gt;14/-1/-1 [1] 12-&gt;13-&gt;14/-1/-1<NewLine>10-7-6-166:2407:2462 [4] NCCL INFO comm 0x7f0ab4002540 rank 12 nranks 16 cudaDev 4 nvmlDev 4 - Init COMPLETE<NewLine>10-7-6-166:2406:2466 [3] NCCL INFO comm 0x7f8e80002540 rank 11 nranks 16 cudaDev 3 nvmlDev 3 - Init COMPLETE<NewLine>10-7-6-166:2408:2468 [5] NCCL INFO comm 0x7f09e8002540 rank 13 nranks 16 cudaDev 5 nvmlDev 5 - Init COMPLETE<NewLine>10-7-6-166:2403:2469 [0] NCCL INFO Ring 01 : 7 -&gt; 8 [receive] via NET/Socket/0<NewLine>10-7-6-166:2403:2469 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread<NewLine>10-7-6-166:2403:2469 [0] NCCL INFO Ring 01 : 8[0] -&gt; 9[1] via P2P/IPC<NewLine>10-7-6-166:2404:2463 [1] NCCL INFO Ring 01 : 9[1] -&gt; 8[0] via P2P/IPC<NewLine>10-7-6-166:2405:2467 [2] NCCL INFO Trees [0] 9-&gt;10-&gt;11/-1/-1 [1] 9-&gt;10-&gt;11/-1/-1<NewLine>10-7-6-166:2410:2465 [7] NCCL INFO Ring 01 : 15[7] -&gt; 14[6] via P2P/IPC<NewLine>10-7-6-166:2409:2464 [6] NCCL INFO Trees [0] 13-&gt;14-&gt;15/-1/-1 [1] 13-&gt;14-&gt;15/-1/-1<NewLine>10-7-6-166:2410:2465 [7] NCCL INFO Trees [0] 14-&gt;15-&gt;-1/-1/-1 [1] 14-&gt;15-&gt;-1/-1/-1<NewLine>10-7-6-166:2405:2467 [2] NCCL INFO comm 0x7fbd7c002540 rank 10 nranks 16 cudaDev 2 nvmlDev 2 - Init COMPLETE<NewLine>10-7-6-166:2409:2464 [6] NCCL INFO comm 0x7f4290002540 rank 14 nranks 16 cudaDev 6 nvmlDev 6 - Init COMPLETE<NewLine>10-7-6-166:2410:2465 [7] NCCL INFO comm 0x7ff674002540 rank 15 nranks 16 cudaDev 7 nvmlDev 7 - Init COMPLETE<NewLine>10-7-6-166:2404:2463 [1] NCCL INFO Trees [0] 8-&gt;9-&gt;10/-1/-1 [1] 8-&gt;9-&gt;10/-1/-1<NewLine>10-7-6-166:2403:2469 [0] NCCL INFO Ring 01 : 0 -&gt; 8 [receive] via NET/Socket/0<NewLine>10-7-6-166:2403:2469 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread<NewLine>10-7-6-166:2404:2463 [1] NCCL INFO comm 0x7fc5b4002540 rank 9 nranks 16 cudaDev 1 nvmlDev 1 - Init COMPLETE<NewLine>10-7-6-166:2403:2469 [0] NCCL INFO Ring 01 : 8 -&gt; 0 [send] via NET/Socket/0<NewLine>10-7-6-166:2403:2469 [0] NCCL INFO Trees [0] 0-&gt;8-&gt;9/-1/-1 [1] -1-&gt;8-&gt;9/0/-1<NewLine>10-7-6-166:2403:2469 [0] NCCL INFO comm 0x7f19d4002540 rank 8 nranks 16 cudaDev 0 nvmlDev 0 - Init COMPLETE<NewLine>*****************************************<NewLine>Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. <NewLine>*****************************************<NewLine></code></pre><NewLine><p>Environment Config</p><NewLine><ul><NewLine><li>2 nodes with 8 GPU (K80)</li><NewLine><li>Fairseq 0.9</li><NewLine><li>PyTorch 1.5</li><NewLine><li>Cuda = 9.2.88</li><NewLine><li>CudaNN = 7.6.4</li><NewLine><li>NCCL = 2.4.8</li><NewLine></ul><NewLine><p>Note: We can able to run training with Apex on a single instance with multiple GPUs. Is there anything we are missing?</p><NewLine><p>Any information and help will be useful.</p><NewLine><p>Thanks in advance</p><NewLine></div>",https://discuss.pytorch.org/u/Maven123456,(Maven123456),Maven123456,"August 17, 2020,  6:53am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Were the training jobs with and without Apex done on the same machines? It seems like GPU-GPU communication is working based on the Ring/Trees logs that you pasted. It might make sense to direct this issue to the Apex GitHub repo since the training is working with vanilla DDP (we’ve also been working on bridging the gap between vanilla DDP and Apex through new features like dynamic bucketing, so the performance difference may not be as much as before).</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you so much <a class=""mention"" href=""/u/osalpekar"">@osalpekar</a> for your replay and for sharing the valuable information.</p><NewLine><blockquote><NewLine><p>Were the training jobs with and without Apex done on the same machines? - No, I have used cloud instances to run the experiment with and without Apex</p><NewLine></blockquote><NewLine><p>I’ll be redirecting this issue to Nvidia Apex GitHub repo and paste the link of that issue here. So you and PyTorch team can refer that.</p><NewLine><p>Once again thank you</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://github.com/NVIDIA/apex/issues/946"" rel=""nofollow noopener"">Here is the link</a> of same issue on Nvidia Apex library</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/osalpekar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Maven123456; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Maven123456; <NewLine> ,"REPLY_DATE 1: August 17, 2020, 10:15pm; <NewLine> REPLY_DATE 2: August 18, 2020,  5:08am; <NewLine> REPLY_DATE 3: August 18, 2020,  5:12am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
92511,MPI Backend with GPU support,2020-08-12T05:45:06.481Z,2,120,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to train my model using 2 nodes in a HPC system. Each node contains 4 Nvidia V100 GPUs. It requires MPI if more than 1 node is used. However, I don’t have enough expertise on MPI. Besides, I have found a paucity of information about Pytorch MPI Backend with GPU support. During the preparation of my model, I intended to train it to a single machine with 8 GPUs. Unfortunately, I don’t have access to that sort of machine. The HPC as mentioned is the only option for me. I have already gone through Open MPI documents and successfully compiled pytorch 1.5.1(from source) with cuda (10.1) and Open MPI (3.0.4) (CUDA-aware). I would highly appreciate if someone provide me a snippet of code to make specific changes to my source code so that I can train the model in the HPC.<br/><NewLine>Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/alam4545,(Mahbub Alam),alam4545,"August 12, 2020,  5:45am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you hit any error when using CUDA-aware MPI backend? Based on <a href=""https://discuss.pytorch.org/t/mpi-cuda-stream/80702"">past discussion</a>, you might need to synchronize CUDA streams in the application code when using CUDA-aware MPI. BTW, is MPI the only option for you, or would Gloo backend work?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>MPI is the only option for me.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>maybe this will work <a href=""https://github.com/horovod/horovod"" rel=""nofollow noopener"">https://github.com/horovod/horovod</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/alam4545; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Pedro_Lima; <NewLine> ,"REPLY_DATE 1: August 12, 2020,  2:21pm; <NewLine> REPLY_DATE 2: August 16, 2020,  3:39am; <NewLine> REPLY_DATE 3: August 17, 2020,  1:47am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
92842,PySyft: dimension out of range,2020-08-14T20:29:01.697Z,1,71,"<div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li><NewLine><p>I have a model that I am Testing, this model has undergone training. I have a sy.Virtualworker that I send a tensor to. I send the model to this virtual worker. I get the error <code>IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)</code></p><NewLine></li><NewLine><li><NewLine><p>I have looked at other errors with this similar error, however, this might have an issue with what i’m doing with the pysyft material.</p><NewLine></li><NewLine><li><NewLine><p>the goal is to get a prediction from the model based on the input. I believed the input to be of similar dimensions to what the model would work with.</p><NewLine></li><NewLine><li><NewLine><p>I either have an issue with how im dealing with the pysyft materia. As I have worked with this code successfully when doing a non-pysyft version and have seen the intended results.</p><NewLine></li><NewLine></ol><NewLine><p>full error list:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\frameworks\torch\tensors\interpreters\native.py"", line 333, in handle_func_command<NewLine>    cmd, args_, kwargs_, return_args_type=True<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\generic\frameworks\hook\hook_args.py"", line 157, in unwrap_args_from_function<NewLine>    new_args = hook_args(args_)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\generic\frameworks\hook\hook_args.py"", line 356, in &lt;lambda&gt;<NewLine>    return lambda x: f(lambdas, x)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\generic\frameworks\hook\hook_args.py"", line 535, in three_fold<NewLine>    lambdas[1](args_[1], **kwargs),<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\generic\frameworks\hook\hook_args.py"", line 331, in &lt;lambda&gt;<NewLine>    else lambda i: forward_func[type(i)](i)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\frameworks\torch\hook\hook_args.py"", line 30, in &lt;lambda&gt;<NewLine>    else (_ for _ in ()).throw(PureFrameworkTensorFoundError),<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\frameworks\torch\hook\hook_args.py"", line 30, in &lt;genexpr&gt;<NewLine>    else (_ for _ in ()).throw(PureFrameworkTensorFoundError),<NewLine>syft.exceptions.PureFrameworkTensorFoundError<NewLine>During handling of the above exception, another exception occurred:<NewLine>Traceback (most recent call last):<NewLine>  File ""&lt;input&gt;"", line 1, in &lt;module&gt;<NewLine>  File ""B:\tools and software\PyCharm 2020.1\plugins\python\helpers\pydev\_pydev_bundle\pydev_umd.py"", line 197, in runfile<NewLine>    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script<NewLine>  File ""B:\tools and software\PyCharm 2020.1\plugins\python\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile<NewLine>    exec(compile(contents+""\n"", file, 'exec'), glob, loc)<NewLine>  File ""B:/projects/GRA/FederatedLearningAnalysis/anomaly-detection/PyTroch_singleworker_combined_train_test.py"", line 266, in &lt;module&gt;<NewLine>    app.run(main)<NewLine>  File ""B:\tools and software\Anaconda\envs\pysyft-pytorch\lib\site-packages\absl\app.py"", line 299, in run<NewLine>    _run_main(main, args)<NewLine>  File ""B:\tools and software\Anaconda\envs\pysyft-pytorch\lib\site-packages\absl\app.py"", line 250, in _run_main<NewLine>    sys.exit(main(argv))<NewLine>  File ""B:/projects/GRA/FederatedLearningAnalysis/anomaly-detection/PyTroch_singleworker_combined_train_test.py"", line 261, in main<NewLine>    tr=tr, df_malicious=load_mal_data(), features=features)<NewLine>  File ""B:/projects/GRA/FederatedLearningAnalysis/anomaly-detection/PyTroch_singleworker_combined_train_test.py"", line 131, in test_with_data<NewLine>    Y_pred = model.predict(torch.from_numpy(X_test_scaled).float())<NewLine>  File ""B:/projects/GRA/FederatedLearningAnalysis/anomaly-detection/PyTroch_singleworker_combined_train_test.py"", line 204, in predict<NewLine>    x_pred = self.model(x)<NewLine>  File ""B:\tools and software\Anaconda\envs\pysyft-pytorch\lib\site-packages\torch\nn\modules\module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""B:/projects/GRA/FederatedLearningAnalysis/anomaly-detection/PyTroch_singleworker_combined_train_test.py"", line 183, in forward<NewLine>    x = torch.tanh(self.fc1(x))<NewLine>  File ""B:\tools and software\Anaconda\envs\pysyft-pytorch\lib\site-packages\torch\nn\modules\module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""B:\tools and software\Anaconda\envs\pysyft-pytorch\lib\site-packages\torch\nn\modules\linear.py"", line 87, in forward<NewLine>    return F.linear(input, self.weight, self.bias)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\generic\frameworks\hook\hook.py"", line 336, in overloaded_func<NewLine>    response = handle_func_command(command)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\frameworks\torch\tensors\interpreters\native.py"", line 343, in handle_func_command<NewLine>    response = new_type.handle_func_command(new_command)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\generic\pointers\object_pointer.py"", line 213, in handle_func_command<NewLine>    response = owner.send_command(location, cmd_name=cmd, args_=args_, kwargs_=kwargs_)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\workers\base.py"", line 626, in send_command<NewLine>    ret_val = self.send_msg(message, location=recipient)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\workers\base.py"", line 274, in send_msg<NewLine>    bin_response = self._send_msg(bin_message, location)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\workers\virtual.py"", line 16, in _send_msg<NewLine>    return location._recv_msg(message)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\workers\virtual.py"", line 20, in _recv_msg<NewLine>    return self.recv_msg(message)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\workers\base.py"", line 310, in recv_msg<NewLine>    response = self._message_router[type(msg)](msg)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\workers\base.py"", line 451, in execute_tensor_command<NewLine>    return self.execute_computation_action(cmd.action)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\workers\base.py"", line 514, in execute_computation_action<NewLine>    response = command(*args_, **kwargs_)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\generic\frameworks\hook\hook.py"", line 336, in overloaded_func<NewLine>    response = handle_func_command(command)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\frameworks\torch\tensors\interpreters\native.py"", line 367, in handle_func_command<NewLine>    response = cls._get_response(cmd, args_, kwargs_)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\frameworks\torch\tensors\interpreters\native.py"", line 401, in _get_response<NewLine>    response = command_method(*args_, **kwargs_)<NewLine>  File ""B:\tools and software\Anaconda\envs\pysyft-pytorch\lib\site-packages\torch\nn\functional.py"", line 1370, in linear<NewLine>    ret = torch.addmm(bias, input, weight.t())<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\generic\frameworks\hook\hook.py"", line 336, in overloaded_func<NewLine>    response = handle_func_command(command)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\frameworks\torch\tensors\interpreters\native.py"", line 367, in handle_func_command<NewLine>    response = cls._get_response(cmd, args_, kwargs_)<NewLine>  File ""C:\Users\OMEGA-Money\AppData\Roaming\Python\Python36\site-packages\syft\frameworks\torch\tensors\interpreters\native.py"", line 401, in _get_response<NewLine>    response = command_method(*args_, **kwargs_)<NewLine>IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)<NewLine></code></pre><NewLine><p>CODE bits:</p><NewLine><pre><code class=""lang-auto"">    def predict(self, x):<NewLine>        x = x.send('testing')<NewLine>        self.model.send(x.location)<NewLine>        x_pred = self.model(x)<NewLine>        mse = np.mean(np.power(x.get().data.numpy() - x_pred.get().data.numpy(), 2), axis=1)<NewLine>        y_pred = mse &gt; self.threshold<NewLine>        return y_pred.astype(int)<NewLine></code></pre><NewLine><p>CODE 2:</p><NewLine><pre><code class=""lang-auto"">X_test = df.drop(columns=[""malicious""]).values<NewLine>    X_test_scaled = scaler.transform(X_test)<NewLine>    Y_test = df[""malicious""]<NewLine>    Y_pred = model.predict(torch.from_numpy(X_test_scaled).float())<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/GIGA-Money,(Giga Money),GIGA-Money,"August 14, 2020,  8:29pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If more code is needed, just say.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> do you know who would be the best person to answer PySyft questions?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ravikantsingh"">@RavikantSingh</a> is a member of OpenMined and might be able to help. I cannot find the user account of Andrew Trask (and unsure, if he’s active in this board).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/GIGA-Money; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: August 14, 2020,  8:29pm; <NewLine> REPLY_DATE 2: August 14, 2020,  9:22pm; <NewLine> REPLY_DATE 3: August 15, 2020,  6:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
92770,torch.utils.data.DataLoader issue,2020-08-14T06:43:44.343Z,2,87,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I followed <a href=""https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py"" rel=""nofollow noopener"">https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py</a> to create my own ImageFolder  (I called it ImageFolderSuperpixel, folder_sp.py). <strong>It works FINE in a single GPU but it meets bugs in a single node, multiple GPUs</strong>. Anyone can tell me what is going on here?</p><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “”, line 1, in <br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “/usr/lib64/python3.6/multiprocessing/spawn.py”, line 105, in spawn_main<br/><NewLine>exitcode = _main(fd)<br/><NewLine>File “/usr/lib64/python3.6/multiprocessing/spawn.py”, line 115, in _main<br/><NewLine>self = reduction.pickle.load(from_parent)<br/><NewLine>_pickle.UnpicklingError: pickle data was truncated<br/><NewLine>File “”, line 1, in <br/><NewLine>File “/usr/lib64/python3.6/multiprocessing/spawn.py”, line 105, in spawn_main<br/><NewLine>exitcode = _main(fd)<br/><NewLine>File “/usr/lib64/python3.6/multiprocessing/spawn.py”, line 115, in _main<br/><NewLine>self = reduction.pickle.load(from_parent)<br/><NewLine>_pickle.UnpicklingError: pickle data was truncated<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “”, line 1, in <br/><NewLine>File “/usr/lib64/python3.6/multiprocessing/spawn.py”, line 105, in spawn_main<br/><NewLine>exitcode = _main(fd)<br/><NewLine>File “/usr/lib64/python3.6/multiprocessing/spawn.py”, line 115, in _main<br/><NewLine>self = reduction.pickle.load(from_parent)<br/><NewLine>_pickle.UnpicklingError: pickle data was truncated<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “main.py”, line 102, in <br/><NewLine>main()<br/><NewLine>File “main.py”, line 45, in main<br/><NewLine>classification.start(dataset_path, checkpoints_path, args, **CONFIG[args.dataset])<br/><NewLine>File “/nfs/hpc/share/coe_hanweiku/xxxnet-pytorch/utils/classification.py”, line 304, in start<br/><NewLine>mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))<br/><NewLine>File “/nfs/hpc/share/coe_hanweiku/xxxnet-pytorch/venv2/lib64/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 200, in spawn<br/><NewLine>return start_processes(fn, args, nprocs, join, daemon, start_method=‘spawn’)<br/><NewLine>File “/nfs/hpc/share/coe_hanweiku/xxxnet-pytorch/venv2/lib64/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 158, in start_processes<br/><NewLine>while not context.join():<br/><NewLine>File “/nfs/hpc/share/coe_hanweiku/xxxnet-pytorch/venv2/lib64/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 108, in join<br/><NewLine>(error_index, name)<br/><NewLine>Exception: process 1 terminated with signal SIGKILL</p><NewLine><pre><code class=""lang-python"">""""""Custom Image Datasets API <NewLine><NewLine>Image datasets API have two input image directories, which could provide the <NewLine>interface for superpixel research<NewLine><NewLine>Author: Weikun Han &lt;weikunhan@gmail.com&gt;<NewLine><NewLine>Reference: <NewLine>- https://github.com/pytorch/vision/blob/master/torchvision/datasets/vision.py<NewLine>- https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py<NewLine><NewLine>""""""<NewLine><NewLine>import os<NewLine>import random<NewLine>import torch<NewLine>import torch.utils.data as data<NewLine>from PIL import Image<NewLine><NewLine><NewLine>class VisionDataset(data.Dataset):<NewLine>    _repr_indent = 4<NewLine><NewLine>    def __init__(self, root, root_sp, transforms=None, transform=None, target_transform=None):<NewLine>        if isinstance(root, torch._six.string_classes):<NewLine>            root = os.path.expanduser(root)<NewLine>        if isinstance(root_sp, torch._six.string_classes):<NewLine>            root_sp= os.path.expanduser(root_sp)   <NewLine>        self.root = root<NewLine>        self.root_sp = root_sp<NewLine>        has_transforms = transforms is not None<NewLine>        has_separate_transform = transform is not None or target_transform is not None<NewLine>        if has_transforms and has_separate_transform:<NewLine>            raise ValueError(""Only transforms or transform/target_transform can ""<NewLine>                             ""be passed as argument"")<NewLine>        # for backwards-compatibility<NewLine>        self.transform = transform<NewLine>        self.target_transform = target_transform<NewLine>        if has_separate_transform:<NewLine>            transforms = StandardTransform(transform, target_transform)<NewLine>        self.transforms = transforms<NewLine><NewLine>    def __getitem__(self, index):<NewLine>        raise NotImplementedError<NewLine><NewLine>    def __len__(self):<NewLine>        raise NotImplementedError<NewLine><NewLine>    def __repr__(self):<NewLine>        head = ""Dataset "" + self.__class__.__name__<NewLine>        body = [""Number of datapoints: {}"".format(self.__len__())]<NewLine>        if self.root is not None:<NewLine>            body.append(""Root location: {}"".format(self.root))<NewLine>        if self.root_sp is not None:<NewLine>            body.append(""Root superpixel location: {}"".format(self.root_sp))            <NewLine>        body += self.extra_repr().splitlines()<NewLine>        if hasattr(self, ""transforms"") and self.transforms is not None:<NewLine>            body += [repr(self.transforms)]<NewLine>        lines = [head] + ["" "" * self._repr_indent + line for line in body]<NewLine>        return '\n'.join(lines)<NewLine><NewLine>    def _format_transform_repr(self, transform, head):<NewLine>        lines = transform.__repr__().splitlines()<NewLine>        return ([""{}{}"".format(head, lines[0])] +<NewLine>                [""{}{}"".format("" "" * len(head), line) for line in lines[1:]])<NewLine><NewLine>    def extra_repr(self):<NewLine>        return """"<NewLine><NewLine><NewLine>class StandardTransform(object):<NewLine><NewLine>    def __init__(self, transform=None, target_transform=None):<NewLine>        self.transform = transform<NewLine>        self.target_transform = target_transform<NewLine><NewLine>    def __call__(self, input, target):<NewLine>        if self.transform is not None:<NewLine>            input = self.transform(input)<NewLine>        if self.target_transform is not None:<NewLine>            target = self.target_transform(target)<NewLine>        return input, target<NewLine><NewLine>    def _format_transform_repr(self, transform, head):<NewLine>        lines = transform.__repr__().splitlines()<NewLine>        return ([""{}{}"".format(head, lines[0])] +<NewLine>                [""{}{}"".format("" "" * len(head), line) for line in lines[1:]])<NewLine><NewLine>    def __repr__(self):<NewLine>        body = [self.__class__.__name__]<NewLine>        if self.transform is not None:<NewLine>            body += self._format_transform_repr(self.transform,<NewLine>                                                ""Transform: "")<NewLine>        if self.target_transform is not None:<NewLine>            body += self._format_transform_repr(self.target_transform,<NewLine>                                                ""Target transform: "")<NewLine>        return '\n'.join(body)<NewLine>    <NewLine><NewLine>def has_file_allowed_extension(filename, extensions):<NewLine>    """"""Checks if a file is an allowed extension.<NewLine><NewLine>    Args:<NewLine>        filename (string): path to a file<NewLine>        extensions (tuple of strings): extensions to consider (lowercase)<NewLine><NewLine>    Returns:<NewLine>        bool: True if the filename ends with one of given extensions<NewLine>    """"""<NewLine>    return filename.lower().endswith(extensions)<NewLine><NewLine>def is_image_file(filename):<NewLine>    """"""Checks if a file is an allowed image extension.<NewLine><NewLine>    Args:<NewLine>        filename (string): path to a file<NewLine><NewLine>    Returns:<NewLine>        bool: True if the filename ends with a known image extension<NewLine>    """"""<NewLine>    return has_file_allowed_extension(filename, IMG_EXTENSIONS)<NewLine><NewLine>def make_dataset(directory, class_to_idx, extensions=None, is_valid_file=None):<NewLine>    instances = []<NewLine>    directory = os.path.expanduser(directory)<NewLine>    both_none = extensions is None and is_valid_file is None<NewLine>    both_something = extensions is not None and is_valid_file is not None<NewLine>    if both_none or both_something:<NewLine>        raise ValueError(""Both extensions and is_valid_file cannot be None or not None at the same time"")<NewLine>    if extensions is not None:<NewLine>        def is_valid_file(x):<NewLine>            return has_file_allowed_extension(x, extensions)<NewLine>    for target_class in sorted(class_to_idx.keys()):<NewLine>        class_index = class_to_idx[target_class]<NewLine>        target_dir = os.path.join(directory, target_class)<NewLine>        if not os.path.isdir(target_dir):<NewLine>            continue<NewLine>        for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):<NewLine>            for fname in sorted(fnames):<NewLine>                path = os.path.join(root, fname)<NewLine>                if is_valid_file(path):<NewLine>                    item = path, class_index<NewLine>                    instances.append(item)<NewLine>    return instances<NewLine><NewLine><NewLine>class DatasetFolder(VisionDataset):<NewLine>    """"""A generic data loader where the samples are arranged in this way: ::<NewLine><NewLine>        root/class_x/xxx.ext<NewLine>        root/class_x/xxy.ext<NewLine>        root/class_x/xxz.ext<NewLine><NewLine>        root/class_y/123.ext<NewLine>        root/class_y/nsdf3.ext<NewLine>        root/class_y/asd932_.ext<NewLine><NewLine>    Args:<NewLine>        root (string): Root directory path.<NewLine>        root_sp (string): Root directory path for superpixel.<NewLine>        loader (callable): A function to load a sample given its path.<NewLine>        extensions (tuple[string]): A list of allowed extensions.<NewLine>            both extensions and is_valid_file should not be passed.<NewLine>        transform (callable, optional): A function/transform that takes in<NewLine>            a sample and returns a transformed version.<NewLine>            E.g, ``transforms.RandomCrop`` for images.<NewLine>        target_transform (callable, optional): A function/transform that takes<NewLine>            in the target and transforms it.<NewLine>        is_valid_file (callable, optional): A function that takes path of a file<NewLine>            and check if the file is a valid file (used to check of corrupt files)<NewLine>            both extensions and is_valid_file should not be passed.<NewLine><NewLine>     Attributes:<NewLine>        classes (list): List of the class names sorted alphabetically.<NewLine>        class_to_idx (dict): Dict with items (class_name, class_index).<NewLine>        samples (list): List of (sample path, class_index) tuples<NewLine>        targets (list): The class_index value for each image in the dataset<NewLine>    """"""<NewLine><NewLine>    def __init__(self, root, root_sp, loader, extensions=None, transform=None,<NewLine>                 target_transform=None, is_valid_file=None):<NewLine>        super(DatasetFolder, self).__init__(root, root_sp, transform=transform,<NewLine>                                            target_transform=target_transform)<NewLine>        classes, class_to_idx = self._find_classes(self.root)<NewLine>        classes_sp, class_to_idx_sp = self._find_classes(self.root_sp)<NewLine>        samples = make_dataset(self.root, class_to_idx, extensions, is_valid_file)<NewLine>        samples_sp = make_dataset(self.root_sp, class_to_idx_sp, extensions, is_valid_file)<NewLine>        if len(samples) == 0:<NewLine>            msg = ""Found 0 files in subfolders of: {}\n"".format(self.root)<NewLine>            if extensions is not None:<NewLine>                msg += ""Supported extensions are: {}"".format("","".join(extensions))<NewLine>            raise RuntimeError(msg)<NewLine>        if len(samples_sp) == 0:<NewLine>            msg = ""Found 0 files in subfolders of: {}\n"".format(self.root_sp)<NewLine>            if extensions is not None:<NewLine>                msg += ""Supported extensions are: {}"".format("","".join(extensions))<NewLine>            raise RuntimeError(msg)<NewLine>        if len(samples) != len(samples_sp):<NewLine>            msg = ""Image files is not equal to superpixel files.\n""<NewLine>            if extensions is not None:<NewLine>                msg += ""Supported extensions are: {}"".format("","".join(extensions))<NewLine>            raise RuntimeError(msg)<NewLine>        self.loader = loader<NewLine>        self.extensions = extensions<NewLine>        self.classes = classes<NewLine>        self.classes_sp = classes_sp<NewLine>        self.class_to_idx = class_to_idx<NewLine>        self.class_to_idx_sp = class_to_idx_sp<NewLine>        self.samples = samples<NewLine>        self.samples_sp = samples_sp<NewLine>        self.targets = [s[1] for s in samples]<NewLine>        self.targets_sp = [s[1] for s in samples_sp]<NewLine><NewLine>    def _find_classes(self, dir):<NewLine>        """"""<NewLine>        Finds the class folders in a dataset.<NewLine><NewLine>        Args:<NewLine>            dir (string): Root directory path.<NewLine><NewLine>        Returns:<NewLine>            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.<NewLine><NewLine>        Ensures:<NewLine>            No class is a subdirectory of another.<NewLine>        """"""<NewLine>        classes = [d.name for d in os.scandir(dir) if d.is_dir()]<NewLine>        classes.sort()<NewLine>        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}<NewLine>        return classes, class_to_idx<NewLine><NewLine>    def __getitem__(self, index):<NewLine>        """"""<NewLine>        Args:<NewLine>            index (int): Index<NewLine><NewLine>        Returns:<NewLine>            tuple: (sample, target) where target is class_index of the target class.<NewLine>        """"""<NewLine>        path, target = self.samples[index]<NewLine>        path_sp, target_sp = self.samples_sp[index]<NewLine>        sample = self.loader(path).convert('RGB')<NewLine>        sample_sp = self.loader(path_sp)<NewLine>        if self.transform is not None:<NewLine>            torch.manual_seed(1234)<NewLine>            random.seed(1234)<NewLine>            sample = self.transform(sample)<NewLine>            torch.manual_seed(1234)<NewLine>            random.seed(1234)<NewLine>            sample_sp = self.transform(sample_sp)<NewLine>        if self.target_transform is not None:<NewLine>            torch.manual_seed(4321)<NewLine>            random.seed(4321)<NewLine>            target = self.target_transform(target)<NewLine>            torch.manual_seed(4321)<NewLine>            random.seed(4321)<NewLine>            target_sp = self.target_transform(target_sp)<NewLine>        return sample, target, sample_sp, target_sp<NewLine><NewLine>    def __len__(self):<NewLine>        return len(self.samples)<NewLine><NewLine>    <NewLine>IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')<NewLine><NewLine>def pil_loader(path):<NewLine>    img = Image.open(path)<NewLine>    return img<NewLine><NewLine>def accimage_loader(path):<NewLine>    import accimage<NewLine>    try:<NewLine>        return accimage.Image(path)<NewLine>    except IOError:<NewLine>        # Potentially a decoding problem, fall back to PIL.Image<NewLine>        return pil_loader(path)<NewLine><NewLine>def default_loader(path):<NewLine>    from torchvision import get_image_backend<NewLine>    if get_image_backend() == 'accimage':<NewLine>        return accimage_loader(path)<NewLine>    else:<NewLine>        return pil_loader(path)<NewLine>            <NewLine><NewLine>class ImageFolderSuperpixel(DatasetFolder):<NewLine>    """"""A generic data loader where the images are arranged in this way: ::<NewLine><NewLine>        root/dog/xxx.png<NewLine>        root/dog/xxy.png<NewLine>        root/dog/xxz.png<NewLine><NewLine>        root/cat/123.png<NewLine>        root/cat/nsdf3.png<NewLine>        root/cat/asd932_.png<NewLine><NewLine>    Args:<NewLine>        root (string): Root directory path.<NewLine>        root_sp (string): Root directory path for superpixel.<NewLine>        transform (callable, optional): A function/transform that  takes in an PIL image<NewLine>            and returns a transformed version. E.g, ``transforms.RandomCrop``<NewLine>        target_transform (callable, optional): A function/transform that takes in the<NewLine>            target and transforms it.<NewLine>        loader (callable, optional): A function to load an image given its path.<NewLine>        is_valid_file (callable, optional): A function that takes path of an Image file<NewLine>            and check if the file is a valid file (used to check of corrupt files)<NewLine><NewLine>     Attributes:<NewLine>        classes (list): List of the class names sorted alphabetically.<NewLine>        class_to_idx (dict): Dict with items (class_name, class_index).<NewLine>        imgs (list): List of (image path, class_index) tuples<NewLine>    """"""<NewLine><NewLine>    def __init__(self, root, root_sp, transform=None, target_transform=None,<NewLine>                 loader=default_loader, is_valid_file=None):<NewLine>        super(ImageFolderSuperpixel, self).__init__(root, root_sp, loader, IMG_EXTENSIONS if is_valid_file is None else None,<NewLine>                                                    transform=transform,<NewLine>                                                    target_transform=target_transform,<NewLine>                                                    is_valid_file=is_valid_file)<NewLine>        self.imgs = self.samples<NewLine>        self.imgs_sp = self.samples_sp<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/weikunhan,(Weikun Han),weikunhan,"August 14, 2020,  6:43am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> I changed to the ImageFolder class and there is no problem! Therefore, I am sure that my ImageFolderSuperpixel class have some problems that I cannot find it.</p><NewLine><p>The example to use this API, the main purpose for this API is to load two image folder at same time (ImageFolder only support loading one image dir):</p><NewLine><pre><code class=""lang-python"">dataset_path = './data/imagenet'<NewLine>traindir = os.path.join(dataset_path, 'train')<NewLine>traindir_sp = os.path.join(dataset_path, 'train')<NewLine>train_dataset = ImageFolderSuperpixel(<NewLine>    traindir,<NewLine>    traindir_sp,<NewLine>    transforms.Compose([<NewLine>        transforms.RandomResizedCrop(224),<NewLine>        transforms.RandomHorizontalFlip(),<NewLine>        transforms.ToTensor(),<NewLine>    ]))<NewLine><NewLine>train_loader = torch.utils.data.DataLoader(<NewLine>    train_dataset, batch_size=1, shuffle=False,<NewLine>    num_workers=1, pin_memory=True, sampler=None)<NewLine><NewLine>for i, (images, target, images_sp, _) in enumerate(train_loader):</code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I can’t see any obvious errors in your code.<br/><NewLine>Could you use <code>num_workers=0</code> and rerun the code?<br/><NewLine>This should give you a better error message in case a worker is failing to load the data.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for helping! I try to use one node with 2 GPUs, there is no problem. Next, I try to use one node with 3 or 4 GPUs but not work, I found ONLY 2 GPUs is working…</p><NewLine><p><img alt=""image"" data-base62-sha1=""jWPjTCb5VS6K4BOAFxlAcVUPcYE"" height=""273"" src=""https://discuss.pytorch.org/uploads/default/original/3X/8/b/8bcfae19f87e9453cc4cfb4edea894cad06372a8.png"" width=""591""/></p><NewLine><p><strong>Therefore</strong>, I try to understand why I can only use one node with 2 GPUs. The answer is node memory. I use default setting which only provide 10GB in each node.  Compare with official Pytorch datasets.ImageFolder(), 10GB memory is OK for one node with 4 or 6 GPUs. However, since I create ImageFolderSuperpixel which contains too much information, it need more memory than official Pytorch datasets.ImageFolder(). After I apply for 40GB in each node. I can run with each node with 4GPUs.</p><NewLine><p>Thanks</p><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “”, line 1, in <br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “”, line 1, in <br/><NewLine>File “/usr/lib64/python3.6/multiprocessing/spawn.py”, line 105, in spawn_main<br/><NewLine>exitcode = _main(fd)<br/><NewLine>File “/usr/lib64/python3.6/multiprocessing/spawn.py”, line 115, in _main<br/><NewLine>self = reduction.pickle.load(from_parent)<br/><NewLine>_pickle.UnpicklingError: pickle data was truncated<br/><NewLine>File “/usr/lib64/python3.6/multiprocessing/spawn.py”, line 105, in spawn_main<br/><NewLine>exitcode = _main(fd)<br/><NewLine>File “/usr/lib64/python3.6/multiprocessing/spawn.py”, line 115, in _main<br/><NewLine>self = reduction.pickle.load(from_parent)<br/><NewLine>_pickle.UnpicklingError: pickle data was truncated<br/><NewLine>/usr/lib64/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 11 leaked semaphores to clean up at shutdown<br/><NewLine>len(cache))<br/><NewLine>/usr/lib64/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 11 leaked semaphores to clean up at shutdown<br/><NewLine>len(cache))<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “”, line 1, in <br/><NewLine>File “/usr/lib64/python3.6/multiprocessing/spawn.py”, line 105, in spawn_main<br/><NewLine>exitcode = _main(fd)<br/><NewLine>File “/usr/lib64/python3.6/multiprocessing/spawn.py”, line 115, in _main<br/><NewLine>self = reduction.pickle.load(from_parent)<br/><NewLine>_pickle.UnpicklingError: pickle data was truncated<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “main2.py”, line 102, in <br/><NewLine>main()<br/><NewLine>File “main2.py”, line 45, in main<br/><NewLine>classification.start(dataset_path, checkpoints_path, args, **CONFIG[args.dataset])<br/><NewLine>File “/nfs/hpc/share/coe_hanweiku/xxxnet-pytorch/utils/classificationtest.py”, line 304, in start<br/><NewLine>mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))<br/><NewLine>File “/nfs/hpc/share/coe_hanweiku/xxxnet-pytorch/venv2/lib64/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 200, in spawn<br/><NewLine>return start_processes(fn, args, nprocs, join, daemon, start_method=‘spawn’)<br/><NewLine>File “/nfs/hpc/share/coe_hanweiku/xxxnet-pytorch/venv2/lib64/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 158, in start_processes<br/><NewLine>while not context.join():<br/><NewLine>File “/nfs/hpc/share/coe_hanweiku/xxxnet-pytorch/venv2/lib64/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 108, in join<br/><NewLine>(error_index, name)<br/><NewLine>Exception: process 1 terminated with signal SIGKILL<br/><NewLine>/usr/lib64/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 11 leaked semaphores to clean up at shutdown<br/><NewLine>len(cache))</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/weikunhan; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/weikunhan; <NewLine> ,"REPLY_DATE 1: August 14, 2020,  6:54am; <NewLine> REPLY_DATE 2: August 14, 2020, 10:10am; <NewLine> REPLY_DATE 3: August 15, 2020, 12:57am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
92494,Should I add @autoncast() in sub-module with mutiple GPUs,2020-08-12T02:05:29.365Z,0,70,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I am using automatic mixed precision with DataParallel in a single process.<br/><NewLine>I read the example <a href=""https://pytorch.org/docs/stable/notes/amp_examples.html#dataparallel-in-a-single-process"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/notes/amp_examples.html#dataparallel-in-a-single-process</a> and it says the <span class=""mention"">@autocast</span> should be added in MyModel before forward.<br/><NewLine>My question is, should I add the <span class=""mention"">@autocast</span> in the subModel.<br/><NewLine>For example.<br/><NewLine>‘’’<br/><NewLine>MyModel(nn.Module)<br/><NewLine>…<br/><NewLine>self. conv1 = subModel(…)</p><NewLine><pre><code>@autocast()<NewLine>def forward()<NewLine></code></pre><NewLine><p>‘’’</p><NewLine></div>",https://discuss.pytorch.org/u/Yu_Shen,(Yu Shen),Yu_Shen,"August 12, 2020,  3:41am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc AMP author <a class=""mention"" href=""/u/mcarilli"">@mcarilli</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Anything that runs under autocast in a particular thread will have autocast enabled.</p><NewLine><p><code>MyModel.forward</code> is what DP runs in a side thread.  If <code>MyModel.forward</code> is decorated with <code>@autocast()</code>, that takes care of enabling autocast for the side thread.</p><NewLine><p>If <code>subModel.forward</code> runs within MyModel’s forward, you don’t need to additionally decorate <code>subModel.forward</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mcarilli; <NewLine> ,"REPLY_DATE 1: August 12, 2020,  3:06am; <NewLine> REPLY_DATE 2: August 15, 2020,  2:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
92262,Unexpected hang up when using DistributedDataParallel on two machines,2020-08-10T09:17:23.919Z,3,116,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone. I am working on training models across multiple machines. Following the instruction from the documents, I write following codes:</p><NewLine><ul><NewLine><li>On machine 1</li><NewLine></ul><NewLine><pre><code class=""lang-python"">import torch<NewLine><NewLine>torch.distributed.init_process_group(backend='nccl', world_size=2, rank=0, init_method='tcp://172.16.0.246:3456')<NewLine><NewLine>net = torch.nn.Linear(256, 128).cuda()<NewLine>net = torch.nn.parallel.DistributedDataParallel(net, [0], 0)<NewLine></code></pre><NewLine><ul><NewLine><li>On machine 2</li><NewLine></ul><NewLine><pre><code class=""lang-python"">import torch<NewLine><NewLine>torch.distributed.init_process_group(backend='nccl', world_size=2, rank=1, init_method='tcp://172.16.0.246:3456')<NewLine><NewLine>net = torch.nn.Linear(256, 128).cuda()<NewLine>net = torch.nn.parallel.DistributedDataParallel(net, [0], 0)<NewLine></code></pre><NewLine><p>In which <code>172.16.0.246</code> is the IP of machine 1. However, the code hang up unexpectedly when calling function <code>_distributed_broadcast_coalesced</code> in the initialization of DistributedDataParallel.</p><NewLine><p>Is there anyone knows what did I do wrong?</p><NewLine></div>",https://discuss.pytorch.org/u/IcarusWizard,(Xingyuan Zhang),IcarusWizard,"August 10, 2020,  9:17am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/icaruswizard"">@IcarusWizard</a>, what error did you see? Does <code>gloo</code> backend work for you?</p><NewLine><p>Can you run the following command to check if the <code>hostname</code> can resolve to the expected IP on both machines?</p><NewLine><pre><code class=""lang-auto"">getent hosts `hostname`<NewLine></code></pre><NewLine><p>If the resolved IP is wrong, you can set <a href=""https://pytorch.org/docs/stable/distributed.html#common-environment-variables"" rel=""nofollow noopener""><code>NCCL_SOCKET_IFNAME </code></a> env var to point to the right nic (e.g., eth0).</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey Shen!</p><NewLine><p>The strange thing is that there is actually no error. The code stuck at <code>_distributed_broadcast_coalesced</code> and cannot be terminated by <code>Ctrl+C</code>.</p><NewLine><p>I have tried <code>gloo</code>, and it works smoothly which may suggest it is not an issue related to firewall. I also have set <code>GLOO_SOCKET_IFNAME</code> and <code>NCCL_SOCKET_IFNAME</code> to the correct interface on both machine.</p><NewLine><p>And for the command you suggested, it returns <code>127.0.1.1       icarus-Polixir</code> on machine 1, and</p><NewLine><pre><code class=""lang-auto"">fe80::b62e:99ff:fe72:d1a1 polixir-G291-Z20-00<NewLine>fe80::98a3:19ff:fe05:3c61 polixir-G291-Z20-00<NewLine>fe80::42:adff:fe62:bb24 polixir-G291-Z20-00<NewLine>fe80::d01c:dff:fe28:8b6f polixir-G291-Z20-00<NewLine>fe80::1c3d:c8ff:fe62:76cc polixir-G291-Z20-00<NewLine></code></pre><NewLine><p>on machine 2. I don’t know if it’s related to the issue.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If <code>NCCL_SOCKET_IFNAME</code> points to the correct interface, it should be fine even if the hostname resolves to wrong address, as the latter is a fallback of the former.</p><NewLine><p>And as it has already reached the broadcast op in DDP, I would assume the rendezvous in <code>init_process_group</code> was successful. Could you please confirm this by adding the following code right after <code>init_process_group</code> and see if it also hang at this allreduce?</p><NewLine><pre><code class=""lang-python"">print(""rendezvous done"")<NewLine>tmp = torch.ones(2, 2)<NewLine>torch.distributed.all_reduce(tmp)<NewLine>print(tmp)<NewLine></code></pre><NewLine><p>Another thing that we could try is to set the following env vars and see if there is any NCCL logs that stand out.</p><NewLine><pre><code class=""lang-auto"">export NCCL_DEBUG=INFO<NewLine>export NCCL_DEBUG_SUBSYS=ALL<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>It stuck on the <code>all_reduce</code> operation too. I get some new information from NCCL logs.</p><NewLine><p>On machine 1:</p><NewLine><pre><code class=""lang-auto"">rendezvous done<NewLine>icarus-Polixir:637574:637574 [0] NCCL INFO Bootstrap : Using [0]wlp0s20f3:172.16.0.246&lt;0&gt;<NewLine>icarus-Polixir:637574:637574 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine><NewLine>icarus-Polixir:637574:637574 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]<NewLine>icarus-Polixir:637574:637574 [0] NCCL INFO NET/Socket : Using [0]wlp0s20f3:172.16.0.246&lt;0&gt;<NewLine>NCCL version 2.4.8+cuda10.2<NewLine>icarus-Polixir:637574:637587 [0] NCCL INFO Setting affinity for GPU 0 to ff<NewLine>icarus-Polixir:637574:637587 [0] NCCL INFO CUDA Dev 0[0], Socket NIC distance :  PHB<NewLine>icarus-Polixir:637574:637587 [0] NCCL INFO Channel 00 :    0   1<NewLine>icarus-Polixir:637574:637587 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for GPU 0[0] / HCA 0 (distance 2 &gt;= 2)<NewLine>icarus-Polixir:637574:637587 [0] NCCL INFO Ring 00 : 1 -&gt; 0 [receive] via NET/Socket/0<NewLine>icarus-Polixir:637574:637587 [0] NCCL INFO NET/Socket: Using 1 threads and 1 sockets per thread<NewLine>icarus-Polixir:637574:637587 [0] NCCL INFO Ring 00 : 0 -&gt; 1 [send] via NET/Socket/0<NewLine></code></pre><NewLine><p>On machine 2:</p><NewLine><pre><code class=""lang-auto"">rendezvous done<NewLine>polixir-G291-Z20-00:2672:2672 [0] NCCL INFO Bootstrap : Using [0]enp129s0f1:172.16.16.122&lt;0&gt;<NewLine>polixir-G291-Z20-00:2672:2672 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>polixir-G291-Z20-00:2672:2672 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/RoCE ; OOB enp129s0f1:172.16.16.122&lt;0&gt;<NewLine>polixir-G291-Z20-00:2672:2763 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,ffff0000,00000000<NewLine>polixir-G291-Z20-00:2672:2763 [0] NCCL INFO CUDA Dev 0[4], IB NIC distance :  SYS<NewLine>polixir-G291-Z20-00:2672:2763 [0] NCCL INFO NET/IB : GPU Direct RDMA Disabled for GPU 0[4] / HCA 0 (distance 4 &gt;= 2)<NewLine>polixir-G291-Z20-00:2672:2763 [0] NCCL INFO Ring 00 : 0 -&gt; 1 [receive] via NET/IB/0<NewLine>polixir-G291-Z20-00:2672:2763 [0] NCCL INFO Ring 00 : 1 -&gt; 0 [send] via NET/IB/0<NewLine>polixir-G291-Z20-00:2672:2763 [0] NCCL INFO NET/IB: Dev 0 Port 1 qpn 2358 mtu 3 GID 0 (80FE/A1D172FEFF992EB6)<NewLine></code></pre><NewLine><p>First thing I noticed is that it trying to find <code>libnccl-net.so</code>. However I cannot find this file in official release of NCCL. I have tried to run this two scripts both on machine 1, and it works just fine. Thus I think it may not be the source of problem.</p><NewLine><p>Any other thoughts?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you install NCCL yourself or the one bundled within PyTorch?</p><NewLine><p>Not sure if I interpreted the logs correctly, but looks like machine 1 is trying to use TCP while machine 2 is trying to use IB? What if you set <code>NCCL_IB_DISABLE</code> on both machine? <a href=""https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-ib-disable"" rel=""nofollow noopener"">https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-ib-disable</a></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>IT WORKS after disabling IB. Seems like IB is a hardware related feature, and the web interface on machine 1 simply doesn’t support it.</p><NewLine><p>I am using the NCCL bundled with PyTorch. I have also tried to install NCCL myself, but it makes no different.</p><NewLine><p>Thanks a lot for your help!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/IcarusWizard; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/IcarusWizard; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/IcarusWizard; <NewLine> ,"REPLY_DATE 1: August 10, 2020,  2:40pm; <NewLine> REPLY_DATE 2: August 11, 2020,  2:04am; <NewLine> REPLY_DATE 3: August 11, 2020,  2:17pm; <NewLine> REPLY_DATE 4: August 12, 2020,  6:28am; <NewLine> REPLY_DATE 5: August 13, 2020,  2:07am; <NewLine> REPLY_DATE 6: August 13, 2020,  2:07am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> 
92557,Multi GPU training on single node with DistributedDataParallel,2020-08-12T12:24:00.951Z,1,61,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m new to distributed training.</p><NewLine><p>When I train with DistributedDataParallel do I get the functionality of DataParallel, meaning can I assume that on a single node if there is more than one GPU then all GPUs will be utilized on that node?</p><NewLine><p>Thanks,<br/><NewLine>Zlapp</p><NewLine></div>",https://discuss.pytorch.org/u/zlapp,(Zlapp),zlapp,"August 12, 2020, 12:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yep, DistributedDataParallel (DDP) can utilize multiple GPUs on the same node, but it works differently than DataParallel (DP). DDP uses multiple processes, one process per GPU, while DP is single-process multi-thread.</p><NewLine><p>See this page for the comparison between the two: <a href=""https://pytorch.org/tutorials/beginner/dist_overview.html#data-parallel-training"" rel=""nofollow noopener"">https://pytorch.org/tutorials/beginner/dist_overview.html#data-parallel-training</a></p><NewLine><p>and this to get started with DDP: <a href=""https://pytorch.org/docs/stable/notes/ddp.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/notes/ddp.html</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Great, thanks for the answer and references</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/zlapp; <NewLine> ,"REPLY_DATE 1: August 12, 2020,  2:10pm; <NewLine> REPLY_DATE 2: August 12, 2020,  2:10pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
92273,Error in DistributedDataParallel,2020-08-10T10:54:10.068Z,7,78,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am trying to use DistributedDataParallel for my job. I wrote two following codes but none of them is working properly. It would be great kind if someone helps me to find the problem.</p><NewLine><pre><code class=""lang-auto"">class Model(nn.Module):<NewLine>    # Our model<NewLine><NewLine>    def __init__(self):<NewLine>        super(Model, self).__init__()<NewLine>        <NewLine>        self.fc1 = nn.Conv2d(1,10,3)<NewLine>        self.bn1 = nn.BatchNorm2d(10)<NewLine>        self.fc2= nn.Conv2d(10,20,3)<NewLine>        self.bn2 = nn.BatchNorm2d(20)<NewLine>        self.fc3= nn.Linear(11520,10)<NewLine>        <NewLine>    def forward(self,x):<NewLine>        <NewLine>        print(f'inout_size: {x.size()}')<NewLine>        <NewLine>        x = F.relu(self.fc1(x))<NewLine>        <NewLine>        x = self.bn1(x)<NewLine>        <NewLine>        x = F.relu(self.fc2(x))<NewLine>        <NewLine>        x = self.bn2(x)<NewLine>        <NewLine>        x = x.view(x.size(0),-1)<NewLine>        <NewLine>        x = self.fc3(x)<NewLine>        print(f'output_size: {x.size()}')<NewLine>        return(x)<NewLine>########################################    <NewLine><NewLine><NewLine><NewLine>def train(args):<NewLine>    <NewLine>    ########################################<NewLine>    rank =args.gpui<NewLine>    <NewLine>    dist.init_process_group(backed = 'nccl',<NewLine>                           init_method = 'env://',<NewLine>                           world_size= args.world_size,<NewLine>                           rank=rank)<NewLine>    <NewLine>    torch.manual_seed(0)<NewLine>    <NewLine>    model = Model()<NewLine>    <NewLine>    torch.cuda.set_device(args.gpui)<NewLine>    model= model.to(device)<NewLine>    optimizer = optim.Adam(model.parameters(),lr=0.1)<NewLine>    lr_sch = lr_scheduler.StepLR(optimizer,step_size=2,gamma=0.1)<NewLine>    criterion = nn.CrossEntropyLoss().to(device)<NewLine>    <NewLine>    ######################################<NewLine>    model = nn.DistributedDataParallel(model, device_ids = [args.gpui])<NewLine>    #####################################<NewLine><NewLine>    <NewLine>    mnist =torchvision.datasets.MNIST('./data',train= True,download=True,<NewLine>                                      transform =transforms.ToTensor())<NewLine>    <NewLine>    ####################################<NewLine>    train_sampler = torch.utils.data.distributed.DistributedSampler(mnist,<NewLine>                                                                    num_replicas=args.world_size,<NewLine>                                                                    rank = rank)<NewLine>    <NewLine>    ###################################<NewLine>    <NewLine>    dataloader = DataLoader(mnist,batch_size=32,num_workers =4,pin_memory=True,<NewLine>                                               sampler = train_sampler)<NewLine>    <NewLine>    #####################################<NewLine>    <NewLine>    <NewLine>    for epoch in range(num_epochs):<NewLine><NewLine>        total_loss =0<NewLine>        <NewLine>        for X,y in dataloader:   <NewLine>        <NewLine>            X= X.to(device)<NewLine>            y = y.long().to(device)<NewLine>            pred = model(X)<NewLine>            loss = criterion(pred,y)<NewLine>            t_loss+= loss.item()<NewLine>            optimizer.zero_grad()<NewLine>            loss.backward()<NewLine>            optimizer.step()<NewLine>        print(f'Loss: {t_loss/len(dataloader)}')<NewLine>if __name__=='__main__':<NewLine>    <NewLine>    parser = argparse.ArgumentParser()<NewLine>    <NewLine>    parser.add_argument('-n', '--nodes', default=1,<NewLine>                        type=int, metavar='N')<NewLine>    parser.add_argument('-g', '--gpus', default=1, type=int,<NewLine>                        help='number of gpus per node')<NewLine>    <NewLine>    parser.add_argument('-gi', '--gpui', default=3, type=int,<NewLine>                        help='the index of gpu')<NewLine>    <NewLine>    parser.add_argument('-nr', '--nr', default=0, type=int,<NewLine>                        help='ranking within the nodes')<NewLine>    <NewLine>    parser.add_argument('--epochs', default=2, type=int, <NewLine>                        metavar='N',<NewLine>                        help='number of total epochs to run')<NewLine>    args = parser.parse_args()<NewLine>    #########################################################<NewLine>    args.world_size = args.gpus * args.nodes                #  it is equal to the total number of gpus, because we use each gpu per node <NewLine>    os.environ['MASTER_ADDR'] = '172.20.24.55'              #  it tells which IP address it should look for process 0<NewLine>    os.environ['MASTER_PORT'] = '8890'                      #<NewLine>    mp.spawn(train,args=(args,),nprocs=args.world_size)         #<NewLine></code></pre><NewLine><p>I got the following error,</p><NewLine><pre><code class=""lang-auto"">--&gt; 125     mp.spawn(train,args=(args,),nprocs=args.world_size)         #<NewLine>    126     #########################################################<NewLine>    127 <NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in spawn(fn, args, nprocs, join, daemon, start_method)<NewLine>    198                ' torch.multiprocessing.start_process(...)' % start_method)<NewLine>    199         warnings.warn(msg)<NewLine>--&gt; 200     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')<NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in start_processes(fn, args, nprocs, join, daemon, start_method)<NewLine>    147             daemon=daemon,<NewLine>    148         )<NewLine>--&gt; 149         process.start()<NewLine>    150         error_queues.append(error_queue)<NewLine>    151         processes.append(process)<NewLine><NewLine>~/anaconda3/lib/python3.7/multiprocessing/process.py in start(self)<NewLine>    110                'daemonic processes are not allowed to have children'<NewLine>    111         _cleanup()<NewLine>--&gt; 112         self._popen = self._Popen(self)<NewLine>    113         self._sentinel = self._popen.sentinel<NewLine>    114         # Avoid a refcycle if the target function holds an indirect<NewLine><NewLine>~/anaconda3/lib/python3.7/multiprocessing/context.py in _Popen(process_obj)<NewLine>    282         def _Popen(process_obj):<NewLine>    283             from .popen_spawn_posix import Popen<NewLine>--&gt; 284             return Popen(process_obj)<NewLine>    285 <NewLine>    286     class ForkServerProcess(process.BaseProcess):<NewLine><NewLine>~/anaconda3/lib/python3.7/multiprocessing/popen_spawn_posix.py in __init__(self, process_obj)<NewLine>     30     def __init__(self, process_obj):<NewLine>     31         self._fds = []<NewLine>---&gt; 32         super().__init__(process_obj)<NewLine>     33 <NewLine>     34     def duplicate_for_child(self, fd):<NewLine><NewLine>~/anaconda3/lib/python3.7/multiprocessing/popen_fork.py in __init__(self, process_obj)<NewLine>     18         self.returncode = None<NewLine>     19         self.finalizer = None<NewLine>---&gt; 20         self._launch(process_obj)<NewLine>     21 <NewLine>     22     def duplicate_for_child(self, fd):<NewLine><NewLine>~/anaconda3/lib/python3.7/multiprocessing/popen_spawn_posix.py in _launch(self, process_obj)<NewLine>     40         tracker_fd = semaphore_tracker.getfd()<NewLine>     41         self._fds.append(tracker_fd)<NewLine>---&gt; 42         prep_data = spawn.get_preparation_data(process_obj._name)<NewLine>     43         fp = io.BytesIO()<NewLine>     44         set_spawning_popen(self)<NewLine><NewLine>~/anaconda3/lib/python3.7/multiprocessing/spawn.py in get_preparation_data(name)<NewLine>    170     # or through direct execution (or to leave it alone entirely)<NewLine>    171     main_module = sys.modules['__main__']<NewLine>--&gt; 172     main_mod_name = getattr(main_module.__spec__, ""name"", None)<NewLine>    173     if main_mod_name is not None:<NewLine>    174         d['init_main_from_name'] = main_mod_name<NewLine><NewLine>AttributeError: module '__main__' has no attribute '__spec__'<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/887574002,(Mjavan),887574002,"August 10, 2020, 10:54am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>One error I noticed is that, when using <a href=""https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn"" rel=""nofollow noopener""><code>spawn</code></a>, it will pass the <code>rank</code> as the first argument to the target function, followed by the args you provided. So the signature of the <code>train</code> function should be <code>train(rank, args)</code>.</p><NewLine><p>But above does not seem to be the cause of the logged error. That error does not seem to be PyTorch related, see this discussion: <a href=""https://stackoverflow.com/questions/45720153/python-multiprocessing-error-attributeerror-module-main-has-no-attribute"" rel=""nofollow noopener"">https://stackoverflow.com/questions/45720153/python-multiprocessing-error-attributeerror-module-main-has-no-attribute</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your answer. What about the following code? I tried it in another way. And I found the below error.</p><NewLine><pre><code class=""lang-auto""><NewLine>class Model(nn.Module):<NewLine>    # Our model<NewLine>    def __init__(self):<NewLine>        super(Model, self).__init__()<NewLine>        self.fc1 = nn.Conv2d(1,10,3)<NewLine>        self.bn1 = nn.BatchNorm2d(10)<NewLine>        self.fc2= nn.Conv2d(10,20,3)<NewLine>        self.bn2 = nn.BatchNorm2d(20)<NewLine>        self.fc3= nn.Linear(11520,10)<NewLine>        <NewLine>    def forward(self,x):<NewLine>        print(f'inout_size: {x.size()}')<NewLine>        x = F.relu(self.fc1(x))<NewLine>        x = self.bn1(x)<NewLine>        x = F.relu(self.fc2(x))<NewLine>        x = self.bn2(x)<NewLine>        x = x.view(x.size(0),-1)<NewLine>        x = self.fc3(x)<NewLine>        print(f'output_size: {x.size()}')<NewLine>        return(x)<NewLine>########################################    <NewLine><NewLine><NewLine><NewLine>def train(gpu):<NewLine>    rank = gpu<NewLine>    <NewLine>    dist.init_process_group(backed = 'nccl',<NewLine>                           init_method = 'env://',<NewLine>                           world_size= 4,<NewLine>                           rank=rank)<NewLine>    <NewLine>    torch.manual_seed(0)<NewLine>    <NewLine>    model = Model()<NewLine>    <NewLine>    torch.cuda.set_device(gpu)<NewLine>    model= model.to(device)<NewLine>    optimizer = optim.Adam(model.parameters(),lr=0.1)<NewLine>    lr_sch = lr_scheduler.StepLR(optimizer,step_size=2,gamma=0.1)<NewLine>    criterion = nn.CrossEntropyLoss().to(device)<NewLine>    <NewLine>    ######################################<NewLine>    model = nn.DistributedDataParallel(model, device_ids = [gpu])<NewLine>    #####################################<NewLine><NewLine>    <NewLine>    mnist =torchvision.datasets.MNIST('./data',train= True,download=True,<NewLine>                                      transform =transforms.ToTensor())<NewLine>    ####################################<NewLine>    train_sampler = torch.utils.data.distributed.DistributedSampler(mnist,<NewLine>                                                                    num_replicas=4,<NewLine>                                                                    rank = rank)<NewLine>    <NewLine>    ###################################<NewLine>    <NewLine>    dataloader = DataLoader(mnist,batch_size=32,num_workers =4,pin_memory=True,<NewLine>                                               sampler = train_sampler)<NewLine>    <NewLine>    #####################################<NewLine>    <NewLine>    <NewLine>    for epoch in range(10):<NewLine>        total_loss =0<NewLine>        for X,y in dataloader:   <NewLine>            X= X.to(device)<NewLine>            y = y.long().to(device)<NewLine>            pred = model(X)<NewLine>            loss = criterion(pred,y)<NewLine>            t_loss+= loss.item()<NewLine>            optimizer.zero_grad()<NewLine>            loss.backward()<NewLine>            optimizer.step()  <NewLine>        print(f'Loss: {t_loss/len(dataloader)}')<NewLine>     def main():<NewLine>      os.environ['MASTER_ADDR'] = '172.20.24.55' ### the IP of vm_gpu02<NewLine>      os.environ['MASTER_PORT'] = '9000'<NewLine>      mp.spawn(train,nprocs=4)<NewLine><NewLine>if __name__=='__main__':<NewLine>      main()<NewLine><NewLine>process 3 terminated with exit code 1<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Which line threw the error? Could you please paste the trace as well?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""92273"" data-username=""887574002""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/8/8dc957/40.png"" width=""20""/> 887574002:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">    torch.cuda.set_device(gpu)<NewLine>    model= model.to(device)<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>I might miss sth, but looks like the <code>device</code> var is undefined? Did you mean <code>gpu</code> instead?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh, yes. Sorry, I was running several codes today simultaneously. that’s why I didn’t notice this mistake.</p><NewLine><p>Hey, I corrected the mistake, it wasn’t the problem though. Here you can see the traceback error. Actually I want to run the model on 4 gpus and I declared it as <code>nproc=4</code>, but I do not know if I should add something else to my code or not? At the moment it only reads one gpu.</p><NewLine><pre><code class=""lang-auto"">class Model(nn.Module):<NewLine>    # Our model<NewLine>    def __init__(self):<NewLine>        super(Model, self).__init__()<NewLine>        self.fc1 = nn.Conv2d(1,10,3)<NewLine>        self.bn1 = nn.BatchNorm2d(10)<NewLine>        self.fc2= nn.Conv2d(10,20,3)<NewLine>        self.bn2 = nn.BatchNorm2d(20)<NewLine>        self.fc3= nn.Linear(11520,10) <NewLine>    def forward(self,x):<NewLine>        print(f'inout_size: {x.size()}')<NewLine>        x = F.relu(self.fc1(x))<NewLine>        x = self.bn1(x)<NewLine>        x = F.relu(self.fc2(x))<NewLine>        x = self.bn2(x)<NewLine>        x = x.view(x.size(0),-1)<NewLine>        x = self.fc3(x)<NewLine>        print(f'output_size: {x.size()}')<NewLine>        return(x)<NewLine>########################################    <NewLine>def train(gpu):<NewLine>    rank = gpu <NewLine>    dist.init_process_group(backed = 'nccl',<NewLine>                           init_method = 'env://',<NewLine>                           world_size= 4,<NewLine>                           rank=rank)<NewLine>    torch.manual_seed(0)<NewLine>    model = Model()  <NewLine>    torch.cuda.set_device(gpu)<NewLine>    model= model.to(gpu)<NewLine>    optimizer = optim.Adam(model.parameters(),lr=0.1)<NewLine>    lr_sch = lr_scheduler.StepLR(optimizer,step_size=2,gamma=0.1)<NewLine>    criterion = nn.CrossEntropyLoss().to(gpu)<NewLine>    ######################################<NewLine>    model = nn.DistributedDataParallel(model, device_ids = [gpu])<NewLine>    #####################################<NewLine>    mnist =torchvision.datasets.MNIST('./data',train= True,download=True,<NewLine>                                      transform =transforms.ToTensor())<NewLine>    ####################################<NewLine>    train_sampler = torch.utils.data.distributed.DistributedSampler(mnist,<NewLine>                                                                    num_replicas=4,<NewLine>                                                                    rank = rank)<NewLine>    ###################################<NewLine>    dataloader = DataLoader(mnist,batch_size=32,num_workers =4,pin_memory=True,<NewLine>                                               sampler = train_sampler)<NewLine>    #####################################<NewLine>    for epoch in range(10):<NewLine>        total_loss =0<NewLine>        for X,y in dataloader:   <NewLine>            X= X.to(gpu)<NewLine>            y = y.long().to(gpu)<NewLine>            pred = model(X)<NewLine>            loss = criterion(pred,y)<NewLine>            t_loss+= loss.item()<NewLine>            optimizer.zero_grad()<NewLine>            loss.backward()<NewLine>            optimizer.step()<NewLine>        print(f'Loss: {t_loss/len(dataloader)}')<NewLine>def main():<NewLine>    os.environ['MASTER_ADDR'] = '172.20.24.55' ### the IP of vm_gpu02<NewLine>    os.environ['MASTER_PORT'] = '9000'<NewLine>    mp.spawn(train,nprocs=4)<NewLine>if __name__=='__main__': <NewLine>    main()<NewLine><NewLine>Exception                                 Traceback (most recent call last)<NewLine>&lt;ipython-input-10-e18ebd33df91&gt; in &lt;module&gt;<NewLine>      1 if __name__=='__main__':<NewLine>      2 <NewLine>----&gt; 3     main()<NewLine><NewLine>&lt;ipython-input-9-331de420a7b8&gt; in main()<NewLine>      5     os.environ['MASTER_PORT'] = '9000'<NewLine>      6 <NewLine>----&gt; 7     mp.spawn(train,nprocs=4)<NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in spawn(fn, args, nprocs, join, daemon, start_method)<NewLine>    198                ' torch.multiprocessing.start_process(...)' % start_method)<NewLine>    199         warnings.warn(msg)<NewLine>--&gt; 200     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')<NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in start_processes(fn, args, nprocs, join, daemon, start_method)<NewLine>    156 <NewLine>    157     # Loop on join until it returns True or raises an exception.<NewLine>--&gt; 158     while not context.join():<NewLine>    159         pass<NewLine>    160 <NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in join(self, timeout)<NewLine>    111                 raise Exception(<NewLine>    112                     ""process %d terminated with exit code %d"" %<NewLine>--&gt; 113                     (error_index, exitcode)<NewLine>    114                 )<NewLine>    115 <NewLine><NewLine>Exception: process 2 terminated with exit code 1<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Found a few errors when debugging this locally:</p><NewLine><ol><NewLine><li>in DDP ctor, the arg name is <code>backend</code> instead of <code>backed</code><NewLine></li><NewLine><li>DDP is from <code>torch.nn.parallel</code> package instead of <code>torch.nn</code><NewLine></li><NewLine><li><NewLine><code>t_loss</code> is used before definition.</li><NewLine></ol><NewLine><p>The following code works for me. I tried it on 2 GPUs, as I only have 2 in my dev env. Some general suggestion for debugging: 1) it will be helpful if you can locate which line threw the error, 2) it will be easier to debug if you start from a simpler version and gradually add complexity to the code.</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import os<NewLine>import torch.multiprocessing as mp<NewLine>import torch.distributed as dist<NewLine>import torch.optim as optim<NewLine>import torch.optim.lr_scheduler as lr_scheduler<NewLine>import torchvision<NewLine>import torchvision.transforms as transforms<NewLine>from torch.utils.data import DataLoader<NewLine><NewLine>class Model(nn.Module):<NewLine>    # Our model<NewLine>    def __init__(self):<NewLine>        super(Model, self).__init__()<NewLine>        self.fc1 = nn.Conv2d(1,10,3)<NewLine>        self.bn1 = nn.BatchNorm2d(10)<NewLine>        self.fc2= nn.Conv2d(10,20,3)<NewLine>        self.bn2 = nn.BatchNorm2d(20)<NewLine>        self.fc3= nn.Linear(11520,10)<NewLine>    def forward(self,x):<NewLine>        print(f'inout_size: {x.size()}')<NewLine>        x = F.relu(self.fc1(x))<NewLine>        x = self.bn1(x)<NewLine>        x = F.relu(self.fc2(x))<NewLine>        x = self.bn2(x)<NewLine>        x = x.view(x.size(0),-1)<NewLine>        x = self.fc3(x)<NewLine>        print(f'output_size: {x.size()}')<NewLine>        return(x)<NewLine>########################################<NewLine>def train(gpu):<NewLine>    print(""1111"")<NewLine>    rank = gpu<NewLine>    dist.init_process_group(backend = 'nccl',<NewLine>                           init_method = 'env://',<NewLine>                           world_size= 2,<NewLine>                           rank=rank)<NewLine>    print(""2222"")<NewLine>    torch.manual_seed(0)<NewLine>    model = Model()<NewLine>    torch.cuda.set_device(gpu)<NewLine>    model= model.to(gpu)<NewLine>    optimizer = optim.Adam(model.parameters(),lr=0.1)<NewLine>    lr_sch = lr_scheduler.StepLR(optimizer,step_size=2,gamma=0.1)<NewLine>    criterion = nn.CrossEntropyLoss().to(gpu)<NewLine>    ######################################<NewLine>    model = nn.parallel.DistributedDataParallel(model, device_ids = [gpu])<NewLine>    #####################################<NewLine>    mnist =torchvision.datasets.MNIST('./data',train= True,download=True,<NewLine>                                      transform =transforms.ToTensor())<NewLine>    ####################################<NewLine>    train_sampler = torch.utils.data.distributed.DistributedSampler(mnist,<NewLine>                                                                    num_replicas=4,<NewLine>                                                                    rank = rank)<NewLine>    ###################################<NewLine>    dataloader = DataLoader(mnist,batch_size=32,num_workers =4,pin_memory=True,<NewLine>                                               sampler = train_sampler)<NewLine>    #####################################<NewLine>    t_loss = None<NewLine>    for epoch in range(2):<NewLine>        total_loss =0<NewLine>        for X,y in dataloader:<NewLine>            X= X.to(gpu)<NewLine>            y = y.long().to(gpu)<NewLine>            pred = model(X)<NewLine>            loss = criterion(pred,y)<NewLine>            t_loss= loss.item() if t_loss is None else t_loss + loss.item()<NewLine>            optimizer.zero_grad()<NewLine>            loss.backward()<NewLine>            optimizer.step()<NewLine>        print(f'Loss: {t_loss/len(dataloader)}')<NewLine><NewLine>def main():<NewLine>    os.environ['MASTER_ADDR'] = 'localhost' ### the IP of vm_gpu02<NewLine>    os.environ['MASTER_PORT'] = '9000'<NewLine>    mp.spawn(train,nprocs=2)<NewLine>if __name__=='__main__':<NewLine>    main()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, thank you for your answer. Regarding your code, when we run the code isn’t it needed to give the gpu index? How dose coed understand on which gpus should run the process.</p><NewLine><p>And my second question is a bout <code>num_replicas</code>, shouldn’t it be equal to num_process?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""8"" data-topic=""92273"" data-username=""887574002""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/8/8dc957/40.png"" width=""20""/> 887574002:</div><NewLine><blockquote><NewLine><p>Hi, thank you for your answer. Regarding your code, when we run the code isn’t it needed to give the gpu index? How dose coed understand on which gpus should run the process.</p><NewLine></blockquote><NewLine></aside><NewLine><p>A process can access any visible GPU. The one-process-per-GPU requirement is from DDP to avoid NCCL comm hang. Ideally, we should set <code>CUDA_VISIBLE_DEVICES</code> for each process accordingly, so that each process only sees one GPU and <code>cuda:0</code> on each process points to a different GPU. But if you are confident that no code would accidentally access a different GPU, directly doing <code>.to(gpu)</code> would be sufficient. We are using the id of the process provided by <code>mp.spawn</code> as the gpu id.</p><NewLine><blockquote><NewLine><p>And my second question is a bout  <code>num_replicas</code> , shouldn’t it be equal to num_process?</p><NewLine></blockquote><NewLine><p>Yep, you are right.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/887574002; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/887574002; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/887574002; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 10, 2020,  2:48pm; <NewLine> REPLY_DATE 2: August 10, 2020,  3:17pm; <NewLine> REPLY_DATE 3: August 10, 2020,  3:26pm; <NewLine> REPLY_DATE 4: August 10, 2020,  3:27pm; <NewLine> REPLY_DATE 5: August 10, 2020,  5:29pm; <NewLine> REPLY_DATE 6: August 10, 2020,  6:59pm; <NewLine> REPLY_DATE 7: August 11, 2020,  4:42am; <NewLine> REPLY_DATE 8: August 11, 2020,  2:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
92221,Why passing data to subprocess caused model block?,2020-08-10T03:42:18.261Z,7,71,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, when I want to get data from dataloader and  pass it to subprocess, my model and subprocess will block.  But in subprocess create dataloader and get data, model will work normally.</p><NewLine><p>Following code will block</p><NewLine><pre><code class=""lang-auto"">def func(net, d):<NewLine>    out = net(d)<NewLine><NewLine>if __name__ == '__main__':<NewLine>    net = Net(input_w=28*28, width=64, n_layer=3, output_w=10) #dense network<NewLine>    trainloader = get_data() #get trainloader<NewLine>    data, label = iter(trainloader).next()<NewLine>    data = data.view(data.size(0), -1)<NewLine>    with Pool() as pool:      <NewLine>        pool.starmap(func, [(net, data)])<NewLine></code></pre><NewLine><p>Following code works normally</p><NewLine><pre><code class=""lang-auto"">def func(net):<NewLine>    trainloader = get_data() #get trainloader<NewLine>    data, label = iter(trainloader).next()<NewLine>    data = data.view(data.size(0), -1)<NewLine>    out = net(data)<NewLine><NewLine>if __name__ == '__main__':<NewLine>    net = Net(input_w=28*28, width=64, n_layer=3, output_w=10) #dense network<NewLine>    with Pool() as pool:      <NewLine>        pool.starmap(func, [(net, )])<NewLine></code></pre><NewLine><p>I don’t know what reason caused this problem. Thanks everyone.</p><NewLine></div>",https://discuss.pytorch.org/u/chhuang,,chhuang,"August 10, 2020,  3:42am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Does it work if the <code>data</code> tensor is not from <code>DataLoader</code>. Say what if you create the tensor using <code>torch.zeros</code>? Does it still hang?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not working. If I create torch.zeros in func(), it can work. But create in “if <strong>name</strong> == ‘<strong>main</strong>’:” then pass to func(), it hangs.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>hmm, the following code works for me locally. Can you try this in your dev env?</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>from torch.multiprocessing import Pool<NewLine><NewLine>def func(net, d):<NewLine>    out = net(d)<NewLine>    print(out)<NewLine><NewLine>if __name__ == '__main__':<NewLine>    net = torch.nn.Linear(2, 2)<NewLine>    data = torch.zeros(2, 2)<NewLine>    with Pool() as pool:<NewLine>        pool.starmap(func, [(net, data)])<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried your code then I found a strange problem!! If I pass <code>torch.zeros(32, 28*28)</code>, it can work. But!! If passing <code>(64, 28*28)</code>, it hangs. This problem doesn’t happen on my macbook, But happened on Linux PC.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could it be the machine/container has been configured to use a very small shm size?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Your suggestion is helpful for me! I’ll try to adjust shm size. If succed, I’ll reply. Thank you very much.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I typed <code>df -h</code> in terminal and content showed as following :<br/><NewLine>`Filesystem  Size  Used Avail Use% Mounted on’</p><NewLine><p>tmpfs           7.9G   30M  7.9G   1% /dev/shm`</p><NewLine><p>The <code>/dev/shm</code> has 7.9G, this space is big enough for using.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/colesbury"">@colesbury</a> do you know what could cause this error?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/chhuang; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/chhuang; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/chhuang; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/chhuang; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 10, 2020,  3:07pm; <NewLine> REPLY_DATE 2: August 11, 2020,  1:03am; <NewLine> REPLY_DATE 3: August 11, 2020,  1:11am; <NewLine> REPLY_DATE 4: August 11, 2020,  1:48am; <NewLine> REPLY_DATE 5: August 11, 2020,  1:49am; <NewLine> REPLY_DATE 6: August 11, 2020,  2:03am; <NewLine> REPLY_DATE 7: August 11, 2020,  3:03am; <NewLine> REPLY_DATE 8: August 11, 2020,  2:05pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
92285,Why are all threads bound to the same core?,2020-08-10T13:27:35.223Z,1,80,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to use pytorch DDP module to do the distributed training and I use the OpenBLAS as the BLAS. When I execute the following benchmark</p><NewLine><pre><code class=""lang-auto"">import timeit<NewLine>runtimes = []<NewLine>threads = [1] + [t for t in range(2, 49, 2)]<NewLine>for t in threads:<NewLine>    torch.set_num_threads(t)<NewLine>    r = timeit.timeit(setup = ""import torch; x = torch.randn(1024, 1024); y = torch.randn(1024, 1024)"", stmt=""torch.mm(x, y)"", number=100)<NewLine>    runtimes.append(r)<NewLine></code></pre><NewLine><p>I found that different threads were running on different cores.<br/><NewLine>However when I execute my training script, I found all threads are bound to the same core.<br/><NewLine>script:</p><NewLine><pre><code class=""lang-auto"">export GLOO_SOCKET_IFNAME=ib0<NewLine>export NUM_CORES=64<NewLine>export OMP_NUM_THREADS=$NUM_CORES<NewLine>NPROC_PER_NODE=1<NewLine>COMMAND=""$HOME/deepnet_mpi/CosmoFlow.py --epochs=120 --backend=gloo --workers=0 --batch-size=1 --print-freq=50 --data=$HOME/Nbody/datasets/v6""<NewLine><NewLine>python3 -m torch.distributed.launch \<NewLine>--nproc_per_node=$NPROC_PER_NODE \<NewLine>$COMMAND<NewLine></code></pre><NewLine><p>What is the reason of this problem? And this is my environment:</p><NewLine><pre><code class=""lang-auto"">Collecting environment information...<NewLine>PyTorch version: 1.6.0a0+b31f58d<NewLine>Is debug build: No<NewLine>CUDA used to build PyTorch: None<NewLine><NewLine>OS: CentOS Linux release 7.6.1810 (AltArch)<NewLine>GCC version: (GCC) 9.2.0<NewLine>CMake version: version 3.16.5<NewLine><NewLine>Python version: 3.7<NewLine>Is CUDA available: No<NewLine>CUDA runtime version: No CUDA<NewLine>GPU models and configuration: No CUDA<NewLine>Nvidia driver version: No CUDA<NewLine>cuDNN version: No CUDA<NewLine><NewLine>Versions of relevant libraries:<NewLine>[pip3] numpy==1.19.1<NewLine>[pip3] torch==1.6.0a0+b31f58d<NewLine>[conda] Could not collect<NewLine></code></pre><NewLine><p>I found that when multiple processes are set up, different processes also use the same CPU core.</p><NewLine></div>",https://discuss.pytorch.org/u/khalil,(khalil li),khalil,"August 11, 2020,  1:48am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>hmm, I am not aware of any DDP code that would change the threading behavior.</p><NewLine><p>cc <a class=""mention"" href=""/u/vitalyfedyunin"">@VitalyFedyunin</a> do you know what might lead to this behavior?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply, now I have solved this problem. The reason is that I do not set an openmp environment variable</p><NewLine><pre><code class=""lang-auto"">export GOMP_CPU_AFFINITY=0-127<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/khalil; <NewLine> ,"REPLY_DATE 1: August 10, 2020,  8:12pm; <NewLine> REPLY_DATE 2: August 11, 2020,  4:35am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> 
92266,Handling oom error during DDP backward,2020-08-10T09:54:06.068Z,2,82,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m wondering how to deal with occasional OOM error happend during DDP backward.</p><NewLine><p>For <code>forward</code>, oom can be captured simply by a try-catch statement. For backward, however, <code>loss.backward()</code> performs gradient calculation and the registered hooks perform gradient reduction at the same time.</p><NewLine><p>Is it possible to hang due to oom errors during backward in several process so that the other successful processes keep waiting for them? If so, is there a nice way to recover from this problem?</p><NewLine></div>",https://discuss.pytorch.org/u/T_Qri,(T Qri),T_Qri,"August 10, 2020,  9:54am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""92266"" data-username=""T_Qri""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/t_qri/40/24891_2.png"" width=""20""/> T_Qri:</div><NewLine><blockquote><NewLine><p>Is it possible to hang due to oom errors during backward in several process so that the other successful processes keep waiting for them?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, it is. If one process hit OOM and skipped/reran the the backward pass, it would cause de-synchronization across processes in the same group, which would lead to hang or crash.</p><NewLine><blockquote><NewLine><p>If so, is there a nice way to recover from this problem?</p><NewLine></blockquote><NewLine><p>Yep, <a href=""https://pytorch.org/elastic"" rel=""nofollow noopener""><code>TorchElastic</code></a> is built to solve this issue. cc <a class=""mention"" href=""/u/kiuk_chung"">@Kiuk_Chung</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have a look here at:</p><NewLine><ol><NewLine><li><NewLine><a href=""https://pytorch.org/elastic/0.2.0/train_script.html"" rel=""nofollow noopener"">https://pytorch.org/elastic/0.2.0/train_script.html</a>  - for instructions on how to write a “torchelastic compliant” train script</li><NewLine><li><NewLine><a href=""https://pytorch.org/elastic/0.2.0/quickstart.html"" rel=""nofollow noopener"">https://pytorch.org/elastic/0.2.0/quickstart.html</a> - for a quickstart on launching your script with torchelastic</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you, I will try it out.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you, I will have a try.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Kiuk_Chung; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/T_Qri; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/T_Qri; <NewLine> ,"REPLY_DATE 1: August 11, 2020,  2:21am; <NewLine> REPLY_DATE 2: August 10, 2020, 11:58pm; <NewLine> REPLY_DATE 3: August 11, 2020,  2:21am; <NewLine> REPLY_DATE 4: August 11, 2020,  2:21am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
92058,Error in function - signal experiment,2020-08-08T07:15:27.314Z,3,107,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all, apologies in advacne it is my first post. i am trying an experiment with new data and i getting an error i was wondering if somebody could please assist.</p><NewLine><pre><code class=""lang-python"">import math<NewLine><NewLine>import cvxpy as cp<NewLine>import matplotlib.pyplot as plt<NewLine>import numpy as np<NewLine>import pandas as pd<NewLine>import torch<NewLine>from cvxpylayers.torch import CvxpyLayer<NewLine><NewLine>import latexify<NewLine>latexify.latexify()<NewLine><NewLine>torch.set_default_tensor_type(torch.DoubleTensor)<NewLine>%matplotlib inline<NewLine><NewLine><NewLine>N_train=1000<NewLine><NewLine><NewLine><NewLine><NewLine>test=yf.download(""SPY"", start=""2012-01-01"", end=""2017-04-30"")['Close'].to_numpy()<NewLine>outputs=torch.from_numpy(test)<NewLine>inputs=np.linspace(1,100,len(outputs))<NewLine>inputs=torch.from_numpy(inputs)<NewLine><NewLine>X_train = inputs[:N_train]<NewLine>Y_train = outputs[:N_train]<NewLine><NewLine>X_val = inputs[N_train:]<NewLine>Y_val = outputs[N_train:]<NewLine><NewLine><NewLine>len(X_val)<NewLine>len(Y_val)<NewLine><NewLine>def create_layer():<NewLine>   y_cp = cp.Variable(n)<NewLine>   x_minus_y = cp.Variable(n)<NewLine>   <NewLine>   x_param = cp.Parameter(n)<NewLine>   theta_param = cp.Parameter((n, n))<NewLine>   lambda_param = cp.Parameter(pos=True)<NewLine>   objective = (<NewLine>       cp.sum_squares(theta_param @ x_minus_y) +<NewLine>       lambda_param*cp.sum_squares(cp.diff(y_cp))<NewLine>   )<NewLine>   constraints = [<NewLine>       x_minus_y == x_param - y_cp<NewLine>   ]<NewLine>   problem = cp.Problem(cp.Minimize(objective), constraints)<NewLine>   layer = CvxpyLayer(<NewLine>       problem,<NewLine>       parameters=[x_param, theta_param, lambda_param],<NewLine>       variables=[y_cp])<NewLine>   return layer<NewLine>   <NewLine><NewLine>layer = create_layer()<NewLine><NewLine>import torch<NewLine>from torch.utils.data import TensorDataset, DataLoader<NewLine>import numpy as np<NewLine>from cvxpylayers.torch import CvxpyLayer<NewLine><NewLine>torch.set_default_dtype(torch.double)<NewLine><NewLine>from tqdm.notebook import tqdm<NewLine><NewLine><NewLine>def fit(loss, params, X, Y, Xval, Yval, batch_size=128, lr=1e-3, epochs=100, verbose=False, print_every=1, callback=None):<NewLine>   """"""<NewLine><NewLine>   Arguments:<NewLine>       loss: given x and y in batched form, evaluates loss.<NewLine>       params: list of parameters to optimize.<NewLine>       X: input data, torch tensor.<NewLine>       Y: output data, torch tensor.<NewLine>       Xval: input validation data, torch tensor.<NewLine>       Yval: output validation data, torch tensor.<NewLine>   """"""<NewLine><NewLine>   train_dset = TensorDataset(X, Y)<NewLine>   train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)<NewLine>   opt = torch.optim.Adam(params, lr=lr)<NewLine><NewLine>   train_losses = []<NewLine>   val_losses = []<NewLine>   for epoch in tqdm(range(epochs)):<NewLine>       if callback is not None:<NewLine>           callback()<NewLine>           <NewLine>       with torch.no_grad():<NewLine>           val_losses.append(loss(Xval, Yval).item())<NewLine>       if verbose and epoch % print_every == 0:<NewLine>           print(""val loss %03d | %3.5f"" % (epoch + 1, val_losses[-1]))<NewLine><NewLine>       batch = 1<NewLine>       train_losses.append([])<NewLine>       for Xbatch, Ybatch in train_loader:<NewLine>           opt.zero_grad()<NewLine>           l = loss(Xbatch, Ybatch)<NewLine>           l.backward()<NewLine>           opt.step()<NewLine>           train_losses[-1].append(l.item())<NewLine>           if verbose and epoch % print_every == 0:<NewLine>               print(""batch %03d / %03d | %3.5f"" %<NewLine>                     (batch, len(train_loader), np.mean(train_losses[-1])))<NewLine>           batch += 1<NewLine>   return val_losses, train_losses<NewLine><NewLine>theta_tch = torch.eye(n, requires_grad=True)<NewLine>lambda_tch = torch.tensor(0.5, requires_grad=True)<NewLine>params = [theta_tch, lambda_tch]<NewLine><NewLine>def loss_fn(X, actual):<NewLine>   preds = layer(X, theta_tch, lambda_tch)[0]<NewLine>   mse_per_example = (preds - actual).pow(2).mean(axis=1)<NewLine>   return mse_per_example.mean()<NewLine><NewLine><NewLine>val_losses, train_losses =  fit(<NewLine>   loss_fn, params, X_train, Y_train, X_val, Y_val, lr=1e-2, batch_size=8,<NewLine>   epochs=15, verbose=True, print_every=1)<NewLine></code></pre><NewLine><p>The above is the code taken from - <a href=""https://github.com/cvxgrp/cvxpylayers/blob/master/examples/torch/signal_denoising.ipynb"" rel=""nofollow noopener"">https://github.com/cvxgrp/cvxpylayers/blob/master/examples/torch/signal_denoising.ipynb</a></p><NewLine><p>and the error i am getting</p><NewLine><pre><code class=""lang-python"">---------------------------------------------------------------------------<NewLine>ValueError                                Traceback (most recent call last)<NewLine>&lt;ipython-input-58-0b0fb50d4406&gt; in &lt;module&gt;<NewLine>----&gt; 1 val_losses, train_losses =  fit(<NewLine>     2     loss_fn, params, X_train, Y_train, X_val, Y_val, lr=1e-2, batch_size=8,<NewLine>     3     epochs=15, verbose=True, print_every=1)<NewLine><NewLine>&lt;ipython-input-56-f19c59cb9b44&gt; in fit(loss, params, X, Y, Xval, Yval, batch_size, lr, epochs, verbose, print_every, callback)<NewLine>    32 <NewLine>    33         with torch.no_grad():<NewLine>---&gt; 34             val_losses.append(loss(Xval, Yval).item())<NewLine>    35         if verbose and epoch % print_every == 0:<NewLine>    36             print(""val loss %03d | %3.5f"" % (epoch + 1, val_losses[-1]))<NewLine><NewLine>&lt;ipython-input-57-0aead751c22d&gt; in loss_fn(X, actual)<NewLine>     4 <NewLine>     5 def loss_fn(X, actual):<NewLine>----&gt; 6     preds = layer(X, theta_tch, lambda_tch)[0]<NewLine>     7     mse_per_example = (preds - actual).pow(2).mean(axis=1)<NewLine>     8     return mse_per_example.mean()<NewLine><NewLine>~/miniconda3/envs/myenv1/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)<NewLine>   720             result = self._slow_forward(*input, **kwargs)<NewLine>   721         else:<NewLine>--&gt; 722             result = self.forward(*input, **kwargs)<NewLine>   723         for hook in itertools.chain(<NewLine>   724                 _global_forward_hooks.values(),<NewLine><NewLine>~/cvxpylayers/cvxpylayers/torch/cvxpylayer.py in forward(self, solver_args, *params)<NewLine>   150             info=info,<NewLine>   151         )<NewLine>--&gt; 152         sol = f(*params)<NewLine>   153         self.info = info<NewLine>   154         return sol<NewLine><NewLine>~/cvxpylayers/cvxpylayers/torch/cvxpylayer.py in forward(ctx, *params)<NewLine>   224                 p_shape = p.shape if batch_size == 0 else p.shape[1:]<NewLine>   225                 if not np.all(p_shape == param_order[i].shape):<NewLine>--&gt; 226                     raise ValueError(<NewLine>   227                         ""Inconsistent parameter shapes passed in. ""<NewLine>   228                         ""Expected parameter {} to have non-batched shape of ""<NewLine><NewLine>ValueError: Inconsistent parameter shapes passed in. Expected parameter 0 to have non-batched shape of (100,) but got torch.Size([339]).<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Andrew_Czeizler1,(Andrew Czeizler),Andrew_Czeizler1,"August 8, 2020,  9:22am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Tensor shape incorrect, evidently:</p><NewLine><pre><code class=""lang-auto""> to have non-batched shape of (100,) but got torch.Size([339])<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>How do I reshape to provide the code with the correct format. <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> <a class=""mention"" href=""/u/smth"">@smth</a>  was wondering if you would be able to assist. Thank you in advance.<br/><NewLine>Andrew</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m unfortunately not familiar with <code>cvxpylayers</code>, but as <a class=""mention"" href=""/u/iffix"">@iffiX</a> mentioned, the input shape of <code>Xval</code> and/or <code>Yval</code> seems to be wrong.<br/><NewLine>Based on your code it seems tou are trying to pass these tensors directly to the <code>loss</code> function, i.e. without a <code>DataLoader</code>. Could the batch size be missing? If not, I would recommend to check the shapes and make sure which shapes are expected.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah Thank you <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> !! That was very helpful!! im still getting used to pytorch  :),<br/><NewLine>i just assumed i could pass the dataset straight in.</p><NewLine><p>The example is below works  but i am a little unsure about how to fit it with new data.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/cvxgrp/cvxpylayers/blob/master/examples/torch/signal_denoising.ipynb"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/cvxgrp/cvxpylayers/blob/master/examples/torch/signal_denoising.ipynb"" rel=""nofollow noopener"" target=""_blank"">cvxgrp/cvxpylayers/blob/master/examples/torch/signal_denoising.ipynb</a></h4><NewLine><pre><code class=""lang-ipynb"">{<NewLine> ""cells"": [<NewLine>  {<NewLine>   ""cell_type"": ""markdown"",<NewLine>   ""metadata"": {},<NewLine>   ""source"": [<NewLine>    ""# Signal denoising\n"",<NewLine>    ""\n"",<NewLine>    ""This notebook accompanies the paper [Learning Convex Optimization Models](https://web.stanford.edu/~boyd/papers/learning_copt_models.html).""<NewLine>   ]<NewLine>  },<NewLine>  {<NewLine>   ""cell_type"": ""code"",<NewLine>   ""execution_count"": 1,<NewLine>   ""metadata"": {},<NewLine>   ""outputs"": [],<NewLine>   ""source"": [<NewLine>    ""import math\n"",<NewLine>    ""\n"",<NewLine>    ""import cvxpy as cp\n"",<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/cvxgrp/cvxpylayers/blob/master/examples/torch/signal_denoising.ipynb"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Do you have any thoughts about how i could fit new data with the current example.</p><NewLine><p>Kind regards ,<br/><NewLine>Andrew</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>So my next plan was to try a few different ways to format the data correctly.<br/><NewLine>I have not tried this but please observe the below.</p><NewLine><pre><code class=""lang-python"">import yfinance as yf<NewLine>data = yf.download(""SPY"", start=""2008-01-01"", end=""2017-04-30"")['Close']<NewLine>dd=data.to_numpy()<NewLine>s=int(np.ceil(len(dd)/100))<NewLine>s<NewLine><NewLine>def strided_app(a, L, S ):  # Window len = L, Stride len/stepsize = S<NewLine>    nrows = ((a.size-L)//S)+1<NewLine>    n = a.strides[0]<NewLine>    return np.lib.stride_tricks.as_strided(a, shape=(nrows,L), strides=(S*n,n))<NewLine><NewLine>ddd= strided_app(dd, 100, s)<NewLine><NewLine><NewLine><NewLine>def f(x):<NewLine>    # return math.sqrt(x)<NewLine>    return torch.from_numpy(np.array(x))<NewLine><NewLine><NewLine>ffxx=list(map(f, ddd))<NewLine>torch.stack(ffxx)<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Based on the previous post it seems that your code is already working with another dataset and the error is raised, if you are trying to use a new dataset?<br/><NewLine>If that’s the case, could you print the shape of the input tensor as well as some intermediate tensors?</p><NewLine><p>Since your code is not executable, I can just speculate what might be wrong. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Andrew_Czeizler1; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Andrew_Czeizler1; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Andrew_Czeizler1; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: August 8, 2020,  2:27pm; <NewLine> REPLY_DATE 2: August 9, 2020,  2:14pm; <NewLine> REPLY_DATE 3: August 10, 2020,  4:17am; <NewLine> REPLY_DATE 4: August 10, 2020,  1:14pm; <NewLine> REPLY_DATE 5: August 10, 2020,  2:19pm; <NewLine> REPLY_DATE 6: August 10, 2020,  4:10pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
92194,Update the process group in torch.distributed created using dist.init_process_group,2020-08-09T21:22:47.763Z,0,69,"<div class=""post"" itemprop=""articleBody""><NewLine><p><strong>Goal</strong>: Distributed Training with <strong>Dynamic</strong> machine location, where worker’s device location can change.</p><NewLine><p><strong>For e.g.</strong> 4 Worker Parameter Server setting. Now, for first 2 epochs 2 workers are run on Machine 1, but after 2 epochs they are supposed to be run on Machine 2.</p><NewLine><p>I am assuming, since the worker’s machine change after 2 epochs, <code>dist.init_process_group()</code> needs to be initialized. However, reinitializing issues this error.</p><NewLine><p><code>RuntimeError: trying to initialize the default process group twice</code></p><NewLine><p>What’s the correct way to update ‘process_group()’ ?</p><NewLine><p>Solution Ideas:</p><NewLine><ol><NewLine><li>Is there anyway to delete the initialized process group? So that before re-initialization using <code>dist.init_process_group()</code> I can delete the prior process group, thus avoiding the issue.</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/adarsh-kr,(Adarsh Kr),adarsh-kr,"August 9, 2020,  9:52pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/adarsh-kr"">@adarsh-kr</a>,</p><NewLine><p>There is a <code>destroy_process_group</code> API to clear the default <code>ProcessGroup</code><br/><NewLine>instance. If you would like to create multiple <code>ProcessGroup</code> instances, you can do so using the <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.new_group"" rel=""nofollow noopener""><code>new_grou</code></a> API.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/05f00532f52883d29a08d96b2961042cc41573ab/torch/distributed/distributed_c10d.py#L530-L577"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/05f00532f52883d29a08d96b2961042cc41573ab/torch/distributed/distributed_c10d.py#L530-L577"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/05f00532f52883d29a08d96b2961042cc41573ab/torch/distributed/distributed_c10d.py#L530-L577</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""530"" style=""counter-reset: li-counter 529 ;""><NewLine><li>def destroy_process_group(group=group.WORLD):</li><NewLine><li>    """"""</li><NewLine><li>    Destroy a given process group, and deinitialize the distributed package</li><NewLine><li><NewLine></li><li>    Arguments:</li><NewLine><li>        group (ProcessGroup, optional): The process group to be destroyed, if</li><NewLine><li>                                        group.WORLD is given, all process</li><NewLine><li>                                        groups including the default one will</li><NewLine><li>                                        be destroyed.</li><NewLine><li>    """"""</li><NewLine><li>    global _pg_map</li><NewLine><li>    global _pg_names</li><NewLine><li>    global _pg_group_ranks</li><NewLine><li>    global _default_pg</li><NewLine><li>    global _default_pg_init_method</li><NewLine><li>    global _group_count</li><NewLine><li><NewLine></li><li>    if group == GroupMember.NON_GROUP_MEMBER:</li><NewLine><li>        return</li><NewLine><li><NewLine></li></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/05f00532f52883d29a08d96b2961042cc41573ab/torch/distributed/distributed_c10d.py#L530-L577"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 10, 2020,  2:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
91786,How to timeout all_reduce or prevent it from hangs,2020-08-06T01:15:53.514Z,5,159,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Anyone knows how to stop / terminate an all-reduce call properly when it doesn’t get reply from other processes?</p><NewLine><p>Just to explain my question, please see the sample code below. I have 4 processes divided into two sub-groups (group1 and group2), and a shared Queue with 5 elements. Each process will try to get one element from the queue in the while loop until the queue becomes empty. And inside the while loop, each process will do all_reduce with its “neighbor” in the same sub-group.<br/><NewLine>The problem is that when one of the process get the last element, and at the same time, the shared queue is empty and its “neighbor” process already exit the while loop, it will get hanged and waiting forever for the all_reduce reply. Is there any way to set timeout for all_reduce call? Or some other ways to solve this situation?</p><NewLine><p>Thanks. Please see codes attached below.</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>import torch.multiprocessing as mp<NewLine><NewLine>def run(rank, a, q):<NewLine>    dist_init_method = 'tcp://{master_ip}:{master_port}'.format(<NewLine>                master_ip='127.0.0.1', master_port='12346')<NewLine>    world_size = 4<NewLine>    torch.distributed.init_process_group(backend=""nccl"",<NewLine>                                        init_method=dist_init_method,<NewLine>                                        world_size=world_size,<NewLine>                                        rank=rank)<NewLine>    group1 = dist.new_group([0, 1])<NewLine>    group2 = dist.new_group([2, 3])<NewLine>    tensor = torch.ones(1)<NewLine>    device = torch.device('cuda', rank)<NewLine>    tensor = tensor.to(device)<NewLine>    <NewLine>    while not q.empty():<NewLine>        current_index = q.get()<NewLine>        print(f'Process {rank} current index is: {current_index}')<NewLine>        <NewLine>        if rank == 0 or rank == 1:<NewLine>            dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group1)<NewLine>        else:<NewLine>            dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group2)<NewLine>    <NewLine>    print('Rank ', rank, ' has data ', tensor[0])<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    a = 1<NewLine>    ctx = mp.get_context('spawn')<NewLine>    q = ctx.Queue()<NewLine>    for index in range(5):<NewLine>        q.put(index)<NewLine>    <NewLine>    mp.spawn(run, args=(a, q), nprocs=4)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Yi_Zhang,(Yi Zhang),Yi_Zhang,"August 6, 2020,  1:20am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/yi_zhang"">@Yi_Zhang</a>, you can set a timeout in <code>init_process_group</code>. For NCCL backend, it also requires setting <code>NCCL_BLOCKING_WAIT</code> env var to 1.</p><NewLine><p>More explanation can be found here <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group</a>. (search for <code>NCCL_BLOCKING_WAIT </code>)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>, thanks for your reply.<br/><NewLine>I tried with  “gloo”, it does terminate the process and throws Exceptions. But with “nccl”, I add the following line in the bash script to run the .py file,</p><NewLine><pre><code class=""lang-auto"">export NCCL_BLOCKING_WAIT=1<NewLine></code></pre><NewLine><p>However it doesn’t work.<br/><NewLine>Also tried with adding the following line in the .py file itself, but doesn’t work either.</p><NewLine><pre><code class=""lang-auto"">os.environ[""NCCL_BLOCKING_WAIT""] = ""1""<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/yi_zhang"">@Yi_Zhang</a>, did you set the env var within each spawned process (i.e., in <code>run</code> function) and before calling <code>init_process_group</code>?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>, yes, I did that for each process. Don’t understand where is the mistake. Here is the sample code:</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>import torch.multiprocessing as mp<NewLine>import time<NewLine>import datetime<NewLine><NewLine>def run(rank, a, q):<NewLine>   os.environ[""NCCL_BLOCKING_WAIT""] = ""1""<NewLine>   print('Rank ', rank, 'NCCL_BLOCKING_WAIT is: ', os.environ[""NCCL_BLOCKING_WAIT""])<NewLine>   dist_init_method = 'tcp://{master_ip}:{master_port}'.format(<NewLine>               master_ip='127.0.0.1', master_port='12346')<NewLine>   torch.distributed.init_process_group(backend=""nccl"",<NewLine>                                       init_method=dist_init_method,<NewLine>                                       timeout=datetime.timedelta(seconds=5),<NewLine>                                       world_size=4,<NewLine>                                       rank=rank)<NewLine>   group1 = dist.new_group([0, 1])<NewLine>   group2 = dist.new_group([2, 3])<NewLine>   tensor = torch.ones(1)<NewLine><NewLine>   device = torch.device('cuda', rank)<NewLine>   tensor = tensor.to(device)<NewLine><NewLine>   while not q.empty():<NewLine>       print('Rank ', rank, ' in the loop ')<NewLine>       current_index = q.get()<NewLine>       print(f'Process {rank} current index is: {current_index}')<NewLine>       try:<NewLine>           if rank == 0 or rank == 1:<NewLine>               dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group1)<NewLine>               print(f'Process {rank} all_reduce tensor is: {tensor}')<NewLine>           else:<NewLine>               dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group2)<NewLine>               print(f'Process {rank} all_reduce tensor is: {tensor}')<NewLine>       except Exception:<NewLine>           pass<NewLine>   print('Rank ', rank, ' has data ', tensor[0])<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>   a = 1<NewLine>   ctx = mp.get_context('spawn')<NewLine>   q = ctx.Queue()<NewLine>   flag = ctx.Queue()<NewLine>   for index in range(5):<NewLine>       q.put(index)<NewLine>   <NewLine>   mp.spawn(run, args=(a, q), nprocs=4)<NewLine></code></pre><NewLine><p>An update, I tried “gloo” without setting timeout, and it can terminate properly. I’m wondering maybe “gloo” takes care of this situation by itself? It doesn’t have something to do with the “timeout”?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I confirm that I can reproduce this locally.</p><NewLine><p>Hey <a class=""mention"" href=""/u/osalpekar"">@osalpekar</a>, do you know if we miss anything here? Would I be correct if I assume the following code is expected to abort the op in this case?</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/ecb88c5d11895a68e5f20917d27a0debbc0f0697/torch/lib/c10d/ProcessGroupNCCL.cpp#L301-L335"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/ecb88c5d11895a68e5f20917d27a0debbc0f0697/torch/lib/c10d/ProcessGroupNCCL.cpp#L301-L335"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/ecb88c5d11895a68e5f20917d27a0debbc0f0697/torch/lib/c10d/ProcessGroupNCCL.cpp#L301-L335</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""301"" style=""counter-reset: li-counter 300 ;""><NewLine><li>if (blockingWait_) {</li><NewLine><li>  // Use the passed in timeout if provided, otherwise use the default</li><NewLine><li>  // opTimeout for each WorkNCCL object.</li><NewLine><li>  std::chrono::milliseconds workTimeout =</li><NewLine><li>      timeout == kNoTimeout ? opTimeout_ : timeout;</li><NewLine><li>  // Wait for the operation to complete.</li><NewLine><li>  while (!isCompleted()) {</li><NewLine><li>    auto currentTimepoint = std::chrono::steady_clock::now();</li><NewLine><li>    if (std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(</li><NewLine><li>            currentTimepoint - workStartTime_) &gt; workTimeout) {</li><NewLine><li>      // When operation times out due to some errors that are not</li><NewLine><li>      // detected by nccl communicators, ncclCommWatchdog can not check this</li><NewLine><li>      // time out error and thus can not abort ncclComms accordingly.</li><NewLine><li>      // So explicitly abort ncclComms here before throwing this timed out</li><NewLine><li>      // exception to users, after this, ncclCommWatchdog can detect nccl</li><NewLine><li>      // communicators are aborted and clean up devNCCLCommMap_ accordingly.</li><NewLine><li>      // if throwing timed out excepiton without aborting nccl communicators</li><NewLine><li>      // here, it was observed that CUDA GPU will have 100% utilization and</li><NewLine><li>      // can not run new events successfully.</li><NewLine><li>      for (const auto&amp; ncclComm : ncclComms_) {</li><NewLine></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/ecb88c5d11895a68e5f20917d27a0debbc0f0697/torch/lib/c10d/ProcessGroupNCCL.cpp#L301-L335"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> - Yes that is the code block that should abort the op if it times out.</p><NewLine><p><a class=""mention"" href=""/u/yi_zhang"">@Yi_Zhang</a> - There is a workaround. The all_reduce call actually returns an async work handle. You can capture that handle and wait on it as such:</p><NewLine><pre><code class=""lang-auto"">work = dist.all_reduce(..., async_op=True)<NewLine>work.wait(SOME_TIMEOUT)<NewLine></code></pre><NewLine><p>If the all_reduce call times out, then the wait call will throw an exception.</p><NewLine><p>In the meantime, let me try to repro from your most recent code snippet.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> <a class=""mention"" href=""/u/osalpekar"">@osalpekar</a>,  thanks for your reply. I find another way to avoid this situation without using timeout. I just add some checks to make sure the pairs of processes terminate after same number of rounds. But I’m still curious to know if you have any answer for the  timeout issue. Thanks</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Yi_Zhang; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Yi_Zhang; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/osalpekar; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Yi_Zhang; <NewLine> ,"REPLY_DATE 1: August 6, 2020,  1:49am; <NewLine> REPLY_DATE 2: August 6, 2020,  6:39pm; <NewLine> REPLY_DATE 3: August 6, 2020,  7:05pm; <NewLine> REPLY_DATE 4: August 6, 2020,  9:55pm; <NewLine> REPLY_DATE 5: August 6, 2020, 10:20pm; <NewLine> REPLY_DATE 6: August 6, 2020, 11:08pm; <NewLine> REPLY_DATE 7: August 9, 2020,  7:27pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 2 Likes; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> 
91468,Optimizer.step() hangs on linux; multiprocesssing,2020-08-03T11:07:42.043Z,10,125,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Background:<br/><NewLine>I’m doing a distributed PPO (basically gathering data from several worker and training on one learner)</p><NewLine><p>Issue:<br/><NewLine>Data collection works fine but when I train the network with lines below</p><NewLine><pre><code>            self.critic.optimizer.zero_grad()<NewLine>            batch_states_values = self.critic.forward(batch_states)<NewLine>            print('crtitic batch_states_values done')<NewLine>            critic_loss = F.mse_loss(batch_states_values, batch_REFs)<NewLine>            print('crtitic critic_loss done')<NewLine>            critic_loss.backward()<NewLine>            print('crtitic loss backward done')<NewLine>            self.critic.optimizer.step()<NewLine>            print('crtitic step done')<NewLine></code></pre><NewLine><p>And the output shows:</p><NewLine><pre><code>crtitic batch_states_values done<NewLine>crtitic critic_loss done<NewLine>crtitic loss backward done<NewLine></code></pre><NewLine><p>So it appears to be that the program hangs after the loss backward. What could be the cause?<br/><NewLine>It works fine on my windows workstations but hangs when I run it on a linux machine</p><NewLine></div>",https://discuss.pytorch.org/u/Lewis_Liu,(Lewis Liu),Lewis_Liu,"August 3, 2020, 11:16am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/lewis_liu"">@Lewis_Liu</a>, which part of the program is distributed? Since <code>torch.distributed</code> does not support Windows yet, I assume the working version of the program on Windows does not use distributed training?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>BTW, which optimizer are you using?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The training isn’t distributed and <code>torch.distributed</code> isn’t used.</p><NewLine><p>By distributed I mean the workers used to collect data are distributed and the network params are send from trainer to these workers through mp.queue.</p><NewLine><p>Once the data are collected and trainer starts to train, the workers stop working so I suppose there’s no interaction between the workers and the trainer. So what appears really strange to me is that the backward is done but step is not.</p><NewLine><p>I’m using the standard optim.Adam</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you using any CUDA ops? If so, could you please add a <a href=""https://pytorch.org/docs/stable/cuda.html#torch.cuda.synchronize"" rel=""nofollow noopener""><code>torch.cuda.synchronize()</code></a> before every print to make sure that preceding ops are indeed done instead of still pending in the CUDA stream?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Li,</p><NewLine><p>I just added the line and the prints are the same.</p><NewLine><p>FYI, after it hangs there, I killed the program and it showed this. I’m not sure if this is helpful</p><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “test.py”, line 23, in <br/><NewLine>Process Process-2:<br/><NewLine>p.join()<br/><NewLine>File “/apps/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/multiprocessing/process.py”, line 140, in join<br/><NewLine>Process Process-1:<br/><NewLine>res = self._popen.wait(timeout)<br/><NewLine>File “/apps/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/multiprocessing/popen_fork.py”, line 48, in wait<br/><NewLine>return self.poll(os.WNOHANG if timeout == 0.0 else 0)<br/><NewLine>File “/apps/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/multiprocessing/popen_fork.py”, line 28, in poll<br/><NewLine>pid, sts = os.waitpid(self.pid, flag)<br/><NewLine>KeyboardInterrupt</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s weird, is there a way that we can reproduce this locally, so that we can help debug?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Li,</p><NewLine><p>Thanks for the help. I’m afraid it wouldn’t be easy to do it. I’ll try to find a way to convert it so it can be shared.</p><NewLine><p>Meanwhile, what would you say that might be the cause? Any chance this could be the use of mp.queue or mp.Value in the linux environment? If likely, I can try to avoid or alter the way using them</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you using <a href=""https://pytorch.org/docs/stable/multiprocessing.html"" rel=""nofollow noopener""><code>torch.multiprocessing.SimpleQueue</code></a>? If yes, does the program guarantee that the owner of the shared data object is still alive when the user uses it when sharing CPU tensors? And are you using <code>spawn</code> to create processes?</p><NewLine><blockquote><NewLine><p>Unlike CPU tensors, the sending process is required to keep the original tensor as long as the receiving process retains a copy of the tensor. The refcounting is implemented under the hood but requires users to follow the next best practices.</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>I used spawn by adding the line<br/><NewLine>mp.set_start_method(“spawn”, force=True)</p><NewLine><p>I used torch.multiprocessing.Queue instead of the SimpleQueue. The owners are always alive</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Spawn on windows but not on cluster</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group quote-modified"" data-post=""9"" data-topic=""91468"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mrshenli/40/12220_2.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><blockquote><NewLine><p>Unlike CPU tensors, the sending process is required to keep the original tensor as long as the receiving process retains a copy of the tensor. The refcounting is implemented under the hood but requires users to follow the next best practices.</p><NewLine></blockquote><NewLine></blockquote><NewLine></aside><NewLine><p>Interesting fact, on linux cluster, it works if I change the device to ‘CPU’ instead of using a GPU. But on windows, both devices work</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you solved this problem? I have met the same problem when running with multiple machine</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not completely solved but was able to found what was the issue and found a way around. The issue is that the network was somehow shared with other processes. So my practical suggestion would be check everything that might lead to your network being shared/visited. e.g. mistake in using copy.copy or deepcopy to send the statedict</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Lewis_Liu; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Lewis_Liu; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Lewis_Liu; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Lewis_Liu; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Lewis_Liu; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/Lewis_Liu; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/bqhuyy; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/Lewis_Liu; <NewLine> ,"REPLY_DATE 1: August 3, 2020,  2:27pm; <NewLine> REPLY_DATE 2: August 3, 2020,  2:28pm; <NewLine> REPLY_DATE 3: August 3, 2020,  2:46pm; <NewLine> REPLY_DATE 4: August 3, 2020,  2:50pm; <NewLine> REPLY_DATE 5: August 3, 2020,  2:55pm; <NewLine> REPLY_DATE 6: August 3, 2020,  2:58pm; <NewLine> REPLY_DATE 7: August 3, 2020,  3:19pm; <NewLine> REPLY_DATE 8: August 3, 2020,  3:29pm; <NewLine> REPLY_DATE 9: August 3, 2020,  3:38pm; <NewLine> REPLY_DATE 10: August 3, 2020,  4:26pm; <NewLine> REPLY_DATE 11: August 3, 2020,  6:19pm; <NewLine> REPLY_DATE 12: August 9, 2020,  4:13pm; <NewLine> REPLY_DATE 13: August 9, 2020,  4:29pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> 
91950,Take multiple gpu as single one,2020-08-07T11:00:18.345Z,0,68,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Suppose that I have a big model that cannot fit into one gpu, so I have to split the model to different gpus. I’m wondering whether there’s a way to make Pytorch take multiple gpu as a single one, so that we don’t have to split model manually.</p><NewLine></div>",https://discuss.pytorch.org/u/cjwcommuny,,cjwcommuny,"August 7, 2020, 11:00am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I heard <code>nn.DataParallel</code>  for using on two or more Gpu,<br/><NewLine>Taking multiple gpu and using as one is great question, also looking for answer to this</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I’m wondering whether there’s a way to make PyTorch take multiple gpu as a single one, so that we don’t have to split model manually.</p><NewLine></blockquote><NewLine><p>Currently, this is not available, we are working on adding a model partitioning feature.</p><NewLine><p>Manually splitting model shouldn’t be too hard with today’s PyTorch API, you just need to append <code>.to(device)</code> to certain layers and outputs. See this tutorial: <a href=""https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mathematics; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 7, 2020,  1:16pm; <NewLine> REPLY_DATE 2: August 8, 2020, 11:32am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
91328,Code runs faster on local GPU than cluster,2020-08-01T17:18:15.447Z,1,110,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My code runs so much faster on my single GTX 1060 than on the cluster which has 2 GTX 1080 Ti.  This is strange because a few parts of the code run faster on the cluster, although most parts run slower.  For some comparison, enumerating through the dataloader takes ~9 seconds on my local while it takes 550 seconds on the cluster.  Also, calculating loss + backprop takes ~ 1 seconds on my local, while it takes 10 seconds on the cluster.  The code is from this paper <a href=""https://github.com/Philip-Bachman/amdim-public"" rel=""nofollow noopener"">https://github.com/Philip-Bachman/amdim-public</a></p><NewLine></div>",https://discuss.pytorch.org/u/mhong94,,mhong94,"August 1, 2020,  5:18pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>How did you measure the timing in both applications?<br/><NewLine>If the data loading is 55 times slower on the “cluster”, I would recommend to narrow down this issue first.<br/><NewLine>E.g. are you storing the data on a network drive or on a local SSD in both cases?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just called time.time() at different sections and computed their differences for timing.  Can you clarify how to narrow down the issue?  The data is stored on my local SSD, and I’m not exactly sure where on the network it’s stored.  The original code used ImageFolder to load the data, and I tried changing it to a standard dataset, but this did not help</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you are timing CUDA operations you would have to synchronize the code before starting and stopping the timer due to the asynchronous execution of CUDA kernels via <code>torch.cuda.synchronize()</code>.</p><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""91328"" data-username=""mhong94""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/ecc23a/40.png"" width=""20""/> mhong94:</div><NewLine><blockquote><NewLine><p>The data is stored on my local SSD, and I’m not exactly sure where on the network it’s stored.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I don’t understand this explanation. Is the data stored on the SSD in your workstation or on a server in your network (or both)?<br/><NewLine>In the latter case you would introduce the network latency into the training, so I would recommend to store the data on a local SSD.</p><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""91328"" data-username=""mhong94""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/ecc23a/40.png"" width=""20""/> mhong94:</div><NewLine><blockquote><NewLine><p>The original code used ImageFolder to load the data, and I tried changing it to a standard dataset, but this did not help</p><NewLine></blockquote><NewLine></aside><NewLine><p>What do you mean by “standard Dataset”? Did you write a custom <code>Dataset</code>?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>When I run on my local machine, I’m accessing the data on my local SSD.  I copy all code/data over to a storage on my kubernetes pod, then run the code there, using the cluster’s GPUs.  I think that in both cases, they are accessing the data from their local SSDs.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mhong94; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mhong94; <NewLine> ,"REPLY_DATE 1: August 2, 2020,  2:39am; <NewLine> REPLY_DATE 2: August 2, 2020,  7:41am; <NewLine> REPLY_DATE 3: August 3, 2020,  6:18am; <NewLine> REPLY_DATE 4: August 6, 2020,  7:59pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
90526,Spawned Processes with DDP,2020-07-25T16:22:05.599Z,6,110,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I run a model with DDP with 4 spawned processes and each process taking a GPU, I notice that there are a lot of spawned processes with <code>nvidia-smi</code> on the main process. Why are there so many additional processes spawned? I am also using <code>torch=1.4.0</code> and <code>CUDA=10.1.243</code> which is installed through my conda environment.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/9bca424a1ea29ec21b71c03eac5dfb4f48ba3fa2"" href=""https://discuss.pytorch.org/uploads/default/original/3X/9/b/9bca424a1ea29ec21b71c03eac5dfb4f48ba3fa2.png"" title=""gpu_ddp""><img alt=""gpu_ddp"" data-base62-sha1=""meblqTLIyFMMOQ9HHaajRTQSGau"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/b/9bca424a1ea29ec21b71c03eac5dfb4f48ba3fa2_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/original/3X/9/b/9bca424a1ea29ec21b71c03eac5dfb4f48ba3fa2.png"" width=""500""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">gpu_ddp</span><span class=""informations"">642×642 11.6 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/kleingeo,(Geoff Klein),kleingeo,"July 25, 2020,  4:22pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>See discussion <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/torch-distributed-barrier-used-in-multi-node-distributed-data-parallel-training/89711/24"">`torch.distributed.barrier` used in multi-node distributed data-parallel training</a></p><NewLine><p>For more detailed explainations, cc <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""90526"" data-username=""iffiX""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/iffix/40/24443_2.png"" width=""20""/> iffiX:</div><NewLine><blockquote><NewLine><p>For more detailed explainations, c</p><NewLine></blockquote><NewLine></aside><NewLine><p>I took a look at that, but I am a little confused as it still. Is it a back-end issue, or is it normal behaviour?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>should be normal，depending on the backend</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems to happen with both nccl and gloo backends.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Maybe ask <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>, I am not fully clear about their internal implementations.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>So I played around a little more by breaking my pipeline to a very simple training loop, just to ensure I wasn’t doing anything wrong. I then cloned my conda environment and updated the torch and cuda to the versions listed on the Getting Started page (the most up-to-date) and it seems to fix the issue. So I’m not sure if it was a cuda, nccl (in the cudatoolkit), or the updated torch, but updating does fix it.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Those (~500MB CUDA memory consumption) look like CUDA context. It seems all processes somehow used <code>CUDA:0</code> somewhere. It could be caused by 3rd-party libraries or calls like <code>torch.cuda.empty_cache()</code>. If you don’t want to debug the root cause of it, you can avoid it by setting <code>CUDA_VISIBLE_DEVICES</code> env var to make sure that each process only sees one GPU.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I updated both my torch and cudatoolkit (with conda) to the newest versions which seemed to fix the problem. I’m not sure if it was cuda, torch or how torch interacted with cuda, but updating seems to have fixed the bug. I’m not sure if this improves anything though, but the extra processes don’t show up anymore.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kleingeo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/kleingeo; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/kleingeo; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/kleingeo; <NewLine> ,"REPLY_DATE 1: July 25, 2020,  4:44pm; <NewLine> REPLY_DATE 2: July 25, 2020,  5:06pm; <NewLine> REPLY_DATE 3: July 26, 2020,  2:17am; <NewLine> REPLY_DATE 4: July 26, 2020,  2:17pm; <NewLine> REPLY_DATE 5: July 26, 2020,  3:21pm; <NewLine> REPLY_DATE 6: July 26, 2020,  4:22pm; <NewLine> REPLY_DATE 7: July 27, 2020,  4:40pm; <NewLine> REPLY_DATE 8: August 6, 2020,  1:26pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
91684,How to Install NumPy with Python Distribution,2020-08-05T06:37:25.529Z,1,62,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello All, Can anyone tell me how to install NumPy matrix with Python distribution? I have already download anaconda python distribution but still, I don’t know whats the next step is? Can anyone suggest me step to step process?</p><NewLine></div>",https://discuss.pytorch.org/u/Aarti_Yadav,(Aarti Yadav),Aarti_Yadav,"August 5, 2020,  6:37am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think this question is not relevant here.<br/><NewLine>But Anyway, you can use Anaconda to download packages all required for data science at once.<br/><NewLine><a href=""https://www.anaconda.com/products/individual"" rel=""nofollow noopener"">https://www.anaconda.com/products/individual</a></p><NewLine><p>and download and install</p><NewLine><p>You can see lot of tutorials and videos regarding installing Anaconda.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have to install anaconda but I am stuck in some places but my mostly requires was resolved with the help of this <a href=""https://hackr.io/blog/numpy-matrix-multiplication"" rel=""nofollow noopener"">post</a> and thanks for your reference which is useful to me.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mathematics; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Aarti_Yadav; <NewLine> ,"REPLY_DATE 1: August 5, 2020,  6:52am; <NewLine> REPLY_DATE 2: August 6, 2020,  4:04am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
91709,Distributed RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation,2020-08-05T10:36:06.607Z,1,84,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Distributed training of the network shown below gives this error, while without distributed works.<br/><NewLine>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [12, 256, 1, 1]] is at version 2; expected version 1 instead.</p><NewLine><pre><code class=""lang-auto"">[W python_anomaly_mode.cpp:60] Warning: Error detected in CudnnConvolutionBackward. Traceback of forward call that caused the error:<NewLine>  File ""/home/e2r/anaconda3/envs/e2r/lib/python3.7/threading.py"", line 890, in _bootstrap<NewLine>    self._bootstrap_inner()<NewLine>  File ""/home/e2r/anaconda3/envs/e2r/lib/python3.7/threading.py"", line 926, in _bootstrap_inner<NewLine>    self.run()<NewLine>  File ""/home/e2r/anaconda3/envs/e2r/lib/python3.7/threading.py"", line 870, in run<NewLine>    self._target(*self._args, **self._kwargs)<NewLine>  File ""/home/e2r/anaconda3/envs/e2r/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 60, in _worker<NewLine>    output = module(*input, **kwargs)<NewLine>  File ""/home/e2r/anaconda3/envs/e2r/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/net.py"", line 51, in forward<NewLine>    out1 = self.conv2(out0.clone())<NewLine>  File ""/home/e2r/anaconda3/envs/e2r/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/e2r/anaconda3/envs/e2r/lib/python3.7/site-packages/torch/nn/modules/conv.py"", line 419, in forward<NewLine>    return self._conv_forward(input, self.weight)<NewLine>  File ""/home/e2r/anaconda3/envs/e2r/lib/python3.7/site-packages/torch/nn/modules/conv.py"", line 416, in _conv_forward<NewLine>    self.padding, self.dilation, self.groups)<NewLine> (function print_stack)<NewLine>Process Process-1:<NewLine>Traceback (most recent call last):<NewLine>  File ""/home/e2r/anaconda3/envs/e2r/lib/python3.7/multiprocessing/process.py"", line 297, in _bootstrap<NewLine>    self.run()<NewLine>  File ""/home/e2r/anaconda3/envs/e2r/lib/python3.7/multiprocessing/process.py"", line 99, in run<NewLine>    self._target(*self._args, **self._kwargs)<NewLine>  File ""/home/e2r/Desktop/e2r/train.py"", line 51, in init_process<NewLine>    fn(rank, opt)<NewLine>  File ""/home/e2r/Desktop/e2r/train.py"", line 194, in main_worker<NewLine>    trainer.train()<NewLine>  File ""/home/e2r/Desktop/e2r//trainer.py"", line 166, in train<NewLine>    self.run_epoch()<NewLine>  File ""/home/e2r/Desktop/e2r/trainer.py"", line 199, in run_epoch<NewLine>    losses[""loss""].backward()<NewLine>  File ""/home/e2r/anaconda3/envs/e2r/lib/python3.7/site-packages/torch/tensor.py"", line 185, in backward<NewLine>    torch.autograd.backward(self, gradient, retain_graph, create_graph)<NewLine>  File ""/home/e2r/anaconda3/envs/e2r/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 127, in backward<NewLine>    allow_unreachable=True)  # allow_unreachable flag<NewLine>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [12, 256, 1, 1]] is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!<NewLine><NewLine></code></pre><NewLine><p>Enabling anamoly detection traces it back to a 1 x 1 convolution layer. I have tried many things to resolve it such as <code>clone()</code> before giving the layer as input and also having <code>inplace=False</code> in Relu layers. I still cannot resolve it. Here’s the network</p><NewLine><pre><code class=""lang-auto"">class Net(nn.Module):<NewLine>    def __init__(self, num_ch_enc, num_input_features, num_frames_to_predict_for=None, stride=1):<NewLine>        super(Net, self).__init__()<NewLine><NewLine>        self.num_ch_enc = num_ch_enc<NewLine>        self.num_input_features = num_input_features<NewLine><NewLine>        if num_frames_to_predict_for is None:<NewLine>            num_frames_to_predict_for = num_input_features - 1<NewLine>        self.num_frames_to_predict_for = num_frames_to_predict_for<NewLine>        num_frames_to_predict_for_6 = int(6*self.num_frames_to_predict_for)<NewLine><NewLine>        self.squeeze = nn.Conv2d(self.num_ch_enc[-1], 256, 1)<NewLine>        self.conv0 = nn.Conv2d(num_input_features * 256, 256, 3, stride, 1)<NewLine>        self.conv1 = nn.Conv2d(256, 256, 3, stride, 1)<NewLine>        self.conv2 = nn.Conv2d(256, num_frames_to_predict_for_6, 1, 1, 0)<NewLine><NewLine>        self.relu = nn.ReLU(inplace=False)<NewLine><NewLine>    def forward(self, input_features):<NewLine>        last_features = [f[-1] for f in input_features]<NewLine><NewLine>        cat_features = [self.relu(self.squeeze(f)) for f in last_features]<NewLine>        cat_features = torch.cat(cat_features, 1)<NewLine><NewLine>        out = cat_features<NewLine>        out = self.conv0(out)<NewLine>        out = self.relu(out)<NewLine>        out = self.conv1(out)<NewLine>        out = self.relu(out)<NewLine>        out = self.conv1(out)<NewLine>        out0 = self.relu(out)<NewLine><NewLine>        # gives inplace modification error here for multiprocessing for conv2 layer<NewLine>        out1 = self.conv2(out0.clone())<NewLine>        out2 = out1.mean(3).clone()<NewLine>        out3 = out2.mean(2).clone()<NewLine><NewLine>        out4 = 0.01 * out3.clone().view(-1, self.num_frames_to_predict_for, 1, 6)<NewLine><NewLine>        axisangle = out4[..., :3]<NewLine>        translation = out4[..., 3:]<NewLine><NewLine>        return out4[..., :3], out4[..., 3:]<NewLine><NewLine></code></pre><NewLine><p><strong>Edit:</strong><br/><NewLine>I tried to simplify the forward function as follows but I still get an error at the linear layer</p><NewLine><pre><code class=""lang-auto"">def forward(self, x):<NewLine>        x = x[-1][-1]<NewLine><NewLine>        x = self.pose3(x)<NewLine><NewLine>        x = x.view(-1, self.num_frames_to_predict_for * 6* 6* 20)<NewLine>        <NewLine>        # Same errror in the linear layer here<NewLine>        x1 = self.linear(x).clone()<NewLine><NewLine>        x2 = 0.01 * x1.view(-1, self.num_frames_to_predict_for, 1, 6)<NewLine><NewLine>        return x2[..., :3], x2[..., 3:]<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/zshn25,(Zeeshan Khan Suri),zshn25,"August 5, 2020,  8:09pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could the line <code>cat_features = torch.cat(cat_features, 1)</code> be the problem? If you’re just expanding a new column to the end of <code>cat_features</code> you could try .unsqueeze(1) (or whatever the final index is + 1) and see if that works instead?</p><NewLine><p>So replacing,</p><NewLine><pre><code class=""lang-auto"">cat_features = torch.cat(cat_features, 1)<NewLine>out = cat_features<NewLine># gives inplace modification error here for multiprocessing for conv2 layer<NewLine>out = self.conv0(out)<NewLine></code></pre><NewLine><p>with</p><NewLine><pre><code class=""lang-auto"">out = self.conv0(cat_features.unsqueeze(1)) <NewLine></code></pre><NewLine><p>Perhaps give that a go?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, I’m not jsut expanding a new column, <code>cat_features = torch.cat(cat_features, 1)</code> concatenates the features from the list in the channel dimension</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/AlphaBetaGamma96; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/zshn25; <NewLine> ,"REPLY_DATE 1: August 5, 2020,  1:33pm; <NewLine> REPLY_DATE 2: August 5, 2020,  6:12pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
91743,Four p2.xlarge vs two p3.2xlarge,2020-08-05T16:35:29.571Z,0,66,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Each EC2 <em>p3.2xlarge</em> instance has 8 CPU and 1 GPU.  I’m allowed a maximum of 16 CPUs on AWS at any given time, so I’ve been doing distributed training over 2 GPUs.  However, I’ve just noticed that each <em>p2.xlarge</em> instance has 4 CPU and 1 GPU.  This means that I could have 4 of these, which means training over 4 GPUs.  Would this make training faster?</p><NewLine><p>What factors should be taken into consideration?  4 instances do take more time to set up than 2.  Cost is not an issue.  I’m doing mixed precision distributed training with apex.</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/qap,,qap,"August 5, 2020,  4:38pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""91743"" data-username=""qap""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/qap/40/27108_2.png"" width=""20""/> qap:</div><NewLine><blockquote><NewLine><p>This means that I could have 4 of these, which means training over 4 GPUs. Would this make training faster?</p><NewLine></blockquote><NewLine></aside><NewLine><p>This is possible. Hope Figure 9 in this paper can offer some insight: <a href=""https://arxiv.org/pdf/2006.15704.pdf"" rel=""nofollow noopener"">https://arxiv.org/pdf/2006.15704.pdf</a></p><NewLine><blockquote><NewLine><p>What factors should be taken into consideration?</p><NewLine></blockquote><NewLine><p>If the GPUs are the same, the network bandwidth is one of the dominating factor of training speed.</p><NewLine><blockquote><NewLine><p>4 instances do take more time to set up than 2.</p><NewLine></blockquote><NewLine><p>This is one-time setup overhead, instead of per-iteration overhead, right? If so, it should be fine.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 5, 2020,  5:58pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
91634,How to build pytorch with NCCL 2.7.6,2020-08-04T17:15:01.290Z,3,99,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I’m trying build pytorch from source and need to make use of NCCL 2.7.6 or higher. The current pytorch is built using 2.7.3.  Is there a way to do this?</p><NewLine><p>Thank you</p><NewLine></div>",https://discuss.pytorch.org/u/Purvak-L,(Purvak Lapsiya),Purvak-L,"August 4, 2020,  5:25pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can potentially clone your own pytorch repo and upgrade nccl in there only, after that you can recompile from source. Would that work?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I did clone the pytorch repo and build from it. Can you point me where to make that update?<br/><NewLine>I did check cmake/modules/FindNCCL.make.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/purvak-l"">@Purvak-L</a>, you can <code>cd</code> to the NCCL submodule in the third party folder in <a href=""https://github.com/pytorch/pytorch/tree/master/third_party/nccl"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/tree/master/third_party/nccl</a> and manually update the nccl module there.</p><NewLine><p>Another option is to install 2.7.6 locally and set <code>USE_SYSTEM_NCCL=1</code> when building PyTorch. See this issue: <a href=""https://github.com/pytorch/pytorch/issues/32286"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/32286</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""91634"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mrshenli/40/12220_2.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>USE_SYSTEM_NCCL=1</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thank you <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a></p><NewLine><p>This helped! I downloaded the latest NCCL (2.7.6.1) and set the flag. After that I built pytorch from source using python setup.py install</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/agolynski; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Purvak-L; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Purvak-L; <NewLine> ,"REPLY_DATE 1: August 4, 2020,  5:50pm; <NewLine> REPLY_DATE 2: August 4, 2020,  5:53pm; <NewLine> REPLY_DATE 3: August 4, 2020, 11:21pm; <NewLine> REPLY_DATE 4: August 4, 2020, 11:23pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
91474,Back-propagation through time and distributed training,2020-08-03T11:59:46.629Z,0,58,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a Sampler to do backpropagation through time, similar to the one in the <a href=""https://pytorchnlp.readthedocs.io/en/latest/source/torchnlp.samplers.html#torchnlp.samplers.BPTTBatchSampler"" rel=""nofollow noopener"">torchnlp examples</a>. This requires giving the batch size of training as an argument so that batches are properly continuous (e.g. if the dataset is [abcdefghi] and batch size is 3, the batches are [adf] [beh] [cfi]).</p><NewLine><p>My question is, how does this work in a distributed setting? From reading the code of the <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler"" rel=""nofollow noopener"">distributed sampler</a>, I got the impression that each process gets its copy of the sampler. In this case, I would need to know the local batch size in every process to properly separate batches. Is there a general rule to determine batch size per process (like an equal portion per GPU?), and if not, how could one determine the local batch size?</p><NewLine></div>",https://discuss.pytorch.org/u/TVN,,TVN,"August 3, 2020, 11:59am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Is there a general rule to determine batch size per process?</p><NewLine></blockquote><NewLine><p>Yes, if possible, it’s better that the data is evenly distributed on different processes, otherwise the process with lighter workload will have to frequently waiting for the stragglers, causing unnecessary slowdown.</p><NewLine><p>Compare to that, if you are using <code>DistributedDataParallel</code> (DDP) a more important thing is that all processes must have the same number of forward/backward iterations, otherwise collective communications in DDP backward would hang.</p><NewLine><blockquote><NewLine><p>how could one determine the local batch size?</p><NewLine></blockquote><NewLine><p>See discussion here: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769"">Should we split batch_size according to ngpu_per_node when DistributedDataparallel</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 3, 2020,  2:32pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
91409,Pytorch Distributed Data Parallel - How can I pass the same information to all processes,2020-08-02T19:21:27.675Z,2,80,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using Distributed Data Parallel wich instantiates multiple processes to train model on multiple GPUs. I want to be able to save each experiment run backup to a single new folder (let’s say by passing the same timestamp to all the processes). However some processes being delayed by a second leads to a different timestamp within the same experiment run. Is it possible to pass the same information(single timestamp) to all the processes?</p><NewLine><p>I start my script with:</p><NewLine><pre><code class=""lang-auto"">CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 NCCL_LL_THRESHOLD=0 python \<NewLine>-i \<NewLine>-m torch.distributed.launch \<NewLine>--master_port=9997 \<NewLine>--nproc_per_node=8 \<NewLine>main.py .....<NewLine></code></pre><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/amogh112,(Amogh Gupta),amogh112,"August 2, 2020,  7:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Does it work if you let <code>rank0</code> process to <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.broadcast"" rel=""nofollow noopener""><code>broadcast</code></a> its timestamp to other processes?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot, that worked!<br/><NewLine>I have one question though, I broadcast process with local rank 5’s seconds as a tensor, and thus all the processes have the same seconds after broadcasting.<br/><NewLine>So how does it work, is it possible that a faster process uses an old value of the variable that is broadcasted later by the src process? Or do all the processes <em>wait</em> until the value has been broadcasted by the src variable?</p><NewLine><pre><code class=""lang-auto"">Current time on machine is : 3 2020-08-02_21:52:52<NewLine>Current time on machine is : 4 2020-08-02_21:52:52<NewLine>Current time on machine is : 2 2020-08-02_21:52:52<NewLine>Current time on machine is : 5 2020-08-02_21:52:52<NewLine>Current time on machine is : 7 2020-08-02_21:52:52<NewLine>Current time on machine is : 6 2020-08-02_21:52:53<NewLine>Current time on machine is : 1 2020-08-02_21:52:53<NewLine>Current time on machine is : 0 2020-08-02_21:52:53<NewLine><NewLine>Before Broadcasting seconds: tensor([52], device='cuda:4')<NewLine>Before Broadcasting seconds: tensor([52], device='cuda:3')<NewLine>Before Broadcasting seconds: tensor([52], device='cuda:2')<NewLine>Before Broadcasting seconds: tensor([52], device='cuda:7')<NewLine>Before Broadcasting seconds: tensor([52], device='cuda:5')<NewLine>Before Broadcasting seconds: tensor([53], device='cuda:1')<NewLine>Before Broadcasting seconds: tensor([53], device='cuda:0')<NewLine>Before Broadcasting seconds: tensor([53], device='cuda:6')<NewLine><NewLine>&lt;broadcast using torch.distributed.broadcast(LongTensor(seconds), src=5)&gt;<NewLine><NewLine>After Broadcasting seconds  tensor([52], device='cuda:6')<NewLine>After Broadcasting seconds  tensor([52], device='cuda:1')<NewLine>After Broadcasting seconds  tensor([52], device='cuda:7')<NewLine>After Broadcasting seconds  tensor([52], device='cuda:2')<NewLine>After Broadcasting seconds  tensor([52], device='cuda:0')<NewLine>After Broadcasting seconds  tensor([52], device='cuda:5')<NewLine>After Broadcasting seconds  tensor([52], device='cuda:4')<NewLine>After Broadcasting seconds  tensor([52], device='cuda:3')<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>In the <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.broadcast"" rel=""nofollow noopener""><code>broadcast</code></a> API, there is an <code>async_op</code>, which by default is <code>False</code>. If it is <code>False</code>, all processes will block until the value is broadcasted. Otherwise, if it is <code>True</code>, <code>broadcast</code> will be non-blocking and return a Future-like object, and you can call <code>wait()</code> on that object. In this case, the tensor is only guaranteed to hold the result after <code>wait()</code> returns.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/amogh112; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: August 3, 2020,  2:39pm; <NewLine> REPLY_DATE 2: August 3, 2020,  3:48am; <NewLine> REPLY_DATE 3: August 3, 2020,  2:25pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
91118,How to temporarily detach parameters when using DataParallel?,2020-07-30T15:23:45.361Z,1,98,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I need to do something like this:</p><NewLine><pre><code class=""lang-auto"">class MyOp(torch.autograd.Function):<NewLine>        @staticmethod<NewLine>        def forward(ctx, net1, net2, x):<NewLine>            ctx.net1 = net1<NewLine>            ctx.net2 = net2<NewLine>            ctx.save_for_backward(x)<NewLine>            return net1(x)<NewLine><NewLine>        @staticmethod<NewLine>        def backward(ctx, grad):<NewLine>            net1 = ctx.net1<NewLine>            net2 = ctx.net2<NewLine>            x = ctx.saved_tensors<NewLine>            # disable backward for parameters in net2, because I only need the gradient for x by net2.<NewLine>            for params in net2.parameters():<NewLine>                params.requires_grad_(False)<NewLine>            with torch.enable_grad():<NewLine>                y = net2(x)<NewLine>             y.backward(torch.ones_like(x).to(x))<NewLine>             gradx = x.grad.clone().detach()<NewLine>             # enable backward for net2, because it needs to be used in other computations.<NewLine>             for params in net2.parameters():<NewLine>                 params.requires_grad_(True)<NewLine>             return (None, None, gradx)<NewLine></code></pre><NewLine><p>This code works well for single-GPU. However, when I use DataParallel with Multi-GPUs, the gradient is wrong.</p><NewLine><p>I guess maybe it is because there is no lock for multi-processes and there are some gradients backwarded to parameters in net2. How can I correct my code for DataPrallel models?</p><NewLine></div>",https://discuss.pytorch.org/u/LuChengTHU,,LuChengTHU,"July 30, 2020,  3:23pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>My guess the reason why it doesn’t work, is you can no longer get parameters on DataParallel replica.<br/><NewLine>One workaround, (my guess), is to use torch.autograd.grad instead of backward.<br/><NewLine>you can do:</p><NewLine><pre><code class=""lang-auto"">gradx = torch.autograd.grad(y, x, torch.ones_like(x).to(x))[0]<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you set grads to True for net2’s parameters before you start the forward and then set them to false after you are done with the forward? This way the grads should be False for the entire backward pass despite concurrent execution of the backward pass across multiple GPUs.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks! I’ve tried your suggestion and it works!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ruotianluo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/LuChengTHU; <NewLine> ,"REPLY_DATE 1: July 31, 2020,  8:04am; <NewLine> REPLY_DATE 2: July 30, 2020,  8:58pm; <NewLine> REPLY_DATE 3: July 31, 2020,  8:05am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
90392,Using cuda tensor in DataParallel model,2020-07-24T11:42:28.127Z,6,117,"<div class=""post"" itemprop=""articleBody""><NewLine><p>there are model wrapped by <code>nn.DataParallel</code></p><NewLine><pre><code class=""lang-auto"">self.model = Bert(6, 12, 513, 384*4, 64, 64, 2, 384, self.base_task.max_vocab_indexes['input_ids'])<NewLine>self.model = nn.DataParallel(self.model).cuda()<NewLine></code></pre><NewLine><p>and in model there are constant tensor, which name is <code>pos</code>.</p><NewLine><pre><code class=""lang-auto"">class Embedding(nn.Module):<NewLine>    def __init__(self, maxlen, d_model, n_segments, vocab_size, device='cuda'):<NewLine>        super(Embedding, self).__init__()<NewLine>        self.device = device<NewLine>        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding<NewLine>        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding<NewLine>        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding<NewLine>        self.norm = nn.LayerNorm(d_model)<NewLine><NewLine>    def forward(self, x, seg):<NewLine>        seq_len = x.size(1)<NewLine>        pos = torch.arange(seq_len, dtype=torch.long, device=self.device)<NewLine>        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -&gt; (batch_size, seq_len)<NewLine>        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)<NewLine>        return self.norm(embedding)<NewLine></code></pre><NewLine><p>I used this code with just one GPU, and this works.<br/><NewLine>But this time I need to use more gpus, and I have to modify that tensor which was manually located <code>device=self.device</code>, to something “which is dynamically located each gpu at DataParallel”.<br/><NewLine>… But it is hard to me, code below doesn’t works. The tensor just located in cpu, even with <code>.cuda()</code></p><NewLine><p>How can I solve this issue? I am almost struggling with this issue all day…</p><NewLine><pre><code class=""lang-auto"">class Embedding(nn.Module):<NewLine>    def __init__(self, maxlen, d_model, n_segments, vocab_size, device='cuda'):<NewLine>        super(Embedding, self).__init__()<NewLine>        self.device = torch.device('cuda')<NewLine>        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding<NewLine>        self.pos_embed = nn.Embedding(maxlen, d_model).cpu()  # position embedding<NewLine>        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding<NewLine>        self.norm = nn.LayerNorm(d_model)<NewLine>        self.pos = torch.arange(513, dtype=torch.long, requires_grad=False).unsqueeze(0)<NewLine><NewLine>    def forward(self, x, seg):<NewLine>        # seq_len = x.size(1)<NewLine>        print(f'pos device: {self.pos.device}') # printed by ""cpu""<NewLine>        pos = self.pos.expand_as(x)  # (seq_len,) -&gt; (batch_size, seq_len)<NewLine>        cuda_pos = self.pos_embed(pos).cuda()<NewLine>        print(f'pos device: {cuda_pos.device}, x device : {x.device}, seg device: {seg.device}') # printed by ""cpu"" for pos<NewLine>        embedding = self.tok_embed(x) + cuda_pos + self.seg_embed(seg)<NewLine>        return self.norm(embedding)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/cybaj,,cybaj,"July 24, 2020, 11:44am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I also tried add that tensor to model’s input. But with this case, when I use 2 gpus, the result is just about half of one batch, <code>(64 x ...)</code> tensor returned, just half of one batch (128).</p><NewLine><pre><code class=""lang-auto"">0/5120 [00:00&lt;? ?it/s] Traceback (most recent call last):<NewLine>  File ""main.py"", line 234, in &lt;module&gt;<NewLine>    trainer.train()<NewLine>  File ""main.py"", line 116, in train<NewLine>    loss_lm = self.criterion(logits_lm.transpose(1, 2), batch.masked_tokens.transpose(0,1)) # for masked LM<NewLine>  File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py"", line 932, in forward<NewLine>    ignore_index=self.ignore_index, reduction=self.reduction)<NewLine>  File ""/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py"", line 2317, in cross_entropy<NewLine>    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)<NewLine>  File ""/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py"", line 2113, in nll_loss<NewLine>    .format(input.size(0), target.size(0)))<NewLine>ValueError: Expected input batch_size (64) to match target batch_size (128).<NewLine></code></pre><NewLine><p>and below is training loop.</p><NewLine><pre><code class=""lang-auto"">        pos = torch.arange(513, dtype=torch.long, requires_grad=False).unsqueeze(0).to(device=torch.device('cuda'))<NewLine>        for epoch in range(max_epoch):<NewLine>            loss_sum, acc_sum, len_batch_sum = 0., 0., 0.<NewLine>            ds_iter.init_epoch()<NewLine>            tr_total = math.ceil(total_len / self.batch_size)<NewLine>            tq_iter = tqdm(enumerate(ds_iter), total=tr_total, miniters=min_iters, unit_scale=self.batch_size,<NewLine>                           bar_format='{n_fmt}/{total_fmt} [{elapsed}&lt;{remaining} {rate_fmt}] {desc}')<NewLine><NewLine>            self.model.train()<NewLine>            print('epoch starts')<NewLine>            for i, batch in tq_iter:<NewLine>                self.model.zero_grad()<NewLine>                print('batch starts')<NewLine>                device = torch.device('cuda')<NewLine>                print(device)<NewLine>                print(f'batch.input_ids device : {batch.input_ids.device}, batch.segment_ids : {batch.segment_ids.device}, batch.masekd_pos : {batch.masked_pos.device}')<NewLine>                print(f'batch.input_ids shape : {batch.input_ids.shape}, batch.segment_ids : {batch.segment_ids.shape}, batch.masekd_pos : {batch.masked_pos.shape}, pos : {pos.shape}')<NewLine>                logits_lm, logits_clsf = self.model(batch.input_ids.transpose(0,1).to(device=device), batch.segment_ids.transpose(0,1).to(device=device), batch.masked_pos.transpose(0,1).to(device=device), pos.to(device=device))<NewLine>                print(f'logits_lm, logits_clsf shape : {logits_lm.shape}, {logits_clsf.shape}')<NewLine></code></pre><NewLine><p>and these are logs.</p><NewLine><pre><code class=""lang-auto"">epoch starts<NewLine>batch starts<NewLine>cuda<NewLine>batch.input_ids device : cuda:0, batch.segment_ids : cuda:0, batch.masekd_pos : cuda:0<NewLine>batch.input_ids shape : torch.Size([513, 128]), batch.segment_ids : torch.Size([513, 128]), batch.masekd_pos : torch.Size([5, 128]), pos : torch.Size([1, 513])<NewLine>pos device: cuda:0 # log from Embedding layer in the model<NewLine>pos device: cuda:0, x device : cuda:0, seg device: cuda:0 # log from Embedding layer in the model<NewLine>logits_lm, logits_clsf shape : torch.Size([64, 5, 6015]), torch.Size([64, 2])<NewLine><NewLine>logits_lm device: cuda:0, batch target device: cuda:0<NewLine>0/5120 [00:00&lt;? ?it/s] Traceback (most recent call last):<NewLine>  File ""main.py"", line 236, in &lt;module&gt;<NewLine>    trainer.train()<NewLine>  File ""main.py"", line 118, in train<NewLine>    loss_lm = self.criterion(logits_lm.transpose(1, 2), batch.masked_tokens.transpose(0,1)) # for masked LM<NewLine>  File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py"", line 932, in forward<NewLine>    ignore_index=self.ignore_index, reduction=self.reduction)<NewLine>  File ""/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py"", line 2317, in cross_entropy<NewLine>    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)<NewLine>  File ""/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py"", line 2113, in nll_loss<NewLine>    .format(input.size(0), target.size(0)))<NewLine>ValueError: Expected input batch_size (64) to match target batch_size (128).<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/cybaj"">@cybaj</a>, the model needs to be moved to GPU before passing it to <code>DataParallel</code> ctor. Have you tried changing the following code:</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""90392"" data-username=""cybaj""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/c/f04885/40.png"" width=""20""/> cybaj:</div><NewLine><blockquote><NewLine><p><code>self.model = nn.DataParallel(self.model).cuda()</code></p><NewLine></blockquote><NewLine></aside><NewLine><p>to</p><NewLine><pre><code class=""lang-auto"">self.model = nn.DataParallel(self.model.to(""cuda:0""))<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>BTW, here is the <code>DataParallel</code> tutorial: <a href=""https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for reply! <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>, I already tried with <code>to(""cuda:0"")</code> . and as far as I knew <code>cuda()</code> and <code>to(device=""cuda"")</code> is same.<br/><NewLine>Below is the issues.</p><NewLine><ol><NewLine><li>In my model, I need to use some constant tensor, and I defined it in model’s <code>forward</code> as you can see top post.</li><NewLine><li>So I give <code>torch.device('cuda')</code> to model, and in model, locate that tensor to the device. more specified,</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">device = torch.device('cuda')<NewLine>...<NewLine>self.model = Bert(6, 12, 513, 384*4, 64, 64, 2, 384, <NewLine>						self.base_task.max_vocab_indexes['input_ids'], device=device)<NewLine>self.model = nn.DataParallel(self.model)<NewLine>self.model = self.model.cuda()<NewLine></code></pre><NewLine><ul><NewLine><li>I ran it with 2 gpus, and it shows up right.</li><NewLine></ul><NewLine><pre><code class=""lang-auto"">    device_count = torch.cuda.device_count()<NewLine>    print(f'gpu count: {torch.cuda.device_count()}')<NewLine><NewLine>gpu count: 2<NewLine></code></pre><NewLine><ul><NewLine><li>dataset iterator is <code>device=torch.device('cuda')</code><NewLine></li><NewLine><li>DataParallel wrapped model with <code>cuda()</code><NewLine></li><NewLine><li>and model get returned, <code>logits_clsf</code> and <code>logits_lm</code><NewLine></li><NewLine><li>loss was calculated.</li><NewLine><li>and <code>loss.backward()</code> phase, I think stuck. <code>print</code> after <code>loss.backward()</code> wasn’t printed.<br/><NewLine>belows are logs and the model.</li><NewLine></ul><NewLine><pre><code class=""lang-auto"">batch starts<NewLine>logits_lm device: cuda:0, batch target device: cuda:0<NewLine>loss_lm calculated<NewLine>loss_clsf calculated<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">            print('epoch starts')<NewLine>            for i, batch in tq_iter:<NewLine>                self.model.zero_grad()<NewLine>                print('batch starts')<NewLine>                logits_lm, logits_clsf = self.model(batch.input_ids.transpose(0,1), batch.segment_ids.transpose(0,1), batch.masked_pos.transpose(0,1))<NewLine>                print(f'logits_lm device: {logits_lm.device}, batch target device: {batch.masked_tokens.device}')<NewLine>                loss_lm = self.criterion(logits_lm.transpose(1, 2), batch.masked_tokens.transpose(0,1)) # for masked LM<NewLine>                print('loss_lm calculated')<NewLine>                loss_lm = (loss_lm.float()).mean()<NewLine>                loss_clsf = self.criterion(logits_clsf, batch.is_next) # for sentence classification<NewLine>                print('loss_clsf calculated')<NewLine>                loss = loss_lm + loss_clsf<NewLine>                loss.backward()<NewLine>                print('loss backwarded')<NewLine></code></pre><NewLine><p>without any error message or logs, the log <code>loss backwarded</code> didn’t shows up. I think it’s stuck.<br/><NewLine>I don’t know what I have to do.<br/><NewLine>All the post in this thread, are my tries after this happened…</p><NewLine><p>I assumed that,<br/><NewLine>in backward phase,<br/><NewLine>‘replica 0’, which is dedicated to <code>cuda:0</code> is worked well(backward completed well), and waiting for ‘replica 1’, which is dedicated to <code>cuda:1</code> to be completed with it’s backward, but in this case, some error occurred to him and failed to backward, so ‘replica 0’ keeps waiting and thus <code>'loss backwarded'</code> log didn’t show up.</p><NewLine><p>Further more assuming that,<br/><NewLine>even embedding tensor in each replica’s model, looks like splitted well and dedicated well like this,</p><NewLine><pre><code class=""lang-auto"">class Embedding(nn.Module):<NewLine>    def __init__ <NewLine>        ...<NewLine>    def forward(self, x, seg):<NewLine>        seq_len = x.size(1)<NewLine>        pos = torch.arange(seq_len, dtype=torch.long, device=self.device)<NewLine>        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -&gt; (batch_size, seq_len)<NewLine>        pos.requires_grad = False<NewLine>        print(f'pos tensor device: {pos.device}, shape: {pos.shape}')<NewLine>        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)<NewLine>        return self.norm(embedding)<NewLine><NewLine><NewLine>pos tensor device: cuda:1, shape: torch.Size([64, 513])<NewLine>pos tensor device: cuda:0, shape: torch.Size([64, 513])<NewLine><NewLine>embedding tensor device: cuda:1, shape: torch.Size([64, 513, 384])<NewLine>embedding tensor device: cuda:0, shape: torch.Size([64, 513, 384])<NewLine></code></pre><NewLine><p>and about other <code>cuda()</code> defined tensor in the model, too</p><NewLine><pre><code class=""lang-auto"">context tensor device: cuda:1, shape: torch.Size([64, 513, 768])<NewLine>multihead output tensor device: cuda:1, shape: torch.Size([64, 513, 384])<NewLine>context tensor device: cuda:1, shape: torch.Size([64, 513, 768])<NewLine>multihead output tensor device: cuda:1, shape: torch.Size([64, 513, 384])<NewLine>context tensor device: cuda:1, shape: torch.Size([64, 513, 768])<NewLine>multihead output tensor device: cuda:1, shape: torch.Size([64, 513, 384])<NewLine>context tensor device: cuda:0, shape: torch.Size([64, 513, 768])<NewLine>multihead output tensor device: cuda:0, shape: torch.Size([64, 513, 384])<NewLine>context tensor device: cuda:1, shape: torch.Size([64, 513, 768])<NewLine>multihead output tensor device: cuda:1, shape: torch.Size([64, 513, 384])<NewLine>context tensor device: cuda:0, shape: torch.Size([64, 513, 768])<NewLine>multihead output tensor device: cuda:0, shape: torch.Size([64, 513, 384])<NewLine>context tensor device: cuda:1, shape: torch.Size([64, 513, 768])<NewLine>multihead output tensor device: cuda:1, shape: torch.Size([64, 513, 384])<NewLine>context tensor device: cuda:0, shape: torch.Size([64, 513, 768])<NewLine>multihead output tensor device: cuda:0, shape: torch.Size([64, 513, 384])<NewLine>context tensor device: cuda:1, shape: torch.Size([64, 513, 768])<NewLine>multihead output tensor device: cuda:1, shape: torch.Size([64, 513, 384])<NewLine>context tensor device: cuda:0, shape: torch.Size([64, 513, 768])<NewLine>multihead output tensor device: cuda:0, shape: torch.Size([64, 513, 384])<NewLine>context tensor device: cuda:0, shape: torch.Size([64, 513, 768])<NewLine>multihead output tensor device: cuda:0, shape: torch.Size([64, 513, 384])<NewLine>context tensor device: cuda:0, shape: torch.Size([64, 513, 768])<NewLine>multihead output tensor device: cuda:0, shape: torch.Size([64, 513, 384])<NewLine></code></pre><NewLine><p>and the last logit gathered well, too,</p><NewLine><pre><code class=""lang-auto"">logtis_lm shape : torch.Size([128, 5, 6015])<NewLine></code></pre><NewLine><p>There are some error could happened at backward phase, but I cannot even imagine about it…</p><NewLine><p>Or maybe the sum about loss, <code>loss = loss_lm + loss_clsf</code> or <code>loss_lm.float().mean()</code> after get gathered outputs, affects to backward phase, I think but … I don’t know what should I do.</p><NewLine><pre><code class=""lang-auto"">                loss_lm = self.criterion(logits_lm.transpose(1, 2), batch.masked_tokens.transpose(0,1)) # for masked LM<NewLine>                print('loss_lm calculated')<NewLine>                print(f'loss_lm tensor device: {loss_lm.device}, shape: {loss_lm.shape}')<NewLine>                loss_lm = (loss_lm.float()).mean()<NewLine>                print(f'loss_lm tensor device: {loss_lm.device}, shape: {loss_lm.shape}')<NewLine>                loss_clsf = self.criterion(logits_clsf, batch.is_next) # for sentence classification<NewLine>                print('loss_clsf calculated')<NewLine>                print(f'loss_clsf tensor device: {loss_clsf.device}, shape: {loss_clsf.shape}')<NewLine>                loss = loss_lm + loss_clsf<NewLine>                print(f'sumloss tensor device: {loss.device}, shape: {loss.shape}')<NewLine>                loss.backward()<NewLine>                self.optimizer.step()<NewLine>                print('stepped')<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""90392"" data-username=""cybaj""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/c/f04885/40.png"" width=""20""/> cybaj:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">    def forward(self, x, seg):<NewLine>        seq_len = x.size(1)<NewLine>        pos = torch.arange(seq_len, dtype=torch.long, device=self.device)<NewLine>        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -&gt; (batch_size, seq_len)<NewLine>        pos.requires_grad = False<NewLine>        print(f'pos tensor device: {pos.device}, shape: {pos.shape}')<NewLine>        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)<NewLine>        return self.norm(embedding)<NewLine><NewLine><NewLine>pos tensor device: cuda:1, shape: torch.Size([64, 513])<NewLine>pos tensor device: cuda:0, shape: torch.Size([64, 513])<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>Hey <a class=""mention"" href=""/u/cybaj"">@cybaj</a>, I am a little confused about the above code. IIUC, the <code>self.device</code> attribute on both replicas point to <code>cuda:0</code>, as <code>replicate.py</code> is not smart enough to  change that for you. In that case how did <code>pos</code> successfully located to different devices? Looks like <code>expand_as</code> would not automatically change device either:</p><NewLine><pre><code class=""lang-python"">&gt;&gt;&gt; import torch<NewLine>&gt;&gt;&gt; x = torch.arange(10, device=""cuda:0"")<NewLine>&gt;&gt;&gt; y = torch.ones(10, 10).to(1)<NewLine>&gt;&gt;&gt; z = x.expand_as(y)<NewLine>&gt;&gt;&gt; z.device<NewLine>device(type='cuda', index=0)<NewLine>&gt;&gt;&gt; y.device<NewLine>device(type='cuda', index=1)<NewLine>&gt;&gt;&gt; x.device<NewLine>device(type='cuda', index=0)<NewLine></code></pre><NewLine><p>If you would like to get the correct device, can you read that from <code>x.device</code>? (input to forward should be scattered properly.)</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you, <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> . I change all <code>self.device</code> to something like <code>x.device</code> on forwarding.</p><NewLine><p>… It looks like works but still, stuck after <code>loss.backward()</code> .<br/><NewLine>All loss calculated and, loss tensor is on <code>cuda:0</code>, which is default output device.</p><NewLine><pre><code class=""lang-auto"">loss_lm calculated<NewLine>loss_lm : 72.27577209472656<NewLine>loss_lm tensor device: cuda:0, shape: torch.Size([])<NewLine>after mean loss_lm : 72.27577209472656<NewLine>after mean loss_lm tensor device: cuda:0, shape: torch.Size([])<NewLine>loss_clsf calculated<NewLine>loss_clsf : 0.7298979759216309<NewLine>loss_clsf tensor device: cuda:0, shape: torch.Size([])<NewLine>sumloss tensor device: cuda:0, shape: torch.Size([])<NewLine></code></pre><NewLine><p>Why it is stuck when <code>cuda:0</code> loss tensor starts to backward?<br/><NewLine>…</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/cybaj"">@cybaj</a>, could you please share a self-contained min repro program? It will be hard to tell with just printed outputs.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>It is totally my bad, thank you <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>. I learned how to create and use the tensor at module’s forward in DataParallel from your advices.</p><NewLine><p><code>os.environ['CUDA_LAUNCH_BLOCKING'] = '1' </code>  is the fault…</p><NewLine><p>Someone who use this cuda option for checking the logs, because it stops DataParallel process, it is recommend to comment it…</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/cybaj; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/cybaj; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/cybaj; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/cybaj; <NewLine> ,"REPLY_DATE 1: July 24, 2020, 12:52pm; <NewLine> REPLY_DATE 2: July 24, 2020,  5:33pm; <NewLine> REPLY_DATE 3: July 24, 2020,  5:34pm; <NewLine> REPLY_DATE 4: July 25, 2020,  4:23am; <NewLine> REPLY_DATE 5: July 27, 2020,  4:01pm; <NewLine> REPLY_DATE 6: July 29, 2020,  4:22pm; <NewLine> REPLY_DATE 7: July 29, 2020,  5:51pm; <NewLine> REPLY_DATE 8: July 31, 2020,  7:44am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> 
90856,DistributedDataParallel gradient avergaing,2020-07-28T16:11:56.291Z,4,131,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am experimenting with gradient compression techniques for reduced communication during distributed training. However, I found out that DDP by default averages the replica gradients with all-reduce. Is there some way to “turn this OFF”, since I will be aggregating the gradients in an encoded format?</p><NewLine></div>",https://discuss.pytorch.org/u/vineeths,,vineeths,"July 28, 2020,  4:11pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>On the master branch, there is a prototype DDP communication hook feature, which is built for this purpose: <a href=""https://github.com/pytorch/pytorch/issues/39272"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/39272</a></p><NewLine><p>In prior releases (&lt;= v1.6), there is no way to turn gradient averaging off without modifying C++ code and recompile.</p><NewLine><h3>Update</h3><NewLine><p>Synced with Sinan (author of this comm hook feature), this will be reverted due to perf regression. We are investigating.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Examples can be found here: <a href=""https://github.com/pytorch/pytorch/blob/c76fada4a859742ac679013b7428017a782e1432/torch/nn/parallel/distributed.py#L607-L684"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/c76fada4a859742ac679013b7428017a782e1432/torch/nn/parallel/distributed.py#L607-L684</a></p><NewLine><p>IIUC, as of today, the communication bucket is still divided by the world size even if the hook is enabled. We are working on removing that division.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/c76fada4a859742ac679013b7428017a782e1432/torch/csrc/distributed/c10d/reducer.cpp#L355-L356"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/c76fada4a859742ac679013b7428017a782e1432/torch/csrc/distributed/c10d/reducer.cpp#L355-L356"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/c76fada4a859742ac679013b7428017a782e1432/torch/csrc/distributed/c10d/reducer.cpp#L355-L356</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""355"" style=""counter-reset: li-counter 354 ;""><NewLine><li>auto wrapped =</li><NewLine><li>    c10::scalar_to_tensor(double(1.) / process_group_-&gt;getSize());</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks Shen Li.</p><NewLine><p>I had in fact already seen the DDP communication hook PR and had interacted with SInan as well. I was actually looking for something more flexible which would allow me to measure the time and bits during communication.</p><NewLine><p>I will definitely check the comm hook once ready.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I was actually looking for something more flexible which would allow me to measure the time and bits during communication.</p><NewLine></blockquote><NewLine><p>It should be possible to do this in the current proposal of the communication hook. Could you elaborate a bit more on the limitations in the current proposal that might prevent us from doing these measurements?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>As of now, I plan to measure the time taken for gradient accumulation, and the number of bits communicated (for each iteration). I might need to even find the bits communicated for each layer in the future to explore layer wise compression.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>As of now, I plan to measure the time taken for gradient accumulation</p><NewLine></blockquote><NewLine><p>Are you referring to the <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/accumulate_grad.h#L24"" rel=""nofollow noopener"">AccumulateGrad</a> function? If so, the autograd profiler would display time taken for this function.</p><NewLine><blockquote><NewLine><p>and the number of bits communicated (for each iteration)</p><NewLine></blockquote><NewLine><p>This should be possible in the current proposal of the communication hook since you can add up the bits for all the buckets.</p><NewLine><blockquote><NewLine><p>I might need to even find the bits communicated for each layer in the future to explore layer wise compression.</p><NewLine></blockquote><NewLine><p>This is probably something that you can’t do with the existing hook since it provides the entire bucket to the user and currently there is no way to split out individual parameters from the bucket.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vineeths; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/vineeths; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> ,"REPLY_DATE 1: July 28, 2020,  4:39pm; <NewLine> REPLY_DATE 2: July 28, 2020,  4:31pm; <NewLine> REPLY_DATE 3: July 29, 2020,  1:36am; <NewLine> REPLY_DATE 4: July 29, 2020,  8:26pm; <NewLine> REPLY_DATE 5: July 30, 2020, 11:14am; <NewLine> REPLY_DATE 6: July 30, 2020,  7:50pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
91117,Module.state_dict() is wrong when using DataParallel,2020-07-30T15:08:32.005Z,0,68,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a module like this:</p><NewLine><pre><code class=""lang-auto"">class Block(nn.Module):<NewLine>    def __init__(self, net):<NewLine>        super(Block, self).__init__()<NewLine>        self.net = net<NewLine>        self.net_copy = copy.deepcopy(net)<NewLine><NewLine>    def forward(self, x):<NewLine>        self.net_copy.load_state_dict(self.net.state_dict())<NewLine>        return self.net(x)<NewLine></code></pre><NewLine><p>The <code>net</code> is an nn.Sequential() module. When I use Pytorch&gt;=1.5 and use nn.DataParallel in multi-GPUs, It shows that net_copy.state_dict().keys() is different with net.state_dict().keys(). However, when I use Pytorch==1.4 or single-GPU, this problem doesn’t appear. How can I make sure that <code>net</code> and <code>net_copy</code> is exactly the same?</p><NewLine></div>",https://discuss.pytorch.org/u/LuChengTHU,,LuChengTHU,"July 30, 2020,  3:08pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is probably due to this PR: <a href=""https://github.com/pytorch/pytorch/pull/33907"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/33907</a></p><NewLine><p>In v1.5, parameters on replicated models are no longer considered as leaves, as they shouldn’t be. If you really need to access those replicated parameters, you probably can get them from <code>_former_parameters</code> and manually add them into the <code>stat_dict</code>?</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/c93e96fbd9903e576c6c1aa2fe12d8d548ae2d5b/torch/nn/parallel/replicate.py#L148"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/c93e96fbd9903e576c6c1aa2fe12d8d548ae2d5b/torch/nn/parallel/replicate.py#L148"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/c93e96fbd9903e576c6c1aa2fe12d8d548ae2d5b/torch/nn/parallel/replicate.py#L148</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""138"" style=""counter-reset: li-counter 137 ;""><NewLine><li>            replica._parameters[key] = None</li><NewLine><li>    else:</li><NewLine><li>        param_idx = param_indices[param]</li><NewLine><li>        for j in range(num_replicas):</li><NewLine><li>            replica = module_copies[j][i]</li><NewLine><li>            param = param_copies[j][param_idx]</li><NewLine><li>            # parameters in replicas are no longer leaves,</li><NewLine><li>            # so setattr them as non-parameter attributes</li><NewLine><li>            setattr(replica, key, param)</li><NewLine><li>            # expose the parameter for DDP</li><NewLine><li class=""selected"">            replica._former_parameters[key] = param</li><NewLine><li>for key, buf in module._buffers.items():</li><NewLine><li>    if buf is None:</li><NewLine><li>        for j in range(num_replicas):</li><NewLine><li>            replica = module_copies[j][i]</li><NewLine><li>            replica._buffers[key] = None</li><NewLine><li>    else:</li><NewLine><li>        if buf.requires_grad and not detach:</li><NewLine><li>            buffer_copies = buffer_copies_rg</li><NewLine><li>            buffer_idx = buffer_indices_rg[buf]</li><NewLine><li>        else:</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>cc <a class=""mention"" href=""/u/ngimel"">@ngimel</a> please correct me if I am wrong. And any thoughts on whether we should make <code>state_dict()</code> consistent between v1.4 vs v1.5?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: July 31, 2020,  8:05am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
90909,I want to print error messages only once when using multi-GPU,2020-07-29T03:50:21.828Z,1,71,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone, when I train the model with 4 GPUs using pytorch distributed, I see error messages 4 times if there is any, how to make it only appear once? Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/mcorange,,mcorange,"July 29, 2020,  3:50am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you start the training using a function that gets a <code>rank</code> argument passed, I used this:</p><NewLine><pre><code class=""lang-python"">if rank == 0:<NewLine>    print(""error message"")<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you want to suppress both stdout and stderr outputs, you can try this:</p><NewLine><pre><code class=""lang-python"">import sys, os<NewLine><NewLine>f = open(os.devnull, ""w"")<NewLine>if rank != 0:<NewLine>    sys.stdout = f<NewLine>    sys.stderr = f<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks. It is exactly what I wanted.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/siheming; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mcorange; <NewLine> ,"REPLY_DATE 1: July 29, 2020, 12:04pm; <NewLine> REPLY_DATE 2: July 30, 2020, 11:47am; <NewLine> REPLY_DATE 3: July 30, 2020, 11:47am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> REPLY 3 LIKES: ; <NewLine> 
90530,How to switch model from training to evaluation?,2020-07-25T18:43:09.933Z,7,138,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello! I am trying to set up a training script using DistributedDataParallel (DDP) where the model changes between training and evaluation modes. However, when I try to switch into evaluation mode with <code>model=model.eval()</code> <code>model</code> becomes a <code>NoneType</code>. I also tried to use <code>model=model.train(False)</code> but the result was the same.</p><NewLine><p>My issue is reproduceable with modifying the <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">DDP example</a>, thus:</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine>import torch.multiprocessing as mp<NewLine><NewLine>from torch.nn.parallel import DistributedDataParallel as DDP<NewLine><NewLine><NewLine>def setup(rank, world_size):<NewLine>    os.environ['MASTER_ADDR'] = 'localhost'<NewLine>    os.environ['MASTER_PORT'] = '12355'<NewLine><NewLine>    # initialize the process group<NewLine>    dist.init_process_group(""gloo"", rank=rank, world_size=world_size)<NewLine><NewLine><NewLine>def cleanup():<NewLine>    dist.destroy_process_group()<NewLine><NewLine><NewLine>class ToyModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(ToyModel, self).__init__()<NewLine>        self.net1 = nn.Linear(10, 10)<NewLine>        self.drop1 = nn.Dropout(p=0.6)<NewLine>        self.relu = nn.ReLU()<NewLine>        self.net2 = nn.Linear(10, 5)<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.net2(self.relu(self.drop1(self.net1(x))))<NewLine><NewLine><NewLine>def demo_basic(rank, world_size):<NewLine>    print(f""Running basic DDP example on rank {rank}."")<NewLine>    setup(rank, world_size)<NewLine><NewLine>    # create model and move it to GPU with id rank<NewLine>    model = ToyModel().to(rank)<NewLine>    ddp_model = DDP(model, device_ids=[rank])<NewLine><NewLine>    loss_fn = nn.MSELoss()<NewLine>    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)<NewLine><NewLine>    # Training mode<NewLine>    print(""Training"")<NewLine>    optimizer.zero_grad()<NewLine>    outputs = ddp_model(torch.randn(20, 10))<NewLine>    labels = torch.randn(20, 5).to(rank)<NewLine>    loss_fn(outputs, labels).backward()<NewLine>    optimizer.step()<NewLine><NewLine>    # Evaluation mode<NewLine>    print(""Evaluating"")<NewLine>    ddp_model = ddp_model.eval()<NewLine>    outputs = ddp_model(torch.randn(20, 10))<NewLine><NewLine>    cleanup()<NewLine><NewLine><NewLine>def run_demo(demo_fn, world_size):<NewLine>    mp.spawn(demo_fn,<NewLine>             args=(world_size,),<NewLine>             nprocs=world_size,<NewLine>             join=True)<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    run_demo(demo_basic, 1)<NewLine><NewLine></code></pre><NewLine><p>What is the proper way of switching between modes DDP? (Or it is not intended to be switched?)</p><NewLine><p>Thank you in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/Dudly01,,Dudly01,"July 26, 2020,  9:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am pinging this as somebody might have more insight into it than me <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, this issue hasn’t been fixed in 1.5.0, but has been fixed in 1.5.1:</p><NewLine><p>v1.5.0:</p><NewLine><pre><code class=""lang-auto"">    def train(self, mode=True):<NewLine>        super(DistributedDataParallel, self).train(mode)<NewLine>        for module in self._module_copies[1:]:<NewLine>            module.train(mode)<NewLine></code></pre><NewLine><p>is not returning <code>self</code></p><NewLine><p>v1.5.1</p><NewLine><pre><code class=""lang-auto"">    def train(self, mode=True):<NewLine>        self.training = mode<NewLine>        for module in self.children():<NewLine>            module.train(mode)<NewLine>        return self<NewLine></code></pre><NewLine><p>is returning self.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your answer.</p><NewLine><p>Strangely enough I am using the version 1.5.1 and the line returning <code>self</code> is present in the <code>train()</code> function. I even tried to reinstall 1.5.1 after cleaning conda cache. Then I created a a new conda environment and installed pytorch with python 3.8 (as I originally was using 3.7). However, the problem was still there.</p><NewLine><p>The only thing I did not try was to insall the nightly-builds, as I could not download it within 7 minutes and lost patience. <img alt="":sweat_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/sweat_smile.png?v=9"" title="":sweat_smile:""/></p><NewLine><p>However, if the intended way of switching is not different from the non DistributedDataParallel case then I am glad. I was just starting out with DistributedDataParallel and was not sure whether its possible to switch modes, or one has to define the mode before using the wrapper or some other magic.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like we still miss that return at least in master. I am not sure whether some earlier changes were applied but got revert or not. Adding it in <a href=""https://github.com/pytorch/pytorch/pull/42131"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/42131</a></p><NewLine><blockquote><NewLine><p>I was just starting out with DistributedDataParallel and was not sure whether it’s possible to switch modes, or one has to define the mode before using the wrapper or some other magic.</p><NewLine></blockquote><NewLine><p>DDP’s <code>train()</code> and <code>eval()</code> should work as expected. Just please remember to wrap it with <code>torch.no_grad()</code> when running in eval mode.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just tried it out with 1.6.0 but it seems your commit did not make it into it (or the issue is elsewhere <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/> )</p><NewLine><p>On the other hand, thank you very much for mentioning <code>torch.no_grad()</code>! It was a feature I was not aware of yet, and helped me out tremendously.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Lastly, thank you <a class=""mention"" href=""/u/iffix"">@iffiX</a> and <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> for taking your time to answer.<br/><NewLine>Both of you were a big help!</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""90530"" data-username=""Dudly01""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/d/eada6e/40.png"" width=""20""/> Dudly01:</div><NewLine><blockquote><NewLine><p>I just tried it out with 1.6.0 but it seems your commit did not make it into it (or the issue is elsewhere <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/> )</p><NewLine></blockquote><NewLine></aside><NewLine><p>It will be included in v1.7. The branch cut date for v1.6 was a few weeks ago.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>In the meantime I also realized that as my only intention is to switch my model to evaluation mode, I can also accomplish it with <code>model.eval()</code> and there is no real need for using <code>model = model.eval()</code>.</p><NewLine><p>I leave this here for future reference aiding ppl like me. <img alt="":slightly_smiling_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slightly_smiling_face.png?v=9"" title="":slightly_smiling_face:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Dudly01; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Dudly01; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Dudly01; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Dudly01; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Dudly01; <NewLine> ,"REPLY_DATE 1: July 26, 2020,  8:47pm; <NewLine> REPLY_DATE 2: July 29, 2020,  7:11pm; <NewLine> REPLY_DATE 3: July 27, 2020,  8:16pm; <NewLine> REPLY_DATE 4: July 29, 2020,  7:11pm; <NewLine> REPLY_DATE 5: July 29, 2020,  7:10pm; <NewLine> REPLY_DATE 6: July 29, 2020,  7:13pm; <NewLine> REPLY_DATE 7: July 29, 2020,  8:42pm; <NewLine> REPLY_DATE 8: July 29, 2020,  9:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> 
89248,Distributed.all_reduce returns strange results,2020-07-15T03:57:18.041Z,5,126,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I use DDP module to train ImageNet. To collect training metrics from different GPUs, I use distributed.all_reduce. Here are some related codes:</p><NewLine><pre><code class=""lang-auto"">local_rank = args.local_rank<NewLine>torch.cuda.set_device(local_rank)<NewLine>device = torch.device(""cuda"", local_rank)<NewLine><NewLine>for epoch in range(args.num_epoch + args.warmup_epoch):    <NewLine>    start = time.time()<NewLine>    train_loss, train_acc = utils.train_one_epoch(net, train_loader, criterion, optimizer, scheduler)<NewLine>    val_loss, val_acc = utils.test_one_epoch(net, val_loader, criterion)<NewLine>    #train_loss, train_acc, val_loss, val_acc are floating numbers <NewLine>    reduce_tensor = torch.tensor([train_loss, train_acc, val_loss, val_acc]).to(device)<NewLine>    torch.distributed.all_reduce(reduce_tensor)<NewLine>    reduce_tensor /= args.num_gpus<NewLine>    # args.num_gpus = 8<NewLine>    time_used = (time.time() - start) / 60.<NewLine><NewLine>    if local_rank == 0:<NewLine>        print('Epoch %d train loss %.3f acc: %.3f%%; val loss: %.3f acc %.3f%%; use %.3f mins.'%<NewLine>            (epoch, reduce_tensor[0], reduce_tensor[1], reduce_tensor[2], reduce_tensor[3], time_used))<NewLine></code></pre><NewLine><p>I only get wrong results in the last epoch. Here are some logs:</p><NewLine><p>log1:</p><NewLine><pre><code class=""lang-auto"">Epoch 97 train loss 0.892 acc: 77.805%; val loss: 0.930 acc 77.010%; use 8.296 mins.<NewLine>Epoch 98 train loss 0.887 acc: 77.922%; val loss: 0.931 acc 77.024%; use 8.305 mins.<NewLine>Epoch 99 train loss 0.422 acc: 38.989%; val loss: 0.459 acc 38.506%; use 8.300 mins.<NewLine></code></pre><NewLine><p>All metrics are 4/8 of expected. It seems that results from 4 GPUs are 0.</p><NewLine><p>log2:</p><NewLine><pre><code class=""lang-auto"">Epoch 96 train loss 0.973 acc: 75.933%; val loss: 0.967 acc 76.188%; use 9.449 mins.<NewLine>Epoch 97 train loss 0.969 acc: 76.003%; val loss: 0.967 acc 76.148%; use 9.459 mins.<NewLine>Epoch 98 train loss 0.969 acc: 76.029%; val loss: 0.967 acc 76.228%; use 9.445 mins.<NewLine>Epoch 99 train loss 1.333 acc: 104.523%; val loss: 1.326 acc 104.876%; use 9.452 mins.<NewLine></code></pre><NewLine><p>All metrics are 11/8 of expected: 1.333 / (11/8)=0.969. It seems that results from 3 GPUs are repeated in all reduce. <strong>The strange results only happen in the final epoch</strong>. What could be the possible reasons?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/KaiHoo,(Kai Hu),KaiHoo,"July 15, 2020,  4:00am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Your program seems to be correct, some questions:</p><NewLine><ol><NewLine><li>what backend are you using?</li><NewLine></ol><NewLine><p>and could you please add these tests?:</p><NewLine><ol><NewLine><li>change device to “cpu”, will the error be the same?</li><NewLine><li>print <code>rank, train_loss, train_acc, val_loss, val_acc</code> in each of your process, before <code>all_reduce</code><NewLine></li><NewLine></ol><NewLine><p>theoretically this problem should not happen</p><NewLine><ol><NewLine><li>the default gloo backend supports gpu <code>all_reduce</code> and <code>broadcast</code><NewLine></li><NewLine><li>you cannot repeat or let out a process, since <code>all_reduce</code> is blocking</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well, if you use nccl, then it must be a cuda tensor. please go on <img alt="":slightly_smiling_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slightly_smiling_face.png?v=9"" title="":slightly_smiling_face:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for responding! I use NCCL as backend:</p><NewLine><pre><code class=""lang-auto"">torch.distributed.init_process_group(backend=""nccl"")<NewLine></code></pre><NewLine><p>If I change the device to ‘cpu’, there is an error: Tensors must be CUDA and dense<br/><NewLine>I will try to print them before all_reduce, and see what happens.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/kaihoo"">@KaiHoo</a> can you print the <code>reduce_tensor</code> before you pass it to <code>all_reduce</code>, so that we can narrow down whether it is the <code>all_reduce</code> or the DDP training/testing that’s mal-bahaving.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/iffix"">@iffiX</a> <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> Hello, sorry for the late reply. Here are part of the codes:</p><NewLine><pre><code class=""lang-auto"">Epoch 95 train loss 1.056 acc: 74.176%; val loss: 0.954 acc 75.958%; use 12.457 mins.<NewLine>Epoch 96 train loss 1.048 acc: 74.339%; val loss: 0.949 acc 75.998%; use 12.459 mins.<NewLine>Epoch 97 train loss 1.028 acc: 74.815%; val loss: 0.946 acc 76.232%; use 12.455 mins.<NewLine>Epoch 98 train loss 1.027 acc: 74.855%; val loss: 0.946 acc 76.236%; use 12.475 mins.<NewLine>Rank 5 train loss 1.026 acc: 74.890%; val loss: 0.972 acc 75.600%<NewLine>Rank 6 train loss 1.025 acc: 74.815%; val loss: 0.924 acc 76.352%<NewLine>Rank 3 train loss 1.025 acc: 74.889%; val loss: 0.957 acc 75.632%<NewLine>Rank 1 train loss 1.032 acc: 74.757%; val loss: 0.929 acc 76.960%<NewLine>Rank 7 train loss 1.023 acc: 75.038%; val loss: 0.930 acc 76.512%<NewLine>Rank 2 train loss 1.019 acc: 75.013%; val loss: 0.958 acc 76.144%<NewLine>Exception ignored in: &lt;function WeakValueDictionary.__init__.&lt;locals&gt;.remove at 0x7ffb5070eea0&gt;<NewLine>Traceback (most recent call last):<NewLine>  File ""/usr/lib/python3.5/weakref.py"", line 117, in remove<NewLine>TypeError: 'NoneType' object is not callable<NewLine>Rank 0 train loss 1.022 acc: 74.841%; val loss: 0.951 acc 76.304%<NewLine>Epoch 99 train loss 0.513 acc: 37.451%; val loss: 0.470 acc 38.097%; use 12.467 mins.<NewLine>Rank 4 train loss 1.023 acc: 74.984%; val loss: 0.947 acc 76.304%<NewLine></code></pre><NewLine><p>Since the strange results only happen in the final epoch, I only print the metrics for the last epoch. The order of logs are exactly what I got, though the ‘Rank 4’ line should be printed before the ‘Epoch 99 train’ line:</p><NewLine><pre><code class=""lang-auto"">for epoch in range(args.num_epoch + args.warmup_epoch):    <NewLine>    start = time.time()<NewLine>    train_loss, train_acc = utils.train_one_epoch(net, train_loader, <NewLine>        criterion, optimizer, mean_and_std, scheduler, args)<NewLine>    val_loss, val_acc = utils.test_one_epoch(net, val_loader, criterion, mean_and_std)<NewLine>    reduce_tensor = torch.tensor([train_loss, train_acc, val_loss, val_acc]).to(device)<NewLine>    if epoch == args.num_epoch + args.warmup_epoch - 1:<NewLine>        print('Rank %d train loss %.3f acc: %.3f%%; val loss: %.3f acc %.3f%%'%<NewLine>            (local_rank, reduce_tensor[0], reduce_tensor[1], reduce_tensor[2], reduce_tensor[3]))<NewLine>    torch.distributed.all_reduce(reduce_tensor)<NewLine>    reduce_tensor /= args.num_gpus<NewLine>    time_used = (time.time() - start) / 60.<NewLine><NewLine>    if local_rank == 0:<NewLine>        print('Epoch %d train loss %.3f acc: %.3f%%; val loss: %.3f acc %.3f%%; use %.3f mins.'%<NewLine>            (epoch, reduce_tensor[0], reduce_tensor[1], reduce_tensor[2], reduce_tensor[3], time_used))<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""89248"" data-username=""KaiHoo""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/kaihoo/40/22284_2.png"" width=""20""/> KaiHoo:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">Exception ignored in: &lt;function WeakValueDictionary.__init__.&lt;locals&gt;.remove at 0x7ffb5070eea0&gt;<NewLine>Traceback (most recent call last):<NewLine>  File ""/usr/lib/python3.5/weakref.py"", line 117, in remove<NewLine>TypeError: 'NoneType' object is not callable<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>Is the above error expected? How did you handle this? If this is handled by skipping/redoing that iteration, it might cause allreduce mismatch.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""89248"" data-username=""KaiHoo""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/kaihoo/40/22284_2.png"" width=""20""/> KaiHoo:</div><NewLine><blockquote><NewLine><p><code>File ""/usr/lib/python3.5/weakref.py"", line 117, in remove</code></p><NewLine></blockquote><NewLine></aside><NewLine><p>I have no idea about this error, though nothing happened. This bug is reported to PyTorch, but seems a bug of python:<br/><NewLine></p><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/2229"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/2229"" rel=""nofollow noopener"" target=""_blank"">[minor] Random ignored exception on exit of pytorch scripts</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2017-07-28"" data-format=""ll"" data-time=""11:37:13"" data-timezone=""UTC"">11:37AM - 28 Jul 17 UTC</span><NewLine></div><NewLine><div class=""date""><NewLine>          closed <span class=""discourse-local-date"" data-date=""2017-08-03"" data-format=""ll"" data-time=""00:14:15"" data-timezone=""UTC"">12:14AM - 03 Aug 17 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/cdluminate"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""cdluminate"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars3.githubusercontent.com/u/5723047?v=4"" width=""20""/><NewLine>          cdluminate<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">This happends randomly and I have no idea under what condition can this problem be reproduced.<NewLine>Exception ignored in: &lt;function WeakValueDictionary.__init__.&lt;locals&gt;.remove at...</p><NewLine></div><NewLine><div class=""labels""><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/23760"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/23760"" rel=""nofollow noopener"" target=""_blank""> torch.nn.parallel.data_parallel crashes machine and has a weakref bug</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-08-03"" data-format=""ll"" data-time=""17:04:10"" data-timezone=""UTC"">05:04PM - 03 Aug 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/PetrochukM"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""PetrochukM"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/7424737?v=4"" width=""20""/><NewLine>          PetrochukM<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">🐛 Bug<NewLine>For some reason, this happened:<NewLine>Exception ignored in: &lt;function WeakValueDictionary.__init__.&lt;locals&gt;.remove at 0x7f4b13e53158&gt;<NewLine>Traceback (most recent call last):<NewLine> File ""/usr/lib/python3.5/weakref.py"", line 117, in...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">high priority</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">module: data parallel</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/KaiHoo; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/KaiHoo; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/KaiHoo; <NewLine> ,"REPLY_DATE 1: July 15, 2020,  4:59am; <NewLine> REPLY_DATE 2: July 15, 2020,  5:10am; <NewLine> REPLY_DATE 3: July 15, 2020,  5:10am; <NewLine> REPLY_DATE 4: July 16, 2020,  3:07pm; <NewLine> REPLY_DATE 5: July 26, 2020,  3:53am; <NewLine> REPLY_DATE 6: July 27, 2020,  3:28pm; <NewLine> REPLY_DATE 7: July 29, 2020,  2:55pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
90581,DistributedDataParallel behaves weirdly,2020-07-26T14:15:28.873Z,7,253,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>to speed up my training I was looking into pytorches DistributedDataParallel, since the docs state that DataParallel has a lot of overhead which reduces the speed. I tested a couple of hyperparameters and found weird behavior, which left me wondering if I oversaw something.</p><NewLine><p>I am running on a linux-64 bit cluster node with 64 cores, 350+ GB of ram and 4 Nvidia Tesla V100 (16 GB). I tested the stable 1.5.0 version and nightly 1.7.0.dev20200720 because I wanted to use automated mixed precision as another speed up.</p><NewLine><p>The model I was testing which is a BERT model from the transformer library, with a single linear layer and a BCEWithLogitsLoss.</p><NewLine><p>I tested three different training modes (all single machine): 1. a single GPU. 2. multi GPU with DataParallel. 3. multi GPU with DistributedDataParallel.<br/><NewLine>Then I tested memory_pin, num_workers for the dataloader and mixed precision if possible.</p><NewLine><p>code for reference:</p><NewLine><pre><code class=""lang-python"">import os<NewLine>from datetime import datetime<NewLine>from argparse import ArgumentParser<NewLine>import torch<NewLine>import torch.multiprocessing as mp<NewLine>import torch.distributed as dist<NewLine>from transformers import AdamW, BertConfig<NewLine><NewLine>from prediction_module import path_vocab, path_raw_uniprot<NewLine>from prediction_module.protein_datasets import ProteinBertLabeledDataset<NewLine>from prediction_module.helpers import get_logger<NewLine><NewLine>logger = get_logger(__file__)<NewLine><NewLine><NewLine>def train_model_dp(dataset, batch_size=4, n_steps=1000, num_workers=0, parallel=True, mixed_pres=False, pin_memory=False):<NewLine>    from prediction_module.protein_models import ProteinBertForMultiLabel<NewLine>    if mixed_pres:<NewLine>        ProteinBertForMultiLabel.forward = torch.cuda.amp.autocast()(ProteinBertForMultiLabel.forward)<NewLine>    torch.manual_seed(0)<NewLine>    config = BertConfig(<NewLine>        vocab_size=dataset.tokenizer.vocab_size,<NewLine>        num_labels=dataset.num_labels,<NewLine>        max_position_embeddings=dataset.input_size<NewLine>    )<NewLine>    model = ProteinBertForMultiLabel(config)<NewLine>    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")<NewLine><NewLine>    if torch.cuda.device_count() &gt; 1 and parallel:<NewLine>        batch_size = batch_size * torch.cuda.device_count()<NewLine>        model = torch.nn.DataParallel(model)<NewLine><NewLine>    logger.debug(f""testing: {batch_size=} {num_workers=} {parallel=} {mixed_pres=} {pin_memory=}"")<NewLine>    dataloader = torch.utils.data.DataLoader(dataset,<NewLine>                                             batch_size=batch_size,<NewLine>                                             collate_fn=dataset.collate_fn,<NewLine>                                             shuffle=False,<NewLine>                                             num_workers=num_workers,<NewLine>                                             pin_memory=pin_memory)<NewLine><NewLine>    model.to(device)<NewLine>    model.train()<NewLine><NewLine>    optimizer = AdamW(model.parameters(), lr=1e-5)  # create optimizer<NewLine>    if mixed_pres:<NewLine>        scaler = torch.cuda.amp.GradScaler()<NewLine>    start = datetime.now()<NewLine><NewLine>    for epoch in range(1):  # loop over the dataset multiple times<NewLine>        for i, inputs in enumerate(dataloader):<NewLine>            for k, v in inputs.items():<NewLine>                if isinstance(v, torch.Tensor):<NewLine>                    inputs[k] = v.to(device, non_blocking=True)<NewLine>            # zero the parameter gradients<NewLine>            optimizer.zero_grad()<NewLine>            if mixed_pres:<NewLine>                with torch.cuda.amp.autocast():<NewLine>                    outputs = model(**inputs)<NewLine>                    loss = outputs[0]<NewLine>                    loss = loss.mean()<NewLine>                # Backward and optimize<NewLine>                scaler.scale(loss).backward()<NewLine>                scaler.step(optimizer)<NewLine>                scaler.update()<NewLine>            else:<NewLine>                # forward + backward + optimize<NewLine>                outputs = model(**inputs)<NewLine>                loss = outputs[0]<NewLine>                loss = loss.mean()<NewLine>                loss.backward()<NewLine>                optimizer.step()<NewLine>            if i &gt;= n_steps:<NewLine>                break<NewLine>    logger.debug(""Training complete in: %s. normalized by batch size: %s"", str(datetime.now() - start), str((datetime.now() - start) / batch_size))<NewLine><NewLine>def train_start(rank, world_size, batch_size=4, mixed_pres=False, pin_memory=True, num_workers=0, n_steps=1000, epochs=1):<NewLine>    from prediction_module.protein_models import ProteinBertForMultiLabel<NewLine>    if mixed_pres:<NewLine>        ProteinBertForMultiLabel.forward = torch.cuda.amp.autocast()(ProteinBertForMultiLabel.forward)<NewLine>    os.environ['MASTER_ADDR'] = 'localhost'<NewLine>    os.environ['MASTER_PORT'] = '12355'<NewLine><NewLine>    # initialize the process group<NewLine>    dist.init_process_group(""nccl"", rank=rank, world_size=world_size)<NewLine>    torch.manual_seed(0)<NewLine>    torch.cuda.set_device(rank)<NewLine>    dataset = ProteinBertLabeledDataset(<NewLine>        vocab=path_vocab,<NewLine>        csv_path=os.path.join(path_raw_uniprot, ""raw_data.csv""),<NewLine>        h5_path=os.path.join(path_raw_uniprot, ""metled_go_data.h5"")<NewLine>    )<NewLine>    config = BertConfig(<NewLine>        vocab_size=dataset.tokenizer.vocab_size,<NewLine>        num_labels=dataset.num_labels,<NewLine>        max_position_embeddings=dataset.input_size<NewLine>    )<NewLine>    model = ProteinBertForMultiLabel(config)<NewLine>    model.cuda(rank)<NewLine>    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank], output_device=rank,<NewLine>                                                      find_unused_parameters=True)<NewLine>    model.train()<NewLine>    optimizer = AdamW(model.parameters(), lr=1e-5)  # create optimizer<NewLine>    if mixed_pres:<NewLine>        scaler = torch.cuda.amp.GradScaler()<NewLine>    # Data loading code<NewLine>    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=world_size, rank=rank)<NewLine>    train_loader = torch.utils.data.DataLoader(dataset=dataset,<NewLine>                                               batch_size=batch_size,<NewLine>                                               shuffle=False,<NewLine>                                               num_workers=num_workers,<NewLine>                                               pin_memory=pin_memory,<NewLine>                                               sampler=train_sampler,<NewLine>                                               collate_fn=dataset.collate_fn)<NewLine><NewLine>    start = datetime.now()<NewLine>    for epoch in range(epochs):<NewLine>        for i, inputs in enumerate(train_loader):<NewLine>            for k, v in inputs.items():<NewLine>                if isinstance(v, torch.Tensor):<NewLine>                    inputs[k] = v.cuda(rank, non_blocking=True)<NewLine>            optimizer.zero_grad()<NewLine>            if mixed_pres:<NewLine>                with torch.cuda.amp.autocast():<NewLine>                    outputs = model(**inputs)<NewLine>                    loss = outputs[0]<NewLine>                    loss = loss.mean()<NewLine>                # Backward and optimize<NewLine>                scaler.scale(loss).backward()<NewLine>                scaler.step(optimizer)<NewLine>                scaler.update()<NewLine>            else:<NewLine>                outputs = model(**inputs)<NewLine>                loss = outputs[0]<NewLine>                loss = loss.mean()<NewLine>                loss.backward()<NewLine>                optimizer.step()<NewLine>            if i &gt;= n_steps:<NewLine>                break<NewLine>    if rank == 0:<NewLine>        logger.debug(""Training complete in: %s"", str(datetime.now() - start))<NewLine>    dist.destroy_process_group()<NewLine><NewLine><NewLine>def train_model_ddp(world_size=4, mixed_pres=False, batch_size=4, pin_memory=False, num_workers=0, n_steps=1000):<NewLine>    logger.debug(f""testing: {batch_size=} {num_workers=} {mixed_pres=} {pin_memory=}"")<NewLine>    mp.spawn(train_start,<NewLine>             args=(world_size, batch_size, mixed_pres, pin_memory, num_workers, n_steps),<NewLine>             nprocs=world_size,<NewLine>             join=True)<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    try:<NewLine>        from torch.cuda.amp import autocast<NewLine>        mp_avail = True<NewLine>    except ImportError:<NewLine>        mp_avail = False<NewLine>    parser = ArgumentParser()<NewLine>    parser.add_argument(""--test-dp"", dest=""test_dp"", default=False, const=True, nargs=""?"")<NewLine>    parser.add_argument(""--test-ddp"", dest=""test_ddp"", default=False, const=True, nargs=""?"")<NewLine>    args = parser.parse_args()<NewLine>    args_dict = vars(args)<NewLine>    logger.debug(""torch version: %s"", torch.__version__)<NewLine>    if args_dict[""test_dp""]:<NewLine>        dataset = ProteinBertLabeledDataset(<NewLine>            vocab=path_vocab,<NewLine>            csv_path=os.path.join(path_raw_uniprot, ""raw_data.csv""),<NewLine>            h5_path=os.path.join(path_raw_uniprot, ""metled_go_data.h5"")<NewLine>        )<NewLine>        logger.debug(""testing single gpu"")<NewLine>        train_model_dp(dataset, parallel=False)<NewLine>        train_model_dp(dataset, parallel=False)<NewLine>        if mp_avail:<NewLine>            train_model_dp(dataset, parallel=False, mixed_pres=True)<NewLine>        train_model_dp(dataset, parallel=False, num_workers=8)<NewLine>        train_model_dp(dataset, parallel=False, num_workers=16)<NewLine>        train_model_dp(dataset, parallel=False, pin_memory=True)<NewLine>        logger.debug(""testing dp"")<NewLine>        train_model_dp(dataset)<NewLine>        train_model_dp(dataset, num_workers=8)<NewLine>        train_model_dp(dataset, num_workers=16)<NewLine>        train_model_dp(dataset, pin_memory=True)<NewLine>        if mp_avail:<NewLine>            train_model_dp(dataset, mixed_pres=True)<NewLine>    if args_dict[""test_ddp""]:<NewLine>        logger.debug(""testing ddp"")<NewLine>        train_model_ddp()<NewLine>        train_model_ddp(pin_memory=True)<NewLine>        train_model_ddp(num_workers=8)<NewLine>        train_model_ddp(num_workers=16)<NewLine>        if mp_avail:<NewLine>            train_model_ddp(mixed_pres=True)<NewLine><NewLine></code></pre><NewLine><p>The results:</p><NewLine><blockquote><NewLine><p>testing single gpu<br/><NewLine>torch version: 1.5.0<br/><NewLine>testing: batch_size=4 num_workers=0 parallel=False mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:02:48.407579. normalized by batch size: 0:00:42.101900<br/><NewLine>testing: batch_size=4 num_workers=0 parallel=False mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:02:47.146963. normalized by batch size: 0:00:41.786745<br/><NewLine>testing: batch_size=4 num_workers=8 parallel=False mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:02:49.422436. normalized by batch size: 0:00:42.355613<br/><NewLine>testing: batch_size=4 num_workers=16 parallel=False mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:02:50.284026. normalized by batch size: 0:00:42.571010<br/><NewLine>testing: batch_size=4 num_workers=0 parallel=False mixed_pres=False pin_memory=True<br/><NewLine>Training complete in: 0:02:47.878925. normalized by batch size: 0:00:41.969736<br/><NewLine>testing dp<br/><NewLine>testing: batch_size=16 num_workers=0 parallel=True mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:05:32.129513. normalized by batch size: 0:00:20.758095<br/><NewLine>testing: batch_size=16 num_workers=8 parallel=True mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:05:28.702392. normalized by batch size: 0:00:20.543900<br/><NewLine>testing: batch_size=16 num_workers=16 parallel=True mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:05:29.794879. normalized by batch size: 0:00:20.612181<br/><NewLine>testing: batch_size=16 num_workers=0 parallel=True mixed_pres=False pin_memory=True<br/><NewLine>Training complete in: 0:05:24.955569. normalized by batch size: 0:00:20.309724</p><NewLine></blockquote><NewLine><blockquote><NewLine><p>torch version: 1.7.0.dev20200720<br/><NewLine>testing single gpu<br/><NewLine>testing: batch_size=4 num_workers=0 parallel=False mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:02:50.061025. normalized by batch size: 0:00:42.515261<br/><NewLine>testing: batch_size=4 num_workers=0 parallel=False mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:02:48.032688. normalized by batch size: 0:00:42.008176<br/><NewLine>testing: batch_size=4 num_workers=0 parallel=False mixed_pres=True pin_memory=False<br/><NewLine>Training complete in: 0:01:54.984463. normalized by batch size: 0:00:28.746120<br/><NewLine>testing: batch_size=4 num_workers=8 parallel=False mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:02:50.344483. normalized by batch size: 0:00:42.586124<br/><NewLine>testing: batch_size=4 num_workers=16 parallel=False mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:02:51.148356. normalized by batch size: 0:00:42.787092<br/><NewLine>testing: batch_size=4 num_workers=0 parallel=False mixed_pres=False pin_memory=True<br/><NewLine>Training complete in: 0:02:48.677086. normalized by batch size: 0:00:42.169276<br/><NewLine>testing dp<br/><NewLine>testing: batch_size=16 num_workers=0 parallel=True mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:05:30.977989. normalized by batch size: 0:00:20.686125<br/><NewLine>testing: batch_size=16 num_workers=8 parallel=True mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:05:26.893676. normalized by batch size: 0:00:20.430856<br/><NewLine>testing: batch_size=16 num_workers=16 parallel=True mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:05:28.139827. normalized by batch size: 0:00:20.508740<br/><NewLine>testing: batch_size=16 num_workers=0 parallel=True mixed_pres=False pin_memory=True<br/><NewLine>Training complete in: 0:05:22.767213. normalized by batch size: 0:00:20.172952<br/><NewLine>testing: batch_size=16 num_workers=0 parallel=True mixed_pres=True pin_memory=False<br/><NewLine>Training complete in: 0:04:26.452442. normalized by batch size: 0:00:16.653278</p><NewLine></blockquote><NewLine><blockquote><NewLine><p>torch version: 1.5.0<br/><NewLine>testing ddp<br/><NewLine>testing: batch_size=4 num_workers=0 mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:04:59.752312<br/><NewLine>testing: batch_size=4 num_workers=0 mixed_pres=False pin_memory=True<br/><NewLine>Training complete in: 0:04:59.236787<br/><NewLine>testing: batch_size=4 num_workers=8 mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:12:16.935697</p><NewLine></blockquote><NewLine><blockquote><NewLine><p>torch version: 1.7.0.dev20200720<br/><NewLine>testing ddp<br/><NewLine>testing: batch_size=4 num_workers=0 mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:05:02.979028<br/><NewLine>testing: batch_size=4 num_workers=0 mixed_pres=False pin_memory=True<br/><NewLine>Training complete in: 0:05:03.088308<br/><NewLine>testing: batch_size=4 num_workers=8 mixed_pres=False pin_memory=False<br/><NewLine>Training complete in: 0:11:05.255453<br/><NewLine>testing: batch_size=4 num_workers=0 mixed_pres=True pin_memory=False<br/><NewLine>Training complete in: 0:05:10.881854</p><NewLine></blockquote><NewLine><p>My interpretation<br/><NewLine>Training on a single GPU takes about 2:50 minutes for all parameters except mixed precision, which increases speed to around 2 minutes.<br/><NewLine>So perfect parallelization would mean that the same time would be required with 4 GPUs if every single GPU gets a mini-batch with size 4, correct?<br/><NewLine>DataParallel seems to behave very similar to the hyperparameters, training takes around 5:25 mintues, except for mixed precision, which decreases it to 4:25 minutes.</p><NewLine><p>Now to DistributedDataParallel:<br/><NewLine>Increasing the number of workers seems to slow down training by a lot.<br/><NewLine>Mixed precision has no effect on training speed (even though I observed on the GPUs that the required ram was decreased compared to not using it, and similar to the ram required for the mixed precision during DataParallel).</p><NewLine><p>This is the first time using pytorch, so if I oversaw anything please let me know. Otherwise I would be interested what caused these effects.</p><NewLine></div>",https://discuss.pytorch.org/u/siheming,,siheming,"July 26, 2020,  2:16pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, lets get to your points one by one</p><NewLine><ol><NewLine><li>DDP and DP is slow</li><NewLine></ol><NewLine><blockquote><NewLine><p>My interpretation<br/><NewLine>Training on a single GPU takes about 2:50 minutes for all parameters except mixed precision, which increases speed to around 2 minutes.<br/><NewLine>So perfect parallelization would mean that the same time would be required with 4 GPUs if every single GPU gets a mini-batch with size 4, correct?<br/><NewLine>DataParallel seems to behave very similar to the hyperparameters, training takes around 5:25 mintues, except for mixed precision, which decreases it to 4:25 minutes.</p><NewLine></blockquote><NewLine><blockquote><NewLine><p>Now to DistributedDataParallel:<br/><NewLine>Increasing the number of workers seems to slow down training by a lot.<br/><NewLine>Mixed precision has no effect on training speed (even though I observed on the GPUs that the required ram was decreased compared to not using it, and similar to the ram required for the mixed precision during DataParallel).</p><NewLine></blockquote><NewLine><p>Your interpretation is correct, indeed, however, in python, whether you use thread based parallelism or process based parallelism, you are faced with extremely high costs:</p><NewLine><ol><NewLine><li>for DP, it is mainly GIL cost</li><NewLine><li>for DDP, is is mainly process communication cost.<br/><NewLine>The best way to increase efficiency is “batching”, because that part all happens in C/C++/CUDA domain, therefore, in order to fully display the power of DDP, you must make sure that your model is dealling with about 100<em>800</em>800*3 size of data (about 100 frames of images), even using this much data in a forward process on ResNet probably would take less than a second on powerful GPUs (eg: V100, your GPU), I am not sure how big your model <code>prediction_module.protein_models</code> it is, if it is not large enough, then <code>batch=4</code> (16/4=4) per process probably is too small, and overhead of inter-process communication would demonstrate its annoying existence in this condition.</li><NewLine></ol><NewLine><p>More workers, more slowly, it is true in python, whether you are using threads or processes, unless there is no communication overhead (for thread, it is GIL, for process, it is repeated serialization &amp; deserialization, inter-process synchronization, etc.)</p><NewLine><p>DistributedSampler in this case could also be a major overhead contributor, since internally the DataLoader will use <code>_MultiProcessingDataLoaderIter</code>, which uses a inter process queue to get data from sub-processes, if your data is not big enough and number is large, then it is very likely that repeated serialization-deserialization would contribute <strong>a lot</strong> to the slowiness, because for cpu tensors, they have to be moved to the shared memory first, then the handle will be serialized, there is no way for you to avoid this nightmare, if you are using the inbuit datasampler.</p><NewLine><p>It is possible for you to customize your own dataloader, maybe load all data at once to your GPUs, (I believe V100 has enough memory to hold all of them, and you should have multiple V100s), however, it would <strong>require a huge quantity of time</strong> to debug your impementation.</p><NewLine><p>In summary:<br/><NewLine><em>Maybe…</em>, you should increase your batch size.<br/><NewLine>Live with this.<br/><NewLine>Make your own implementation.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the answer!</p><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""90581"" data-username=""iffiX""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/iffix/40/24443_2.png"" width=""20""/> iffiX:</div><NewLine><blockquote><NewLine><p>DistributedSampler in this case could also be a major overhead contributor, since internally the DataLoader will use <code>_MultiProcessingDataLoaderIter</code> , which uses a inter process queue to get data from sub-processes</p><NewLine></blockquote><NewLine></aside><NewLine><p>I was just testing some bottlenecks in my code an it indeed seems like the DistributedSampler is a major culprit, when used with <code>shuffle=True</code>. This might be because the dataset I was using in the tests has a length of 320 million samples. And it might also be the reason why I wasnt using bigger batch sizes because 10 GB of RAM seems to be occupied by the DistributedSampler.</p><NewLine><p>So I decided to use the sampler on a single GPU and look at the effects.</p><NewLine><p>code:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import os<NewLine>from transformers import AdamW, BertConfig, TrainingArguments, Trainer<NewLine>from datetime import datetime<NewLine><NewLine>from prediction_module import path_vocab, path_storage<NewLine>from prediction_module.protein_datasets import ProteinBertMaskedLMDataset<NewLine>from prediction_module.protein_models import ProteinBertForMaskedLM<NewLine><NewLine><NewLine>def train_single(dist_sampler=False, n_steps=100, shuffle=False):<NewLine>    rank = 0<NewLine>    dataset = ProteinBertMaskedLMDataset(<NewLine>        path_vocab, os.path.join(path_storage, ""data"", ""uniparc"", ""uniparc_train_sorted.h5""),<NewLine>    )<NewLine>    config = BertConfig(<NewLine>        vocab_size=dataset.tokenizer.vocab_size,<NewLine>        max_position_embeddings=dataset.input_size<NewLine>    )<NewLine>    model = ProteinBertForMaskedLM(config)<NewLine>    model.cuda(rank)<NewLine>    model.train()<NewLine><NewLine>    optimizer = AdamW(model.parameters(), lr=1e-5)  # create optimizer<NewLine>    # Data loading code<NewLine><NewLine>    sampler = DistributedSampler(dataset, 1, rank, shuffle=shuffle) if dist_sampler else None<NewLine>    loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False, num_workers=0, pin_memory=False,<NewLine>                                         sampler=sampler, collate_fn=dataset.collate_fn)<NewLine>    print(""start trainig"")<NewLine>    start = datetime.now()<NewLine>    for epoch in range(1):<NewLine>        for i, inputs in enumerate(loader):<NewLine>            for k, v in inputs.items():<NewLine>                if isinstance(v, torch.Tensor):<NewLine>                    inputs[k] = v.cuda(rank, non_blocking=True)<NewLine>            optimizer.zero_grad()<NewLine>            outputs = model(**inputs)<NewLine>            loss = outputs[0]<NewLine>            loss = loss.mean()<NewLine>            loss.backward()<NewLine>            optimizer.step()<NewLine>            if i &gt;= n_steps:<NewLine>                break<NewLine>    print(""Training complete in:"", str(datetime.now() - start))<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    train_single()<NewLine>    train_single(True)<NewLine>    train_single(True, shuffle=True)<NewLine><NewLine></code></pre><NewLine><p>The results:</p><NewLine><blockquote><NewLine><p>start trainig<br/><NewLine>Training complete in: 0:00:07.233262<br/><NewLine>start trainig<br/><NewLine>Training complete in: 0:00:19.662416<br/><NewLine>start trainig<br/><NewLine>Training complete in: 0:03:29.437496</p><NewLine></blockquote><NewLine><p>Additionally, the RAM on the GPU required for the first two function calls is about 3,5 GB and for the last one about 14 GB. I am not sure what is going on here, but that seems very weird.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think the data loader might have cached something resulting in that 10.5 GB extra memory, I am not very familiar with its internal design so this answer might be wrong.</p><NewLine><p>Anyway, you have 350GB+ memory, why worry about that?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Because the memory is allocated on the GPU. And 10.5 GB random allocation on the GPU is not that nice. (edited the previous post to make that more clear)</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Weird, seems that multiple processes have occupied your GPU, could you please post the result of <code>nvidia-smi</code> ?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here is the output:<br/><NewLine>during the first two runs it looks like this:</p><NewLine><pre><code class=""lang-auto"">+-----------------------------------------------------------------------------+<NewLine>| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |<NewLine>|-------------------------------+----------------------+----------------------+<NewLine>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<NewLine>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<NewLine>|===============================+======================+======================|<NewLine>|   0  Tesla V100-PCIE...  Off  | 00000000:18:00.0 Off |                    0 |<NewLine>| N/A   44C    P0    38W / 250W |   2857MiB / 16160MiB |      0%   E. Process |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   1  Tesla V100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |<NewLine>| N/A   43C    P0    27W / 250W |     12MiB / 16160MiB |      0%   E. Process |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   2  Tesla V100-PCIE...  Off  | 00000000:86:00.0 Off |                    0 |<NewLine>| N/A   41C    P0    27W / 250W |     12MiB / 16160MiB |      0%   E. Process |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   3  Tesla V100-PCIE...  Off  | 00000000:AF:00.0 Off |                    0 |<NewLine>| N/A   41C    P0    28W / 250W |     12MiB / 16160MiB |      0%   E. Process |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>+-----------------------------------------------------------------------------+<NewLine>| Processes:                                                       GPU Memory |<NewLine>|  GPU       PID   Type   Process name                             Usage      |<NewLine>|=============================================================================|<NewLine>|    0    284149      C   python                                      2845MiB |<NewLine>+-----------------------------------------------------------------------------+<NewLine><NewLine></code></pre><NewLine><p>During the last run:</p><NewLine><pre><code class=""lang-auto"">+-----------------------------------------------------------------------------+<NewLine>| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |<NewLine>|-------------------------------+----------------------+----------------------+<NewLine>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<NewLine>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<NewLine>|===============================+======================+======================|<NewLine>|   0  Tesla V100-PCIE...  Off  | 00000000:18:00.0 Off |                    0 |<NewLine>| N/A   46C    P0    43W / 250W |  14527MiB / 16160MiB |      0%   E. Process |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   1  Tesla V100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |<NewLine>| N/A   42C    P0    27W / 250W |     12MiB / 16160MiB |      0%   E. Process |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   2  Tesla V100-PCIE...  Off  | 00000000:86:00.0 Off |                    0 |<NewLine>| N/A   41C    P0    27W / 250W |     12MiB / 16160MiB |      0%   E. Process |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   3  Tesla V100-PCIE...  Off  | 00000000:AF:00.0 Off |                    0 |<NewLine>| N/A   41C    P0    28W / 250W |     12MiB / 16160MiB |      0%   E. Process |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>+-----------------------------------------------------------------------------+<NewLine>| Processes:                                                       GPU Memory |<NewLine>|  GPU       PID   Type   Process name                             Usage      |<NewLine>|=============================================================================|<NewLine>|    0    284149      C   python                                     14515MiB |<NewLine>+-----------------------------------------------------------------------------+<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hmmmmmmmmmmmmm, <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/iffix"">@iffiX</a> for covering distributed training questions!</p><NewLine><p><a class=""mention"" href=""/u/siheming"">@siheming</a> I wonder if those are cached blocks. Can you print memory summary using <a href=""https://pytorch.org/docs/stable/cuda.html#torch.cuda.memory_summary"" rel=""nofollow noopener""><code>torch.cuda.memory_summary</code></a>?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""9"" data-topic=""90581"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mrshenli/40/12220_2.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>Can you print memory summary using <a href=""https://pytorch.org/docs/stable/cuda.html#torch.cuda.memory_summary"" rel=""nofollow noopener""> <code>torch.cuda.memory_summary</code> </a>?</p><NewLine></blockquote><NewLine></aside><NewLine><p>when should I call it ? as soon as the memory is filled or after training? or does it not matter?</p><NewLine><p>Also I wanted to come back to this question because I have not seen a satisfying answer yet or understood why this might be the case:</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""90581"" data-username=""siheming""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/siheming/40/27189_2.png"" width=""20""/> siheming:</div><NewLine><blockquote><NewLine><p>Mixed precision has no effect on training speed (even though I observed on the GPUs that the required ram was decreased compared to not using it, and similar to the ram required for the mixed precision during DataParallel).</p><NewLine></blockquote><NewLine></aside><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""10"" data-topic=""90581"" data-username=""siheming""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/siheming/40/27189_2.png"" width=""20""/> siheming:</div><NewLine><blockquote><NewLine><p>when should I call it ? as soon as the memory is filled or after training? or does it not matter?</p><NewLine></blockquote><NewLine></aside><NewLine><p>It depends on when do you wants to inspect the memory usage. It prints current cached memory, allocated memory, etc. I would try to print it every few iterations.</p><NewLine><blockquote><NewLine><p>Mixed precision has no effect on training speed (even though I observed on the GPUs that the required ram was decreased compared to not using it, and similar to the ram required for the mixed precision during DataParallel).</p><NewLine></blockquote><NewLine><p>Are you using PyTorch v1.6+? I saw the DDP + AMP example is only available in v1.6+ docs:</p><NewLine><p><a class=""onebox"" href=""https://pytorch.org/docs/1.5.0/notes/amp_examples.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/1.5.0/notes/amp_examples.html</a><br/><NewLine><a class=""onebox"" href=""https://pytorch.org/docs/master/notes/amp_examples.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/master/notes/amp_examples.html</a></p><NewLine><p>cc the author of <code>torch.cuda.amp</code> <a class=""mention"" href=""/u/mcarilli"">@mcarilli</a></p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""11"" data-topic=""90581"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mrshenli/40/12220_2.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>It depends on when do you wants to inspect the memory usage. It prints current cached memory, allocated memory, etc. I would try to print it every few iterations.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I’ll get on that tomorrow and post the results.</p><NewLine><aside class=""quote no-group"" data-post=""11"" data-topic=""90581"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mrshenli/40/12220_2.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>Are you using PyTorch v1.6+? I saw the DDP + AMP example is only available in v1.6+ docs:</p><NewLine><p><a href=""https://pytorch.org/docs/1.5.0/notes/amp_examples.html"" rel=""nofollow noopener"">https://pytorch.org/docs/1.5.0/notes/amp_examples.html</a><br/><NewLine><a href=""https://pytorch.org/docs/master/notes/amp_examples.html"" rel=""nofollow noopener"">https://pytorch.org/docs/master/notes/amp_examples.html</a></p><NewLine><p>cc the author of <code>torch.cuda.amp</code> <a class=""mention"" href=""/u/mcarilli"">@mcarilli</a></p><NewLine></blockquote><NewLine></aside><NewLine><p>yes I tested both 1.5 and 1.7, and saw speed up when using 1.7 with AMP in both single GPU mode and DataParallel mode, but not in DistributedDataParallel.</p><NewLine><p>Should I @ him aswell or is yours sufficient?</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""11"" data-topic=""90581"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mrshenli/40/12220_2.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>I would try to print it every few iterations.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I adjusted the code like to print before during and after training:</p><NewLine><pre><code class=""lang-auto"">    print(torch.cuda.memory_summary())<NewLine>    sampler = torch.utils.data.DistributedSampler(dataset, 1, rank, shuffle=shuffle) if dist_sampler else None<NewLine>    loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False, num_workers=0, pin_memory=False,<NewLine>                                         sampler=sampler, collate_fn=dataset.collate_fn)<NewLine>    print(""start trainig"")<NewLine>    start = datetime.now()<NewLine>    for epoch in range(1):<NewLine>        for i, inputs in enumerate(loader):<NewLine>            for k, v in inputs.items():<NewLine>                if isinstance(v, torch.Tensor):<NewLine>                    inputs[k] = v.cuda(rank, non_blocking=True)<NewLine>            if i % 10 == 0:<NewLine>                print(torch.cuda.memory_summary())<NewLine>            optimizer.zero_grad()<NewLine>            outputs = model(**inputs)<NewLine>            loss = outputs[0]<NewLine>            loss = loss.mean()<NewLine>            loss.backward()<NewLine>            optimizer.step()<NewLine>            if i &gt;= n_steps:<NewLine>                break<NewLine>    print(""Training complete in:"", str(datetime.now() - start))<NewLine>    print(torch.cuda.memory_summary())<NewLine></code></pre><NewLine><p>and then called the function three times again like before:</p><NewLine><pre><code class=""lang-auto""><NewLine>    train_single()<NewLine>    train_single(dist_sampler=True)<NewLine>    train_single(dist_sampler=True, shuffle=True)<NewLine></code></pre><NewLine><p>This is what happens during the third function call (DistributedSampler with shuffle=True). The other outputs did not show any irregularities so i’ll leave them out (and due to the character limit…). First output is before the dataloader is build, second print is after the first Iteration, third after the tenth iteration</p><NewLine><pre><code class=""lang-auto"">|===========================================================================|<NewLine>|                  PyTorch CUDA memory summary, device ID 0                 |<NewLine>|---------------------------------------------------------------------------|<NewLine>|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |<NewLine>|===========================================================================|<NewLine>|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Allocated memory      |  340029 KB |    1692 MB |  350966 MB |  350634 MB |<NewLine>|       from large pool |  339456 KB |    1677 MB |  321965 MB |  321633 MB |<NewLine>|       from small pool |     573 KB |     133 MB |   29001 MB |   29001 MB |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Active memory         |  340029 KB |    1692 MB |  350966 MB |  350634 MB |<NewLine>|       from large pool |  339456 KB |    1677 MB |  321965 MB |  321633 MB |<NewLine>|       from small pool |     573 KB |     133 MB |   29001 MB |   29001 MB |<NewLine>|---------------------------------------------------------------------------|<NewLine>| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |<NewLine>|       from large pool |    1700 MB |    1700 MB |    1700 MB |       0 B  |<NewLine>|       from small pool |     142 MB |     142 MB |     142 MB |       0 B  |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Non-releasable memory |   51138 KB |  215041 KB |  393608 MB |  393558 MB |<NewLine>|       from large pool |   49664 KB |  186288 KB |  362929 MB |  362880 MB |<NewLine>|       from small pool |    1474 KB |   36244 KB |   30679 MB |   30678 MB |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Allocations           |     204    |    1077    |  251092    |  250888    |<NewLine>|       from large pool |      75    |     459    |  126113    |  126038    |<NewLine>|       from small pool |     129    |     756    |  124979    |  124850    |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Active allocs         |     204    |    1077    |  251092    |  250888    |<NewLine>|       from large pool |      75    |     459    |  126113    |  126038    |<NewLine>|       from small pool |     129    |     756    |  124979    |  124850    |<NewLine>|---------------------------------------------------------------------------|<NewLine>| GPU reserved segments |     156    |     156    |     156    |       0    |<NewLine>|       from large pool |      85    |      85    |      85    |       0    |<NewLine>|       from small pool |      71    |      71    |      71    |       0    |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Non-releasable allocs |      20    |     177    |  133360    |  133340    |<NewLine>|       from large pool |      19    |      80    |   87375    |   87356    |<NewLine>|       from small pool |       1    |      98    |   45985    |   45984    |<NewLine>|===========================================================================|<NewLine><NewLine>start trainig<NewLine>|===========================================================================|<NewLine>|                  PyTorch CUDA memory summary, device ID 0                 |<NewLine>|---------------------------------------------------------------------------|<NewLine>|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |<NewLine>|===========================================================================|<NewLine>|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Allocated memory      |  340079 KB |    1692 MB |  350966 MB |  350634 MB |<NewLine>|       from large pool |  339456 KB |    1677 MB |  321965 MB |  321633 MB |<NewLine>|       from small pool |     623 KB |     133 MB |   29001 MB |   29001 MB |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Active memory         |  340079 KB |    1692 MB |  350966 MB |  350634 MB |<NewLine>|       from large pool |  339456 KB |    1677 MB |  321965 MB |  321633 MB |<NewLine>|       from small pool |     623 KB |     133 MB |   29001 MB |   29001 MB |<NewLine>|---------------------------------------------------------------------------|<NewLine>| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |<NewLine>|       from large pool |    1700 MB |    1700 MB |    1700 MB |       0 B  |<NewLine>|       from small pool |     142 MB |     142 MB |     142 MB |       0 B  |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Non-releasable memory |   51089 KB |  215041 KB |  393608 MB |  393558 MB |<NewLine>|       from large pool |   49664 KB |  186288 KB |  362929 MB |  362880 MB |<NewLine>|       from small pool |    1425 KB |   36244 KB |   30679 MB |   30678 MB |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Allocations           |     207    |    1077    |  251095    |  250888    |<NewLine>|       from large pool |      75    |     459    |  126113    |  126038    |<NewLine>|       from small pool |     132    |     756    |  124982    |  124850    |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Active allocs         |     207    |    1077    |  251095    |  250888    |<NewLine>|       from large pool |      75    |     459    |  126113    |  126038    |<NewLine>|       from small pool |     132    |     756    |  124982    |  124850    |<NewLine>|---------------------------------------------------------------------------|<NewLine>| GPU reserved segments |     156    |     156    |     156    |       0    |<NewLine>|       from large pool |      85    |      85    |      85    |       0    |<NewLine>|       from small pool |      71    |      71    |      71    |       0    |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Non-releasable allocs |      20    |     177    |  133360    |  133340    |<NewLine>|       from large pool |      19    |      80    |   87375    |   87356    |<NewLine>|       from small pool |       1    |      98    |   45985    |   45984    |<NewLine>|===========================================================================|<NewLine><NewLine>|===========================================================================|<NewLine>|                  PyTorch CUDA memory summary, device ID 0                 |<NewLine>|---------------------------------------------------------------------------|<NewLine>|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |<NewLine>|===========================================================================|<NewLine>|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Allocated memory      |    1328 MB |   11131 MB |  494742 MB |  493413 MB |<NewLine>|       from large pool |    1326 MB |   11128 MB |  465663 MB |  464337 MB |<NewLine>|       from small pool |       2 MB |     133 MB |   29078 MB |   29075 MB |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Active memory         |    1328 MB |   11131 MB |  494742 MB |  493413 MB |<NewLine>|       from large pool |    1326 MB |   11128 MB |  465663 MB |  464337 MB |<NewLine>|       from small pool |       2 MB |     133 MB |   29078 MB |   29075 MB |<NewLine>|---------------------------------------------------------------------------|<NewLine>| GPU reserved memory   |   13646 MB |   13646 MB |   13646 MB |       0 B  |<NewLine>|       from large pool |   13504 MB |   13504 MB |   13504 MB |       0 B  |<NewLine>|       from small pool |     142 MB |     142 MB |     142 MB |       0 B  |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Non-releasable memory |  158950 KB |    3717 MB |  474414 MB |  474259 MB |<NewLine>|       from large pool |  157520 KB |    3717 MB |  443648 MB |  443494 MB |<NewLine>|       from small pool |    1430 KB |      35 MB |   30766 MB |   30765 MB |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Allocations           |     816    |    1077    |  265399    |  264583    |<NewLine>|       from large pool |     297    |     496    |  134867    |  134570    |<NewLine>|       from small pool |     519    |     756    |  130532    |  130013    |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Active allocs         |     816    |    1077    |  265399    |  264583    |<NewLine>|       from large pool |     297    |     496    |  134867    |  134570    |<NewLine>|       from small pool |     519    |     756    |  130532    |  130013    |<NewLine>|---------------------------------------------------------------------------|<NewLine>| GPU reserved segments |     297    |     297    |     297    |       0    |<NewLine>|       from large pool |     226    |     226    |     226    |       0    |<NewLine>|       from small pool |      71    |      71    |      71    |       0    |<NewLine>|---------------------------------------------------------------------------|<NewLine>| Non-releasable allocs |     109    |     191    |  140979    |  140870    |<NewLine>|       from large pool |      74    |     154    |   92152    |   92078    |<NewLine>|       from small pool |      35    |      98    |   48827    |   48792    |<NewLine>|===========================================================================|<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>, I think I figured out what is going on. The DataLoader I am using has dynamic clipping and the inputs are sorted by input length. So without shuffling only small inputs are used (in the ballpark of shape(4, 100)), while with shuffling much larger inputs are being used (around shape(4,1000)). And I would guess this could also explain some of the speed difference in training.</p><NewLine><p>However, I am still slightly confused why a SequentialSampler is so much faster than a DistributedSampler with num_replicates=1 and shuffling=False.</p><NewLine><p>So I guess only my question about amp is left.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you’re strongly dataloader bound in the DDP case, any Amp speedup may be negligible/not observable.</p><NewLine><p>One simple thing you can try is, don’t use the dataloader at all.  In each DDP process, create a single dummy batch of data and use that through all the timing iterations.</p><NewLine><p>First try it without amp, which gives an idea how strongly dataloader bound you are overall.  For example, if switching to dummy data gives a 2X speedup right away, you know the dataloader is a big bottleneck.</p><NewLine><p>Then try it with amp, and see if you observe a speedup relative to the above dummy data+no amp case, which gives an idea if Amp is working.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""15"" data-topic=""90581"" data-username=""mcarilli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mcarilli/40/20871_2.png"" width=""20""/> mcarilli:</div><NewLine><blockquote><NewLine><p>One simple thing you can try is, don’t use the dataloader at all. In each DDP process, create a single dummy batch of data and use that through all the timing iterations.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thanks for the suggestion I will try that. Is there some rough number on how fast my dataloder should be in seconds or relative to the model?</p><NewLine><p>Also thanks to everyone for helping out and explaining!</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""15"" data-topic=""90581"" data-username=""mcarilli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mcarilli/40/20871_2.png"" width=""20""/> mcarilli:</div><NewLine><blockquote><NewLine><p>First try it without amp, which gives an idea how strongly dataloader bound you are overall. For example, if switching to dummy data gives a 2X speedup right away, you know the dataloader is a big bottleneck.</p><NewLine><p>Then try it with amp, and see if you observe a speedup relative to the above dummy data+no amp case, which gives an idea if Amp is working.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Here are the results of the test. I tested the native speed of my model, then the speed with dummy data and then dummy data and Amp:</p><NewLine><blockquote><NewLine><p>testing ddp<br/><NewLine>testing: model=mlm batch_size=4 num_workers=0 mixed_pres=False pin_memory=False use_dummy_data=False<br/><NewLine>Training complete in: 0:03:47.287354<br/><NewLine>model=mlm batch_size=4 num_workers=0 mixed_pres=False pin_memory=False use_dummy_data=True<br/><NewLine>Training complete in: 0:02:40.539612<br/><NewLine>testing: model=mlm batch_size=4 num_workers=0 mixed_pres=True pin_memory=False use_dummy_data=True<br/><NewLine>Training complete in: 0:02:41.635868</p><NewLine></blockquote><NewLine><p>so while my dataloader seems to slow down the training quite a bit I still see no speedup using Amp.</p><NewLine><p>I very lazily updated my training loop to:</p><NewLine><pre><code class=""lang-python"">    for epoch in range(epochs):<NewLine>        if not use_dummy_data:<NewLine>            for i, inputs in enumerate(train_loader):<NewLine>                for k, v in inputs.items():<NewLine>                    if isinstance(v, torch.Tensor):<NewLine>                        inputs[k] = v.cuda(rank, non_blocking=True)<NewLine>                optimizer.zero_grad()<NewLine>                if mixed_pres:<NewLine>                    with torch.cuda.amp.autocast():<NewLine>                        outputs = model(**inputs)<NewLine>                        loss = outputs[0]<NewLine>                        loss = loss.mean()<NewLine>                    # Backward and optimize<NewLine>                    scaler.scale(loss).backward()<NewLine>                    scaler.step(optimizer)<NewLine>                    scaler.update()<NewLine>                else:<NewLine>                    outputs = model(**inputs)<NewLine>                    loss = outputs[0]<NewLine>                    loss = loss.mean()<NewLine>                    loss.backward()<NewLine>                    optimizer.step()<NewLine>                if i &gt;= n_steps:<NewLine>                    break<NewLine>        else:<NewLine>            inputs = next(enumerate(train_loader))[1]<NewLine>            inputs.to(f""cuda:{rank}"")<NewLine>            for i in range(n_steps):<NewLine>                optimizer.zero_grad()<NewLine>                if mixed_pres:<NewLine>                    with torch.cuda.amp.autocast():<NewLine>                        outputs = model(**inputs)<NewLine>                        loss = outputs[0]<NewLine>                        loss = loss.mean()<NewLine>                    # Backward and optimize<NewLine>                    scaler.scale(loss).backward()<NewLine>                    scaler.step(optimizer)<NewLine>                    scaler.update()<NewLine>                else:<NewLine>                    outputs = model(**inputs)<NewLine>                    loss = outputs[0]<NewLine>                    loss = loss.mean()<NewLine>                    loss.backward()<NewLine>                    optimizer.step()<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/siheming; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/siheming; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/siheming; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/siheming; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/siheming; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/siheming; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/siheming; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/mcarilli; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/siheming; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/siheming; <NewLine> ,"REPLY_DATE 1: July 27, 2020,  1:43pm; <NewLine> REPLY_DATE 2: July 27, 2020,  2:57pm; <NewLine> REPLY_DATE 3: July 27, 2020,  2:52pm; <NewLine> REPLY_DATE 4: July 27, 2020,  2:57pm; <NewLine> REPLY_DATE 5: July 27, 2020,  3:03pm; <NewLine> REPLY_DATE 6: July 28, 2020,  8:16am; <NewLine> REPLY_DATE 7: July 27, 2020,  5:01pm; <NewLine> REPLY_DATE 8: July 27, 2020,  6:38pm; <NewLine> REPLY_DATE 9: July 27, 2020,  7:41pm; <NewLine> REPLY_DATE 10: July 27, 2020,  7:56pm; <NewLine> REPLY_DATE 11: July 27, 2020, 10:16pm; <NewLine> REPLY_DATE 12: July 28, 2020,  8:07am; <NewLine> REPLY_DATE 13: July 28, 2020,  3:41pm; <NewLine> REPLY_DATE 14: July 28, 2020,  3:51pm; <NewLine> REPLY_DATE 15: July 28, 2020,  6:39pm; <NewLine> REPLY_DATE 16: July 29, 2020, 12:00pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: 1 Like; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: 1 Like; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> 
62289,MAML inner loop parallelization,2019-11-27T07:55:38.424Z,1,196,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I want to parallelize the inner loop of MAML.<br/><NewLine>Each inner loop of the MAML will produce individual loss along with individual gradient graphs,<br/><NewLine>and after the iteration, I have to aggregate the losses followed by backpropagation.</p><NewLine><p>My naive idea is replacing the loop to map.<br/><NewLine>To do this, I guess I need to aggregate the loss from multiple threads.<br/><NewLine>(e.g. torch.mean(torch.stack(list_of_loss_from_multiple_threads))</p><NewLine><p>Is it possible to aggregate graphs from worker threads and then do the backprop at once?</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/zzoon91,(Junhyeok),zzoon91,"November 27, 2019,  7:58am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, it’s pretty tough to give any concrete advice without first knowing what exactly you’re doing and what you’ve tried. Would you mind posting a snippet of code that indicates the inner loop that you’d like to parallelize, and any instructions needed to run the code? Thanks!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I actually want to try this out, I am afraid that the gains in parallelizing the inner loop might be outweighed by communication overhead, were you able to see speed ups ?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>further, you dont have to aggregate the loss from the different threads, you could compute the gradients within each thread and then aggregate the gradients from different threads.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, I gave up parallelizing MAML. So I don’t have results that might help your concern. <img alt="":sweat_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/sweat_smile.png?v=9"" title="":sweat_smile:""/></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""62289"" data-username=""zzoon91""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/zzoon91/40/27230_2.png"" width=""20""/> zzoon91:</div><NewLine><blockquote><NewLine><p>Is it possible to aggregate graphs from worker threads and then do the backprop at once?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, this is possible, and this is how <code>DataParallel</code> is implemented. The <code>parallel_apply()</code> in the code below will launch multiple threads with each creating their own autograd graph.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/2de549518e7f0ce2820650b401cd21a9901c74a9/torch/nn/parallel/data_parallel.py#L147-L162"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/2de549518e7f0ce2820650b401cd21a9901c74a9/torch/nn/parallel/data_parallel.py#L147-L162"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/2de549518e7f0ce2820650b401cd21a9901c74a9/torch/nn/parallel/data_parallel.py#L147-L162</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""147"" style=""counter-reset: li-counter 146 ;""><NewLine><li>def forward(self, *inputs, **kwargs):</li><NewLine><li>    if not self.device_ids:</li><NewLine><li>        return self.module(*inputs, **kwargs)</li><NewLine><li><NewLine></li><li>    for t in chain(self.module.parameters(), self.module.buffers()):</li><NewLine><li>        if t.device != self.src_device_obj:</li><NewLine><li>            raise RuntimeError(""module must have its parameters and buffers ""</li><NewLine><li>                               ""on device {} (device_ids[0]) but found one of ""</li><NewLine><li>                               ""them on device: {}"".format(self.src_device_obj, t.device))</li><NewLine><li><NewLine></li><li>    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)</li><NewLine><li>    if len(self.device_ids) == 1:</li><NewLine><li>        return self.module(*inputs[0], **kwargs[0])</li><NewLine><li>    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])</li><NewLine><li>    outputs = self.parallel_apply(replicas, inputs, kwargs)</li><NewLine><li>    return self.gather(outputs, self.output_device)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/rvarm1; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Sudarshan_Babu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Sudarshan_Babu; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/zzoon91; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: December 3, 2019, 12:18am; <NewLine> REPLY_DATE 2: July 20, 2020,  4:43pm; <NewLine> REPLY_DATE 3: July 20, 2020,  4:45pm; <NewLine> REPLY_DATE 4: July 28, 2020,  3:34pm; <NewLine> REPLY_DATE 5: July 28, 2020,  4:04pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
90736,How does DistributedDataParallel handle parameters whose requires_grad flag is False?,2020-07-27T22:26:40.596Z,0,55,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I’m trying to figure out what happen behind the scenes in DistributedDataParallel when it comes to parameters that do not require a gradient. I cannot find a clear answer on this in the documentations.</p><NewLine><p>Assume we have three layers: A --&gt; B --&gt; C. Suppose that A and B both have their <code>requires_grad</code> flag as False. If this model is wrapped in DistributedDataParallel, <strong>will there be any process communication that needs to be done in the backward pass of layer A or C?</strong> Specifically with the sharing of gradients.</p><NewLine><p>My problem is that I have a large model and I am extremely bottle necked by process communication. I would like to freeze some of my layers such that less gradients need to be shared. I understand that depending on my model, the computation cost may be the same but I really need to bring down the communication cost.</p><NewLine></div>",https://discuss.pytorch.org/u/ayalaa2,(Alex Ayala),ayalaa2,"July 27, 2020, 10:27pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ayalaa2"">@ayalaa2</a>, DistributedDataParallel’s (DDP) ctor would go through all parameters and skip the ones whose <code>requires_grad=False</code>. So, there won’t be communication on those grad, but you will have to set their <code>require_grad</code> field before passing it to DDP. After the ctor, changing the <code>requires_grad</code> attribute makes no difference. See the code below in DDP ctor.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/6bd88f581a6323d026e08b65ffee75bfe162501f/torch/nn/parallel/distributed.py#L456-L464"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/6bd88f581a6323d026e08b65ffee75bfe162501f/torch/nn/parallel/distributed.py#L456-L464"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/6bd88f581a6323d026e08b65ffee75bfe162501f/torch/nn/parallel/distributed.py#L456-L464</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""456"" style=""counter-reset: li-counter 455 ;""><NewLine><li># Build tuple of (module, parameter) for all parameters that require grads.</li><NewLine><li>modules_and_parameters = [</li><NewLine><li>    [</li><NewLine><li>        (module, parameter)</li><NewLine><li>        for module in replica.modules()</li><NewLine><li>        for parameter in filter(</li><NewLine><li>            lambda parameter: parameter.requires_grad,</li><NewLine><li>            parameters(module, recurse=False))</li><NewLine><li>    ] for replica in self._module_copies]</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Another way to skip communication is to use the <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel.no_sync"" rel=""nofollow noopener""><code>no_sync</code></a> context manager.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: July 28, 2020,  6:38pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
87875,Distributed Model Parallel Using Distributed RPC,2020-07-03T17:28:14.548Z,14,357,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am a looking into the Distributed RPC API. In particular in this example</p><NewLine><p><a href=""https://pytorch.org/tutorials/intermediate/rpc_tutorial.html#distributed-rnn-using-distributed-autograd-and-distributed-optimizer"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/rpc_tutorial.html#distributed-rnn-using-distributed-autograd-and-distributed-optimizer</a>.</p><NewLine><p>I have a question when we have to scale the model into multiple machines.</p><NewLine><p>Think of an instance where we have 16 layers (L1 — L16) where we have 4 machines each with 4 GPU devices. So my model will scale into all 16 GPUs in 4 machines such that each layer occupies the memory of a one GPU device. Assume a theoretical scenario memory is enough for computation.</p><NewLine><p>When I need to do such a task, my training script must be written in such a way that if the original model was M, now I have M1 – M16 smaller models which depends upon the output of the previous model in the sequence.</p><NewLine><p>I am not sure whether this is the best way to do this. If this is wrong, please explain the best practices with the RPC API.</p><NewLine><p>Furthermore, here M1–M4 makes sense and that can be done by <code>to(device)</code> and when I have to send the input from M4 to M5, M4 is in GPU:3 of machine 1, M5 is in the GPU:0 of machine 2, I need to some how use an RPC call and send that data to the machine 2. This is the same case for all boundary conditions. Data could be sent some how via a synchronization mechanism.</p><NewLine><p>Is this something possible with the existing APIs. I am not quite clear how DistributedOptimizer and Distributed Autograd could handle this.</p><NewLine><p>These are some questions I have about the distributed model parallel.</p><NewLine><p>Thank You,<br/><NewLine>Vibhatha</p><NewLine></div>",https://discuss.pytorch.org/u/Vibhatha_Abeykoon,(Vibhatha Abeykoon),Vibhatha_Abeykoon,"July 3, 2020,  5:28pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/vibhatha_abeykoon"">@Vibhatha_Abeykoon</a>, thanks for the question, this actually relates to several WIP projects that we are working on now.</p><NewLine><blockquote><NewLine><p>When I need to do such a task, my training script must be written in such a way that if the original model was M, now I have M1 – M16 smaller models which depends upon the output of the previous model in the sequence.</p><NewLine></blockquote><NewLine><p>In this case, you need 4 instead of 16 smaller models, and within each model you can use <code>Tensor.to(device)</code> to move data across GPUs as you mentioned below. For pipeline parallelism using RPC, <a href=""https://github.com/pytorch/tutorials/pull/948/files"" rel=""nofollow noopener"">this tutorial</a> can serve as a reference (will be released with v1.6).</p><NewLine><blockquote><NewLine><p>I am not sure whether this is the best way to do this. If this is wrong, please explain the best practices with the RPC API.</p><NewLine></blockquote><NewLine><p>This is <strong>not</strong> the most convenient way to support pipeline parallelism. RPC is a lower-level API that offers flexibility but would require additional application code to orchestrate. One of the projects that we are looking into is to provide a higher-level API, e.g., a <code>DistributedPipelineParallel</code> (DPP) (similar to the <code>DistributedDataParallel</code>) which, ideally, can automatically divide the original model and place model shards (maybe) by using additional configuration hints or specific model structure (e.g., <code>nn.Sequential</code>). But this is still in discussion and no committed release date for it yet. Please do comment if you have suggestions or requirements on this feature.</p><NewLine><blockquote><NewLine><p>I need to some how use an RPC call and send that data to the machine 2. This is the same case for all boundary conditions.</p><NewLine></blockquote><NewLine><p>If you want the distributed autograd to automatically take care of the backward pass across machines, then yes, you will need to use RPC to send the intermediate output form machine 1 to machine 2. As of v1.6, RPC only accepts CPU tensors, so you will need to first move the tensor from <code>cuda:3</code> to <code>cpu</code> on machine 1 and then move the received tensor from <code>cpu</code> to <code>cuda:0</code> on machine 2. We explicitly added this restriction to avoid  unintentional device mapping errors through RPC. And we are working on a new device placement API (similar to the <code>map_location</code> in <code>torch.load</code>) to make this easier, where application can define default device mappings between each pair of nodes and directly pass GPU tensors to RPC. We hope we can get this done in v1.7.</p><NewLine><blockquote><NewLine><p>Data could be sent some how via a synchronization mechanism.</p><NewLine></blockquote><NewLine><p>What do you mean by “a synchronization mechanism” here?</p><NewLine><blockquote><NewLine><p>Is this something possible with the existing APIs. I am not quite clear how DistributedOptimizer and Distributed Autograd could handle this.</p><NewLine></blockquote><NewLine><p>Yep, the tutorial linked above shows an example.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> This tutorial is wonderful. I was writing one and I think I can learn a lot from yours. I had questions when I was attempting this.</p><NewLine><p>What I meant from synchronization is as the Model-Shard in Machine 1 needs to complete compute to start the Model-Shard2, even in pipeline case. Please correct me if I am wrong.<br/><NewLine>So the Machine 2 Model-Shard2 must wait, until it gets the weights from Machine 2. And again, I am still going through your tutorial and may have answers in there.</p><NewLine><p>One more thing, is how are we deploying this multi-machine model parallel module?<br/><NewLine>Will there be modifications to torch.distributed.launch? What is the current method to launch this<br/><NewLine>as we do for DDP?</p><NewLine><p>The Pipeline parallel API would be just great. I was attempting to wrap this with Torchgpipe or Pipedream and I also felt like it is better if it can come within the PyTorch APIs.</p><NewLine><p>I have a few suggestions, if we can make use of to(device) call into to(machine:device) kind of an API endpoint, it will be much easier to work, but I am not quite sure how the changes should reflect internally.</p><NewLine><p>The DPP module needs a few features.</p><NewLine><ol><NewLine><li>How to partition the model, (a profiler based auto-partitioned or a manual one so that user can say how to partition). Model partition in a manual way and saying .to(device) is not going to work when we have to deal with complex and large models. So if this could be handled internally, it will be ideal for all users.</li><NewLine><li>From the knowledge of using <a href=""https://github.com/kakaobrain/torchgpipe"" rel=""nofollow noopener"">Torchgpipe</a>, Rematerialization as a custom option within DPP would be a great option. There are couple reasons for this, some applications need performance using pipelining rather than saving memory. So in this case one could have the ability to turn it off and on depending on the training job. DPP could have a flag rematerialztion=False/True to make it disabled or enabled for the user.</li><NewLine><li>With the <a href=""https://github.com/msr-fiddle/pipedream"" rel=""nofollow noopener"">Pipedream</a> work, it was very clear that the multi-machine involvement could be very useful for training and their profiler usage is important in getting a better DPP.</li><NewLine><li>Enabling checkpointing internally for DPP would be easier as handling this manually could be troublesome. When the shards are distributed across machines the checkpoint itself needs to be a distributed entity (in my understanding).</li><NewLine></ol><NewLine><p>These could be great additions to DPP if possible.</p><NewLine><p>Regrading the tutorial:</p><NewLine><p><a href=""https://github.com/pytorch/tutorials/blob/release/1.6/intermediate_source/dist_pipeline_parallel_tutorial.rst"" rel=""nofollow noopener"">PP Tutorial</a></p><NewLine><p>The <code>ResnetBase</code> and <code>self._lock</code> are not clear. Does the lock represents thread lock or something else?<br/><NewLine>Is it possible to access the code for ResnetBase?</p><NewLine><p>Thank You,<br/><NewLine>Vibhatha</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""87875"" data-username=""Vibhatha_Abeykoon""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/vibhatha_abeykoon/40/17261_2.png"" width=""20""/> Vibhatha_Abeykoon:</div><NewLine><blockquote><NewLine><p>What I meant from synchronization is as the Model-Shard in Machine 1 needs to complete compute to start the Model-Shard2, even in pipeline case. Please correct me if I am wrong.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yep, this is correct. That tutorial uses <code>RRef.to_here()</code> to block wait for the result. The downside is that this would block one RPC thread until <code>to_here()</code> returns. If this is a concern, the <code>async_execution</code> decorator can help. [<a href=""https://github.com/pytorch/tutorials/pull/1045/files"" rel=""nofollow noopener"">tutorial</a>]</p><NewLine><blockquote><NewLine><p>One more thing, is how are we deploying this multi-machine model parallel module?<br/><NewLine>Will there be modifications to torch.distributed.launch? What is the current method to launch this<br/><NewLine>as we do for DDP?</p><NewLine></blockquote><NewLine><p>We don’t have a helper launching script for RPC yet as of v1.6. The RPC processes will need to be launched manually or programmably in application code. Added <a href=""https://github.com/pytorch/pytorch/issues/40974"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/40974</a> to track.</p><NewLine><blockquote><NewLine><p>I have a few suggestions, if we can make use of to(device) call into to(machine:device) kind of an API endpoint, it will be much easier to work, but I am not quite sure how the changes should reflect internally.</p><NewLine></blockquote><NewLine><p>Right! This aligns with the remote device feature that we would love to build on top of RPC, but we don’t have bandwidth to cover that yet. It won’t be very hard to convert every operation of a remote device tensor into an RPC invocation, however that will be too slow due to the per-op comm overhead. Ideally, we should have a remote Tensor type that can do op fusing when possible, similar to lazy tensor. But as this would require a lot of effort and we haven’t seen too many requests for this yet, this feature didn’t make into our top priorities for now. We will come back to re-evaluate after the next release.</p><NewLine><blockquote><NewLine><p>How to partition the model, (a profiler based auto-partitioned or a manual one so that user can say how to partition). Model partition in a manual way and saying .to(device) is not going to work when we have to deal with complex and large models. So if this could be handled internally, it will be ideal for all users.</p><NewLine></blockquote><NewLine><blockquote><NewLine><p>With the <a href=""https://github.com/msr-fiddle/pipedream"" rel=""nofollow noopener"">Pipedream</a> work, it was very clear that the multi-machine involvement could be very useful for training and their profiler usage is important in getting a better DPP.</p><NewLine></blockquote><NewLine><p>Thanks a lot for all the suggestions!! Totally agree. Profiling is great and can definitely provide an easier entry point, especially when the application does not need to squeeze out the last bit of performance. For more perf-centric use cases, we might still want to allow users to hand-craft model partitioning and placement, maybe, by accepting some hints/configs.</p><NewLine><blockquote><NewLine><p>Enabling checkpointing internally for DPP would be easier as handling this manually could be troublesome. When the shards are distributed across machines the checkpoint itself needs to be a distributed entity (in my understanding).</p><NewLine></blockquote><NewLine><p>Exactly, this is a feature gap in RPC. We might be able to add checkpointing support to the WIP <a href=""https://github.com/pytorch/pytorch/blob/00651b8c93354da13ab30c3fefd8cd529dc883e7/torch/distributed/nn/api/remote_module.py"" rel=""nofollow noopener"">RemoteModel</a> feature and build DPP on top.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> Thank you so much for this detailed explanation. I will try to design on top of what is offered from the PyTorch APIs. The plan you suggested is great and hope to use these in near future.</p><NewLine><p>Regrading the tutorial:</p><NewLine><p><a href=""https://github.com/pytorch/tutorials/blob/release/1.6/intermediate_source/dist_pipeline_parallel_tutorial.rst"" rel=""nofollow noopener"">PP Tutorial</a></p><NewLine><p>The <code>ResnetBase</code> and <code>self._lock</code> are not clear. Does the lock represents thread lock or something else?<br/><NewLine>Is it possible to access the code for ResnetBase?</p><NewLine><p>Thank you,<br/><NewLine>Vibhatha.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>The  <code>ResnetBase</code>  and  <code>self._lock</code>  are not clear. Does the lock represents thread lock or something else?</p><NewLine></blockquote><NewLine><p>Ah, thanks for the catch. Yep, this is a thread lock to prevent race. The full example code is here: <a href=""https://github.com/pytorch/examples/blob/master/distributed/rpc/pipeline/main.py"" rel=""nofollow noopener"">https://github.com/pytorch/examples/blob/master/distributed/rpc/pipeline/main.py</a></p><NewLine><p>Let me add the missing <code>ResNetBase</code> to the tutorial.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> Thank you very much. I will try it.  <img alt="":+1:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/+1.png?v=9"" title="":+1:""/></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a></p><NewLine><p>Just following up with you based on the performance factor.</p><NewLine><p>For distributed model parallelism, could MPI-collective communication be a better choice than distributed RPC? I mean these are two different models designed to serve two different purposes. But, at the end of the day what we would be doing is sending or receiving data from one point to another point. In terms of performance does PyTorch Distributed RPC outperforms MPI-Collectives (especially ISend/IRecv, Send/Recv)? Is this something PyTorch community already considered and decided to go with RPCs instead of MPI libraries?</p><NewLine><p>But I understand the currently distributed optimizer, Autograd and those extensible components have been written to support RPC. But does MPI stand a chance here?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/vibhatha_abeykoon"">@Vibhatha_Abeykoon</a></p><NewLine><p>We will announce a new RPC backend in v1.6, which is called <a href=""https://pytorch.org/docs/master/rpc.html#tensorpipe-backend"" rel=""nofollow noopener"">TensorPipe</a>. This is a P2P comm library and is designed to automatically figure out the best comm media between two RPC workers, e.g., shm, tcp, nvlink, ib, etc. (This is still WIP) We will gradually make TensorPipe the default RPC backend and retire ProcessGroup RPC backend due to the perf reasons as you noticed. The original reason for adding ProcessGroup RPC backend is to have a working comm module to unblock other parts of the system, and also buy us time to design better solutions.</p><NewLine><p>Regarding MPI, we probably will not develop an RPC backend on top of MPI but we do welcome OSS contributions or it might be possible to add MPI as a channel type in TensorPipe. One downside of using MPI is that there are different implementations does not seem to be one implementation that rules all use cases. That’s also one reason why PyTorch does not include MPI as a submodule but require the users to provide a MPI installation and compile from source.</p><NewLine><p>Is there any specific reason for requesting MPI RPC backend?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>.</p><NewLine><p>The main reason is that there are tons of <strong>Scientific Applications written on MPI</strong> and it would be really hard to port them back to a different backend. These applications will do an MPI_Init somewhere at the very beginning of the program. In the early part of the program, there are specific scientific data pre-processing, shallow/complex algorithms applied to pre-process the data. Then comes the DL workload. Such applications are very common in the high-performance computing domain. So supporting MPI-backend could be very vital to support such applications seamlessly without breaking the data pipeline/training.</p><NewLine><p>I understand there are many MPI-implementations. But MPI still can be left to the user to install, but the specifications are mostly consistent in most of the MPI implementations. All it needs is a wrapper library to wrap the collective communication calls. PyTorch already has this API in C10D. Please correct me if I am wrong.</p><NewLine><p>Tensorpipe seems to be a very interesting project that could glue all these together.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see. Technically, it shouldn’t be too hard to let the ProcessGroup RPC backend to use MPI, as it only requires its send/recv features. One options could be adding one field to ProcessGroup RPC backend construction time options, and let users decide whether they want to use Gloo, or NCCL (&gt; 2.7), or MPI.</p><NewLine><p>cc <a class=""mention"" href=""/u/lcw"">@lcw</a> any thoughts on MPI + TensorPipe?  Does it make sense to add MPI as a channel type for TensorPipe?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t understand the argument for using MPI in RPC: what does the fact that other libraries use the MPI API have to do with the RPC library using it under the hood? AFAIK, MPI is not incompatible with Gloo or TensorPipe: the same process can use them both, in different parts of the code. Also, the fact that RPC uses MPI internally does not help with porting MPI code to RPC: it would still have to be rewritten to use the RPC interface.</p><NewLine><p>A good reason would be if there was a difference in performance. Have you reason to believe there is?</p><NewLine><p>If we were stuck with the ProcessGroup agent only, then I could agree that we should allow it to use MPI instead of Gloo, but as it’s slated to go away in favor of the TensorPipe-based one this change may not end up being so useful. TensorPipe is natively asynchronous, and thus suits really well the use-case of RPC, contrary to Gloo and MPI which are blocking. We have already proven that TensorPipe outperforms Gloo for RPC. It may be different for MPI, as some MPI implementations use specialized backends, but that’s what TensorPipe is also going to do.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>My two cents on the above discussion too: ideally you shouldn’t specialize your code to handle differently the transfers between GPUs on the same node and between different nodes. By doing so you couple your code with your deployment, meaning you need to rewrite some parts to change from 4 GPUs/host to single-GPU hosts and so on. With the TensorPipe agent you will be able to perform RPC calls between GPUs on a node and the data will still be transferred over NVLink just as if you had done t.to(…). So with no performance overhead you get code that is resilient to a topology change.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/lcw"">@lcw</a> It is not an argument, just asking if this is something possible with the current implementations you have. With MPI asynchronous calls you can still get asynchronous attribute to the code.</p><NewLine><p><a href=""https://www.mpich.org/static/docs/latest/www3/MPI_Isend.html"" rel=""nofollow noopener"">ISend</a>, <a href=""https://www.mpich.org/static/docs/latest/www3/MPI_Irecv.html"" rel=""nofollow noopener"">IRecv</a></p><NewLine><p>Correct me if I am wrong.</p><NewLine><aside class=""quote no-group"" data-post=""12"" data-topic=""87875"" data-username=""lcw""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/l/b5e925/40.png"" width=""20""/> lcw:</div><NewLine><blockquote><NewLine><p>Also, the fact that RPC uses MPI internally does not help with porting MPI code to RPC: it would still have to be rewritten to use the RPC interface.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, we have to still re-write, but unless MPI backend support is there, the communication on RPC channels will have different performance. Have you benchmarked the performance of Gloo, TensorPipe RPC vs MPI? The reason for asking the MPI compatibility is that, a program would not only have a training script. It has a data pre-processing functions, training and post-processing based on the trained model. If a system designed with an MPI backend is used to be as the main framework where PyTorch act as a library within the code, in those cases the support from MPI is immensely important. The use cases of using PyTorch is getting complex and complex, I think that is why a library like Tensorpipe is also coming into play.</p><NewLine><p>I just wanted to point out a possible usage of MPI within the distribution.</p><NewLine><aside class=""quote no-group"" data-post=""13"" data-topic=""87875"" data-username=""lcw""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/l/b5e925/40.png"" width=""20""/> lcw:</div><NewLine><blockquote><NewLine><p>With the TensorPipe agent you will be able to perform RPC calls between GPUs on a node and the data will still be transferred over NVLink just as if you had done t.to(…). So with no performance overhead you get code that is resilient to a topology chang</p><NewLine></blockquote><NewLine></aside><NewLine><p>This is really useful for model parallelism and writing complex networks with feedback loops.</p><NewLine><p>Is Tensor-pipe going to be a standalone library or is this going to be adopted in <code>torch.distributed.rpc</code> ?</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>MPI is not appropriate for serving as a new backend of RPC.<br/><NewLine>Their model are totally different and inherently incompatible with each other.</p><NewLine><p>Now the serious explanation, to put it simply:</p><NewLine><p>MPI style is tightly coupled, P2P primitives like recv, send, irecv, isend are there simply because you wouldn’t want to introduce an additional library to complete a simple P2P commnication, like collecting a state or log.</p><NewLine><p>RPC style is complete decoupled, services are there and you can access it if a process want and have the permission to. Therefore synchronization becomes a disaster because processes are <strong>distributed</strong>.</p><NewLine><p>Tensorpipe is mainly just a smart payload delivery layer, it is there because the important “decision” feature will greatly improve the performance of rpc, since unoptimized rpc libraries such as “GRPC”, “Dubbo” “Swift” does not handle tensors on defferent devices well. It is designed for the distributed scenario. And it can also handle elasticity and dynamic size (Eg: initialize your program with different number of process-roles, esbecially important if you want to add some springness to your application) very well, in this case, MPI is just <strong>way too rigid</strong>.</p><NewLine><p>BTW, distributed applications are <strong>complex</strong>, you cannot expect pytorch to be any simpler because it is already very simple, its rpc api could be considered “overly simple and crude” if you compare it to an industrial grade rpc framework “Dubbo” designed by Alibaba:</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/1bc0cea37218ca28d3fba1017f3a24d958fb4b9b"" href=""https://discuss.pytorch.org/uploads/default/original/3X/1/b/1bc0cea37218ca28d3fba1017f3a24d958fb4b9b.jpeg"" title=""image""><img alt=""image"" data-base62-sha1=""3XvYHYNDY8MPVFKKzBCCaJqiKNl"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/1/b/1bc0cea37218ca28d3fba1017f3a24d958fb4b9b_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/original/3X/1/b/1bc0cea37218ca28d3fba1017f3a24d958fb4b9b.jpeg"" width=""667""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">900×674 255 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>So, in a word, please, don’t mix these two things together, they are different.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can use MPI to implement rpc, techincally, but the performance could be really bad, for example, in order to send a message of arbitrary length, in MPI you need to send the size to your target, then the target has to allocate the memory, then you can send the payload, since MPI is not a raw connection like tcp or infiniband, you would expect more delay in these two communications, and you have to deal with process failures! MPI will fail if any component process has failed, and that’s why we would like to remove that behavior in rpc, see <a href=""https://discuss.pytorch.org/t/how-to-catch-exceptions-caused-by-rpc-exactly/88856/10"">88856</a>.</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/iffix"">@iffiX</a> I would like to ask you to keep the discussion objective.<br/><NewLine>If you disagree with something, explain your position and keep the discussion alive.<br/><NewLine>While the majority of your post is a great explanation, the first part is unfortunately not. <img alt="":confused:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=9"" title="":confused:""/></p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>Inappropriate apart removed and updated, sorry for any issues cased by the provoking part in the comment. also <a class=""mention"" href=""/u/vibhatha_abeykoon"">@Vibhatha_Abeykoon</a></p><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/vibhatha_abeykoon"">@Vibhatha_Abeykoon</a> <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a><br/><NewLine>I am reviewing this topic today and I have a few suggestions.</p><NewLine><p>I will use “process/device” (Eg: “worker:0/cuda:0”) as a location descriptor of a tensor, and tensor is the only holder of all data.</p><NewLine><p>The first thing is:</p><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""87875"" data-username=""Vibhatha_Abeykoon""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/vibhatha_abeykoon/40/17261_2.png"" width=""20""/> Vibhatha_Abeykoon:</div><NewLine><blockquote><NewLine><p>How to partition the model</p><NewLine></blockquote><NewLine></aside><NewLine><p>I have a primitive design for this purpose: an <a>assigner</a> and a simple <a>wrapper</a>, assigner currently won’t partition a model auto matically, instead it will just assign user specified partitions based on a series of heuristics (GPU mem, GPU power, CPU mem, CPU power, Model complexity, bandwidth of connection between models), but it could be also reworked to fulfill your purpose, and you just need to wrap all of your submodules in the wrapper, the wrapper just stores input/output location descritors, nothing more.</p><NewLine><p>Simply speaking, partitioning just require users to specify the input and output (process/device) requirements for a module/model, and then a smart assigner to assign partitions to process/device.</p><NewLine><p>Theoretically a dynamic profiler is much better than a static heuristic assigner, since it actively detects hotspots and try to even the load on all of your nodes, but this introduces additional issues: Is evening the load across nodes increasing the performance? How much cost have the additional transmission introduced? Is the decreasing the load not pushing your GPUs to their full capacity (kernel launching cost should be considered)? So there are possibilities that this solution does not meet with the theoretical standard.</p><NewLine><p>I beilieve <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> has studied about this issue, from his <a href=""https://mrshenli.github.io/"" rel=""nofollow noopener"">profile page</a>. I think many more experiments are needed to determine the best scheme.</p><NewLine><p>The second thing is:</p><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""87875"" data-username=""Vibhatha_Abeykoon""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/vibhatha_abeykoon/40/17261_2.png"" width=""20""/> Vibhatha_Abeykoon:</div><NewLine><blockquote><NewLine><p>I have a few suggestions, if we can make use of to(device) call into to(machine:device) kind of an API endpoint, it will be much easier to work, but I am not quite sure how the changes should reflect internally.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I think it won’t be too defficult if <code>rpc.pair</code> in <a href=""https://github.com/pytorch/pytorch/issues/41546"" rel=""nofollow noopener"">#41546</a> is implemented,</p><NewLine><pre><code class=""lang-auto"">tensor.to(""worker:1/cuda:0"")<NewLine></code></pre><NewLine><p>is equivalent to</p><NewLine><pre><code class=""lang-auto""># suppose there is a tensor on process ""worker:0"" and device ""cuda:0"" of this process<NewLine># move to process ""worker:1"" and device ""cuda:1"" of that process<NewLine># take care when torch.cuda.set_device is used<NewLine><NewLine>def pair_and_move_to(tensor, device, uuid):<NewLine>    # uuid should be a unique identifier to identify this tensor<NewLine>    # could be process_name:tensor_ptr<NewLine>    tensor = tensor.to(device)<NewLine>    rpc.pair(uuid, tensor)<NewLine>    return RRef(tensor)<NewLine><NewLine># on worker:0 when .to is invoked:<NewLine>rpc.rpc_sync(""worker:1"", rpc.pair, args=(tensor, ""cuda:1"", tensor.some_uuid))<NewLine></code></pre><NewLine><p>And for implementing <code>Distributed Model Parallel Using Distributed RPC</code>, there are many model parallel methods, it depends on your model, algorithm framework and application. For DDP compatible RPC solutions specifically, I have implementations of a <a href=""https://github.com/iffiX/machin/blob/master/machin/parallel/server/param_server.py"" rel=""nofollow noopener"">gradient reduction server</a> in my framework, which should be able to do the exact same thing as DDP does, however, this server implementation is also based on new API RFC <a href=""https://github.com/pytorch/pytorch/issues/41546"" rel=""nofollow noopener"">#41546</a>, which haven’t been implemented in pytorch. I have made a simple <a href=""https://github.com/iffiX/machin/blob/master/machin/parallel/distributed/world.py"" rel=""nofollow noopener"">wrapper</a> upon current primitive RPC APIs for this RFC, it is not efficient since two primitive RPC requests have to be made per wrapped high level RPC API, but it is tested and workable, if you would like to take a look.</p><NewLine><p>From my <em>personal</em> point of view, if torch could provide a way to “expose” a resource or service upon the RPC module, even <a href=""https://github.com/pytorch/pytorch/blob/00651b8c93354da13ab30c3fefd8cd529dc883e7/torch/distributed/nn/api/remote_module.py"" rel=""nofollow noopener"">RemoteModule</a> could be easily implemented, since it is basically create a module on a remote process, then expose its “__call__()” method as a service on the global scope,  <a href=""https://github.com/pytorch/pytorch/issues/41546"" rel=""nofollow noopener"">#41546</a> could solve this problem.</p><NewLine><p>Summary:<br/><NewLine>You can achieve all functions, using current torch APIs, if you don’t mind 20% ~ 30% efficiency loss, and spend a little?(much) time to construct your wheel, if you don’t, you can also use mine <img alt="":laughing:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/laughing.png?v=9"" title="":laughing:""/>. It would be definetly better if torch could just provide these functions, with more optimizations.</p><NewLine><p>And <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> <a class=""mention"" href=""/u/kiuk_chung"">@Kiuk_Chung</a>, please chime in and offer some feedback and precious suggestions on  <a href=""https://github.com/pytorch/pytorch/issues/41546"" rel=""nofollow noopener"">#41546</a>, there are some torchelastic issues I am not very familiar with and need some help <img alt="":thinking:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/thinking.png?v=9"" title="":thinking:""/> <img alt="":slightly_smiling_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slightly_smiling_face.png?v=9"" title="":slightly_smiling_face:""/></p><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh, and about:</p><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""87875"" data-username=""Vibhatha_Abeykoon""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/vibhatha_abeykoon/40/17261_2.png"" width=""20""/> Vibhatha_Abeykoon:</div><NewLine><blockquote><NewLine><p>deploying this multi-machine model parallel module?<br/><NewLine>Will there be modifications to torch.distributed.launch? What is the current method to launch this<br/><NewLine>as we do for DDP?</p><NewLine></blockquote><NewLine></aside><NewLine><p>That’s complex, my personal experience says that you should try to split and group your process functionalities, for example, grouping them by “role”:</p><NewLine><p>(Image from RFC <a href=""https://github.com/pytorch/pytorch/issues/41425"" rel=""nofollow noopener"">#41425</a>)<br/><NewLine><a class=""onebox"" href=""https://user-images.githubusercontent.com/43595115/87468085-cf977a80-c5cd-11ea-9be9-35cb3c580523.png"" rel=""nofollow noopener"" target=""_blank""><NewLine><img height=""173"" src=""https://user-images.githubusercontent.com/43595115/87468085-cf977a80-c5cd-11ea-9be9-35cb3c580523.png"" width=""690""/><NewLine></a><NewLine></p><NewLine><p>This idea comes from “micro services”. It makes your application logic much more clearer to understand. RFC proposal <a href=""https://github.com/pytorch/pytorch/issues/41546"" rel=""nofollow noopener"">#41546</a> also contains an automatic role-based launcher implementation to address this issue.</p><NewLine><p>However, role based design is not compatiable with:</p><NewLine><pre><code class=""lang-auto"">if __name__ == ""__main__"":<NewLine>    ...<NewLine>    tensor.to(""worker:0/cuda:1"")<NewLine>    # do some computation<NewLine>    tensor.to(""worker:1/cuda:0"")<NewLine></code></pre><NewLine><p>because you are manually specifying every destination and location,</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Vibhatha_Abeykoon; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Vibhatha_Abeykoon; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Vibhatha_Abeykoon; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Vibhatha_Abeykoon; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Vibhatha_Abeykoon; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/lcw; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/lcw; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/Vibhatha_Abeykoon; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: July 3, 2020,  6:45pm; <NewLine> REPLY_DATE 2: July 3, 2020,  8:25pm; <NewLine> REPLY_DATE 3: July 4, 2020,  1:15am; <NewLine> REPLY_DATE 4: July 4, 2020,  1:23am; <NewLine> REPLY_DATE 5: July 4, 2020,  1:31am; <NewLine> REPLY_DATE 6: July 4, 2020,  1:32am; <NewLine> REPLY_DATE 7: July 15, 2020, 12:56pm; <NewLine> REPLY_DATE 8: July 15, 2020,  2:21pm; <NewLine> REPLY_DATE 9: July 15, 2020,  2:39pm; <NewLine> REPLY_DATE 10: July 15, 2020,  2:55pm; <NewLine> REPLY_DATE 11: July 15, 2020,  4:10pm; <NewLine> REPLY_DATE 12: July 15, 2020,  4:14pm; <NewLine> REPLY_DATE 13: July 16, 2020,  1:11pm; <NewLine> REPLY_DATE 14: July 16, 2020,  7:37pm; <NewLine> REPLY_DATE 15: July 16, 2020,  3:23pm; <NewLine> REPLY_DATE 16: July 16, 2020,  7:32pm; <NewLine> REPLY_DATE 17: July 16, 2020,  7:39pm; <NewLine> REPLY_DATE 18: July 19, 2020,  5:41am; <NewLine> REPLY_DATE 19: July 19, 2020,  5:26am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> REPLY 11 LIKES: 1 Like; <NewLine> REPLY 12 LIKES: 2 Likes; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: 1 Like; <NewLine> REPLY 16 LIKES: 1 Like; <NewLine> REPLY 17 LIKES: 2 Likes; <NewLine> REPLY 18 LIKES: 1 Like; <NewLine> REPLY 19 LIKES: 1 Like; <NewLine> 
90472,DataParallel vs increasing # workers in data loader,2020-07-25T00:05:40.725Z,0,53,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to understand what the differences are in using DataParallel vs increasing the num_workers in the DataLoader.</p><NewLine><p>It seems that DataParallel divides the batch uniformly across the available GPUs, allowing the forward and backward passes to be done on each split up batch in parallel.</p><NewLine><p>But what does increasing the num_workers in DataLoader do? That is, does each process generated to consume a new batch?</p><NewLine></div>",https://discuss.pytorch.org/u/l_s,(s nothing),l_s,"July 25, 2020, 12:05am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If num_worker is &gt; 0 then that much amount of separated processes will be spawned to do the data loading job. Each process will generate a single batch. This prevents bottleneck due to dataloading as multiple processes are working on it compared to num_workers=0 where after the forward pass gpu waits for the next batch of data to be loaded.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/gouthamvgk; <NewLine> ,"REPLY_DATE 1: July 25, 2020, 12:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
47205,DistributedDataParallel loss compute and backpropogation?,2019-06-06T05:27:34.094Z,7,849,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to get <code>DistributedDataParallel</code> to work on a code, using <a href=""http://github.com/pytorch/fairseq"" rel=""nofollow noopener"">pytorch/fairseq</a> as a reference implementation. I’m finding the implementation there difficult to comprehend. I’ve opened an <a href=""https://github.com/pytorch/fairseq/issues/779"" rel=""nofollow noopener"">issue</a> for the same. Below is a (hopefully) complete relevant extract. The uncommented segment I’ve already got working and loss in converging.</p><NewLine><pre><code class=""lang-python"">    def train_step(self, sample):<NewLine>        self.model.train()<NewLine>        self._optimizer.zero_grad()<NewLine>        sample = move_to(sample, self.device)<NewLine>        loss, batch_sizes = self.model(sample)<NewLine>        # 1: Is the below done implicitly<NewLine>        #    seems to be missing in fairseq code.<NewLine>        # all-gather([loss, batch_sizes])<NewLine>        # loss = loss.sum()/batch_sizes.sum()<NewLine>        loss.backward()<NewLine>         # 2: Something similar to the following <NewLine>        #    exist. what is happening here?<NewLine>        # for p in parameters-optimized:<NewLine>        #      p.grad = p.grad*distributed_world_size/batch_sizes.sum()<NewLine>        self._optimizer.step()<NewLine>        return loss.item()<NewLine></code></pre><NewLine><p>My concerns are:</p><NewLine><ol><NewLine><li>Shouldn’t I be doing an all gather as indicated in code? Is this done implicitly?</li><NewLine><li>What is happening in the second segment?</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/jerinphilip,(Jerin Philip),jerinphilip,"June 9, 2019,  6:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/jerinphilip"">@jerinphilip</a>,</p><NewLine><ol><NewLine><li>Why would you need a gather on the loss? I can see how you might think the loss aggregation is needed for distributed training but what happens is the following. Each process computes its own output, using its own input, with its own activations, and computes its own loss. Then on <code>loss.backward()</code> all processes reduce their <em>gradients</em>. As <code>loss.backward()</code> returns, the gradients of your model parameters will be the same, and the optimizer in each process will perform the exact same update to the model parameters.</li><NewLine><li>This normalizes the gradients w.r.t. the total number of processes. If you end up using <code>torch.nn.parallel.DistributedDataParallel</code>, this is already done for you. It is possible this is still a part of fairseq as earlier versions had a custom approach for distributed data parallelism, whereas newer versions can use the upstream wrapper directly (IIRC).</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! I need an advice. I have 4 processes/gpus with DDP.  Should I implement Ioss reduction by sum (using all_reduce) before backward pass,  or is it enough just for gradients to be automatically averaged by DDP? Could increasing the learningrate by a factor of x4 compensate for the division by number of gpus done by the averaging? I am trying to get a DDP run equivalent to Dataparallel.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""47205"" data-username=""Andras_Iani""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/andras_iani/40/26560_2.png"" width=""20""/> Andras_Iani:</div><NewLine><blockquote><NewLine><p>Should I implement Ioss reduction by sum (using all_reduce) before backward pass, or is it enough just for gradients to be automatically averaged by DDP?</p><NewLine></blockquote><NewLine></aside><NewLine><p>It is <strong>not</strong> necessary to use another allreduce to sum all loss. And additional allreduce might have considerable negative impact on training speed.</p><NewLine><blockquote><NewLine><p>Could increasing the learningrate by a factor of x4 compensate for the division by number of gpus done by the averaging?</p><NewLine></blockquote><NewLine><p>This is not guaranteed and the loss function itself also plays a role here. See this discussion: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769"">Should we split batch_size according to ngpu_per_node when DistributedDataparallel</a></p><NewLine><blockquote><NewLine><p>I am trying to get a DDP run equivalent to Dataparallel.</p><NewLine></blockquote><NewLine><p>There is a subtle difference between DP and DDP. IIUC, with DP, the grads from replicated models are accumulated (i.e., sum) into the <code>param.grad</code> field in the original model, but DDP’s gradient is averaged. Not 100% confident, but I feel if we would like to let DDP behave as similar to DP as possible, we probably should multiple DDP’s result gradient by <code>world_size</code>. Whether that is the same as using 4X learning rate, might depend on the optimizer algorithm.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you! This is extremely helpful!</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am working with fcos loss. The authors of fcos treat the case of DDP and implement reduction of the loss components inside the loss script. I should get rid of that part of their code then and do not use reduction before backward. I will use reduction just for plotting the loss values (after backward) in the training script.<br/><NewLine>Is it ok in your opinion? Thanks again!</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""47205"" data-username=""Andras_Iani""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/andras_iani/40/26560_2.png"" width=""20""/> Andras_Iani:</div><NewLine><blockquote><NewLine><p>I will use reduction just for plotting the loss values (after backward) in the training script.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yep, this should be OK.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for the useful explanations.</p><NewLine><p>From the discussion above I understand that the reason why one shouldn’t do an all_gather sum of the losses when training Distributed Data Parallel mode is that these all gather operations can slow down the process.</p><NewLine><p>Are there any other reasons why the loss tensors should not be summed other than performance reasons?</p><NewLine><p>I ask this because in case the loss tensors are small, if an all_gather sum is performed when computing the losses, this will result in identical losses for all processes.  Therefore gradient averaging over processes will simply divide the losses by the number of processes.</p><NewLine><p>This has the advantage of mimicking the behavior of DataParallel and of providing consistent results independently of the number of processes being run without the need to adjust learning rates, batch sizes, etc.</p><NewLine><p>In short, when the cost of doing an all_gather sum of the losses is low, are there any other reasons beyond performance not to do it?  And isn’t the consistent behavior independently of the number of processes an advantage?</p><NewLine><p>Thank you</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""8"" data-topic=""47205"" data-username=""KikoAumond""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/k/e480ec/40.png"" width=""20""/> KikoAumond:</div><NewLine><blockquote><NewLine><p>I ask this because in case the loss tensors are small, if an all_gather sum is performed when computing the losses, this will result in identical losses for all processes. Therefore gradient averaging over processes will simply divide the losses by the number of processes.</p><NewLine></blockquote><NewLine></aside><NewLine><p>The reason this is not sufficient is because the gradient computation depends on both loss and activation. And the activation depends on the input data, which is different in all processes. Therefore, even if loss is communicated, you will still need to communicate either gradients or activation to make sure all model parameters in all processes are consistent. Otherwise, if only communicating loss and then do backward locally, models from different processes might diverge.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>That makes sense.  Thank you.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Andras_Iani; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Andras_Iani; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Andras_Iani; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/KikoAumond; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/KikoAumond; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  5:51am; <NewLine> REPLY_DATE 2: July 10, 2020, 10:13pm; <NewLine> REPLY_DATE 3: July 10, 2020, 10:33pm; <NewLine> REPLY_DATE 4: July 11, 2020,  9:46am; <NewLine> REPLY_DATE 5: July 11, 2020, 10:15am; <NewLine> REPLY_DATE 6: July 11, 2020,  3:49pm; <NewLine> REPLY_DATE 7: July 24, 2020,  1:21am; <NewLine> REPLY_DATE 8: July 24, 2020,  5:38pm; <NewLine> REPLY_DATE 9: July 24, 2020,  6:43pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> 
90409,Pytorch NCCL Point to Point,2020-07-24T13:55:52.757Z,1,87,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>According to NCCL documentation, since NCCL 2.7 Point-to-point communication can be achieved using ncclSend and ncclRecv. However in Pytorch, the newest stable version still doesn’t support send and receive when using NCCL as backend. I’m wondering is there anyway to achieve point to point communication between GPUs in Pytorch? And is there any way to integrate ncclSend and ncclRecv in Pytorch distributed?</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Yi_Zhang,(Yi Zhang),Yi_Zhang,"July 24, 2020,  1:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/yi_zhang"">@Yi_Zhang</a>, we are working on adding P2P to NCCL ProcessGroup backend. We just bumped up the NCCL submodule version in <a href=""https://github.com/pytorch/pytorch/pull/41608"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/41608</a>.</p><NewLine><p>For now, to work around it, you can create a sub group of 2 ranks, and  then use <code>dist.broadcast(tensor, src, group=sub_group)</code> to mimic P2P send/recv. PipeDream is already using that.</p><NewLine><p>If you need general P2P support, you could try the <a href=""https://pytorch.org/docs/master/rpc.html"" rel=""nofollow noopener"">RPC API</a>. A caveat is that we are still working on improving support for GPU tensors. <a href=""https://github.com/pytorch/pytorch/issues/41369"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/41369</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for the reply.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Yi_Zhang; <NewLine> ,"REPLY_DATE 1: July 24, 2020,  4:09pm; <NewLine> REPLY_DATE 2: July 24, 2020,  5:41pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
90411,PyTorch: How to parallelize over multiple GPU using multiprocessing.pool,2020-07-24T14:27:32.359Z,1,87,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have the following code which I am trying to parallelize over multiple GPUs in PyTorch:</p><NewLine><pre><code class=""lang-auto"">import numpy as np<NewLine>import torch<NewLine>from torch.multiprocessing import Pool<NewLine><NewLine>X = np.array([[1, 3, 2, 3], [2, 3, 5, 6], [1, 2, 3, 4]])<NewLine>X = torch.DoubleTensor(X).cuda()<NewLine><NewLine>def X_power_func(j):<NewLine>    X_power = X**j<NewLine>    return X_power<NewLine><NewLine>if __name__ == '__main__':<NewLine>  with Pool(processes = 2) as p:   # Parallelizing over 2 GPUs<NewLine>    results = p.map(X_power_func, range(4))<NewLine><NewLine>results<NewLine></code></pre><NewLine><p>But when I ran the code, I am getting this error:</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>RemoteTraceback                           Traceback (most recent call last)<NewLine>RemoteTraceback: <NewLine>""""""<NewLine>Traceback (most recent call last):<NewLine>  File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 119, in worker<NewLine>    result = (True, func(*args, **kwds))<NewLine>  File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 44, in mapstar<NewLine>    return list(map(*args))<NewLine>  File ""&lt;ipython-input-35-6529ab6dac60&gt;"", line 11, in X_power_func<NewLine>    X_power = X**j<NewLine>RuntimeError: CUDA error: initialization error<NewLine>""""""<NewLine><NewLine>The above exception was the direct cause of the following exception:<NewLine><NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-35-6529ab6dac60&gt; in &lt;module&gt;()<NewLine>     14 if __name__ == '__main__':<NewLine>     15   with Pool(processes = 1) as p:<NewLine>---&gt; 16     results = p.map(X_power_func, range(8))<NewLine>     17 <NewLine>     18 results<NewLine><NewLine>1 frames<NewLine>/usr/lib/python3.6/multiprocessing/pool.py in get(self, timeout)<NewLine>    642             return self._value<NewLine>    643         else:<NewLine>--&gt; 644             raise self._value<NewLine>    645 <NewLine>    646     def _set(self, i, obj):<NewLine><NewLine>RuntimeError: CUDA error: initialization error<NewLine></code></pre><NewLine><p>Where have I gone wrong? Any help would really be appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/Leockl,(Leo Chow),Leockl,"July 25, 2020,  5:57am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""90411"" data-username=""Leockl""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/l/e9a140/40.png"" width=""20""/> Leockl:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">import numpy as np<NewLine>import torch<NewLine>from torch.multiprocessing import Pool<NewLine><NewLine>X = np.array([[1, 3, 2, 3], [2, 3, 5, 6], [1, 2, 3, 4]])<NewLine>X = torch.DoubleTensor(X).cuda()<NewLine><NewLine>def X_power_func(j):<NewLine>    X_power = X**j<NewLine>    return X_power<NewLine><NewLine>if __name__ == '__main__':<NewLine>  with Pool(processes = 2) as p:   # Paralleizing over 2 GPUs<NewLine>    results = p.map(X_power_func, range(8))<NewLine><NewLine>results<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>By default, doing .cuda() will copy your tensor to device cuda:0. I do not see anywhere you have specified the device ids for multiple GPUs. Besides, <code>results</code> outside the scope of main will result in error. CUDA initialization error will be gone by using  <code>mp.set_start_method('spawn', force=True)</code> before spawning the process pool, however, that would still not give you a correct implementation for what you are trying to do.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Many thanks <a class=""mention"" href=""/u/bapi"">@bapi</a></p><NewLine><p>I added <code>mp.set_start_method('spawn', force=True)</code> into the code below. Would this be right?</p><NewLine><pre><code class=""lang-auto"">import numpy as np<NewLine>import torch<NewLine>import torch.multiprocessing as mp<NewLine>from torch.multiprocessing import Pool<NewLine><NewLine>X = np.array([[1, 3, 2, 3], [2, 3, 5, 6], [1, 2, 3, 4]])<NewLine>X = torch.DoubleTensor(X).cuda()<NewLine><NewLine>def X_power_func(j):<NewLine>    X_power = X**j<NewLine>    return X_power<NewLine><NewLine>if __name__ == '__main__':<NewLine>  mp.set_start_method('spawn', force=True)<NewLine>  with Pool(processes = 1) as p:   # Paralleizing over 2 GPUs<NewLine>    results = p.map(X_power_func, range(2))<NewLine><NewLine>results<NewLine></code></pre><NewLine><p>Also, how do I specify the device ids for multiple GPUs for my code?</p><NewLine><p>Sorry if I have too many questions.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/bapi; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Leockl; <NewLine> ,"REPLY_DATE 1: July 24, 2020,  3:36pm; <NewLine> REPLY_DATE 2: July 24, 2020,  4:08pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
90205,DistributedSampler,2020-07-22T23:40:37.280Z,0,75,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How does the DistributedSampler (together with ddp) split the dataset to different gpus? I know it will split the dataset to num_gpus chunks and each chunk will go to one of the gpus. Is it randomly sampled or sequentially?</p><NewLine></div>",https://discuss.pytorch.org/u/ginobilinie,(No Name),ginobilinie,"July 22, 2020, 11:40pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>First, it checks if the dataset size is divisible by <code>num_replicas</code>. If not, extra samples are added.</p><NewLine><p>If <code>shuffle</code> is turned on, it performs random permutation before subsampling.<br/><NewLine>You should use <code>set_epoch</code> function to modify the random seed for that.</p><NewLine><p>Then the DistributedSampler simply subsamples the data among the whole dataset.<br/><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/torch/utils/data/distributed.py#L68"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/utils/data/distributed.py#L68</a></p><NewLine><pre><code class=""lang-auto""># subsample<NewLine>indices = indices[self.rank:self.total_size:self.num_replicas]<NewLine></code></pre><NewLine><p>Note that adding extra data could cause at evaluation time due to the duplicated data.<br/><NewLine>I personally use a custom sampler (<a href=""https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py"" rel=""nofollow noopener"">DistributedEvalSampler</a>) when testing my models.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you so much for your answer. I’m now clear about it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/seungjun; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ginobilinie; <NewLine> ,"REPLY_DATE 1: July 23, 2020,  1:13am; <NewLine> REPLY_DATE 2: July 23, 2020,  6:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
89711,`torch.distributed.barrier` used in multi-node distributed data-parallel training,2020-07-18T15:59:46.894Z,24,385,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I was trying to improve one of my multi-node distributed training examples (<a href=""https://leimao.github.io/blog/PyTorch-Distributed-Training/"" rel=""nofollow noopener"">https://leimao.github.io/blog/PyTorch-Distributed-Training/</a>) by adding some torch.distributed.barrier so that I could do some multiprocess-unsafe actions, such as data download and folder creation. After adding the torch.distributed.barrier, the training could still be done on a single-node multi-GPU machine. However, it got halted on a multi-node multi-GPU machine. Can anyone suggest if it is a PyTorch bug or it is my problem? Thank you.<br/><NewLine>Here is also the modified script that has torch.distributed.barrier:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch.utils.data.distributed import DistributedSampler<NewLine>from torch.utils.data import DataLoader<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine><NewLine>import torchvision<NewLine>import torchvision.transforms as transforms<NewLine><NewLine>import argparse<NewLine>import os<NewLine>import random<NewLine>import numpy as np<NewLine><NewLine>def set_random_seeds(random_seed=0):<NewLine><NewLine>    torch.manual_seed(random_seed)<NewLine>    torch.backends.cudnn.deterministic = True<NewLine>    torch.backends.cudnn.benchmark = False<NewLine>    np.random.seed(random_seed)<NewLine>    random.seed(random_seed)<NewLine><NewLine>def evaluate(model, device, test_loader):<NewLine><NewLine>    model.eval()<NewLine><NewLine>    correct = 0<NewLine>    total = 0<NewLine>    with torch.no_grad():<NewLine>        for data in test_loader:<NewLine>            images, labels = data[0].to(device), data[1].to(device)<NewLine>            outputs = model(images)<NewLine>            _, predicted = torch.max(outputs.data, 1)<NewLine>            total += labels.size(0)<NewLine>            correct += (predicted == labels).sum().item()<NewLine><NewLine>    accuracy = correct / total<NewLine><NewLine>    return accuracy<NewLine><NewLine>def main():<NewLine><NewLine>    num_epochs_default = 100<NewLine>    batch_size_default = 256 # 1024<NewLine>    learning_rate_default = 0.1<NewLine>    random_seed_default = 0<NewLine>    model_dir_default = ""saved_models""<NewLine>    model_filename_default = ""resnet_distributed.pth""<NewLine><NewLine>    # Each process runs on 1 GPU device specified by the local_rank argument.<NewLine>    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)<NewLine>    parser.add_argument(""--local_rank"", type=int, help=""Local rank. Necessary for using the torch.distributed.launch utility."")<NewLine>    parser.add_argument(""--num_epochs"", type=int, help=""Number of training epochs."", default=num_epochs_default)<NewLine>    parser.add_argument(""--batch_size"", type=int, help=""Training batch size for one process."", default=batch_size_default)<NewLine>    parser.add_argument(""--learning_rate"", type=float, help=""Learning rate."", default=learning_rate_default)<NewLine>    parser.add_argument(""--random_seed"", type=int, help=""Random seed."", default=random_seed_default)<NewLine>    parser.add_argument(""--model_dir"", type=str, help=""Directory for saving models."", default=model_dir_default)<NewLine>    parser.add_argument(""--model_filename"", type=str, help=""Model filename."", default=model_filename_default)<NewLine>    parser.add_argument(""--resume"", action=""store_true"", help=""Resume training from saved checkpoint."")<NewLine>    argv = parser.parse_args()<NewLine><NewLine>    local_rank = argv.local_rank<NewLine>    num_epochs = argv.num_epochs<NewLine>    batch_size = argv.batch_size<NewLine>    learning_rate = argv.learning_rate<NewLine>    random_seed = argv.random_seed<NewLine>    model_dir = argv.model_dir<NewLine>    model_filename = argv.model_filename<NewLine>    resume = argv.resume<NewLine><NewLine>    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs<NewLine>    torch.distributed.init_process_group(backend=""nccl"")<NewLine>    # torch.distributed.init_process_group(backend=""gloo"")<NewLine><NewLine>    if local_rank != 0:<NewLine>        torch.distributed.barrier()<NewLine><NewLine>    # Create directories outside the PyTorch program<NewLine>    # Only create directory in one process because it is not multiprocess safe<NewLine>    if not os.path.exists(model_dir):<NewLine>        os.makedirs(model_dir)<NewLine><NewLine>    # Prepare dataset and dataloader<NewLine>    transform = transforms.Compose([<NewLine>        transforms.RandomCrop(32, padding=4),<NewLine>        transforms.RandomHorizontalFlip(),<NewLine>        transforms.ToTensor(),<NewLine>        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),<NewLine>    ])<NewLine><NewLine>    train_set = torchvision.datasets.CIFAR10(root=""data"", train=True, download=True, transform=transform) <NewLine>    test_set = torchvision.datasets.CIFAR10(root=""data"", train=False, download=True, transform=transform)<NewLine><NewLine>    model_filepath = os.path.join(model_dir, model_filename)<NewLine><NewLine>    # We need to use seeds to make sure that the models initialized in different processes are the same<NewLine>    set_random_seeds(random_seed=random_seed)<NewLine><NewLine>    # Encapsulate the model on the GPU assigned to the current process<NewLine>    model = torchvision.models.resnet18(pretrained=False)<NewLine><NewLine>    if local_rank == 0:<NewLine>        torch.distributed.barrier()<NewLine><NewLine>    device = torch.device(""cuda:{}"".format(local_rank))<NewLine>    model = model.to(device)<NewLine>    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)<NewLine><NewLine>    # We only save the model who uses device ""cuda:0""<NewLine>    # To resume, the device for the saved model would also be ""cuda:0""<NewLine>    if resume == True:<NewLine>        map_location = {""cuda:0"": ""cuda:{}"".format(local_rank)}<NewLine>        ddp_model.load_state_dict(torch.load(model_filepath, map_location=map_location))<NewLine><NewLine>    # Restricts data loading to a subset of the dataset exclusive to the current process<NewLine>    train_sampler = DistributedSampler(dataset=train_set)<NewLine><NewLine>    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, sampler=train_sampler, num_workers=8)<NewLine>    # Test loader does not have to follow distributed sampling strategy<NewLine>    test_loader = DataLoader(dataset=test_set, batch_size=128, shuffle=False, num_workers=8)<NewLine><NewLine>    criterion = nn.CrossEntropyLoss()<NewLine>    optimizer = optim.SGD(ddp_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)<NewLine><NewLine>    # Loop over the dataset multiple times<NewLine>    for epoch in range(num_epochs):<NewLine><NewLine>        print(""Local Rank: {}, Epoch: {}, Training ..."".format(local_rank, epoch))<NewLine>        <NewLine>        # Save and evaluate model routinely<NewLine>        if epoch % 10 == 0:<NewLine>            if local_rank == 0:<NewLine>                accuracy = evaluate(model=ddp_model, device=device, test_loader=test_loader)<NewLine>                torch.save(ddp_model.state_dict(), model_filepath)<NewLine>                print(""-"" * 75)<NewLine>                print(""Epoch: {}, Accuracy: {}"".format(epoch, accuracy))<NewLine>                print(""-"" * 75)<NewLine><NewLine>        ddp_model.train()<NewLine><NewLine>        for data in train_loader:<NewLine>            inputs, labels = data[0].to(device), data[1].to(device)<NewLine>            optimizer.zero_grad()<NewLine>            outputs = ddp_model(inputs)<NewLine>            loss = criterion(outputs, labels)<NewLine>            loss.backward()<NewLine>            optimizer.step()<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    <NewLine>    main()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/leimao,(Lei Mao),leimao,"July 18, 2020,  3:59pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>barrier()</code> requires all processes in your process group to join, so this is incorrect:</p><NewLine><pre><code class=""lang-auto"">if local_rank == 0:<NewLine>        torch.distributed.barrier()<NewLine></code></pre><NewLine><p>Remember, all collective APIs of <code>torch.distributed</code>(i.e. not include P2P API: <code>send, recv, isend, irecv</code>), requires <strong>all processes</strong> in your created process group, either the implicit global group or a sub group created by <code>torch.distributed.new_group</code>, to execute.</p><NewLine><p>Will this solve your problem? Please have a test and respond. <img alt="":blush:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/blush.png?v=9"" title="":blush:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <a class=""mention"" href=""/u/iffix"">@iffiX</a> for the insightful response. I am not sure if I fully understood, but I do have:</p><NewLine><pre><code class=""lang-auto"">    if local_rank != 0:<NewLine>        torch.distributed.barrier()<NewLine></code></pre><NewLine><p>earlier in the code. The purpose is to pause the execution of all the local ranks except for the first local rank to create directory and download dataset without conflicts. Once the first local rank completed the download and directory creation, the reset of local ranks could use the downloaded dataset and directory.</p><NewLine><p>In your opinion, how should I modify my code in particular? Thank you.</p><NewLine><p>Best,</p><NewLine><p>Lei</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>just do:</p><NewLine><pre><code class=""lang-auto"">torch.distributed.barrier()<NewLine></code></pre><NewLine><p>without <code>if</code>,<br/><NewLine>since:</p><NewLine><blockquote><NewLine><p>This collective blocks processes until the whole group enters this function</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much <a class=""mention"" href=""/u/iffix"">@iffiX</a>. I removed the if statement, and here is the code I was running:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch.utils.data.distributed import DistributedSampler<NewLine>from torch.utils.data import DataLoader<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine><NewLine>import torchvision<NewLine>import torchvision.transforms as transforms<NewLine><NewLine>import argparse<NewLine>import os<NewLine>import random<NewLine>import numpy as np<NewLine><NewLine>def set_random_seeds(random_seed=0):<NewLine><NewLine>    torch.manual_seed(random_seed)<NewLine>    torch.backends.cudnn.deterministic = True<NewLine>    torch.backends.cudnn.benchmark = False<NewLine>    np.random.seed(random_seed)<NewLine>    random.seed(random_seed)<NewLine><NewLine>def evaluate(model, device, test_loader):<NewLine><NewLine>    model.eval()<NewLine><NewLine>    correct = 0<NewLine>    total = 0<NewLine>    with torch.no_grad():<NewLine>        for data in test_loader:<NewLine>            images, labels = data[0].to(device), data[1].to(device)<NewLine>            outputs = model(images)<NewLine>            _, predicted = torch.max(outputs.data, 1)<NewLine>            total += labels.size(0)<NewLine>            correct += (predicted == labels).sum().item()<NewLine><NewLine>    accuracy = correct / total<NewLine><NewLine>    return accuracy<NewLine><NewLine>def main():<NewLine><NewLine>    num_epochs_default = 100<NewLine>    batch_size_default = 256 # 1024<NewLine>    learning_rate_default = 0.1<NewLine>    random_seed_default = 0<NewLine>    model_dir_default = ""saved_models""<NewLine>    model_filename_default = ""resnet_distributed.pth""<NewLine><NewLine>    # Each process runs on 1 GPU device specified by the local_rank argument.<NewLine>    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)<NewLine>    parser.add_argument(""--local_rank"", type=int, help=""Local rank. Necessary for using the torch.distributed.launch utility."")<NewLine>    parser.add_argument(""--num_epochs"", type=int, help=""Number of training epochs."", default=num_epochs_default)<NewLine>    parser.add_argument(""--batch_size"", type=int, help=""Training batch size for one process."", default=batch_size_default)<NewLine>    parser.add_argument(""--learning_rate"", type=float, help=""Learning rate."", default=learning_rate_default)<NewLine>    parser.add_argument(""--random_seed"", type=int, help=""Random seed."", default=random_seed_default)<NewLine>    parser.add_argument(""--model_dir"", type=str, help=""Directory for saving models."", default=model_dir_default)<NewLine>    parser.add_argument(""--model_filename"", type=str, help=""Model filename."", default=model_filename_default)<NewLine>    parser.add_argument(""--resume"", action=""store_true"", help=""Resume training from saved checkpoint."")<NewLine>    argv = parser.parse_args()<NewLine><NewLine>    local_rank = argv.local_rank<NewLine>    num_epochs = argv.num_epochs<NewLine>    batch_size = argv.batch_size<NewLine>    learning_rate = argv.learning_rate<NewLine>    random_seed = argv.random_seed<NewLine>    model_dir = argv.model_dir<NewLine>    model_filename = argv.model_filename<NewLine>    resume = argv.resume<NewLine><NewLine>    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs<NewLine>    torch.distributed.init_process_group(backend=""nccl"")<NewLine>    # torch.distributed.init_process_group(backend=""gloo"")<NewLine><NewLine>    if local_rank != 0:<NewLine>        torch.distributed.barrier()<NewLine><NewLine>    # Create directories outside the PyTorch program<NewLine>    # Only create directory in one process because it is not multiprocess safe<NewLine>    if not os.path.exists(model_dir):<NewLine>        os.makedirs(model_dir)<NewLine><NewLine>    # Prepare dataset and dataloader<NewLine>    transform = transforms.Compose([<NewLine>        transforms.RandomCrop(32, padding=4),<NewLine>        transforms.RandomHorizontalFlip(),<NewLine>        transforms.ToTensor(),<NewLine>        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),<NewLine>    ])<NewLine><NewLine>    train_set = torchvision.datasets.CIFAR10(root=""data"", train=True, download=True, transform=transform) <NewLine>    test_set = torchvision.datasets.CIFAR10(root=""data"", train=False, download=True, transform=transform)<NewLine><NewLine>    model_filepath = os.path.join(model_dir, model_filename)<NewLine><NewLine>    # We need to use seeds to make sure that the models initialized in different processes are the same<NewLine>    set_random_seeds(random_seed=random_seed)<NewLine><NewLine>    # Encapsulate the model on the GPU assigned to the current process<NewLine>    model = torchvision.models.resnet18(pretrained=False)<NewLine><NewLine>    '''<NewLine>    if local_rank != 0:<NewLine>        torch.distributed.barrier()<NewLine>    '''<NewLine>    torch.distributed.barrier()<NewLine><NewLine>    device = torch.device(""cuda:{}"".format(local_rank))<NewLine>    model = model.to(device)<NewLine>    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)<NewLine><NewLine>    # We only save the model who uses device ""cuda:0""<NewLine>    # To resume, the device for the saved model would also be ""cuda:0""<NewLine>    if resume == True:<NewLine>        map_location = {""cuda:0"": ""cuda:{}"".format(local_rank)}<NewLine>        ddp_model.load_state_dict(torch.load(model_filepath, map_location=map_location))<NewLine><NewLine>    # Restricts data loading to a subset of the dataset exclusive to the current process<NewLine>    train_sampler = DistributedSampler(dataset=train_set)<NewLine><NewLine>    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, sampler=train_sampler, num_workers=8)<NewLine>    # Test loader does not have to follow distributed sampling strategy<NewLine>    test_loader = DataLoader(dataset=test_set, batch_size=128, shuffle=False, num_workers=8)<NewLine><NewLine>    criterion = nn.CrossEntropyLoss()<NewLine>    optimizer = optim.SGD(ddp_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)<NewLine><NewLine>    # Loop over the dataset multiple times<NewLine>    for epoch in range(num_epochs):<NewLine><NewLine>        print(""Local Rank: {}, Epoch: {}, Training ..."".format(local_rank, epoch))<NewLine>        <NewLine>        # Save and evaluate model routinely<NewLine>        if epoch % 10 == 0:<NewLine>            if local_rank == 0:<NewLine>                accuracy = evaluate(model=ddp_model, device=device, test_loader=test_loader)<NewLine>                torch.save(ddp_model.state_dict(), model_filepath)<NewLine>                print(""-"" * 75)<NewLine>                print(""Epoch: {}, Accuracy: {}"".format(epoch, accuracy))<NewLine>                print(""-"" * 75)<NewLine><NewLine>        ddp_model.train()<NewLine><NewLine>        for data in train_loader:<NewLine>            inputs, labels = data[0].to(device), data[1].to(device)<NewLine>            optimizer.zero_grad()<NewLine>            outputs = ddp_model(inputs)<NewLine>            loss = criterion(outputs, labels)<NewLine>            loss.backward()<NewLine>            optimizer.step()<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    <NewLine>    main()<NewLine></code></pre><NewLine><p>However, it still got halted.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>My former implementation was actually inspired by the implementation from HuggingFace transformer. Here is how they were using the <code>torch.distributed.barrier</code>.<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/huggingface/transformers/blob/c76c3cebed3c707178d9f721349c5abd5206a57f/examples/run_glue.py#L360"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/huggingface/transformers/blob/c76c3cebed3c707178d9f721349c5abd5206a57f/examples/run_glue.py#L360"" rel=""nofollow noopener"" target=""_blank"">huggingface/transformers/blob/c76c3cebed3c707178d9f721349c5abd5206a57f/examples/run_glue.py#L360</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""350"" style=""counter-reset: li-counter 349 ;""><NewLine><li>        with open(output_eval_file, ""w"") as writer:</li><NewLine><li>            logger.info(""***** Eval results {} *****"".format(prefix))</li><NewLine><li>            for key in sorted(result.keys()):</li><NewLine><li>                logger.info(""  %s = %s"", key, str(result[key]))</li><NewLine><li>                writer.write(""%s = %s\n"" % (key, str(result[key])))</li><NewLine><li><NewLine></li><li>    return results</li><NewLine><li><NewLine></li><li><NewLine></li><li>def load_and_cache_examples(args, task, tokenizer, evaluate=False):</li><NewLine><li class=""selected"">    if args.local_rank not in [-1, 0] and not evaluate:</li><NewLine><li>        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache</li><NewLine><li><NewLine></li><li>    processor = processors[task]()</li><NewLine><li>    output_mode = output_modes[task]</li><NewLine><li>    # Load data features from cache or dataset file</li><NewLine><li>    cached_features_file = os.path.join(</li><NewLine><li>        args.data_dir,</li><NewLine><li>        ""cached_{}_{}_{}_{}"".format(</li><NewLine><li>            ""dev"" if evaluate else ""train"",</li><NewLine><li>            list(filter(None, args.model_name_or_path.split(""/""))).pop(),</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/huggingface/transformers/blob/c76c3cebed3c707178d9f721349c5abd5206a57f/examples/run_glue.py#L401"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/huggingface/transformers/blob/c76c3cebed3c707178d9f721349c5abd5206a57f/examples/run_glue.py#L401"" rel=""nofollow noopener"" target=""_blank"">huggingface/transformers/blob/c76c3cebed3c707178d9f721349c5abd5206a57f/examples/run_glue.py#L401</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""391"" style=""counter-reset: li-counter 390 ;""><NewLine><li>        max_length=args.max_seq_length,</li><NewLine><li>        output_mode=output_mode,</li><NewLine><li>        pad_on_left=bool(args.model_type in [""xlnet""]),  # pad on the left for xlnet</li><NewLine><li>        pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],</li><NewLine><li>        pad_token_segment_id=4 if args.model_type in [""xlnet""] else 0,</li><NewLine><li>    )</li><NewLine><li>    if args.local_rank in [-1, 0]:</li><NewLine><li>        logger.info(""Saving features into cached file %s"", cached_features_file)</li><NewLine><li>        torch.save(features, cached_features_file)</li><NewLine><li><NewLine></li><li class=""selected"">if args.local_rank == 0 and not evaluate:</li><NewLine><li>    torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache</li><NewLine><li><NewLine></li><li># Convert to Tensors and build dataset</li><NewLine><li>all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)</li><NewLine><li>all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)</li><NewLine><li>all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)</li><NewLine><li>if output_mode == ""classification"":</li><NewLine><li>    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)</li><NewLine><li>elif output_mode == ""regression"":</li><NewLine><li>    all_labels = torch.tensor([f.label for f in features], dtype=torch.float)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>I did not run their code in person by the way.<NewLine>        </div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""89711"" data-username=""leimao""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/leimao/40/26860_2.png"" width=""20""/> leimao:</div><NewLine><blockquote><NewLine><p>However, it got halted on a multi-node multi-GPU machine. Can anyone suggest if it is a PyTorch bug or it is my problem?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Hey <a class=""mention"" href=""/u/leimao"">@leimao</a>, which line caused the hang on rank0 and other ranks?</p><NewLine><p>BTW, how did you launch the program? Are the following parameters used in this experiment? And it works if you remove the barrier + save/load code?</p><NewLine><pre><code class=""lang-auto"">$ python -m torch.distributed.launch --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=""192.168.0.1"" --master_port=1234 resnet_ddp.py<NewLine>$ python -m torch.distributed.launch --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=""192.168.0.1"" --master_port=1234 resnet_ddp.py<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>One more question, during the execution, did you set <code>resume</code> to True or False? I am not confident if <code>ddp_model.load_state_dict</code> can restore all DDP states properly. We don’t have tests covering that yet. It might be safer to save <code>ddp_model.module</code> and then reconstruct DDP instances from the loaded <code>ddp_model.module</code>.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>. I believe it got halted in the second <code>torch.distributed.barrier</code>. Because I could see the data download/preprocessing was successful on both nodes:</p><NewLine><pre><code class=""lang-auto"">Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine></code></pre><NewLine><p>I was using the following command to start this time, only changing the number of gpus from 8 to 4 for each node. It runs fine with or without barriers if I only train using one single node.</p><NewLine><pre><code class=""lang-auto"">$ python -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=0 --master_addr=""192.168.0.1"" --master_port=1234 resnet_ddp.py<NewLine>$ python -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=1 --master_addr=""192.168.0.1"" --master_port=1234 resnet_ddp.py<NewLine></code></pre><NewLine><p>I have not tried removing the saving model code but will give it a shot.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>For now, resume is always False during my test, i.e., it is always training from scratch. So we could safely ignore those code for now.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>To put it simply, if you just want process to execute mkdir, download, etc, then you should:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import argparse<NewLine><NewLine><NewLine>def main():<NewLine>    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs<NewLine>    torch.distributed.init_process_group(backend=""nccl"")<NewLine><NewLine>    parser = argparse.ArgumentParser()<NewLine>    parser.add_argument(""--local_rank"", type=int)<NewLine>    args = parser.parse_args()<NewLine>    local_rank = args.local_rank<NewLine>    <NewLine>    torch.distributed.barrier()<NewLine><NewLine>    if local_rank == 0:<NewLine>        print(local_rank)<NewLine>    <NewLine>    torch.distributed.barrier()<NewLine><NewLine>    print(""{} exit"".format(local_rank))<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    main()<NewLine></code></pre><NewLine><p>this will print:</p><NewLine><pre><code class=""lang-auto"">0<NewLine>0 exit<NewLine>2 exit<NewLine>1 exit3 exit<NewLine></code></pre><NewLine><p>And should not</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import argparse<NewLine><NewLine><NewLine>def main():<NewLine>    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs<NewLine>    torch.distributed.init_process_group(backend=""nccl"")<NewLine><NewLine>    parser = argparse.ArgumentParser()<NewLine>    parser.add_argument(""--local_rank"", type=int)<NewLine>    args = parser.parse_args()<NewLine>    local_rank = args.local_rank<NewLine>    <NewLine>    if local_rank != 0:<NewLine>        torch.distributed.barrier()<NewLine><NewLine>    print(local_rank)<NewLine>    <NewLine>    if local_rank == 0:<NewLine>        torch.distributed.barrier()<NewLine><NewLine>    print(""{} exit"".format(local_rank))<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    main()<NewLine></code></pre><NewLine><p>which will print</p><NewLine><pre><code class=""lang-auto"">0<NewLine>0 exit<NewLine>2<NewLine>2 exit<NewLine>13<NewLine>3 exit<NewLine><NewLine>1 exit<NewLine></code></pre><NewLine><p><strong>barrier is just a barrier, it requires all processes in the group to reach one barrier function, no matter where it is placed</strong>, so the second function basically delays all other processes (except 0), unless the code in between two barriers is a not-effective (equal to return / pass) once any process has executed it   (Eg: process 0), you are not going to get your expected result.</p><NewLine><p>And please make sure that your CUDA runtime has the same major &amp; minor version as your the CUDA version your torch you have built with, 9 is not compatible with 10, so you are likely to experience some issues when using “nccl” or cuda tensor computations.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much for repeating all the experiments <a class=""mention"" href=""/u/iffix"">@iffiX</a>. I wanted to download CIFAR-10 dataset using local rank 0, and once the local rank 0 has downloaded the dataset, local rank 1, 2, and 3 could proceed and use the downloaded cache for data preprocessing.</p><NewLine><pre><code class=""lang-auto"">    train_set = torchvision.datasets.CIFAR10(root=""data"", train=True, download=True, transform=transform) <NewLine>    test_set = torchvision.datasets.CIFAR10(root=""data"", train=False, download=True, transform=transform)<NewLine></code></pre><NewLine><p>However, I don’t see your solution,</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import argparse<NewLine><NewLine><NewLine>def main():<NewLine>    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs<NewLine>    torch.distributed.init_process_group(backend=""nccl"")<NewLine><NewLine>    parser = argparse.ArgumentParser()<NewLine>    parser.add_argument(""--local_rank"", type=int)<NewLine>    args = parser.parse_args()<NewLine>    local_rank = args.local_rank<NewLine>    <NewLine>    torch.distributed.barrier()<NewLine><NewLine>    if local_rank == 0:<NewLine>        print(local_rank)<NewLine>    <NewLine>    torch.distributed.barrier()<NewLine><NewLine>    print(""{} exit"".format(local_rank))<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    main()<NewLine></code></pre><NewLine><p>in particular, is able to do this.<br/><NewLine>The printout of your second code snippet, in particular,</p><NewLine><pre><code class=""lang-auto"">def main():<NewLine>    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs<NewLine>    torch.distributed.init_process_group(backend=""nccl"")<NewLine><NewLine>    parser = argparse.ArgumentParser()<NewLine>    parser.add_argument(""--local_rank"", type=int)<NewLine>    args = parser.parse_args()<NewLine>    local_rank = args.local_rank<NewLine>    <NewLine>    if local_rank != 0:<NewLine>        torch.distributed.barrier()<NewLine><NewLine>    print(local_rank)<NewLine>    <NewLine>    if local_rank == 0:<NewLine>        torch.distributed.barrier()<NewLine><NewLine>    print(""{} exit"".format(local_rank))<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    main()<NewLine></code></pre><NewLine><p>is expected and it is also what I was trying to implement. I want local rank 0 to do all the stuff once, then local rank 1, 2, and 3 start to the stuff in their own processes.</p><NewLine><p>I think my CUDA version is compatible with PyTorch. I am using CUDA 10.2 + PyTorch 1.51.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>The “asynchronous barrier” was also used in the HuggingFace example that I mentioned above. Since many people are using HuggingFace, I think their code at least runs fine on single node.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>I thought of inelegant way to get around:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch.utils.data.distributed import DistributedSampler<NewLine>from torch.utils.data import DataLoader<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine><NewLine>import torchvision<NewLine>import torchvision.transforms as transforms<NewLine><NewLine>import argparse<NewLine>import os<NewLine>import random<NewLine>import numpy as np<NewLine><NewLine>def set_random_seeds(random_seed=0):<NewLine><NewLine>    torch.manual_seed(random_seed)<NewLine>    torch.backends.cudnn.deterministic = True<NewLine>    torch.backends.cudnn.benchmark = False<NewLine>    np.random.seed(random_seed)<NewLine>    random.seed(random_seed)<NewLine><NewLine>def evaluate(model, device, test_loader):<NewLine><NewLine>    model.eval()<NewLine><NewLine>    correct = 0<NewLine>    total = 0<NewLine>    with torch.no_grad():<NewLine>        for data in test_loader:<NewLine>            images, labels = data[0].to(device), data[1].to(device)<NewLine>            outputs = model(images)<NewLine>            _, predicted = torch.max(outputs.data, 1)<NewLine>            total += labels.size(0)<NewLine>            correct += (predicted == labels).sum().item()<NewLine><NewLine>    accuracy = correct / total<NewLine><NewLine>    return accuracy<NewLine><NewLine>def main():<NewLine><NewLine>    num_epochs_default = 100<NewLine>    batch_size_default = 256 # 1024<NewLine>    learning_rate_default = 0.1<NewLine>    random_seed_default = 0<NewLine>    model_dir_default = ""saved_models""<NewLine>    model_filename_default = ""resnet_distributed.pth""<NewLine><NewLine>    # Each process runs on 1 GPU device specified by the local_rank argument.<NewLine>    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)<NewLine>    parser.add_argument(""--local_rank"", type=int, help=""Local rank. Necessary for using the torch.distributed.launch utility."")<NewLine>    parser.add_argument(""--num_epochs"", type=int, help=""Number of training epochs."", default=num_epochs_default)<NewLine>    parser.add_argument(""--batch_size"", type=int, help=""Training batch size for one process."", default=batch_size_default)<NewLine>    parser.add_argument(""--learning_rate"", type=float, help=""Learning rate."", default=learning_rate_default)<NewLine>    parser.add_argument(""--random_seed"", type=int, help=""Random seed."", default=random_seed_default)<NewLine>    parser.add_argument(""--model_dir"", type=str, help=""Directory for saving models."", default=model_dir_default)<NewLine>    parser.add_argument(""--model_filename"", type=str, help=""Model filename."", default=model_filename_default)<NewLine>    parser.add_argument(""--resume"", action=""store_true"", help=""Resume training from saved checkpoint."")<NewLine>    argv = parser.parse_args()<NewLine><NewLine>    local_rank = argv.local_rank<NewLine>    num_epochs = argv.num_epochs<NewLine>    batch_size = argv.batch_size<NewLine>    learning_rate = argv.learning_rate<NewLine>    random_seed = argv.random_seed<NewLine>    model_dir = argv.model_dir<NewLine>    model_filename = argv.model_filename<NewLine>    resume = argv.resume<NewLine><NewLine>    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs<NewLine>    torch.distributed.init_process_group(backend=""nccl"")<NewLine>    # torch.distributed.init_process_group(backend=""gloo"")<NewLine><NewLine>    # torch.distributed.barrier()<NewLine>    # Create directories outside the PyTorch program<NewLine>    # Only create directory in one process because it is not multiprocess safe<NewLine>    if local_rank == 0:<NewLine>        if not os.path.exists(model_dir):<NewLine>            os.makedirs(model_dir)<NewLine><NewLine>    # Prepare dataset and dataloader<NewLine>    transform = transforms.Compose([<NewLine>        transforms.RandomCrop(32, padding=4),<NewLine>        transforms.RandomHorizontalFlip(),<NewLine>        transforms.ToTensor(),<NewLine>        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),<NewLine>    ])<NewLine><NewLine><NewLine>    if local_rank == 0:<NewLine>        train_set = torchvision.datasets.CIFAR10(root=""data"", train=True, download=True, transform=transform) <NewLine>        test_set = torchvision.datasets.CIFAR10(root=""data"", train=False, download=True, transform=transform)<NewLine>        <NewLine>    torch.distributed.barrier()<NewLine><NewLine>    train_set = torchvision.datasets.CIFAR10(root=""data"", train=True, download=True, transform=transform) <NewLine>    test_set = torchvision.datasets.CIFAR10(root=""data"", train=False, download=True, transform=transform)<NewLine><NewLine>    model_filepath = os.path.join(model_dir, model_filename)<NewLine><NewLine><NewLine>    # We need to use seeds to make sure that the models initialized in different processes are the same<NewLine>    set_random_seeds(random_seed=random_seed)<NewLine><NewLine>    # Encapsulate the model on the GPU assigned to the current process<NewLine>    model = torchvision.models.resnet18(pretrained=False)<NewLine><NewLine>    device = torch.device(""cuda:{}"".format(local_rank))<NewLine>    model = model.to(device)<NewLine>    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)<NewLine><NewLine>    # We only save the model who uses device ""cuda:0""<NewLine>    # To resume, the device for the saved model would also be ""cuda:0""<NewLine>    if resume == True:<NewLine>        map_location = {""cuda:0"": ""cuda:{}"".format(local_rank)}<NewLine>        ddp_model.load_state_dict(torch.load(model_filepath, map_location=map_location))<NewLine><NewLine>    # Restricts data loading to a subset of the dataset exclusive to the current process<NewLine>    train_sampler = DistributedSampler(dataset=train_set)<NewLine><NewLine>    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, sampler=train_sampler, num_workers=8)<NewLine>    # Test loader does not have to follow distributed sampling strategy<NewLine>    test_loader = DataLoader(dataset=test_set, batch_size=128, shuffle=False, num_workers=8)<NewLine><NewLine>    criterion = nn.CrossEntropyLoss()<NewLine>    optimizer = optim.SGD(ddp_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)<NewLine><NewLine>    # Loop over the dataset multiple times<NewLine>    for epoch in range(num_epochs):<NewLine><NewLine>        print(""Local Rank: {}, Epoch: {}, Training ..."".format(local_rank, epoch))<NewLine>        <NewLine>        # Save and evaluate model routinely<NewLine>        if epoch % 10 == 0:<NewLine>            if local_rank == 0:<NewLine>                accuracy = evaluate(model=ddp_model, device=device, test_loader=test_loader)<NewLine>                torch.save(ddp_model.state_dict(), model_filepath)<NewLine>                print(""-"" * 75)<NewLine>                print(""Epoch: {}, Accuracy: {}"".format(epoch, accuracy))<NewLine>                print(""-"" * 75)<NewLine><NewLine>        ddp_model.train()<NewLine><NewLine>        for data in train_loader:<NewLine>            inputs, labels = data[0].to(device), data[1].to(device)<NewLine>            optimizer.zero_grad()<NewLine>            outputs = ddp_model(inputs)<NewLine>            loss = criterion(outputs, labels)<NewLine>            loss.backward()<NewLine>            optimizer.step()<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    <NewLine>    main()<NewLine></code></pre><NewLine><p>But it still got stuck.<br/><NewLine>On node 0:</p><NewLine><pre><code class=""lang-auto"">100.0%Extracting data/cifar-10-python.tar.gz to data<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Local Rank: 3, Epoch: 0, Training ...<NewLine>Local Rank: 2, Epoch: 0, Training ...<NewLine>Local Rank: 1, Epoch: 0, Training ...<NewLine>Local Rank: 0, Epoch: 0, Training ...<NewLine></code></pre><NewLine><p>On node 1:</p><NewLine><pre><code class=""lang-auto"">100.0%Extracting data/cifar-10-python.tar.gz to data<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> I commented the model saving code but still got halted.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>after reading your code a little bit more carefully I agree that you may use the second solution since all processes needs to create the data loader, so the problem is not there.<br/><NewLine>Could you please try to add some printing functions such as:</p><NewLine><pre><code class=""lang-auto"">print(""line230"")<NewLine>...<NewLine>print(""line232"")<NewLine></code></pre><NewLine><p>to show <strong>exactly</strong> where you code has halted? current log is way to limited to determine the exact statement which caused you code to halt.<br/><NewLine>And don’t forget to take care of <code>ddp_model.load_state_dict(torch.load(model_filepath, map_location=map_location))</code> after solving the halting issue, as <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> said.</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> In your tutorial (<a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#save-and-load-checkpoints"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#save-and-load-checkpoints</a>), I saw you were using <code>ddp_model.load_state_dict</code> to load model parameters. Is this method untested and unfavored?<br/><NewLine>I remember the example I documented in my blog post works perfectly. I tested model resuming a while ago and it worked fine. It’s having problems only when I tried to add some barrier functions a few days ago.<br/><NewLine>Thank you.</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/iffix"">@iffiX</a> <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> It seems that I have located where the halting is happening. Running the following code:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch.utils.data.distributed import DistributedSampler<NewLine>from torch.utils.data import DataLoader<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine><NewLine>import torchvision<NewLine>import torchvision.transforms as transforms<NewLine><NewLine>import argparse<NewLine>import os<NewLine>import random<NewLine>import numpy as np<NewLine><NewLine>def set_random_seeds(random_seed=0):<NewLine><NewLine>    torch.manual_seed(random_seed)<NewLine>    torch.backends.cudnn.deterministic = True<NewLine>    torch.backends.cudnn.benchmark = False<NewLine>    np.random.seed(random_seed)<NewLine>    random.seed(random_seed)<NewLine><NewLine>def evaluate(model, device, test_loader):<NewLine><NewLine>    model.eval()<NewLine><NewLine>    correct = 0<NewLine>    total = 0<NewLine>    with torch.no_grad():<NewLine>        for data in test_loader:<NewLine>            images, labels = data[0].to(device), data[1].to(device)<NewLine>            outputs = model(images)<NewLine>            _, predicted = torch.max(outputs.data, 1)<NewLine>            total += labels.size(0)<NewLine>            correct += (predicted == labels).sum().item()<NewLine><NewLine>    accuracy = correct / total<NewLine><NewLine>    return accuracy<NewLine><NewLine>def main():<NewLine><NewLine>    num_epochs_default = 100<NewLine>    batch_size_default = 256 # 1024<NewLine>    learning_rate_default = 0.1<NewLine>    random_seed_default = 0<NewLine>    model_dir_default = ""saved_models""<NewLine>    model_filename_default = ""resnet_distributed.pth""<NewLine><NewLine>    # Each process runs on 1 GPU device specified by the local_rank argument.<NewLine>    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)<NewLine>    parser.add_argument(""--local_rank"", type=int, help=""Local rank. Necessary for using the torch.distributed.launch utility."")<NewLine>    parser.add_argument(""--num_epochs"", type=int, help=""Number of training epochs."", default=num_epochs_default)<NewLine>    parser.add_argument(""--batch_size"", type=int, help=""Training batch size for one process."", default=batch_size_default)<NewLine>    parser.add_argument(""--learning_rate"", type=float, help=""Learning rate."", default=learning_rate_default)<NewLine>    parser.add_argument(""--random_seed"", type=int, help=""Random seed."", default=random_seed_default)<NewLine>    parser.add_argument(""--model_dir"", type=str, help=""Directory for saving models."", default=model_dir_default)<NewLine>    parser.add_argument(""--model_filename"", type=str, help=""Model filename."", default=model_filename_default)<NewLine>    parser.add_argument(""--resume"", action=""store_true"", help=""Resume training from saved checkpoint."")<NewLine>    argv = parser.parse_args()<NewLine><NewLine>    local_rank = argv.local_rank<NewLine>    num_epochs = argv.num_epochs<NewLine>    batch_size = argv.batch_size<NewLine>    learning_rate = argv.learning_rate<NewLine>    random_seed = argv.random_seed<NewLine>    model_dir = argv.model_dir<NewLine>    model_filename = argv.model_filename<NewLine>    resume = argv.resume<NewLine><NewLine>    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs<NewLine>    torch.distributed.init_process_group(backend=""nccl"")<NewLine>    # torch.distributed.init_process_group(backend=""gloo"")<NewLine><NewLine>    if local_rank != 0:<NewLine>        torch.distributed.barrier()<NewLine>    <NewLine>    print(""Local Rank: {} | Location: {}"".format(local_rank, 0))<NewLine><NewLine>    # Create directories outside the PyTorch program<NewLine>    # Only create directory in one process because it is not multiprocess safe<NewLine>    if not os.path.exists(model_dir):<NewLine>        os.makedirs(model_dir)<NewLine><NewLine>    # Prepare dataset and dataloader<NewLine>    transform = transforms.Compose([<NewLine>        transforms.RandomCrop(32, padding=4),<NewLine>        transforms.RandomHorizontalFlip(),<NewLine>        transforms.ToTensor(),<NewLine>        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),<NewLine>    ])<NewLine><NewLine>    train_set = torchvision.datasets.CIFAR10(root=""data"", train=True, download=True, transform=transform) <NewLine>    test_set = torchvision.datasets.CIFAR10(root=""data"", train=False, download=True, transform=transform)<NewLine><NewLine>    model_filepath = os.path.join(model_dir, model_filename)<NewLine><NewLine>    # We need to use seeds to make sure that the models initialized in different processes are the same<NewLine>    set_random_seeds(random_seed=random_seed)<NewLine><NewLine>    # Encapsulate the model on the GPU assigned to the current process<NewLine>    model = torchvision.models.resnet18(pretrained=False)<NewLine><NewLine>    print(""Local Rank: {} | Location: {}"".format(local_rank, 1))<NewLine><NewLine>    if local_rank == 0:<NewLine>        torch.distributed.barrier()<NewLine><NewLine>    print(""Local Rank: {} | Location: {}"".format(local_rank, 2))<NewLine><NewLine>    device = torch.device(""cuda:{}"".format(local_rank))<NewLine>    model = model.to(device)<NewLine>    print(""Local Rank: {} | Location: {}"".format(local_rank, 2.1))<NewLine>    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)<NewLine>    print(""Local Rank: {} | Location: {}"".format(local_rank, 2.2))<NewLine><NewLine>    # We only save the model who uses device ""cuda:0""<NewLine>    # To resume, the device for the saved model would also be ""cuda:0""<NewLine>    if resume == True:<NewLine>        map_location = {""cuda:0"": ""cuda:{}"".format(local_rank)}<NewLine>        ddp_model.load_state_dict(torch.load(model_filepath, map_location=map_location))<NewLine><NewLine>    <NewLine>    # Restricts data loading to a subset of the dataset exclusive to the current process<NewLine>    train_sampler = DistributedSampler(dataset=train_set)<NewLine>    <NewLine><NewLine>    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, sampler=train_sampler, num_workers=8)<NewLine>    # Test loader does not have to follow distributed sampling strategy<NewLine>    test_loader = DataLoader(dataset=test_set, batch_size=128, shuffle=False, num_workers=8)<NewLine>    print(""Local Rank: {} | Location: {}"".format(local_rank, 2.3))<NewLine><NewLine>    criterion = nn.CrossEntropyLoss()<NewLine>    optimizer = optim.SGD(ddp_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)<NewLine><NewLine>    # Loop over the dataset multiple times<NewLine>    for epoch in range(num_epochs):<NewLine><NewLine>        print(""Local Rank: {}, Epoch: {}, Training ..."".format(local_rank, epoch))<NewLine><NewLine>        print(""Local Rank: {} | Location: {}"".format(local_rank, 3))<NewLine>        <NewLine>        # Save and evaluate model routinely<NewLine>        if epoch % 10 == 0:<NewLine>            if local_rank == 0:<NewLine>                accuracy = evaluate(model=ddp_model, device=device, test_loader=test_loader)<NewLine>                torch.save(ddp_model.state_dict(), model_filepath)<NewLine>                print(""-"" * 75)<NewLine>                print(""Epoch: {}, Accuracy: {}"".format(epoch, accuracy))<NewLine>                print(""-"" * 75)<NewLine><NewLine>        print(""Local Rank: {} | Location: {}"".format(local_rank, 4))<NewLine><NewLine>        ddp_model.train()<NewLine><NewLine>        for data in train_loader:<NewLine>            inputs, labels = data[0].to(device), data[1].to(device)<NewLine>            optimizer.zero_grad()<NewLine>            outputs = ddp_model(inputs)<NewLine>            loss = criterion(outputs, labels)<NewLine>            loss.backward()<NewLine>            optimizer.step()<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    <NewLine>    main()<NewLine></code></pre><NewLine><p>For the node 0:</p><NewLine><pre><code class=""lang-auto"">Extracting data/cifar-10-python.tar.gz to data<NewLine>Files already downloaded and verified<NewLine>Local Rank: 0 | Location: 1<NewLine>Local Rank: 0 | Location: 2<NewLine>Local Rank: 2 | Location: 0<NewLine>Local Rank: 3 | Location: 0<NewLine>Local Rank: 1 | Location: 0<NewLine>Local Rank: 0 | Location: 2.1<NewLine>Local Rank: 0 | Location: 2.2<NewLine>Local Rank: 0 | Location: 2.3<NewLine>Local Rank: 0, Epoch: 0, Training ...<NewLine>Local Rank: 0 | Location: 3<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Local Rank: 2 | Location: 1<NewLine>Local Rank: 2 | Location: 2<NewLine>Local Rank: 1 | Location: 1<NewLine>Local Rank: 1 | Location: 2<NewLine>Local Rank: 3 | Location: 1<NewLine>Local Rank: 3 | Location: 2<NewLine>Local Rank: 2 | Location: 2.1<NewLine>Local Rank: 1 | Location: 2.1<NewLine>Local Rank: 3 | Location: 2.1<NewLine>Local Rank: 2 | Location: 2.2<NewLine>Local Rank: 2 | Location: 2.3<NewLine>Local Rank: 1 | Location: 2.2<NewLine>Local Rank: 1 | Location: 2.3<NewLine>Local Rank: 2, Epoch: 0, Training ...<NewLine>Local Rank: 2 | Location: 3<NewLine>Local Rank: 2 | Location: 4<NewLine>Local Rank: 1, Epoch: 0, Training ...<NewLine>Local Rank: 1 | Location: 3<NewLine>Local Rank: 1 | Location: 4<NewLine>Local Rank: 3 | Location: 2.2<NewLine>Local Rank: 3 | Location: 2.3<NewLine>Local Rank: 3, Epoch: 0, Training ...<NewLine>Local Rank: 3 | Location: 3<NewLine>Local Rank: 3 | Location: 4<NewLine></code></pre><NewLine><p>For the node 1:</p><NewLine><pre><code class=""lang-auto"">Extracting data/cifar-10-python.tar.gz to data<NewLine>Files already downloaded and verified<NewLine>Local Rank: 0 | Location: 1<NewLine>Local Rank: 0 | Location: 2<NewLine>Local Rank: 2 | Location: 0<NewLine>Local Rank: 3 | Location: 0<NewLine>Local Rank: 1 | Location: 0<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Files already downloaded and verified<NewLine>Local Rank: 0 | Location: 2.1<NewLine>Local Rank: 2 | Location: 1<NewLine>Local Rank: 2 | Location: 2<NewLine>Local Rank: 1 | Location: 1<NewLine>Local Rank: 1 | Location: 2<NewLine>Local Rank: 3 | Location: 1<NewLine>Local Rank: 3 | Location: 2<NewLine>Local Rank: 2 | Location: 2.1<NewLine>Local Rank: 1 | Location: 2.1<NewLine>Local Rank: 3 | Location: 2.1<NewLine></code></pre><NewLine><p>So the second node got halted in</p><NewLine><pre><code class=""lang-auto"">ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p>Since you are running 1.5.1, I just dive into 1.5.1 code and can verify that the newest <code>DistributedDataParallel</code> do have a <code>_sync_params</code> class method which will broadcast all parameters and buffers, then set local params with inplace operation <code>_set</code>:</p><NewLine><pre><code class=""lang-auto"">def _sync_params(self):<NewLine>        with torch.no_grad():<NewLine>            # only do intra-node parameters sync for replicated single-device<NewLine>            # CUDA modules<NewLine>            if self.device_ids and len(self.device_ids) &gt; 1:<NewLine>                # intra-node parameter sync<NewLine>                result = torch.cuda.comm.broadcast_coalesced(<NewLine>                    self.modules_params[0],<NewLine>                    self.device_ids,<NewLine>                    self.broadcast_bucket_size)<NewLine>                for tensors, module_params in zip(result[1:],<NewLine>                                                  self.modules_params[1:]):<NewLine>                    for tensor, param in zip(tensors, module_params):<NewLine>                        param.set_(tensor)<NewLine>                        # Assume we have just run the optimizer and zeroed the<NewLine>                        # grads of the parameters on the root model. We need<NewLine>                        # to zero the grads on all model replicas as well.<NewLine>                        # This snippet is copied from torch.optim.Optimizer.<NewLine>                        if param.grad is not None:<NewLine>                            param.grad.detach_()<NewLine>                            param.grad.zero_()<NewLine></code></pre><NewLine><p>And _sync_params will be invoked when you perform a forward operation, if syncing is enabled:</p><NewLine><pre><code class=""lang-auto"">def forward(self, *inputs, **kwargs):<NewLine>        if self.require_forward_param_sync:<NewLine>            self._sync_params()<NewLine></code></pre><NewLine><p>so load_state_dict() should work, theoretically, because newly loaded params will be broadcasted to other processes.<br/><NewLine>Sorry about my outdated knowledge above</p><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think your code is correct, there really isn’t any visible issue with:</p><NewLine><pre><code class=""lang-auto"">    model = model.to(device)<NewLine>    print(""Local Rank: {} | Location: {}"".format(local_rank, 2.1))<NewLine>    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)<NewLine></code></pre><NewLine><p>My knowledge is not enough to explain this behavior, some possible debug solutions:</p><NewLine><ol><NewLine><li>will “gloo” halt?</li><NewLine><li>insert some more print tracer into pytorch source code</li><NewLine></ol><NewLine><p>It most likely would be a problem of nccl becasue DDP basically does these things in initialization:</p><NewLine><ol><NewLine><li><NewLine><p>call dist._broadcast_coleased to broadcast parameters to all groups</p><NewLine><p>dist._broadcast_coleased is defined in <code>torch/csrc/distributed/c10d/comm.cpp</code>,<br/><NewLine>however, since it is a private function, there is no indication about whether it is blocking etc, I only know that it is invoked by all processes.</p><NewLine></li><NewLine><li><NewLine><p>call _ddp_init_helper, which basically only do some local operations like:</p><NewLine><pre><code class=""lang-auto"">Initialization helper function that does the following:<NewLine><NewLine>     (1) replicating the module from device[0] to the other devices<NewLine>     (2) bucketing the parameters for reductions<NewLine>     (3) resetting the bucketing states<NewLine>     (4) registering the grad hooks<NewLine>     (5) passing a handle of DDP to SyncBatchNorm Layer<NewLine></code></pre><NewLine></li><NewLine></ol><NewLine><p>You can check nccl installation with, but this might not help you much if the “gloo” backend also halts:<br/><NewLine><aside class=""quote"" data-post=""5"" data-topic=""83667""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/kiuk_chung/40/22504_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/unhandled-system-error-when-training-with-multi-nodes/83667/5"">'unhandled system error' when training with multi nodes</a> <a class=""badge-wrapper bullet"" href=""/c/distributed/12""><span class=""badge-category-bg"" style=""background-color: #0088CC;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""">distributed</span></a><NewLine></div><NewLine><blockquote><NewLine>    Here’s one way to see if nccl is installed on the node: <NewLine>locate nccl| grep ""libnccl.so"" | tail -n1 | sed -r 's/^.*\.so\.//'<NewLine>  </blockquote><NewLine></aside><NewLine></p><NewLine><p><img alt="":slightly_frowning_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slightly_frowning_face.png?v=9"" title="":slightly_frowning_face:""/> Sorry that I cannot help you more with this problem.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/leimao; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/leimao; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/leimao; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/leimao; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/leimao; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/leimao; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/leimao; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/leimao; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/leimao; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/leimao; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/leimao; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: July 19, 2020,  1:44am; <NewLine> REPLY_DATE 2: July 19, 2020,  6:05am; <NewLine> REPLY_DATE 3: July 19, 2020,  8:14am; <NewLine> REPLY_DATE 4: July 19, 2020,  5:58pm; <NewLine> REPLY_DATE 5: July 19, 2020,  6:01pm; <NewLine> REPLY_DATE 6: July 19, 2020,  8:09pm; <NewLine> REPLY_DATE 7: July 19, 2020,  8:16pm; <NewLine> REPLY_DATE 8: July 19, 2020,  8:23pm; <NewLine> REPLY_DATE 9: July 19, 2020,  8:25pm; <NewLine> REPLY_DATE 10: July 20, 2020,  2:25am; <NewLine> REPLY_DATE 11: July 20, 2020,  6:53am; <NewLine> REPLY_DATE 12: July 20, 2020,  6:55am; <NewLine> REPLY_DATE 13: July 20, 2020,  7:00am; <NewLine> REPLY_DATE 14: July 20, 2020,  7:08am; <NewLine> REPLY_DATE 15: July 20, 2020,  7:45am; <NewLine> REPLY_DATE 16: July 20, 2020,  4:00pm; <NewLine> REPLY_DATE 17: July 20, 2020,  4:21pm; <NewLine> REPLY_DATE 18: July 20, 2020,  5:11pm; <NewLine> REPLY_DATE 19: July 20, 2020,  5:55pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> REPLY 18 LIKES: ; <NewLine> REPLY 19 LIKES: ; <NewLine> 
90081,Pytorch distributed concurrent queue/buffer,2020-07-22T03:10:38.967Z,0,71,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,</p><NewLine><p>I am wondering whether there is anyway for pytorch distributed to build one concurrent queue(or buffer) between parameter server and workers.</p><NewLine><p>So that, every worker can work as a producer to send the msg to the concurrent queue.</p><NewLine><p>And the parameter server can work as consumer to consume msg from concurrent queue.</p><NewLine><p>Besides, parameter server can detect the length of the concurrent queue.</p><NewLine><p>Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/ryuxin,,ryuxin,"July 22, 2020,  3:17am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ryuxin"">@ryuxin</a>, can this be implemented as a wrapper on top of the <a href=""https://pytorch.org/docs/master/rpc.html"" rel=""nofollow noopener"">RPC API</a>? For example, can you implement the queuing logic as an RPC target function? Some related tutorials:</p><NewLine><ol><NewLine><li><a href=""https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html</a></li><NewLine><li><a href=""https://github.com/pytorch/tutorials/blob/release/1.6/intermediate_source/rpc_async_execution.rst"" rel=""nofollow noopener"">https://github.com/pytorch/tutorials/blob/release/1.6/intermediate_source/rpc_async_execution.rst</a></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the hint!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ryuxin; <NewLine> ,"REPLY_DATE 1: July 22, 2020,  3:55pm; <NewLine> REPLY_DATE 2: July 22, 2020,  4:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
35702,Multiprocessing failed using single GPU,2019-01-27T10:31:26.333Z,0,2129,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am new to the machine learning community. For some reasons, I try to parallelly do inference using multi-core CPU and single GPU, however I just got following runtime errors.</p><NewLine><pre><code class=""lang-auto"">THCudaCheck FAIL file=c:\a\w\1\s\tmp_conda_3.6_091443\conda\conda-bld\pytorch_1544087948354\work\torch\csrc\generic\StorageSharing.cpp line=232 error=71 : operation not supported<NewLine>File ""C:\Users\Anaconda3\lib\site-packages\torch\multiprocessing\reductions.py"", line 213, in reduce_tensor<NewLine>    (device, handle, storage_size_bytes, storage_offset_bytes) = storage._share_cuda_()<NewLine>RuntimeError: cuda runtime error (71) : operation not supported at c:\a\w\1\s\tmp_conda_3.6_091443\conda\conda-bld\pytorch_1544087948354\work\torch\csrc\generic\StorageSharing.cpp:232<NewLine></code></pre><NewLine><p>The following is a simplified example which can reproduce the errors.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch import nn<NewLine><NewLine># model used to do inference<NewLine>class Model(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Model, self).__init__()<NewLine>        self.fc1 = nn.Linear(100,1)<NewLine>        <NewLine>    def forward(self,x):<NewLine>        return self.fc1(x)<NewLine>    <NewLine># class running inference<NewLine>class A(object):<NewLine>    def __init__(self):<NewLine>        pass<NewLine><NewLine>    def do_something(self, model):<NewLine>        # do something<NewLine>        x = torch.randn(100).view(-1)<NewLine>        print(model.forward(x))<NewLine>    <NewLine>    def run(self):<NewLine>        mp = torch.multiprocessing.get_context('spawn')<NewLine>        processes = []<NewLine><NewLine>        for i in range(2):<NewLine>            p = mp.Process(target=self.do_something, args=(Model().cuda(),))<NewLine>            processes.append(p)<NewLine><NewLine>        for p in processes:<NewLine>            p.start()<NewLine><NewLine>if __name__ == '__main__':<NewLine>    a = A()<NewLine>    a.run()<NewLine></code></pre><NewLine><p>It would be greatly appreciated if anyone can help solve this problem. By the way, my PC runs on Windows 10 with one GTX 1070 GPU.</p><NewLine></div>",https://discuss.pytorch.org/u/Lorentz.B,,Lorentz.B,"January 27, 2019, 10:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""onebox"" href=""https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>In your example you could choose to instantiate your model in the sub process. Then you won’t need to share CUDA tensors between the parent and the child process.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,Have you solve it? I have the same problem.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/peterjc123; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/xxxxxi-gg; <NewLine> ,"REPLY_DATE 1: January 27, 2019,  3:15pm; <NewLine> REPLY_DATE 2: January 27, 2019,  5:56pm; <NewLine> REPLY_DATE 3: July 21, 2020, 12:55am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
89841,Running on specific GPU device,2020-07-20T06:48:48.598Z,9,328,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to specify specify which single GPU to run code on within Python code, by setting the GPU index visible to PyTorch. Here’s what I’ve tried:</p><NewLine><pre><code class=""lang-auto"">for i in range(8): #8 gpus<NewLine>    os.environ[""CUDA_AVAILABLE_DEVICES""] = str(i)<NewLine>    print(torch.cuda.device_count())<NewLine>    # this line always outputs 8 (all 8 devices) instead of 1...<NewLine>    ...<NewLine></code></pre><NewLine><p>I’m using PyTorch 1.0.0. How do I specify which GPU machine (by index or otherwise) to run code on (without using <code>.to(device)</code>) within Python code?</p><NewLine></div>",https://discuss.pytorch.org/u/CCL,,CCL,"July 20, 2020,  2:10pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ccl"">@CCL</a>, you will need to set the <code>CUDA_AVAILABLE_DEVICES</code> env var before launching the process. Sth like:</p><NewLine><pre><code class=""lang-auto"">$ CUDA_AVAILABLE_DEVICES=0 python main.py<NewLine></code></pre><NewLine><p>If you just want to set the default device, you can use <a href=""https://pytorch.org/docs/stable/cuda.html#torch.cuda.set_device"" rel=""nofollow noopener""><code>set_device</code></a></p><NewLine><h3>Update</h3><NewLine><p>Please ignore the code above. I miss-read the variable name. It should be <code>CUDA_VISIBLE_DEVICES</code></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi thanks for the reply, is it possible to set it with python instead of in shell?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Hi thanks for the reply, is it possible to set it with python instead of in shell?</p><NewLine></blockquote><NewLine><p>Yes, but you need to make sure it is set before initializing the CUDA context. See the code below:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import os<NewLine>os.environ[""CUDA_VISIBLE_DEVICES""] = ""1"" <NewLine>torch.cuda.device_count()  # print 1<NewLine>os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1""  <NewLine>torch.cuda.device_count()  # still print 1<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you mean that once it’s set, it cannot be changed? For my case, I’m hoping to make GPU 0 visible on the 1st iteration, GPU 1 visible on the 2nd, etc till GPU 7 and iter 8. Is there a way to do this from Python? Thanks a lot!</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Do you mean that once it’s set, it cannot be changed?</p><NewLine></blockquote><NewLine><p>I believe so.</p><NewLine><blockquote><NewLine><p>For my case, I’m hoping to make GPU 0 visible on the 1st iteration, GPU 1 visible on the 2nd, etc till GPU 7 and iter 8. Is there a way to do this from Python?</p><NewLine></blockquote><NewLine><p>Can this be done by explicitly passing <code>torch.cuda.device(i)</code> to tensors/modules or use <code>torch.cuda.set_device(i)</code>? Is there any reason that you would like to change the visible devices?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to do some parallelism but can’t figure out how to initialise processes with different ranks, each process on 1 different GPU. I am modifying some distributed computing code, but instead of having numerous nodes, I only have 1 8 GPU machine to work with.</p><NewLine><p>(Pls bear with me, I’m a beginner in distributed computing!) So far I’ve worked out that the line <code>dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=1, rank=args.rank)</code> intialises the same process on all 8 GPUs, but this causes problems later in my code, where I need to get the specific GPU machine index. I tried to do this with <code>torch.cuda.current_device()</code> but it also returns 0 despite <code>nvidia-smi</code> showing that all 8 GPUs have been used.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""89841"" data-username=""CCL""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/c/dbc845/40.png"" width=""20""/> CCL:</div><NewLine><blockquote><NewLine><p>So far I’ve worked out that the line <code>dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=1, rank=args.rank)</code> intialises the same process on all 8 GPUs</p><NewLine></blockquote><NewLine></aside><NewLine><p>The <code>init_process_group</code> API only sets up the process where this function is invoked. And, as the <code>world_size</code> is set to 1, It only expects one process in the distributed training gang. If you would like to use multiple processes, please see this <a href=""https://pytorch.org/docs/master/notes/ddp.html#example"" rel=""nofollow noopener"">example</a>.</p><NewLine><blockquote><NewLine><p>but this causes problems later in my code, where I need to get the specific GPU machine index.</p><NewLine></blockquote><NewLine><p>To get machine index, will it work if you use <code>args.rank</code>?</p><NewLine><blockquote><NewLine><p>I tried to do this with  <code>torch.cuda.current_device()</code>  but it also returns 0 despite  <code>nvidia-smi</code>  showing that all 8 GPUs have been used.</p><NewLine></blockquote><NewLine><p>The <code>torch.cuda.current_device()</code>  returns the current device. By default, it is the first GPU, which is indeed indexed by 0.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just tried using args.rank, but it seems like they all return rank 0. I’m really quite lost on how to do parallelism.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>How did you launch those 8 processes? Did you launch it using similar code in the <a href=""https://pytorch.org/docs/master/notes/ddp.html#example"" rel=""nofollow noopener"">example</a> or the <a href=""https://github.com/pytorch/examples/blob/master/distributed/ddp/README.md"" rel=""nofollow noopener"">launching script</a>?</p><NewLine><p>And how did you set <code>args.rank</code>? I presume it’s command line args + argparse?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>It will be helpful to have a self-contained min repro code. So that we can help debug.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/CCL; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/CCL; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/CCL; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/CCL; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: July 20, 2020,  2:52pm; <NewLine> REPLY_DATE 2: July 20, 2020,  2:39pm; <NewLine> REPLY_DATE 3: July 20, 2020,  2:56pm; <NewLine> REPLY_DATE 4: July 20, 2020,  3:00pm; <NewLine> REPLY_DATE 5: July 20, 2020,  3:05pm; <NewLine> REPLY_DATE 6: July 20, 2020,  3:15pm; <NewLine> REPLY_DATE 7: July 20, 2020,  7:17pm; <NewLine> REPLY_DATE 8: July 20, 2020,  7:17pm; <NewLine> REPLY_DATE 9: July 20, 2020,  7:17pm; <NewLine> REPLY_DATE 10: July 20, 2020,  3:44pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> 
89786,Pytorch tensor division hangs in multiprocess linux,2020-07-19T15:31:32.608Z,0,58,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m experiencing a hanging in process. it happens at the following codes</p><NewLine><p>samping_priorities = (self.priority_memory[0:upper_bound] / self.priority_memory[0:upper_bound].sum()).cpu().detach().numpy()</p><NewLine><p>batch_idx = T.LongTensor(np.random.choice(upper_bound, batch_size,p=samping_priorities[0:upper_bound].cpu().detach().numpy()))</p><NewLine><p>samping_priorities is a 2000000*1 tensor.<br/><NewLine>upper_bound is the range I’m interested in and upper_bound+=1 through iterations</p><NewLine><p>at the beginning everything is okay. then I noticed when upper_bound exceeds 32768, the process hangs between the first line and second line</p><NewLine><p>It works fine on my windows workstation but hangs in linux cluster. What could be the cause and how can i fix it</p><NewLine></div>",https://discuss.pytorch.org/u/Lewis_Liu,(Lewis Liu),Lewis_Liu,"July 19, 2020,  3:32pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""89786"" data-username=""Lewis_Liu""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/lewis_liu/40/12953_2.png"" width=""20""/> Lewis_Liu:</div><NewLine><blockquote><NewLine><p>at the beginning everything is okay. then I noticed when upper_bound exceeds 32768, the process hangs between the first line and second line</p><NewLine></blockquote><NewLine></aside><NewLine><p>This sounds like a <code>int16_t</code> overflow bug (or that might hit some different branch). Could you please create an issue in <a href=""https://github.com/pytorch/pytorch/issues"" rel=""nofollow noopener"">pytorch repo</a>? Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: July 19, 2020,  7:56pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
89655,MPI does not gracefully terminate,2020-07-17T23:02:18.899Z,3,114,"<div class=""post"" itemprop=""articleBody""><NewLine><p>So I have been following the code and tutorial on using pytorch to do distributed machine learning <a href=""https://pytorch.org/tutorials/intermediate/dist_tuto.html"" rel=""nofollow noopener"">here</a>. I am able to run the code  (and it completes all the tasks) but my program does not terminate and I need to manually kill it using ctrl+C. The exact code is <a href=""https://github.com/seba-1511/dist_tuto.pth/blob/gh-pages/train_dist.py"" rel=""nofollow noopener"">here</a></p><NewLine><p>Right now after completing the task, it hangs after displaying the following warning messages</p><NewLine><blockquote><NewLine><pre><code>/anaconda3/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:406: UserWarning: For MPI backend, world_size (0) and rank (0) are ignored since they are assigned by the MPI runtime.<NewLine>  ""MPI runtime."".format(world_size, rank))<NewLine>train_dist.py:72: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.<NewLine>  return F.log_softmax(x)<NewLine>/anaconda3/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:125: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead<NewLine>  warnings.warn(""torch.distributed.reduce_op is deprecated, please use ""<NewLine></code></pre><NewLine></blockquote><NewLine><p>I run the code using basic command</p><NewLine><blockquote><NewLine><pre><code> /anaconda3/bin/mpirun -np 3 --host node-0,node-1,node-2 python train_dist.py<NewLine></code></pre><NewLine></blockquote><NewLine><p>Do I need to add something in the code to exit gracefully?</p><NewLine></div>",https://discuss.pytorch.org/u/Archie_Nidhi,(Archie Nidhi),Archie_Nidhi,"July 18, 2020,  3:27am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""89655"" data-username=""Archie_Nidhi""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/archie_nidhi/40/26847_2.png"" width=""20""/> Archie_Nidhi:</div><NewLine><blockquote><NewLine><p>Do I need to add something in the code to exit gracefully?</p><NewLine></blockquote><NewLine></aside><NewLine><p>This shouldn’t be necessary. Which MPI implementation are you using?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried to run <a href=""https://github.com/seba-1511/dist_tuto.pth/blob/fb8293ae710279d580dc273da3702a4b21b1cf49/train_dist.py"" rel=""nofollow noopener"">the code</a> with gloo backend to check if it is a MPI-only problem, but initially hits the following error:</p><NewLine><pre><code class=""lang-auto"">...<NewLine>    self._target(*self._args, **self._kwargs)<NewLine>  File ""test.py"", line 132, in init_processes<NewLine>    fn(rank, size)<NewLine>  File ""test.py"", line 103, in run<NewLine>    train_set, bsz = partition_dataset()<NewLine>  ...<NewLine>ValueError: batch_size should be a positive integer value, but got batch_size=64.0<NewLine></code></pre><NewLine><p>After fixing that, hits the error below:</p><NewLine><pre><code class=""lang-auto"">  File ""test.py"", line 132, in init_processes<NewLine>    fn(rank, size)<NewLine>  File ""test.py"", line 118, in run<NewLine>    epoch_loss += loss.data[0]<NewLine>IndexError: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item&lt;T&gt;()` in C++ to convert a 0-dim tensor to a number<NewLine></code></pre><NewLine><p>Can we have a min example that can reproduce the hang issue? Thanks!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I figured the issue. The mpi part was stuck because one of the processes were waiting for the other process to send some data.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah, I resolved that issues by converting batch_size to integer and loss.data[0] to loss.data.item()</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Archie_Nidhi; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Archie_Nidhi; <NewLine> ,"REPLY_DATE 1: July 18, 2020,  4:01pm; <NewLine> REPLY_DATE 2: July 18, 2020,  4:07pm; <NewLine> REPLY_DATE 3: July 19, 2020,  2:59am; <NewLine> REPLY_DATE 4: July 19, 2020,  3:00am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> 
89631,How to package project with pytorch CUDA/CPP extension through setuptools,2020-07-17T17:01:16.672Z,1,72,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello. I want to package my project and upload to pypi.</p><NewLine><p>My project contains some CUDA/CPP files, and I complie it as <a href=""https://pytorch.org/tutorials/advanced/cpp_extension.html"" rel=""nofollow noopener"">the tutorial</a>. It works well.</p><NewLine><p>In my project, I run setup.py for <code>roi_align</code> and then run another setup.py for <code>rod_align</code>. The files are listed as follows. Note that I used PyTorch to confirm whether GPU is available, then choose to build CPP extension or CUDA extension.</p><NewLine><pre><code class=""lang-auto"">├── autocrop<NewLine>│   ├── __init__.py<NewLine>│   ├── cropper.py<NewLine>│   ├── model<NewLine>│   │   ├── __init__.py<NewLine>│   │   ├── cropping_model.py<NewLine>│   │   ├── moblienetv2.py<NewLine>│   │   ├── rod_align<NewLine>│   │   │   ├── __init__.py<NewLine>│   │   │   ├── build<NewLine>│   │   │   ├── functions<NewLine>│   │   │   │   ├── __init__.py<NewLine>│   │   │   │   └── rod_align.py<NewLine>│   │   │   ├── modules<NewLine>│   │   │   │   ├── __init__.py<NewLine>│   │   │   │   └── rod_align.py<NewLine>│   │   │   ├── setup.py # for rod_align<NewLine>│   │   │   └── src<NewLine>│   │   │       ├── rod_align.cpp<NewLine>│   │   │       ├── rod_align.h<NewLine>│   │   │       ├── rod_align_cuda.cpp<NewLine>│   │   │       ├── rod_align_cuda.h<NewLine>│   │   │       ├── rod_align_kernel.cu<NewLine>│   │   │       └── rod_align_kernel.h<NewLine>│   │   ├── roi_align<NewLine>│   │   │   ├── __init__.py<NewLine>│   │   │   ├── functions<NewLine>│   │   │   │   ├── __init__.py<NewLine>│   │   │   │   └── roi_align.py<NewLine>│   │   │   ├── modules<NewLine>│   │   │   │   ├── __init__.py<NewLine>│   │   │   │   └── roi_align.py<NewLine>│   │   │   ├── setup.py # for roi_align<NewLine>│   │   │   └── src<NewLine>│   │   │       ├── roi_align.cpp<NewLine>│   │   │       ├── roi_align.h<NewLine>│   │   │       ├── roi_align_cuda.cpp<NewLine>│   │   │       ├── roi_align_cuda.h<NewLine>│   │   │       ├── roi_align_kernel.cu<NewLine>│   │   │       └── roi_align_kernel.h<NewLine>│   │   └── shufflenetv2.py<NewLine>│   └── utils.py<NewLine>├── demo.py<NewLine>├── imgs<NewLine>│   └── demo.jpg<NewLine>└── setup.py # This is the file I want to write, run once to package all the project<NewLine></code></pre><NewLine><p>Now I want to package my whole project. I am confused about how to write the setup file. (just run once).</p><NewLine><p>Can I copy the code <code>setup(xxxx)</code> from roi and rod setup.py to the final setup.py and write another <code>setup(xxxx)</code> after them, just like:</p><NewLine><pre><code class=""lang-python"">from setuptools import setup<NewLine>setup(roi)<NewLine>setup(rod)<NewLine>setuptools.setup(<NewLine>    name=""autocrop"",<NewLine>    python_requires='&gt;=3.6',<NewLine>    install_requires=[<NewLine>        ""torch&gt;=1.1"",<NewLine>        ""torchvision&gt;=0.3.0"",<NewLine>        ""numpy"",<NewLine>    ]<NewLine>xxxxxxx<NewLine>)<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Howie,(Howie),Howie,"July 17, 2020,  5:06pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>, do you know who can answer packaging questions?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/seemethere"">@seemethere</a> might be the right person.<br/><NewLine>Also, Nikita Shulda, but I cannot find his user name (and unsure, if he’s registered here).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 18, 2020,  3:52pm; <NewLine> REPLY_DATE 2: July 19, 2020,  1:43am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
89644,Pytorch MP manager dict hang in linux,2020-07-17T20:17:41.060Z,3,100,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to share the weights of networks between processes using multiprocessing manager.dict<br/><NewLine>The code is as follows</p><NewLine><pre><code>    for name in critic_state_dict:<NewLine>        self.shared_networks.critic[name] = T.tensor(critic_state_dict[name].clone().cpu().detach().numpy())<NewLine></code></pre><NewLine><p>This works fine in windows. But when I use a cluster, it hangs in the middle of the for loop</p><NewLine><p>How do I fix this? Or if I want to periodically share the weights among processes, how to do it properly?<br/><NewLine>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/Lewis_Liu,(Lewis Liu),Lewis_Liu,"July 17, 2020,  8:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/lewis_liu"">@Lewis_Liu</a>,</p><NewLine><p>Did you use fork or spawn?</p><NewLine><blockquote><NewLine><p>Or if I want to periodically share the weights among processes, how to do it properly?</p><NewLine></blockquote><NewLine><p>One solution is to create a multiprocessing <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html#reuse-buffers-passed-through-a-queue"" rel=""nofollow noopener"">queue</a>, and pass that queue to child processes. Then, in the loop, use that queue to pass shared tensors. The test below can serve as an example:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/fc8bca094cc23a2394214c5cdbc8392a3d279e8c/test/distributed/test_c10d_spawn.py#L166-L184"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/fc8bca094cc23a2394214c5cdbc8392a3d279e8c/test/distributed/test_c10d_spawn.py#L166-L184"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/fc8bca094cc23a2394214c5cdbc8392a3d279e8c/test/distributed/test_c10d_spawn.py#L166-L184</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""166"" style=""counter-reset: li-counter 165 ;""><NewLine><li>@classmethod</li><NewLine><li>def _test_allgather_process(</li><NewLine><li>        cls, rank, filename, shared_tensors, world_size, init_pg, c2p, p2c):</li><NewLine><li>    pg = init_pg(rank, filename, world_size)</li><NewLine><li>    xs = [shared_tensors[rank]]</li><NewLine><li>    ys = [[torch.zeros_like(xs[0]) for i in range(world_size)]]</li><NewLine><li>    pg.allgather(ys, xs).wait()</li><NewLine><li>    for i in range(world_size):</li><NewLine><li>        c2p.put((rank, torch.ones(2, 2) * i, ys[0][i].to(""cpu"")))</li><NewLine><li><NewLine></li><li>    p2c.get()</li><NewLine><li><NewLine></li><li>@unittest.skipIf(not TEST_MULTIGPU, ""At least 2 CUDA GPUS needed"")</li><NewLine><li>def test_shared_allgather_gloo(self):</li><NewLine><li>    self._test_multiprocess(</li><NewLine><li>        ProcessGroupShareTensorTest._test_allgather_process,</li><NewLine><li>        [torch.ones(2, 2).to(i) * i for i in range(self.world_size)],</li><NewLine><li>        ProcessGroupShareTensorTest._init_pg_gloo,</li><NewLine><li>        self.world_size)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Li,</p><NewLine><p>I switched to using Queue. But I cannot avoid firstly getting the net tensors from state_dict right?</p><NewLine><p>I believe it’s spawn on my windows workstation and it’s fork on the linux cluster if i’m correct</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""89644"" data-username=""Lewis_Liu""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/lewis_liu/40/12953_2.png"" width=""20""/> Lewis_Liu:</div><NewLine><blockquote><NewLine><p>I switched to using Queue. But I cannot avoid firstly getting the net tensors from state_dict right?</p><NewLine></blockquote><NewLine></aside><NewLine><p>I don’t have the full context here. Can you let the processing holding the state_dict be the writer to the queue?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yep.</p><NewLine><p>The network is trained and updated for a step. After this, the process has only one sole task that is to write the state_dict into the queue. Other processes doesn’t have direct access to the network except through the queue</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Lewis_Liu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Lewis_Liu; <NewLine> ,"REPLY_DATE 1: July 18, 2020,  3:49pm; <NewLine> REPLY_DATE 2: July 18, 2020,  4:06pm; <NewLine> REPLY_DATE 3: July 18, 2020,  4:09pm; <NewLine> REPLY_DATE 4: July 18, 2020,  4:23pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
89558,Error training distributed Model,2020-07-17T08:27:41.771Z,0,54,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am working to distribute the layers a neural network like alexNet in three devices (Edge, Fog, and Cloud), sending the results of the inferences to the other device. I’m currently trying to train it as a single model and then break it down into sub-models.</p><NewLine><p>The neural network has the following model:</p><NewLine><pre><code class=""lang-auto""># AlexNet<NewLine># EDGE MODEL<NewLine><NewLine>edge_layers1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=2),nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2))<NewLine>edge_layers2 = nn.Sequential(nn.Linear(8, 10), nn.ReLU(inplace=True))<NewLine><NewLine><NewLine># FOG MODEL<NewLine><NewLine>fog_layers1 = nn.Sequential(nn.Conv2d(64, 192, kernel_size=3, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2))<NewLine>fog_layers2 = nn.Sequential(nn.Linear(4, 10), nn.ReLU(inplace=True))<NewLine><NewLine># CLOUD MODEL <NewLine><NewLine>cloud_layers1 = nn.Sequential(nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True), <NewLine>                              nn.Conv2d(512, 4096, kernel_size=3, padding=1), nn.ReLU(inplace=True))<NewLine>cloud_layers2 = nn.Sequential(nn.Linear(4, 1024), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(1024, 4096), nn.ReLU(inplace=True), nn.Linear(4096, 10))<NewLine><NewLine><NewLine>class AlexNet_Distributed(nn.Module):<NewLine><NewLine>    def __init__(self, num_classes=10):<NewLine>        super(AlexNet_Distributed, self).__init__()<NewLine>        self.layers1 = edge_layers1<NewLine>        self.layers2 = edge_layers2<NewLine>        self.layers3 = fog_layers1<NewLine>        self.layers4 = fog_layers2<NewLine>        self.layers5 = cloud_layers1<NewLine>        self.layers6 = cloud_layers2<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.layers1(x)<NewLine>        y = self.layers2(x)<NewLine>        <NewLine>        x = self.layers3(x)<NewLine>        z = self.layers4(x)<NewLine>        <NewLine>        x = self.layers5(x)<NewLine>        x = self.layers6(x)<NewLine>        return x<NewLine>    <NewLine>net = AlexNet_Distributed()<NewLine></code></pre><NewLine><p>After training it with the next implementation, I get this error. Does anyone know why?</p><NewLine><pre><code class=""lang-auto"">criterion = nn.CrossEntropyLoss()<NewLine>optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)<NewLine>for epoch in range(2):  # loop over the dataset multiple times<NewLine><NewLine>    running_loss = 0.0<NewLine>    for i, data in enumerate(trainloader, 0):<NewLine>        # get the inputs; data is a list of [inputs, labels]<NewLine>        inputs, labels = data<NewLine><NewLine>        # zero the parameter gradients<NewLine>        optimizer.zero_grad()<NewLine><NewLine>        # forward + backward + optimize<NewLine>        outputs = net(inputs)<NewLine>        print(outputs[3].size())<NewLine>        loss = criterion(outputs, labels)<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine><NewLine>        # print statistics<NewLine>        running_loss += loss.item()<NewLine>        if i % 2000 == 1999:    # print every 2000 mini-batches<NewLine>            print('[%d, %5d] loss: %.3f' %<NewLine>                  (epoch + 1, i + 1, running_loss / 2000))<NewLine>            running_loss = 0.0<NewLine><NewLine>print('Finished Training')<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">---&gt; 16         loss = criterion(outputs, labels)<NewLine>RuntimeError: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Fernando_Gallego,(Fernando Gallego),Fernando_Gallego,"July 17, 2020,  8:27am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Based on discussions in <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/only-batches-of-spatial-targets-supported-non-empty-3d-tensors-but-got-targets-of-size-1-1-256-256/49134"">Only batches of spatial targets supported (non-empty 3D tensors) but got targets of size: : [1, 1, 256, 256]</a>. It might be due to the size of <code>outputs</code> or <code>labels</code>. Could you please print out their sizes?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: July 18, 2020,  3:59pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
89366,Weight Constraining on DataParallel gives nan loss,2020-07-15T18:36:47.211Z,1,66,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am training a Conv network on CIFAR 10 with 2 GPUs. I am using <code>DataParallel()</code> to parallelize the model.</p><NewLine><p>The model is:</p><NewLine><pre><code class=""lang-python""> class CNNModel(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        <NewLine>        #Conv 1<NewLine>        self.cnn1 = nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3,stride=1,padding=1)<NewLine>        self.activation = nn.ELU()<NewLine>        <NewLine>        #maxpool1<NewLine>        self.maxpool1 = nn.MaxPool2d(kernel_size=2)<NewLine>        <NewLine>        #Conv 2<NewLine>        self.cnn2 = nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1,padding=1)<NewLine><NewLine>        #Conv 3<NewLine>        self.cnn3 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1,padding=1)<NewLine>        self.cnn4 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1)<NewLine>        self.cnn5 = nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=1)<NewLine>        self.cnn6 = nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=1)<NewLine><NewLine>        <NewLine>        #Maxpool 2<NewLine>        self.maxpool2 = nn.MaxPool2d(kernel_size=2)<NewLine>        <NewLine>        # 8 = (32/2)/2<NewLine>        self.fc1 = nn.Linear(128*8*8,10)<NewLine>        <NewLine>    def forward(self,x):<NewLine>        # x is of size (64,1,28,28)<NewLine>        # reshape to (64,784)<NewLine>        out = self.activation(self.cnn1(x))<NewLine>        out = self.maxpool1(out)<NewLine><NewLine>        out = self.activation(self.cnn2(out))<NewLine><NewLine>        out = self.activation(self.cnn3(out))<NewLine>        out = self.activation(self.cnn4(out))<NewLine>        out = self.activation(self.cnn5(out))<NewLine>        out = self.activation(self.cnn6(out))<NewLine>        <NewLine>        out = self.maxpool2(out)<NewLine>        out = out.view(out.shape[0],-1)<NewLine>        <NewLine>        out = self.fc1(out)<NewLine>        return out<NewLine></code></pre><NewLine><p>The weight constraints I am using are:</p><NewLine><pre><code class=""lang-python"">class weightConstraint(object):<NewLine>    def __init__(self):<NewLine>        pass<NewLine>    <NewLine>    def __call__(self,module):<NewLine>        if hasattr(module,'weight'):<NewLine>            print(""Entered"")<NewLine>            w=module.weight.data<NewLine>            w[torch.where(w&lt;0)] = 0<NewLine>            module.weight.data=w<NewLine></code></pre><NewLine><p>I tried using 2 methods to apply these constaints:<br/><NewLine><strong>Technique 1:</strong></p><NewLine><pre><code class=""lang-python"">model = CNNModel()<NewLine>constaint = weightConstraint()<NewLine>for key,val in model._modules.items():<NewLine>    if hasattr(val,'weight'):<NewLine>        print(key)<NewLine>        val.apply(constaint)<NewLine>model = nn.DataParallel(model).to(device)<NewLine></code></pre><NewLine><p><strong>Technique 2:</strong></p><NewLine><pre><code class=""lang-python"">model = CNNModel()<NewLine>model = nn.DataParallel(model).to(device)<NewLine><NewLine>constaint = weightConstraint()<NewLine>for key,val in model._modules['module']._modules.items():<NewLine>    if hasattr(val,'weight'):<NewLine>        print(key)<NewLine>        val.apply(constaint)<NewLine></code></pre><NewLine><p>Training in both cases gives me loss as <code>nan</code> (and accuracy as 0.10 which means random assignment) from the very first epoch</p><NewLine></div>",https://discuss.pytorch.org/u/varunchhangani,(Varun Chhangani),varunchhangani,"July 15, 2020,  6:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>it is hard to tell what it is going on here. How is your training on single GPU without DP wrapping?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems to be working well without the constraint</p><NewLine><p>Anyways, the code now seems to work sometimes, and otherwise for rest of the time</p><NewLine><p>I guess it might be poor model to start with</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Yanli_Zhao; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/varunchhangani; <NewLine> ,"REPLY_DATE 1: July 15, 2020,  9:59pm; <NewLine> REPLY_DATE 2: July 18, 2020,  3:16pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
89117,How to save nn.Sequential as a model?,2020-07-14T08:39:59.702Z,9,187,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am trying to decompose ResNet into three different devices, for this, I would need to be able to save their nn.sequential as a separate model. Training them all together but being able to load their models separately on each device.</p><NewLine><p>Do any of you know how to save nn.sequential  as a model? I 'm working as every morning</p><NewLine></div>",https://discuss.pytorch.org/u/Fernando_Gallego,(Fernando Gallego),Fernando_Gallego,"July 14, 2020,  8:39am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>if you won’t change your model-device mapping, you can just save your model directly using <code>t.save</code> and load it with <code>t.load</code>.<br/><NewLine>If you really want to save <code>nn.sequential</code>, you can also save it direcly using <code>t.save</code> and load it with <code>t.load</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>But if, for instance, I have a neural network with a structure like this:</p><NewLine><p>layers1 = nn.sequential(…)<br/><NewLine>layers2 = nn.sequential(…)<br/><NewLine>layers3 = nn.sequential(…)</p><NewLine><p>And I would like to save the model so that device1 loads layers1, device2 layers2…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>for example, suppose you initialize a model like:</p><NewLine><pre><code class=""lang-auto"">class YourModel(nn.Module):<NewLine>    def __init__(self, dev_list=[""cpu"", ""cuda:1"", ""cuda:0""]):<NewLine>        self.fc1 = nn.sequential(nn.Linear(5, 5).to(dev_list[0]))  # on device cpu<NewLine>        self.fc2 = nn.sequential(nn.Linear(5, 5).to(dev_list[1]))  # on device ""cuda:1""<NewLine>        self.fc3 = nn.sequential(nn.Linear(5, 5).to(dev_list[2]))  # on device ""cuda:0""<NewLine>        self.dev = dev_list<NewLine><NewLine>    def forward(x):<NewLine>        x = self.fc1(x).to(self.dev[1])<NewLine>        x = self.fc2(x).to(self.dev[1])<NewLine>        return self.fc3(x).to(""cpu"")<NewLine></code></pre><NewLine><p>then:</p><NewLine><pre><code class=""lang-auto"">t.save(YourModel(), ""model.pt"")<NewLine>model = t.load(""model.pt"")<NewLine></code></pre><NewLine><p>Device mapping will be saved along with your model, don’t worry about it</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>So, if I wanted to map only fc1 on device 1, could I select it on the load?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>See <a>https://pytorch.org/docs/stable/torch.html?highlight=load#torch.load</a>, especially the <code>map_location</code> part</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>But in this solution, the devices must be connected and assigned. My goal is to deploy the neural network, in a distributed way, decomposing each sequential in different devices and sending the inference through the network.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you please clarify your design a liitle bit more? your description of “distributed” and “decomposing” is pretty vague.</p><NewLine><p>Is it a multi-process application? How do you map your gpus to your processes? How do you map your model to your devices? I am really sorry that I cannot help you more if you don’t <strong>give a clear definition of the architecture</strong> you would like to achieve. It would be better if you can draw a graph or show your code.</p><NewLine><p>If you just want to split <code>layer1</code>, <code>layer2</code> and <code>layer3</code> to different devices, you can simply save them individually with <code>torch.save</code> and <code>torch.load</code>, torch will take the charge to pickle whatever passed to it, including parameters and your custom attributes such as the ones set by <code>self.some_attr</code> in <code>__init__</code>.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s an architecture, fog, edge and cloud. Based on the example above:</p><NewLine><p>class YourModel(nn.Module):<br/><NewLine>def <strong>init</strong>(self):<br/><NewLine>self.fc1 = nn.sequential()<br/><NewLine>self.fc2 = nn.sequential()<br/><NewLine>self.fc3 = nn.sequential()<br/><NewLine>I intend to save fc1, fc2 and fc3 separately. In this way I could make a first prediction in the device that has the model fc1 and sending the inference to the second device, make the next prediction with greater accuracy. The third one would work in the same way.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see, one more question, will you move the model around, such as on a different machine with different gpu number, or are you loading the whole model on the same devices?</p><NewLine><p>If you don’t, and you really want to save them seperately to different files, maybe for better inspection or archive perpose, then:</p><NewLine><pre><code class=""lang-auto"">def save(your_model):<NewLine>    torch.save(your_model.fc1, ""fc1.pt"")<NewLine>    torch.save(your_model.fc2, ""fc2.pt"")<NewLine>    torch.save(your_model.fc3, ""fc3.pt"")<NewLine></code></pre><NewLine><p>If you do, then you will have to decide which device each part of your model would locate on, eg: suppose on your training machine you have 3 gpus, and on your inference machine you have 1 gpu.<br/><NewLine>def save(your_model):</p><NewLine><pre><code class=""lang-auto"">def save(your_model):<NewLine>    torch.save(your_model.fc1, ""fc1.pt"")<NewLine>    torch.save(your_model.fc2, ""fc2.pt"")<NewLine>    torch.save(your_model.fc3, ""fc3.pt"")<NewLine><NewLine>def map(your_model):<NewLine>    your_model.fc1 = torch.load(""fc1.pt"", map_location=torch.device('cuda:0'))<NewLine>    your_model.fc2 = torch.load(""fc2.pt"", map_location=torch.device('cuda:0'))<NewLine>    your_model.fc3 = torch.load(""fc3.pt"", map_location=torch.device('cuda:0'))<NewLine></code></pre><NewLine><p>by the way,</p><NewLine><aside class=""quote no-group"" data-post=""8"" data-topic=""89117"" data-username=""Fernando_Gallego""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/fernando_gallego/40/26683_2.png"" width=""20""/> Fernando_Gallego:</div><NewLine><blockquote><NewLine><p>the devices must be connected and assigned</p><NewLine></blockquote><NewLine></aside><NewLine><p>Maybe you have some wrong idea, there is not such a “connected device” concept in pytorch, you can perform a complex forward() operation or a simple add() operation on some input <code>x</code> locating on device <code>cuda:[number]</code> or <code>cpu</code> simply because the operands (tensors) locates on the same device, if torch needs to fetch it somewhere else, it will complain and throw an error.</p><NewLine><h2>About saving the model</h2><NewLine><p>There are many ways to save your model, typically you will want to save the <code>OrderedDict</code> returned by <code>model.state_dict()</code>, the keys are your parameter names such as “linear.weight” or “linear.bias”, and values are <code>nn.Parameter</code>, its <code>.data</code> attribute is just a Tensor. You may load a state dict into your model like:</p><NewLine><pre><code class=""lang-auto"">def prep_load_state_dict(model: nn.Module,<NewLine>                         state_dict: Any):<NewLine>    """"""<NewLine>    Automatically load a **loaded state dictionary**<NewLine><NewLine>    Note:<NewLine>        This function handles tensor device remapping.<NewLine>    """"""<NewLine>    for name, param in model.named_parameters():<NewLine>        state_dict[name].to(param.device)<NewLine>    model.load_state_dict(state_dict)<NewLine></code></pre><NewLine><h2>About torch.save and torch.load</h2><NewLine><p>If you know the <code>pickle</code> concept in python, then you will get what <code>torch.save</code> does. <code>pickle</code> serialize a object into binary string:</p><NewLine><pre><code class=""lang-auto"">buffer = io.BytesIO()<NewLine>t.save(t.zeros([5]), buffer)<NewLine>print(buffer.getvalue())<NewLine></code></pre><NewLine><p>will yield:</p><NewLine><pre><code class=""lang-auto"">b'\x80\x02\x8a\nl\xfc\x9cF\xf9 j\xa8P\x19.\x80\x02M\xe9\x03.\x80\x02}q\x00(X\n\x00\x00\x00type_sizesq\x01}q\x02(X\x03\x00\x00\x00intq\x03K\x04X\x04\x00\x00\x00longq\x04K\x04X\x05\x00\x00\x00shortq\x......<NewLine></code></pre><NewLine><p>you can serialize whatever you like into this, cuda tensor will essentially be saved as “raw data” + “device descriptor cuda:0”.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""11"" data-topic=""89117"" data-username=""iffiX""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/iffix/40/24443_2.png"" width=""20""/> iffiX:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">def save(your_model):<NewLine>    torch.save(your_model.fc1, ""fc1.pt"")<NewLine>    torch.save(your_model.fc2, ""fc2.pt"")<NewLine>    torch.save(your_model.fc3, ""fc3.pt"")<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>Thanks a lot, I think the solution were:</p><NewLine><p>def save(your_model):<br/><NewLine>torch.save(your_model.fc1, “fc1.pt”)<br/><NewLine>torch.save(your_model.fc2, “fc2.pt”)<br/><NewLine>torch.save(your_model.fc3, “fc3.pt”)</p><NewLine><p>I’ll try then.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Great! post your issues if you have any</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m also trying to do something similar but in my scenario I construct a whole model using only <code>nn.Sequential</code> and then I just want to save it. I don’t have a class defined for it so something like <a href=""https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch"" rel=""nofollow noopener"">https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch</a> won’t work for me.</p><NewLine><hr/><NewLine><p>My current attempt uses pickle but I keep getting warning for using pickle:</p><NewLine><pre><code class=""lang-auto"">FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead<NewLine>  warnings.warn(""pickle support for Storage will be removed in 1.5. Use `torch.save` instead"", FutureWarning)<NewLine></code></pre><NewLine><hr/><NewLine><p>I think they just want us to use <code>torch.save</code> and <code>torch.load</code>. I stopped getting warning when I did that.</p><NewLine><p>My (full) code:</p><NewLine><pre><code class=""lang-auto""><NewLine># creating data and running through a nn and saving it<NewLine><NewLine>import torch<NewLine>import torch.nn as nn<NewLine><NewLine>from pathlib import Path<NewLine>from collections import OrderedDict<NewLine><NewLine>import numpy as np<NewLine><NewLine>import pickle<NewLine><NewLine>path = Path('~/data/tmp/').expanduser()<NewLine>path.mkdir(parents=True, exist_ok=True)<NewLine><NewLine>num_samples = 3<NewLine>Din, Dout = 1, 1<NewLine>lb, ub = -1, 1<NewLine><NewLine>x = torch.torch.distributions.Uniform(low=lb, high=ub).sample((num_samples, Din))<NewLine><NewLine>f = nn.Sequential(OrderedDict([<NewLine>    ('f1', nn.Linear(Din,Dout)),<NewLine>    ('out', nn.SELU())<NewLine>]))<NewLine>y = f(x)<NewLine><NewLine># save data torch to numpy<NewLine>x_np, y_np = x.detach().cpu().numpy(), y.detach().cpu().numpy()<NewLine>np.savez(path / 'db', x=x_np, y=y_np)<NewLine><NewLine>print(x_np)<NewLine># save model<NewLine>with open('db_saving_seq', 'wb') as file:<NewLine>    pickle.dump({'f': f}, file)<NewLine><NewLine># load model<NewLine>with open('db_saving_seq', 'rb') as file:<NewLine>    db = pickle.load(file)<NewLine>    f2 = db['f']<NewLine><NewLine># test that it outputs the right thing<NewLine>y2 = f2(x)<NewLine><NewLine>y_eq_y2 = y == y2<NewLine>print(y_eq_y2)<NewLine><NewLine>db2 = {'f': f, 'x': x, 'y': y}<NewLine>torch.save(db2, path / 'db_f_x_y')<NewLine><NewLine>print('Done')<NewLine><NewLine>db3 = torch.load(path / 'db_f_x_y')<NewLine>f3 = db3['f']<NewLine>x3 = db3['x']<NewLine>y3 = db3['y']<NewLine>yy3 = f3(x3)<NewLine><NewLine>y_eq_y3 = y == y3<NewLine>print(y_eq_y3)<NewLine><NewLine>y_eq_yy3 = y == yy3<NewLine>print(y_eq_yy3)<NewLine></code></pre><NewLine><p>did you try that? Is there a reason why that’s not enough for you?</p><NewLine><hr/><NewLine><p>cross-posted: <a href=""https://stackoverflow.com/questions/62923052/how-does-one-save-torch-nn-sequential-models-in-pytorch-properly"" rel=""nofollow noopener"">https://stackoverflow.com/questions/62923052/how-does-one-save-torch-nn-sequential-models-in-pytorch-properly</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Fernando_Gallego; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Fernando_Gallego; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Fernando_Gallego; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Fernando_Gallego; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Fernando_Gallego; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/Brando_Miranda; <NewLine> ,"REPLY_DATE 1: July 14, 2020, 11:07am; <NewLine> REPLY_DATE 2: July 15, 2020,  8:00am; <NewLine> REPLY_DATE 3: July 15, 2020,  8:07am; <NewLine> REPLY_DATE 4: July 15, 2020,  2:45pm; <NewLine> REPLY_DATE 5: July 15, 2020,  2:51pm; <NewLine> REPLY_DATE 6: July 15, 2020,  3:43pm; <NewLine> REPLY_DATE 7: July 15, 2020,  3:57pm; <NewLine> REPLY_DATE 8: July 15, 2020,  4:02pm; <NewLine> REPLY_DATE 9: July 15, 2020,  4:28pm; <NewLine> REPLY_DATE 10: July 15, 2020,  4:34pm; <NewLine> REPLY_DATE 11: July 15, 2020,  4:39pm; <NewLine> REPLY_DATE 12: July 15, 2020,  8:11pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> 
89001,How to decay lr with distributed training using test acc?,2020-07-13T13:40:22.111Z,6,133,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I use the following code, but I think it may be wrong!<br/><NewLine>lr_broadcast = torch.tensor([0.0]).cuda()<br/><NewLine>if distribute:<br/><NewLine>if local_rank == 0:<br/><NewLine>lr_broadcast = torch.tensor([optimizer.state_dict()[‘param_groups’][0][‘lr’]]).cuda()<br/><NewLine>dist.all_reduce(lr_broadcast)<br/><NewLine>optimizer.state_dict()[‘param_groups’][0][‘lr’] = lr_broadcast.item()<br/><NewLine>print(local_rank, optimizer.state_dict()[‘param_groups’][0][‘lr’])</p><NewLine></div>",https://discuss.pytorch.org/u/MRGAO1996,(Mrgao1996),MRGAO1996,"July 13, 2020,  1:40pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrgao1996"">@MRGAO1996</a></p><NewLine><p>Could you please format the code with proper indention?</p><NewLine><p>If this line is with in the <code>if local_rank == 0</code> block, then this is indeed wrong. You need to call <code>all_reduce</code> on all processes, as it is a collective communication.</p><NewLine><pre><code class=""lang-python"">if local_rank == 0:<NewLine>    dist.all_reduce(lr_broadcast)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">if distribute and local_rank == 0:<NewLine>    acc = verification(...)<NewLine>    # lr_sched is torch.optim.lr_scheduler.ReduceLROnPlateau()<NewLine>    lr_sched.step(acc)<NewLine>lr_broadcast = torch.tensor([0.0]).cuda()<NewLine>if distribute:<NewLine>    if local_rank == 0:<NewLine>        lr_broadcast = torch.tensor([optimizer.state_dict()['param_groups'][0]['lr']]).cuda()<NewLine>    dist.all_reduce(lr_broadcast)<NewLine>    optimizer.state_dict()['param_groups'][0]['lr'] = lr_broadcast.item()<NewLine>    print(local_rank, optimizer.state_dict()['param_groups'][0]['lr'])<NewLine></code></pre><NewLine><p>When I run this code with 2 GPUs, I found two mistakes.<br/><NewLine>First, dist.all_reduct() failed. I try to print lr_broadcast after all_reduce, but different on GPU0 and GPU1.<br/><NewLine>Second, after I pass the lr_broadcast value to optimizer.state_dict, I print the lr in optimizer, but the value is differ from lr_broadcast.<br/><NewLine>So, I got confused.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I noticed the code called <code>.cuda()</code> without specifying a GPU id. Did you set <code>CUDA_VISIBLE_DEVICES</code> or call <code>torch.cuda.set_device()</code>? Could you please share a self-contained repro that shows the error?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well, I knew how to change my code to succeed with “all_reduce”, but I’m still a little comfused.</p><NewLine><p>My code before:(fail)</p><NewLine><pre><code class=""lang-auto"">from torch.nn.parallel import DistributedDataParallel as DDP<NewLine><NewLine>def main():<NewLine>	processes = []<NewLine>	for rank in range(world_size):<NewLine>		p = Process(target=run, args=(rank, world_size))<NewLine>		p.start()<NewLine>		processes.append(p)<NewLine>	for p in processes:<NewLine>		p.join()<NewLine><NewLine>def run(local_rank, world_size):<NewLine>    os.environ['MASTER_ADDR'] = '127.0.0.1'<NewLine>    os.environ['MASTER_PORT'] = '29501'<NewLine>	torch.cuda.set_device(local_rank)<NewLine>    dist.init_process_group(world_size=world_size, rank=local_rank, backend='nccl')<NewLine>	net = Net().cuda()<NewLine>	criterion = ...<NewLine>	optimizer = ...<NewLine>	net = DDP(net, device_ids=[local_rank])<NewLine>	for ep in range(2):<NewLine>		# train<NewLine>		net.train()<NewLine>		for i, (images, labels) in enumerate(train_loader):<NewLine>			optimizer.zero_grad()<NewLine>			images = image.cuda()<NewLine>			labels = labels.cuda()<NewLine>			outputs = net(images)<NewLine>			loss = criterion(outputs, labels)<NewLine>			loss.backward()<NewLine>			optimizer.step()<NewLine>		# verification<NewLine>		if local_rank != 0:<NewLine>            val_acc = torch.tensor([0.0]).cuda()<NewLine>		# only do verification on rank0<NewLine>		if local_rank == 0:<NewLine>			net.eval()<NewLine>			# simplify verification simulation<NewLine>			input = torch.randn(8, 3, 112, 112).cuda()<NewLine>			with torch.no_grad():<NewLine>				output = net(input)<NewLine>			val_acc = torch.tensor([0.0009]).cuda()<NewLine>		dist.all_reduct(val_acc)<NewLine>		print(local_rank, val_acc)<NewLine><NewLine>main()<NewLine></code></pre><NewLine><p>The code below shows:</p><NewLine><pre><code class=""lang-auto"">1 tensor([0.0], device='cuda:1')<NewLine>0 tensor([0.0009], device='cuda:0')<NewLine></code></pre><NewLine><p>And, in epoch 2, local_rank 1 will hang up.</p><NewLine><p>My solution is that, change the line “output = net(input)” to “output = net.module(input)”, it succeed.</p><NewLine><pre><code class=""lang-auto"">0 tensor([0.0009], device='cuda:0')<NewLine>1 tensor([0.0009], device='cuda:1')<NewLine></code></pre><NewLine><p>So I want to know why this happen?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you please explain this <img alt="":slightly_smiling_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slightly_smiling_face.png?v=9"" title="":slightly_smiling_face:""/></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/mrgao1996"">@MRGAO1996</a></p><NewLine><p>Does your model use any buffers (e.g., running mean in <code>BatchNorm</code>)?</p><NewLine><p>As you have wrapped the second forward with <code>with torch.no_grad():</code>, rank 0 will skip the code in <code>if torch.is_grad_enabled() and ...</code> branch (see DDP forward below), but the behavior of the next <code>forward</code> will be different on two ranks, as rank 0 would skip <code>_sync_params()</code> but rank 1 would execute that. But this method should only make a difference when your model has buffers.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/d5ae4a07ef5b2e77cf51737fb0a3aafc2e71231d/torch/nn/parallel/distributed.py#L562-L590"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/d5ae4a07ef5b2e77cf51737fb0a3aafc2e71231d/torch/nn/parallel/distributed.py#L562-L590"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/d5ae4a07ef5b2e77cf51737fb0a3aafc2e71231d/torch/nn/parallel/distributed.py#L562-L590</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""562"" style=""counter-reset: li-counter 561 ;""><NewLine><li>def forward(self, *inputs, **kwargs):</li><NewLine><li>    if self.require_forward_param_sync:</li><NewLine><li>        self._sync_params()</li><NewLine><li><NewLine></li><li>    if self.device_ids:</li><NewLine><li>        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)</li><NewLine><li>        if len(self.device_ids) == 1:</li><NewLine><li>            output = self.module(*inputs[0], **kwargs[0])</li><NewLine><li>        else:</li><NewLine><li>            outputs = self.parallel_apply(self._module_copies[:len(inputs)], inputs, kwargs)</li><NewLine><li>            output = self.gather(outputs, self.output_device)</li><NewLine><li>    else:</li><NewLine><li>        output = self.module(*inputs, **kwargs)</li><NewLine><li><NewLine></li><li>    if torch.is_grad_enabled() and self.require_backward_grad_sync:</li><NewLine><li>        self.require_forward_param_sync = True</li><NewLine><li>        # We'll return the output object verbatim since it is a freeform</li><NewLine><li>        # object. We need to find any tensors in this object, though,</li><NewLine><li>        # because we need to figure out which parameters were used during</li><NewLine><li>        # this forward pass, to ensure we short circuit reduction for any</li><NewLine></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/d5ae4a07ef5b2e77cf51737fb0a3aafc2e71231d/torch/nn/parallel/distributed.py#L562-L590"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/MRGAO1996; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/MRGAO1996; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/MRGAO1996; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: July 13, 2020,  3:49pm; <NewLine> REPLY_DATE 2: July 14, 2020,  1:49am; <NewLine> REPLY_DATE 3: July 14, 2020,  2:46pm; <NewLine> REPLY_DATE 4: July 16, 2020,  3:00am; <NewLine> REPLY_DATE 5: July 16, 2020,  3:02am; <NewLine> REPLY_DATE 6: July 16, 2020,  2:42pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
89303,GLOO and infiniband,2020-07-15T10:40:02.260Z,0,121,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>Can GLOO work with infiniband?</p><NewLine><p>Our RTX2080ti GPUs do not support GPUDirect/RDMA anyway, so the only thing we want is to work out of the box in reasonable BW which will not become the bottleneck, we are doing P2P communication.</p><NewLine><p>edit: well I see that here <a href=""https://github.com/facebookincubator/gloo"" rel=""nofollow noopener"">https://github.com/facebookincubator/gloo</a> they say its supported, but I wonder if you still have anything further to say on integration with Pytorch.</p><NewLine><p>edit2: especially since here <a href=""https://pytorch.org/docs/stable/distributed.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/distributed.html</a> its written than GLOO does not support infiniband.</p><NewLine></div>",https://discuss.pytorch.org/u/seliad,(Saar Eliad),seliad,"July 15, 2020, 11:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>GLOO does have an ibverbs transport <a href=""https://github.com/facebookincubator/gloo/tree/master/gloo/transport/ibverbs"" rel=""nofollow noopener"">https://github.com/facebookincubator/gloo/tree/master/gloo/transport/ibverbs</a>. However, it was never used or tested with PyTorch. That may be the reason that PyTorch doc says GLOO does not support infiniband.</p><NewLine><p>We are about to test GLOO ibverbs transport over RDMA, and integrate with PyTorch on HPC scenarios. For now, GLOO ibverbs hasn’t been integrated to PyTorch yet.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Filed an issue <a href=""https://github.com/pytorch/pytorch/issues/41485"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/41485</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jiayisuse; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jiayisuse; <NewLine> ,"REPLY_DATE 1: July 15, 2020,  5:22pm; <NewLine> REPLY_DATE 2: July 15, 2020,  5:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> 
89214,Worker blocked in forward pass when parallelized,2020-07-14T19:36:55.953Z,0,78,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,</p><NewLine><p>I’m facing a deadlock I can’t solve by myself.<br/><NewLine>I’m trying to implement Reactor algorithm (<a href=""https://openreview.net/forum?id=rkHVZWZAZ"" rel=""nofollow noopener"">https://openreview.net/forum?id=rkHVZWZAZ</a>) to train an AI playing at French Tarot card game.<br/><NewLine>When I tried to implement training worker parallelisation, things seemed to work well until I tried to increase the size of some hidden layer.<br/><NewLine>Then worker seemed to block (like infinity loop) when passing through the first layer of the first forward pass (more precisely, the first matmul -&gt; checked with pudb debugger).</p><NewLine><p>I tried a few things:</p><NewLine><ul><NewLine><li>When worker is called from the main process, everything is fine, whatever the layers size are</li><NewLine><li>When worker exploration is performed through a separate thread (inside the secondary process, the main thread of that secondary process being the training), the exploration is ok, and the blocking occures into the training</li><NewLine><li>At the contrary, if the exploration &amp; training are performed alternatively, the blocking occures into the first exploration step</li><NewLine></ul><NewLine><p>The multiprocess (and multithread) are performed with fork (linux environment), by subclassing multiprocess and multithread classes, and then by calling start method.<br/><NewLine>Forward pass is only performed on local copy of the shared network (one copy for exploration, another for training).<br/><NewLine>I verified copy process : local networks seem to match perfectly shared network.</p><NewLine><p>I suspect some secondary process memory issue, but I have no other clue or direction to follow.<br/><NewLine>The network dimension are pretty reasonable I think :</p><NewLine><ul><NewLine><li>one input linear layer 78 -&gt; n</li><NewLine><li>two parallel recurrent layer (GRU) n,m -&gt; n (m = hidden state dimension)</li><NewLine><li>three head with two linear layers each<NewLine><ul><NewLine><li>actor layer m -&gt; n -&gt; 78</li><NewLine><li>advantage layer  m -&gt; n -&gt; 78</li><NewLine><li>value layer m -&gt; n -&gt; 1<br/><NewLine>the degrees of freedom are n &amp; m, which where 80 initially. Blocking occures in input layer when n or m is greater than 110… (although m does not appear in input layer dimension …)<br/><NewLine>training process is performed on 14 successive steps x batchs of 5 trajectories.</li><NewLine></ul><NewLine></li><NewLine></ul><NewLine><p>Does that kind of issue seems familiar to anyone ?</p><NewLine><p>Thanks by advance</p><NewLine></div>",https://discuss.pytorch.org/u/driou,(Adrien Barny),driou,"July 14, 2020,  7:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/driou"">@driou</a></p><NewLine><p>Are you using <code>torch.multiprocessing</code> only or does your code also used any feature from <code>torch.distributed</code> or <code>torch.nn.parallel</code>?</p><NewLine><p>If it just <code>torch.multiprocessing</code>, it might relates to forking the processing. If the parent process used any CUDA-related feature, there will be a CUDA context on it which does not fork. Even if it is CPU only, the fork could have broken the OMP internal states see discussion here: <a href=""https://github.com/pytorch/pytorch/issues/41197"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/41197</a></p><NewLine><p>Can you check if using spawn from <code>torch.multiprocessing</code> solves the problem?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey,</p><NewLine><p>I’m indeed using torch.multiprocessing (and not torch.distributed nor torch.nn.parallel).<br/><NewLine>I’m not using any CUDA-related feature : it’s CPU only (in fact I did not performed the full pytorch install with CUDA).<br/><NewLine>I will try with spawn, but I choosed fork on purpose, because it was more practical with the ability to init process before starting it.</p><NewLine><p>Before that, let’s have a look on that thread you send me: the symptom looks like very similar !</p><NewLine><p>Thanks a lot for your help</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/driou; <NewLine> ,"REPLY_DATE 1: July 14, 2020,  8:51pm; <NewLine> REPLY_DATE 2: July 15, 2020,  4:06pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
88856,How to catch exceptions caused by rpc exactly,2020-07-12T07:48:25.820Z,12,251,"<div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> How to catch exceptions <code>rpc.async</code>, <code>rpc.sync</code> and <code>rpc.remote</code> thrown in the <strong>caller</strong> under the following conditions, suppose a timeout is set globally (or per call):</p><NewLine><ol><NewLine><li>during execution, the target process crashes and exits, also closing down all rpc execution threads.</li><NewLine><li>during execution, connection to the target process is closed</li><NewLine><li>during execution, the timeout limit is reached</li><NewLine><li>during execution, an exception is raised in the executed function</li><NewLine></ol><NewLine><p>Based on my experiments, my partial answer is:</p><NewLine><ol><NewLine><li>Not known ?</li><NewLine><li>A <code>RuntimeError</code>, something like “peer reset”</li><NewLine><li>An uncatchable <code>std::runtime_error</code>, something like:</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">terminate called after throwing an instance of 'std::runtime_error'<NewLine>  what():  RPC ran for more than 5000 milliseconds and timed out.<NewLine></code></pre><NewLine><ol start=""4""><NewLine><li>the exception thrown by the function, not the original exception, but wrapped in a udf exception and reraised on the caller side.</li><NewLine></ol><NewLine><p>The third one troubles me the most because <code>std::runtime_error</code> will cause an ugly <code>Fatal Python Error</code>:</p><NewLine><pre><code class=""lang-auto"">Fatal Python error: Aborted<NewLine><NewLine>Thread 0x00007f916abab700 (most recent call first):<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/distributed/world.py"", line 63 in _rpc_call_remote_method<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/torch/distributed/rpc/internal.py"", line 153 in _run_function<NewLine><NewLine>Thread 0x00007f91693a8700 (most recent call first):<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/distributed/world.py"", line 75 in _rpc_get_remote_paired_value<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/torch/distributed/rpc/internal.py"", line 153 in _run_function<NewLine><NewLine>Thread 0x00007f9163fff700 (most recent call first):<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/distributed/world.py"", line 75 in _rpc_get_remote_paired_value<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/torch/distributed/rpc/internal.py"", line 153 in _run_function<NewLine><NewLine>Thread 0x00007f91527fc700 (most recent call first):<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/torch/distributed/rpc/api.py"", line 554 in rpc_sync<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/torch/distributed/rpc/api.py"", line 77 in wrapper<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/distributed/world.py"", line 756 in _rpc_paired_class_call<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/distributed/world.py"", line 597 in rpc_paired_class_sync<NewLine>  File ""/home/Administrator/iffi/Projects/machin/test/parallel/distributed/test_world.py"", line 97 in main<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/distributed/world.py"", line 46 in _exec_role<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 862 in run<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/thread.py"", line 47 in run<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap<NewLine><NewLine>Thread 0x00007f9152ffd700 (most recent call first):<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/distributed/election.py"", line 423 in _task_timeout<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 862 in run<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/thread.py"", line 47 in run<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap<NewLine><NewLine>Thread 0x00007f91537fe700 (most recent call first):<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/distributed/election.py"", line 435 in _task_keep_alive<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 862 in run<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/thread.py"", line 47 in run<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap<NewLine><NewLine>Thread 0x00007f9153fff700 (most recent call first):<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 297 in wait<NewLine>  File ""/usr/lib/python3.5/queue.py"", line 173 in get<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/distributed/election.py"", line 491 in _task_handle<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 862 in run<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/thread.py"", line 47 in run<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap<NewLine><NewLine>Thread 0x00007f9160ff9700 (most recent call first):<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 293 in wait<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/event.py"", line 66 in wait<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/distributed/role_dispatcher.py"", line 234 in _task_dispatch<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 862 in run<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/thread.py"", line 47 in run<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap<NewLine><NewLine>Thread 0x00007f91617fa700 (most recent call first):<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 293 in wait<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/event.py"", line 66 in wait<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/distributed/world.py"", line 302 in _task_run_dispatched_roles<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 862 in run<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/thread.py"", line 47 in run<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner<NewLine>  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap<NewLine><NewLine>Thread 0x00007f91e4362700 (most recent call first):<NewLine>  File ""/home/Administrator/iffi/Projects/machin/test/parallel/distributed/test_world.py"", line 145 in subproc_start_world_with_roles<NewLine>  File ""/home/Administrator/iffi/Projects/machin/test/parallel/util_run_multi.py"", line 16 in process_main<NewLine>  File ""/usr/lib/python3.5/multiprocessing/process.py"", line 93 in run<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/process.py"", line 52 in run<NewLine>  File ""/usr/lib/python3.5/multiprocessing/process.py"", line 249 in _bootstrap<NewLine>  File ""/usr/lib/python3.5/multiprocessing/popen_fork.py"", line 74 in _launch<NewLine>  File ""/usr/lib/python3.5/multiprocessing/popen_fork.py"", line 20 in __init__<NewLine>  File ""/usr/lib/python3.5/multiprocessing/context.py"", line 267 in _Popen<NewLine>  File ""/home/Administrator/iffi/Projects/machin/machin/parallel/process.py"", line 25 in _Popen<NewLine>  File ""/usr/lib/python3.5/multiprocessing/process.py"", line 105 in start<NewLine>  File ""/home/Administrator/iffi/Projects/machin/test/parallel/util_run_multi.py"", line 27 in processes<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/fixtures.py"", line 788 in call_fixture_func<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/fixtures.py"", line 964 in pytest_fixture_setup<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/callers.py"", line 187 in _multicall<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/manager.py"", line 87 in &lt;lambda&gt;<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/manager.py"", line 93 in _hookexec<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/hooks.py"", line 286 in __call__<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/fixtures.py"", line 914 in execute<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/fixtures.py"", line 584 in _compute_fixture_value<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/fixtures.py"", line 503 in _get_active_fixturedef<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/fixtures.py"", line 487 in getfixturevalue<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/fixtures.py"", line 477 in _fillfixtures<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/fixtures.py"", line 297 in fillfixtures<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/python.py"", line 1483 in setup<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/runner.py"", line 373 in prepare<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/runner.py"", line 123 in pytest_runtest_setup<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/callers.py"", line 187 in _multicall<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/manager.py"", line 87 in &lt;lambda&gt;<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/manager.py"", line 93 in _hookexec<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/hooks.py"", line 286 in __call__<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/runner.py"", line 217 in &lt;lambda&gt;<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/runner.py"", line 244 in from_call<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/runner.py"", line 217 in call_runtest_hook<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/runner.py"", line 186 in call_and_report<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/runner.py"", line 94 in runtestprotocol<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/runner.py"", line 85 in pytest_runtest_protocol<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/callers.py"", line 187 in _multicall<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/manager.py"", line 87 in &lt;lambda&gt;<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/manager.py"", line 93 in _hookexec<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/hooks.py"", line 286 in __call__<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/main.py"", line 272 in pytest_runtestloop<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/callers.py"", line 187 in _multicall<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/manager.py"", line 87 in &lt;lambda&gt;<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/manager.py"", line 93 in _hookexec<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/hooks.py"", line 286 in __call__<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/main.py"", line 247 in _main<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/main.py"", line 191 in wrap_session<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/main.py"", line 240 in pytest_cmdline_main<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/callers.py"", line 187 in _multicall<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/manager.py"", line 87 in &lt;lambda&gt;<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/manager.py"", line 93 in _hookexec<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/pluggy/hooks.py"", line 286 in __call__<NewLine>  File ""/home/Administrator/iffi/Projects/machin/venv/lib/python3.5/site-packages/_pytest/config/__init__.py"", line 125 in main<NewLine>  File ""/data/software/pycharm/pycharm-2020.1.2/plugins/python/helpers/pycharm/_jb_pytest_runner.py"", line 43 in &lt;module&gt;<NewLine></code></pre><NewLine><p>Is there any clean way to deal with the first three conditions? The fourth one is simple. And why pybind11 is not converting the third <code>std::runtime_error</code> to a catchable python <code>RuntimeError</code> ?</p><NewLine></div>",https://discuss.pytorch.org/u/iffiX,(Iffi),iffiX,"July 12, 2020, 10:27am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/iffix"">@iffiX</a></p><NewLine><blockquote><NewLine><ol><NewLine><li>during execution, the target process crashes and exits, also closing down all rpc execution threads.</li><NewLine><li>during execution, connection to the target process is closed</li><NewLine></ol><NewLine></blockquote><NewLine><p>Prior to v1.6, ProcessGroup is the only available backend, which requires all processes to be alive. So RPC gang cannot survive these failures. Even if you can catch it in application code, it will leave subsequent RPC behaviors in an undefined state, unless there is a global recovery process (we do have plans to providing this).</p><NewLine><p>We will introduce <a href=""https://github.com/pytorch/tensorpipe"" rel=""nofollow noopener"">TensorPipe</a> backend for RPC in v1.6, which is a P2P comm library. But in the first experimental version, it still has some part depends on ProcessGroup, so I think it still wouldn’t tolerate such failures in v1.6. There is also an ongoing project to provide elasticity to RPC.</p><NewLine><p>cc <a class=""mention"" href=""/u/lcw"">@lcw</a> <a class=""mention"" href=""/u/agolynski"">@agolynski</a></p><NewLine><blockquote><NewLine><ol start=""3""><NewLine><li>during execution, the timeout limit is reached</li><NewLine></ol><NewLine></blockquote><NewLine><p>This should thrown a <code>RuntimeError</code> for the timeout. And try-except on <code>RuntimeError</code> type should be able to catch it. See the code below:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/67a4f375cdf06113ca959b4e16739edb666f243f/torch/testing/_internal/distributed/rpc/rpc_test.py#L2501-L2502"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/67a4f375cdf06113ca959b4e16739edb666f243f/torch/testing/_internal/distributed/rpc/rpc_test.py#L2501-L2502"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/67a4f375cdf06113ca959b4e16739edb666f243f/torch/testing/_internal/distributed/rpc/rpc_test.py#L2501-L2502</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""2501"" style=""counter-reset: li-counter 2500 ;""><NewLine><li>with self.assertRaisesRegex(RuntimeError, error_str):</li><NewLine><li>    fut.wait()</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>If this is not so in your application, then it is a bug that we need to fix. Please let us know how we can reproduce it. Or if you have other suggestions on how we should report timeout error, please also let us know your suggestions.</p><NewLine><p>cc <a class=""mention"" href=""/u/rvarm1"">@rvarm1</a></p><NewLine><blockquote><NewLine><ol start=""4""><NewLine><li>during execution, an exception is raised in the executed function</li><NewLine></ol><NewLine></blockquote><NewLine><p>If there is an exception during remote execution, this should be thrown on the caller side. See the code below:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/67a4f375cdf06113ca959b4e16739edb666f243f/torch/testing/_internal/distributed/rpc/rpc_test.py#L1625-L1631"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/67a4f375cdf06113ca959b4e16739edb666f243f/torch/testing/_internal/distributed/rpc/rpc_test.py#L1625-L1631"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/67a4f375cdf06113ca959b4e16739edb666f243f/torch/testing/_internal/distributed/rpc/rpc_test.py#L1625-L1631</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""1625"" style=""counter-reset: li-counter 1624 ;""><NewLine><li>@dist_init</li><NewLine><li>def test_py_raise_in_user_func(self):</li><NewLine><li>    n = self.rank + 1</li><NewLine><li>    dst_rank = n % self.world_size</li><NewLine><li>    fut = rpc.rpc_async(worker_name(dst_rank), raise_func)</li><NewLine><li>    with self.assertRaises(ValueError):</li><NewLine><li>        fut.wait()</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>This is how the Python error is captured:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/67a4f375cdf06113ca959b4e16739edb666f243f/torch/distributed/rpc/internal.py#L152-L171"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/67a4f375cdf06113ca959b4e16739edb666f243f/torch/distributed/rpc/internal.py#L152-L171"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/67a4f375cdf06113ca959b4e16739edb666f243f/torch/distributed/rpc/internal.py#L152-L171</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""152"" style=""counter-reset: li-counter 151 ;""><NewLine><li>def _run_function(python_udf):</li><NewLine><li>    r""""""</li><NewLine><li>    This function is exclusively called from C++.</li><NewLine><li>    See ``torch/csrc/distributed/rpc/python_rpc_handler.cpp``.</li><NewLine><li><NewLine></li><li>    Runs a Python UDF and returns its return value.</li><NewLine><li>    Wraps any exception in ``RemoteException`` if the function raises.</li><NewLine><li>    """"""</li><NewLine><li>    try:</li><NewLine><li>        if isinstance(python_udf, AttributeError):</li><NewLine><li>            raise python_udf</li><NewLine><li>        result = python_udf.func(*python_udf.args, **python_udf.kwargs)</li><NewLine><li>    except Exception as e:</li><NewLine><li>        # except str = exception info + traceback string</li><NewLine><li>        except_str = (</li><NewLine><li>            f""On {_get_current_rpc_agent().get_worker_info()}:\n""</li><NewLine><li>            f""{repr(e)}\n{traceback.format_exc()}""</li><NewLine><li>        )</li><NewLine><li>        result = RemoteException(except_str, type(e))</li><NewLine><li>    return result</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>This is how non-user error is captured:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/67a4f375cdf06113ca959b4e16739edb666f243f/torch/csrc/distributed/rpc/request_callback_impl.cpp#L907-L921"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/67a4f375cdf06113ca959b4e16739edb666f243f/torch/csrc/distributed/rpc/request_callback_impl.cpp#L907-L921"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/67a4f375cdf06113ca959b4e16739edb666f243f/torch/csrc/distributed/rpc/request_callback_impl.cpp#L907-L921</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""907"" style=""counter-reset: li-counter 906 ;""><NewLine><li>try {</li><NewLine><li>  processRpc(*rpc, messageType, id, retFuture);</li><NewLine><li>} catch (py::error_already_set&amp; e) {</li><NewLine><li>  retFuture-&gt;markCompleted(handleError(e, messageType, id));</li><NewLine><li>  // There are request callback impls in Python, where Python</li><NewLine><li>  // exceptions could be thrown. For releasing Python exception</li><NewLine><li>  // py::objects, GIL must be held.</li><NewLine><li>  py::gil_scoped_acquire acquire;</li><NewLine><li>  e.restore(); // Release ownership on py::objects and also restore</li><NewLine><li>               // Python Error Indicator.</li><NewLine><li>  PyErr_Clear(); // Clear the Python Error Indicator as we has</li><NewLine><li>                 // recorded the exception in the response message.</li><NewLine><li>} catch (std::exception&amp; e) {</li><NewLine><li>  retFuture-&gt;markCompleted(handleError(e, messageType, id));</li><NewLine><li>}</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Please let us know if we missed any. Thanks!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>BTW, which version of PyTorch are you using currently?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>My local development version is 1.5.0, my test version is your latest docker build.</p><NewLine><p><img alt="":pensive:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/pensive.png?v=9"" title="":pensive:""/> It’s really sad to hear that currently pytorch rpc cannot handle the 1st and 2nd condition, since that’s what my application code is designed to do. I will try to repoduce the 3rd condition with simpler code, but that might be very difficult since currently there is no way to log all events just before the “fatal abort” happens.</p><NewLine><p>Anyway, thanks for your response! I still want to ask: what kind of P2P mechanism are you going to provide in the TensorPipe backend? could you please clarify your and your collegues’ plans a little bit more?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""88856"" data-username=""iffiX""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/iffix/40/24443_2.png"" width=""20""/> iffiX:</div><NewLine><blockquote><NewLine><p>It’s really sad to hear that currently pytorch rpc cannot handle the 1st and 2nd condition, since that’s what my application code is designed to do.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Could you please elaborate more on the requirement? We understand it is important to provide failure-recovery + elasticity, but haven’t seen people explicitly requesting this yet, so that didn’t get into our top-priority in past releases. If this is a commonly required feature that might block many use cases, we will reprioritize work items and try to get this done sooner.</p><NewLine><blockquote><NewLine><p>Anyway, thanks for your response! I still want to ask: what kind of P2P mechanism are you going to provide in the TensorPipe backend? could you please clarify your and your collegues’ plans a little bit more?</p><NewLine></blockquote><NewLine><p>This will be released in v1.6 very soon. TensorPipe no longer requires rendezvous or full participation from all processes. So technically, crashed processes do not prevent the rest processes to function correctly (not yet so in v1.6). And it should also be faster than ProcessGroup-based RPC backend. Eventually, we will retire the ProcessGroup-based RPC backend, and make TensorPipe RPC backend as the default option.</p><NewLine><p>cc the main contributor of TensorPipe <a class=""mention"" href=""/u/lcw"">@lcw</a> <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually its just an experimental failure tolerance feature, when I implemented feature, I was expecting your rpc layer to be similar to a normal unreliable connection, that is, a lossy link between 2 processes will not affect any other processes, if it fail, it just fail silently between these 2 processes only, and throws a detectable error. It doesn’t have to be recoverable. Elasticity is also not required, currently it only deals with a preset number of processes, suppose you have 100 work “roles”, you may run it with 1 process or 200 processes, the system is fully distributed and can tollerate losses, it will reschedule the failed role to other healthy processes.</p><NewLine><p>It’s not an imporatant core feature, although I spent quite some time to implement and test it. I have removed it from my framework today, for now <img alt="":rofl:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/rofl.png?v=9"" title="":rofl:""/></p><NewLine><p>I have read descriptions of the TensorPipe, its great to have a smart backend which can choose the best way to move data around transprently, looking forward to it. <img alt="":blush:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/blush.png?v=9"" title="":blush:""/></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry about the inconvenience!</p><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""88856"" data-username=""iffiX""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/iffix/40/24443_2.png"" width=""20""/> iffiX:</div><NewLine><blockquote><NewLine><p>a lossy link between 2 processes will not affect any other processes, if it fail, it just fail silently between these 2 processes only, and throws a detectable error. It doesn’t have to be recoverable. Elasticity is also not required, currently it only deals with a preset number of processes</p><NewLine></blockquote><NewLine></aside><NewLine><p>I see. In this case, looks like we just need to:</p><NewLine><ol><NewLine><li>remove the ProcessGroup usage from TensorPipe RPC agent</li><NewLine><li>let TensorPipe throw proper errors.</li><NewLine></ol><NewLine><p><a class=""mention"" href=""/u/lcw"">@lcw</a> did I miss anything?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>If that is the case, I can try to restructure my code and submit this feature as a pull request to your distributed module, if you think this is valuable for your rpc module.</p><NewLine><p>The feature is structured in the following way:</p><NewLine><pre><code class=""lang-auto"">rpc layer &lt;-&gt; election layer -&gt; role dispatcher layer &lt;-&gt; role runner &lt;-&gt; wrapped highlevel apis<NewLine></code></pre><NewLine><p>High level apis works like:</p><NewLine><pre><code class=""lang-auto"">WORLD                               (only initialize once, tell it what roles you want to run)<NewLine>\-&gt; create_collective_group(...)    (create a sub group which supports irecv, isend, all_gather etc.)<NewLine>\-&gt; create_rpc_group(name, roles)   (create a rpc group)<NewLine>\-&gt; get_rpc_group(name, role)       (find a group handle created by a local role or remote role)<NewLine><NewLine>CollectiveGroup<NewLine>\-&gt; send<NewLine>\-&gt; recv<NewLine>\-&gt; isend<NewLine>\-&gt; irecv<NewLine>...<NewLine>\-&gt; barrier<NewLine><NewLine>RpcGroup<NewLine>\-&gt; rpc_pair(key, value)           (pair a value to the group, so that it can be accessed, locally or remotely)<NewLine>\-&gt; rpc_get_paired(key)            (get a paired value in this group)<NewLine>\-&gt; rpc_sync<NewLine>\-&gt; rpc_async<NewLine>\-&gt; rpc_remote<NewLine>\-&gt; rpc_paired_class_sync          (invoke a method on the paired class instance)<NewLine>\-&gt; rpc_paired_class_async<NewLine>\-&gt; rpc_paired_class_remote<NewLine>\-&gt; rpc_paired_model_sync          (perform a forward operation on the registered model)<NewLine>\-&gt; rpc_paired_model_async<NewLine>\-&gt; rpc_paired_model_remote   <NewLine></code></pre><NewLine><p>Users may start a role, register a service on this role, and access this service from another role like:</p><NewLine><pre><code class=""lang-auto"">class WorkerService(object):<NewLine>    # An example stateful service class<NewLine>    _count = 0<NewLine><NewLine>    def counter(self):    <NewLine>        self._count += 1<NewLine>        return self._count<NewLine><NewLine><NewLine>class Worker(RoleBase):<NewLine>    def __init__(self, index):<NewLine>        super(Worker, self).__init__(index)<NewLine>        self.service = WorkerService()<NewLine>        self.group = get_world().create_rpc_group(""Employees"", roles=[<NewLine>            ""Worker:0"", ""Worker:1"", ""Worker:2""<NewLine>        ])<NewLine>        # expose service<NewLine>        self.group.rpc_pair(""worker_service"", self.service)<NewLine><NewLine>    def main(self):<NewLine>        while True:<NewLine>            pass<NewLine><NewLine>class Manager(RoleBase):<NewLine>    def __init__(self, index):<NewLine>        super(Manager, self).__init__(index)<NewLine>        self.group = get_world().get_rpc_group(""Employee"", ""Worker:{}"".format(index))<NewLine><NewLine>    def main(self):<NewLine>       for i in range(10):<NewLine>            self.group.rpc_paired_class_sync(<NewLine>                to=""Worker:{}"".format(self.role_index),<NewLine>                cls_method=WorkerService.counter,<NewLine>                name=""worker_service"",<NewLine>            )<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    # suppose process rank is `rank`<NewLine>    # suppose there are 3 processes<NewLine>    # start 3 managers and 3 workers<NewLine>    world = World(world_size=3, rank=rank, <NewLine>                  roles={""Worker"": (Worker, 3),<NewLine>                         ""Manager"": (Manager, 3)},<NewLine>                  rpc_timeout=0.5, election_timeout=0.3, logging=True)<NewLine></code></pre><NewLine><p>What do you think about this design?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot for this proposal!!</p><NewLine><p>There was a proposal for the role-based RPC design with some similar concepts from <a class=""mention"" href=""/u/kiuk_chung"">@Kiuk_Chung</a>, we might be able to join forces there.</p><NewLine><p><a class=""mention"" href=""/u/iffix"">@iffiX</a> Regarding the PR submission, we are grateful about the intent for contributing! Before making decisions on the commitment, may I ask how much bandwidth will you be able to allocate to this project? Since this can be a major new feature, most likely we will need to go through the formal design proposal review --&gt; API review --&gt; code review process and will also need examples (in <a href=""https://github.com/pytorch/examples"" rel=""nofollow noopener"">pytorch/examples</a> repo) and tutorials (in <a href=""https://github.com/pytorch/tutorials"" rel=""nofollow noopener"">pytorch/tutorials</a> repo) for it. This can become a very involved effort, so that we will need to make sure we have all the things we need to hit the finishing line.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hmm, I can do that, I will finish testing the rest of my framework first, might take a week or so, then pour in major effort to complete this, currently I am studing &amp; working at home full time. I am really proud to hear some affirmation!</p><NewLine><p>So is there any document for the whole process of the proposal? It would be much easier if there is any template to refer to.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>BTW, I would also like to see the proposal of your role-based RPC design from Kiuk_Chung <img alt="":blush:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/blush.png?v=9"" title="":blush:""/></p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Awesome!! And yes, let’s wait for comments from <a class=""mention"" href=""/u/kiuk_chung"">@Kiuk_Chung</a></p><NewLine><p>If this looks OK, we can then start from publishing an RFC design issue. Here are some examples:</p><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/23110"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/23110"" rel=""nofollow noopener"" target=""_blank"">[RFC] RPC Based Distributed Model Parallel</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-07-19"" data-format=""ll"" data-time=""21:33:47"" data-timezone=""UTC"">09:33PM - 19 Jul 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/mrshenli"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""mrshenli"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/16999635?v=4"" width=""20""/><NewLine>          mrshenli<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">with @pritamdamania87 @zhaojuanmao @aazzolini @gqchen @pietern @satgera @ezyang @zdevito @suo @manojkris @gchanan @soumith @dzhulgakov @yifuwang @bddppq @joxu-cn @dwarakrajagopal @jspisak<NewLine>PyTorch currently provides...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">feature</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">module: rpc</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/38174"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/38174"" rel=""nofollow noopener"" target=""_blank"">[RFC] Join-based API to support uneven inputs in DDP</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-05-09"" data-format=""ll"" data-time=""02:00:25"" data-timezone=""UTC"">02:00AM - 09 May 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/rohan-varma"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""rohan-varma"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/8039770?v=4"" width=""20""/><NewLine>          rohan-varma<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">🚀 Feature<NewLine>with @pritamdamania87 @mrshenli @zhaojuanmao<NewLine>This RFC is to summarize the current proposal for supporting uneven inputs across different DDP processes. Related...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">feature</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">module: distributed</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/36071"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/36071"" rel=""nofollow noopener"" target=""_blank"">[RFC] Async User Function for RPC</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-04-06"" data-format=""ll"" data-time=""16:04:04"" data-timezone=""UTC"">04:04PM - 06 Apr 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/mrshenli"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""mrshenli"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/16999635?v=4"" width=""20""/><NewLine>          mrshenli<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">Impact<NewLine>Currently, every RPC request occupies an RPC thread on the server side until done. However, if there are nested RPC calls...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">feature</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">module: rpc</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/39272"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/39272"" rel=""nofollow noopener"" target=""_blank"">[RFC] DDP Communication Hook</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-05-29"" data-format=""ll"" data-time=""21:10:01"" data-timezone=""UTC"">09:10PM - 29 May 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/pritamdamania87"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""pritamdamania87"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars1.githubusercontent.com/u/9958665?v=4"" width=""20""/><NewLine>          pritamdamania87<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">Motivation<NewLine>There are several algorithms like GossipGrad and gradient compression which involve different communication strategies for parameter syncs while running Distributed DataParallel...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">enhancement</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">module: distributed</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>This will draw people into the discussion.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/iffix"">@iffiX</a>, <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> I’ll publish the <code>torch.distributed.app</code> proposal that we’ve reviewed internally as a RFC on github shortly - just need to format it in markdown style.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/iffix"">@iffiX</a> – here it is: <a href=""https://github.com/pytorch/pytorch/issues/41425"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/41425</a>. Looking forward to collaborating on this!</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/iffix"">@iffiX</a> A lot of interesting stuff here, let me get to it in order.</p><NewLine><p>Your points <span class=""hashtag"">#1</span> (failure of a node) and <span class=""hashtag"">#2</span> (failure of a link) are indeed handled poorly by the ProcessGroup backend as they bring all other nodes and links down with them. The TensorPipe backend is more resilient and only fails the affected requests. At this stage, a failed link will remain failed forever (although we’re discussing how to improve this in <a href=""https://github.com/pytorch/pytorch/issues/40936"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/40936</a>, please chime in there). A failed node is harder to deal with, as nodes can be stateful and thus restarting it doesn’t mean that the other nodes can resume talking to it as if nothing had happened. The current approach chosen by Elastic is to restart all nodes when one fails. That’s drastic but safe. Also, even in the TensorPipe backend, we currently have nodes exchange their addresses at the beginning, so if a node (re-)joins later it won’t be able to make its address known to the other ones.</p><NewLine><p>The other point where the TensorPipe backend performs a collective operation among all nodes is during <em>graceful</em> shutdown. It is an intrinsic requirement of the API that this method operates like a barrier. Currently that is implemented using process groups and thus suffers from their problems, but even if we reimplemented it on top of TensorPipe it wouldn’t be much different: a failed link/node would cause all others to fail too. I don’t see a way around it while still upholding that requirement. However, if you have your own logic on the nodes’ lifetimes, you probably can do an <em>ungraceful</em> shutdown and thus avoid this whole problem entirely: in that case the TensorPipe backend never uses its process group so it doesn’t matter if it fails.</p><NewLine><p>As for your point <span class=""hashtag"">#3</span>, to me it looks like we’re raising an exception in a function marked as <code>noexcept</code>. If that’s the case it should be an easy fix, but it would greatly help if you could provide a C++ stack trace. Just attach GDB to the process, set up exception catching (<code>catch throw</code>) and then let the process resume as usual (<code>continue</code>). When GDB then catches an exception you can get the stack trace through <code>backtrace</code>. There may be harmless exceptions that are fired before that last fatal one, so please make sure you get the right one. It would also help if your build has debug symbols, I don’t know if the default PyTorch one does.</p><NewLine><p>Point <span class=""hashtag"">#4</span> is particularly tricky, and also relates to the ones above. Dealing with exceptions is hard because, in principle, the RPC module is available to both C++ and Python, which have different exception types and thus it’s hard to preserve and convert between them. Moreover, the future objects have an error state which only contains a string message (see <a href=""https://github.com/pytorch/pytorch/blob/86a2bdc35e03fbe4d66fa2666b49efc4f84855fc/torch/csrc/utils/future.h#L11-L23"" rel=""nofollow noopener"">here</a>), so there is no way to store a “error type” in there. The way the RPC module currently propagates the type from the remote UDF to the client is a bit hacky: it involves marking the future as successful, with a result value that contains the exception type and message, and then use an “unwrap” function that fires when accessing that value that raises it instead of returning it (see <a href=""https://github.com/pytorch/pytorch/blob/86a2bdc35e03fbe4d66fa2666b49efc4f84855fc/torch/csrc/distributed/rpc/init.cpp#L624-L636"" rel=""nofollow noopener"">here</a>). So it means that an errored UDF will return a successful future, but from Python there’s no way to distinguish them. A real failed future (which you get from example in case of an I/O error) will instead always raise a RuntimeError just because it has no way of specifying anything else.</p><NewLine><p>All these problems are in principle solvable (storing a Python exception in C++ as pybind11::error_already_set, converting a C++ exception to Python using pybind11), but this requires changes to the future class which is a top-level util of PyTorch used also by other modules (like JIT) and that makes it hard to change it.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think dropping all processes if one connection has failed is pretty costly, but acceptable under most conditions.<br/><NewLine>I will try to collect the stack trace for point <span class=""hashtag"">#3</span> once I got time.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/Kiuk_Chung; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/Kiuk_Chung; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/lcw; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: July 13, 2020,  3:45pm; <NewLine> REPLY_DATE 2: July 13, 2020,  3:46pm; <NewLine> REPLY_DATE 3: July 14, 2020,  3:16am; <NewLine> REPLY_DATE 4: July 14, 2020,  2:38pm; <NewLine> REPLY_DATE 5: July 14, 2020,  3:08pm; <NewLine> REPLY_DATE 6: July 14, 2020,  3:36pm; <NewLine> REPLY_DATE 7: July 14, 2020,  4:04pm; <NewLine> REPLY_DATE 8: July 14, 2020,  4:24pm; <NewLine> REPLY_DATE 9: July 14, 2020,  4:41pm; <NewLine> REPLY_DATE 10: July 14, 2020,  4:32pm; <NewLine> REPLY_DATE 11: July 14, 2020,  5:30pm; <NewLine> REPLY_DATE 12: July 14, 2020,  7:04pm; <NewLine> REPLY_DATE 13: July 14, 2020,  7:54pm; <NewLine> REPLY_DATE 14: July 15, 2020, 11:48am; <NewLine> REPLY_DATE 15: July 15, 2020,  3:58pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: 2 Likes; <NewLine> REPLY 13 LIKES: 2 Likes; <NewLine> REPLY 14 LIKES: 1 Like; <NewLine> REPLY 15 LIKES: ; <NewLine> 
89148,Data scattering with DistributedDataParallel,2020-07-14T12:46:52.220Z,2,101,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I’m trying to load my data with <code>DistributedSampler</code> class in order to train model on multiple GPUs. The model is wrapped with <code>DistributedDataParallel</code>. The data is successfully loaded on my 2x GPUs. Here my code snippets:</p><NewLine><pre><code class=""lang-auto""><NewLine>            # distributed learning<NewLine>            if torch.cuda.device_count() &gt; 1:<NewLine>                 model = torch.nn.parallel.DistributedDataParallel(self.net, device_ids=[range(self.num_gpus)])<NewLine>            else:<NewLine>                model = self.net<NewLine><NewLine>            iteration = infos[""iteration""]<NewLine>            epoch_start = infos[""epoch""]<NewLine>            <NewLine>            model.train()<NewLine>            for epoch in range(epoch_start, cfg.TRAIN.MAX_EPOCH):<NewLine>                    self.setup_dataloader(epoch=epoch)<NewLine><NewLine>                    for _, blobs in enumerate(self.loader):<NewLine>                        print(""blobs.size"", len(blobs))<NewLine>                        print(blobs)<NewLine>                        loss_dict = model.forward(blobs)<NewLine></code></pre><NewLine><p>blobs is list of dicts which include tensors, objects in images  + other additional information (It’s a object detection task based on Faster CNN).<br/><NewLine>After calling model.forward(blobs), there is a error reported as:</p><NewLine><pre><code class=""lang-auto"">TypeError: list indices must be integers or slices, not range<NewLine></code></pre><NewLine><p>The corresponding traceback of this error:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""tools/train.py"", line 456, in &lt;module&gt;<NewLine>    trainer.train(args)<NewLine>  File ""tools/train.py"", line 372, in train<NewLine>    loss_dict = model.forward(blobs)<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 445, in forward<NewLine>    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 471, in scatter<NewLine>    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py"", line 36, in scatter_kwargs<NewLine>    inputs = scatter(inputs, target_gpus, dim) if inputs else []<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py"", line 28, in scatter<NewLine>    res = scatter_map(inputs)<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py"", line 15, in scatter_map<NewLine>    return list(zip(*map(scatter_map, obj)))<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py"", line 17, in scatter_map<NewLine>    return list(map(list, zip(*map(scatter_map, obj))))<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py"", line 19, in scatter_map<NewLine>    return list(map(type(obj), zip(*map(scatter_map, obj.items()))))<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py"", line 15, in scatter_map<NewLine>    return list(zip(*map(scatter_map, obj)))<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py"", line 13, in scatter_map<NewLine>    return Scatter.apply(target_gpus, None, dim, obj)<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py"", line 88, in forward<NewLine>    streams = [_get_stream(device) for device in target_gpus]<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py"", line 88, in &lt;listcomp&gt;<NewLine>    streams = [_get_stream(device) for device in target_gpus]<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py"", line 115, in _get_stream<NewLine>    if _streams[device] is None:<NewLine>TypeError: list indices must be integers or slices, not range<NewLine>Traceback (most recent call last):<NewLine>  File ""tools/train.py"", line 456, in &lt;module&gt;<NewLine>    trainer.train(args)<NewLine>  File ""tools/train.py"", line 372, in train<NewLine>    loss_dict = model.forward(blobs)<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 445, in forward<NewLine>    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 471, in scatter<NewLine>    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py"", line 36, in scatter_kwargs<NewLine>    inputs = scatter(inputs, target_gpus, dim) if inputs else []<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py"", line 28, in scatter<NewLine>    res = scatter_map(inputs)<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py"", line 15, in scatter_map<NewLine>    return list(zip(*map(scatter_map, obj)))<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py"", line 17, in scatter_map<NewLine>    return list(map(list, zip(*map(scatter_map, obj))))<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py"", line 19, in scatter_map<NewLine>    return list(map(type(obj), zip(*map(scatter_map, obj.items()))))<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py"", line 15, in scatter_map<NewLine>    return list(zip(*map(scatter_map, obj)))<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py"", line 13, in scatter_map<NewLine>    return Scatter.apply(target_gpus, None, dim, obj)<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py"", line 88, in forward<NewLine>    streams = [_get_stream(device) for device in target_gpus]<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py"", line 88, in &lt;listcomp&gt;<NewLine>    streams = [_get_stream(device) for device in target_gpus]<NewLine>  File ""/vol/.conda/envs/.env36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py"", line 115, in _get_stream<NewLine>    if _streams[device] is None:<NewLine>TypeError: list indices must be integers or slices, not range<NewLine></code></pre><NewLine><p>As far as I know that if the input of model is tensor data, there will be no problem to train model on mutliple GPUs distributedly. Might it be possible that a list is employed to pass the data in model.forward() methods.</p><NewLine><p>It works if I launch the model only on single GPU.</p><NewLine><p>Thanks in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/Anakin,(Anakin),Anakin,"July 15, 2020, 10:20am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I made a typo for passing devices : instead of <code>torch.cuda.set_device(args.local_rank)</code>, I passed wrong parameter to <code>torch.cuda.set_device(range(2))</code>.</p><NewLine><p>After fixing this typo, I still have the same problem as posted as <a href=""https://discuss.pytorch.org/t/running-model-on-multiple-gpus-runtimeerror-caught-runtimeerror-in-replica-0-on-device-0/88687/2"">How to scatter list data on multiple GPUs</a></p><NewLine><p>Thanks for any inputs.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Checked the <code>scatter</code> implementation, and looks like it can scatter tensors in dictionaries properly. What is the structure of the <code>blobs</code> var that <code>scatter</code> fails to handle?</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/13dd53b3d2ba16d353ff1fe3c535c9dd79c19e8d/torch/nn/parallel/scatter_gather.py#L5-L31"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/13dd53b3d2ba16d353ff1fe3c535c9dd79c19e8d/torch/nn/parallel/scatter_gather.py#L5-L31"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/13dd53b3d2ba16d353ff1fe3c535c9dd79c19e8d/torch/nn/parallel/scatter_gather.py#L5-L31</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""5"" style=""counter-reset: li-counter 4 ;""><NewLine><li>def scatter(inputs, target_gpus, dim=0):</li><NewLine><li>    r""""""</li><NewLine><li>    Slices tensors into approximately equal chunks and</li><NewLine><li>    distributes them across given GPUs. Duplicates</li><NewLine><li>    references to objects that are not tensors.</li><NewLine><li>    """"""</li><NewLine><li>    def scatter_map(obj):</li><NewLine><li>        if isinstance(obj, torch.Tensor):</li><NewLine><li>            return Scatter.apply(target_gpus, None, dim, obj)</li><NewLine><li>        if isinstance(obj, tuple) and len(obj) &gt; 0:</li><NewLine><li>            return list(zip(*map(scatter_map, obj)))</li><NewLine><li>        if isinstance(obj, list) and len(obj) &gt; 0:</li><NewLine><li>            return list(map(list, zip(*map(scatter_map, obj))))</li><NewLine><li>        if isinstance(obj, dict) and len(obj) &gt; 0:</li><NewLine><li>            return list(map(type(obj), zip(*map(scatter_map, obj.items()))))</li><NewLine><li>        return [obj for targets in target_gpus]</li><NewLine><li><NewLine></li><li>    # After scatter_map is called, a scatter_map cell will exist. This cell</li><NewLine><li>    # has a reference to the actual function scatter_map, which has references</li><NewLine><li>    # to a closure that has a reference to the scatter_map cell (because the</li><NewLine></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/13dd53b3d2ba16d353ff1fe3c535c9dd79c19e8d/torch/nn/parallel/scatter_gather.py#L5-L31"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><pre><code class=""lang-python"">&gt;&gt;&gt; x = {1:1, 2:2}<NewLine>&gt;&gt;&gt; x.items()<NewLine>dict_items([(1, 1), (2, 2)])<NewLine>&gt;&gt;&gt; import torch<NewLine>&gt;&gt;&gt; from torch.nn.parallel.scatter_gather import scatter<NewLine>&gt;&gt;&gt; scatter(x, target_gpus=[0, 1])<NewLine>[{1: 1, 2: 2}, {1: 1, 2: 2}]<NewLine>&gt;&gt;&gt; y = {1: torch.zeros(4, 4).to(0), 2: torch.zeros(4, 4).to(0)}<NewLine>&gt;&gt;&gt; scatter(y, target_gpus=[0, 1])<NewLine>[{1: tensor([[0., 0., 0., 0.],<NewLine>        [0., 0., 0., 0.]], device='cuda:0'), 2: tensor([[0., 0., 0., 0.],<NewLine>        [0., 0., 0., 0.]], device='cuda:0')}, {1: tensor([[0., 0., 0., 0.],<NewLine>        [0., 0., 0., 0.]], device='cuda:1'), 2: tensor([[0., 0., 0., 0.],<NewLine>        [0., 0., 0., 0.]], device='cuda:1')}]<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>, Thanks for your reply.</p><NewLine><p>I solved this issue already.  Basically,  there are two methods (three, plus your idea:D).<br/><NewLine>These ideas are mostly based on  <code>DistributedSampler</code> and <code>DistributedDataParallel</code> . Using the <code>DistributedSampler</code>, a subset of data can be loaded correctly in this process.</p><NewLine><p>We can either use <code>torch.multiprocessing</code> to spawn a process manually to launch our training procedure,</p><NewLine><pre><code class=""lang-auto"">def main():<NewLine>   # Args defintion and loading<NewLine>    os.environ['MASTER_ADDR'] = 'localhost'<NewLine>    os.environ['MASTER_PORT'] = '8080'<NewLine>    mp.spawn(train, nprocs=args.num_gpus, args=(args,))<NewLine><NewLine>def train(gpu, args):<NewLine>    # Initialize the distributed package&amp;group<NewLine>    torch.distributed.init_process_group(backend='nccl', init_method='env://', world_size=args.world_size, rank=args.local_rank)<NewLine>    torch.manual_seed(0)<NewLine><NewLine>    # Initial model<NewLine>    model = RCNN(cfg)<NewLine>    torch.cuda.set_device(gpu)<NewLine>    model.cuda(gpu)<NewLine>    <NewLine>    # Wrap the model<NewLine>    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])<NewLine><NewLine>    # Initial dataset<NewLine>    dataset = Dataset(cfg, is_train=True, split=""train"")<NewLine><NewLine>    # Kick-off iteration<NewLine>    for epoch in range(cfg.TRAIN.MAX_EPOCH):<NewLine>        loader = setup_dataloader(dataset,  is_distributed=True, epoch)<NewLine>            <NewLine>        for _, blobs in enumerate(loader):<NewLine>             loss_dict = model.forward(blobs)<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    main()<NewLine></code></pre><NewLine><p>Passing args by using:</p><NewLine><pre><code class=""lang-auto"">python3  tools/train_mp.py  --num_gpus 2<NewLine></code></pre><NewLine><p>or we can use what torch already encapsulated:</p><NewLine><pre><code class=""lang-auto"">class TrainingWrapper(object):<NewLine>    def __init__(self, args):<NewLine>        self.setup_logging()<NewLine>        self.args = args<NewLine><NewLine>        # Initialize the distributed package&amp;group<NewLine>        self.num_gpus = torch.cuda.device_count()<NewLine>        print(""world_size:%d\local_rank:%d"" % (args.num_gpus, args.local_rank))<NewLine>        self.distributed = self.num_gpus &gt; 1<NewLine>        if self.distributed:<NewLine>            torch.distributed.init_process_group(<NewLine>                backend=""nccl"",<NewLine>                init_method=""env://"",<NewLine>                world_size=args.num_gpus,<NewLine>                rank=args.local_rank<NewLine>            )<NewLine>        self.device = args.local_rank <NewLine>        <NewLine>       # This line is very important!<NewLine>        torch.cuda.set_device(self.device)<NewLine><NewLine>        # Initial model<NewLine>        model = RCNN(cfg)<NewLine><NewLine>        # Distributed learning<NewLine>        if self.distributed:<NewLine>            model = torch.nn.parallel.DistributedDataParallel(<NewLine>                model.cuda(self.device),<NewLine>                device_ids=[self.args.local_rank],<NewLine>                output_device=[self.args.local_rank],<NewLine>                broadcast_buffers=False<NewLine>            )<NewLine>        else:<NewLine>            model = torch.nn.DataParallel(model).cuda()<NewLine><NewLine>    def train(self):<NewLine><NewLine>        # Initial dataset<NewLine>         dataset = Dataset(cfg, is_train=True, split=""train"")<NewLine><NewLine>        # Kick-off iteration<NewLine>       for epoch in range(cfg.TRAIN.MAX_EPOCH):<NewLine>            loader = setup_dataloader(dataset,  is_distributed=True, epoch)<NewLine>            <NewLine>            for _, blobs in enumerate(loader):<NewLine>                 loss_dict = model.forward(blobs)<NewLine><NewLine>if  __name__ == ""__main__"":<NewLine>    trainer = TrainingWrapper()<NewLine>    trainer.train()<NewLine></code></pre><NewLine><p>And passing following args for this above script:</p><NewLine><pre><code class=""lang-auto"">python3 -m torch.distributed.launch --nproc_per_node=2 tools/train_ddp.py  --exp_id boa --config_file experiments/config.yaml --num_gpus 2<NewLine></code></pre><NewLine><p>As so far, I can just use list of dict objects in order to feed the data in model.<br/><NewLine>Hopefully it helps somebody else somehow.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Anakin; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Anakin; <NewLine> ,"REPLY_DATE 1: July 14, 2020,  1:59pm; <NewLine> REPLY_DATE 2: July 14, 2020,  4:08pm; <NewLine> REPLY_DATE 3: July 15, 2020,  9:39am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
89095,How do I map Joblib&rsquo;s Parallel function to PyTorch&rsquo;s DistributedDataParallel,2020-07-14T05:42:00.274Z,4,160,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have the following code below which uses Joblib’s Parallel and I want to implement this in PyTorch and run it with GPUs. I am reading through PyTorch’s DistributedDataParallel <a href=""https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel"" rel=""nofollow noopener"">documentation</a>, but I can’t seem to figure this out.</p><NewLine><pre><code class=""lang-auto"">import numpy as np<NewLine>import torch<NewLine>from joblib import Parallel, delayed<NewLine>from torch.nn.parallel import DistributedDataParallel as DDP<NewLine><NewLine>X = np.array([[1, 3, 2, 3], [2, 3, 5, 6], [1, 2, 3, 4]])<NewLine>X = torch.DoubleTensor(X).cuda()<NewLine><NewLine>def X_power_func(j):<NewLine>    X_power = X**j<NewLine>    return X_power<NewLine><NewLine>results = Parallel(n_jobs = 4)(delayed(X_power_func)(j) for j in range(8))   # how do I map this to <NewLine>                                                                             # PyTorch's<NewLine>                                                                             # DistributedDataParallel<NewLine></code></pre><NewLine><p>Any help would really be appreciated. Many thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/Leockl,(Leo Chow),Leockl,"July 14, 2020,  5:43am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>use torch.multiprocessing.pool</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/iffix"">@iffiX</a>. Do you know in which situations that we would use <code>torch.multiprocessing</code> and <code>DistributedDataParallel</code>?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>DistributedDataParallel</code> is designed for asynchronously let the model perform forward and backward process, internnaly it synchronously perform gradient reduction and parameter updating.</p><NewLine><p><code>torch.multiprocessing</code> is a simple derivative of the vanilla <code>multiprocessing</code> module, it only replaces the default queue implementation used in the vanilla module, and implements an efficient way to pass around cuda tensors (data remains on gpu, only a pointer to data is passed to subprocess pool workers).</p><NewLine><p>Pool is designed for carrying out general unit tasks by a group of homogeneous workers with no context, such as your:</p><NewLine><pre><code class=""lang-auto"">def X_power_func(j):<NewLine>    X_power = X**j<NewLine>    return X_power<NewLine></code></pre><NewLine><p>Pool is essentially the same as <code>joblib</code></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok many thanks <a class=""mention"" href=""/u/iffix"">@iffiX</a> for the detailed answer.</p><NewLine><p>So essentially use <code>DistributedDataParallel</code> for neural network stuff (which involves things like forward and backward processes) that you want to parallelize, and use <code>torch.multiprocessing</code> for non-neural network stuff that you want to parallelize.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>thats correct! <img alt="":slightly_smiling_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slightly_smiling_face.png?v=9"" title="":slightly_smiling_face:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Leockl; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Leockl; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: July 15, 2020,  5:58am; <NewLine> REPLY_DATE 2: July 15, 2020,  5:39am; <NewLine> REPLY_DATE 3: July 15, 2020,  5:58am; <NewLine> REPLY_DATE 4: July 15, 2020,  6:04am; <NewLine> REPLY_DATE 5: July 15, 2020,  6:04am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
89259,What is the difference between using tensor.cuda() and tensor.to(torch.device(“cuda:0”)),2020-07-15T05:32:05.601Z,3,122,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Using PyTorch, what is the difference between the following two methods in sending a tensor to GPU:</p><NewLine><p>Method 1:</p><NewLine><pre><code class=""lang-auto"">X = np.array([[1, 3, 2, 3], [2, 3, 5, 6], [1, 2, 3, 4]])<NewLine>X = torch.DoubleTensor(X).cuda()<NewLine></code></pre><NewLine><p>Method 2:</p><NewLine><pre><code class=""lang-auto"">X = np.array([[1, 3, 2, 3], [2, 3, 5, 6], [1, 2, 3, 4]])<NewLine>X = torch.DoubleTensor(X)<NewLine><NewLine>device = torch.device(""cuda:0"")<NewLine>X = X.to(device)<NewLine></code></pre><NewLine><p>Similarly, is there any difference in the same two methods above when applied to sending a model to GPU:</p><NewLine><p>Method A:</p><NewLine><pre><code class=""lang-auto"">gpumodel = model.cuda()<NewLine></code></pre><NewLine><p>Method B:</p><NewLine><pre><code class=""lang-auto"">device = torch.device(""cuda:0"")<NewLine>gpumodel = model.to(device)<NewLine></code></pre><NewLine><p>Many thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/Leockl,(Leo Chow),Leockl,"July 15, 2020,  5:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>there is no difference</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>There might be a difference, if you were resetting the default CUDA device via <code>torch.cuda.set_device()</code> as seen in this code snippet:</p><NewLine><pre><code class=""lang-python"">torch.cuda.set_device('cuda:1')<NewLine>x = torch.randn(1).cuda()<NewLine>print(x)<NewLine>&gt; tensor([0.9038], device='cuda:1') # uses the default device now<NewLine><NewLine>y = torch.randn(1).to('cuda:0')<NewLine>print(y)<NewLine>&gt; tensor([-0.7296], device='cuda:0') # explicitly specify cuda:0<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok thanks <a class=""mention"" href=""/u/iffix"">@iffiX</a> for confirming they are both essentially doing the same thing.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah yes, thats important, I forgot this <img alt="":slightly_smiling_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slightly_smiling_face.png?v=9"" title="":slightly_smiling_face:""/></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok many thanks <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> for the more detailed answer where the 2nd method is specifying which GPU device to use and the 1st method is just using the default GPU device.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Leockl; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Leockl; <NewLine> ,"REPLY_DATE 1: July 15, 2020,  5:54am; <NewLine> REPLY_DATE 2: July 15, 2020,  5:53am; <NewLine> REPLY_DATE 3: July 15, 2020,  5:55am; <NewLine> REPLY_DATE 4: July 15, 2020,  5:56am; <NewLine> REPLY_DATE 5: July 15, 2020,  5:58am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
55725,How to check if irecv got a message?,2019-09-11T18:12:55.280Z,0,148,"<div class=""post"" itemprop=""articleBody""><NewLine><p>say I have:</p><NewLine><pre><code class=""lang-auto"">t = tensor.torch(0)<NewLine>req = dist.irecv(t, rank)<NewLine></code></pre><NewLine><p>how can i check if i have messages from some rank (stored in buffer or something). From c++ examples ive seen there are probe commands:</p><NewLine><pre><code class=""lang-auto"">MPI_Probe(0, 0, MPI_COMM_WORLD, &amp;status);<NewLine></code></pre><NewLine><p>to check message length.<br/><NewLine>One possible solution would be to check if tensor != 0 but that doesnt seem to be the right way to do it</p><NewLine></div>",https://discuss.pytorch.org/u/maris_ancans,(Maris Ancans),maris_ancans,"September 11, 2019,  6:13pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you get a better solution?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>We probably should set the <code>complete_</code> flag in the send/recv work object and expose that as a Python API. Similar issue exists in our <a href=""https://pytorch.org/docs/master/futures.html"" rel=""nofollow noopener""><code>Future API</code></a>.</p><NewLine><p><a class=""mention"" href=""/u/yanli_zhao"">@Yanli_Zhao</a> mentioned that this is also asked in <a href=""https://github.com/pytorch/pytorch/issues/30723"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/30723</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>added an issue to track: <a href=""https://github.com/pytorch/pytorch/issues/41428"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/41428</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Yong-gui; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: July 14, 2020,  6:49pm; <NewLine> REPLY_DATE 2: July 14, 2020,  8:55pm; <NewLine> REPLY_DATE 3: July 14, 2020,  9:27pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
89096,Getting RuntimeError when running the parameter server tutorial,2020-07-14T05:45:50.379Z,0,137,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there,</p><NewLine><p>I’m trying to run <a href=""https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html"" rel=""nofollow noopener"">this</a> tutorial locally for one parameter server and two workers.</p><NewLine><p>The problem is I’m getting the below error:<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “/usr/lib/python3.6/multiprocessing/process.py”, line 258, in _bootstrap<br/><NewLine>self.run()<br/><NewLine>File “/usr/lib/python3.6/multiprocessing/process.py”, line 93, in run<br/><NewLine>self._target(*self._args, **self._kwargs)<br/><NewLine>File “rpc_parameter_server.py”, line 228, in run_worker<br/><NewLine>run_training_loop(rank, num_gpus, train_loader, test_loader)<br/><NewLine>File “rpc_parameter_server.py”, line 187, in run_training_loop<br/><NewLine>dist_autograd.backward(cid, [loss])<br/><NewLine>RuntimeError: Error on Node 0: one of the variables needed for gradient computation has been modified by an inplace operation: [CPUFloatType [32, 1, 3, 3]] is at version 5; expected version 4 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).</p><NewLine><p>Here’s my torch version if needed:<br/><NewLine>pip3 freeze | grep torch<br/><NewLine>torch==1.5.1+cpu<br/><NewLine>torchtext==0.6.0<br/><NewLine>torchvision==0.6.1+cpu</p><NewLine><p>Thanks in advance for any advice!</p><NewLine></div>",https://discuss.pytorch.org/u/akrisilias21,(Andreas Krisilias),akrisilias21,"July 14, 2020,  5:48am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/rvarm1"">@rvarm1</a>, I wonder if we need a lock in <code>ParameterServer.forward</code>, otherwise if the execution of <code>forward</code> got sliced into multiple pieces, interleaving execution from different RPC threads could mess up the autograd graph state?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: July 14, 2020,  3:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
88830,Custom Parameter Server (PS) not improving,2020-07-11T20:53:05.225Z,2,130,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to train over a custom parameter server, but it checks all the boxes for setting weights and updating gradients… but for some reason it won’t improve accuracy over my data.</p><NewLine><p>I’m using redisai as my database to host all my training/test data and global and all my worker weights/grads. I know the opinion of using an external database is a performance hit, but this is something I want to do for my own experience.</p><NewLine><p>The PS holds the global model that the workers read from to their local model and update before pushing gradients back to the PS to update the global model.</p><NewLine><p>PS:<br/><NewLine>Setting weights:</p><NewLine><pre><code class=""lang-auto"">for k,v in model.state_dict().items():<NewLine>    conn.tensorset(k,v.cpu().numpy())<NewLine></code></pre><NewLine><p>Getting gradients:</p><NewLine><pre><code class=""lang-auto"">msd = model.state_dict()<NewLine>for name, param in model.named_parameters():<NewLine>    msd[name].grad = conn.tensorget(f'{name}_grad')<NewLine>model.load_state_dict(msd)<NewLine>optimizer.step()<NewLine>optimizer.zero_grad()<NewLine></code></pre><NewLine><p>Worker:<br/><NewLine>Getting weights:</p><NewLine><pre><code class=""lang-auto"">lmsd = model.state_dict()<NewLine>for k,v in model.state_dict().items():<NewLine>    lmsd[name].data.copy_(conn.tensorget(f'{name}_data')<NewLine>model.load_state_dict(lmsd)<NewLine></code></pre><NewLine><p>Setting grads:</p><NewLine><pre><code class=""lang-auto"">for name, param in model.named_parameters():<NewLine>    conn.tensorset(f'{name}_grad', param.grad.data.cpu().numpy())<NewLine></code></pre><NewLine><p>I can’t honestly figure out why my global model won’t improve.</p><NewLine><p>I have a work around that involves the global model making a single backward pass after setting the gradients from the worker model (as if to accumulate them) and that seems to be working; but I can’t fully understand who or why.</p><NewLine><pre><code class=""lang-auto"">msd = model.state_dict()<NewLine>for name, param in model.named_parameters():<NewLine>    msd[name].grad = conn.tensorget(f'{name}_grad')<NewLine>model.load_state_dict(msd)<NewLine># build a single batch input<NewLine>out = model(input)<NewLine>criterion(out, label).backward()<NewLine>optimizer.step()<NewLine>optimizer.zero_grad()<NewLine></code></pre><NewLine><p>Does the grad_fn need to be retained for the optimizer to make an update to the weights? I didn’t think it did and that was only at the gradient setting level during the backward pass.</p><NewLine><p>Making a backward pass in the PS seems counter intuitive to the purpose and general workflow of the PS.</p><NewLine><p>Hopefully somebody has some insight as to why the PS is not improving without a backward pass.</p><NewLine></div>",https://discuss.pytorch.org/u/bPangolin,(Benjamin),bPangolin,"July 11, 2020,  9:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The first evident error in your code is:</p><NewLine><pre><code class=""lang-auto"">msd = model.state_dict()<NewLine>for name, param in model.named_parameters():<NewLine>    msd[name].grad = conn.tensorget(f'{name}_grad')<NewLine>model.load_state_dict(msd)<NewLine></code></pre><NewLine><p>Since <code>load_state_dict</code> will not load the gradient, but only load registered <code>nn.Parameter()</code> and buffers of model, you will have to iterate parameters of your model directly and put gradients there like:</p><NewLine><pre><code class=""lang-auto"">                ...<NewLine>                # Assign gradients to the managed model and<NewLine>                # perform optimization.<NewLine>                if self.model is not None and self.optimizer is not None:<NewLine>                    self.optimizer.zero_grad()<NewLine>                    with t.no_grad():<NewLine>                        for k, v in self.model.parameters():<NewLine>                            v.grad = grad_dict[k].to(v.device)<NewLine>                    self.optimizer.step()<NewLine></code></pre><NewLine><p>Therefore, the “solution” you have discovered is basically performing a optimization step on your “PS” end, but your pushed gradients are not utilized.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, <a class=""mention"" href=""/u/iffix"">@iffiX</a><br/><NewLine>That definitely helps leverage the workers gradients - the model is able to find solutions A LOT faster now the model is actually able to explore different possibilities.</p><NewLine><p>I tried it without the backward step at the PS and it causes predictions to become <code>nan</code>. However, if I keep the backward step in and <em>actually</em> (now) accumulate the gradients from the worker and the single batch in the PS, then the model is able to find solutions.</p><NewLine><p>Once setting the gradient directly into the global model, technically I should be able to perform an optimizer step and update the global weights.</p><NewLine><p>Any idea why the predictions result in <code>nan</code> when there isn’t a backward step in the PS?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>According to my observation, <code>nan</code> could be caused by a lot of things: inappropriately designed reward, the model itself, invalid input data etc.</p><NewLine><p>I would suggest you print “.grad” attribute for every parameter out, along with their comming source (worker rank), normally before your <code>nan</code> occurs you will see several ridiculously large gradients like 1e8 or 1e11.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think I fixed that issue. I didn’t seem to be passing the right weights from the PS to the workers.</p><NewLine><pre><code class=""lang-auto"">for k, v in self.model.state_dict().items():<NewLine>    self.conn.tensorset(k, v.cpu().numpy())<NewLine></code></pre><NewLine><p>Which is what I wrote above, but I actually had <code>v.data.cpu().numpy()</code> in my code.</p><NewLine><p>Now my problem is that my model gives empty predicts.</p><NewLine><p>I am distributing a UNet for image segmentation. I’m utilizing gradient accumulation as a tradeoff for the framework performance using a redisai db. Even though the weights and gradients are being passed properly now, my model is decreasing it’s loss… but also accuracy (IoU) md resulting in empty predictions.</p><NewLine><p>It might be the gradient accumulation and batch normalization or it could be the ReLU activations on the decoding half of the model.</p><NewLine><p>Unless someone knows how UNets nuances perform over a distributed framework?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hmmm, I have not tested UNet in a distributed scenario, that’s the realm of real scientific studies <img alt="":blush:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/blush.png?v=9"" title="":blush:""/><br/><NewLine>Maybe reddit of comuper vision is a better place? Or try to ask a new question in the “comuter vision” block of the pytorch forum?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote quote-modified"" data-post=""1"" data-topic=""89082""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v4/letter/b/3bc359/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/distributed-unet-over-parameter-server-output-empty/89082"">Distributed UNet over Parameter Server Output Empty</a> <a class=""badge-wrapper bullet"" href=""/c/vision/5""><span class=""badge-category-bg"" style=""background-color: #AB9364;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""Topics related to either pytorch/vision or vision research related topics"">vision</span></a><NewLine></div><NewLine><blockquote><NewLine>    I have distributed a custom UNet over a parameter server (PS) (over a network and not locally distributed). The workers receive weights, compute the gradient and push it back to the PS. <NewLine>I am accumulating gradients on workers to minimize network usage. After a few gradient pushes, the UNet starts outputting empty predicting as loss decreases but so does accuracy (testing the framework with interval model testing). <NewLine>import torch.nn as nn<NewLine>import torch<NewLine>import torch.nn.functional as F<NewLine>import torch.q…<NewLine>  </blockquote><NewLine></aside><NewLine><p>Link to that question for anyone in the future’s use.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/bPangolin; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/bPangolin; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/bPangolin; <NewLine> ,"REPLY_DATE 1: July 12, 2020,  9:46am; <NewLine> REPLY_DATE 2: July 12, 2020, 10:07pm; <NewLine> REPLY_DATE 3: July 13, 2020,  2:34am; <NewLine> REPLY_DATE 4: July 14, 2020,  2:12am; <NewLine> REPLY_DATE 5: July 14, 2020,  2:34am; <NewLine> REPLY_DATE 6: July 14, 2020,  2:58am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> 
88000,Why does applying DataParallel to an Embedding sometimes lead to &ldquo;incomplete&rdquo; batch parts?,2020-07-05T10:44:22.264Z,3,100,"<div class=""post"" itemprop=""articleBody""><NewLine><h2><NewLine><img alt="":bug:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/bug.png?v=9"" title="":bug:""/> Bug</h2><NewLine><p>I have only a functional description of the bug. Still trying to make a MWE.</p><NewLine><p>Sometimes when using an <code>Embedding</code> in a model with <code>DataParallel</code> I hit errors like so:</p><NewLine><pre><code class=""lang-auto"">x = ... # shape: [&lt;batch part&gt;, &lt;something&gt;]<NewLine>y = self.embed(x) # shape: [&lt;WRONG batch part&gt;, &lt;something&gt;, &lt;emb dim&gt;]<NewLine>#^ the return value of the Embedding is ""incomplete"" on some GPUs.<NewLine># Noticed it most on cuda:1, but that shouldn't matter.<NewLine># E.g. &lt;batch&gt; = 2000, &lt;wrong batch&gt; = 11<NewLine></code></pre><NewLine><h2>To Reproduce</h2><NewLine><p>Steps to reproduce the behavior:</p><NewLine><ol><NewLine><li>Have a model with <code>Embedding</code>.</li><NewLine><li>Use <code>DataParallel</code> on the model such that you’re close to saturating your system.</li><NewLine><li>Things shouldn’t end up working, if the bug is reproducible.</li><NewLine><li>You don’t need anything fancy. Just some pretrained model in <code>eval</code> mode and pass it some input.</li><NewLine></ol><NewLine><h2>Expected behavior</h2><NewLine><p>DataParallel models behave functionally identically to normal models. Convergence and gradient descent questions notwithstanding.</p><NewLine><h2>Environment</h2><NewLine><ul><NewLine><li>PyTorch Version (e.g., 1.0): 1.5 ~ 1.6.x-dev</li><NewLine><li>OS (e.g., Linux): Ubuntu 16.04 LTS</li><NewLine><li>How you installed PyTorch (<code>conda</code>, <code>pip</code>, source): conda</li><NewLine><li>Build command you used (if compiling from source): N/A</li><NewLine><li>Python version: 3.7.7</li><NewLine><li>CUDA/cuDNN version: 10.2</li><NewLine><li>GPU models and configuration: Geforce GTX Titan X * 2</li><NewLine><li>Any other relevant information: N/A</li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/Enamex,,Enamex,"July 5, 2020, 10:44am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/enamex"">@Enamex</a>, I cannot reproduce the error with the following code (mostly borrowed from <a href=""https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"" rel=""nofollow noopener"">this tutorial</a>).<br/><NewLine>Could you please share a min repro of this error? Thanks!</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine><NewLine>torch.manual_seed(1)<NewLine><NewLine><NewLine>CONTEXT_SIZE = 2<NewLine>EMBEDDING_DIM = 10<NewLine># We will use Shakespeare Sonnet 2<NewLine>test_sentence = """"""When forty winters shall besiege thy brow,<NewLine>And dig deep trenches in thy beauty's field,<NewLine>Thy youth's proud livery so gazed on now,<NewLine>Will be a totter'd weed of small worth held:<NewLine>Then being asked, where all thy beauty lies,<NewLine>Where all the treasure of thy lusty days;<NewLine>To say, within thine own deep sunken eyes,<NewLine>Were an all-eating shame, and thriftless praise.<NewLine>How much more praise deserv'd thy beauty's use,<NewLine>If thou couldst answer 'This fair child of mine<NewLine>Shall sum my count, and make my old excuse,'<NewLine>Proving his beauty by succession thine!<NewLine>This were to be new made when thou art old,<NewLine>And see thy blood warm when thou feel'st it cold."""""".split()<NewLine># we should tokenize the input, but we will ignore that for now<NewLine># build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)<NewLine>trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])<NewLine>            for i in range(len(test_sentence) - 2)]<NewLine># print the first 3, just so you can see what they look like<NewLine>print(trigrams[:3])<NewLine><NewLine>vocab = set(test_sentence)<NewLine>word_to_ix = {word: i for i, word in enumerate(vocab)}<NewLine><NewLine><NewLine>class NGramLanguageModeler(nn.Module):<NewLine><NewLine>    def __init__(self, vocab_size, embedding_dim, context_size):<NewLine>        super(NGramLanguageModeler, self).__init__()<NewLine>        self.embeddings = nn.Embedding(vocab_size, embedding_dim)<NewLine>        self.linear1 = nn.Linear(context_size * embedding_dim, 128)<NewLine>        self.linear2 = nn.Linear(128, vocab_size)<NewLine><NewLine>    def forward(self, inputs):<NewLine>        embeds = self.embeddings(inputs).view((1, -1))<NewLine>        out = F.relu(self.linear1(embeds))<NewLine>        out = self.linear2(out)<NewLine>        log_probs = F.log_softmax(out, dim=1)<NewLine>        return log_probs<NewLine><NewLine><NewLine>losses = []<NewLine>loss_function = nn.NLLLoss()<NewLine>model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)<NewLine>optimizer = optim.SGD(model.parameters(), lr=0.001)<NewLine><NewLine>model = torch.nn.DataParallel(model.to(""cuda:0""))<NewLine><NewLine>for epoch in range(10):<NewLine>    total_loss = 0<NewLine>    for context, target in trigrams:<NewLine><NewLine>        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words<NewLine>        # into integer indices and wrap them in tensors)<NewLine>        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)<NewLine>        context_idxs = torch.stack([context_idxs, context_idxs])<NewLine><NewLine>        # Step 2. Recall that torch *accumulates* gradients. Before passing in a<NewLine>        # new instance, you need to zero out the gradients from the old<NewLine>        # instance<NewLine>        model.zero_grad()<NewLine><NewLine>        # Step 3. Run the forward pass, getting log probabilities over next<NewLine>        # words<NewLine>        log_probs = model(context_idxs.to(""cuda:0"")).cpu()<NewLine><NewLine>        # Step 4. Compute your loss function. (Again, Torch wants the target<NewLine>        # word wrapped in a tensor)<NewLine>        #loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))<NewLine><NewLine>        # Step 5. Do the backward pass and update the gradient<NewLine>        log_probs.sum().backward()<NewLine>        optimizer.step()<NewLine><NewLine>        # Get the Python number from a 1-element Tensor by calling tensor.item()<NewLine>        #total_loss += loss.item()<NewLine>    #losses.append(total_loss)<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ll try to get an example written today!<br/><NewLine>If it helps, the models I was trying to evaluate are fairseq-based (I had to dive a bit to get the actual nn.Module out from underneath the tasks…) and are loaded using <code>Model.from_pretrained(...)</code>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>On further investigation, it seems to be a problem with LSTM/RNN?</p><NewLine><p>They’re getting split on the sequence dimension instead of the batch dimension, when in <code>batch_first=False</code> mode. I don’t own the module I’m trying to run in parallel, and this error is its guts, so not sure where to go from there.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/ngimel"">@ngimel</a> for LSTM + DataParallel questions.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Enamex; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Enamex; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: July 5, 2020, 10:22pm; <NewLine> REPLY_DATE 2: July 11, 2020,  9:19am; <NewLine> REPLY_DATE 3: July 13, 2020,  3:32am; <NewLine> REPLY_DATE 4: July 13, 2020,  3:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
88912,Active learning and distributed data parallel,2020-07-12T22:14:07.065Z,2,85,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I recently started looking into using pytorch for an active learning project and have some queries in using distributed data parallel. As per my understanding, distributed data parallel creates a process per gpu to run the model. Active learning approaches usually have two stages: annotation and training. In the annotation stage, I need to run the model on the unannotated samples and get annotations for selected samples and in the training stage, the model needs to be trained on the annotated dataset.  I am trying to use DistributedDataParallel but am not sure how to.</p><NewLine><p>My workflow will roughly look like:</p><NewLine><ol><NewLine><li>Run the model on unannotated samples (across all gpus)</li><NewLine><li>rank samples using the output from previous step based on some criterion (main worker, rank 0)</li><NewLine><li>add new samples to the training dataset (all workers’ data loaders need to be updated) and then run training.</li><NewLine></ol><NewLine><p>This cycle will repeat till a budget is hit (that budget needs to be synced across all processes in order to be able to terminate them).</p><NewLine><p>I am not clear on how I get the output of the model from all processes (step 1) back into the main process, run things like the sample ranking (step 2) on the main process while the other processes wait for the input from the main process before starting the training cycle again. Also, how do I sync variables like budget across processes?  It would be great if I can get a pointer to a resource which helps me understand this more.  I have looked at the imagenet training script in the github repo but that didn’t help me understand this process.</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/abhay,,abhay,"July 13, 2020,  1:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have been looking into the distributed operations defined <a href=""https://pytorch.org/docs/stable/distributed.html"" rel=""nofollow noopener"">here</a>. Are operations like gather and reduce blocking? For example, after running the model on unlabeled data, I can call gather to get them on a single host. If I do that, do the other processes get blocked as well. The main process then needs to select samples for the labeled set and then broadcast them to all the processes. How do I block the other processes to receive this broadcast before beginning the training cycle?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The collective communications are designed for tightly coupled processes, to give you a better idea of how they work, I borrowed some images from mpi:<br/><NewLine><img alt=""image"" data-base62-sha1=""ecKYAfMKAvBdma2Jr47g32U0GP3"" height=""154"" src=""https://discuss.pytorch.org/uploads/default/original/3X/6/3/638f9f193a3998a9d1251ce140adebb277743a05.png"" width=""280""/><br/><NewLine><img alt=""image"" data-base62-sha1=""3ylyBEGFhgRL87KpxBY0OOF1WyN"" height=""340"" src=""https://discuss.pytorch.org/uploads/default/original/3X/1/8/18e87c687f2ad849b761c0e68e74375da9f0491d.png"" width=""287""/><br/><NewLine><img alt=""image"" data-base62-sha1=""fnCuu3RnN96pGC1oebWT4FLHEUK"" height=""169"" src=""https://discuss.pytorch.org/uploads/default/original/3X/6/b/6bcc27f3ec011808d576480906a657b51c40e156.png"" width=""211""/><br/><NewLine>Each collective communication primitive is a <strong>blocking</strong> process for <strong>all processes</strong> in your created processgroup (whether it is the global WORLD group or a sub group of the global group).</p><NewLine><p>I would suggest you do it in the following way:</p><NewLine><pre><code class=""lang-auto"">import torch.distributed as dist<NewLine><NewLine># make sure all processes in your_group will run this step<NewLine>model = (model, ..., output_device=your_device, process_group=your_group)<NewLine>your input per process= ...<NewLine><NewLine># output is only visible to that process<NewLine># parameters are synchronized behind the stage<NewLine>output = model(your_input) <NewLine><NewLine># make sure all processes in your_group will run this step<NewLine>dist.gather(..., dst=0)<NewLine># perform ranking on process 0<NewLine>dist.scatter(..., src=0)<NewLine><NewLine># all workers are now synchronized<NewLine></code></pre><NewLine><p>One <strong>crutial</strong> thing to notice: the receive buffer (tensor) must be equal or larger than the tensor you have sent, otherwise nasty errors will be thrown.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much for the images! I was confused with some of the collective operations and that image really helped clear things up.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/abhay; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/abhay; <NewLine> ,"REPLY_DATE 1: July 13, 2020,  1:37am; <NewLine> REPLY_DATE 2: July 13, 2020,  4:51am; <NewLine> REPLY_DATE 3: July 13, 2020,  4:51am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
88726,My multi-GPU training OAR job keeps being killed,2020-07-10T20:01:10.119Z,3,235,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I encountered a very strange problem that repeatedly happened.</p><NewLine><p>I have a training started with the following command on a Linux server:</p><NewLine><pre><code class=""lang-auto"">oarsub -l ""host=1/gpuid=4,walltime=480:0:0"" \<NewLine>""/home/username/.env/py37/bin/python -m torch.distributed.launch --nproc_per_node=4 --use_env main.py --coco_path /data/coco --output_dir /home/username/code/output --resume /home/username/code/output/checkpoint.pth""<NewLine></code></pre><NewLine><p>After a few hours, the training was killed. And this happened every time I restarted it. Our system admin could not figure out what was wrong.</p><NewLine><p>The std error messages (content of <code>OAR.&lt;jobID&gt;.stderr</code>) are the following:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""/home/username/.local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main<NewLine>    ""__main__"", mod_spec)<NewLine>  File ""/home/username/.local/lib/python3.7/runpy.py"", line 85, in _run_code<NewLine>    exec(code, run_globals)<NewLine>  File ""/home/username/.env/py37/lib/python3.7/site-packages/torch/distributed/launch.py"", line 263, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""/home/username/.env/py37/lib/python3.7/site-packages/torch/distributed/launch.py"", line 259, in main<NewLine>    cmd=cmd)<NewLine>subprocess.CalledProcessError: Command '['/home/username/.env/py37/bin/python', '-u', 'main.py', '--coco_path', '/data/coco', '--output_dir', '/home/username/code/output', '--resume', '/home/username/code/output/checkpoint.pth']' died with &lt;Signals.SIGKILL: 9&gt;.<NewLine></code></pre><NewLine><p>In the std output file <code>OAR.&lt;jobID&gt;.stdout</code>, the last lines are the following:</p><NewLine><blockquote><NewLine><hr/><NewLine><p>Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.</p><NewLine><hr/><NewLine></blockquote><NewLine><p>This message is displayed only at the end of <code>OAR.&lt;jobID&gt;.stdout</code>, when the crash happened, so maybe it has something to do with the crash.</p><NewLine><p>Could you please help? Thank you very much in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/f10w,,f10w,"July 10, 2020,  8:01pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""88726"" data-username=""f10w""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/f10w/40/1352_2.png"" width=""20""/> f10w:</div><NewLine><blockquote><NewLine><p>Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.</p><NewLine></blockquote><NewLine></aside><NewLine><p>This is printed immediately after you run <code>launch.py</code>. See the code below:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/0fc0a9308ab1e8f6b65d2565c45f54afb449d438/torch/distributed/launch.py#L215-L222"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/0fc0a9308ab1e8f6b65d2565c45f54afb449d438/torch/distributed/launch.py#L215-L222"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/0fc0a9308ab1e8f6b65d2565c45f54afb449d438/torch/distributed/launch.py#L215-L222</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""215"" style=""counter-reset: li-counter 214 ;""><NewLine><li>if 'OMP_NUM_THREADS' not in os.environ and args.nproc_per_node &gt; 1:</li><NewLine><li>    current_env[""OMP_NUM_THREADS""] = str(1)</li><NewLine><li>    print(""*****************************************\n""</li><NewLine><li>          ""Setting OMP_NUM_THREADS environment variable for each process ""</li><NewLine><li>          ""to be {} in default, to avoid your system being overloaded, ""</li><NewLine><li>          ""please further tune the variable for optimal performance in ""</li><NewLine><li>          ""your application as needed. \n""</li><NewLine><li>          ""*****************************************"".format(current_env[""OMP_NUM_THREADS""]))</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>During the few hours when the job is running, did the <code>DistributedDataParallel</code> training making progress as expected? You can, e.g., print some logs in every iteration to check this.</p><NewLine><p>And since the command line contains a path to checkpoint, I assume the training job would write into that checkpoint file periodically? Did the job successfully generated any checkpoint?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply. The training made progress (and saved checkpoints) for a few hours before being killed.</p><NewLine><p>Here’s the beginning of the log file (<code>head -n 20 OAR.&lt;jobID&gt;.stdout</code>):</p><NewLine><blockquote><NewLine><p>| distributed init (rank 2): env://<br/><NewLine>| distributed init (rank 3): env://<br/><NewLine>| distributed init (rank 0): env://<br/><NewLine>| distributed init (rank 1): env://<br/><NewLine>git:<br/><NewLine>sha: ae03a2d6e52a9ec1b67f85437d0a275c5abbe9ac, status: has uncommited changes, branch: master</p><NewLine><p>Namespace(aux_loss=True, backbone=‘resnet50’, batch_size=2, bbox_loss_coef=5, clip_max_norm=0.1, coco_panoptic_path=None, coco_path=’/data/coco’, dataset_file=‘coco’, dec_layers=6, device=‘cuda’, dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend=‘nccl’, dist_url=‘env://’, distributed=True, dropout=0.1, enc_layers=6, eos_coef=0.1, epochs=300, eval=False, frozen_weights=None, giou_loss_coef=2, gpu=0, hidden_dim=256, lr=0.0001, lr_backbone=1e-05, lr_drop=200, mask_loss_coef=1, masks=False, nheads=8, num_queries=100, num_workers=2, output_dir=’/home/username/code/output’, position_embedding=‘sine’, pre_norm=False, rank=0, remove_difficult=False, resume=’/home/username/code/output/checkpoint.pth’, seed=42, set_cost_bbox=5, set_cost_class=1, set_cost_giou=2, start_epoch=0, weight_decay=0.0001, world_size=4)<br/><NewLine>number of params: 41302368<br/><NewLine>loading annotations into memory…<br/><NewLine>Done (t=22.53s)<br/><NewLine>creating index…<br/><NewLine>index created!<br/><NewLine>loading annotations into memory…<br/><NewLine>Done (t=0.75s)<br/><NewLine>creating index…<br/><NewLine>index created!<br/><NewLine>Start training<br/><NewLine>Epoch: [23]  [    0/14786]  eta: 7:42:07  lr: 0.000100  class_error: 22.68  loss: 10.4300 (10.4300)  loss_bbox: 0.3688 (0.3688)  loss_bbox_0: 0.3812 (0.3812)  loss_bbox_1: 0.4038 (0.4038)  loss_bbox_2: 0.3718 (0.3718)  loss_bbox_3: 0.3781 (0.3781)  loss_bbox_4: 0.3690 (0.3690)  loss_ce: 0.5279 (0.5279)  loss_ce_0: 0.6643 (0.6643)  loss_ce_1: 0.5894 (0.5894)  loss_ce_2: 0.5849 (0.5849)  loss_ce_3: 0.5311 (0.5311)  loss_ce_4: 0.5083 (0.5083)  loss_giou: 0.8055 (0.8055)  loss_giou_0: 0.8359 (0.8359)  loss_giou_1: 0.7730 (0.7730)  loss_giou_2: 0.7711 (0.7711)  loss_giou_3: 0.7646 (0.7646)  loss_giou_4: 0.8013 (0.8013)  cardinality_error_unscaled: 8.8750 (8.8750)  cardinality_error_0_unscaled: 13.2500 (13.2500)  cardinality_error_1_unscaled: 12.7500 (12.7500)  cardinality_error_2_unscaled: 8.1250 (8.1250)  cardinality_error_3_unscaled: 8.1250 (8.1250)  cardinality_error_4_unscaled: 8.6250 (8.6250)  class_error_unscaled: 22.6786 (22.6786)  loss_bbox_unscaled: 0.0738 (0.0738)  loss_bbox_0_unscaled: 0.0762 (0.0762)  loss_bbox_1_unscaled: 0.0808 (0.0808)  loss_bbox_2_unscaled: 0.0744 (0.0744)  loss_bbox_3_unscaled: 0.0756 (0.0756)  loss_bbox_4_unscaled: 0.0738 (0.0738)  loss_ce_unscaled: 0.5279 (0.5279)  loss_ce_0_unscaled: 0.6643 (0.6643)  loss_ce_1_unscaled: 0.5894 (0.5894)  loss_ce_2_unscaled: 0.5849 (0.5849)  loss_ce_3_unscaled: 0.5311 (0.5311)  loss_ce_4_unscaled: 0.5083 (0.5083)  loss_giou_unscaled: 0.4027 (0.4027)  loss_giou_0_unscaled: 0.4180 (0.4180)  loss_giou_1_unscaled: 0.3865 (0.3865)  loss_giou_2_unscaled: 0.3855 (0.3855)  loss_giou_3_unscaled: 0.3823 (0.3823)  loss_giou_4_unscaled: 0.4006 (0.4006)  time: 1.8753  data: 0.4317  max mem: 2509<br/><NewLine>Epoch: [23]  [   10/14786]  eta: 2:39:48  lr: 0.000100  class_error: 30.30  loss: 11.0897 (10.6174)  loss_bbox: 0.3473 (0.3555)  loss_bbox_0: 0.3888 (0.3989)  loss_bbox_1: 0.3834 (0.3796)  loss_bbox_2: 0.3662 (0.3772)  loss_bbox_3: 0.3590 (0.3603)  loss_bbox_4: 0.3520 (0.3548)  loss_ce: 0.5279 (0.5271)  loss_ce_0: 0.6043 (0.6137)  loss_ce_1: 0.5870 (0.5653)  loss_ce_2: 0.5627 (0.5542)  loss_ce_3: 0.5400 (0.5402)  loss_ce_4: 0.5083 (0.5214)  loss_giou: 0.8325 (0.8231)  loss_giou_0: 0.9057 (0.8922)  loss_giou_1: 0.8793 (0.8482)  loss_giou_2: 0.8800 (0.8514)  loss_giou_3: 0.8392 (0.8296)  loss_giou_4: 0.8540 (0.8247)  cardinality_error_unscaled: 9.1250 (10.3182)  cardinality_error_0_unscaled: 13.2500 (13.6477)  cardinality_error_1_unscaled: 12.7500 (12.3068)  cardinality_error_2_unscaled: 9.6250 (10.9091)  cardinality_error_3_unscaled: 9.1250 (10.1705)  cardinality_error_4_unscaled: 9.1250 (10.0341)  class_error_unscaled: 22.6786 (23.9301)  loss_bbox_unscaled: 0.0695 (0.0711)  loss_bbox_0_unscaled: 0.0778 (0.0798)  loss_bbox_1_unscaled: 0.0767 (0.0759)  loss_bbox_2_unscaled: 0.0732 (0.0754)  loss_bbox_3_unscaled: 0.0718 (0.0721)  loss_bbox_4_unscaled: 0.0704 (0.0710)  loss_ce_unscaled: 0.5279 (0.5271)  loss_ce_0_unscaled: 0.6043 (0.6137)  loss_ce_1_unscaled: 0.5870 (0.5653)  loss_ce_2_unscaled: 0.5627 (0.5542)  loss_ce_3_unscaled: 0.5400 (0.5402)  loss_ce_4_unscaled: 0.5083 (0.5214)  loss_giou_unscaled: 0.4162 (0.4116)  loss_giou_0_unscaled: 0.4528 (0.4461)  loss_giou_1_unscaled: 0.4397 (0.4241)  loss_giou_2_unscaled: 0.4400 (0.4257)  loss_giou_3_unscaled: 0.4196 (0.4148)  loss_giou_4_unscaled: 0.4270 (0.4123)  time: 0.6489  data: 0.0493  max mem: 3574</p><NewLine></blockquote><NewLine><p>And here’s the end (<code>tail -n 4 OAR.&lt;jobID&gt;.stdout</code>):</p><NewLine><blockquote><NewLine><p>Epoch: [23]  [ 4730/14786]  eta: 1:07:17  lr: 0.000100  class_error: 40.61  loss: 9.2283 (10.3881)  loss_bbox: 0.3610 (0.3562)  loss_bbox_0: 0.4111 (0.4107)  loss_bbox_1: 0.3788 (0.3752)  loss_bbox_2: 0.3783 (0.3661)  loss_bbox_3: 0.3680 (0.3598)  loss_bbox_4: 0.3660 (0.3571)  loss_ce: 0.4747 (0.5366)  loss_ce_0: 0.5628 (0.6116)  loss_ce_1: 0.5367 (0.5860)  loss_ce_2: 0.5133 (0.5601)  loss_ce_3: 0.4722 (0.5455)  loss_ce_4: 0.4595 (0.5364)  loss_giou: 0.7070 (0.7790)  loss_giou_0: 0.7854 (0.8533)  loss_giou_1: 0.7170 (0.8021)  loss_giou_2: 0.7175 (0.7903)  loss_giou_3: 0.7327 (0.7818)  loss_giou_4: 0.7180 (0.7802)  cardinality_error_unscaled: 7.7500 (9.0408)  cardinality_error_0_unscaled: 10.8750 (11.5247)  cardinality_error_1_unscaled: 10.1250 (11.1548)  cardinality_error_2_unscaled: 8.3750 (9.9196)  cardinality_error_3_unscaled: 7.3750 (9.3645)  cardinality_error_4_unscaled: 7.7500 (9.0276)  class_error_unscaled: 30.4464 (32.1511)  loss_bbox_unscaled: 0.0722 (0.0712)  loss_bbox_0_unscaled: 0.0822 (0.0821)  loss_bbox_1_unscaled: 0.0758 (0.0750)  loss_bbox_2_unscaled: 0.0757 (0.0732)  loss_bbox_3_unscaled: 0.0736 (0.0720)  loss_bbox_4_unscaled: 0.0732 (0.0714)  loss_ce_unscaled: 0.4747 (0.5366)  loss_ce_0_unscaled: 0.5628 (0.6116)  loss_ce_1_unscaled: 0.5367 (0.5860)  loss_ce_2_unscaled: 0.5133 (0.5601)  loss_ce_3_unscaled: 0.4722 (0.5455)  loss_ce_4_unscaled: 0.4595 (0.5364)  loss_giou_unscaled: 0.3535 (0.3895)  loss_giou_0_unscaled: 0.3927 (0.4267)  loss_giou_1_unscaled: 0.3585 (0.4011)  loss_giou_2_unscaled: 0.3587 (0.3952)  loss_giou_3_unscaled: 0.3664 (0.3909)  loss_giou_4_unscaled: 0.3590 (0.3901)  time: 0.4095  data: 0.0113  max mem: 7106</p><NewLine><hr/><NewLine><p>Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.</p><NewLine><hr/><NewLine></blockquote><NewLine><p>The OMP message is at the very end of the log file, so it doesn’t seem to be printed right after launch.</p><NewLine><p>Please let me know if you need further information. Thanks!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>hmm, this is weird. One possibility could be the print buffer from the main process wasn’t full at the beginning and then was only flushed on exit, leading to the message to be shown in the end. But I am not sure if this is case.</p><NewLine><p>One reason for this behavior might be some DDP process hits OOM (or some other error) after a while and crashed, causing other DDP processes to hang. Did you try to do any try-except around DDP in <code>main.py</code>? If so, you might want to try <a href=""https://pytorch.org/elastic"" rel=""nofollow noopener"">https://pytorch.org/elastic</a>, as try-except in one process could lead to DDP communication de-sync/hang/timeout.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>In the code, the <code>print</code> function is tweaked to print on the master process only (and do nothing on the others), so the content of <code>OAR.&lt;jobID&gt;.stdout</code> comes from the master. The thing I’m not sure about is the file <code>OAR.&lt;jobID&gt;.stderr</code>. If there is an OOM error, on <em>any</em> process, then the error message should be added to <code>OAR.&lt;jobID&gt;.stderr</code>, right? Because this has nothing to do with the <code>print</code> function I guess.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/f10w; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/f10w; <NewLine> ,"REPLY_DATE 1: July 11, 2020,  4:00pm; <NewLine> REPLY_DATE 2: July 11, 2020,  4:37pm; <NewLine> REPLY_DATE 3: July 11, 2020,  5:39pm; <NewLine> REPLY_DATE 4: July 11, 2020,  8:55pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
88731,DistributedDataParallel causes Dataloader workers to utilize GPU memory,2020-07-10T21:00:18.888Z,3,225,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone, I’m dealing with a very bizarre problem that I’m not sure how to solve.</p><NewLine><p>I’m finding that whenever I use DistributedDataParallel where <em>each</em> process creates a Dataloader with num_workers &gt; 0 set, I see that in <code>nvidia-smi</code> that several worker processes are spawned that are each utilizing about 500 MiB.</p><NewLine><p>Whenever I don’t use DistributedDataParallel, the only process I see utilizing GPU memory is the main process (no worker processes are shown). I am extremely confident that there is no code in the dataset get methods or collate functions that moves the tensors to GPU memory.</p><NewLine><p>This issue can persist in two different ways. If I wrap my model in DDP first, I notice these processes claim GPU memory when the dataloader is being created. Vice-versa, if I make my dataloader first, I notice these processes claim GPU memory when the model is being wrapped in DDP.</p><NewLine><p>I don’t believe this is expected behavior, I don’t understand why the worker processes would even want GPU memory when they should just be handling fetching the data into RAM. Any guidance on this problem will be appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/ayalaa2,(Alex Ayala),ayalaa2,"July 11, 2020, 12:13am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Continue discussion from <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/should-i-use-spawn-method-to-start-multi-processing/85521"">Should I use 'spawn' method to start multi-processing?</a></p><NewLine><p>Could you please show the output of <code>nvdia-smi</code>? Do you see any process id that appears on both GPUs?</p><NewLine><p>A min repro code will be helpful.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m in the process of reproducing the issue in a separate code-base. In an initial isolated test, I’m actually <strong>not</strong> seeing this issue, but I am seeing it the code-base I’m working on… I’m not able to share that code-base, but I’m going to continue trying to replicate the problem.</p><NewLine><p>For now, here’s a screenshot of what I’m seeing:<br/><NewLine><img alt=""image"" data-base62-sha1=""uThJNPmQWEyBCvhp9IMVb4w2i7C"" height=""197"" src=""https://discuss.pytorch.org/uploads/default/original/3X/d/8/d8811ac1a17338aa9141c83cb7a32ae6d45fd1d0.png"" width=""573""/></p><NewLine><p>3190 and 3257 both look normal to me. The other processes are workers that are spawn that start to utilize GPU memory somehow. I’m not very familiar with the inner-workings of CUDA, but could it be that the worker processes are running into code that thinks it needs a CUDA context? Thus allocating room for it on each worker process?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""88731"" data-username=""ayalaa2""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ayalaa2/40/21195_2.png"" width=""20""/> ayalaa2:</div><NewLine><blockquote><NewLine><p>I’m not very familiar with the inner-workings of CUDA, but could it be that the worker processes are running into code that thinks it needs a CUDA context? Thus allocating room for it on each worker process?</p><NewLine></blockquote><NewLine></aside><NewLine><p>I am not sure, but the size (445MB) does look like CUDA context. cc DataLoader experts <a class=""mention"" href=""/u/vitalyfedyunin"">@VitalyFedyunin</a> <a class=""mention"" href=""/u/simonw"">@SimonW</a> <a class=""mention"" href=""/u/vincentqb"">@vincentqb</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like CUDA context to me. So probably your dataset code somehow uses CUDA. Also, if you are using <code>spawn</code> (default for windows and mac), make sure to wrap all code that may initialize CUDA in <code>if __name__ == '__main__'</code></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have narrowed it down! I found that the dataset object is being assigned a class method from a nn.Module class. <strong>Thus the solution is to not assign a module’s class method to the dataset</strong>. Here is a code snippet that reproduces the problem. NOTE that this problem does not occur if DDP is not used.</p><NewLine><p>I would like to have a high level understanding of this issue. Why would assigning a class method to the dataset cause the worker processes to have CUDA context? Additionally, why is this only occurring when using the DDP module?</p><NewLine><pre><code class=""lang-auto"">import torch.multiprocessing as mp<NewLine>import torch.distributed as dist<NewLine>import torch.nn as nn<NewLine>import torchvision<NewLine>import os<NewLine><NewLine>from torch.nn.parallel import DistributedDataParallel as DDP<NewLine>from torchvision.datasets import MNIST<NewLine>from torch.utils.data import DataLoader<NewLine>import time<NewLine><NewLine>class SimpleModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.linear1 = nn.Linear(10, 10)<NewLine><NewLine>    def forward(self, x):<NewLine>        return x<NewLine>    <NewLine>    def preprocess(self, batch):<NewLine>        return batch<NewLine><NewLine><NewLine>def main():<NewLine>    <NewLine>    mnist_dataset = MNIST(<NewLine>        'mnist', train=True, download=True,<NewLine>        transform=torchvision.transforms.ToTensor()<NewLine>    )<NewLine>    model = SimpleModel()<NewLine>    is_parallel = True<NewLine><NewLine>    if is_parallel:<NewLine>        mp.spawn(train_wrapper, nprocs=2, join=True,<NewLine>                 args=(model, mnist_dataset))<NewLine>    else:<NewLine>        train(model, mnist_dataset, False, 'cuda:1')<NewLine><NewLine><NewLine>def train_wrapper(rank, model, train_data):<NewLine>    os.environ['MASTER_ADDR'] = '127.0.0.1'<NewLine>    os.environ['MASTER_PORT'] = '12345'<NewLine><NewLine>    devices = ['cuda:1', 'cuda:2']<NewLine><NewLine>    dist.init_process_group(backend='nccl', rank=rank, world_size=len(devices))<NewLine>    train(model, train_data, True, devices[rank])<NewLine>    dist.destroy_process_group()<NewLine><NewLine><NewLine>def train(model, train_data, is_parallel, device):<NewLine><NewLine><NewLine>    # NOTE: THIS IS THE PROBLEM LINE<NewLine>    # If you comment this line out, the issue no longer persists<NewLine>    train_data.preprocess = model.preprocess<NewLine>    <NewLine>    train_loader = DataLoader(<NewLine>        train_data,<NewLine>        num_workers=4,<NewLine>        batch_size=16<NewLine>    )<NewLine><NewLine>    model = model.to(device)<NewLine><NewLine>    if is_parallel:<NewLine>        model = DDP(model, device_ids=[device])<NewLine>    <NewLine>    x = iter(train_loader)<NewLine>    time.sleep(10)<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    main()<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ayalaa2; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/SimonW; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ayalaa2; <NewLine> ,"REPLY_DATE 1: July 11, 2020,  4:04pm; <NewLine> REPLY_DATE 2: July 11, 2020,  5:05pm; <NewLine> REPLY_DATE 3: July 11, 2020,  5:43pm; <NewLine> REPLY_DATE 4: July 11, 2020,  5:50pm; <NewLine> REPLY_DATE 5: July 11, 2020,  5:58pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 2 Likes; <NewLine> 
85521,Should I use &lsquo;spawn&rsquo; method to start multi-processing?,2020-06-15T13:27:53.898Z,5,155,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I use <code>torch.distributed </code> to train my model.<br/><NewLine>When I use <code>torch.multiprocessing.set_start_method('spawn')</code>, the gpu usage memory will be increased with the increasing <code>num_workers</code>.<br/><NewLine>However, when I don’t use  <code>torch.multiprocessing.set_start_method('spawn')</code>, the gpu usage memory is consistent with different <code>num_workers</code>.</p><NewLine><p>Therefore, should I use <code>spawn</code> to start multi-processing ?<br/><NewLine>What’s the influence of the <code>set_start_method('spawn')</code> ?<br/><NewLine>Why the increasing <code>num_workers</code> increases the gpu usage memory when <code>spawn</code> mode?</p><NewLine></div>",https://discuss.pytorch.org/u/ycszen,(Ycszen),ycszen,"June 15, 2020,  1:27pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>When using GPU, I believe <code>spawn</code> should be used, as according to this <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html#cuda-in-multiprocessing"" rel=""nofollow noopener"">multiprocessing best practices</a> page, CUDA context (~500MB) does not fork. This could also be the reason why you see increasing GPU memory footprint when using more spawned processes, as each process will have its dedicated CUDA context.</p><NewLine><p>Curious, can you allocate a different GPU to each different process? Or do they have to use the same GPU in your application?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>For distributed parallel training, we do allocate one process for one GPU to train model.</p><NewLine><p>However, in the dataloader, because it also adopts the multi-processing, the increasing <code>num_workers</code> will increase the GPU memory footprint.</p><NewLine><p>In my opinion, the increased GPU memory footprint due to the increasing <code>num_workers</code> can not help to training (faster of better performance).</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If those dataloader processes do not use GPU, I guess they can use fork instead?</p><NewLine><p>cc <a class=""mention"" href=""/u/vincentqb"">@vincentqb</a> for DataLoader questions.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I want to bump this post, I’m having this exact problem right now. Each additional worker my processes are spawning for data loading is resulting in an increase of about 500 MiB <em>per</em> worker.</p><NewLine><p>Does anyone know how to fix this?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""85521"" data-username=""ayalaa2""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ayalaa2/40/21195_2.png"" width=""20""/> ayalaa2:</div><NewLine><blockquote><NewLine><p>Each additional worker my processes are spawning for data loading is resulting in an increase of about 500 MiB <em>per</em> worker.</p><NewLine></blockquote><NewLine></aside><NewLine><p>The 500MB is about the size of a CUDA context. Does those processes use GPUs?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>They shouldn’t.</p><NewLine><p>Specifically, I’m trying to run my code with 2 GPUs, thus I spawn two processes. Each initialize their own DataLoader object with <code>num_workers=2</code>. I find that there’s 6 processes in total using GPU memory, I definitely expect the two main processes to utilize GPU memory but I don’t understand why the dataloader worker processes are also utilizing GPU memory.</p><NewLine><p>If I run my code without DDP, I do not see this issue.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Let’s continue discussions in <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/distributeddataparallel-causes-dataloader-workers-to-utilize-gpu-memory/88731"">DistributedDataParallel causes Dataloader workers to utilize GPU memory</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ycszen; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ayalaa2; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ayalaa2; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 15, 2020,  3:32pm; <NewLine> REPLY_DATE 2: June 16, 2020,  2:56am; <NewLine> REPLY_DATE 3: June 16, 2020,  3:25am; <NewLine> REPLY_DATE 4: July 11, 2020, 12:24am; <NewLine> REPLY_DATE 5: July 11, 2020, 12:31am; <NewLine> REPLY_DATE 6: July 11, 2020, 12:35am; <NewLine> REPLY_DATE 7: July 11, 2020,  4:04pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
88711,Manually reduce gradients from multiple GPUs encounter unexpected crash,2020-07-10T16:47:33.119Z,1,82,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m trying to implement a customized DataParallel, where I want to manually reduce gradients from replicas in multiple GPUs. The reason I’m doing it is that I want each GPU to accumulate gradients for several iterations before doing one reduce gradients operation across multi-GPUs, hence reducing the communication overhead. When using 2 GPUs it seems to work. However, when using 4 GPUs after some iteration the program simply crashes without any error message. I think it’s some lower level C code crashes. Do you have any idea why? Here is my code:<br/><NewLine>`class DataParallelAccumulation(DataParallel):<br/><NewLine>def <strong>init</strong>(self, module, device_ids=None, output_device=None, dim=0):<br/><NewLine>super().<strong>init</strong>(module, device_ids=device_ids, output_device=output_device, dim=dim)<br/><NewLine>if len(self.device_ids) &gt; 1:<br/><NewLine>self.replicas = self.replicate(self.module, self.device_ids, detach=True)</p><NewLine><pre><code>def forward(self, *inputs, **kwargs):<NewLine>    if not self.device_ids:<NewLine>        return self.module(*inputs, **kwargs)<NewLine>    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)<NewLine>    if len(self.device_ids) == 1:<NewLine>        return self.module(*inputs[0], **kwargs[0])<NewLine>    outputs = self.parallel_apply(self.replicas[:len(inputs)], inputs, kwargs)<NewLine>    return outputs<NewLine><NewLine>def reduce_grads(self):<NewLine>    if len(self.device_ids) &gt; 1:<NewLine>        for parameters in zip(self.module.parameters(), *[r.parameters() for r in self.replicas]):<NewLine>            destination_device = parameters[0].get_device()<NewLine>            parameters[0].grad = (comm.reduce_add([p.grad for p in parameters[1:]],<NewLine>                destination=destination_device))<NewLine><NewLine>def synchronize(self):<NewLine>    if len(self.device_ids) &gt; 1:<NewLine>        self.replicas = self.replicate(self.module, self.device_ids, detach=True)<NewLine><NewLine>def replicate(self, module, device_ids, detach=False):<NewLine>    replicas = replicate(module, device_ids, detach=detach)<NewLine>    return replicas<NewLine></code></pre><NewLine><p>`</p><NewLine></div>",https://discuss.pytorch.org/u/Xiaopeng_Li,(Xiaopeng Li),Xiaopeng_Li,"July 10, 2020,  5:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/xiaopeng_li"">@Xiaopeng_Li</a>, which version of PyTorch are you using? After <a href=""https://github.com/pytorch/pytorch/pull/33907"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/33907</a>, the <code>replicate</code> method is no longer supposed to be used this way. It will only replicate non-leaf parameters, and as a result, <code>replicated_model.parameters()</code> will be empty. Can you double check if this is the case in your dev env?</p><NewLine><p>If you really need this, you can try to access the <code>_former_parameters</code> attribute in replicated models. See the code below. But there is no guarantee on how long this attribute can stay.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/0edbe6b063d2525ceaf89f0c603f6e35b3118686/torch/nn/parallel/distributed.py#L347-L357"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/0edbe6b063d2525ceaf89f0c603f6e35b3118686/torch/nn/parallel/distributed.py#L347-L357"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/0edbe6b063d2525ceaf89f0c603f6e35b3118686/torch/nn/parallel/distributed.py#L347-L357</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""347"" style=""counter-reset: li-counter 346 ;""><NewLine><li>def parameters(m, recurse=True):</li><NewLine><li>    def model_parameters(m):</li><NewLine><li>        ps = m._former_parameters.values() \</li><NewLine><li>            if hasattr(m, ""_former_parameters"") \</li><NewLine><li>            else m.parameters(recurse=False)</li><NewLine><li>        for p in ps:</li><NewLine><li>            yield p</li><NewLine><li><NewLine></li><li>    for m in m.modules() if recurse else [m]:</li><NewLine><li>        for p in model_parameters(m):</li><NewLine><li>            yield p</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> Thanks for the reply. I’m using 1.4.0. In this version the replicated_model.parameters() do exist, and I understand that in 1.5.0 it no longer exist.<br/><NewLine>As I mentioned, it works for several iteration of training, but crashes at some point. Every time it crashes at different iterations, pretty weired. Am I doing <code>reduce_grads </code> correctly?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Close this issue because similar issue is already discussed in <a href=""https://github.com/pytorch/pytorch/pull/19577"" rel=""nofollow noopener"">pull-19577</a> and <a href=""https://github.com/pytorch/pytorch/pull/21736"" rel=""nofollow noopener"">pull-21736</a>, and the PyTorch team have worked out the solution, that is using <code>no_sync()</code> context manager.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Xiaopeng_Li; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Xiaopeng_Li; <NewLine> ,"REPLY_DATE 1: July 10, 2020,  5:33pm; <NewLine> REPLY_DATE 2: July 10, 2020,  5:39pm; <NewLine> REPLY_DATE 3: July 11, 2020,  5:00am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
88698,"subprocess.CalledProcessError: Command &lsquo;[&lsquo;/home/amax/miniconda3/bin/python&rsquo;, &lsquo;-u&rsquo;, &lsquo;play.py&rsquo;, &lsquo;&ndash;local_rank=1&rsquo;]&rsquo; returned non-zero exit status 2",2020-07-10T14:48:41.138Z,0,195,"<div class=""post"" itemprop=""articleBody""><NewLine><p>i want to use DDP to train model ,use num 6th,7th gpu.<br/><NewLine>this code core is :</p><NewLine><pre><code class=""lang-auto"">import datetime<NewLine>import torch.utils.data.dataloader as dataloader<NewLine>import sys<NewLine>import pdb<NewLine>from termcolor import cprint<NewLine>import torch<NewLine>from matplotlib import cm<NewLine>from tqdm import tqdm<NewLine>import time<NewLine>import shutil<NewLine>import nibabel as nib<NewLine>import pdb<NewLine>import argparse<NewLine>import os<NewLine><NewLine>from torch.utils.data.distributed import DistributedSampler<NewLine><NewLine><NewLine><NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine><NewLine>    parser = argparse.ArgumentParser('setup record')<NewLine><NewLine>    # default method l<NewLine>    parser.add_argument(""--DDP"", default=True)<NewLine><NewLine>    # optimizer and scheuler<NewLine>    parser.add_argument(""--lr"", default=5e-5)<NewLine>    parser.add_argument(""--opt"", default='adam',<NewLine>                        choices=['adam', 'sgd'])<NewLine>    parser.add_argument(""--num_gpus"", default=[6, 7])<NewLine>    parser.add_argument(""--bs"", default=4)<NewLine><NewLine>    args = parser.parse_args()<NewLine>    os.environ[""CUDA_VISIBLE_DEVICES""] = ','.join(map(str, args.num_gpus))<NewLine><NewLine>    if args.DDP:<NewLine>        print('init ddp')<NewLine>        os.environ[""CUDA_VISIBLE_DEVICES""] = ','.join(map(str, args.num_gpus))<NewLine>        torch.distributed.init_process_group(backend=""nccl"")<NewLine>        local_rank = torch.distributed.get_rank()<NewLine>        torch.cuda.set_device(local_rank)<NewLine>        device = torch.device(""cuda"", local_rank)<NewLine><NewLine>    if args.seed is not None:<NewLine>        numpy.random.seed(args.seed)<NewLine>        torch.manual_seed(args.seed)<NewLine>        torch.cuda.manual_seed(args.seed)<NewLine><NewLine>    torch.backends.cudnn.benchmark = True<NewLine>    net = build_model()<NewLine>    if len(args.num_gpus) &gt; 1 and not args.DDP:<NewLine>        # pdb.set_trace()<NewLine>        # net = BalancedDataParallel(args.maingpu_bs, net, dim=0).cuda()<NewLine>        net = torch.nn.DataParallel(net).cuda()<NewLine>        print('net to multi-gpu')<NewLine><NewLine>    if len(args.num_gpus) &gt; 1 and args.DDP:<NewLine>        print('using DDP model')<NewLine>        net = torch.nn.parallel.DistributedDataParallel(net,<NewLine>                                                        device_ids=[local_rank],<NewLine>                                                        output_device=local_rank, find_unused_parameters=True)<NewLine>    dataset = build_datset()<NewLine>    if args.DDP:<NewLine>        print('using ddp dataloader')<NewLine>        train_loader = torch.utils.data.DataLoader(dataset, batch_size=args.bs, shuffle=True,<NewLine>                                                   num_workers=args.works, pin_memory=True,<NewLine>                                                   sampler=DistributedSampler(dataset))<NewLine>    else:<NewLine>        train_loader = torch.utils.data.DataLoader(dataset, batch_size=args.bs, shuffle=True,<NewLine>                                                   num_workers=args.works, pin_memory=True)<NewLine>    """"""""""""<NewLine><NewLine>    """"""<NewLine>    Training<NewLine>    """"""<NewLine>    print('setting dataloader')<NewLine><NewLine></code></pre><NewLine><p>what should i do???<br/><NewLine>thank you</p><NewLine></div>",https://discuss.pytorch.org/u/xwjBupt,(Xwj Bupt),xwjBupt,"July 10, 2020,  2:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/xwjbupt"">@xwjBupt</a></p><NewLine><ol><NewLine><li>you will need to launch two DDP processes, operating on <code>cuda:6</code> and <code>cuda:7</code> respectively.</li><NewLine><li><NewLine><code>CUDA_VISIBLE_DEVICES</code> need to be set before launching the main process.</li><NewLine></ol><NewLine><p>Sth like:</p><NewLine><pre><code class=""lang-auto"">CUDA_VISIBLE_DEVICES=6,7 python main.py<NewLine></code></pre><NewLine><p>And then, in <code>main.py</code>, you can launch two DDP sub-processes and set <code>device_ids</code> to <code>[0]</code> and <code>[1]</code> respectively. See this example: <a href=""https://pytorch.org/docs/stable/notes/ddp.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/notes/ddp.html</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: July 10, 2020,  3:10pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
88484,PyTorch hangs in thread after large torch.zeros call in main process,2020-07-09T03:06:37.292Z,0,75,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, first time posting, apologies if I made a mistake in the categorization or anything.</p><NewLine><p>I am trying to have a generator load objects in the background, and I am encountering an extremely strange bug which I have distilled down to the following example. When I try to run the following code, it hangs when trying to call <code>torch.zeros</code> in <code>split_loader_creator</code>, but if I remove the seemingly irrelevant line <code>torch.zeros(152*4, 168*4).float()</code> near the end, it seemingly can make progress. It also seems fine if I change 152<em>4 and 168</em>4 to much smaller numbers. This is on PyTorch 1.5.1, and I do not encounter the issue on 1.4.0. Am I somehow doing this multiprocessing incorrectly? Would really appreciate any help, and please let me know if I can provide more information that might be helpful.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import multiprocessing<NewLine>import atexit<NewLine><NewLine>def split_loader_creator():<NewLine>    for i in range(20):<NewLine>        yield torch.zeros(10, 170, 70)<NewLine><NewLine>def background_generator_helper(gen_creator):<NewLine>    def _bg_gen(gen_creator, conn):<NewLine>        gen = gen_creator()<NewLine>        while conn.recv():<NewLine>            try:<NewLine>                conn.send(next(gen))<NewLine>            except StopIteration:<NewLine>                conn.send(StopIteration)<NewLine>                return<NewLine>            except Exception:<NewLine>                import traceback<NewLine>                traceback.print_exc()<NewLine><NewLine>    parent_conn, child_conn = multiprocessing.Pipe()<NewLine>    p = multiprocessing.Process(target=_bg_gen, args=(gen_creator, child_conn))<NewLine>    p.start()<NewLine>    atexit.register(p.terminate)<NewLine><NewLine>    parent_conn.send(True)<NewLine>    while True:<NewLine>        parent_conn.send(True)<NewLine>        x = parent_conn.recv()<NewLine>        if x is StopIteration:<NewLine>            return<NewLine>        else:<NewLine>            yield x<NewLine><NewLine>def background_generator(gen_creator): # get several processes in the background fetching batches in parallel to keep up with gpu<NewLine>    generator = background_generator_helper(gen_creator)<NewLine>    while True:<NewLine>        batch = next(generator)<NewLine>        if batch is StopIteration:<NewLine>            return<NewLine>        yield batch<NewLine><NewLine>torch.zeros(152*4, 168*4).float()<NewLine><NewLine>data_loader = background_generator(split_loader_creator)<NewLine>for i, batch in enumerate(data_loader):<NewLine>    print(i)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/kevinyang,(Kevin Yang),kevinyang,"July 9, 2020,  3:06am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""88484"" data-username=""kevinyang""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/kevinyang/40/26473_2.png"" width=""20""/> kevinyang:</div><NewLine><blockquote><NewLine><p>This is on PyTorch 1.5.1, and I do not encounter the issue on 1.4.0.</p><NewLine></blockquote><NewLine></aside><NewLine><p>This sounds like a regression. And I confirm I can reproduce the reported behavior.</p><NewLine><p>Could you please submit an issue to <a href=""https://github.com/pytorch/pytorch/issues"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues</a> to report this bug?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, I’ll give that a try.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kevinyang; <NewLine> ,"REPLY_DATE 1: July 9, 2020,  4:21pm; <NewLine> REPLY_DATE 2: July 9, 2020,  6:19pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
88579,Distributed Data Parallel Param Sync in Forward,2020-07-09T16:05:11.865Z,2,105,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I have a question about the DistributedDataParallel Module.</p><NewLine><p>In the forward function call of the DDP, there is a <code>sycn_param</code> call which broadcasts the model parameters from rank 0 to all the other ranks to <code>keep the same state</code> of the model across all processes.</p><NewLine><p><a href=""https://github.com/pytorch/pytorch/blob/7f73f1d591afba823daa4a99a939217fb54d7688/torch/nn/parallel/distributed.py#L440"" rel=""nofollow noopener"">Forwad Function in DDP</a></p><NewLine><p><a href=""https://github.com/pytorch/pytorch/blob/7f73f1d591afba823daa4a99a939217fb54d7688/torch/nn/parallel/distributed.py#L487"" rel=""nofollow noopener"">Sync_call</a></p><NewLine><p>I want to clarify the actual use of this function call.</p><NewLine><p>Think of a case where all processes started and did initialize the model using a set of known weights. Then across all processes, the weights are uniform. In such a case is this useful? Because in every forward call, this synch is called and it could slow down the training. Please correct me if I am wrong.</p><NewLine><p>Another case is where a user wants to add a mutation to the model weights in each process after a synch call (all-reduce). Think of an instance where a mutation helps to discover diversity in the training. And the all-reduce step ensembles this diversity. In such cases, having this call sync_param would cancel the effect the user expects?</p><NewLine><p>What is the main expectation of the sync call? I referred to the <a href=""https://pytorch.org/docs/stable/notes/ddp.html"" rel=""nofollow noopener"">docs</a>, but I couldn’t get a clear picture.</p><NewLine><p>Thank You,<br/><NewLine>Vibhatha.</p><NewLine></div>",https://discuss.pytorch.org/u/Vibhatha_Abeykoon,(Vibhatha Abeykoon),Vibhatha_Abeykoon,"July 9, 2020,  4:05pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/vibhatha_abeykoon"">@Vibhatha_Abeykoon</a></p><NewLine><p>That <code>_sync_params</code> call is there for two purposes:</p><NewLine><ol><NewLine><li>Intra rank/process parameter sync: this is only for the legacy single-process multi-device use case, where each process operates on multiple model replicas. And this is not a recommended way to use DDP.</li><NewLine><li>Inter rank/process <strong>buffer</strong> sync: this does not sync parameters, and this will be skipped if your model does not have buffers (e.g., <code>running_mean</code> in <code>BatchNorm</code> layers).</li><NewLine></ol><NewLine><blockquote><NewLine><p>What is the main expectation of the sync call?</p><NewLine></blockquote><NewLine><p>For many use cases, that <code>_sync_params</code> will be a no-op.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>. Thank you for the response.</p><NewLine><p>I see. The first point is clear to me.</p><NewLine><p>About the second point, so this would be the case to support the functionality<br/><NewLine>of some specific layers like <code>running_mean</code> in <code>BatchNorm</code> layers.  But for<br/><NewLine>general cases, this is also skipped, so there won’t be such synchs.</p><NewLine><p>Is this an assumption that we can make when we use DDP?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""88579"" data-username=""Vibhatha_Abeykoon""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/vibhatha_abeykoon/40/17261_2.png"" width=""20""/> Vibhatha_Abeykoon:</div><NewLine><blockquote><NewLine><p>About the second point, so this would be the case to support the functionality<br/><NewLine>of some specific layers like <code>running_mean</code> in <code>BatchNorm</code> layers. But for<br/><NewLine>general cases, this is also skipped, so there won’t be such synchs.</p><NewLine><p>Is this an assumption that we can make when we use DDP?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yep, this is correct.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Vibhatha_Abeykoon; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: July 9, 2020,  8:17pm; <NewLine> REPLY_DATE 2: July 9, 2020,  4:39pm; <NewLine> REPLY_DATE 3: July 9, 2020,  8:17pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
87220,Issue with using DataParallel (includes minimal code),2020-06-28T18:16:32.012Z,22,246,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have the following (minimal) code that runs on GPU and I’m trying to run it in multiple GPUs using <code>nn.DataParallel</code>:</p><NewLine><pre><code class=""lang-auto"">import math<NewLine>import torch<NewLine>import pickle<NewLine>import time<NewLine><NewLine>import numpy as np<NewLine>import torch.optim as optim<NewLine><NewLine>from torch import nn<NewLine><NewLine>print('device_count()', torch.cuda.device_count())<NewLine>for i in range(torch.cuda.device_count()):<NewLine>    print('get_device_name', torch.cuda.get_device_name(i))<NewLine><NewLine>def _data(dimension, num_examples):<NewLine>    num_mislabeled_examples = 20<NewLine><NewLine>    ground_truth_weights = np.random.normal(size=dimension) / math.sqrt(dimension)<NewLine>    ground_truth_threshold = 0<NewLine><NewLine>    features = np.random.normal(size=(num_examples, dimension)).astype(<NewLine>        np.float32) / math.sqrt(dimension)<NewLine>    labels = (np.matmul(features, ground_truth_weights) &gt;<NewLine>              ground_truth_threshold).astype(np.float32)<NewLine>    mislabeled_indices = np.random.choice(<NewLine>        num_examples, num_mislabeled_examples, replace=False)<NewLine>    labels[mislabeled_indices] = 1 - labels[mislabeled_indices]<NewLine><NewLine>    return torch.tensor(labels), torch.tensor(features)<NewLine><NewLine>class tools:<NewLine>    def __init__(self):<NewLine>        self.name = 'x_2'<NewLine><NewLine>    def SomeFunc(self, model, input_):<NewLine>        print(model.first_term(input_)[0])        <NewLine><NewLine><NewLine>class predictor(nn.Module):<NewLine>    def __init__(self, dim):<NewLine>        super(predictor, self).__init__()<NewLine>        self.weights = torch.nn.Parameter(torch.zeros(dim, 1, requires_grad=True))<NewLine>        self.threshold = torch.nn.Parameter(torch.zeros(1, 1, requires_grad=True))<NewLine><NewLine>    def first_term(self, features):<NewLine>        return features @ self.weights<NewLine><NewLine>    def forward(self, features):<NewLine>        return self.first_term(features) - self.threshold<NewLine><NewLine>class HingeLoss(nn.Module):<NewLine><NewLine>    def __init__(self):<NewLine>        super(HingeLoss, self).__init__()<NewLine>        self.relu = nn.ReLU()<NewLine><NewLine>    def forward(self, output, target):<NewLine>        all_ones = torch.ones_like(target)<NewLine>        labels = 2 * target - all_ones<NewLine>        losses = all_ones - torch.mul(output.squeeze(1), labels)<NewLine><NewLine>        return torch.norm(self.relu(losses))<NewLine><NewLine>class function(object):<NewLine>    def __init__(self, epochs):<NewLine><NewLine>        dim = 10<NewLine>        N = 100<NewLine>        self.target, self.features = _data(dim, N)<NewLine><NewLine>        self.epochs = epochs <NewLine>        self.model = predictor(dim).to('cuda')<NewLine>        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-3)<NewLine>        self.target = self.target.to('cuda')<NewLine>        self.features = self.features.to('cuda')<NewLine>        self.loss_function = HingeLoss().to('cuda')<NewLine><NewLine>        self.tools = tools()<NewLine><NewLine>    def train(self):<NewLine><NewLine>        self.model.train()<NewLine>        for epoch in range(self.epochs):<NewLine>            self.optimizer.zero_grad()<NewLine>            output = self.model(self.features)<NewLine>            # self.tools.SomeFunc(self.model, self.features)<NewLine>            print(output.is_cuda)<NewLine>            loss = self.loss_function(output, self.target)<NewLine>            loss.backward()<NewLine>            print('For epoch {}, loss is: {}.'.format(epoch, loss.item()))<NewLine>            self.optimizer.step()<NewLine><NewLine>def main():<NewLine>    model = function(1000)<NewLine>    print(torch.cuda.device_count())<NewLine>    if False: # This is Flag<NewLine>        if torch.cuda.device_count() &gt; 1:<NewLine>            model.model = nn.DataParallel(model.model)<NewLine>    t = time.time()<NewLine>    model.train()<NewLine>    print('elapsed: {}'.format(time.time() - t))<NewLine><NewLine>if __name__ == '__main__':<NewLine>    main()<NewLine></code></pre><NewLine><ol><NewLine><li><NewLine><p>I have 4 GPU cards (device_count = 4). When I set the flag (indicated with the comment <code>This is Flag</code>) to <code>True</code>, it takes 15.78 seconds to run the code. When I set it to False, it takes 0.71 seconds. Why? How could it be fixed?</p><NewLine></li><NewLine><li><NewLine><p>When I uncomment the line <code>self.tools.SomeFunc(self.model, self.features)</code> and set the flag to <code>True</code>, I receive the following error:</p><NewLine></li><NewLine></ol><NewLine><blockquote><NewLine><p>AttributeError: ‘DataParallel’ object has no attribute ‘first_term’</p><NewLine></blockquote><NewLine><p>How should I fix this? Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/blade,,blade,"June 28, 2020,  6:19pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""87220"" data-username=""blade""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/blade/40/23861_2.png"" width=""20""/> blade:</div><NewLine><blockquote><NewLine><p>I have 4 GPU cards (device_count = 4). When I set the flag (indicated with the comment <code>This is Flag</code> ) to <code>True</code> , it takes 15.78 seconds to run the code. When I set it to False, it takes 0.71 seconds. Why? How could it be fixed?</p><NewLine></blockquote><NewLine></aside><NewLine><p>One thing is that you will need a <code>torch.cuda.synchronize()</code> before calling <code>time.time()</code> to make sure all pending CUDA kernals in the stream are finished.</p><NewLine><p>You can also use <a href=""https://pytorch.org/docs/stable/cuda.html#torch.cuda.Event.elapsed_time"" rel=""nofollow noopener""><code>elapsed_time</code></a> to measure. See discussion <a href=""https://discuss.pytorch.org/t/does-it-ever-make-sense-to-try-model-parallelism-even-if-the-model-fits/86432/14"">here</a>.</p><NewLine><p>If you are looking for the most performant solution, <code>DistributedDataParallel</code> should be the way to go. [<a href=""https://pytorch.org/docs/master/notes/ddp.html"" rel=""nofollow noopener"">example</a>]</p><NewLine><blockquote><NewLine><p>When I uncomment the line  <code>self.tools.SomeFunc(self.model, self.features)</code>  and set the flag to  <code>True</code> , I receive the following error:</p><NewLine></blockquote><NewLine><p>Looks like <code>self.model</code> is a <code>DataParallel</code> instance? If so, <code>DataParallel</code> does not have the <code>first_term</code> attribute. If this attribute is on the model instance you passed to <code>DataParallel</code>, you can access the original model instance through <code>self.model.module</code> (see DataParallel code <a href=""https://github.com/pytorch/pytorch/blob/df8d6eeb19423848b20cd727bc4a728337b73829/torch/nn/parallel/data_parallel.py#L131"" rel=""nofollow noopener"">here</a>) which should have the <code>first_term</code> attribute.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Regarding the first part, calling <code>torch.cuda.synchronize()</code> did not help. It still seems that using 4 GPUs instead of 1 makes the code 15 times slower.</p><NewLine><p>After fixing the second part, my output for</p><NewLine><pre><code class=""lang-auto"">print(model.module.first_term(input_)[0])<NewLine></code></pre><NewLine><p>is always on the first worker:</p><NewLine><pre><code class=""lang-auto"">tensor([0.0020], device='cuda:0', grad_fn=&lt;SelectBackward&gt;)<NewLine></code></pre><NewLine><p>So it is not even sharing the work between the different worker.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""87220"" data-username=""blade""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/blade/40/23861_2.png"" width=""20""/> blade:</div><NewLine><blockquote><NewLine><p>So it is not even sharing the work between the different worker.</p><NewLine></blockquote><NewLine></aside><NewLine><p>What is “worker” here? Do you mean GPU? If so, isn’t that expected?  IIUC, in your code, <code>model</code> is the <code>DataParallel</code> instance. So only the forward function of <code>model</code> would utilize multiple GPUs. See the data parallel implementation below:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/4104ab8b187d6023b0cc80b77e6944126009c532/torch/nn/parallel/data_parallel.py#L141-L156"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/4104ab8b187d6023b0cc80b77e6944126009c532/torch/nn/parallel/data_parallel.py#L141-L156"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/4104ab8b187d6023b0cc80b77e6944126009c532/torch/nn/parallel/data_parallel.py#L141-L156</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""141"" style=""counter-reset: li-counter 140 ;""><NewLine><li>def forward(self, *inputs, **kwargs):</li><NewLine><li>    if not self.device_ids:</li><NewLine><li>        return self.module(*inputs, **kwargs)</li><NewLine><li><NewLine></li><li>    for t in chain(self.module.parameters(), self.module.buffers()):</li><NewLine><li>        if t.device != self.src_device_obj:</li><NewLine><li>            raise RuntimeError(""module must have its parameters and buffers ""</li><NewLine><li>                               ""on device {} (device_ids[0]) but found one of ""</li><NewLine><li>                               ""them on device: {}"".format(self.src_device_obj, t.device))</li><NewLine><li><NewLine></li><li>    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)</li><NewLine><li>    if len(self.device_ids) == 1:</li><NewLine><li>        return self.module(*inputs[0], **kwargs[0])</li><NewLine><li>    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])</li><NewLine><li>    outputs = self.parallel_apply(replicas, inputs, kwargs)</li><NewLine><li>    return self.gather(outputs, self.output_device)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Regarding the slowdown, I can reproduce it locally with two GPUs:</p><NewLine><p>Using DataParallel on 2 GPUs</p><NewLine><pre><code class=""lang-auto"">For epoch 999, loss is: 8.4603910446167.<NewLine>elapsed: 3.1627652645111084<NewLine></code></pre><NewLine><p>Not using DataParallel</p><NewLine><pre><code class=""lang-auto"">For epoch 999, loss is: 8.323615074157715.<NewLine>elapsed: 1.192000389099121<NewLine></code></pre><NewLine><p>Then I go back to inspect the model:</p><NewLine><pre><code class=""lang-python"">    def __init__(self, dim):<NewLine>        super(predictor, self).__init__()<NewLine>        self.weights = torch.nn.Parameter(torch.zeros(dim, 1, requires_grad=True))<NewLine>        self.threshold = torch.nn.Parameter(torch.zeros(1, 1, requires_grad=True))<NewLine></code></pre><NewLine><p>My suspicion is that the model parameters are so light that the overhead of GIL contention, input scattering, output gathering, and model replication in DataParallel forward pass overshadows the speedup brought by multi-gpu training. Are these parameters used in real applications or are you trying to profile <code>DataParallel</code> performance?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""87220"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>See the data parallel implementation below:</p><NewLine></blockquote><NewLine></aside><NewLine><p>I didn’t quite get your point on this one.</p><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""87220"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>My suspicion is that the model parameters are so light that the overhead of GIL contention, input scattering, output gathering, and model replication in DataParallel forward pass overshadows the speedup brought by multi-gpu training.</p><NewLine></blockquote><NewLine></aside><NewLine><p>That makes sense. Let me try it on a more sophisticated code and update the answer.</p><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""87220"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>Are these parameters used in real applications or are you trying to profile <code>DataParallel</code> performance?</p><NewLine></blockquote><NewLine></aside><NewLine><p>I’m just making sure I understand how to use DataParallel correctly. Although way simpler, this sample code mimics the general structure of my actual code. So, there is nothing wrong with my implementation?</p><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""87220"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>you can access the original model instance through <code>self.model.module</code> (see DataParallel code <a href=""https://github.com/pytorch/pytorch/blob/df8d6eeb19423848b20cd727bc4a728337b73829/torch/nn/parallel/data_parallel.py#L131"" rel=""nofollow noopener"">here </a>) which should have the <code>first_term</code> attribute.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thanks, based on your response I used <a href=""https://github.com/pytorch/pytorch/issues/16885#issuecomment-551779897"" rel=""nofollow noopener"">this wrapper</a> to get access to attributes without altering the code itself.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""87220"" data-username=""blade""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/blade/40/23861_2.png"" width=""20""/> blade:</div><NewLine><blockquote><NewLine><p>I didn’t quite get your point on this one.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I might have misunderstood the original question.</p><NewLine><blockquote><NewLine><p>After fixing the second part, my output for</p><NewLine><pre><code class=""lang-auto"">print(model.module.first_term(input_)[0])<NewLine></code></pre><NewLine><p>is always on the first worker:</p><NewLine><pre><code class=""lang-auto"">tensor([0.0020], device='cuda:0', grad_fn=&lt;SelectBackward&gt;)<NewLine></code></pre><NewLine></blockquote><NewLine><p>Is the question about why the output of <code>print(model.module.first_term(input_)[0])</code> always on <code>cuda:0</code>?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes: I’m under impression that there are 2 ways of parallelizing a PyTorch code: <code>DistributedDataParallel</code> and <code>DataParallel</code>. In the former each layer of the network is assigned to a particular processor while in the latter, each processor takes a portion of the training data and all the processors go through all the code (<a href=""https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html#results"" rel=""nofollow noopener"">like here</a>). Although <code>DistributedDataParallel</code> is preferred (though I’m not sure why, except for multi-node processing, perhaps?), it looks hairy and I decided to start with <code>DataParallel</code>. Hence, I expected all the processors call <code>first_term()</code> when they get to that part of the code. What am I missing?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group quote-modified"" data-post=""8"" data-topic=""87220"" data-username=""blade""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/blade/40/23861_2.png"" width=""20""/> blade:</div><NewLine><blockquote><NewLine><p>Although <code>DistributedDataParallel</code> is preferred (though I’m not sure why)</p><NewLine></blockquote><NewLine></aside><NewLine><p>This is mostly due to performance reasons. As of today, DataParallel (DP) replicates model in its forward pass, while DistributedDataParallel (DDP) replicates models in its ctor. That means DP would replicate model once in every iteration. Besides, DP also suffers from GIL contention as it is single-process-multi-thread. DDP does not hit this problem, as each model replica runs in its own process. More info about DDP can be found <a href=""https://pytorch.org/docs/master/notes/ddp.html"" rel=""nofollow noopener"">here</a> and <a href=""https://arxiv.org/abs/2006.15704"" rel=""nofollow noopener"">here</a>.</p><NewLine><blockquote><NewLine><p>Hence, I expected all the processors call  <code>first_term()</code>  when they get to that part of the code. What am I missing?</p><NewLine></blockquote><NewLine><p>What happens in DP’s forward function is: 1) replicate model to all devices 2) scatter inputs to all devices 3) launch multiple threads in parallel, where each threads processes an input split using one model replica on one device 4) gather outputs to the same device.</p><NewLine><p>Given the above, if you change the <code>predictor</code> code to the following. You will see it prints multiple devices.</p><NewLine><pre><code class=""lang-python"">class predictor(nn.Module):<NewLine>     ....<NewLine><NewLine>    def forward(self, features):<NewLine>        print(self.first_term(features).device)<NewLine>        return self.first_term(features) - self.threshold<NewLine></code></pre><NewLine><p>However, for the following code:</p><NewLine><pre><code class=""lang-python"">    def SomeFunc(self, model, input_):<NewLine>        print(model.first_term(input_)[0])<NewLine></code></pre><NewLine><p>If it is called outside of a forward pass or if the <code>model</code> argument is not a model replica (the <code>self</code> argument in <code>predictor.forward</code> method), then it won’t show different devices.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group quote-modified"" data-post=""6"" data-topic=""87220"" data-username=""blade""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/blade/40/23861_2.png"" width=""20""/> blade:</div><NewLine><blockquote><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""87220"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>My suspicion is that the model parameters are so light that the overhead of GIL contention, input scattering, output gathering, and model replication in DataParallel forward pass overshadows the speedup brought by multi-gpu training.</p><NewLine></blockquote><NewLine></aside><NewLine><p>That makes sense. Let me try it on a more sophisticated code and update the answer.</p><NewLine></blockquote><NewLine></aside><NewLine><p>So I tried on another code, with 1 GPU my code ran in 434 sec while with 2 GPUs it took 864 sec. So it shouldn’t be from the price we pay for parallelization. Also, using your line <code>print(self.first_term(features).device)</code> it uses all processors at each step so the code is not running in series by each GPU.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""10"" data-topic=""87220"" data-username=""blade""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/blade/40/23861_2.png"" width=""20""/> blade:</div><NewLine><blockquote><NewLine><p>So I tried on another code, with 1 GPU my code ran in 434 sec while with 2 GPUs it took 864 sec.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Can we profile how much of the 434s are spent in the forward pass when DP is <strong>not</strong> present? And how much of that is spent on GPU? This can be measured using <a href=""https://pytorch.org/docs/stable/cuda.html#torch.cuda.Event.elapsed_time"" rel=""nofollow noopener""><code>elapsed_time</code></a> . See <a href=""https://discuss.pytorch.org/t/does-it-ever-make-sense-to-try-model-parallelism-even-if-the-model-fits/86432/14"">this discussion</a>.</p><NewLine><p>Note that multi-thread cannot parallelize normal Python ops due to Python GIL, and the parallelism only kicks in when the execution does not require GIL (e.g., CPU/GPU ops that explicitly drops GIL).</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not sure if I’m doing right. Am I doing it right for the sample code below?</p><NewLine><pre><code class=""lang-auto"">def main():<NewLine>    model = function(1000)<NewLine>    print(torch.cuda.device_count())<NewLine>    if True:<NewLine>        if torch.cuda.device_count() &gt; 1:<NewLine>            model.model = MyDataParallel(model.model)<NewLine><NewLine>    start = time.monotonic()<NewLine>    s = torch.cuda.current_stream()<NewLine>    e_start = torch.cuda.Event(enable_timing=True)<NewLine>    e_finish = torch.cuda.Event(enable_timing=True)<NewLine>    s.record_event(e_start)<NewLine><NewLine>    model.train()<NewLine><NewLine>    torch.cuda.synchronize()<NewLine>    s.record_event(e_finish)<NewLine>    e_finish.synchronize()<NewLine>    end = time.monotonic()<NewLine><NewLine>    print('Forward latency is: {} '.format(e_start.elapsed_time(e_finish)))<NewLine>    print(""end - start = "", end - start)<NewLine><NewLine>if __name__ == '__main__':<NewLine>    main()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>This looks correct to me. Does <code>MyDataParallel</code> use <code>DataParallel</code> internally?</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, it’s just a wrapper so that I can access the attributes (so I don’t need to add <code>.module</code>). If so, below is the output for my actual code:</p><NewLine><blockquote><NewLine><p>Forward latency is: 437033.9375<br/><NewLine>end - start =  437.031393879035</p><NewLine></blockquote><NewLine><p>So what do you think is wrong with the parallel implementation that takes double as much time to run with two GPUs?</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""14"" data-topic=""87220"" data-username=""blade""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/blade/40/23861_2.png"" width=""20""/> blade:</div><NewLine><blockquote><NewLine><p>So what do you think is wrong with the parallel implementation that takes double as much time to run with two GPUs?</p><NewLine></blockquote><NewLine></aside><NewLine><p>This is surprising to me. I would assume at least the CUDA ops can run in parallel. Could you please share the full code used in this test? We will investigate.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sure, I can share it privately. Could you please send me your Github id?</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, it’s <code>mrshenli</code>.</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>Done, let me know if you received it.</p><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p>Got it and will investigate. Thanks for sharing.</p><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>, I was wondering if you have any comments yet as to why this happens? Thanks.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/blade; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/blade; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/blade; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/blade; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/blade; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/blade; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/blade; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/blade; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/blade; <NewLine> ,"REPLY_DATE 1: June 28, 2020,  9:41pm; <NewLine> REPLY_DATE 2: June 30, 2020, 11:50am; <NewLine> REPLY_DATE 3: June 30, 2020,  2:22pm; <NewLine> REPLY_DATE 4: June 30, 2020,  2:39pm; <NewLine> REPLY_DATE 5: June 30, 2020,  3:21pm; <NewLine> REPLY_DATE 6: June 30, 2020,  3:26pm; <NewLine> REPLY_DATE 7: June 30, 2020,  3:43pm; <NewLine> REPLY_DATE 8: June 30, 2020,  3:52pm; <NewLine> REPLY_DATE 9: June 30, 2020,  5:50pm; <NewLine> REPLY_DATE 10: June 30, 2020,  5:57pm; <NewLine> REPLY_DATE 11: June 30, 2020,  8:35pm; <NewLine> REPLY_DATE 12: June 30, 2020,  8:11pm; <NewLine> REPLY_DATE 13: June 30, 2020,  8:37pm; <NewLine> REPLY_DATE 14: July 1, 2020,  5:24pm; <NewLine> REPLY_DATE 15: July 2, 2020,  7:18am; <NewLine> REPLY_DATE 16: July 2, 2020,  7:52pm; <NewLine> REPLY_DATE 17: July 2, 2020,  8:43pm; <NewLine> REPLY_DATE 18: July 2, 2020,  8:44pm; <NewLine> REPLY_DATE 19: July 4, 2020, 11:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: 1 Like; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: 1 Like; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: 1 Like; <NewLine> REPLY 17 LIKES: ; <NewLine> REPLY 18 LIKES: 1 Like; <NewLine> REPLY 19 LIKES: ; <NewLine> 
88248,DDP Learning-Rate,2020-07-07T14:29:28.785Z,0,197,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was a bit confused how DDP (with NCCL) reduces gradients and the effect this has on the learning-rate that needs to be set.</p><NewLine><p>Would the below example be a correct way to interpret this -&gt; that <strong>DDP and DP should have the same learning-rate if scaled out to the same effective batch-size</strong>?</p><NewLine><p>Assume set contains 80 samples<br/><NewLine>Single-gpu LR = 0.1<br/><NewLine>Total-grad-distance = LR * g * (samples/batch-size)</p><NewLine><ol><NewLine><li><NewLine><p>Single-gpu<br/><NewLine>batch = 8<br/><NewLine>gradient = 8g/8 = g<br/><NewLine>total-grad-distance = 0.1 * g * 10 = g</p><NewLine></li><NewLine><li><NewLine><p>DP (2-gpu, 1 node)<br/><NewLine>batch = 16<br/><NewLine>gradient = 16g/16 = g<br/><NewLine>total-grad-distance = 0.1 * g * 5 = 0.5g<br/><NewLine>-&gt; thus scale LR by 2</p><NewLine></li><NewLine><li><NewLine><p>DDP (2-gpu, 1 node OR 1-gpu, 2 nodes)<br/><NewLine>batch-per-process = 8<br/><NewLine>gradient = (8g/8) + (8g/8) / 2 = g<br/><NewLine>total-grad-distance = 0.1 * g * 5 = 0.5g<br/><NewLine>-&gt; thus scale LR by 2?</p><NewLine></li><NewLine></ol><NewLine><p>Or does allreduce just <a href=""https://pytorch.org/docs/stable/notes/ddp.html#implementation"" rel=""nofollow noopener"">sum the gradients</a> in which case:</p><NewLine><blockquote><NewLine><p>and  <code>ProcessGroup::allreduce()</code>  to sum gradients.</p><NewLine></blockquote><NewLine><ol start=""3""><NewLine><li>DDP (2-gpu, 1 node OR 1-gpu, 2 nodes)<br/><NewLine>batch-per-process = 8<br/><NewLine>gradient = (8g/8) + (8g/8) = 2g<br/><NewLine>total-grad-distance = 0.1 * 2g* 5 = g<br/><NewLine>-&gt; thus leave LR the same as single-GPU</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/Ilia_Karmanov,(Ilia Karmanov),Ilia_Karmanov,"July 7, 2020,  2:49pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you maintain the same batch size between single GPU and DP/DDP, according to your calculations you do not need to adjust LR ?</p><NewLine><p>PS:<br/><NewLine>In DDP grads are averaged: <a href=""https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel"" rel=""nofollow noopener"">https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel</a></p><NewLine><blockquote><NewLine><p>During the backwards pass, gradients from each node are averaged.</p><NewLine></blockquote><NewLine><p>PPS: <a href=""https://arxiv.org/pdf/1706.02677.pdf"" rel=""nofollow noopener"">https://arxiv.org/pdf/1706.02677.pdf</a></p><NewLine><blockquote><NewLine><p>Linear Scaling Rule: When the minibatch size is multiplied by k, multiply the learning rate by k.</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>More discussions can be found at <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769"">Should we split batch_size according to ngpu_per_node when DistributedDataparallel</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vfdev-5; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: July 7, 2020,  3:08pm; <NewLine> REPLY_DATE 2: July 8, 2020,  2:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
88296,How to use &ldquo;break&rdquo; in DistributedDataParallel training,2020-07-07T22:13:59.851Z,2,131,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using DistributedDataParallel to train the model on multiple GPUs. If I would like to stop the process early, how could I achieve it? Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/jetcai1900,,jetcai1900,"July 7, 2020, 10:13pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is this about uneven inputs on different processes? See:</p><NewLine><ol><NewLine><li><a href=""https://github.com/pytorch/pytorch/issues/33148"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/33148</a></li><NewLine><li><a href=""https://github.com/pytorch/pytorch/issues/38174"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/38174</a></li><NewLine></ol><NewLine><p>If all processes know when to exit, simply break the loop would work. The tricky case is when one processes breaks the loop but other processes proceed as mentioned in the above two issues.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""88296"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>If all processes know when to exit, simply break the loop would work. The tricky case is when one processes breaks the loop but other processes proceed as mentioned in the above two issues.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Indeed this is what I meet. one process breaks the loop while others continue. The condition when the process breaks is the loss in eval dataset increases (overfitting). Do you have any ideas? Thanks.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ideally, we should address this in DDP and close <a href=""https://github.com/pytorch/pytorch/issues/38174"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/38174</a>. Before that takes place, you can use <code>all_reduce</code> synchronize some signal across all processes. See <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/multiprocessing-barrier-blocks-all-processes/80345/10"">Multiprocessing - Barrier Blocks all Processes?</a></p><NewLine><p>One thing to note is that, this might have perf impacts, especially when the model is light and its forward pass runs faster than communicating the signal.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your help. Probably I would set a fixed epoch number to address this, which is simple thought is not optimal.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jetcai1900; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jetcai1900; <NewLine> ,"REPLY_DATE 1: July 7, 2020, 10:30pm; <NewLine> REPLY_DATE 2: July 7, 2020, 11:36pm; <NewLine> REPLY_DATE 3: July 8, 2020,  1:25am; <NewLine> REPLY_DATE 4: July 8, 2020,  3:13am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
87063,Saving a models state_dict into redis cache,2020-06-26T21:48:53.396Z,4,155,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m building a distributed parameter/server type architecture and wanting to communicate model updates through table solutions on Azure.</p><NewLine><p>I’m having a hard time finding any useful information about saving a models state_dict into a redis cache. I’ve given up on Azure Cosmos tables because of the size limit (64kb) per entity and looked toward redis since model state_dict params/weights are much larger, even for a small model.</p><NewLine><p>Does anyone have any recommendations for me on how to pursue this?</p><NewLine></div>",https://discuss.pytorch.org/u/bPangolin,(Benjamin),bPangolin,"June 26, 2020,  9:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is this question about 1) whether Redis is an appropriate storage to save model states or 2) how to configure Azure to run Redis or 3) how to build parameter server using PyTorch?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Azure is simply the platform I’m developing on. I am looking for the answer to 1) Is redis an appropriate storage to save model parameters and weights?</p><NewLine><p>I’ve recently learned about redisAI but it does not have an Azure equivalent service and would have to be deployed on a dedicated VM.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""87063"" data-username=""bPangolin""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/b/3bc359/40.png"" width=""20""/> bPangolin:</div><NewLine><blockquote><NewLine><p>I am looking for the answer to 1) Is redis an appropriate storage to save model parameters and weights?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Hmm, isn’t this mainly depend on the data size and IO pattern? Or does being a DNN model make any difference?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am curious why are you communicating model updates via external DB. Normally model updates are communicated via collective communication ranks or something like EASGD (<a href=""https://arxiv.org/abs/1412.6651"" rel=""nofollow noopener"">https://arxiv.org/abs/1412.6651</a>). Is your goal: debugging, logging, or improved reliability here? Seems like updating model via external DB would be a performance hit?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m testing out parallelizing across multiple worker nodes in a parameter-server type architecture. I’m using redisAI to handle the model weight and gradient sharing between primary and worker nodes. At each node the worker retains it’s own parameter set and delivers gradients to the primary node, which updates the global model. I have four workers, so I combine each workers update together</p><NewLine><p>Sequentially performing worker updates, global update, and then worker reads of updated global model is a performance hit. I’m also testing out a update-and-continue scheme where the worker will push it’s gradients to the global model and then continue with it’s own path instead of adjusting to the global model.</p><NewLine><pre><code class=""lang-auto"">beta = 0.25<NewLine>gmsd = model.state_dict()<NewLine>for name, param in model.named_parameters():<NewLine>            worker_001_data = redisai_conn.tensorget(f'worker_001:{name}_grad')<NewLine>            worker_002_data = redisai_conn.tensorget(f'worker_002:{name}_grad')<NewLine>            worker_003_data = redisai_conn.tensorget(f'worker_003:{name}_grad')<NewLine>            worker_004_data = redisai_conn.tensorget(f'worker_004:{name}_grad')<NewLine>            tens = worker_001_data*beta + worker_002_data*beta + worker_003_data*beta + worker_004_data*beta<NewLine>            worker_ten = torch.from_numpy(tens).to(self.device)<NewLine>            if gmsd[name].grad == None:<NewLine>                gmsd[name].grad = (worker_ten)<NewLine>            else:<NewLine>                gmsd[name].grad.copy_(worker_ten)<NewLine><NewLine>model.load_state_dict(gmsd)<NewLine></code></pre><NewLine><p>My goal is improved reliability in the global models predictions.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see, for this use case, an alternative is to use <code>torch.distributed.rpc</code> to connect the parameter server with trainers, and then let the parameter server periodically flush checkpoints to the external storage. So that you don’t have to pay the checkpointing overhead in every iteration.</p><NewLine><p>Some related resources:</p><NewLine><ol><NewLine><li>Building <a href=""https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf"" rel=""nofollow noopener"">HogWild!</a> PS using <code>torch.distributed.rpc</code>: <a href=""https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html</a><NewLine></li><NewLine><li>Batch updating PS (requires v1.6+): <a href=""https://github.com/pytorch/tutorials/blob/release/1.6/intermediate_source/rpc_async_execution.rst"" rel=""nofollow noopener"">https://github.com/pytorch/tutorials/blob/release/1.6/intermediate_source/rpc_async_execution.rst</a><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/bPangolin; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/agolynski; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/bPangolin; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 28, 2020,  9:44pm; <NewLine> REPLY_DATE 2: June 28, 2020, 10:05pm; <NewLine> REPLY_DATE 3: June 29, 2020,  2:05am; <NewLine> REPLY_DATE 4: June 29, 2020,  3:16pm; <NewLine> REPLY_DATE 5: July 7, 2020,  1:52am; <NewLine> REPLY_DATE 6: July 7, 2020,  7:21pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
65485,Distributed Data Parallel single node maximum number of GPUs,2019-12-31T22:09:34.716Z,4,628,"<div class=""post"" itemprop=""articleBody""><NewLine><p>DistributedDataParallel imagenet training example breaks throwing the following error: <code>RuntimeError: NCCL error in: /tmp/pip-req-build-4baxydiv/torch/lib/c10d/ProcessGroupNCCL.cpp:400, unhandled cuda error</code> on running it on a single node with 10 GPUs. The same runs perfectly fine as soon as the number of GPUs in the environment is set to 8. For DataParallel, somewhere it is mentioned that at present it does not run on more than 8 GPUs; however, I could not find similar info about DDP (I may have missed it). Moreover, as all the processes load their own module locally on each of the devices without a broadcast during initialization, is not it unexpected?</p><NewLine></div>",https://discuss.pytorch.org/u/bapi,(Bapi Chatterjee),bapi,"December 31, 2019, 10:09pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Which NCCL version are you using?<br/><NewLine>Could you rerun your script with <code>NCCL_DEBUG=DEBUG python ...</code> and post the log here?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""65485""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ptrblck/40/1823_2.png"" width=""20""/> ptrblck:</div><NewLine><blockquote><NewLine><p>NCCL_DEBUG=DEBUG</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thanks for the response.<br/><NewLine>That flag does not generate much info other than the usual output. I reran the code with <code>NCCL_DEBUG=INFO</code>, below is the log (machine is named gpu123):</p><NewLine><blockquote><NewLine><p>$ NCCL_DEBUG=INFO python main.py -a resnet18 --dist-url ‘tcp://127.0.0.1:6840’ --dist-backend ‘nccl’ --multiprocessing-distributed --world-size 1 --local_rank 0 ~/tiny-imagenet-200<br/><NewLine>Use GPU: 3 for training                                                                                                                                   Use GPU: 5 for training                                                                                                                                   Use GPU: 7 for training                                                                                                                                   Use GPU: 9 for training                                                                                                                                   Use GPU: 6 for training                                                                                                                                   Use GPU: 1 for training                                                                                                                                   Use GPU: 0 for training                                                                                                                                   Use GPU: 4 for training                                                                                                                                   =&gt; creating model ‘resnet18’                                                                                                                              Use GPU: 8 for training                                                                                                                                   =&gt; creating model ‘resnet18’                                                                                                                              Use GPU: 2 for training                                                                                                                                   =&gt; creating model ‘resnet18’                                                                                                                              =&gt; creating model ‘resnet18’                                                                                                                              =&gt; creating model ‘resnet18’                                                                                                                              =&gt; creating model ‘resnet18’                                                                                                                              =&gt; creating model ‘resnet18’                                                                                                                              =&gt; creating model ‘resnet18’                                                                                                                              =&gt; creating model ‘resnet18’                                                                                                                              =&gt; creating model ‘resnet18’                                                                                                                              gpu123:7932:7932 [0] NCCL INFO Bootstrap : Using [0]ib0:10.36.192.223&lt;0&gt;                                                                                  gpu123:7932:7932 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).                                                                             gpu123:7932:7932 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB ; OOB ib0:10.36.192.223&lt;0&gt;                                                                   NCCL version 2.4.8+cuda10.0                                                                                                                               gpu123:7941:7941 [9] NCCL INFO Bootstrap : Using [0]ib0:10.36.192.223&lt;0&gt;                                                                                  gpu123:7936:7936 [4] NCCL INFO Bootstrap : Using [0]ib0:10.36.192.223&lt;0&gt;                                                                                  gpu123:7934:7934 [2] NCCL INFO Bootstrap : Using [0]ib0:10.36.192.223&lt;0&gt;                                                                                  gpu123:7933:7933 [1] NCCL INFO Bootstrap : Using [0]ib0:10.36.192.223&lt;0&gt;                                                                                  gpu123:7937:7937 [5] NCCL INFO Bootstrap : Using [0]ib0:10.36.192.223&lt;0&gt;                                                                                  gpu123:7935:7935 [3] NCCL INFO Bootstrap : Using [0]ib0:10.36.192.223&lt;0&gt;                                                                                  gpu123:7941:7941 [9] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).                                                                             gpu123:7936:7936 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).                                                                             gpu123:7934:7934 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).                                                                             gpu123:7933:7933 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).                                                                             gpu123:7937:7937 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).                                                                             gpu123:7935:7935 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).                                                                             gpu123:7940:7940 [8] NCCL INFO Bootstrap : Using [0]ib0:10.36.192.223&lt;0&gt;                                                                                  gpu123:7940:7940 [8] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).                                                                             gpu123:7941:7941 [9] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB ; OOB ib0:10.36.192.223&lt;0&gt;                                                                   gpu123:7935:7935 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB ; OOB ib0:10.36.192.223&lt;0&gt;                                                                   gpu123:7933:7933 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB ; OOB ib0:10.36.192.223&lt;0&gt;                                                                   gpu123:7936:7936 [4] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB ; OOB ib0:10.36.192.223&lt;0&gt;                                                                   gpu123:7934:7934 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB ; OOB ib0:10.36.192.223&lt;0&gt;                                                                   gpu123:7937:7937 [5] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB ; OOB ib0:10.36.192.223&lt;0&gt;                                                                   gpu123:7940:7940 [8] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB ; OOB ib0:10.36.192.223&lt;0&gt;                                                                   gpu123:7932:8011 [0] NCCL INFO Setting affinity for GPU 0 to 0fff                                                                                         gpu123:7941:8013 [9] NCCL INFO Setting affinity for GPU 9 to 0fff                                                                                         gpu123:7940:8025 [8] NCCL INFO Setting affinity for GPU 8 to 0fff                                                                                         gpu123:7934:8022 [2] NCCL INFO Setting affinity for GPU 2 to 0fff                                                                                         gpu123:7937:8023 [5] NCCL INFO Setting affinity for GPU 5 to 0fff                                                                                         gpu123:7935:8017 [3] NCCL INFO Setting affinity for GPU 3 to 0fff                                                                                         gpu123:7936:8020 [4] NCCL INFO Setting affinity for GPU 4 to 0fff                                                                                         gpu123:7933:8018 [1] NCCL INFO Setting affinity for GPU 1 to 0fff                                                                                         gpu123:7939:7939 [7] NCCL INFO Bootstrap : Using [0]ib0:10.36.192.223&lt;0&gt;                                                                                  gpu123:7939:7939 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).                                                                             gpu123:7939:7939 [7] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB ; OOB ib0:10.36.192.223&lt;0&gt;                                                                   gpu123:7938:7938 [6] NCCL INFO Bootstrap : Using [0]ib0:10.36.192.223&lt;0&gt;                                                                                  gpu123:7938:7938 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).                                                                             gpu123:7938:7938 [6] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB ; OOB ib0:10.36.192.223&lt;0&gt;                                                                   gpu123:7939:8027 [7] NCCL INFO Setting affinity for GPU 7 to 0fff                                                                                         gpu123:7938:8029 [6] NCCL INFO Setting affinity for GPU 6 to 0fff                                                                                         gpu123:7932:8011 [0] NCCL INFO Channel 00 :    0   1   2   3   4   5   6   7   8   9                                                                      gpu123:7935:8017 [3] NCCL INFO Ring 00 : 3[3] -&gt; 4[4] via P2P/IPC                                                                                         gpu123:7938:8029 [6] NCCL INFO Ring 00 : 6[6] -&gt; 7[7] via P2P/IPC                                                                                         gpu123:7936:8020 [4] NCCL INFO Ring 00 : 4[4] -&gt; 5[5] via P2P/IPC                                                                                         gpu123:7937:8023 [5] NCCL INFO Ring 00 : 5[5] -&gt; 6[6] via P2P/IPC                                                                                         gpu123:7939:8027 [7] NCCL INFO Ring 00 : 7[7] -&gt; 8[8] via P2P/IPC                                                                                         gpu123:7941:8013 [9] NCCL INFO Ring 00 : 9[9] -&gt; 0[0] via P2P/IPC                                                                                         gpu123:7940:8025 [8] NCCL INFO Ring 00 : 8[8] -&gt; 9[9] via P2P/IPC                                                                                         gpu123:7933:8018 [1] NCCL INFO Ring 00 : 1[1] -&gt; 2[2] via P2P/IPC                                                                                         gpu123:7932:8011 [0] NCCL INFO Ring 00 : 0[0] -&gt; 1[1] via P2P/IPC                                                                                         gpu123:7934:8022 [2] NCCL INFO Ring 00 : 2[2] -&gt; 3[3] via P2P/IPC                                                                                                                                                                                                                                                   gpu123:7941:8013 [9] transport/p2p.cc:574 NCCL WARN failed to open CUDA IPC handle : 60 peer mapping resources exhausted                                  gpu123:7941:8013 [9] NCCL INFO init.cc:669 -&gt; 1                                                                                                           gpu123:7941:8013 [9] NCCL INFO init.cc:815 -&gt; 1                                                                                                           gpu123:7941:8013 [9] NCCL INFO init.cc:951 -&gt; 1                                                                                                           gpu123:7941:8013 [9] NCCL INFO misc/group.cc:69 -&gt; 1 [Async thread]                                                                                                                                                                                                                                                 gpu123:7932:8011 [0] transport/p2p.cc:604 NCCL WARN failed to open CUDA IPC handle : 60 peer mapping resources exhausted                                  gpu123:7932:8011 [0] NCCL INFO init.cc:679 -&gt; 1                                                                                                           gpu123:7932:8011 [0] NCCL INFO init.cc:815 -&gt; 1                                                                                                           gpu123:7932:8011 [0] NCCL INFO init.cc:951 -&gt; 1                                                                                                           gpu123:7932:8011 [0] NCCL INFO misc/group.cc:69 -&gt; 1 [Async thread]                                                                                       gpu123:7935:8017 [3] NCCL INFO comm 0x2abf58001e10 rank 3 nranks 10 cudaDev 3 nvmlDev 3 - Init COMPLETE                                                   gpu123:7934:8022 [2] NCCL INFO comm 0x2b8408001e10 rank 2 nranks 10 cudaDev 2 nvmlDev 2 - Init COMPLETE                                                   gpu123:7937:8023 [5] NCCL INFO comm 0x2b011c001e10 rank 5 nranks 10 cudaDev 5 nvmlDev 5 - Init COMPLETE                                                   gpu123:7936:8020 [4] NCCL INFO comm 0x2b4b28001e10 rank 4 nranks 10 cudaDev 4 nvmlDev 4 - Init COMPLETE                                                   gpu123:7938:8029 [6] NCCL INFO comm 0x2b58d8001e10 rank 6 nranks 10 cudaDev 6 nvmlDev 6 - Init COMPLETE                                                   gpu123:7933:8018 [1] NCCL INFO comm 0x2af070001e10 rank 1 nranks 10 cudaDev 1 nvmlDev 1 - Init COMPLETE                                                   gpu123:7939:8027 [7] NCCL INFO comm 0x2b2180001e10 rank 7 nranks 10 cudaDev 7 nvmlDev 7 - Init COMPLETE                                                   gpu123:7940:8025 [8] NCCL INFO comm 0x2b015c001e10 rank 8 nranks 10 cudaDev 8 nvmlDev 8 - Init COMPLETE                                                   /nfs/scistore08/alistgrp/bchatter/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 11 leaked semaphores to clean up at shutdown                                                                                                                   len(cache))                                                                                                                                             /nfs/scistore08/alistgrp/bchatter/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 11 leaked semaphores to clean up at shutdown                                                                                                                   len(cache))                                                                                                                                             /nfs/scistore08/alistgrp/bchatter/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 11 leaked semaphores to clean up at shutdown                                                                                                                   len(cache))                                                                                                                                             /nfs/scistore08/alistgrp/bchatter/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 11 leaked semaphores to clean up at shutdown                                                                                                                   len(cache))                                                                                                                                             /nfs/scistore08/alistgrp/bchatter/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 11 leaked semaphores to clean up at shutdown                                                                                                                   len(cache))                                                                                                                                             /nfs/scistore08/alistgrp/bchatter/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 11 leaked semaphores to clean up at shutdown                                                                                                                   len(cache))                                                                                                                                             Traceback (most recent call last):                                                                                                                          File “main.py”, line 425, in                                                                                                                        main()                                                                                                                                                  File “main.py”, line 109, in main                                                                                                                           mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))                                                                               File “/nfs/scistore08/alistgrp/bchatter/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py”, line 171, in spawn                           while not spawn_context.join():                                                                                                                         File “/nfs/scistore08/alistgrp/bchatter/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py”, line 118, in join                            raise Exception(msg)                                                                                                                                  Exception:                                                                                                                                                                                                                                                                                                          – Process 9 terminated with the following error:                                                                                                         Traceback (most recent call last):                                                                                                                          File “/nfs/scistore08/alistgrp/bchatter/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py”, line 19, in _wrap                            fn(i, *args)                                                                                                                                            File “/nfs/scistore08/alistgrp/bchatter/workspace/async-opt/dist_data_parallel/imagenet_training/main.py”, line 151, in main_worker                         model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])                                                                         File “/nfs/scistore08/alistgrp/bchatter/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/distributed.py”, line 298, in <strong>init</strong>                      self.broadcast_bucket_size)                                                                                                                             File “/nfs/scistore08/alistgrp/bchatter/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/distributed.py”, line 480, in _distributed_broadcast_coalesced                                                                                                                                                        dist._broadcast_coalesced(self.process_group, tensors, buffer_size)                                                                                   RuntimeError: NCCL error in: /tmp/pip-req-build-4baxydiv/torch/lib/c10d/ProcessGroupNCCL.cpp:400, unhandled cuda error</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>torch.cuda.nccl.version()</code> outputs 2408.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/bapi"">@bapi</a></p><NewLine><blockquote><NewLine><p>For DataParallel, somewhere it is mentioned that at present it does not run on more than 8 GPUs;</p><NewLine></blockquote><NewLine><p>Just curious, could you please point me to the doc with this claim? This is new to me, I wasn’t aware there is such a limitation in DP.</p><NewLine><blockquote><NewLine><p>however, I could not find similar info about DDP (I may have missed it).</p><NewLine></blockquote><NewLine><p>We recently tested DDP using 256 GPUs, and it runs fine. Could this error be sth specific to the imagenet example? cc <a class=""mention"" href=""/u/fmassa"">@fmassa</a> for vision questions</p><NewLine><blockquote><NewLine><p>Moreover, as all the processes load their own module locally on each of the devices without a broadcast during initialization, is not it unexpected?</p><NewLine></blockquote><NewLine><p>There is a broadcast in DDP ctor. Please see the link below:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/c71ec1c717e5b225f28ef3bacde416c69f3c4d77/torch/nn/parallel/distributed.py#L324-L329"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/c71ec1c717e5b225f28ef3bacde416c69f3c4d77/torch/nn/parallel/distributed.py#L324-L329"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/c71ec1c717e5b225f28ef3bacde416c69f3c4d77/torch/nn/parallel/distributed.py#L324-L329</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""324"" style=""counter-reset: li-counter 323 ;""><NewLine><li># Sync params and buffers</li><NewLine><li>module_states = list(self.module.state_dict().values())</li><NewLine><li>if len(module_states) &gt; 0:</li><NewLine><li>    self._distributed_broadcast_coalesced(</li><NewLine><li>        module_states,</li><NewLine><li>        self.broadcast_bucket_size)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>, (sorry for such a late read/response to this message)</p><NewLine><p>So, I figured out that using either the flag NCCL_P2P_LEVEL=0 or NCCL_P2P_DISABLE=1, DDP runs fine on a machine with &gt;8 GPUs. Here I am specifically talking about 10 GPUs in the same machine. I am not sure about the topology of the 256 GPUs that you mentioned.</p><NewLine><p>Right now, I can not locate the doc page where I had seen that nn.dataparallel does not run (efficiently or something?) on more than 8 GPUs in a machine. Maybe it was in the previous version of the doc? In any case, I will confirm this one by testing it on a machine with 10 GPUs that I have access to.</p><NewLine><p>Thanks.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""65485"" data-username=""bapi""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/bapi/40/10124_2.png"" width=""20""/> bapi:</div><NewLine><blockquote><NewLine><p>I am not sure about the topology of the 256 GPUs that you mentioned.</p><NewLine></blockquote><NewLine></aside><NewLine><p>In that test, each node only has 8 GPUs.</p><NewLine><blockquote><NewLine><p>So, I figured out that using either the flag NCCL_P2P_LEVEL=0 or NCCL_P2P_DISABLE=1, DDP runs fine on a machine with &gt;8 GPUs.</p><NewLine></blockquote><NewLine><p>I see. We don’t have tests covering &gt; 8GPUs per node cases yet. This is an important message, thanks for sharing!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/bapi; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/bapi; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/bapi; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: December 31, 2019, 11:30pm; <NewLine> REPLY_DATE 2: January 1, 2020,  1:05am; <NewLine> REPLY_DATE 3: January 1, 2020, 12:38am; <NewLine> REPLY_DATE 4: July 1, 2020,  5:50pm; <NewLine> REPLY_DATE 5: July 7, 2020,  3:35pm; <NewLine> REPLY_DATE 6: July 7, 2020,  7:06pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> 
88170,Confusion about &lsquo;master_port&rsquo;,2020-07-07T02:38:00.873Z,1,60,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, there. Recently I used multiple cpu cores for training. On my own PC, macbook 2017 (1 cpu 4 cores), I just set os.environ[‘MASTER_PORT’] as one single value and multiprocesses could run on the same server. However, when I migrated codes to the cluster in order to use more cores, I need to give a different value to os.environ[‘MASTER_PORT’]  for each process. If not, the permission denied as below.</p><NewLine><pre><code class=""lang-auto"">store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)<NewLine>RuntimeError: Permission denied<NewLine></code></pre><NewLine><p>I don’t know much about the reason here, could someone explain it?</p><NewLine></div>",https://discuss.pytorch.org/u/Meraki,(Teng Ma),Meraki,"July 7, 2020,  2:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/meraki"">@Meraki</a> the <code>MASTER_PORT</code> needs to be set to the same value for all processes, otherwise, they cannot conduct rendezvous correctly. This error might be caused by other configurations. It might be helpful to print the value of the following environment variables on all processes right before <code>init_process_group</code> is called: “MASTER_ADDR”, “MASTER_PORT”, “RANK”, “WORLD_SIZE”</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yep, you are right. I just figure out the reason is that on Linux, you need root permissions to open a port below 1024. That’s why the permison denied.</p><NewLine><p>Thanks for your help.<a class=""mention"" href=""/u/mrshenli"">@mrshenli</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Meraki; <NewLine> ,"REPLY_DATE 1: July 7, 2020,  2:45am; <NewLine> REPLY_DATE 2: July 7, 2020,  3:32am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
88163,Update self defined parameters when using distributed data parallel,2020-07-07T00:12:27.983Z,1,67,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using distributed data parallel to train the model on multiple gpus. I meet one problem: I have used register_buffer to define one parameter. In addition, I need to manually update it. How could I achieve this? I tried to do the same update as the model is trained on one gpu, but the results are not correct. It seems that the value of this parameter is not synchronized over gpus. Thanks a lot</p><NewLine></div>",https://discuss.pytorch.org/u/jetcai1900,,jetcai1900,"July 7, 2020, 12:12am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If this is a model parameter, any reason for using <code>register_buffer</code> instead of <code>register_parameter</code>?</p><NewLine><blockquote><NewLine><p>In addition, I need to manually update it. How could I achieve this?</p><NewLine></blockquote><NewLine><p>If it is a parameter (not a buffer) and if you don’t expect the autograd engine to compute gradients for you, you can set its <code>.requires_grad</code> field to <code>False</code> before passing the model to the DDP ctor. Then, DDP won’t sync it’s grads and optimizer won’t update the parameter value.</p><NewLine><blockquote><NewLine><p>I tried to do the same update as the model is trained on one gpu, but the results are not correct. It seems that the value of this parameter is not synchronized over gpus.</p><NewLine></blockquote><NewLine><p>I might miss something. Looks like you want to <strong>manually</strong> update a parameter, but still want DDP to help synchronize the parameter across the GPUs/processes? I don’t fully understand the use case, could you please elaborate this? If this parameter is manually updated, would it be possible to let all processes to set it to the same value?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><strong>""I might miss something. Looks like you want to  <strong>manually</strong>  update a parameter, but still want DDP to help synchronize the parameter across the GPUs/processes""</strong>’<br/><NewLine>Yes. This is what I would like to achieve.</p><NewLine><p>For example<br/><NewLine>class model():<br/><NewLine>def <strong>init</strong>(self):<br/><NewLine>a = torch.zeros((3, 1))<br/><NewLine>self.register_buffer(“a”, a)</p><NewLine><p>def update_a(self, b):<br/><NewLine>self.a.add_(b)<br/><NewLine>b is a vector that is dynamic with respect to the input data. So I could not manually set it to the same value.</p><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""88163"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>If this parameter is manually updated, would it be possible to let all processes to set it to the same value?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Let me know whether I describe the problem clearly? Thanks a lot for your help.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jetcai1900; <NewLine> ,"REPLY_DATE 1: July 7, 2020,  2:38am; <NewLine> REPLY_DATE 2: July 7, 2020,  3:17am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
87659,Torch distributed not working on two machines [nccl backend],2020-07-01T23:15:29.088Z,5,257,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am running a simple application on two machines with 2 gpus each, it is throwing me an error. The application works fine on a single machine with 2gpus.</p><NewLine><p>The NCCL info error info in here</p><NewLine><p>dml4:26072:26072 [1] NCCL INFO Bootstrap : Using [0]XXXXXX&lt;0&gt; [1]enp0s20f0u1u6:169.254.95.120&lt;0&gt; [2]virbr0:192.168.122.1&lt;0&gt;<br/><NewLine>dml4:26071:26071 [0] NCCL INFO Bootstrap : Using [0]XXXXX&lt;0&gt; [1]enp0s20f0u1u6:169.254.95.120&lt;0&gt; [2]virbr0:XXXXX&lt;0&gt;<br/><NewLine>dml4:26072:26072 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<br/><NewLine>dml4:26071:26071 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<br/><NewLine>dml4:26072:26072 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE ; OOB enp88s0:9.1.44.100&lt;0&gt;<br/><NewLine>dml4:26071:26071 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE ; OOB enp88s0:9.1.44.100&lt;0&gt;<br/><NewLine>dml4:26072:26240 [1] NCCL INFO Setting affinity for GPU 1 to ffff,f00000ff,fff00000<br/><NewLine>dml4:26071:26242 [0] NCCL INFO Setting affinity for GPU 0 to 0fffff00,000fffff<br/><NewLine>dml4:26072:26240 [1] NCCL INFO CUDA Dev 1[1], IB NIC distance :  SYS<br/><NewLine>dml4:26071:26242 [0] NCCL INFO CUDA Dev 0[0], IB NIC distance :  NODE<br/><NewLine>dml4:26071:26242 [0] NCCL INFO Ring 00 : 1 -&gt; 2 [receive] via NET/IB/0<br/><NewLine>dml4:26071:26242 [0] NCCL INFO Ring 00 : 2[0] -&gt; 3[1] via direct shared memory<br/><NewLine>dml4:26072:26240 [1] NCCL INFO Ring 00 : 3 -&gt; 0 [send] via NET/IB/0</p><NewLine><p>dml4:26072:26240 [1] misc/ibvwrap.cc:252 NCCL WARN Call to ibv_reg_mr failed<br/><NewLine>dml4:26072:26240 [1] NCCL INFO transport/net_ib.cc:601 -&gt; 2<br/><NewLine>dml4:26072:26240 [1] NCCL INFO include/net.h:24 -&gt; 2<br/><NewLine>dml4:26072:26240 [1] NCCL INFO transport/net.cc:360 -&gt; 2<br/><NewLine>dml4:26072:26240 [1] NCCL INFO init.cc:669 -&gt; 2<br/><NewLine>dml4:26072:26240 [1] NCCL INFO init.cc:815 -&gt; 2<br/><NewLine>dml4:26072:26240 [1] NCCL INFO init.cc:951 -&gt; 2<br/><NewLine>dml4:26072:26240 [1] NCCL INFO misc/group.cc:69 -&gt; 2 [Async thread]</p><NewLine><p>dml4:26071:26242 [0] misc/ibvwrap.cc:252 NCCL WARN Call to ibv_reg_mr failed<br/><NewLine>dml4:26071:26242 [0] NCCL INFO transport/net_ib.cc:601 -&gt; 2<br/><NewLine>dml4:26071:26242 [0] NCCL INFO include/net.h:24 -&gt; 2<br/><NewLine>dml4:26071:26242 [0] NCCL INFO transport/net.cc:388 -&gt; 2<br/><NewLine>dml4:26071:26242 [0] NCCL INFO init.cc:679 -&gt; 2<br/><NewLine>dml4:26071:26242 [0] NCCL INFO init.cc:815 -&gt; 2<br/><NewLine>dml4:26071:26242 [0] NCCL INFO init.cc:951 -&gt; 2<br/><NewLine>dml4:26071:26242 [0] NCCL INFO misc/group.cc:69 -&gt; 2 [Async thread]<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “conv_dist.py”, line 118, in <br/><NewLine>main()<br/><NewLine>File “conv_dist.py”, line 51, in main<br/><NewLine>mp.spawn(train, nprocs=args.gpus, args=(args,), join=True)<br/><NewLine>File “/work/tools/envs/dine2/lib/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 200, in spawn<br/><NewLine>return start_processes(fn, args, nprocs, join, daemon, start_method=‘spawn’)<br/><NewLine>File “work/tools/envs/dine2/lib/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 158, in start_processes<br/><NewLine>while not context.join():<br/><NewLine>File “/work/tools/envs/dine2/lib/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 119, in join<br/><NewLine>raise Exception(msg)<br/><NewLine>Exception:</p><NewLine><p>– Process 0 terminated with the following error:<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “/work/tools/envs/dine2/lib/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 20, in _wrap<br/><NewLine>fn(i, *args)<br/><NewLine>File “/us4j4248/pt_dist/conv_dist.py”, line 75, in train<br/><NewLine>model = DDP(model, device_ids=[gpu])<br/><NewLine>File “/work/tools/envs/dine2/lib/python3.6/site-packages/torch/nn/parallel/distributed.py”, line 285, in <strong>init</strong><br/><NewLine>self.broadcast_bucket_size)<br/><NewLine>File “/work/tools/envs/dine2/lib/python3.6/site-packages/torch/nn/parallel/distributed.py”, line 496, in _distributed_broadcast_coalesced<br/><NewLine>dist._broadcast_coalesced(self.process_group, tensors, buffer_size)<br/><NewLine>RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1591914838379/work/torch/lib/c10d/ProcessGroupNCCL.cpp:514, unhandled system error, NCCL version 2.4.8</p><NewLine><p>ps: I have removed the ip addresses above.</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/nash,(Nash),nash,"July 1, 2020, 11:15pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/nash"">@nash</a>, could you please share a minimum repro of this error?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey Shen,<br/><NewLine>I am running a simple application using &lt;torch.distributed.launch&gt; on two machines each having 2 gpus. It throws me the above error.<br/><NewLine>CUDA 10.2<br/><NewLine>Pytorch 1.5.1<br/><NewLine>NCCL backend as<br/><NewLine><em>libnccl-devel-2.5.6-1+cuda10.2.x86_64</em><br/><NewLine><em>libnccl-2.5.6-1+cuda10.2.x86_64</em><br/><NewLine><em>libnccl-static-2.5.6-1+cuda10.2.x86_64</em></p><NewLine><p>But torch.cuda.nccl.version() shows me 2.4.<br/><NewLine>Ran NCCL tests – they are working fine.</p><NewLine><pre><code class=""lang-auto"">def train(args):<NewLine>    current_env = os.environ.copy()<NewLine>    dist.init_process_group(backend='nccl', init_method='env://')<NewLine>    model = ConvNet()<NewLine>    torch.cuda.set_device(args.local_rank)<NewLine>    model.cuda(args.local_rank)<NewLine>    batch_size = 256<NewLine>    # define loss function (criterion) and optimizer<NewLine>    criterion = nn.CrossEntropyLoss().cuda(args.local_rank)<NewLine>    optimizer = torch.optim.SGD(model.parameters(), 1e-4)<NewLine><NewLine>    model = DDP(model, device_ids=[args.local_rank])<NewLine>    # Data loading code<NewLine>    train_dataset = torchvision.datasets.MNIST(root='./data',<NewLine>                                               train=True,<NewLine>                                               transform=transforms.ToTensor(),<NewLine>                                               download=True)<NewLine><NewLine>    train_sampler = torch.utils.data.distributed.DistributedSampler(<NewLine>        train_dataset, num_replicas=int(current_env[""WORLD_SIZE""]), rank=args.local_rank)<NewLine><NewLine>    train_loader = torch.utils.data.DataLoader(<NewLine>    	dataset=train_dataset,<NewLine>        batch_size=batch_size,<NewLine>        shuffle=False,         <NewLine>        num_workers=0,<NewLine>        pin_memory=True,<NewLine>        sampler=train_sampler)  <NewLine></code></pre><NewLine><p>Plz let me know if you need any more info.<br/><NewLine>Thanks</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/nash"">@nash</a>, thanks for sharing the code. The code looks correct to me, except the rank argument in <code>DistributedSampler</code> might need to be global rank (i.e., <code>current_env[""RANK""]</code>) instead of the local rank? But this is not the cause of the error, as the error was thrown in DDP ctor when it tries to run broadcast.</p><NewLine><p>Regarding the error:</p><NewLine><ol start=""0""><NewLine><li>Curious, does <code>gloo</code> backend work?</li><NewLine><li>Could you please print out the env vars set by the launching script? Sth as what <a href=""https://github.com/pytorch/examples/tree/master/distributed/ddp#argument-passing-convention"" rel=""nofollow noopener"">this example</a> does to <code>env_dict</code>.</li><NewLine><li>Could you try run the following command on both nodes? If the return IP is not what you intended, you might need to set either <code>GLOO_SOCKET_IFNAME</code> or <code>NCCL_SOCKET_IFNAME</code> as mentioned <a href=""https://pytorch.org/docs/stable/distributed.html#choosing-the-network-interface-to-use"" rel=""nofollow noopener"">here</a> depending on which backend you are using.<pre><code class=""lang-auto"">getent hosts `hostname`<NewLine></code></pre><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> thanks for your response.</p><NewLine><ol><NewLine><li>Gloo backend works perfectly fine with this code. NCCL backend throws the error.</li><NewLine></ol><NewLine><p>2</p><NewLine><pre><code class=""lang-auto"">here are the outputs of environ based on Gloo backend.<NewLine>Node 0 output<NewLine>Initializing process group with: {'MASTER_ADDR': 'a.b.c.d', 'MASTER_PORT': '12354', 'RANK': '1', 'WORLD_SIZE': '4'}<NewLine>[3196] Initializing process group with: {'MASTER_ADDR': 'a.b.c.d', 'MASTER_PORT': '12354', 'RANK': '0', 'WORLD_SIZE': '4'}<NewLine>[3196] world_size = 4, rank = 0, backend=gloo<NewLine>[3197] world_size = 4, rank = 1, backend=gloo<NewLine>Node 1 output<NewLine>Initializing process group with: {'MASTER_ADDR': 'a.b.c.d', 'MASTER_PORT': '12354', 'RANK': '2', 'WORLD_SIZE': '4'}<NewLine>[89966] Initializing process group with: {'MASTER_ADDR': 'a.b.c.d', 'MASTER_PORT': '12354', 'RANK': '3', 'WORLD_SIZE': '4'}<NewLine>[89966] world_size = 4, rank = 3, backend=gloo<NewLine>[89965] world_size = 4, rank = 2, backend=gloo<NewLine></code></pre><NewLine><ol start=""3""><NewLine><li>the return is the domain name addresses of each host (a.b.c.d) etc. I am not setting anywhere SOCKET_IFNAME</li><NewLine></ol><NewLine><p>My system has NCCL 2.5 while torch (torch.cuda.nccl.version()) shows 2.4.8<br/><NewLine>Could this be the problem?<br/><NewLine>How can I upgrade NCCL version in torch.</p><NewLine><p>Thanks.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>If Gloo works fine then it means all the env vars and configs should be correct.</p><NewLine><blockquote><NewLine><p>How can I upgrade NCCL version in torch.</p><NewLine></blockquote><NewLine><p>That will require modify pytorch NCCL submodule and recompile. Like this <a href=""https://github.com/pytorch/pytorch/pull/40622"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/40622</a>. You can pull this PR can compile from it, which should be using NCCL 2.7.3.</p><NewLine><p>Another option is to set <code>export USE_SYSTEM_NCCL=1</code>, and then compile from source, then it should be using the 2.5 that you installed on the machine.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>.<br/><NewLine>As you mentioned that pytorch has NCCL precompiled and both nodes use the same version of NCCL.<br/><NewLine>Does that mean NCCL version is not the problem?</p><NewLine><p>Did you notice this “misc/ibvwrap.cc:252 NCCL WARN Call to ibv_reg_mr failed” in the logs.</p><NewLine><p>I tried to build torch from source, I hit another roadblock there as well.<br/><NewLine>“Performing Test SUPPORT_GLIBCXX_USE_C99 - Failed”</p><NewLine><p>thanks.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>An error in ibv (i.e., InfiniBand verbs) indicates problems with GPU Direct, which NCCL tries to use for RDMA but which Gloo doesn’t. You can try to confirm that this is indeed the issue by running with the <code>NCCL_IB_DISABLE=1</code> env var. That may work but would probably end up being slower. In that case you might want to follow the instructions here to troubleshoot InfiniBand issues: <a href=""https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#gpu-direct"" rel=""nofollow noopener"">https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#gpu-direct</a></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/lcw"">@lcw</a>, thanks. Indeed it is NCCL issue, and setting NCCL_IB_DISABLE =1 works fine.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/nash; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/nash; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/nash; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/lcw; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/nash; <NewLine> ,"REPLY_DATE 1: July 2, 2020,  1:40am; <NewLine> REPLY_DATE 2: July 2, 2020,  2:06am; <NewLine> REPLY_DATE 3: July 2, 2020,  7:51pm; <NewLine> REPLY_DATE 4: July 2, 2020,  9:07pm; <NewLine> REPLY_DATE 5: July 2, 2020,  9:15pm; <NewLine> REPLY_DATE 6: July 2, 2020, 10:47pm; <NewLine> REPLY_DATE 7: July 6, 2020,  7:28pm; <NewLine> REPLY_DATE 8: July 6, 2020,  7:30pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 3 Likes; <NewLine> REPLY 8 LIKES: ; <NewLine> 
87921,Shared memory with torch.multiprocessing,2020-07-04T08:02:22.485Z,7,558,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey folks,</p><NewLine><p>I have a server with large amounts of RAM, but slow storage and I want to speed up training by having my dataset in the RAM. I also use <code>DDP</code> which means there are going to be multiple processes per GPU. On top of that, I use multiple <code>num_workers</code> in my <code>dataloader</code> so having a simple Python list as a  caxhe would mean multiple caches which eats up a lot of memory.</p><NewLine><p>The natural solution is to use shared memory. And this is how I use it</p><NewLine><ol><NewLine><li>In the launch process, do</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">if __name__ == '__main__':<NewLine>    import argparse<NewLine>    import os<NewLine>    import torch.multiprocessing as mp<NewLine>    import ctypes<NewLine><NewLine>    shared_base = mp.Array(ctypes.c_byte, 80000*3*256*256, lock=True)<NewLine>    with shared_base.get_lock():<NewLine>        shared_array = np.ctypeslib.as_array(shared_base.get_obj())<NewLine>        img_cache = shared_array.reshape(80000, 256, 256, 3)<NewLine><NewLine>    use_cache = mp.Array(ctypes.c_float, 1, lock=False)<NewLine>    use_cache[0] = -3.14<NewLine></code></pre><NewLine><ol start=""2""><NewLine><li>This cache is sent to each process as</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">mp.spawn(main, nprocs=ngpus_per_node, args=(args, img_cache, use_cache))<NewLine></code></pre><NewLine><ol start=""3""><NewLine><li>Each process takes it this shared memory and gives it to a dataset object</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">        dset = SVAE_FFHQ(args.data_folder, transform, 32, 64, args.hidden_size, img_cache, use_cache)<NewLine><NewLine></code></pre><NewLine><ol start=""4""><NewLine><li>The <code>SVAE_FFHQ</code> class does looks like this:</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">class SVAE_FFHQ(data.Dataset):<NewLine>    def __init__(self, root_dir, transform=None, top_size=32, bottom_size=64, dim=256, img_cache=None, use_cache=None):<NewLine>        super().__init__()<NewLine>        ...<NewLine>        self.img_cache = img_cache<NewLine>        self.use_cache = use_cache<NewLine><NewLine><NewLine>    def _use_cache(self):<NewLine>        self.use_cache[0] = 3.14<NewLine>        print('Using cache')<NewLine><NewLine>    def __getitem__(self, idx):<NewLine>        path, lbl = self.dset.samples[idx]<NewLine><NewLine>        if self.use_cache[0] &lt; 0:<NewLine>            with open(path, 'rb') as f:<NewLine>                    img = Image.open(f)<NewLine>                    img = img.convert('RGB')<NewLine>                    img = img.resize((256, 256), Image.LANCZOS)<NewLine>            <NewLine>            self.img_cache[idx] = deepcopy(np.asarray(img))<NewLine>            del img<NewLine>        <NewLine>        return self.transform(Image.fromarray(self.img_cache[idx], 'RGB'))<NewLine></code></pre><NewLine><p>This to me seems fine, but what happens is</p><NewLine><ol><NewLine><li>The shared memory is pickled and not replicated across the multiple spawned processes which means my memory requirments increase with the number of processes spawned.</li><NewLine><li>This isn’t any faster than reading data off of slow HDDs</li><NewLine></ol><NewLine><p>Any insight into these problems?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/wamreyaz,(wamreyaz),wamreyaz,"July 4, 2020,  8:02am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>I cannot replicate your first problem with the below code snippet, memory is not pickled as you can see in the screenshot, only the main process holds the 1GB shared memory array:</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">import ctypes<NewLine>import time<NewLine>import numpy as np<NewLine>import multiprocessing as mp<NewLine><NewLine><NewLine>def subproc(array):<NewLine>    with array.get_lock():<NewLine>        np_array = np.ctypeslib.as_array(array.get_obj())<NewLine>        print(np_array[1000])<NewLine><NewLine>    # keep process showing in ""top""<NewLine>    begin = time.time()<NewLine>    while time.time() - begin &lt; 10:<NewLine>        a = 1 * 100<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    array = mp.Array(ctypes.c_byte, 1000*1024*1024, lock=True)<NewLine>    with array.get_lock():<NewLine>        np_array = np.ctypeslib.as_array(array.get_obj())<NewLine>        np_array.fill(100)<NewLine>    print(""allocated"")<NewLine>    p = mp.Process(target=subproc, args=(array,))<NewLine>    p2 = mp.Process(target=subproc, args=(array,))<NewLine>    p.start()<NewLine>    p2.start()<NewLine>    print(""started"")<NewLine><NewLine>    # keep process showing in ""top""<NewLine>    begin = time.time()<NewLine>    while time.time() - begin &lt; 10:<NewLine>        a = 1 * 100<NewLine>    p.join()<NewLine>    p2.join()<NewLine>    print(""joined"")<NewLine></code></pre><NewLine><p><img alt=""image"" data-base62-sha1=""3SV2fKZvFJc6JvEJKYvFyIesbsE"" height=""68"" src=""https://discuss.pytorch.org/uploads/default/original/3X/1/b/1b3bcfa32a454d99f05015ae09fb325ab2e2d59c.png"" width=""536""/></p><NewLine><ol start=""2""><NewLine><li>I think the second question might be related to <code>Image.fromarray</code>, see this <a href=""https://stackoverflow.com/questions/11172929/can-you-create-a-pil-image-with-cython-without-copying-memory-redundantly"" rel=""nofollow noopener"">issue</a><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for implementing this by yourself.</p><NewLine><p>I must ask a couple of questions though:</p><NewLine><ol><NewLine><li>What was the version of Python you used? I know that some stuff changed in how Python pickles data since version 3.8.</li><NewLine><li>I use mp.spawn to start the processes, where mp is imported from torch.multiprocessing.</li><NewLine></ol><NewLine><p>Could that be a problem?</p><NewLine><p>Anyways this looks interesting. Thanks again.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am using the default python3.5.2 installation from ubuntu16.04</p><NewLine><p>I have added <code>mp.set_start_method(""spawn"")</code> and still cannot reproduce your issue, it would be better if you can share a minimal problematic code snippet <img alt="":blush:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/blush.png?v=9"" title="":blush:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks again.</p><NewLine><p>Let me also try your code on my machine. And create a minimal code implementation that I can share here.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is the code I run</p><NewLine><pre><code class=""lang-auto"">import ctypes<NewLine>import time<NewLine>import numpy as np<NewLine>import torch.multiprocessing as mp<NewLine><NewLine>def subproc2(gpu, array):<NewLine>    with array.get_lock():<NewLine>        np_array = np.ctypeslib.as_array(array.get_obj())<NewLine>        print(np_array[1000])<NewLine>        if gpu == 0:<NewLine>            np_array[999] = 0<NewLine>        elif gpu == 1:<NewLine>            np_array[1000] = 1 <NewLine><NewLine>    # keep process showing in ""top""<NewLine>    begin = time.time()<NewLine>    while time.time() - begin &lt; 10:<NewLine>        a = 1 * 100<NewLine><NewLine>    return 0<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    mp.set_start_method('spawn')<NewLine>    array = mp.Array(ctypes.c_byte, 1000*1024*1024, lock=True)<NewLine>    with array.get_lock():<NewLine>        np_array = np.ctypeslib.as_array(array.get_obj())<NewLine>        np_array.fill(100)<NewLine><NewLine>    print(""allocated"")<NewLine>    mp.spawn(subproc2, args=(array,), nprocs=2)<NewLine><NewLine>    # keep process showing in ""top""<NewLine>    print(np_array[999:10001])<NewLine>    begin = time.time()<NewLine>    while time.time() - begin &lt; 100:<NewLine>        a = 1 * 100<NewLine>    <NewLine>    print('done')<NewLine></code></pre><NewLine><p>The observations are :</p><NewLine><ol><NewLine><li>Memory is only ever allocated in the main process.</li><NewLine><li>If I don’t set the start method to <code>spawn</code> on Linux, I get a <code>SIGSEGV</code> even on access in <code>subproc2</code>.</li><NewLine></ol><NewLine><p>Now what I want to test next is if I pass this memory on to dataloaders as is - an <code>np.array</code>, if it will increase memory consumption.</p><NewLine><p>Let me do that now.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">import ctypes<NewLine>import time<NewLine>import numpy as np<NewLine>import torch.multiprocessing as mp<NewLine>import torch<NewLine>from torch.utils.data import Dataset<NewLine><NewLine>class SharedDataset1(Dataset):<NewLine>    def __init__(self, shared_mem):<NewLine>        super(SharedDataset1, self).__init__()<NewLine>        self.shared_mem = shared_mem<NewLine><NewLine>    def __len__(self):<NewLine>        return 10000<NewLine><NewLine>    def __getitem__(self, idx):<NewLine>        return torch.randn(3, 32, 32)<NewLine><NewLine>def np_before(gpu, array, num_workers):<NewLine>    with array.get_lock():<NewLine>        np_array = np.ctypeslib.as_array(array.get_obj())<NewLine><NewLine>    dataset = SharedDataset1(np_array)<NewLine>    dataloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=num_workers)<NewLine><NewLine>    for img in dataloader:<NewLine>        c = img+0.1<NewLine>        time.sleep(3)<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    mp.set_start_method('spawn')<NewLine>    array = mp.Array(ctypes.c_byte, 10000*1024*1024, lock=True)<NewLine>    with array.get_lock():<NewLine>        np_array = np.ctypeslib.as_array(array.get_obj())<NewLine>        np_array.fill(100)<NewLine><NewLine>    print(""allocated"")<NewLine>    mp.spawn(np_before, args=(array, 1), nprocs=2)<NewLine>    print(""started"")<NewLine><NewLine>    begin = time.time()<NewLine>    while time.time() - begin &lt; 100:<NewLine>        a = 1 * 100<NewLine></code></pre><NewLine><p>This gives the following error</p><NewLine><blockquote><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “mem_data.py”, line 64, in <br/><NewLine>mp.spawn(np_before, args=(array, 1), nprocs=2)<br/><NewLine>File “/home/parawr/.conda/envs/faclab/lib/python3.7/site-packages/torch/multiprocessing/spawn.py”, line 200, in spawn<br/><NewLine>return start_processes(fn, args, nprocs, join, daemon, start_method=‘spawn’)<br/><NewLine>File “/home/parawr/.conda/envs/faclab/lib/python3.7/site-packages/torch/multiprocessing/spawn.py”, line 158, in start_processes<br/><NewLine>while not context.join():<br/><NewLine>File “/home/parawr/.conda/envs/faclab/lib/python3.7/site-packages/torch/multiprocessing/spawn.py”, line 119, in join<br/><NewLine>raise Exception(msg)</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “/home/parawr/.conda/envs/faclab/lib/python3.7/site-packages/torch/multiprocessing/spawn.py”, line 20, in _wrap<br/><NewLine>fn(i, *args)<br/><NewLine>File “/home/parawr/Projects/shared/mem_data.py”, line 40, in np_before<br/><NewLine>for img in dataloader:<br/><NewLine>File “/home/parawr/.conda/envs/faclab/lib/python3.7/site-packages/torch/utils/data/dataloader.py”, line 279, in <strong>iter</strong><br/><NewLine>return _MultiProcessingDataLoaderIter(self)<br/><NewLine>File “/home/parawr/.conda/envs/faclab/lib/python3.7/site-packages/torch/utils/data/dataloader.py”, line 719, in <strong>init</strong><br/><NewLine>w.start()<br/><NewLine>File “/home/parawr/.conda/envs/faclab/lib/python3.7/multiprocessing/process.py”, line 112, in start<br/><NewLine>self._popen = self._Popen(self)<br/><NewLine>File “/home/parawr/.conda/envs/faclab/lib/python3.7/multiprocessing/context.py”, line 223, in _Popen<br/><NewLine>return _default_context.get_context().Process._Popen(process_obj)<br/><NewLine>File “/home/parawr/.conda/envs/faclab/lib/python3.7/multiprocessing/context.py”, line 284, in _Popen<br/><NewLine>return Popen(process_obj)<br/><NewLine>File “/home/parawr/.conda/envs/faclab/lib/python3.7/multiprocessing/popen_spawn_posix.py”, line 32, in <strong>init</strong><br/><NewLine>super().<strong>init</strong>(process_obj)<br/><NewLine>File “/home/parawr/.conda/envs/faclab/lib/python3.7/multiprocessing/popen_fork.py”, line 20, in <strong>init</strong><br/><NewLine>self._launch(process_obj)<br/><NewLine>File “/home/parawr/.conda/envs/faclab/lib/python3.7/multiprocessing/popen_spawn_posix.py”, line 47, in _launch<br/><NewLine>reduction.dump(process_obj, fp)<br/><NewLine>File “/home/parawr/.conda/envs/faclab/lib/python3.7/multiprocessing/reduction.py”, line 60, in dump<br/><NewLine>ForkingPickler(file, protocol).dump(obj)<br/><NewLine>OverflowError: cannot serialize a bytes object larger than 4 GiB</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>This thing works</p><NewLine><pre><code class=""lang-auto"">def np_after(gpu, array, num_workers):<NewLine>    dataset = SharedDataset1(array)<NewLine>    dataloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=num_workers)<NewLine><NewLine>    for ii, img in enumerate(dataloader):<NewLine>        print(ii)<NewLine>        c = img+0.1<NewLine>        time.sleep(1)<NewLine>    return 0<NewLine><NewLine>class SharedDataset2(Dataset):<NewLine>    def __init__(self, shared_mem):<NewLine>        super(SharedDataset2, self).__init__()<NewLine>        with shared_mem.get_lock():<NewLine>            self.shared_mem = np.ctypeslib.as_array(shared_mem.get_obj())<NewLine><NewLine>    def __len__(self):<NewLine>        return 10000<NewLine><NewLine>    def __getitem__(self, idx):<NewLine>        return torch.randn(3, 32, 32)<NewLine></code></pre><NewLine><p>I think it might have to do with what Python pickles - the array/shared_mem is a <code>SynchronizedArray</code> object which Python <em>knows</em> no to pickle but share, while in the <code>np_before</code> function it is now an <code>np.ndarray</code> which Python tries to pickle.</p><NewLine><p>Let me now try and fix the PIL issue. I will benchmark it to see if it actually speed up data-loading.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>I will also have to check validity - I wouldn’t want the workers from the dataloader overwriting each others’  data</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>The problem source is correct, since the dataset passed to <code>Dataloader</code> will be distributed to workers, and therefore will be pickled. Good work, looking forward to your result. <img alt="":blush:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/blush.png?v=9"" title="":blush:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/wamreyaz; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/wamreyaz; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/wamreyaz; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/wamreyaz; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/wamreyaz; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/wamreyaz; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/wamreyaz; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: July 5, 2020,  5:13pm; <NewLine> REPLY_DATE 2: July 5, 2020,  8:32pm; <NewLine> REPLY_DATE 3: July 6, 2020,  1:19am; <NewLine> REPLY_DATE 4: July 6, 2020, 10:59am; <NewLine> REPLY_DATE 5: July 6, 2020, 12:01pm; <NewLine> REPLY_DATE 6: July 6, 2020, 12:21pm; <NewLine> REPLY_DATE 7: July 6, 2020, 12:23pm; <NewLine> REPLY_DATE 8: July 6, 2020, 12:29pm; <NewLine> REPLY_DATE 9: July 6, 2020, 12:30pm; <NewLine> REPLY_DATE 10: July 6, 2020,  3:51pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> 
88015,Tools suggestion for testing with RPC api,2020-07-05T13:43:47.301Z,2,90,"<div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> This question is not that closely related to pytorch, but it is really hard to find related resources on stackoverflow etc. My framework (mentioned in <a href=""https://discuss.pytorch.org/t/about-distributed-autograd-and-optimizer/84047/6"">last post</a>) development have finally reached a critical stage, I am testing the distributed part now.</p><NewLine><p>So do you know any python libraries such as <a href=""https://github.com/NetSys/demi"" rel=""nofollow noopener"">DEMi</a>, <a href=""https://github.com/nccgroup/fuzzowski"" rel=""nofollow noopener"">fuzzowski</a> which can simulate a connection layer and let users <strong>send &amp; recv</strong>, <strong>shift, reorder &amp; delay</strong>, <strong>poke &amp; inspect</strong> messages? Since I need to simulate the RPC layer, these two libraries metioned above are not appropriate for this task. What are the development team of pytorch using to test your rpc service? Any idea would be great.</p><NewLine><p>BTW, my library is hosted at <a href=""https://github.com/iffiX/machin"" rel=""nofollow noopener"">https://github.com/iffiX/machin</a> <img alt="":smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smile.png?v=9"" title="":smile:""/> Come and have a look! the tutorial is not complete though, but api documentation and tests for most core functions should be close to complete now, will release the first milestone soon.</p><NewLine></div>",https://discuss.pytorch.org/u/iffiX,(Iffi),iffiX,"July 5, 2020,  1:44pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/iffix"">@iffiX</a>, thanks a lot for sharing the exciting project!! We will certainly study and share with other team members. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>Regarding tests for delay/drop/retry messages, <a class=""mention"" href=""/u/osalpekar"">@osalpekar</a> implemented a <a href=""https://github.com/pytorch/pytorch/blob/1e64bf4c40ef82d6bc3dcc42b3874353f7632be0/torch/csrc/distributed/rpc/testing/faulty_process_group_agent.h"" rel=""nofollow noopener"">faulty agent</a> for that purpose, and the majority of faulty tests can be found <a href=""https://github.com/pytorch/pytorch/blob/1e64bf4c40ef82d6bc3dcc42b3874353f7632be0/torch/testing/_internal/distributed/rpc/rpc_test.py#L3398"" rel=""nofollow noopener"">here</a>.</p><NewLine><blockquote><NewLine><p>So do you know any python libraries such as <a href=""https://github.com/NetSys/demi"" rel=""nofollow noopener"">DEMi</a>, <a href=""https://github.com/nccgroup/fuzzowski"" rel=""nofollow noopener"">fuzzowski</a> which can simulate a connection layer and let users  <strong>send &amp; recv</strong> ,  <strong>shift, reorder &amp; delay</strong> ,  <strong>poke &amp; inspect</strong>  messages? Since I need to simulate the RPC layer, these two libraries metioned above are not appropriate for this task.</p><NewLine></blockquote><NewLine><p>If this is just for simulating RPCs, does mock work in this case? Sth like <a href=""https://github.com/pytorch/pytorch/pull/39728/files"" rel=""nofollow noopener"">this</a>.</p><NewLine><p>Also cc TensorPipe expert <a class=""mention"" href=""/u/lcw"">@lcw</a>, do you know any good tools for this purpose?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Mock is not sufficient, because I would like to implement tests based on fuzzy testing for <code>machin.parallel.distributed.election.ElectionGroupStableBase</code> and other higher level modules based on this core. Common white box testing and black box testing are not good enough for testing distributed core functions, so my testing idea comes from this <a href=""https://github.com/asatarin/testing-distributed-systems"" rel=""nofollow noopener"">github repo</a> and the blog post from its “Fuzzing” section <a href=""https://colin-scott.github.io/blog/2015/10/07/fuzzing-raft-for-fun-and-profit/"" rel=""nofollow noopener"">blog</a>.</p><NewLine><p>I think I will try to implement a simple framework myself today, maybe generate fuzzy data with <a href=""https://github.com/jtpereyda/boofuzz"" rel=""nofollow noopener"">boofuzz</a> and write a rpc simulation layer. I am still looking forward to other ideas of your team! <img alt="":blush:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/blush.png?v=9"" title="":blush:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I made a rpc mocker &amp; fuzzer, should be sufficient for now.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: July 5, 2020, 10:07pm; <NewLine> REPLY_DATE 2: July 6, 2020,  1:37am; <NewLine> REPLY_DATE 3: July 6, 2020,  9:54am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
87984,Load Data / Models to a specific CPU,2020-07-05T03:29:06.017Z,0,93,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m implementing an algorithm which requires a lot of model evaluations. I want to parallelize model evaluations by using CPU. The code segment I want to parallelize is here (simplified for readability):</p><NewLine><pre><code class=""lang-python"">def test_img(network, datatest):<NewLine>    network.eval()<NewLine><NewLine>    data_loader = DataLoader(datatest, batch_size=args.bs)<NewLine><NewLine>    for idx, (data, target) in enumerate(data_loader):<NewLine>        result = network(data)<NewLine></code></pre><NewLine><p>And for one evaluation it takes around 1.5s on CPU. The networks for evaluation are different, but the dataset is the same. I tried to use joblib.Parallel to parallelize this process:</p><NewLine><pre><code class=""lang-auto"">results = Parallel(n_jobs=num_cpu, prefer=""threads"")(delayed(test_img)(network_lst[i], dataset) for i in range(N))<NewLine></code></pre><NewLine><p>However, it seems that there are no improvement by using this method (it will take 15s for 10 evaluations). I specifically count the runtime for <code>result = network(data)</code> line, and it takes 8 seconds. Therefore I think neither the network evaluation nor the dataloader are parallelized. Are there any way for us to load network and data to a specific CPU core, like <code>data.to('cpu:0')</code>? Is it even possible to use CPU for model evaluation parallelization?</p><NewLine><p>Any suggestions are appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/t326wang,,t326wang,"July 5, 2020,  3:29am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>By default pytorch will use multiple cpu cores to calculate:<br/><NewLine><img alt=""image"" data-base62-sha1=""dXU0guhBB81uRxjQmVWz4NArasi"" height=""61"" src=""https://discuss.pytorch.org/uploads/default/original/3X/6/1/61e1c2b098ebfef2656624c76ff933dc5e4a0cca.png"" width=""677""/></p><NewLine><pre><code class=""lang-auto"">import time<NewLine>import multiprocessing as mp<NewLine>import torch as t<NewLine><NewLine>def subproc():<NewLine>    # keep process showing in ""top""<NewLine>    begin = time.time()<NewLine>    while time.time() - begin &lt; 10:<NewLine>        a = t.ones([1000, 1000]) * t.ones([1000, 1000])<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    p = mp.Process(target=subproc, args=())<NewLine>    p2 = mp.Process(target=subproc, args=())<NewLine>    p.start()<NewLine>    p2.start()<NewLine>    print(""started"")<NewLine><NewLine>    p.join()<NewLine>    p2.join()<NewLine>    print(""joined"")<NewLine></code></pre><NewLine><p>Seems that you are using “threads”, not good for python, you must use processes:</p><NewLine><pre><code class=""lang-auto"">results = Parallel(n_jobs=num_cpu, prefer=""threads"")(delayed(test_img)(network_lst[i], dataset) for i in range(N))<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: July 5, 2020,  5:20pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
87849,Why the second barrier is used in the DDP tutorial?,2020-07-03T13:03:50.792Z,2,95,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am reading the <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">DistributedDataparallel tutorial</a>. The last line from the following snippet confuses me:</p><NewLine><pre><code class=""lang-auto"">   if rank == 0:<NewLine>        # All processes should see same parameters as they all start from same<NewLine>        # random parameters and gradients are synchronized in backward passes.<NewLine>        # Therefore, saving it in one process is sufficient.<NewLine>        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)<NewLine><NewLine>    # Use a barrier() to make sure that process 1 loads the model after process<NewLine>    # 0 saves it.<NewLine>    dist.barrier()<NewLine>    # configure map_location properly<NewLine>    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}<NewLine>    ddp_model.load_state_dict(<NewLine>        torch.load(CHECKPOINT_PATH, map_location=map_location))<NewLine><NewLine>    optimizer.zero_grad()<NewLine>    outputs = ddp_model(torch.randn(20, 10))<NewLine>    labels = torch.randn(20, 5).to(rank)<NewLine>    loss_fn = nn.MSELoss()<NewLine>    loss_fn(outputs, labels).backward()<NewLine>    optimizer.step()<NewLine><NewLine>    # Use a barrier() to make sure that all processes have finished reading the<NewLine>    # checkpoint<NewLine>    **dist.barrier()**<NewLine></code></pre><NewLine><ol><NewLine><li>If the last line is used to ensure all processes finish reading, why does not it directly follow <code>ddp_model.load_state_dict</code>?</li><NewLine><li>For each iteration, do we need to call <code>dist.barrier()</code>?</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/tengerye,(Tengerye),tengerye,"July 3, 2020,  1:03pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>If the last line is used to ensure all processes finish reading, why does not it directly follow  <code>ddp_model.load_state_dict</code> ?</p><NewLine></blockquote><NewLine><p>Good catch! The original reason for adding that barrier is to guard the file deletion below:</p><NewLine><pre><code class=""lang-python"">    if rank == 0:<NewLine>        os.remove(CHECKPOINT_PATH)<NewLine></code></pre><NewLine><p>But looking at it again, this barrier is not necessary. Because the <code>backward()</code> on the DDP model is also a synchronization point as it calls <code>AllReduce</code> internally. Let me remove that.</p><NewLine><blockquote><NewLine><p>For each iteration, do we need to call  <code>dist.barrier()</code> ?</p><NewLine></blockquote><NewLine><p>No. Two common reasons of using a <code>barrier</code> are</p><NewLine><ol><NewLine><li>to avoid <code>AllReduce</code> timeout caused by skewed workloads across DDP processes</li><NewLine><li>code after <code>barrier()</code> on rank A depends on the completion of code before <code>barrier()</code> on node B.</li><NewLine></ol><NewLine><p>If none of the bot is a concern in your use case, then <code>barrier</code> shouldn’t be necessary.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>If I just want to save a model, I don’t need <code>dist.barrier()</code>, right?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""3"" data-topic=""87849"" data-username=""tengerye""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/t/a6a055/40.png"" width=""20""/> tengerye:</div><NewLine><blockquote><NewLine><p>If I just want to save a model, I don’t need <code>dist.barrier()</code> , right?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yep, that should be fine. If only rank 0 saves the model and that might take very long, you can set the <code>timeout</code> argument in <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group"" rel=""nofollow noopener""><code>init_process_group</code></a> to a larger value. The default is 30min.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tengerye; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: July 4, 2020,  4:23am; <NewLine> REPLY_DATE 2: July 4, 2020,  1:56pm; <NewLine> REPLY_DATE 3: July 4, 2020,  4:09pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
87546,torch.nn.parallel.DistributedDataParallel() problem about &ldquo;NoneType Error&rdquo;\ CalledProcessError\backward,2020-07-01T07:34:14.036Z,4,110,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In a GAN-based model, contains one generator model and three discriminator model, all the models are wrapped in torch.nn.parallel.DistributedDataParallel() with different argument process_group, the loss function contains two parts, like this:<br/><NewLine>d_total_loss = d_real_loss + d_fake_loss<br/><NewLine>and the backpropgation is : d_total_loss.backward()<br/><NewLine>when I run the program, the error is:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/66ba0ca7ff774a089fb5d02bdf07d670dd959392"" href=""https://discuss.pytorch.org/uploads/default/original/3X/6/6/66ba0ca7ff774a089fb5d02bdf07d670dd959392.png"" title=""image""><img alt=""image"" data-base62-sha1=""eELjhKwA88pJgETYkhyNm2lz7iy"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/6/6/66ba0ca7ff774a089fb5d02bdf07d670dd959392_2_10x10.png"" height=""115"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/6/6/66ba0ca7ff774a089fb5d02bdf07d670dd959392_2_690x115.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/6/6/66ba0ca7ff774a089fb5d02bdf07d670dd959392_2_690x115.png, https://discuss.pytorch.org/uploads/default/optimized/3X/6/6/66ba0ca7ff774a089fb5d02bdf07d670dd959392_2_1035x172.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/6/6/66ba0ca7ff774a089fb5d02bdf07d670dd959392_2_1380x230.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">2008×335 25.5 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine>But, when I run d_real_loss.backward() or d_fake_loss.backward(), the program could run normally.<br/><NewLine>What’s more, I have another problem that is when I use generator_model.train() in my program and run it, there will be an error:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/c08105476b3ec8fa3c256905533510b11166c865"" href=""https://discuss.pytorch.org/uploads/default/original/3X/c/0/c08105476b3ec8fa3c256905533510b11166c865.png"" title=""image""><img alt=""image"" data-base62-sha1=""rsY6bmTQzZuiB2umsNSfUKv1Ch7"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/0/c08105476b3ec8fa3c256905533510b11166c865_2_10x10.png"" height=""173"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/0/c08105476b3ec8fa3c256905533510b11166c865_2_690x173.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/0/c08105476b3ec8fa3c256905533510b11166c865_2_690x173.png, https://discuss.pytorch.org/uploads/default/optimized/3X/c/0/c08105476b3ec8fa3c256905533510b11166c865_2_1035x259.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/c/0/c08105476b3ec8fa3c256905533510b11166c865_2_1380x346.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">2018×507 29.5 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine>Could you give me some advice to solve these problems?</p><NewLine></div>",https://discuss.pytorch.org/u/lzkzls,(Lzkzls),lzkzls,"July 1, 2020,  7:34am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/lzkzls"">@lzkzls</a> could you please share a minimal reproduce-able example code?</p><NewLine><p>The first error picture does not seem to be a DDP error. Does the code run correctly without DDP? Looks like the autograd graph generating <code>d_real_loss</code> and <code>d_fake_loss</code> share some operators/parameters.</p><NewLine><p>The second error picture seems to suggest the <code>generator_model</code> is a None object? It will be helpful to see a self-contained repro of this error.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>When I use torch.nn.DataParallel(), the code run correctly.<br/><NewLine>Thank you so much!<br/><NewLine>Here is a minimal reproduce-able example:<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/lzkzls/DDPtest/blob/master/DDP_test.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/lzkzls/DDPtest/blob/master/DDP_test.py"" rel=""nofollow noopener"" target=""_blank"">lzkzls/DDPtest/blob/master/DDP_test.py</a></h4><NewLine><pre><code class=""lang-py"">import torch, torchvision<NewLine>import torch.nn as nn<NewLine>import torch.distributed as dist<NewLine>import torchvision.transforms as transforms<NewLine>import torch.optim as optim<NewLine><NewLine><NewLine>#input (1,28,28)<NewLine>class Discriminator(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Discriminator, self).__init__()<NewLine>        self.conv2 = nn.ModuleList()<NewLine>        self.conv2.append(nn.Sequential(nn.Conv2d(1, 16, 3, stride=2, padding=1),<NewLine>                                        nn.BatchNorm2d(16),<NewLine>                                        nn.LeakyReLU(negative_slope=0.2)<NewLine>        ))<NewLine>        <NewLine>        self.conv2.append(nn.Sequential(nn.Conv2d(16, 32, 3, stride=2, padding=1),<NewLine>                                        nn.BatchNorm2d(32),<NewLine>                                        nn.LeakyReLU(negative_slope=0.2)<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/lzkzls/DDPtest/blob/master/DDP_test.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>and I run this code with command: sudo CUDA_VISIBLE_DEVICES=0,1,2,3 python3 -m torch.distributed.launch --nproc_per_node=4 DDP_test.py.<br/><NewLine>note: When I remove the comment of line 77 and 78, there will be an error:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/ee53c8d3d89a5dd3e53786ba51488cdcfe201e82"" href=""https://discuss.pytorch.org/uploads/default/original/3X/e/e/ee53c8d3d89a5dd3e53786ba51488cdcfe201e82.png"" title=""image""><img alt=""image"" data-base62-sha1=""y0l90TaCFpV9ZOyO1u6kiVNHIEa"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/e/ee53c8d3d89a5dd3e53786ba51488cdcfe201e82_2_10x10.png"" height=""81"" src=""https://discuss.pytorch.org/uploads/default/original/3X/e/e/ee53c8d3d89a5dd3e53786ba51488cdcfe201e82.png"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1348×160 8.39 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/lzkzls"">@lzkzls</a></p><NewLine><p>The following code works for me. I found two errors:</p><NewLine><ol><NewLine><li>The original code didn’t set local_rank correctly. It needs be read the local_rank argument instead of hardcoding to 0.</li><NewLine><li>For DDP, you need to call forward and backward interleavingly, instead of two forward followed by one backward. This is fixed by letting the forward function of <code>Discriminator</code> taking both fake and real images.</li><NewLine></ol><NewLine><pre><code class=""lang-python"">import argparse<NewLine>import torch, torchvision<NewLine>import torch.nn as nn<NewLine>import torch.distributed as dist<NewLine>import torchvision.transforms as transforms<NewLine>import torch.optim as optim<NewLine><NewLine><NewLine>#input (1,28,28)<NewLine>class Discriminator(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Discriminator, self).__init__()<NewLine>        self.conv2 = nn.ModuleList()<NewLine>        self.conv2.append(nn.Sequential(nn.Conv2d(1, 16, 3, stride=2, padding=1),<NewLine>                                        nn.BatchNorm2d(16),<NewLine>                                        nn.LeakyReLU(negative_slope=0.2)<NewLine>        ))<NewLine><NewLine>        self.conv2.append(nn.Sequential(nn.Conv2d(16, 32, 3, stride=2, padding=1),<NewLine>                                        nn.BatchNorm2d(32),<NewLine>                                        nn.LeakyReLU(negative_slope=0.2)<NewLine>                        ))<NewLine>        self.conv2.append(nn.Sequential(nn.Conv2d(32, 64, 3, stride=2, padding=1),<NewLine>                                        nn.BatchNorm2d(64),<NewLine>                                        nn.LeakyReLU(negative_slope=0.2)<NewLine>        ))<NewLine>        self.conv2.append(nn.Sequential(nn.Conv2d(64, 1, 3, stride=2),<NewLine>                                        nn.BatchNorm2d(1),<NewLine>                                        nn.LeakyReLU(negative_slope=0.2)<NewLine>        ))<NewLine>    def forward(self, fake, real):<NewLine>        for conv_layer in self.conv2:<NewLine>            fake = conv_layer(fake)<NewLine>            real = conv_layer(real)<NewLine><NewLine>        return fake.view(-1,1), real.view(-1, 1)<NewLine><NewLine>class Generator(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Generator, self).__init__()<NewLine>        self.deconv2 = nn.ModuleList()<NewLine>        self.deconv2.append(nn.Sequential(nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2,padding=1),<NewLine>                            nn.BatchNorm2d(32),<NewLine>                            nn.LeakyReLU()<NewLine>        ))<NewLine>        self.deconv2.append(nn.Sequential(nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2,padding=1),<NewLine>                            nn.BatchNorm2d(16),<NewLine>                            nn.LeakyReLU()<NewLine>        ))<NewLine>        self.deconv2.append(nn.Sequential(nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2,padding=1),<NewLine>                            nn.BatchNorm2d(1),<NewLine>                            nn.LeakyReLU()<NewLine>        ))<NewLine>    def forward(self, x):<NewLine>        for layer in self.deconv2:<NewLine>            x = layer(x)<NewLine><NewLine>        return x<NewLine><NewLine>parser = argparse.ArgumentParser()<NewLine>parser.add_argument(""--local_rank"", type=int, default=0)<NewLine>parser.add_argument(""--local_world_size"", type=int, default=1)<NewLine>args = parser.parse_args()<NewLine><NewLine>local_rank = args.local_rank<NewLine>dist.init_process_group(backend='nccl', init_method='env://')<NewLine><NewLine>disciminator_model = Discriminator()<NewLine>generator_model = Generator()<NewLine><NewLine>torch.cuda.set_device(local_rank)<NewLine>disciminator_model.cuda(local_rank)<NewLine>generator_model.cuda(local_rank)<NewLine><NewLine>pg1 = dist.new_group(range(dist.get_world_size()))<NewLine>pg2 = dist.new_group(range(dist.get_world_size()))<NewLine>disciminator_model = torch.nn.parallel.DistributedDataParallel(disciminator_model, device_ids=[local_rank],<NewLine>                                                                output_device=local_rank, process_group=pg1)<NewLine>generator_model = torch.nn.parallel.DistributedDataParallel(generator_model, device_ids=[local_rank],<NewLine>                                                                output_device=local_rank, process_group=pg2)<NewLine><NewLine># disciminator_model = disciminator_model.train()<NewLine># generator_model = generator_model.train()<NewLine><NewLine>g_optimizer = optim.Adam(params=generator_model.parameters(), lr=1e-4)<NewLine>d_optimizer = optim.Adam(params=disciminator_model.parameters(), lr =1e-4)<NewLine>bcelog_loss = nn.BCEWithLogitsLoss().cuda(local_rank)<NewLine><NewLine>train_dataset = torchvision.datasets.MNIST(root='../../data',<NewLine>                                           train=True,<NewLine>                                           transform=transforms.ToTensor(),<NewLine>                                           download=True)<NewLine><NewLine>train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)<NewLine>batch_size = 8<NewLine>train_loader = torch.utils.data.DataLoader(train_dataset,<NewLine>                                           batch_size=batch_size,<NewLine>                                           shuffle=False,<NewLine>                                           num_workers=4,<NewLine>                                           pin_memory=True,<NewLine>                                           sampler=train_sampler)<NewLine><NewLine>for epoch in range(100):<NewLine>    for i, (images, _) in enumerate(train_loader):<NewLine>        images = images.cuda(local_rank, non_blocking=True)<NewLine>        real_tensor = torch.full((batch_size,1), 1, dtype=torch.float32).cuda(local_rank)<NewLine>        fake_tensor = torch.zeros((batch_size,1), dtype=torch.float32).cuda(local_rank)<NewLine>        noise_tensor = torch.rand((batch_size, 64, 4, 4))<NewLine>        gen_image = generator_model(noise_tensor)<NewLine><NewLine>        d_fake, d_real = disciminator_model(gen_image, images)<NewLine>        #d_real = disciminator_model(images)<NewLine><NewLine>        d_fake_loss = bcelog_loss(d_fake, fake_tensor)<NewLine>        d_real_loss = bcelog_loss(d_real, real_tensor)<NewLine><NewLine>        d_total_loss = d_fake_loss + d_real_loss<NewLine><NewLine>        g_optimizer.zero_grad()<NewLine>        d_optimizer.zero_grad()<NewLine><NewLine>        d_total_loss.backward()<NewLine>        g_optimizer.step()<NewLine>        d_optimizer.step()<NewLine>        if i % 10 == 0:<NewLine>            print(f""processed {i} images"")<NewLine>    print(""current epoch: "", epoch)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you! You are so great! What should I do if I have two different Discriminator class?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>What should I do if I have two different Discriminator class?</p><NewLine></blockquote><NewLine><p>As long as forward and backward on one DDP instance is called alternatively, it should work. So, there are at least two options:</p><NewLine><ol><NewLine><li>Wrap the two Discriminators into one <code>nn.Module</code>, say <code>CombinedDiscriminator</code>, and then pass the <code>CombinedDiscriminator</code> to DDP ctor.</li><NewLine><li>Create a dedicated DDP instance (with dedicated ProcessGroup instance) for each Discriminator.</li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/lzkzls; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/lzkzls; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: July 1, 2020,  5:33pm; <NewLine> REPLY_DATE 2: July 2, 2020,  8:14am; <NewLine> REPLY_DATE 3: July 2, 2020,  8:29pm; <NewLine> REPLY_DATE 4: July 3, 2020,  7:16am; <NewLine> REPLY_DATE 5: July 4, 2020,  1:08am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
87771,NCC version and Pytorch NCCL version mismatch,2020-07-02T19:10:20.442Z,0,70,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have NCCL 2.5 installed on the system, but torch.cuda.nccl.version() shows 2.4.8.<br/><NewLine>On a single machine with 2 gpus, it works fine.</p><NewLine></div>",https://discuss.pytorch.org/u/nash,(Nash),nash,"July 2, 2020,  7:10pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/nash"">@nash</a>, NCCL is packaged in PyTorch as a submodule. The current version if 2.4.8. If you would like to use your own version, you can set <code>USE_SYSTEM_NCCL=1</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: July 2, 2020,  8:42pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
87538,Model trained by DistributedDataParallel didn&rsquo;t perform as well as single GPU,2020-07-01T06:38:11.166Z,1,124,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am working on DistributedDataParallel, trying to speed up training process. However, after two epochs, the distributed did not perform as well as the normal.</p><NewLine><h1>the log of distributed verision:</h1><NewLine><pre><code class=""lang-bash"">Epoch [1/2], Step [100/150], Loss: 2.1133<NewLine>Epoch [2/2], Step [100/150], Loss: 1.9204<NewLine>Training complete in: 0:00:27.426653<NewLine>Dev loss: 1.8674346208572388<NewLine></code></pre><NewLine><h1>the log of normal version</h1><NewLine><pre><code class=""lang-bash"">Epoch [1/2], Step [100/600], Loss: 2.1626<NewLine>Epoch [1/2], Step [200/600], Loss: 1.9929<NewLine>Epoch [1/2], Step [300/600], Loss: 1.9224<NewLine>Epoch [1/2], Step [400/600], Loss: 1.7479<NewLine>Epoch [1/2], Step [500/600], Loss: 1.6264<NewLine>Epoch [1/2], Step [600/600], Loss: 1.5411<NewLine>Epoch [2/2], Step [100/600], Loss: 1.4387<NewLine>Epoch [2/2], Step [200/600], Loss: 1.3243<NewLine>Epoch [2/2], Step [300/600], Loss: 1.2894<NewLine>Epoch [2/2], Step [400/600], Loss: 1.1754<NewLine>Epoch [2/2], Step [500/600], Loss: 1.1271<NewLine>Epoch [2/2], Step [600/600], Loss: 1.1246<NewLine>Training complete in: 0:00:53.779830<NewLine>Dev loss: 1.1193695068359375<NewLine></code></pre><NewLine><h1>the source code</h1><NewLine><h2>distributed version</h2><NewLine><pre><code class=""lang-python"">import os<NewLine>from datetime import datetime<NewLine>import argparse<NewLine>import torch.multiprocessing as mp<NewLine>import torchvision<NewLine>import torchvision.transforms as transforms<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.distributed as dist<NewLine><NewLine>class ConvNet(nn.Module):<NewLine>    def __init__(self, num_classes=10):<NewLine>        super(ConvNet, self).__init__()<NewLine>        self.layer1 = nn.Sequential(<NewLine>            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),<NewLine>            nn.BatchNorm2d(16),<NewLine>            nn.ReLU(),<NewLine>            nn.MaxPool2d(kernel_size=2, stride=2))<NewLine>        self.layer2 = nn.Sequential(<NewLine>            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),<NewLine>            nn.BatchNorm2d(32),<NewLine>            nn.ReLU(),<NewLine>            nn.MaxPool2d(kernel_size=2, stride=2))<NewLine>        self.fc = nn.Linear(7*7*32, num_classes)<NewLine><NewLine>    def forward(self, x):<NewLine>        out = self.layer1(x)<NewLine>        out = self.layer2(out)<NewLine>        out = out.reshape(out.size(0), -1)<NewLine>        out = self.fc(out)<NewLine>        return out<NewLine><NewLine>def train(gpu, args):<NewLine>    rank = args.nr * args.gpus + gpu<NewLine>    dist.init_process_group(backend='nccl', init_method='env://', world_size=args.world_size, rank=rank)<NewLine>    torch.manual_seed(0)<NewLine>    model = ConvNet()<NewLine>    torch.cuda.set_device(gpu)<NewLine>    model.cuda(gpu)<NewLine>    batch_size = 100<NewLine>    # define loss function (criterion) and optimizer<NewLine>    criterion = nn.CrossEntropyLoss().cuda(gpu)<NewLine>    optimizer = torch.optim.SGD(model.parameters(), 1e-4)<NewLine>    # Wrap the model<NewLine>    model = nn.parallel.DistributedDataParallel(model, device_ids=[gpu])<NewLine>    # Data loading code<NewLine>    train_dataset = torchvision.datasets.MNIST(root='./data',<NewLine>                                               train=True,<NewLine>                                               transform=transforms.ToTensor(),<NewLine>                                               download=True)<NewLine>    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,<NewLine>                                                                    num_replicas=args.world_size,<NewLine>                                                                    rank=rank)<NewLine>    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,<NewLine>                                               batch_size=batch_size,<NewLine>                                               shuffle=False,<NewLine>                                               num_workers=0,<NewLine>                                               pin_memory=True,<NewLine>                                               sampler=train_sampler)<NewLine><NewLine>    start = datetime.now()<NewLine>    total_step = len(train_loader)<NewLine>    for epoch in range(args.epochs):<NewLine>        train_sampler.set_epoch(epoch)<NewLine>        for i, (images, labels) in enumerate(train_loader):<NewLine>            images = images.cuda(non_blocking=True)<NewLine>            labels = labels.cuda(non_blocking=True)<NewLine>            # Forward pass<NewLine>            outputs = model(images)<NewLine>            loss = criterion(outputs, labels)<NewLine><NewLine>            # Backward and optimize<NewLine>            optimizer.zero_grad()<NewLine>            loss.backward()<NewLine>            optimizer.step()<NewLine>            if (i + 1) % 100 == 0 and gpu == 0:<NewLine>                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, args.epochs, i + 1, total_step,<NewLine>                                                                         loss.item()))<NewLine>    if gpu == 0:<NewLine>        print(""Training complete in: "" + str(datetime.now() - start))<NewLine>        dev_dataset = torchvision.datasets.MNIST(root='./data',<NewLine>                                                 train=False,<NewLine>                                                 transform=transforms.ToTensor(),<NewLine>                                                 download=False)<NewLine>        dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset,<NewLine>                                                 batch_size=batch_size,<NewLine>                                                 shuffle=True,<NewLine>                                                 num_workers=0,<NewLine>                                                 pin_memory=True)<NewLine>        _ = model.eval()<NewLine>        with torch.no_grad():<NewLine>            y_hat = []<NewLine>            y = []<NewLine>            for i, (images, labels) in enumerate(dev_loader):<NewLine>                y.append(labels.cuda(non_blocking=True))<NewLine>                y_hat.append(model(images.cuda(non_blocking=True)))<NewLine>            y_hat = torch.cat(y_hat)<NewLine>            y = torch.cat(y)<NewLine>            loss = criterion(y_hat, y)<NewLine>            print(f'Dev loss: {loss.item()}')<NewLine><NewLine>def main():<NewLine>    parser = argparse.ArgumentParser()<NewLine>    parser.add_argument('-n', '--nodes', default=1, type=int, metavar='N')<NewLine>    parser.add_argument('-g', '--gpus', default=1, type=int,<NewLine>                        help='number of gpus per node')<NewLine>    parser.add_argument('-nr', '--nr', default=0, type=int,<NewLine>                        help='ranking within the nodes')<NewLine>    parser.add_argument('--epochs', default=2, type=int, metavar='N',<NewLine>                        help='number of total epochs to run')<NewLine>    args = parser.parse_args()<NewLine>    ###################################################<NewLine>    args.world_size = args.gpus * args.nodes                                 #<NewLine>    os.environ['MASTER_ADDR'] = HOST                                      #<NewLine>    os.environ['MASTER_PORT'] = PORT                                      #<NewLine>    mp.spawn(train, nprocs=args.gpus, args=(args, ))                    #<NewLine>    ###################################################<NewLine><NewLine>if __name__ == '__main__':<NewLine>    """"""<NewLine>    Epoch [1/2], Step [100/150], Loss: 2.1133<NewLine>    Epoch [2/2], Step [100/150], Loss: 1.9204<NewLine>    Training complete in: 0:00:27.426653<NewLine>    Dev loss: 1.8674346208572388<NewLine>    """"""<NewLine>    main()<NewLine></code></pre><NewLine><h2>the single gpu version</h2><NewLine><pre><code class=""lang-python"">import os<NewLine>from datetime import datetime<NewLine>import argparse<NewLine>import torch.multiprocessing as mp<NewLine>import torchvision<NewLine>import torchvision.transforms as transforms<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.distributed as dist<NewLine><NewLine>class ConvNet(nn.Module):<NewLine>    def __init__(self, num_classes=10):<NewLine>        super(ConvNet, self).__init__()<NewLine>        self.layer1 = nn.Sequential(<NewLine>            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),<NewLine>            nn.BatchNorm2d(16),<NewLine>            nn.ReLU(),<NewLine>            nn.MaxPool2d(kernel_size=2, stride=2))<NewLine>        self.layer2 = nn.Sequential(<NewLine>            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),<NewLine>            nn.BatchNorm2d(32),<NewLine>            nn.ReLU(),<NewLine>            nn.MaxPool2d(kernel_size=2, stride=2))<NewLine>        self.fc = nn.Linear(7*7*32, num_classes)<NewLine><NewLine>    def forward(self, x):<NewLine>        out = self.layer1(x)<NewLine>        out = self.layer2(out)<NewLine>        out = out.reshape(out.size(0), -1)<NewLine>        out = self.fc(out)<NewLine>        return out<NewLine><NewLine>def train(gpu, args):<NewLine>    torch.manual_seed(0)<NewLine>    model = ConvNet()<NewLine>    torch.cuda.set_device(gpu)<NewLine>    model.cuda(gpu)<NewLine>    batch_size = 100<NewLine>    # define loss function (criterion) and optimizer<NewLine>    criterion = nn.CrossEntropyLoss().cuda(gpu)<NewLine>    optimizer = torch.optim.SGD(model.parameters(), 1e-4)<NewLine>    # Data loading code<NewLine>    train_dataset = torchvision.datasets.MNIST(root='./data',<NewLine>                                               train=True,<NewLine>                                               transform=transforms.ToTensor(),<NewLine>                                               download=True)<NewLine>    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,<NewLine>                                               batch_size=batch_size,<NewLine>                                               shuffle=True,<NewLine>                                               num_workers=0,<NewLine>                                               pin_memory=True)<NewLine><NewLine>    start = datetime.now()<NewLine>    total_step = len(train_loader)<NewLine>    for epoch in range(args.epochs):<NewLine>        for i, (images, labels) in enumerate(train_loader):<NewLine>            images = images.cuda(non_blocking=True)<NewLine>            labels = labels.cuda(non_blocking=True)<NewLine>            # Forward pass<NewLine>            outputs = model(images)<NewLine>            loss = criterion(outputs, labels)<NewLine><NewLine>            # Backward and optimize<NewLine>            optimizer.zero_grad()<NewLine>            loss.backward()<NewLine>            optimizer.step()<NewLine>            if (i + 1) % 100 == 0 and gpu == 0:<NewLine>                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(<NewLine>                    epoch + 1,<NewLine>                    args.epochs,<NewLine>                    i + 1,<NewLine>                    total_step,<NewLine>                    loss.item())<NewLine>                   )<NewLine>    if gpu == 0:<NewLine>        print(""Training complete in: "" + str(datetime.now() - start))<NewLine>        dev_dataset = torchvision.datasets.MNIST(root='./data',<NewLine>                                                 train=False,<NewLine>                                                 transform=transforms.ToTensor(),<NewLine>                                                 download=False)<NewLine>        dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset,<NewLine>                                                 batch_size=batch_size,<NewLine>                                                 shuffle=True,<NewLine>                                                 num_workers=0,<NewLine>                                                 pin_memory=True)<NewLine>        _ = model.eval()<NewLine>        with torch.no_grad():<NewLine>            y_hat = []<NewLine>            y = []<NewLine>            for i, (images, labels) in enumerate(dev_loader):<NewLine>                y.append(labels.cuda(non_blocking=True))<NewLine>                y_hat.append(model(images.cuda(non_blocking=True)))<NewLine>            y_hat = torch.cat(y_hat)<NewLine>            y = torch.cat(y)<NewLine>            loss = criterion(y_hat, y)<NewLine>            print(f'Dev loss: {loss.item()}')<NewLine><NewLine>def main():<NewLine>    parser = argparse.ArgumentParser()<NewLine>    parser.add_argument('-n', '--nodes', default=1, type=int, metavar='N')<NewLine>    parser.add_argument('-g', '--gpus', default=1, type=int,<NewLine>                        help='number of gpus per node')<NewLine>    parser.add_argument('-nr', '--nr', default=0, type=int,<NewLine>                        help='ranking within the nodes')<NewLine>    parser.add_argument('--epochs', default=2, type=int, metavar='N',<NewLine>                        help='number of total epochs to run')<NewLine>    args = parser.parse_args()<NewLine>    train(0, args)<NewLine><NewLine>if __name__ == '__main__':<NewLine>    """"""<NewLine>    Epoch [1/2], Step [100/600], Loss: 2.1626<NewLine>    Epoch [1/2], Step [200/600], Loss: 1.9929<NewLine>    Epoch [1/2], Step [300/600], Loss: 1.9224<NewLine>    Epoch [1/2], Step [400/600], Loss: 1.7479<NewLine>    Epoch [1/2], Step [500/600], Loss: 1.6264<NewLine>    Epoch [1/2], Step [600/600], Loss: 1.5411<NewLine>    Epoch [2/2], Step [100/600], Loss: 1.4387<NewLine>    Epoch [2/2], Step [200/600], Loss: 1.3243<NewLine>    Epoch [2/2], Step [300/600], Loss: 1.2894<NewLine>    Epoch [2/2], Step [400/600], Loss: 1.1754<NewLine>    Epoch [2/2], Step [500/600], Loss: 1.1271<NewLine>    Epoch [2/2], Step [600/600], Loss: 1.1246<NewLine>    Training complete in: 0:00:53.779830<NewLine>    Dev loss: 1.1193695068359375<NewLine>    """"""<NewLine>    main()<NewLine></code></pre><NewLine><p>any help is appreciated</p><NewLine></div>",https://discuss.pytorch.org/u/Jerry_h,,Jerry_h,"July 1, 2020,  6:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>ops, I just found it’s an issue about learning rate.</p><NewLine><p>When I set learning rate into 4e-4, four times of the rate using on single GPU.</p><NewLine><pre><code class=""lang-bash"">Epoch [1/2], Step [100/150], Loss: 1.7276<NewLine>Epoch [2/2], Step [100/150], Loss: 1.2062<NewLine>Training complete in: 0:00:18.275619<NewLine>Dev loss: 1.1129298210144043<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Right, learning rate, batch size, loss function can all play a role here. For example, if using the same per-process batch size, then the DDP gang collectively will consume more samples with larger world size, and hence the learning rate will need to adjust accordingly.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Jerry_h; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: July 1, 2020,  9:12am; <NewLine> REPLY_DATE 2: July 1, 2020,  5:35pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
87134,How can I keep the batch_size per gpu in DDP?,2020-06-27T18:58:20.044Z,2,123,"<div class=""post"" itemprop=""articleBody""><NewLine><p>If my model original batch-size is 32, when I use two gpus ,one node per gpu, I use batch-size 16(32/ngpu), but if the number of gpus is 3 or any odd number, we should keep the size like 32/3=10 or limit the number of gpus to 2?<br/><NewLine>Any help is welcome.</p><NewLine></div>",https://discuss.pytorch.org/u/111344,(祥棉 陈),111344,"June 27, 2020,  6:58pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/111344"">@111344</a>, if you are looking for mathematical equivalence, you will need at least two things:</p><NewLine><ol><NewLine><li>each DDP process processes <code>batch_size / num_gpu</code> samples: this allows DDP collectively process the same amount of inputs as local training.</li><NewLine><li><NewLine><code>loss_fn(model([sample1, sample2])) == (loss_fn(model([sample1])) + loss_fn(model[sample2])) /2</code>: this is because DDP uses AllReduce to compute the average gradients across all processes. If the above condition is not met, then average gradients across all DDP processes are not equivalent the the local training gradients.</li><NewLine></ol><NewLine><p>However, practically, applications usually do <strong>not</strong> have to satisfy the above conditions. Did you see any training accuracy degradation when scale up to 3 GPUs?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I haven’t tested GPU = 3, but I will take the time to verify it and then reply to you</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think there is no difference between gpu=2 or 3.<br/><NewLine>In my experiment:<br/><NewLine>batch-size=8  gpu=2  --&gt;batch_size=4 for single gpu<br/><NewLine>batch-size=8  gpu=3  --&gt;batch_size=2 for single gpu(so total batch_size is 6)<br/><NewLine>batch-size=8 or 6, under normal circumstances, it does not have much impact on performance<br/><NewLine>For some task which are very sensitive to batch_size may need to take it into account</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/111344; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/111344; <NewLine> ,"REPLY_DATE 1: June 28, 2020,  9:31pm; <NewLine> REPLY_DATE 2: June 29, 2020,  1:34pm; <NewLine> REPLY_DATE 3: June 30, 2020,  5:04pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
85501,Is there a way to selectively load model params into GPU during forward pass?,2020-06-15T10:08:04.558Z,5,135,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I came across some way to change the GPU for component modules of a larger model in the following link:</p><NewLine><p><a href=""https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html</a></p><NewLine><p>However, my model runs in a distributed environment (ddp) and have several sub-modules holding all of which for a forward-backward cycle each of which will decrease the size of batch I can fit for an update. But for each forward/backward, not all of these are active and therefore are not required to stay in GPU (as in I can load these for a single-batch) and otherwise leave it in the non-gpu memory.</p><NewLine><p>In other words, I have parameters <code>Theta + theta[t]</code> (for t=1…T), where t is a particular task. I want to only load a single <code>theta[t]</code> for a forward and backward pass into the GPU and fit larger batches. Currently I’m holding all <code>theta[t]</code> in the GPU.</p><NewLine><p>Is it possible to use the same semantics if it’s the same (sub)-module (<code>theta[t]</code>) to achieve the intention described above?</p><NewLine></div>",https://discuss.pytorch.org/u/jerinphilip,(Jerin Philip),jerinphilip,"June 15, 2020, 10:08am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/jerinphilip"">@jerinphilip</a>, I believe this is possible. You can use <code>Tensor.to(device)</code> to move the parameters to the GPUs in the forward pass, and the <code>to</code> (i.e., copy) operator should be added into the autograd graph, so that the backward pass will compute gradients for the original on-CPU parameters properly. Let me know if it didn’t work.</p><NewLine><p>Note that, although this can reduce the footprint on GPU memory, DDP would still need to communicate the same amount of parameters, as that is determined at DDP construction time. And as those parameters are on CPU, you won’t be able to use NCCL which might cause considerable slow down.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Where can I read more on the DDP communications setup? Thanks in advance.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/jerinphilip"">@jerinphilip</a>, this page briefly describes DDP: <a href=""https://pytorch.org/docs/master/notes/ddp.html"" rel=""nofollow noopener"">https://pytorch.org/docs/master/notes/ddp.html</a></p><NewLine><p>We have a paper with more comprehensive details. Let me upload that to archive.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""85501"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>Note that, although this can reduce the footprint on GPU memory, DDP would still need to communicate the same amount of parameters, as that is determined at DDP construction time. And as those parameters are on CPU, you won’t be able to use NCCL which might cause considerable slow down.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Where do I obtain details corresponding to this particular information? Isn’t only <code>.grad</code> meant to be communicated and the workers applying the updates individually? If my parameters of  <code>theta[t]</code> has only gradients for the particular task, would this help the case? I’m reading the <strong>Forward Pass</strong> section of <a href=""https://pytorch.org/docs/master/notes/ddp.html#internal-design"" rel=""nofollow noopener"">Internal Design</a>, with <code>find_unused_parameters</code>, it is possible to operate on a subgraph, correct(?). I already have this enabled.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Where do I obtain details corresponding to this particular information?</p><NewLine></blockquote><NewLine><p>We need to go through some internal approval process to publicly disclose that paper. It will take some time. For now  <a href=""https://pytorch.org/docs/master/notes/ddp.html"" rel=""nofollow noopener"">https://pytorch.org/docs/master/notes/ddp.html</a> is the best place for overall intro. The implementation of DDP is linked below:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/ebd869153c6adb37507d2ecb6a9fe3fd495fbb6e/torch/nn/parallel/distributed.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/ebd869153c6adb37507d2ecb6a9fe3fd495fbb6e/torch/nn/parallel/distributed.py"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/ebd869153c6adb37507d2ecb6a9fe3fd495fbb6e/torch/nn/parallel/distributed.py</a></h4><NewLine><pre><code class=""lang-py"">from contextlib import contextmanager<NewLine>import copy<NewLine>import itertools<NewLine><NewLine>import torch<NewLine><NewLine>import torch.cuda.comm<NewLine>import torch.distributed as dist<NewLine><NewLine>if dist.is_available():<NewLine>    from torch.distributed.distributed_c10d import _get_default_group<NewLine><NewLine>from ..modules import Module<NewLine>from .replicate import replicate<NewLine>from .scatter_gather import scatter_kwargs, gather<NewLine>from .parallel_apply import parallel_apply<NewLine>from torch.cuda._utils import _get_device_index<NewLine><NewLine><NewLine>def _find_tensors(obj):<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/ebd869153c6adb37507d2ecb6a9fe3fd495fbb6e/torch/nn/parallel/distributed.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/ebd869153c6adb37507d2ecb6a9fe3fd495fbb6e/torch/csrc/distributed/c10d/reducer.cpp"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/ebd869153c6adb37507d2ecb6a9fe3fd495fbb6e/torch/csrc/distributed/c10d/reducer.cpp"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/ebd869153c6adb37507d2ecb6a9fe3fd495fbb6e/torch/csrc/distributed/c10d/reducer.cpp</a></h4><NewLine><pre><code class=""lang-cpp"">#include &lt;torch/csrc/distributed/c10d/reducer.h&gt;<NewLine><NewLine>#include &lt;functional&gt;<NewLine><NewLine>#include &lt;c10/core/DeviceGuard.h&gt;<NewLine>#include &lt;c10/util/Exception.h&gt;<NewLine>#include &lt;torch/csrc/autograd/engine.h&gt;<NewLine>#include &lt;torch/csrc/autograd/function_hook.h&gt;<NewLine>#include &lt;torch/csrc/autograd/functions/accumulate_grad.h&gt;<NewLine>#include &lt;torch/csrc/autograd/profiler.h&gt;<NewLine>#include &lt;torch/csrc/autograd/utils/lambda_post_hook.h&gt;<NewLine>#include &lt;torch/csrc/distributed/c10d/comm.h&gt;<NewLine>#include &lt;torch/csrc/utils/hash.h&gt;<NewLine>#include &lt;torch/csrc/utils/memory.h&gt;<NewLine><NewLine>namespace c10d {<NewLine>namespace {<NewLine><NewLine>inline int64_t current_time_in_nanos() {<NewLine>  return torch::autograd::profiler::getTime();<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/ebd869153c6adb37507d2ecb6a9fe3fd495fbb6e/torch/csrc/distributed/c10d/reducer.cpp"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><blockquote><NewLine><p>Isn’t only  <code>.grad</code>  meant to be communicated and the workers applying the updates individually?</p><NewLine></blockquote><NewLine><p>No. Currently at construction time, DDP creates a mapping from parameters to buckets, and always communicate all buckets even if some gradients are not used in one iteration. The reason for doing so is that it is possible process 1 only computes grad A and process 2 only computes grad B. However, AllReduce operation requires all processes to provide the same set of input tensors. So in this case, both process 1 and 2 need to communicate grad A and B. DDP can use another communication to first figure out which grads are used globally. However, if block waiting for this signal, there will be no overlap between communication and computation, which could result in &gt;30% slowdown in some cases.</p><NewLine><blockquote><NewLine><p>If my parameters of  <code>theta[t]</code>  has only gradients for the particular task, would this help the case?</p><NewLine></blockquote><NewLine><p>It helps to skip computation but not communication. DDP always communicates all parameters in the model you passed to DDP constructor.</p><NewLine><blockquote><NewLine><p>I’m reading the  <strong>Forward Pass</strong>  section of <a href=""https://pytorch.org/docs/master/notes/ddp.html#internal-design"" rel=""nofollow noopener"">Internal Design </a>, with  <code>find_unused_parameters</code> , it is possible to operate on a subgraph, correct(?)</p><NewLine></blockquote><NewLine><p>That flag only allows DDP to skip waiting for grads of those parameters. The communication phase is the same regardless the value of <code>find_unused_parameters</code>.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""onebox pdf""><NewLine><header class=""source""><NewLine><a href=""https://arxiv.org/pdf/2006.15704.pdf"" rel=""nofollow noopener"" target=""_blank"">arxiv.org</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://arxiv.org/pdf/2006.15704.pdf"" rel=""nofollow noopener"" target=""_blank""><span class=""pdf-onebox-logo""></span></a><NewLine><h3><a href=""https://arxiv.org/pdf/2006.15704.pdf"" rel=""nofollow noopener"" target=""_blank"">2006.15704.pdf</a></h3><NewLine><p class=""filesize"">1161.94 KB</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>This the relevant paper?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yep, it is the paper. Sorry, I forgot to update it here today.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerinphilip; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jerinphilip; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/jerinphilip; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 15, 2020,  3:27pm; <NewLine> REPLY_DATE 2: June 15, 2020,  6:20pm; <NewLine> REPLY_DATE 3: June 15, 2020,  6:32pm; <NewLine> REPLY_DATE 4: June 16, 2020, 12:30pm; <NewLine> REPLY_DATE 5: June 16, 2020,  2:20pm; <NewLine> REPLY_DATE 6: June 30, 2020,  2:59pm; <NewLine> REPLY_DATE 7: June 30, 2020,  3:12pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
87429,Multiprocessing/ Distributed question regarding loss reporting,2020-06-30T10:10:08.505Z,1,67,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello there,</p><NewLine><p>so I’ve been following the imagenet example <a href=""https://github.com/pytorch/examples/blob/e9e76722dad4f4569651a8d67ca1d10607db58f9/imagenet/main.py"" rel=""nofollow noopener"">https://github.com/pytorch/examples/blob/e9e76722dad4f4569651a8d67ca1d10607db58f9/imagenet/main.py</a>) on how to use multiprocessing and I have a question on loss/ statistics reporting.</p><NewLine><p>Currently, to my understanding, the way the example is being structured, every GPU on every Node gets one instance of the entire model in memory by spawning a separate process and executing the <code>main_worker()</code> method.</p><NewLine><p>The imagenet example uses some custom classes like <code>AverageMeter</code> and <code>ProgressMeter</code> to report progress during the training/ validation. However from what I can tell each process will have its own meters/ progress to report.</p><NewLine><p>Thus if I were to execute the example I would get a progress report for each of the processes running.</p><NewLine><p>Instead of getting multiple progress reports and using console logging, I would like to use <code>SummaryWriter</code> and Tensorboard to monitor the progress of my training.</p><NewLine><p>Going through documentation (<a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#save-and-load-checkpoints"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#save-and-load-checkpoints</a>) I came across a section on loading checkpoints that states:</p><NewLine><p><code>When using DDP, one optimization is to save the model in only one process and then load it to all processes, reducing write overhead. This is correct because all processes start from the same parameters and gradients are synchronized in backward passes, and hence optimizers should keep setting parameters to the same values. </code></p><NewLine><p>That got me thinking, that perhaps I could create a <code>SummaryWriter</code> only on the process with <code>rank == 0</code> and use that writer to only report statistics to tensorboard. I’ve implemented it as such and it seems to be working.</p><NewLine><p>However I’d like to ask whether my approach is correct or not. Is there a better/ different way to do this?</p><NewLine></div>",https://discuss.pytorch.org/u/mobius,(Paris),mobius,"June 30, 2020, 10:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/mobius"">@mobius</a>, I believe this is correct. With DDP, every <code>backward()</code> pass is a global synchronization point. So all processes will run the same number of iterations and hence should have the same number of progress steps. Therefore, reporting the progress in one rank should be sufficient. (more details <a href=""https://pytorch.org/docs/master/notes/ddp.html"" rel=""nofollow noopener"">link1</a>, <a href=""https://arxiv.org/abs/2006.15704"" rel=""nofollow noopener"">link2</a>)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for the quick response <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>! Great job on the distributed/ multiprocessing part of PyTorch, its really intuitive. Also your tutorials have been super helpful! I was not aware of the paper you’ve mentioned, I will definitely check it out!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mobius; <NewLine> ,"REPLY_DATE 1: June 30, 2020,  2:45pm; <NewLine> REPLY_DATE 2: June 30, 2020,  2:52pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
87348,Correct way to declare multiple optimizers for single model in DDP,2020-06-29T20:40:00.148Z,0,74,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Given the pseudo model below:</p><NewLine><pre><code class=""lang-auto"">class model(nn.Module):<NewLine>     def __init__(self):<NewLine>            self.alpha_params1 = nn.Parameters(&lt;size&gt;, requires_grad=True)<NewLine>            self.alpha_params2 = nn.Parameters(&lt;size&gt;, requires_grad=True)<NewLine><NewLine>           &lt; typical Conv2d layers&gt;....<NewLine>     <NewLine>    def forward(self, x):<NewLine>           &lt;feed forward&gt;<NewLine>            return output<NewLine><NewLine>   def net_parameters(self, filtered_name='alpha_', recurse=True):<NewLine>          # return torch layer params      <NewLine>          for name, param in self.named_parameters(recurse=recurse):   <NewLine>                if filtered_name not in name:<NewLine>                      yield param  <NewLine>   def extra_params(self):<NewLine>          # return user defined params<NewLine>          return [self.alpha_params1, self.alpha_params2]         <NewLine></code></pre><NewLine><p>So above is my pseudo model code.</p><NewLine><pre><code class=""lang-auto"">net = model() # instantiate model above<NewLine>optimizer_1 = Adam(model.net_parameters, lr=0.001,...)<NewLine>optimizer_2 = Adam(model.extra_params, lr=0.003,...)<NewLine>criterion = L1()<NewLine>##<NewLine># typical Apex Distributed Data Parrallel initialization <NewLine>##<NewLine>for epoch in epochs:<NewLine>  for data1, data2 in dataloader:<NewLine>       output = net(data1.data)<NewLine>       loss = criterion(output, data1.gt)<NewLine>       loss.backward()<NewLine>       optimizer_1.zero_grad()<NewLine>       optimizer_1.step()<NewLine><NewLine>       output2 = net(data2.data)<NewLine>       loss2 = criterion(output2, data2.gt)<NewLine>       loss2.backward()<NewLine>       optimizer_2.zero_grad()<NewLine>       optimizer_2.step()<NewLine>  # save checkpoint<NewLine> torch.save(net.module.state_dict(), f""state_dict_{epoch}.pth"")<NewLine> torch.save(net.module.extra_params(), f""extra_params_{epoch}.pth"")<NewLine>  <NewLine><NewLine></code></pre><NewLine><p>Above is my pseudo code for model instantiation and training.</p><NewLine><p>At every 10 epoch intervals, I checkpoint by saving model.state_dict() as well as model’s alpha parameters separately. I then compare the value of my alpha parameters between different epochs. What I found is that both parameters from separate epochs are identical in values as well the model’s weights. It seems no update is happening.  Any help is appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/Scott_Hoang,(Scott Hoang),Scott_Hoang,"June 29, 2020,  8:51pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""87348"" data-username=""Scott_Hoang""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/scott_hoang/40/9907_2.png"" width=""20""/> Scott_Hoang:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">       loss2.backward()<NewLine>       optimizer_2.zero_grad()<NewLine>       optimizer_2.step()<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>I believe you need to call <code>zero_grad()</code> either before <code>backward()</code> or after <code>step()</code>. Otherwise, the grad is always 0 when optimizer <code>step</code> tries to use it.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>yep silly mistakes that cost me 4 days of gpu time <img alt="":stuck_out_tongue:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/stuck_out_tongue.png?v=9"" title="":stuck_out_tongue:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Scott_Hoang; <NewLine> ,"REPLY_DATE 1: June 29, 2020, 11:23pm; <NewLine> REPLY_DATE 2: June 29, 2020, 11:23pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
87253,Using all_gather() in the forward pass in DDP throws RuntimeError,2020-06-29T03:11:10.365Z,0,100,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I would like to gather some intermediate output feature across different GPUs, somewhat like SyncBN, but it prompts out an error as below. To reproduce this problem, I have built a <a href=""https://github.com/Ze-Yang/Bug_Shooting"" rel=""nofollow noopener"">toy model</a> in Github, just a few lines of codes. Any help is appreciated. Thanks a lot.</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by (1) passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`; (2) making sure all `forward` function outputs participate in calculating loss. If you already have done the above two steps, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable). (prepare_for_backward at /opt/conda/conda-bld/pytorch_1579027003190/work/torch/csrc/distributed/c10d/reducer.cpp:514)<NewLine></code></pre><NewLine><p>Toy model to reproduce the error:</p><NewLine><pre><code class=""lang-python"">import os<NewLine>import argparse<NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>from torch.nn.parallel import DistributedDataParallel<NewLine>import torch.optim as optim<NewLine>import torch.multiprocessing as mp<NewLine>import comm<NewLine><NewLine><NewLine>parser = argparse.ArgumentParser(description='Distributed Data Parallel')<NewLine>parser.add_argument('--world-size', type=int, default=2,<NewLine>                    help='Number of GPU(s).')<NewLine><NewLine><NewLine>class ToyModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(ToyModel, self).__init__()<NewLine>        self.stem = nn.Linear(10, 10)<NewLine>        self.branch1 = nn.Sequential(<NewLine>            nn.Linear(10, 10),<NewLine>            nn.ReLU(),<NewLine>            nn.Linear(10, 10))<NewLine>        self.branch2 = nn.Sequential(<NewLine>            nn.Linear(10, 10),<NewLine>            nn.ReLU(),<NewLine>            nn.Linear(10, 10))<NewLine><NewLine>    def forward(self, x):<NewLine>        x1 = F.relu(self.stem(x))  # [20, 10]<NewLine>        branch1 = self.branch1(x1[:10])<NewLine>        branch2 = self.branch2(x1[10:])<NewLine>        branch1_list = [torch.empty_like(branch1, device='cuda') for _ in range(dist.get_world_size())]<NewLine>        dist.all_gather(branch1_list, branch1)<NewLine>        # branch1_list = comm.all_gather(branch1)<NewLine>        pred_weight = torch.cat(branch1_list, dim=0).mean(0, keepdim=True).expand(5, -1)  # [5, 10]<NewLine>        out = branch2.mm(pred_weight.t())<NewLine>        return out<NewLine><NewLine><NewLine>def demo_basic(rank, world_size):<NewLine>    print(f""Running basic DDP example on rank {rank}."")<NewLine>    setup(rank, world_size)<NewLine><NewLine>    # create model and move it to GPU with id rank<NewLine>    model = ToyModel().to('cuda')<NewLine>    ddp_model = DistributedDataParallel(model, device_ids=[dist.get_rank()], broadcast_buffers=False)<NewLine>    ddp_model.train()<NewLine><NewLine>    loss_fn = nn.MSELoss()<NewLine>    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)<NewLine><NewLine>    for _ in range(5):<NewLine>        optimizer.zero_grad()<NewLine>        inputs = torch.randn((20, 10), device='cuda')<NewLine>        outputs = ddp_model(inputs)<NewLine>        labels = torch.randn_like(outputs).to('cuda')<NewLine>        loss_fn(outputs, labels).backward()<NewLine>        optimizer.step()<NewLine><NewLine>    cleanup()<NewLine><NewLine><NewLine>def run_demo(demo_fn, world_size):<NewLine>    mp.spawn(demo_fn,<NewLine>             args=(world_size,),<NewLine>             nprocs=world_size,<NewLine>             join=True)<NewLine><NewLine><NewLine>def setup(rank, world_size):<NewLine>    os.environ['MASTER_ADDR'] = 'localhost'<NewLine>    os.environ['MASTER_PORT'] = '12355'<NewLine><NewLine>    # initialize the process group<NewLine>    dist.init_process_group(""NCCL"", rank=rank, world_size=world_size)<NewLine>    torch.cuda.set_device(rank)<NewLine><NewLine><NewLine>def cleanup():<NewLine>    dist.destroy_process_group()<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    args = parser.parse_args()<NewLine>    run_demo(demo_basic, args.world_size)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/zeyang,,zeyang,"June 29, 2020,  3:14pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I believe this is the same issue discussed here: <a href=""https://github.com/pytorch/pytorch/issues/40690"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/40690</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 29, 2020,  3:41pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
87282,"Under the Mnist-Hogwild framework, how to use multi-gpus computing？",2020-06-29T09:11:22.322Z,0,61,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I execute the code example of mnist_hogwild, I find that multiple processes are running parallelly on one gpu.<br/><NewLine>Question: Can multiple processes be executed in parallel on multiple GPUs?</p><NewLine></div>",https://discuss.pytorch.org/u/Srwzx,(zxwang),Srwzx,"June 29, 2020,  9:11am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, that should be possible. Just move/copy the model to different devices. Did you encounter any error when doing that?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 29, 2020,  2:15pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
87083,Is multiprocess Dataloader for IterableDataset keeping the order of original interable?,2020-06-27T06:16:47.882Z,0,61,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I have a question here.</p><NewLine><p>I derive the IterableDataset so it can yield my data. In particular, my input data is a list that I care for the order because later I need to make sure that my output still follows the same order as input. Although more complicated than this, essentially it can be thought as  [1,2,3,4…]</p><NewLine><p>Then I use Dataloader with multiple workers like the following</p><NewLine><pre><code class=""lang-auto"">loader = DataLoader(iterable_dataset, batch_size=256, num_workers=2, worker_init_fn=worker_init_fn)<NewLine>for batch in loader:<NewLine>    print(batch)<NewLine></code></pre><NewLine><p>However, I find the batch is strictly following the original order of the iterable [1,2,3,4]. Even I on purpose delay the first worker which is outputting 1 and 2, and the second worker for 3 and 4, this for loop is still yielding data in the original order, i.e., 1, 2, 3, 4. That makes me believe that the async DataLoader here, although they are processing data in parallel (I timing the worker process right before each yield, and I am pretty sure both workers start to work separately at time 0), they will communicate to always keep the original order of the dataset. For example, if 2,3,4 are all ready at the earlier time but they will be blocked and wait for 1 to finish and finally only allow the yield order to be 1,2,3,4</p><NewLine><p>Is this an expected behavior? It looks true to me and I feel it is sub-optimal as it is not a real queue. If this is the case, I am okay but I am curious which line of source code takes care of that, if it is not, could you please hint if I have some misunderstanding or code error so it looks like this?</p><NewLine><p>Thank you so much!</p><NewLine></div>",https://discuss.pytorch.org/u/Emerald01,(Tian),Emerald01,"June 28, 2020, 12:12am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/vincentqb"">@vincentqb</a> for dataloader questions</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 28, 2020,  9:41pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
87042,Manually edit tensor grad before DDP gradient sync,2020-06-26T18:11:43.497Z,5,99,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I wonder if it is valid to manually edit a tensor’s grad of a DDP model before syncing the gradient. This is what I am trying to do:</p><NewLine><pre><code class=""lang-auto"">1. base_model = MyModel().cuda(device_ids[0])<NewLine>2. ddp_model = DDP(base_model, device_ids)<NewLine><NewLine>3. outputs = ddp_model(input)<NewLine>4. loss1, loss2 = loss_fn(outputs)<NewLine><NewLine>5. with ddp_model.no_sync():<NewLine>6.     local_loss = loss1 + 0. * loss2<NewLine>7.     local_loss.backward(retain_graph=True)<NewLine>8.     for p in base_model.sub_model2.parameters():<NewLine>9.         p.grad *= 0.<NewLine>10.     for p in base_model.sub_model3.parameters():<NewLine>11.         p.grad *= 0.<NewLine><NewLine>12. local_loss = 0. * loss1 + loss2<NewLine>13. local_loss.backward()<NewLine><NewLine>14. optimizer.step()<NewLine></code></pre><NewLine><p>As shown in the code snippet above, I manually modify the base_model’s gradient at lines 8 - 11 before syncing the gradient at line 13. My goal is to use loss1 to only update sub_model1 in the base_model, and use loss2 to update the whole base_model.</p><NewLine><p>The code runs without error, but I am concerned if this manual modification of tensor gradient will cause any issue to the gradient sync mechanism in DDP.</p><NewLine></div>",https://discuss.pytorch.org/u/albert.cwkuo,(Albert),albert.cwkuo,"June 26, 2020,  9:53pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/albert.cwkuo"">@albert.cwkuo</a></p><NewLine><p>With the above code, I think DDP still syncs all grads for both <code>loss1</code> and <code>loss2</code>, because the flag controlled by <code>no_sync</code> ctx manager is used when calling <code>DistributedDataParallel.forward()</code>. So, as the forward is out of the <code>no_sync</code> context, DDP would still prepare to sync all grads during the backward pass.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/5036c94a6e868963e0354fc04c92e204d8d77677/torch/nn/parallel/distributed.py#L477-L498"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/5036c94a6e868963e0354fc04c92e204d8d77677/torch/nn/parallel/distributed.py#L477-L498"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/5036c94a6e868963e0354fc04c92e204d8d77677/torch/nn/parallel/distributed.py#L477-L498</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""477"" style=""counter-reset: li-counter 476 ;""><NewLine><li>@contextmanager</li><NewLine><li>def no_sync(self):</li><NewLine><li>    r""""""</li><NewLine><li>    A context manager to disable gradient synchronizations across DDP</li><NewLine><li>    processes. Within this context, gradients will be accumulated on module</li><NewLine><li>    variables, which will later be synchronized in the first</li><NewLine><li>    forward-backward pass exiting the context.</li><NewLine><li><NewLine></li><li>    Example::</li><NewLine><li><NewLine></li><li>        &gt;&gt;&gt; ddp = torch.nn.DistributedDataParallel(model, pg)</li><NewLine><li>        &gt;&gt;&gt; with ddp.no_sync():</li><NewLine><li>        ...   for input in inputs:</li><NewLine><li>        ...     ddp(input).backward()  # no synchronization, accumulate grads</li><NewLine><li>        ... ddp(another_input).backward()  # synchronize grads</li><NewLine><li>    """"""</li><NewLine><li>    old_require_backward_grad_sync = self.require_backward_grad_sync</li><NewLine><li>    self.require_backward_grad_sync = False</li><NewLine><li>    try:</li><NewLine><li>        yield</li><NewLine></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/5036c94a6e868963e0354fc04c92e204d8d77677/torch/nn/parallel/distributed.py#L477-L498"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>How is <code>MyModel</code> implemented. Does it contain two independent submodules <code>sub_model1</code> and <code>sub_model2</code> and do sth like the following?</p><NewLine><pre><code class=""lang-python"">class MyModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        self.sub_model1 = SomeModel1()<NewLine>        self.sub_model2 = SomeModel2()<NewLine><NewLine>    def forward(self, input):<NewLine>        return self.sub_model1(input), self.sub_model2(input)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> for your reply. This is what <code>MyModel</code> do internally.</p><NewLine><pre><code class=""lang-python"">class MyModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        self.sub_model1 = SomeModel1()<NewLine>        self.sub_model2 = SomeModel2()<NewLine><NewLine>    def forward(self, input):<NewLine>        out1 = self.sub_model2(input)<NewLine>        out2 = self.sub_model1(out1)<NewLine>        return out1, out2<NewLine></code></pre><NewLine><p>I want the gradient to be synced eventually when I call <code>backward()</code> at line 11, so the above code seems correct? My goal is to accumulate gradient from both loss1 and loss2 to sub_model2 and accumulate gradient only from loss2 to sub_model1. That’s why I try to zero out grad in sub_model1.</p><NewLine><p>Note that I use <code>local_loss = loss1 + 0.0 * loss2</code> and <code>local_loss = 0.0 * loss1 + loss2</code> to mask out part of the loss before calling <code>local_loss.backward()</code>.</p><NewLine><p>----------------update------------</p><NewLine><pre><code class=""lang-python"">class MyModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        self.sub_model1 = SomeModel1()<NewLine>        self.sub_model2 = SomeModel2()<NewLine>        self.sub_model3 = SomeModel3()<NewLine><NewLine>    def forward(self, input):<NewLine>        out1 = self.sub_model2(self.sub_model1(input))<NewLine>        out2 = self.sub_model3(out1)<NewLine>        return out1, out2<NewLine></code></pre><NewLine><p>My goal is to accumulate gradient from both loss1 and loss2 to sub_model1 and accumulate gradient only from loss2 to sub_model2 and sub_model3. That’s why I try to zero out grad in sub_model2 and sub_model3.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""87042"" data-username=""albert.cwkuo""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/albert.cwkuo/40/12567_2.png"" width=""20""/> albert.cwkuo:</div><NewLine><blockquote><NewLine><p>My goal is to accumulate gradient from both loss1 and loss2 to sub_model2 and accumulate gradient only from loss2 to sub_model1.</p><NewLine></blockquote><NewLine></aside><NewLine><p>In that case, calling backward once on <code>loss1+loss2</code> might be sufficient? Is the following result what you want?</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine>class MyModel(nn.Module):<NewLine><NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        with torch.no_grad():<NewLine>            self.net1 = nn.Linear(1, 1)<NewLine>            self.net1.weight.copy_(torch.ones(1, 1))<NewLine>            self.net1.bias.copy_(torch.zeros(1))<NewLine>            self.net2 = nn.Linear(1, 1)<NewLine>            self.net2.weight.copy_(torch.ones(1, 1))<NewLine>            self.net2.bias.copy_(torch.zeros(1))<NewLine><NewLine>    def forward(self, x):<NewLine>        out1 = self.net1(x)<NewLine>        out2 = self.net2(out1)<NewLine>        return out1 + out2<NewLine><NewLine><NewLine>print(""=============="")<NewLine>model = MyModel()<NewLine>model(torch.ones(1, 1)).sum().backward()<NewLine>print(""net1 grad is: "", model.net1.weight.grad)<NewLine>print(""net2 grad is: "", model.net2.weight.grad)<NewLine><NewLine>print(""=============="")<NewLine>model = MyModel()<NewLine>model.net1(torch.ones(1, 1)).sum().backward()<NewLine>print(""net1 grad is: "", model.net1.weight.grad)<NewLine>print(""net1 grad is: "", model.net2.weight.grad)<NewLine><NewLine>print(""=============="")<NewLine>model = MyModel()<NewLine>model.net2(torch.ones(1, 1)).sum().backward()<NewLine>print(""net1 grad is: "", model.net1.weight.grad)<NewLine>print(""net2 grad is: "", model.net2.weight.grad)<NewLine></code></pre><NewLine><p>outputs are</p><NewLine><pre><code class=""lang-auto"">==============<NewLine>net1 grad is:  tensor([[2.]])<NewLine>net2 grad is:  tensor([[1.]])<NewLine>==============<NewLine>net1 grad is:  tensor([[1.]])<NewLine>net2 grad is:  None<NewLine>==============<NewLine>net1 grad is:  None<NewLine>net2 grad is:  tensor([[1.]])<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Okay that makes sense, but let me clarify how MyModel works internally so that the proposed solution may not work in the case. The reply has been updated.</p><NewLine><p>In short, I have 3 subnets inside MyModel. One of the loss depends on sub_model1 and sub_model2, but I only want to update sub_model1 with this loss. Therefore I need to zero out the grad in sub_model2 when calling backward on that loss.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""87042"" data-username=""albert.cwkuo""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/albert.cwkuo/40/12567_2.png"" width=""20""/> albert.cwkuo:</div><NewLine><blockquote><NewLine><p>My goal is to accumulate gradient from both loss1 and loss2 to sub_model1 and accumulate gradient only from loss2 to sub_model2 and sub_model3.</p><NewLine></blockquote><NewLine></aside><NewLine><p>With the above statement, should the forward function be something like below?</p><NewLine><pre><code class=""lang-python"">    def forward(self, input):<NewLine>        out1 = self.sub_model1(input)<NewLine>        out2 = self.sub_model3(self.sub_model2(out1))<NewLine>        return out1, out2<NewLine></code></pre><NewLine><p>With this code, <code>out1.sum().backward()</code> will only compute grads for <code>sub_model1</code>, and <code>out2.sum().backward()</code> will compute grads for all sub-models. And <code>(out1 + out2).sum().backward()</code> should meet the cited statement above.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>In my use case it’s</p><NewLine><pre><code class=""lang-python"">    def forward(self, input):<NewLine>        out1 = self.sub_model2(self.sub_model1(input))<NewLine>        out2 = self.sub_model3(out1)<NewLine>        return out1, out2<NewLine></code></pre><NewLine><p>That’s the tricky part. That’s why I want to zero out the gradient of sub_mode2 after calling backward on loss1 (loss1 is computed on out1).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/albert.cwkuo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/albert.cwkuo; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/albert.cwkuo; <NewLine> ,"REPLY_DATE 1: June 26, 2020,  6:33pm; <NewLine> REPLY_DATE 2: June 26, 2020,  9:51pm; <NewLine> REPLY_DATE 3: June 26, 2020,  7:23pm; <NewLine> REPLY_DATE 4: June 26, 2020,  9:57pm; <NewLine> REPLY_DATE 5: June 26, 2020, 10:54pm; <NewLine> REPLY_DATE 6: June 26, 2020, 11:17pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
86988,Resume DDP training after out of memory error,2020-06-26T04:35:28.519Z,0,65,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am training a NAS network that would sometimes be out of memory on the GPUs due to different predicted model configurations on each run.</p><NewLine><pre><code class=""lang-auto"">def check_oom(func):<NewLine>    def wrapper(*args, **kwargs):<NewLine>            try:<NewLine>                  return func(*arg, **kwargs)<NewLine>            except RuntimeError:<NewLine>                  torch.cuda.empty_cache()<NewLine>                  return None<NewLine>   return wrapper<NewLine><NewLine># model's forward function<NewLine>@check_oom<NewLine>def forward(model, input):<NewLine>       return model(input)<NewLine><NewLine># main loop<NewLine>def main():<NewLine>      dataset = Dataloader(...)<NewLine>      net = Model(...)<NewLine>      for input in dataset:<NewLine>             output = forward(net , input)<NewLine>             (....... training code)<NewLine>     torch.cuda.synchronize()<NewLine><NewLine></code></pre><NewLine><p>above is the pseudo-code I use for my training. However, in practice, OOM events will hang the entire training with GPUs 's maxed out at 100% utility rate.<br/><NewLine>What can I do ?</p><NewLine></div>",https://discuss.pytorch.org/u/Scott_Hoang,(Scott Hoang),Scott_Hoang,"June 26, 2020,  4:35am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/scott_hoang"">@Scott_Hoang</a>, yes, this is the expected behavior, as OOM in one of the process will lead to AllReduce communication desync across the entire DDP gang.  <a href=""https://pytorch.org/elastic"" rel=""nofollow noopener"">https://pytorch.org/elastic</a> is  built to solve this problem. It will destruct all DDP instances across all processes, reconstruct a new gang, and then recover from the previous checkpoint.</p><NewLine><p>cc <a class=""mention"" href=""/u/kiuk_chung"">@Kiuk_Chung</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is exactly what I am looking for. Thank!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Scott_Hoang; <NewLine> ,"REPLY_DATE 1: June 26, 2020,  4:42pm; <NewLine> REPLY_DATE 2: June 26, 2020,  4:42pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
86951,Architecture of distributed Pytorch,2020-06-25T20:37:15.815Z,0,78,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I have a question about the architecture of distributed PyTorch!</p><NewLine><p>When I run some examples, I saw that we can send and receive directly from worker A to worker B.<br/><NewLine>Why do we need MASTER_PORT and MASTER_ADDRESS?<br/><NewLine>For port, we can understand that they need this number to recognize other workers which belong to the same program or not. However, I do not understand why we need master_add?</p><NewLine><p>if it is a Master-Slave model, I think that is no problem, and Master worker will manage all works.</p><NewLine><p>Thanks,</p><NewLine></div>",https://discuss.pytorch.org/u/ph0123,(chau phuong),ph0123,"June 25, 2020,  8:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ph0123"">@ph0123</a></p><NewLine><p>The reason is that when we implement <code>torch.distributed.rpc</code>, we would like to abstract out the comm layer and reuse whatever is available in <code>torch.disributed</code>. At that time, <code>ProcessGroup</code> is the only option we have, which requires a rendezvous during initialization. The master port and address is needed for that rendezvous. Subsequent communications do not go through the master address.</p><NewLine><p>As of v1.6.0, we added a new P2P comm backend implementation, <a href=""https://pytorch.org/docs/master/rpc.html#tensorpipe-backend"" rel=""nofollow noopener"">https://pytorch.org/docs/master/rpc.html#tensorpipe-backend</a>. And we do plan to remove the requirement for <code>MASTER_PORT</code>/<code>MASTER_ADDRESS</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 25, 2020,  9:44pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
86707,How to gather a list of strings from different rank machines in DDP mode with NCCL backend?,2020-06-24T11:26:18.337Z,1,66,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>These days I’ve accelerated the training of models with DistributedDataParallel. NCCL is used as the backend of torch.distributed. Currently, I try to do validation with a list of strings stored in the memory. However, with the multi-process mechanism, it’s hard to share the list across different ranks than in DP mode. Is there any good way to solve the problem?</p><NewLine></div>",https://discuss.pytorch.org/u/sherylwang,(Xiaqing Xu),sherylwang,"June 24, 2020, 11:26am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There was an PR to provide such a feature for general Python objects, but not landed yet. You can copy that code for now.</p><NewLine><aside class=""onebox githubpullrequest""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/28811/files"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Pull Request""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 12 16"" width=""60""><path d=""M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/pull/28811"" rel=""nofollow noopener"" target=""_blank"">[distributed] implement all_gather for arbitrary python objects</a><NewLine></h4><NewLine><div class=""branches""><NewLine><code>pytorch:gh/rohan-varma/25/base</code> ← <code>pytorch:gh/rohan-varma/25/head</code><NewLine></div><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-10-28"" data-format=""ll"" data-time=""21:40:01"" data-timezone=""UTC"">09:40PM - 28 Oct 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/rohan-varma"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""rohan-varma"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/8039770?v=4"" width=""20""/><NewLine>          rohan-varma<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""9 commits changed 2 files with 93 additions and 0 deletions""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/28811/files"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+93</span><NewLine><span class=""removed"">-0</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply. This design is interesting. <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/sherylwang; <NewLine> ,"REPLY_DATE 1: June 24, 2020,  2:21pm; <NewLine> REPLY_DATE 2: June 25, 2020,  2:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
86763,AttributeError: &lsquo;torch.distributed.rpc.Future&rsquo; object has no attribute &lsquo;then&rsquo;,2020-06-24T16:28:16.057Z,0,84,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am trying to use <code>then</code> method that is explained <a href=""https://pytorch.org/docs/master/futures.html"" rel=""nofollow noopener"">here</a> along with <code>rpc_async</code>. Yet, I get this error:<br/><NewLine>AttributeError: ‘torch.distributed.rpc.Future’ object has no attribute ‘then’</p><NewLine><p>I tried just importing <code>torch.futures.Future</code> and I got also an error:<br/><NewLine>ModuleNotFoundError: No module named ‘torch.futures’</p><NewLine><p>Yet, importing <code>torch.distributed.rpc.Future</code> works. But, the imported class does not have a constructor, where running <code>inspect.getmembers(Future, predicate=inspect.isfunction)</code> gives an empty array.</p><NewLine><p>I have the latest torch version (1.5.1).</p><NewLine><p>Would you please help me with this issue?</p><NewLine><p>Thanks,<br/><NewLine>Arsany</p><NewLine></div>",https://discuss.pytorch.org/u/aguirguis,(Arsany Guirguis),aguirguis,"June 24, 2020,  4:28pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/aguirguis"">@aguirguis</a>, the <code>torch.futures</code> package is introduced as an experimental feature in v1.6. If you use the nightly binary or compile from master, the <code>then</code> API should be there.</p><NewLine><p>Sorry, we should have mentioned that it is only available for v1.6+</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 24, 2020,  7:22pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
86601,Model&rsquo;s parameters update during DDP training,2020-06-23T19:13:02.493Z,7,115,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m using DDP to train Neural Architecture Search networks which contained a controller and a model network. During training, my controller predictss a model’s architecture that maximize reward. the call looks like this.</p><NewLine><pre><code class=""lang-auto""># both model and controller are torch.nn.DistributedDataParallel<NewLine>arch = controller.forward(conditions)<NewLine>model.module.set_arch(arch) # modified model internal architecture.<NewLine>output = model.forward(input)...<NewLine></code></pre><NewLine><p>However, in DDP docs I noticed the following:</p><NewLine><blockquote><NewLine><p>… warning::<br/><NewLine>You should never try to change your model’s parameters after wrapping<br/><NewLine>up your model with DistributedDataParallel. In other words, when<br/><NewLine>wrapping up your model with DistributedDataParallel, the constructor of<br/><NewLine>DistributedDataParallel will register the additional gradient<br/><NewLine>reduction functions on all the parameters of the model itself at the<br/><NewLine>time of construction. If you change the model’s parameters after<br/><NewLine>the DistributedDataParallel construction, this is not supported and<br/><NewLine>unexpected behaviors can happen, since some parameters’ gradient<br/><NewLine>reduction functions might not get called.</p><NewLine></blockquote><NewLine><p>So I’m just wondering what is the correct way to do this? or if NAS is not suitable with DDP.</p><NewLine></div>",https://discuss.pytorch.org/u/Scott_Hoang,(Scott Hoang),Scott_Hoang,"June 23, 2020,  7:13pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p><code>model.module.set_arch(arch) # modified model internal architecture.</code></p><NewLine></blockquote><NewLine><ol><NewLine><li>By doing the above, are you removing parameters from the model or adding new parameters into the model? If yes, then it won’t work with DDP, as DDP creates communication buckets at construction time using the parameters returned by <code>model.parameters()</code> field. Hence, if the <code>model.parameters()</code> returns a different set of parameters, DDP won’t adapt to it.<NewLine><ul><NewLine><li>To make it work, you can create a new DDP instance using the modified model whenever the model gets updated. But all DDP processes need to do the same at the same time using the same model.</li><NewLine></ul><NewLine></li><NewLine><li>If it just changes the value of those parameters, it should be fine.</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>can you clarify the different between modifying and replacing?</p><NewLine><pre><code class=""lang-auto"">def __init__(self):<NewLine>    self._arch =  torch.variable(&lt;shape&gt; , required_grad=True)<NewLine>def set_arch(self, arch):<NewLine>    self._arch = arch # is this modifying or replacing? <NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I believe this is replacing. You can use <code>self._arch.copy_(arch)</code> to override the value. See the code below.</p><NewLine><pre><code class=""lang-python"">import torch<NewLine><NewLine>x = torch.zeros(2, 2)<NewLine>y = torch.ones(2, 2)<NewLine>print(""x storage: "", x.data_ptr())<NewLine>print(""y storage: "", y.data_ptr())<NewLine>x = y<NewLine>print(""x storage: "", x.data_ptr())<NewLine>z = torch.zeros(2, 2) + 2<NewLine>print(""z storage: "", z.data_ptr())<NewLine>x.copy_(z)<NewLine>print(""x storage: "", x.data_ptr()) <NewLine>print(x)<NewLine></code></pre><NewLine><p>outputs are:</p><NewLine><pre><code class=""lang-auto"">x storage:  94191491020800                                                                                                                                           y storage:  94191523992320                                                                                                                                           <NewLine>x storage:  94191523992320                                                                                                                                        <NewLine>z storage:  94191523994816                                                                                                                                     <NewLine>x storage:  94191523992320                                                                                                                                    <NewLine>tensor([[2., 2.],                                                                                                                                               <NewLine>        [2., 2.]])   <NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>This might be it. if DDP wrapper kept a ptr to my arch settings, then it will not see the new value since it with a different pointer.<br/><NewLine>So does that mean that DDP.module params is a stale copy of our model??</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>So does that mean that DDP.module params is a stale copy of our model??</p><NewLine></blockquote><NewLine><p>I believe so. As DDP remembers the variables at construction time:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/09285070a70d146b158db1e1e44b2c031a5c70b0/torch/csrc/distributed/c10d/reducer.cpp#L32"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/09285070a70d146b158db1e1e44b2c031a5c70b0/torch/csrc/distributed/c10d/reducer.cpp#L32"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/09285070a70d146b158db1e1e44b2c031a5c70b0/torch/csrc/distributed/c10d/reducer.cpp#L32</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""22"" style=""counter-reset: li-counter 21 ;""><NewLine><li>}</li><NewLine><li><NewLine></li><li>} // namespace</li><NewLine><li><NewLine></li><li>Reducer::Reducer(</li><NewLine><li>    std::vector&lt;std::vector&lt;torch::autograd::Variable&gt;&gt; replicas,</li><NewLine><li>    std::vector&lt;std::vector&lt;size_t&gt;&gt; bucket_indices,</li><NewLine><li>    std::shared_ptr&lt;c10d::ProcessGroup&gt; process_group,</li><NewLine><li>    std::vector&lt;std::vector&lt;bool&gt;&gt; expect_sparse_gradients,</li><NewLine><li>    int64_t bucket_bytes_cap)</li><NewLine><li class=""selected"">    : replicas_(std::move(replicas)),</li><NewLine><li>      process_group_(std::move(process_group)),</li><NewLine><li>      expect_sparse_gradients_(std::move(expect_sparse_gradients)),</li><NewLine><li>      expect_autograd_hooks_(false),</li><NewLine><li>      require_finalize_(false),</li><NewLine><li>      next_bucket_(0),</li><NewLine><li>      has_marked_unused_parameters_(false),</li><NewLine><li>      local_used_maps_reduced_(false),</li><NewLine><li>      backward_stats_base_(0),</li><NewLine><li>      has_rebuilt_bucket_(false),</li><NewLine><li>      bucket_bytes_cap_(bucket_bytes_cap) {</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>And there might be more than that. DDP might not be able to read that value at all. Because DDP registers a backward hook on each parameter, and relying on that hook to notify DDP when and what to read. Those hooks are installed at DDP construction time as well. If you create a new variable and assign it to <code>self._arch</code>, that hook might be lost.</p><NewLine><p>cc <a class=""mention"" href=""/u/alband"">@albanD</a> is the above statement on variable hook correct?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Yes I think this note explicitly warns you against doing this. You should not change the Parameters.</p><NewLine><p>As a side note, you should never call the <code>forward()</code> of your module directly but call <code>module(input)</code>.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>what if i modified my forward function such that</p><NewLine><pre><code class=""lang-auto"">&gt; forward(input, arch):<NewLine>&gt;      self._arch = arch<NewLine></code></pre><NewLine><p>will this works?<br/><NewLine>Also does DDP keeps DDP.module value up-to-date?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>IIUC, that will still remove DDP autograd hooks on <code>self._arch</code>.</p><NewLine><p>Question, do you need the backward pass to compute the gradients for <code>self._arch</code>? If not, you can explicitly setting <code>self._arch.requires_grad = False</code> before passing the model to DDP ctor to tell DDP to ignore <code>self._arch</code>. Then, the above assignment would work.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you. My model is now performing as expected <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Scott_Hoang; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Scott_Hoang; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Scott_Hoang; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Scott_Hoang; <NewLine> ,"REPLY_DATE 1: June 23, 2020,  8:29pm; <NewLine> REPLY_DATE 2: June 23, 2020,  9:17pm; <NewLine> REPLY_DATE 3: June 23, 2020, 10:06pm; <NewLine> REPLY_DATE 4: June 23, 2020, 10:10pm; <NewLine> REPLY_DATE 5: June 23, 2020, 10:18pm; <NewLine> REPLY_DATE 6: June 23, 2020, 11:16pm; <NewLine> REPLY_DATE 7: June 23, 2020, 11:48pm; <NewLine> REPLY_DATE 8: June 24, 2020,  3:46pm; <NewLine> REPLY_DATE 9: June 24, 2020,  3:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> 
86432,Does it ever make sense to try model parallelism even if the model fits?,2020-06-22T16:03:48.254Z,9,203,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a model that fits perfectly fine on any one of my GPUs (4x1080Tis) but I had the bright idea that maybe I could speed up a forward pass (at inference time) by partitioning up one of the layers (a very “tall” Conv2d - i.e. &gt;20 output channels) across all of my GPUs. So I used DDP to map across my GPUs and surprisingly (or maybe not?) forward pass actually gets slower with increasing number of GPUs. Is this to be expected?</p><NewLine><p>I’m not an expert on the execution pipeline on GPUs but is it the case that any individual CUDA kernel (e.g. my “tall” Conv2d) gets executed in parallel? I’m guessing that that’s my issue - that the layer I’m partitioning up already gets executed in parallel and the scatter/gather just adds copy (and process instantiation) latencies.</p><NewLine></div>",https://discuss.pytorch.org/u/makslevental,(Maksim Levental),makslevental,"June 22, 2020,  4:04pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""86432"" data-username=""makslevental""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/makslevental/40/15605_2.png"" width=""20""/> makslevental:</div><NewLine><blockquote><NewLine><p>maybe I could speed up a forward pass (at inference time) by partitioning up one of the layers (a very “tall” Conv2d - i.e. &gt;20 output channels) across all of my GPUs. So I used DDP to map across my GPUs</p><NewLine></blockquote><NewLine></aside><NewLine><p>Hey <a class=""mention"" href=""/u/makslevental"">@makslevental</a> could you please elaborate on how did you manage to use DDP after splitting one layer? Does it mean each DDP process now sees a different model? Or is it true that each DDP process no longer has exclusive access to its own GPU?</p><NewLine><blockquote><NewLine><p>and surprisingly (or maybe not?) forward pass actually gets slower with increasing number of GPUs. Is this to be expected?</p><NewLine></blockquote><NewLine><p>Since with the split, each forward pass will do cross-device communications, so it is possible to see slowdowns.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> I split my network across the layer boundary so yes each DDP now sees a different model - imagine instead of a 48 output channel Conv2d applying 4x12 output channel Conv2ds.</p><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""86432"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>Since with the split, each forward pass will do cross-device communications, so it is possible to see slowdowns.</p><NewLine></blockquote><NewLine></aside><NewLine><p>why is there cross-talk? I with <code>with torch.no_grad()</code> and <code>for p in model.parameters(): p.requires_grad = False</code> in every <code>run</code>.<br/><NewLine>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""86432"" data-username=""makslevental""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/makslevental/40/15605_2.png"" width=""20""/> makslevental:</div><NewLine><blockquote><NewLine><p>I split my network across the layer boundary so yes each DDP now sees a different model</p><NewLine></blockquote><NewLine></aside><NewLine><p>I see. Is this forward only for inference or do you also run backward for training? If it is the latter, this might kill the correctness of DDP, as DDP expects the model in each process to be exactly the same, otherwise, the AllReduce communication across DDP process could mess up the gradients.</p><NewLine><blockquote><NewLine><p>why is there cross-talk? I with  <code>with torch.no_grad()</code>  and  <code>for p in model.parameters(): p.requires_grad = False</code>  in every  <code>run</code> .</p><NewLine></blockquote><NewLine><p>Looks like you are doing inference instead of training? In this case, don’t you need to somehow gather/combine the outputs from the four different <code>Conv2d</code> layers from 4 different DDP processes? Otherwise, how did you get the final inference result?</p><NewLine><p>BTW, since this is inference only, why do you need DDP?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry actually I just realized I’ve completely misspoken. I’m not wrapping my model in DDP. I was planning on doing this and then I realized it replicates across nodes where as I need to send distinct (but related) models to nodes.</p><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""86432"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>Looks like you are doing inference instead of training? In this case, don’t you need to somehow gather/combine the outputs from the four different <code>Conv2d</code> layers from 4 different DDP processes? Otherwise, how did you get the final inference result?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes this is correct, I do a map and then I plan on doing a concat to reconstruct the output channels as if they all came from the same Conv2d. I think you can basically get the idea from this code snippet</p><NewLine><pre><code class=""lang-python"">def run(rank, size):<NewLine>    with torch.no_grad():<NewLine>        image_pth = Path(os.path.dirname(os.path.realpath(__file__))) / Path(<NewLine>            ""../simulation/screenshot.png""<NewLine>        )<NewLine><NewLine>        screenshot = SimulPLIF(img_path=image_pth, num_repeats=1, load_truth=False)<NewLine>        img_height, img_width = screenshot[0].squeeze(0).numpy().shape<NewLine><NewLine>        from nn_dog import PIN_MEMORY<NewLine><NewLine>        train_dataloader = DataLoader(screenshot, batch_size=1, pin_memory=PIN_MEMORY)<NewLine>        dog = DifferenceOfGaussiansFFT(<NewLine>            img_height=img_height,<NewLine>            img_width=img_width,<NewLine>            sigma_bins=48 // size,<NewLine>            max_sigma=30,<NewLine>        ).to(rank, non_blocking=PIN_MEMORY)<NewLine>        for p in dog.parameters():<NewLine>            p.requires_grad = False<NewLine>        dog.eval()<NewLine>        torch.cuda.synchronize(rank)<NewLine><NewLine>        dogs = []<NewLine>        for i in range(10):<NewLine>            img_tensor = next(iter(train_dataloader))<NewLine>            img_tensor = img_tensor.to(rank)<NewLine>            torch.cuda.synchronize(rank)<NewLine>            dogs.append(dog(img_tensor))<NewLine>        return dogs<NewLine><NewLine><NewLine>def init_process(rank_size_fn, backend=""nccl""):<NewLine>    rank, size, fn = rank_size_fn<NewLine>    """""" Initialize the distributed environment. """"""<NewLine>    os.environ[""MASTER_ADDR""] = ""127.0.0.1""<NewLine>    os.environ[""MASTER_PORT""] = ""29500""<NewLine>    dist.init_process_group(backend, rank=rank, world_size=size)<NewLine>    return fn(rank, size)<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    set_start_method(""spawn"")<NewLine><NewLine>    size = 4<NewLine>    pool = Pool(processes=size)<NewLine>    start = time.monotonic()<NewLine>    res = pool.map(init_process, [(i, size, run) for i in range(size)])<NewLine>    end = time.monotonic()<NewLine>    print(end - start)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see. How did you measure the latency of the forward pass? Did you use <a href=""https://pytorch.org/docs/stable/cuda.html#torch.cuda.Event.elapsed_time"" rel=""nofollow noopener""><code>elapsed_time</code></a> from <a href=""https://pytorch.org/docs/stable/cuda.html#streams-and-events"" rel=""nofollow noopener"">CUDA events</a> to wrap <code>dog(img_tensor)</code>?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>it’s a rough measure but it’s right there</p><NewLine><pre><code class=""lang-python"">    start = time.monotonic()<NewLine>    res = pool.map(init_process, [(i, size, run) for i in range(size)])<NewLine>    end = time.monotonic()<NewLine>    print(end - start)<NewLine></code></pre><NewLine><p>then since in <code>run</code> i repeat 10 times i reason that i’m amortizing process instantiation across those 10 inference passes. and so between 1 and 4 GPUs <code>end-start</code> just grows.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""86432"" data-username=""makslevental""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/makslevental/40/15605_2.png"" width=""20""/> makslevental:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">        for i in range(10):<NewLine>            img_tensor = next(iter(train_dataloader))<NewLine>            img_tensor = img_tensor.to(rank)<NewLine>            torch.cuda.synchronize(rank)<NewLine>            dogs.append(dog(img_tensor))<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>Could you please wrap the above code with some time measure and check how much percentage does it contribute to the total delay of <code>end-start</code>?</p><NewLine><p>BTW, you might be able to avoid the <code>torch.cuda.synchronize(rank)</code> above, as the copy  (<code>tensor.to</code>) should have synchronize the destination device properly.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>okay i removed the sync and enclosed the loop in time.monotonic.</p><NewLine><pre><code class=""lang-auto"">2 gpus:<NewLine>1.2319540430326015<NewLine>1.3151386808604002<NewLine>total: 6.296403981978074<NewLine><NewLine>3 gpus:<NewLine>1.1622967889998108<NewLine>1.3194731972180307<NewLine>1.3116707119625062<NewLine>total: 5.875259702792391<NewLine><NewLine>4 gpus:<NewLine>1.1516663811635226<NewLine>1.4554521720856428<NewLine>1.76222850009799<NewLine>1.8313195349182934<NewLine>total: 6.504983321996406<NewLine></code></pre><NewLine><p>i ran this several times it’s consistent.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>The large variance (1.15 vs 1.83) seems to suggest there are some sort of contention. I wonder if that is caused by the data loading. What if we use CUDA event <code>elapsed_time</code> to measure <code>dog(img_tensor)</code>? Note that <code>time. monotonic()</code> does not guarantee to give the correct measurements as there could still be CUDA ops pending in stream.</p><NewLine><p>Or is it possible to get a self-contained example that we can investigate locally? E.g., using <code>torch.rand</code> to create random inputs instead loading from <code>screenshot.png</code></p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>this is self-contained; except for the standard stuff (numpy, pytorch) you only need opt_einsum</p><NewLine><pre><code class=""lang-python""><NewLine>import math<NewLine>import numbers<NewLine>import os<NewLine>import time<NewLine>from functools import partial<NewLine>from typing import Tuple<NewLine><NewLine>import numpy as np<NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>from opt_einsum import contract<NewLine>from torch import nn<NewLine>from torch.multiprocessing import set_start_method, Pool<NewLine><NewLine>class DifferenceOfGaussiansFFT(nn.Module):<NewLine>    def __init__(<NewLine>            self,<NewLine>            *,<NewLine>            img_height: int,<NewLine>            img_width: int,<NewLine>            min_sigma: int = 1,<NewLine>            max_sigma: int = 10,<NewLine>            sigma_bins: int = 50,<NewLine>            truncate: float = 5.0,<NewLine>    ):<NewLine>        super(DifferenceOfGaussiansFFT, self).__init__()<NewLine>        self.img_height = img_height<NewLine>        self.img_width = img_width<NewLine>        self.signal_ndim = 2<NewLine><NewLine>        self.sigma_list = np.concatenate(<NewLine>            [<NewLine>                np.linspace(min_sigma, max_sigma, sigma_bins),<NewLine>                [max_sigma + (max_sigma - min_sigma) / (sigma_bins - 1)],<NewLine>            ]<NewLine>        )<NewLine>        sigmas = torch.from_numpy(self.sigma_list)<NewLine>        self.register_buffer(""sigmas"", sigmas)<NewLine>        # print(""gaussian pyramid sigmas: "", len(sigmas), sigmas)<NewLine><NewLine>        # accommodate largest filter<NewLine>        self.max_radius = int(truncate * max(sigmas) + 0.5)<NewLine>        max_bandwidth = 2 * self.max_radius + 1<NewLine>        # pad fft to prevent aliasing<NewLine>        padded_height = img_height + max_bandwidth - 1<NewLine>        padded_width = img_width + max_bandwidth - 1<NewLine>        # round up to next power of 2 for cheaper fft.<NewLine>        self.fft_height = 2 ** math.ceil(math.log2(padded_height))<NewLine>        self.fft_width = 2 ** math.ceil(math.log2(padded_width))<NewLine>        self.pad_input = nn.ConstantPad2d(<NewLine>            (0, self.fft_width - img_width, 0, self.fft_height - img_height), 0<NewLine>        )<NewLine><NewLine>        self.f_gaussian_pyramid = []<NewLine>        kernel_pad = nn.ConstantPad2d(<NewLine>            # left, right, top, bottom<NewLine>            (0, self.fft_width - max_bandwidth, 0, self.fft_height - max_bandwidth),<NewLine>            0,<NewLine>        )<NewLine>        for i, s in enumerate(sigmas):<NewLine>            radius = int(truncate * s + 0.5)<NewLine>            width = 2 * radius + 1<NewLine>            kernel = torch_gaussian_kernel(width=width, sigma=s.item())<NewLine><NewLine>            # this is to align all of the kernels so that the eventual fft shifts a fixed amount<NewLine>            center_pad_size = self.max_radius - radius<NewLine>            if center_pad_size &gt; 0:<NewLine>                centered_kernel = nn.ConstantPad2d(center_pad_size, 0)(kernel)<NewLine>            else:<NewLine>                centered_kernel = kernel<NewLine><NewLine>            padded_kernel = kernel_pad(centered_kernel)<NewLine><NewLine>            f_kernel = torch.rfft(<NewLine>                padded_kernel, signal_ndim=self.signal_ndim, onesided=True<NewLine>            )<NewLine>            self.f_gaussian_pyramid.append(f_kernel)<NewLine><NewLine>        self.f_gaussian_pyramid = nn.Parameter(<NewLine>            torch.stack(self.f_gaussian_pyramid, dim=0), requires_grad=False<NewLine>        )<NewLine><NewLine>    def forward(self, input: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:<NewLine>        img_height, img_width = list(input.size())[-self.signal_ndim:]<NewLine>        assert (img_height, img_width) == (self.img_height, self.img_width)<NewLine><NewLine>        padded_input = self.pad_input(input)<NewLine>        f_input = torch.rfft(padded_input, signal_ndim=self.signal_ndim, onesided=True)<NewLine>        f_gaussian_images = comp_mul(self.f_gaussian_pyramid, f_input)<NewLine>        gaussian_images = torch.irfft(<NewLine>            f_gaussian_images,<NewLine>            signal_ndim=self.signal_ndim,<NewLine>            onesided=True,<NewLine>            signal_sizes=padded_input.shape[1:],<NewLine>        )<NewLine><NewLine>        # fft induces a shift so needs to be undone<NewLine>        gaussian_images = gaussian_images[<NewLine>                          :,  # batch dimension<NewLine>                          :,  # filter dimension<NewLine>                          self.max_radius: self.img_height + self.max_radius,<NewLine>                          self.max_radius: self.img_width + self.max_radius,<NewLine>                          ]<NewLine><NewLine>        return gaussian_images<NewLine><NewLine><NewLine>def torch_gaussian_kernel(<NewLine>        width: int = 21, sigma: int = 3, dim: int = 2<NewLine>) -&gt; torch.Tensor:<NewLine>    """"""Gaussian kernel<NewLine><NewLine>    Parameters<NewLine>    ----------<NewLine>    width: bandwidth of the kernel<NewLine>    sigma: std of the kernel<NewLine>    dim: dimensions of the kernel (images -&gt; 2)<NewLine><NewLine>    Returns<NewLine>    -------<NewLine>    kernel : gaussian kernel<NewLine><NewLine>    """"""<NewLine><NewLine>    if isinstance(width, numbers.Number):<NewLine>        width = [width] * dim<NewLine>    if isinstance(sigma, numbers.Number):<NewLine>        sigma = [sigma] * dim<NewLine>    kernel = 1<NewLine>    meshgrids = torch.meshgrid(<NewLine>        [torch.arange(size, dtype=torch.float32) for size in width]<NewLine>    )<NewLine>    for size, std, mgrid in zip(width, sigma, meshgrids):<NewLine>        mean = (size - 1) / 2<NewLine>        kernel *= (<NewLine>                1<NewLine>                / (std * math.sqrt(2 * math.pi))<NewLine>                * torch.exp(-(((mgrid - mean) / std) ** 2) / 2)<NewLine>        )<NewLine><NewLine>    # Make sure sum of values in gaussian kernel equals 1.<NewLine>    kernel = kernel / torch.sum(kernel)<NewLine>    return kernel<NewLine><NewLine><NewLine>def chunks(lst, n):<NewLine>    """"""Yield successive n-sized chunks from lst.""""""<NewLine>    for i in range(0, len(lst), n):<NewLine>        yield lst[i: i + n]<NewLine><NewLine><NewLine>def comp_mul(x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:<NewLine>    """"""Complex multiplies two complex 3d tensors<NewLine><NewLine>    x = (x_real, x_im)<NewLine>    y = (y_real, y_im)<NewLine>    x*y = (x_real*y_real - x_im*y_im, x_real*y_im + x_im*y_real)<NewLine><NewLine>    Last dimension is x2 with x[..., 0] real and x[..., 1] complex.<NewLine>    Dimensions (-3,-2) must be equal of both a and b must be the same.<NewLine><NewLine>    Examples<NewLine>    ________<NewLine>    &gt;&gt;&gt; f_filters = torch.rand((20, 1024, 1024, 2))<NewLine>    &gt;&gt;&gt; f_imgs = torch.rand((5, 1024, 1024, 2))<NewLine>    &gt;&gt;&gt; f_filtered_imgs = comp_mul(f_filters, f_imgs)<NewLine><NewLine>    Parameters<NewLine>    ----------<NewLine>    x : Last dimension is (a,b) of a+ib<NewLine>    y : Last dimension is (a,b) of a+ib<NewLine><NewLine>    Returns<NewLine>    -------<NewLine>    z : x*y<NewLine><NewLine>    """"""<NewLine><NewLine>    # hadamard product of every filter against every batch image<NewLine>    op = partial(contract, ""fuv,buv-&gt;bfuv"")<NewLine>    assert x.shape[-1] == y.shape[-1] == 2<NewLine>    x_real, x_im = x.unbind(-1)<NewLine>    y_real, y_im = y.unbind(-1)<NewLine>    z = torch.stack(<NewLine>        [op(x_real, y_real) - op(x_im, y_im), op(x_real, y_im) + op(x_im, y_real)],<NewLine>        dim=-1,<NewLine>    )<NewLine>    return z<NewLine><NewLine><NewLine>def run(rank, size):<NewLine>    with torch.no_grad():<NewLine>        img_tensor = torch.rand((1, 1, 1000, 1000))<NewLine><NewLine>        dog = DifferenceOfGaussiansFFT(<NewLine>            img_height=1000,<NewLine>            img_width=1000,<NewLine>            sigma_bins=48 // size,<NewLine>            max_sigma=30,<NewLine>        ).to(rank, non_blocking=True)<NewLine>        for p in dog.parameters():<NewLine>            p.requires_grad = False<NewLine>        dog.eval()<NewLine>        torch.cuda.synchronize(rank)<NewLine><NewLine>        dogs = []<NewLine>        start = time.monotonic()<NewLine>        for i in range(10):<NewLine>            img_tensor = img_tensor.to(rank)<NewLine>            # torch.cuda.synchronize(rank)<NewLine>            dogs.append(dog(img_tensor))<NewLine>        end = time.monotonic()<NewLine>        print(end - start)<NewLine>        return dogs<NewLine><NewLine><NewLine>def init_process(rank_size_fn, backend=""nccl""):<NewLine>    rank, size, fn = rank_size_fn<NewLine>    """""" Initialize the distributed environment. """"""<NewLine>    os.environ[""MASTER_ADDR""] = ""127.0.0.1""<NewLine>    os.environ[""MASTER_PORT""] = ""29500""<NewLine>    dist.init_process_group(backend, rank=rank, world_size=size)<NewLine>    return fn(rank, size)<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    set_start_method(""spawn"")<NewLine><NewLine>    size = 2<NewLine>    pool = Pool(processes=size)<NewLine>    start = time.monotonic()<NewLine>    res = pool.map(init_process, [(i, size, run) for i in range(size)])<NewLine>    end = time.monotonic()<NewLine>    print(end - start)<NewLine>    pool.close()<NewLine><NewLine>    size = 3<NewLine>    pool = Pool(processes=size)<NewLine>    start = time.monotonic()<NewLine>    res = pool.map(init_process, [(i, size, run) for i in range(size)])<NewLine>    end = time.monotonic()<NewLine>    print(end - start)<NewLine>    pool.close()<NewLine><NewLine>    size = 4<NewLine>    pool = Pool(processes=size)<NewLine>    start = time.monotonic()<NewLine>    res = pool.map(init_process, [(i, size, run) for i in range(size)])<NewLine>    end = time.monotonic()<NewLine>    print(end - start)<NewLine>    pool.close()<NewLine>    # print(res)<NewLine><NewLine></code></pre><NewLine><p>thanks for helping me with this btw!</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>I conda installed <code>opt_einsum</code> but hit the following error. Is there a specific version of <code>opt_einsum</code> that I should use? The installed one is <code>opt_einsum-3.2.1</code>.</p><NewLine><pre><code class=""lang-python"">Traceback (most recent call last):<NewLine>  File ""/private/home/shenli/local/miniconda/envs/torchdev/lib/python3.8/multiprocessing/pool.py"", line 125, in worker<NewLine>    result = (True, func(*args, **kwds))<NewLine>  File ""/private/home/shenli/local/miniconda/envs/torchdev/lib/python3.8/multiprocessing/pool.py"", line 48, in mapstar<NewLine>    return list(map(*args))<NewLine>  File ""/scratch/shenli/pytorch/test.py"", line 229, in init_process<NewLine>    return fn(rank, size)<NewLine>  File ""/scratch/shenli/pytorch/test.py"", line 215, in run<NewLine>    dogs.append(dog(img_tensor))<NewLine>  File ""/scratch/shenli/pytorch/torch/nn/modules/module.py"", line 722, in _call_impl<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/scratch/shenli/pytorch/test.py"", line 89, in forward<NewLine>    f_gaussian_images = comp_mul(self.f_gaussian_pyramid, f_input)<NewLine>  File ""/scratch/shenli/pytorch/test.py"", line 185, in comp_mul<NewLine>    [op(x_real, y_real) - op(x_im, y_im), op(x_real, y_im) + op(x_im, y_real)],<NewLine>  File ""/private/home/shenli/local/miniconda/envs/torchdev/lib/python3.8/site-packages/opt_einsum/contract.py"", line 473, in contract<NewLine>    operands, contraction_list = contract_path(*operands,<NewLine>  File ""/private/home/shenli/local/miniconda/envs/torchdev/lib/python3.8/site-packages/opt_einsum/contract.py"", line 222, in contract_path<NewLine>    raise ValueError(""Einstein sum subscript '{}' does not contain the ""<NewLine>ValueError: Einstein sum subscript 'buv' does not contain the correct number of indices for operand 1.<NewLine><NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>it’s because i got the dimensions of <code>img_tensor</code> wrong. i guess it should be <code>(1,1000,1000)</code>.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>I only have two GPUs, so I tested size == 1 and size == 2 using CUDA events. It looks like the forward pass of 2 GPUs are actually faster? I attached the code I am running below:</p><NewLine><pre><code class=""lang-auto"">====== size = 1  ======<NewLine>Iteration 0 forward latency is 340.7067565917969<NewLine>Iteration 1 forward latency is 46.39555358886719<NewLine>Iteration 2 forward latency is 46.37984085083008<NewLine>Iteration 3 forward latency is 46.37712097167969<NewLine>Iteration 4 forward latency is 46.3746223449707<NewLine>Iteration 5 forward latency is 46.35868835449219<NewLine>Iteration 6 forward latency is 46.370174407958984<NewLine>Iteration 7 forward latency is 46.40425491333008<NewLine>Iteration 8 forward latency is 46.36265563964844<NewLine>Iteration 9 forward latency is 46.36454391479492<NewLine>end - start =  0.7640056293457747<NewLine>====== size = 2  ======<NewLine>Iteration 0 forward latency is 336.1044616699219<NewLine>Iteration 1 forward latency is 26.22003173828125<NewLine>Iteration 2 forward latency is 27.49286460876465<NewLine>Iteration 3 forward latency is 26.249248504638672<NewLine>Iteration 4 forward latency is 26.69696044921875<NewLine>Iteration 5 forward latency is 26.118335723876953<NewLine>Iteration 6 forward latency is 27.30339241027832<NewLine>Iteration 7 forward latency is 23.886367797851562<NewLine>Iteration 8 forward latency is 23.869632720947266<NewLine>Iteration 9 forward latency is 23.936511993408203<NewLine>end - start =  0.5738828824833035<NewLine>Iteration 0 forward latency is 312.13189697265625<NewLine>Iteration 1 forward latency is 24.0633602142334<NewLine>Iteration 2 forward latency is 23.685983657836914<NewLine>Iteration 3 forward latency is 23.70742416381836<NewLine>Iteration 4 forward latency is 23.703231811523438<NewLine>Iteration 5 forward latency is 23.78976058959961<NewLine>Iteration 6 forward latency is 23.779136657714844<NewLine>Iteration 7 forward latency is 23.787424087524414<NewLine>Iteration 8 forward latency is 23.791616439819336<NewLine>Iteration 9 forward latency is 23.80246353149414<NewLine>end - start =  2.9916703598573804<NewLine></code></pre><NewLine><pre><code class=""lang-python"">import math<NewLine>import numbers<NewLine>import os<NewLine>import time<NewLine>from functools import partial<NewLine>from typing import Tuple<NewLine><NewLine>import numpy as np<NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>from opt_einsum import contract<NewLine>from torch import nn<NewLine>from torch.multiprocessing import set_start_method, Pool<NewLine><NewLine>class DifferenceOfGaussiansFFT(nn.Module):<NewLine>    def __init__(<NewLine>            self,<NewLine>            *,<NewLine>            img_height: int,<NewLine>            img_width: int,<NewLine>            min_sigma: int = 1,<NewLine>            max_sigma: int = 10,<NewLine>            sigma_bins: int = 50,<NewLine>            truncate: float = 5.0,<NewLine>    ):<NewLine>        super(DifferenceOfGaussiansFFT, self).__init__()<NewLine>        self.img_height = img_height<NewLine>        self.img_width = img_width<NewLine>        self.signal_ndim = 2<NewLine><NewLine>        self.sigma_list = np.concatenate(<NewLine>            [<NewLine>                np.linspace(min_sigma, max_sigma, sigma_bins),<NewLine>                [max_sigma + (max_sigma - min_sigma) / (sigma_bins - 1)],<NewLine>            ]<NewLine>        )<NewLine>        sigmas = torch.from_numpy(self.sigma_list)<NewLine>        self.register_buffer(""sigmas"", sigmas)<NewLine>        # print(""gaussian pyramid sigmas: "", len(sigmas), sigmas)<NewLine><NewLine>        # accommodate largest filter<NewLine>        self.max_radius = int(truncate * max(sigmas) + 0.5)<NewLine>        max_bandwidth = 2 * self.max_radius + 1<NewLine>        # pad fft to prevent aliasing<NewLine>        padded_height = img_height + max_bandwidth - 1<NewLine>        padded_width = img_width + max_bandwidth - 1<NewLine>        # round up to next power of 2 for cheaper fft.<NewLine>        self.fft_height = 2 ** math.ceil(math.log2(padded_height))<NewLine>        self.fft_width = 2 ** math.ceil(math.log2(padded_width))<NewLine>        self.pad_input = nn.ConstantPad2d(<NewLine>            (0, self.fft_width - img_width, 0, self.fft_height - img_height), 0<NewLine>        )<NewLine><NewLine>        self.f_gaussian_pyramid = []<NewLine>        kernel_pad = nn.ConstantPad2d(<NewLine>            # left, right, top, bottom<NewLine>            (0, self.fft_width - max_bandwidth, 0, self.fft_height - max_bandwidth),<NewLine>            0,<NewLine>        )<NewLine>        for i, s in enumerate(sigmas):<NewLine>            radius = int(truncate * s + 0.5)<NewLine>            width = 2 * radius + 1<NewLine>            kernel = torch_gaussian_kernel(width=width, sigma=s.item())<NewLine><NewLine>            # this is to align all of the kernels so that the eventual fft shifts a fixed amount<NewLine>            center_pad_size = self.max_radius - radius<NewLine>            if center_pad_size &gt; 0:<NewLine>                centered_kernel = nn.ConstantPad2d(center_pad_size, 0)(kernel)<NewLine>            else:<NewLine>                centered_kernel = kernel<NewLine><NewLine>            padded_kernel = kernel_pad(centered_kernel)<NewLine><NewLine>            f_kernel = torch.rfft(<NewLine>                padded_kernel, signal_ndim=self.signal_ndim, onesided=True<NewLine>            )<NewLine>            self.f_gaussian_pyramid.append(f_kernel)<NewLine><NewLine>        self.f_gaussian_pyramid = nn.Parameter(<NewLine>            torch.stack(self.f_gaussian_pyramid, dim=0), requires_grad=False<NewLine>        )<NewLine><NewLine>    def forward(self, input: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:<NewLine>        img_height, img_width = list(input.size())[-self.signal_ndim:]<NewLine>        assert (img_height, img_width) == (self.img_height, self.img_width)<NewLine><NewLine>        padded_input = self.pad_input(input)<NewLine>        f_input = torch.rfft(padded_input, signal_ndim=self.signal_ndim, onesided=True)<NewLine>        f_gaussian_images = comp_mul(self.f_gaussian_pyramid, f_input)<NewLine>        gaussian_images = torch.irfft(<NewLine>            f_gaussian_images,<NewLine>            signal_ndim=self.signal_ndim,<NewLine>            onesided=True,<NewLine>            signal_sizes=padded_input.shape[1:],<NewLine>        )<NewLine><NewLine>        # fft induces a shift so needs to be undone<NewLine>        gaussian_images = gaussian_images[<NewLine>                          :,  # batch dimension<NewLine>                          :,  # filter dimension<NewLine>                          self.max_radius: self.img_height + self.max_radius,<NewLine>                          self.max_radius: self.img_width + self.max_radius,<NewLine>                          ]<NewLine><NewLine>        return gaussian_images<NewLine><NewLine><NewLine>def torch_gaussian_kernel(<NewLine>        width: int = 21, sigma: int = 3, dim: int = 2<NewLine>) -&gt; torch.Tensor:<NewLine>    """"""Gaussian kernel<NewLine><NewLine>    Parameters<NewLine>    ----------<NewLine>    width: bandwidth of the kernel<NewLine>    sigma: std of the kernel<NewLine>    dim: dimensions of the kernel (images -&gt; 2)<NewLine><NewLine>    Returns<NewLine>    -------<NewLine>    kernel : gaussian kernel<NewLine><NewLine>    """"""<NewLine><NewLine>    if isinstance(width, numbers.Number):<NewLine>        width = [width] * dim<NewLine>    if isinstance(sigma, numbers.Number):<NewLine>        sigma = [sigma] * dim<NewLine>    kernel = 1<NewLine>    meshgrids = torch.meshgrid(<NewLine>        [torch.arange(size, dtype=torch.float32) for size in width]<NewLine>    )<NewLine>    for size, std, mgrid in zip(width, sigma, meshgrids):<NewLine>        mean = (size - 1) / 2<NewLine>        kernel *= (<NewLine>                1<NewLine>                / (std * math.sqrt(2 * math.pi))<NewLine>                * torch.exp(-(((mgrid - mean) / std) ** 2) / 2)<NewLine>        )<NewLine><NewLine>    # Make sure sum of values in gaussian kernel equals 1.<NewLine>    kernel = kernel / torch.sum(kernel)<NewLine>    return kernel<NewLine><NewLine><NewLine>def chunks(lst, n):<NewLine>    """"""Yield successive n-sized chunks from lst.""""""<NewLine>    for i in range(0, len(lst), n):<NewLine>        yield lst[i: i + n]<NewLine><NewLine><NewLine>def comp_mul(x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:<NewLine>    """"""Complex multiplies two complex 3d tensors<NewLine><NewLine>    x = (x_real, x_im)<NewLine>    y = (y_real, y_im)<NewLine>    x*y = (x_real*y_real - x_im*y_im, x_real*y_im + x_im*y_real)<NewLine><NewLine>    Last dimension is x2 with x[..., 0] real and x[..., 1] complex.<NewLine>    Dimensions (-3,-2) must be equal of both a and b must be the same.<NewLine><NewLine>    Examples<NewLine>    ________<NewLine>    &gt;&gt;&gt; f_filters = torch.rand((20, 1024, 1024, 2))<NewLine>    &gt;&gt;&gt; f_imgs = torch.rand((5, 1024, 1024, 2))<NewLine>    &gt;&gt;&gt; f_filtered_imgs = comp_mul(f_filters, f_imgs)<NewLine><NewLine>    Parameters<NewLine>    ----------<NewLine>    x : Last dimension is (a,b) of a+ib<NewLine>    y : Last dimension is (a,b) of a+ib<NewLine><NewLine>    Returns<NewLine>    -------<NewLine>    z : x*y<NewLine><NewLine>    """"""<NewLine><NewLine>    # hadamard product of every filter against every batch image<NewLine>    op = partial(contract, ""fuv,buv-&gt;bfuv"")<NewLine>    assert x.shape[-1] == y.shape[-1] == 2<NewLine>    x_real, x_im = x.unbind(-1)<NewLine>    y_real, y_im = y.unbind(-1)<NewLine>    z = torch.stack(<NewLine>        [op(x_real, y_real) - op(x_im, y_im), op(x_real, y_im) + op(x_im, y_real)],<NewLine>        dim=-1,<NewLine>    )<NewLine>    return z<NewLine><NewLine><NewLine>def run(rank, size):<NewLine>    with torch.no_grad():<NewLine>        img_tensor = torch.rand((1, 1000, 1000))<NewLine><NewLine>        dog = DifferenceOfGaussiansFFT(<NewLine>            img_height=1000,<NewLine>            img_width=1000,<NewLine>            sigma_bins=48 // size,<NewLine>            max_sigma=30,<NewLine>        ).to(rank, non_blocking=True)<NewLine>        for p in dog.parameters():<NewLine>            p.requires_grad = False<NewLine>        dog.eval()<NewLine>        torch.cuda.synchronize(rank)<NewLine><NewLine>        dogs = []<NewLine>        start = time.monotonic()<NewLine>        s = torch.cuda.current_stream(rank)<NewLine>        e_start = torch.cuda.Event(enable_timing=True)<NewLine>        e_finish = torch.cuda.Event(enable_timing=True)<NewLine>        for i in range(10):<NewLine>            img_tensor = img_tensor.to(rank)<NewLine>            # torch.cuda.synchronize(rank)<NewLine>            s.record_event(e_start)<NewLine>            dogs.append(dog(img_tensor))<NewLine>            s.record_event(e_finish)<NewLine>            e_finish.synchronize()<NewLine>            print(f""Iteration {i} forward latency is {e_start.elapsed_time(e_finish)}"")<NewLine>        end = time.monotonic()<NewLine>        print(""end - start = "", end - start)<NewLine>        torch.cuda.synchronize(rank)<NewLine>        return dogs<NewLine><NewLine><NewLine>def init_process(rank_size_fn, backend=""nccl""):<NewLine>    rank, size, fn = rank_size_fn<NewLine>    """""" Initialize the distributed environment. """"""<NewLine>    os.environ[""MASTER_ADDR""] = ""127.0.0.1""<NewLine>    os.environ[""MASTER_PORT""] = ""29500""<NewLine>    dist.init_process_group(backend, rank=rank, world_size=size)<NewLine>    return fn(rank, size)<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    set_start_method(""spawn"")<NewLine><NewLine>    size = 1<NewLine>    print(""====== size = 1  ======"")<NewLine>    pool = Pool(processes=size)<NewLine>    start = time.monotonic()<NewLine>    res = pool.map(init_process, [(i, size, run) for i in range(size)])<NewLine>    end = time.monotonic()<NewLine>    #print(end - start)<NewLine>    pool.close()<NewLine><NewLine>    print(""====== size = 2  ======"")<NewLine><NewLine>    size = 2<NewLine>    pool = Pool(processes=size)<NewLine>    start = time.monotonic()<NewLine>    res = pool.map(init_process, [(i, size, run) for i in range(size)])<NewLine>    end = time.monotonic()<NewLine>    #print(end - start)<NewLine>    pool.close()<NewLine>    # print(res)<NewLine>import math<NewLine>import numbers<NewLine>import os<NewLine>import time<NewLine>from functools import partial<NewLine>from typing import Tuple<NewLine><NewLine>import numpy as np<NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>from opt_einsum import contract<NewLine>from torch import nn<NewLine>from torch.multiprocessing import set_start_method, Pool<NewLine><NewLine>class DifferenceOfGaussiansFFT(nn.Module):<NewLine>    def __init__(<NewLine>            self,<NewLine>            *,<NewLine>            img_height: int,<NewLine>            img_width: int,<NewLine>            min_sigma: int = 1,<NewLine>            max_sigma: int = 10,<NewLine>            sigma_bins: int = 50,<NewLine>            truncate: float = 5.0,<NewLine>    ):<NewLine>        super(DifferenceOfGaussiansFFT, self).__init__()<NewLine>        self.img_height = img_height<NewLine>        self.img_width = img_width<NewLine>        self.signal_ndim = 2<NewLine><NewLine>        self.sigma_list = np.concatenate(<NewLine>            [<NewLine>                np.linspace(min_sigma, max_sigma, sigma_bins),<NewLine>                [max_sigma + (max_sigma - min_sigma) / (sigma_bins - 1)],<NewLine>            ]<NewLine>        )<NewLine>        sigmas = torch.from_numpy(self.sigma_list)<NewLine>        self.register_buffer(""sigmas"", sigmas)<NewLine>        # print(""gaussian pyramid sigmas: "", len(sigmas), sigmas)<NewLine><NewLine>        # accommodate largest filter<NewLine>        self.max_radius = int(truncate * max(sigmas) + 0.5)<NewLine>        max_bandwidth = 2 * self.max_radius + 1<NewLine>        # pad fft to prevent aliasing<NewLine>        padded_height = img_height + max_bandwidth - 1<NewLine>        padded_width = img_width + max_bandwidth - 1<NewLine>        # round up to next power of 2 for cheaper fft.<NewLine>        self.fft_height = 2 ** math.ceil(math.log2(padded_height))<NewLine>        self.fft_width = 2 ** math.ceil(math.log2(padded_width))<NewLine>        self.pad_input = nn.ConstantPad2d(<NewLine>            (0, self.fft_width - img_width, 0, self.fft_height - img_height), 0<NewLine>        )<NewLine><NewLine>        self.f_gaussian_pyramid = []<NewLine>        kernel_pad = nn.ConstantPad2d(<NewLine>            # left, right, top, bottom<NewLine>            (0, self.fft_width - max_bandwidth, 0, self.fft_height - max_bandwidth),<NewLine>            0,<NewLine>        )<NewLine>        for i, s in enumerate(sigmas):<NewLine>            radius = int(truncate * s + 0.5)<NewLine>            width = 2 * radius + 1<NewLine>            kernel = torch_gaussian_kernel(width=width, sigma=s.item())<NewLine><NewLine>            # this is to align all of the kernels so that the eventual fft shifts a fixed amount<NewLine>            center_pad_size = self.max_radius - radius<NewLine>            if center_pad_size &gt; 0:<NewLine>                centered_kernel = nn.ConstantPad2d(center_pad_size, 0)(kernel)<NewLine>            else:<NewLine>                centered_kernel = kernel<NewLine><NewLine>            padded_kernel = kernel_pad(centered_kernel)<NewLine><NewLine>            f_kernel = torch.rfft(<NewLine>                padded_kernel, signal_ndim=self.signal_ndim, onesided=True<NewLine>            )<NewLine>            self.f_gaussian_pyramid.append(f_kernel)<NewLine><NewLine>        self.f_gaussian_pyramid = nn.Parameter(<NewLine>            torch.stack(self.f_gaussian_pyramid, dim=0), requires_grad=False<NewLine>        )<NewLine><NewLine>    def forward(self, input: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:<NewLine>        img_height, img_width = list(input.size())[-self.signal_ndim:]<NewLine>        assert (img_height, img_width) == (self.img_height, self.img_width)<NewLine><NewLine>        padded_input = self.pad_input(input)<NewLine>        f_input = torch.rfft(padded_input, signal_ndim=self.signal_ndim, onesided=True)<NewLine>        f_gaussian_images = comp_mul(self.f_gaussian_pyramid, f_input)<NewLine>        gaussian_images = torch.irfft(<NewLine>            f_gaussian_images,<NewLine>            signal_ndim=self.signal_ndim,<NewLine>            onesided=True,<NewLine>            signal_sizes=padded_input.shape[1:],<NewLine>        )<NewLine><NewLine>        # fft induces a shift so needs to be undone<NewLine>        gaussian_images = gaussian_images[<NewLine>                          :,  # batch dimension<NewLine>                          :,  # filter dimension<NewLine>                          self.max_radius: self.img_height + self.max_radius,<NewLine>                          self.max_radius: self.img_width + self.max_radius,<NewLine>                          ]<NewLine><NewLine>        return gaussian_images<NewLine><NewLine><NewLine>def torch_gaussian_kernel(<NewLine>        width: int = 21, sigma: int = 3, dim: int = 2<NewLine>) -&gt; torch.Tensor:<NewLine>    """"""Gaussian kernel<NewLine><NewLine>    Parameters<NewLine>    ----------<NewLine>    width: bandwidth of the kernel<NewLine>    sigma: std of the kernel<NewLine>    dim: dimensions of the kernel (images -&gt; 2)<NewLine><NewLine>    Returns<NewLine>    -------<NewLine>    kernel : gaussian kernel<NewLine><NewLine>    """"""<NewLine><NewLine>    if isinstance(width, numbers.Number):<NewLine>        width = [width] * dim<NewLine>    if isinstance(sigma, numbers.Number):<NewLine>        sigma = [sigma] * dim<NewLine>    kernel = 1<NewLine>    meshgrids = torch.meshgrid(<NewLine>        [torch.arange(size, dtype=torch.float32) for size in width]<NewLine>    )<NewLine>    for size, std, mgrid in zip(width, sigma, meshgrids):<NewLine>        mean = (size - 1) / 2<NewLine>        kernel *= (<NewLine>                1<NewLine>                / (std * math.sqrt(2 * math.pi))<NewLine>                * torch.exp(-(((mgrid - mean) / std) ** 2) / 2)<NewLine>        )<NewLine><NewLine>    # Make sure sum of values in gaussian kernel equals 1.<NewLine>    kernel = kernel / torch.sum(kernel)<NewLine>    return kernel<NewLine><NewLine><NewLine>def chunks(lst, n):<NewLine>    """"""Yield successive n-sized chunks from lst.""""""<NewLine>    for i in range(0, len(lst), n):<NewLine>        yield lst[i: i + n]<NewLine><NewLine><NewLine>def comp_mul(x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:<NewLine>    """"""Complex multiplies two complex 3d tensors<NewLine><NewLine>    x = (x_real, x_im)<NewLine>    y = (y_real, y_im)<NewLine>    x*y = (x_real*y_real - x_im*y_im, x_real*y_im + x_im*y_real)<NewLine><NewLine>    Last dimension is x2 with x[..., 0] real and x[..., 1] complex.<NewLine>    Dimensions (-3,-2) must be equal of both a and b must be the same.<NewLine><NewLine>    Examples<NewLine>    ________<NewLine>    &gt;&gt;&gt; f_filters = torch.rand((20, 1024, 1024, 2))<NewLine>    &gt;&gt;&gt; f_imgs = torch.rand((5, 1024, 1024, 2))<NewLine>    &gt;&gt;&gt; f_filtered_imgs = comp_mul(f_filters, f_imgs)<NewLine><NewLine>    Parameters<NewLine>    ----------<NewLine>    x : Last dimension is (a,b) of a+ib<NewLine>    y : Last dimension is (a,b) of a+ib<NewLine><NewLine>    Returns<NewLine>    -------<NewLine>    z : x*y<NewLine><NewLine>    """"""<NewLine><NewLine>    # hadamard product of every filter against every batch image<NewLine>    op = partial(contract, ""fuv,buv-&gt;bfuv"")<NewLine>    assert x.shape[-1] == y.shape[-1] == 2<NewLine>    x_real, x_im = x.unbind(-1)<NewLine>    y_real, y_im = y.unbind(-1)<NewLine>    z = torch.stack(<NewLine>        [op(x_real, y_real) - op(x_im, y_im), op(x_real, y_im) + op(x_im, y_real)],<NewLine>        dim=-1,<NewLine>    )<NewLine>    return z<NewLine><NewLine><NewLine>def run(rank, size):<NewLine>    with torch.no_grad():<NewLine>        img_tensor = torch.rand((1, 1000, 1000))<NewLine><NewLine>        dog = DifferenceOfGaussiansFFT(<NewLine>            img_height=1000,<NewLine>            img_width=1000,<NewLine>            sigma_bins=48 // size,<NewLine>            max_sigma=30,<NewLine>        ).to(rank, non_blocking=True)<NewLine>        for p in dog.parameters():<NewLine>            p.requires_grad = False<NewLine>        dog.eval()<NewLine>        torch.cuda.synchronize(rank)<NewLine><NewLine>        dogs = []<NewLine>        start = time.monotonic()<NewLine>        s = torch.cuda.current_stream(rank)<NewLine>        e_start = torch.cuda.Event(enable_timing=True)<NewLine>        e_finish = torch.cuda.Event(enable_timing=True)<NewLine>        for i in range(10):<NewLine>            img_tensor = img_tensor.to(rank)<NewLine>            # torch.cuda.synchronize(rank)<NewLine>            s.record_event(e_start)<NewLine>            dogs.append(dog(img_tensor))<NewLine>            s.record_event(e_finish)<NewLine>            e_finish.synchronize()<NewLine>            print(f""Iteration {i} forward latency is {e_start.elapsed_time(e_finish)}"")<NewLine>        end = time.monotonic()<NewLine>        print(""end - start = "", end - start)<NewLine>        torch.cuda.synchronize(rank)<NewLine>        return dogs<NewLine><NewLine><NewLine>def init_process(rank_size_fn, backend=""nccl""):<NewLine>    rank, size, fn = rank_size_fn<NewLine>    """""" Initialize the distributed environment. """"""<NewLine>    os.environ[""MASTER_ADDR""] = ""127.0.0.1""<NewLine>    os.environ[""MASTER_PORT""] = ""29500""<NewLine>    dist.init_process_group(backend, rank=rank, world_size=size)<NewLine>    return fn(rank, size)<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    set_start_method(""spawn"")<NewLine><NewLine>    size = 1<NewLine>    print(""====== size = 1  ======"")<NewLine>    pool = Pool(processes=size)<NewLine>    start = time.monotonic()<NewLine>    res = pool.map(init_process, [(i, size, run) for i in range(size)])<NewLine>    end = time.monotonic()<NewLine>    #print(end - start)<NewLine>    pool.close()<NewLine><NewLine>    print(""====== size = 2  ======"")<NewLine><NewLine>    size = 2<NewLine>    pool = Pool(processes=size)<NewLine>    start = time.monotonic()<NewLine>    res = pool.map(init_process, [(i, size, run) for i in range(size)])<NewLine>    end = time.monotonic()<NewLine>    #print(end - start)<NewLine>    pool.close()<NewLine>    # print(res)<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> okay thanks I’ll try this. But also this means all of my other timings have been wrong. So thanks for showing me how to use cuda events too.</p><NewLine><p>edit:</p><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> it turns out you need a synchronize after all</p><NewLine><pre><code class=""lang-python"">        start = time.monotonic()<NewLine>        s = torch.cuda.current_stream(rank)<NewLine>        e_start = torch.cuda.Event(enable_timing=True)<NewLine>        e_finish = torch.cuda.Event(enable_timing=True)<NewLine>        s.record_event(e_start)<NewLine>        for i in range(10):<NewLine>            img_tensor = img_tensor_cpu.to(rank)<NewLine>            # torch.cuda.synchronize(rank)<NewLine>            dogs.append(dog(img_tensor))<NewLine>        torch.cuda.synchronize(rank)<NewLine>        s.record_event(e_finish)<NewLine>        e_finish.synchronize()<NewLine>        end = time.monotonic()<NewLine></code></pre><NewLine><p>gives me this</p><NewLine><pre><code class=""lang-auto"">====== size = 1  ======<NewLine><NewLine>rank 0 Iteration 9 forward latency is 1283.0596923828125<NewLine>end - start =  1.2832060940563679<NewLine><NewLine><NewLine>====== size = 2  ======<NewLine><NewLine>rank 0 Iteration 9 forward latency is 626.5835571289062<NewLine>end - start =  0.6267032357864082<NewLine>rank 1 Iteration 9 forward latency is 640.3717041015625<NewLine>end - start =  0.6404897100292146<NewLine><NewLine><NewLine>====== size = 3  ======<NewLine><NewLine>rank 0 Iteration 9 forward latency is 443.1278076171875<NewLine>end - start =  0.44322703895159066<NewLine>rank 1 Iteration 9 forward latency is 471.8766174316406<NewLine>end - start =  0.47198665188625455<NewLine>rank 2 Iteration 9 forward latency is 461.29559326171875<NewLine>end - start =  0.46140363393351436<NewLine><NewLine><NewLine>====== size = 4  ======<NewLine><NewLine>rank 0 Iteration 9 forward latency is 397.9264221191406<NewLine>end - start =  0.3981346560176462<NewLine>rank 2 Iteration 9 forward latency is 374.9112243652344<NewLine>end - start =  0.3749916541855782<NewLine>rank 3 Iteration 9 forward latency is 360.9978942871094<NewLine>end - start =  0.3610941809602082<NewLine>rank 1 Iteration 9 forward latency is 362.57073974609375<NewLine>end - start =  0.3626508240122348<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> i got everything working. thanks for your help!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/makslevental; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/makslevental; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/makslevental; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/makslevental; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/makslevental; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/makslevental; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/makslevental; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/makslevental; <NewLine> ,"REPLY_DATE 1: June 22, 2020,  4:18pm; <NewLine> REPLY_DATE 2: June 22, 2020,  4:26pm; <NewLine> REPLY_DATE 3: June 22, 2020,  6:10pm; <NewLine> REPLY_DATE 4: June 22, 2020,  6:16pm; <NewLine> REPLY_DATE 5: June 22, 2020,  6:26pm; <NewLine> REPLY_DATE 6: June 22, 2020,  6:35pm; <NewLine> REPLY_DATE 7: June 22, 2020,  6:47pm; <NewLine> REPLY_DATE 8: June 22, 2020,  7:58pm; <NewLine> REPLY_DATE 9: June 22, 2020,  9:02pm; <NewLine> REPLY_DATE 10: June 22, 2020,  9:15pm; <NewLine> REPLY_DATE 11: June 22, 2020,  9:30pm; <NewLine> REPLY_DATE 12: June 22, 2020,  9:31pm; <NewLine> REPLY_DATE 13: June 22, 2020,  9:42pm; <NewLine> REPLY_DATE 14: June 23, 2020, 12:24am; <NewLine> REPLY_DATE 15: June 23, 2020,  1:15pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: 1 Like; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: 1 Like; <NewLine> REPLY 15 LIKES: ; <NewLine> 
86473,DataParallel Reshape Problems,2020-06-22T22:55:29.739Z,0,105,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a model that I would like to parallelize so that inference would be much faster. The input vector that I have to this network is at one point 195xHxW. My network then reshapes it -1x3xHxW, which should normally work (65x3xHxW). But because DataParallel wraps the network it divides the 195xHxW tensor into n pieces, where n is the amount of gpus. However, when dividing the tensor it does it in such a way that the last tensor can no longer be reshaped. Is there a way to get DataParallel to work with the model so that the tensor can still be properly reshaped?</p><NewLine></div>",https://discuss.pytorch.org/u/WR01,,WR01,"June 22, 2020, 10:55pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m assuming that since your tensor of shape 195<em>H</em>W is a single training example, you don’t want it to be split by N GPUs, since 195 is not the batch size in this case? If you pass in your training examples in the form such as batch_size * C * H * W for example then DataParallel/DDP should divide along the batch size dim.</p><NewLine><p>Could you potentially paste a reproduction of this issue and the associated error message that you get?<br/><NewLine>Btw, if you are using utilities provided by PyTorch such as the <a href=""https://pytorch.org/docs/stable/data.html"" rel=""nofollow noopener"">DataLoader</a>  you can configure the <code>drop_last</code> argument to ensure that batch sizes are even.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/rvarm1; <NewLine> ,"REPLY_DATE 1: June 23, 2020,  8:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
33984,Torch.nn.parallel.data_parallel for distributed training: backward pass model update,2019-01-07T05:31:09.802Z,4,717,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When using torch.nn.parallel.data_parallel for distributed training, models are copied onto multiple GPU’s and can complete a forward pass without the copied models effecting each other. However, how do the copied models interact during the backward pass? How are the model weights updated on each GPU?</p><NewLine><p>When reading the documentation [1] I see the explanation:<br/><NewLine>“gradients from each replica are summed into the original module”</p><NewLine><p>but I’m not sure how to interpret this statement. My best guess is that each GPU’s model is updated with the sum of all GPU’s model’s gradients, which I would then interpret that there is locking across GPU’s so they each start training on a new mini-batch only after they all finish processing their current mini-batch.</p><NewLine><p>[1] <a href=""https://pytorch.org/docs/master/nn.html?highlight=dataparallel#dataparallel-layers-multi-gpu-distributed"" rel=""nofollow noopener"">https://pytorch.org/docs/master/nn.html?highlight=dataparallel#dataparallel-layers-multi-gpu-distributed</a></p><NewLine></div>",https://discuss.pytorch.org/u/hotcheetos_puff,(Edgar Barraza),hotcheetos_puff,"January 7, 2019,  5:31am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Each gpu compute the gradients for it’s part of the batch. Then they are accumulated on the “main” model where the weight update is done. Then this “main” model shares it’s weight to all the other gpus so that all models have the same weight.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Alban,</p><NewLine><p>Thanks for clarifying.  Does this mean that this parallelization utilizes locking, ensuring that each GPU model updates its weights from the “main” model before moving on to the next mini-batch?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes the locking is builtin and the weights will properly be updated before they are used.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, I am facing somme issue with data parallel. I am training on 4 v100 with a batchsize of 1. The time of forward pass seems to scale but the time of backward pass is taking 4* times in comparison to 1 V100. So there is no significant boost in speed when using 4 gpus. I guess the backward pass is taking place on single gpu. I am using nn.parallel.DataParallel, is there any solution to this problem? I can share more details if you want.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/sanchit2843"">@sanchit2843</a></p><NewLine><p>When using <code>batch_size == 1</code>, <code>DataParallel</code> won’t be able to parallelize the computation, as the input cannot be chunked by the <code>scatter</code> linked below.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/df8d6eeb19423848b20cd727bc4a728337b73829/torch/nn/parallel/data_parallel.py#L151"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/df8d6eeb19423848b20cd727bc4a728337b73829/torch/nn/parallel/data_parallel.py#L151"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/df8d6eeb19423848b20cd727bc4a728337b73829/torch/nn/parallel/data_parallel.py#L151</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""141"" style=""counter-reset: li-counter 140 ;""><NewLine><li>def forward(self, *inputs, **kwargs):</li><NewLine><li>    if not self.device_ids:</li><NewLine><li>        return self.module(*inputs, **kwargs)</li><NewLine><li><NewLine></li><li>    for t in chain(self.module.parameters(), self.module.buffers()):</li><NewLine><li>        if t.device != self.src_device_obj:</li><NewLine><li>            raise RuntimeError(""module must have its parameters and buffers ""</li><NewLine><li>                               ""on device {} (device_ids[0]) but found one of ""</li><NewLine><li>                               ""them on device: {}"".format(self.src_device_obj, t.device))</li><NewLine><li><NewLine></li><li class=""selected"">    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)</li><NewLine><li>    if len(self.device_ids) == 1:</li><NewLine><li>        return self.module(*inputs[0], **kwargs[0])</li><NewLine><li>    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])</li><NewLine><li>    outputs = self.parallel_apply(replicas, inputs, kwargs)</li><NewLine><li>    return self.gather(outputs, self.output_device)</li><NewLine><li><NewLine></li><li>def replicate(self, module, device_ids):</li><NewLine><li>    return replicate(module, device_ids, not torch.is_grad_enabled())</li><NewLine><li><NewLine></li><li>def scatter(self, inputs, kwargs, device_ids):</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><blockquote><NewLine><p>but the time of backward pass is taking 4* times in comparison to 1 V100</p><NewLine></blockquote><NewLine><p>Can you share a minimum repro of this? In general, the 4X slowdown is possible due to Python GIL contention (which you can avoid by using <a href=""https://pytorch.org/docs/master/notes/ddp.html"" rel=""nofollow noopener""><code>DistributedDataParallel</code></a>). But I feel it this should <strong>not</strong> be applicable here as  each batch only contains one sample, and the replicated models are not really involved in forward and backward.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey thanks for the answer, actually I think it is because of all 4 backpropogation going with one gpu only. Is there any solution with which I can run it with one batch size. And what is the meaning of repro? Thanks in advance.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried with batchsize of 2 as well. The epoch time with single gpu is 2.5 hours and with four gpu is 3.3 hrs. Any solution will be helpful.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hotcheetos_puff; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/sanchit2843; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/sanchit2843; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/sanchit2843; <NewLine> ,"REPLY_DATE 1: January 7, 2019,  9:10am; <NewLine> REPLY_DATE 2: January 7, 2019,  9:37am; <NewLine> REPLY_DATE 3: January 9, 2019,  9:40pm; <NewLine> REPLY_DATE 4: June 22, 2020,  4:29pm; <NewLine> REPLY_DATE 5: June 22, 2020,  6:18pm; <NewLine> REPLY_DATE 6: June 22, 2020,  7:28pm; <NewLine> REPLY_DATE 7: June 22, 2020,  7:49pm; <NewLine> ",REPLY 1 LIKES: 3 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 3 Likes; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
85945,Why pytorch needs so much memory to execute distributed training?,2020-06-18T15:15:03.901Z,13,317,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Today I want to do distributed training in many nodes, however the memory decrease rapidly.</p><NewLine><p><img alt=""image"" data-base62-sha1=""gTha55U4DvSwfMRL7IRdLtPS1uW"" height=""280"" src=""https://discuss.pytorch.org/uploads/default/original/3X/7/6/76626668b2bf3663b47e7dca2f9db6a8556818a2.png"" width=""526""/></p><NewLine><p>I execute the command 'dstat -c -m '.We can see the available memory decrease from 64GB to 177MB.</p><NewLine><p>My batch_size is 1 and datasets include 4000 samples,every sample is about 16MB.I use the gloo as the backend.<br/><NewLine>data_loader code like this</p><NewLine><pre><code class=""lang-auto"">class HDF5Dataset(data.Dataset):<NewLine>    """"""Represents an abstract HDF5 dataset.<NewLine>    <NewLine>    Input params:<NewLine>        file_path: Path to the folder containing the dataset (one or multiple HDF5 files).<NewLine>        recursive: If True, searches for h5 files in subdirectories.<NewLine>        transform: PyTorch transform to apply to every data instance (default=None).<NewLine>        divide: use only 1//divide dataset<NewLine>    """"""<NewLine>    def __init__(self, file_path, recursive, transform=None, divide =1):<NewLine>        super().__init__()<NewLine>        self.data_info = []<NewLine>        self.data_cache = {} #dict<NewLine>        self.transform = transform<NewLine>        self.length = 0<NewLine>        self.divide = divide<NewLine><NewLine>        # Search for all h5 files<NewLine>        p = Path(file_path)<NewLine>        assert(p.is_dir())<NewLine>        <NewLine>        <NewLine>        if recursive:<NewLine>            files = sorted(p.glob('**/*.hdf5'))<NewLine>        else:<NewLine>            files = sorted(p.glob('*.hdf5'))<NewLine>        if len(files) &lt; 1:<NewLine>            raise RuntimeError('No hdf5 datasets found')<NewLine>        <NewLine>        self.length = len(files) // divide<NewLine>        self.data_info = files[0: self.length]<NewLine>               <NewLine>        #print(""File Prepared !\n"")<NewLine>            <NewLine>    def __getitem__(self, index):<NewLine>        # get data<NewLine>        y, x = self.get_data(index)<NewLine>        <NewLine>        if self.transform:<NewLine>            x = self.transform(x)<NewLine>        else:<NewLine>            x = torch.from_numpy(x)<NewLine>            x = torch.reshape(x, (1,128,128,128))<NewLine>            x = x.type(torch.FloatTensor)<NewLine><NewLine>        # get label<NewLine>        y = torch.from_numpy(y)<NewLine>        y = y.type(torch.FloatTensor)<NewLine>        return (x, y)<NewLine><NewLine>    def get_data(self, i):<NewLine>        fp = self.data_info[i] #list - dict<NewLine>        #print(fp)<NewLine>        <NewLine>        try:<NewLine>            with h5py.File(fp,'r') as h5_file:<NewLine>        <NewLine>                label = h5_file.get('label')[()]            <NewLine>                dataset = h5_file.get('dataset')[()]<NewLine>            <NewLine>                return (label, dataset)<NewLine>        except IOError:<NewLine>            print('Error:File {filepath} is broken.'.format(filepath=fp))<NewLine>            i=i+1<NewLine>            return self.get_data(i)<NewLine>        <NewLine>        <NewLine>    def __len__(self):<NewLine>        return self.length<NewLine></code></pre><NewLine><p>I want to know why pytorch DDP training need more memory with the nodes expand. And how to reduce memory usage？</p><NewLine></div>",https://discuss.pytorch.org/u/khalil,(khalil li),khalil,"June 19, 2020,  7:17am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What is the <code>world_size</code> in your application? Each process will have its own model replica, so the total memory consumption is expected to be larger than <code>world_size</code> X (non-DDP memory footprint). Besides, DDP also creates buffers for communication, which will contribute another ~1.5X if not considering optimizer.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your answer.I set the parameter nproc_per_node to 1 and I guess the world_size is 1.There is only one process per node.However, it still consumes a lot of memory. DDP adopts the data parallelism, I think when I train this model in more nodes, there is little memory to be consumed.<br/><NewLine>I found that the node cache always uses more than 20GB of memory and if I can not reduce the memory usage,I should choose another framework to do distributed training = = !</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/khalil"">@khalil</a></p><NewLine><p>How large is your model?</p><NewLine><blockquote><NewLine><p>My batch_size is 1 and datasets include 4000 samples,every sample is about 16MB.</p><NewLine></blockquote><NewLine><p>Does this actually mean the memory is consumed by the data loader? Can you check the memory consumption when using the same model and same data loader <strong>without</strong> DDP?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/vincentqb"">@vincentqb</a> <a class=""mention"" href=""/u/alband"">@albanD</a> for dataloder questions</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you,<a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>. I have trained this model in one node and it consumed 30GB on the cache.Just like this，<br/><NewLine><img alt=""image"" data-base62-sha1=""iJiYGkyd6ds9Ib1KxGWfayvYRPV"" height=""43"" src=""https://discuss.pytorch.org/uploads/default/original/3X/8/3/83463676db34a618fb87c6b003f5627d023bdd37.png"" width=""286""/> .<br/><NewLine><img alt=""image"" data-base62-sha1=""t5cvLFFW77anWI92DDzYci3ABTz"" height=""132"" src=""https://discuss.pytorch.org/uploads/default/original/3X/c/b/cbd593a08180fde0f1065621564743696a6b07d9.png"" width=""286""/></p><NewLine><p>I guess there are so many data to be loaded.Could you help me check my data_loader code?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/khalil"">@khalil</a></p><NewLine><p>I am trying to identify which component hogs memory. Could you please share some more details about the sentence below.</p><NewLine><blockquote><NewLine><p>I have trained this model in one node and it consumed 30GB on the cache</p><NewLine></blockquote><NewLine><p>By “trained this model in one node”, do you still use DDP and the same data loader? It will be helpful to know the memory footprint of the following cases:</p><NewLine><ol><NewLine><li>Train a local model <strong>without</strong> data loader and <strong>without</strong> DDP. Just feed the model with some randomly generated tensors.</li><NewLine><li>Train a local model <strong>with</strong> data loader but without DDP.</li><NewLine><li>Wrap the model with DDP, and train it with data loader.</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you,<a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,I will try it.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have created a small dataset which only contains 30 samples.However the memory still decreased rapidly.(batch_size=1)</p><NewLine><p><img alt=""image"" data-base62-sha1=""vsPxjOH7Cc1Zyhkg3myC6QSC1HU"" height=""151"" src=""https://discuss.pytorch.org/uploads/default/original/3X/d/c/dc85b85614e1c8dc78df1f16e1a6c9119b4d37d6.png"" width=""424""/></p><NewLine><p>So I think this issue is not caused by dataloader and I am sure reason is the node expansion.</p><NewLine><p>When I do DDP training in one node,the memory problem is not serious.When I expand the nodes,the memory problem is serious.I want to know if this is pytorch’s problem?</p><NewLine><p>My model like this<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/1ce6a5d7b981a495c1044f4c6bfcbcec8914f8d9"" href=""https://discuss.pytorch.org/uploads/default/original/3X/1/c/1ce6a5d7b981a495c1044f4c6bfcbcec8914f8d9.png"" title=""image""><img alt=""image"" data-base62-sha1=""47FwQL5zC3p4GlZ8sZ3YqU1BJjz"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/1/c/1ce6a5d7b981a495c1044f4c6bfcbcec8914f8d9_2_10x10.png"" height=""250"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/1/c/1ce6a5d7b981a495c1044f4c6bfcbcec8914f8d9_2_690x250.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/1/c/1ce6a5d7b981a495c1044f4c6bfcbcec8914f8d9_2_690x250.png, https://discuss.pytorch.org/uploads/default/optimized/3X/1/c/1ce6a5d7b981a495c1044f4c6bfcbcec8914f8d9_2_1035x375.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/1/c/1ce6a5d7b981a495c1044f4c6bfcbcec8914f8d9.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1139×413 72.2 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/khalil"">@khalil</a></p><NewLine><p>Could you please provide details of the following question?</p><NewLine><ol start=""0""><NewLine><li><NewLine><p>Which version of PyTorch are you using?</p><NewLine></li><NewLine><li><NewLine><p>What’s the size of your model (how many GB)? You can get this by doing the following:</p><NewLine><pre><code class=""lang-python"">model_size = 0<NewLine>for p in model.parameters():<NewLine>    model_size += p.numel() * p.element_size()<NewLine></code></pre><NewLine><p>Given the picture shown in the previous post, it looks like the model is about 27GB? If that is the case, then, yes, DDP would use another 27GB as comm buffers. We are working on improving this: <a href=""https://github.com/pytorch/pytorch/issues/39022"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/39022</a></p><NewLine><p>I am curious how did you train this model locally without DDP? If the model is 27GB, after the backward pass, the grads will also consume 27GB. So local training without DDP will also use &gt;54GB memory?</p><NewLine><p>And if your optimizer uses any momentum etc., it’s likely the optimizer will also consume a few more X of 27GB. But looks like there are only 64GB memory available? Is it because your optimizer does not contain any states?</p><NewLine></li><NewLine><li><NewLine><p>Does this getting worse when you use more nodes (e.g., scale from 2 nodes to 3 nodes)?</p><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,Thanks for your general help.The details are as follows:</p><NewLine><ol start=""0""><NewLine><li><NewLine><p>The version of PyTorch is 1.5.0</p><NewLine></li><NewLine><li><NewLine><p>I use your code to test the size of my model and it printed this sentence: model_size is 9865484.So I think the size of model are 10MB…</p><NewLine></li><NewLine><li><NewLine><p>When I scaled from 2 nodes to 4 nodes,the available memory started to decrease.The memory usage went up with the number of nodes increased.</p><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I use your code to test the size of my model and it printed this sentence: model_size is 9865484.So I think the size of model are 10MB</p><NewLine></blockquote><NewLine><p>In this case DDP should only consume 10MB more memory for communication buffers.</p><NewLine><p>BTW, as the model is just 10MB, do you know why even without DDP it consumes 30GB? And have you seen OOM errors?</p><NewLine><blockquote><NewLine><p>When I scaled from 2 nodes to 4 nodes, the available memory started to decrease.The memory usage went up with the number of nodes increased.</p><NewLine></blockquote><NewLine><p>This is also not expected with DDP. DDP’s memory footprint should be constant, regardless of how many nodes are used.</p><NewLine><p>Do you have a minimum repro that we can investigate?</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Dear <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,forgive me for not being clear,I said it consumes 30GB which means the cache<br/><NewLine>consumes 30GB.I guess the memory is used to load the disk data.I tried to use a small datasets<br/><NewLine>which only contains 40 samples(Every sample is 16MB).And in this case,the cache consumption is about 2.5GB.However,the memory problem still exist.You can see this picture:<br/><NewLine><img alt=""image"" data-base62-sha1=""vsPxjOH7Cc1Zyhkg3myC6QSC1HU"" height=""151"" src=""https://discuss.pytorch.org/uploads/default/original/3X/d/c/dc85b85614e1c8dc78df1f16e1a6c9119b4d37d6.png"" width=""424""/></p><NewLine><p>Now I think the problem is not caused by dataloader.</p><NewLine><p>And you say DDP’s memory footprint should be constant,regardless of how many nodes are used.I test the maximum memory footprint as the nodes expand,it just like this<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/20bcaa1469b6cd976ebbba3850b544ed7a05958a"" href=""https://discuss.pytorch.org/uploads/default/original/3X/2/0/20bcaa1469b6cd976ebbba3850b544ed7a05958a.png"" title=""node-memory""><img alt=""node-memory"" data-base62-sha1=""4FBupUr3Hig4dyHyFcLvdebUCsW"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/0/20bcaa1469b6cd976ebbba3850b544ed7a05958a_2_10x10.png"" height=""202"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/0/20bcaa1469b6cd976ebbba3850b544ed7a05958a_2_690x202.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/0/20bcaa1469b6cd976ebbba3850b544ed7a05958a_2_690x202.png, https://discuss.pytorch.org/uploads/default/optimized/3X/2/0/20bcaa1469b6cd976ebbba3850b544ed7a05958a_2_1035x303.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/2/0/20bcaa1469b6cd976ebbba3850b544ed7a05958a.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">node-memory</span><span class=""informations"">1366×401 5.92 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>The memory footprint does not include the cache consumption.</p><NewLine><p>And when I test the memory footprint about 32 nodes,I found a memory allocated error which indicates the memory has run out.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the detailed information. This is weird. This is the first time that we saw the memory footprint per machine increases significantly with the total number of machines.</p><NewLine><blockquote><NewLine><p>I guess the memory is used to load the disk data.I tried to use a small datasets<br/><NewLine>which only contains 40 samples(Every sample is 16MB).And in this case,the cache consumption is about 2.5GB.</p><NewLine></blockquote><NewLine><p>Q1: This does not add up. The model is 10MB, and the dataset is 40X16MB = 640MB.  I assume you do not use CUDA, as you mainly concerned about CPU memory. In this case, I would assume the total memory consumption to be less than 1GB. Do you know where does the other 1.5GB come from?</p><NewLine><p>Q2: Can you try to the following to see if it is indeed used by the process instead of some os cache.</p><NewLine><pre><code class=""lang-python"">import psutil<NewLine><NewLine>process = psutil.Process(os.getpid())<NewLine><NewLine>for _ in get_batch():<NewLine>    ....<NewLine>    # print this in every iteration<NewLine>    print(""Memory used: "", process.memory_info().rss)<NewLine></code></pre><NewLine><p>Q3: Do you have a minimum reprodueable example that we can investigate locally?</p><NewLine><p>Q4: BTW, how do you launch your distributed training job? Are you using <code>torch.distributed.launch</code>. If so, could you please share the command you are using?</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,Sorry,I can not reply you in time.</p><NewLine><p>Now I found the other 1.5GB comes from system.There is 1.5GB consumption before I start this job so I think cache consumption is right.</p><NewLine><p>I use the psutil module to test the process memory footprint.The result just like the following photo:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/59e36f955fa72bba53ddfe9d516fabda0acf955d"" href=""https://discuss.pytorch.org/uploads/default/original/3X/5/9/59e36f955fa72bba53ddfe9d516fabda0acf955d.png"" title=""image""><img alt=""image"" data-base62-sha1=""cPbLZbfaMu0MGdl5K7V0vfYGJhX"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/5/9/59e36f955fa72bba53ddfe9d516fabda0acf955d_2_10x10.png"" height=""166"" src=""https://discuss.pytorch.org/uploads/default/original/3X/5/9/59e36f955fa72bba53ddfe9d516fabda0acf955d.png"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1096×265 13.4 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine>(1 node)</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/4ccbc3baa2d75d32bee1e658b84dcad40cfa8622"" href=""https://discuss.pytorch.org/uploads/default/original/3X/4/c/4ccbc3baa2d75d32bee1e658b84dcad40cfa8622.png"" title=""image""><img alt=""image"" data-base62-sha1=""aXmQMuuUO0JyOcOiN1CMXMXzBWG"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/c/4ccbc3baa2d75d32bee1e658b84dcad40cfa8622_2_10x10.png"" height=""193"" src=""https://discuss.pytorch.org/uploads/default/original/3X/4/c/4ccbc3baa2d75d32bee1e658b84dcad40cfa8622.png"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1226×343 16.2 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine>(16 node)</p><NewLine><p>So the process memory footprint per machine does not increase with the total number of machines.However,the total memory footprint per machine increases significantly.</p><NewLine><p>Then I found the process numbers increase significantly with the number of machines.<br/><NewLine><img alt=""image"" data-base62-sha1=""s5UXNLTgFwBFiExMifmECHNKdJD"" height=""276"" src=""https://discuss.pytorch.org/uploads/default/original/3X/c/4/c4e8268d4dfc2cb0f080deba2c25ca373d0f4e01.png"" width=""494""/></p><NewLine><p>I set nproc_per_node to 1 so there should be one process in every machine.However,it is not.<br/><NewLine>There is my command:<br/><NewLine>bash.sh</p><NewLine><pre><code class=""lang-auto"">MIP=$ip<NewLine>MPORT=$port<NewLine>NPROC_PER_NODE=1<NewLine>HOSTLIST=$hostlist<NewLine>COMMAND=$HOME/sample.py --epochs=120 --workers=0 --batch-size=1 --print-freq=50 --data=$HOME/datasets/v3<NewLine>RANK=0<NewLine>for node in $HOSTLIST; do<NewLine>        echo $node<NewLine>        ssh -q $node \<NewLine>                python -m torch.distributed.launch \<NewLine>                --nproc_per_node=$NPROC_PER_NODE \<NewLine>                --nnodes=$SLURM_JOB_NUM_NODES \<NewLine>                --node_rank=$RANK \<NewLine>                --master_addr=""$MIP"" --master_port=$MPORT \<NewLine>                $COMMAND &gt; ""log_v1_""${SLURM_JOB_ID}""_""${RANK}"".out"" &amp;<NewLine>        RANK=$((RANK+1))<NewLine>done<NewLine></code></pre><NewLine><p>sample.py</p><NewLine><pre><code class=""lang-auto"">....<NewLine>if __name__ == '__main__':<NewLine>    if torch.distributed.is_available():<NewLine>        dist.init_process_group(backend='gloo',init_method='env://',timeout=datetime.timedelta(seconds=600))<NewLine>        main()<NewLine></code></pre><NewLine><p>In addition,I do not use CUDA and I train this model in CPUs.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>I do not think pytorch is the cause of the mistake.I will check my shell script,thank you <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/khalil; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/khalil; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/khalil; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/khalil; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/khalil; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/khalil; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/khalil; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/khalil; <NewLine> ,"REPLY_DATE 1: June 18, 2020,  6:18pm; <NewLine> REPLY_DATE 2: June 19, 2020,  1:48am; <NewLine> REPLY_DATE 3: June 19, 2020,  2:13am; <NewLine> REPLY_DATE 4: June 19, 2020,  2:14am; <NewLine> REPLY_DATE 5: June 19, 2020,  2:29am; <NewLine> REPLY_DATE 6: June 19, 2020,  2:35am; <NewLine> REPLY_DATE 7: June 19, 2020,  7:02am; <NewLine> REPLY_DATE 8: June 19, 2020,  7:16am; <NewLine> REPLY_DATE 9: June 19, 2020,  2:50pm; <NewLine> REPLY_DATE 10: June 19, 2020,  4:54pm; <NewLine> REPLY_DATE 11: June 19, 2020,  5:41pm; <NewLine> REPLY_DATE 12: June 20, 2020,  3:08am; <NewLine> REPLY_DATE 13: June 20, 2020,  4:20pm; <NewLine> REPLY_DATE 14: June 21, 2020,  3:30am; <NewLine> REPLY_DATE 15: June 21, 2020,  3:38pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: 1 Like; <NewLine> REPLY 15 LIKES: 1 Like; <NewLine> 
86178,Learning Rate Schedulers with multiple Google Cloud TPU cores,2020-06-20T13:06:12.000Z,0,49,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi All!<br/><NewLine>I wanted to use a CosineAnnealingWithRestarts schedulers on Google Cloud TPU cores, with distributed parallel training.<br/><NewLine>I couldn’t find a tutorial which shows how to do that.</p><NewLine><p>Please help me out.</p><NewLine></div>",https://discuss.pytorch.org/u/chhablanig,(Gunjan Chhablani),chhablanig,"June 20, 2020,  1:06pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/chhablanig"">@chhablanig</a></p><NewLine><p>Do you have a working training script without using LR schedulers?</p><NewLine><p>cc <a class=""mention"" href=""/u/vincentqb"">@vincentqb</a> for optimizer question<br/><NewLine>cc <a class=""mention"" href=""/u/ailzhang"">@ailzhang</a> for XLA question</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 20, 2020,  4:22pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
86089,Pytorch DDP failed to resume after validation using a clone of the model,2020-06-19T16:10:51.090Z,2,93,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">dataloader = ....(create ddp loader with ddp settings)<NewLine>opts = ...parse() # user options<NewLine>master = opts.local_rank == 0<NewLine><NewLine>model = create_model(opt)<NewLine>model_ema = model.clone().eval() # keeping track of exponential moving average for model's weights<NewLine><NewLine>for data in dataloader():<NewLine>     # typical training code ... forward, backward and the likes <NewLine>      update_ema_weights(model_ema, model.state_dict()) # update the weights for model's team<NewLine>      if opt.validate:<NewLine>         if master:<NewLine>               for data in valid_dataloader():<NewLine>                     output = model_ema(data).... # typical validate code<NewLine>         torch.cuda.synchronize()<NewLine></code></pre><NewLine><p>given the above pseudo-code, after validation, my DDP process will hang on all GPUs.<br/><NewLine>However, if I use model instead of model_ema for validation, it will not. Does anyone know how to fix this?</p><NewLine></div>",https://discuss.pytorch.org/u/Scott_Hoang,(Scott Hoang),Scott_Hoang,"June 19, 2020,  4:13pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Would I be correct if I assume <code>model</code> (and <code>model_ema</code> as well) is a <code>DistributedDataParallel</code> instance? If so, the <code>forward</code> method of <code>DistributedDataParallel</code> will set some internal flags, which could cause hang.</p><NewLine><p>If you just want to evaluate, you can use <code>model.module</code> to retrieve the original non-DDP module. And then clone, set <code>eval()</code>, run <code>forward</code> on the original non-DDP module. This should keep DDP states intact.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see. I will give it a shot. But it doesn’t explain why if I use “model” to evaluate, no hang occur.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Good point. How did you implement the <code>model.clone()</code> method? IIUC, neither <code>nn.Module</code> nor <code>DistributedDataParallel</code> has a <code>clone()</code> method. Is this a <code>copy.deepcopy</code>?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>i use deepcopy to clone the model.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Scott_Hoang; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Scott_Hoang; <NewLine> ,"REPLY_DATE 1: June 19, 2020,  5:46pm; <NewLine> REPLY_DATE 2: June 19, 2020,  9:25pm; <NewLine> REPLY_DATE 3: June 19, 2020,  9:46pm; <NewLine> REPLY_DATE 4: June 19, 2020, 10:31pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
85982,Parallel on multiple process but only do optimization step on process 0,2020-06-18T20:44:35.600Z,0,54,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to implement parallelization as follows but not sure if it is possible.<br/><NewLine>For example, train data with multiple processes (CPU cores). Have each process deal with independent batches. Instead of taking the optimization step independently for each batch, I want to gather loss and gradient from all processes and only take optimization step on Process 0.<br/><NewLine>Is that possible to do that with torch.distributed package?</p><NewLine><p>Currently, I followed instructions from <a href=""https://pytorch.org/tutorials/intermediate/dist_tuto.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/dist_tuto.html</a>. And have it work.</p><NewLine></div>",https://discuss.pytorch.org/u/stclaireva,(stclaireva),stclaireva,"June 18, 2020,  8:50pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/stclaireva"">@stclaireva</a></p><NewLine><blockquote><NewLine><p>Is that possible to do that with torch.distributed package?</p><NewLine></blockquote><NewLine><p>This is possible. You can</p><NewLine><ol><NewLine><li>Run forward-backward locally.</li><NewLine><li>Use <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather"" rel=""nofollow noopener""><code>all_gather</code></a> or <a href=""https://github.com/pytorch/pytorch/blob/d14d47b9b5c0b860e0e285c91a1c7e74163a5bca/torch/distributed/distributed_c10d.py#L1194"" rel=""nofollow noopener""><code>all_gather_coalesced</code></a> to collect all gradients into rank 0.</li><NewLine><li>Manually add/average those gradients into <code>param.grad</code> on rank 0.</li><NewLine><li>Run <code>optimizer.step()</code> on rank 0 to update parameters.</li><NewLine><li>Let rank 0 <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.broadcast"" rel=""nofollow noopener"">broadcast</a> the updated parameters to other ranks.</li><NewLine><li>go to step 1 to start a new iteration</li><NewLine></ol><NewLine><p>Curious, why do you need the above algorithm instead of letting DistributedDataParallel handle it for you?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 18, 2020,  9:05pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
58184,How to use nn.parallel.DistributedDataParallel,2019-10-14T13:51:04.941Z,11,749,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to use nn.parallel.DistributedDataParallel to train my model on single machine with 8 2080TI GPUs. I set distributed config as torch.distributed.init_process_group(backend=‘nccl’, init_method=‘tcp://localhost:1088’, rank=0, world_size=1)<br/><NewLine>However, no GPU works in train. And if I use nn.DataParallel, this problem is not existence.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/08ebb7ff6e28498bc6180db00b9b7b50f2979dc7"" href=""https://discuss.pytorch.org/uploads/default/original/3X/0/8/08ebb7ff6e28498bc6180db00b9b7b50f2979dc7.jpeg"" title=""WX20191014-213647@2x.jpg""><img alt=""WX20191014-213647%402x"" data-base62-sha1=""1gUQnytBzE9ocNpAF5kfigxGcED"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/8/08ebb7ff6e28498bc6180db00b9b7b50f2979dc7_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/8/08ebb7ff6e28498bc6180db00b9b7b50f2979dc7_2_466x500.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/8/08ebb7ff6e28498bc6180db00b9b7b50f2979dc7_2_466x500.jpeg, https://discuss.pytorch.org/uploads/default/optimized/3X/0/8/08ebb7ff6e28498bc6180db00b9b7b50f2979dc7_2_699x750.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/0/8/08ebb7ff6e28498bc6180db00b9b7b50f2979dc7_2_932x1000.jpeg 2x"" width=""466""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">WX20191014-213647@2x.jpg</span><span class=""informations"">1180×1264 411 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/mingyang94,(Amy),mingyang94,"October 14, 2019,  1:51pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>DistributedDataParallel (DDP) is multi process training. For you case, you would get best performance with 8 DDP processes, where the i-th process calls:</p><NewLine><pre><code class=""lang-python"">torch.distributed.init_process_group(<NewLine>    backend=‘nccl’, <NewLine>    init_method=‘tcp://localhost:1088’, <NewLine>    rank=i, <NewLine>    world_size=8<NewLine>)<NewLine></code></pre><NewLine><p>Or, you could set env vars and use <code>https://github.com/pytorch/pytorch/blob/master/torch/distributed/launch.py</code></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply. Now I can run my training program and GPU works, but nothing print in terminal. Why?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you share a code snippet? What do you expect to see in terminal?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am in a very similar situation where I have a single node and 8 GPUs.  I used the following resource as a guideline for distributed parallel training: <a href=""https://github.com/dnddnjs/pytorch-multigpu"" rel=""nofollow noopener"">https://github.com/dnddnjs/pytorch-multigpu</a></p><NewLine><p>I was able to run this example fine, but when I try to load the model, I get the following error:<br/><NewLine>Expected tensor for argument <span class=""hashtag"">#1</span> ‘input’ to have the same device as tensor for argument <span class=""hashtag"">#2</span> ‘weight’; but device 4 does not equal 0 (while checking arguments for cudnn_convolution)</p><NewLine><p>Could this be a problem in how I am loading the training data?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, it means that your input and model weight are not on the same device, just like your input on GPU-0 while your model weight on GPU-1. Note that both input and weight must be obtained by same device. I think it may be caused by the wrong load way. Can you show your loading code or give an example?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply.  I believe I found the error.  The original code was written for a single GPU and unfortunately there were multiple places where cuda:0 was hardcoded.</p><NewLine><p>However, I do have a followup question.  From all the examples I have seen so far, the dataloader is associated with DistributedSampler: <a href=""https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html</a></p><NewLine><p>In many cases the dataloader loads files in a directory.  In the case of using DDP, the DistributedSampler would be used to allocate files for each GPU such that each GPU would get a unique distribution of samples from the total dataset (I am assuming that the total data items is integer divisible by the number of GPUs).</p><NewLine><p>In my case I am loading 3D data and then take patches of this data, which serves as the training input of the network.  So one data file corresponds to more than one training input.</p><NewLine><p>What I do currently is that I load the data by loading the files in a folder.  Then I use a custom sampler that loads the patch indices and has an iterator that passes patches when called.  I feed this sampler to a Dataloader, where I pass in the whole data, the sampler, and batch size.  This works fine for one GPU.</p><NewLine><p>I am now moving on to converting my code to DDP.  I could put the DistributedSampler after my custom sampler, but I worry about the idea of multiple GPUs accessing the same file (again the input is a patch and different patches can come from the same file).  Am I correct to say that this would be a problem?</p><NewLine><p>Another approach could be to put the DistributedSampler before my current sampler.  But I am a bit unsure how to hook up this DistributedSampler to my existing code.</p><NewLine><p>I suppose yet another method could be to bypass using torch.utils.data.distributed.DistributedSampler and perhaps instead have my initial Dataset have a <strong>getitem</strong> that distributes the files among the GPUs in a manner similar to the DistributedSampler, and then keep the rest of my hooks the same.  Or alternatively, in the main loop I could have the logic for handling the distribution of files and pass this into the spawned process.</p><NewLine><p>Would one approach be better than others?  Or should I be using another approach altogether?  Does the DDP work properly if the code does not use the DistributedSampler?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a></p><NewLine><ol><NewLine><li>Can you please define the terms: 1) node, 2) process in the context you are using them?</li><NewLine><li>If I want to train my model on 4 GPUs, do you call it 4 processes? or 1 process?</li><NewLine><li>Does <code>init_method</code> correspond to the address of my PC or to the GPU I’m accessing on a cluster?</li><NewLine><li>In <a href=""https://pytorc.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">this</a> tutorial, what were you referring to as machine?</li><NewLine></ol><NewLine><p><a class=""mention"" href=""/u/mingyang94"">@mingyang94</a></p><NewLine><ol><NewLine><li>Can you please explain how you arrived at:</li><NewLine></ol><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""58184"" data-username=""mingyang94""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/a4c791/40.png"" width=""20""/> mingyang94:</div><NewLine><blockquote><NewLine><p>init_method</p><NewLine></blockquote><NewLine></aside><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/spabho"">@spabho</a></p><NewLine><blockquote><NewLine><ol><NewLine><li>Can you please define the terms: 1) node, 2) process in the context you are using them?</li><NewLine></ol><NewLine></blockquote><NewLine><p>We usually use one node/machine/server to represent one physical computer which can be equipped with multiple GPUs.</p><NewLine><p>One process is in the context of process/thread.</p><NewLine><blockquote><NewLine><p>If I want to train my model on 4 GPUs, do you call it 4 processes? or 1 process?</p><NewLine></blockquote><NewLine><p>In this case, using 4 processes with DDP should give you the best performance.</p><NewLine><blockquote><NewLine><p>Does  <code>init_method</code>  correspond to the address of my PC or to the GPU I’m accessing on a cluster?</p><NewLine></blockquote><NewLine><p>It corresponds to the address of your PC. It is giving some information for the 4 DDP processes to perform rendezvous.</p><NewLine><blockquote><NewLine><p>In <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">this</a> tutorial, what were you referring to as machine?</p><NewLine></blockquote><NewLine><p>Machines should always refer to node/server/machine/computer.</p><NewLine><p>To be clear, there are three concepts involved in DDP training:</p><NewLine><ul><NewLine><li><NewLine><strong>Node/Machine/Server</strong>: a physical computer that can contain multiple GPUs. It can also talk to other node through network.</li><NewLine><li><NewLine><strong>GPU</strong>: just one GPU</li><NewLine><li><NewLine><strong>Process</strong>: Each process should run its own DDP instance. Usually Each DDP instance should exclusively operate on one GPU (if your model can fit in one GPU), and DDP instances will talk to each other through network.</li><NewLine></ul><NewLine><p><a href=""https://pytorch.org/docs/master/notes/ddp.html#example"" rel=""nofollow noopener"">This</a> example might serve better as a starting point for DDP.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>I was looking at the <a href=""https://pytorch.org/docs/master/notes/ddp.html#example"" rel=""nofollow noopener"">tutorial</a> you mentioned.</p><NewLine><p>In the example, it says that</p><NewLine><blockquote><NewLine><p>This example uses a <a href=""https://pytorch.org/docs/master/generated/torch.nn.Linear.html#torch.nn.Linear"" rel=""nofollow noopener""> <code>torch.nn.Linear</code> </a> as the local model, wraps it with DDP, and then runs one forward pass, one backward pass, and an optimizer step on the DDP model. After that, parameters on the local model will be updated, and all models on different processes should be exactly the same.</p><NewLine></blockquote><NewLine><p>I’m just wondering what it means by local model, and what’s the difference between the local model and the models on different processes.</p><NewLine><p>Thanks!</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""10"" data-topic=""58184"" data-username=""rzhang63""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/r/da6949/40.png"" width=""20""/> rzhang63:</div><NewLine><blockquote><NewLine><p>I’m just wondering what it means by local model, and what’s the difference between the local model and the models on different processes.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Hey <a class=""mention"" href=""/u/rzhang63"">@rzhang63</a>, each process runs its own DDP model, which wraps the local model. DDP does not own any parameters (but it will create buffers/buckets for communication). So from the perspective of parameters, they are the same.</p><NewLine><p>The reason for calling them local model vs DDP model is that, local model by itself does not perform communication across processes. DDP takes care of communication to make sure that different local models on difference processes are always in sync, as if all processes are operating on the same global module.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>Thank you for your reply. I was running the example code in the <a href=""https://pytorch.org/docs/master/notes/ddp.html#example"" rel=""nofollow noopener"">tutorial</a> but I got the following error:</p><NewLine><blockquote><NewLine><p>AttributeError: Can’t get attribute ‘demo_basic’ on &lt;module ‘<strong>main</strong>’ (built-in)&gt;</p><NewLine></blockquote><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/abd3f5c070894d72d079df2deb066c56db145cdb"" href=""https://discuss.pytorch.org/uploads/default/original/3X/a/b/abd3f5c070894d72d079df2deb066c56db145cdb.png"" title=""image""><img alt=""image"" data-base62-sha1=""ow3LIhc9ETcCy0El0yYpCgLNTLR"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/b/abd3f5c070894d72d079df2deb066c56db145cdb_2_10x10.png"" height=""269"" src=""https://discuss.pytorch.org/uploads/default/original/3X/a/b/abd3f5c070894d72d079df2deb066c56db145cdb.png"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1136×443 27.9 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>Do you know why this happened?</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""12"" data-topic=""58184"" data-username=""rzhang63""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/r/da6949/40.png"" width=""20""/> rzhang63:</div><NewLine><blockquote><NewLine><p>Thank you for your reply. I was running the example code in the <a href=""https://pytorch.org/docs/master/notes/ddp.html#example"" rel=""nofollow noopener"">tutorial</a> but I got the following error:</p><NewLine></blockquote><NewLine></aside><NewLine><p>The link provided above points to the DDP example, but <code>demo_basic</code> is one function from <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html</a>. Are you mixing these two?</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>The code I used is</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import tempfile<NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine>import torch.multiprocessing as mp<NewLine><NewLine>from torch.nn.parallel import DistributedDataParallel as DDP<NewLine><NewLine>def setup(rank, world_size):<NewLine>    os.environ['MASTER_ADDR'] = 'localhost'<NewLine>    os.environ['MASTER_PORT'] = '12355'<NewLine>    <NewLine>    # initialize the process group<NewLine>    dist.init_process_group(""nccl"", rank=rank, world_size=world_size)<NewLine><NewLine>def cleanup():<NewLine>    dist.destroy_process_group()<NewLine><NewLine>def demo_basic(rank, world_size):<NewLine>    print(f""Running basic DDP example on rank {rank}."")<NewLine>    setup(rank, world_size)<NewLine>    <NewLine>    # create model and move it to GPU with id rank<NewLine>    model = nn.Linear(10, 10).to(rank)<NewLine>    ddp_model = DDP(model, device_ids=[rank])<NewLine>    <NewLine>    loss_fn = nn.MSELoss()<NewLine>    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)<NewLine>    <NewLine>    optimizer.zero_grad()<NewLine>    outputs = ddp_model(torch.randn(20, 10))<NewLine>    labels = torch.randn(20, 5).to(rank)<NewLine>    loss_fn(outputs, labels).backward()<NewLine>    optimizer.step()<NewLine>    <NewLine>    cleanup()<NewLine><NewLine>def main():<NewLine>    world_size = 2<NewLine>    mp.spawn(demo_basic,<NewLine>        args=(world_size,),<NewLine>        nprocs=world_size,<NewLine>        join=True)<NewLine><NewLine>if __name__==""__main__"":<NewLine>    main()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/rzhang63"">@rzhang63</a> sorry that I mis-read your log picture. Are you using Windows? Currently PT Distributed does not support windows yet. We have a poll here: <a href=""https://github.com/pytorch/pytorch/issues/37068"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/37068</a></p><NewLine><p>If running on Linux, it should work with a minor fix.</p><NewLine><p>Change</p><NewLine><pre><code class=""lang-python"">labels = torch.randn(20, 5).to(rank)<NewLine></code></pre><NewLine><p>to</p><NewLine><pre><code class=""lang-python"">labels = torch.randn(20, 10).to(rank)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just switched to Linux and changed the labels to <code>labels = torch.randn(20, 10).to(rank)</code>, but I still got the following error:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/c860a30b5310fe1498f10eb96dbcd5dcc7936cfe"" href=""https://discuss.pytorch.org/uploads/default/original/3X/c/8/c860a30b5310fe1498f10eb96dbcd5dcc7936cfe.png"" title=""image""><img alt=""image"" data-base62-sha1=""sACxkP1AZVt1CGx44Zd6wAvyC06"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/8/c860a30b5310fe1498f10eb96dbcd5dcc7936cfe_2_10x10.png"" height=""245"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/8/c860a30b5310fe1498f10eb96dbcd5dcc7936cfe_2_690x245.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/8/c860a30b5310fe1498f10eb96dbcd5dcc7936cfe_2_690x245.png, https://discuss.pytorch.org/uploads/default/optimized/3X/c/8/c860a30b5310fe1498f10eb96dbcd5dcc7936cfe_2_1035x367.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/c/8/c860a30b5310fe1498f10eb96dbcd5dcc7936cfe_2_1380x490.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1678×598 214 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>This seems to be a multiprocessing pickle issue. How did you launch it. Is it sth like <code>python test.py</code> from command line or through notebook?</p><NewLine><p>And can you confirm you see the same error even if you remove all <code>torch.distributed</code> code? Say make <code>demo_basic</code> into the following and remove other functions?</p><NewLine><pre><code class=""lang-python"">def demo_basic(rank, world_size):<NewLine>    pass<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>This looks relevant to the error you are seeing: <a href=""https://github.com/ipython/ipython/issues/10894"" rel=""nofollow noopener"">https://github.com/ipython/ipython/issues/10894</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mingyang94; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/solarflarefx; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Lausanne; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/solarflarefx; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/spabho; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/rzhang63; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/rzhang63; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/rzhang63; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/rzhang63; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: October 14, 2019,  9:04pm; <NewLine> REPLY_DATE 2: October 15, 2019,  1:39am; <NewLine> REPLY_DATE 3: October 16, 2019,  2:00am; <NewLine> REPLY_DATE 4: February 11, 2020,  6:09pm; <NewLine> REPLY_DATE 5: February 12, 2020,  2:58am; <NewLine> REPLY_DATE 6: February 13, 2020,  4:22am; <NewLine> REPLY_DATE 7: February 21, 2020,  5:49pm; <NewLine> REPLY_DATE 8: March 11, 2020,  2:29pm; <NewLine> REPLY_DATE 9: June 18, 2020,  2:48am; <NewLine> REPLY_DATE 10: June 18, 2020,  2:39pm; <NewLine> REPLY_DATE 11: June 18, 2020,  2:51pm; <NewLine> REPLY_DATE 12: June 18, 2020,  6:13pm; <NewLine> REPLY_DATE 13: June 18, 2020,  8:21pm; <NewLine> REPLY_DATE 14: June 18, 2020,  8:28pm; <NewLine> REPLY_DATE 15: June 18, 2020,  8:41pm; <NewLine> REPLY_DATE 16: June 18, 2020,  8:57pm; <NewLine> REPLY_DATE 17: June 18, 2020,  8:59pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> 
85403,How can I use the Distributed instead of dataparallel,2020-06-14T07:50:07.153Z,5,158,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I used Distributed dataparallel to replace dataparallel,the result of the validation set becomes very poor, as in the case of overfitting. I used 4 GPUs, one process per GPU, keeping the learning rate and batchsize unchanged.The following is all the code related to DPP：</p><NewLine><pre><code class=""lang-auto"">dist.init_process_group(backend='nccl')<NewLine>torch.cuda.set_device(args.local_rank)<NewLine>device = torch.device(""cuda"", args.local_rank)<NewLine><NewLine>train_sampler = torch.utils.data.distributed.DistributedSampler(train_set)<NewLine>train_loader = torch.utils.data.DataLoader(<NewLine>        train_set, batch_size=args.batch_size,<NewLine>        num_workers=args.workers,sampler=train_sampler, pin_memory=True, shuffle=(train_sampler is None))<NewLine>val_sampler = torch.utils.data.distributed.DistributedSampler(val_set)<NewLine>val_loader = torch.utils.data.DataLoader(<NewLine>        val_set, batch_size=args.batch_size,<NewLine>        num_workers=args.workers, pin_memory=True, shuffle=False,sampler=val_sampler)<NewLine><NewLine>model = models.__dict__[args.arch](network_data).to(device)<NewLine>model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank])<NewLine>cudnn.benchmark = True<NewLine>for epoch in tqdm(range(args.start_epoch, args.epochs)):<NewLine>    # train for one epoch<NewLine>    train_sampler.set_epoch(epoch)<NewLine><NewLine>    train_loss=train(......)<NewLine>    dist.reduce(train_loss, 0, op=dist.ReduceOp.SUM)<NewLine>    print(train_loss/nb_gpus)<NewLine><NewLine>    test_loss=validate(.....)<NewLine>    dist.reduce(test_loss, 0, op=dist.ReduceOp.SUM)<NewLine>    print(test_loss/nb_gpus)<NewLine></code></pre><NewLine><p><img alt=""image"" data-base62-sha1=""xr85WTbM32F8oRxHSOeaJflDf7p"" height=""242"" src=""https://discuss.pytorch.org/uploads/default/original/3X/e/a/ea58da8315adf60e27727d89d873d602e995ca7f.png"" width=""412""/><br/><NewLine><strong>blue curve is the result of validation set</strong></p><NewLine></div>",https://discuss.pytorch.org/u/111344,(祥棉 陈),111344,"June 14, 2020,  2:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/111344"">@111344</a></p><NewLine><p>If each DDP (DistributedDataParallel) process is using the same batch size as you passed to DataParallel, then I think you need to divide the reduced loss by <code>world_size</code>. Otherwise, you are summing together losses from <code>world_size</code> batches.</p><NewLine><p>Another thing is that batch size and learning rate might need to change when switched to DDP. Check out the discussions below:</p><NewLine><ol><NewLine><li><a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769"">Should we split batch_size according to ngpu_per_node when DistributedDataparallel</a></li><NewLine><li><a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/is-average-the-correct-way-for-the-gradient-in-distributeddataparallel-with-multi-nodes/34260"">Is average the correct way for the gradient in DistributedDataParallel with multi nodes?</a></li><NewLine></ol><NewLine><p>And this briefly explains how DDP works: <a href=""https://pytorch.org/docs/master/notes/ddp.html"" rel=""nofollow noopener"">https://pytorch.org/docs/master/notes/ddp.html</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your answer,it helped me a lot. <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/><br/><NewLine>One conclusion I got from these materials is that I should set torch.utils.data.DataLoader(batch_size=args.batch_size/world_size）<br/><NewLine>lr still be 1xlr.<br/><NewLine>Is this correct?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""85403"" data-username=""111344""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/111344/40/25561_2.png"" width=""20""/> 111344:</div><NewLine><blockquote><NewLine><p>torch.utils.data.DataLoader(batch_size=args.batch_size/world_size）</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, this should let the DDP gang collectively process the same number of samples compared to the single process case. But it may or may not stay mathematically equivalent due to the loss function. DDP is taking average of grads across processes. So if the loss function is calculating sum loss of all samples or if <code>(loss(x) + loss(y)) / 2 != loss([x, y]) / 2</code>, it won’t be mathematically equivalent. Hence, it might take some efforts to optimizer the lr and batch size when using DDP.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey,sorry for late reply.<br/><NewLine>My loss function is defined as follows：<br/><NewLine><code>loss = torch.norm(target_flow - input_flow, 2, 1)/batch_size</code><br/><NewLine>In <a href=""https://discuss.pytorch.org/t/is-average-the-correct-way-for-the-gradient-in-distributeddataparallel-with-multi-nodes/34260"">https://discuss.pytorch.org/t/is-average-the-correct-way-for-the-gradient-in-distributeddataparallel-with-multi-nodes/34260</a><br/><NewLine>there are some discussions on how to calculate loss,it seems that DDP will automatically do batchsize average operation on loss,so do I need to manually average the loss?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""85403"" data-username=""111344""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/111344/40/25561_2.png"" width=""20""/> 111344:</div><NewLine><blockquote><NewLine><p>there are some discussions on how to calculate loss,it seems that DDP will automatically do batchsize average operation on loss,so do I need to manually average the loss?</p><NewLine></blockquote><NewLine></aside><NewLine><p>No, you don’t need to manually average the loss. When using DDP, losses are local to every process, and DDP will automatically average gradients for all parameters using AllReduce communication.</p><NewLine><blockquote><NewLine><p>My loss function is defined as follows：<br/><NewLine><code>loss = torch.norm(target_flow - input_flow, 2, 1)/batch_size</code></p><NewLine></blockquote><NewLine><p>The <code>batch_size</code> here is the per-process input batch size, right?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes,it’s per-process batch_size.<br/><NewLine>In fact, I think the problem is basically solved after dividing Batchsize by ngpus (although performance is still slightly behind DP, but this should be a tuning problem)<br/><NewLine>Thank you for your help. Best wishes!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/111344; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/111344; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/111344; <NewLine> ,"REPLY_DATE 1: June 17, 2020,  3:00pm; <NewLine> REPLY_DATE 2: June 14, 2020,  7:19pm; <NewLine> REPLY_DATE 3: June 14, 2020,  7:49pm; <NewLine> REPLY_DATE 4: June 15, 2020,  6:27am; <NewLine> REPLY_DATE 5: June 15, 2020,  2:59pm; <NewLine> REPLY_DATE 6: June 17, 2020,  2:59pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> 
33056,Multiprocessing failed with Torch.distributed.launch module,2018-12-26T02:21:15.467Z,17,10731,"<div class=""post"" itemprop=""articleBody""><NewLine><p>During training MNIST example dataset in PyTorch, I met the this RuntimeError on the Master node</p><NewLine><pre><code class=""lang-auto"">File ""./torch-dist/mnist-dist.py"", line 201, in &lt;module&gt;<NewLine>    init_processes(args.rank, args.world_size, run, args.batch_size, backend=args.backend)<NewLine>  File ""./torch-dist/mnist-dist.py"", line 196, in init_processes<NewLine>    dist.init_process_group(backend=backend, world_size=world_size, rank=rank, init_method=""env://"")<NewLine>  File ""/home/dl/anaconda2/envs/torch-dist-py3.6/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 354, in init_process_group<NewLine>    store, rank, world_size = next(rendezvous(url))<NewLine>  File ""/home/dl/anaconda2/envs/torch-dist-py3.6/lib/python3.6/site-packages/torch/distributed/rendezvous.py"", line 143, in _env_rendezvous_handler<NewLine>    store = TCPStore(master_addr, master_port, start_daemon)<NewLine>RuntimeError: Address already in use<NewLine><NewLine></code></pre><NewLine><p>It seemed that multiprocessing with launch utility had problem<br/><NewLine>I ran the code by invoking the launch utility as <a href=""https://pytorch.org/docs/stable/distributed.html#launch-utility"" rel=""nofollow noopener"">documentation</a> suggested</p><NewLine><pre><code class=""lang-auto"">python -m torch.distributed.launch --nproc_per_node=2 --nnode=2 --node_rank=0 --master_addr='10.0.3.29' --master_port=9901 ./torch-dist/mnist-dist.py <NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/leo-mao,(leo),leo-mao,"December 31, 2018,  7:01am",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here is my code, hope it help</p><NewLine><pre><code class=""lang-auto"">import argparse<NewLine>import time<NewLine>import torch<NewLine><NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.backends.cudnn as cudnn<NewLine>import torch.distributed as dist<NewLine>import torch.utils.data<NewLine>import torch.utils.data.distributed<NewLine>import torch.optim as optim<NewLine><NewLine>import torch.distributed<NewLine><NewLine><NewLine>from torchvision import datasets, transforms<NewLine>from torch.autograd import Variable<NewLine><NewLine>parser = argparse.ArgumentParser(description='PyTorch MNIST Example')<NewLine>parser.add_argument('--batch-size', type=int, default=256, metavar='N',<NewLine>                    help='input batch size for training (default: 64)')<NewLine>parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',<NewLine>                    help='input batch size for testing (default: 1000)')<NewLine>parser.add_argument('--epochs', type=int, default=5, metavar='N', help='number of epochs to train (default: 10)')<NewLine><NewLine>parser.add_argument('--lr', type=float, default=0.01, metavar='LR', help='learning rate (default: 0.01)')<NewLine>parser.add_argument('--momentum', type=float, default=0.5, metavar='M', help='SGD momentum (default: 0.5)')<NewLine>parser.add_argument('--no-cuda', action='store_true', default=False, help='disables CUDA training')<NewLine>parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default: 1)')<NewLine>parser.add_argument('--log-interval', type=int, default=10, metavar='N',<NewLine>                    help='how many batches to wait before logging training status')<NewLine>parser.add_argument('--backend', type=str, default='nccl')<NewLine>parser.add_argument('--rank', type=int, default=0)<NewLine>parser.add_argument('--world-size', type=int, default=1)<NewLine>parser.add_argument('--local_rank', type=int)<NewLine>args = parser.parse_args()<NewLine>args.cuda = not args.no_cuda and torch.cuda.is_available()<NewLine><NewLine><NewLine>class Net(nn.Module):<NewLine><NewLine>    def __init__(self):<NewLine>        super(Net, self).__init__()<NewLine>        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)<NewLine>        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)<NewLine>        self.conv2_drop = nn.Dropout2d()<NewLine>        self.fc1 = nn.Linear(320, 50)<NewLine>        self.fc2 = nn.Linear(50, 10)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = F.relu(F.max_pool2d(self.conv1(x), 2))<NewLine>        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))<NewLine>        x = x.view(-1, 320)<NewLine>        x = F.relu(self.fc1(x))<NewLine>        x = F.dropout(x, training=self.training)<NewLine>        x = self.fc2(x)<NewLine>        return F.log_softmax(x, dim=0)<NewLine><NewLine><NewLine>def average_gradients(model):<NewLine>    """""" Gradient averaging""""""<NewLine>    size = float(dist.get_world_size())<NewLine><NewLine>    for param in model.parameters():<NewLine>        dist.all_reduce_multigpu(param.grad.data, op=dist.ReduceOp.SUM)<NewLine>        param.grad.data /= size<NewLine><NewLine><NewLine>def summary_print(rank, loss, accuracy, average_epoch_time, tot_time):<NewLine>    import logging<NewLine>    size = float(dist.get_world_size())<NewLine>    summaries = torch.tensor([loss, accuracy, average_epoch_time, tot_time], requires_grad=False, device='cuda')<NewLine>    dist.reduce_multigpu(summaries, 0, op=dist.ReduceOp.SUM)<NewLine>    if rank == 0:<NewLine>        summaries /= size<NewLine>        logging.critical('\n[Summary]System : Average epoch time(ex. 1.): {:.2f}s, Average total time : {:.2f}s '<NewLine>                         'Average loss: {:.4f}\n, Average accuracy: {:.2f}%'<NewLine>                         .format(summaries[2], summaries[3], summaries[0], summaries[1] * 100))<NewLine><NewLine><NewLine>def train(model, optimizer, train_loader, epoch):<NewLine>    model.train()<NewLine>    for batch_idx, (data, target) in enumerate(train_loader):<NewLine>        if args.cuda:<NewLine>            data, target = data.cuda(), target.cuda()<NewLine>        data, target = Variable(data), Variable(target)<NewLine>        optimizer.zero_grad()<NewLine>        output = model(data)<NewLine>        loss = F.nll_loss(output, target)<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine>        if args.world_size &gt; 1:<NewLine>            average_gradients(model)<NewLine>        if batch_idx % args.log_interval == 0:<NewLine>            print('Train Epoch {} - {} / {:3.0f} \tLoss  {:.6f}'.format(<NewLine>                epoch, batch_idx, 1.0 * len(train_loader.dataset) / len(data), loss))<NewLine><NewLine><NewLine>def test(test_loader, model):<NewLine>    model.eval()<NewLine>    test_loss = 0<NewLine>    correct = 0<NewLine>    for data, target in test_loader:<NewLine>        if args.cuda:<NewLine>            data, target = data.cuda(), target.cuda()<NewLine>        # Varibale(data, volatile=True)<NewLine>        data, target = Variable(data, requires_grad=False), Variable(target)<NewLine>        output = model(data)<NewLine>        test_loss += F.nll_loss(output, target, reduction='sum')<NewLine>        pred = output.data.max(1, keepdim=True)[1]<NewLine>        correct += pred.eq(target.data.view_as(pred)).cpu().sum()<NewLine><NewLine>    test_loss /= len(test_loader.dataset)<NewLine>    print('\nTest set : Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'<NewLine>          .format(test_loss, correct, len(test_loader.dataset),<NewLine>                  100. * correct / len(test_loader.dataset)))<NewLine>    return test_loss, float(correct) / len(test_loader.dataset)<NewLine><NewLine><NewLine>def config_print(rank, batch_size, world_size):<NewLine>    print('----Torch Config----')<NewLine>    print('rank : {}'.format(rank))<NewLine>    print('mini batch-size : {}'.format(batch_size))<NewLine>    print('world-size : {}'.format(world_size))<NewLine>    print('backend : {}'.format(args.backend))<NewLine>    print('--------------------')<NewLine><NewLine><NewLine>def run(rank, batch_size, world_size):<NewLine>    """""" Distributed Synchronous SGD Example """"""<NewLine>    config_print(rank, batch_size, world_size)<NewLine><NewLine>    train_dataset = datasets.MNIST('../MNIST_data/', train=True,<NewLine>                                   transform=transforms.Compose([transforms.ToTensor(),<NewLine>                                                                 transforms.Normalize((0.1307,), (0.3081,))]))<NewLine><NewLine>    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=world_size,<NewLine>                                                                    rank=rank)<NewLine><NewLine>    kwargs = {'num_workers': args.world_size, 'pin_memory': True} if args.cuda else {}<NewLine><NewLine>    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)<NewLine>    test_loader = torch.utils.data.DataLoader(datasets.MNIST('../MNIST_data/', train=False,<NewLine>                                                             transform=transforms.Compose(<NewLine>                                                                 [transforms.ToTensor(),<NewLine>                                                                  transforms.Normalize((0.1307,), (0.3081,))])),<NewLine>                                              batch_size=args.test_batch_size, shuffle=True, **kwargs)<NewLine><NewLine>    model = Net()<NewLine><NewLine>    if args.cuda:<NewLine>        torch.cuda.manual_seed(args.seed)<NewLine>        torch.cuda.set_device(args.local_rank)<NewLine>        device = torch.device('cuda', args.local_rank)<NewLine>        model.cuda(device=device)<NewLine>        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)<NewLine>        cudnn.benchmark = True<NewLine>    else:<NewLine>        device = torch.device('cpu')<NewLine><NewLine>    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)<NewLine><NewLine>    torch.manual_seed(args.seed)<NewLine>    tot_time = 0<NewLine>    first_epoch = 0<NewLine><NewLine>    for epoch in range(1, args.epochs + 1):<NewLine>        train_sampler.set_epoch(epoch)<NewLine>        start_cpu_secs = time.time()<NewLine>        train(model, optimizer, train_loader, epoch)<NewLine>        end_cpu_secs = time.time()<NewLine>        # print('start_cpu_secs {}'.format())<NewLine>        print(""Epoch {} of took {:.3f}s"".format(<NewLine>            epoch, end_cpu_secs - start_cpu_secs))<NewLine><NewLine>        tot_time += end_cpu_secs - start_cpu_secs<NewLine>        print('Current Total time : {:.3f}s'.format(tot_time))<NewLine>        if epoch == 1:<NewLine>            first_epoch = tot_time<NewLine><NewLine>    test_loss, accuracy = test(test_loader, model)<NewLine><NewLine>    if args.epochs &gt; 1:<NewLine>        average_epoch_time = float(tot_time - first_epoch) / (args.epochs - 1)<NewLine>        print('Average epoch time(ex. 1.) : {:.3f}s'.format(average_epoch_time))<NewLine>        print(""Total time : {:.3f}s"".format(tot_time))<NewLine>        if args.world_size &gt; 1:<NewLine>            summary_print(rank, test_loss, accuracy, average_epoch_time, tot_time)<NewLine><NewLine><NewLine>def init_processes(rank, world_size, fn, batch_size, backend='gloo'):<NewLine>    import os<NewLine>    os.environ['MASTER_ADDR'] = '10.0.3.29'<NewLine>    os.environ['MASTER_PORT'] = '9901'<NewLine>    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'<NewLine>    os.environ['NCCL_DEBUG'] = 'INFO'<NewLine>    os.environ['GLOO_SOCKET_IFNAME'] = 'enp0s31f6'<NewLine>    dist.init_process_group(backend=backend, world_size=world_size, rank=rank, init_method=""env://"")<NewLine>    fn(rank, batch_size, world_size)<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    init_processes(args.rank, args.world_size, run, args.batch_size, backend=args.backend)<NewLine>    torch.multiprocessing.set_start_method('spawn')<NewLine>    # processes = []<NewLine>    # for rank in range(1):<NewLine>    #     p = Process(target=init_processes,<NewLine>    #                 args=(rank, args.world_size, run, args.batch_size, args.backend))<NewLine>    #     p.start()<NewLine>    #     processes.append(p)<NewLine>    #<NewLine>    # for p in processes:<NewLine>    #     p.join()<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>check <code>ps -elf | grep python</code>, and see if you have any processes from previous runs that still have not been killed. Maybe they are occupying that port and are still alive.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thx for reply, no background process was found and the port was always available.</p><NewLine><p>I have fixed a typo in my command, where <code>--master_addr ='10.0.3.29' --master_port=9901</code> had been typed as <code>--master_addr ='10.0.3.29' --master_port='10.0.3.29'</code>.</p><NewLine><p>However the error has remained and at least one process has launched with the output</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""mnist-dist.py"", line 203, in &lt;module&gt;<NewLine>    init_processes(args.rank, args.world_size, run, args.batch_size, backend=args.backend)<NewLine>  File ""mnist-dist.py"", line 198, in init_processes<NewLine>    dist.init_process_group(backend=backend, world_size=world_size, rank=rank, init_method=""env://"")<NewLine>  File ""/home/dl/anaconda2/envs/torch-dist-py3.6/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 354, in init_process_group<NewLine>    store, rank, world_size = next(rendezvous(url))<NewLine>  File ""/home/dl/anaconda2/envs/torch-dist-py3.6/lib/python3.6/site-packages/torch/distributed/rendezvous.py"", line 143, in _env_rendezvous_handler<NewLine>    store = TCPStore(master_addr, master_port, start_daemon)<NewLine>RuntimeError: Address already in use<NewLine>dl-Server:10648:10648 [1] NCCL INFO NET : Using interface enp0s31f6:10.0.13.29&lt;0&gt;<NewLine>dl-Server:10648:10648 [1] NCCL INFO NET/IB : Using interface enp0s31f6 for sideband communication<NewLine>dl-Server:10648:10648 [1] NCCL INFO Using internal Network Socket<NewLine>dl-Server:10648:10648 [1] NCCL INFO NET : Using interface enp0s31f6:10.0.13.29&lt;0&gt;<NewLine>dl-Server:10648:10648 [1] NCCL INFO NET/Socket : 1 interfaces found<NewLine>NCCL version 2.3.7+cuda9.0<NewLine>dl-Server:10648:10648 [1] NCCL INFO rank 0 nranks 1<NewLine>dl-Server:10648:10675 [1] NCCL INFO comm 0x7fd38c00d3f0 rank 0 nranks 1<NewLine>dl-Server:10648:10675 [1] NCCL INFO CUDA Dev 1, IP Interfaces : enp0s31f6(PHB) <NewLine>dl-Server:10648:10675 [1] NCCL INFO Using 256 threads<NewLine>dl-Server:10648:10675 [1] NCCL INFO Min Comp Cap 6<NewLine>dl-Server:10648:10675 [1] NCCL INFO comm 0x7fd38c00d3f0 rank 0 nranks 1 - COMPLETE<NewLine>Train Epoch 1 - 0 / 234         Loss  5.576234<NewLine>Train Epoch 1 - 10 / 234        Loss  5.541703<NewLine>Train Epoch 1 - 20 / 234        Loss  5.515630<NewLine>Train Epoch 1 - 30 / 234        Loss  5.514045<NewLine>Train Epoch 1 - 40 / 234        Loss  5.485974<NewLine>Train Epoch 1 - 50 / 234        Loss  5.462833<NewLine>Train Epoch 1 - 60 / 234        Loss  5.422739<NewLine>Train Epoch 1 - 70 / 234        Loss  5.374931<NewLine>Train Epoch 1 - 80 / 234        Loss  5.342307<NewLine>Train Epoch 1 - 90 / 234        Loss  5.291063<NewLine>Train Epoch 1 - 100 / 234       Loss  5.220443<NewLine>Train Epoch 1 - 110 / 234       Loss  5.083968<NewLine>Train Epoch 1 - 120 / 234       Loss  5.002171<NewLine>Train Epoch 1 - 130 / 234       Loss  4.953607<NewLine>Train Epoch 1 - 140 / 234       Loss  4.894170<NewLine>Train Epoch 1 - 150 / 234       Loss  4.805832<NewLine>Train Epoch 1 - 160 / 234       Loss  4.792961<NewLine>Train Epoch 1 - 170 / 234       Loss  4.732522<NewLine>Train Epoch 1 - 180 / 234       Loss  4.770869<NewLine>Train Epoch 1 - 190 / 234       Loss  4.688779<NewLine>Train Epoch 1 - 200 / 234       Loss  4.725927<NewLine>Train Epoch 1 - 210 / 234       Loss  4.620460<NewLine>Train Epoch 1 - 220 / 234       Loss  4.605740<NewLine>Train Epoch 1 - 230 / 234       Loss  4.563363<NewLine>Epoch 1 of took 5.263s<NewLine>Current Total time : 5.263s<NewLine>Train Epoch 2 - 0 / 234         Loss  4.555817<NewLine>Train Epoch 2 - 10 / 234        Loss  4.603082<NewLine>Train Epoch 2 - 20 / 234        Loss  4.618500<NewLine>Train Epoch 2 - 30 / 234        Loss  4.520389<NewLine>Train Epoch 2 - 40 / 234        Loss  4.531864<NewLine>Train Epoch 2 - 50 / 234        Loss  4.467782<NewLine>Train Epoch 2 - 60 / 234        Loss  4.447100<NewLine>Train Epoch 2 - 70 / 234        Loss  4.424728<NewLine>Train Epoch 2 - 80 / 234        Loss  4.433639<NewLine>Train Epoch 2 - 90 / 234        Loss  4.372109<NewLine>Train Epoch 2 - 100 / 234       Loss  4.435561<NewLine>Train Epoch 2 - 110 / 234       Loss  4.351253<NewLine>Train Epoch 2 - 120 / 234       Loss  4.306677<NewLine>Train Epoch 2 - 130 / 234       Loss  4.343150<NewLine>Train Epoch 2 - 140 / 234       Loss  4.243150<NewLine>Train Epoch 2 - 150 / 234       Loss  4.347620<NewLine>Train Epoch 2 - 160 / 234       Loss  4.217095<NewLine>Train Epoch 2 - 170 / 234       Loss  4.255800<NewLine>Train Epoch 2 - 180 / 234       Loss  4.282191<NewLine>Train Epoch 2 - 190 / 234       Loss  4.249407<NewLine>Train Epoch 2 - 200 / 234       Loss  4.209113<NewLine>Train Epoch 2 - 210 / 234       Loss  4.194527<NewLine>Train Epoch 2 - 220 / 234       Loss  4.220213<NewLine>Train Epoch 2 - 230 / 234       Loss  4.201759<NewLine>Epoch 2 of took 5.524s<NewLine>Current Total time : 10.787s<NewLine>Train Epoch 3 - 0 / 234         Loss  4.158279<NewLine>Train Epoch 3 - 10 / 234        Loss  4.111032<NewLine>Train Epoch 3 - 20 / 234        Loss  4.147989<NewLine>Train Epoch 3 - 30 / 234        Loss  4.255434<NewLine>Train Epoch 3 - 40 / 234        Loss  4.111946<NewLine>Train Epoch 3 - 50 / 234        Loss  4.111733<NewLine>Train Epoch 3 - 60 / 234        Loss  4.176547<NewLine>Train Epoch 3 - 70 / 234        Loss  4.063233<NewLine>Train Epoch 3 - 80 / 234        Loss  4.079793<NewLine>Train Epoch 3 - 90 / 234        Loss  4.042555<NewLine>Train Epoch 3 - 100 / 234       Loss  4.050662<NewLine>Train Epoch 3 - 110 / 234       Loss  4.066662<NewLine>Train Epoch 3 - 120 / 234       Loss  4.090621<NewLine>Train Epoch 3 - 130 / 234       Loss  4.015823<NewLine>Train Epoch 3 - 140 / 234       Loss  4.092526<NewLine>Train Epoch 3 - 150 / 234       Loss  4.045942<NewLine>Train Epoch 3 - 160 / 234       Loss  4.048071<NewLine>Train Epoch 3 - 170 / 234       Loss  3.984233<NewLine>Train Epoch 3 - 180 / 234       Loss  3.942847<NewLine>Train Epoch 3 - 190 / 234       Loss  3.943717<NewLine>Train Epoch 3 - 200 / 234       Loss  3.959996<NewLine>Train Epoch 3 - 210 / 234       Loss  4.059554<NewLine>Train Epoch 3 - 220 / 234       Loss  3.918130<NewLine>Train Epoch 3 - 230 / 234       Loss  4.074725<NewLine>Epoch 3 of took 5.308s<NewLine>Current Total time : 16.095s<NewLine>Train Epoch 4 - 0 / 234         Loss  3.944645<NewLine>Train Epoch 4 - 10 / 234        Loss  3.923414<NewLine>Train Epoch 4 - 20 / 234        Loss  3.944232<NewLine>Train Epoch 4 - 30 / 234        Loss  3.978234<NewLine>Train Epoch 4 - 40 / 234        Loss  3.950741<NewLine>Train Epoch 4 - 50 / 234        Loss  3.913695<NewLine>Train Epoch 4 - 60 / 234        Loss  3.907088<NewLine>Train Epoch 4 - 70 / 234        Loss  4.026055<NewLine>Train Epoch 4 - 80 / 234        Loss  3.854659<NewLine>Train Epoch 4 - 90 / 234        Loss  3.954557<NewLine>Train Epoch 4 - 100 / 234       Loss  3.880200<NewLine>Train Epoch 4 - 110 / 234       Loss  3.911777<NewLine>Train Epoch 4 - 120 / 234       Loss  3.866536<NewLine>Train Epoch 4 - 130 / 234       Loss  3.957554<NewLine>Train Epoch 4 - 140 / 234       Loss  3.930515<NewLine>Train Epoch 4 - 150 / 234       Loss  3.950871<NewLine>Train Epoch 4 - 160 / 234       Loss  3.845739<NewLine>Train Epoch 4 - 170 / 234       Loss  3.905876<NewLine>Train Epoch 4 - 180 / 234       Loss  3.884211<NewLine>Train Epoch 4 - 190 / 234       Loss  4.034623<NewLine>Train Epoch 4 - 200 / 234       Loss  3.863284<NewLine>Train Epoch 4 - 210 / 234       Loss  3.899471<NewLine>Train Epoch 4 - 220 / 234       Loss  3.837218<NewLine>Train Epoch 4 - 230 / 234       Loss  3.862398<NewLine>Epoch 4 of took 5.271s<NewLine>Current Total time : 21.366s<NewLine>Train Epoch 5 - 0 / 234         Loss  3.878444<NewLine>Train Epoch 5 - 10 / 234        Loss  3.919256<NewLine>Train Epoch 5 - 20 / 234        Loss  3.872842<NewLine>Train Epoch 5 - 30 / 234        Loss  3.926296<NewLine>Train Epoch 5 - 40 / 234        Loss  3.787506<NewLine>Train Epoch 5 - 50 / 234        Loss  3.959824<NewLine>Train Epoch 5 - 60 / 234        Loss  3.830777<NewLine>Train Epoch 5 - 70 / 234        Loss  3.883856<NewLine>Train Epoch 5 - 80 / 234        Loss  3.877614<NewLine>Train Epoch 5 - 90 / 234        Loss  3.846863<NewLine>Train Epoch 5 - 100 / 234       Loss  3.908530<NewLine>Train Epoch 5 - 110 / 234       Loss  3.819784<NewLine>Train Epoch 5 - 120 / 234       Loss  3.798816<NewLine>Train Epoch 5 - 130 / 234       Loss  3.757388<NewLine>Train Epoch 5 - 140 / 234       Loss  3.837136<NewLine>Train Epoch 5 - 150 / 234       Loss  3.855000<NewLine>Train Epoch 5 - 160 / 234       Loss  3.821057<NewLine>Train Epoch 5 - 170 / 234       Loss  3.777124<NewLine>Train Epoch 5 - 180 / 234       Loss  3.714392<NewLine>Train Epoch 5 - 190 / 234       Loss  3.776406<NewLine>Train Epoch 5 - 200 / 234       Loss  3.886733<NewLine>Train Epoch 5 - 210 / 234       Loss  3.927509<NewLine>Train Epoch 5 - 220 / 234       Loss  3.719052<NewLine>Train Epoch 5 - 230 / 234       Loss  3.785564<NewLine>Epoch 5 of took 5.216s<NewLine>Current Total time : 26.582s<NewLine><NewLine>Test set : Average loss: 4.8523, Accuracy: 9453/10000 (94%)<NewLine><NewLine>Average epoch time(ex. 1.) : 5.330s<NewLine>Total time : 26.582s<NewLine><NewLine></code></pre><NewLine><p>Would you like to have a look <img alt="":grin:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/grin.png?v=6"" title="":grin:""/><img alt="":grin:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/grin.png?v=6"" title="":grin:""/><img alt="":grin:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/grin.png?v=6"" title="":grin:""/>?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Maybe the way I was using launch utility was to blame, do you have any idea? Was I suppose to type same command on the master node and other worker nodes?</p><NewLine><p>Any suggestion would be welcome, since I’ve been stuck for too long. <img alt="":zipper_mouth_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/zipper_mouth_face.png?v=6"" title="":zipper_mouth_face:""/><img alt="":zipper_mouth_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/zipper_mouth_face.png?v=6"" title="":zipper_mouth_face:""/><img alt="":zipper_mouth_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/zipper_mouth_face.png?v=6"" title="":zipper_mouth_face:""/></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/leo-mao"">@leo-mao</a> actually I have reproduced your issue, I am taking a look.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/leo-mao"">@leo-mao</a>, you should not set world_size and rank in torch.distributed.init_process_group, they are automatically set by torch.distributed.launch.</p><NewLine><p>So please change that to dist.init_process_group(backend=backend, init_method=“env://”)</p><NewLine><p>Also, you should not set WORLD_SIZE, RANK env variables in your code either since they will be set by launch utility.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>you are right, it works after I delete <code>rank</code> and <code>world_size</code> parameter in <code>torch.distributed.init_process_group</code>, thanks a lot</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I have met a situation which was little different with <a class=""mention"" href=""/u/leo-mao"">@leo-mao</a>. I want to train my model using one machine (node) which has multi-GPUs with torch.distributed. Since each time I just used 2 GPUs, I want to run several models at the same time. The problem is when I have started one model running with torch.distributed, others will get an error info "" RuntimeError: Address already in use "". I set the initial way as <a class=""mention"" href=""/u/teng-li"">@teng-li</a> says. Maybe I ignore something like port setting ? I am confused about it. I will be very appreciate if someone can give me some suggestions.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Got the same problem as <a class=""mention"" href=""/u/lausanne"">@Lausanne</a> has. Any thought into this?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/memray"">@memray</a> Hi, I have solved my questions. The reason why this bug happened is that two programme used the same port. So my solution is using random port in your command line.<br/><NewLine>For example, you can write your sh command as "" python -m torch.distributed.launch --nproc_per_node=$NGPUS --master_port=$RANDOM  train.py "". Just use random number to occupy port. Hope my finding can solve your problem.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am working on distributed.launch module recently, I have some question.</p><NewLine><ol><NewLine><li>I think with the launch and distributedDataParallel(model), you don’t need to average grads manually.<br/><NewLine>2.During your training, does your gpu0 have more memory usage than the other gpus? I found that the other gpus have extra memory usage in gpu0, it’s annoying.</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/zeal"">@zeal</a> Regarding 1, yes you don’t need to manually average gradients. Regarding 2, this is possible if you have some code that somehow uses GPU 0 at some time during execution of your program. This is not an invariant of using the distributed data parallel module.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s weird. I found out it seems like the Gpu cache release problem of pytorch. I add ‘torch.cuda.empty_cached’ in somewhere of my code and every gpu have same memory usage. But the program runs rather slower since the empty_cached was add in a for loop.<br/><NewLine>I still cannot found out what’s wrong. Does it in theory, if you use distributed training, every gpu will have the same memory usage? I know that if you use dataparallel module, the gpu0 will have more memory consumtion.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks. worked for me.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Worked for me! Thanks!! <img alt="":smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smile.png?v=9"" title="":smile:""/></p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Worked for me. Thanks.</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have tried to donot set the rank and world_size, but it shows that “ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set”</p><NewLine><pre><code>os.environ['MASTER_ADDR'] = '171.65.34.137'<NewLine>os.environ['MASTER_PORT'] = '2000901'<NewLine><NewLine>#dist.init_process_group(backend, rank=rank, world_size=size)<NewLine>dist.init_process_group(backend)<NewLine></code></pre><NewLine><p>Do you have any idea what’s this comes from?</p><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you please provide an example script to reproduce this, and the arguments that you’re passing in to DDP? thanks!</p><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi sorry for the late reply. I have solved the issues.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/leo-mao; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/leo-mao; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/leo-mao; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/teng-li; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/leo-mao; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Lausanne; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/memray; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Lausanne; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/zeal; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/zeal; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/cognitiverobot; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/Cyril-JZ; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/ginobilinie; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/Liangqiong_Qu; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/rvarm1; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/Liangqiong_Qu; <NewLine> ,"REPLY_DATE 1: December 26, 2018,  2:22am; <NewLine> REPLY_DATE 2: December 28, 2018, 11:21pm; <NewLine> REPLY_DATE 3: December 29, 2018,  9:20am; <NewLine> REPLY_DATE 4: December 29, 2018,  9:25am; <NewLine> REPLY_DATE 5: December 31, 2018,  1:27am; <NewLine> REPLY_DATE 6: January 7, 2019,  7:05am; <NewLine> REPLY_DATE 7: January 7, 2019,  7:07am; <NewLine> REPLY_DATE 8: February 10, 2019,  2:21am; <NewLine> REPLY_DATE 9: February 14, 2019,  4:43am; <NewLine> REPLY_DATE 10: February 26, 2019,  3:28am; <NewLine> REPLY_DATE 11: March 19, 2019,  3:09am; <NewLine> REPLY_DATE 12: March 19, 2019,  4:04pm; <NewLine> REPLY_DATE 13: March 22, 2019,  3:18am; <NewLine> REPLY_DATE 14: September 5, 2019, 12:07am; <NewLine> REPLY_DATE 15: November 25, 2019, 12:22pm; <NewLine> REPLY_DATE 16: December 16, 2019,  6:25pm; <NewLine> REPLY_DATE 17: February 11, 2020,  9:25pm; <NewLine> REPLY_DATE 18: February 12, 2020, 10:07pm; <NewLine> REPLY_DATE 19: February 15, 2020, 11:54pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 6 Likes; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: 2 Likes; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 4 Likes; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: 9 Likes; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> REPLY 18 LIKES: ; <NewLine> REPLY 19 LIKES: 1 Like; <NewLine> 
35273,DistributedSampler for validation set in ImageNet example,2019-01-22T04:41:00.590Z,1,738,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The <a href=""https://github.com/pytorch/examples/blob/master/imagenet/main.py"" rel=""nofollow noopener"">ImageNet example</a> has a DistributedSampler for the training loader, but not the validation loader. This would appear to have every rank processing the entire data for the validation set. Is this necessary, or could a DistributedSampler be used for the validation loader also, to apply the multiple nodes to processing the validation set?</p><NewLine></div>",https://discuss.pytorch.org/u/churchillmic,(Michael),churchillmic,"January 22, 2019,  4:41am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/churchillmic"">@churchillmic</a>,<br/><NewLine>I have the same query. Did you able to find the answer for this?</p><NewLine><p>Thanks<br/><NewLine>Anil</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found a couple examples where a DistributedSampler was used for the validation or test set. I’m still not sure why the official Imagenet example doesn’t use it, it still seems wasteful to me. Here are a few of the examples:<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/huggingface/pytorch-pretrained-BERT/blob/c9fd3505678d581388fb44ba1d79ac41e8fb28a4/examples/extract_features.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/huggingface/pytorch-pretrained-BERT/blob/c9fd3505678d581388fb44ba1d79ac41e8fb28a4/examples/extract_features.py"" rel=""nofollow noopener"" target=""_blank"">huggingface/pytorch-pretrained-BERT/blob/c9fd3505678d581388fb44ba1d79ac41e8fb28a4/examples/extract_features.py</a></h4><NewLine><pre><code class=""lang-py""># coding=utf-8<NewLine># Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.<NewLine>#<NewLine># Licensed under the Apache License, Version 2.0 (the ""License"");<NewLine># you may not use this file except in compliance with the License.<NewLine># You may obtain a copy of the License at<NewLine>#<NewLine>#     http://www.apache.org/licenses/LICENSE-2.0<NewLine>#<NewLine># Unless required by applicable law or agreed to in writing, software<NewLine># distributed under the License is distributed on an ""AS IS"" BASIS,<NewLine># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<NewLine># See the License for the specific language governing permissions and<NewLine># limitations under the License.<NewLine>""""""Extract pre-computed feature vectors from a PyTorch BERT model.""""""<NewLine><NewLine>from __future__ import absolute_import<NewLine>from __future__ import division<NewLine>from __future__ import print_function<NewLine><NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/huggingface/pytorch-pretrained-BERT/blob/c9fd3505678d581388fb44ba1d79ac41e8fb28a4/examples/extract_features.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/Jongchan/Pytorch-Horovod-Examples/blob/master/examples/cifar100/main_horovod.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/Jongchan/Pytorch-Horovod-Examples/blob/master/examples/cifar100/main_horovod.py"" rel=""nofollow noopener"" target=""_blank"">Jongchan/Pytorch-Horovod-Examples/blob/master/examples/cifar100/main_horovod.py</a></h4><NewLine><pre><code class=""lang-py"">from __future__ import print_function<NewLine><NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.init as init<NewLine>import torch.optim as optim<NewLine>import torch.nn.functional as F<NewLine>import torch.backends.cudnn as cudnn<NewLine>import config as cf<NewLine><NewLine>import torchvision<NewLine>import torchvision.transforms as transforms<NewLine>import torchvision.datasets as datasets<NewLine><NewLine>import os<NewLine>import sys<NewLine>import time<NewLine>import argparse<NewLine>import datetime<NewLine><NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/Jongchan/Pytorch-Horovod-Examples/blob/master/examples/cifar100/main_horovod.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It is not necessary to have every rank process the entire validation set. You can use a distributed sampler and average the errors afterwards to achieve the same result.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually, you cannot use ddp sampler to achieve validation. You can see <a href=""https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler"" rel=""nofollow noopener"">DistributedSampler</a>; note that the dataset has added extra samples to make it evenly divisible. Therefore, if your dataset is very small, the final result may be different. The official implementation is right.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/anil_batra; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/churchillmic; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Hzzone; <NewLine> ,"REPLY_DATE 1: March 3, 2019,  7:04am; <NewLine> REPLY_DATE 2: March 5, 2019,  3:15am; <NewLine> REPLY_DATE 3: March 5, 2019,  4:23pm; <NewLine> REPLY_DATE 4: June 15, 2020,  5:49am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
73761,Change rank of machines manually,2020-03-19T13:40:41.614Z,3,174,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am trying to deploy a crash-resilient distributed deployment in PyTorch. Assume that node of rank 0 controls the learning process. Now, if this node fails/crashes, I want node 1 to continue the job of node 0 and somehow notifies other nodes of the change. One way to do this is to assign the rank 0 to node 1 so that all nodes can communicate directly as usual with node 0 (which was node 1 before the change). Is there any way to do this?</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/aguirguis,(Arsany Guirguis),aguirguis,"March 19, 2020,  1:40pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/aguirguis"">@aguirguis</a>, if you are looking for elastic training for distributed data parallel, <a href=""https://github.com/pytorch/elastic"" rel=""nofollow noopener"">torchelastic</a> is built for this purpose. It will conduct re-rendezvous on living nodes when failure occurs.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>. <a class=""mention"" href=""/u/aguirguis"">@aguirguis</a>, here’s the quickstart guide for TorchElastic (<a href=""http://pytorch.org/elastic/0.2.0rc0/quickstart.html"" rel=""nofollow noopener"">http://pytorch.org/elastic/0.2.0rc0/quickstart.html</a>). If you are familiar with <code>torch.distributed.launch</code> things should look familiar. We’ve also written a kubernetes controller in collaboration with EKS, which you can check out here: <a href=""http://pytorch.org/elastic/0.2.0rc0/kubernetes.html"" rel=""nofollow noopener"">http://pytorch.org/elastic/0.2.0rc0/kubernetes.html</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> and <a class=""mention"" href=""/u/kiuk_chung"">@Kiuk_Chung</a> for your responses; they are really helpful.<br/><NewLine>I have two follow-up questions:</p><NewLine><ol><NewLine><li>According to this design, I have to run an etcd server, right? this is still a single point of failure. Is there any way to circumvent that?</li><NewLine><li>Is there any way to force some specific values for ranks to some specific nodes?</li><NewLine></ol><NewLine><p>Thanks a lot.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li><NewLine><p>You can run a fault tolerant etcd cluster by having etcd backed by multiple machines. You can find more information about it under the “Deployments” section in the FAQ page: <a href=""https://etcd.io/docs/v3.4.0/faq/"" rel=""nofollow noopener"">https://etcd.io/docs/v3.4.0/faq/</a></p><NewLine></li><NewLine><li><NewLine><p>Short answer is no. There are two ranks in elastic: 1. node_rank (its called GROUP_RANK), and 2. worker_rank (RANK). We don’t allow the ranks to be manually overridden because “sticky” ranks and elasticity do not play well together. Could you describe the use-case you have in mind? This way I can brainstorm a way to make things work without hardcoding ranks to specific nodes.</p><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/kiuk_chung"">@Kiuk_Chung</a> for your answers. I will check the etcd FAQ.<br/><NewLine>I’m thinking of a parameter server deployment with a crash tolerance to the central server. I want to deploy, let’s say, 2 servers so that if one crashes, the other one takes over. Yet, I want this to be transparent to the workers, i.e., they still send their gradients normally to the process with rank 0.<br/><NewLine>Is there an easy and cheap way to do this currently?</p><NewLine><p>Thanks!</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could do this using torch rpc. Say your parameter servers follow some naming convention like “ps:##” and you know the total ps replicas. Then you could round robin or fall back to the surviving parameter servers if an rpc call fails.</p><NewLine><p>How do you plan on keeping the data in the parameter servers replicated so that you are tolerant to ps failures?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, I was actually thinking of using rpc. My only concern is about its performance, compared to the other collective primitives (e.g., gather, broadcast…etc.). Can you comment on this performance comparison?</p><NewLine><p>The problem of consistency among PSes could be solved using checkpoints. Assume there is one primary PS, I’d let this PS only to update the model and periodically checkpoint it to some persistent database. If it crashes, the backup PS first loads the latest checkpoint and then continues training normally. How does that sound?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Kiuk_Chung; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/aguirguis; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Kiuk_Chung; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/aguirguis; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Kiuk_Chung; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/aguirguis; <NewLine> ,"REPLY_DATE 1: April 24, 2020,  4:23pm; <NewLine> REPLY_DATE 2: April 24, 2020,  4:23pm; <NewLine> REPLY_DATE 3: April 25, 2020,  2:48pm; <NewLine> REPLY_DATE 4: June 10, 2020, 12:34am; <NewLine> REPLY_DATE 5: June 10, 2020,  3:58pm; <NewLine> REPLY_DATE 6: June 12, 2020,  6:31am; <NewLine> REPLY_DATE 7: June 12, 2020,  3:40pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
84931,Where does the execution happen in the RPC-based parameter server tutorial?,2020-06-10T16:09:42.170Z,3,147,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>In this nice <a href=""https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html"" rel=""nofollow noopener"">tutorial</a>, it is described how to implement a parameter server (PS) deployment with RPC.<br/><NewLine>What confuses me is that it is not clear where does the execution (forward and backward passes) happen in this example…is it on the trainer machines or the PS machine?<br/><NewLine>It makes sense that these computations happen on the trainers yet, the forward pass is defined as a part of the PS code, which is called remotely by the trainer! would you please clarify this part?<br/><NewLine>Also, as far as I understand, usually, the PS collects the trainers’ gradients and aggregates them (for instance by averaging) to update the global model. Does this aggregation part exist in the tutorial? if yes, would you please explain where? if no, then what’s the strategy to incorporate all trainers’ work in the global model.<br/><NewLine>A follow-up question on that last point: how can I apply more sophisticated aggregation functions on the gradients (let’s say I want to take median instead of mean on the PS machine)…how can I do this?</p><NewLine><p>Thank you very much!</p><NewLine></div>",https://discuss.pytorch.org/u/aguirguis,(Arsany Guirguis),aguirguis,"June 10, 2020,  4:09pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/aguirguis"">@aguirguis</a></p><NewLine><blockquote><NewLine><p>What confuses me is that it is not clear where does the execution (forward and backward passes) happen in this example…is it on the trainer machines or the PS machine?</p><NewLine></blockquote><NewLine><p>The executions scattered on both trainer and PS:</p><NewLine><ul><NewLine><li>Forward pass: input (on trainer) -&gt; parameter (on PS) -&gt; output (on trainer) -&gt; loss (on trainer)</li><NewLine></ul><NewLine><p>So, for each trainer, there are three pieces of autograd graphs scattered on the trainer and the PS, and those are connected by RPC.</p><NewLine><blockquote><NewLine><p>It makes sense that these computations happen on the trainers yet, the forward pass is defined as a part of the PS code, which is called remotely by the trainer! would you please clarify this part?</p><NewLine></blockquote><NewLine><p>Yes. Both <code>TrainerNet</code> and <code>ParameterServer</code> have a <code>forward</code> function as they are both <code>nn.Module</code> subclasses. <code>TrainerNet</code> forward calls <code>ParameterServer</code> forward. An analogy would be if you have <code>nn.Sequential(nn.Linear(10, 10))</code>, both <code>Sequential</code> and <code>Linear</code> implement their own <code>forward</code> and <code>Sequential</code>'s forward calls <code>Linear</code>'s forward. This is also why the autograd graph scatters on both the trainer and the parameter server.</p><NewLine><blockquote><NewLine><p>Also, as far as I understand, usually, the PS collects the trainers’ gradients and aggregates them (for instance by averaging) to update the global model. Does this aggregation part exist in the tutorial?</p><NewLine></blockquote><NewLine><p>No, the aggregation is not part of the tutorial. Instead, it is doing sth similar to <a href=""https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf"" rel=""nofollow noopener"">Hogwild!</a> training. The aggregation method you mentioned are one type of synchronized training, where all trainers need to wait for that aggregation to finish before they can proceed. This works, and it is also possible to implement this synchronous aggregation using <code>torch.distributed.rpc</code>, but it won’t scale to large workload on large clusters, as any straggler will kill the performance.</p><NewLine><blockquote><NewLine><p>if no, then what’s the strategy to incorporate all trainers’ work in the global model.</p><NewLine></blockquote><NewLine><p>One option would be letting the PS to send parameters to trainers, and then get back gradients. Then, the PS aggregates the grads as you mentioned above, and updates its parameters. The PS can keep doing this until the model converges. This is similar to this <a href=""https://pytorch.org/tutorials/intermediate/rpc_tutorial.html#distributed-reinforcement-learning-using-rpc-and-rref"" rel=""nofollow noopener"">RL tutorial</a> where the agent tells observers what to do and the agent owns the policy model.</p><NewLine><blockquote><NewLine><p>A follow-up question on that last point: how can I apply more sophisticated aggregation functions on the gradients (let’s say I want to take median instead of mean on the PS machine)…how can I do this?</p><NewLine></blockquote><NewLine><p>If adopting the RL tutorial above, then the PS can collect the gradients from different trainers in a list and compute the medium accordingly.</p><NewLine><p>cc the author of the PS tutorial <a class=""mention"" href=""/u/rvarm1"">@rvarm1</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> for your detailed answers. I have a few follow-up questions:</p><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""84931"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>input (on trainer) -&gt; parameter (on PS) -&gt; output (on trainer)</p><NewLine></blockquote><NewLine></aside><NewLine><p>Does this mean the input is sent to the PS first to propagate through the network parameters in the PS and then at the end, the output is sent back to the trainer?<br/><NewLine>What confuses me here is that it seems that the <code>forward</code> function of <code>TrainerNet</code> is just dummy and all what it does is calling that of <code>ParameterServer</code>. As far as I understand, in the PS architecture, data never leaves the trainer machine and that the whole gradient computation process should be done entirely locally on the trainer machine.<br/><NewLine>If you can describe all the communication that happens in one training iteration, that would be great. For instance, assume that we have one PS machine and two trainer machines. PS has the model and each trainer has a few data samples. What is sent to whom?<br/><NewLine>Hogwild! assumes shared memory so, the setup is inherently different from that of the PS, right? I cannot entirely digest how/why do you blend these two setups. Would you please clarify?</p><NewLine><p>Thanks a lot.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""84931"" data-username=""aguirguis""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/aguirguis/40/21870_2.png"" width=""20""/> aguirguis:</div><NewLine><blockquote><NewLine><p>Does this mean the input is sent to the PS first to propagate through the network parameters in the PS and then at the end, the output is sent back to the trainer?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes.</p><NewLine><blockquote><NewLine><p>What confuses me here is that it seems that the  <code>forward</code>  function of  <code>TrainerNet</code>  is just dummy and all what it does is calling that of  <code>ParameterServer</code> .</p><NewLine></blockquote><NewLine><p>Yes, you are right. In this specific case, as <code>x</code> does not require grad, there is no need to link it to the distributed autograd graph. So there are only two pieces of autograd graph on PS and the trainer. (I was wrong when saying there three pieces in previous comments.)</p><NewLine><blockquote><NewLine><p>As far as I understand, in the PS architecture, data never leaves the trainer machine and that the whole gradient computation process should be done entirely locally on the trainer machine.</p><NewLine></blockquote><NewLine><p>There are different ways to implement this. Imagine there is a super large embedding table and the trainer only holds a several lookup indices in each iteration. One solution is to do training all on trainer, but then the application will need to implement update functions that converts indices and gradients from the trainer back to the embedding table gradients. Another option is let autograd engine taking care of this, and simply calling <code>loss.backward()</code> on trainer will be sufficient to update embedding table on ps.</p><NewLine><blockquote><NewLine><p>If you can describe all the communication that happens in one training iteration, that would be great. For instance, assume that we have one PS machine and two trainer machines. PS has the model and each trainer has a few data samples. What is sent to whom?If you can describe all the communication that happens in one training iteration, that would be great. For instance, assume that we have one PS machine and two trainer machines. PS has the model and each trainer has a few data samples. What is sent to whom?</p><NewLine></blockquote><NewLine><p>Sure. Since trainers are independent in that tutorial IIUC, I will only describe what happens between a PS-trainer pair.</p><NewLine><p>In forward pass, there are two comms: 1) trainer -&gt; ps to send input sample 2) ps -&gt; trainer to send the output<br/><NewLine>In the backward pass, there is one comm: trainer -&gt; ps to send the gradients for the model outputs, which will then trigger local autograd engine on the ps to compute gradients on the model.<br/><NewLine>In the optimizer step pass, there is one comm: trainer -&gt; ps tell the local optimizer on ps to update model parameters. (It is possible to pack this into the comm during the backward pass using hooks.)</p><NewLine><p>Since there are two trainers accessing the same model, instead of storing the grads in <code>param.grad</code>, the ps will put those grads in dedicated contexts associated with each distributed autograd context, and those grads will later be consumed by the distributed optimizer.</p><NewLine><p>More details about dist autograd can be found here: <a href=""https://pytorch.org/docs/stable/rpc/distributed_autograd.html#distributed-autograd-design"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/rpc/distributed_autograd.html#distributed-autograd-design</a></p><NewLine><blockquote><NewLine><p>Hogwild! assumes shared memory so, the setup is inherently different from that of the PS, right? I cannot entirely digest how/why do you blend these two setups. Would you please clarify?</p><NewLine></blockquote><NewLine><p>Right, the original paper was mainly focusing on shm. But the lock-free spirit can be apply to distributed training as well. This is especially useful for training using large dataset with large embedding tables.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> for your detailed answers. Now, everything is clear to me.<br/><NewLine>Probably a better design for my use case is to use local optimizer and autograd as in the RL tutorial you referred to before.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/aguirguis; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/aguirguis; <NewLine> ,"REPLY_DATE 1: June 12, 2020,  3:32pm; <NewLine> REPLY_DATE 2: June 11, 2020, 12:14pm; <NewLine> REPLY_DATE 3: June 11, 2020,  6:06pm; <NewLine> REPLY_DATE 4: June 12, 2020,  3:32pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
84938,Sending a bulk data by RPC,2020-06-10T16:48:14.163Z,11,288,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I am using RPC to send and receive data with multiple CPUs.<br/><NewLine>Currently, I use “gloo” backend.<br/><NewLine>My code is:</p><NewLine><pre><code class=""lang-auto"">rpc.rpc_async(my_target, add_outlinks, args=(arr_send[i],source))<NewLine></code></pre><NewLine><p>I have nearly 1 million objects.<br/><NewLine>For each object, I send to (size-1) workers to run the function add_outlinks.</p><NewLine><p>Totally, with 1 million objects, we have to send 1*(size-1) times.<br/><NewLine>arr_send[i] is from 10 to 1 000 000 numbers.</p><NewLine><p>I run the same function on UPC++, It takes around several minutes. However, with torch-RPC, I set timeout to 10000s ~ 166 minutes, and the program is topped by timeout.</p><NewLine><p>Could you tell me the best way to send data with Pytorch?</p><NewLine><p>Thanks,</p><NewLine></div>",https://discuss.pytorch.org/u/ph0123,(chau phuong),ph0123,"June 10, 2020,  4:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to consolidate multiple <code>arr_send[i]</code> into one RPC? Is there any reason that they need to be sent using dedicated RPCs?</p><NewLine><p>Curious:</p><NewLine><ol><NewLine><li>After 10000s, how many objects are processed?</li><NewLine><li>Is distributed autograd/optimizer also used in this case?</li><NewLine><li>Are any of those CPUs locate on the same machine? (so that shm can be helpful)</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I use as</p><NewLine><pre><code class=""lang-auto"">def add_outlinks(arr, source):<NewLine>    for dest in arr:<NewLine>        if int(dest) in _local_dict:<NewLine>            _local_dict[dest].in_links.append(int(source))<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">    rpc.init_rpc(my_name, rank=rank, world_size=size,rpc_backend_options=rpc.ProcessGroupRpcBackendOptions(num_send_recv_threads=16,rpc_timeout=datetime.timedelta(seconds=10000)))  # initial_rpc<NewLine><NewLine>    #CALL rpc TO OTHER RANKS<NewLine>    if rank==0:<NewLine>        print(""add-link..."")<NewLine>    try:<NewLine>        array_rpc = list(range(0, size))<NewLine>        count=0<NewLine>        for it in _local_dict:<NewLine>            count = count+1<NewLine>            arr_send = []<NewLine>            for i in range(0, size):<NewLine>                arr_send.append([])<NewLine>            u = _local_dict[it]<NewLine>            source = u.vertexId<NewLine><NewLine>            for i in u.links:<NewLine>                arr_send[int(i) % size].append(int(i))<NewLine>            for i in array_rpc:<NewLine>                my_target = ""worker"" + str(i)<NewLine>                if len(arr_send[i])&gt;0:<NewLine>                    rpc.rpc_async(my_target, add_outlinks, args=(arr_send[i],source))<NewLine>    except:<NewLine>	   print(""rank "",rank,"" run "",count,""/"",len(_local_dict))<NewLine>    rpc.api._wait_all_workers()<NewLine>    print(""shutdown.... rpc... "", rank)<NewLine>    rpc.api._wait_all_workers()<NewLine>    rpc.shutdown()<NewLine><NewLine></code></pre><NewLine><ul><NewLine><li>arr_send[i] will send to rank i<br/><NewLine>For elements in _local_dict, we can run parallel.</li><NewLine></ul><NewLine><p><strong>1.     After 10000s, how many objects are processed?</strong></p><NewLine><p>–&gt; the outputs are as below. The worker0 is not see in the output. I try to print “count” value. But there is no output for “count” variable.</p><NewLine><pre><code class=""lang-auto"">....<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:45462<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:44970<NewLine>....<NewLine><NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:19635<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:27553<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:44970<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:566] Read error [2001:700:4a01:10::38]:47501: Connection reset by peer<NewLine>Traceback (most recent call last):<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap<NewLine>    self.run()<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/multiprocessing/process.py"", line 93, in run<NewLine>    self._target(*self._args, **self._kwargs)<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>  File ""pagerank.py"", line 380, in init_process<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:566] Read error [2001:700:4a01:10::38]:44931: Connection reset by peer<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/site-packages/torch/distributed/rpc/api.py"", line 77, in wrapper<NewLine>    return func(*args, **kwargs)<NewLine>...<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:378] writev [2001:700:4a01:10::38]:22942: Connection reset by peer<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/site-packages/torch/distributed/rpc/api.py"", line 240, in shutdown<NewLine>    _wait_all_workers()<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/site-packages/torch/distributed/rpc/api.py"", line 77, in wrapper<NewLine>    return func(*args, **kwargs)<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:566] Read error [2001:700:4a01:10::38]:5848: Connection reset by peer<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:566] Read error [2001:700:4a01:10::38]:50095: Connection reset by peer<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:29331<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:566] Read error [2001:700:4a01:10::38]:57022: Connection reset by peer<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/site-packages/torch/distributed/rpc/api.py"", line 165, in _wait_all_workers<NewLine>    args=(sequence_id, self_worker_name,),<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:15236<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:2720<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:23214<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:38547<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:50607<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/site-packages/torch/distributed/rpc/api.py"", line 77, in wrapper<NewLine>    return func(*args, **kwargs)<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>...<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:12173<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/site-packages/torch/distributed/rpc/api.py"", line 554, in rpc_sync<NewLine>    return fut.wait()<NewLine>RuntimeError: Encountered exception in ProcessGroupAgent::enqueueSend: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:566] Read error [2001:700:4a01:10::38]:57022: Connection reset by peer<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:3715<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:2693<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine><NewLine>During handling of the above exception, another exception occurred:<NewLine><NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:9877<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:3715<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>Traceback (most recent call last):<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:378] writev [2001:700:4a01:10::38]:22942: Connection reset by peer<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:566] Read error [2001:700:4a01:10::38]:36780: Connection reset by peer<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap<NewLine>    self.run()<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:566] Read error [2001:700:4a01:10::38]:47501: Connection reset by peer<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:566] Read error [2001:700:4a01:10::38]:44931: Connection reset by peer<NewLine><NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:566] Read error [2001:700:4a01:10::38]:3213: Connection reset by peer<NewLine>Traceback (most recent call last):<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap<NewLine>    self.run()<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:13723<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/multiprocessing/process.py"", line 93, in run<NewLine>    self._target(*self._args, **self._kwargs)<NewLine>  File ""pagerank.py"", line 380, in init_process<NewLine>    print(""shutdown.... rpc... "", rank)<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:9140<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/site-packages/torch/distributed/rpc/api.py"", line 77, in wrapper<NewLine>    return func(*args, **kwargs)<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/site-packages/torch/distributed/rpc/api.py"", line 240, in shutdown<NewLine>    _wait_all_workers()<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/site-packages/torch/distributed/rpc/api.py"", line 77, in wrapper<NewLine>    return func(*args, **kwargs)<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/site-packages/torch/distributed/rpc/api.py"", line 165, in _wait_all_workers<NewLine>    args=(sequence_id, self_worker_name,),<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/site-packages/torch/distributed/rpc/api.py"", line 77, in wrapper<NewLine>    return func(*args, **kwargs)<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/site-packages/torch/distributed/rpc/api.py"", line 554, in rpc_sync<NewLine>    return fut.wait()<NewLine><NewLine>RuntimeError: Encountered exception in ProcessGroupAgent::enqueueSend: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:566] Read error [2001:700:4a01:10::38]:9889: Connection reset by peer<NewLine><NewLine>During handling of the above exception, another exception occurred:<NewLine><NewLine>Traceback (most recent call last):<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>  File ""/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap<NewLine>    self.run()<NewLine>KeyboardInterrupt<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: Application timeout caused pair closure<NewLine>[E thread_pool.cpp:112] Exception in thread pool task: [/opt/conda/conda-bld/pytorch_1587428228634/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [2001:700:4a01:10::38]:34716<NewLine><NewLine><NewLine><NewLine></code></pre><NewLine><p>I changed the code to print count as</p><NewLine><pre><code class=""lang-auto"">for it in _local_dict:<NewLine>     if(rank==0:)<NewLine>            count = count+1<NewLine>           print(count)<NewLine><NewLine></code></pre><NewLine><p>–&gt; all elements in _local_dict is run. However, the program stopped by timeout.</p><NewLine><p><strong>2.     Is distributed autograd/optimizer also used in this case?</strong></p><NewLine><p>–&gt; Not yet. It is at <a href=""https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html</a> ??</p><NewLine><p><strong>3.     Are any of those CPUs locate on the same machine? (so that shm can be helpful)</strong><br/><NewLine>–&gt;  I use 32 CPUs on this machine. (All of my CPU on the machine.)</p><NewLine><p>Thanks</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I try with small data (~60 elements in _local_dict), and it worked.<br/><NewLine>Perhaps, there are problems with a lot of rpc call?</p><NewLine><p>For real data, size of _local_dict is nearly 190000 elements - 32 workers. With each element, I called 32 times for rpc(…).</p><NewLine><p>For each worker, we have to call 190000<em>32 times for rpc(…). We have 32 workers. Hence, totally, there are 190000</em>32*32 times for calling RPC.</p><NewLine><p>Is there any problem with that?</p><NewLine><p>Thanks,</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I delete RPC and try to run with the local</p><NewLine><pre><code class=""lang-auto"">_local_dict<NewLine></code></pre><NewLine><p>each worker holds 1/32 of total objects. Then, I run the program. It takes nearly 5s.<br/><NewLine>However, It can not finish with RPC (timeout=10000s).</p><NewLine><p>Perhaps, the problem has belonged to the Queue size of RPC backend.</p><NewLine><p>Please help!<br/><NewLine>Thanks,</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I add future variable for the rpc.rpc_async.<br/><NewLine>It worked. However, it is very slow.<br/><NewLine>It took around 30 minutes to finish this procedure.</p><NewLine><aside class=""quote no-group quote-modified"" data-post=""3"" data-topic=""84938"" data-username=""ph0123""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ph0123/40/22888_2.png"" width=""20""/> ph0123:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto""> for it in _local_dict:<NewLine>            count = count+1<NewLine>            arr_send = []<NewLine>            for i in range(0, size):<NewLine>                arr_send.append([])<NewLine>            u = _local_dict[it]<NewLine>            source = u.vertexId<NewLine><NewLine>            for i in u.links:<NewLine>                arr_send[int(i) % size].append(int(i))<NewLine>            futs = [] #add here<NewLine>            for i in array_rpc:<NewLine>                my_target = ""worker"" + str(i)<NewLine>                if len(arr_send[i])&gt;0:<NewLine>                    futs.append(rpc.rpc_async(my_target, add_outlinks, args=(arr_send[i],source)))<NewLine>            for fut in futs: #add here<NewLine>                fut.wait() #add here<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>I try to put</p><NewLine><pre><code class=""lang-auto""> for fut in futs: #add here<NewLine>                fut.wait() #add here<NewLine></code></pre><NewLine><p>outside of the loop. The program is not finished and stopped by time out.</p><NewLine><p>Could you explain why? and could you please suggest some ways to decrease the excuse time? I run the program with UPC++, it take some minutes to finish (~ 5-6 minutes)</p><NewLine><p>Thanks</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry about the delay.</p><NewLine><p><code>rpc.api._wait_all_workers()</code> is not supposed to be directly called by applications. I was only referencing it as one example to use rank 0 as a coordinator. The recommended way is doing sth like below:</p><NewLine><pre><code class=""lang-python"">futs = []<NewLine>for i in array_rpc:<NewLine>    futs.append(rpc.rpc_async(my_target, add_outlinks, args=(arr_send[i],source)))<NewLine><NewLine>...<NewLine><NewLine>for fut in futs:<NewLine>    fut.wait()<NewLine></code></pre><NewLine><p>Allow me some time, still reading the contents above.</p><NewLine><h2>Edits</h2><NewLine><p>I see you are already using future in the latest version.</p><NewLine><blockquote><NewLine><p>Could you explain why? and could you please suggest some ways to decrease the excuse time? I run the program with UPC++, it take some minutes to finish (~ 5-6 minutes)</p><NewLine></blockquote><NewLine><p>Given that the comp only takes 5s, it looks like pickle and comm takes the majority of the time? Curious, can you profile the time taken for one <code>rpc_sync</code>?</p><NewLine><p>The communication pattern above looks like an allToAll. Is it guaranteed that the data sent from rank x to rank y will be of the same size. If so, you can use the <a href=""https://pytorch.org/docs/master/distributed.html#torch.distributed.all_to_all"" rel=""nofollow noopener""><code>allToAll API</code></a> (not in v1.5, only available on master now, will come to v1.6).</p><NewLine><p><s>Regarding ways to speed up, one easy attempt would be increasing the number of threads. The program currently sets <code>num_send_recv_threads=16</code>. You could try 64. This could help speeding up deserialization on the server side. But the serializer on the sender side still occurs inline, we probably should also offload that to thread-pool in future releases.</s></p><NewLine><p>I take that back. I think one major reason is that the pickle and execution both require Python GIL, and hence they won’t run in parallel on the server side. Can you try using <code>TorchScript</code> functions? You can find examples <a href=""https://pytorch.org/docs/stable/rpc.html"" rel=""nofollow noopener"">here</a> by searching for <code>@torch.jit.script</code>. Even if we use <code>@torch.jit.script</code> functions, the pickle/unpickle would still require GIL as those are Python objects, but the execution can run in parallel.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""84938"" data-username=""ph0123""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ph0123/40/22888_2.png"" width=""20""/> ph0123:</div><NewLine><blockquote><NewLine><p>the outputs are as below. The worker0 is not see in the output. I try to print “count” value. But there is no output for “count” variable.</p><NewLine></blockquote><NewLine></aside><NewLine><p>It means the program didn’t git the <code>except</code> branch below:</p><NewLine><pre><code class=""lang-python"">    except:<NewLine>	   print(""rank "",rank,"" run "",count,""/"",len(_local_dict))<NewLine></code></pre><NewLine><p>If there are exceptions occur when processing the RPC in the try block, it will be thrown locally when you call <code>fut.wait()</code> or <code>rref.to_here()</code>.</p><NewLine><p>Given the above code, the timeout won’t occur in the try block, as <code>rpc_async</code> always returns immediately with a future object.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""84938"" data-username=""ph0123""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ph0123/40/22888_2.png"" width=""20""/> ph0123:</div><NewLine><blockquote><NewLine><p>I use 32 CPUs on this machine. (All of my CPU on the machine.)</p><NewLine></blockquote><NewLine></aside><NewLine><p>In this case, the new TensorPipe (coming in v1.6) RPC backend might be useful, as it provides a shm channel to send RPC data. Will keep you posted on that.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your responses.</p><NewLine><p>I will wait the 1.6 version and try to work with rpc.ref.<br/><NewLine>I will compare and share with you about the results.</p><NewLine><p>Thanks,</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ph0123"">@ph0123</a></p><NewLine><p>Thanks. Regarding the <code>TorchScript</code> functions, double checked with the team, it’s only the serialization on the caller side is holding GIL as of today. The derserialization on the callee can run in parallel. Curious to see how much <code>TorchScript</code> functions can help.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I try torch.jit.script as</p><NewLine><pre><code class=""lang-auto"">@torch.jit.script<NewLine>def add_outlinks(arr, source):<NewLine>    for dest in arr:<NewLine>        if _local_dict.get(dest):<NewLine>            _local_dict[dest].in_links.append(int(source))<NewLine></code></pre><NewLine><p>On my program, I defined that _loca_dict is a global variable. Therefore, there is no error here. When I add “<span class=""mention"">@torch.jit.script</span>”. The terminal shown the error as</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""/mnt/c/python_project/Pytorch/test.py"", line 222, in &lt;module&gt;<NewLine>    @torch.jit.script<NewLine>  File ""/home/cnphuong/.local/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1290, in script<NewLine>    fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))<NewLine>RuntimeError: <NewLine>attribute lookup is not defined on python value of type 'dict':<NewLine>  File ""/mnt/c/python_project/Pytorch/test.py"", line 225<NewLine>def add_outlinks(arr, source):<NewLine>    for dest in arr:<NewLine>        if _local_dict.get(dest):<NewLine>           ~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>            _local_dict[dest].in_links.append(int(source))<NewLine></code></pre><NewLine><p>I have to install pytorch from source or not?<br/><NewLine>My pytorch version is 1.5 and cpu only.</p><NewLine><p>Thanks,</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>That probably means <code>TorchScript</code> does not support <s><code>dict.get</code></s> global variable yet.</p><NewLine><p>cc JIT expert <a class=""mention"" href=""/u/michael_suo"">@Michael_Suo</a></p><NewLine><p>Let me try this locally.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just check with JIT team, <code>TorchScript</code> functions does not support global variables. One alternative is to do it through <code>RRef.local_value()</code>. Let me prepare an example for you.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>thank you so much!<br/><NewLine>Best regards,</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ph0123"">@ph0123</a></p><NewLine><p>I tried this <a href=""https://github.com/pytorch/pytorch/pull/39900"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/39900</a></p><NewLine><p>But it actually exposes another gap in <code>TorchScript</code> function type annotation. JIT team is investigating.</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Fixes are coming to master:</p><NewLine><aside class=""onebox githubpullrequest""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/39932"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Pull Request""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 12 16"" width=""60""><path d=""M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/pull/39932"" rel=""nofollow noopener"" target=""_blank"">[rpc] use annotation_str for RRef type serialization</a><NewLine></h4><NewLine><div class=""branches""><NewLine><code>pytorch:gh/wanchaol/111/base</code> ← <code>pytorch:gh/wanchaol/111/head</code><NewLine></div><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-06-12"" data-format=""ll"" data-time=""06:49:30"" data-timezone=""UTC"">06:49AM - 12 Jun 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/wanchaol"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""wanchaol"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/9443650?v=4"" width=""20""/><NewLine>          wanchaol<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""1 commits changed 2 files with 3 additions and 3 deletions""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/39932/files"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+3</span><NewLine><span class=""removed"">-3</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine><aside class=""onebox githubpullrequest""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/39933"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Pull Request""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 12 16"" width=""60""><path d=""M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/pull/39933"" rel=""nofollow noopener"" target=""_blank"">[rpc] fix RRef alias annotation</a><NewLine></h4><NewLine><div class=""branches""><NewLine><code>pytorch:gh/wanchaol/112/base</code> ← <code>pytorch:gh/wanchaol/112/head</code><NewLine></div><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-06-12"" data-format=""ll"" data-time=""06:49:39"" data-timezone=""UTC"">06:49AM - 12 Jun 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/wanchaol"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""wanchaol"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/9443650?v=4"" width=""20""/><NewLine>          wanchaol<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""1 commits changed 2 files with 24 additions and 3 deletions""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/39933/files"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+24</span><NewLine><span class=""removed"">-3</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 10, 2020,  6:27pm; <NewLine> REPLY_DATE 2: June 10, 2020,  9:58pm; <NewLine> REPLY_DATE 3: June 10, 2020, 10:04pm; <NewLine> REPLY_DATE 4: June 11, 2020,  8:33am; <NewLine> REPLY_DATE 5: June 11, 2020,  1:19pm; <NewLine> REPLY_DATE 6: June 11, 2020,  4:02pm; <NewLine> REPLY_DATE 7: June 11, 2020,  3:22pm; <NewLine> REPLY_DATE 8: June 11, 2020,  3:26pm; <NewLine> REPLY_DATE 9: June 11, 2020,  7:09pm; <NewLine> REPLY_DATE 10: June 11, 2020,  7:13pm; <NewLine> REPLY_DATE 11: June 11, 2020,  8:07pm; <NewLine> REPLY_DATE 12: June 11, 2020,  9:02pm; <NewLine> REPLY_DATE 13: June 11, 2020,  9:11pm; <NewLine> REPLY_DATE 14: June 11, 2020,  9:29pm; <NewLine> REPLY_DATE 15: June 12, 2020,  1:50am; <NewLine> REPLY_DATE 16: June 12, 2020,  2:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: 1 Like; <NewLine> REPLY 13 LIKES: 1 Like; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: 1 Like; <NewLine> REPLY 16 LIKES: ; <NewLine> 
84503,Multi model and multi forward in distirbuted data parallel,2020-06-07T08:59:29.511Z,8,196,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Q1: If I have two models named A and B, both wrapped with DDP,  and loss = A(B(inputs)), will DDP work? It seems that gradients will be sync when loss.backward() is called.</p><NewLine><p>Q2: If loss = A(B(inputs1), B(inputs2)), will DDP work ? The forward funciton of B is called twice . btw, I don’t know what does reducer.prepare_for_backward do…</p><NewLine></div>",https://discuss.pytorch.org/u/zhouzh,(zhihaozhou),zhouzh,"June 7, 2020,  8:59am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""84503"" data-username=""zhouzh""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/z/919ad9/40.png"" width=""20""/> zhouzh:</div><NewLine><blockquote><NewLine><p>Q1: If I have two models named A and B, both wrapped with DDP, and loss = A(B(inputs)), will DDP work?</p><NewLine></blockquote><NewLine></aside><NewLine><p>It should work. This is using the output from <code>B(inputs)</code> to connect two graphs together. The <code>AllReduce</code> communication from A and B won’t run interleavingly I think. If it hangs somehow, you could trying setting the <a href=""https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel"" rel=""nofollow noopener""><code>process_group</code></a> argument of two DDP instances to different <code>ProcessGroup</code> objects created using the <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.new_group"" rel=""nofollow noopener""><code>new_group</code></a> API. This will fully decouple the communication of A and B.</p><NewLine><blockquote><NewLine><p>It seems that gradients will be sync when loss.backward() is called.</p><NewLine></blockquote><NewLine><p>Yes, see this page for more detail: <a href=""https://pytorch.org/docs/stable/notes/ddp.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/notes/ddp.html</a></p><NewLine><blockquote><NewLine><p>Q2: If loss = A(B(inputs1), B(inputs2)), will DDP work ? The forward funciton of B is called twice . btw, I don’t know what does reducer.prepare_for_backward do…</p><NewLine></blockquote><NewLine><p>This won’t work. DDP requires forward and backward to run alternatively. The above code would run forward on B twice before one backward, which would mess up DDP internal states. However, the following would work. Suppose the local module wrapped by B is C</p><NewLine><pre><code class=""lang-python""><NewLine>class Wrapper(nn.Module):<NewLine>    def __init__(self):<NewLine>        self.c = C()<NewLine><NewLine>    def forward(inputs):<NewLine>        return self.c(inputs[0]), self.c(inputs[1])<NewLine><NewLine>B = DistributedDataParallel(Wrapper(), ...)<NewLine><NewLine>loss = A(B([input21, inputs2]))<NewLine></code></pre><NewLine><p>This is basically using a sheer wrapper over C to process two inputs in one forward call.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I have a different but related problem.<br/><NewLine>I have a detection model with unfixed input size. At some extreme case, the RuntimeError with OOM occurs, so I wrap the forward+backward around try+except</p><NewLine><pre><code class=""lang-auto"">for images, targets in data_loader:<NewLine>        images = images.to(device)<NewLine>        targets = [target.to(device) for target in targets]<NewLine>        try:<NewLine>            loss_dict = model(images, targets)<NewLine>            losses = sum(loss for loss in loss_dict.values())<NewLine>            optimizer.zero_grad()<NewLine>            losses .backward()<NewLine>        except Exception as ex:<NewLine>            torch.cuda.ipc_collect()<NewLine>            torch.cuda.empty_cache()<NewLine>            continue<NewLine></code></pre><NewLine><p>this would help to some extend, until:</p><NewLine><blockquote><NewLine><p>RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing its output (the return value of <code>forward</code>). You can enable unused parameter detection by passing the keyword argument <code>find_unused_parameters=True</code> to <code>torch.nn.parallel.DistributedDataParallel</code>. If you already have this argument set, then the distributed data parallel module wasn’t able to locate the output tensors in the return value of your module’s <code>forward</code> function. Please include the structure of the return value of <code>forward</code> of your module when reporting this issue (e.g. list, dict, iterable). (prepare_for_backward at /opt/conda/conda-bld/pytorch_1556653099582/work/torch/csrc/distributed/c10d/reducer.cpp:408)</p><NewLine></blockquote><NewLine><p>To be clear, I already set find_unused_parameters=True, and broadcast_buffers=False. My guess is some internal state was strained somehow. Is it possible to reset all the state during my except catch?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/qianyizhang"">@qianyizhang</a></p><NewLine><p>Yes, the error is expected. Because, say you have two DDP processes, X and Y. If process X hits a OOM error in the forward pass of one iteration but Y runs correctly, as a result, X would skip its backward pass in that iteration causing a de-synchronization. DDP itself cannot recover from this error.</p><NewLine><p>However, <a href=""https://pytorch.org/elastic"" rel=""nofollow noopener"">torchelastic</a> is built to solve this exact problem. It would kill the entire DDP gang, reconstruct a new DDP gang, and revert to the previous checkpoint when such OOM occurs. cc <a class=""mention"" href=""/u/kiuk_chung"">@Kiuk_Chung</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Adding a bit more context to <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>’s comments, you <em>could</em> try to reset the DDP state by calling <code>destroy_process_group()</code> and re-initializing it, however that doesn’t guarantee that your tensors (distributed among multiple workers) are also reset. In short, a complete state reset on the worker is application dependent (and often non-trivial). For transient exceptions you can use torchelastic to launch your workers, and just let the worker process throw the exception out and fail. Elastic will monitor the worker pids and will restart the world if it detects that one (or more) workers have failed. Note, that due to this behavior, (assuming you have checkpoints) you will lose progress between checkpoints.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/kiuk_chung"">@Kiuk_Chung</a>  Can your elaborate more on the last part?<br/><NewLine>My goal is to keep the training process going with minimum withdraw from failed synchronization.<br/><NewLine>I assume your elastic feature offers the flexibility of leaving + rejoining the sync pool at any given point?<br/><NewLine>If one failed worker = all process have to restart from last checkpoint, it’s basically the same as I ran a background monitor process constantly checks-&gt;kills-&gt;restarts the whole process, which is not very efficient…</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>the funny part is the mentioned RuntimeError only happens 10% of the time, while the training process could tough it out most of times as if one worker has a slow start (by retraining a second round). I suspect it hits OOM before finish computing first bucket of gradients…</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Its the later, elastic monitors worker pids and restarts the world whenever one or more pids fail. Elastic was designed to be more general purpose to fit a variety of distributed training use-cases. For pure torch rpc apps it might be possible to allow workers to leave/join the job on the fly, but if you are using process groups the backend may not allow this by design - for instance if you are using NCCL backend, NCCL itself does not allow resizing of communicators unless you destroy them and restart them.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/kiuk_chung"">@Kiuk_Chung</a> I see.<br/><NewLine>But is it possible to reset DDP state on one worker (with NCCL backend)? Without resetting everything, which is somewhat expansive.<br/><NewLine>After all, in my case the connection is not lost, it’s simply halted with OOM. The ideal scenario could be:<br/><NewLine>(assuming a bad case where bucket 1 gradients already reduced, and stuck on bucket2 with worker#0 OOM)</p><NewLine><ol><NewLine><li>worker#0 rerun another batch and only sync bucket2 + later, to “catch up” with the rest.</li><NewLine><li>worker#0 continue to sync while only sending 0.0 gradients without further computation.</li><NewLine></ol><NewLine><p>I imagine solution#1 is very tricky if not impossible, whereas solution#2 is very much feasible?<br/><NewLine>If so, can you point me some directions to make it work? (files, watch-outs, etc.) Thanks!</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, this helps me a lot !</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>If the collective operation fails (with an OOM or other exception) and you are able to catch it you <em>may</em> be able to reissue the operation. Whether this works or not depends on whether the distributed backend is still in a good state or not. With NCCL once the state goes bad you have to destroy the process group and re initialize it. You could try to salvage your processes but you’d have to call destroy and initialize on all your workers together. There may be other states in your application that goes out of sync whenever there is an exception only observed in a subset of your workers. if you are able to recover that state you can try to restore the workers into a well defined point in time. Based on what we’ve observed with distributed applications it’s non trivial to restore the full application state (ddp + user code) in a distributed setting and the cleanest restore is to tear the processes down and restore from a checkpoint. This is why elastic was designed as such. Trade off is restart overhead versus correctness and maintainability. Note that with elastic you are simply restarting your worker processes so the penalty you pay is your initialization time (loading the model, allocating mem for data, etc) and not actually restarting the node or container.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/qianyizhang; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Kiuk_Chung; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/qianyizhang; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/qianyizhang; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Kiuk_Chung; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/qianyizhang; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/zhouzh; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Kiuk_Chung; <NewLine> ,"REPLY_DATE 1: June 7, 2020,  3:38pm; <NewLine> REPLY_DATE 2: June 8, 2020,  7:07am; <NewLine> REPLY_DATE 3: June 8, 2020,  2:17pm; <NewLine> REPLY_DATE 4: June 8, 2020,  5:27pm; <NewLine> REPLY_DATE 5: June 9, 2020,  6:29am; <NewLine> REPLY_DATE 6: June 9, 2020,  6:35am; <NewLine> REPLY_DATE 7: June 9, 2020,  5:56pm; <NewLine> REPLY_DATE 8: June 10, 2020,  4:02am; <NewLine> REPLY_DATE 9: June 11, 2020, 11:45am; <NewLine> REPLY_DATE 10: June 12, 2020,  6:17am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 2 Likes; <NewLine> REPLY 4 LIKES: 2 Likes; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> 
84972,Lightning vs Ignite,2020-06-10T22:59:11.692Z,2,615,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Currently, we have <a href=""https://github.com/PyTorchLightning/pytorch-lightning"" rel=""nofollow noopener"">Lightning</a> and <a href=""https://github.com/pytorch/ignite#tutorials"" rel=""nofollow noopener"">Ignite</a> as a high-level library to help with training neural networks in PyTorch. Which of them is easier to train in a multi GPU environment?</p><NewLine></div>",https://discuss.pytorch.org/u/Aldebaran,(Celso França),Aldebaran,"June 10, 2020, 10:59pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have used PyTorch Lightning. (While I can’t compare the two, as I haven’t used Ignite).</p><NewLine><p>It has been the smoothest experience as far as I have come across, w.r.t multi-GPU training. Changing from a single GPU to a multi-GPU setup is as simple as setting num_gpus in trainer.fit() to as many as you’d like to use.</p><NewLine><p>TPU support is also integrated, where you’d just specify num_tpu_cores, without changing any code.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much for your contribution.<br/><NewLine>I started using Ignite after read the exciting <a href=""https://pytorch.org/ignite/quickstart.html"" rel=""nofollow noopener"">quickstart</a> guide showing the essentials for define and training a simple model. But the provided examples which use multi GPU training do not seem to follow the same simplicity.</p><NewLine><p>Currently, the stable (v0.3.0) release relies only on native PyTorch distributed API where users need to manually set up distributed proc group, wrap model with <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">nn.parallel.DistributedDataParallel</a> and execute the script with torch.distributed.launch tool, or use mp.spawn.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I would go with Lightning then.</p><NewLine><p>The documentation is pretty clear and readable.</p><NewLine><p><a class=""onebox"" href=""https://pytorch-lightning.readthedocs.io/en/0.7.6/multi_gpu.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch-lightning.readthedocs.io/en/0.7.6/multi_gpu.html</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/sree_harsha; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Aldebaran; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/sree_harsha; <NewLine> ,"REPLY_DATE 1: June 11, 2020,  1:59pm; <NewLine> REPLY_DATE 2: June 11, 2020,  2:34pm; <NewLine> REPLY_DATE 3: June 11, 2020,  2:48pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
83245,Torch not able to utilize GPU ram properly,2020-05-28T14:22:32.801Z,8,195,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am training Albert language model using huggingface transformer. While training I notice that on my p3dn instance,gpu 0 is almost completely used but others have around 50% ram unused. I am getting only 85 batch size on this system and above this OOM.</p><NewLine><pre><code class=""lang-auto"">+-----------------------------------------------------------------------------+<NewLine>| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |<NewLine>|-------------------------------+----------------------+----------------------+<NewLine>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<NewLine>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<NewLine>|===============================+======================+======================|<NewLine>|   0  Tesla V100-SXM2...  On   | 00000000:00:16.0 Off |                    0 |<NewLine>| N/A   77C    P0   291W / 300W |  30931MiB / 32510MiB |    100%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   1  Tesla V100-SXM2...  On   | 00000000:00:17.0 Off |                    0 |<NewLine>| N/A   71C    P0   255W / 300W |  18963MiB / 32510MiB |    100%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   2  Tesla V100-SXM2...  On   | 00000000:00:18.0 Off |                    0 |<NewLine>| N/A   71C    P0    95W / 300W |  18963MiB / 32510MiB |     98%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   3  Tesla V100-SXM2...  On   | 00000000:00:19.0 Off |                    0 |<NewLine>| N/A   68C    P0    89W / 300W |  18963MiB / 32510MiB |     72%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   4  Tesla V100-SXM2...  On   | 00000000:00:1A.0 Off |                    0 |<NewLine>| N/A   68C    P0    78W / 300W |  18963MiB / 32510MiB |    100%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   5  Tesla V100-SXM2...  On   | 00000000:00:1B.0 Off |                    0 |<NewLine>| N/A   69C    P0    96W / 300W |  18963MiB / 32510MiB |     65%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   6  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |<NewLine>| N/A   69C    P0    79W / 300W |  18963MiB / 32510MiB |     95%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   7  Tesla V100-SXM2...  On   | 00000000:00:1D.0 Off |                    0 |<NewLine>| N/A   74C    P0    80W / 300W |  18963MiB / 32510MiB |     12%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine></code></pre><NewLine><p>I was using default setting for it using data parallel.<br/><NewLine>I tried distributed training also using <code>python -m torch.distributed.launch --nproc_per_node 8 test_lm.py </code> but It started new job for each and every GPU.</p><NewLine><pre><code class=""lang-auto"">Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. <NewLine>*****************************************<NewLine>Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated<NewLine>Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated<NewLine>/language_model/lm/lib/python3.6/site-packages/transformers/tokenization_utils.py:830: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.<NewLine>  category=FutureWarning,<NewLine>/language_model/lm/lib/python3.6/site-packages/transformers/tokenization_utils.py:830: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.<NewLine>  category=FutureWarning,<NewLine>Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated<NewLine>/language_model/lm/lib/python3.6/site-packages/transformers/tokenization_utils.py:830: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.<NewLine>  category=FutureWarning,<NewLine>Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated<NewLine>/language_model/lm/lib/python3.6/site-packages/transformers/tokenization_utils.py:830: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.<NewLine>  category=FutureWarning,<NewLine>Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated<NewLine>/language_model/lm/lib/python3.6/site-packages/transformers/tokenization_utils.py:830: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.<NewLine>  category=FutureWarning,<NewLine>Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated<NewLine>/language_model/lm/lib/python3.6/site-packages/transformers/tokenization_utils.py:830: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.<NewLine>  category=FutureWarning,<NewLine>Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated<NewLine>/language_model/lm/lib/python3.6/site-packages/transformers/tokenization_utils.py:830: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.<NewLine>  category=FutureWarning,<NewLine>Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated<NewLine>/language_model/lm/lib/python3.6/site-packages/transformers/tokenization_utils.py:830: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.<NewLine>  category=FutureWarning,<NewLine></code></pre><NewLine><p>Can anyone suggest what I should do for efficient training?</p><NewLine></div>",https://discuss.pytorch.org/u/karan_purohit,(karan purohit),karan_purohit,"May 28, 2020,  2:22pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like other processes have might stepped into <code>cuda:0</code>. Have you tried setting <code>CUDA_VISIBLE_DEVICES</code> to make sure that each process only sees one GPU?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, I didnt. Usually thats not the case and havent experience such issue</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you please also share the process pids using each device from <code>nvidia-smi</code></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>PIDS for each device</p><NewLine><pre><code class=""lang-auto"">+-----------------------------------------------------------------------------+<NewLine>| Processes:                                                       GPU Memory |<NewLine>|  GPU       PID   Type   Process name                             Usage      |<NewLine>|=============================================================================|<NewLine>|    0     23094      C   python                                     26309MiB |<NewLine>|    1     23094      C   python                                     14341MiB |<NewLine>|    2     23094      C   python                                     14341MiB |<NewLine>|    3     23094      C   python                                     14341MiB |<NewLine>|    4     23094      C   python                                     14341MiB |<NewLine>|    5     23094      C   python                                     14341MiB |<NewLine>|    6     23094      C   python                                     14341MiB |<NewLine>|    7     23094      C   python                                     14341MiB |<NewLine>+-----------------------------------------------------------------------------+<NewLine></code></pre><NewLine><p>Using distributed training will help here?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have a similar problem.  I used <code>DistributedDataParallel</code> and <code>python -m torch.distributed.launch --nproc_per_node=8</code>.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/b0645b44ce718f040fcb81095c528f6feeeb407f"" href=""https://discuss.pytorch.org/uploads/default/original/3X/b/0/b0645b44ce718f040fcb81095c528f6feeeb407f.png"" title=""image""><img alt=""image"" data-base62-sha1=""par2X9ZZlcAaJ2Ior5cl29KAZwH"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/0/b0645b44ce718f040fcb81095c528f6feeeb407f_2_10x10.png"" height=""416"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/0/b0645b44ce718f040fcb81095c528f6feeeb407f_2_690x416.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/0/b0645b44ce718f040fcb81095c528f6feeeb407f_2_690x416.png, https://discuss.pytorch.org/uploads/default/optimized/3X/b/0/b0645b44ce718f040fcb81095c528f6feeeb407f_2_1035x624.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/b/0/b0645b44ce718f040fcb81095c528f6feeeb407f.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1300×784 89.4 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/tyan"">@Tyan</a></p><NewLine><p>The figure you shared looks a little different from the one <a class=""mention"" href=""/u/karan_purohit"">@karan_purohit</a> attached. Looks like all processes step into <code>cuda:0</code>, which could happen if they use <code>cuda:0</code> as the default device and then some tensors/context were unintentionally created there. E.g., when you call <code>empty_cache()</code> without a device context, or create some cuda tensor without specifying device affinity.</p><NewLine><p>Can you try setting <code>CUDA_VISIBLE_DEVICES</code> for all processes so that each process exclusively works on one device?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/karan_purohit"">@karan_purohit</a></p><NewLine><p>Looks like there is only one process using GPUs in your application while there should be 8 processes? Did you create DDP instances with proper device ids in all processes? Could you please share a min snippet of your Python script that can reproduce this behavior?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a><br/><NewLine>Thanks a billion for your reply. I didn’t set <code>CUDA_VISIBLE_DEVICES</code>,  I set env as follows:</p><NewLine><pre><code class=""lang-auto"">def train(args):<NewLine>    torch.backends.cudnn.benchmark = True<NewLine>    dist.init_process_group('nccl')<NewLine>    torch.cuda.set_device(args.local_rank)<NewLine>    device = torch.device('cuda', args.local_rank)<NewLine>    ....<NewLine>    model = model.to(device)<NewLine>    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)<NewLine>    criterion = criterion.to(device)<NewLine>    ...<NewLine>    <NewLine>        lr_imgs = lr_imgs.to(device)<NewLine>        hr_imgs = hr_imgs.to(device)<NewLine></code></pre><NewLine><p>simiar  settings in another model training,  gpu utilization is balance. It’s very strange.</p><NewLine><p>I tried set <code>CUDA_VISIBLE_DEVICES </code>, it didn’t work.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/tyan"">@Tyan</a></p><NewLine><p>How did you set <code>CUDA_VISIBLE_DEVICES</code>? Is it sth like <code>os.environ[""CUDA_VISIBLE_DEVICES""]=f""{args.local_rank}""</code> in every individual process before running any cuda related code?</p><NewLine><p>Besides, can you try swapping the order of the following two lines? I am not 100% sure, but <code>ProcessGroupNCCL</code> might create CUDA context on the default device.</p><NewLine><pre><code class=""lang-python"">    dist.init_process_group('nccl')<NewLine>    torch.cuda.set_device(args.local_rank)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a><br/><NewLine>Yeah. I have tried what you said. It didn’t work. Current setting:</p><NewLine><pre><code class=""lang-auto"">def train(args):<NewLine>    # Env<NewLine>    os.environ[""CUDA_VISIBLE_DEVICES""] = str(args.local_rank)<NewLine>    torch.backends.cudnn.benchmark = True<NewLine>    torch.cuda.set_device(args.local_rank)<NewLine>    dist.init_process_group('nccl')<NewLine>    device = torch.device('cuda', args.local_rank)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s weird. Can you share a min repro so that we can debug locally?</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a><br/><NewLine>I checked the code again. I found in <code>utils.py</code>, some variables occupy the gpu:</p><NewLine><pre><code class=""lang-auto"">device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")<NewLine><NewLine># Some constants<NewLine>rgb_weights = torch.FloatTensor([65.481, 128.553, 24.966]).to(device)<NewLine>imagenet_mean = torch.FloatTensor([0.485, 0.456, 0.406]).unsqueeze(1).unsqueeze(2)<NewLine>imagenet_std = torch.FloatTensor([0.229, 0.224, 0.225]).unsqueeze(1).unsqueeze(2)<NewLine>imagenet_mean_cuda = torch.FloatTensor([0.485, 0.456, 0.406]).to(device).unsqueeze(0).unsqueeze(2).unsqueeze(3)<NewLine>imagenet_std_cuda = torch.FloatTensor([0.229, 0.224, 0.225]).to(device).unsqueeze(0).unsqueeze(2).unsqueeze(3)<NewLine></code></pre><NewLine><p>Very sorry to trouble you.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/karan_purohit; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/karan_purohit; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Tyan; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Tyan; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Tyan; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/Tyan; <NewLine> ,"REPLY_DATE 1: May 28, 2020,  3:49pm; <NewLine> REPLY_DATE 2: May 28, 2020,  3:52pm; <NewLine> REPLY_DATE 3: May 28, 2020,  7:02pm; <NewLine> REPLY_DATE 4: June 1, 2020, 10:18am; <NewLine> REPLY_DATE 5: June 10, 2020,  9:31am; <NewLine> REPLY_DATE 6: June 10, 2020,  1:55pm; <NewLine> REPLY_DATE 7: June 10, 2020,  1:57pm; <NewLine> REPLY_DATE 8: June 11, 2020,  1:12am; <NewLine> REPLY_DATE 9: June 11, 2020,  1:21am; <NewLine> REPLY_DATE 10: June 11, 2020,  1:31am; <NewLine> REPLY_DATE 11: June 11, 2020,  1:36am; <NewLine> REPLY_DATE 12: June 11, 2020,  3:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: 1 Like; <NewLine> 
84865,Multiple nodes with Pytorch (Only CPUs),2020-06-10T08:31:27.830Z,11,186,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>For single node, I set</p><NewLine><pre><code class=""lang-auto"">os.environ['MASTER_ADDR'] = 'localhost'<NewLine>os.environ['MASTER_PORT'] = '29500'<NewLine></code></pre><NewLine><p>and the size is as input parameter.</p><NewLine><p>However, with multiple nodes, we have to set differently. But I did now know how to set it?</p><NewLine><p>For example, I know the node names with 4 nodes as below.</p><NewLine><pre><code class=""lang-auto"">C1-01<NewLine>C1-02<NewLine>C2-01<NewLine>C2-02<NewLine></code></pre><NewLine><p>When I submit the job, the node names will change.<br/><NewLine>How to set MASTER_ADDR for the program?</p><NewLine><p>Thanks,</p><NewLine></div>",https://discuss.pytorch.org/u/ph0123,(chau phuong),ph0123,"June 10, 2020,  9:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ph0123"">@ph0123</a> do you mean before submitting jobs, neither node name nor node IP addresses are known to you?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I submit the batch file. In the batch file, I can get the node names of all nodes, which assigned by the server.<br/><NewLine>For each submission, the node names will be changed.</p><NewLine><p>I also have another question. When I run with real data (big data), the gloo backend is stopped after 60 seconds in my program.<br/><NewLine>How to set up the time out for this situation. The program will run with 20-30 minutes.</p><NewLine><p>Thanks,</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""84865"" data-username=""ph0123""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ph0123/40/22888_2.png"" width=""20""/> ph0123:</div><NewLine><blockquote><NewLine><p>I submit the batch file. In the batch file, I can get the node names of all nodes, which assigned by the server.<br/><NewLine>For each submission, the node names will be changed.</p><NewLine></blockquote><NewLine></aside><NewLine><p>In that case, I wonder if it would be possible to programmably figure out the master. E.g., ask all processes to sort all node names and then always use the first one?</p><NewLine><blockquote><NewLine><p>How to set up the time out for this situation. The program will run with 20-30 minutes.</p><NewLine></blockquote><NewLine><p>Are you using RPC or DDP? For RPC, checkout this PR (<a href=""https://github.com/pytorch/pytorch/pull/38577"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/38577</a>). For DDP, <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group"" rel=""nofollow noopener""><code>init_process_group</code></a> does take a timeout argument.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>The 60 second  timeout might be the default RPC timeout. Which version of PyTorch are you using?</p><NewLine><p>In v1.4, there is a hidden <a href=""https://github.com/pytorch/pytorch/blob/v1.4.0/test/rpc_test.py#L1232"" rel=""nofollow noopener""><code>_set_rpc_timeout</code></a> API.</p><NewLine><p>In v1.5, you can customize the <a href=""https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.ProcessGroupRpcBackendOptions"" rel=""nofollow noopener""><code>ProcessGroupRpcBackendOptions</code></a> to provide a timeout.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>We will be adding per-RPC timeout in v1.6.  See</p><NewLine><ol><NewLine><li><a href=""https://github.com/pytorch/pytorch/issues/32686"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/32686</a></li><NewLine><li><a href=""https://github.com/pytorch/pytorch/issues/36000"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/36000</a></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks,<br/><NewLine>For multiple nodes, I think I can print out to a file. The first name node is the master node.<br/><NewLine>Let me try the timeout.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>HI,<br/><NewLine>I try to set the time out as in the git. <a href=""https://github.com/pytorch/pytorch/issues/32686"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/32686</a></p><NewLine><pre><code class=""lang-auto"">rpc.rpc_async(my_target, add_outlinks, args=(arr_send[i],source),timeout=None)<NewLine></code></pre><NewLine><p>The error:</p><NewLine><pre><code class=""lang-auto"">TypeError: rpc_async() got an unexpected keyword argument 'timeout'<NewLine></code></pre><NewLine><p>Perhaps, rpc is not have the “timeout” parameter.</p><NewLine><p>Thanks,</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>That per-RPC timeout is only for v1.6 which hasn’t been released yet. The feature available on master now though, but you will need to either use nightly binary or build from source to get that feature.  If you are using v1.4 or v1.5, please try the other two options mentioned above.</p><NewLine><p>What does the following code print in your environment?</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>torch.__version__<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>I used pytorch v1.5<br/><NewLine>Thanks,</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Cool, then below should be the way to go. The link below points to the doc that contains an example.</p><NewLine><blockquote><NewLine><p>In v1.5, you can customize the <a href=""https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.ProcessGroupRpcBackendOptions"" rel=""nofollow noopener""> <code>ProcessGroupRpcBackendOptions</code> </a> to provide a timeout.</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes,</p><NewLine><pre><code class=""lang-auto"">rpc.init_rpc(my_name, rank=rank, world_size=size, rpc_backend_options=rpc.ProcessGroupRpcBackendOptions(num_send_recv_threads=16,datetime.timedelta(seconds=1000)))  # initial_rpc<NewLine><NewLine></code></pre><NewLine><p>Output:</p><NewLine><pre><code class=""lang-auto"">    rpc.init_rpc(my_name, rank=rank, world_size=size, rpc_backend_options=rpc.ProcessGroupRpcBackendOptions(num_send_recv_threads=16,datetime.timedelta(seconds=1000)))  # initial_rpc<NewLine>                                                                                                                                    ^<NewLine>SyntaxError: positional argument follows keyword argument<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for reporting, it’s an error in the doc, I think it needs to be:</p><NewLine><pre><code class=""lang-python"">rpc.ProcessGroupRpcBackendOptions(<NewLine>    num_send_recv_threads=16,<NewLine>    rpc_timeout=datetime.timedelta(seconds=1000)<NewLine>)<NewLine></code></pre><NewLine><p>Let me try.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-python"">import datetime, os<NewLine>from torch.distributed import rpc<NewLine><NewLine>os.environ['MASTER_ADDR'] = 'localhost'<NewLine>os.environ['MASTER_PORT'] = '29500'<NewLine><NewLine>rpc.init_rpc(<NewLine>    ""worker"",<NewLine>    rank=0,<NewLine>    world_size=1,<NewLine>    rpc_backend_options=rpc.ProcessGroupRpcBackendOptions(<NewLine>        num_send_recv_threads=16,<NewLine>        rpc_timeout = datetime.timedelta(seconds=1000)  # note that this will change to float type to support TorchScript integration. <NewLine>    )<NewLine>)<NewLine><NewLine>rpc.shutdown()<NewLine></code></pre><NewLine><p>Yep, above should work. Will submit a PR to fix.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""onebox githubpullrequest""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/39787"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Pull Request""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 12 16"" width=""60""><path d=""M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/pull/39787"" rel=""nofollow noopener"" target=""_blank"">[Easy Review] Fix ProcessGroupRpcBackendOptions Doc</a><NewLine></h4><NewLine><div class=""branches""><NewLine><code>pytorch:gh/mrshenli/193/base</code> ← <code>pytorch:gh/mrshenli/193/head</code><NewLine></div><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-06-10"" data-format=""ll"" data-time=""16:12:46"" data-timezone=""UTC"">04:12PM - 10 Jun 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/mrshenli"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""mrshenli"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/16999635?v=4"" width=""20""/><NewLine>          mrshenli<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""1 commits changed 1 files with 1 additions and 1 deletions""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/39787/files"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+1</span><NewLine><span class=""removed"">-1</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 10, 2020,  2:00pm; <NewLine> REPLY_DATE 2: June 10, 2020,  3:00pm; <NewLine> REPLY_DATE 3: June 10, 2020,  3:19pm; <NewLine> REPLY_DATE 4: June 10, 2020,  3:24pm; <NewLine> REPLY_DATE 5: June 10, 2020,  3:27pm; <NewLine> REPLY_DATE 6: June 10, 2020,  3:29pm; <NewLine> REPLY_DATE 7: June 10, 2020,  3:42pm; <NewLine> REPLY_DATE 8: June 10, 2020,  3:51pm; <NewLine> REPLY_DATE 9: June 10, 2020,  3:52pm; <NewLine> REPLY_DATE 10: June 10, 2020,  3:53pm; <NewLine> REPLY_DATE 11: June 10, 2020,  3:59pm; <NewLine> REPLY_DATE 12: June 10, 2020,  4:09pm; <NewLine> REPLY_DATE 13: June 10, 2020,  4:10pm; <NewLine> REPLY_DATE 14: June 10, 2020,  4:13pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: 1 Like; <NewLine> REPLY 13 LIKES: 2 Likes; <NewLine> REPLY 14 LIKES: ; <NewLine> 
84841,Failed to load model trained by DDP for inference,2020-06-10T05:25:12.223Z,0,137,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have trained my model with 4 GPU by DDP (torch.nn.parallel.DistributedDataParallel)</p><NewLine><p>But I wanna load the model for inference on machine with only single GPU , the codes are listed as below:</p><NewLine><pre><code class=""lang-auto"">m = Model()<NewLine>m.load_state_dict(torch.load('model.pth'))<NewLine></code></pre><NewLine><p>it raises errors listed as below</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Error(s) in loading state_dict for ObjectDetector:<NewLine>	Missing key(s) in state_dict: ""backbone.block1.0.block.0.weight"", ""backbone.block1.0.block.0.bias"", ""backbone.block1.0.block.1.weight"", ""backbone.block1.0.block.1.bias"", ""backbone.block1.0.block.1.running_mean"", ""backbone.block1.0.block.1.running_var"", ""backbone.block1.1.block.0.weight"", ""backbone.block1.1.block.0.bias"", ""backbone.block1.1.block.1.weight"", ""backbone.block1.1.block.1.bias"", ""backbone.block1.1.block.1.running_mean"", ""backbone.block1.1.block.1.running_var"", ""backbone.block1.2.conv_block1.block.0.weight"", ""backbone.block1.2.conv_block1.block.0.bias"", ""backbone.block1.2.conv_block1.block.1.weight"", ""backbone.block1.2.conv_block1.block.1.bias"", ""backbone.block1.2.conv_block1.block.1.running_mean"", ""backbone.block1.2.conv_block1.block.1.running_var"", ""backbone.block1.2.conv_block2.block.0.weight"", ""backbone.block1.2.conv_block2.block.0.bias"", ""backbone.block1.2.conv_block2.block.1.weight"", ""backbone.block1.2.conv_block2.block.1.bias"", ""backbone.block1.2.conv_block2.block.1.running_mean"", ""backbone.block1.2.conv_block2.block.1.running_var"", ""backbone.block2.0.block.0.weight"", ""backbone.block2.0.block.0.bias"", ""backbone.block2.0.block.1.weight"", ""backbone.block2.0.block.1.bias"", ""backbone.block2.0.block.1.running_mean"", ""backbone.block2.0.block.1.running_var"", ""backbone.block2.1.conv_block1.block.0.weight"", ""backbone.block2.1.conv_block1.block.0.bias"", ""backbone.block2.1.conv_block1.block.1.weight"", <NewLine><NewLine>.......<NewLine>""backbone.block5.1.conv_block1.block.1.running_var"", <NewLine></code></pre><NewLine><p>I have tried</p><NewLine><pre><code class=""lang-auto"">m.load_state_dict(torch.load('model.pth'), strict=False)<NewLine></code></pre><NewLine><p>But the inference results are very strange not as expected</p><NewLine><p>How should I fix it? thanks</p><NewLine></div>",https://discuss.pytorch.org/u/Wilson_Ho,(Wilson Ho),Wilson_Ho,"June 10, 2020,  5:30am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have resolved the issues. It’s related to  <a href=""https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686"">1686</a></p><NewLine><pre><code class=""lang-auto"">state_dict = torch.load(weight_path)<NewLine><NewLine>from collections import OrderedDict<NewLine>new_state_dict = OrderedDict()<NewLine>for k, v in state_dict.items():<NewLine>    name = k[7:] # remove 'module.' of DataParallel/DistributedDataParallel<NewLine>    new_state_dict[name] = v<NewLine><NewLine>m.load_state_dict(new_state_dict)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Wilson_Ho; <NewLine> ,"REPLY_DATE 1: June 10, 2020,  6:08am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
84698,Unable to use distributed autograd,2020-06-09T00:54:39.015Z,0,114,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m trying to use distributed autograd. But I’m running into this error.<br/><NewLine>I have initialized the process group with dist.init_process_group()</p><NewLine><pre><code class=""lang-auto"">with dist_autograd.context() as context_id:<NewLine>  File ""/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/distributed/autograd/__init__.py"", line 33, in __enter__<NewLine>    self.autograd_context = _new_context()<NewLine>RuntimeError: Need to initialize distributed autograd using torch.distributed.autograd.init()<NewLine></code></pre><NewLine><p>But I see no init method in torch.distributed.autograd</p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; from torch.distributed import autograd<NewLine>autog&gt;&gt;&gt; autograd<NewLine>&lt;module 'torch.distributed.autograd' from '/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/distributed/autograd/__init__.py'&gt;<NewLine>&gt;&gt;&gt; autograd.<NewLine>autograd.DistAutogradContext(  autograd.division              autograd.sys<NewLine>autograd.absolute_import       autograd.get_gradients(        autograd.torch<NewLine>autograd.backward(             autograd.is_available(         autograd.unicode_literals<NewLine>autograd.context(              autograd.print_function<NewLine>&gt;&gt;&gt; autograd.<NewLine>autograd.DistAutogradContext(  autograd.division              autograd.sys<NewLine>autograd.absolute_import       autograd.get_gradients(        autograd.torch<NewLine>autograd.backward(             autograd.is_available(         autograd.unicode_literals<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/rahul003,(Rahul),rahul003,"June 9, 2020,  1:47pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/rahul003"">@rahul003</a>,</p><NewLine><p>Distributed autograd is using RPC, so you need to call <code>init_rpc</code> instead of <code>init_process_group</code>. See the toy example below:</p><NewLine><pre><code class=""lang-python"">import torch.multiprocessing as mp<NewLine>import torch.distributed.rpc as rpc<NewLine>import torch.distributed.autograd as dist_autograd<NewLine>import os<NewLine>import torch<NewLine>from torch import optim<NewLine>from torch.distributed.optim import DistributedOptimizer<NewLine><NewLine><NewLine>def train():<NewLine>    with dist_autograd.context() as context_id:<NewLine>        t1 = torch.rand((3, 3), requires_grad=True)<NewLine>        t2 = torch.rand((3, 3), requires_grad=True)<NewLine>        loss = t1 + t2<NewLine>        dist_autograd.backward(context_id, [loss.sum()])<NewLine>        grads = dist_autograd.get_gradients(context_id)<NewLine>        print(grads[t1])<NewLine>        print(grads[t2])<NewLine><NewLine>def run_worker(rank, world_size):<NewLine>    os.environ['MASTER_ADDR'] = 'localhost'<NewLine>    os.environ['MASTER_PORT'] = '29500'<NewLine>    if rank == 1:<NewLine>        rpc.init_rpc(""worker0"", rank=rank, world_size=world_size)<NewLine>        train()<NewLine>    else:<NewLine>        rpc.init_rpc(""worker1"", rank=rank, world_size=world_size)<NewLine>        pass<NewLine><NewLine>    # block until all rpcs finish<NewLine>    rpc.shutdown()<NewLine><NewLine><NewLine>if __name__==""__main__"":<NewLine>    world_size = 2<NewLine>    mp.spawn(run_worker, args=(world_size, ), nprocs=world_size, join=True)<NewLine></code></pre><NewLine><p>For more complete tutorials, please see the following links:</p><NewLine><ol><NewLine><li>RL example and RNN example: <a href=""https://pytorch.org/tutorials/intermediate/rpc_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/rpc_tutorial.html</a><NewLine></li><NewLine><li>Parameter server example: <a href=""https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html</a><NewLine></li><NewLine><li>Distributed pipeline example: <a href=""https://github.com/pytorch/examples/tree/master/distributed/rpc/pipeline"" rel=""nofollow noopener"">https://github.com/pytorch/examples/tree/master/distributed/rpc/pipeline</a><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 9, 2020,  8:01pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
84761,Return value with rpc,2020-06-09T12:48:52.652Z,3,88,"<div class=""post"" itemprop=""articleBody""><NewLine><p>HI all,<br/><NewLine>I try to run 2 processes. Each process will call “rpc” to other process. The return value is got by future variable. However, the error is at “final += fut.wait()”</p><NewLine><pre><code class=""lang-auto""><NewLine>import os<NewLine><NewLine>from torch.multiprocessing import Process<NewLine>import torch.distributed.rpc as rpc<NewLine><NewLine><NewLine><NewLine>def my_sum(arr):<NewLine>    res = 0<NewLine>    for i in arr:<NewLine>        res += i<NewLine>    return res<NewLine><NewLine><NewLine><NewLine>def init_process(rank, size, fn, backend='gloo'):<NewLine>    """""" Initialize the distributed environment. """"""<NewLine>    os.environ['MASTER_ADDR'] = '127.0.0.1'<NewLine>    os.environ['MASTER_PORT'] = '29500'<NewLine><NewLine>    my_name = ""worker"" + str(rank)<NewLine><NewLine>    rpc.init_rpc(my_name, rank=rank, world_size=size)  # initial_rpc<NewLine><NewLine>    array_rpc = list(range(0, size))<NewLine><NewLine>    arr_send = []<NewLine>    for i in range(0, size):<NewLine>        temp = []<NewLine>        arr_send.append(temp)<NewLine><NewLine>    arr_send[0].append(1)<NewLine>    arr_send[0].append(2)<NewLine>    arr_send[1].append(3)<NewLine>    arr_send[1].append(4)<NewLine><NewLine>    futs=[]<NewLine>    for i in array_rpc:<NewLine>        my_target = ""worker"" + str(i)<NewLine>        futs.append(rpc.rpc_async(my_target, my_sum, args=(arr_send[i])))<NewLine><NewLine>    final = 0<NewLine>    for fut in futs:<NewLine>        final += fut.wait()<NewLine><NewLine>    print(""results = :"",final,"" in rank "", rank)<NewLine>    rpc.api._wait_all_workers()<NewLine>    rpc.shutdown()<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    size = 2<NewLine>    processes = []<NewLine>    for rank in range(size):<NewLine>        p = Process(target=init_process, args=(rank, size, my_sum))<NewLine>        p.start()<NewLine>        processes.append(p)<NewLine><NewLine>    for p in processes:<NewLine>        p.join()<NewLine></code></pre><NewLine><p>Is there any problem with future variable? I try to use float(fut), but there is still a problem.</p><NewLine><p>error is as below</p><NewLine><pre><code class=""lang-auto"">Process Process-1:<NewLine>Process Process-2:<NewLine>Traceback (most recent call last):<NewLine>  File ""/usr/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap<NewLine>    self.run()<NewLine>  File ""/usr/lib/python3.6/multiprocessing/process.py"", line 93, in run<NewLine>    self._target(*self._args, **self._kwargs)<NewLine>  File ""/mnt/c/python_project/Pytorch/test_lib.py"", line 44, in init_process<NewLine>    final += fut.wait()<NewLine>  File ""/home/cnphuong/.local/lib/python3.6/site-packages/torch/distributed/rpc/internal.py"", line 163, in _handle_exception<NewLine>    raise result.exception_type(result.msg)<NewLine>TypeError: TypeError('my_sum() takes 1 positional argument but 2 were given',)<NewLine>Traceback (most recent call last):<NewLine>  File ""/home/cnphuong/.local/lib/python3.6/site-packages/torch/distributed/rpc/internal.py"", line 153, in _run_function<NewLine>    result = python_udf.func(*python_udf.args, **python_udf.kwargs)<NewLine>TypeError: my_sum() takes 1 positional argument but 2 were given<NewLine><NewLine>Traceback (most recent call last):<NewLine>  File ""/usr/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap<NewLine>    self.run()<NewLine>  File ""/usr/lib/python3.6/multiprocessing/process.py"", line 93, in run<NewLine>    self._target(*self._args, **self._kwargs)<NewLine>  File ""/mnt/c/python_project/Pytorch/test_lib.py"", line 44, in init_process<NewLine>    final += fut.wait()<NewLine>  File ""/home/cnphuong/.local/lib/python3.6/site-packages/torch/distributed/rpc/internal.py"", line 163, in _handle_exception<NewLine>    raise result.exception_type(result.msg)<NewLine>TypeError: TypeError('my_sum() takes 1 positional argument but 2 were given',)<NewLine>Traceback (most recent call last):<NewLine>  File ""/home/cnphuong/.local/lib/python3.6/site-packages/torch/distributed/rpc/internal.py"", line 153, in _run_function<NewLine>    result = python_udf.func(*python_udf.args, **python_udf.kwargs)<NewLine>TypeError: my_sum() takes 1 positional argument but 2 were given<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/ph0123,(chau phuong),ph0123,"June 9, 2020, 12:52pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""84761"" data-username=""ph0123""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ph0123/40/22888_2.png"" width=""20""/> ph0123:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">TypeError: my_sum() takes 1 positional argument but 2 were given<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>This error message suggests the <code>args</code> was incorrect in the <code>rpc.rpc_async</code> call. It misses a comma after <code>arr_send[i]</code>, change it to the following should work:</p><NewLine><pre><code class=""lang-python"">        futs.append(rpc.rpc_async(my_target, my_sum, args=(arr_send[i], )))<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Wow.</p><NewLine><p>Could you explain why it is?<br/><NewLine>My function has only one parameter. Why we need to add “,” after the parameter?</p><NewLine><p>Thanks,</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Could you explain why it is?</p><NewLine></blockquote><NewLine><p>Sure. If you don’t add a comma, Python would not recognize it as a tuple. See the following code:</p><NewLine><pre><code class=""lang-python"">Python 3.8.2 (default, Mar 26 2020, 15:53:00)<NewLine>[GCC 7.3.0] :: Anaconda, Inc. on linux<NewLine>Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.<NewLine>&gt;&gt;&gt; (5) == 5<NewLine>True<NewLine>&gt;&gt;&gt; (5, ) == 5<NewLine>False<NewLine>&gt;&gt;&gt; print( (5) )<NewLine>5<NewLine>&gt;&gt;&gt; print( (5,) )<NewLine>(5,)<NewLine>&gt;&gt;&gt; type((5))<NewLine>&lt;class 'int'&gt;<NewLine>&gt;&gt;&gt; type((5,))<NewLine>&lt;class 'tuple'&gt;<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 9, 2020,  2:08pm; <NewLine> REPLY_DATE 2: June 9, 2020,  2:28pm; <NewLine> REPLY_DATE 3: June 9, 2020,  2:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
83667,&lsquo;unhandled system error&rsquo; when training with multi nodes,2020-06-01T08:44:56.005Z,3,427,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I met an error when I use DDP for multi node (two node, two GPUs each) training and ‘nccl’ backend (It runs perfect when I use ‘gloo’). The environment is Ubuntu 16.04+python3.5+pytorch1.5.0+cuda10.1.<br/><NewLine>My code is based on the demo code on the offical website to test distributed training.</p><NewLine><pre><code class=""lang-auto"">def setup(rank, world_size):<NewLine>    os.environ['NCCL_DEBUG'] = 'INFO'<NewLine>    os.environ['NCCL_SOCKET_IFNAME'] = 'eno1'<NewLine>    os.environ['NCCL_IB_DISABLE'] = '1'<NewLine>    dist.init_process_group(<NewLine>        ""nccl"", rank=rank, init_method='tcp://162.105.146.176:22222', world_size=world_size)<NewLine><NewLine>class ToyModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(ToyModel, self).__init__()<NewLine>        self.net1 = nn.Linear(10, 10)<NewLine>        self.relu = nn.ReLU()<NewLine>        self.net2 = nn.Linear(10, 5)<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.net2(self.relu(self.net1(x)))<NewLine> ...<NewLine></code></pre><NewLine><p>The error code when using ‘nccl’ is as following,</p><NewLine><pre><code class=""lang-auto"">ptwop-176:1755:1755 [0] NCCL INFO Bootstrap : Using [0]eno1:162.105.146.176&lt;0&gt;<NewLine>ptwop-176:1755:1755 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine><NewLine>ptwop-176:1755:1755 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]<NewLine>ptwop-176:1755:1755 [0] NCCL INFO NET/Socket : Using [0]eno1:162.105.146.176&lt;0&gt;<NewLine>NCCL version 2.4.8+cuda10.1<NewLine>ptwop-176:1756:1756 [1] NCCL INFO Bootstrap : Using [0]eno1:162.105.146.176&lt;0&gt;<NewLine>ptwop-176:1756:1756 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine><NewLine>ptwop-176:1756:1756 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]<NewLine>ptwop-176:1756:1756 [1] NCCL INFO NET/Socket : Using [0]eno1:162.105.146.176&lt;0&gt;<NewLine>ptwop-176:1755:1870 [0] NCCL INFO Setting affinity for GPU 0 to 5555,55555555<NewLine>ptwop-176:1756:1871 [1] NCCL INFO Setting affinity for GPU 1 to 5555,55555555<NewLine><NewLine>ptwop-176:1756:1871 [1] include/socket.h:390 NCCL WARN Connect to 162.105.146.178&lt;35007&gt; failed : No route to host<NewLine>ptwop-176:1756:1871 [1] NCCL INFO bootstrap.cc:100 -&gt; 2<NewLine><NewLine>ptwop-176:1756:1871 [1] NCCL INFO bootstrap.cc:337 -&gt; 2<NewLine>ptwop-176:1755:1869 [0] include/socket.h:390 NCCL WARN Connect to 162.105.146.178&lt;54473&gt; failed : No route to host<NewLine>ptwop-176:1756:1871 [1] NCCL INFO init.cc:695 -&gt; 2<NewLine>ptwop-176:1755:1869 [0] NCCL INFO bootstrap.cc:100 -&gt; 2<NewLine>ptwop-176:1756:1871 [1] NCCL INFO init.cc:951 -&gt; 2<NewLine>ptwop-176:1755:1869 [0] NCCL INFO bootstrap.cc:226 -&gt; 2<NewLine>ptwop-176:1756:1871 [1] NCCL INFO misc/group.cc:69 -&gt; 2 [Async thread]<NewLine>Traceback (most recent call last):<NewLine>  File ""test.py"", line 73, in &lt;module&gt;<NewLine>    run_demo(demo_basic, 2, 3)<NewLine>  File ""test.py"", line 45, in run_demo<NewLine>    join=True)<NewLine>  File ""/home/xukun/setsuna/lib/python3.5/site-packages/torch/multiprocessing/spawn.py"", line 200, in spawn<NewLine>    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')<NewLine>  File ""/home/xukun/setsuna/lib/python3.5/site-packages/torch/multiprocessing/spawn.py"", line 158, in start_processes<NewLine>    while not context.join():<NewLine>  File ""/home/xukun/setsuna/lib/python3.5/site-packages/torch/multiprocessing/spawn.py"", line 119, in join<NewLine>    raise Exception(msg)<NewLine>Exception: <NewLine><NewLine>-- Process 1 terminated with the following error:<NewLine>Traceback (most recent call last):<NewLine>  File ""/home/xukun/setsuna/lib/python3.5/site-packages/torch/multiprocessing/spawn.py"", line 20, in _wrap<NewLine>    fn(i, *args)<NewLine>  File ""/home/xukun/graph/multi_node/test.py"", line 57, in demo_basic<NewLine>    ddp_model = DDP(model.to(rank), device_ids=[rank])<NewLine>  File ""/home/xukun/setsuna/lib/python3.5/site-packages/torch/nn/parallel/distributed.py"", line 285, in __init__<NewLine>    self.broadcast_bucket_size)<NewLine>  File ""/home/xukun/setsuna/lib/python3.5/site-packages/torch/nn/parallel/distributed.py"", line 483, in _distributed_broadcast_coalesced<NewLine>    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)<NewLine>RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:514, unhandled system error, NCCL version 2.4.8<NewLine></code></pre><NewLine><p>What can I do to avoid this error?</p><NewLine></div>",https://discuss.pytorch.org/u/miyanoo,(miyanoo),miyanoo,"June 1, 2020,  1:54pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>From this log message: <code>ptwop-176:1755:1869 [0] include/socket.h:390 NCCL WARN Connect to 162.105.146.178&lt;54473&gt; failed : No route to host</code>. I’m assuming the other node’s ip is <code>162.105.146.178</code>? Could you validate the following:</p><NewLine><ol><NewLine><li>See if the issue reproduces on a single-node multi-gpu setup.</li><NewLine><li>Can you ping <code>162.105.146.178</code> from <code>162.105.146.176</code>?</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, the nodes’ ips are <code>162.105.146.178</code> and <code>162.105.146.176</code>.</p><NewLine><ol><NewLine><li>The issue cannot reproduce on a single-node multi-gpu setup, and everything runs well.</li><NewLine><li>The two nodes can ping each other successfully, and it runs well when I change the communication backend to “gloo” from “nccl” with nearly no code changed (minus some modification to make Tensors and model on cpu).</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for validating. Which version of NCCL are you using? And could you validate that NCCL has been successfully installed on both nodes by running: <a href=""https://github.com/NVIDIA/nccl-tests"" rel=""nofollow noopener"">https://github.com/NVIDIA/nccl-tests</a>.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here’s one way to see if nccl is installed on the node:</p><NewLine><pre><code class=""lang-auto"">locate nccl| grep ""libnccl.so"" | tail -n1 | sed -r 's/^.*\.so\.//'<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Kiuk_Chung; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/miyanoo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Kiuk_Chung; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Kiuk_Chung; <NewLine> ,"REPLY_DATE 1: June 8, 2020,  5:57pm; <NewLine> REPLY_DATE 2: June 9, 2020,  3:00am; <NewLine> REPLY_DATE 3: June 9, 2020,  4:23am; <NewLine> REPLY_DATE 4: June 9, 2020,  4:34am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
78529,Distributed pagerank with pytorch,2020-04-26T16:58:47.382Z,13,319,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to implement Pagerank with libtorch. I finished the OpenMPI version with Pagerank. I try to read libtorch documents <a href=""https://pytorch.org/tutorials/intermediate/dist_tuto.html"" rel=""nofollow noopener"">here</a>.</p><NewLine><p>However, I did not see any function like RPC in OpenMPI.</p><NewLine><p>Is there possible to implement Pagerank with libtorch?</p><NewLine></div>",https://discuss.pytorch.org/u/ph0123,(chau phuong),ph0123,"April 26, 2020,  4:58pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It should be possible. And there are several levels of APIs that you can use:</p><NewLine><ol><NewLine><li>send/recv APIs: <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.send"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/distributed.html#torch.distributed.send</a><NewLine></li><NewLine><li>collective communication APIs: <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_reduce"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_reduce</a><NewLine></li><NewLine><li>RPC APIs:<br/><NewLine>a. <a href=""https://pytorch.org/docs/stable/rpc.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/rpc.html</a><br/><NewLine>b. <a href=""https://pytorch.org/tutorials/intermediate/rpc_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/rpc_tutorial.html</a><br/><NewLine>c. <a href=""https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html</a><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you so much!<br/><NewLine>I will try.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I try to use RPC, but it seem to difficult.<br/><NewLine>My idea is that rank0 will call my_add function, and rank1 will do it and return the value for rank0.</p><NewLine><pre><code class=""lang-auto"">""""""RPC with Torch""""""<NewLine>""""""run.py:""""""<NewLine>#!/usr/bin/env python<NewLine>import os<NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>from torch.multiprocessing import Process<NewLine>import torch.distributed.rpc as rpc<NewLine><NewLine>def my_add(a,b):<NewLine>    return a+b<NewLine><NewLine>def run(rank, size):<NewLine>    tensor = torch.zeros(1)<NewLine>    if rank == 0:<NewLine>        rpc.init_rpc(""worker0"", rank=0, world_size=size)<NewLine>        ret = rpc.rpc_sync(""worker1"", my_add, args=(2,3))<NewLine>        print(ret)<NewLine>    else:<NewLine>        rpc.init_rpc(""worker1"", rank=1, world_size=size)<NewLine>    rpc.shutdown()<NewLine>    #print('Rank ', rank, ' has data ', tensor[0])<NewLine><NewLine>def init_process(rank, size, fn, backend='gloo'):<NewLine>    """""" Initialize the distributed environment. """"""<NewLine>    os.environ['MASTER_ADDR'] = '127.0.0.1'<NewLine>    os.environ['MASTER_PORT'] = '29500'<NewLine>    dist.init_process_group(backend, rank=rank, world_size=size)<NewLine>    fn(rank, size)<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    size = 2<NewLine>    processes = []<NewLine>    for rank in range(size):<NewLine>        p = Process(target=init_process, args=(rank, size, run))<NewLine>        p.start()<NewLine>        processes.append(p)<NewLine><NewLine>    for p in processes:<NewLine>        p.join()<NewLine></code></pre><NewLine><p>I try to run but the error is as below</p><NewLine><pre><code class=""lang-auto"">Process Process-45:<NewLine>Process Process-46:<NewLine>Traceback (most recent call last):<NewLine>Traceback (most recent call last):<NewLine>  File ""/Users/cnphuong/opt/anaconda3/lib/python3.7/multiprocessing/process.py"", line 297, in _bootstrap<NewLine>    self.run()<NewLine>  File ""/Users/cnphuong/opt/anaconda3/lib/python3.7/multiprocessing/process.py"", line 297, in _bootstrap<NewLine>    self.run()<NewLine>  File ""/Users/cnphuong/opt/anaconda3/lib/python3.7/multiprocessing/process.py"", line 99, in run<NewLine>    self._target(*self._args, **self._kwargs)<NewLine>  File ""/Users/cnphuong/opt/anaconda3/lib/python3.7/multiprocessing/process.py"", line 99, in run<NewLine>    self._target(*self._args, **self._kwargs)<NewLine>  File ""&lt;ipython-input-22-0e6d083442bd&gt;"", line 29, in init_process<NewLine>    fn(rank, size)<NewLine>  File ""&lt;ipython-input-22-0e6d083442bd&gt;"", line 29, in init_process<NewLine>    fn(rank, size)<NewLine>  File ""&lt;ipython-input-22-0e6d083442bd&gt;"", line 16, in run<NewLine>    rpc.init_rpc(""worker0"", rank=0, world_size=size)<NewLine>  File ""&lt;ipython-input-22-0e6d083442bd&gt;"", line 20, in run<NewLine>    rpc.init_rpc(""worker1"", rank=1, world_size=size)<NewLine>  File ""/Users/cnphuong/opt/anaconda3/lib/python3.7/site-packages/torch/distributed/rpc/__init__.py"", line 77, in init_rpc<NewLine>    store, _, _ = next(rendezvous_iterator)<NewLine>  File ""/Users/cnphuong/opt/anaconda3/lib/python3.7/site-packages/torch/distributed/rpc/__init__.py"", line 88, in init_rpc<NewLine>    api._init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)<NewLine>  File ""/Users/cnphuong/opt/anaconda3/lib/python3.7/site-packages/torch/distributed/rendezvous.py"", line 172, in _env_rendezvous_handler<NewLine>    store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)<NewLine>  File ""/Users/cnphuong/opt/anaconda3/lib/python3.7/site-packages/torch/distributed/rpc/api.py"", line 283, in _init_rpc_backend<NewLine>    rpc_backend_options=rpc_backend_options,<NewLine>  File ""/Users/cnphuong/opt/anaconda3/lib/python3.7/site-packages/torch/distributed/rpc/backend_registry.py"", line 75, in init_backend<NewLine>    return backend.value.init_backend_handler(*args, **kwargs)<NewLine>RuntimeError: Address already in use<NewLine>  File ""/Users/cnphuong/opt/anaconda3/lib/python3.7/site-packages/torch/distributed/rpc/backend_registry.py"", line 101, in _process_group_init_backend_handler<NewLine>    ""Default process group must not be initialized before init_rpc.""<NewLine>RuntimeError: Default process group must not be initialized before init_rpc.<NewLine></code></pre><NewLine><p>Please help!<br/><NewLine>Thanks.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>RuntimeError: Default process group must not be initialized before <code>init_rpc</code>.</p><NewLine></blockquote><NewLine><p>As the error suggested, <code>init_process_group</code> cannot be called before <code>init_rpc</code>, as currently DDP and RPC do not work together. We are working on dropping this requirement: <a href=""https://github.com/pytorch/pytorch/issues/33583"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/33583</a></p><NewLine><p>For the code snippet above, removing <code>dist.init_process_group(backend, rank=rank, world_size=size)</code> should work.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you so much!<br/><NewLine>On MPI, I can use distributed object to remote other workers do something with other worker’s distributed object.<br/><NewLine>On pytorch, I saw that it has <strong>Remote Reference (RRef)</strong>. However, I did not see how to create distributed object with Pytorch. Could you please suggest in this case?</p><NewLine><p>For example,<br/><NewLine>worker1 holds a dictionary with key and value DIC1.<br/><NewLine>worker0 will send an array KEY_SENDs with the keys to worker1. Worker1 will check DIC1 and return an array with the values from KEY_SENDs and DIC1.</p><NewLine><p>I try with</p><NewLine><pre><code class=""lang-auto"">def get_values(dic1, arr):<NewLine>    rest =  np.array([])<NewLine>    for i in arr:<NewLine>        rest= np.append(rest,dic1[i])<NewLine>    return rest<NewLine>        <NewLine><NewLine>def run(rank, size):<NewLine>    tensor = torch.zeros(1)<NewLine>    my_name = ""worker""+str(rank)<NewLine>    if rank == 0:<NewLine>        thisdict =	{4:2,2:6,3:8}<NewLine>        rpc.init_rpc(my_name,rank=0, world_size=size)<NewLine>        target = 1<NewLine>        target_name = ""worker""+str(target)<NewLine>        ret = rpc.rpc_sync(target_name, get_values, args=(thisdict,[1,2]))<NewLine>        print(str(ret) + "" is in rank 0 from rank 1"")<NewLine>    else:<NewLine>        thisdict =	{2:1,1:3,4:2}<NewLine>        rpc.init_rpc(my_name,rank=rank, world_size=size)<NewLine>    rpc.shutdown()<NewLine>    print(my_name)<NewLine><NewLine></code></pre><NewLine><p>I run it but no results were show on this example.</p><NewLine><p>Thanks,</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ph0123"">@ph0123</a></p><NewLine><p>I am not sure if I fully understand the use case. I created an example to show how to do intersect of a local dict and a remote dict. It shouldn’t too hard to extend this to do, e.g., a union or just sending keys. Let me know if this answers the question.</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.distributed.rpc as rpc<NewLine>import torch.multiprocessing as mp<NewLine>import os<NewLine><NewLine><NewLine>def create_dict():<NewLine>    return {1:'a', 2:'b', 3:'c'}<NewLine><NewLine><NewLine>def intersect_dict(dict1, dict2_rref):<NewLine>    ret = {}<NewLine>    for key in dict2_rref.local_value():<NewLine>        if key in dict1:<NewLine>            ret[key] = dict1[key]<NewLine><NewLine>    return ret<NewLine><NewLine><NewLine>def run(dst):<NewLine>    dict1 = {1:'a', 3:'c', 4:'d'}<NewLine>    dict2_rref = rpc.remote(dst, create_dict)<NewLine>    intersect = rpc.rpc_sync(dst, intersect_dict, args=(dict1, dict2_rref))<NewLine>    print(intersect)<NewLine><NewLine><NewLine>def run_worker(rank, world_size):<NewLine>    os.environ['MASTER_ADDR'] = 'localhost'<NewLine>    os.environ['MASTER_PORT'] = '29500'<NewLine>    if rank == 1:<NewLine>        rpc.init_rpc(""worker0"", rank=rank, world_size=world_size)<NewLine>        run(""worker1"")<NewLine>    else:<NewLine>        rpc.init_rpc(""worker1"", rank=rank, world_size=world_size)<NewLine><NewLine>    rpc.shutdown()<NewLine><NewLine><NewLine>if __name__==""__main__"":<NewLine>    world_size = 2<NewLine>    mp.spawn(run_worker, args=(world_size, ), nprocs=world_size, join=True)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>That’s so good.<br/><NewLine>However, I try to run your code with my PC. the output is</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>Exception                                 Traceback (most recent call last)<NewLine>&lt;ipython-input-1-c234795e743f&gt; in &lt;module&gt;<NewLine>     37     world_size = 2<NewLine>     38 <NewLine>---&gt; 39     mp.spawn(run_worker, args=(world_size, ), nprocs=world_size, join=True)<NewLine><NewLine>~/opt/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in spawn(fn, args, nprocs, join, daemon, start_method)<NewLine>    198                ' torch.multiprocessing.start_process(...)' % start_method)<NewLine>    199         warnings.warn(msg)<NewLine>--&gt; 200     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')<NewLine><NewLine>~/opt/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in start_processes(fn, args, nprocs, join, daemon, start_method)<NewLine>    156 <NewLine>    157     # Loop on join until it returns True or raises an exception.<NewLine>--&gt; 158     while not context.join():<NewLine>    159         pass<NewLine>    160 <NewLine><NewLine>~/opt/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in join(self, timeout)<NewLine>    111                 raise Exception(<NewLine>    112                     ""process %d terminated with exit code %d"" %<NewLine>--&gt; 113                     (error_index, exitcode)<NewLine>    114                 )<NewLine>    115 <NewLine><NewLine>Exception: process 1 terminated with exit code 1<NewLine></code></pre><NewLine><p>I try with</p><NewLine><pre><code class=""lang-auto"">""""""RPC with Torch""""""<NewLine>""""""run.py:""""""<NewLine>#!/usr/bin/env python<NewLine>import os<NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>from torch.multiprocessing import Process<NewLine>import torch.distributed.rpc as rpc<NewLine>import numpy as np<NewLine><NewLine>def create_dict():<NewLine>    return {1:'a', 2:'b', 3:'c'}<NewLine><NewLine><NewLine>def intersect_dict(dict1, dict2_rref):<NewLine>    ret = {}<NewLine>    for key in dict2_rref.local_value():<NewLine>        if key in dict1:<NewLine>            ret[key] = dict1[key]<NewLine>    return ret<NewLine><NewLine>def run(dst):<NewLine>    dict1 = {1:'a', 3:'c', 4:'d'}<NewLine>    dict2_rref = rpc.remote(dst, create_dict)<NewLine>    intersect = rpc.rpc_sync(dst, intersect_dict, args=(dict1, dict2_rref))<NewLine>    print(intersect)<NewLine>        <NewLine>def init_process(rank, size, fn, backend='gloo'):<NewLine>    """""" Initialize the distributed environment. """"""<NewLine>    os.environ['MASTER_ADDR'] = '127.0.0.1'<NewLine>    os.environ['MASTER_PORT'] = '29500'<NewLine>    #dist.init_process_group(backend, rank=rank, world_size=size)<NewLine>    #fn(rank, size)<NewLine>    if rank == 1:<NewLine>        rpc.init_rpc(""worker0"", rank=rank, world_size=world_size)<NewLine>        fn(""worker1"")<NewLine>    else:<NewLine>        rpc.init_rpc(""worker1"", rank=rank, world_size=world_size)<NewLine><NewLine>    rpc.shutdown()<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    size = 2<NewLine>    processes = []<NewLine>    for rank in range(size):<NewLine>        p = Process(target=init_process, args=(rank, size, run))<NewLine>        p.start()<NewLine>        processes.append(p)<NewLine><NewLine>    for p in processes:<NewLine>        p.join()<NewLine></code></pre><NewLine><p>And it worked on my PC.</p><NewLine><p>Could you please explain why mp.spawn error in this case?</p><NewLine><p>thanks,</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can I create the dict2 next to rpc.init_rpc as</p><NewLine><pre><code class=""lang-auto"">if rank == 1:<NewLine>        rpc.init_rpc(""worker0"", rank=rank, world_size=world_size)<NewLine>        fn(""worker1"")<NewLine>    else:<NewLine>        # initial dict2 here.<NewLine>        rpc.init_rpc(""worker1"", rank=rank, world_size=world_size)<NewLine></code></pre><NewLine><p>My target is that each rank will hold their dictionary, and this dictionary is always available in the program. In your code, If I repeat the run() function, The dict1 is initial again.</p><NewLine><p>For example, rank 0 call run function on rank 1. Your codes is good. But I wanna run it several time, and after calling function on rank 1. The rank 1 changes something in the dictionary. Then, in the next iteration, rank0 wanna get the value from updated dictionary on rank1.</p><NewLine><p>Thanks,</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s weird, I don’t know why <code>mp.spawn</code> would fail. Is it because it has to be <code>127.0.0.1</code> instead of <code>localhost</code> in your env? We can try add more logs/try-except to identify which line crashed.</p><NewLine><blockquote><NewLine><p>My target is that each rank will hold their dictionary, and this dictionary is always available in the program. In your code, If I repeat the run() function, The dict1 is initial again.</p><NewLine></blockquote><NewLine><p>If this is the only concern, you can move that <code>rpc.remote</code> call to <code>create_dict()</code> out of the <code>run</code> function and do it before the training loop?</p><NewLine><blockquote><NewLine><p>For example, rank 0 call run function on rank 1. Your codes is good. But I wanna run it several time, and after calling function on rank 1. The rank 1 changes something in the dictionary. Then, in the next iteration, rank0 wanna get the value from updated dictionary on rank1.</p><NewLine></blockquote><NewLine><p>Sure, there are many ways to solve this. For example, you can define the dict as a global value so that each process will have its own copy. Then in <code>intersect_dict</code>, just read from that global value instead of passing the <code>RRef</code> around.</p><NewLine><p>Or, you can have a master that tells all workers to initialize their states upfront and then get <code>RRef</code>s of those states as return values.</p><NewLine><p>If you need to construct an <code>RRef</code> from that dict, you can also use <code>rpc.RRef(dict_obj)</code> to create a local <code>RRef</code> and then pass that around through RPC.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>Thank you so much!<br/><NewLine>I search the documents, but I did not know how to make a dictionary as a global value with PyTorch.<br/><NewLine>I worked with MPI and UPC++. MPI provides a <strong>distributed object</strong>, which helps each worker to hold their values. UPC++ also provides <strong>distributed objects</strong> that are a similar name on all workers but different memory locations. Additionally, UPC++ creates <strong>Global Pointers</strong> to remote from other workers. I quit easy.</p><NewLine><p>However, I did not know these kinds of variables in Pytorch documents.<br/><NewLine>Do you have any suggestion?</p><NewLine><p>I have another question. If i run with several iterations, is there any function to waiting all workers as barrier() function.</p><NewLine><p>Thanks.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ph0123"">@ph0123</a></p><NewLine><p>I was referring to Python global vars, sth like:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.distributed.rpc as rpc<NewLine>import torch.multiprocessing as mp<NewLine>import os<NewLine><NewLine><NewLine>_local_dict = {}<NewLine><NewLine><NewLine>def intersect_dict(dict1):<NewLine>    ret = {}<NewLine>    for key in _local_dict:<NewLine>        if key in dict1:<NewLine>            ret[key] = dict1[key]<NewLine><NewLine>    return ret<NewLine><NewLine><NewLine>def run(dst):<NewLine>    intersect = rpc.rpc_sync(dst, intersect_dict, args=(_local_dict,))<NewLine>    print(intersect)<NewLine><NewLine><NewLine>def run_worker(rank, world_size):<NewLine>    os.environ['MASTER_ADDR'] = '127.0.0.1'<NewLine>    os.environ['MASTER_PORT'] = '29500'<NewLine>    if rank == 0:<NewLine>        _local_dict.update({1:'a', 3:'c', 4:'d'})<NewLine>        rpc.init_rpc(""worker0"", rank=rank, world_size=world_size)<NewLine>        run(""worker1"")<NewLine>    else:<NewLine>        _local_dict.update({1:'a', 2:'b', 3:'c'})<NewLine>        rpc.init_rpc(""worker1"", rank=rank, world_size=world_size)<NewLine><NewLine>    rpc.shutdown()<NewLine><NewLine><NewLine>if __name__==""__main__"":<NewLine>    world_size = 2<NewLine>    mp.spawn(run_worker, args=(world_size, ), nprocs=world_size, join=True)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Wow. It worked.<br/><NewLine>Thank you for your supports.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I try with the loop.<br/><NewLine>After each iteration, I used <code>dist.barrier()</code>, and the program is not stop because of waiting others rpc.</p><NewLine><p>What is the problem with it?</p><NewLine><p>Thanks,</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>the program is not stop because of waiting others rpc.</p><NewLine></blockquote><NewLine><p>What does this mean by “the program is not stop”? If you are referring to the behavior that some RPC requests are still running in background, then yes, this is the expected behavior. Because RPC server has its own thread pool to handle requests, and <code>dist.barrier</code> only blocks the thread that runs it. Why do you want to combine <code>dist.barrier()</code> with RPC? Is this just to conclude an iteration?</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>HI,<br/><NewLine>I did not know this is not work with RPC.<br/><NewLine>Because of my algorithm, step (k+1) is depended on step k.<br/><NewLine>I want to make sure that all worker finished before starting new iteration.</p><NewLine><blockquote><NewLine><p>each worker hold their own dictionary.<br/><NewLine>step k:<br/><NewLine>call rpc to other workers to check and update dictionary values, which is based on value on step (k-1).<br/><NewLine><span class=""hashtag"">#I</span> put barrier here<br/><NewLine>step k+1:<br/><NewLine>call rpc to other workers to check and update dictionary values, which is based on value on step (k).<br/><NewLine>…</p><NewLine></blockquote><NewLine><p>with RPC pytorch, Could you please sugges any function like dis.barrier()?</p><NewLine><p>Thanks,</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey,</p><NewLine><p><code>dist.barrier()</code> and RPC should both work as expected. The contract <code>dist.barrier()</code> offers is only blocking until all participating threads reach the same barrier. So, as RPC threads in background are not calling <code>dist.barrier</code>, they are not part of that synchronization.</p><NewLine><p>If you would like to synchronize all RPCs, one option is doing sth similar to <code>_wait_all_workers</code> as linked below. It uses rank 0 as a coordinator to tell all others when to proceed.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/479b04e26a59d20b72ad5fdaec819caaad49af75/torch/distributed/rpc/api.py#L137-L199"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/479b04e26a59d20b72ad5fdaec819caaad49af75/torch/distributed/rpc/api.py#L137-L199"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/479b04e26a59d20b72ad5fdaec819caaad49af75/torch/distributed/rpc/api.py#L137-L199</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""137"" style=""counter-reset: li-counter 136 ;""><NewLine><li>@_require_initialized</li><NewLine><li>def _wait_all_workers():</li><NewLine><li>    r""""""</li><NewLine><li>    Block until all local and remote RPC processes reach this method and wait</li><NewLine><li>    for all outstanding work to complete. Every RPC process must call this</li><NewLine><li>    method before exit to perform a graceful shutdown. This should be used to</li><NewLine><li>    terminate the RPC framework, and there is no guarantee that the RPC</li><NewLine><li>    framework will work after this method returns.</li><NewLine><li>    """"""</li><NewLine><li>    assert (</li><NewLine><li>        _ALL_WORKER_NAMES is not None</li><NewLine><li>    ), ""`_ALL_WORKER_NAMES` is not initialized for `def _wait_all_workers`.""</li><NewLine><li>    leader_worker_name = sorted(_ALL_WORKER_NAMES)[0]</li><NewLine><li><NewLine></li><li>    self_worker_name = _get_current_rpc_agent().get_worker_info().name</li><NewLine><li><NewLine></li><li>    global _wait_all_workers_sequence_id</li><NewLine><li>    with _wait_all_workers_dict_lock:</li><NewLine><li>        sequence_id = _wait_all_workers_sequence_id</li><NewLine><li>        _wait_all_workers_sequence_id += 1</li><NewLine></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/479b04e26a59d20b72ad5fdaec819caaad49af75/torch/distributed/rpc/api.py#L137-L199"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>In general, all we need for a synchronization is to join background threads into the current thread that calls <code>dist.barrier</code>. For example, the following code won’t work, as <code>some_func</code> is running on a different thread.</p><NewLine><pre><code class=""lang-python"">rpc.rpc_async(to, some_func)<NewLine>dist.barrier()<NewLine></code></pre><NewLine><p>However, the code below should work, as it blocks the current thread before calling the <code>barrier</code></p><NewLine><pre><code class=""lang-python"">fut = rpc.rpc_async(to, some_func)<NewLine>fut.wait()<NewLine>dist.barrier()<NewLine></code></pre><NewLine><p>So, as a summary, if you would like to use the <code>dist.barrier</code> to synchronize RPCs, the application will need to collect the futures created in one iteration, and wait for all of them to complete before calling <code>barrier</code>.</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I got it.<br/><NewLine>Thank you so much!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/ph0123; <NewLine> ,"REPLY_DATE 1: April 26, 2020,  8:03pm; <NewLine> REPLY_DATE 2: April 26, 2020,  8:03pm; <NewLine> REPLY_DATE 3: May 26, 2020, 10:14pm; <NewLine> REPLY_DATE 4: May 26, 2020, 10:33pm; <NewLine> REPLY_DATE 5: May 27, 2020,  9:45am; <NewLine> REPLY_DATE 6: May 27, 2020,  2:48pm; <NewLine> REPLY_DATE 7: May 27, 2020,  3:42pm; <NewLine> REPLY_DATE 8: May 27, 2020,  5:56pm; <NewLine> REPLY_DATE 9: May 27, 2020,  6:18pm; <NewLine> REPLY_DATE 10: May 27, 2020,  7:59pm; <NewLine> REPLY_DATE 11: May 27, 2020,  8:01pm; <NewLine> REPLY_DATE 12: May 27, 2020,  9:44pm; <NewLine> REPLY_DATE 13: June 6, 2020,  4:20pm; <NewLine> REPLY_DATE 14: June 6, 2020,  5:54pm; <NewLine> REPLY_DATE 15: June 6, 2020,  7:28pm; <NewLine> REPLY_DATE 16: June 6, 2020, 11:22pm; <NewLine> REPLY_DATE 17: June 7, 2020, 10:16pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: 1 Like; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: 1 Like; <NewLine> 
83299,Load dataparallel model,2020-05-29T01:52:49.716Z,2,392,"<div class=""post"" itemprop=""articleBody""><NewLine><p>hi,i am loading the model under DataParallel,below is my code:</p><NewLine><pre><code class=""lang-auto"">#training <NewLine>model = build_model()<NewLine>model = nn.DataParallel(model)<NewLine>model.cuda()<NewLine>....training...<NewLine>torch.save(model,name)<NewLine><NewLine>#eval<NewLine>net = torch.load(name)<NewLine></code></pre><NewLine><p>i save the whole model under DataParallel form,not just state_dict,and i load the model directly for eval,i am wondering is this right?will it get the same performance as save the state_dict and load the state_dict?  thank you so much!!</p><NewLine></div>",https://discuss.pytorch.org/u/xwjBupt,(Xwj Bupt),xwjBupt,"May 29, 2020,  1:52am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You should get the same performance.<br/><NewLine>However, your current approach assumes that all source files contain the same definitions and are in the same location. Also, <code>torch.load</code> would load the <code>nn.DataParallel</code> model and might assume the same number of GPUs in your system (not sure about it and you would have to test it).<br/><NewLine>Given these disadvantages I always recommend to store the <code>state_dict</code> in order to get some flexibility of loading the model later.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>ok ,i understand thank you!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi! i use torch.save(net) to save model structure and weights ,but i change the model strcucture later,if i use newnet = torch.load(net) to load the saved .pth file now,will the newnet forward dataflow flow as i change the model structure before or after?thanks !!!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I would assume that the new model definition will be used, but I never tested it.<br/><NewLine>Note that I’m not using this approach, as it might break in several ways. E.g. you might not be able to load the model again, if you change the file structure or the definition too much.</p><NewLine><p>Creating the new model instance directly and just loading the <code>state_dict</code> is the cleaner way.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>i understand,but the reason why i use the torch.save() not the state_dict is that i hope to load the model and weight directly while don`t need to define the model first,because the my model structure may change many times,if i just save the state dict when i was training, i may not remember the exactly structure version when i evaluate the model,but load state dict need to define the model structure first which may cause the state dict between new model and loaded model do not match</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think you can avoid this issue.<br/><NewLine>When you are saving and loading the <code>state_dict</code>, you would need to create the model first, that’s correct.<br/><NewLine>The model definition might have changed and you might get mismatches in the <code>state_dict</code>.</p><NewLine><p>However, in your current approach, the same mismatch might happen and might be hidden behind the <code>torch.load</code> model.<br/><NewLine>E.g. this simple use case demonstrates it:</p><NewLine><pre><code class=""lang-python""># model.py<NewLine>import torch<NewLine>import torch.nn as nn<NewLine><NewLine>class MyModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(MyModel, self).__init__()<NewLine>        self.conv1 = nn.Conv2d(1, 3, 3, 1, 1)<NewLine>        self.fc = nn.Linear(3*24*24, 1)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.conv1(x)<NewLine>        x = x.view(x.size(0), -1)<NewLine>        x = self.fc(x)<NewLine>        return x<NewLine><NewLine># save.py<NewLine>import torch<NewLine><NewLine>from model import MyModel<NewLine><NewLine>net = MyModel()<NewLine>x = torch.randn(1, 1, 24, 24)<NewLine>out = net(x)<NewLine><NewLine>torch.save(net, 'tmp.pt')<NewLine><NewLine># load.py<NewLine>import torch<NewLine><NewLine>net = torch.load('tmp.pt')<NewLine>x = torch.randn(1, 1, 24, 24)<NewLine>out = net(x)<NewLine>print(out.shape)<NewLine></code></pre><NewLine><p>After changing the linear layer in <code>model.py</code> to <code>nn.Linear(3*24*24, 10)</code> and executing <code>load.py</code> I get:</p><NewLine><pre><code class=""lang-python"">/opt/conda/lib/python3.6/site-packages/torch/serialization.py:644: SourceChangeWarning: source code of class 'model.MyModel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.<NewLine></code></pre><NewLine><p>Which already sounds hard to debug, but at least the code is still running. Note that the code still returns an output of <code>[1, 1]</code>, which corresponds to the initial definition not the new one, which might be even harder to debug.</p><NewLine><p>After changing the name of <code>self.fc</code> to <code>self.fc_new</code> and executing <code>load.py</code> I get:</p><NewLine><pre><code class=""lang-python"">/opt/conda/lib/python3.6/site-packages/torch/serialization.py:644: SourceChangeWarning: source code of class 'model.MyModel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.<NewLine>  warnings.warn(msg, SourceChangeWarning)<NewLine>Traceback (most recent call last):<NewLine>  File ""load.py"", line 5, in &lt;module&gt;<NewLine>    out = net(x)<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 577, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/workspace/src/model.py"", line 13, in forward<NewLine>    x = self.fc_new(x)<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 621, in __getattr__<NewLine>    type(self).__name__, name))<NewLine>torch.nn.modules.module.ModuleAttributeError: 'MyModel' object has no attribute 'fc_new'<NewLine></code></pre><NewLine><p>As already said, use it at your own risk, but I would strongly advice against it, as I think debugging these issues in a “real” model might take a lot of time.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xwjBupt; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/xwjBupt; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/xwjBupt; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: May 29, 2020,  9:44am; <NewLine> REPLY_DATE 2: May 29, 2020, 10:19am; <NewLine> REPLY_DATE 3: June 5, 2020,  9:22am; <NewLine> REPLY_DATE 4: June 5, 2020,  9:34am; <NewLine> REPLY_DATE 5: June 5, 2020,  9:42am; <NewLine> REPLY_DATE 6: June 5, 2020, 10:00am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
80000,Freezing problem while using cuda tensor in multiprocessing environment,2020-05-06T12:59:19.770Z,1,163,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The main process in the below code snippet will freeze after serveral iterations.</p><NewLine><p>I think it’s related to how the data structure stack in pytorch works, tensors are built upon storage classes, and storage classes are build upon raw cudaMalloc regions. I can understand what <code>recuce_tensor()</code> and <code>rebuild_cuda_tensor()</code> are doing, but I am not sure why creating a new tensor (since g_tensor has been reassigned) after  a blocking starmap would somehow causing the main process to freeze.</p><NewLine><p><strong>Code example</strong></p><NewLine><pre><code class=""lang-auto"">import itertools as it<NewLine>import torch as t<NewLine>import torch.multiprocessing as mp<NewLine><NewLine><NewLine>def infer(id, tensor):<NewLine>    print(id)<NewLine>    print(tensor)<NewLine>    # del tensor immediately doesn't solve the problem<NewLine>    del tensor<NewLine><NewLine># some global tensor<NewLine>g_tensor = t.full([1000, 1000], 2, device=""cuda:0"")<NewLine>g_tensor.share_memory_()<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    ctx = mp.get_context(""spawn"")<NewLine>    pool = ctx.Pool(2)<NewLine>    for i in range(10000000):<NewLine>        print(""start"")<NewLine>        pool.starmap(infer, zip(range(5), it.repeat(g_tensor)))<NewLine><NewLine>        # cpu tensors work just fine<NewLine>        # for cuda tensors:<NewLine>        # if I delete the global tensor, reassign it with a new cuda tensor<NewLine>        # or if I use a tensor created dynamically in each iteration<NewLine>        # the program freezes after 2 iterations.<NewLine>        # Comment out the following lines and everything will work fine.<NewLine>        del g_tensor<NewLine>        g_tensor = t.full([1000, 1000], 2, device=""cuda:0"")<NewLine>        g_tensor.share_memory_()<NewLine></code></pre><NewLine><h2><strong>Environment</strong></h2><NewLine><ul><NewLine><li>PyTorch Version (e.g., 1.0): 1.1.0</li><NewLine><li>OS (e.g., Linux): Linux</li><NewLine><li>How you installed PyTorch (conda, pip, source): pip</li><NewLine><li>Python version: 3.5</li><NewLine><li>CUDA/cuDNN version: 9.1/7.2.1</li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/iffiX,(Iffi),iffiX,"June 4, 2020,  4:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a><br/><NewLine>I am sorting out my framework today and as of pytorch version 1.5.0, this problem still persists. Is it normal?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Tried this locally, it hangs with a non-deterministic number of iterations (4, 11, etc.), and it hangs at <code>del g_tensor</code>. I suspect it is due to <a href=""https://github.com/pytorch/pytorch/blob/master/c10/cuda/CUDACachingAllocator.cpp"" rel=""nofollow noopener"">CUDACachingAllocator</a>, where it might need to keep the memory block alive until all other processes finish using it? But it is a per-process data structure, so it does not have the global view.  I am not sure if this is the reason for this hang.</p><NewLine><p>Call for help cc <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> <a class=""mention"" href=""/u/colesbury"">@colesbury</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for following up on this. I filed an issue to track the problem on GitHub:</p><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/39541"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/39541"" target=""_blank"">Deadlock with shared CUDA tensors and multiprocessing (spawn)</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-06-04"" data-format=""ll"" data-time=""21:48:25"" data-timezone=""UTC"">09:48PM - 04 Jun 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/colesbury"" target=""_blank""><NewLine><img alt=""colesbury"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars1.githubusercontent.com/u/655866?v=4"" width=""20""/><NewLine>          colesbury<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">🐛 Bug<NewLine>Reported by iffiX in https://discuss.pytorch.org/t/freezing-problem-while-using-cuda-tensor-in-multiprocessing-environment/80000.<NewLine>(I have moved the code into a main() function to rule out some other multiprocessing bugs)<NewLine>import...</p><NewLine></div><NewLine><div class=""labels""><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>It looks like the deadlock is in <code>CudaIPCSentData</code> destructor, which looks separate from the caching allocator code.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/colesbury; <NewLine> ,"REPLY_DATE 1: June 4, 2020,  4:25pm; <NewLine> REPLY_DATE 2: June 4, 2020,  7:56pm; <NewLine> REPLY_DATE 3: June 4, 2020,  9:51pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 3 Likes; <NewLine> 
84047,About distributed autograd and optimizer,2020-06-03T13:04:45.162Z,5,151,"<div class=""post"" itemprop=""articleBody""><NewLine><p>What’s the best practice of using distributed autograd and optimizer?<br/><NewLine>Is it designed to apply the master-slave paradigm, where a master distributes task to slaves and collect results, then performes gradient optimization.<br/><NewLine>Or is it designed to apply a client-host paradigm, where clients actively push results (such as gradients) to the host and host performs post processing like reduction?</p><NewLine><p>I have read your documentation and code in examples. Personally I think it is the first one, though I am not sure what your future plans are, I am currently laying more application layers on rpc and I would like to know your design ideas.</p><NewLine></div>",https://discuss.pytorch.org/u/iffiX,(Iffi),iffiX,"June 3, 2020,  1:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The second question: since your implementation of rpc is using <a href=""https://pytorch.org/docs/stable/rpc/distributed_autograd.html#fast-mode-algorithm"" rel=""nofollow noopener"">the FAST &amp; SMART mode algorithm</a>, I have also read the detailed <a href=""https://github.com/pytorch/pytorch/issues/23110"" rel=""nofollow noopener"">PR</a> on your github. I think your workers are <strong>stateful</strong>, so what will happen if during an execution of the rpc context, some worker fail? like the below scenario in your rpc parameter server example:</p><NewLine><pre><code class=""lang-auto"">       with dist_autograd.context() as cid:<NewLine>            model_output = net(data)<NewLine>            target = target.to(model_output.device)<NewLine>            loss = F.nll_loss(model_output, target)<NewLine>            if i % 5 == 0:<NewLine>                print(f""Rank {rank} training batch {i} loss {loss.item()}"")<NewLine>            dist_autograd.backward(cid, [loss])<NewLine>            # Ensure that dist autograd ran successfully and gradients were<NewLine>            # returned.<NewLine>            assert remote_method(<NewLine>                ParameterServer.get_dist_gradients,<NewLine>                net.param_server_rref,<NewLine>                cid) != {}<NewLine>            opt.step(cid)<NewLine></code></pre><NewLine><p>what kind of exeptions will be thrown on the if the failure occurs in forward &amp; backward pass?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/iffix"">@iffiX</a>, thanks for the question.</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""84047"" data-username=""iffiX""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/iffix/40/24443_2.png"" width=""20""/> iffiX:</div><NewLine><blockquote><NewLine><p>Is it designed to apply the master-slave paradigm, where a master distributes task to slaves and collect results, then performes gradient optimization.<br/><NewLine>Or is it designed to apply a client-host paradigm, where clients actively push results (such as gradients) to the host and host performs post processing like reduction?</p><NewLine></blockquote><NewLine></aside><NewLine><p>They are designed in the way that all RPC workers are peers, such that each RPC worker has its own server running in background, and any RPC worker A can talk to any other RPC worker B by sending a function to run on B. In the foreseeable future, we are not going to change this design.</p><NewLine><p>We do see many applications using RPC to build master-slave applications, where one worker serves as a master telling everyone else what to do. One example is <a href=""https://pytorch.org/tutorials/intermediate/rpc_tutorial.html#distributed-reinforcement-learning-using-rpc-and-rref"" rel=""nofollow noopener"">this tutorial</a> where the agent is serving as the master, and commanding all observers.</p><NewLine><p>It is also possible to build server-client applications using the RPC API. Here is an example: <a href=""https://github.com/pytorch/examples/pull/702/files"" rel=""nofollow noopener"">https://github.com/pytorch/examples/pull/702/files</a></p><NewLine><p>In general, we try to make the RPC API as flexible as possible, and hope it can support a wide range of applications. It’s up to the application developers to decide how to decompose the entire logic into multiple functions and how to stitch those functions together using RPC.</p><NewLine><p>Regarding distributed autograd and distributed optimizer, the worker that creates the distributed autograd context serves as the driver for its backward pass and optimizer step. But there can be multiple drivers in a cluster. One example would be <a href=""https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html"" rel=""nofollow noopener"">this tutorial</a>, where we launch one Parameter-Server process that passively sits there waiting for requests from trainers and we also launch multiple trainer processes with each trainer actively running a training loop. In this case, every trainer serves as a driver of its own backward pass and optimizer step.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""84047"" data-username=""iffiX""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/iffix/40/24443_2.png"" width=""20""/> iffiX:</div><NewLine><blockquote><NewLine><p>I think your workers are <strong>stateful</strong> , so what will happen if during an execution of the rpc context, some worker fail?</p><NewLine></blockquote><NewLine></aside><NewLine><p>This is true, they are stateful, and states including RRef context, distributed autograd context and application states. In the current version, an RPC gang cannot survive any node crash. We are actively collaborating with <a href=""https://pytorch.org/elastic"" rel=""nofollow noopener"">torchelastic</a> to provide fault-tolerance and elasticity support.</p><NewLine><blockquote><NewLine><p>what kind of exeptions will be thrown on the if the failure occurs in forward &amp; backward pass?</p><NewLine></blockquote><NewLine><p>If the error is recoverable, e.g., just an exception instead of hard node crash, RPC should throw the same exception (or a <code>RuntimeError</code>, this is not great, still working on improvements) on the caller. Below are some example tests:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/c767d65cafc2615bcee9ec1bbc8d5dadc07ea0e8/torch/testing/_internal/distributed/rpc/dist_autograd_test.py#L1204-L1210"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/c767d65cafc2615bcee9ec1bbc8d5dadc07ea0e8/torch/testing/_internal/distributed/rpc/dist_autograd_test.py#L1204-L1210"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/c767d65cafc2615bcee9ec1bbc8d5dadc07ea0e8/torch/testing/_internal/distributed/rpc/dist_autograd_test.py#L1204-L1210</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""1204"" style=""counter-reset: li-counter 1203 ;""><NewLine><li>def test_backward_autograd_engine_error(self):</li><NewLine><li>    with dist_autograd.context() as context_id:</li><NewLine><li>        t1 = torch.rand((3, 3), requires_grad=True)</li><NewLine><li>        t2 = torch.rand((3, 3), requires_grad=True)</li><NewLine><li>        # Perform some ops before error simulation.</li><NewLine><li>        tmp = (t1 + t2) * (t1 + t2)</li><NewLine><li>        t3 = SimulateBackwardError.apply(tmp)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/c767d65cafc2615bcee9ec1bbc8d5dadc07ea0e8/torch/testing/_internal/distributed/rpc/rpc_test.py#L1284-L1290"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/c767d65cafc2615bcee9ec1bbc8d5dadc07ea0e8/torch/testing/_internal/distributed/rpc/rpc_test.py#L1284-L1290"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/c767d65cafc2615bcee9ec1bbc8d5dadc07ea0e8/torch/testing/_internal/distributed/rpc/rpc_test.py#L1284-L1290</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""1284"" style=""counter-reset: li-counter 1283 ;""><NewLine><li>@dist_init</li><NewLine><li>def test_py_raise_in_user_func(self):</li><NewLine><li>    n = self.rank + 1</li><NewLine><li>    dst_rank = n % self.world_size</li><NewLine><li>    fut = rpc.rpc_async(worker_name(dst_rank), raise_func)</li><NewLine><li>    with self.assertRaises(ValueError):</li><NewLine><li>        fut.wait()</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <strong>mrshenli</strong>, you are so helpful!</p><NewLine><p><strong>About my question:</strong><br/><NewLine>Well the first question is actually about the autograd based on rpc, and not rpc apis themselves, but on hindsight I think it is not really a good question, so lets set it aside. The second answer is sufficient. I think I cannot use the distributed autograd now as they are a little bit too brittle.</p><NewLine><p><strong>About distributed programming:</strong><br/><NewLine>Indeed, I have noticed your amazing work on <code>torchelastic</code>, I have thought about your rpc design, and find that it could be very hard for you to implement a mechanism which rejoins an rpc worker after their crash (whether a hard node crash or a soft crash like an exception), since you are blocking all processes on init and exchange their connection information, it would be overly complex to try and recover these information on crash.</p><NewLine><p>Therefore, I instead worked out another solution to provide some robustness for raw rpc apis, since under some conditions the “map-reduce” (very similar so I will use this description, since workers will receive their work portion when they reach the start barrier) paradigm in torchelastic Rendezvous is not very handy, and rpc is just right.</p><NewLine><p>My solution basically implements a stable distributed election process &amp; perfect failure detector based on rpc, and let each worker in the rpc group takes one/many role(s), when a worker failes (hard/soft), its role will be reassigned to other workers and this worker is marked as tainted  permanently. Users only needs to declare the number of total workers on start and  specify what a role have to do when they are: initialized, running, and stopped. Adds a little bit overhead on start &amp; worker failure, but near zero overhead during a normal rpc call.</p><NewLine><p>My future plan is to integrate my framework with NNI tuner developed by the microsoft team, so I can have some taste of distributed RL training on kubernetes.</p><NewLine><p>That’s all for now, thank you again! <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""84047"" data-username=""iffiX""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/iffix/40/24443_2.png"" width=""20""/> iffiX:</div><NewLine><blockquote><NewLine><p>since you are blocking all processes on init and exchange their connection information, it would be overly complex to try and recover these information on crash.</p><NewLine></blockquote><NewLine></aside><NewLine><p>The rendezvous in RPC init will be gone in future releases (not sure which release yet <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/> ). We are thinking about using <code>c10d::Store</code> + listeners to support dynamic worker join/leave.</p><NewLine><blockquote><NewLine><p>My solution basically implements a stable distributed election process &amp; perfect failure detector based on rpc, and let each worker in the rpc group takes one/many role(s), when a worker failes (hard/soft), its role will be reassigned to other workers and this worker is marked as tainted permanently. Users only needs to declare the number of total workers on start and specify what a role have to do when they are: initialized, running, and stopped. Adds a little bit overhead on start &amp; worker failure, but near zero overhead during a normal rpc call.</p><NewLine></blockquote><NewLine><p>This is awesome! Are these in an open-source repo? It will be great if we could have the opportunity to learn more about it.  cc <a class=""mention"" href=""/u/kiuk_chung"">@Kiuk_Chung</a></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Dynamic worker join/leave is great! With that we can just discard failed workers and start new ones, looking forward to that.</p><NewLine><p>I am still writing docs for my framework, and haven’t begin the unit test &amp; CI/CD process yet <img alt="":rofl:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/rofl.png?v=9"" title="":rofl:""/> <img alt="":rofl:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/rofl.png?v=9"" title="":rofl:""/></p><NewLine><p>Currently my framework aims more at RL things, so tests for RL algortihms will come first, distributed part is mainly aimed at resource hungry algorithms such as DQN-APEX, DDPG-APEX, IMPALA etc., and multi agent scenes. I really hope to complete code, doc and test for major RL algorithms part <strong>before July</strong>, then completes distributed scenes <strong>before August / September</strong> so you will have to wait a l-i-t-t-le more. <img alt="":rofl:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/rofl.png?v=9"" title="":rofl:""/> <img alt="":rofl:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/rofl.png?v=9"" title="":rofl:""/></p><NewLine><p>They will be in a new github repo, and I will post updates as soon as possible.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Great! Looking forward to that!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 3, 2020,  1:40pm; <NewLine> REPLY_DATE 2: June 3, 2020,  2:53pm; <NewLine> REPLY_DATE 3: June 3, 2020,  3:35pm; <NewLine> REPLY_DATE 4: June 3, 2020,  3:39pm; <NewLine> REPLY_DATE 5: June 3, 2020,  3:41pm; <NewLine> REPLY_DATE 6: June 3, 2020,  3:50pm; <NewLine> REPLY_DATE 7: June 3, 2020,  3:51pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> 
83954,How does pytorch transfer data between GPUs,2020-06-02T21:31:55.862Z,2,162,"<div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""onebox"" href=""https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html</a><br/><NewLine>This page explains how we can split a big nn model on multiple GPUs. But I don’t understand when the intermediate training result needs to be transferred from one GPU to another (by using “.to(‘cuda:1’)”), does pytorch runtime moves the data to CPU mem first and then to another gpu’s mem, or pytorch actually utilizes some direct data transfer technique between GPUs like SLI?</p><NewLine><p>Any help would be appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/Yanan,(tuinannan g),Yanan,"June 2, 2020,  9:31pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/yanan"">@Yanan</a></p><NewLine><p>In this case, the tensor will be directly copied from device to device using <code>cudaMemcpyAsync</code>. The C++ implementation is linked below:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/89c0efb30b5bad49e717c1b1a797bac3c62e8b7e/aten/src/ATen/native/cuda/Copy.cu#L56-L67"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/89c0efb30b5bad49e717c1b1a797bac3c62e8b7e/aten/src/ATen/native/cuda/Copy.cu#L56-L67"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/89c0efb30b5bad49e717c1b1a797bac3c62e8b7e/aten/src/ATen/native/cuda/Copy.cu#L56-L67</a></h4><NewLine><pre class=""onebox""><code class=""lang-cu""><ol class=""start lines"" start=""56"" style=""counter-reset: li-counter 55 ;""><NewLine><li>if (memcpy_eligible) {</li><NewLine><li>  void *dst = iter.data_ptr(0);</li><NewLine><li>  void *src = iter.data_ptr(1);</li><NewLine><li>  size_t size = numel * iter.element_size(0);</li><NewLine><li>  if (src != dst || src_device != dst_device) {</li><NewLine><li>    // Perform the copy</li><NewLine><li>    AT_CUDA_CHECK(cudaMemcpyAsync(</li><NewLine><li>        dst, src, size,</li><NewLine><li>        cudaMemcpyDeviceToDevice,</li><NewLine><li>        copy_stream));</li><NewLine><li>  }</li><NewLine><li>} else {</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> Thanks! This really helps.<br/><NewLine>I have another question that does data copy  between GPUs in DataParallel module utilize the same kernel?<br/><NewLine>Sorry I forgot to ask this before.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I have another question that does data copy between GPUs in DataParallel module utilize the same kernel?</p><NewLine></blockquote><NewLine><p>Yes. <code>DataParallel</code> calls into scatter. Its C++ implementation is linked below. It’s basically calling <code>Tensor.to</code> in a loop</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/89c0efb30b5bad49e717c1b1a797bac3c62e8b7e/torch/csrc/cuda/comm.cpp#L201-L206"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/89c0efb30b5bad49e717c1b1a797bac3c62e8b7e/torch/csrc/cuda/comm.cpp#L201-L206"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/89c0efb30b5bad49e717c1b1a797bac3c62e8b7e/torch/csrc/cuda/comm.cpp#L201-L206</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""201"" style=""counter-reset: li-counter 200 ;""><NewLine><li>chunks[chunk] =</li><NewLine><li>    chunks[chunk].to(</li><NewLine><li>        {DeviceType::CUDA, device_index},</li><NewLine><li>        /*non_blocking=*/true,</li><NewLine><li>        /*copy=*/false,</li><NewLine><li>        /*memory_format=*/at::MemoryFormat::Preserve);</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Yanan; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 2, 2020,  9:39pm; <NewLine> REPLY_DATE 2: June 2, 2020,  9:43pm; <NewLine> REPLY_DATE 3: June 2, 2020,  9:54pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
83335,PyTorch distributed example code hang. Deadlock?,2020-05-29T07:32:00.587Z,2,260,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to run the example code from the pytorch distributed tutorial (<a href=""https://pytorch.org/tutorials/intermediate/dist_tuto.html"" rel=""nofollow noopener"">dist_tuto.html</a>).</p><NewLine><p>Here is my exact code:</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>from torch.multiprocessing import Process<NewLine><NewLine><NewLine>def run(rank, size):<NewLine>    tensor = torch.zeros(1)<NewLine>    if rank == 0:<NewLine>        tensor += 1<NewLine>        dist.send(tensor=tensor, dst=1)<NewLine>    else:<NewLine>        dist.recv(tensor=tensor, src=0)<NewLine>    print(""Rank "", rank, "" has data "", tensor[0])<NewLine><NewLine><NewLine>def init_process(rank, size, fn, backend=""gloo""):<NewLine>    """""" Initialize the distributed environment. """"""<NewLine>    os.environ[""MASTER_ADDR""] = ""127.0.0.0""<NewLine>    os.environ[""MASTER_PORT""] = ""29511""<NewLine>    dist.init_process_group(backend, rank=rank, world_size=size)<NewLine>    fn(rank, size)<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    size = 2<NewLine>    processes = []<NewLine>    for rank in range(size):<NewLine>        p = Process(target=init_process, args=(rank, size, run))<NewLine>        p.start()<NewLine>        processes.append(p)<NewLine><NewLine>    for p in processes:<NewLine>        p.join()<NewLine><NewLine></code></pre><NewLine><p>In terminal, I run it with <code>python run.py</code>. It runs successfully both on my server and on my friend’s Mac. But, it hangs there forever on my own Mac. I tried to restart my Mac, and reinstall python(v3.8.2)/torch(v1.5.0). No success.</p><NewLine><p>If we interrupt it with “control-c”, the last line of trace is:</p><NewLine><pre><code class=""lang-auto"">  File ""/usr/local/anaconda3/envs/p382/lib/python3.8/subprocess.py"", line 1762, in _try_wait<NewLine>    (pid, sts) = os.waitpid(self.pid, wait_flags)<NewLine></code></pre><NewLine><p>I suspect there is a deadlook. Any approach to avoid it?</p><NewLine></div>",https://discuss.pytorch.org/u/Hao_Yuan,(Hao Yuan),Hao_Yuan,"May 29, 2020,  7:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/hao_yuan"">@Hao_Yuan</a></p><NewLine><p>A few things could cause this issue.</p><NewLine><ol><NewLine><li><NewLine><code>127.0.0.0</code> is not a valid IP in some envs. Could you please check that. If it is indeed invalid, can you try <code>127.0.0.1</code> or <code>localhost</code> or other valid IP addresses?</li><NewLine><li>Some other process is occupying that port. Can you try a different port number?</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>.</p><NewLine><p>Somehow, my Mac is better today. I got a warning and the results.</p><NewLine><pre><code class=""lang-auto"">Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (operator() at ../torch/lib/c10d/ProcessGroupGloo.cpp:496)<NewLine>Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (operator() at ../torch/lib/c10d/ProcessGroupGloo.cpp:496)<NewLine>Rank  1  has data  tensor(1.)<NewLine>Rank  0  has data  tensor(1.)<NewLine></code></pre><NewLine><p>I think the issue is that it takes long time to “resolve hostname”. After <code>export GLOO_SOCKET_IFNAME=en0</code>. I got the result quickly.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Curious, how is the <code>hostname</code> configured in your env? Can you try:</p><NewLine><pre><code class=""lang-auto"">getent hosts `hostname`<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>No <code>getnet</code> command in my mac. I googled a similar command.</p><NewLine><pre><code class=""lang-auto"">$ dscacheutil -q host -a name 'hostname'<NewLine></code></pre><NewLine><p>Not show any result.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see. The <code>hostname</code> wasn’t configured, so that it was unable to resolve when <code>GLOO_SOCKET_IFNAME</code> wasn’t present.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Hao_Yuan; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Hao_Yuan; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: May 29, 2020,  2:29pm; <NewLine> REPLY_DATE 2: May 29, 2020,  6:28pm; <NewLine> REPLY_DATE 3: May 29, 2020,  7:43pm; <NewLine> REPLY_DATE 4: June 1, 2020,  6:39am; <NewLine> REPLY_DATE 5: June 15, 2020,  6:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> 
83710,Does DataParallel applies own attribute changes to replicas?,2020-06-01T13:50:51.757Z,0,64,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello.<br/><NewLine>I have a question about DataParallel.</p><NewLine><p>Assume I have a model, which have its own attribute ‘mode’. The model’s forward flow can be changed by setting this ‘mode’.<br/><NewLine>For example:</p><NewLine><pre><code class=""lang-auto"">class myModel(nn.Module):<NewLine>    def __init__(self, mode='A'):<NewLine>        # do essential initialization...<NewLine>        self.mode = mode<NewLine>        self.selectible_layers = nn.ModuleDict(<NewLine>            {'A': moduleA(), 'B': moduleB(), ...}<NewLine>        )<NewLine><NewLine>    def forward(self, x):<NewLine>         # forwarding...<NewLine>         x = self.selectible_layers[self.mode](x)<NewLine>         # forwarding...<NewLine>         return x<NewLine></code></pre><NewLine><p>When I parallelize this model into multiple GPUs, then I would access this  ‘mode’ attribute by using <code>model.module</code> because nn.DataParallel don’t know about ‘mode’.</p><NewLine><p>Then,</p><NewLine><pre><code class=""lang-auto"">model = myModel(mode='A') # initialize with mode 'A'<NewLine>model = nn.DataParallel(model, device_ids = [0, 1, 2, 3]) # distributed into multiple GPUs<NewLine><NewLine>for mode in ['A', 'B', 'C', 'D']:<NewLine>    model.module.mode = mode # change mode<NewLine>    <NewLine>    # (?)<NewLine></code></pre><NewLine><p>In (?) on above code, does nn.DataParallel guarantee that all model replicas in multiple GPUs have same ‘mode’ when changing it?<br/><NewLine>I worried about that the ‘mode’ in replicas still have mode ‘A’, while original model(on host GPU; maybe 0)'s ‘mode’ changes.</p><NewLine><p>+)<br/><NewLine>I tried myself but I don’t know how to access each replica in multiple GPUs. How to access them?</p><NewLine></div>",https://discuss.pytorch.org/u/FruitVinegar,(NHK),FruitVinegar,"June 1, 2020,  1:51pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The following method is called by <code>DataParallel</code> to create replicas. So the attributes in <code>__dict__</code> should be replicated as well. But as the flow is “DataParallel forward” -&gt; “replicate models” -&gt; “app model forward”, you need to make sure that the <code>mode</code> is set properly before calling DataParallel forward.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/3001facd7a3942efc7c7e8a42b720b9c884387b3/torch/nn/modules/module.py#L1210-L1219"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/3001facd7a3942efc7c7e8a42b720b9c884387b3/torch/nn/modules/module.py#L1210-L1219"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/3001facd7a3942efc7c7e8a42b720b9c884387b3/torch/nn/modules/module.py#L1210-L1219</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""1210"" style=""counter-reset: li-counter 1209 ;""><NewLine><li>def _replicate_for_data_parallel(self):</li><NewLine><li>    replica = self.__new__(type(self))</li><NewLine><li>    replica.__dict__ = self.__dict__.copy()</li><NewLine><li><NewLine></li><li>    # replicas do not have parameters themselves, the replicas reference the original</li><NewLine><li>    # module.</li><NewLine><li>    replica._parameters = OrderedDict()</li><NewLine><li>    replica._buffers = replica._buffers.copy()</li><NewLine><li>    replica._modules = replica._modules.copy()</li><NewLine><li>    replica._is_replica = True</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><blockquote><NewLine><p>I tried myself but I don’t know how to access each replica in multiple GPUs. How to access them?</p><NewLine></blockquote><NewLine><p>The replicas are created in every forward pass of <code>DataParallel</code> module. To access them and check, you can modify your model’s forward function and print out the mode value.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/3001facd7a3942efc7c7e8a42b720b9c884387b3/torch/nn/parallel/data_parallel.py#L154-L156"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/3001facd7a3942efc7c7e8a42b720b9c884387b3/torch/nn/parallel/data_parallel.py#L154-L156"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/3001facd7a3942efc7c7e8a42b720b9c884387b3/torch/nn/parallel/data_parallel.py#L154-L156</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""154"" style=""counter-reset: li-counter 153 ;""><NewLine><li>replicas = self.replicate(self.module, self.device_ids[:len(inputs)])</li><NewLine><li>outputs = self.parallel_apply(replicas, inputs, kwargs)</li><NewLine><li>return self.gather(outputs, self.output_device)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 3, 2020,  5:43am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
83559,Inference Multiple Models on a Single GPU,2020-05-31T09:28:32.096Z,1,120,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to build a system and i need to do inference on 60 segmentation models at same time ( Same models but different inputs). I wonder if this is possible to do on pytorch?<br/><NewLine>I am not sure about what kinda system i need to use for this, i am planning to use 4X RTX8000 and if it is not enough i can use two system with 4X RTX8000 each or with a better gpu.</p><NewLine><p>Would i lose too much performance because of using multiple models? How many models i can put on a gpu, what the performance of it depends on? Is it just vRAM of gpu or processing speed? Sorry for asking such trivial questions but i really couldn’t come up with an answer by searching.</p><NewLine><p>You can assume that i am going to use Yolact, <a href=""https://github.com/dbolya/yolact"" rel=""nofollow noopener"">https://github.com/dbolya/yolact</a><br/><NewLine>I would be very happy if you can help me, thank you so much and sorry for my English/grammar.</p><NewLine></div>",https://discuss.pytorch.org/u/Vival,,Vival,"May 31, 2020,  9:29am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The simplest and probably the most efficient method whould be concatenate your samples in dimension 0 (i.e. the batch dimension).<br/><NewLine>If that is too much for one gpu, then wrap your model in <code>DistributedDataParallel</code> and let it handle the batched data.<br/><NewLine>Do not use multiple models unless they hold different parameters. It’s unecessary.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It makes sense. I was planning to use 30 robotic arms, 2 model for each. By doing like that i guess every robotic arm will get info about what to do on same time. I think i can use that, i 've read about running multiple models on tensorflow is possible so that is why i wanted to know if it is possible on pytorch, thanks for answer again.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Vival; <NewLine> ,"REPLY_DATE 1: May 31, 2020, 10:27am; <NewLine> REPLY_DATE 2: May 31, 2020, 10:36am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
83436,How to set random seed when it is in distributed training?,2020-05-30T02:49:18.772Z,1,107,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone!</p><NewLine><p>Now I am training a model using <code>torch.distributed</code>, but I am not sure how to set the random seeds. For example, this is my current code:</p><NewLine><pre><code class=""lang-python"">def main():<NewLine>    np.random.seed(args.seed)<NewLine>    torch.manual_seed(args.seed)<NewLine>    torch.cuda.manual_seed(args.seed)<NewLine><NewLine>    cudnn.enabled = True<NewLine>    cudnn.benchmark = True<NewLine>    cudnn.deterministic = True <NewLine><NewLine>    mp.spawn(main_worker, nprocs=args.ngpus, args=(args,))<NewLine></code></pre><NewLine><p>And should I move the</p><NewLine><pre><code class=""lang-python"">    np.random.seed(args.seed)<NewLine>    torch.manual_seed(args.seed)<NewLine>    torch.cuda.manual_seed(args.seed)<NewLine><NewLine>    cudnn.enabled = True<NewLine>    cudnn.benchmark = True<NewLine>    cudnn.deterministic = True <NewLine></code></pre><NewLine><p>into the function main_worker() to make sure every process has the correct seed and cudnn settings? By the way, I have tried this and this behavior will make the training process 2 times slower, which really confused me.</p><NewLine><p>Thank you very much for any help!</p><NewLine></div>",https://discuss.pytorch.org/u/sunshk1227,,sunshk1227,"May 30, 2020,  2:49am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Each process should execute the seeding code.<br/><NewLine>The slowdown might come from e.g. <code>cudnn.deterministic = True</code>, as this will use the default algorithm, which might be slower than the others.<br/><NewLine>Also, <code>cudnn.benchmark = True</code> won’t have any effect, if you set <code>cudnn.deterministic = True</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much! I get it!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/sunshk1227; <NewLine> ,"REPLY_DATE 1: May 31, 2020,  7:22am; <NewLine> REPLY_DATE 2: May 31, 2020,  7:44am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> 
83154,Can RPC leverage multicore?,2020-05-28T01:07:06.396Z,0,172,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using torch.distributed.rpc. I can set the rpc to have many threads using <strong>rpc_backend_options</strong>, but it seems like it is not being mapped onto idle CPUs that I have.<br/><NewLine>Specifically, to test out, I’ve sent 1-4 asynchronous RPC calls to a server which has 80 CPUs.<br/><NewLine>Below is the code for reference.</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import time<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import numpy as np<NewLine>from torch.multiprocessing import Process<NewLine>import torch.distributed.rpc as rpc<NewLine><NewLine>class Net(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Net, self).__init__()<NewLine>        self.l0 = nn.Linear(2, 2)<NewLine>        W = np.random.normal(0, 1, size=(2,2)).astype(np.float32)<NewLine>        self.l0.weight.data = torch.tensor(W, requires_grad=True)<NewLine>        self.l1 = nn.Linear(2, 2)<NewLine>        W = np.random.normal(0, 1, size=(2,2)).astype(np.float32)<NewLine>        self.l1.weight.data = torch.tensor(W, requires_grad=True)<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.l1(self.l0(x))<NewLine><NewLine>def test(t):<NewLine>    print(""RPC called"")<NewLine>    for i in range(100000):<NewLine>        t2 = t*1.000001<NewLine>    return t2<NewLine><NewLine>def run(i):<NewLine>    rpc.init_rpc(""Rank""+str(i), rank=i, world_size=2)<NewLine>    if i == 0:<NewLine>        with torch.autograd.profiler.profile(True, False) as prof:<NewLine>            net = Net()<NewLine>            optimizer = torch.optim.SGD(net.parameters(), lr=0.1)<NewLine>            input = torch.tensor([1.0, 2.0])<NewLine>            reqs = []<NewLine>            reqs.append(rpc.rpc_async(""Rank1"", test, args=(input,)))<NewLine>            reqs.append(rpc.rpc_async(""Rank1"", test, args=(input,)))<NewLine>            reqs.append(rpc.rpc_async(""Rank1"", test, args=(input,)))<NewLine>            reqs.append(rpc.rpc_async(""Rank1"", test, args=(input,)))<NewLine>            #reqs.append(rpc.rpc_async(""Rank1"", test, args=(input,)))<NewLine>            for req in reqs:<NewLine>                input += req.wait()<NewLine>            print(""RPC Done"")<NewLine>            y = net(input)<NewLine>            optimizer.zero_grad()<NewLine>            y.sum().backward()<NewLine>            optimizer.step()<NewLine>        print(prof.key_averages().table(sort_by=""cpu_time_total""))<NewLine>        prof.export_chrome_trace(""test.json"")<NewLine>    else:<NewLine>        pass<NewLine><NewLine>    rpc.shutdown()<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    os.environ['MASTER_ADDR'] = ""localhost""<NewLine>    os.environ['MASTER_PORT'] = ""29500""<NewLine>    ps = []<NewLine>    for i in [0, 1]:<NewLine>        p = Process(target=run, args=(i,))<NewLine>        p.start()<NewLine>        ps.append(p)<NewLine><NewLine>    for p in ps:<NewLine>        p.join()<NewLine></code></pre><NewLine><p>As you can see, I am just doing some compute-intensive work on the server using RPC.<br/><NewLine>Below is the result from my profiler.<br/><NewLine>When I do 1 RPC call:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/0225b05c5f78dc7da0ac4d682f45ca63509e104e"" href=""https://discuss.pytorch.org/uploads/default/original/3X/0/2/0225b05c5f78dc7da0ac4d682f45ca63509e104e.png"" title=""Screen Shot 2020-05-27 at 8.58.57 PM""><img alt=""Screen Shot 2020-05-27 at 8.58.57 PM"" data-base62-sha1=""iZHAybQnkdWkXxzCOfWEVwnMia"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/2/0225b05c5f78dc7da0ac4d682f45ca63509e104e_2_10x10.png"" height=""99"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/2/0225b05c5f78dc7da0ac4d682f45ca63509e104e_2_690x99.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/2/0225b05c5f78dc7da0ac4d682f45ca63509e104e_2_690x99.png, https://discuss.pytorch.org/uploads/default/optimized/3X/0/2/0225b05c5f78dc7da0ac4d682f45ca63509e104e_2_1035x148.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/0/2/0225b05c5f78dc7da0ac4d682f45ca63509e104e_2_1380x198.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screen Shot 2020-05-27 at 8.58.57 PM</span><span class=""informations"">3226×464 42.1 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine>When I do 4 RPC calls:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/b04d85d3d1bae1f964d8964c0c120bd3a2d0d35b"" href=""https://discuss.pytorch.org/uploads/default/original/3X/b/0/b04d85d3d1bae1f964d8964c0c120bd3a2d0d35b.png"" title=""Screen Shot 2020-05-27 at 8.59.42 PM""><img alt=""Screen Shot 2020-05-27 at 8.59.42 PM"" data-base62-sha1=""p9E7QtfgjNmxyTJgraePMRwQh7B"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/0/b04d85d3d1bae1f964d8964c0c120bd3a2d0d35b_2_10x10.png"" height=""70"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/0/b04d85d3d1bae1f964d8964c0c120bd3a2d0d35b_2_690x70.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/0/b04d85d3d1bae1f964d8964c0c120bd3a2d0d35b_2_690x70.png, https://discuss.pytorch.org/uploads/default/optimized/3X/b/0/b04d85d3d1bae1f964d8964c0c120bd3a2d0d35b_2_1035x105.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/b/0/b04d85d3d1bae1f964d8964c0c120bd3a2d0d35b_2_1380x140.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screen Shot 2020-05-27 at 8.59.42 PM</span><span class=""informations"">3208×326 49.2 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>Default RPC init makes 4 send_recv_threads. So it should be able to “concurrently” run my 4 RPC requests. However, as you can see, the time to finish the RPC requests grew almost linearly (from 460ms to 2200ms) with 4 requests, meaning that they are using only one core and are not being processed in parallel (i.e., concurrent, but not parallel).</p><NewLine><p>I know that python threads (unlike processes) cannot execute in parallel on different cores. Is RPC threads also (because they are threads) cannot run in parallel on different cores?<br/><NewLine>Is there a way to run different RPC request received on different cores? Or should I manually spawn processes in the receiving server side to run the requests in parallel and leverage my multicore server?</p><NewLine><p>Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/kmaeng,,kmaeng,"May 28, 2020,  1:48am",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is likely due to Python GIL on the server side. Can you torchscript (<a href=""https://pytorch.org/docs/stable/jit.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/jit.html</a>) the <code>test</code> method and try again? That should avoid GIL.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Probably <strong>Yes</strong> for torch jit scripts, and <strong>No</strong> for regular python functions.</p><NewLine><p>I will describe my understanding of the framework in details. For c++ files, their root directory is (git master) <code>torch/csrc/distributed</code>.</p><NewLine><p>Internally, torch rpc framework does the following things:</p><NewLine><ol><NewLine><li><NewLine><p>entry point: <code>rpc.async</code>, <code>rpc.sync</code> use the same <code>_invoke_rpc()</code> while <code>rpc.remote</code> use its own implementation. For these three methods, they all categorize your calls into three categories: <code>builtin</code> is for python builtin function, <code>udf</code> is for user defined functions, <code>jit</code> is for torch script. Then all three lower level calls go into the torch C library.</p><NewLine></li><NewLine><li><NewLine><p>c library: the c-python interface is defined in <code>rpc/init.cpp</code>, which uses <code>pybind11</code> to define the interfaces, on calling the interface, call guards (constructed before wrapped functions) are <code>gil_scoped_release</code> so GIL is released here for all three interface categories. The wrapped functions are defined in <code>rpc/python_functions.cpp</code> they will find your target rpc process (c++ class <code>RpcAgent</code>) and send your message to it. GIL will be reaquired after finishing the c function call due to the automatic deconstruction of call gaurds.</p><NewLine></li><NewLine><li><NewLine><p>rpc agent: <code>RpcAgent</code> class will initialize <code>cb_</code> member on construction, which is a <code>unique_ptr</code> of type <code>RequestCallback</code>, there are two derived classes of <code>RpcAgent</code>: <code>TensorPipeAgent</code> and <code>ProcessGroupAgent</code>. In the rpc use case you are dealing with <code>ProcessGroupAgent</code>, in its member function <code>handleRecv()</code> it will use the <code>cb_</code>member to handle the call back. All these agents are defined in <code>rpc/&lt;agent_name_lower_case&gt;.cpp</code> so you should be able to find them easily.</p><NewLine></li><NewLine><li><NewLine><p>request callback: Request callback is an abstract functor defined in <code>request_callback.h</code> and <code>request_callback.cpp</code>, it has a virtual <code>processMessage()</code> method. Its real implementation in <code>request_callback_impl.cpp</code> defines this method and calls its member method <code>processRpc()</code>.</p><NewLine></li><NewLine><li><NewLine><p>process rpc: This function handles rpc calls based on its message type, <code>SCRIPT_CALL</code> and <code>SCRIPT_REMOTE_CALL</code> will handle jit scripted calls, however in <code>PYTHON_CALL</code>:</p><NewLine><pre><code>  {<NewLine>       py::gil_scoped_acquire acquire;<NewLine>       serializedPyObj =<NewLine>       std::make_shared&lt;SerializedPyObj&gt;(pythonRpcHandler.serialize(<NewLine>       pythonRpcHandler.runPythonUdf(std::move(upc).movePythonUdf())));<NewLine>  }<NewLine></code></pre><NewLine></li><NewLine></ol><NewLine><p>There is a <code>py::gil_scoped_acquire</code>, according to the pybind11 definition, this will hold the gil lock until <code>acquire</code> is deconstructed (aka leaving this c++ scope), so <strong>Nope</strong>, you cannot leverage multicore by using multi-threads used in rpc.</p><NewLine><p><strong>Note</strong> this explanation is valid for commit hash <code>176174a68ba2d36b9a5aaef0943421682ecc66d4</code> and release till 1.5.0, as I can see in their source code that they are planning to further abstract away the switch case in <code>processRpc()</code> to an abstract <code>execute()</code> method of <code>RpcCommandBase</code></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>So your code is theoretically equivalent to using the ThreadPool(4).map from multiprocessing library. The test code is as follows:</p><NewLine><pre><code class=""lang-auto"">import torch as t<NewLine>from multiprocessing.pool import ThreadPool<NewLine>from utils.helper_classes import Timer<NewLine><NewLine>def test(t):<NewLine>    print(""RPC called"")<NewLine>    for i in range(100000):<NewLine>        t2 = t*1.000001<NewLine>    return t2<NewLine><NewLine>def test1():<NewLine>    tm = Timer()<NewLine>    ts=t.Tensor([1,2])<NewLine>    tm.begin()<NewLine>    for i in range(4):<NewLine>        test(ts)<NewLine>    print(tm.end())<NewLine><NewLine>def test2():<NewLine>    tm = Timer()<NewLine>    ts = t.Tensor([1,2])<NewLine>    pool = ThreadPool(4)<NewLine>    tm.begin()<NewLine>    pool.map(test, (ts, ts, ts, ts))<NewLine>    print(tm.end())<NewLine></code></pre><NewLine><p>And my test result is:<br/><NewLine>test1: 2.091 s<br/><NewLine>test2: 2.404 s</p><NewLine><p>You can dispatch sub processes in the rpc call to work around the GIL.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/iffiX; <NewLine> ,"REPLY_DATE 1: May 28, 2020,  4:05am; <NewLine> REPLY_DATE 2: May 29, 2020,  8:47am; <NewLine> REPLY_DATE 3: May 29, 2020,  9:17am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
82162,Run pretrained model in parallel,2020-05-20T10:48:38.479Z,2,177,"<div class=""post"" itemprop=""articleBody""><NewLine><p>If there any good practices to parallel model prediction calculations, not training. So if we have a pretrained pth-model how to run it on a few GPUs? Is there a doc with this specific topic covered?</p><NewLine></div>",https://discuss.pytorch.org/u/tonyrewin,(Tony),tonyrewin,"May 21, 2020,  5:03am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You should have a look at the <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel"" rel=""nofollow noopener""><code>nn.DataParallel</code></a> and <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel"" rel=""nofollow noopener""><code>nn.distributed.DistributedDataParallel</code></a> docs. Both are fairly easy objects to work with. It will basically split your input in the batch dimension across the ids of the GPUs that you pass at initialization.</p><NewLine><p>Note that some model architectures are not able to be parallelized across devices.</p><NewLine><p>Also, if you are just running inference, you may not see any benefit to multi-GPU parallelization. Or even using a GPU at all. You might try running the model after a call to <code>model.eval()</code> if you are experiencing performance issues.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>It will basically split your input in the batch dimension across the ids of the GPUs that you pass at initialization.</p><NewLine></blockquote><NewLine><p>So this code is enough:</p><NewLine><pre><code class=""lang-auto"">    model = torch.nn.Module(options)<NewLine><NewLine>    ...<NewLine><NewLine>    if torch.cuda.is_available():<NewLine>        ids = [i for i in range(torch.cuda.device_count())]<NewLine>        model = torch.nn.DataParallel(model, device_ids=ids).cuda()<NewLine>        os.environ['CUDA_VISIBLE_DEVICES'] = ids.join(',')[:-1]<NewLine>        print(""Using "", len(ids), "" GPUs!"")<NewLine><NewLine>    ...<NewLine><NewLine>    model_results = model(input)<NewLine></code></pre><NewLine><blockquote><NewLine><p>Note that some model architectures are not able to be parallelized across devices.</p><NewLine></blockquote><NewLine><p>What does it depend on?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(<NewLine>RuntimeError: Error(s) in loading state_dict for DataParallel:<NewLine>        Missing key(s) in state_dict: ""module.conv_first.weight"", <NewLine>really long list here...<NewLine></code></pre><NewLine><p>Should we prepare the code anyhow for wrapping in DataParallel?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>nn.DataParallel</code> adds a <code>.module</code> attribute to the model, so that you might see these key errors while trying to load a <code>state_dict</code> from a plain PyTorch model.<br/><NewLine>You could either add/remove the <code>.module</code> keys manually or store and load the <code>state_dict</code> using the plain model without the <code>nn.DataParallel</code> wrapper.<br/><NewLine>To store the <code>state_dict</code> you would use <code>torch.save(model.module.state_dict(), path)</code>, while you could just load the <code>state_dict</code> before wrapping the model into <code>nn.DataParallel</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tymokvo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tonyrewin; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tonyrewin; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: May 21, 2020,  5:30am; <NewLine> REPLY_DATE 2: May 21, 2020,  7:46am; <NewLine> REPLY_DATE 3: May 24, 2020, 10:53am; <NewLine> REPLY_DATE 4: May 25, 2020,  3:11am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
81942,"RPC with raw irecv, reduce, &hellip; distributed primitives",2020-05-19T07:25:22.650Z,5,202,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to use torch.distributed.rpc (since 1.4.0) with irecv, reduce, broadcast… primitives together? In my experience, it is possible to use send&amp;recv primitives after <code>init_rpc()</code> call, but I am not sure whether a blocking recv / reduce / all_gather call will interfere with rpc api.</p><NewLine><p>If you have any experience with this, please let me know, thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/iffiX,(Iffi),iffiX,"May 19, 2020,  7:30am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, this is possible, but there is one caveat. The current implementation of <code>init_rpc()</code> sets the default process group. It actually shouldn’t do that, and we are fixing it (see <a href=""https://github.com/pytorch/pytorch/issues/33583"" rel=""nofollow noopener"">this issue</a>). With the current RPC implementation, I see two ways to implement this feature.</p><NewLine><h3>Option 1</h3><NewLine><p>First, call <code>init_rpc</code>() and let it set the default process group. Then use the <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.new_group"" rel=""nofollow noopener"">new_group</a> API to create new set of process group instances, and only call irecv/reduce/broadcast on the new process group instances. This would avoid messing up states of the default process group that RPC is running on.</p><NewLine><h3>Option 2</h3><NewLine><p>Directly create process group using its constructor. See the test below as an example:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/f6f1384811b9cc722f650ed9ead8ee99938c009a/test/distributed/test_c10d.py#L1501-L1502"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/f6f1384811b9cc722f650ed9ead8ee99938c009a/test/distributed/test_c10d.py#L1501-L1502"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/f6f1384811b9cc722f650ed9ead8ee99938c009a/test/distributed/test_c10d.py#L1501-L1502</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""1501"" style=""counter-reset: li-counter 1500 ;""><NewLine><li>store = c10d.FileStore(self.file_name, self.world_size)</li><NewLine><li>pg = c10d.ProcessGroupGloo(store, self.rank, self.world_size)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Just curious, do you mind share more details of your use case? What’s the motivation to combine RPC with collective communications? I saw some requirement for this to support combining RPC with DDP. Is this also your use case?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your response! Yeah, I have noticed that issue on github. I am also kind of expecting using some solution similar to your option 1, and now reassured by your answer, I am ready to use it.</p><NewLine><p>The reason why I would like to use RPC with collective communications together is that while RPC mechanism is great for implementing point to point communication paradigm, it is not very handy when it comes to implementing algorithms where serveral processes are closely correlated and doing exactly the same things in parallel, that is the job for collective comm primitives. And it is very hard to implement on-demand calling using collective comm primitives as well.</p><NewLine><p>An example of <strong>a part of</strong> my actual application is a distributed buffer used in IMPALA and APEX, where workers are continuously pulling models from trainers and doing rollouts. The topology of workers and trainers are preset and given by user, active service discovery is not considered. “Lost of a single worker does not affect any other peers” is an optional, but important feature.</p><NewLine><p>I have one more question, <strong>how do you handle errors</strong> like a worker disconnects from the group? As far as I’m concerned, there is just a timeout argument in the rpc api and no explicit error handling descriptions for rpc or collective in the document. Maybe ZeroMQ is a better alternative if we are concerned about robustness?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for sharing more details.</p><NewLine><blockquote><NewLine><p>I have one more question, <strong>how do you handle errors</strong> like a worker disconnects from the group?</p><NewLine></blockquote><NewLine><p>We are working on elasticity support to allow workers to dynamically join and leave.</p><NewLine><blockquote><NewLine><p>As far as I’m concerned, there is just a timeout argument in the rpc api and no explicit error handling descriptions for rpc or collective in the document.</p><NewLine></blockquote><NewLine><p>For timeout, there are per-RPC timeout on master branch for <code>rpc_sync</code> and <code>rpc_async</code>. For timeout in <code>remote</code>, the following PR is add it.</p><NewLine><aside class=""onebox githubpullrequest""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/38590"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Pull Request""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 12 16"" width=""60""><path d=""M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/pull/38590"" rel=""nofollow noopener"" target=""_blank"">Implement timeout support for RRefs</a><NewLine></h4><NewLine><div class=""branches""><NewLine><code>pytorch:gh/rohan-varma/129/base</code> ← <code>pytorch:gh/rohan-varma/129/head</code><NewLine></div><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-05-16"" data-format=""ll"" data-time=""01:33:20"" data-timezone=""UTC"">01:33AM - 16 May 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/rohan-varma"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""rohan-varma"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/8039770?v=4"" width=""20""/><NewLine>          rohan-varma<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""4 commits changed 23 files with 394 additions and 37 deletions""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/38590/files"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+394</span><NewLine><span class=""removed"">-37</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><blockquote><NewLine><p>no explicit error handling descriptions for rpc or collective in the document</p><NewLine></blockquote><NewLine><p>Right, currently, there is only a <code>wait</code> API on the returned <code>Future</code> object. Currently, you will need to try-except the <code>wait</code>. In the C++ implementation, there are error handling APIs on the <code>Future</code>, let us expose it to Python as well. Besides this, do you also need things like error listener for errors occurs in background RPC threads or something else?</p><NewLine><blockquote><NewLine><p>Maybe ZeroMQ is a better alternative if we are concerned about robustness?</p><NewLine></blockquote><NewLine><p>It depends on what do you need in the application. If you just need P2P communication without tensor/autograd support, then any matured RPC-like system can do. If you need autograd, or would like to do direct device-2-device copy (coming to v1.6), or would like to integrate with TorchScript to speed up training, then <code>torch.distributed.rpc</code> might be better. The latter is a relatively new project and there are still many gaps in the features, but we’d love to drive it together with the community into a better shape.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your detailed reply, currently, my project does not need more error in-depth error handling apart from try-excepting the <code>wait</code>, after a thorough consideration, I think rpc and collective comm are sufficient for the most necessary functions, for now. Whether using some more mature RPC frameworks really depends on my users requests, and I think it is unecessary at this moment.</p><NewLine><p>Looking forward to more updates on elasticity enhancement. <img alt="":laughing:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/laughing.png?v=9"" title="":laughing:""/></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><s>According to the definition of function <code>_invoke_remote_builtin</code> and <code>_invoke_remote_python_udf</code><br/><NewLine>in <code>pytorch/torch/csrc/distributed/rpc/init.cpp</code>, L557-591:</s></p><s><NewLine><pre><code class=""lang-auto"">     module.def(<NewLine>      ""_invoke_rpc_builtin"",<NewLine>      [](const WorkerInfo&amp; dst,<NewLine>         const std::string&amp; opName,<NewLine>         const float rpcTimeoutSeconds,<NewLine>         const py::args&amp; args,<NewLine>         const py::kwargs&amp; kwargs) {<NewLine>        return std::make_shared&lt;jit::PythonFutureWrapper&gt;(<NewLine>            pyRpcBuiltin(dst, opName, args, kwargs, rpcTimeoutSeconds));<NewLine>      },<NewLine>      py::call_guard&lt;py::gil_scoped_acquire&gt;());<NewLine></code></pre><NewLine></s><p><s></s><s><br/><NewLine>, which are called under the hood by <code>rpc.remote</code>, <code>rpc.rpc_async</code>, <code>rpc.rpc_sync</code>, is it correct to say that the <code>rf</code> parameter in <code>_invoke_rpc</code>:</s></p><s><NewLine><pre><code class=""lang-auto"">def _invoke_rpc(to, func, rpc_type, args=None, kwargs=None):<NewLine>    if not callable(func):<NewLine>        raise TypeError(""function should be callable."")<NewLine><NewLine>    qualified_name = torch.jit._find_builtin(func)<NewLine>    dst_worker_info = _to_worker_info(to)<NewLine>    # If profiling is enabled, kick off the timer and retrieve back a<NewLine>    # RecordFunction instance.<NewLine>    rf = None<NewLine>    ...<NewLine></code></pre><NewLine></s><p><s>defines timeout in <strong>seconds</strong> for each individual rpc call?</s></p><NewLine><p>Sorry I mixed up the python api code in 1.5.0 release and current master branch code. Now master branch provides timeout argument in python api:</p><NewLine><pre><code class=""lang-auto"">@_require_initialized<NewLine>def rpc_sync(to, func, args=None, kwargs=None, timeout=UNSET_RPC_TIMEOUT):<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, timeout arg is now available in <code>rpc_sync</code> and <code>rpc_async</code> APIs on master. Support for <code>remote</code> API will come soon.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/iffiX; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: May 20, 2020, 10:14am; <NewLine> REPLY_DATE 2: May 20, 2020, 10:20am; <NewLine> REPLY_DATE 3: May 20, 2020,  1:52pm; <NewLine> REPLY_DATE 4: May 20, 2020,  2:00pm; <NewLine> REPLY_DATE 5: May 21, 2020,  4:30am; <NewLine> REPLY_DATE 6: May 21, 2020,  2:46pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
82204,Run n models in parallel on single GPU,2020-05-20T16:02:35.453Z,0,145,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to train n models (per n, I have f times t data points). I can load all data onto a single GPU. I assign the dataloader batches and each batch gets a number of minibatches. Each minibatch holds the data to train one model (one n). The data per n is rather small, but the number of models is large. The ‘problem’ that I am facing is that the batches are executed sequentially rather than in parallel on the single GPU. Is there a way to parallelize the batches on the single GPU to ensure scaling to a large number of models quicker? The bare, individual training time per model is improved by a factor of X using GPUs over CPU (not given any parallelization).</p><NewLine><p>Thanks in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/twikwik,,twikwik,"May 20, 2020,  4:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think you should be able to spawn multiple processes on a single GPU (using torch.multiprocessing - <a href=""https://pytorch.org/docs/stable/multiprocessing.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/multiprocessing.html</a>), and train each model in a separate process. You may need to tune the number of processes you spawn, since performance may be degraded with too many processes due to resource contention.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/osalpekar; <NewLine> ,"REPLY_DATE 1: May 20, 2020,  6:34pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
82122,Chunked loss produces error with DistributedDataParallel,2020-05-20T07:29:49.774Z,0,79,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am having a problem with chunked loss calculation when using DistributedDataParallel (the code works fine on a single GPU). I use a single node with 4 GPU’s, and am training a transformer model for NLP. Instead of feeding the whole batch to the final linear layer that maps to the vocabulary dimension (called generator in the code below), I split the batch up in chunks. This is common practice, see e.g. <a href=""http://nlp.seas.harvard.edu/2018/04/03/attention.html"" rel=""nofollow noopener"">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a> (class MultiGPULossCompute).</p><NewLine><p>The code I use for loss calculation (where <code>x</code> are the model activations):</p><NewLine><pre><code class=""lang-auto"">x_copy = x.clone().detach()<NewLine>x_copy.requires_grad = True<NewLine><NewLine>chunk_loss_all = 0.0<NewLine>for chunk_start in range(0, batch_size, chunk_size):<NewLine>	# Calculate loss per chunk<NewLine>	chunk_end = min(chunk_start + chunk_size, batch_size)<NewLine>	chunk_predictions = generator(x_copy[chunk_start:chunk_end])<NewLine>        chunk_loss = criterion(chunk_predictions.contiguous().view(-1, chunk_predictions.size(-1)),<NewLine>        		                              y[chunk_start:chunk_end].contiguous().view(-1))<NewLine>	chunk_loss_all += chunk_loss<NewLine><NewLine># backward for chunk losses<NewLine>chunk_loss_all.backward()<NewLine><NewLine># backward through rest of the model<NewLine>x_gradients = x_copy.grad.view_as(x)<NewLine>x.backward(gradient=x_gradients)<NewLine>optimizer.step()<NewLine>optimizer.zero_grad()<NewLine></code></pre><NewLine><p>The error that is produced:</p><NewLine><pre><code class=""lang-auto"">  File ""loss/compute.py"", line 75, in chunked_loss<NewLine>    x.backward(gradient=x_gradients)<NewLine>  File ""/home/dstap1/anaconda3/envs/logos/lib/python3.8/site-packages/torch/tensor.py"", line 195, in backward<NewLine>    torch.autograd.backward(self, gradient, retain_graph, create_graph)<NewLine>  File ""/home/dstap1/anaconda3/envs/logos/lib/python3.8/site-packages/torch/autograd/__init__.py"", line 97, in backward<NewLine>    Variable._execution_engine.run_backward(<NewLine>RuntimeError: has_marked_unused_parameters_ INTERNAL ASSERT FAILED at /opt/conda/conda-bld/pytorch_1579022027550/work/torch/csrc/distributed/c10d/reducer.cpp:290, please report a bug to PyTorch.<NewLine></code></pre><NewLine><p>The problem is probably in the multiple <code>.backward()</code> calls. I don’t know how to rewrite my code to solve this problem. Any ideas?</p><NewLine></div>",https://discuss.pytorch.org/u/davidstap,(David Stap),davidstap,"May 20, 2020,  7:30am",,,,,
75174,How to share CPU memory in distributed training?,2020-04-03T01:22:09.838Z,3,130,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I am trying to train ImageNet on a 8GPU machine with DDP mode. However, my machine is not good at reading large scale small files. I have to make a tar file of the whole dataset (130Gb), read the tar file into memory and extract them in memory. I have a CPU memory of 360Gb. So it would be OK to use DataParallel mode. But it seems I cannot use DistributedDataParallel since I need to load the dataset 8 times. Is there any method that I can train the model with DDP mode?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/KaiHoo,(Kai Hu),KaiHoo,"April 3, 2020,  1:22am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>One option is to use <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html#reuse-buffers-passed-through-a-queue"" rel=""nofollow noopener""><code>torch.multiprocessing.Queue</code></a> as the shared memory. The main process can prepare multiple queues and then pass one queue to each DDP processes. The main process reads from the file and dispatch data items to the queue, while DDP processes wait on their own queue for that data item.</p><NewLine><p>Another option is to split the tar file into multiple smaller pieces and let each DDP process read a different one.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello! Thanks for your answer! I have one more question about the second option. Does that mean something like this?</p><NewLine><p>I split the tar file into data_1.tar, data_2.tar, …, data_8.tar. For the k^{th} GPU, i.e., local_rank = k, the process read data_k.tar and build data loader with data_k.tar. Then I get 8 different data loaders (their data are different). In this case, I guess I should set shuffle=True and do not need a train sampler?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I split the tar file into data_1.tar, data_2.tar, …, data_8.tar. For the k^{th} GPU, i.e., local_rank = k, the process read data_k.tar and build data loader with data_k.tar. Then I get 8 different data loaders (their data are different). In this case, I guess I should set shuffle=True and do not need a train sampler?</p><NewLine></blockquote><NewLine><p>Yes, but one caveat is that those input data splits need to generate the same number of input batches for DDP. If, say rank 0 processes 3 batches and rank 1 process 4 batches, rank 1 would hang on the last batch.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/KaiHoo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: May 20, 2020, 12:32am; <NewLine> REPLY_DATE 2: May 19, 2020, 12:30pm; <NewLine> REPLY_DATE 3: May 20, 2020, 12:32am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
80702,MPI cuda stream,2020-05-11T13:31:32.481Z,3,295,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When we do</p><NewLine><pre><code class=""lang-auto"">with torch.cuda.stream(stream):<NewLine>    torch.distributed.isend(...)<NewLine></code></pre><NewLine><p>Will if effect the stream (cuda-aware) MPI uses for communication, or rather its some inside MPI implementation detail?</p><NewLine></div>",https://discuss.pytorch.org/u/seliad,(Saar Eliad),seliad,"May 11, 2020,  1:31pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Given the implementation below, it does not seem <code>ProcessGroupMPI</code> uses any dedicated CUDA streams. So I would assume it’s fully delegated to MPI’s implementation?</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/f314d9a0774062a20015ae522d33eadd45293328/torch/lib/c10d/ProcessGroupMPI.cpp#L791-L814"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/f314d9a0774062a20015ae522d33eadd45293328/torch/lib/c10d/ProcessGroupMPI.cpp#L791-L814"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/f314d9a0774062a20015ae522d33eadd45293328/torch/lib/c10d/ProcessGroupMPI.cpp#L791-L814</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""791"" style=""counter-reset: li-counter 790 ;""><NewLine><li>std::shared_ptr&lt;ProcessGroup::Work&gt; ProcessGroupMPI::send(</li><NewLine><li>    std::vector&lt;at::Tensor&gt;&amp; tensors,</li><NewLine><li>    int dstRank,</li><NewLine><li>    int tag) {</li><NewLine><li>  checkSingleTensor(tensors);</li><NewLine><li><NewLine></li><li>  auto&amp; tensor = tensors[0];</li><NewLine><li>  MPI_Request request = MPI_REQUEST_NULL;</li><NewLine><li><NewLine></li><li>  {</li><NewLine><li>    c10::DeviceGuard guard(tensor.device());</li><NewLine><li>    std::unique_lock&lt;std::mutex&gt; globalLock(pgGlobalMutex_);</li><NewLine><li>    MPI_CHECK(MPI_Isend(</li><NewLine><li>        tensor.data_ptr(),</li><NewLine><li>        tensor.numel(),</li><NewLine><li>        mpiDatatype.at(tensor.scalar_type()),</li><NewLine><li>        dstRank,</li><NewLine><li>        tag,</li><NewLine><li>        pgComm_,</li><NewLine><li>        &amp;request));</li><NewLine></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/f314d9a0774062a20015ae522d33eadd45293328/torch/lib/c10d/ProcessGroupMPI.cpp#L791-L814"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>You made me open the black box  <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>I verified its an inside MPI implementation detail. I found it in their code.<br/><NewLine><a href=""https://github.com/open-mpi/ompi/blob/e9e4d2a4bc4ca34d8f426a5f175e3a9eabe50a66/opal/mca/common/cuda/common_cuda.c"" rel=""nofollow noopener"">for example openmpi</a><br/><NewLine>they use their own streams.</p><NewLine><p><strong>This is critical,</strong><br/><NewLine>because unless the streams they create can be accessed somehow  (so far I did not find a way to do it in the cuda manual. but ill look deeper),<br/><NewLine>it means that the only way to change the behavior is editing the MPI C code and compiling.</p><NewLine><p><strong>Why do normal pytorch users should care?</strong><br/><NewLine>Because for normal cuda-aware usage this is super-duper risky, as their streams don’t wait for the our streams, meaning cuda-aware MPI is prune to failure unless we fully synchronize our streams before each MPI call.<br/><NewLine>This would result in slower (or incorrect) program.<br/><NewLine>(I personally spent tons of time debugging this…)</p><NewLine><p>I’d like to know what you think, maybe we should open an issue.</p><NewLine><p>By the way, I wonder why in the file you mention irecv uses <code>MPI_ANY_SOURCE</code>:<br/><NewLine>is that intentional?<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/f314d9a0774062a20015ae522d33eadd45293328/torch/lib/c10d/ProcessGroupMPI.cpp#L856"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/f314d9a0774062a20015ae522d33eadd45293328/torch/lib/c10d/ProcessGroupMPI.cpp#L856"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/f314d9a0774062a20015ae522d33eadd45293328/torch/lib/c10d/ProcessGroupMPI.cpp#L856</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""846"" style=""counter-reset: li-counter 845 ;""><NewLine><li>  auto&amp; tensor = tensors[0];</li><NewLine><li>  MPI_Request request = MPI_REQUEST_NULL;</li><NewLine><li><NewLine></li><li>  {</li><NewLine><li>    c10::DeviceGuard guard(tensor.device());</li><NewLine><li>    std::unique_lock&lt;std::mutex&gt; globalLock(pgGlobalMutex_);</li><NewLine><li>    MPI_CHECK(MPI_Irecv(</li><NewLine><li>        tensor.data_ptr(),</li><NewLine><li>        tensor.numel(),</li><NewLine><li>        mpiDatatype.at(tensor.scalar_type()),</li><NewLine><li class=""selected"">        MPI_ANY_SOURCE,</li><NewLine><li>        tag,</li><NewLine><li>        pgComm_,</li><NewLine><li>        &amp;request));</li><NewLine><li>  }</li><NewLine><li><NewLine></li><li>  return std::make_shared&lt;AsyncWork&gt;(tensor, request);</li><NewLine><li>}</li><NewLine><li><NewLine></li><li>std::shared_ptr&lt;ProcessGroup::Work&gt; ProcessGroupMPI::barrier(</li><NewLine><li>    const BarrierOptions&amp; opts) {</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""80702"" data-username=""seliad""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/seliad/40/16357_2.png"" width=""20""/> seliad:</div><NewLine><blockquote><NewLine><p>By the way, I wonder why in the file you mention irecv uses <code>MPI_ANY_SOURCE</code> :<br/><NewLine>is that intentional?</p><NewLine></blockquote><NewLine></aside><NewLine><p>I am not aware of the history here. <a class=""mention"" href=""/u/pietern"">@pietern</a> and <a class=""mention"" href=""/u/teng-li"">@teng-li</a> would know more.</p><NewLine><blockquote><NewLine><p><strong>Why do normal pytorch users should care?</strong><br/><NewLine>Because for normal cuda-aware usage this is super-duper risky, as their streams don’t wait for the our streams, meaning cuda-aware MPI is prune to failure unless we fully synchronize our streams before each MPI call.<br/><NewLine>This would result in slower (or incorrect) program.</p><NewLine></blockquote><NewLine><p>I agree, full synchronization is not acceptable here. Can MPI take a CUDA stream as an argument and then work on that stream like NCCL does? If this is possible we can let <code>ProcessGroupMPI</code> manage the streams and use CUDA event to synchronize.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Created an issue in openmpi repo.<br/><NewLine></p><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/open-mpi/ompi/issues/7733"" rel=""nofollow noopener"" target=""_blank"">github.com/open-mpi/ompi</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/open-mpi/ompi/issues/7733"" rel=""nofollow noopener"" target=""_blank"">synchronize cuda-aware mpi streams</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-05-13"" data-format=""ll"" data-time=""14:02:54"" data-timezone=""UTC"">02:02PM - 13 May 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/saareliad"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""saareliad"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars1.githubusercontent.com/u/22762845?v=4"" width=""20""/><NewLine>          saareliad<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">Background information<NewLine><NewLine>v4.0.3<NewLine><NewLine>installed from source (tar)<NewLine><NewLine>cuda aware mpi<NewLine><NewLine>cuda 10.2<NewLine><NewLine>This is not a system problem, but suspected behavior/implementation issue in cuda-aware MPI. it...</p><NewLine></div><NewLine><div class=""labels""><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> As far as I know,  MPI doesn’t support what you suggest so better ask them directly?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a><br/><NewLine>I think what  <a href=""https://github.com/open-mpi/ompi/issues/7733#issuecomment-629806195"" rel=""nofollow noopener"">https://github.com/open-mpi/ompi/issues/7733#issuecomment-629806195</a><br/><NewLine>Suggests is what should be implemented inside Pytorch in cpp, if we want to use MPI process group correctly.</p><NewLine><p>Maybe add optional event argument to torch.dist calls (<code>cuda_event_to_sync_with</code> or something).</p><NewLine><p>As far as I know, callbacks on cuda events are not exposed to python API.<br/><NewLine>(Too bad they aren’t, actually)</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see. Thanks for sharing!</p><NewLine><blockquote><NewLine><p>Maybe add optional event argument to torch.dist calls ( <code>cuda_event_to_sync_with</code>  or something).</p><NewLine></blockquote><NewLine><p>I am not sure whether we should add this to c10d Python API if this is only required by the MPI backend. Could you please add an issue on GH to kick off the discussion on the pitch? Let’s discuss there to see what are the options.</p><NewLine><blockquote><NewLine><p>As far as I know, callbacks on cuda events are not exposed to python API.<br/><NewLine>(Too bad they aren’t, actually)</p><NewLine></blockquote><NewLine><p>We are actually exploring CUDA event callback for RPC, and also considering using it to handle CUDA errors. Let me create an issue to track this.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/38671"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/38671"" rel=""nofollow noopener"" target=""_blank"">Add CUDA callback to Python API</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-05-18"" data-format=""ll"" data-time=""18:38:43"" data-timezone=""UTC"">06:38PM - 18 May 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/mrshenli"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""mrshenli"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/16999635?v=4"" width=""20""/><NewLine>          mrshenli<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">As discussed in this forum thread. CUDA stream/event callback can be a useful feature. It might be helpful to add it...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">feature</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">module: cuda</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">module: rpc</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/seliad; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/seliad; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/seliad; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: May 11, 2020,  4:03pm; <NewLine> REPLY_DATE 2: May 11, 2020,  9:41pm; <NewLine> REPLY_DATE 3: May 12, 2020,  2:21pm; <NewLine> REPLY_DATE 4: May 13, 2020,  2:26pm; <NewLine> REPLY_DATE 5: May 18, 2020,  5:33am; <NewLine> REPLY_DATE 6: May 18, 2020,  6:29pm; <NewLine> REPLY_DATE 7: May 18, 2020,  6:38pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
81811,"Loss goes down on GPU0, but up on other 3 when using DistributedDataParallel",2020-05-18T12:25:55.509Z,2,60,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,</p><NewLine><p>I have stumbled upon a problem when using DistributedDataParallel. Strangely, after a few epochs of successful training, loss goes up for a while. I noticed that both train/validation losses got down for the batches which are on GPU0, but go up for the other 3 GPUs. I believe I’m doing something wrong with DistributedDataParallel, but can’t find a bug. Did anyone see a similar problem, or can guess what the reason can be?</p><NewLine><p>In the chart you can see training and validation losses for GPU0 and average of all 4.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/c51901ec4bee4067e6dac1bf77663d2ab02ec9fd"" href=""https://discuss.pytorch.org/uploads/default/original/3X/c/5/c51901ec4bee4067e6dac1bf77663d2ab02ec9fd.png"" title=""Screenshot from 2020-05-18 05-16-58""><img alt=""Screenshot from 2020-05-18 05-16-58"" data-base62-sha1=""s7BDGfZRbrYQ4R0CPe6i9XAxcUB"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/5/c51901ec4bee4067e6dac1bf77663d2ab02ec9fd_2_10x10.png"" height=""388"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/5/c51901ec4bee4067e6dac1bf77663d2ab02ec9fd_2_690x388.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/5/c51901ec4bee4067e6dac1bf77663d2ab02ec9fd_2_690x388.png, https://discuss.pytorch.org/uploads/default/optimized/3X/c/5/c51901ec4bee4067e6dac1bf77663d2ab02ec9fd_2_1035x582.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/c/5/c51901ec4bee4067e6dac1bf77663d2ab02ec9fd_2_1380x776.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screenshot from 2020-05-18 05-16-58</span><span class=""informations"">1920×1080 323 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/Martun_Karapetyan,(Martun Karapetyan),Martun_Karapetyan,"May 18, 2020, 12:43pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/martun_karapetyan"">@Martun_Karapetyan</a>, DDP should have kept all model replicas in sync, i.e., all model replicas should have the same parameter values. Could you please check if this is true in your use case, say by using <code>all_gather</code> to collect all model parameters into one rank and compare?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the help.</p><NewLine><p>I checked the model parameters, they were in perfect sync.</p><NewLine><p>I had another stupid bug. I used ReduceLROnPlateau when the validation accuracy plateaued, but each process looked at the validation accuracy of its subset of data. 1st process reduced the learning rate first, the others reduced it 1 epoch later, hence the problem.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Martun_Karapetyan; <NewLine> ,"REPLY_DATE 1: May 18, 2020,  2:57pm; <NewLine> REPLY_DATE 2: May 18, 2020,  4:44pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
81248,RuntimeError: Stop_waiting response is expected,2020-05-14T18:10:45.962Z,2,145,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I use two gpus to train my model, I got RuntimeError below:</p><NewLine><p>Process SpawnProcess-2:<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py”, line 258, in _bootstrap<br/><NewLine>self.run()<br/><NewLine>File “/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py”, line 93, in run<br/><NewLine>self._target(*self._args, **self._kwargs)<br/><NewLine>File “/home/ubuntu/ogb/ogb/graphproppred/m.py”, line 190, in run<br/><NewLine>main(rank, dev_id, args)<br/><NewLine>File “/home/ubuntu/ogb/ogb/graphproppred/m.py”, line 149, in main<br/><NewLine>train(args[‘gnn’], model, device, train_loader, criterion, optimizer, args[‘num_devices’], rank)<br/><NewLine>File “/home/ubuntu/ogb/ogb/graphproppred/m.py”, line 41, in train<br/><NewLine>optimizer.backward_and_step(loss)<br/><NewLine>File “/home/ubuntu/ogb/ogb/graphproppred/utils.py”, line 146, in backward_and_step<br/><NewLine>self._sync_gradient()<br/><NewLine>File “/home/ubuntu/ogb/ogb/graphproppred/utils.py”, line 127, in _sync_gradient<br/><NewLine>dist.all_reduce(p.grad.data, op=dist.ReduceOp.SUM)<br/><NewLine>File “/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py”, line 902, in all_reduce<br/><NewLine>work = _default_pg.allreduce([tensor], opts)<br/><NewLine>RuntimeError: Stop_waiting response is expected</p><NewLine><p>Process SpawnProcess-1:<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py”, line 258, in _bootstrap<br/><NewLine>self.run()<br/><NewLine>File “/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py”, line 93, in run<br/><NewLine>self._target(*self._args, **self._kwargs)<br/><NewLine>File “/home/ubuntu/ogb/ogb/graphproppred/m.py”, line 190, in run<br/><NewLine>main(rank, dev_id, args)<br/><NewLine>File “/home/ubuntu/ogb/ogb/graphproppred/m.py”, line 149, in main<br/><NewLine>train(args[‘gnn’], model, device, train_loader, criterion, optimizer, args[‘num_devices’], rank)<br/><NewLine>File “/home/ubuntu/ogb/ogb/graphproppred/m.py”, line 41, in train<br/><NewLine>optimizer.backward_and_step(loss)<br/><NewLine>File “/home/ubuntu/ogb/ogb/graphproppred/utils.py”, line 146, in backward_and_step<br/><NewLine>self._sync_gradient()<br/><NewLine>File “/home/ubuntu/ogb/ogb/graphproppred/utils.py”, line 127, in _sync_gradient<br/><NewLine>dist.all_reduce(p.grad.data, op=dist.ReduceOp.SUM)<br/><NewLine>File “/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py”, line 902, in all_reduce<br/><NewLine>work = _default_pg.allreduce([tensor], opts)<br/><NewLine>RuntimeError: Stop_waiting response is expected</p><NewLine><p>Here is the code where the error occurred:</p><NewLine><pre><code>def _sync_gradient(self):<NewLine>    """"""Average gradients across all subprocesses.""""""<NewLine>    for param_group in self.optimizer.param_groups:<NewLine>        for p in param_group['params']:<NewLine>            if p.requires_grad and p.grad is not None:<NewLine>                # print(p.grad.data.shape, p.grad.data.device)<NewLine>                dist.all_reduce(p.grad.data, op=dist.ReduceOp.SUM)<NewLine>                p.grad.data /= self.n_processes<NewLine></code></pre><NewLine><p>Ps. When I do “print(p.grad.data.shape, p.grad.data.device)”, I find the grads are normal and has the same shape [1,300] on 2 different gpus. So I’m confused why it stopped here.</p><NewLine></div>",https://discuss.pytorch.org/u/yangkz,(Zach),yangkz,"May 14, 2020,  6:10pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is the result of <code>p.requires_grad and p.grad is not None</code> always the same across all process and all parameters? If allreduce ops on different processes could run into desync.</p><NewLine><p>Which backend are you using (NCCL/Gloo/MPI) and which PyTorch version are you using? It will be helpful to have a min repro of this error.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply!<br/><NewLine>Here is my short code:</p><NewLine><pre><code class=""lang-auto"">def main(rank, dev_id, args):<NewLine>    torch.distributed.init_process_group(backend=""nccl"",<NewLine>                                         init_method='tcp://localhost:22',<NewLine>                                         world_size=args['num_devices'],<NewLine>                                         rank=dev_id)<NewLine>    model = mymodel.to(dev_id)<NewLine>    optimizer = optim.Adam(model.parameters(), lr=args['lr'])<NewLine>    for epochs:<NewLine>        pred = model(inputs)<NewLine>        loss = criterion(pred, label)<NewLine>        optimizer.zero_grad()<NewLine>        loss.backward()<NewLine><NewLine>        for param_group in optimizer.param_groups:<NewLine>            for p in param_group['params']:<NewLine>                if p.requires_grad and p.grad is not None:<NewLine>                    # print(p.grad.data.shape, p.grad.data.device) Ps. We can get grad information here<NewLine>                    dist.all_reduce(p.grad.data, op=dist.ReduceOp.SUM)<NewLine>                    p.grad.data /= n_processes<NewLine>        optimizer.step()<NewLine>    torch.distributed.barrier()<NewLine><NewLine>mp = torch.multiprocessing.get_context('spawn')<NewLine>for id, device_id in enumerate(devices):<NewLine>        procs.append(mp.Process(target=main, args=(id, device_id, args), daemon=True))<NewLine>        procs[-1].start()<NewLine>for p in procs:<NewLine>    p.join()<NewLine></code></pre><NewLine><p>The pytorch version I’m using is 1.4.0 and all my codes are running in AWS instance.<br/><NewLine>Ps. The port of init_method can only be 22 which is strange otherwise it got Runtime Error like this:</p><NewLine><p>File “/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/distributed/rendezvous.py”, line 120, in _tcp_rendezvous_handler<br/><NewLine>store = TCPStore(result.hostname, result.port, world_size, start_daemon)<br/><NewLine>RuntimeError: connect() timed out.</p><NewLine><p>Thanks for your time and reading!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The error has been fixed.<br/><NewLine>‘Stop_waiting response is expected’ error occurred in TCPStore.cpp. So it was actually the communication problem. It works finally when I reinstalled NCCL: <a href=""https://github.com/NVIDIA/nccl.git"" rel=""nofollow noopener"">https://github.com/NVIDIA/nccl.git</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/yangkz; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/yangkz; <NewLine> ,"REPLY_DATE 1: May 14, 2020,  7:07pm; <NewLine> REPLY_DATE 2: May 15, 2020,  8:02am; <NewLine> REPLY_DATE 3: May 18, 2020,  1:46am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
81677,PyTorch Distributed Data Parallel Process 0 terminated with SIGKILL,2020-05-17T18:31:43.166Z,0,401,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I am relatively new to PyTorch Distributed Parallel and I have access to GPU nodes with Infiniband so I think I can use the NCCL Backend. I am using Slurm scripts to submit my jobs on these resources. The following is an example of a SLURM script that I am using to submit a job. <strong>NOTE HERE</strong> that I am using OpenMPI to launch multiple instances of my docker container on the different nodes in the job. The docker container that I am using for this job is linked here.<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/LordVoldemort28/docker-deep-machine-learning/blob/master/Dockerfile"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/LordVoldemort28/docker-deep-machine-learning/blob/master/Dockerfile"" rel=""nofollow noopener"" target=""_blank"">LordVoldemort28/docker-deep-machine-learning/blob/master/Dockerfile</a></h4><NewLine><pre><code class=""lang-"">FROM unlhcc/cuda-ubuntu:9.2<NewLine>MAINTAINER Rahul Prajapati &lt;rahul.prajapati90904@gmail.com&gt;<NewLine><NewLine>#github https://github.com/LordVoldemort28/docker-deep-machine-learning<NewLine><NewLine>#Credit - waleedka/modern-deep-learning<NewLine>#https://hub.docker.com/r/waleedka/modern-deep-learning/ <NewLine><NewLine># Supress warnings about missing front-end. As recommended at:<NewLine># http://stackoverflow.com/questions/22466255/is-it-possibe-to-answer-dialog-questions-when-installing-under-docker<NewLine>ARG DEBIAN_FRONTEND=noninteractive<NewLine><NewLine>RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \ <NewLine>    apt-utils \ <NewLine>    git \ <NewLine>    curl \ <NewLine>    vim \ <NewLine>    unzip \ <NewLine>    wget \<NewLine>    build-essential cmake \ <NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/LordVoldemort28/docker-deep-machine-learning/blob/master/Dockerfile"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine><strong>SLURM FILE</strong><NewLine><pre><code class=""lang-auto"">#!/bin/sh<NewLine>#SBATCH --ntasks-per-node=2<NewLine>#SBATCH --time=168:00:00<NewLine>#SBATCH --partition=gpu<NewLine>#SBATCH --mem=80gb<NewLine>#SBATCH --nodes=2<NewLine>#SBATCH --gres=gpu:2<NewLine>#SBATCH --constraint=gpu_32gb<NewLine>#SBATCH --job-name=binary_classification<NewLine>#SBATCH --output=binary_classification.out<NewLine><NewLine>pwd; hostname; date<NewLine>env | grep SLURM | sort<NewLine><NewLine>ulimit -s unlimited<NewLine>ulimit -c unlimited<NewLine><NewLine>export PYTHONPATH=$WORK/tf-gpu-pkgs<NewLine><NewLine>module purge<NewLine>module load singularity compiler/gcc/4.8 openmpi<NewLine>module list<NewLine><NewLine>mpirun singularity exec $WORK/pyopencv.sif python3 -u $@ --multiprocessing_distributed --dist_backend='nccl' --rank=0 --use_adam=1 --benchmarks=1 --benchmark_arch='vgg19' --batch_size=128 --test=1 --transfer=0 --dataset='binary_dataset'<NewLine>cgget -r memory.max_usage_in_bytes /slurm/uid_${UID}/job_${SLURM_JOBID}/<NewLine>mem_report<NewLine></code></pre><NewLine><p>When I run the job, I get the following error and I am not sure what is exactly causing this issue. My implementation is similar to the ImageNet example on PyTorch distributed. I have not been able to make this work for weeks now and would really appreciate any help on this since I don’t have much experience with distributed systems.</p><NewLine><p><strong>ERROR RECEIVED</strong></p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""distributed_main.py"", line 391, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""distributed_main.py"", line 138, in main<NewLine>    args=(ngpus_per_node, args))<NewLine>  File ""/usr/local/lib/python3.5/dist-packages/torch/multiprocessing/spawn.py"", line 171, in spawn<NewLine>    while not spawn_context.join():<NewLine>  File ""/usr/local/lib/python3.5/dist-packages/torch/multiprocessing/spawn.py"", line 107, in join<NewLine>    (error_index, name)<NewLine>Exception: process 0 terminated with signal SIGKILL<NewLine>-------------------------------------------------------<NewLine>Primary job  terminated normally, but 1 process returned<NewLine>a non-zero exit code.. Per user-direction, the job has been aborted.<NewLine>-------------------------------------------------------<NewLine>--------------------------------------------------------------------------<NewLine>mpirun detected that one or more processes exited with non-zero status, thus causing<NewLine>the job to be terminated. The first process to do so was:<NewLine><NewLine>  Process name: [[53625,1],0]<NewLine>  Exit code:    1<NewLine></code></pre><NewLine><p>Thank you,<br/><NewLine>Ayush</p><NewLine></div>",https://discuss.pytorch.org/u/ayushm-agrawal,(Ayush Manish Agrawal),ayushm-agrawal,"May 17, 2020,  6:31pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you tried other backend types (Gloo, MPI), do they fail with the same error?</p><NewLine><p>How do you initialize process group and construct DistributedDataParallel?</p><NewLine><p>For debugging, we would first try a minimum DDP example like <a href=""https://pytorch.org/docs/master/notes/ddp.html#example"" rel=""nofollow noopener"">this one</a> and make sure it works correctly with the given environment. And then switch to more complex models.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: May 17, 2020,  6:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
79577,Concurrency concerns on the example of parameter server using RPC,2020-05-03T21:23:50.528Z,13,245,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Dear <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>I have noticed that your team/colleague released a new tutorial on the parameter server using the RPC framework (<a href=""https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html"" rel=""nofollow noopener"">rpc_param_server_tutorial</a>). I really appreciate the example with detailed and helpful explanations, and it seems to me that it can work with multiple trainers accessing to the same parameter server. I think the code below makes sure there is only one parameter server can be created by the trainers.</p><NewLine><pre><code class=""lang-auto""># The global parameter server instance.<NewLine>param_server = None<NewLine># A lock to ensure we only have one parameter server.<NewLine>global_lock = Lock()<NewLine><NewLine><NewLine>def get_parameter_server(num_gpus=0):<NewLine>    """"""<NewLine>    Returns a singleton parameter server to all trainer processes<NewLine>    """"""<NewLine>    global param_server<NewLine>    # Ensure that we get only one handle to the ParameterServer.<NewLine>    with global_lock:<NewLine>        if not param_server:<NewLine>            # construct it once<NewLine>            param_server = ParameterServer(num_gpus=num_gpus)<NewLine>        return param_server<NewLine><NewLine>def run_parameter_server(rank, world_size):<NewLine>    # The parameter server just acts as a host for the model and responds to<NewLine>    # requests from trainers.<NewLine>    # rpc.shutdown() will wait for all workers to complete by default, which<NewLine>    # in this case means that the parameter server will wait for all trainers<NewLine>    # to complete, and then exit.<NewLine>    print(""PS master initializing RPC"")<NewLine>    rpc.init_rpc(name=""parameter_server"", rank=rank, world_size=world_size)<NewLine>    print(""RPC initialized! Running parameter server..."")<NewLine>    rpc.shutdown()<NewLine>    print(""RPC shutdown on parameter server."")<NewLine></code></pre><NewLine><p>However, when it comes to Distributed Autograd, forward, and back passes using the training loop below:</p><NewLine><pre><code class=""lang-auto"">def run_training_loop(rank, num_gpus, train_loader, test_loader):<NewLine>...<NewLine>    for i, (data, target) in enumerate(train_loader):<NewLine>        with dist_autograd.context() as cid:<NewLine>            model_output = net(data)<NewLine>            target = target.to(model_output.device)<NewLine>            loss = F.nll_loss(model_output, target)<NewLine>            if i % 5 == 0:<NewLine>                print(f""Rank {rank} training batch {i} loss {loss.item()}"")<NewLine>            dist_autograd.backward(cid, [loss])<NewLine>            # Ensure that dist autograd ran successfully and gradients were<NewLine>            # returned.<NewLine>            assert remote_method(<NewLine>                ParameterServer.get_dist_gradients,<NewLine>                net.param_server_rref,<NewLine>                cid) != {}<NewLine>            opt.step(cid)<NewLine><NewLine>     print(""Training complete!"")<NewLine>     print(""Getting accuracy...."")<NewLine>     get_accuracy(test_loader, net)<NewLine></code></pre><NewLine><p>How would I make sure there are no concurrency issues? For example, if you have two trainers and have a situation where one trainer is doing the forward propagation and the other is doing the backward pass, how to make sure the two processes are not conflicting with each other?</p><NewLine><p>Thanks,</p><NewLine></div>",https://discuss.pytorch.org/u/ZiyiZhu,(Ziyi Zhu),ZiyiZhu,"May 3, 2020,  9:23pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ziyizhu"">@ZiyiZhu</a></p><NewLine><p>That’s a great question! I have two comments on this concurrent param updating approach:</p><NewLine><ol><NewLine><li>Each trainer has its dedicated gradients on the parameter server. When using distributed autograd, the computed gradients are stored in the autograd context (instead of in <code>param.grad</code>), which can be identified by the unique <code>cid</code>. So there is no concern on race errors for gradient computation.</li><NewLine><li>It is true that there can be multiple trainers updating the same parameter concurrently, and it is true that the parameter might change after a trainer computes the gradients and before it applies the gradients to the parameter. This idea is partially borrowed from <a href=""https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf"" rel=""nofollow noopener"">the Hogwild! paper</a>. And the approach of using “not perfectly up-to-date gradients” can also be found in other projects (e.g., <a href=""https://www.microsoft.com/en-us/research/uploads/prod/2019/08/fiddle_pipedream_sosp19.pdf"" rel=""nofollow noopener"">PipeDream</a>).</li><NewLine></ol><NewLine><p>In general, this is a trade-off between model accuracy and training speed. And in practice, we saw several use cases where this helped to accelerate training a lot with little or none accuracy penalty.</p><NewLine><p>cc <a class=""mention"" href=""/u/rvarm1"">@rvarm1</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Dear <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>Thank you very much for your explanations with suggested paper references. I will follow up if I still have more questions on the RPC when I finish reading the papers.</p><NewLine><p>Best,<br/><NewLine>Ziyi</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Dear <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>I have briefly gone through the <a href=""https://www.microsoft.com/en-us/research/uploads/prod/2019/08/fiddle_pipedream_sosp19.pdf"" rel=""nofollow noopener"">PipeDream</a> paper. Now I understand better how this parameter RPC example can be implicitly pipelined which speeds up the training phase for model parallelism. However, I still have several questions below and hope you could help to answer:</p><NewLine><ol><NewLine><li><NewLine><p>This <a href=""https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html"" rel=""nofollow noopener"">rpc_parameter_example</a> seems to me not a strict parameter server strategy for data parallelism training. Multiple trainers can update the “parameter server” however that is done separately by the distributed  Autograd which means there is no averaging for the gradients of each trainer, IIUC? Is a typical parameter server strategy looking more like<br/><NewLine><img alt=""image"" data-base62-sha1=""fxVGcJBHs6XyhQqOGE8VRufc5JJ"" height=""428"" src=""https://discuss.pytorch.org/uploads/default/original/3X/6/c/6cf67ed17792fc2d71384f7ffb79ba49081877b7.png"" width=""445""/><br/><NewLine>from <a href=""https://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf"" rel=""nofollow noopener"">Scaling Distributed</a> paper instead? If I want to do this parameter server strategy (averaging the gradients of each trainer ) for data-parallel , is it possible to do that with RPC?</p><NewLine></li><NewLine><li><NewLine><p>I remember last time you mentioned that by default the Distributed Data Parallel (<a href=""https://pytorch.org/docs/stable/notes/ddp.html"" rel=""nofollow noopener"">DDP</a>) uses ring allreduce for averaging the gradients. Is ring allreduce better than parameter server all the time? But can the DDP use parameter server strategy instead?</p><NewLine></li><NewLine><li><NewLine><p>In the <a href=""https://www.microsoft.com/en-us/research/uploads/prod/2019/08/fiddle_pipedream_sosp19.pdf"" rel=""nofollow noopener"">PipeDream</a> paper, the workload can be fine-grained and controlled as below:<br/><NewLine><img alt=""image"" data-base62-sha1=""hEabU7UcpkiOkt24v1HU4Jizfl9"" height=""355"" src=""https://discuss.pytorch.org/uploads/default/original/3X/7/b/7baf3f04037b0332b273258765bdb758c05407c3.png"" width=""613""/><br/><NewLine>Would RPC be able to do the same? Or what would be the order/priority for the RPC to choose running which job (1,2,3,4 forward or 1,2,3,4 backward) on the GPU?</p><NewLine></li><NewLine></ol><NewLine><p>Thank you very much!<br/><NewLine>Best,</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>I was referring to this part of the PipeDream paper:</li><NewLine></ol><NewLine><blockquote><NewLine><p>Each backward pass in a stage results<br/><NewLine>in weight updates; the next forward pass uses the latest version<br/><NewLine>of weights available, and “stashes"" a copy of these weights to use<br/><NewLine>during the corresponding backward pass. Although the forward<br/><NewLine>pass will not see updates from incomplete in-flight mini-batches,<br/><NewLine>learning is still effective because model weights change relatively<br/><NewLine>slowly and bounded staleness has been found effective in improving training speeds</p><NewLine></blockquote><NewLine><p>And the Hogwild paper linked above also mentioned sth with similar no-lock spirit:</p><NewLine><blockquote><NewLine><p>In this work, we propose a simple strategy for eliminating the overhead associated with locking:<br/><NewLine>run SGD in parallel without locks, a strategy that we call Hogwild!. In Hogwild!, processors are<br/><NewLine>allowed equal access to shared memory and are able to update individual components of memory<br/><NewLine>at will. Such a lock-free scheme might appear doomed to fail as processors could overwrite each<br/><NewLine>other’s progress. However, when the data access is sparse, meaning that individual SGD steps only<br/><NewLine>modify a small part of the decision variable, we show that memory overwrites are rare and that<br/><NewLine>they introduce barely any error into the computation when they do occur. We demonstrate both<br/><NewLine>theoretically and experimentally a near linear speedup with the number of processors on commonly<br/><NewLine>occurring sparse learning problems.</p><NewLine></blockquote><NewLine><p>In general, if lock is necessary (e.g., due to unacceptable accuracy drop), applications can do so by explicitly acquiring locks, but this will certainly have impact on training speed. So it is up to the application to decide how to play with the trade-off.</p><NewLine><ol start=""2""><NewLine><li><NewLine></li><NewLine></ol><NewLine><blockquote><NewLine><p>Is ring allreduce better than parameter server all the time?</p><NewLine></blockquote><NewLine><p>No, the merit of allreduce is that it can (actually depend on the loss function) achieve mathematical equivalence with local training. But whether synchronous training is better than asynchronous training is an open question. <code>DistributedDataParallel</code> in PyTorch is using allreduce, but there are also that types of DDP, e.g., <a href=""https://openreview.net/forum?id=SkxJ8REYPH"" rel=""nofollow noopener"">SlowMo</a> is using gossip.</p><NewLine><blockquote><NewLine><p>But can the DDP use parameter server strategy instead?</p><NewLine></blockquote><NewLine><p>Yes, it certainly can. The holgwild paper linked above can be one example of using parameter-server-based data parallel on the “decision variable”, e.g., an embedding table.</p><NewLine><ol start=""3""><NewLine><li><NewLine></li><NewLine></ol><NewLine><p>Currently the RPC package does not support priority-based communication/execution yet, but this might be doable from the application side. The callee can maintain priority queues to order incoming tasks and use an RPC argument to indicate the priority. The <a href=""https://github.com/pytorch/pytorch/issues/36071"" rel=""nofollow noopener"">WIP async user function</a> will help to reduce the callee-side overhead for this use case.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>Thank you very much again for the provided explanations and references. I will look into them and hopefully have efficient implementations of DDP and RPC in Pytorch.</p><NewLine><p>Best,<br/><NewLine>Ziyi</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Dear <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>Following up on the <a href=""https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf"" rel=""nofollow noopener"">Hogwild!</a> paper mentioned previously. I also found out that Pytorch has the <a href=""https://github.com/pytorch/examples/tree/master/mnist_hogwild"" rel=""nofollow noopener"">PytorchHogwild</a> example using multiprocessing techniques.</p><NewLine><p>I would want to redo this and then extend it for multi-machine and distributed training, either using existing Pytorch DDP or custom design built upon <a href=""https://pytorch.org/docs/stable/distributed.html"" rel=""nofollow noopener"">distributed communication APIs</a>.</p><NewLine><p>The big picture is shown below:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/c4e7646b72ce1f5ee0d15c505e9aacf85525b49a"" href=""https://discuss.pytorch.org/uploads/default/original/3X/c/4/c4e7646b72ce1f5ee0d15c505e9aacf85525b49a.png"" title=""image""><img alt=""image"" data-base62-sha1=""s5Tl4nnNH4dSUctrEgh6rJwWJvk"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/4/c4e7646b72ce1f5ee0d15c505e9aacf85525b49a_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/original/3X/c/4/c4e7646b72ce1f5ee0d15c505e9aacf85525b49a.png"" width=""663""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">737×555 20.2 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>Within the Machine#0 !Hogwild can be performed. Does this make sense to you?</p><NewLine><p>However, when I am making a simple example to test the torch.multiprocessing first locally, it seems that multiprocessing cannot work. Please see the result below:</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import torch<NewLine>from torch import nn<NewLine>import torch.distributed as dist<NewLine>import torch.multiprocessing as mp<NewLine>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<NewLine>print(torch.__version__)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">&gt;&gt; 1.4.0<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">class Net(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__() <NewLine>        self.out = nn.Linear(in_features=5, out_features=1)<NewLine><NewLine>    def forward(self, t):<NewLine>        # output<NewLine>        t = self.out(t)<NewLine>        return t<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">def addNet(model):<NewLine>    for para in model.parameters():<NewLine>        tmp = torch.ones_like(para.data)<NewLine>        para.data = para.data + tmp<NewLine>        print(para.data)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">torch.manual_seed(101)<NewLine>model = Net()<NewLine>model.share_memory()<NewLine>for para in model.parameters():<NewLine>    print(para.data)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; tensor([[-0.2701, -0.0445, -0.3659,  0.3463, -0.1884]])<NewLine>&gt;&gt;&gt; tensor([-0.4306])<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">num_processes = 2<NewLine>processes = []<NewLine>for rank in range(num_processes):<NewLine>    p = mp.Process(target=addNet, args=(model,))<NewLine>    p.start()<NewLine>    processes.append(p)<NewLine>for p in processes:<NewLine>    p.join()<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; tensor([[0.7299, 0.9555, 0.6341, 1.3463, 0.8116]])<NewLine>&gt;&gt;&gt; tensor([[0.7299, 0.9555, 0.6341, 1.3463, 0.8116]])<NewLine>&gt;&gt;&gt; tensor([0.5694])<NewLine>&gt;&gt;&gt; tensor([0.5694])<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">for para in model.parameters():<NewLine>    print(para.data)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; tensor([[-0.2701, -0.0445, -0.3659,  0.3463, -0.1884]])<NewLine>&gt;&gt;&gt; tensor([-0.4306])<NewLine></code></pre><NewLine><p>It seems to me that the children processes just made a copy of the NN in the parent process and the NN was not being shared among the parent and two children processes. Did I miss anything from <a href=""https://github.com/pytorch/examples/tree/master/mnist_hogwild"" rel=""nofollow noopener"">PytorchHogwild</a>? Or it will be very much appreciated if you could share any thoughts.</p><NewLine><p>Best,<br/><NewLine>Ziyi</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ziyizhu"">@ZiyiZhu</a> did you use the “spawn” mode? If you run the example as is, does it work?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a></p><NewLine><p>It seems not. Even the addNet is not printing anything.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/da5a5b93615e3d2e64a007a8f3028034ba2fffe9"" href=""https://discuss.pytorch.org/uploads/default/original/3X/d/a/da5a5b93615e3d2e64a007a8f3028034ba2fffe9.png"" title=""image""><img alt=""image"" data-base62-sha1=""v9DG6mAy31zsmyLU1v4D4MOJOsx"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/a/da5a5b93615e3d2e64a007a8f3028034ba2fffe9_2_10x10.png"" height=""332"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/a/da5a5b93615e3d2e64a007a8f3028034ba2fffe9_2_690x332.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/a/da5a5b93615e3d2e64a007a8f3028034ba2fffe9_2_690x332.png, https://discuss.pytorch.org/uploads/default/optimized/3X/d/a/da5a5b93615e3d2e64a007a8f3028034ba2fffe9_2_1035x498.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/d/a/da5a5b93615e3d2e64a007a8f3028034ba2fffe9_2_1380x664.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1446×696 31.9 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>I sometimes run into weird errors when using multiprocessing in notebook. Does it work if you directly launch the script from command line? Let me try that locally.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh I see, you need to use the inplace <code>add_</code>, sth like:</p><NewLine><pre><code class=""lang-python"">import os<NewLine>import torch<NewLine>from torch import nn<NewLine>import torch.distributed as dist<NewLine>import torch.multiprocessing as mp<NewLine>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<NewLine>print(torch.__version__)<NewLine><NewLine>class Net(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.out = nn.Linear(in_features=5, out_features=1)<NewLine><NewLine>    def forward(self, t):<NewLine>        # output<NewLine>        t = self.out(t)<NewLine>        return t<NewLine><NewLine><NewLine>def addNet(model):<NewLine>    for para in model.parameters():<NewLine>        tmp = torch.ones_like(para.data)<NewLine>        with torch.no_grad():<NewLine>            para.add_(tmp)<NewLine>            print(para.data)<NewLine><NewLine>if __name__==""__main__"":<NewLine>    mp.set_start_method('spawn')<NewLine><NewLine>    torch.manual_seed(101)<NewLine>    model = Net()<NewLine>    model.share_memory()<NewLine>    for para in model.parameters():<NewLine>        print(para.data)<NewLine><NewLine>    num_processes = 2<NewLine>    processes = []<NewLine>    for rank in range(num_processes):<NewLine>        p = mp.Process(target=addNet, args=(model, ))<NewLine>        p.start()<NewLine>        processes.append(p)<NewLine><NewLine>    for p in processes:<NewLine>        p.join()<NewLine><NewLine><NewLine>    for para in model.parameters():<NewLine>        print(para.data)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>Thank you very much for running the code on your end. Yes, I tested and it can work by launching the script from command line. However, the Jupter notebook cannot work, which is interesting.</p><NewLine><p>Going back to the first question, do you see any potential problems with this?</p><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""79577"" data-username=""ZiyiZhu""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/z/ce7236/40.png"" width=""20""/> ZiyiZhu:</div><NewLine><blockquote><NewLine><p>The big picture is shown below:</p><NewLine><p><img alt=""image"" data-base62-sha1=""s5Tl4nnNH4dSUctrEgh6rJwWJvk"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/4/c4e7646b72ce1f5ee0d15c505e9aacf85525b49a_2_10x10.png"" height=""498"" src=""https://discuss.pytorch.org/uploads/default/original/3X/c/4/c4e7646b72ce1f5ee0d15c505e9aacf85525b49a.png"" width=""662""/></p><NewLine></blockquote><NewLine></aside><NewLine><p>Thank you!</p><NewLine><p>Best,<br/><NewLine>Ziyi</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hii <a class=""mention"" href=""/u/ziyizhu"">@ZiyiZhu</a>, I think the code would run, and it’s like using multiple NCCL allreduce in a HogWild manner, but I am not confident on the correctness on DDP in this use case. The allreduce is not an atomic operation, it needs to go through the ring (say we are using ring allreduce here) twice to collect and propagate the values. So, it is possible that, after allreduce, model parameters on different processes in the same group are no longer in sync. This breaks DDP’s assumption, and this gap could get larger over more iterations. You might need some extra code to re-sync DDP model parameters every n iterations.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>I guess we can only find out by training some real models with this scheme. Async training is full of surprises.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see, and thank you very much for the insights. I would try to re-sync after some iterations or maybe use other schemes such as ps-worker instead.</p><NewLine><p>Best,</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> ,"REPLY_DATE 1: May 3, 2020, 10:25pm; <NewLine> REPLY_DATE 2: May 5, 2020,  7:14pm; <NewLine> REPLY_DATE 3: May 7, 2020,  6:25am; <NewLine> REPLY_DATE 4: May 7, 2020,  3:01pm; <NewLine> REPLY_DATE 5: May 8, 2020,  3:09pm; <NewLine> REPLY_DATE 6: May 15, 2020,  7:34pm; <NewLine> REPLY_DATE 7: May 15, 2020,  7:42pm; <NewLine> REPLY_DATE 8: May 15, 2020,  7:50pm; <NewLine> REPLY_DATE 9: May 15, 2020,  7:54pm; <NewLine> REPLY_DATE 10: May 15, 2020,  8:04pm; <NewLine> REPLY_DATE 11: May 15, 2020,  8:22pm; <NewLine> REPLY_DATE 12: May 15, 2020, 10:06pm; <NewLine> REPLY_DATE 13: May 15, 2020, 10:15pm; <NewLine> REPLY_DATE 14: May 15, 2020, 10:31pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: 1 Like; <NewLine> REPLY 14 LIKES: ; <NewLine> 
81342,How to divide the dataset when it is distributed,2020-05-15T09:32:39.180Z,0,83,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone!</p><NewLine><p>I am a beginner in PyTorch. Now I want to divide a dataset into two parts: the train set and validation set when using <code>torch.distributed</code>. I know that on a single GPU  I can do this using a sampler:</p><NewLine><pre><code class=""lang-auto"">indices = list(range(len(train_data)))<NewLine>train_loader = torch.utils.data.DataLoader(<NewLine>      train_data, batch_size=args.batch_size,<NewLine>      sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),<NewLine>      pin_memory=True, num_workers=2)<NewLine></code></pre><NewLine><p>But when I want to train it in a parallel way using <code>torch.distributed</code>, I have to use another sampler, namely, <code>sampler =  torch.utils.data.distributed.DistributedSampler(train_data)</code></p><NewLine><p>So how should I do to use the two samplers, so that I can divide the dataset and distribute it at the same time?</p><NewLine><p>Thank you very much for any help!</p><NewLine></div>",https://discuss.pytorch.org/u/sunshk1227,,sunshk1227,"May 15, 2020,  9:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah, I find a solution with the help of Szymon Maszke. Use <code>torch.utils.data.random_split</code> instead. Namely,</p><NewLine><pre><code class=""lang-auto"">train_data, val_data = torch.utils.data.random_split(<NewLine>      train_data, (num_train, num_val))<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/vincentqb"">@vincentqb</a> just in case if there are different suggestions.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/sunshk1227; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: May 15, 2020, 10:29am; <NewLine> REPLY_DATE 2: May 15, 2020,  2:36pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
81359,Does DistributedOptimizer support zero_grad and lr_scheduling?,2020-05-15T11:27:16.777Z,0,159,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I try to use distributed_rpc to implement parameter server. I would like to use zero_grad and lr_scheduling feature in the trainer. But it seems that the DistributedOptimizer does not support this. Is there any workaround?</p><NewLine></div>",https://discuss.pytorch.org/u/Kunlin_Yang,(Kunlin Yang),Kunlin_Yang,"May 15, 2020, 10:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/kunlin_yang"">@Kunlin_Yang</a></p><NewLine><h3><code>zero_grad</code></h3><NewLine><p>The reason <code>DistributedOptimizer</code> does not provide a <code>zero_grad</code> API is because the gradients of each backward pass is stored in its own dedicated context (instead of in <code>param.grad</code>), and the context will be cleared when exiting the <code>with dist_autograd.context() as context_id:</code> scope. So <code>zero_grad</code> is not needed here. We can certainly add it if necessary, and it won’t be too hard to implement it in the application code either. E.g.,</p><NewLine><pre><code class=""lang-python"">def zero_grad(pr, context_id):<NewLine>    dist_autograd.get_gradients(context_id)[pr.local_value()].zero_()<NewLine><NewLine><NewLine>with dist_autograd.context() as context_id:<NewLine>    # omitting forward-backward-optstep here<NewLine>    futs = []<NewLine>    for pr in param_rrefs:<NewLine>        futs.append(rpc.rpc_async(pr.owner(), zero_grad, args=(pr, context_id)))<NewLine>    [fut.wait() for fut in futs]<NewLine></code></pre><NewLine><p>Or is there a different reason you would like to use the <code>zero_grad</code> API?</p><NewLine><h3><code>lr_scheduling</code></h3><NewLine><p>Currently, there is no distributed implementation or lr scheduling yet. I created an issue to track this: <a href=""https://github.com/pytorch/pytorch/issues/38548"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/38548</a></p><NewLine><p>For now, you will need to do that using the raw RPC API. You can access the RRefs of the remote optimizers through <code>DistributedOptimizer().remote_optimizers</code>, so it can be sth like:</p><NewLine><pre><code class=""lang-python"">def create_lr_schheduler(opt_rref):<NewLine>     # create and return lr_schheduler<NewLine><NewLine>def lrs_step(lrs_rref):<NewLine>    lrs_rref.local_value().step()<NewLine><NewLine>opt = DistributedOptimizer(...)<NewLine>lrs_rrefs = []<NewLine>for opt_rref in opt.remote_optimizers:<NewLine>    lrs_rrefs = rpc.remote(opt_rref.owner(), create_lr_schheduler, args=(opt_rref,))<NewLine><NewLine>with dist_autograd.context() as context_id:<NewLine>    # omitting forward-backward-optstep here<NewLine>    futs = []<NewLine>    for lrs_rref iin lrs_rrefs:<NewLine>        futs.append(rpc.rpc_async(lrs_rref.owner(), lrs_step, args=(lrs_rref,)))<NewLine>    [fut.wait() for fut in futs]<NewLine></code></pre><NewLine><p>If you are using master branch, the above code can be simplified with <a href=""https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.RRef.rpc_async"" rel=""nofollow noopener""><code>RRef.rpc_async()</code></a> API.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: May 15, 2020, 10:45pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
81264,RPC agent timeout error when not doing anything,2020-05-14T20:32:21.594Z,2,177,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using torch.distributed.rpc library and the RPC agent crashes if I don’t do any RPC call for 30 minutes.<br/><NewLine>Is this an expected behavior of the RPC library? I think it crashes because it thinks something went wrong when there is no RPC traffic for 30 minutes; however, it times out and crashes even when there was no RPC traffic because my code was written that way!</p><NewLine><p>Below is a code snippet which can reproduce the problematic behavior. If waiting for 30 minutes is too long to test, you can change the hardcoded value in /rpc/backend_registry.py file (detailed in the comment).</p><NewLine><pre><code class=""lang-auto"">import torch.distributed.rpc as rpc<NewLine>from torch.multiprocessing import Process<NewLine><NewLine>import os<NewLine>import time<NewLine><NewLine>def do_nothing():<NewLine>    pass<NewLine><NewLine>def test(rank, size):<NewLine>    rpc.init_rpc(""Rank""+str(rank), rank=rank, world_size=size)<NewLine>    print(""Rank %s rpc init"" % rank)<NewLine><NewLine>    i = 0<NewLine>    # To test easily, I changed &lt;PATH_TO_TORCH_LIB&gt;/torch/distributed/rpc/backend_registry.py.<NewLine>    # Under def _process_group_init_backend_handler(),<NewLine>    # I changed the below line<NewLine>    # &gt;&gt; process_group_timeout = rpc_constants.DEFAULT_PROCESS_GROUP_TIMEOUT<NewLine>    # (which makes the timeout 30 minutes), to somewhat shorter value, e.g.,<NewLine>    # &gt;&gt; process_group_timeout = datetime.timedelta(seconds=10).<NewLine>    # Otherwise, if I wait for 30 min the problem still occurs.<NewLine><NewLine>    ## Loop that does not do anything for a long time...<NewLine>    while i &lt; 10:<NewLine>        time.sleep(1)<NewLine>        print(""Rank %s %s sec passed..."" % (rank, i))<NewLine><NewLine>        ## Uncommenting the below two lines makes the crash go away!<NewLine>        ## I.e., generating some RPC traffic.<NewLine>        #target = rank ^ 0x1<NewLine>        #rpc.rpc_sync(""Rank""+str(target), do_nothing)<NewLine>        i += 1<NewLine><NewLine>    rpc.shutdown()<NewLine>    print(""Rank %s rpc shutdown"" % rank)<NewLine>    pass<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    os.environ['MASTER_ADDR'] = ""localhost""<NewLine>    os.environ['MASTER_PORT'] = ""29502""<NewLine><NewLine>    processes = []<NewLine>    for rank in [0,1]:<NewLine>        p = Process(target=test, args=(rank, 2, ))<NewLine>        p.start()<NewLine>        processes.append(p)<NewLine><NewLine>    for p in processes:<NewLine>        p.join()<NewLine><NewLine></code></pre><NewLine><p>The error message is:</p><NewLine><pre><code class=""lang-auto"">[E process_group_agent.cpp:664] Encountered exception in ProcessGroupAgent::listenLoop(): [/pytorch/third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:84] Timed out waiting 10000ms for recv operation to complete on worker 1. This means that the RPC agent is in an unhealthy state and unusable.<NewLine></code></pre><NewLine><p>I wonder if this is a bug, an expected behavior, or if I am using the API in an incorrect way. If it is an expected behavior, is there any workaround?</p><NewLine><p>I am mainly experiencing this behavior because my code has a process that does not do any RPC calls, but instead calls functions under pytorch.distributed, such as distributed.all_reduce().<br/><NewLine>I first tried not initializing rpc at all in those processes, but instead calling distributed.init_process_group().<br/><NewLine>However, this made rpc.init_rpc() calls in other processes to hang or crash; I am suspecting that the problem happens because rpc.init_rpc() calls distributed.init_process_group() internally and somehow they don’t play well when some processes call init_process_group() via init_rpc() and others call it directly…<br/><NewLine>If rpc timing out after 30 min is an expected behavior, maybe I need to find a way to make some processes to call rpc.init_rpc() and others to call distributed.init_process_group() without failure.</p><NewLine><p>Thank you in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/kmaeng,,kmaeng,"May 14, 2020,  8:34pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/kmaeng"">@kmaeng</a>, there are actually RPC activity for graceful RPC shutdown. See the code below. It’s basically using an RPC to prevent the idle process from exiting too early.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/9d0e935b489a33b87711b9d5d3525a7282ab89c1/torch/distributed/rpc/api.py#L133-L192"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/9d0e935b489a33b87711b9d5d3525a7282ab89c1/torch/distributed/rpc/api.py#L133-L192"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/9d0e935b489a33b87711b9d5d3525a7282ab89c1/torch/distributed/rpc/api.py#L133-L192</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""133"" style=""counter-reset: li-counter 132 ;""><NewLine><li>@_require_initialized</li><NewLine><li>def _wait_all_workers():</li><NewLine><li>    r""""""</li><NewLine><li>    Block until all local and remote RPC processes reach this method and wait</li><NewLine><li>    for all outstanding work to complete. Every RPC process must call this</li><NewLine><li>    method before exit to perform a graceful shutdown. This should be used to</li><NewLine><li>    terminate the RPC framework, and there is no guarantee that the RPC</li><NewLine><li>    framework will work after this method returns.</li><NewLine><li>    """"""</li><NewLine><li>    assert (</li><NewLine><li>        _ALL_WORKER_NAMES is not None</li><NewLine><li>    ), ""`_ALL_WORKER_NAMES` is not initialized for `def _wait_all_workers`.""</li><NewLine><li>    leader_worker_name = sorted(_ALL_WORKER_NAMES)[0]</li><NewLine><li><NewLine></li><li>    self_worker_name = _get_current_rpc_agent().get_worker_info().name</li><NewLine><li><NewLine></li><li>    global _wait_all_workers_sequence_id</li><NewLine><li>    with _wait_all_workers_dict_lock:</li><NewLine><li>        sequence_id = _wait_all_workers_sequence_id</li><NewLine><li>        _wait_all_workers_sequence_id += 1</li><NewLine></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/9d0e935b489a33b87711b9d5d3525a7282ab89c1/torch/distributed/rpc/api.py#L133-L192"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>To get around this, you can increase the default RPC timeout. Depending on the version you are using, you can either provide the timeout value in <a href=""https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.ProcessGroupRpcBackendOptions"" rel=""nofollow noopener""><code>init_rpc</code></a> (with v1.5), or directly call <a href=""https://github.com/pytorch/pytorch/blob/v1.4.1/test/rpc_test.py#L1232"" rel=""nofollow noopener""><code>rpc._set_rpc_timeout</code></a> (with v1.4).</p><NewLine><p>Or if you know for sure when a process can safely exit, you can use <code>shutdown(graceful=False)</code> and do the termination detection in application code.</p><NewLine><p>Per op (rpc_sync/rpc_async/remote) timeout are coming soon.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your response. I don’t fully understand your answer.<br/><NewLine>Can you clarify some points?</p><NewLine><ol><NewLine><li><NewLine><p>I don’t understand what you are trying to show with the code you linked. Are you saying I can use the _wait_all_workers() or a snippet of the code inside to work around the issue (I am already calling rpc.shutdown() at the end that internally calls this. It is just that the processes dies before reaching here)?<br/><NewLine>My main issue is that the rpc process group agent is killed after 30 minutes of being idle. Are you suggesting I just let it die and do graceful=False so that other processes do not die while trying to shutdown rpc?</p><NewLine></li><NewLine><li><NewLine><p>I am using v1.5 and tried giving timeout value to init_rpc, but did not work. I followed the code and figured the 30 min timeout only changes when I change this line:<br/><NewLine><a href=""https://github.com/pytorch/pytorch/blob/91f451a5e69d2969d730744e98e059d05e63a84d/torch/distributed/rpc/backend_registry.py#L114"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/91f451a5e69d2969d730744e98e059d05e63a84d/torch/distributed/rpc/backend_registry.py#L114</a><br/><NewLine>As you can see, that line and below takes the rpc_constants.DEFAULT_PROCESS_GROUP_TIMEOUT and calls dist.init_process_group(), without using the provided value by me. From my testing, that value was generating the error I am keep seeing (afterward, when calling ProcessGroupAgent, the timeout value I provided is getting passed, but not for the dist.init_process_group()).</p><NewLine></li><NewLine><li><NewLine><p>In summary, is it normal to see the error message I posted if I call init_rpc and don’t use it for 30 minutes?</p><NewLine></li><NewLine></ol><NewLine><p>Thank you for your help. I really appreciate it.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see, sorry that I misread the original question.</p><NewLine><blockquote><NewLine><p>In summary, is it normal to see the error message I posted if I call init_rpc and don’t use it for 30 minutes?</p><NewLine></blockquote><NewLine><p>This is indeed a bug, and I think it is due to the following code, where the ProcessGroup RPC agent’s recvAnysource timed out. We should have passed rpc timeout to process group or set it to infinity in the listen loop.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/1f87f15ba3cd46fe5474f7783ff80ef06950e157/torch/csrc/distributed/rpc/process_group_agent.cpp#L710-L715"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/1f87f15ba3cd46fe5474f7783ff80ef06950e157/torch/csrc/distributed/rpc/process_group_agent.cpp#L710-L715"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/1f87f15ba3cd46fe5474f7783ff80ef06950e157/torch/csrc/distributed/rpc/process_group_agent.cpp#L710-L715</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""710"" style=""counter-reset: li-counter 709 ;""><NewLine><li>void ProcessGroupAgent::listenLoopInternal() {</li><NewLine><li>  while (rpcAgentRunning_.load()) {</li><NewLine><li>    // rank, tensor size, message type</li><NewLine><li>    std::vector&lt;torch::Tensor&gt; preamble = {torch::empty({4}, {torch::kInt64})};</li><NewLine><li>    auto work = pg_-&gt;recvAnysource(preamble, pg_-&gt;getRank());</li><NewLine><li>    {</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Thanks for flagging this, do you want to create an issue on github to track this? We will fix. Thanks!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""81264"" data-username=""kmaeng""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/kmaeng/40/23707_2.png"" width=""20""/> kmaeng:</div><NewLine><blockquote><NewLine><ul><NewLine><li>As you can see, that line and below takes the rpc_constants.DEFAULT_PROCESS_GROUP_TIMEOUT and calls dist.init_process_group(), without using the provided value by me. From my testing, that value was generating the error I am keep seeing (afterward, when calling ProcessGroupAgent, the timeout value I provided is getting passed, but not for the dist.init_process_group()).</li><NewLine></ul><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, exactly, we have a tracking issue related to this problem: <a href=""https://github.com/pytorch/pytorch/issues/33583"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/33583</a></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I submitted an issue (<a href=""https://github.com/pytorch/pytorch/issues/38531"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/38531</a>), thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kmaeng; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/kmaeng; <NewLine> ,"REPLY_DATE 1: May 14, 2020,  8:49pm; <NewLine> REPLY_DATE 2: May 14, 2020,  9:56pm; <NewLine> REPLY_DATE 3: May 15, 2020,  3:07am; <NewLine> REPLY_DATE 4: May 14, 2020, 10:44pm; <NewLine> REPLY_DATE 5: May 15, 2020,  3:07am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
79976,Cuda context size per process,2020-05-06T11:33:25.702Z,4,159,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I noticed that there is a large (~500MB) cuda context created per process on GPU.<br/><NewLine>can see it simply by doing:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>torch.randn(1,device=0)<NewLine></code></pre><NewLine><p>it takes 500MB (used to take 750MB in previous versions).<br/><NewLine>when multiprocessing on same GPU this is a lot of unneeded memory.<br/><NewLine>How can we work around this?</p><NewLine></div>",https://discuss.pytorch.org/u/seliad,(Saar Eliad),seliad,"May 6, 2020, 11:33am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do multiple processes have to work on the same set of GPUs? Can each process work on an exclusive set of GPUs and use <code>CUDA_VISIBLE_DEVICES</code> to control which devices they see?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I explicitly want multi processes using the same GPU.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am not aware if there is a way to avoid the per-process CUDA context or reduce its size. <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> and <a class=""mention"" href=""/u/alband"">@albanD</a> might know more.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think it’s possible to reuse a single CUDA context between processes, but haven’t looked deeply into it.<br/><NewLine>We expect the best performance using a single process per GPU.<br/><NewLine>What is your use case <a class=""mention"" href=""/u/seliad"">@seliad</a> that you want to use multiple processes on the same device?<br/><NewLine>Are you seeing any performance gains (regardless of the wasted memory)?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>For processes: A,B<br/><NewLine>Each has to do<br/><NewLine>(1) distributed communication (e.g A–&gt;C , B-&gt;D)<br/><NewLine>(2) share parameters (A&lt;-&gt;B)</p><NewLine><p>For the distributed communication I need different ranks (Im currently using cuda-aware MPI).</p><NewLine><p>Even if there is an option to use distributed communication with threads (i think that there is in mpi, not sure if Pytorch supports it), in python it is a pretty bad Idea.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I also noticed that when using 2 processes communicating through a <code>Queue</code>,<br/><NewLine>The sender process (e.g sending from device0 to device1, with <code>copy_</code>) thholds this cuda contex on both devices. I created the buffer at the sender and sent it  through a queue (reusing it) as recommended  in the docs.<br/><NewLine>(push communication model)</p><NewLine><p>Another option is to have a thread in the receiver waiting on that queue and pulling from it, I guess that won’t cause this extra memory?</p><NewLine><p>So, I wonder, maybe it could be (theoretically, and very partially) solved by creating all tensors in a single process (single owner), and sending them to all processes sharing the device.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/seliad; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/seliad; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/seliad; <NewLine> ,"REPLY_DATE 1: May 6, 2020,  2:08pm; <NewLine> REPLY_DATE 2: May 6, 2020,  2:30pm; <NewLine> REPLY_DATE 3: May 6, 2020,  2:41pm; <NewLine> REPLY_DATE 4: May 7, 2020, 12:20am; <NewLine> REPLY_DATE 5: May 7, 2020,  6:00am; <NewLine> REPLY_DATE 6: May 13, 2020,  2:42pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
80789,RuntimeError: unable to open shared memory object (while using model.share_memory()),2020-05-12T03:15:07.525Z,1,147,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I would like to put multiple models into CPU memory with shared memory support. So that I can easily transfer models among multiple processes.<br/><NewLine>I am using code logic as following:</p><NewLine><pre><code class=""lang-auto"">...<NewLine>a_list_of_models = load_models()<NewLine><NewLine>for model in a_list_of_models:<NewLine>    model.share_memory()<NewLine>...<NewLine></code></pre><NewLine><p>The code works fine with 4-5 models, but if I launched more models, I got <code>RuntimeError: unable to open shared memory object</code>.</p><NewLine><p>I saw many other people have the same runtime error while using dataloader. While I am not using dataloader here, so I am not able to set the parameter <code>num_worker</code> to be 0.</p><NewLine><p>I have also checked the shared memory limitation on my machine is unlimited. And the physical memory is more than enough to hold 100 models.</p><NewLine><p>The most relevant topic is this one: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/how-to-configure-shared-memory-size/20088"">How to configure shared memory size?</a><br/><NewLine>but, unfortunately, no answer there.</p><NewLine><p>any suggestions?</p><NewLine></div>",https://discuss.pytorch.org/u/zarzen,(Zhang Zhen),zarzen,"May 12, 2020,  3:15am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Could you possibly share a more comprehensive repro of the issue such as the models that you are loading, if possible?</p><NewLine><p>How did you check the shared memory limit on your machine? Does the output of  <code>ipcs  -lm</code> indicate that the shared memory is unlimited (this should tell you the max no. of SHM segments and max SHM size)? Also, just to confirm, have you checked if any other processes are taking up too much shared memory on your machine?</p><NewLine><p>To increase the shared memory limit, you can try setting the <code>kernel.shmmax</code> parameter (i.e. <code>sysctl -w kernel.shmmax=...</code>), and follow the steps here: <a href=""https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/tuning_and_optimizing_red_hat_enterprise_linux_for_oracle_9i_and_10g_databases/sect-oracle_9i_and_10g_tuning_guide-setting_shared_memory-setting_shmmni_parameter"" rel=""nofollow noopener"">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/tuning_and_optimizing_red_hat_enterprise_linux_for_oracle_9i_and_10g_databases/sect-oracle_9i_and_10g_tuning_guide-setting_shared_memory-setting_shmmni_parameter</a> to increase the # of shared memory segments available.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Rohan,</p><NewLine><p>here is the code snippet for reproducing the bug:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torchvision import models<NewLine>import time<NewLine><NewLine>def main():<NewLine>    """"""""""""<NewLine>    model_list = [<NewLine>        ['resnet152', models.resnet152],<NewLine>        ['inception_v3', models.inception_v3],<NewLine>        ['vgg16', models.vgg16],<NewLine>        ['vgg19', models.vgg19],<NewLine>        ['vgg19_bn', models.vgg19_bn],<NewLine>        ['densenet201', models.densenet201],<NewLine>        ['densenet169', models.densenet169],<NewLine>        ['resnet152-2', models.resnet152],<NewLine>        ['resnet152-3', models.resnet152],<NewLine>        ['resnet152-4', models.resnet152],<NewLine>        ['resnet152-5', models.resnet152],<NewLine>        ['resnet152-6', models.resnet152],<NewLine>        ['resnet152-7', models.resnet152],<NewLine>        ['resnet152-8', models.resnet152],<NewLine>        ['resnet152-9', models.resnet152]<NewLine>    ]<NewLine>    models_dict = {}<NewLine><NewLine>    for m in model_list:<NewLine>        model = m[1](pretrained=True)<NewLine>        model.share_memory()<NewLine>        models_dict[m[0]] = model<NewLine>        print('loaded ', m[0])<NewLine><NewLine>    # while True:<NewLine>    #     time.sleep(1)<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    main()<NewLine></code></pre><NewLine><p>Here is the outputs of <code>ipcs -lm</code>:</p><NewLine><pre><code class=""lang-auto""><NewLine>------ Shared Memory Limits --------<NewLine>max number of segments = 4096<NewLine>max seg size (kbytes) = 18014398509465599<NewLine>max total shared memory (kbytes) = 18014398509481980<NewLine>min seg size (bytes) = 1<NewLine></code></pre><NewLine><p>I have tried to increase <code>max number of segments</code> to <code>8192</code> (using <code>sysctl -w kernel.shmmni=8192</code>), it does not help.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Another interesting fact:<br/><NewLine>before the program got crash, I didn’t see any file created in <code>/dev/shm</code> folder. While the disk usage of <code>/dev/shm</code> is increasing (based on <code>df -h</code> command)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/rvarm1; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/zarzen; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/zarzen; <NewLine> ,"REPLY_DATE 1: May 12, 2020,  4:47am; <NewLine> REPLY_DATE 2: May 12, 2020,  4:43pm; <NewLine> REPLY_DATE 3: May 12, 2020,  5:39pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
80751,DDP - Batch Norm Issue,2020-05-11T20:12:59.421Z,1,173,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am having the issue that everyone else has, where a model that uses BatchNorm has poorer accuracy when using DDP:</p><NewLine><p>According to this, I am suppose to patch Batch Norm somehow:<br/><NewLine></p><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/Microsoft/human-pose-estimation.pytorch/issues/8"" rel=""nofollow noopener"" target=""_blank"">github.com/Microsoft/human-pose-estimation.pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/Microsoft/human-pose-estimation.pytorch/issues/8"" rel=""nofollow noopener"" target=""_blank"">Why do you disable cudnn for batch_norm?</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2018-08-25"" data-format=""ll"" data-time=""15:30:48"" data-timezone=""UTC"">03:30PM - 25 Aug 18 UTC</span><NewLine></div><NewLine><div class=""date""><NewLine>          closed <span class=""discourse-local-date"" data-date=""2018-11-06"" data-format=""ll"" data-time=""10:59:42"" data-timezone=""UTC"">10:59AM - 06 Nov 18 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/jin-s13"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""jin-s13"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars0.githubusercontent.com/u/11788150?v=4"" width=""20""/><NewLine>          jin-s13<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">Thank you for releasing the code. The README reads that it is required to disable cudnn for batch_norm. Would you please...</p><NewLine></div><NewLine><div class=""labels""><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><pre><code class=""lang-auto"">def monkey_patch_bn():<NewLine>    # print(inspect.getsource(torch.nn.functional.batch_norm))<NewLine>    def batch_norm(input, running_mean, running_var, weight=None, bias=None,<NewLine>                   training=False, momentum=0.1, eps=1e-5):<NewLine>        if training:<NewLine>            size = input.size()<NewLine>            size_prods = size[0]<NewLine>            for i in range(len(size) - 2):<NewLine>                size_prods *= size[i + 2]<NewLine>            if size_prods == 1:<NewLine>                raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))<NewLine><NewLine>        return torch.batch_norm(<NewLine>            input, weight, bias, running_mean, running_var,<NewLine>            training, momentum, eps, False<NewLine>        )<NewLine>    torch.nn.functional.batch_norm = batch_norm<NewLine></code></pre><NewLine><p>But I am not sure how to do it if my code is like this:</p><NewLine><pre><code class=""lang-auto"">def convbn(in_planes, out_planes, kernel_size, stride, pad, dilation, bn_running_avg=False):<NewLine>    return nn.Sequential(nn.Conv2d(in_planes, out_planes,<NewLine>                                   kernel_size=kernel_size, stride=stride,<NewLine>                                   padding=dilation if dilation &gt; 1 else pad,<NewLine>                                   dilation=dilation, bias=False),<NewLine>                         nn.BatchNorm2d(out_planes,<NewLine>                                        track_running_stats=bn_running_avg))<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/soulslicer,(Raaj),soulslicer,"May 11, 2020,  8:14pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Could you try disabling the CuDNN backend with:<br/><NewLine><code>torch.backends.cudnn.enabled = False</code>? According to posts such as <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/training-performance-degrades-with-distributeddataparallel/47152/13"">Training performance degrades with DistributedDataParallel</a>, can improve training.</p><NewLine><p>Also, have you given <code>SyncBatchNorm</code> (<a href=""https://pytorch.org/docs/stable/nn.html#syncbatchnorm"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/nn.html#syncbatchnorm</a>) a try? This will make batch statistics be computed across all GPUs in usage, instead of being computed separately for the batches passed to  each device. (Note that as per the documentation, you’ll have to change your code to spawn a single process per-GPU if you’re not training that way already)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Okay but this is only useful if i am having running_mean_stats enabled right?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/rvarm1; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/soulslicer; <NewLine> ,"REPLY_DATE 1: May 11, 2020, 10:59pm; <NewLine> REPLY_DATE 2: May 12, 2020,  4:13pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
78297,2 Docker containers slower than 1 with DistributedDataParallel,2020-04-25T00:03:31.757Z,0,208,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m struggling with Docker containerization seeming to negate the speedup of distributed training:</p><NewLine><ul><NewLine><li>GPU training with nn.parallel.DistributedDataParallel</li><NewLine><li>2 processes with 1 GPU each are about 2x faster than 1 process with 1 GPU when run directly on a Google Compute Engine n1-standard-16 instance.</li><NewLine><li><NewLine><strong>Same 2 processes each in one Docker container with one GPU on Google Kubernetes Engine are slower than 1 process with 1 GPU</strong>, whether that process is in a container or not. Both containers are again on a single n1-standard-16 machine.</li><NewLine><li>Per-process batch size always = 3. I measure speed through the time taken to accumulate gradients to an equivalent batch size of 24. Should take 4 iterations with 2 GPUs or 8 with one.</li><NewLine><li>(if it’s relevant) using AMP with opt level O1.</li><NewLine></ul><NewLine><p>Slow communication due to containerization? Failure to use GPU-GPU communication?</p><NewLine><p>Containers:</p><NewLine><ul><NewLine><li>Image based on pytorch/pytorch:1.3-cuda10.1-cudnn7-devel</li><NewLine><li>Request 24 GB memory</li><NewLine><li>Seem to use more CPU than raw processes</li><NewLine><li>Use host network and IPC namespace for the init_process_group TCP initialization to work (is this the best way?)</li><NewLine></ul><NewLine><p>I tried:</p><NewLine><ul><NewLine><li>NCCL</li><NewLine><li>Gloo (bit slower than NCCL)</li><NewLine><li>Putting containers on 2 separate machines (quite a bit slower)</li><NewLine></ul><NewLine><p>NCCL initialization logs:</p><NewLine><pre><code class=""lang-auto"">NCCL INFO Bootstrap : Using [0]eth0:10.128.0.72&lt;0&gt; [1]cbr0:10.44.78.1&lt;0&gt; [2]vetha22648b8:fe80::286b:3cff:fef3:6eea%vetha22648b8&lt;0&gt;<NewLine>NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]<NewLine>NCCL INFO NET/Socket : Using [0]eth0:10.128.0.72&lt;0&gt; [1]cbr0:10.44.78.1&lt;0&gt; [2]vetha22648b8:fe80::286b:3cff:fef3:6eea%vetha22648b8&lt;0&gt;<NewLine>NCCL version 2.4.8+cuda10.1<NewLine>NCCL INFO Bootstrap : Using [0]eth0:10.128.0.72&lt;0&gt; [1]cbr0:10.44.78.1&lt;0&gt; [2]vetha22648b8:fe80::286b:3cff:fef3:6eea%vetha22648b8&lt;0&gt;<NewLine>NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).<NewLine>misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]<NewLine>NCCL INFO NET/Socket : Using [0]eth0:10.128.0.72&lt;0&gt; [1]cbr0:10.44.78.1&lt;0&gt; [2]vetha22648b8:fe80::286b:3cff:fef3:6eea%vetha22648b8&lt;0&gt;<NewLine>NCCL INFO Setting affinity for GPU 0 to ffff<NewLine>NCCL INFO Setting affinity for GPU 0 to ffff<NewLine>NCCL INFO Could not find real path of /sys/class/net/cbr0/device<NewLine>NCCL INFO include/net.h:19 -&gt; 2<NewLine>NCCL INFO Could not find real path of /sys/class/net/vetha22648b8/device<NewLine>NCCL INFO include/net.h:19 -&gt; 2<NewLine>NCCL INFO CUDA Dev 0[0], Socket NIC distance :  PHB SYS SYS<NewLine>NCCL INFO Could not find real path of /sys/class/net/cbr0/device<NewLine>NCCL INFO include/net.h:19 -&gt; 2<NewLine>NCCL INFO Could not find real path of /sys/class/net/vetha22648b8/device<NewLine>NCCL INFO include/net.h:19 -&gt; 2<NewLine>NCCL INFO CUDA Dev 0[1], Socket NIC distance :  PHB SYS SYS<NewLine>NCCL INFO Channel 00 :    0   1<NewLine>NCCL INFO Ring 00 : 0 -&gt; 1 [receive] via NET/Socket/0<NewLine>NCCL INFO NET/Socket: Using 1 threads and 1 sockets per thread<NewLine>NCCL INFO Ring 00 : 1 -&gt; 0 [send] via NET/Socket/0<NewLine>NCCL INFO Ring 00 : 1 -&gt; 0 [receive] via NET/Socket/0<NewLine>NCCL INFO NET/Socket: Using 1 threads and 1 sockets per thread<NewLine>NCCL INFO Ring 00 : 0 -&gt; 1 [send] via NET/Socket/0<NewLine>NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled<NewLine>NCCL INFO comm 0x7f478c0019e0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 - Init COMPLETE<NewLine>NCCL INFO Launch mode Parallel<NewLine>NCCL INFO comm 0x7f82300019e0 rank 1 nranks 2 cudaDev 0 nvmlDev 1 - Init COMPLETE<NewLine></code></pre><NewLine><p>Any help would be greatly appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/AdamKing,(Adam King),AdamKing,"April 25, 2020, 12:03am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, since you mentioned that 2 processes with 1 GPU each attains the expected speedup in the Google cloud env, it leads me to think that there may be a difference in configuration between that and the Docker env. Could you try the following and see if it works for you?</p><NewLine><p>Setting <code>export OMP_NUM_THREADS=1</code> as explained in <a href=""https://github.com/pytorch/pytorch/issues/22451"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/22451</a></p><NewLine><p>For your hypothesis about slower GPU to GPU communication, it may be worthwhile to debug which portions of the training are slower on the Docker instances. You can add instrumentation to determine which parts of training (initialization, data loading, forward, backward, etc) are slower than on Compute Engine. One way to do this would be to use the <code>torch.cuda.Event.elapsed_time()</code> API (<a href=""https://pytorch.org/docs/stable/_modules/torch/cuda/streams.html#Event.elapsed_time"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/_modules/torch/cuda/streams.html#Event.elapsed_time</a>) to record GPU computations (an example is available here: <a href=""https://pytorch.org/docs/stable/notes/cuda.html#cuda-semantics"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/notes/cuda.html#cuda-semantics</a>)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/rvarm1; <NewLine> ,"REPLY_DATE 1: May 12, 2020,  5:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
80625,torch.cuda.streams.Stream in multiprocess context causes error: can&rsquo;t pickle Stream objects,2020-05-11T03:26:24.968Z,0,120,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,I added data prefetching by using cuda stream like this:</p><NewLine><pre><code class=""lang-auto"">class data_prefetcher():<NewLine>    def __init__(self, loader):<NewLine>        self.loader = iter(loader)<NewLine>        self.stream = torch.cuda.Stream()<NewLine>        self.mean = torch.tensor([0.485 * 255, 0.456 * 255, 0.406 * 255]).cuda().view(1,3,1,1)<NewLine>        self.std = torch.tensor([0.229 * 255, 0.224 * 255, 0.225 * 255]).cuda().view(1,3,1,1)<NewLine>        self.preload()<NewLine><NewLine>    def preload(self):<NewLine>        try:<NewLine>            self.next_input, self.next_target = next(self.loader)<NewLine>        except StopIteration:<NewLine>            self.next_input = None<NewLine>            self.next_target = None<NewLine>            return<NewLine><NewLine>        with torch.cuda.stream(self.stream):<NewLine>            self.next_input = self.next_input.cuda(non_blocking=True)<NewLine>            self.next_target = self.next_target.cuda(non_blocking=True)<NewLine>          <NewLine>            self.next_input = self.next_input.float()<NewLine>            self.next_input = self.next_input.sub_(self.mean).div_(self.std)<NewLine><NewLine>    def next(self):<NewLine>        torch.cuda.current_stream().wait_stream(self.stream)<NewLine>        input = self.next_input<NewLine>        target = self.next_target<NewLine>        if input is not None:<NewLine>            input.record_stream(torch.cuda.current_stream())<NewLine>        if target is not None:<NewLine>            target.record_stream(torch.cuda.current_stream())<NewLine>        self.preload()<NewLine>        return input, target<NewLine></code></pre><NewLine><p>(come from <a href=""https://github.com/NVIDIA/apex/blob/master/examples/imagenet/main_amp.py#L256"" rel=""nofollow noopener"">https://github.com/NVIDIA/apex/blob/master/examples/imagenet/main_amp.py#L256</a>)</p><NewLine><p>to training logic,it works fine in single GPU training,but when moving to multiprocess context,I got error like this:</p><NewLine><pre><code class=""lang-auto"">  File ""/usr/lib/python3.6/multiprocessing/process.py"", line 105, in start<NewLine>    self._popen = self._Popen(self)<NewLine>  File ""/usr/lib/python3.6/multiprocessing/context.py"", line 223, in _Popen<NewLine>    return _default_context.get_context().Process._Popen(process_obj)<NewLine>  File ""/usr/lib/python3.6/multiprocessing/context.py"", line 284, in _Popen<NewLine>    return Popen(process_obj)<NewLine>  File ""/usr/lib/python3.6/multiprocessing/popen_spawn_posix.py"", line 32, in __init__<NewLine>    super().__init__(process_obj)<NewLine>  File ""/usr/lib/python3.6/multiprocessing/popen_fork.py"", line 19, in __init__<NewLine>    self._launch(process_obj)<NewLine>  File ""/usr/lib/python3.6/multiprocessing/popen_spawn_posix.py"", line 47, in _launch<NewLine>    reduction.dump(process_obj, fp)<NewLine>  File ""/usr/lib/python3.6/multiprocessing/reduction.py"", line 60, in dump<NewLine>    ForkingPickler(file, protocol).dump(obj)<NewLine>TypeError: can't pickle Stream objects<NewLine></code></pre><NewLine><p>After debuging,I found <strong>torch.cuda.streams.Stream</strong> triggered this exception,Question are:</p><NewLine><p>1,Isn’t it possible to use cuda stream in torch.multiprocess context?<br/><NewLine>2,If not,any examples?</p><NewLine></div>",https://discuss.pytorch.org/u/Alex_Luya,(Alex Luya),Alex_Luya,"May 11, 2020,  7:34am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/alex_luya"">@Alex_Luya</a></p><NewLine><p>If all you need is syncing streams across processes, you can use the <a href=""https://pytorch.org/docs/stable/cuda.html#torch.cuda.Event.ipc_handle"" rel=""nofollow noopener"">ipc_handle()</a> API to pass CUDA events across processes. See the example in the <a href=""https://github.com/pytorch/pytorch/blob/f314d9a0774062a20015ae522d33eadd45293328/test/test_multiprocessing.py#L590-L623"" rel=""nofollow noopener"">test</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: May 11, 2020,  3:53pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
80165,Cuda out of memory error apper after loading DistributedDataParallel model:,2020-05-07T12:09:21.892Z,1,125,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I used four GPUs to train a model. My training strategy is divided into two stages. In the first stage, the model is trained normally, and then in the second stage, the model is loaded with the optimal model of the first stage. Continue Training, but at this stage it appeared Cuda out of memory error.</p><NewLine><p>This is the error:</p><NewLine><pre><code class=""lang-auto"">/root/anaconda3/envs/python367/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown<NewLine>  len(cache))<NewLine>/root/anaconda3/envs/python367/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown<NewLine>  len(cache))<NewLine>/root/anaconda3/envs/python367/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown<NewLine>  len(cache))<NewLine>Traceback (most recent call last):<NewLine>  File ""dogs_test3.py"", line 573, in &lt;module&gt;<NewLine>    my_launch(args)<NewLine>  File ""dogs_test3.py"", line 563, in my_launch<NewLine>    mp.spawn(train,nprocs=world_size,args=(args,))<NewLine>  File ""/root/anaconda3/envs/python367/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 171, in spawn<NewLine>    while not spawn_context.join():<NewLine>  File ""/root/anaconda3/envs/python367/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 118, in join<NewLine>    raise Exception(msg)<NewLine>Exception: <NewLine><NewLine>-- Process 1 terminated with the following error:<NewLine>Traceback (most recent call last):<NewLine>  File ""/root/anaconda3/envs/python367/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 19, in _wrap<NewLine>    fn(i, *args)<NewLine>  File ""/root/dogs_test/dogs_test3.py"", line 538, in train<NewLine>    global_feat, local_feat, cls_score = model(image)<NewLine>  File ""/root/anaconda3/envs/python367/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda3/envs/python367/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 447, in forward<NewLine>    output = self.module(*inputs[0], **kwargs[0])<NewLine>  File ""/root/anaconda3/envs/python367/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/dogs_test/dogs_test3.py"", line 213, in forward<NewLine>    x = self.backbone(x)<NewLine>  File ""/root/anaconda3/envs/python367/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda3/envs/python367/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 100, in forward<NewLine>    input = module(input)<NewLine>  File ""/root/anaconda3/envs/python367/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda3/envs/python367/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 100, in forward<NewLine>    input = module(input)<NewLine>  File ""/root/anaconda3/envs/python367/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda3/envs/python367/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 100, in forward<NewLine>    input = module(input)<NewLine>  File ""/root/anaconda3/envs/python367/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda3/envs/python367/lib/python3.6/site-packages/geffnet/efficientnet_builder.py"", line 237, in forward<NewLine>    x = self.conv_pwl(x)<NewLine>  File ""/root/anaconda3/envs/python367/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda3/envs/python367/lib/python3.6/site-packages/torch/nn/modules/conv.py"", line 345, in forward<NewLine>    return self.conv2d_forward(input, self.weight)<NewLine>  File ""/root/anaconda3/envs/python367/lib/python3.6/site-packages/torch/nn/modules/conv.py"", line 342, in conv2d_forward<NewLine>    self.padding, self.dilation, self.groups)<NewLine>RuntimeError: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 1; 10.76 GiB total capacity; 6.98 GiB already allocated; 129.69 MiB free; 7.17 GiB reserved in total by PyTorch)<NewLine></code></pre><NewLine><p>This is  my code:</p><NewLine><pre><code class=""lang-auto"">def my_launch(args):<NewLine>    world_size=args['num_machines']*args['num_gpus_per_machine']<NewLine>    args['world_size']=world_size<NewLine>    os.environ['MASTER_ADDR']='127.0.0.1'<NewLine>    os.environ['MASTER_PORT']='27925'<NewLine>    mp.spawn(train,nprocs=world_size,args=(args,))<NewLine></code></pre><NewLine><p>I commented out the code of step1 and loaded the checkpoint directly</p><NewLine><pre><code class=""lang-auto"">def train(gpu,args):<NewLine>    rank=gpu<NewLine>    dist.init_process_group(<NewLine>        backend='nccl',<NewLine>        init_method='env://',<NewLine>        world_size=args['world_size'],<NewLine>        rank=rank<NewLine>    )<NewLine>    torch.manual_seed(0)<NewLine>    torch.cuda.set_device(gpu)<NewLine><NewLine>    train_info, valid_info = stratification_kfold(names, image_label, 5)<NewLine>    train_names, valid_names = train_info[0], valid_info[0]<NewLine>    train_ds = TrainDataset(train_names, image_label, label_map_image, transform_train)<NewLine>    valid_ds = TestDataset(valid_names, image_label, transform_valid)<NewLine>    valid_dl = Data.DataLoader(valid_ds, batch_size=8, drop_last=True)<NewLine><NewLine>    train_sampler=Data.distributed.DistributedSampler(train_ds,num_replicas=args['world_size'],rank=0)<NewLine>    train_dl = Data.DataLoader(train_ds, batch_size=8, collate_fn=train_collate, shuffle=False,sampler=train_sampler, drop_last=True)<NewLine><NewLine>    step1_epochs = 30<NewLine>    step2_epochs = 30<NewLine>    criterion = Criterion()<NewLine>    early_stop = EarlyStopping()<NewLine><NewLine>    model = myNet()<NewLine>    model.cuda(gpu)<NewLine>    model=nn.parallel.DistributedDataParallel(model,device_ids=[gpu])<NewLine>    dist.barrier()<NewLine>    map_loacation={'cuda:%d'%0:'cuda:%d'%gpu}<NewLine><NewLine>    #<NewLine>    # step1_optimizer = torch.optim.SGD(model.parameters(), lr=0.9, weight_decay=0.0001)<NewLine>    # for epoch in range(step1_epochs):<NewLine>    #     with tqdm(total=len(train_dl)) as pbar:<NewLine>    #         train_loss = 0<NewLine>    #         steps = len(train_dl)<NewLine>    #         for image, labels in train_dl:<NewLine>    #             model.train()<NewLine>    #             step1_optimizer.zero_grad()<NewLine>    #          <NewLine>    #             image = image.cuda(gpu).float()<NewLine>    #             labels=labels.cuda(gpu)<NewLine>    #             global_feat, local_feat, cls_score = model(image)<NewLine>    #             loss = criterion(global_feat, local_feat, cls_score, labels,gpu)<NewLine>    #             train_loss += loss<NewLine>    #             loss.backward()<NewLine>    #             step1_optimizer.step()<NewLine>    #             pbar.update(1)<NewLine>    #         print('train_loss:{}'.format(train_loss / steps))<NewLine>    #         model.eval()<NewLine>    #         metric = evaluate(model, valid_dl)<NewLine>    #         early_stop(metric, model)<NewLine>    #         if early_stop.early_stop:<NewLine>    #             break<NewLine><NewLine>    checkpoint_path = '/root/dogs/step2.pt'<NewLine>    checkpoint = torch.load(checkpoint_path,map_location=map_loacation)<NewLine>    model.load_state_dict(checkpoint['state'])<NewLine>    dist.barrier()<NewLine>    step2_optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001)<NewLine>    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(step2_optimizer, T_0=5, T_mult=2)<NewLine>    early_stop.counter = 0<NewLine>    early_stop.early_stop = False<NewLine>    early_stop.best_score = 0<NewLine>    early_stop.patience = 8<NewLine>    for epoch in range(step2_epochs):<NewLine>        with tqdm(total=len(train_dl)) as pbar:<NewLine>            train_loss = 0<NewLine>            steps = len(train_dl)<NewLine>            for image, labels in train_dl:<NewLine>                model.train()<NewLine>                step2_optimizer.zero_grad()<NewLine>           <NewLine>                image = image.cuda(gpu).float()<NewLine>                labels=labels.cuda(gpu)<NewLine>                global_feat, local_feat, cls_score = model(image)<NewLine>                loss = criterion(global_feat, local_feat, cls_score, labels,gpu)<NewLine>                train_loss += loss<NewLine>                loss.backward()<NewLine>                step2_optimizer.step()<NewLine>                pbar.update(1)<NewLine>            print('train_loss:{}'.format(train_loss / steps))<NewLine>            model.eval()<NewLine>            metric = evaluate(model, criterion)<NewLine>            scheduler.step()<NewLine>            early_stop(metric, model)<NewLine>            if early_stop.early_stop:<NewLine>                break<NewLine></code></pre><NewLine><p>I saved the checkpoint of the model in early_stop</p><NewLine><pre><code class=""lang-auto""><NewLine>class EarlyStopping:<NewLine>    """"""Early stops the training if validation loss doesn't improve after a given patience.""""""<NewLine><NewLine>    def __init__(self, patience=4, best_score=None,delta=0):<NewLine> <NewLine>        self.patience = patience<NewLine>   <NewLine>        self.counter = 0<NewLine>        self.best_score = best_score<NewLine>        self.early_stop = False<NewLine>        self.delta = delta<NewLine><NewLine>    def __call__(self, val_metric, model):<NewLine><NewLine>        score = val_metric<NewLine><NewLine>        if self.best_score is None:<NewLine>            self.best_score = score<NewLine>            self.save_checkpoint(val_metric, model)<NewLine>        elif score &lt; self.best_score + self.delta:<NewLine>            self.counter += 1<NewLine>            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')<NewLine>            if self.counter &gt;= self.patience:<NewLine>                self.early_stop = True<NewLine>        else:<NewLine>            self.best_score = score<NewLine>            self.save_checkpoint(val_metric, model)<NewLine>            self.counter = 0<NewLine><NewLine>    def save_checkpoint(self, metric, model):<NewLine>        state = {'best_metric': metric, 'state': model.state_dict()}<NewLine>        torch.save(state, '/root/dogs/step2.pt')<NewLine></code></pre><NewLine><p>Why does cuda out of memory error appear after loading checkpoint?</p><NewLine></div>",https://discuss.pytorch.org/u/111220,(beilei_villagers),111220,"May 7, 2020,  2:49pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>RuntimeError: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 1; 10.76 GiB total capacity; 6.98 GiB already allocated; 129.69 MiB free; 7.17 GiB reserved in total by PyTorch)</p><NewLine></blockquote><NewLine><p>Can you try running <code>torch.cuda.empty_cache()</code> to free up the reserved 7.17GB memory? These reserved memory might be full of small blocks that cannot accommodate the requested 126MB.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Another thing that could help is, instead of using <code>torch.cuda.set_device(gpu)</code>, you can try setting <code>CUDA_VISIBLE_DEVICES</code>, this sometimes can avoid creating unnecessary CUDA context on <code>cuda:0</code>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks，it works well</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/111220; <NewLine> ,"REPLY_DATE 1: May 7, 2020,  3:42pm; <NewLine> REPLY_DATE 2: May 7, 2020,  3:44pm; <NewLine> REPLY_DATE 3: May 8, 2020, 10:39am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
76261,PyTorch Distributed is going out of CPU RAM,2020-04-11T00:56:01.387Z,7,173,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have two separate models in my algorithm. A large model that resides on CPU and a small model that goes to GPU. I am using the DDP to train the small model on multiple GPUs while the large model remains on CPU. I have 4 GPUs and I observe that the CPU model also loads 4 times and cause OOM in CPU. Is there anyway to keep a sigle CPU model and multiple GPU models in DDP?</p><NewLine></div>",https://discuss.pytorch.org/u/maralm,(Maral),maralm,"April 11, 2020, 12:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you mean you want to share the same CPU model across 4 DDP processes? If so, you can use <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html#reuse-buffers-passed-through-a-queue"" rel=""nofollow noopener"">torch.multiprocessing.Queue</a> to share the model. Does the CPU model participate in training or just inference?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>sorry for late reply, missed your comment. Yes, I want to use DDP but have a single copy in the CPU. my model runs forward and backward on GPU and optimizer on CPU.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Yes, I want to use DDP but have a single copy in the CPU.</p><NewLine></blockquote><NewLine><p>In this case, you might need to use <a href=""https://pytorch.org/docs/stable/multiprocessing.html#sharing-cuda-tensors"" rel=""nofollow noopener"">multiprocess communication</a> to share tensors.</p><NewLine><p>Sorry that I still don’t fully understand the use case. Some pseudo code would be helpful.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you. I am trying to load the cpu model in one thread and broadcast in gpu. However, I have an issue with the barrier. Here is my code:</p><NewLine><pre><code class=""lang-auto"">if args.local_rank in [-1,0]:<NewLine>    for net1,net2 in zip(self.encoder.layer[0].named_parameters(), model_cpu.bert.encoder.layer[i].named_parameters()):<NewLine>         net1[1].data.copy_(net2[1].data.clone(), non_blocking=True)<NewLine> <NewLine>torch.distributed.barrier()<NewLine><NewLine>if args.local_rank not in [-1,0]:<NewLine>    for name, p in self.encoder.layer[0].named_parameters():<NewLine>        torch.distributed.broadcast(p, src=0)<NewLine></code></pre><NewLine><p>my code stucks at the barrier. Any idea what could be wrong with my code? Should I even use barrier?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""76261"" data-username=""maralm""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/e36b37/40.png"" width=""20""/> maralm:</div><NewLine><blockquote><NewLine><p>I am trying to load the cpu model in one thread and broadcast in gpu.</p><NewLine></blockquote><NewLine></aside><NewLine><p>The default process group is per-process object, instead of per-thread. Is this just a typo and you actually mean “process”?</p><NewLine><pre><code class=""lang-python"">if args.local_rank not in [-1,0]:<NewLine>    for name, p in self.encoder.layer[0].named_parameters():<NewLine>        torch.distributed.broadcast(p, src=0)<NewLine></code></pre><NewLine><p>For collective communications, it requires all ranks to make the same number of <code>c10d</code> API invocations in the same order. It seems that, with the above code, rank 0 is not participating in the broadcast? If you need broadcast in a subgroup, you will need to first create a subgroup using the <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.new_group"" rel=""nofollow noopener""><code>new_group</code></a> API and then call <code>boradcast</code> in that group.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, yes I mean process. I was thinking to load the model on cpu in one process, transfer it to the gpu 0 (which is in the same process) and from gpu 0, copy weights to other gpus in other processes. Broadcasting is the correct way to do so?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I was thinking to load the model on cpu in one process, transfer it to the gpu 0 (which is in the same process) and from gpu 0, copy weights to other gpus in other processes. Broadcasting is the correct way to do so?</p><NewLine></blockquote><NewLine><p>Yes, this looks correct to me. <a href=""https://github.com/pytorch/pytorch/blob/96b512be07ce9789cfe3daa0b1c97e5641861715/torch/nn/parallel/distributed.py#L280-L285"" rel=""nofollow noopener""><code>DistributedDataParallel</code></a> is actually doing the similar thing in its constructor.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you, it worked!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/maralm; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/maralm; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/maralm; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/maralm; <NewLine> ,"REPLY_DATE 1: April 11, 2020,  6:54pm; <NewLine> REPLY_DATE 2: May 3, 2020, 11:39pm; <NewLine> REPLY_DATE 3: May 4, 2020,  2:48pm; <NewLine> REPLY_DATE 4: May 5, 2020, 10:29pm; <NewLine> REPLY_DATE 5: May 6, 2020,  1:13am; <NewLine> REPLY_DATE 6: May 6, 2020,  1:27am; <NewLine> REPLY_DATE 7: May 6, 2020,  1:31am; <NewLine> REPLY_DATE 8: May 7, 2020,  7:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> 
78951,Run pytorch on jupyter notebook,2020-04-29T11:16:26.931Z,4,205,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I try to run example from tutorial with “GLoo” backend and Point to Point communication.</p><NewLine><pre><code class=""lang-auto"">""""""run.py:""""""<NewLine>#!/usr/bin/env python<NewLine>import os<NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>from torch.multiprocessing import Process<NewLine><NewLine>def run(rank, size):<NewLine>    tensor = torch.zeros(1)<NewLine>    if rank == 0:<NewLine>        tensor += 1<NewLine>        # Send the tensor to process 1<NewLine>        dist.send(tensor=tensor, dst=1)<NewLine>    else:<NewLine>        # Receive tensor from process 0<NewLine>        dist.recv(tensor=tensor, src=0)<NewLine>    print('Rank ', rank, ' has data ', tensor[0])<NewLine><NewLine>def init_process(rank, size, fn, backend='gloo'):<NewLine>    """""" Initialize the distributed environment. """"""<NewLine>    os.environ['MASTER_ADDR'] = '127.0.0.1'<NewLine>    os.environ['MASTER_PORT'] = '29500'<NewLine>    dist.init_process_group(backend, rank=rank, world_size=size)<NewLine>    fn(rank, size)<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    size = 2<NewLine>    processes = []<NewLine>    for rank in range(size):<NewLine>        p = Process(target=init_process, args=(rank, size, run))<NewLine>        p.start()<NewLine>        processes.append(p)<NewLine><NewLine>    for p in processes:<NewLine>        p.join()<NewLine>    print(""done"")<NewLine></code></pre><NewLine><p>When I run it, only “done” is printed on jupyter notebook.<br/><NewLine>How to run it with python?<br/><NewLine>Thanks,</p><NewLine></div>",https://discuss.pytorch.org/u/ph0123,(chau phuong),ph0123,"April 29, 2020, 11:16am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried this with colab, but cannot reproduce this problem. Sometimes there are weird behavior when using multiprocessing in notebook. If you directly launch this program using command line, are the outputs as expected?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes. It work with python.<br/><NewLine>But I wanna ask about run it on jupyter.<br/><NewLine>How do it work on Jupyter? This is my question.<br/><NewLine>Thanks,</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>As I cannot reproduce the error on my Jupyter notebook, I can only guess why the message from subprocess is not shown. Given that the main process prints “done”, I would assume the sub-processes are launched correctly. But since the subprocess didn’t print the message, it could be either 1) sub-process crashed 2) sub-process is not printing to stdout. For 1), you can check the <code>exitcode</code> of the subprocess, adding more logs will also help. For 2) you will need check local configures to see if it is redirected, or you explicitly redirect that print to file.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""78951"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>o file.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Hi,<br/><NewLine>Thank you so much!</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I find the solution for that.<br/><NewLine>I run jupyter on macbook, and It worked.<br/><NewLine>On Window, the program only printed “done”.</p><NewLine><p>Thanks,</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>PyTorch distributed package does not support Windows yet. So most likely the subprocess crashed as <code>init_process_group</code>  is not available on Windows.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: April 29, 2020,  2:21pm; <NewLine> REPLY_DATE 2: April 29, 2020,  6:08pm; <NewLine> REPLY_DATE 3: April 29, 2020,  6:42pm; <NewLine> REPLY_DATE 4: April 29, 2020,  6:58pm; <NewLine> REPLY_DATE 5: May 5, 2020,  9:02am; <NewLine> REPLY_DATE 6: May 5, 2020,  2:10pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
67298,Training using DDP with world_size 4 on a multi-gpu machine runs with only two GPUs being used,2020-01-21T18:28:53.179Z,0,129,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to run a test to see how the synchronization works. I assume that at the end of each batch, DDP would wait for the processes on the world_size GPUs to reach the synchronization point like backward pass to synchronize gradients. If only 2 GPUS processes started, I assume that at the end of first batch, the synchronization on the existing 2 GPUS would time out as the other two never started. What I observed is that the training continued with only 2 GPU processes. How to explain this? Is my understanding not correct?</p><NewLine></div>",https://discuss.pytorch.org/u/Daisy_Deng,(Daisy Deng),Daisy_Deng,"January 22, 2020,  6:28pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry about the delay.</p><NewLine><blockquote><NewLine><p>I assume that at the end of each batch, DDP would wait for the processes on the world_size GPUs to reach the synchronization point like backward pass to synchronize gradients. If only 2 GPUS processes started, I assume that at the end of first batch, the synchronization on the existing 2 GPUS would time out as the other two never started</p><NewLine></blockquote><NewLine><p>Yes, this is correct.</p><NewLine><blockquote><NewLine><p>What I observed is that the training continued with only 2 GPU processes. How to explain this? Is my understanding not correct?</p><NewLine></blockquote><NewLine><p>It should block on the DDP construct or the backward call. Could you please share a code snippet that reproduces the above behavior?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: May 4, 2020,  9:02pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
79453,Training in parallel,2020-05-02T19:36:37.335Z,0,70,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m training a VAE similar to the implementation in PyTorch’s Github. The main function looks like:</p><NewLine><pre><code class=""lang-auto"">if __name__ == ""__main__"":<NewLine>    for epoch in range(1, args.epochs + 1):<NewLine>        train(epoch)<NewLine></code></pre><NewLine><p>Assuming that I have another input parameter for training function, <code>pi</code>, I would like to write a code that trains multiple models with different parameter <code>pi</code>.</p><NewLine><pre><code class=""lang-auto"">if __name__ == ""__main__"":<NewLine>    for i in range(10):<NewLine>        pi = get_param(seed=i)<NewLine>        for epoch in range(1, args.epochs + 1):<NewLine>            train(epoch, pi)<NewLine></code></pre><NewLine><p>My question is how can I run this in parallel on GPU, such that each core trains a single model.</p><NewLine></div>",https://discuss.pytorch.org/u/blade,,blade,"May 2, 2020,  7:38pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If your GPU is already fully utilized, you won’t be able to train models in parallel and the calls will be added to the queue.<br/><NewLine>On the other hand, if you have multiple devices, you could run the training routines on each device and they will be run in parallel.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: May 3, 2020,  3:55am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
79180,DDP and iterable datasets,2020-04-30T21:31:34.692Z,0,133,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How are folks using iterable datasets with DDP? <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset"" rel=""nofollow noopener"">The example for splitting</a> an <code>IterableDataset</code> across workers (or DDP processes) seems a little silly – if I had random access to my dataset (<code>iter_start</code>), I wouldn’t be using an iterable dataset in the first place.</p><NewLine><p>Has anyone come across / built a better solution?</p><NewLine></div>",https://discuss.pytorch.org/u/sharvil,(Sharvil Nanavati),sharvil,"April 30, 2020,  9:31pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/vincentqb"">@vincentqb</a> for data loader question <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is a recurring requestm, e.g. <a href=""https://github.com/pytorch/pytorch/pull/32420"" rel=""nofollow noopener"">here</a> or <a href=""https://github.com/pytorch/pytorch/pull/26547"" rel=""nofollow noopener"">here</a>. Please feel free to suggest a mechanism <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vincentqb; <NewLine> ,"REPLY_DATE 1: April 30, 2020, 10:13pm; <NewLine> REPLY_DATE 2: May 2, 2020, 12:24am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
79273,Is Pytorch not using GPU for training?,2020-05-01T14:24:45.625Z,1,96,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there, I am working on a project called <a href=""https://github.com/todeschini-felipe/udacity/blob/master/deep-learning-v2-pytorch-master/project-dog-classification/dog_app.ipynb"" rel=""nofollow noopener"">dog_app.py</a>, within <code>conda</code> environment and a Windows 10 machine.</p><NewLine><p>Although I have (apparently) configured everything to use GPU, its usage barely goes above 2%. I am moving the model to <code>cuda()</code>, as well as my data. Why GPU is not being used at all? How do I debug that?</p><NewLine><pre><code class=""lang-auto"">use_cuda = torch.cuda.is_available()<NewLine>model_scratch = Net()<NewLine>if use_cuda:<NewLine>    model_scratch.cuda()<NewLine>    print(""Let's use"", torch.cuda.device_count(), ""GPU(s)!"")<NewLine><NewLine># Prints ""Let's use 1 GPU(s)!""<NewLine><NewLine>...<NewLine>def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):<NewLine>...<NewLine>        model.train()<NewLine>        for batch_idx, (data, target) in enumerate(loaders['train']):<NewLine>            if use_cuda:<NewLine>                data, target = data.cuda(), target.cuda()<NewLine></code></pre><NewLine><p>I found this test on another thread on the subject, but allocating memory on the GPU worked just fine:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>a = torch.cuda.FloatTensor(10000)<NewLine>print(""Allocated:"", round(torch.cuda.memory_allocated(0)/10243,1), ""GB"")<NewLine><NewLine>b = torch.cuda.FloatTensor(20000)<NewLine>print(""Allocated:"", round(torch.cuda.memory_allocated(0)/10243,1), ""GB"")<NewLine><NewLine>#Output:<NewLine># Allocated: 3.9 GB<NewLine># Allocated: 11.8 GB<NewLine></code></pre><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/fe0d6e42289c6828729ac3c2a38548769a3b2cc0"" href=""https://discuss.pytorch.org/uploads/default/original/3X/f/e/fe0d6e42289c6828729ac3c2a38548769a3b2cc0.png"" title=""image""><img alt=""image"" data-base62-sha1=""Afs3gj7KbeVA62xcTZ9emxveqD6"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/e/fe0d6e42289c6828729ac3c2a38548769a3b2cc0_2_10x10.png"" height=""247"" src=""https://discuss.pytorch.org/uploads/default/original/3X/f/e/fe0d6e42289c6828729ac3c2a38548769a3b2cc0.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1162×417 19.3 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/todeschini.felipe,,todeschini.felipe,"May 1, 2020,  2:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>If you’re using windows, you need to be careful as CUDA computations are not reported in the task manager. You will have a to check with nvidia-smi from a command line I think.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/alband"">@albanD</a>, thanks for your reply. It took me a while to figure out how to use the tool, but it seems I have only short bursts of usage. Is that how it is supposed to work?</p><NewLine><pre><code class=""lang-auto"">C:\Program Files\NVIDIA Corporation\NVSMI&gt;nvidia-smi --format=csv --query-gpu=utilization.gpu,fan.speed,temperature.gpu,power.draw -l 1<NewLine>utilization.gpu [%], fan.speed [%], temperature.gpu, power.draw [W]<NewLine>6 %, 0 %, 58, 50.59 W<NewLine>1 %, 0 %, 59, 135.16 W<NewLine>1 %, 0 %, 58, 50.50 W<NewLine>51 %, 0 %, 58, 50.59 W<NewLine>0 %, 0 %, 58, 144.52 W<NewLine>0 %, 0 %, 58, 50.15 W<NewLine>0 %, 0 %, 58, 50.25 W<NewLine>59 %, 0 %, 59, 50.83 W<NewLine>0 %, 0 %, 58, 136.92 W<NewLine>0 %, 0 %, 58, 50.39 W<NewLine>62 %, 0 %, 59, 50.83 W<NewLine>0 %, 0 %, 59, 50.39 W<NewLine>0 %, 0 %, 59, 50.59 W<NewLine>0 %, 0 %, 61, 62.24 W<NewLine>0 %, 0 %, 59, 50.49 W<NewLine>0 %, 0 %, 59, 50.59 W<NewLine>0 %, 0 %, 60, 50.83 W<NewLine>0 %, 0 %, 59, 50.49 W<NewLine>0 %, 0 %, 59, 50.39 W<NewLine>0 %, 0 %, 60, 50.74 W<NewLine>0 %, 0 %, 60, 50.83 W<NewLine>36 %, 0 %, 61, 51.42 W<NewLine>0 %, 0 %, 60, 50.74 W<NewLine>0 %, 0 %, 60, 50.74 W <NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It will depend a lot on your network. But if it is not too big or your dataloader is not fast enough then yes that is expected.<br/><NewLine>You can try adding workers to the dataloader to make sure this is not the bottleneck. Otherwise increasing the batch size (if you have enough memory) should increase the usage.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/todeschini.felipe; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: May 1, 2020,  2:27pm; <NewLine> REPLY_DATE 2: May 1, 2020,  2:47pm; <NewLine> REPLY_DATE 3: May 1, 2020,  3:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
79081,How to synchronize a variable across entire batch in ddp,2020-04-30T06:51:20.446Z,7,115,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using ddp to train my model now. In my model I want to calculate the standard deviation across the batch. However since I am wondering if i can calculate the standard deviation across the entire batch instead of within each device. The standard deviation will be part of my computation graph.</p><NewLine><p>I feel that this is similar to synchronize batchnorm and should be doable. How would I go about doing this? Here is an example of what I want to do</p><NewLine><pre><code class=""lang-auto"">def forward(self, input):<NewLine>        feats = conv(input)<NewLine>        batch, channel, height, width = feats.shape<NewLine>        stddev = feats.view(batch, -1, 1, channel, height, width)<NewLine>        stddev = torch.sqrt(stddev.var(0, unbiased=False) + 1e-8)<NewLine>        stddev = stddev.mean([2, 3, 4], keepdims=True).squeeze(2)<NewLine>        stddev = stddev.repeat(batch, 1, height, width)<NewLine>        feats = torch.cat([feats, stddev], 1)<NewLine>        output = conv_last(feats)<NewLine>        return output<NewLine></code></pre><NewLine><p>Basically when I compute stddev, I want it to do it over entire batch.</p><NewLine></div>",https://discuss.pytorch.org/u/hij,,hij,"April 30, 2020,  7:21am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/hij"">@hij</a></p><NewLine><p>Buffers are broadcast from rank 0 to other processes in the beginning of every forward pass. See the code below:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/1f8267931176cc9ecbf00493e5a359a57baca3df/torch/nn/parallel/distributed.py#L509-L513"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/1f8267931176cc9ecbf00493e5a359a57baca3df/torch/nn/parallel/distributed.py#L509-L513"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/1f8267931176cc9ecbf00493e5a359a57baca3df/torch/nn/parallel/distributed.py#L509-L513</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""509"" style=""counter-reset: li-counter 508 ;""><NewLine><li># Synchronize buffers across processes.</li><NewLine><li># The process with rank 0 is considered the authoritative copy.</li><NewLine><li>self._distributed_broadcast_coalesced(</li><NewLine><li>    self.modules_buffers[0],</li><NewLine><li>    self.broadcast_bucket_size)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>If this is what you need, you can register the <code>stddev</code> as a buffer. If you need sth different (e.g., square <code>stddev</code> and sum it across all processes over the wire), you can call <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_reduce"" rel=""nofollow noopener""><code>allreduce</code></a> or <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather"" rel=""nofollow noopener""><code>allgather</code></a> in the forward function to do that.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>If i understand you correctly, registering it as a buffer only allows the stddev of rank 0 to be distributed to other processes. What I want to do is allow all tensors in all processes to contribute to calculating stddev.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""79081"" data-username=""hij""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/hij/40/14113_2.png"" width=""20""/> hij:</div><NewLine><blockquote><NewLine><p>If i understand you correctly, registering it as a buffer only allows the stddev of rank 0 to be distributed to other processes.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes.</p><NewLine><blockquote><NewLine><p>What I want to do is allow all tensors in all processes to contribute to calculating stddev.</p><NewLine></blockquote><NewLine><p>You can do this by using the collective communication APIs (allreduce/allgather) in the forward pass. One caveat here is that, the collective communication API requires all processes in the same group to invoke the same API in the same order, otherwise, it might hang or the result might be wrong. If you are not sure whether the allreduce/allgather for <code>stddev</code> would interleave with other collectives, you can use the <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.new_group"" rel=""nofollow noopener""><code>new_group</code></a> API to create a dedicated process group for collecting/summing <code>stddev.</code></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have read that all_gather do not retain the gradient information. For my application, I want stddev to be part of the computation graph. How would I go about doing this? And could you also point me to an example/tutorial for these usage. I have not done any distributed training so I am not sure how to use these functions.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to understand the use case here:</p><NewLine><blockquote><NewLine><p>I have read that all_gather do not retain the gradient information. For my application, I want stddev to be part of the computation graph. How would I go about doing this?</p><NewLine></blockquote><NewLine><p>IIUC, <code>stddev</code> in this case is an intermediate output in <code>forward</code> and it’s <strong>not</strong> a model parameter? So you need its gradient during the backward pass to compute parameter gradients, but you don’t need to retain its gradient for optimizer <code>step()</code>? If above is true, why existing parameter gradient synchronization in DDP not sufficient?</p><NewLine><blockquote><NewLine><p>And could you also point me to an example/tutorial for these usage.</p><NewLine></blockquote><NewLine><p>Sure, below is the tutorial, and please search for “All-Reduce example.”</p><NewLine><p><a class=""onebox"" href=""https://pytorch.org/tutorials/intermediate/dist_tuto.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/intermediate/dist_tuto.html</a></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I might have been confused. So this code snippet is part of my model which I intend to train. However, since I have limited gpu memory, I can only train with batch size of 1. And there isn’t a point to calculate stddev over a batchsize of 1 even if I do DDP. So in this case, would the allgather allreduce work?</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""79081"" data-username=""hij""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/hij/40/14113_2.png"" width=""20""/> hij:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">def forward(self, input):<NewLine>        feats = conv(input)<NewLine>        batch, channel, height, width = feats.shape<NewLine>        stddev = feats.view(batch, -1, 1, channel, height, width)<NewLine>        stddev = torch.sqrt(stddev.var(0, unbiased=False) + 1e-8)<NewLine>        stddev = stddev.mean([2, 3, 4], keepdims=True).squeeze(2)<NewLine>        stddev = stddev.repeat(batch, 1, height, width)<NewLine>        feats = torch.cat([feats, stddev], 1)<NewLine>        output = conv_last(feats)<NewLine>        return output<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""79081"" data-username=""hij""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/hij/40/14113_2.png"" width=""20""/> hij:</div><NewLine><blockquote><NewLine><p>However, since I have limited gpu memory, I can only train with batch size of 1. And there isn’t a point to calculate stddev over a batchsize of 1 even if I do DDP. So in this case, would the allgather allreduce work?</p><NewLine></blockquote><NewLine></aside><NewLine><p>I see. I am not sure if the result would still be correct in this case even if <code>allgather</code> and <code>allreduce</code> can retain gradients. IIUC, if this is trained without DDP (assume there are large enough GPU memory), then both <code>feats</code> and <code>stddev</code> are calculated based on all inputs. When trained with DDP, <code>feats</code> are now only derived from local inputs, and you would like to have <code>stddev</code> to be based on global inputs. So, when you cat <code>feats</code> and <code>stddev</code>, the output of the forward now represents a different thing. I am not sure if the loss function can handle that. Even if it can, what does averaging gradient mean in this case?</p><NewLine><p>If above (local <code>feats</code> + global <code>stddev</code>) is the expected behavior, there might be a few ways to implement this.</p><NewLine><ol><NewLine><li><NewLine><p>Implement a custom autograd function. E.g., its forward function can use an <code>allgather</code> to collect <code>stddev</code> from all processes, and its backward function can use another <code>allgather</code> to collect gradients and then extract the part belongs to the local <code>stddev</code> and sum them up.</p><NewLine></li><NewLine><li><NewLine><p>Use <a href=""https://pytorch.org/docs/stable/rpc.html"" rel=""nofollow noopener""><code>torch.distributed.rpc</code></a>. There can be a master and a few workers, where each worker calculates the <code>feats</code> and <code>stddev</code> for its own input, and then the master gathers all <code>feats</code> and <code>stddev</code> to compute the final loss. Some more tutorials for this:<br/><NewLine>a. <a href=""https://pytorch.org/tutorials/intermediate/rpc_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/rpc_tutorial.html</a><br/><NewLine>b. <a href=""https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html</a></p><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>In this case, since there is no batch dependencies in feat, would it be different if it is local feat + global stddev vs global feat + global stddev? Since stddev is concatenated to each feat tensor separately, they should have the same effect?</p><NewLine><p>I will try your suggestions. Thank you!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hij; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/hij; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/hij; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/hij; <NewLine> ,"REPLY_DATE 1: April 30, 2020,  3:09pm; <NewLine> REPLY_DATE 2: April 30, 2020,  3:18pm; <NewLine> REPLY_DATE 3: April 30, 2020,  3:25pm; <NewLine> REPLY_DATE 4: April 30, 2020,  3:30pm; <NewLine> REPLY_DATE 5: April 30, 2020,  3:53pm; <NewLine> REPLY_DATE 6: April 30, 2020,  3:57pm; <NewLine> REPLY_DATE 7: April 30, 2020,  4:21pm; <NewLine> REPLY_DATE 8: April 30, 2020,  5:11pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
77601,DistributedDataParallel sub-linear scaling (~85-90% of linear with 2 GPUs),2020-04-20T19:04:57.922Z,3,280,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am having issues getting DistributedDataParallel to perform well (2 GPUs on the same host perform at ~85-90% of linear scaling, and it gets worse as GPUs or hosts are added). From slack, it seems other users are able to get much closer to 99% of linear with small numbers of nodes/GPUs.</p><NewLine><p>I’m seeing this 85-90% scaling behavior on the (shared) work cluster, and on a 2 GPU system I have at home. I haven’t tested the full cross product, but I’ve seen the same behavior on Ubuntu 14.04 and 18.04; CUDA 9.1, 10.0, and 10.2; stock PyTorch 1.4 DDP and NVIDIA Apex DDP; resnet 50, 152, and some toy models. All used fake data from torchvision with batch sizes that use up the majority of GPU RAM.</p><NewLine><p>The training script is here (with light edits to remove comments, etc.): <a href=""https://gist.github.com/elistevens/7edacdafdb45747a22da2ef0c6ce1af3"" rel=""nofollow noopener"">https://gist.github.com/elistevens/7edacdafdb45747a22da2ef0c6ce1af3</a><br/><NewLine><code>OMP_NUM_THREADS=4 EPOCHS=2 EPOCH_SIZE=3840 BATCH_SIZE=64 NODES=1 GPUS=2 ~/v/bin/python min_ddp.py</code> etc.</p><NewLine><p>The numbers here are from my 18.04 home system with 2x 1080 Tis. There’s roughly a three-second slowdown for the 2 GPU case, resulting in training going from 22 seconds (1 GPU, 1 epoch) to 25 seconds (2 GPUs, 2 epochs). About a second and a half of that is the <code>{method 'acquire' of '_thread.lock' objects}</code> and the rest seems to be <code>mul_</code>, <code>add_</code> etc. methods of <code>torch._C._TensorBase</code> objects.</p><NewLine><p>Is this expected? Am I missing something that would cause performance to be poor like this?</p><NewLine><p>Thanks for any help. More detailed data is below.</p><NewLine><pre><code class=""lang-auto"">1 GPU<NewLine>         308413 function calls (297131 primitive calls) in 22.053 seconds<NewLine><NewLine>   Ordered by: internal time<NewLine><NewLine>   ncalls  tottime  percall  cumtime  percall filename:lineno(function)<NewLine>       60    5.305    0.088    5.305    0.088 {method 'run_backward' of 'torch._C._EngineBase' objects}<NewLine>    19320    3.509    0.000    3.509    0.000 {method 'mul_' of 'torch._C._TensorBase' objects}<NewLine>    19320    3.372    0.000    3.372    0.000 {method 'add_' of 'torch._C._TensorBase' objects}<NewLine>     9660    2.298    0.000    2.298    0.000 {method 'addcdiv_' of 'torch._C._TensorBase' objects}<NewLine>     9660    2.124    0.000    2.124    0.000 {method 'sqrt' of 'torch._C._TensorBase' objects}<NewLine>       60    1.741    0.029   14.598    0.243 /home/elis/v/lib/python3.6/site-packages/torch/optim/adam.py:49(step)<NewLine>     9660    1.499    0.000    1.499    0.000 {method 'addcmul_' of 'torch._C._TensorBase' objects}<NewLine>      224    0.671    0.003    0.671    0.003 {method 'acquire' of '_thread.lock' objects}<NewLine>      120    0.548    0.005    0.548    0.005 {method 'to' of 'torch._C._TensorBase' objects}<NewLine>     3180    0.141    0.000    0.141    0.000 {built-in method conv2d}<NewLine>...<NewLine><NewLine>2 GPUs<NewLine>         312342 function calls (301058 primitive calls) in 25.171 seconds<NewLine><NewLine>   Ordered by: internal time<NewLine><NewLine>   ncalls  tottime  percall  cumtime  percall filename:lineno(function)<NewLine>       60    5.355    0.089    5.355    0.089 {method 'run_backward' of 'torch._C._EngineBase' objects}<NewLine>    19320    4.015    0.000    4.015    0.000 {method 'mul_' of 'torch._C._TensorBase' objects}<NewLine>    19320    3.668    0.000    3.668    0.000 {method 'add_' of 'torch._C._TensorBase' objects}<NewLine>     9660    2.407    0.000    2.407    0.000 {method 'sqrt' of 'torch._C._TensorBase' objects}<NewLine>     9660    2.339    0.000    2.339    0.000 {method 'addcdiv_' of 'torch._C._TensorBase' objects}<NewLine>      264    2.089    0.008    2.089    0.008 {method 'acquire' of '_thread.lock' objects}<NewLine>       60    1.800    0.030   15.833    0.264 /home/elis/v/lib/python3.6/site-packages/torch/optim/adam.py:49(step)<NewLine>     9660    1.566    0.000    1.566    0.000 {method 'addcmul_' of 'torch._C._TensorBase' objects}<NewLine>      120    0.561    0.005    0.561    0.005 {method 'to' of 'torch._C._TensorBase' objects}<NewLine>      105    0.275    0.003    0.275    0.003 {built-in method posix.waitpid}<NewLine>     3180    0.252    0.000    0.252    0.000 {built-in method conv2d}<NewLine>...<NewLine><NewLine>g2-g1    function<NewLine>delta  <NewLine>1.418,   {method 'acquire' of '_thread.lock' objects}<NewLine>0.506,   {method 'mul_' of 'torch._C._TensorBase' objects}<NewLine>0.296,   {method 'add_' of 'torch._C._TensorBase' objects}<NewLine>0.283,   {method 'sqrt' of 'torch._C._TensorBase' objects}<NewLine>0.184,   {built-in method posix.waitpid}<NewLine>0.111,    {built-in method conv2d}<NewLine>0.067,   {method 'addcmul_' of 'torch._C._TensorBase' objects}<NewLine>0.059,   /home/elis/v/lib/python3.6/site-packages/torch/optim/adam.py:49(step)<NewLine>0.05,    {method 'run_backward' of 'torch._C._EngineBase' objects}<NewLine>0.049,   {built-in method _posixsubprocess.fork_exec}<NewLine>0.041,   {method 'addcdiv_' of 'torch._C._TensorBase' objects}<NewLine>0.037,   {built-in method relu_}<NewLine>0.023,   {built-in method batch_norm}<NewLine>0.015,   {built-in method max_pool2d}<NewLine>0.013,   {method 'to' of 'torch._C._TensorBase' objects}<NewLine>0.008,   {built-in method torch.distributed._broadcast_coalesced}<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/elistevens,(Eli Stevens),elistevens,"April 20, 2020,  7:05pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/elistevens"">@elistevens</a></p><NewLine><p>Looks like you already using DDP with one device per processes, which is the recommended setup.</p><NewLine><p>Can you try different <code>OMP_NUM_THREADS</code> configurations?  Does it speed up or slow down if you set <code>OMP_NUM_THREADS</code> to 1? Sometimes <code>DataLoader</code> can also cause slowdowns. If does it affect the performance if you get rid of the <code>DataLoader</code> by using synthetic generated input/output (just for testing purpose)?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, I’m using one process per GPU.</p><NewLine><p><code>OMP_NUM_THREADS</code> at 4 doesn’t have much of a difference from 1; leaving it unset has a very slight performance regression.</p><NewLine><p>I am already using synthetic data using the <code>torchvision.datasets.FakeData</code>; each data loader process takes up about 15% CPU with 4 procs, and 60% CPU with one process. The overall scaling jumps to 92% of linear using 1 worker process, but drops to ~65% with <code>num_workers</code> set to zero (so all of the data stuff happens in the main process).</p><NewLine><p>If I get rid of the <code>DataLoader</code> entirely, and just do:</p><NewLine><pre><code class=""lang-auto"">x = torch.rand((batch_size, 3, 224, 224), device='cuda:' + str(gpu_ndx))<NewLine>y = torch.randint(0, 100, size=(batch_size,), dtype=torch.long, device='cuda:' + str(gpu_ndx))<NewLine></code></pre><NewLine><p>Inside the training loop, then I see a 6% performance improvement in the single-GPU case, and the two-GPU case jumps to 94% of linear (based off of the improved single GPU perf). The primary causes of slowdown are now basic math methods of <code>torch._C._TensorBase</code>:</p><NewLine><pre><code class=""lang-auto"">0.498, {method 'mul_' of 'torch._C._TensorBase' objects}<NewLine>0.254, {method 'add_' of 'torch._C._TensorBase' objects}<NewLine>0.176, {method 'sqrt' of 'torch._C._TensorBase' objects}<NewLine>0.153, {method 'addcdiv_' of 'torch._C._TensorBase' objects}<NewLine>0.07, {method 'run_backward' of 'torch._C._EngineBase' objects}<NewLine>0.064, /home/elis/v/lib/python3.6/site-packages/torch/optim/adam.py:49(step)<NewLine>0.03, {built-in method conv2d}<NewLine>0.022, {method 'addcmul_' of 'torch._C._TensorBase' objects}<NewLine>0.012, {built-in method batch_norm}<NewLine>0.011, {built-in method zeros_like}<NewLine></code></pre><NewLine><p>The 94% of linear scaling remains the case even if I move the creation of x and y outside the training loop. Switching from <code>Adam</code> to <code>SGD</code> speeds things up marginally, but doesn’t change the ratio.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried to remove all the dataloader overhead and profiling overhead and see about 98% scaling:</p><NewLine><pre><code class=""lang-auto"">$ OMP_NUM_THREADS=1 EPOCHS=1 EPOCH_SIZE=3840 BATCH_SIZE=64 NODES=1 GPUS=1 python /tmp/min_ddp.py<NewLine>2020-04-21 14:51:31.024092 torch.cuda.set_device(0); torch.distributed.init_process_group('nccl', rank=0, world_size=1)<NewLine>2020-04-21 14:51:33.167023 Epoch 1, dl: 60<NewLine>2020-04-21 14:52:08.255089 training loop time: 35.08807826042175 seconds<NewLine>$ OMP_NUM_THREADS=1 EPOCHS=2 EPOCH_SIZE=3840 BATCH_SIZE=64 NODES=1 GPUS=2 python /tmp/min_ddp.py<NewLine>2020-04-21 14:52:15.271820 torch.cuda.set_device(1); torch.distributed.init_process_group('nccl', rank=1, world_size=2)<NewLine>2020-04-21 14:52:15.278892 torch.cuda.set_device(0); torch.distributed.init_process_group('nccl', rank=0, world_size=2)<NewLine>2020-04-21 14:52:18.939304 Epoch 1, dl: 30<NewLine>2020-04-21 14:52:18.939501 Epoch 1, dl: 30<NewLine>2020-04-21 14:52:37.220102 Epoch 2, dl: 30<NewLine>2020-04-21 14:52:37.220168 Epoch 2, dl: 30<NewLine>2020-04-21 14:52:54.701512 training loop time: 35.76222109794617 seconds<NewLine></code></pre><NewLine><p>Code changes:</p><NewLine><pre><code class=""lang-auto"">import datetime<NewLine>import math<NewLine>import os<NewLine>import time<NewLine><NewLine>import torch<NewLine>import torch.distributed<NewLine>import torch.multiprocessing<NewLine>from torch import nn<NewLine>from torch.nn import functional as F<NewLine>from torch.utils.data import DataLoader<NewLine>from torch.nn.parallel import DataParallel<NewLine><NewLine>from torch.nn.parallel import DistributedDataParallel<NewLine>#from apex.parallel import DistributedDataParallel<NewLine><NewLine>import torchvision<NewLine><NewLine><NewLine>num_nodes = int(os.environ['NODES'])<NewLine>num_gpus = int(os.environ['GPUS'])<NewLine><NewLine><NewLine>def main(ddp_wrapper=None, sampler_cls=None, gpu_ndx=0):<NewLine>    epoch_size = int(os.environ['EPOCH_SIZE'])<NewLine>    ds = torchvision.datasets.FakeData(<NewLine>        epoch_size,<NewLine>        num_classes=100,<NewLine>        transform=torchvision.transforms.ToTensor(),<NewLine>    )<NewLine><NewLine>    dl = DataLoader(<NewLine>        ds,<NewLine>        batch_size=int(os.environ['BATCH_SIZE']),<NewLine>        num_workers=4,<NewLine>        pin_memory=True,<NewLine>        sampler=sampler_cls(ds) if sampler_cls else None,<NewLine>    )<NewLine><NewLine>    model = torchvision.models.resnet50()<NewLine>    model = model.to('cuda')<NewLine><NewLine>    if ddp_wrapper:<NewLine>        model = ddp_wrapper(model)<NewLine><NewLine>    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)<NewLine><NewLine>    '''<NewLine>    import cProfile, pstats, io<NewLine>    pr = cProfile.Profile()<NewLine>    pr.enable()<NewLine>    '''<NewLine><NewLine>    batch_size = int(os.environ['BATCH_SIZE'])<NewLine>    x = torch.rand((batch_size, 3, 224, 224), device='cuda:' + str(gpu_ndx))<NewLine>    y = torch.randint(0, 100, size=(batch_size,), dtype=torch.long, device='cuda:' + str(gpu_ndx))<NewLine>    start_ts = time.time()<NewLine>    for epoch_ndx in range(1, int(os.environ['EPOCHS']) + 1):<NewLine>        iters = int(epoch_size/batch_size/num_gpus)<NewLine>        print(datetime.datetime.now(), f""Epoch {epoch_ndx}, dl: {iters}"")<NewLine>        for i in range(iters):<NewLine>            optimizer.zero_grad()<NewLine><NewLine>            #x, y = batch_tup<NewLine>            x = x.to('cuda')<NewLine>            y = y.to('cuda')<NewLine><NewLine>            y_hat = model(x)<NewLine>            loss_var = F.cross_entropy(y_hat, y)<NewLine><NewLine>            loss_var.backward()<NewLine>            optimizer.step()<NewLine>    end_ts = time.time()<NewLine><NewLine>    #pr.disable()<NewLine><NewLine>    if gpu_ndx == 0:<NewLine>        '''<NewLine>        pr.dump_stats('/tmp/min_profile.out')<NewLine>#        pstats.Stats(pr).sort_stats('cumulative').print_stats()<NewLine>        pstats.Stats(pr).sort_stats('tot').print_stats()<NewLine>        '''<NewLine><NewLine>        print(datetime.datetime.now(), f""training loop time: {end_ts - start_ts} seconds"")<NewLine>        '''<NewLine>        print('\n'.join(<NewLine>            ['min ddp', 'cluster']<NewLine>            + [os.environ[x] for x in ['NODES', 'GPUS', 'BATCH_SIZE', 'EPOCH_SIZE', 'EPOCHS', 'OMP_NUM_THREADS']]<NewLine>            + [f'{end_ts - start_ts}']<NewLine>            + [f""{int(os.environ['EPOCH_SIZE']) * int(os.environ['EPOCHS']) / (end_ts - start_ts) / int(os.environ['GPUS'])}""]<NewLine>            + [f""{int(os.environ['EPOCH_SIZE']) * int(os.environ['EPOCHS']) / (end_ts - start_ts) / int(os.environ['GPUS']) / 1.737005}""]<NewLine>        ))<NewLine>        '''<NewLine><NewLine><NewLine>def ddp_spawn(gpu_ndx):<NewLine>    node_rank = 0<NewLine>    rank = num_gpus * node_rank + gpu_ndx<NewLine>    world_size = num_nodes * num_gpus<NewLine><NewLine>    print(datetime.datetime.now(), f""torch.cuda.set_device({gpu_ndx}); torch.distributed.init_process_group('nccl', rank={rank}, world_size={world_size})"")<NewLine><NewLine>    torch.cuda.set_device(gpu_ndx)<NewLine>    torch.distributed.init_process_group('nccl', rank=rank, world_size=world_size)<NewLine><NewLine>    main(<NewLine>        ddp_wrapper=lambda m: DistributedDataParallel(m, [gpu_ndx]),<NewLine>        sampler_cls=torch.utils.data.distributed.DistributedSampler,<NewLine>        gpu_ndx=gpu_ndx,<NewLine>    )<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    os.environ['MASTER_ADDR'] = 'localhost'<NewLine>    os.environ['MASTER_PORT'] = '1234'<NewLine><NewLine>    torch.multiprocessing.spawn(ddp_spawn, nprocs=num_gpus, args=())<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s odd; I see about 95% using the code you posted.</p><NewLine><pre><code class=""lang-auto"">$ OMP_NUM_THREADS=1 NUM_WORKERS=1 EPOCHS=2 EPOCH_SIZE=3840 BATCH_SIZE=64 NODES=1 GPUS=2 ~/v/bin/python forum_min_ddp.py<NewLine>2020-04-21 15:18:35.059667 torch.cuda.set_device(1); torch.distributed.init_process_group('nccl', rank=1, world_size=2)<NewLine>2020-04-21 15:18:35.059667 torch.cuda.set_device(0); torch.distributed.init_process_group('nccl', rank=0, world_size=2)<NewLine>2020-04-21 15:18:36.918569 Epoch 1, dl: 30<NewLine>2020-04-21 15:18:36.918801 Epoch 1, dl: 30<NewLine>2020-04-21 15:18:48.006334 Epoch 2, dl: 30<NewLine>2020-04-21 15:18:48.007357 Epoch 2, dl: 30<NewLine>2020-04-21 15:18:59.082558 training loop time: 22.16376233100891 seconds<NewLine><NewLine>$ OMP_NUM_THREADS=1 NUM_WORKERS=1 EPOCHS=1 EPOCH_SIZE=3840 BATCH_SIZE=64 NODES=1 GPUS=1 ~/v/bin/python forum_min_ddp.py<NewLine>2020-04-21 15:19:09.327511 torch.cuda.set_device(0); torch.distributed.init_process_group('nccl', rank=0, world_size=1)<NewLine>2020-04-21 15:19:11.018629 Epoch 1, dl: 60<NewLine>2020-04-21 15:19:31.962914 training loop time: 20.944292783737183 seconds<NewLine></code></pre><NewLine><p>Could you post your output from <code>nvidia-smi -q</code>?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""77601"" data-username=""elistevens""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/elistevens/40/6555_2.png"" width=""20""/> elistevens:</div><NewLine><blockquote><NewLine><p>That’s odd; I see about 95% using the code you posted.</p><NewLine><pre><code class=""lang-auto""><NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>I should’ve mentioned I was running on master and not 1.4, although I’m not sure if that matters.</p><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""77601"" data-username=""elistevens""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/elistevens/40/6555_2.png"" width=""20""/> elistevens:</div><NewLine><blockquote><NewLine><p>Could you post your output from <code>nvidia-smi -q</code> ?</p><NewLine></blockquote><NewLine></aside><NewLine><p><code>nvidia-smi -q</code> seems to include some sensitive information like serial numbers and UUID, was there something specific you’d like to know about my setup? Happy to share that information.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ahh sorry, I didn’t realize there was potentially sensitive info in there.  I was mostly going to visually diff it with what I have here and see if anything jumped out at me.</p><NewLine><p>For example, here is what I see when I’m actually running training: <a href=""https://gist.github.com/elistevens/dbe5564873a1f55c4ac98594cfd31c63"" rel=""nofollow noopener"">https://gist.github.com/elistevens/dbe5564873a1f55c4ac98594cfd31c63</a></p><NewLine><p>This is my home system; it’s two 1080 Tis running on PCIe 3.0 8x slots (it’s an older consumer motherboard).</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Eli,<br/><NewLine>TL;DR: I suspect the main reason for the disparity you’re observing is that each epoch, the dataloader processes are shutdown and recreated, and that is not free.</p><NewLine><p>I was able to reproduce your issue with your training script, although I had about 95% scaling from the start on my machine (Ubuntu 18.04.2, 28-core Intel Core i9-9940X CPU @ 3.30GHz, 2x Quadro RTX 5000, PyTorch 1.3.1, CUDA 10.0, NCCL 2.4.8) with the parameters</p><NewLine><pre><code class=""lang-auto"">OMP_NUM_THREADS=4 EPOCHS=2 EPOCH_SIZE=3840 BATCH_SIZE=64 NODES=1 GPUS=2<NewLine></code></pre><NewLine><p>vs.</p><NewLine><pre><code class=""lang-auto"">OMP_NUM_THREADS=4 EPOCHS=1 EPOCH_SIZE=3840 BATCH_SIZE=64 NODES=1 GPUS=1<NewLine></code></pre><NewLine><p>Looking at the GPU utilization with <code>nvtop</code>, I noticed a dip in GPU usage between the epochs with <code>GPUS=2</code>. I knew that dataloader processes are destroyed and then recreated from scratch every epoch, so <code>GPUS=1 EPOCHS=1</code> version would only do it once, while <code>GPUS=2 EPOCHS=2</code> would have to do it twice. So I decided to remove this unfairness: I made the script to always do just 1 epoch, and instead scale the dataset size like:</p><NewLine><pre><code class=""lang-auto"">ds = torchvision.datasets.FakeData(<NewLine>    int(os.environ['EPOCH_SIZE']) * int(os.environ['EPOCHS']),<NewLine></code></pre><NewLine><p>This gave me 99% scaling, in fact even more when I set <code>EPOCH_SIZE=38400</code> (10x). And, the invocations counts became equal between <code>GPUS=1</code> and <code>2</code> for the top 20 functions from your pstats output. (Except the <code>{method 'acquire' of '_thread.lock' objects}</code> which has 4 extra invocations in <code>GPUS=2</code> case. That one is coming from <code>pin_memory</code> thread, if you set <code>pin_memory=False</code>, all those <code>acquire</code>s go away, but training, as expected, gets slower in both cases, although <code>GPUS=1</code> case suffers more than <code>GPUS=2</code>).</p><NewLine><p>BTW there is a <a href=""https://github.com/pytorch/pytorch/issues/15849#issuecomment-573921048"" rel=""nofollow noopener"">way to not recreate dataloader processes</a> each epoch and just loop over and over.</p><NewLine><p>Hope this helps and you can replicate!</p><NewLine><details><NewLine><summary><NewLine>my pstats for GPUS=1</summary><NewLine><pre><code class=""lang-auto"">   ncalls  tottime  percall  cumtime  percall filename:lineno(function)<NewLine>      600   50.489    0.084   50.489    0.084 {method 'run_backward' of 'torch._C._EngineBase' objects}<NewLine>   193200   37.203    0.000   37.203    0.000 {method 'mul_' of 'torch._C._TensorBase' objects}<NewLine>   193200   32.657    0.000   32.657    0.000 {method 'add_' of 'torch._C._TensorBase' objects}<NewLine>    96600   20.367    0.000   20.367    0.000 {method 'sqrt' of 'torch._C._TensorBase' objects}<NewLine>    96600   19.994    0.000   19.994    0.000 {method 'addcdiv_' of 'torch._C._TensorBase' objects}<NewLine>      600   15.513    0.026  140.393    0.234 /opt/conda/lib/python3.6/site-packages/torch/optim/adam.py:49(step)<NewLine>    96600   14.561    0.000   14.561    0.000 {method 'addcmul_' of 'torch._C._TensorBase' objects}<NewLine>     1200    3.360    0.003    3.360    0.003 {method 'to' of 'torch._C._TensorBase' objects}<NewLine>    31800    1.367    0.000    1.367    0.000 {built-in method conv2d}<NewLine>      600    1.046    0.002    1.046    0.002 {built-in method torch.distributed._broadcast_coalesced}<NewLine>    31800    0.919    0.000    0.919    0.000 {built-in method batch_norm}<NewLine>    31800    0.704    0.000    1.962    0.000 /opt/conda/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py:58(forward)<NewLine>     1840    0.496    0.000    0.496    0.000 {method 'acquire' of '_thread.lock' objects}<NewLine>    96439    0.359    0.000    0.359    0.000 {method 'zero_' of 'torch._C._TensorBase' objects}<NewLine>    29400    0.307    0.000    0.307    0.000 {built-in method relu_}<NewLine>110400/600    0.254    0.000    5.682    0.009 /opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:531(__call__)<NewLine>     9600    0.223    0.000    4.182    0.000 /opt/conda/lib/python3.6/site-packages/torchvision/models/resnet.py:95(forward)<NewLine>        5    0.219    0.044    0.219    0.044 {built-in method _posixsubprocess.fork_exec}<NewLine>   353400    0.175    0.000    0.175    0.000 /opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:571(__getattr__)<NewLine>       53    0.116    0.002    0.116    0.002 {built-in method posix.waitpid}<NewLine>    31800    0.096    0.000    1.057    0.000 /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1643(batch_norm)<NewLine>      600    0.087    0.000    0.485    0.001 /opt/conda/lib/python3.6/site-packages/torch/optim/optimizer.py:159(zero_grad)<NewLine></code></pre><NewLine></details><NewLine><details><NewLine><summary><NewLine>my pstats for GPUS=2</summary><NewLine><pre><code class=""lang-auto"">   ncalls  tottime  percall  cumtime  percall filename:lineno(function)<NewLine>      600   50.391    0.084   50.391    0.084 {method 'run_backward' of 'torch._C._EngineBase' objects}<NewLine>   193200   37.327    0.000   37.327    0.000 {method 'mul_' of 'torch._C._TensorBase' objects}<NewLine>   193200   32.815    0.000   32.815    0.000 {method 'add_' of 'torch._C._TensorBase' objects}<NewLine>    96600   20.583    0.000   20.583    0.000 {method 'sqrt' of 'torch._C._TensorBase' objects}<NewLine>    96600   20.394    0.000   20.394    0.000 {method 'addcdiv_' of 'torch._C._TensorBase' objects}<NewLine>      600   15.658    0.026  141.634    0.236 /opt/conda/lib/python3.6/site-packages/torch/optim/adam.py:49(step)<NewLine>    96600   14.755    0.000   14.755    0.000 {method 'addcmul_' of 'torch._C._TensorBase' objects}<NewLine>     1200    3.549    0.003    3.549    0.003 {method 'to' of 'torch._C._TensorBase' objects}<NewLine>    31800    1.394    0.000    1.394    0.000 {built-in method conv2d}<NewLine>      600    1.104    0.002    1.104    0.002 {built-in method torch.distributed._broadcast_coalesced}<NewLine>    31800    0.924    0.000    0.924    0.000 {built-in method batch_norm}<NewLine>    31800    0.713    0.000    1.975    0.000 /opt/conda/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py:58(forward)<NewLine>     1844    0.550    0.000    0.550    0.000 {method 'acquire' of '_thread.lock' objects}<NewLine>    96439    0.369    0.000    0.369    0.000 {method 'zero_' of 'torch._C._TensorBase' objects}<NewLine>    29400    0.316    0.000    0.316    0.000 {built-in method relu_}<NewLine>110400/600    0.261    0.000    5.870    0.010 /opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:531(__call__)<NewLine>     9600    0.242    0.000    4.253    0.000 /opt/conda/lib/python3.6/site-packages/torchvision/models/resnet.py:95(forward)<NewLine>        5    0.215    0.043    0.215    0.043 {built-in method _posixsubprocess.fork_exec}<NewLine>   353400    0.174    0.000    0.174    0.000 /opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:571(__getattr__)<NewLine>       53    0.152    0.003    0.152    0.003 {built-in method posix.waitpid}<NewLine>      600    0.118    0.000    0.118    0.000 {built-in method addmm}<NewLine>    31800    0.098    0.000    1.063    0.000 /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1643(batch_norm)<NewLine>      600    0.090    0.000    0.499    0.001 /opt/conda/lib/python3.6/site-packages/torch/optim/optimizer.py:159(zero_grad)<NewLine></code></pre><NewLine></details><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s a good point; I hadn’t considered the disparity introduced by having a different number of epochs. While fixing up my testing script, I stumbled across what I think is a key culprit: thermal throttling.</p><NewLine><p>My home setup has the two GPUs in adjacent slots, and what I think is happening is that the airflow into the top GPU is being warmed by the backplate of the bottom GPU, because if I heat the GPUs up with a job the top hits 90C and the <code>pclck</code> from <code>nvidia-smi dmon</code> drops, but it drops <em>more</em> with a 2 GPU job.</p><NewLine><p>The hint that clued me in was the 2-GPU times getting worse as I increased the epoch size, rather than better.</p><NewLine><p>While I had tested on work systems, those earlier tests might have suffered from issues with epoch counts, etc. I’m going to rerun those tests with my updated testing script on work systems and see what the results are. I’ll report back when I have them (probably tomorrow).</p><NewLine><p>Thank you to everyone who took the time to read, comment, and/or run my testing script.  <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Short follow up: with the suggested changes, I was able to get scaling at 98.5% of linear with 2 GPUs on the work cluster. Thanks again!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/elistevens; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/elistevens; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/elistevens; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/megaserg; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/elistevens; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/elistevens; <NewLine> ,"REPLY_DATE 1: April 21, 2020,  2:09am; <NewLine> REPLY_DATE 2: April 21, 2020,  7:46pm; <NewLine> REPLY_DATE 3: April 21, 2020,  9:57pm; <NewLine> REPLY_DATE 4: April 21, 2020, 10:35pm; <NewLine> REPLY_DATE 5: April 21, 2020, 11:26pm; <NewLine> REPLY_DATE 6: April 22, 2020, 12:47am; <NewLine> REPLY_DATE 7: April 22, 2020,  8:04am; <NewLine> REPLY_DATE 8: April 22, 2020,  5:45am; <NewLine> REPLY_DATE 9: April 30, 2020,  1:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 3 Likes; <NewLine> REPLY 8 LIKES: 3 Likes; <NewLine> REPLY 9 LIKES: 3 Likes; <NewLine> 
78876,Error install from source on server,2020-04-28T21:38:38.614Z,5,123,"<div class=""post"" itemprop=""articleBody""><NewLine><p>HI,<br/><NewLine>I follow instructions from pytorch git as below.</p><NewLine><blockquote><NewLine><ol><NewLine><li><NewLine><p>install dependencies<br/><NewLine>conda install numpy ninja pyyaml mkl mkl-include setuptools cmake cffi<br/><NewLine>(cmake error: conda install -c anaconda cmake)</p><NewLine></li><NewLine><li><NewLine><p>clone pytorch<br/><NewLine>git clone --recursive <a href=""https://github.com/pytorch/pytorch"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch</a><br/><NewLine>cd pytorch</p><NewLine></li><NewLine><li><NewLine><p>export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/…/""}<br/><NewLine>python setup.py install</p><NewLine></li><NewLine></ol><NewLine></blockquote><NewLine><p>The error is as below.</p><NewLine><blockquote><NewLine><p>MakeFiles/c10.dir/util/numa.cpp.o -c …/c10/util/numa.cpp<br/><NewLine>…/c10/util/numa.cpp:6:10: fatal error: numa.h: No such file or directory<br/><NewLine><span class=""hashtag"">#include</span> &lt;numa.h&gt;<br/><NewLine>^~~~~~~~<br/><NewLine>compilation terminated.<br/><NewLine>[1684/4092] Building CXX object third_p…akeFiles/dnnl_cpu.dir/cpu_reorder.cpp.o<br/><NewLine>ninja: build stopped: subcommand failed.<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “setup.py”, line 740, in <br/><NewLine>build_deps()<br/><NewLine>File “setup.py”, line 323, in build_deps<br/><NewLine>cmake=cmake)<br/><NewLine>File “/cluster/home/cnphuong/pytorch/tools/build_pytorch_libs.py”, line 62, in build_caffe2<br/><NewLine>cmake.build(my_env)<br/><NewLine>File “/cluster/home/cnphuong/pytorch/tools/setup_helpers/cmake.py”, line 340, in build<br/><NewLine>self.run(build_args, my_env)<br/><NewLine>File “/cluster/home/cnphuong/pytorch/tools/setup_helpers/cmake.py”, line 141, in run<br/><NewLine>check_call(command, cwd=self.build_dir, env=env)<br/><NewLine>File “/cluster/home/cnphuong/.conda/envs/Pytorch_ENV/lib/python3.6/subprocess.py”, line 311, in check_call<br/><NewLine>raise CalledProcessError(retcode, cmd)<br/><NewLine>subprocess.CalledProcessError: Command ‘[‘cmake’, ‘–build’, ‘.’, ‘–target’, ‘install’, ‘–config’, ‘Release’, ‘–’, ‘-j’, ‘64’]’ returned non-zero exit status 1.</p><NewLine></blockquote><NewLine><p>When run setup.py I saw some output <a href=""https://drive.google.com/open?id=1QzKKK6Ul4o9b37bIVf_x0byjV0FYYjl7"" rel=""nofollow noopener"">here</a>.</p><NewLine></div>",https://discuss.pytorch.org/u/ph0123,(chau phuong),ph0123,"April 28, 2020, 10:00pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you please create an issue on github to track this?</p><NewLine><p>And this does not seem to be relevant to <code>torch.diistributed</code>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi,<br/><NewLine>Because Distributed CPUs is only supported by building from source. I think most of people with this tag have more experiments than others.</p><NewLine><p>Thanks,</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ph0123"">@ph0123</a>, <code>DistributedDataParallel</code> with CPU model should be supported by default in the release binaries. You can enable this mode by passing in a CPU model and do <strong>not</strong> provide a <code>device_ids</code> argument. If this is all you need, you don’t need to compile from source I think? Did you hit any error when trying to run DDP with CPU models?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""78876"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p><code>tedDataParallel</code> with CPU model should be supported by default in the release binaries. You can enable this mode by passing in a CPU model and do <strong>not</strong> provide a <code>device_ids</code> argument. If this is all you need, you don’t need to compile from source I think? Did you hit any error when trying to run DDP with CPU models</p><NewLine></blockquote><NewLine></aside><NewLine><p>Dear mrshenli,</p><NewLine><p>Thanks, last time, I install from sources, and some errors when run the program with Distributed Parallel. I check the tutorial, and I have to install from source to run Distributed CPUs.</p><NewLine><p>But now I read the documents again, it do not need install from sources.</p><NewLine><p>I will try. Thank you so much!<br/><NewLine>Thanks,</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>Please see</p><NewLine><pre><code class=""lang-auto"">**MPI Backend**<NewLine><NewLine>The Message Passing Interface (MPI) is a standardized tool from the field of high-performance computing. It allows to do point-to-point and collective communications and was the main inspiration for the API of `torch.distributed` . Several implementations of MPI exist (e.g. [Open-MPI](https://www.open-mpi.org/), [MVAPICH2](http://mvapich.cse.ohio-state.edu/), [Intel MPI](https://software.intel.com/en-us/intel-mpi-library)) each optimized for different purposes. The advantage of using the MPI backend lies in MPI’s wide availability - and high-level of optimization - on large computer clusters. [Some](https://developer.nvidia.com/mvapich) [recent](https://developer.nvidia.com/ibm-spectrum-mpi) [implementations](https://www.open-mpi.org/) are also able to take advantage of CUDA IPC and GPU Direct technologies in order to avoid memory copies through the CPU.<NewLine><NewLine>Unfortunately, PyTorch’s binaries can not include an MPI implementation and we’ll have to recompile it by hand. Fortunately, this process is fairly simple given that upon compilation, PyTorch will look *by itself* for an available MPI implementation. The following steps install the MPI backend, by installing PyTorch [from source](https://github.com/pytorch/pytorch#from-source).<NewLine><NewLine>1. Create and activate your Anaconda environment, install all the pre-requisites following [the guide](https://github.com/pytorch/pytorch#from-source), but do **not** run `python setup.py install` yet.<NewLine>2. Choose and install your favorite MPI implementation. Note that enabling CUDA-aware MPI might require some additional steps. In our case, we’ll stick to Open-MPI *without* GPU support: `conda install -c conda-forge openmpi`<NewLine>3. Now, go to your cloned PyTorch repo and execute `python setup.py install` .<NewLine><NewLine>In order to test our newly installed backend, a few modifications are required.<NewLine><NewLine>1. Replace the content under `if __name__ == '__main__':` with `init_process(0, 0, run, backend='mpi')` .<NewLine>2. Run `mpirun -n 4 python myscript.py` .<NewLine><NewLine>The reason for these changes is that MPI needs to create its own environment before spawning the processes. MPI will also spawn its own processes and perform the handshake described in [Initialization Methods](https://pytorch.org/tutorials/intermediate/dist_tuto.html#initialization-methods), making the `rank` and `size` arguments of `init_process_group` superfluous. This is actually quite powerful as you can pass additional arguments to `mpirun` in order to tailor computational resources for each process. (Things like number of cores per process, hand-assigning machines to specific ranks, and [some more](https://www.open-mpi.org/faq/?category=running#mpirun-hostfile)) Doing so, you should obtain the same familiar output as with the other communication backends.<NewLine></code></pre><NewLine><p>Thanks,</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh I see, you are trying to use MPI. Is MPI the only option, or will Gloo or NCCL also be acceptable?</p><NewLine><p>And yes, MPI backend needs building from source.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>BTW, the build log shown <a href=""https://drive.google.com/file/d/1QzKKK6Ul4o9b37bIVf_x0byjV0FYYjl7/view"" rel=""nofollow noopener"">here</a> does not seem to be complete. Could you please also paste the last few screens of logs?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I try to work with MPI backend but it did not work.<br/><NewLine>I change to Gloo backend.<br/><NewLine>Thanks,</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/ph0123; <NewLine> ,"REPLY_DATE 1: April 28, 2020, 10:26pm; <NewLine> REPLY_DATE 2: April 29, 2020, 12:44am; <NewLine> REPLY_DATE 3: April 29, 2020,  2:08am; <NewLine> REPLY_DATE 4: April 29, 2020,  8:24am; <NewLine> REPLY_DATE 5: April 29, 2020,  8:42am; <NewLine> REPLY_DATE 6: April 29, 2020,  2:03pm; <NewLine> REPLY_DATE 7: April 29, 2020,  2:08pm; <NewLine> REPLY_DATE 8: April 29, 2020,  6:06pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> 
78925,How to measure DDP time breakdown?,2020-04-29T07:29:18.232Z,0,104,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am trying to use DistribuedDataDarallel for multi-node data-parallelism.</p><NewLine><p>I want to know how can I measure the time breakdown for data load, forward, backward, communication?</p><NewLine><p>Also, for calculating FLops, I am going to use the repository[<a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/calculating-flops-of-a-given-pytorch-model/3711"">Calculating flops of a given pytorch model</a>] in github. Does anyone know the good way for calculating Flops.</p><NewLine></div>",https://discuss.pytorch.org/u/dujiangsu,(DouJS),dujiangsu,"April 29, 2020,  7:29am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If the program uses GPUs, you can use <a href=""https://pytorch.org/docs/stable/cuda.html#torch.cuda.Event.elapsed_time"" rel=""nofollow noopener"">elapsed_time</a> to measure the time spent on forward, backward, and optimizer. It is harder to break down computation and communication of the backward pass, as DDP tries to overlap these two and DDP conducts communication on dedicated CUDA streams that are not visible from the application side. Besides, communications are launched as soon as a gradient bucket is ready, meaning that it may or may not always saturate the bandwidth. To get around this, you can run local forward-backward, and then explicitly using allreduce from application side to conduct gradient synchronization after the backward pass. This will expose opportunities to measure that from application.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: April 29, 2020,  2:16pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
78681,DDP Error for more than 2 nodes,2020-04-27T16:19:03.936Z,1,102,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I tried DistributedDataParallel with nccl backend.</p><NewLine><p>It works when I use 1 or 2 nodes (each with 4 V100).</p><NewLine><p>However, error happens when further scaling to 3 or 4 nodes, and always a node with the following error (other nodes reports differently and looks correct).</p><NewLine><p>gpu45:169732:170179 [0] transport/net_ib.cc:789 NCCL WARN NET/IB : Got completion with error 12, opcode 1, len 11155, vendor err 129</p><NewLine><p>I tried Pytorch Version: 1.2-cuda10.0 1.4-cuda10.1 1.5-cuda10.1<br/><NewLine>And the nccl version is 2.4.8.</p><NewLine><p>The nccl_debug info of 4 nodes are listed below: <a href=""http://49.234.107.127:81/index.php/s/wktxGy47Zxmd6gE"" rel=""nofollow noopener"">Node0</a>, <a href=""http://49.234.107.127:81/index.php/s/WmNR8T6R4bZxLJb"" rel=""nofollow noopener"">Node1</a>, <a href=""http://49.234.107.127:81/index.php/s/kBd4gBKE5jfLFjq"" rel=""nofollow noopener"">Node2</a>, [Node3] (<a href=""http://49.234.107.127:81/index.php/s/5B2wEFHSFCWSHfm"" rel=""nofollow noopener"">http://49.234.107.127:81/index.php/s/5B2wEFHSFCWSHfm</a>) .</p><NewLine><p>When I tried 4 nodes, the [ NCCL WARN NET/IB] always happens in the third node.<br/><NewLine>If I exclude the node (gpu45), and only run on the other three nodes, the [ NCCL WARN NET/IB] also happens.</p><NewLine></div>",https://discuss.pytorch.org/u/dujiangsu,(DouJS),dujiangsu,"April 27, 2020,  4:26pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This might be relevant to <a href=""https://github.com/NVIDIA/nccl/issues/214"" rel=""nofollow noopener"">this issue</a> in NCCL repo, and <a href=""https://github.com/NVIDIA/nccl/issues/214#issuecomment-489242319"" rel=""nofollow noopener"">this comment</a> seems fixed it.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot!<br/><NewLine>I give up using NCCL temporarily and it should relate to the system setting.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dujiangsu; <NewLine> ,"REPLY_DATE 1: April 27, 2020,  4:26pm; <NewLine> REPLY_DATE 2: April 29, 2020,  7:40am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
78717,How can I gather tensors from specific ranks,2020-04-27T19:54:28.656Z,1,163,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to gather tensors from specific ranks  in each rank (For example, I want gather ranks=[0,1] in rank0&amp;rank1,  and gather ranks=[2,3] in rank2&amp;3). I implement by initial new group:</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import random<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.multiprocessing as mp<NewLine>import torch.distributed as dist<NewLine><NewLine>import torch.utils.data<NewLine>import torch.utils.data.distributed<NewLine>from torch.multiprocessing import Process<NewLine><NewLine><NewLine>from absl import flags<NewLine>from absl import app<NewLine><NewLine>FLAGS = flags.FLAGS<NewLine><NewLine>flags.DEFINE_integer('nodes_num', 1, 'machine num')<NewLine>flags.DEFINE_integer('ngpu', 4, 'ngpu per node')<NewLine>flags.DEFINE_integer('world_size', 4, 'FLAGS.nodes_num*FLAGS.ngpu')<NewLine>flags.DEFINE_integer('node_rank', 0, 'rank of machine, 0 to nodes_num-1')<NewLine>flags.DEFINE_integer('rank', 0, 'rank of total threads, 0 to FLAGS.world_size-1, will be re-compute in main_worker func')<NewLine><NewLine>@torch.no_grad()<NewLine>def group_gather(tensor, rank, ngpu_per_node):<NewLine>    #ranks = [0,1]<NewLine>    if rank == 0 or rank == 1:<NewLine>        ranks = [0,1]<NewLine>    if rank == 2 or rank == 3:<NewLine>        ranks = [2,3]<NewLine>    print('ranks: ', ranks)<NewLine>    group = dist.new_group(ranks = ranks)<NewLine>    tensors_gather = [torch.ones_like(tensor) for _ in range(2)]<NewLine>    torch.distributed.all_gather(tensors_gather, tensor, group, async_op=False)<NewLine>    output = torch.cat(tensors_gather, dim=0)<NewLine>    print('gather out shape: ', output.shape)<NewLine>    return output<NewLine><NewLine><NewLine>class ToyModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(ToyModel, self).__init__()<NewLine>        self.fc = nn.Linear(3,2)<NewLine><NewLine>    def forward(self, x, rank, ngpu_per_node):<NewLine>        x_gather = group_gather(x, rank, ngpu_per_node)<NewLine>        out = self.fc(x_gather)<NewLine>        return out <NewLine><NewLine>def main(argv):<NewLine>    del argv<NewLine>    FLAGS.append_flags_into_file('tmp.cfg')<NewLine>    os.environ['MASTER_ADDR'] = '127.0.0.1'<NewLine>    os.environ['MASTER_PORT'] = str(random.randint(1,100000))<NewLine>    mp.spawn(main_worker, nprocs=FLAGS.ngpu, args=())<NewLine><NewLine>def main_worker(gpu_rank):<NewLine>    FLAGS._parse_args(FLAGS.read_flags_from_files(['--flagfile=./tmp.cfg']), True)<NewLine>    FLAGS.mark_as_parsed()<NewLine>    FLAGS.rank = FLAGS.node_rank * FLAGS.ngpu + gpu_rank # rank among FLAGS.world_size<NewLine>    dist.init_process_group(<NewLine>        backend='nccl',<NewLine>        init_method='env://',<NewLine>        world_size=FLAGS.world_size,<NewLine>        rank=FLAGS.rank)<NewLine>    model = ToyModel()<NewLine>    torch.cuda.set_device(gpu_rank)<NewLine>    model.cuda()<NewLine>    model = torch.nn.parallel.DistributedDataParallel(<NewLine>        model, device_ids=[gpu_rank])<NewLine><NewLine>    x = torch.randn(4,3).cuda()<NewLine>    model(x, FLAGS.rank, FLAGS.ngpu)<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    app.run(main)<NewLine><NewLine><NewLine></code></pre><NewLine><p>In group_gather(…), I init new group according to thread’s rank.</p><NewLine><p>But this scripts can not always work well, It may crash in some times, and raise error:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""/root/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 19, in _wrap<NewLine>    fn(i, *args)<NewLine>  File ""/root/test_distcomm/test_group_gather.py"", line 78, in main_worker<NewLine>    model(x, FLAGS.rank, FLAGS.ngpu)<NewLine>  File ""/root/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 447, in forward<NewLine>    output = self.module(*inputs[0], **kwargs[0])<NewLine>  File ""/root/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/test_distcomm/test_group_gather.py"", line 48, in forward<NewLine>    x_gather = group_gather(x, gpu_rank, ngpu_per_node)<NewLine>  File ""/root/anaconda3/lib/python3.6/site-packages/torch/autograd/grad_mode.py"", line 49, in decorate_no_grad<NewLine>    return func(*args, **kwargs)<NewLine>  File ""/root/test_distcomm/test_group_gather.py"", line 35, in group_gather<NewLine>    torch.distributed.all_gather(tensors_gather, tensor, group, async_op=False)<NewLine>  File ""/root/anaconda3/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 1153, in all_gather<NewLine>    work = group.allgather([tensor_list], [tensor])<NewLine>RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:410, unhandled system error, NCCL version 2.4.8<NewLine><NewLine></code></pre><NewLine><p>I think the logic in code is correct, and I can not figure out where is wrong.</p><NewLine><p>I run this code with 4 nvidia-t4 gpus with cuda10.1, my pytorch version is 1.4.0.</p><NewLine><p>You can simply run this code with ‘python main.py’ (may need pip install absl-py)</p><NewLine></div>",https://discuss.pytorch.org/u/hhxx,(hhxx),hhxx,"April 27, 2020,  7:54pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If I set ranks in group_gather func as [0,1] consistently, this code can work well all the time</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The <code>new_group</code> API requires all processes to call with the same <code>ranks</code> argument if even if they do not participate in the new group. See the API doc here: <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.new_group"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/distributed.html#torch.distributed.new_group</a></p><NewLine><p>In the code above, the following code breaks the above assumption.</p><NewLine><pre><code class=""lang-python"">    if rank == 0 or rank == 1:<NewLine>        ranks = [0,1]<NewLine>    if rank == 2 or rank == 3:<NewLine>        ranks = [2,3]<NewLine>    print('ranks: ', ranks)<NewLine>    group = dist.new_group(ranks = ranks)<NewLine></code></pre><NewLine><p>It needs to be modified to the following:</p><NewLine><pre><code class=""lang-python"">    g1 = dist.new_group(ranks = [0, 1])<NewLine>    g2 = dist.new_group(ranks = [2, 3])<NewLine>    # check rank to see if the current process should use g1 or g2<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""78717"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>In the code above, the following code breaks the above assumption.</p><NewLine><pre><code class=""lang-auto""><NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, it works well now! Thanks very much</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hhxx; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/hhxx; <NewLine> ,"REPLY_DATE 1: April 27, 2020,  7:57pm; <NewLine> REPLY_DATE 2: April 27, 2020,  8:45pm; <NewLine> REPLY_DATE 3: April 27, 2020,  8:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
77927,Distributed training is even slower sometimes,2020-04-22T21:27:41.127Z,4,158,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Used DistributedDataParallel, but find that the speed with 4 nodes (4 GPUs per node) is sometimes even slower. Sometimes, the job runs 4300 images for each second, which is normal. If it is 4300 at the beginning, the job will always be running at this fast speed. But sometimes, the job is running at 1000 images per second, and the whole job will be at this speed always. The jobs are running in a cluster, and in different physical machines but same machine type.</p><NewLine><p>For the job with problematic issues, the GPU utility is always 98%~100%. Pytorch version = 1.4; CUDA=10.1; Ubuntu 16.04 docker image. NCCL is for sure to use the Infinity band with the following logs. The time cost of data loading is also very small (less than 1%).</p><NewLine><p>Are there any idea to debug?</p><NewLine><p>41de615398b349e78486287e94d4883b000000:1573:1573 [0] NCCL INFO NET/IB : Using [0]mlx4_0:1/IB ; OOB eth0:10.0.0.4&lt;0&gt;</p><NewLine></div>",https://discuss.pytorch.org/u/amsword,(Jianfeng Wang),amsword,"April 22, 2020,  9:27pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do the jobs always run on the exactly same set of machines?</p><NewLine><p>If not, can there be any straggler in cluster? Or can different network layouts (same rack, different rack, etc.) play a role here?</p><NewLine><p>For debugging, it will be helpful to identify which step (forward, backward, opt.step) takes longer (on all nodes) when the throughput drops to 1000 images/s. <a href=""https://pytorch.org/docs/stable/cuda.html#torch.cuda.Event.elapsed_time"" rel=""nofollow noopener"">elapsed_time</a> should be able to tell that. All communication in DDP occurs in the backward pass. If all steps are the same but the backward pass takes long for all processes, then it might be caused by network issues. If some processes suffers from slower data loading, or slower forward, or slower optimizer, it looks more like a straggler problem.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply. I checked again. They are not running on the same machines. For the problematic job, i killed it and re-run it. The job was scheduled on those 4 machines again and the speed is still 1000. Then, i submit the same job again, which are scheduled on another 4 machines. The new job is running 4k. So, the problem might be the issue of machines or rack as you suggested.</p><NewLine><p>One more question is that if the network has some issues on those machines or straggler issues, would it be possible that the GPU is still utilizing 98%~100%? The GPUs are fully utilized and i was thinking there is no network issue.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""77927"" data-username=""amsword""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/amsword/40/8461_2.png"" width=""20""/> amsword:</div><NewLine><blockquote><NewLine><p>One more question is that if the network has some issues on those machines or straggler issues, would it be possible that the GPU is still utilizing 98%~100%? The GPUs are fully utilized and i was thinking there is no network issue.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Not 100% sure, but if GPU reports block waiting for AllReduce as busy, then slow network or straggler could lead to 100% utilization for the entire group. This can be measured by submitting a job that only runs allreduce. BTW, based one past observations, GPU would (sometimes?) report 100% utilization even if DDP hangs. So I think this is possible.</p><NewLine><p>cc <a class=""mention"" href=""/u/ngimel"">@ngimel</a> in case you know how CUDA would behave here <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> is correct, if there are straggler GPUs, other GPUs waiting for them would report 100% utilization with AllReduce.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks very much <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> <a class=""mention"" href=""/u/ngimel"">@ngimel</a> for the explanation of ALLReduce leading 100%. One more question is, is there any way to detect which GPU is the straggler (among 16 GPUs)?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/amsword; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ngimel; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/amsword; <NewLine> ,"REPLY_DATE 1: April 22, 2020, 10:02pm; <NewLine> REPLY_DATE 2: April 22, 2020, 10:39pm; <NewLine> REPLY_DATE 3: April 23, 2020, 12:33am; <NewLine> REPLY_DATE 4: April 23, 2020,  5:37am; <NewLine> REPLY_DATE 5: April 26, 2020,  5:00pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> 
77748,Is it expected for DistributedDataParallel to use more memory on 1 GPU in a 1GPU:1process setup?,2020-04-21T19:46:25.581Z,5,250,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it expected for <code>nn.parallel.DistributedDataParallel</code> in a 1GPU:1Process setup to use a little extra memory on one of the GPUs? The use isn’t exorbitant ( 3614 MiB vs. 4189 MiB). If so, what is this extra memory used for? Is it the <code>all_reduce</code> call on the gradients? If not what would this be attributed to?</p><NewLine><ul><NewLine><li>1 Gpu per 1 process spun up with: <code>mp.spawn(run, nprocs=args.num_replicas, args=(args.num_replicas,))</code><NewLine></li><NewLine><li>Entire module wrapped with <code>nn.parallel.DistributedDataParallel</code><NewLine></li><NewLine><li>loss function is a member of above module.</li><NewLine><li>pytorch 1.4</li><NewLine></ul><NewLine><p>Multiprocessing init created via:</p><NewLine><pre><code class=""lang-python"">def handle_multiprocessing_logic(rank, num_replicas):<NewLine>    """"""Sets the appropriate flags for multi-process jobs.""""""<NewLine>    args.gpu = rank  # Set the GPU device to use<NewLine><NewLine>    if num_replicas &gt; 1:<NewLine>        torch.distributed.init_process_group(<NewLine>            backend='nccl', init_method='env://',<NewLine>            world_size=args.num_replicas, rank=rank<NewLine>        )<NewLine><NewLine>        # Update batch size appropriately<NewLine>        args.batch_size = args.batch_size // num_replicas<NewLine><NewLine>        # Set the cuda device<NewLine>        torch.cuda.set_device(rank)<NewLine>        print(""Replica {} / {} using GPU: {}"".format(<NewLine>            rank + 1, num_replicas, torch.cuda.get_device_name(rank)))<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/jramapuram,(Jason Ramapuram),jramapuram,"April 21, 2020,  9:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, this is expected currently, because DDP creates buckets to consolidate gradient communication. Checkout <a href=""https://github.com/pytorch/pytorch/blob/1f8267931176cc9ecbf00493e5a359a57baca3df/torch/nn/parallel/distributed.py#L173"" rel=""nofollow noopener"">this</a> and  <a href=""https://github.com/pytorch/pytorch/blob/1f8267931176cc9ecbf00493e5a359a57baca3df/torch/csrc/distributed/c10d/reducer.cpp#L108"" rel=""nofollow noopener"">this</a>. We could potentially mitigate this problem by setting <code>param.grad</code> to point to different offsets in the bucket so that we don’t need two copies of grads.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>tracking issue: <a href=""https://github.com/pytorch/pytorch/issues/37030"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/37030</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the response; just to clarify: you mean it is expected that 1 of the GPUs in say a 2 GPU (single-process-single-gpu) DDP setup will use more memory because of bucketing? Wouldn’t the buckets be of the same size on both devices?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>oh, sorry, I misread the question. I thought you mean DDP uses a little more memory than local model.</p><NewLine><blockquote><NewLine><p>you mean it is expected that 1 of the GPUs in say a 2 GPU (single-process-single-gpu) DDP setup will use more memory because of bucketing?</p><NewLine></blockquote><NewLine><p>no, they should be the same I think</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>The general problem of more memory makes sense in the current way things are setup (from your src links), let me see if I can come up with a minimum viable example for this effect (more mem on one of the GPUs).</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is the DDP process the only process using that GPU? The extra size ~500MB looks like an extra cuda context. Does this behavior still persist if you set <code>CUDA_VISIBLE_DEVICES </code> env var properly (instead of using <code>torch.cuda.set_device(rank)</code>) before launching each process?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""7"" data-topic=""77748"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>Is the DDP process the only process using that GPU? The extra size ~500MB looks like an extra cuda context. Does this behavior still persist if you set <code>CUDA_VISIBLE_DEVICES </code> env var properly (instead of using <code>torch.cuda.set_device(rank)</code> ) before launching each process?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yup, I don’t have X or anything else running: <img alt=""image"" data-base62-sha1=""iPMyw4nnlFB3nZtkJ9NIj9REHnl"" height=""340"" src=""https://discuss.pytorch.org/uploads/default/original/3X/8/4/8401a5dd6daf338c37be2fc57f980e556cf21203.png"" width=""562""/></p><NewLine><p>I don’t set <code>CUDA_VISIBLE_DEVICES</code> but I do call <code>torch.cuda.set_device(rank)</code> before instantiating the model / optimizers, etc (see function in initial post).</p><NewLine><p><strong>Edit</strong>: not sure how I would be able to set the ENV var when using <code>mp.spawn</code> such that it doesn’t apply to both processes.</p><NewLine><p><strong>Edit2</strong>: didn’t realize children can modify their ENV independently of the parent and other processes: <a href=""https://stackoverflow.com/questions/24642811/set-env-var-in-python-multiprocessing-process"" rel=""nofollow noopener"">https://stackoverflow.com/questions/24642811/set-env-var-in-python-multiprocessing-process</a></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Using pytotch 1.5, setting <code>CUDA_VISIBLE_DEVICES</code> appropriately per process, doing any CUDA related stuff before the ENV var is set and changing the DDP constructor with:</p><NewLine><pre><code class=""lang-python"">device_ids=[0],   # set w/cuda environ var<NewLine>output_device=0,  # set w/cuda environ var<NewLine></code></pre><NewLine><p>it seems to be working as expected:</p><NewLine><p><img alt=""image"" data-base62-sha1=""4EsvaueFI45gYp1s1MbH6tsLUcP"" height=""220"" src=""https://discuss.pytorch.org/uploads/default/original/3X/2/0/209b87f0f083c3adc9af2ca8a8b8378862f9f4a3.png"" width=""571""/></p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>It might be relevant to <a href=""https://discuss.pytorch.org/t/distributed-training-creates-multiple-processes-in-gpu0/77881/4"">this post</a>. If <code>CUDA_VISIBLE_DEVICES</code> is not set to one device per process, and the application program calls <code>clear_cache</code> somewhere without a device context, it could try to initialize the CUDA context on device 0.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jramapuram; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/jramapuram; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/jramapuram; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/jramapuram; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: April 21, 2020,  9:55pm; <NewLine> REPLY_DATE 2: April 21, 2020,  9:58pm; <NewLine> REPLY_DATE 3: April 21, 2020,  9:59pm; <NewLine> REPLY_DATE 4: April 21, 2020, 10:01pm; <NewLine> REPLY_DATE 5: April 21, 2020, 10:04pm; <NewLine> REPLY_DATE 6: April 26, 2020,  1:06pm; <NewLine> REPLY_DATE 7: April 21, 2020, 10:18pm; <NewLine> REPLY_DATE 8: April 27, 2020,  1:39am; <NewLine> REPLY_DATE 9: April 26, 2020,  3:54pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> 
78440,Does DistributedDataParallel split data at the batch dimension?,2020-04-26T02:17:43.417Z,3,131,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Let’s consider the batch size is 64 and we want to run in a cluster of two nodes, where each node contains 4 GPUs. How the batch will be split? Will the batch first split into two and thus each node will get a batch of data size 32, and finally, each node will split the data among the four GPUs, thus each GPU will get a batch of data size 8? is this the way the will be split in DistributedDataParallel mode?<br/><NewLine>Thanks in advance for the clarification.</p><NewLine></div>",https://discuss.pytorch.org/u/akashs,(Md Mofijul Islam (Akash)),akashs,"April 26, 2020,  2:17am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If using the recommended DistributedDataParallel (DDP) mode, where there is a dedicated process for each GPU, DDP does not split input data. Each process will have its own data loader and its own DDP instance. DDP only help to automatically compute the global averaged gradient in the backward pass. DDP will run in this mode when <code>device_ids</code> only contains a single device, or there is only one visible device. See <a href=""https://pytorch.org/docs/master/notes/ddp.html"" rel=""nofollow noopener"">this</a> for more details.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the clarification. Just to confirm my understanding: that means each node will work with a separate data loader and thus each node the batch size will be 64, am I right?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>In this case, as each node has 4 GPUs, each node will launch 4 processes with each process creating its own dataloader and DDP instance. So, each node will actually have 4 data loaders.</p><NewLine><p>If you would like to run batch size of 64 across 2 node (8 gpus), then each data loader should load data size of 64 / 8 = 8.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot for the explanation <img alt="":slightly_smiling_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slightly_smiling_face.png?v=9"" title="":slightly_smiling_face:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/akashs; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/akashs; <NewLine> ,"REPLY_DATE 1: April 26, 2020,  2:44am; <NewLine> REPLY_DATE 2: April 26, 2020,  2:51am; <NewLine> REPLY_DATE 3: May 8, 2020,  9:25am; <NewLine> REPLY_DATE 4: April 26, 2020,  2:56am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> 
77881,Distributed training creates multiple processes in GPU0,2020-04-22T15:15:46.806Z,1,135,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’ve recently started using the distributed training framework for PyTorch and followed the <a href=""https://github.com/pytorch/examples/blob/master/imagenet/main.py"" rel=""nofollow noopener"">imagenet</a> example. I’m using multi-node multi-GPU training. While running the code, during the 1st epoch itself, I see multiple processes starting at GPU 0 of both the servers. They are not present initially when I start the training. From the GPU memory usage, it seems that the other processes are some copy of the model (they all have a fixed usage like 571M). Since running an epoch takes ~12 hours for my use case, debugging step by step is not exactly a feasible solution. I’ve ensured that I pass <code>args.gpu </code> as argument whenever I do a <code>.cuda()</code> call. Also, the model loading/saving is done as suggested in the imagenet example.</p><NewLine><p>Are there any pointers to the probable cause of the issue (or some intelligent ways to debug the code)? Thanks in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/Soumya_Sanyal,(Soumya Sanyal),Soumya_Sanyal,"April 22, 2020,  3:15pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you please share the cmd you used to launch the processes?</p><NewLine><p>Does the problem disappear if you set <code>CUDA_VISIBLE_DEVICES</code> when launching the process and not passing in <code>--gpu</code> (let it use the default and only visible one)?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Shen,</p><NewLine><p>I always set the <code>CUDA_VISIBLE_DEVICES</code> for each run using <code>export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7</code> for example. The code runs on all the 8 GPUs with full utilization, so multiprocessing is surely working. The command I use is as follows  on the two servers I’m using (with appropriate IP and port set):</p><NewLine><pre><code class=""lang-auto"">python train_distributed.py --dist-url 'tcp://ip:port' --dist-backend 'gloo' --multiprocessing-distributed --world-size 2 --rank 0<NewLine>python train_distributed.py --dist-url 'tcp://ip:port' --dist-backend 'gloo' --multiprocessing-distributed --world-size 2 --rank 1<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Found the bug. So we need to be careful with setting the right GPU context while calling <code>clear_cache()</code> function, otherwise it allocates fixed memory on GPU0 for the other GPUs. Relevant issue <a href=""https://github.com/pytorch/pytorch/issues/25752"" rel=""nofollow noopener"">here</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Soumya_Sanyal; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Soumya_Sanyal; <NewLine> ,"REPLY_DATE 1: April 22, 2020,  3:31pm; <NewLine> REPLY_DATE 2: April 22, 2020,  5:01pm; <NewLine> REPLY_DATE 3: April 25, 2020, 11:04pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
78384,If set random seed across multi-gpu is necessary in DistributedDataParallel？,2020-04-25T13:52:32.703Z,0,129,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Following imagenet-example: <a href=""https://github.com/pytorch/examples/blob/master/imagenet/main.py"" rel=""nofollow noopener"">https://github.com/pytorch/examples/blob/master/imagenet/main.py</a>, It seems that  <strong>seed</strong>  is not set in default (default is None):<br/><NewLine><code>parser.add_argument('--seed', default=None, type=int, help='seed for initializing training. ')</code></p><NewLine><p>But when we use DistributedDataParallel mode, if seed is not set, the initialized parameters across multi-gpu will be different, resulting in different model param is kept in different gpus during training process (although we only save ckpt in rank0 gpu).</p><NewLine><p>I am not sure whether this phenomenon will cause unknown errors, or may lead to an unstable results? Is it safe for me not to set the initialization seed？</p><NewLine></div>",https://discuss.pytorch.org/u/hhxx,(hhxx),hhxx,"April 25, 2020,  1:52pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This should be fine, because <code>DistributedDataParallel</code> broadcasts model states from rank 0 to all other ranks at construction time. See the code below:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/34284c127930dc12d612c47cab44cf09b432b522/torch/nn/parallel/distributed.py#L280-L285"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/34284c127930dc12d612c47cab44cf09b432b522/torch/nn/parallel/distributed.py#L280-L285"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/34284c127930dc12d612c47cab44cf09b432b522/torch/nn/parallel/distributed.py#L280-L285</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""280"" style=""counter-reset: li-counter 279 ;""><NewLine><li># Sync params and buffers</li><NewLine><li>module_states = list(self.module.state_dict().values())</li><NewLine><li>if len(module_states) &gt; 0:</li><NewLine><li>    self._distributed_broadcast_coalesced(</li><NewLine><li>        module_states,</li><NewLine><li>        self.broadcast_bucket_size)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: April 25, 2020,  9:56pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
66651,Strange behaviour of GLOO tcp transport,2020-01-14T15:02:45.954Z,17,804,"<div class=""post"" itemprop=""articleBody""><NewLine><p>HI all.<br/><NewLine>I have strange problem: I’m trying to run 2 tasks on 2 machines via following<br/><NewLine>trivial script:</p><NewLine><pre><code>dist.init_process_group(backend = ""gloo"",init_method = 'tcp://192.168.0.1:29500',rank = irank,world_size = iwsize)<NewLine>arg = None<NewLine>if(dist.get_rank()==0):<NewLine>    arg = Dist_Trainer()<NewLine>run(dist.get_rank(),dist.get_world_size(),arg)<NewLine></code></pre><NewLine><p>When I run them on one machine, all works fine.<br/><NewLine>But when I start process with rank = 0 on one machine,<br/><NewLine>and process with rank = 1 on another machine,<br/><NewLine>process with rank = 0 fails with the following  output:</p><NewLine><p>python train_dist.py 0 2<br/><NewLine>RANK: 0 wsize: 2<br/><NewLine>terminate called after throwing an instance of ‘gloo::IoException’                                                                                                                                                   what():  [/opt/conda/conda-bld/pytorch_1544176307774/work/third_party/gloo/gloo/transport/tcp/pair.cc:724] connect [127.0.0.1]:45965: Connection refused</p><NewLine><p>This happens <em>only</em> when I start process with rank=1. If I don’t started it,<br/><NewLine>process with rank =0 is waiting for connection.</p><NewLine><p>i.e.,I assume that tcp connection happens, but then process with rank = 0<br/><NewLine>tries to work with 127.0.0.1?</p><NewLine><p>Upd: I tried  setting export GLOO_SOCKET_IFNAME=enp2s0,<br/><NewLine>the problem still remains.</p><NewLine></div>",https://discuss.pytorch.org/u/Oleg_Ivanov,(Oleg Ivanov),Oleg_Ivanov,"January 14, 2020,  3:06pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like rank 0 is working with [127.0.0.1]:45965. Have you unset <code>MASTER_ADDR</code> and <code>MASTER_PORT</code> environment vars before launching the script?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, they were unset.</p><NewLine><p>By the way, if I swap scripts with rank=0 and rank=1 on these machines,<br/><NewLine>then script with rank=1crashes:</p><NewLine><p>python train_dist.py 1 2<br/><NewLine>RANK: 1 wsize: 2<br/><NewLine>terminate called after throwing an instance of ‘gloo::IoException’<br/><NewLine>what():  [/opt/conda/conda-bld/pytorch_1544176307774/work/third_party/gloo/gloo/transport/tcp/pair.cc:724] connect [127.0.1.1]:3978: Connection refused</p><NewLine><p>Script with rank=0 still waiting for connection</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/oleg_ivanov"">@Oleg_Ivanov</a>,</p><NewLine><p>Have you solved this problem? Same as here.</p><NewLine><p>Thanks,<br/><NewLine>Ziyi</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you just run the following without any other code, does it fail?</p><NewLine><pre><code class=""lang-python"">import torch.distributed as dist<NewLine><NewLine># on rank 0<NewLine>dist.init_process_group(<NewLine>    backend = ""gloo"",<NewLine>    init_method = 'tcp://192.168.0.1:29500',<NewLine>    rank = 0,<NewLine>    world_size = 2<NewLine>)<NewLine><NewLine># on rank 1<NewLine>dist.init_process_group(<NewLine>    backend = ""gloo"",<NewLine>    init_method = 'tcp://192.168.0.1:29500',<NewLine>    rank = 1,<NewLine>    world_size = 2<NewLine>)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ziyizhu"">@ZiyiZhu</a>  Are you trying to run this with RPC? Currently <code>init_rpc</code> does not work together with <code>init_process_group</code>. There are work around to create non-default process groups. Or we can also add a fix to <code>init_rpc</code> if necessary. This is the tracking issue: <a href=""https://github.com/pytorch/pytorch/issues/33583"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/33583</a></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>This is different from the RPC problem. Back then I was using Google Cloud VMs. The torch.distributed and RPC worked fine there.</p><NewLine><p>However, just recently we built up new servers with GPU in our lab and connect them using an electrical packet switch. They can ping each other using the internal IP. For me now it is 10.1.1.101 for rank 0 and 10.1.1.102 for rank 1. So I run the following:</p><NewLine><pre><code class=""lang-auto"">import torch.distributed as dist<NewLine><NewLine># on rank 0<NewLine>dist.init_process_group(<NewLine>    backend = ""gloo"",<NewLine>    init_method = 'tcp://10.1.1.101:29500',<NewLine>    rank = 0,<NewLine>    world_size = 2<NewLine>)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">import torch.distributed as dist<NewLine><NewLine># on rank 1<NewLine>dist.init_process_group(<NewLine>    backend = ""gloo"",<NewLine>    init_method = 'tcp://10.1.1.101:29500',<NewLine>    rank = 1,<NewLine>    world_size = 2<NewLine>)<NewLine></code></pre><NewLine><p>However, it failed with</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-1-532df564c254&gt; in &lt;module&gt;<NewLine>      6     init_method = 'tcp://10.1.1.101:29500',<NewLine>      7     rank = 1,<NewLine>----&gt; 8     world_size = 2<NewLine>      9 )<NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py in init_process_group(backend, init_method, timeout, world_size, rank, store, group_name)<NewLine>    401             store,<NewLine>    402             group_name=group_name,<NewLine>--&gt; 403             timeout=timeout)<NewLine>    404 <NewLine>    405     _pg_group_ranks[_default_pg] = {i: i for i in range(_default_pg.size())}<NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py in _new_process_group_helper(world_size, rank, group_ranks, backend, store, group_name, timeout)<NewLine>    469                 rank,<NewLine>    470                 world_size,<NewLine>--&gt; 471                 timeout=timeout)<NewLine>    472             _pg_map[pg] = (Backend.GLOO, store)<NewLine>    473             _pg_names[pg] = group_name<NewLine><NewLine>RuntimeError: [/opt/conda/conda-bld/pytorch_1587428398394/work/third_party/gloo/gloo/transport/tcp/pair.cc:769] connect [127.0.0.1]:31662: Connection refused<NewLine></code></pre><NewLine><p>Which I guess is the same problem for <a class=""mention"" href=""/u/oleg_ivanov"">@Oleg_Ivanov</a> too. In terms of</p><NewLine><pre><code class=""lang-auto"">export GLOO_SOCKET_IFNAME=eno2<NewLine></code></pre><NewLine><p>Should I simply do it in any terminal? eno2 is my NIC.<br/><NewLine><img alt=""image"" data-base62-sha1=""tAi1G9Fo6wfuElbqHsbFZy9tNLq"" height=""154"" src=""https://discuss.pytorch.org/uploads/default/original/3X/c/f/cf593cd47885bd0ea96d176c8225a5b754f87768.png"" width=""597""/></p><NewLine><p>Please let me know if you have any thoughts. Thank you very much for your help!</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""66651"" data-username=""ZiyiZhu""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/z/ce7236/40.png"" width=""20""/> ZiyiZhu:</div><NewLine><blockquote><NewLine><p>Should I simply do it in any terminal?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, either set it in terminal or pass <code>GLOO_SOCKET_IFNAME=eno2</code> as a prefix to the command that launches the process.</p><NewLine><p>Another cause might be hostname to ip mapping. IIUC, Gloo would try to resolve the the ip using the hostname. What does the following command return for you?</p><NewLine><pre><code class=""lang-auto"">getent hosts `hostname`<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>oh. This may be the problem. In the new servers (10.1.1.101 &amp; 10.1.1.102), getent hosts <code>hostname</code> returns nothing.</p><NewLine><p>I am also testing the torch.distributed with some old servers in my lab now and they can work. In one of the old servers, it does return IPs currently in use.<br/><NewLine><img alt=""image"" data-base62-sha1=""p2BIh89e9YHcTuaUPCCe1QYHWGr"" height=""37"" src=""https://discuss.pytorch.org/uploads/default/original/3X/a/f/af81d428b2fe9093551180df9a72e018913bed57.png"" width=""368""/><br/><NewLine>Where I am using <strong>10.0.1.101</strong> and <strong>10.0.1.102</strong> for testing the torch.distributed (old servers).</p><NewLine><p>I will figure this out in the new servers and let you know! Thank you!</p><NewLine><p>Best,<br/><NewLine>Ziyi</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>Problem solved. Like in the old server, I made <code>10.1.1.101</code> as a host in <code>/etc/hosts</code> and updated it in <code>/etc/hostname</code>. Now if I run <code>getent hosts hostname</code>, <code>10.1.1.101 host1</code> will pop up like in the screenshot below</p><NewLine><aside class=""quote no-group quote-modified"" data-post=""9"" data-topic=""66651"" data-username=""ZiyiZhu""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/z/ce7236/40.png"" width=""20""/> ZiyiZhu:</div><NewLine><blockquote><NewLine><p><img alt=""image"" data-base62-sha1=""p2BIh89e9YHcTuaUPCCe1QYHWGr"" height=""37"" src=""https://discuss.pytorch.org/uploads/default/original/3X/a/f/af81d428b2fe9093551180df9a72e018913bed57.png"" width=""368""/></p><NewLine></blockquote><NewLine></aside><NewLine><p>However, it happens to be one of the NIC port (<code>eno2</code>)'s IP. What if I want to use another ethernet port which is 10.1.2.101 for another NIC port (<code>eno3</code>), do I need to change the <code>/etc/hostname</code> every time?</p><NewLine><p>Thank you,</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looking at the code, this is not the expected behavior. It would always first try <code>GLOO_SOCKET_IFNAME</code> if that’s available. Somehow, it didn’t pick up the env var.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/945d7a7408891e25bc54a65015724f6d2de644e6/torch/csrc/distributed/c10d/init.cpp#L603-L615"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/945d7a7408891e25bc54a65015724f6d2de644e6/torch/csrc/distributed/c10d/init.cpp#L603-L615"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/945d7a7408891e25bc54a65015724f6d2de644e6/torch/csrc/distributed/c10d/init.cpp#L603-L615</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""603"" style=""counter-reset: li-counter 602 ;""><NewLine><li>char* ifnameEnv = getenv(GLOO_SOCKET_IFNAME_ENV);</li><NewLine><li>if (ifnameEnv) {</li><NewLine><li>  for (const auto&amp; iface : split(',', ifnameEnv)) {</li><NewLine><li>    options.devices.push_back(</li><NewLine><li>        ::c10d::ProcessGroupGloo::createDeviceForInterface(iface));</li><NewLine><li>  }</li><NewLine><li>} else {</li><NewLine><li>  // If no hostname is specified, this function looks up</li><NewLine><li>  // the machine's hostname and returns a device instance</li><NewLine><li>  // associated with the address that the hostname resolves to.</li><NewLine><li>  options.devices.push_back(</li><NewLine><li>      ::c10d::ProcessGroupGloo::createDefaultDevice());</li><NewLine><li>}</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><pre><code class=""lang-auto"">            char* ifnameEnv = getenv(GLOO_SOCKET_IFNAME_ENV);<NewLine>            if (ifnameEnv) {<NewLine>              for (const auto&amp; iface : split(',', ifnameEnv)) {<NewLine>                options.devices.push_back(<NewLine>                    ::c10d::ProcessGroupGloo::createDeviceForInterface(iface));<NewLine>              }<NewLine>            } else {<NewLine>              // If no hostname is specified, this function looks up<NewLine>              // the machine's hostname and returns a device instance<NewLine>              // associated with the address that the hostname resolves to.<NewLine>              options.devices.push_back(<NewLine>                  ::c10d::ProcessGroupGloo::createDefaultDevice());<NewLine>            }<NewLine></code></pre><NewLine><p>Can you try reading the <code>GLOO_SOCKET_IFNAME</code> env var immediately before <code>init_process_group</code> from Python and see if that gives the correct result?</p><NewLine><p>Let me check Gloo code.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Gloo part looks correct to me:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/facebookincubator/gloo/blob/7b58938c5d87380f88a5266035dda1041d45626e/gloo/transport/tcp/device.cc#L142-L162"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/facebookincubator/gloo/blob/7b58938c5d87380f88a5266035dda1041d45626e/gloo/transport/tcp/device.cc#L142-L162"" rel=""nofollow noopener"" target=""_blank"">facebookincubator/gloo/blob/7b58938c5d87380f88a5266035dda1041d45626e/gloo/transport/tcp/device.cc#L142-L162</a></h4><NewLine><pre class=""onebox""><code class=""lang-cc""><ol class=""start lines"" start=""142"" style=""counter-reset: li-counter 141 ;""><NewLine><li>std::shared_ptr&lt;transport::Device&gt; CreateDevice(const struct attr&amp; src) {</li><NewLine><li>  struct attr attr = src;</li><NewLine><li><NewLine></li><NewLine><li>  if (attr.iface.size() &gt; 0) {</li><NewLine><li>    // Initialize attributes using network interface name</li><NewLine><li>    lookupAddrForIface(attr);</li><NewLine><li>  } else {</li><NewLine><li>    // Initialize attributes using hostname/IP address</li><NewLine><li>    // If not already specified, use this machine's hostname</li><NewLine><li>    if (attr.hostname.size() == 0) {</li><NewLine><li>      std::array&lt;char, HOST_NAME_MAX&gt; hostname;</li><NewLine><li>      auto rv = gethostname(hostname.data(), hostname.size());</li><NewLine><li>      GLOO_ENFORCE_EQ(rv, 0);</li><NewLine><li>      attr.hostname = hostname.data();</li><NewLine><li>    }</li><NewLine><li>    lookupAddrForHostname(attr);</li><NewLine><li>  }</li><NewLine><li><NewLine></li><NewLine><li>  auto device = std::make_shared&lt;Device&gt;(attr);</li><NewLine><li>  return std::shared_ptr&lt;transport::Device&gt;(device);</li><NewLine><li>}</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Another way to test, is to use some non-exist interface, e.g.</p><NewLine><pre><code class=""lang-auto"">export GLOO_SOCKET_IFNAME=nonexist<NewLine></code></pre><NewLine><p>And then check if <code>init_process_group</code> throws the follow error for you:</p><NewLine><pre><code class=""lang-auto"">    dist.init_process_group(""gloo"", rank=rank, world_size=world_size)<NewLine>  File ""/scratch/shenli/pytorch/torch/distributed/distributed_c10d.py"", line 425, in init_process_group<NewLine>    _default_pg = _new_process_group_helper(<NewLine>  File ""/scratch/shenli/pytorch/torch/distributed/distributed_c10d.py"", line 499, in _new_process_group_helper<NewLine>    pg = ProcessGroupGloo(<NewLine>RuntimeError: [enforce fail at ../third_party/gloo/gloo/transport/tcp/device.cc:83] ifa != nullptr. Unable to find address for: nonexist<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>After I</p><NewLine><pre><code class=""lang-auto"">export GLOO_SOCKET_IFNAME=nonexist<NewLine></code></pre><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/9c09774b18f4a4382973db0c9f3e29d51edfbc9f"" href=""https://discuss.pytorch.org/uploads/default/original/3X/9/c/9c09774b18f4a4382973db0c9f3e29d51edfbc9f.png"" title=""image""><img alt=""image"" data-base62-sha1=""mgmLvbZj5Kyb2NeKWY9S5iCXZ8P"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/c/9c09774b18f4a4382973db0c9f3e29d51edfbc9f_2_10x10.png"" height=""441"" src=""https://discuss.pytorch.org/uploads/default/original/3X/9/c/9c09774b18f4a4382973db0c9f3e29d51edfbc9f.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1107×708 36.8 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>This is the error I got. Does it seem that it bypasses the <code>nonexist</code> and look at some others? If I add the master_address then it will just hang there for the second rank to come in.</p><NewLine><p>Thanks,</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>can you keep/uncomment the <code>init_method</code> line or set <code>MASTER_ADDR</code> and <code>MASTER_PORT</code>? It seems failed during Python land arg checking due to missing master addr/port, before entering C++ pybind methods.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>I guess I found the problem. If I do <code>export GLOO_SOCKET_IFNAME=nonexist</code> in the terminal then it does not become an environment variable in the Jupter Notebook. But see it in Python launched from that terminal directly.</p><NewLine><p>So I guess I have to do the other way around and set that in the JupyterNotebook explicitly? Here is the result if I do what you suggested.</p><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""15"" data-topic=""66651"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>can you keep/uncomment the <code>init_method</code> line or set <code>MASTER_ADDR</code> and <code>MASTER_PORT</code> ? It seems failed during Python land arg checking due to missing master addr/port, before entering C++ pybind methods.</p><NewLine></blockquote><NewLine></aside><NewLine><pre><code class=""lang-auto"">import torch.distributed as dist<NewLine>import os<NewLine>​<NewLine>print(os.environ.get('GLOO_SOCKET_IFNAME'))<NewLine>​<NewLine>os.environ['MASTER_ADDR'] = 'localhost'<NewLine>os.environ['MASTER_PORT'] = '23456'<NewLine>​<NewLine>os.environ['GLOO_SOCKET_IFNAME']='nonexist'<NewLine>print(os.environ.get('GLOO_SOCKET_IFNAME'))<NewLine># on rank 0<NewLine>dist.init_process_group(<NewLine>    backend = ""gloo"",<NewLine>    init_method = 'tcp://10.1.1.101:29500',<NewLine>    rank = 0,<NewLine>    world_size = 1<NewLine>)<NewLine>​<NewLine>​<NewLine>​<NewLine>None<NewLine>nonexist<NewLine>----------------------------------------------------------------------<NewLine>RuntimeError                         Traceback (most recent call last)<NewLine>&lt;ipython-input-1-ad5d77a63395&gt; in &lt;module&gt;<NewLine>     14     init_method = 'tcp://10.1.1.101:29500',<NewLine>     15     rank = 0,<NewLine>---&gt; 16     world_size = 1<NewLine>     17 )<NewLine>     18 <NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py in init_process_group(backend, init_method, timeout, world_size, rank, store, group_name)<NewLine>    401             store,<NewLine>    402             group_name=group_name,<NewLine>--&gt; 403             timeout=timeout)<NewLine>    404 <NewLine>    405     _pg_group_ranks[_default_pg] = {i: i for i in range(_default_pg.size())}<NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py in _new_process_group_helper(world_size, rank, group_ranks, backend, store, group_name, timeout)<NewLine>    469                 rank,<NewLine>    470                 world_size,<NewLine>--&gt; 471                 timeout=timeout)<NewLine>    472             _pg_map[pg] = (Backend.GLOO, store)<NewLine>    473             _pg_names[pg] = group_name<NewLine><NewLine>RuntimeError: [enforce fail at /opt/conda/conda-bld/pytorch_1587428398394/work/third_party/gloo/gloo/transport/tcp/device.cc:83] ifa != nullptr. Unable to find address for: nonexist<NewLine></code></pre><NewLine><p>Thanks,<br/><NewLine>Ziyi</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah, I see. Yep, setting it directly in notebook should work I think.</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>To follow up with the issue below, I wonder if you could let me know more about why currently we cannot use the “nccl” as a backend for the communication for RCP? We have to explicitly copy the data to CPU and then do the transmission.</p><NewLine><p>What are the concerns that the RPC does not do something similar to the DDP that has the GPU directly access to the NIC for the model parallel algorithm?</p><NewLine><p>Thank you</p><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""66651"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>Hey <a class=""mention"" href=""/u/ziyizhu"">@ZiyiZhu</a> Are you trying to run this with RPC? Currently <code>init_rpc</code> does not work together with <code>init_process_group</code> . There are work around to create non-default process groups. Or we can also add a fix to <code>init_rpc</code> if necessary. This is the tracking issue:</p><NewLine></blockquote><NewLine></aside><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>To follow up with the issue below, I wonder if you could let me know more about why currently we cannot use the “nccl” as a backend for the communication for RCP?</p><NewLine></blockquote><NewLine><p>This is because NCCL does not support p2p (send/recv) communication yet when we develop RPC. It is possible to use NCCL broadcast to mimic that send/recv, but that’s too hackish.</p><NewLine><p>The p2p comm is coming to NCCL in v2.7. When that is ready, we probably can add it to <code>ProcessGroupAgent</code> or the new <code>TensorPipeAgent</code> (the latter is a more performant RPC agent implementation and should be able to use the best channels, e.g., IB/ETH/NvLink/etc.). See this PR: <a href=""https://github.com/pytorch/pytorch/pull/35483"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/35483</a></p><NewLine><blockquote><NewLine><p>We have to explicitly copy the data to CPU and then do the transmission.</p><NewLine></blockquote><NewLine><p>For Gloo backend even if application don’t copy the tensor from CUDA to CPU, Gloo would need to do that internally anyway. Hence, this explicit copy in application not a perf limitation when using Gloo backend.</p><NewLine><p>We used to do that GPU-to-CPU copy implicitly in v1.4, but later realized that applications could run into unexpected errors if the destination device is not available on the callee. E.g., when I do <code>rpc.rpc_sync(...., args=(torch.zeros(2).to(3),))</code> and if <code>cuda:3</code> is not available on the callee, it would throw an error. So, we decided to make it explicit for applications.</p><NewLine><blockquote><NewLine><p>What are the concerns that the RPC does not do something similar to the DDP that has the GPU directly access to the NIC for the model parallel algorithm?</p><NewLine></blockquote><NewLine><p>From the API level, the difference is that DDP is supposed to run on a set of homogeneous servers, and RPC should be able to support heterogeneous clusters. So the device mismatch in RPC can be common. We are adding explicit device placement support (sth. similar to <code>map_location</code> on <code>torch.save</code> and <code>torch.load</code>) to the RPC API. <a href=""https://github.com/pytorch/pytorch/issues/33991"" rel=""nofollow noopener"">This</a> is an early issue to track. <a class=""mention"" href=""/u/osalpekar"">@osalpekar</a> is working on a design RFC for that. Look forward to hear your comments when that RFC is posted. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much for your detailed explanations! I agree that explicit can avoid lots of unexpected errors and really look forward to seeing RFC design.</p><NewLine><p>Best,<br/><NewLine>Ziyi</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Oleg_Ivanov; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> ,"REPLY_DATE 1: January 16, 2020,  5:14pm; <NewLine> REPLY_DATE 2: January 17, 2020,  6:05am; <NewLine> REPLY_DATE 3: April 24, 2020,  2:07am; <NewLine> REPLY_DATE 4: April 24, 2020,  2:33am; <NewLine> REPLY_DATE 5: April 24, 2020,  2:36am; <NewLine> REPLY_DATE 6: April 24, 2020,  2:53am; <NewLine> REPLY_DATE 7: April 24, 2020,  3:47am; <NewLine> REPLY_DATE 8: April 24, 2020,  4:13am; <NewLine> REPLY_DATE 9: April 24, 2020,  2:54pm; <NewLine> REPLY_DATE 10: April 24, 2020,  3:22pm; <NewLine> REPLY_DATE 11: April 24, 2020,  3:31pm; <NewLine> REPLY_DATE 12: April 24, 2020,  3:40pm; <NewLine> REPLY_DATE 13: April 24, 2020,  4:04pm; <NewLine> REPLY_DATE 14: April 24, 2020,  4:10pm; <NewLine> REPLY_DATE 15: April 24, 2020,  4:31pm; <NewLine> REPLY_DATE 16: April 24, 2020,  5:10pm; <NewLine> REPLY_DATE 17: April 24, 2020,  6:37pm; <NewLine> REPLY_DATE 18: April 24, 2020,  7:03pm; <NewLine> REPLY_DATE 19: April 25, 2020,  3:17am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: 1 Like; <NewLine> REPLY 16 LIKES: 1 Like; <NewLine> REPLY 17 LIKES: ; <NewLine> REPLY 18 LIKES: 1 Like; <NewLine> REPLY 19 LIKES: ; <NewLine> 
52032,Multi GPU (2080 ti) training crashes PC,2019-07-30T09:35:42.024Z,7,695,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My build:<br/><NewLine>Asrock z390 extreme4<br/><NewLine>Intel 8700k<br/><NewLine>x2 2080 ti x2<br/><NewLine>Cooler Master v1200 Platinum</p><NewLine><p>Ubuntu 18.04</p><NewLine><p>Cuda 10.0<br/><NewLine>nccl 2.4.0-2<br/><NewLine>pytorch was installed according to guide on <a href=""http://pytorch.org"" rel=""nofollow noopener"">pytorch.org</a></p><NewLine><p>So I’ve got something interesting: pc crashes right after I try running imagenet script for multi gpu from official pytorch repository. It doesn’t crash pc if I start training with apex mixed precision. Training on a single 2080 also didn’t cause reboot.</p><NewLine><p>What didn’t work:</p><NewLine><ol><NewLine><li>decreasing batch size</li><NewLine><li>limiting power consumption of gpu’s via nvidia-smi</li><NewLine><li>changing motherboard, cpu, power supply</li><NewLine><li>changing 2080 ti vendor</li><NewLine></ol><NewLine><p>For some reason everything worked after I switched both 2080 ti’s with 1080 ti’s. So it seems pytorch (or some nvidia software) isn’t fully compatible with multiple 2080 ti’s? Has anyone encountered this?</p><NewLine></div>",https://discuss.pytorch.org/u/mstrfx,(Petya),mstrfx,"July 30, 2019,  9:35am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Two 2080TIs should work, so I think it might be a hardware issue.<br/><NewLine>However, it seems you’ve already changed a lot of parts of your system.<br/><NewLine>Regarding point 3 and 4, it seems you completely rebuilt your system.</p><NewLine><p>Does your code only crash using the ImageNet example or also a very small model, e.g. a single linear layer and <code>DataParallel</code>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I can corroborate. It happens with me too. torch distributed data parallel fails on 2080Ti with &gt;1 gpu, however works well with titan-x, titan-xp or 1080Ti.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Same here, fails on two 2080Ti, works with two 1080Ti. In the script, I just load a <code>resnet50</code> from <code>torchvision.models</code> and do inference on it. It does not crash however, if I run the same script twice at the same time, each using one GPU. So it does not seem to a power supply issue</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Okay, so now I have done some runs with the following code:</p><NewLine><pre><code class=""lang-auto"">import os<NewLine><NewLine>from tqdm import tqdm<NewLine>from torchvision.models import resnet50, resnet18<NewLine>from torch.utils.data import DataLoader<NewLine>from torch.utils.data import Dataset<NewLine>import torch<NewLine>import torch.nn as nn<NewLine><NewLine>n = 1000000<NewLine><NewLine>class RandomDs(Dataset):<NewLine>    def __init__(self, ):<NewLine>        pass<NewLine><NewLine>    def __len__(self):<NewLine>        return n<NewLine><NewLine>    def __getitem__(self, index):<NewLine>        return torch.rand(3, 512, 256)<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine><NewLine>    dataset = RandomDs()<NewLine>    data_loader = DataLoader(dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=False)<NewLine><NewLine><NewLine>    model = resnet50()<NewLine><NewLine>    # os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""<NewLine>    device = torch.device('cuda:0')<NewLine>    model = nn.DataParallel(model)<NewLine>    <NewLine>    model.to(device)<NewLine><NewLine>    with torch.no_grad():<NewLine>        for batch in tqdm(data_loader):<NewLine>            batch = batch.to(device)<NewLine>            model(batch)<NewLine><NewLine></code></pre><NewLine><p>I changed the <code>input shape to the network</code>, <code>batch size</code>, <code>number of workers</code>, <code>model type</code>, <code>torch version</code>, and whether it runs with <code>DataParallel</code> or as two scripts (so two scripts, running at the same time, so that both GPUs are in use). These are the results:</p><NewLine><div class=""md-table""><NewLine><table><NewLine><thead><NewLine><tr><NewLine><th>input shape</th><NewLine><th>batch_size</th><NewLine><th>num_workers</th><NewLine><th>model</th><NewLine><th>parallel</th><NewLine><th>torch version</th><NewLine><th>crash</th><NewLine><th>memory (GB each)</th><NewLine></tr><NewLine></thead><NewLine><tbody><NewLine><tr><NewLine><td>3, 256, 256</td><NewLine><td>128</td><NewLine><td>4</td><NewLine><td>resnet50</td><NewLine><td>yes</td><NewLine><td>1.2.0</td><NewLine><td>No</td><NewLine><td>~2.3</td><NewLine></tr><NewLine><tr><NewLine><td>3, 512, 256</td><NewLine><td>128</td><NewLine><td>4</td><NewLine><td>resnet50</td><NewLine><td>yes</td><NewLine><td>1.2.0</td><NewLine><td>Yes</td><NewLine><td>~3.3</td><NewLine></tr><NewLine><tr><NewLine><td>3, 256, 256</td><NewLine><td>256</td><NewLine><td>4</td><NewLine><td>resnet50</td><NewLine><td>yes</td><NewLine><td>1.2.0</td><NewLine><td>Yes</td><NewLine><td>~3.3</td><NewLine></tr><NewLine><tr><NewLine><td>3, 256, 256</td><NewLine><td>256</td><NewLine><td>4</td><NewLine><td>resnet50</td><NewLine><td>two scripts</td><NewLine><td>1.2.0</td><NewLine><td>No</td><NewLine><td>~5.3 (single)</td><NewLine></tr><NewLine><tr><NewLine><td>3, 256, 256</td><NewLine><td>256</td><NewLine><td>16</td><NewLine><td>resnet50</td><NewLine><td>yes</td><NewLine><td>1.2.0</td><NewLine><td>Yes</td><NewLine><td>~3.3</td><NewLine></tr><NewLine><tr><NewLine><td>3, 256, 256</td><NewLine><td>256</td><NewLine><td>4</td><NewLine><td>resnet18</td><NewLine><td>yes</td><NewLine><td>1.2.0</td><NewLine><td>Yes</td><NewLine><td>~2.2</td><NewLine></tr><NewLine><tr><NewLine><td>3, 512, 256</td><NewLine><td>128</td><NewLine><td>4</td><NewLine><td>resnet50</td><NewLine><td>yes</td><NewLine><td>1.4.0</td><NewLine><td>Yes</td><NewLine><td>~3.3</td><NewLine></tr><NewLine></tbody><NewLine></table><NewLine></div><p>It seems to be memory related, but given that I am using 2*2080Ti and I have 64GB ram, it’s not an OOM. Any ideas what else I can test?</p><NewLine><p>I am using cuda 10.2 and tried pytroch version 1.2 and 1.4</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Does your machine restart or how does the crash look for you?<br/><NewLine>If that’s the case, was PyTorch (or other libs) working before or is it a new setup?<br/><NewLine>Did you run some stress tests on the devices?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>The machine shuts down immediately and then restarts. It is more or less a new system: It has been used for other tasks before, but not for ML. I’ve run <a href=""http://wili.cc/blog/gpu-burn.html"" rel=""nofollow noopener"">GPU-burn</a> for an hour with Double precision and tensor cores enabled and it seemed to work fine.</p><NewLine><p>The only times when it doesn’t crash is when I am using a small input (shape/bs) or when I’m not using <code>DataParallel</code>. Could this be related to the communication between the GPUs?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could run e.g. <a href=""https://github.com/NVIDIA/nccl-tests"">nccl-test</a> to check the communication.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Seems fine:</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/bf9c4c8d4cb09f9166f45a55a46312e16be8a9bc"" href=""https://discuss.pytorch.org/uploads/default/original/3X/b/f/bf9c4c8d4cb09f9166f45a55a46312e16be8a9bc.png"" title=""Screenshot from 2020-04-21 13-45-54""><img alt=""Screenshot from 2020-04-21 13-45-54"" data-base62-sha1=""rl445EaG2uR7lCXBy5O3S2aG6ra"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/f/bf9c4c8d4cb09f9166f45a55a46312e16be8a9bc_2_10x10.png"" height=""469"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/f/bf9c4c8d4cb09f9166f45a55a46312e16be8a9bc_2_690x469.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/f/bf9c4c8d4cb09f9166f45a55a46312e16be8a9bc_2_690x469.png, https://discuss.pytorch.org/uploads/default/original/3X/b/f/bf9c4c8d4cb09f9166f45a55a46312e16be8a9bc.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/b/f/bf9c4c8d4cb09f9166f45a55a46312e16be8a9bc.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screenshot from 2020-04-21 13-45-54</span><span class=""informations"">957×651 101 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>I’m really out of ideas on what to test. The only way I can replicate the crashes or find any abnormalities is by using PyTorch DataParallel. GPU burn runs fine, nccl-test runs fine, utilizing both GPUs at the same time individually is fine. What else could I do in order to identify the cause of the problem?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you check system logs to see if there is something there mentioning why the machine restarted? Usually on Linux there might be some information in <code>/var/log/messages</code>.</p><NewLine><p>Another option might be to post on Nvidia forums to check if there might be some system issue.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s another good idea.<br/><NewLine><a class=""mention"" href=""/u/aljost"">@AljoSt</a> try to grab for <code>Xid</code> and post the result here, please.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could not find anything interesting in kernlog, syslog or dmesg. But what I did find was this <a href=""https://github.com/pytorch/pytorch/issues/3022"" rel=""nofollow noopener"">github issue</a> where people have a similar problem. GPU burn etc cannot make the system crash and PSU is powerful enough (on paper). They say, that PyTorch causes a short big power surge which can cause the PSU to fail, even if it’s big enough to support the system on heavy load in other circumstances (gpu burn etc).</p><NewLine><p>I put the 2 2080Tis into a system with a 1600w PSU and they seemed to work fine. The PSU in the other system had 850w. Obv. I cannot guarantee that this is the cause/solution due to many variables being changed but as other people have had similar experiences, I will take it for now (and will post again here if there are new developments). Thanks for your help!</p><NewLine><p>(and btw, since many people have the same problem which seems to originate in PyTorch, I think it would be good if the relevant developers could have a look into that)</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""12"" data-topic=""52032"" data-username=""AljoSt""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/aljost/40/23042_2.png"" width=""20""/> AljoSt:</div><NewLine><blockquote><NewLine><p>(and btw, since many people have the same problem which seems to originate in PyTorch, I think it would be good if the relevant developers could have a look into that)</p><NewLine></blockquote><NewLine></aside><NewLine><p>Since a 2080Ti can take &gt;300W, a 850W PSU might not be enough for the system.<br/><NewLine>I’m not sure, if artificially limiting the GPU is a valid use case.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah interesting, I didn’t know that, thank you. I have difficulties finding the max power draw (the highest I saw in some random blog post was around 330w). Is it vendor specific (so would it differ say between MSI 2080Ti and EVGA 2080TI?) or is there a “fixed” number?</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>It should be vendor specific, as some vendors might increase the clock speeds and add a better cooling solution to the device. You won’t see much difference, but it’s not strictly a single number.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your help!</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi guys <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/></p><NewLine><p>Just wanted to let you know it is not related to only pytorch framework. Happens also with tensorflow.</p><NewLine><p>It feels like modern power supplies can’t handle 300w + 300w simultaneously during training. I got double 2080 ti with another power supply (with cheap 2$ synchronizer) and everything works like a charm.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hay; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/AljoSt; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/AljoSt; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/AljoSt; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/AljoSt; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/AljoSt; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/AljoSt; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/AljoSt; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/mstrfx; <NewLine> ,"REPLY_DATE 1: July 30, 2019,  9:41pm; <NewLine> REPLY_DATE 2: November 13, 2019,  2:13am; <NewLine> REPLY_DATE 3: April 20, 2020,  9:57am; <NewLine> REPLY_DATE 4: April 20, 2020,  1:47pm; <NewLine> REPLY_DATE 5: April 20, 2020,  2:13pm; <NewLine> REPLY_DATE 6: April 20, 2020,  2:27pm; <NewLine> REPLY_DATE 7: April 20, 2020, 10:34pm; <NewLine> REPLY_DATE 8: April 21, 2020, 12:51pm; <NewLine> REPLY_DATE 9: April 21, 2020,  7:51pm; <NewLine> REPLY_DATE 10: April 22, 2020,  7:11am; <NewLine> REPLY_DATE 11: April 23, 2020, 10:43am; <NewLine> REPLY_DATE 12: April 24, 2020,  6:56am; <NewLine> REPLY_DATE 13: April 24, 2020,  8:29am; <NewLine> REPLY_DATE 14: April 24, 2020,  8:34am; <NewLine> REPLY_DATE 15: April 24, 2020,  9:18am; <NewLine> REPLY_DATE 16: April 24, 2020, 10:38am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> 
78025,Multiple GPU model getting RuntimeError: Caught RuntimeError in replica 0 on device 0,2020-04-23T10:27:50.865Z,0,143,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I’m trying to move a single GPU model to a machine with 4 GPUs, only I’m on a timeline to use this machine.</p><NewLine><p>I’m getting the following error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-28-4b69b40dcdef&gt; in &lt;module&gt;<NewLine>     18             break<NewLine>     19 <NewLine>---&gt; 20         y_pred = combined_model(image, numerical_data, categorical_data)<NewLine>     21         single_loss = criterion(y_pred, label)<NewLine>     22 <NewLine><NewLine>~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)<NewLine>    530             result = self._slow_forward(*input, **kwargs)<NewLine>    531         else:<NewLine>--&gt; 532             result = self.forward(*input, **kwargs)<NewLine>    533         for hook in self._forward_hooks.values():<NewLine>    534             hook_result = hook(self, input, result)<NewLine><NewLine>~/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py in forward(self, *inputs, **kwargs)<NewLine>    150             return self.module(*inputs[0], **kwargs[0])<NewLine>    151         replicas = self.replicate(self.module, self.device_ids[:len(inputs)])<NewLine>--&gt; 152         outputs = self.parallel_apply(replicas, inputs, kwargs)<NewLine>    153         return self.gather(outputs, self.output_device)<NewLine>    154 <NewLine><NewLine>~/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py in parallel_apply(self, replicas, inputs, kwargs)<NewLine>    160 <NewLine>    161     def parallel_apply(self, replicas, inputs, kwargs):<NewLine>--&gt; 162         return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])<NewLine>    163 <NewLine>    164     def gather(self, outputs, output_device):<NewLine><NewLine>~/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py in parallel_apply(modules, inputs, kwargs_tup, devices)<NewLine>     83         output = results[i]<NewLine>     84         if isinstance(output, ExceptionWrapper):<NewLine>---&gt; 85             output.reraise()<NewLine>     86         outputs.append(output)<NewLine>     87     return outputs<NewLine><NewLine>~/miniconda3/lib/python3.7/site-packages/torch/_utils.py in reraise(self)<NewLine>    392             # (https://bugs.python.org/issue2651), so we work around it.<NewLine>    393             msg = KeyErrorMessage(msg)<NewLine>--&gt; 394         raise self.exc_type(msg)<NewLine><NewLine>RuntimeError: Caught RuntimeError in replica 0 on device 0.<NewLine>Original Traceback (most recent call last):<NewLine>  File ""/home/scott.farmers/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 60, in _worker<NewLine>    output = module(*input, **kwargs)<NewLine>  File ""/home/scott.farmers/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""&lt;ipython-input-25-86287e73cc1f&gt;"", line 34, in forward<NewLine>    x = torch.cat(embeddings, 1)<NewLine>RuntimeError: cuda runtime error (710) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/THC/THCGeneral.cpp:313<NewLine></code></pre><NewLine><p>Here is my model:</p><NewLine><pre><code class=""lang-auto"">class Image_Embedd(nn.Module):<NewLine><NewLine>    def __init__(self, embedding_size):<NewLine>        '''<NewLine>        Args<NewLine>        ---------------------------<NewLine>        embedding_size: Contains the embedding size for the categorical columns<NewLine>        num_numerical_cols: Stores the total number of numerical columns<NewLine>        output_size: The size of the output layer or the number of possible outputs.<NewLine>        layers: List which contains number of neurons for all the layers.<NewLine>        p: Dropout with the default value of 0.5<NewLine>        <NewLine>        '''<NewLine>        super().__init__()    <NewLine>        <NewLine>        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])<NewLine>        self.embedding_dropout = nn.Dropout(p = .04)<NewLine>        <NewLine>        self.cnn = models.resnet50(pretrained=False).cuda()<NewLine>        <NewLine>        self.cnn.fc = nn.Linear(self.cnn.fc.in_features, 1000)<NewLine>        self.fc1 = nn.Linear(1000, 1077)<NewLine>        self.fc2 = nn.Linear(1077, 128)<NewLine>        self.fc3 = nn.Linear(128, 2)<NewLine>        <NewLine>        <NewLine>    #define the foward method<NewLine>    def forward(self, image, x_numerical, x_categorical):<NewLine>        <NewLine>        embeddings = []<NewLine>        for i, e in enumerate(self.all_embeddings):<NewLine>            embeddings.append(e(x_categorical[:,i]))<NewLine>            <NewLine>        x = torch.cat(embeddings, 1)<NewLine>        x = self.embedding_dropout(x)<NewLine>        x1 = self.cnn(image)<NewLine>        x2 = x_numerical<NewLine>        <NewLine>        x3 = torch.cat((x1, x2), dim = 1)<NewLine>        x4 = torch.cat((x, x3), dim = 1)<NewLine>        x4 = F.relu(self.fc2(x4))<NewLine>        x4 = self.fc3(x4)<NewLine>        x4 = F.log_softmax(x4)<NewLine>        return x4<NewLine><NewLine>device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")<NewLine>torch.manual_seed(101)<NewLine>combined_model = Image_Embedd(embedding_size=train_categorical_embedding_sizes)<NewLine>criterion = torch.nn.NLLLoss()<NewLine>optimizer = torch.optim.Adam(combined_model.parameters(), lr=0.001)<NewLine>scheduler = ReduceLROnPlateau(optimizer, 'min', patience = 4, verbose = True, min_lr = .00000001)<NewLine>exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)<NewLine>if torch.cuda.device_count() &gt; 1:<NewLine>    print(""Let's use"", torch.cuda.device_count(), ""GPUs!"")<NewLine>    combined_model = nn.DataParallel(combined_model)<NewLine>combined_model.to(device)<NewLine><NewLine>epochs = 5000<NewLine>aggregated_losses = []<NewLine><NewLine>max_trn_batch = 11053<NewLine><NewLine>for i in range(epochs):<NewLine>    for b, (image, label, policy, numerical_data, categorical_data) in enumerate(train_loader):<NewLine>        image = image.to(device)<NewLine>        label = label.to(device)<NewLine>        numerical_data = numerical_data.to(device)<NewLine>        categorical_data = categorical_data.to(device)<NewLine>        <NewLine>        #count batches<NewLine>        b += 1<NewLine>        <NewLine>        #throttle teh batches<NewLine>        if b == max_trn_batch:<NewLine>            break<NewLine>        <NewLine>        y_pred = combined_model(image, numerical_data, categorical_data)<NewLine>        single_loss = criterion(y_pred, label)<NewLine>        <NewLine>        # statistics<NewLine>        print(f'epoch: {i:3}, batch: {b:3}, loss: {single_loss.item():10.8f}')<NewLine><NewLine>        optimizer.zero_grad()<NewLine>        single_loss.backward()<NewLine>        optimizer.step()<NewLine>    <NewLine>    aggregated_losses.append(single_loss.cpu().data.numpy())<NewLine>    scheduler.step(single_loss)<NewLine><NewLine><NewLine>print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')<NewLine></code></pre><NewLine><p>I’m not sure what I’m doing wrong.  I followed or try to follow the tutorial here:<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html</a></p><NewLine></div>",https://discuss.pytorch.org/u/Jordan_Howell,(Jordan Howell),Jordan_Howell,"April 23, 2020, 10:27am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like some layers of the model lives on GPU and others live on CPU. Is this intentional? <code>DataParallel</code> does not support mixed CPU-GPU model, all layers of the same model need to live on the same GPU.</p><NewLine><p>If you have multi-GPU model, e.g., some layers live on <code>cuda:0</code> and others live on <code>cuda:1</code>, you can try <code>DistributedDataParallel</code>. Check out <a href=""https://github.com/pytorch/pytorch/blob/8d6a8d2b3fd2a6ec788378843fc518824acf274b/torch/nn/parallel/distributed.py#L157"" rel=""nofollow noopener"">this</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>nope.  i’m reworking it now. Thank you for pointing that out.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Jordan_Howell; <NewLine> ,"REPLY_DATE 1: April 23, 2020,  4:49pm; <NewLine> REPLY_DATE 2: April 23, 2020,  4:42pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
77832,Same parameters in different models,2020-04-22T09:50:34.609Z,1,88,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">import concurrent.futures<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.optim as optimizer<NewLine>from torch.distributions import Categorical<NewLine><NewLine>class mymodel1(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(mymodel1,self).__init__()<NewLine>        self.weight = nn.Linear(3,2)<NewLine>        <NewLine>    def forward(self, X):<NewLine>        out = self.weight(X)<NewLine>        out = nn.Softmax(dim = 0)(out)<NewLine>        return out<NewLine><NewLine>class mymodel2(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(mymodel2,self).__init__()<NewLine>        self.weight = nn.Linear(3,2)<NewLine>        <NewLine>    def forward(self, X):<NewLine>        out = self.weight(X)<NewLine>        out = nn.Softmax(dim = 0)(out)<NewLine>        return out<NewLine><NewLine><NewLine>class mymodel3(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(mymodel3,self).__init__()<NewLine>        self.weight = nn.Linear(3,2)<NewLine>        <NewLine>    def forward(self, X):<NewLine>        out = self.weight(X)<NewLine>        out = nn.Softmax(dim = 0)(out)<NewLine>        return out<NewLine><NewLine><NewLine>        <NewLine>def doTrain(model, X):     <NewLine>    a1 = model()<NewLine>    return list(a1.parameters())<NewLine><NewLine>X = torch.randn(12,3)<NewLine><NewLine><NewLine>updatedParams = []<NewLine>results = []<NewLine><NewLine><NewLine>with concurrent.futures.ProcessPoolExecutor() as executor:<NewLine>    f1 = executor.submit(doTrain, mymodel1, X[0*4:(0+1)*4])<NewLine>    f2 = executor.submit(doTrain, mymodel2, X[1*4:(1+1)*4])<NewLine>    f3 = executor.submit(doTrain, mymodel3, X[2*4:(2+1)*4])<NewLine>    <NewLine><NewLine>print(f1.result())<NewLine>print(f2.result())<NewLine>print(f3.result())<NewLine></code></pre><NewLine><p>Output</p><NewLine><pre><code class=""lang-auto"">[Parameter containing:<NewLine>tensor([[-0.3413, -0.4291,  0.0850],<NewLine>        [-0.4270, -0.4523, -0.3700]], requires_grad=True), Parameter containing:<NewLine>tensor([0.5327, 0.2588], requires_grad=True)]<NewLine>[Parameter containing:<NewLine>tensor([[-0.3413, -0.4291,  0.0850],<NewLine>        [-0.4270, -0.4523, -0.3700]], requires_grad=True), Parameter containing:<NewLine>tensor([0.5327, 0.2588], requires_grad=True)]<NewLine>[Parameter containing:<NewLine>tensor([[-0.3413, -0.4291,  0.0850],<NewLine>        [-0.4270, -0.4523, -0.3700]], requires_grad=True), Parameter containing:<NewLine>tensor([0.5327, 0.2588], requires_grad=True)]<NewLine></code></pre><NewLine><p>Can somebody tell me why I am getting same parameters returned from the different processes although the models are different ?</p><NewLine></div>",https://discuss.pytorch.org/u/circa,,circa,"April 22, 2020,  9:50am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This might be because the per-process RNG are initialized with the same seed by default? Can you try if manually changing seed using <a href=""https://pytorch.org/docs/stable/random.html#torch.random.manual_seed"" rel=""nofollow noopener""><code>manual_seed</code></a> solves the problem?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks,<br/><NewLine>I used<br/><NewLine><code>torch.manual_seed(num)</code><br/><NewLine>with a different value for num in the <strong>init</strong> method of each of the  models and that did it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/circa; <NewLine> ,"REPLY_DATE 1: April 22, 2020,  2:26pm; <NewLine> REPLY_DATE 2: April 23, 2020,  9:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
77891,DataParallel vs DistributedDataParallel,2020-04-22T16:42:01.151Z,2,172,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>what is the difference between</p><NewLine><p><code>model = nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])</code></p><NewLine><p>and</p><NewLine><p><code>model = nn.DataParallel(model, device_ids=[args.gpu])</code></p><NewLine><p>?</p><NewLine></div>",https://discuss.pytorch.org/u/Deeply,(Deeply),Deeply,"April 22, 2020,  4:42pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>DistributedDataParallel</code> is multi-process parallelism, where those processes can live on different machines. So, for <code>model = nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])</code>, this creates one DDP instance on one process, there could be other DDP instances from other processes in the same group working together with this DDP instance.  Check out this <a href=""https://pytorch.org/docs/master/notes/ddp.html"" rel=""nofollow noopener"">https://pytorch.org/docs/master/notes/ddp.html</a></p><NewLine><p><code>DataParallel</code> is single-process multi-thread parallelism. It’s basically a wrapper of scatter + paralllel_apply + gather. For <code>model = nn.DataParallel(model, device_ids=[args.gpu])</code>, since it only works on a single device, it’s the same as just using the original model on GPU with id <code>args.gpu</code>. See <a href=""https://github.com/pytorch/pytorch/blob/df8d6eeb19423848b20cd727bc4a728337b73829/torch/nn/parallel/data_parallel.py#L153"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/df8d6eeb19423848b20cd727bc4a728337b73829/torch/nn/parallel/data_parallel.py#L153</a></p><NewLine><p><code>DataParallel</code> is easier to use, as you don’t need additional code to setup process groups, and a one-line change should be sufficient to enable it.</p><NewLine><p><code>DistributedDataParallel</code> is faster and scalable. If you have multiple GPUs or machines and care about training speed, <code>DistributedDataParallel</code> should be the way to go.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>But <code>DataParallel</code> also enable multiple GPUs, in one node/machine, right?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, but <code>DataParallel</code> cannot scale beyond one machine. It is slower than <code>DistributedDataParallel</code> even in a single machine with multiple GPUs due to GIL contention across multiple threads and the extra overhead introduced by scatter and gather and per-iteration model replication.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Deeply; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: April 24, 2020, 12:02pm; <NewLine> REPLY_DATE 2: April 22, 2020,  8:47pm; <NewLine> REPLY_DATE 3: April 24, 2020, 12:02pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
77758,Understanding torch.distributed.DistributedDataParallel from a torchvision&rsquo;s reference example,2020-04-21T21:26:26.383Z,3,188,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I would like to ask some questions regarding the DDP code used in the <code>torchvision</code>'s <a href=""https://github.com/pytorch/vision/blob/master/references/classification/train.py"" rel=""nofollow noopener"">reference example on classification</a>. An example of using this script is given as follows, on a machine with 8 GPUs:</p><NewLine><pre><code class=""lang-bash"">python -m torch.distributed.launch --nproc_per_node=8 --use_env train.py --model resnext50_32x4d --epochs 100<NewLine></code></pre><NewLine><p>My first question concerns the saving and loading of checkpoints.</p><NewLine><p><a href=""https://github.com/pytorch/vision/blob/d6ee8757eca7b74b98e5f0d434a565eb7b1c410b/references/classification/train.py#L211"" rel=""nofollow noopener"">This</a> is how a checkpoint is saved in the script:</p><NewLine><pre><code class=""lang-auto"">checkpoint = {<NewLine>    'model': model_without_ddp.state_dict(),<NewLine>    'optimizer': optimizer.state_dict(),<NewLine>    'lr_scheduler': lr_scheduler.state_dict(),<NewLine>    'epoch': epoch,<NewLine>    'args': args}<NewLine>utils.save_on_master(<NewLine>    checkpoint,<NewLine>    os.path.join(args.output_dir, 'model_{}.pth'.format(epoch)))<NewLine>utils.save_on_master(<NewLine>    checkpoint,<NewLine>    os.path.join(args.output_dir, 'checkpoint.pth'))<NewLine></code></pre><NewLine><p>But in the <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">DDP tutorial</a>, it seems necessary that <code>torch.distributed.barrier()</code> is called somewhere:</p><NewLine><pre><code class=""lang-auto""># Use a barrier() to make sure that process 1 loads the model after process 0 saves it.<NewLine>dist.barrier()<NewLine>...<NewLine># Use a barrier() to make sure that all processes have finished reading the checkpoint<NewLine>dist.barrier()<NewLine></code></pre><NewLine><p>Why is <code>dist.barrier()</code> not necessary in the above reference example?</p><NewLine><p>My second question is about the validation stage.</p><NewLine><p><a href=""https://github.com/pytorch/vision/blob/d6ee8757eca7b74b98e5f0d434a565eb7b1c410b/references/classification/train.py#L204"" rel=""nofollow noopener"">This</a> is how it’s done in the script:</p><NewLine><pre><code class=""lang-python"">for epoch in range(args.start_epoch, args.epochs):<NewLine>    if args.distributed:<NewLine>        train_sampler.set_epoch(epoch)<NewLine>    train_one_epoch(model, criterion, optimizer, data_loader, device, epoch, args.print_freq, args.apex)<NewLine>    lr_scheduler.step()<NewLine>    evaluate(model, criterion, data_loader_test, device=device)<NewLine></code></pre><NewLine><p>Doesn’t this mean that the <code>evaluate()</code> function is called on <strong>all</strong> the processes (i.e. all the GPUs in this case)? Shouldn’t we rather do something like this:</p><NewLine><pre><code class=""lang-auto"">for epoch in range(args.start_epoch, args.epochs):<NewLine>    if args.distributed:<NewLine>        train_sampler.set_epoch(epoch)<NewLine>    train_one_epoch(model, criterion, optimizer, data_loader, device, epoch, args.print_freq, args.apex)<NewLine>    lr_scheduler.step()<NewLine>    if torch.distributed.get_rank() == 0: # master<NewLine>        evaluate(model, criterion, data_loader_test, device=device)<NewLine>        # save checkpoint here as well<NewLine></code></pre><NewLine><p>But then, again, shouldn’t we wait, using <code>dist.barrier()</code>, for all the processes to finish the computations and for the master to gather the gradients, before evaluating the model?</p><NewLine><p>Thank you very much in advance for your help!</p><NewLine></div>",https://discuss.pytorch.org/u/f10w,,f10w,"April 22, 2020, 11:54am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Why is  <code>dist.barrier()</code>  not necessary in the above reference example?</p><NewLine></blockquote><NewLine><p>IIUC, the torchvision example only saves the checkpoint to file in epoch but is not reading from it unless it is recovering from a crash? In that case it is not a hard requirement to perform the barrier, because</p><NewLine><ol><NewLine><li>it does not need to ensure non-master processes are reading stale checkpoints.</li><NewLine><li>non-master processes will just block on DDP backward (AllReduce) and waiting for the master to join.</li><NewLine></ol><NewLine><blockquote><NewLine><p>Doesn’t this mean that the  <code>evaluate()</code>  function is called on  <strong>all</strong>  the processes (i.e. all the GPUs in this case)?</p><NewLine></blockquote><NewLine><p>cc <a class=""mention"" href=""/u/fmassa"">@fmassa</a> for this implementation</p><NewLine><blockquote><NewLine><p>But then, again, shouldn’t we wait, using  <code>dist.barrier()</code> , for all the processes to finish the computations and for the master to gather the gradients, before evaluating the model?</p><NewLine></blockquote><NewLine><p>The gradients are synchronized in DDP backward using AllReduce operations. So, there is no need to add another barrier here to do that. As soon as <code>loss.backward()</code> returns, the local gradients should be representing the global average. However, it might need a <code>barrier</code> here for a different reason. If the <code>evaluate()</code> step takes too long, non-master processes could timeout on <code>AllReduce</code>. If that happens, <code>barrier</code> might help.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> Thanks for your prompt reply!</p><NewLine><blockquote><NewLine><p>IIUC, the torchvision example only saves the checkpoint to file in epoch but is not reading from it unless it is recovering from a crash?</p><NewLine></blockquote><NewLine><p>Well, the script has a resume code as follows:</p><NewLine><pre><code class=""lang-auto"">if args.resume:<NewLine>    checkpoint = torch.load(args.resume, map_location='cpu')<NewLine>    model_without_ddp.load_state_dict(checkpoint['model'])<NewLine>    optimizer.load_state_dict(checkpoint['optimizer'])<NewLine>    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])<NewLine>    args.start_epoch = checkpoint['epoch'] + 1<NewLine></code></pre><NewLine><p>so I guess it does what you’ve described, which is a usual scenario. But then are you saying that the <code>demo_checkpoint()</code> example given in the <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">tutorial</a> handles a different scenario (other than resuming a training)?</p><NewLine><blockquote><NewLine><p>The gradients are synchronized in DDP backward using AllReduce operations. So, there is no need to add another barrier here to do that. As soon as  <code>loss.backward()</code>  returns, the local gradients should be representing the global average.</p><NewLine></blockquote><NewLine><p>Thanks! This has cleared up a lot of things for me. I guess the gradients are averaged? In this case, it seems that the learning rate should be scaled up by the number of GPUs.</p><NewLine><p>Besides, the tutorial also notes that</p><NewLine><blockquote><NewLine><p>if training starts from random parameters, you might want to make sure that all DDP processes use the same initial values. Otherwise, global gradient synchronizes will not make sense.</p><NewLine></blockquote><NewLine><p>but I don’t see this being taken into account anywhere in the reference script.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Well, the script has a resume code as follows:</p><NewLine></blockquote><NewLine><p>Not 100% sure about how this example would be used. <a class=""mention"" href=""/u/fmassa"">@fmassa</a> would know more.  Given the code, it looks the resume mode is designed for starting from pre-trained models or resume from crash. In these cases, the checkpoint file is ready before launching the script, so it should be fine.</p><NewLine><blockquote><NewLine><p>you saying that the  <code>demo_checkpoint()</code>  example given in the <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">tutorial</a> handles a different scenario</p><NewLine></blockquote><NewLine><p>It is not targeting any specific use case, just wanted to make sure the example code can run as is. The main information it tries to convey is that applications need to make sure checkpoints are ready before loading them. We previous saw users running into weird errors caused by reading too soon.</p><NewLine><blockquote><NewLine><p>if training starts from random parameters, you might want to make sure that all DDP processes use the same initial values. Otherwise, global gradient synchronizes will not make sense.</p><NewLine></blockquote><NewLine><p>DDP handles this by broadcasting model weights from rank 0 to others at construction time. However, if the application modified model weights after constructing DDP and if that resulted in inconsistent weights across processes, DDP won’t be able to recover, as the broadcast only happens once in ctor.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> Great. Thank you so much for the explanations!<br/><NewLine>I hope <a class=""mention"" href=""/u/fmassa"">@fmassa</a> could join the discussion and clarify the points for which you mentioned him earlier, especially the one related to <code>evaluate()</code>. I tested the code and it seems that this function is called only once across the processes.</p><NewLine><p>The training loop looks like this:</p><NewLine><pre><code class=""lang-auto"">for epoch in range(args.start_epoch, args.epochs):<NewLine>    if args.distributed:<NewLine>        train_sampler.set_epoch(epoch)<NewLine>    train_one_epoch(...)<NewLine>    lr_scheduler.step()<NewLine>    evaluate(model, criterion, data_loader_test, device=device)<NewLine></code></pre><NewLine><p>where the evaluation function looks like:</p><NewLine><pre><code class=""lang-auto"">def evaluate(model, criterion, data_loader, device, print_freq=100):<NewLine>    model.eval()<NewLine>    metric_logger = utils.MetricLogger(delimiter=""  "")<NewLine>    header = 'Test:'<NewLine>    with torch.no_grad():<NewLine>        for image, target in metric_logger.log_every(data_loader, print_freq, header):<NewLine>            ...<NewLine>    # gather the stats from all processes<NewLine>    metric_logger.synchronize_between_processes()<NewLine><NewLine>    print(' * Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f}'<NewLine>          .format(top1=metric_logger.acc1, top5=metric_logger.acc5))<NewLine>    return metric_logger.acc1.global_avg<NewLine></code></pre><NewLine><p>This function has some <code>print()</code> to display the accuracy. In my experiment, that string is only displayed once, which means the function is called only once. Why? There is no <code>is_main_process()</code> check. Why isn’t this function called on all processes? I’m confused…</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve just realized that we shouldn’t wrap the evaluation phase inside <code>if torch.distributed.get_rank() == 0:</code>, because <code>data_loader_test</code> also splits the data across all the processes.</p><NewLine><p>And for the <code>print()</code> part, <a href=""https://github.com/pytorch/vision/blob/d6ee8757eca7b74b98e5f0d434a565eb7b1c410b/references/classification/utils.py#L194"" rel=""nofollow noopener"">this code</a> explains why the message is displayed only once.</p><NewLine><p>So now I understand what <a class=""mention"" href=""/u/fmassa"">@fmassa</a> did. Thanks.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/f10w; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/f10w; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/f10w; <NewLine> ,"REPLY_DATE 1: April 21, 2020,  9:41pm; <NewLine> REPLY_DATE 2: April 21, 2020, 10:27pm; <NewLine> REPLY_DATE 3: April 21, 2020, 10:56pm; <NewLine> REPLY_DATE 4: April 22, 2020, 11:47am; <NewLine> REPLY_DATE 5: April 22, 2020,  4:16pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
55391,Module &lsquo;torch.distributed&rsquo; has no attribute &lsquo;is_initialized&rsquo;,2019-09-06T20:56:27.656Z,1,1014,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am running inference using mmdetection (<a href=""https://github.com/open-mmlab/mmdetection"" rel=""nofollow noopener"">https://github.com/open-mmlab/mmdetection</a>) and I get the above error for this piece of code;</p><NewLine><pre><code class=""lang-auto"">    model = init_detector(""faster_rcnn_r50_fpn_1x.py"", ""faster_rcnn_r50_fpn_1x_20181010-3d1b3351.pth"", device='cuda:0')<NewLine>    img = 'intersection_unlabeled/frame0.jpg'<NewLine>    result = inference_detector(model, img)<NewLine>    show_result_pyplot(img, result, model.CLASSES)<NewLine></code></pre><NewLine><p>And the full error log is:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""C:/Users/sarim/PycharmProjects/thesis/pytorch_learning.py"", line 355, in &lt;module&gt;<NewLine>    test()<NewLine>  File ""C:/Users/sarim/PycharmProjects/thesis/pytorch_learning.py"", line 338, in test<NewLine>    model = init_detector(""faster_rcnn_r50_fpn_1x.py"", ""faster_rcnn_r50_fpn_1x_20181010-3d1b3351.pth"", device='cuda:0')<NewLine>  File ""C:\Users\sarim\PycharmProjects\thesis\mmdetection\mmdet\apis\inference.py"", line 36, in init_detector<NewLine>    checkpoint = load_checkpoint(model, checkpoint)<NewLine>  File ""c:\users\sarim\appdata\local\programs\python\python37\lib\site-packages\mmcv\runner\checkpoint.py"", line 188, in load_checkpoint<NewLine>    load_state_dict(model, state_dict, strict, logger)<NewLine>  File ""c:\users\sarim\appdata\local\programs\python\python37\lib\site-packages\mmcv\runner\checkpoint.py"", line 96, in load_state_dict<NewLine>    rank, _ = get_dist_info()<NewLine>  File ""c:\users\sarim\appdata\local\programs\python\python37\lib\site-packages\mmcv\runner\utils.py"", line 21, in get_dist_info<NewLine>    initialized = dist.is_initialized()<NewLine>AttributeError: module 'torch.distributed' has no attribute 'is_initialized'<NewLine></code></pre><NewLine><p>My pytorch version is 1.1.0</p><NewLine></div>",https://discuss.pytorch.org/u/zimmer550,(Sarim Mehdi),zimmer550,"September 6, 2019,  8:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you check your PyTorch version again (<code>print(torch.__version__)</code>), as <a href=""https://pytorch.org/docs/1.1.0/distributed.html#torch.distributed.is_initialized"" rel=""nofollow noopener""><code>torch.distributed.is_initialized()</code></a> is in Pytorch 1.1.0.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Of course, I did that before. Here is proof:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/6586617c104db6f9015ada023d6439b5c640c563"" href=""https://discuss.pytorch.org/uploads/default/original/2X/6/6586617c104db6f9015ada023d6439b5c640c563.png"" title=""cmd.PNG""><img alt=""cmd"" data-base62-sha1=""eu88jhSEsT9ySQa3iiFcJdVckSL"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/6/6586617c104db6f9015ada023d6439b5c640c563_2_10x10.png"" height=""404"" src=""https://discuss.pytorch.org/uploads/default/original/2X/6/6586617c104db6f9015ada023d6439b5c640c563.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">cmd.PNG</span><span class=""informations"">1103×646 21 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>Anyways, I got it to work on windows by commenting out the lines that use distributed training. I read here that pytorch doesn’t support distributed training on windows but it does so on Linux:<br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/facebookresearch/maskrcnn-benchmark/issues/257"" rel=""nofollow noopener"" target=""_blank"">github.com/facebookresearch/maskrcnn-benchmark</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://github.com/shuxp"" rel=""nofollow noopener""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars1.githubusercontent.com/u/7609868?v=2&amp;s=96"" width=""60""/><NewLine></a><NewLine><h4><a href=""https://github.com/facebookresearch/maskrcnn-benchmark/issues/257"" rel=""nofollow noopener"" target=""_blank"">Issue: AttributeError: module 'torch.distributed' has no attribute 'is_initialized'</a></h4><NewLine><div class=""date"" style=""margin-top:10px;""><NewLine><div class=""user"" style=""margin-top:10px;""><NewLine>	opened by <a href=""https://github.com/shuxp"" rel=""nofollow noopener"" target=""_blank"">shuxp</a><NewLine>	on <a href=""https://github.com/facebookresearch/maskrcnn-benchmark/issues/257"" rel=""nofollow noopener"" target=""_blank"">2018-12-10</a><NewLine></div><NewLine><div class=""user""><NewLine>	closed by <a href=""https://github.com/shuxp"" rel=""nofollow noopener"" target=""_blank"">shuxp</a><NewLine>	on <a href=""https://github.com/facebookresearch/maskrcnn-benchmark/issues/257"" rel=""nofollow noopener"" target=""_blank"">2018-12-10</a><NewLine></div><NewLine></div><NewLine><pre class=""content"" style=""white-space: pre-wrap;"">❓ Questions and Help<NewLine>I installed according 'conda install pytorch-nightly-cpu -c pytorch' (torch-nightly1.0.0.dev20181209), also I installed pytorch stable from official site following...</pre><NewLine><div class=""labels""><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>And this makes sense because I encountered no such issue when running the same code on Google Colab</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for checking and good to hear you’re figured it out. My second guess would also be that you’re using a platform which does not support distributed training.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I wish <code>dist.is_initialized()</code> just returned always false instead of bombing out. This way the code is more cleaner between different platforms for non-distributed use. BTW, it seems same thing happens for methods like <code>is_gloo_available()</code> etc.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>There is <code>torch.distributed.is_available()</code> API to check if <code>distributed</code> package is available. APIs from <code>distributed</code> package is only available when <code>is_available</code> returns true. Let me add that to our docs.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://github.com/pytorch/pytorch/pull/37021"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/37021</a> is landed. The API doc of <code>torch.distributed.is_available</code> is added to master.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/zimmer550; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/sytelus; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: September 6, 2019, 11:18pm; <NewLine> REPLY_DATE 2: September 9, 2019,  2:22am; <NewLine> REPLY_DATE 3: September 9, 2019,  8:10am; <NewLine> REPLY_DATE 4: April 21, 2020,  6:22pm; <NewLine> REPLY_DATE 5: April 21, 2020,  7:45pm; <NewLine> REPLY_DATE 6: April 22, 2020,  2:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
77855,Torch.distributed for windows 7/10,2020-04-22T12:05:37.005Z,0,244,"<div class=""post"" itemprop=""articleBody""><NewLine><p>hello,<br/><NewLine>there is any way to run pytorch distributed on windows?</p><NewLine><p>i see at pytorch main page that there is version for windows but when i tried to used it,<br/><NewLine>i get that  torch.distributed.is_available() is False</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/Liron_Mor_Yosef,(Liron Mor Yosef),Liron_Mor_Yosef,"April 22, 2020, 12:05pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>i tried to use windows 7 with torch 1.3.1 and 1.5</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Currently, <code>torch.distributed</code> does not support Windows yet. Just created a poll/issue to track how many people would need this feature: <a href=""https://github.com/pytorch/pytorch/issues/37068"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/37068</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Liron_Mor_Yosef; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: April 22, 2020, 12:07pm; <NewLine> REPLY_DATE 2: April 23, 2020,  7:15am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
77643,Train Pytorch Model on multi-TPU V3 device based TPU Pod,2020-04-21T03:56:33.391Z,1,134,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to train a large model on a TPU V3 Pod with 5 TPU devices. I am very novice on TPU. I already code a model which I train on multi-gpu (4 V100) using DataParallel. I found DataParallel is very easy to incorporate. I have a couple of concerns to train this model on google cloud TPU:</p><NewLine><ul><NewLine><li>Can I train the same learning with <strong>DataParallel</strong>  on could TPU V3 device with 5 TPU ? or do need to do any modifications except changing the library to xla?</li><NewLine><li>Should I use <strong>DataParallel</strong>  or  <strong>DistributedDataParallel</strong> to train the model on TPU Pod?</li><NewLine><li>Does anyone have any experience with pythorch-lightning with multi-tpu device TPU Pod?</li><NewLine></ul><NewLine><p>Sorry for the novice level questions. Any types of resources, suggestions will be a great help.</p><NewLine></div>",https://discuss.pytorch.org/u/akashs,(Md Mofijul Islam (Akash)),akashs,"April 21, 2020,  3:57am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/ailzhang"">@ailzhang</a> for TPU questions <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><ul><NewLine><li>Should I use  <strong>DataParallel</strong>  or  <strong>DistributedDataParallel</strong>  to train the model on TPU Pod?</li><NewLine></ul><NewLine></blockquote><NewLine><p>General guidance fo DataParallel vs DistributedDataParallel: <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#comparison-between-dataparallel-and-distributeddataparallel"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#comparison-between-dataparallel-and-distributeddataparallel</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> ,"REPLY_DATE 1: April 21, 2020,  2:03pm; <NewLine> REPLY_DATE 2: April 21, 2020, 10:02pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
77569,DDP loading kernels every epoch,2020-04-20T14:23:02.629Z,0,89,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am running into an issue with multi-process DDP where it is loading my extra kernels on every run. Normally, this only happens at the beginning when I start the job. Since switching to DDP, it happens on every epoch.</p><NewLine><p>This seems really wasteful. Am I doing something wrong s.t. it is doing that or is this expected behavior?</p><NewLine><p>The following happens before every epoch:</p><NewLine><blockquote><NewLine><p>Using /tmp/torch_extensions as PyTorch extensions root…<br/><NewLine>Detected CUDA files, patching ldflags<br/><NewLine>Emitting ninja build file /tmp/torch_extensions/fused/build.ninja…<br/><NewLine>Building extension module fused…<br/><NewLine>ninja: no work to do.<br/><NewLine>Loading extension module fused…<br/><NewLine>Using /tmp/torch_extensions as PyTorch extensions root…<br/><NewLine>Detected CUDA files, patching ldflags<br/><NewLine>Emitting ninja build file /tmp/torch_extensions/upfirdn2d/build.ninja…<br/><NewLine>Building extension module upfirdn2d…<br/><NewLine>ninja: no work to do.<br/><NewLine>Loading extension module upfirdn2d…<br/><NewLine>Using /tmp/torch_extensions as PyTorch extensions root…<br/><NewLine>Detected CUDA files, patching ldflags<br/><NewLine>Emitting ninja build file /tmp/torch_extensions/fused/build.ninja…<br/><NewLine>Using /tmp/torch_extensions as PyTorch extensions root…<br/><NewLine>Using /tmp/torch_extensions as PyTorch extensions root…<br/><NewLine>Building extension module fused…<br/><NewLine>ninja: no work to do.<br/><NewLine>Loading extension module fused…<br/><NewLine>Loading extension module fused…<br/><NewLine>Using /tmp/torch_extensions as PyTorch extensions root…<br/><NewLine>Using /tmp/torch_extensions as PyTorch extensions root…<br/><NewLine>Detected CUDA files, patching ldflags<br/><NewLine>Emitting ninja build file /tmp/torch_extensions/upfirdn2d/build.ninja…<br/><NewLine>Building extension module upfirdn2d…<br/><NewLine>ninja: no work to do.<br/><NewLine>Loading extension module upfirdn2d…<br/><NewLine>Loading extension module fused…<br/><NewLine>Using /tmp/torch_extensions as PyTorch extensions root…</p><NewLine></blockquote><NewLine><p>The below is a stripped down version of my code.</p><NewLine><pre><code class=""lang-auto"">def train(epoch, step, model, optimizer, scheduler, loader, args, gpu):<NewLine>    model.train()<NewLine><NewLine>    averages = {'total_loss': Averager()}<NewLine><NewLine>    starting_step = step<NewLine>    t = time.time()<NewLine>    optimizer.zero_grad()<NewLine>    for batch_idx, images in enumerate(loader):<NewLine>        step += len(images)<NewLine>        images = images.cuda(gpu)<NewLine>        outputs = model(images)<NewLine>        kl_zs, ll_losses, latents, generations = outputs[:4]<NewLine>        prior_variances, posterior_variances = outputs[4:6]<NewLine><NewLine>        avg_kl_loss = torch.stack(kl_zs).mean()<NewLine>        avg_ll_loss = torch.stack(ll_losses).mean()<NewLine>        avg_kl_loss_penalized = avg_kl_loss * args.kl_lambda<NewLine>        if args.kl_anneal:<NewLine>            anneal_scale = max(0, min(step / args.kl_anneal_end, 1))<NewLine>            avg_kl_loss_penalized *= anneal_scale<NewLine>        total_loss = avg_ll_loss + avg_kl_loss_penalized<NewLine><NewLine>        averages['total_loss'].add(total_loss.item())<NewLine><NewLine>        total_loss.backward()<NewLine>        optimizer.step()<NewLine>        if scheduler:<NewLine>            scheduler.step()<NewLine>        optimizer.zero_grad()<NewLine><NewLine>        if step - starting_step &gt;= args.max_epoch_steps:<NewLine>            break<NewLine><NewLine>    return averages['total_loss'].item(), step<NewLine><NewLine>def main(gpu, args):<NewLine>    os.environ['MASTER_ADDR'] = 'localhost'<NewLine>    os.environ['MASTER_PORT'] = '12355'<NewLine>    dist.init_process_group(backend='nccl', rank=gpu, world_size=args.num_gpus)<NewLine><NewLine>    random.seed(args.seed)<NewLine>    np.random.seed(args.seed)<NewLine>    torch.manual_seed(args.seed)<NewLine><NewLine>    train_loader, test_loader, image_shape = get_loaders_and_shape(args, rank=gpu)<NewLine><NewLine>    model, optimizer = get_model(args, image_shape, gpu)<NewLine>    torch.cuda.set_device(gpu)<NewLine>    model.cuda(gpu)<NewLine>    model = DDP(model, device_ids=[gpu], find_unused_parameters=True)<NewLine><NewLine>    step = 0<NewLine>    start_epoch = 0  # start from epoch 0 or last checkpoint epoch<NewLine>    total_epochs = args.num_epochs<NewLine><NewLine>    for epoch in range(start_epoch, start_epoch + total_epochs):<NewLine>        train_loss, step = train(epoch,<NewLine>                                 step,<NewLine>                                 model,<NewLine>                                 optimizer,<NewLine>                                 scheduler,<NewLine>                                 train_loader,<NewLine>                                 args,<NewLine>                                 gpu)<NewLine>        results['train_loss'].append(train_loss)<NewLine><NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    parser = argparse.ArgumentParser(<NewLine>        description='Independent variational objects.')<NewLine><NewLine>    parser.add_argument('--dataset',<NewLine>                        default='gymnasticsRgb',<NewLine>                        type=str)<NewLine>    parser.add_argument('--num_workers',<NewLine>                        default=4,<NewLine>                        type=int,<NewLine>                        help='number of data workers')<NewLine>    parser.add_argument('--num_gpus',<NewLine>                        default=1,<NewLine>                        type=int,<NewLine>                        help='number of gpus.')<NewLine>    parser.add_argument('--debug',<NewLine>                        action='store_true',<NewLine>                        help='use debug mode (without saving to a directory)')<NewLine>    parser.add_argument('--lr',<NewLine>                        default=3e-4,<NewLine>                        type=float,<NewLine>                        help='learning rate assuming adam.')<NewLine>    parser.add_argument('--weight_decay',<NewLine>                        default=0,<NewLine>                        type=float,<NewLine>                        help='weight decay')<NewLine>    parser.add_argument('--seed', default=0, type=int, help='random seed')<NewLine>    parser.add_argument('--max_epoch_steps', default=200000, type=int)<NewLine>    parser.add_argument('--max_test_steps', default=50000, type=int)<NewLine>    parser.add_argument('--num_epochs', default=250, type=int,<NewLine>                        help='the number of epochs to train for. at 200000 ' \<NewLine>                        'max_epoch steps, this would go for 2500 epochs to ' \<NewLine>                        'reach 5e8 steps.')<NewLine>    parser.add_argument(<NewLine>        '--batch_size',<NewLine>        default=100,<NewLine>        type=int)<NewLine>    parser.add_argument('--optimizer',<NewLine>                        default='adam',<NewLine>                        type=str,<NewLine>                        help='adam or sgd.')<NewLine>    parser.add_argument('--num_transition_layers', type=int, default=4)<NewLine>    parser.add_argument('--num_latents', type=int, default=2)<NewLine>    parser.add_argument('--latent_dim', type=int, default=32)<NewLine>    parser.add_argument('--translation_layer_dim', type=int, default=128)<NewLine>    parser.add_argument(<NewLine>        '--output_variance',<NewLine>        type=float,<NewLine>        default=.25_<NewLine><NewLine>    args = parser.parse_args()<NewLine>    mp.spawn(main, nprocs=args.num_gpus, args=(args,))<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/crnyu,(Cinjon Resnick),crnyu,"April 20, 2020,  2:23pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you share the code snippet where you actually load the extensions? I’m assuming you’re using <a href=""https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load_inline"" rel=""nofollow noopener"">load_inline</a> to load your extra kernels? If so, is this happening for every epoch?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> ,"REPLY_DATE 1: April 21, 2020,  8:59pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
55614,Is my DistributedDataParallel code correct? Why is DistributedDataParallel&rsquo;s performance worse than nn.DataParallel?,2019-09-10T12:13:17.026Z,1,824,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi，I am new to Pytorch DistributedDataParallel.  Object detection,  My model can achieve 79mAP with nn.DataParallel, but with DistributedDataParallel, it only 50+mAP.<br/><NewLine>In both ways, except for batchsize, every parameter is the same. For nn.DataParallel, the batchsize is 32, and 4 GPUs, for DistributedDataParallel, the batchsize is 8 for per GPU,  4 GPUS, So the total number of batchsize is the same. Is my approach correct? Why is DistributedDataParallel performing worse?</p><NewLine><p>Here is .py with nn.DataParallel:</p><NewLine><pre><code class=""lang-auto"">#train.py<NewLine><NewLine>import os<NewLine>os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'<NewLine>import torch<NewLine>from model_dla34 import get_pose_net<NewLine>from loss import CtdetLoss,AverageMeter<NewLine>from torch.utils.data import DataLoader<NewLine>from datasets import My_certernet_datasets<NewLine>import time<NewLine><NewLine>datasets = 'pascal' <NewLine>batch_size = 32<NewLine>start_epoch = 0<NewLine>end_epoch = 70<NewLine>init_lr = 1.25e-4<NewLine><NewLine>def main():<NewLine>    train_data = My_certernet_datasets(mode = 'train',datasets = datasets)<NewLine>    print('there are {} train images'.format(len(train_data)))<NewLine>    train_data_loader = DataLoader(dataset=train_data,<NewLine>                                   num_workers=16,<NewLine>                                   batch_size=batch_size,<NewLine>                                   shuffle=True,<NewLine>                                   pin_memory=True<NewLine>                                   )<NewLine>    model = get_pose_net() <NewLine>    if torch.cuda.device_count() &gt; 1:<NewLine>        model = torch.nn.DataParallel(model)<NewLine>    model = model.to(device)<NewLine>    criterion = CtdetLoss()<NewLine>    optimizer = torch.optim.Adam(model.parameters(), init_lr)<NewLine>    for epoch in range(start_epoch+1,end_epoch+1):<NewLine>        adjust_lr(optimizer,epoch,init_lr)<NewLine>        train(train_data_loader,model,criterion,optimizer,device,epoch,end_epoch)<NewLine>        if epoch % 10 == 0:<NewLine>            save_model(save_dir + '/model_last.pth',epoch,model,optimizer = None)<NewLine><NewLine>def train(train_data_loader,model,criterion,optimizer,device,epoch,end_epoch):<NewLine>    losses = AverageMeter()<NewLine>    model = model.train()<NewLine>    for i ,batch in enumerate(train_data_loader):<NewLine>        start_time = time.time()<NewLine>        for k in batch:<NewLine>            if k != 'meta':<NewLine>                batch[k] = batch[k].to(device)<NewLine>        output = model(batch['input'])<NewLine>        loss_stats = criterion(output,batch)<NewLine>        loss = loss_stats['loss']<NewLine>        optimizer.zero_grad()<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine>        end_time = time.time()<NewLine>        ELA_time = (end_time - start_time)*(end_epoch - epoch)*len(train_data_loader)<NewLine>        ELA_time = time.strftime('%H:%M:%S',time.gmtime(ELA_time))<NewLine>        losses.update(loss.item())<NewLine>        print('[epoch:{},{}/{}]'.format(epoch,i,len(train_data_loader)),'current_loss:%.4f'% losses.current,\<NewLine>            'average_loss:%.4f' % losses.avg,'ELA_time:',ELA_time)<NewLine><NewLine>def adjust_lr(optimizer,epoch,init_lr,lr_step=[45,60]):<NewLine><NewLine>    if epoch in lr_step:<NewLine>        lr = init_lr*(0.1**(lr_step.index(epoch) + 1))<NewLine>        print('Drop LR to',lr)<NewLine>        for param_group in optimizer.param_groups:<NewLine>            param_group['lr'] = lr<NewLine><NewLine>def save_model(path,epoch,model,optimizer = None):<NewLine>    if isinstance(model,torch.nn.DataParallel):<NewLine>        state_dict = model.module.state_dict()<NewLine>    else:<NewLine>        state_dict = model.state_dict()<NewLine>    data = {'epoch':epoch,'state_dict':state_dict}<NewLine>    if not (optimizer is None):<NewLine>        data['optimizer'] = optimizer.state_dict()<NewLine>    torch.save(data,path)<NewLine><NewLine>if __name__ == '__main__':<NewLine>    main()<NewLine></code></pre><NewLine><p>Here is .py with DistributedDataParallel:</p><NewLine><pre><code class=""lang-auto"">#train_DDP.py<NewLine><NewLine>import os<NewLine>os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'<NewLine>import torch<NewLine>from model_dla34 import get_pose_net<NewLine>from loss import CtdetLoss,AverageMeter<NewLine>from torch.utils.data import DataLoader<NewLine>from datasets import My_certernet_datasets<NewLine>import time<NewLine>from torch.utils.data.distributed import DistributedSampler<NewLine>from torch.nn.parallel import DistributedDataParallel<NewLine><NewLine>torch.distributed.init_process_group(backend='nccl')<NewLine>datasets = 'pascal' <NewLine>batch_size = 8<NewLine>start_epoch = 0<NewLine>end_epoch = 70<NewLine>init_lr = 1.25e-4<NewLine><NewLine>local_rank = torch.distributed.get_rank()<NewLine>torch.cuda.set_device(local_rank)<NewLine>device = torch.device('cuda',local_rank)<NewLine><NewLine>def main():<NewLine>    model = get_pose_net()<NewLine>    model.to(device)<NewLine>    model_val = model<NewLine>    if torch.cuda.device_count() &gt; 1:<NewLine>            print(""let's use GPU{}!!!"".format(local_rank))<NewLine>            model = DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank,find_unused_parameters=True)<NewLine>    train_data = My_certernet_datasets(mode = 'train',datasets = datasets)<NewLine>    print('there are {} train images'.format(len(train_data)))<NewLine>    train_data_loader = DataLoader(dataset=train_data,<NewLine>                                   num_workers=16,<NewLine>                                   batch_size=batch_size, <NewLine>                                   sampler=DistributedSampler(train_data)<NewLine>                                   )<NewLine>    criterion = CtdetLoss()<NewLine>    optimizer = torch.optim.Adam(model.parameters(), init_lr)<NewLine>    for epoch in range(start_epoch+1,end_epoch+1):<NewLine>        DistributedSampler(train_data).set_epoch(epoch)<NewLine>        adjust_lr(optimizer,epoch,init_lr)<NewLine>        train(train_data_loader,model,criterion,optimizer,device,epoch,end_epoch)<NewLine>        if epoch % 10 == 0 and local_rank==0:<NewLine>            save_model(save_dir + '/model_last_{}.pth'.format(epoch),epoch,model,optimizer = None)<NewLine><NewLine>def train(train_data_loader,model,criterion,optimizer,device,epoch,end_epoch):<NewLine>    losses = AverageMeter()<NewLine>    model.train()<NewLine>    for i ,batch in enumerate(train_data_loader):<NewLine>        start_time = time.time()<NewLine>        for k in batch:<NewLine>            if k != 'meta':<NewLine>                batch[k] = batch[k].to(device)<NewLine>        output = model(batch['input'])<NewLine>        loss_stats = criterion(output,batch)<NewLine>        loss = loss_stats['loss']<NewLine>        optimizer.zero_grad()<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine>        end_time = time.time()<NewLine>        ELA_time = (end_time - start_time)*(end_epoch - epoch)*len(train_data_loader)<NewLine>        ELA_time = time.strftime('%H:%M:%S',time.gmtime(ELA_time))<NewLine>        losses.update(loss.item())<NewLine>        if local_rank==0:<NewLine>            print('[epoch:{},{}/{}]'.format(epoch,i,len(train_data_loader)),'current_loss:%.4f'% losses.current,\<NewLine>            'average_loss:%.4f' % losses.avg,'ELA_time:',ELA_time)<NewLine><NewLine>def adjust_lr(optimizer,epoch,init_lr,lr_step=[45,60]):<NewLine>    if epoch in lr_step:<NewLine>        lr = init_lr*(0.1**(lr_step.index(epoch) + 1))<NewLine>        print('Drop LR to',lr)<NewLine>        for param_group in optimizer.param_groups:<NewLine>            param_group['lr'] = lr<NewLine><NewLine>def save_model(path,epoch,model,optimizer = None):<NewLine>    if isinstance(model,torch.nn.DataParallel):<NewLine>        state_dict = model.module.state_dict()<NewLine>    else:<NewLine>        state_dict = model.state_dict()<NewLine>    data = {'epoch':epoch,'state_dict':state_dict}<NewLine>    if not (optimizer is None):<NewLine>        data['optimizer'] = optimizer.state_dict()<NewLine>    torch.save(data,path)<NewLine><NewLine>if __name__ == '__main__':<NewLine>    main()<NewLine></code></pre><NewLine><p>Someone can help me? This has been bothering me for a few days. Thank you very much!!!</p><NewLine></div>",https://discuss.pytorch.org/u/Niu,(Longbin Yan),Niu,"September 10, 2019, 12:34pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Gradients are averaged (divided by the number of processes) when using <code>nn.DistributedDataParallel</code>. This is not the case when using <code>nn.DataParallel</code>. You can either multiply them after the call to backward to make them equivalent to the output of <code>nn.DataParallel</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply.<br/><NewLine>I am a bit confused. I think the loss is independent of the number of batchsizes, and the batchsize increases, making the gradient more robust.<br/><NewLine>In my case, for nn.DataParallel, loss is divided by 32(the total batchsize of 4 gpus is 32); for nn.DistributedDataParallel, the loss of single process is divided by 8(per gpu batchsize is 8), at this time, their gradients are the same, even if the average of the gradient is calculated later(divided by number of precesses), the gradient is almost the same as the former.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>What’s in your <code>CtdetLoss</code> definition?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/niu"">@Niu</a> Do you remember what the issue was? Thanks.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Niu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/SimonW; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/f10w; <NewLine> ,"REPLY_DATE 1: September 10, 2019,  1:05pm; <NewLine> REPLY_DATE 2: September 11, 2019,  2:07am; <NewLine> REPLY_DATE 3: September 11, 2019,  8:44pm; <NewLine> REPLY_DATE 4: April 21, 2020,  8:29pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
77493,Dist.all_gather leads to deadlock,2020-04-20T05:12:19.108Z,0,193,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,<br/><NewLine>If all_gather are used more than once,  the next cuda() function will deadlock. And GPU-Utils are 100%.<br/><NewLine>Details:<br/><NewLine>I wrapped <code>all_gather</code> in <code>collect</code>:</p><NewLine><pre><code class=""lang-auto"">def collect(x):<NewLine>    x = x.contiguous()<NewLine>    out_list = [torch.zeros_like(x, device=x.device, dtype=x.dtype)<NewLine>        for _ in range(dist.get_world_size())]<NewLine>dist.all_gather(out_list, x)<NewLine>return torch.cat(out_list, dim=0)<NewLine></code></pre><NewLine><p>Next:</p><NewLine><pre><code class=""lang-auto"">a_all = collect(a)<NewLine>b_all = collect(b)<NewLine>c = torch.rand(10,10).cuda() # deadlock! No error report.<NewLine></code></pre><NewLine><p>The deadlock doesn’t happen if  I don’t collect <code>b_all</code> or don’t use <code>cuda()</code>.<br/><NewLine>My code runs on 4 GPUs using DDP.<br/><NewLine>Pytorch version: 1.4.<br/><NewLine>This problem has been confusing me for a couple of days. I’ll appreciate any help!</p><NewLine></div>",https://discuss.pytorch.org/u/fanl,,fanl,"April 20, 2020,  5:12am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you provide a small repro for the problem that you are seeing? I ran the following script locally on my GPU machine and didn’t notice any deadlocks:</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>from torch.multiprocessing import Process<NewLine><NewLine>def collect(x):<NewLine>    x = x.contiguous()<NewLine>    out_list = [torch.zeros_like(x, device=x.device, dtype=x.dtype)<NewLine>        for _ in range(dist.get_world_size())]<NewLine>    dist.all_gather(out_list, x)<NewLine>    return torch.cat(out_list, dim=0)<NewLine><NewLine>def run(rank, size):<NewLine>    """""" Distributed function to be implemented later. """"""<NewLine>    print ('START: {}'.format(rank))<NewLine>    a = torch.rand(10, 10).cuda(rank)<NewLine>    b = torch.rand(10, 10).cuda(rank)<NewLine>    a_all = collect(a)<NewLine>    b_all = collect(b)<NewLine>    c = torch.rand(10,10).cuda(rank)<NewLine>    print ('DONE : {}'.format(rank))<NewLine><NewLine>def init_process(rank, size, fn, backend='gloo'):<NewLine>    """""" Initialize the distributed environment. """"""<NewLine>    os.environ['MASTER_ADDR'] = '127.0.0.1'<NewLine>    os.environ['MASTER_PORT'] = '29500'<NewLine>    dist.init_process_group(backend, rank=rank, world_size=size)<NewLine>    fn(rank, size)<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    size = 4<NewLine>    processes = []<NewLine>    for rank in range(size):<NewLine>        p = Process(target=init_process, args=(rank, size, run))<NewLine>        p.start()<NewLine>        processes.append(p)<NewLine><NewLine>    for p in processes:<NewLine>        p.join()<NewLine></code></pre><NewLine><p>Running the script:</p><NewLine><pre><code class=""lang-auto"">$ python /tmp/test_allgather.py<NewLine>START: 0<NewLine>START: 1<NewLine>START: 2<NewLine>START: 3<NewLine>DONE : 2<NewLine>DONE : 3<NewLine>DONE : 1<NewLine>DONE : 0<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> ,"REPLY_DATE 1: April 21, 2020,  8:03pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
41531,Combining model weight,2019-04-02T23:44:21.059Z,3,430,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi All,</p><NewLine><p>I have a question on combining model weights accurately.<br/><NewLine>I made one NN and trained the model separately on two datasets. Now I am trying to obtain a single model out these two models by combining the weights.</p><NewLine><p>What are different ways to combine the weights in anyone’s opinion?</p><NewLine><p>Thanks in advance for answering this question</p><NewLine></div>",https://discuss.pytorch.org/u/jaugust,(Jees),jaugust,"April 2, 2019, 11:44pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Jees,</p><NewLine><p>let me try to make sure I understand your use case completely.<br/><NewLine>You have one model architecture and you initialized two models using it.<br/><NewLine>These two model were trained on two different datasets and converged.</p><NewLine><p>Now you would like to “combine” all parameters of both models and create a single one.<br/><NewLine>Using this new single model you would now want to predict both datasets or just one of them?<br/><NewLine>Do both datasets contain the same classes or are the targets completely different?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> for the queries.</p><NewLine><p>You have one model architecture and you initialized two models using it.</p><NewLine><blockquote><NewLine><p>Yes that is very true</p><NewLine></blockquote><NewLine><p>These two model were trained on two different datasets and converged.</p><NewLine><blockquote><NewLine><p>that is also correct</p><NewLine></blockquote><NewLine><p>Now you would like to “combine” all parameters of both models and create a single one.</p><NewLine><blockquote><NewLine><p>that is the idea, yes</p><NewLine></blockquote><NewLine><p>Using this new single model you would now want to predict both datasets or just one of them?</p><NewLine><blockquote><NewLine><p>I want it to predict both of them with reasonable accuracy</p><NewLine></blockquote><NewLine><p>Do both datasets contain the same classes or are the targets completely different?</p><NewLine><blockquote><NewLine><p>completely different (we are not sure but let me consider that case)</p><NewLine></blockquote><NewLine><p>Thanks for queries I hope this can help you to give me some pointers.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>Can we club two weight file into one weight file without loss of catastrophic loss with data A and data B,<br/><NewLine>In our case we trained initial some data and after some time I get some more new data. So I train only new data and club both weight file into one.<br/><NewLine>Number of targets is same class and both data file in same domain.<br/><NewLine>I club both model but I found catastrophic loss will be happen<br/><NewLine>Please suggest me what is the best way</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think combining different trained parameters will automatically result in a good new model, as both models might (and most likely) have converged to different local minima.<br/><NewLine>The mean (I assume you are taking the average of all parameters) will not necessarily yield another minimum on the loss surface.</p><NewLine><p>You could try to fine tune your model by retraining with the new samples and a low learning rate and check the validation accuracy again on the updated dataset.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jaugust; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Dhirendra_Jha; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: April 3, 2019, 10:54am; <NewLine> REPLY_DATE 2: April 10, 2019, 10:16pm; <NewLine> REPLY_DATE 3: April 20, 2020,  6:57am; <NewLine> REPLY_DATE 4: April 20, 2020,  7:12am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
74482,How to correctly launch multi-node training,2020-03-26T22:37:18.191Z,2,620,"<div class=""post"" itemprop=""articleBody""><NewLine><p><code>dist.get_rank()</code> returns either 0 or 1 even though I launch the training with</p><NewLine><p><code>python -m torch.distributed.launch \    --nproc_per_node=4 \ 	 --nnodes=2 \ 	 --node_rank=0 \ 	 --master_addr=""$MASTER_ADDR"" \ 	 --master_port=""$MASTER_PORT"" \    train_dist.py &amp; </code></p><NewLine><p>I am using 2 nodes each with 4 GPUs.</p><NewLine></div>",https://discuss.pytorch.org/u/ankahira,(Albert Njoroge Kahira),ankahira,"March 26, 2020, 10:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ankahira"">@ankahira</a></p><NewLine><p>Two questions:</p><NewLine><ol><NewLine><li>What parameters did you pass to <code>init_process_group</code> invocation in <code>train_dist.py</code>?</li><NewLine><li>Can you check if <code>RANK</code> and <code>WORLD_SZIE</code> are set properly for each process?</li><NewLine></ol><NewLine><p>Sudarshan wrote a <a href=""https://github.com/pytorch/examples/pull/743"" rel=""nofollow noopener"">great example</a> of how to use launcher.py, which might be helpful to you.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually I am abit confused about this. I understand that I should set <code>WORLD-SIZE</code> as Number of nodes i.e 2. I am not sure what I should set as <code>RANK</code> but I set it as 0 and 1 for the two nodes. For the <code>init_process</code>_group` I pass each of the GPUs as in 0, 1, 2 ,3.  Something like this.</p><NewLine><p><code>def init_processes(rank, size, fn, backend='nccl'):     dist.init_process_group(backend, rank=rank, world_size=size)     fn(rank, size)</code></p><NewLine><p><code>if __name__ == ""__main__"": </code><br/><NewLine><code> size = 4     processes = []     for rank in range(size):         p = Process(target=init_processes, args=(rank, size, run))         p.start()         processes.append(p)</code></p><NewLine><pre><code>for p in processes:<NewLine>    p.join()`</code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""74482"" data-username=""ankahira""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ankahira/40/21299_2.png"" width=""20""/> ankahira:</div><NewLine><blockquote><NewLine><p>I should set <code>WORLD-SIZE</code> as Number of nodes i.e 2. I am not sure</p><NewLine></blockquote><NewLine></aside><NewLine><p>If you have 8 processes (4 processes per node with 2 node), world_size should be 8 for <code>init_process_group</code>. But you don’t need to set that, as the launcher script will set the env vars for you properly.</p><NewLine><blockquote><NewLine><p>I am not sure what I should set as  <code>RANK</code>  but I set it as 0 and 1 for the two nodes</p><NewLine></blockquote><NewLine><p>There are two ranks here:</p><NewLine><ul><NewLine><li>node rank: this is what you provide for <code>--node_rank</code> to the launcher script, and it is correct to set it to 0 and 1 for the two nodes.</li><NewLine><li>process rank: this rank should be <code>--node_rank</code> X <code>--nproc_per_node</code> + local GPU id, which should be 0~3 for the four processes in the first node, and 4~7 for the four processes in the second node. But you don’t need to set this for <code>init_process_group</code> either, as the launcher script should have set the env var for you.</li><NewLine></ul><NewLine><p>With the params you provided to the launcher script, the following should be sufficient to init the process group.</p><NewLine><pre><code class=""lang-auto"">dist.init_process_group(backend)<NewLine></code></pre><NewLine><p>If you also need the local rank for DDP, you will need to parse it from the arg, and then pass it to the DDP constructor. Sth like:</p><NewLine><pre><code class=""lang-auto"">    model = torch.nn.parallel.DistributedDataParallel(model,<NewLine>                                                      device_ids=[arg.local_rank],<NewLine>                                                      output_device=arg.local_rank)<NewLine></code></pre><NewLine><p>Check out the <a href=""https://github.com/pytorch/pytorch/blob/f326045b3757236aabe367dfca1894be14ce31ef/torch/distributed/launch.py#L116-L118"" rel=""nofollow noopener"">readme</a> in the launcher script.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much. That is very clear now.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ankahira"">@ankahira</a> Can you please share your train_dist.py please? I still have problem with using the luncher!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ankahira; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ankahira; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Pooria_Taheri; <NewLine> ,"REPLY_DATE 1: March 27, 2020,  2:26pm; <NewLine> REPLY_DATE 2: March 27, 2020,  2:43pm; <NewLine> REPLY_DATE 3: March 27, 2020,  7:22pm; <NewLine> REPLY_DATE 4: March 27, 2020,  7:22pm; <NewLine> REPLY_DATE 5: April 19, 2020,  3:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
77373,Multiple GPU with os CUDA_VISIBLE_DEVICES does not work,2020-04-19T02:03:24.401Z,4,873,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’ve tried to set CUDA_VISIBLE_DEVICES = ‘1’ in main function but when I move the model to cuda, It does not move to GPU1 but GPU0 instead (result in OOM due to GPU0 is in use). Please tell me if I’m wrong.<br/><NewLine>here is my code:<br/><NewLine>in train.py:</p><NewLine><pre><code class=""lang-auto"">def main(config_file_path):<NewLine>    config = SspmYamlConfig(config_file_path)<NewLine><NewLine>    dataloader_cfg = config.get_dataloader_cfg()<NewLine>    trainer_cfg = config.get_trainer_cfg()<NewLine>    logger_cfg = config.get_logger_cfg()<NewLine>    model_cfg = config.get_model_cfg()<NewLine>    pose_dataset_cfg = config.get_pose_dataset_cfg()<NewLine>    data_augmentation_cfg = config.get_augmentation_cfg()<NewLine>    target_generator_cfg = config.get_target_generator_cfg()<NewLine><NewLine>    learning_rate = trainer_cfg['optimizer']['learning_rate']<NewLine>    # parsing device = [1] by config<NewLine>    device = ','.join(list(map(str, trainer_cfg['device'])))<NewLine>    os.environ['CUDA_DEVICE_ORDER']= 'PCI_BUS_ID'<NewLine>    os.environ['CUDA_VISIBLE_DEVICES'] = device <NewLine>    model = getModel(model_cfg)<NewLine>    train_loader = DataLoader(train_dataset, **dataloader_cfg['train'])<NewLine>    val_loader = DataLoader(val_dataset, **dataloader_cfg['val'])<NewLine>    trainer = Trainer(<NewLine>                      model, optimizer, logger,<NewLine>                      writer, config, train_loader, val_loader<NewLine>                      )<NewLine></code></pre><NewLine><p>Trainer class is inherited from BaseTrainer where the model was transferd to cuda</p><NewLine><pre><code class=""lang-auto"">class BaseTrainer(ABC):<NewLine>    def __init__(self, model, optimizer, logger, writer, config):<NewLine>        self.config = config<NewLine>        self.logger = logger<NewLine>        self.writer = writer<NewLine>        self.optimizer = optimizer<NewLine>        self.trainer_config = config.get_trainer_cfg()<NewLine>        self.device_list = self.trainer_config['device'] #device list is [1]<NewLine>        self.device_type = self._check_gpu(self.device_list)<NewLine>        self.device = torch.device(self.device_type)<NewLine>        self.model = model<NewLine>        self.model = self.model.to(self.device)<NewLine>        self.model = torch.nn.DataParallel(self.model)<NewLine>    def _check_gpu(self, gpus):<NewLine>        if len(gpus) &gt; 0 and torch.cuda.is_available():<NewLine>            pynvml.nvmlInit()<NewLine>            for i in gpus:<NewLine>                handle = pynvml.nvmlDeviceGetHandleByIndex(i)<NewLine>                meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)<NewLine>                memused = meminfo.used / 1024 / 1024<NewLine>                self.logger.info('GPU{} used: {}M'.format(i, memused))<NewLine>                if memused &gt; 1000:<NewLine>                    pynvml.nvmlShutdown()<NewLine>                    raise ValueError('GPU{} is occupied!'.format(i))<NewLine>            pynvml.nvmlShutdown()<NewLine>            return 'cuda'<NewLine>        else:<NewLine>            self.logger.info('Using CPU!')<NewLine>            return 'cpu'<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/thancaocuong,(cuongtc),thancaocuong,"April 19, 2020,  2:04am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you are masking devices via <code>CUDA_VISIBLE_DEVICES</code> all visible devices will be mapped to device ids in the range <code>[0, nb_visible_devices]</code>.<br/><NewLine>E.g. if your system has two GPUs and you are using <code>CUDA_VISIBLE_DEVICES=1</code>, you would have to access it inside the script as <code>cuda:0</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>thank you for your quick reply. but I have a question:<br/><NewLine>I have 3 GPUs, when I want to use only GPU1 and 2 (GPU0 is in use). how should I do?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If all devices are the same, use<br/><NewLine><code>CUDA_VISIBLE_DEVICES=1,2 python script.py args</code><br/><NewLine>to run the script and inside the script use <code>cuda:0</code> and <code>cuda:1</code> (or the equivalent <code>.cuda(0)</code>, <code>.cuda(1)</code> commands).</p><NewLine><p>However, if the mapping is not what you expect via <code>nvidia-smi</code>, you could force the PCI bus order order via <code>CUDA_DEVICE_ORDER=PCI_BUS_ID</code> in front of the aforementioned command.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve found that I need to set VISIBLE device at the begining of my script. I’s my mistake, thank you for your help. I will close this topic</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/thancaocuong; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/thancaocuong; <NewLine> ,"REPLY_DATE 1: April 19, 2020,  2:04am; <NewLine> REPLY_DATE 2: April 19, 2020,  2:08am; <NewLine> REPLY_DATE 3: April 19, 2020,  2:28am; <NewLine> REPLY_DATE 4: April 19, 2020,  2:29am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> 
77106,Distributed pytorch with mpi,2020-04-16T20:58:35.759Z,2,611,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all ,</p><NewLine><p>I try to run pytorch with distributed system.<br/><NewLine>I run test1.py as below</p><NewLine><blockquote><NewLine><p>import torch<br/><NewLine>import torch.distributed as dist<br/><NewLine>def main(rank, world):<br/><NewLine>if rank == 0:<br/><NewLine>x = torch.tensor([1., -1.]) # Tensor of interest<br/><NewLine>dist.send(x, dst=1)<br/><NewLine>print(‘Rank-0 has sent the following tensor to Rank-1’)<br/><NewLine>print(x)<br/><NewLine>else:<br/><NewLine>z = torch.tensor([0., 0.]) # A holder for recieving the tensor<br/><NewLine>dist.recv(z, src=0)<br/><NewLine>print(‘Rank-1 has recieved the following tensor from Rank-0’)<br/><NewLine>print(z)<br/><NewLine>if <strong>name</strong> == ‘<strong>main</strong>’:<br/><NewLine>dist.init_process_group(backend=‘mpi’)<br/><NewLine>main(dist.get_rank(), dist.get_world_size())</p><NewLine></blockquote><NewLine><p>Then I run with single machine.</p><NewLine><blockquote><NewLine><p>mpiexec -n 2 python test1.py</p><NewLine></blockquote><NewLine><p>Finally, the error is</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""test1.py"", line 17, in &lt;module&gt;<NewLine>    dist.init_process_group(backend='mpi')<NewLine>  File ""/cluster/home/cnphuong/my_environment/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 392, in init_process_group<NewLine>    timeout=timeout)<NewLine>  File ""/cluster/home/cnphuong/my_environment/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 452, in _new_process_group_helper<NewLine>    raise RuntimeError(""Distributed package doesn't have MPI built in"")<NewLine>RuntimeError: Distributed package doesn't have MPI built in<NewLine>Traceback (most recent call last):<NewLine>  File ""test1.py"", line 17, in &lt;module&gt;<NewLine>    dist.init_process_group(backend='mpi')<NewLine>  File ""/cluster/home/cnphuong/my_environment/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 392, in init_process_group<NewLine>    timeout=timeout)<NewLine>  File ""/cluster/home/cnphuong/my_environment/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 452, in _new_process_group_helper<NewLine>    raise RuntimeError(""Distributed package doesn't have MPI built in"")<NewLine>RuntimeError: Distributed package doesn't have MPI built in<NewLine>-------------------------------------------------------<NewLine>Primary job  terminated normally, but 1 process returned<NewLine>a non-zero exit code. Per user-direction, the job has been aborted.<NewLine>-------------------------------------------------------<NewLine>--------------------------------------------------------------------------<NewLine>mpirun detected that one or more processes exited with non-zero status, thus causing<NewLine>the job to be terminated. The first process to do so was:<NewLine><NewLine>  Process name: [[31741,1],1]<NewLine>  Exit code:    1<NewLine>--------------------------------------------------------------------------<NewLine></code></pre><NewLine><p>I also installed pytorch with</p><NewLine><blockquote><NewLine><p>pip install torch torchvision</p><NewLine></blockquote><NewLine><p>Please help me.<br/><NewLine>Thanks,</p><NewLine></div>",https://discuss.pytorch.org/u/ph0123,(chau phuong),ph0123,"April 16, 2020,  8:58pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You need to build pytorch from source to enable MPI: <a href=""https://pytorch.org/docs/stable/distributed.html#backends-that-come-with-pytorch"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/distributed.html#backends-that-come-with-pytorch</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>HI,</p><NewLine><p>I followed the instruction on git.</p><NewLine><ol><NewLine><li>clone code from git.</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">git clone --recursive https://github.com/pytorch/pytorch<NewLine>cd pytorch<NewLine></code></pre><NewLine><ol start=""2""><NewLine><li>install dependencies.</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">pip install numpy ninja pyyaml mkl mkl-include setuptools cmake cffi<NewLine></code></pre><NewLine><ol start=""3""><NewLine><li>run setup and install. The errors were here.<br/><NewLine>export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/…/""}<br/><NewLine>python setup.py install</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">Building wheel torch-1.6.0a0+32bbf12<NewLine>-- Building version 1.6.0a0+32bbf12<NewLine>cmake -GNinja -DBUILD_PYTHON=True -DBUILD_TEST=True -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/cluster/home/cnphuong/pytorch/torch -DCMAKE_PREFIX_PATH=/cluster/software/Anaconda3/2019.03/bin/../ -DNUMPY_INCLUDE_DIR=/cluster/home/cnphuong/my_environment/lib/python3.6/site-packages/numpy/core/include -DPYTHON_EXECUTABLE=/cluster/home/cnphuong/my_environment/bin/python -DPYTHON_INCLUDE_DIR=/cluster/software/Python/3.6.6-foss-2018b/include/python3.6m -DPYTHON_LIBRARY=/cluster/software/Python/3.6.6-foss-2018b/lib/libpython3.6m.so.1.0 -DTORCH_BUILD_VERSION=1.6.0a0+32bbf12 -DUSE_NUMPY=True /cluster/home/cnphuong/pytorch<NewLine>CMake Error: The source directory ""/cluster/home/cnphuong/pytorch"" does not appear to contain CMakeLists.txt.<NewLine>Specify --help for usage, or press the help button on the CMake GUI.<NewLine>Traceback (most recent call last):<NewLine>  File ""setup.py"", line 738, in &lt;module&gt;<NewLine>    build_deps()<NewLine>  File ""setup.py"", line 320, in build_deps<NewLine>    cmake=cmake)<NewLine>  File ""/cluster/home/cnphuong/pytorch/tools/build_pytorch_libs.py"", line 59, in build_caffe2<NewLine>    rerun_cmake)<NewLine>  File ""/cluster/home/cnphuong/pytorch/tools/setup_helpers/cmake.py"", line 324, in generate<NewLine>    self.run(args, env=my_env)<NewLine>  File ""/cluster/home/cnphuong/pytorch/tools/setup_helpers/cmake.py"", line 141, in run<NewLine>    check_call(command, cwd=self.build_dir, env=env)<NewLine>  File ""/cluster/software/Python/3.6.6-foss-2018b/lib/python3.6/subprocess.py"", line 291, in check_call<NewLine>    raise CalledProcessError(retcode, cmd)<NewLine>subprocess.CalledProcessError: Command '['cmake', '-GNinja', '-DBUILD_PYTHON=True', '-DBUILD_TEST=True', '-DCMAKE_BUILD_TYPE=Release', '-DCMAKE_INSTALL_PREFIX=/cluster/home/cnphuong/pytorch/torch', '-DCMAKE_PREFIX_PATH=/cluster/software/Anaconda3/2019.03/bin/../', '-DNUMPY_INCLUDE_DIR=/cluster/home/cnphuong/my_environment/lib/python3.6/site-packages/numpy/core/include', '-DPYTHON_EXECUTABLE=/cluster/home/cnphuong/my_environment/bin/python', '-DPYTHON_INCLUDE_DIR=/cluster/software/Python/3.6.6-foss-2018b/include/python3.6m', '-DPYTHON_LIBRARY=/cluster/software/Python/3.6.6-foss-2018b/lib/libpython3.6m.so.1.0', '-DTORCH_BUILD_VERSION=1.6.0a0+32bbf12', '-DUSE_NUMPY=True', '/cluster/home/cnphuong/pytorch']' returned non-zero exit status 1.<NewLine></code></pre><NewLine><p>I only want to run with multi CPUs. These step is correct or not?<br/><NewLine>Thanks,</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group quote-modified"" data-post=""3"" data-topic=""77106"" data-username=""ph0123""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ph0123/40/22888_2.png"" width=""20""/> ph0123:</div><NewLine><blockquote><NewLine><p>CMake Error: The source directory “/cluster/home/cnphuong/pytorch” does not appear to contain CMakeLists.txt.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Can you check if that directory has a CMakeLists.txt file? Usually there should be a CMakeLists.txt file in the top level directory when you clone pytorch.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""77106"" data-username=""pritamdamania87""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/p/43a26b/40.png"" width=""20""/> pritamdamania87:</div><NewLine><blockquote><NewLine><p>y has a CMakeLists.txt file? Usually there should be a CMakeLists.txt file in the top level directory when</p><NewLine></blockquote><NewLine></aside><NewLine><p>Oh. I did not see CMakeLists.txt. I will try to clone again.<br/><NewLine>Thanks,</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ph0123; <NewLine> ,"REPLY_DATE 1: April 17, 2020,  9:19am; <NewLine> REPLY_DATE 2: April 17, 2020, 11:01am; <NewLine> REPLY_DATE 3: April 18, 2020, 12:53am; <NewLine> REPLY_DATE 4: April 18, 2020,  5:48pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> 
60255,Dose SyncBN with DDP support different data size in GPUs,2019-11-07T08:54:29.626Z,0,206,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am working with a scripts that each training instance has different mumber of points, so when I applied DDP with syncBN, how will the BN stats in different GPUs to sync? I have traced the source code to here “<a href=""https://github.com/pytorch/pytorch/blob/a4a5b6fcaae26fe241d32a7c4b2091ee69b600bb/torch/nn/modules/_functions.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/a4a5b6fcaae26fe241d32a7c4b2091ee69b600bb/torch/nn/modules/_functions.py</a>”  L33-L43</p><NewLine><pre><code class=""lang-auto"">        # calcualte global mean &amp; invstd<NewLine>        mean, invstd = torch.batch_norm_gather_stats_with_counts(<NewLine>            input,<NewLine>            mean_all,<NewLine>            invstd_all,<NewLine>            running_mean,<NewLine>            running_var,<NewLine>            momentum,<NewLine>            eps,<NewLine>            count_all.view(-1).long().tolist()<NewLine>        )<NewLine></code></pre><NewLine><p>In my case the “mean_all” and “invstd_all” should be weighted average accroding to different “counts” in GPUs, is it the actual situation?</p><NewLine><p>BTW, the syncBN in NVIDIA apex just simply average “mean_all” and “invstd_all” which not support for different counts in GPUs.</p><NewLine><p>Thanks very much</p><NewLine></div>",https://discuss.pytorch.org/u/wbhu,(Hu Wenbo),wbhu,"November 7, 2019,  9:00am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>In my case the “mean_all” and “invstd_all” should be weighted average according to different “counts” in GPUs, is it the actual situation?</p><NewLine></blockquote><NewLine><p>I think you’re right.</p><NewLine><p><code>torch.batch_norm_gather_stats_with_counts</code> leads to <code>aten\src\ATen\native\cuda\Normalization.cuh</code>.</p><NewLine><p>The function you’re finding is <code>batch_norm_reduce_statistics_kernel</code>.<br/><NewLine>In the loop starts at L405, you can see that all statistics are calculated w.r.t. their own <code>count</code>.</p><NewLine><pre><code> for (int j = 0; j &lt; world_size; j++) {<NewLine>  scalar_t count = counts[j];<NewLine>  accscalar_t m = vec_mean[j][i];<NewLine>  accscalar_t v = accscalar_t(1.0) / (vec_invstd[j][i]);<NewLine>  v = (v * v - epsilon) * count;<NewLine>  accscalar_t factor = 1.0 / (n + count);<NewLine>  var_n += v + (avg - m) * (avg - m) * n * count * factor;<NewLine>  avg = n * factor * avg + count * factor * m;<NewLine>  n += count;<NewLine>}</code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/elmirador; <NewLine> ,"REPLY_DATE 1: April 18, 2020,  6:42am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
66495,DistributedDataParallel with single-process slower than sing-gpu,2020-01-13T09:28:09.813Z,6,1046,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi. I have a machine with multi-GPU.<br/><NewLine>And I a wrote training code  with <strong>Single-Process</strong> Multi-GPU according to this <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel"" rel=""nofollow noopener"">docs</a>.</p><NewLine><blockquote><NewLine><p>Single-Process Multi-GPU<br/><NewLine>In this case, a single process will be spawned on each host/node and each process will operate on all the GPUs of the node where it’s running. To use  <code>DistributedDataParallel</code>  in this way, you can simply construct the model as the following:</p><NewLine></blockquote><NewLine><p>But I found that it slower than just using single gpu. There must be something wrong in my code.</p><NewLine><h2>code</h2><NewLine><pre><code class=""lang-python""># file: main.py<NewLine>torch.distributed.init_process_group(backend=""nccl"")<NewLine><NewLine># dataset<NewLine>class RandomDataset(Dataset):<NewLine>    def __getitem__(self, index):<NewLine>        return torch.randn(3,255,255),0<NewLine><NewLine>    def __len__(self):<NewLine>        return 100<NewLine>datasets = RandomDataset()<NewLine>sampler = DistributedSampler(datasets)<NewLine>dataloader = DataLoader(datasets,16,sampler=sampler)<NewLine><NewLine># model<NewLine>model = torch.nn.Sequential(<NewLine>  torchvision.models.resnet101(False),<NewLine>  torch.nn.Linear(1000,2)<NewLine>).cuda()<NewLine>model = DistributedDataParallel(model)<NewLine><NewLine>begin_time = time.time()<NewLine># training loop<NewLine>for i in range(10):<NewLine>    for x, y in dataloader:<NewLine>        x = x.cuda()<NewLine>        y = y.reshape(-1).cuda()<NewLine>        optimizer.zero_grad()<NewLine><NewLine>        output = model(x)<NewLine>        loss = critertion(output,y)<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine>print('Cost:',time.time()-begin_time)<NewLine></code></pre><NewLine><h2>launch with</h2><NewLine><pre><code class=""lang-bash"">CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=1 main.py<NewLine></code></pre><NewLine><h2>Time cost:</h2><NewLine><div class=""md-table""><NewLine><table><NewLine><thead><NewLine><tr><NewLine><th>DistributedDataParallel with single-process and 2-gpu</th><NewLine><th>Single-gpu</th><NewLine></tr><NewLine></thead><NewLine><tbody><NewLine><tr><NewLine><td>22s</td><NewLine><td>19s</td><NewLine></tr><NewLine></tbody><NewLine></table><NewLine></div><h2>GPU memory cost:</h2><NewLine><div class=""md-table""><NewLine><table><NewLine><thead><NewLine><tr><NewLine><th>DistributedDataParallel with single-process and 2-gpu</th><NewLine><th>Single-gpu</th><NewLine></tr><NewLine></thead><NewLine><tbody><NewLine><tr><NewLine><td>3101MiB(GPU 0) /  2895MiB(GPU 1)</td><NewLine><td>4207MiB</td><NewLine></tr><NewLine></tbody><NewLine></table><NewLine></div><p>I’ve been debuging and looking docs for hours. I’ll be appreciated that somebody have a look.<br/><NewLine>thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Jack5358,(Jack),Jack5358,"January 13, 2020,  9:30am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/jack5358"">@Jack5358</a></p><NewLine><p>DistributedDataParallel’s single-process-multi-gpu mode is not recommended, because it does parameter replication, input split, output gather, etc. in every iteration, and Python GIL might get in the way. If you just have one machine, with one process per machine, then it will be very similar to DataParallel.</p><NewLine><p>The recommended solution is to use single-process-single-gpu, which means, in your use case with two GPUs, you can spawn two processes, and each process exclusively works on one GPU. This should be faster than the current setup.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a><br/><NewLine>Do you have code reference for this recommended solution you are proposing? single-process-single-gpu?<br/><NewLine>Thanks<br/><NewLine>gmondaut</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can find some examples under the “Multi-Process Single-GPU” section in <a href=""https://pytorch.org/docs/stable/nn.html#distributeddataparallel"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/nn.html#distributeddataparallel</a>.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I meet the same slower question. I use multi nodes and multi gpus, also with spawn. After checking the code, I find the time costs heavily in optimizer.step(). Any solutions?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/xiefeiwhu"">@xiefeiwhu</a></p><NewLine><p><code>optimizer.step()</code> is not part of DDP forward-backward. Which optimizer are you using? and do you observe the same slowness in local training.</p><NewLine><p>BTW, how did you measure the delay? You might need to use <a href=""https://pytorch.org/docs/stable/cuda.html#torch.cuda.Event.elapsed_time"" rel=""nofollow noopener"">CUDA events</a> to get accurate timing measures, as there could be pending ops in the CUDA stream so that <code>time.time()</code> cannot faithfully represent the time cost.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, I missed this. Yes please checkout <a href=""https://pytorch.org/docs/master/notes/ddp.html#example"" rel=""nofollow noopener"">this example</a> that uses <code>device_ids=[rank]</code> to specify which device DDP should use.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I use Adam as the optimizer. After a carefully check with the code, the slowness seems to be the communication time between the nodes as my model is about 180M params, i.e. 760MB. The computation time is faster than the communication time. Then, I expand the nodes from 2 to 4 and the communication time is bigger but not 2 times which accelerates the training procedure to some extent.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can try ProcessGroupRoundRobin and see if it helps in reducing the communication time. Example usage: <a href=""https://github.com/pytorch/pytorch/blob/master/test/distributed/test_c10d.py#L1511"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/test/distributed/test_c10d.py#L1511</a>. Note that this API is not officially supported yet.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/gmondaut; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/xiefeiwhu; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/xiefeiwhu; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> ,"REPLY_DATE 1: August 4, 2020, 11:33am; <NewLine> REPLY_DATE 2: February 7, 2020,  9:59am; <NewLine> REPLY_DATE 3: February 9, 2020,  2:41am; <NewLine> REPLY_DATE 4: April 16, 2020,  3:23pm; <NewLine> REPLY_DATE 5: April 16, 2020,  7:51pm; <NewLine> REPLY_DATE 6: April 16, 2020,  7:53pm; <NewLine> REPLY_DATE 7: April 17, 2020,  1:38am; <NewLine> REPLY_DATE 8: April 18, 2020, 12:50am; <NewLine> ",REPLY 1 LIKES: 3 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
77244,Distributed ImageNet Example Multi GPU Question,2020-04-17T21:38:21.618Z,1,66,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>In the ImageNet example linked here <a href=""https://github.com/pytorch/examples/blob/master/imagenet/main.py"" rel=""nofollow noopener"">https://github.com/pytorch/examples/blob/master/imagenet/main.py</a>, when we call the main_worker through mp.spawn, how is the main_worker getting the GPU argument? When I try to run this with 2 nodes that have 2 GPUs each, I always see this parameter to be None. It works for multiple nodes with single GPUs.</p><NewLine></div>",https://discuss.pytorch.org/u/ayushm-agrawal,(Ayush Manish Agrawal),ayushm-agrawal,"April 17, 2020,  9:38pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>multiprocessing.spawn</code> will feed the process id as the first argument to the target function. Here is the API doc: <a href=""https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: April 19, 2020, 12:26am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
77249,All_reduce a list of tensors,2020-04-17T22:14:26.366Z,0,97,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am getting the following error when I am trying to all reduce a list of tensor</p><NewLine><p><code>RuntimeError: Tensors must be contiguous</code></p><NewLine><p>Here is a snippet of code</p><NewLine><pre><code class=""lang-auto"">t_ = [...] # list of tensors<NewLine>for t in t_: <NewLine>   dist.all_reduce(t, , op=dist.ReduceOp.SUM, group=group)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/babababadukeduke,(Sarvagya Vatsal Singh),babababadukeduke,"April 17, 2020, 10:15pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Figure it out! I have to make the tensor contiguous before reducing it. This <a href=""https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2"">article</a> provides a nice description of it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/babababadukeduke; <NewLine> ,"REPLY_DATE 1: April 17, 2020, 10:21pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
77114,Custom methods in DistributedDataParallel,2020-04-16T22:18:31.745Z,7,178,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to do multi gpu training with DistributedDataParallel. I wrap it around my model. However my model has a custom function that now i call by doing model.module.function(x). I was wondering if this is ok and if something bad will happen. Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/hij,,hij,"April 16, 2020, 10:18pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What does this custom function do? and when do you call this custom function? If it does not modify parameters and the autograd graph built during the forward pass, it should be OK.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The pseudo code is something like this</p><NewLine><pre><code class=""lang-auto"">output = model(input)<NewLine>output2 = model(input2)<NewLine>final_output = model.module.function(output, output2)<NewLine>loss = loss_function(final_output)<NewLine>optimizer.zero_grad()<NewLine>loss.backward()<NewLine>optimizer.step()<NewLine></code></pre><NewLine><p>Would this be fine? The custom function is just a MLP to classify something. It does not change anything, but I want it to get updated when I call my optimizer.step()</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If <code>model</code> is a <code>DistributedDataParallel</code> (DDP) instance, this won’t work. Because setup some internal states at the end of the forward pass, and does not work if you call <code>forward</code> twice without a backward in between.</p><NewLine><p>However, this can be easily solve by wrapping the two forward and the <code>function</code> invocation into a wrapper model, and then pass that wrapper model to DDP, sth like:</p><NewLine><pre><code class=""lang-auto"">class WrapperModel(nn.Module):<NewLine>  def __init__(self, model) :<NewLine>    super(WrapperModel, self).__init__()<NewLine>    self.model = model<NewLine><NewLine>  def forward(input, input2): <NewLine>    output = model(input)<NewLine>    output2 = model(input2)<NewLine>    final_output = model.module.function(output, output2)<NewLine>    return final_output<NewLine><NewLine>ddp = DistributedDataParallel(WrapperModel(model).to(device), device_ids=[device])<NewLine><NewLine>final_output = ddp.forward(input, input2)<NewLine>loss = loss_function(final_output)<NewLine>optimizer.zero_grad()<NewLine>loss.backward()<NewLine>optimizer.step()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I called broadcast_buffers=False so I didnt have an issue calling forward twice. In that case, is it fine if i call my custom function the way I did and will the gradients be correct?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>If the <code>model.module.function</code> is not using the parameters in the model, it should work.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>A little more details on my method. Pseudo code is</p><NewLine><pre><code class=""lang-auto"">class model(nn.Module):<NewLine>  def __init__(self) :<NewLine>    super(model, self).__init__()<NewLine>    self.encoder = Encoder()<NewLine>    self.decoder = Decoder()<NewLine>    self.mlp = MLP()<NewLine>  def encode(self, x):<NewLine>    return self.encoder(x)<NewLine>  def decode(self, x): <NewLine>    return self.decoder(x)<NewLine>  def classify(self, a, b)<NewLine>    return self.mlp(a, b)<NewLine>  def forward(self, x):<NewLine>    enc = self.encode(x)<NewLine>    out = self.decode(enc)<NewLine>    return enc, out<NewLine># this is my main training script<NewLine>enc, out = model(x)<NewLine>enc2 = enc + d #d is some random perturbations<NewLine>out2 = model.module.decode(enc2)<NewLine>pred = model.module.classify(enc, enc2)<NewLine></code></pre><NewLine><p>There are a bunch of other stuff, but in this scenario, my decode function is using the parameters in model? Would this be an issue? There are no errors when running.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>how do yo compute the final loss (the one where <code>backward</code> is launched from)? I assume both <code>end</code> and <code>out</code> contribute to that loss? If so, this looks OK to me.</p><NewLine><p>This should be an issue for your current use case,  but I want to mention that this probably won’t work correctly with <code>find_unused_parameters=True</code> mode. Because the <code>mlp</code> is used outside of <code>forward</code>, and DDP will find unused parameters using <code>forward</code> output. So in that mode, DDP would treat parameters in <code>mlp</code> as unused parameters although they are actually part of the autograd graph.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>my loss functions is something like</p><NewLine><pre><code class=""lang-auto"">loss1 = adv_loss(out) #make output image look realistic<NewLine>loss2 = adv_loss(out2)<NewLine>loss3 = adv_loss(enc) #make encoding normal distributed<NewLine>loss4 = adv_loss(enc2)<NewLine>loss5 = l1_loss(out, x) # reconstruction loss<NewLine>loss6 = l1_loss(out2, x)<NewLine>loss7 = cross_entropy_loss(pred, GT)<NewLine></code></pre><NewLine><p>I dont have find_unuse_parameters=True and have no error. If i understand what you are saying, the gradients are fine?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, I think the gradients should be fine.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hij; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/hij; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/hij; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/hij; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: April 16, 2020, 10:54pm; <NewLine> REPLY_DATE 2: April 16, 2020, 11:30pm; <NewLine> REPLY_DATE 3: April 17, 2020, 12:51am; <NewLine> REPLY_DATE 4: April 17, 2020,  1:54am; <NewLine> REPLY_DATE 5: April 17, 2020,  2:23am; <NewLine> REPLY_DATE 6: April 17, 2020,  2:48am; <NewLine> REPLY_DATE 7: April 17, 2020,  3:32am; <NewLine> REPLY_DATE 8: April 17, 2020,  5:41am; <NewLine> REPLY_DATE 9: April 18, 2020, 12:23am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> 
76132,Passing results between workers using RPC framework,2020-04-10T05:10:30.419Z,11,252,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Dear <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>I have successfully run the RRN RPC tutorial/example shown in the link below:</p><NewLine><p><a class=""onebox"" href=""https://pytorch.org/tutorials/intermediate/rpc_tutorial.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/intermediate/rpc_tutorial.html</a></p><NewLine><p>This is very helpful for me to understand the basics of the RPC framework and has demonstrated it very clear how to do distributed training with two machines (nodes).<br/><NewLine>However, my question is what if we have 3 or more nodes (workers) and we want to split the model into submodels on each machine/node/worker. How should I use the RPC framework to help to pass the intermediate result of the previous worker to the next worker?</p><NewLine><p>Take the RNN tutorial/model as an example:</p><NewLine><p>In the tutorial, basically it does this in the forward pass:<br/><NewLine>def forward(self, input, hidden):<br/><NewLine># pass input to the remote embedding table and fetch emb tensor back<br/><NewLine>emb = _remote_method(EmbeddingTable.forward, self.emb_table_rref, input)<br/><NewLine>output, hidden = self.rnn(emb, hidden)<br/><NewLine># pass output to the rremote decoder and get the decoded output back<br/><NewLine>decoded = _remote_method(Decoder.forward, self.decoder_rref, output)<br/><NewLine>return decoded, hidden</p><NewLine><ol><NewLine><li><NewLine><p>By using the rpc.remote and rpc_sync, I can have the EmbddingTable and do the forward pass remotely on the worker#1 and get the EmbddingTable’s result back locally.</p><NewLine></li><NewLine><li><NewLine><p>Then I pass the EmbddingTable’s result to my local RNN (worker#0) and get the corresponding RNN result.</p><NewLine></li><NewLine><li><NewLine><p>I have another Decoder remotely on worker#1 again, and I push the RNN result to that Decoder and then get the result back by using rpc_sync</p><NewLine></li><NewLine></ol><NewLine><p>However, what if I have three workers, worker#0 (local), worker#1 and worker#2. But this time, I put the RNN model on remote worker#2 and I want to have the communication like below:</p><NewLine><ol><NewLine><li><NewLine><p>From worker#0, I push the input to the EmbddingTable on worker#1. After worker#1 finishes the calculation, it passes the result to the RNN on worker#2.</p><NewLine></li><NewLine><li><NewLine><p>The RNN on worker#2 calculates and passes the result back to worker#1 for the Decoder.</p><NewLine></li><NewLine><li><NewLine><p>After the Decoder on worker#1 finishes the computation, I (on worker 0) use rpc_sync or to_here to get the final result back to local.</p><NewLine></li><NewLine></ol><NewLine><p>Would you think this is possible and let me know how to do this? Besides, can the Distributed Autograd and the Distributed Optimizer still be applied in this scenario?</p><NewLine><p>One of my thoughts is that if I could make an RRef to each submodule’s output and pass them among the workers.</p><NewLine><p>Thank you very much in advance for your time and help!</p><NewLine><p>Best,<br/><NewLine>Ziyi</p><NewLine></div>",https://discuss.pytorch.org/u/ZiyiZhu,(Ziyi Zhu),ZiyiZhu,"April 10, 2020,  5:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ziyizhu"">@ZiyiZhu</a>, thanks for trying out RPC.</p><NewLine><blockquote><NewLine><p>Would you think this is possible and let me know how to do this?</p><NewLine></blockquote><NewLine><blockquote><NewLine><p>One of my thoughts is that if I could make an RRef to each submodule’s output and pass them among the workers.</p><NewLine></blockquote><NewLine><h3>Solution I</h3><NewLine><p>Yes, this is possible, and using RRef is certainly one proper solution. More specifically, we can let worker 0 serve as a master here. Sth like</p><NewLine><pre><code class=""lang-python""># On worker 0<NewLine>emb_lookup_rref = rpc.remote(""worker1"", EmbeddingTable.forward, args=(input,))<NewLine># note that RNN.forward needs to call to_here() on emb_lookup_rref<NewLine>rnn_result_rref = rpc.remote(""worker2"", RNN.forward, args=(emb_lookup_rref,))<NewLine># similarly Decoder also needs to call to_here() on rnn_result_rref<NewLine>decoded_rref = rpc.remote(""worker1"", Decoder.forward, args=(rnn_result_rref,))<NewLine>final_result = decoded_rref.to_here()<NewLine></code></pre><NewLine><p>Above should work. Although it would result in several additional light-weight messages to manage internal RRef reference count, it shouldn’t slow down training. Because <code>rpc.remote</code> is non-blockng, it returns an RRef immediately. It’s very likely that the RNN module is already waiting on <code>to_here()</code> to get the embedding lookup result even before the EmbeddingTable finished processing the request. So that there shouldn’t be noticeable delay on the critical path.</p><NewLine><h3>Solution II</h3><NewLine><p>An alternative solution is to use nested RPC. You can wrap the EmbeddingTable -&gt; RNN -&gt; Decoder into one module forward function (Say <code>MyModel.forward</code>), and then let worker 0 to run <code>rpc.rpc_sync(""worker1"", MyModel.forward, args=(input,))</code>. Within <code>MyModel.forward</code>, it can also use rpc/remote to communicate with worker 2, sth like:</p><NewLine><pre><code class=""lang-python"">class MyModel(nn.Module):<NewLine>    def forward(self, input):<NewLine>        lookup_result = self.emb(input)<NewLine>        # here is directly pass the lookup result instead of RRef to RNN<NewLine>        rnn_result = rpc.rpc_sync(""worker2"", RNN.forward, inputs=(lookup_result))<NewLine>        return self.decoder(rnn_result)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""76132"" data-username=""ZiyiZhu""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/z/ce7236/40.png"" width=""20""/> ZiyiZhu:</div><NewLine><blockquote><NewLine><p>Besides, can the Distributed Autograd and the Distributed Optimizer still be applied in this scenario?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Sorry I missed this. Yes, both of them would still work in this case, as long as you wrap the top-level (not nested) RPCs with distributed autograd context. All RPCs originated from that context will propagate the context id, so that autograd and optimizer on different workers will be able to find the context properly.</p><NewLine><p>For Distributed Optimizer, as long as 1) you provide a correct list of param RRefs to its constructor 2) its <code>step()</code> function is wrapped by the correct dist autograd context, it should work. It does not care where those parameters live.</p><NewLine><p>BTW, in the next release, we are making dist autograd/optimizer functional. They will take the context id as an argument and does not need to be wrapped by a <code>with context</code> statement anymore.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>Thank you very much for the solutions. I have tried the first solution quickly and it can work! I will try it with the Distributed Autograd and Optimizer once I construct the entire network for training. However, one of my concerns is that when we train the network, there are many iterations and epochs, which means we will have lots of forwarding passes of the following:</p><NewLine><pre><code class=""lang-auto""># On worker 0<NewLine>emb_lookup_rref = rpc.remote(""worker1"", EmbeddingTable.forward, args=(input,))<NewLine># note that RNN.forward needs to call to_here() on emb_lookup_rref<NewLine>rnn_result_rref = rpc.remote(""worker2"", RNN.forward, args=(emb_lookup_rref,))<NewLine># similarly Decoder also needs to call to_here() on rnn_result_rref<NewLine>decoded_rref = rpc.remote(""worker1"", Decoder.forward, args=(rnn_result_rref,))<NewLine>final_result = decoded_rref.to_here()<NewLine></code></pre><NewLine><p>Tons of RRefs (emb_lookup_rref ,  rnn_result_rref , and decoded_rref ) will be created by the rpc.remote . Should I worry about this? Or these RRefs will be deconstructed automatically?</p><NewLine><p>Thank you!!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""76132"" data-username=""ZiyiZhu""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/z/ce7236/40.png"" width=""20""/> ZiyiZhu:</div><NewLine><blockquote><NewLine><p>Or these RRefs will be deconstructed automatically?</p><NewLine></blockquote><NewLine></aside><NewLine><p>RPC will automatically track RRef reference count. <a href=""https://pytorch.org/docs/stable/notes/rref.html"" rel=""nofollow noopener"">This</a> describes the algorithm. The object referenced by the RRef will be deleted automatically when the reference count drops to 0. So, they should be deleted automatically, and we saw it works correctly in intensive training applications. One thing I want to mention is that this relies on Python GC to delete vars like <code>emb_lookup_rref</code> in time though, which should be the case if there is no circular reference that points to the RRef. Let us know if you hit OOM due to RRef. We can probably expose the deletion APIs explicitly if necessary.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Dear <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>I tested the RPC framework with two nodes for a model parallelism implementation. The distributed Autograd and Optimizer can work successfully as the way I constructed them following the template in the RPC tutorial <a href=""https://pytorch.org/tutorials/intermediate/rpc_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/rpc_tutorial.html</a>. However, I do see the memory problem in the GPU and the memory usage grows with the number of epochs. I wonder if you could let me know what could be the problem.</p><NewLine><p>I constructed a very simple CNN for the classification of the FashionMNIST dataset. Then I divided it into two submodels, one for convolutional layers and the other for fully-connected layers as below:</p><NewLine><pre><code class=""lang-auto"">class ConvNet(nn.Module):<NewLine><NewLine>    def __init__(self, device):<NewLine>        super().__init__()<NewLine>        self.device = device<NewLine>        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5).to(self.device)<NewLine>        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5).to(self.device)<NewLine><NewLine>    def forward(self, rref):<NewLine>        t = rref.to_here().to(self.device)<NewLine>        # conv 1<NewLine>        t = self.conv1(t)<NewLine>        t = F.relu(t)<NewLine>        t = F.max_pool2d(t, kernel_size=2, stride=2)<NewLine><NewLine>        # conv 2<NewLine>        t = self.conv2(t)<NewLine>        t = F.relu(t)<NewLine>        t = F.max_pool2d(t, kernel_size=2, stride=2)<NewLine><NewLine>        return t.cpu()<NewLine>    <NewLine>class FCNet(nn.Module):<NewLine><NewLine>    def __init__(self,device):<NewLine>        super().__init__()<NewLine>        self.device = device<NewLine>        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120).to(self.device)<NewLine>        self.fc2 = nn.Linear(in_features=120, out_features=60).to(self.device)<NewLine>        self.out = nn.Linear(in_features=60, out_features=10).to(self.device)<NewLine><NewLine>    def forward(self, rref):<NewLine>       <NewLine>        t = rref.to_here().to(self.device)<NewLine><NewLine>        # fc1<NewLine>        t = t.reshape(-1, 12*4*4)<NewLine>        t = self.fc1(t)<NewLine>        t = F.relu(t)<NewLine><NewLine>        # fc2<NewLine>        t = self.fc2(t)<NewLine>        t = F.relu(t)<NewLine><NewLine>        # output<NewLine>        t = self.out(t)<NewLine>        # don't need softmax here since we'll use cross-entropy as activation.<NewLine><NewLine>        return t.cpu()<NewLine></code></pre><NewLine><p>To wrap them up, I created another CNNModel class for the purpose and perform the forward pass:</p><NewLine><pre><code class=""lang-auto"">class CNNModel(nn.Module):<NewLine>    def __init__(self, connet_wk, fcnet_wk, device):<NewLine>        super(CNNModel, self).__init__()<NewLine><NewLine>        # setup embedding table remotely<NewLine>        self.device = device<NewLine>        <NewLine>        self.convnet_rref = rpc.remote(connet_wk, ConvNet,args=(device,))<NewLine>        # setup LSTM locally<NewLine>        print(self.convnet_rref.to_here())<NewLine>        self.fcnet_rref = rpc.remote(fcnet_wk, FCNet,args=(device,))<NewLine>        print(self.fcnet_rref.to_here())<NewLine>        print('CNN model constructed: ' + 'owner')<NewLine><NewLine><NewLine>    def forward(self, inputreff):<NewLine>        <NewLine>        convnet_forward_rref = rpc.remote(self.convnet_rref.owner(), _call_method, args=(ConvNet.forward, self.convnet_rref, inputreff))<NewLine>        <NewLine>        fcnet_forward_rref = rpc.remote(self.fcnet_rref.owner(), _call_method, args=(FCNet.forward, self.fcnet_rref, convnet_forward_rref))<NewLine>                                                                    <NewLine>        return fcnet_forward_rref<NewLine>    <NewLine>    def parameter_rrefs(self):<NewLine>        remote_params = []<NewLine>        remote_params.extend(_remote_method(_parameter_rrefs, self.convnet_rref))<NewLine>        remote_params.extend(_remote_method(_parameter_rrefs, self.fcnet_rref))<NewLine>        return remote_params<NewLine></code></pre><NewLine><p>For training, I have a trainer to do that using Distributed Autograd and Optimiser:</p><NewLine><pre><code class=""lang-auto""><NewLine>class Trainer(object):<NewLine><NewLine>    def __init__(self, model, optimizer, train_loader, test_loader, device):<NewLine>        self.model = model<NewLine>        self.optimizer = optimizer<NewLine>        self.train_loader = train_loader<NewLine>        self.test_loader = test_loader<NewLine>        self.device = device<NewLine><NewLine>    def fit(self, epochs):<NewLine>        for epoch in range(1, epochs + 1):<NewLine>            train_loss, train_acc = self.train()<NewLine>            test_loss, test_acc = self.evaluate()<NewLine><NewLine>            print(<NewLine>                'Epoch: {}/{},'.format(epoch, epochs),<NewLine>                'train loss: {}, train acc: {},'.format(train_loss, train_acc),<NewLine>                'test loss: {}, test acc: {}.'.format(test_loss, test_acc),<NewLine>            )<NewLine><NewLine>    def train(self):<NewLine><NewLine>        train_loss = Average()<NewLine>        train_acc = Accuracy()<NewLine><NewLine>        for data, target in self.train_loader:<NewLine>            with dist_autograd.context() as context_id:<NewLine>                data_ref = RRef(data)<NewLine><NewLine>                output_ref = self.model(data_ref)<NewLine>                output = output_ref.to_here()<NewLine>                loss = F.cross_entropy(output, target)<NewLine><NewLine>                dist_autograd.backward([loss])<NewLine>                self.optimizer.step()<NewLine><NewLine>                train_loss.update(loss.item(), data.size(0))<NewLine>                train_acc.update(output, target)<NewLine><NewLine>        return train_loss, train_acc<NewLine><NewLine>    def evaluate(self):<NewLine>        self.model.eval()<NewLine><NewLine>        test_loss = Average()<NewLine>        test_acc = Accuracy()<NewLine><NewLine>        with torch.no_grad():<NewLine>            for data, target in self.test_loader:<NewLine>                with dist_autograd.context() as context_id:<NewLine>                    data_ref = RRef(data)<NewLine><NewLine>                    output_ref = self.model(data_ref)<NewLine>                    output = output_ref.to_here()<NewLine>                    loss = F.cross_entropy(output, target)<NewLine><NewLine>                    test_loss.update(loss.item(), data.size(0))<NewLine>                    test_acc.update(output, target)<NewLine><NewLine>        return test_loss, test_acc<NewLine></code></pre><NewLine><p>At the top level, I created a CNNModel, initialized the RPC framework and passed the Distributed Optimizer to the trainer:</p><NewLine><pre><code class=""lang-auto"">**#Worker 0**<NewLine><NewLine>def run(args):<NewLine>    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<NewLine>    print(device)<NewLine>    model = CNNModel(args['host'], args['worker'],device)<NewLine>    <NewLine>    # setup distributed optimizer<NewLine>    opt = DistributedOptimizer(<NewLine>        optim.Adam,<NewLine>        model.parameter_rrefs(),<NewLine>        lr=args['lr'],<NewLine>    )<NewLine><NewLine>    train_loader = MNISTDataLoader(args['root'], args['batch_size'], train=True)<NewLine>    test_loader = MNISTDataLoader(args['root'], args['batch_size'], train=False)<NewLine><NewLine>    trainer = Trainer(model, opt, train_loader, test_loader, device)<NewLine>    trainer.fit(args['epochs'])<NewLine><NewLine>def main():<NewLine>    argv = {'world_size': int(2),<NewLine>            'rank': int(0),<NewLine>            'host': ""worker0"",<NewLine>            'worker': ""worker1"",<NewLine>            'epochs': int(10),<NewLine>            'lr': float(1e-3),<NewLine>            'root': 'data',<NewLine>            'batch_size': int(32)<NewLine>           }<NewLine>    <NewLine>    print(argv)<NewLine>    rpc.init_rpc(argv['host'], rank=argv['rank'], world_size=argv['world_size'])<NewLine>    print('Start Run', argv['rank'])<NewLine>    run(argv)<NewLine>    rpc.shutdown()<NewLine><NewLine>os.environ['MASTER_ADDR'] = '10.142.0.13'#Google Cloud<NewLine>#os.environ['MASTER_ADDR'] = 'localhost' #local<NewLine>os.environ['MASTER_PORT'] = '29505'<NewLine>main()<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">**#Worker 1**<NewLine><NewLine>def main():<NewLine>    argv = {'world_size': int(2),<NewLine>            'rank': int(1),<NewLine>            'host': 'worker0',<NewLine>            'worker': 'worker1',<NewLine>            'epochs': int(10),<NewLine>            'lr': float(1e-3),<NewLine>            'root': 'data',<NewLine>            'batch_size': int(32)<NewLine>           }<NewLine>    <NewLine>    print(argv)<NewLine>    rpc.init_rpc(argv['worker'], rank=argv['rank'], world_size=argv['world_size'])<NewLine>    print('Start Run', argv['rank'])<NewLine>    rpc.shutdown()<NewLine><NewLine>os.environ['MASTER_ADDR'] = '10.142.0.13'#Google Cloud<NewLine>#os.environ['MASTER_ADDR'] = 'localhost' #local<NewLine>os.environ['MASTER_PORT'] = '29505'<NewLine>main()<NewLine></code></pre><NewLine><p>The dataloader is as the following:</p><NewLine><pre><code class=""lang-auto"">from torch.utils import data<NewLine>from torchvision import datasets, transforms<NewLine><NewLine>class MNISTDataLoader(data.DataLoader):<NewLine><NewLine>    def __init__(self, root, batch_size, train=True):<NewLine>        transform = transforms.Compose([<NewLine>            transforms.ToTensor(),<NewLine>            transforms.Normalize((0.1307,), (0.3081,)),<NewLine>        ])<NewLine><NewLine>        dataset = datasets.FashionMNIST(root, train=train, transform=transform, download=True)<NewLine>        <NewLine><NewLine>        super(MNISTDataLoader, self).__init__(<NewLine>            dataset,<NewLine>            batch_size=batch_size,<NewLine>            shuffle=train,<NewLine>        )<NewLine></code></pre><NewLine><p>I showed all the details above but I guess the problem could be the way I constructed the CNNModel for the ConvNet and FCNet. I wonder if you could take a look at the code and give some hints on where could be the problems?</p><NewLine><p>Thank you very much for your time!</p><NewLine><p>Best,<br/><NewLine>Ziyi</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>How fast does the memory usage increase? Does it keeps increasing after every epoch or stabilized after a few epoches?</p><NewLine><p>It could be due to RRef or distributed autograd context wasn’t deleted in time. It might worth provide an API to block waiting for all RPC workers to clear RRefs and dist autograd contexts. cc <a class=""mention"" href=""/u/pritamdamania87"">@pritamdamania87</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, what I wrote in the previous post was not clear. The GPU usage keeps growing while you are training not necessarily having a relation to the epochs. I took a closer look at the GPU memory usage. On worker <span class=""hashtag"">#1</span>, I kept using</p><NewLine><pre><code class=""lang-auto"">nvidia-smi<NewLine></code></pre><NewLine><p>while the program is running and the memory usage kept increasing. The original GPU memory usage should be small and around 500MB. But if I use the RPC it will keep growing, and every second I typed nvidia-smi and I can see a few MB increased and if it is between epochs, I can also see a big jump of increment of memory usage.</p><NewLine><p>Best,<br/><NewLine>Ziyi</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like there is a leak.</p><NewLine><p>For RRef, there is an internal API <code>_rref_context_get_debug_info</code> to check the number of living <code>OwerRRef</code>s (with key <code>num_owner_rrefs</code>). Here are <a href=""https://github.com/pytorch/pytorch/blob/f59e646faa6ec7c388735aeda4a5bd7eb8eb0be2/torch/testing/_internal/distributed/rpc/rpc_test.py#L1380-L1446"" rel=""nofollow noopener"">examples in tests</a>.</p><NewLine><p>Similarly, distributed autograd can also show the number of living backward passes. [<a href=""https://github.com/pytorch/pytorch/blob/f59e646faa6ec7c388735aeda4a5bd7eb8eb0be2/torch/testing/_internal/distributed/rpc/dist_autograd_test.py#L1757"" rel=""nofollow noopener"">example</a>]</p><NewLine><p>BTW, which version of PyTorch are you using? I recall we fixed some memory leaks after v1.4 release, e.g., this <a href=""https://github.com/pytorch/pytorch/pull/31030"" rel=""nofollow noopener"">PR</a>.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>I am using Pytorch 1.4.0 installed by the following command on the official website:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/7fa2734fbfdcc357db2e52787e5ff28fff1974ba"" href=""https://discuss.pytorch.org/uploads/default/original/3X/7/f/7fa2734fbfdcc357db2e52787e5ff28fff1974ba.png"" title=""image""><img alt=""image"" data-base62-sha1=""id6GBB2PQifdL0d3yyf8fJncz46"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/7/f/7fa2734fbfdcc357db2e52787e5ff28fff1974ba_2_10x10.png"" height=""195"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/7/f/7fa2734fbfdcc357db2e52787e5ff28fff1974ba_2_517x195.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/7/f/7fa2734fbfdcc357db2e52787e5ff28fff1974ba_2_517x195.png, https://discuss.pytorch.org/uploads/default/optimized/3X/7/f/7fa2734fbfdcc357db2e52787e5ff28fff1974ba_2_775x292.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/7/f/7fa2734fbfdcc357db2e52787e5ff28fff1974ba.png 2x"" width=""517""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1029×390 20.1 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><pre><code class=""lang-auto"">conda install pytorch torchvision cudatoolkit=10.1 -c pytorch<NewLine></code></pre><NewLine><p>Sure, I will check the posts and see if I can figure out the problem by myself. Please let me know if the version is a problem, and it is much appreciated if you could share more thoughts on the code I provided above.</p><NewLine><p>Thank you!</p><NewLine><p>Best,<br/><NewLine>Ziyi</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Please let me know if the version is a problem</p><NewLine></blockquote><NewLine><p>Yes, the memory leak could be fixed by new PRs landed after v1.4 (v1.4 branch cut was 12/08/2019 IIRC). Can you try the master branch, or the nightly (<code>conda install pytorch torchvision -c pytorch-nightly</code>), or if you can share a self-runnable script, we can try that too.</p><NewLine><blockquote><NewLine><p>if you could share more thoughts on the code I provided above.</p><NewLine></blockquote><NewLine><p>The code you share above looks correct to me.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>Yes, I just tested my code with the PyTorch-nightly version and it does not have the memory leakage issue anymore. Also, the syntax is more similar to what you have shown in the tutorial.</p><NewLine><p>Thank you very much for your help!</p><NewLine><p>Best,<br/><NewLine>Ziyi</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Awesome! Thanks for sharing the result!</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>Sorry, my test back then was okay. However, I just created a new instance on Google Cloud and install the lastest Pytorch-nightly then it raises up this error during training:</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/460a464ff13dff6e493a7cd0c796b27d4cf64bda"" href=""https://discuss.pytorch.org/uploads/default/original/3X/4/6/460a464ff13dff6e493a7cd0c796b27d4cf64bda.png"" title=""image""><img alt=""image"" data-base62-sha1=""9ZBr34vwhnqKT6mxiQih8OfI2RI"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/6/460a464ff13dff6e493a7cd0c796b27d4cf64bda_2_10x10.png"" height=""443"" src=""https://discuss.pytorch.org/uploads/default/original/3X/4/6/460a464ff13dff6e493a7cd0c796b27d4cf64bda.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1016×653 48.4 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>I am here to provide the code for the RPC training. Please take a look if you would have time!<br/><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""16""/><NewLine><a href=""https://github.com/ZiyiZhu/RPC_FashionMNIST"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars0.githubusercontent.com/u/17953005?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/ZiyiZhu/RPC_FashionMNIST"" rel=""nofollow noopener"" target=""_blank"">ZiyiZhu/RPC_FashionMNIST</a></h3><NewLine><p>RPC_FashionMNIST. Contribute to ZiyiZhu/RPC_FashionMNIST development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>Thank you.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>IIUC, this is caused by a race recently introduced in a <a href=""https://github.com/pytorch/pytorch/pull/35101"" rel=""nofollow noopener"">recent PR</a>. <a class=""mention"" href=""/u/pritamdamania87"">@pritamdamania87</a> has <a href=""https://github.com/pytorch/pytorch/pull/36640"" rel=""nofollow noopener"">a fix</a> for that. Btw, this bug is not in v1.5, if you can install v1.5 RC or wait for that fix to land in nightly, this error should disappear.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much for your information and response. Sure I will wait for the fix and the new release.</p><NewLine><p>Best,<br/><NewLine>Ziyi</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/ZiyiZhu; <NewLine> ,"REPLY_DATE 1: April 10, 2020,  3:30pm; <NewLine> REPLY_DATE 2: April 10, 2020,  3:36pm; <NewLine> REPLY_DATE 3: April 10, 2020,  4:13pm; <NewLine> REPLY_DATE 4: April 10, 2020,  5:07pm; <NewLine> REPLY_DATE 5: April 12, 2020,  3:20pm; <NewLine> REPLY_DATE 6: April 12, 2020,  7:22pm; <NewLine> REPLY_DATE 7: April 12, 2020,  7:41pm; <NewLine> REPLY_DATE 8: April 12, 2020,  8:13pm; <NewLine> REPLY_DATE 9: April 12, 2020,  9:02pm; <NewLine> REPLY_DATE 10: April 13, 2020,  1:52am; <NewLine> REPLY_DATE 11: April 14, 2020,  3:33pm; <NewLine> REPLY_DATE 12: April 14, 2020,  3:35pm; <NewLine> REPLY_DATE 13: April 14, 2020,  7:53pm; <NewLine> REPLY_DATE 14: April 15, 2020,  6:29pm; <NewLine> REPLY_DATE 15: April 16, 2020,  1:43pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: 1 Like; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> 
76821,BatchNorm for multi GPU Training,2020-04-15T05:35:04.144Z,1,246,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I did read that PyTorch is not supporting the so called sync BatchNorm. This is needed to<br/><NewLine>train on multi GPU machines. By question is: Are there any plans to implement sync BatchNorm<br/><NewLine>for PyTorch and when will it be released?</p><NewLine><p>An other question: What is the best workaround when you want to train with images and need<br/><NewLine>large batch sizes?</p><NewLine><p>Thanks<br/><NewLine>Philip</p><NewLine></div>",https://discuss.pytorch.org/u/PhilipMay,(Philip May),PhilipMay,"April 15, 2020,  5:35am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://pytorch.org/docs/stable/nn.html?highlight=syncbatchnorm#torch.nn.SyncBatchNorm"">SyncBatchNorm</a> is already in PyTorch.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> ,<br/><NewLine>thanks for the answer.<br/><NewLine>The documentation sais:</p><NewLine><blockquote><NewLine><p>Currently SyncBatchNorm only supports DistributedDataParallel with single GPU per process.</p><NewLine></blockquote><NewLine><p>This “single GPU” kind of irritates me. What does it mean?</p><NewLine><p>I am also asking because detectron2 still uses “FrozenBatchNorm2d”: <a href=""https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/backbone/resnet.py#L50"" rel=""nofollow noopener"">https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/backbone/resnet.py#L50</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>DistributedDataParallel</code> can be used in two different setups as given in the <a href=""https://pytorch.org/docs/master/nn.html#torch.nn.parallel.DistributedDataParallel"">docs</a>.</p><NewLine><ol><NewLine><li>Single-Process Multi-GPU and</li><NewLine><li>Multi-Process Single-GPU, which is the fastest and recommended way.</li><NewLine></ol><NewLine><p><code>SyncBatchNorm</code> will only work in the second approach.</p><NewLine><p>I’m not sure, if you would need <code>SyncBatchNorm</code>, since <code>FrozenBatchNorm</code> seems to fix all buffers:</p><NewLine><blockquote><NewLine><p>BatchNorm2d where the batch statistics and the affine parameters are fixed.<br/><NewLine>It contains non-trainable buffers called<br/><NewLine>“weight” and “bias”, “running_mean”, “running_var”,<br/><NewLine>initialized to perform identity transformation.</p><NewLine></blockquote><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/PhilipMay; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: April 15, 2020,  6:14am; <NewLine> REPLY_DATE 2: April 15, 2020,  6:29am; <NewLine> REPLY_DATE 3: April 15, 2020,  6:34am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
40824,Training model seems not to effect using distributed training with auto mixed-precision,2019-03-25T11:21:10.716Z,6,1983,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone. I’m gonna to use Nvidia Apex package to fast train my model with the help of auto mixed-precision. However even if the the loss continues to drop, the model inference dose not achieve improvement. My training code is as follows:</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import argparse<NewLine>import time<NewLine>import tqdm<NewLine>import cv2<NewLine>import torch<NewLine>import numpy as np<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine>import torch.distributed as dist<NewLine>from config.config import GetConfig, COCOSourceConfig, TrainingOpt<NewLine>from data.mydataset import MyDataset<NewLine>from torch.utils.data import DataLoader<NewLine>from models.posenet import Network<NewLine>from models.loss_model import MultiTaskLoss<NewLine>import warnings<NewLine><NewLine>try:<NewLine>    from apex.parallel import DistributedDataParallel as DDP<NewLine>    from apex.fp16_utils import *<NewLine>    from apex import amp<NewLine>    from apex.multi_tensor_apply import multi_tensor_applier<NewLine>except ImportError:<NewLine>    raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to run this example."")<NewLine><NewLine><NewLine>warnings.filterwarnings(""ignore"")<NewLine><NewLine>parser = argparse.ArgumentParser(description='PoseNet Training')<NewLine>parser.add_argument('--resume', '-r', action='store_true', default=True, help='resume from checkpoint')<NewLine>parser.add_argument('--checkpoint_path', '-p',  default='link2checkpoints_distributed', help='save path')<NewLine>parser.add_argument('--max_grad_norm', default=5, type=float,<NewLine>                    help=(""If the norm of the gradient vector exceeds this, ""<NewLine>                          ""re-normalize it to have the norm equal to max_grad_norm""))<NewLine># FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied automatically by torch.distributed.launch.<NewLine>parser.add_argument(""--local_rank"", default=0, type=int)<NewLine>parser.add_argument('--opt-level', type=str, default='O1')<NewLine>parser.add_argument('--sync_bn',  action='store_true', default=False, help='enabling apex sync BN.')  <NewLine>parser.add_argument('--keep-batchnorm-fp32', type=str, default=None)<NewLine>parser.add_argument('--loss-scale', type=str, default=None)<NewLine>parser.add_argument('--print-freq', '-f', default=10, type=int, metavar='N', help='print frequency (default: 10)')<NewLine><NewLine>torch.backends.cudnn.benchmark = True  <NewLine>use_cuda = torch.cuda.is_available()<NewLine><NewLine>args = parser.parse_args()<NewLine><NewLine>checkpoint_path = args.checkpoint_path<NewLine>opt = TrainingOpt()<NewLine>config = GetConfig(opt.config_name)<NewLine>soureconfig = COCOSourceConfig(opt.hdf5_train_data)<NewLine>train_data = MyDataset(config, soureconfig, shuffle=False, augment=True)  # shuffle in data loader<NewLine><NewLine>soureconfig_val = COCOSourceConfig(opt.hdf5_val_data)<NewLine>val_data = MyDataset(config, soureconfig_val, shuffle=False, augment=True)  # shuffle in data loader<NewLine><NewLine><NewLine>best_loss = float('inf')<NewLine>start_epoch = 0  <NewLine>args.distributed = False<NewLine>if 'WORLD_SIZE' in os.environ:<NewLine>    args.distributed = int(os.environ['WORLD_SIZE']) &gt; 1<NewLine><NewLine>args.gpu = 0<NewLine>args.world_size = 1<NewLine><NewLine># FOR DISTRIBUTED:  If we are running under torch.distributed.launch,<NewLine># the 'WORLD_SIZE' environment variable will also be set automatically.<NewLine>if args.distributed:<NewLine>    args.gpu = args.local_rank<NewLine>    torch.cuda.set_device(args.gpu)<NewLine>    # Initializes the distributed backend which will take care of synchronizing nodes/GPUs<NewLine>    torch.distributed.init_process_group(backend='nccl', init_method='env://')<NewLine>    args.world_size = torch.distributed.get_world_size()  # 获取分布式训练的进程数<NewLine><NewLine>assert torch.backends.cudnn.enabled, ""Amp requires cudnn backend to be enabled.""<NewLine><NewLine>posenet = Network(opt, config, dist=True, bn=False)<NewLine># Actual working batch size on multi-GPUs is 4 times bigger than that on one GPU<NewLine># fixme: add up momentum if the batch grows?<NewLine>optimizer = optim.SGD(posenet.parameters(), lr=opt.learning_rate * args.world_size, momentum=0.9, weight_decay=1e-4)<NewLine><NewLine>scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.2, last_epoch=-1)<NewLine><NewLine><NewLine>if args.sync_bn:<NewLine>    #  This should be done before model = DDP(model, delay_allreduce=True),<NewLine>    #  because DDP needs to see the finalized model parameters<NewLine>    # We rely on torch distributed for synchronization between processes. Only DDP support the apex sync_bn now.<NewLine>    import apex<NewLine>    print(""Using apex synced BN."")<NewLine>    posenet = apex.parallel.convert_syncbn_model(posenet)<NewLine><NewLine>posenet.cuda()<NewLine><NewLine># Initialize Amp.  Amp accepts either values or strings for the optional override arguments,<NewLine># for convenient interoperation with argparse.<NewLine># For distributed training, wrap the model with apex.parallel.DistributedDataParallel.<NewLine># This must be done AFTER the call to amp.initialize.<NewLine>model, optimizer = amp.initialize(posenet, optimizer,<NewLine>                                  opt_level=args.opt_level,<NewLine>                                  keep_batchnorm_fp32=args.keep_batchnorm_fp32,<NewLine>                                  loss_scale=args.loss_scale)  # Dynamic loss scaling is used by default.<NewLine># delay_allreduce delays all communication to the end of the backward pass.<NewLine><NewLine><NewLine>if args.distributed:<NewLine>    # By default, apex.parallel.DistributedDataParallel overlaps communication with computation in the backward pass.<NewLine>    # model = DDP(model)<NewLine>    # delay_allreduce delays all communication to the end of the backward pass.<NewLine>    model = DDP(model, delay_allreduce=True)<NewLine><NewLine><NewLine><NewLine>train_sampler = None<NewLine>val_sampler = None<NewLine># Restricts data loading to a subset of the dataset exclusive to the current process<NewLine># Create DistributedSampler to handle distributing the dataset across nodes when training <NewLine># This can only be called after distributed.init_process_group is called<NewLine><NewLine>if args.distributed:<NewLine>    train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)<NewLine>    val_sampler = torch.utils.data.distributed.DistributedSampler(val_data)<NewLine><NewLine>train_loader = torch.utils.data.DataLoader(train_data, batch_size=opt.batch_size, shuffle=(train_sampler is None),<NewLine>                                           num_workers=16, pin_memory=True, sampler=train_sampler, drop_last=True)<NewLine>val_loader = torch.utils.data.DataLoader(val_data, batch_size=opt.batch_size, shuffle=False,<NewLine>                                         num_workers=4, pin_memory=True, sampler=val_sampler, drop_last=True)<NewLine><NewLine>for param in model.parameters():<NewLine>    if param.requires_grad:<NewLine>        print('Parameters of network: Autograd')<NewLine>        break<NewLine><NewLine><NewLine>#  Update the learning rate for start_epoch times<NewLine>for i in range(start_epoch):<NewLine>    scheduler.step()<NewLine><NewLine><NewLine>def train(epoch):<NewLine>    print('\n ############################# Train phase, Epoch: {} #############################'.format(epoch))<NewLine>    posenet.train()<NewLine><NewLine>    if args.distributed:<NewLine>        train_sampler.set_epoch(epoch)<NewLine>    # train_loss = 0<NewLine>    scheduler.step()<NewLine>    print('\nLearning rate at this epoch is: %0.9f\n' % optimizer.param_groups[0]['lr'])  # scheduler.get_lr()[0]<NewLine><NewLine>    batch_time = AverageMeter()<NewLine>    losses = AverageMeter()<NewLine>    end = time.time()<NewLine><NewLine>    for batch_idx, target_tuple in enumerate(train_loader):<NewLine>        # images.requires_grad_()<NewLine>        # loc_targets.requires_grad_()<NewLine>        # conf_targets.requires_grad_()<NewLine>        if use_cuda:<NewLine>      <NewLine>            target_tuple = [target_tensor.cuda(non_blocking=True) for target_tensor in target_tuple]<NewLine><NewLine>        # target tensor shape: [8,512,512,3], [8, 1, 128,128], [8,43,128,128], [8,36,128,128], [8,36,128,128]<NewLine>        images, mask_misses, heatmaps = target_tuple  # , offsets, mask_offsets<NewLine>        # images = Variable(images)<NewLine>        # loc_targets = Variable(loc_targets)<NewLine>        # conf_targets = Variable(conf_targets)<NewLine><NewLine>        loss = model(images, target_tuple[1:])<NewLine>        optimizer.zero_grad()  # zero the gradient buff<NewLine><NewLine>        if loss.item() &gt; 1e6:<NewLine>            print(""\nLoss is abnormal, drop this batch !"")<NewLine>            loss.zero_()<NewLine>            continue<NewLine><NewLine>        with amp.scale_loss(loss, optimizer) as scaled_loss:<NewLine>            scaled_loss.backward()<NewLine><NewLine>        torch.nn.utils.clip_grad_norm(model.parameters(), args.max_grad_norm)<NewLine>        optimizer.step()  <NewLine><NewLine>     <NewLine>        if batch_idx % args.print_freq == 0:<NewLine>            # Every print_freq iterations, check the loss, accuracy, and speed.<NewLine>            # For best performance, it doesn't make sense to print these metrics every<NewLine>            # iteration, since they incur an allreduce and some host&lt;-&gt;device syncs.<NewLine>         <NewLine>            if args.distributed:<NewLine>                # We manually reduce and average the metrics across processes. In-place reduce tensor.<NewLine>                reduced_loss = reduce_tensor(loss.data)<NewLine>            else:<NewLine>                reduced_loss = loss.data<NewLine><NewLine>            # to_python_float incurs a host&lt;-&gt;device sync<NewLine>            losses.update(to_python_float(reduced_loss), images.size(0))  # update needs average and number<NewLine>            torch.cuda.synchronize()<NewLine>            batch_time.update((time.time() - end) / args.print_freq)<NewLine>            end = time.time()<NewLine><NewLine>            if args.local_rank == 0:  # Print them in the Process 0<NewLine>                print('==================&gt; Epoch: [{0}][{1}/{2}]\t'<NewLine>                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'<NewLine>                      'Speed {3:.3f} ({4:.3f})\t'<NewLine>                      'Loss {loss.val:.10f} ({loss.avg:.4f}) &lt;================ \t'.format(<NewLine>                        epoch, batch_idx, len(train_loader),<NewLine>                        args.world_size * opt.batch_size / batch_time.val,<NewLine>                        args.world_size * opt.batch_size / batch_time.avg,<NewLine>                        batch_time=batch_time,<NewLine>                        loss=losses))<NewLine><NewLine>    global best_loss<NewLine><NewLine>    # train_loss /= (len(train_loader))  # Each GPU process can only see 1/(world_size) training samples per epoch<NewLine><NewLine>    if args.local_rank == 0:<NewLine>        # Write the log file each epoch.<NewLine>        os.makedirs(checkpoint_path, exist_ok=True)<NewLine>        logger = open(os.path.join('./' + checkpoint_path, 'log'), 'a+')<NewLine>        logger.write('\nEpoch {}\ttrain_loss: {}'.format(epoch, losses.avg))  <NewLine>        logger.flush()<NewLine>        logger.close()<NewLine><NewLine>        if losses.avg &lt; best_loss:<NewLine>            # Update the best_loss if the average loss drops<NewLine>            best_loss = losses.avg<NewLine>            print('Saving model checkpoint...')<NewLine>            state = {<NewLine>                # not posenet.state_dict(). then, we don't ge the ""module"" string to begin with<NewLine>                'weights': model.module.state_dict(),<NewLine>                'optimizer_weight': optimizer.state_dict(),<NewLine>                'train_loss': losses.avg,<NewLine>                'epoch': epoch<NewLine>            }<NewLine>            torch.save(state, './' + checkpoint_path + '/PoseNet_' + str(epoch) + '_epoch.pth')<NewLine><NewLine><NewLine>def test(epoch):<NewLine>    print('\n ############################# Test phase, Epoch: {} #############################'.format(epoch))<NewLine>    posenet.eval()<NewLine>  <NewLine>    if args.distributed:<NewLine>        train_sampler.set_epoch(epoch)  <NewLine>    batch_time = AverageMeter()<NewLine>    losses = AverageMeter()<NewLine>    end = time.time()<NewLine><NewLine>    for batch_idx, target_tuple in enumerate(val_loader):<NewLine>        # images.requires_grad_()<NewLine>        # loc_targets.requires_grad_()<NewLine>        # conf_targets.requires_grad_()<NewLine>        if use_cuda:<NewLine>    <NewLine>            target_tuple = [target_tensor.cuda(non_blocking=True) for target_tensor in target_tuple]<NewLine><NewLine>        # target tensor shape: [8,512,512,3], [8, 1, 128,128], [8,43,128,128], [8,36,128,128], [8,36,128,128]<NewLine>        images, mask_misses, heatmaps = target_tuple  # , offsets, mask_offsets<NewLine><NewLine>        with torch.no_grad():<NewLine>            _, loss = model(images, target_tuple[1:])<NewLine><NewLine>        if args.distributed:<NewLine>            # We manually reduce and average the metrics across processes. In-place reduce tensor.<NewLine>            reduced_loss = reduce_tensor(loss.data)<NewLine>        else:<NewLine>            reduced_loss = loss.data<NewLine><NewLine>        # to_python_float incurs a host&lt;-&gt;device sync<NewLine>        losses.update(to_python_float(reduced_loss), images.size(0))  # update needs average and number<NewLine>        torch.cuda.synchronize() <NewLine>        batch_time.update((time.time() - end))<NewLine>        end = time.time()<NewLine><NewLine>        if args.local_rank == 0:  # Print them in the Process 0<NewLine>            print('==================&gt;Test: [{0}/{1}]\t'<NewLine>                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'<NewLine>                  'Speed {2:.3f} ({3:.3f})\t'<NewLine>                  'Loss {loss.val:.4f} ({loss.avg:.4f})\t'.format(<NewLine>                   batch_idx, len(val_loader),<NewLine>                   args.world_size * opt.batch_size / batch_time.val,<NewLine>                   args.world_size * opt.batch_size / batch_time.avg,<NewLine>                   batch_time=batch_time, loss=losses))<NewLine><NewLine>    if args.local_rank == 0:  # Print them in the Process 0<NewLine>        # Write the log file each epoch.<NewLine>        os.makedirs(checkpoint_path, exist_ok=True)<NewLine>        logger = open(os.path.join('./' + checkpoint_path, 'log'), 'a+')<NewLine>        logger.write('\tval_loss: {}'.format(losses.avg)) <NewLine>        logger.flush()<NewLine>        logger.close()<NewLine><NewLine><NewLine>def adjust_learning_rate(optimizer, epoch, step, len_epoch):<NewLine>    """"""LR schedule that should yield 76% converged accuracy with batch size 256""""""<NewLine>    factor = epoch // 30<NewLine><NewLine>    if epoch &gt;= 80:<NewLine>        factor = factor + 1<NewLine><NewLine>    lr = args.lr*(0.1**factor)<NewLine><NewLine>    """"""Warmup""""""<NewLine>    if epoch &lt; 5:<NewLine>        lr = lr*float(1 + step + epoch*len_epoch)/(5.*len_epoch)  # len_epoch=len(train_loader)<NewLine><NewLine>    # if(args.local_rank == 0):<NewLine>    #     print(""epoch = {}, step = {}, lr = {}"".format(epoch, step, lr))<NewLine><NewLine>    for param_group in optimizer.param_groups:<NewLine>        param_group['lr'] = lr<NewLine><NewLine><NewLine>class AverageMeter(object):<NewLine>    """"""Computes and stores the average and current value""""""<NewLine>    def __init__(self):<NewLine>        self.val = 0<NewLine>        self.avg = 0<NewLine>        self.sum = 0<NewLine>        self.count = 0<NewLine><NewLine>    def update(self, val, n=1):<NewLine>        self.val = val<NewLine>        self.sum += val * n<NewLine>        self.count += n<NewLine>        self.avg = self.sum / self.count<NewLine><NewLine><NewLine>def reduce_tensor(tensor):<NewLine>    # Reduces the tensor data across all machines<NewLine>    # If we print the tensor, we can get:<NewLine>    # tensor(334.4330, device='cuda:1') *********************, here is cuda:  cuda:1<NewLine>    # tensor(359.1895, device='cuda:3') *********************, here is cuda:  cuda:3<NewLine>    # tensor(263.3543, device='cuda:2') *********************, here is cuda:  cuda:2<NewLine>    # tensor(340.1970, device='cuda:0') *********************, here is cuda:  cuda:0<NewLine>    rt = tensor.clone()  # The function operates in-place.<NewLine>    dist.all_reduce(rt, op=dist.reduce_op.SUM)<NewLine>    rt /= args.world_size<NewLine>    return rt<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    for epoch in range(start_epoch, start_epoch + 80):<NewLine>        train(epoch)<NewLine>        test(epoch)<NewLine><NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/jia_lee,(Jia Lee),jia_lee,"March 25, 2019, 11:21am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>To be more specific, I have followed the ImageNet example in Nvidia Apex. I write the loss function inside my Network module which is like the follows:</p><NewLine><pre><code class=""lang-auto"">class Network(torch.nn.Module):<NewLine>    """"""<NewLine>    Wrap the network module as well as the loss module on all GPUs to balance the computation among GPUs.<NewLine>    """"""<NewLine>    def __init__(self, opt, config, bn=False, dist=False):<NewLine>        super(Network, self).__init__()<NewLine>        self.posenet = PoseNet(opt.nstack, opt.hourglass_inp_dim, config.num_layers, bn=bn)<NewLine>        # If we use train_parallel, we implement the parallel loss . And if we use train_distributed,<NewLine>        # we should use single process loss because each process on these 4 GPUs  is independent<NewLine>        self.criterion = MultiTaskLoss(opt, config) if dist else MultiTaskLossParallel(opt, config)<NewLine><NewLine>    def forward(self, inp_imgs, target_tuple):<NewLine>        # Batch will be divided and Parallel Model will call this forward on every GPU<NewLine>        output_tuple = self.posenet(inp_imgs)<NewLine>        loss = self.criterion(output_tuple, target_tuple)<NewLine><NewLine>        if not self.training:<NewLine>            # output will be concatenated  along batch channel automatically after the parallel model return<NewLine>            return output_tuple, loss<NewLine>        else:<NewLine>            # output will be concatenated  along batch channel automatically after the parallel model return<NewLine>            return loss<NewLine></code></pre><NewLine><p><strong>The training loss seems normal:</strong></p><NewLine><div class=""md-table""><NewLine><table><NewLine><thead><NewLine><tr><NewLine><th>Epoch 0</th><NewLine><th>train_loss: 589.6713480631511</th><NewLine><th>val_loss: 536.4533081054688</th><NewLine></tr><NewLine></thead><NewLine><tbody><NewLine><tr><NewLine><td>Epoch 1</td><NewLine><td>train_loss: 446.2322041829427</td><NewLine><td>val_loss: 440.89935302734375</td><NewLine></tr><NewLine><tr><NewLine><td>Epoch 2</td><NewLine><td>train_loss: 436.07487325032554</td><NewLine><td>val_loss: 433.20953369140625</td><NewLine></tr><NewLine><tr><NewLine><td>Epoch 3</td><NewLine><td>train_loss: 433.3325126139323</td><NewLine><td>val_loss: 396.94744873046875</td><NewLine></tr><NewLine><tr><NewLine><td>Epoch 4</td><NewLine><td>train_loss: 425.1072373453776</td><NewLine><td>val_loss: 406.3310546875</td><NewLine></tr><NewLine><tr><NewLine><td>Epoch 5</td><NewLine><td>train_loss: 418.57773783365883</td><NewLine><td>val_loss: 392.5045166015625</td><NewLine></tr><NewLine><tr><NewLine><td>Epoch 6</td><NewLine><td>train_loss: 409.60796936035155</td><NewLine><td>val_loss: 419.2001037597656</td><NewLine></tr><NewLine><tr><NewLine><td>Epoch 7</td><NewLine><td>train_loss: 410.79097737630207</td><NewLine><td>val_loss: 409.8291320800781</td><NewLine></tr><NewLine><tr><NewLine><td>Epoch 8</td><NewLine><td>train_loss: 404.4842706298828</td><NewLine><td>val_loss: 407.05352783203125</td><NewLine></tr><NewLine><tr><NewLine><td>Epoch 9</td><NewLine><td>train_loss: 399.4785394287109</td><NewLine><td>val_loss: 388.7215881347656</td><NewLine></tr><NewLine><tr><NewLine><td>Epoch 10</td><NewLine><td>train_loss: 389.387607421875</td><NewLine><td>val_loss: 379.6018981933594</td><NewLine></tr><NewLine><tr><NewLine><td>Epoch 11</td><NewLine><td>train_loss: 386.5943516031901</td><NewLine><td>val_loss: 397.2137451171875</td><NewLine></tr><NewLine><tr><NewLine><td>Epoch 12</td><NewLine><td>train_loss: 382.25890686035154</td><NewLine><td>val_loss: 376.7177734375</td><NewLine></tr><NewLine><tr><NewLine><td>Epoch 13</td><NewLine><td>train_loss: 387.2037613932292</td><NewLine><td>val_loss: 360.4934387207031</td><NewLine></tr><NewLine><tr><NewLine><td>Epoch 14</td><NewLine><td>train_loss: 379.99100199381513</td><NewLine><td>val_loss: 377.1543884277344</td><NewLine></tr><NewLine><tr><NewLine><td>Epoch 15</td><NewLine><td>train_loss: 381.0046073404948</td><NewLine><td>val_loss: 378.36041259765625</td><NewLine></tr><NewLine><tr><NewLine><td>Epoch 16</td><NewLine><td>train_loss: 378.6185076904297</td><NewLine><td>val_loss: 365.29205322265625</td><NewLine></tr><NewLine><tr><NewLine><td>Epoch 17</td><NewLine><td>train_loss: 380.5766967773437</td><NewLine><td>val_loss: 364.39569091796875</td><NewLine></tr><NewLine><tr><NewLine><td>Epoch 18</td><NewLine><td>train_loss: 382.2865834554037</td><NewLine><td>val_loss: 368.50152587890625</td><NewLine></tr><NewLine></tbody><NewLine></table><NewLine></div><p>But the model seems not to have been trained well and the prediction results refuse to get better （which is bad actually).<br/><NewLine>I have struggled with this problem for a while. If I don’t use distributed training or Apex auto mixed-precision and I only wrap my <strong>Network</strong> module with <strong>torch.nn.parallel.DataParallel</strong>, everything goes fine and the prediction is good.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Numerical issues are notoriously hard to debug.</p><NewLine><p>Can you isolate the issue to either distributed <strong>or</strong> mixed precision?</p><NewLine><p><a class=""mention"" href=""/u/mcarilli"">@mcarilli</a> Any ideas?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>This may well be an Apex bug.  About a week ago, for a few days the combination of dynamic loss scaling + Apex DDP was broken in Apex master.  I fixed it in <a href=""https://github.com/NVIDIA/apex/commit/8437d29505fcc7fad28183395abd89a09a17efe6"" rel=""nofollow noopener"">https://github.com/NVIDIA/apex/commit/8437d29505fcc7fad28183395abd89a09a17efe6</a>, so maybe a fresh clone + reinstall of Apex will resolve the issue.  Be sure to clean the old install before rebuilding:<br/><NewLine>pip uninstall apex<br/><NewLine>cd apex_repo_dir<br/><NewLine>rm -rf build (if present)<br/><NewLine>rm -rf apex.egg-info (if present)<br/><NewLine>git pull<br/><NewLine>pip install -v --no-cache-dir --global-option=""–cpp_ext"" --global-option=""–cuda_ext"" .</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply. The problem has not yet solved.</p><NewLine><p>If I remove the clip_norm in the training step, the gradient will explode after some batches. The training process looks like okay before explosion. No norm operation is used in my case. All input tensors and ground truth tensors are normalized into [0,1]. L2 loss and weight_decay are used. I have no idea which detail should I concentrate on.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you try a fresh clone and install of Apex?</p><NewLine><p>Gradient clipping does require special treatment for compatibility with all opt_levels:  <a href=""https://nvidia.github.io/apex/advanced.html#gradient-clipping"" rel=""nofollow noopener"">https://nvidia.github.io/apex/advanced.html#gradient-clipping</a></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, I have followed your instruction to reinstall Apex. My problem is strange. I printed the abnormal value  during the distributed training:<br/><NewLine>First, my model has various scales of feature map predicted （cascaded CNN and in each stage/stack has 5 scale output)</p><NewLine><p><strong>This is normal for some batches, and the element-wise output should in the range of [0,1] (gaussian heatmap regression)</strong></p><NewLine><pre><code class=""lang-auto"">heatmap L2 loss per stack.........   [ 69848.01   59730.246 223546.12   60869.35 ]<NewLine> heatmap L2 loss per stack.........   [15058.608 13271.5   13770.041 13684.25 ]<NewLine> heatmap L2 loss per stack.........   [3515.2559 3062.7026 2899.563  2879.3105]<NewLine> heatmap L2 loss per stack.........   [ 84485.94  76283.11 219553.47  77723.48]<NewLine> heatmap L2 loss per stack.........   [ 70769.086  63346.633 209632.16   64268.496]<NewLine> heatmap L2 loss per stack.........   [18312.457 17451.66  17986.875 17935.975]<NewLine></code></pre><NewLine><p>However, the loss is abnormal suddenly , and the elements of the output become very large such as 223, and then grow rapidly and resulting in gradient explosion.</p><NewLine><pre><code class=""lang-auto"">Dangerous! Check pred, gt, mask_miss: ======&gt;  tensor(223.1250, device='cuda:2', dtype=torch.float16, grad_fn=&lt;MaxBackward1&gt;) tensor(1., device='cuda:2') tensor(1., device='cuda:2')<NewLine> heatmap L2 loss per stack.........   [0. 0. 0. 0.]<NewLine>Dangerous! Check pred, gt, mask_miss: ======&gt;  tensor(223.1250, device='cuda:1', dtype=torch.float16, grad_fn=&lt;MaxBackward1&gt;) tensor(1., device='cuda:1') tensor(1., device='cuda:1')<NewLine>Dangerous! Check pred, gt, mask_miss: ======&gt;  tensor(222.7500, device='cuda:3', dtype=torch.float16, grad_fn=&lt;MaxBackward1&gt;) tensor(1., device='cuda:3') tensor(1., device='cuda:3')<NewLine>Dangerous! Check pred, gt, mask_miss: ======&gt;  tensor(222.7500, device='cuda:0', dtype=torch.float16, grad_fn=&lt;MaxBackward1&gt;) tensor(1., device='cuda:0') tensor(1., device='cuda:0')<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Update. Problem has been solved. I add clamp into loss value and change the weight of multi-scale losses. It seems that the auto loss scale in Apex is not perfect yet.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Would you explain more in your code, I have the same issue.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jia_lee; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mcarilli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jia_lee; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mcarilli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/jia_lee; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/jia_lee; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Abdelpakey; <NewLine> ,"REPLY_DATE 1: March 25, 2019, 11:31am; <NewLine> REPLY_DATE 2: March 25, 2019,  5:03pm; <NewLine> REPLY_DATE 3: March 26, 2019,  2:39am; <NewLine> REPLY_DATE 4: March 26, 2019,  1:47pm; <NewLine> REPLY_DATE 5: March 26, 2019,  3:54pm; <NewLine> REPLY_DATE 6: March 27, 2019,  8:38am; <NewLine> REPLY_DATE 7: March 28, 2019,  1:42am; <NewLine> REPLY_DATE 8: April 14, 2020,  8:49am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 2 Likes; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
76663,Torch.multiprocess.spawn with join=False gives the following error,2020-04-14T01:35:46.243Z,0,272,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello all,<br/><NewLine>I am trying to implement Distributed Parallel for my model and I followed the ImageNet example for this. I am pretty new to distributed programming, so I am not sure about a bunch of things. When I use torch.multiprocess.spawn with join=True, there is no output that is printed. When I change the join to False, I get the following error below.</p><NewLine><pre><code class=""lang-auto"">&lt;torch.multiprocessing.spawn.SpawnContext object at 0x2b49eee8a3c8&gt;<NewLine>Traceback (most recent call last):<NewLine>  File ""&lt;string&gt;"", line 1, in &lt;module&gt;<NewLine>  File ""/usr/lib/python3.5/multiprocessing/spawn.py"", line 106, in spawn_main<NewLine>    exitcode = _main(fd)<NewLine>  File ""/usr/lib/python3.5/multiprocessing/spawn.py"", line 116, in _main<NewLine>    self = pickle.load(from_parent)<NewLine>  File ""/usr/lib/python3.5/multiprocessing/synchronize.py"", line 111, in __setstate__<NewLine>    self._semlock = _multiprocessing.SemLock._rebuild(*state)<NewLine>FileNotFoundError: [Errno 2] No such file or directory<NewLine>Traceback (most recent call last):<NewLine>  File ""&lt;string&gt;"", line 1, in &lt;module&gt;<NewLine>  File ""/usr/lib/python3.5/multiprocessing/spawn.py"", line 106, in spawn_main<NewLine>    exitcode = _main(fd)<NewLine>  File ""/usr/lib/python3.5/multiprocessing/spawn.py"", line 116, in _main<NewLine>    self = pickle.load(from_parent)<NewLine>  File ""/usr/lib/python3.5/multiprocessing/synchronize.py"", line 111, in __setstate__<NewLine>    self._semlock = _multiprocessing.SemLock._rebuild(*state)<NewLine>FileNotFoundError: [Errno 2] No such file or directory<NewLine></code></pre><NewLine><p>I am submitting this job through a SLURM script that I have put below as well.</p><NewLine><pre><code class=""lang-auto"">#!/bin/sh<NewLine>#SBATCH --ntasks=4<NewLine>#SBATCH --time=60:00:00<NewLine>#SBATCH --partition=gpu<NewLine>#SBATCH --mem=64gb<NewLine>#SBATCH --nodes=2<NewLine>#SBATCH --gres=gpu:2<NewLine>#SBATCH --constraint=gpu_32gb<NewLine>#SBATCH --job-name=test<NewLine>#SBATCH --output=.../out_files/test.out<NewLine><NewLine>export PYTHONPATH=$WORK/tf-gpu-pkgs<NewLine>module load singularity<NewLine>singularity exec docker://&lt;user&gt;/pytorch-opencv:latest	python3 -u $@ --use_adam=1 --multiprocessing_distributed --benchmarks=0 --benchmark_arch='vgg19' --batch_size=128 --test=0 --transfer=0 --dataset='&lt;dataset-here&gt;'<NewLine></code></pre><NewLine><p>My code is like the ImageNet example and I am not sure what I am doing wrong.</p><NewLine><p>Thank you,</p><NewLine></div>",https://discuss.pytorch.org/u/ayushm-agrawal,(Ayush Manish Agrawal),ayushm-agrawal,"April 14, 2020,  1:35am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you capturing the <code>SpawnContext</code> object returned by the call to <code>torch.multiprocess.spawn</code>? This <code>SpawnContext</code> is returned only when <code>join=False</code>, and must be saved for the spawned processes to coordinate IPC. If you allow the object to be destructed, you will see this error.</p><NewLine><p>Here is a GitHub issue with some more information: <a href=""https://github.com/pytorch/pytorch/issues/30461"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/30461</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello Omkar,<br/><NewLine>Thank you for replying. The weird issue is that I don’t see the terminated print statement when I use join=True. With the issue that you linked to me, when I spawn the process, shouldn’t I be seeing the print statements from my <strong>main_worker</strong> function before I hit the terminated print statement? I apologize if this question isn’t framed right. I am new to distributed and don’t understand the system that well.</p><NewLine><pre><code class=""lang-auto""> ctx = mp.spawn(main_worker, nprocs=ngpus_per_node,<NewLine>                 args=(ngpus_per_node, args), join=False)<NewLine>        time.sleep(3)<NewLine>        print('terminated')<NewLine>        ctx.join()<NewLine>    else:<NewLine>        # Simply call main_worker function<NewLine>        main_worker(args.gpu, ngpus_per_node, args)<NewLine><NewLine><NewLine>def main_worker(gpu, ngpus_per_node, args):<NewLine>    global best_acc1<NewLine>    print(gpu)<NewLine>    args.gpu = gpu<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">diff --git a/repro_org.py b/repro.py<NewLine>index be44c3d..e971db4 100644<NewLine>--- a/repro_org.py<NewLine>+++ b/repro.py<NewLine>@@ -6,7 +6,8 @@ def worker(nproc, arg1, arg2, arg3):<NewLine>     test = True<NewLine> <NewLine> if __name__ == '__main__':<NewLine>-    mp.spawn(worker, (None, None, None), nprocs=1, join=False)<NewLine>+    ctx = mp.spawn(worker, (None, None, None), nprocs=1, join=False)<NewLine>     time.sleep(3)<NewLine>     print('terminated')<NewLine>+    ctx.join()<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/osalpekar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ayushm-agrawal; <NewLine> ,"REPLY_DATE 1: April 14, 2020,  2:00am; <NewLine> REPLY_DATE 2: April 14, 2020,  2:29am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
76333,Distributed Machine Learning on multiple cores,2020-04-11T15:09:29.963Z,4,226,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was looking into training machine learning models in multiple cores. To be more clear, suppose I have “N” machine learning units (for eg. three layered neural network [in-hid-out] ). Each of the units are identical to each other. However, I want to train each network with different input of same nature (for eg. If I have 10 machine learning units with MNIST data as input, each of the 10 units will be trained on different sets of data). You can think of it as training the networks with MNIST in 10 geographically dispersed location where we are not sure which network will have what set of inputs. However, at somepoint I want to communicate between the machine learning models while updating the weights. I want to distribute same weights to all the models. For people who know federated learning, its like applying federated learning in multiple CPU/GPU cores.</p><NewLine><p>Is it possible to do something like this in multiple cores of a CPU or GPU? Or is there any documentation that you can provide me?</p><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> <a class=""mention"" href=""/u/rasbt"">@rasbt</a></p><NewLine></div>",https://discuss.pytorch.org/u/abodh_ltd,(Abodh Poudyal),abodh_ltd,"April 11, 2020,  3:17pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>IIUC, this is typical <a href=""https://pytorch.org/docs/master/notes/ddp.html"" rel=""nofollow noopener""><code>DistributedDataParallel</code></a> training? If so, yes, PyTorch natively support that. Here is <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">another tutorial</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am not sure what to call this. But its kind of distributed training where each of the neural network in different processes communicate while updating the weight. Would the approaches you provided do the same?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>P.S. I don’t want to distribute a single set of input to multiple nodes/processes. Suppose I have 2 nodes N1 and N2, then I need to send a set of input for N1 and another set of input for N2, which is different than the set for N1 (and not collected in batches from a common data set). I am not sure if I explained it correctly. Sorry about that.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>But its kind of distributed training where each of the neural network in different processes communicate while updating the weight</p><NewLine></blockquote><NewLine><p>Does it have to communicate parameters instead of gradients? If all you need is to keep the parameters on all processes in sync, communicating gradients should be sufficient I think.</p><NewLine><blockquote><NewLine><p>I don’t want to distribute a single set of input to multiple nodes/processes.</p><NewLine></blockquote><NewLine><p>Yep, DDP does not split inputs, instead each process need to prepare its own inputs.</p><NewLine><p>One question is, does the parameters/gradients communication occur in a synchronized or asynchronized fashion? “Synchronized” means all processes communicate at exactly the same time, while asynchronized can be something like gossip. DDP only works for synchronized use cases. If you need asynchronized communication, you can directly use c10d (allreduce, allgather, broadcast, etc.) and create multiple sub-groups to perform communicatioin.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/8f4682282a262d04e764a930f227012c92ddbee7"" href=""https://discuss.pytorch.org/uploads/default/original/3X/8/f/8f4682282a262d04e764a930f227012c92ddbee7.png"" title=""distributed""><img alt=""distributed"" data-base62-sha1=""krtlcbiT38iEHdqysulENbPxkDt"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/f/8f4682282a262d04e764a930f227012c92ddbee7_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/f/8f4682282a262d04e764a930f227012c92ddbee7_2_514x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/f/8f4682282a262d04e764a930f227012c92ddbee7_2_514x500.png, https://discuss.pytorch.org/uploads/default/optimized/3X/8/f/8f4682282a262d04e764a930f227012c92ddbee7_2_771x750.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/8/f/8f4682282a262d04e764a930f227012c92ddbee7.png 2x"" width=""514""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">distributed</span><span class=""informations"">980×953 97.5 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>I am not a Machine Learning savvy, so please mind the errors while I write this:</p><NewLine><p>I think the figure will explain this right. Since I will be gathering the weights of all the networks residing at different processes, I need to pass the weight parameters right? And the weight gathering needs to happen asynchronously, at different point of time. But to get a start, at this point, we can assume that the weight gathering happens synchronously, at the same point of time.</p><NewLine><p>Furthermore, the gathered weights need to be averaged at the root process and the aggregated weight should be broadcasted again to the networks for next epoch. I am not sure if you get this due to my weird way of explaining things but thanks for being modest and replying promptly.</p><NewLine><p>If you could tell me a specific way to handle this, I could narrow down my scope of researching the documents and tutorials. Thank you again.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>P.S. each of the networks will have local training epochs (hence update the weights in different period of time)</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Based on the diagram and explanations, it seems like you are trying to train the network, with each node training on its own data, and ensuring that the parameters stay in sync across all the nodes. If this is the case, you will want to communicate the gradients, and not the weights.</p><NewLine><p>DDP works perfectly for synchronous distributed training. Each node will independently perform the forward and backward pass on its own batch of data. Then, each node will send its computed gradients to every other node. Once each node has the gradients for all other nodes, they will independently average all the gradients and run the optimizer to perform the gradient update. One note is that there is no “root” process responsible for aggregating the gradients (what you’re describing is similar to a parameter server). In DDP, the nodes communicate with each to exchange gradients.</p><NewLine><p>For asynchronous distributed training, you can use the c10d communication primitives as <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> described above.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/abodh_ltd; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/abodh_ltd; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/abodh_ltd; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/abodh_ltd; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/osalpekar; <NewLine> ,"REPLY_DATE 1: April 14, 2020,  6:44am; <NewLine> REPLY_DATE 2: April 12, 2020, 10:11pm; <NewLine> REPLY_DATE 3: April 12, 2020, 10:28pm; <NewLine> REPLY_DATE 4: April 13, 2020,  1:39am; <NewLine> REPLY_DATE 5: April 13, 2020,  7:58pm; <NewLine> REPLY_DATE 6: April 13, 2020,  9:08pm; <NewLine> REPLY_DATE 7: April 13, 2020,  9:31pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 2 Likes; <NewLine> 
68286,What is the best practices of logging in distributed training?,2020-01-31T22:08:57.085Z,0,213,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Did some googling and found very few discussions on this matter. Is it best to perform all-reduce on, say, loss values, and keep track of it within the process with rank 0, like what the official tutorial recommends for checkpoints?</p><NewLine></div>",https://discuss.pytorch.org/u/XYJin,,XYJin,"January 31, 2020, 10:08pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>That seems to be the case, check out how they do it in the <a href=""https://github.com/facebookresearch/maskrcnn-benchmark/blob/57eec25b75144d9fb1a6857f32553e1574177daf/maskrcnn_benchmark/engine/trainer.py#L88"" rel=""nofollow noopener"">implementation of Mask-RCNN</a>. They use <code>reduce</code> instead of <code>all_reduce</code> because, for logging, you only need the reduced and averaged values on the rank 0 process.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ibro45; <NewLine> ,"REPLY_DATE 1: April 9, 2020, 11:24pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
75507,Free async request object,2020-04-06T07:27:49.486Z,4,166,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I noticed that each async torch.distributed request object holds a pointer to the sent tensor,<br/><NewLine>therefore the buffer memory is not freed in the sender process until we explicitly call wait() or is_completed().<br/><NewLine>I am looking for a way to overcome this.<br/><NewLine>any suggestions?</p><NewLine></div>",https://discuss.pytorch.org/u/seliad,(Saar Eliad),seliad,"April 6, 2020,  7:27am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like only <code>work</code> from Gloo and MPI ProcessGroup holds those tensors. Does it work if you do not hold that async work/request object in application? Gloo and MPI ProcessGroup both have a queue and a runLoop to hold those work objects alive until processed.</p><NewLine><p>Curious, if you don’t call <code>wait()</code> how do you know when the communication is finished and safe to consume the output?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>If we destroy async objects before completion we get<br/><NewLine>“Attempted destruction of AsyncWork before work has completed, terminating the program.”</p><NewLine><p>Therefore I must save in application all the sent request objects.</p><NewLine><p>(Answering your question: I don’t consume the output of these (<code>isend</code>) messages)<br/><NewLine>( of course, the receiver calls <code>wait()</code> before consuming)<br/><NewLine>I’m using MPI (cuda-aware openmpi), with async p2p messages.</p><NewLine><p>The goal is simple</p><NewLine><ol><NewLine><li>send tons of isend messages,</li><NewLine><li>get memory freed automatically on each completion</li><NewLine><li>finally, <code>wait()</code> on all the isends at the end of the program.</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see. Only send/recv/recvAnysource APIs returns <code>AsyncWork</code>, and the <code>AsyncWork</code> is not stored in the queue. Other collectives use a different <code>WorkEntry</code> data structure, which are stored in the queue. Looks to me we should consolidate these APIs to implement the same behavior.</p><NewLine><p><a class=""mention"" href=""/u/teng-li"">@teng-li</a> <a class=""mention"" href=""/u/pietern"">@pietern</a> Any reason for having different async work data structures for MPI ProcessGroup?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Created <a href=""https://github.com/pytorch/pytorch/issues/36140"" rel=""nofollow noopener"">an issue</a> to track this. In the mean time, does it work for you if you put the async work into a queue and launch a separate thread to wait and dequeue? The wait API does release GIL, so it shouldn’t be blocking the main thread. This will lead to a similar behavior if we consolidate send/recv with collective async work. It won’t be perfect, as it does not guarantee immediately free tensors when comm is done when an earlier send/recv finished later. A better solution would need to install callbacks to MPI thread, which requires larger revamp and I am not sure if MPI supports that.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Only send/recv/recvAnysource APIs returns  <code>AsyncWork</code> and the  <code>AsyncWork</code>  is not stored in the queue</p><NewLine></blockquote><NewLine><p>You mean <code>isends</code> too right?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""75507"" data-username=""seliad""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/seliad/40/16357_2.png"" width=""20""/> seliad:</div><NewLine><blockquote><NewLine><p>You mean <code>isends</code> too right?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Oh, sorry, I meant the C++ send/recv/recvAnysource API. The <code>isend</code> API is Python only. Both send and isend call into the same C++ send API, the only difference is whether it waits on the work.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> I tried the solution with the cleaner thread and it doesn’t work:<br/><NewLine>seems like the <code>wait()</code> in the cleaner thread stops the whole process.<br/><NewLine><a href=""https://github.com/pytorch/pytorch/blob/945d7a7408891e25bc54a65015724f6d2de644e6/torch/lib/c10d/ProcessGroupMPI.cpp#L217"" rel=""nofollow noopener"">I think this is were its at in code.</a><br/><NewLine>I could only make it work with</p><NewLine><pre><code class=""lang-auto"">while not r.is_completed():<NewLine>    pass<NewLine></code></pre><NewLine><p>but performance suffered a lot (~2x slowdown) compared my previous solution.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/seliad; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/seliad; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/seliad; <NewLine> ,"REPLY_DATE 1: April 6, 2020,  3:13pm; <NewLine> REPLY_DATE 2: April 7, 2020, 12:40pm; <NewLine> REPLY_DATE 3: April 7, 2020,  2:15pm; <NewLine> REPLY_DATE 4: April 7, 2020,  2:34pm; <NewLine> REPLY_DATE 5: April 7, 2020,  2:48pm; <NewLine> REPLY_DATE 6: April 7, 2020,  2:52pm; <NewLine> REPLY_DATE 7: April 9, 2020, 11:02am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
75838,Simultaneous communications in different process groups,2020-04-08T10:47:01.912Z,3,249,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Referring to this old issue <a href=""https://github.com/pytorch/pytorch/issues/14528"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/14528</a>, which was closed, I need to do communications (all_reduce/reduce/broadcast) in two or more different groups simultaneously. For example, if a process’ rank belongs to say the group g0, it could participate in that communication only.</p><NewLine><p>Essentially, referring to <a href=""https://github.com/pytorch/pytorch/issues/14528"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/14528</a>, I would need to apply if conditional –</p><NewLine><pre><code class=""lang-auto""> if local_rank in g0: <NewLine>torch.distributed.all_reduce(t, group=g0) <NewLine>elif local_rank in g1: <NewLine>torch.distributed.all_reduce(t, group=g1)<NewLine></code></pre><NewLine><p>– for participating in all_reduce so that simultaneous communications could happen in non-intersecting groups.</p><NewLine><p>However, that results in  <code>NCCL INFO Call to connect returned Connection refused, retrying</code>. Whereas, doing these all_reduce operations sequentially, which essentially would imply that the collective communications in different groups are sequentiallized, works fine.</p><NewLine><p>Is there something that I am missing?</p><NewLine></div>",https://discuss.pytorch.org/u/bapi,(Bapi Chatterjee),bapi,"April 8, 2020, 10:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/bapi"">@bapi</a></p><NewLine><p>I tried the following, and it worked for me:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.multiprocessing as mp<NewLine>import os<NewLine><NewLine>def run(rank, world_size):<NewLine>    os.environ['MASTER_ADDR'] = 'localhost'<NewLine>    os.environ['MASTER_PORT'] = '29500'<NewLine><NewLine>    torch.cuda.set_device(rank)<NewLine>    # global group<NewLine>    torch.distributed.init_process_group(backend='nccl', rank=rank, world_size=world_size)<NewLine>    # torch.distributed.init_process_group(backend='gloo', init_method='env://')<NewLine>    g0 = torch.distributed.new_group(ranks=[0,1,2,3])<NewLine>    g1 = torch.distributed.new_group(ranks=[4,5,6,7])<NewLine>    # tensor to bcast over group<NewLine>    t = torch.tensor([1]).float().cuda().fill_(rank)<NewLine>    if rank &lt; 4:<NewLine>        torch.distributed.all_reduce(t, group=g0)<NewLine>    else:<NewLine>        torch.distributed.all_reduce(t, group=g1)<NewLine>    print('rank: {} - val: {}'.format(rank, t.item()))<NewLine><NewLine><NewLine>def main():<NewLine>    world_size = 8<NewLine>    mp.spawn(run,<NewLine>        args=(world_size,),<NewLine>        nprocs=world_size,<NewLine>        join=True)<NewLine><NewLine>if __name__==""__main__"":<NewLine>    main()<NewLine></code></pre><NewLine><p>outputs:</p><NewLine><pre><code class=""lang-auto"">$ python test.py <NewLine>rank: 0 - val: 6.0<NewLine>rank: 1 - val: 6.0<NewLine>rank: 3 - val: 6.0<NewLine>rank: 2 - val: 6.0<NewLine>rank: 7 - val: 22.0<NewLine>rank: 5 - val: 22.0<NewLine>rank: 6 - val: 22.0<NewLine>rank: 4 - val: 22.0<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""75838"" data-username=""bapi""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/bapi/40/10124_2.png"" width=""20""/> bapi:</div><NewLine><blockquote><NewLine><p>if local_rank in g0:</p><NewLine></blockquote><NewLine></aside><NewLine><p>I am not sure if you acquired <code>g0</code> and <code>g1</code> from <code>new_group</code> API. If so, they are process group objects. So the above check would result in the following error:</p><NewLine><pre><code class=""lang-auto"">TypeError: argument of type 'object' is not iterable<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Dear <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>, thanks very much for your response. Indeed that works.</p><NewLine><p>Let me post my code adapted to your nice illustration that I actually need to work with as a part of a much longer implementation:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.multiprocessing as mp<NewLine>import os<NewLine><NewLine><NewLine>def run(rank, groups, world_size):<NewLine>    print(""Rank "", rank, "" Group "", groups[str(rank)])<NewLine>    os.environ['MASTER_ADDR'] = 'localhost'<NewLine>    os.environ['MASTER_PORT'] = '29500'<NewLine>    torch.cuda.set_device(groups[str(rank)]['gpu'])<NewLine>    # global group<NewLine>    torch.distributed.init_process_group(backend='nccl',<NewLine>                                         rank=rank,<NewLine>                                         world_size=world_size)<NewLine>    my_group = torch.distributed.new_group(ranks=groups[str(rank)]['grp'])<NewLine>    # tensor to bcast over group<NewLine>    t = torch.tensor([1]).float().cuda().fill_(rank)<NewLine>    torch.distributed.all_reduce(t, group=my_group)<NewLine>    print('rank: {} - val: {}'.format(rank, t.item()))<NewLine><NewLine><NewLine>def assign_groups(num_masters, workers_per_master, available_gpus):<NewLine>    groups = {}<NewLine>    distranks = num_masters<NewLine>    gpu_allocated = 1<NewLine>    for i in range(num_masters):<NewLine>        my_group = [i]<NewLine>        gpus = [available_gpus[0]]<NewLine>        for j in range(workers_per_master):<NewLine>            my_group.append(distranks)<NewLine>            gpus.append(available_gpus[gpu_allocated])<NewLine>            distranks += 1<NewLine>            gpu_allocated += 1<NewLine>        for r, g in zip(my_group, gpus):<NewLine>            groups.update({<NewLine>                str(r): {<NewLine>                    'grp': my_group,<NewLine>                    'gpu': g<NewLine>                }<NewLine>            })<NewLine>    return groups<NewLine><NewLine><NewLine>def main():<NewLine>    num_masters = 3<NewLine>    workers_per_master = 1<NewLine>    available_gpus = [0, 1, 2, 3]<NewLine>    groups = assign_groups(num_masters, workers_per_master, available_gpus)<NewLine>    world_size = 6<NewLine>    mp.spawn(run, args=(groups, world_size), nprocs=world_size, join=True)<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    main()<NewLine><NewLine></code></pre><NewLine><p>Essentially, I have 3 master processes here and each of the masters has a worker. The masters with their respective workers make a group. Thus there are three groups altogether. Although in the above code the masters are assigned to cuda:0 and each of the workers to the next cuda devices, it may change depending on the setting. Thus it is immaterial in this illustration whether I comment out the line <code>torch.cuda.set_device(groups[str(rank)]['gpu'])</code>.</p><NewLine><p>Now, for sure for each of the masters and workers, I have its assigned group as <code>groups[str(rank)]['grp']</code>. Thus, when I run this code it should behave like those distributed communications being called concurrently as also illustrated by your example. However, it results in <code>NCCL INFO Call to connect returned Connection refused</code>.</p><NewLine><p>Definitely I am doing something wrong here, not sure what.<br/><NewLine>Thanks again.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think I figured out what exactly I was missing. So, basically each of the new process groups should be defined at each of the processes before using them in any manner whatsoever.</p><NewLine><p>A bit modified version of your example that most likely solves my purpose is the following:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.multiprocessing as mp<NewLine>import os<NewLine><NewLine><NewLine>def run(rank, world_size):<NewLine>    os.environ['MASTER_ADDR'] = 'localhost'<NewLine>    os.environ['MASTER_PORT'] = '29500'<NewLine>    # global group<NewLine>    torch.distributed.init_process_group(backend='nccl',<NewLine>                                         rank=rank,<NewLine>                                         world_size=world_size)<NewLine>    # torch.distributed.init_process_group(backend='gloo', init_method='env://')<NewLine>    g0 = torch.distributed.new_group(ranks=[0, 2])<NewLine>    g1 = torch.distributed.new_group(ranks=[1, 3])<NewLine>    # tensor to bcast over group<NewLine>    t = torch.tensor([1]).float().cuda().fill_(rank)<NewLine>    if rank in [0, 2]:<NewLine>        my_group = g0<NewLine>    else:<NewLine>        my_group = g1<NewLine>    torch.distributed.all_reduce(t, group=my_group)<NewLine>    print('rank: {} - val: {}'.format(rank, t.item()))<NewLine><NewLine><NewLine>def main():<NewLine>    world_size = 4<NewLine>    mp.spawn(run, args=(world_size,), nprocs=world_size, join=True)<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    main()<NewLine><NewLine></code></pre><NewLine><p>It works. <img alt="":slightly_smiling_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slightly_smiling_face.png?v=9"" title="":slightly_smiling_face:""/></p><NewLine><p>Thanks again.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Happy to see that worked <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>Yes, the <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.new_group"" rel=""nofollow noopener""><code>new_group</code></a> requires the following:</p><NewLine><blockquote><NewLine><p>This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Exactly. Thanks! <img alt="":slightly_smiling_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slightly_smiling_face.png?v=9"" title="":slightly_smiling_face:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/bapi; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/bapi; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/bapi; <NewLine> ,"REPLY_DATE 1: April 8, 2020,  7:06pm; <NewLine> REPLY_DATE 2: April 8, 2020,  7:09pm; <NewLine> REPLY_DATE 3: April 8, 2020,  8:33pm; <NewLine> REPLY_DATE 4: April 8, 2020,  8:56pm; <NewLine> REPLY_DATE 5: April 8, 2020,  9:30pm; <NewLine> REPLY_DATE 6: April 8, 2020,  9:31pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
65441,Shared Memory with mpi-backend,2019-12-31T07:04:20.670Z,1,162,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to create a shared memory / tensors among openmpi spawned process group? I know this can be done with the processes created by torch.multiprocessing package. But in my case I am unable to make this package work with openmpi processes.</p><NewLine></div>",https://discuss.pytorch.org/u/Usama-Zafar,(Usama Zafar),Usama-Zafar,"December 31, 2019,  7:04am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’d like to know the answer for this too.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>One solution might be</p><NewLine><ol><NewLine><li>create a shared memory buffer using MPI</li><NewLine><li>create a numpy <code>ndarray</code> from that shared memory (<a href=""https://stackoverflow.com/questions/32485122/shared-memory-in-mpi4py"" rel=""nofollow noopener"">example</a>)</li><NewLine><li>create a PyTorch tensor from that numpy <code>ndarray</code> (<a href=""https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#converting-numpy-array-to-torch-tensor"" rel=""nofollow noopener"">example</a>)</li><NewLine></ol><NewLine><p>The numpy ndarray and the tensor should then share the same storage, which is on the shared memory.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I didn’t say it, but I meant sharing cuda tensors.<br/><NewLine>(e.g: doing something like Hogwild example- but having the model on GPU instead of CPU and share its parameters with 2 MPI process on the same node)</p><NewLine><p>(For CPU tensors, your answer is correct, I read once a blog post demonstrating it, and all we need to modify is using shared MPI mem instead).<br/><NewLine>However, my 2 cents on this: it works out of the box only for host shared memory. Somehow couldn’t create shared+pinned host memory with pytorch easily. (I managed to do it with ugly hack: os.fork. I didn’t bother to make it work for MPI too)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/seliad; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/seliad; <NewLine> ,"REPLY_DATE 1: April 6, 2020,  7:22am; <NewLine> REPLY_DATE 2: April 6, 2020,  3:00pm; <NewLine> REPLY_DATE 3: April 7, 2020,  1:06pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
71648,Distributed Data Parallel with Multiple Losses,2020-03-01T22:02:54.834Z,7,865,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am using DistributedDataParallel with nccl. I have two losses which are averaged before calling backward. But backward doesn’t work. Here is the part of the code that is problematic:</p><NewLine><pre><code class=""lang-auto"">self.inputs['qa_in'][i] = Variable (self.inputs['qa_in'][i].data, requires_grad=True)<NewLine><NewLine>self.outputs['qa_outputs'][i] = self.qa_outputs(self.inputs['qa_in'][i])<NewLine><NewLine>start_logits, end_logits = self.outputs['qa_outputs'][i].split(1, dim=-1)<NewLine>start_logits = start_logits.squeeze(-1)<NewLine>end_logits = end_logits.squeeze(-1)<NewLine><NewLine>ignored_index = start_logits.size(1)<NewLine>        <NewLine>start_positions_ubatches[i].clamp_(0, ignored_index)<NewLine>end_positions_ubatches[i].clamp_(0, ignored_index)<NewLine><NewLine>loss_fct = CrossEntropyLoss(ignore_index=ignored_index)<NewLine><NewLine>start_loss = loss_fct(start_logits, start_positions_ubatches[i])<NewLine>end_loss = loss_fct(end_logits, end_positions_ubatches[i])<NewLine><NewLine>self.outputs['loss_out'][i] = (start_loss + end_loss) / 2<NewLine>self.outputs['loss_out'][i].backward( retain_graph=True)<NewLine></code></pre><NewLine><p>and I get the following error:</p><NewLine><p>File “/home/suncast/venv3/lib/python3.6/site-packages/torch/autograd/<strong>init</strong>.py”, line 99, in backward<br/><NewLine>allow_unreachable=True)  # allow_unreachable flag<br/><NewLine>RuntimeError: Expected to mark a variable ready only once. This error is caused by use of a module parameter outside the <code>forward</code> function. The return value of the <code>forward</code> function is inspected by the distributed data parallel wrapper to figure out if any of the module’s parameters went unused. If this is the case, it knows they won’t receive gradients in a backward pass. If any of those parameters are then used outside <code>forward</code>, this error condition is triggered. You can disable unused parameter detection by passing the keyword argument <code>find_unused_parameters=False</code> to <code>torch.nn.parallel.DistributedDataParallel</code>. (mark_variable_ready at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:342)</p><NewLine><p>I think the problem is the self.qa_outputs parameters are used twice in backward but I don’t know how to solve this. I don’t have any problem without distributed.</p><NewLine></div>",https://discuss.pytorch.org/u/maralm,(Maral),maralm,"March 1, 2020, 10:05pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you tried disabling unused parameter detection by passing <code>find_unused_parameters = False</code> to <code>torch.nn.parallel.DistributedDataParallel</code>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes. I get the following error when I set it to False:</p><NewLine><p>File “/home/suncast/venv3/lib/python3.6/site-packages/torch/autograd/<strong>init</strong>.py”, line 99, in backward<br/><NewLine>allow_unreachable=True)  # allow_unreachable flag<br/><NewLine>RuntimeError: has_marked_unused_parameters_ INTERNAL ASSERT FAILED at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:290, please report a bug to PyTorch.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Anyone knows how to solve this?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/maralm"">@maralm</a></p><NewLine><p>From your post, it is unclear which part is the DDP model. My assumption is that:</p><NewLine><ol><NewLine><li><NewLine><code>self.inputs['qa_in'][i]</code>: this is input to DDP forward</li><NewLine><li><NewLine><code>self.qa_outputs</code>: this is your DDP model</li><NewLine><li><NewLine><code>self.outputs['qa_outputs'][i]</code>: this is your DDP outputs</li><NewLine></ol><NewLine><blockquote><NewLine><p>I think the problem is the self.qa_outputs parameters are used twice in backward but I don’t know how to solve this. I don’t have any problem without distributed.</p><NewLine></blockquote><NewLine><p>This should be fine, the autograd engine should be able to manage backward inputs and dependencies from <code>start_loss</code> and <code>end_loss</code> properly.</p><NewLine><p>Two questions:</p><NewLine><ol><NewLine><li>Does it work if you directly call <code>self.outputs['qa_outputs'][i].sum().backward()</code> after line 3?</li><NewLine><li>Does any of the model parameters or outputs participates in other forward/backward passes?</li><NewLine></ol><NewLine><p>It will be very helpful for us to debug if you could share a minimum repro example. As we don’t know what happens outside of the posted code snippet, we can only make assumptions.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>,</p><NewLine><p>Thanks for your reply.<br/><NewLine>Your assumption is correct and self.qa_outputs is just a linear layer.</p><NewLine><p>Regarding your questions:</p><NewLine><ol><NewLine><li>No it doesn’t work with that.</li><NewLine><li>No, I am trying to just run a forward layer and compute backward with autograd.backward on that layer instead of running loss.backward().</li><NewLine></ol><NewLine><p>Basically, I have a large model and when I run it in a conventional way for forward and backward(loss.backward()), it works fine. But, I have a new implementation which runs backward layer by layer using autograd.backward. Using that, the algorithm works fine on a single gpu but face this error in distributed. I tried it on a different model which doesn’t have multiple losses and it is fine. In this case that I add multiple losses, the error comes.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""71648"" data-username=""maralm""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/e36b37/40.png"" width=""20""/> maralm:</div><NewLine><blockquote><NewLine><p>No, I am trying to just run a forward layer and compute backward with autograd.backward on that layer instead of running loss.backward().</p><NewLine></blockquote><NewLine></aside><NewLine><p>I see. DDP does not work for this case yet. Currently, all outputs you get from <code>DistributedDataPararlel.forward()</code> must participate in the same backward pass, otherwise, it would mess up with DDP’s internal communication state. Hope this can help explain that: <a href=""https://pytorch.org/docs/master/notes/ddp.html#internal-design"" rel=""nofollow noopener"">https://pytorch.org/docs/master/notes/ddp.html#internal-design</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I tried it on a different model which doesn’t have multiple losses and it is fine. In this case that I add multiple losses, the error comes.</p><NewLine></blockquote><NewLine><p>I might have misunderstand the use case. Adding up multiple losses should work, and this is different from running layer-by-layer backward, right? Would I be correct if I assume the code snippet you shared above is adding two losses together instead of doing layer-by-layer backward?</p><NewLine><p>It would be helpful if you could share a minimum repro for this error. Thanks!</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>No this is the same issue.<br/><NewLine>To simplify, let’s assume that I want to find the gradients for the last layer of the network only which includes a linear classifier and loss (using autograd.backward()). If I use a linear layer with single loss, DDP works with autograd but when I add two losses, it gives that error.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/osalpekar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/maralm; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/maralm; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/maralm; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/maralm; <NewLine> ,"REPLY_DATE 1: March 2, 2020,  7:01pm; <NewLine> REPLY_DATE 2: March 2, 2020,  9:19pm; <NewLine> REPLY_DATE 3: March 20, 2020,  1:57am; <NewLine> REPLY_DATE 4: March 20, 2020,  2:27pm; <NewLine> REPLY_DATE 5: March 20, 2020,  8:51pm; <NewLine> REPLY_DATE 6: April 5, 2020,  4:15pm; <NewLine> REPLY_DATE 7: April 5, 2020,  4:21pm; <NewLine> REPLY_DATE 8: April 6, 2020, 11:37pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
75359,Using spectral_norm with DistributedDataParallel makes backward() fail,2020-04-04T17:56:41.207Z,2,160,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!</p><NewLine><p>My training works if I use mutiple GPUs on a single machine (i.e.<code> DataParallel</code>). However, if I try to train on two machines (by using <code>DistributedDataParallel</code>), I get the following error on <code>.backward()</code>:</p><NewLine><pre><code class=""lang-auto"">one of the variables needed for gradient computation has been modified by<NewLine>an inplace operation: [torch.cuda.FloatTensor [256, 256, 5, 5]] is at version 2;<NewLine>expected version 1 instead. Hint: the backtrace further above shows the<NewLine>operation that failed to compute its gradient. The variable in question was <NewLine>changed in there or anywhere later. Good luck!<NewLine></code></pre><NewLine><p><code>torch.autograd.set_detect_anomaly(True)</code> points me to <code>spectral_norm</code>'s code that updates the weight:</p><NewLine><pre><code class=""lang-auto"">  File "".../python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__                                                                                                                  <NewLine>    hook(self, input)                                                                                                                                                                                                                        <NewLine>  File "".../python3.6/site-packages/torch/nn/utils/spectral_norm.py"", line 99, in __call__                                                                                                              <NewLine>    setattr(module, self.name, self.compute_weight(module, do_power_iteration=module.training))                                                                                                                                              <NewLine>  File "".../python3.6/site-packages/torch/nn/utils/spectral_norm.py"", line 86, in compute_weight                                                                                                        <NewLine>    weight = weight / sigma <NewLine></code></pre><NewLine><p>This definitely does not look like an inplace operation.<br/><NewLine>The same error occurs even if I use  <code>DistributedDataParallel</code> for a single machine.</p><NewLine><p>Any suggestions or ideas are more than welcome. Thanks in advance.</p><NewLine><h3>Versions</h3><NewLine><p>PyTorch: 1.1.0<br/><NewLine>CUDA: 9.0.176</p><NewLine></div>",https://discuss.pytorch.org/u/bornabesic,,bornabesic,"April 4, 2020,  5:57pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/bornabesic"">@bornabesic</a>,</p><NewLine><ol><NewLine><li>Can you try if setting <code>broadcast_buffers=False</code> in <code>DistributedDataParallel</code> constructor works for you?</li><NewLine><li>If not, can you try PyTorch v1.4?</li><NewLine><li>If it still does not work, could you please provide code for minimum repro? Thanks!</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a></p><NewLine><p>The problem happens regardless of the value of <code>broadcast_buffers</code>.</p><NewLine><p>I managed to narrow down my search for the source of the problem.<br/><NewLine>The error occurs if I use <strong>multiple GPUs per machine</strong> AND <strong>multiple forward passes</strong> of the module before <code>backward()</code>.<br/><NewLine>Otherwise, it works just fine.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p><strong>multiple forward passes</strong>  of the module before  <code>backward()</code> .</p><NewLine></blockquote><NewLine><p>Do you mean running multiple forward pass on the same DDP instance before launching the backward pass? If so, it is expected to hit errors due to <a href=""https://github.com/pytorch/pytorch/blob/df8d6eeb19423848b20cd727bc4a728337b73829/torch/nn/parallel/distributed.py#L454-L464"" rel=""nofollow noopener""><code>prepare_for_backward</code></a>. But I would expect it throws a different error though. A work around for this is to wrap your multiple forward pass into one <code>YourModule.forward</code> function, and then use DDP to wrap <code>YourModule</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/bornabesic; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: April 5, 2020,  4:08pm; <NewLine> REPLY_DATE 2: April 5, 2020,  9:15pm; <NewLine> REPLY_DATE 3: April 6, 2020,  2:39am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
74530,Functional Conv2 accept a batch of weights,2020-03-27T13:00:58.248Z,2,113,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>At the moment I am trying to implement a meta-learning algorithms and the size of the model is quite large so I am also trying  to use DataParallel. However, I am currently encountering an issue with one GPU taking the brunt of the load and running out of memory. This is since I generate weights for each sample in my batch, which means I have to loop over these weights and apply a functional conv and thus this operation cant be data paralleled and it ends up on the same GPU.</p><NewLine><p>Is there any easy way to feed a batch of weights to a functional conv or are there any plans to implement this in pytorch in the near future?</p><NewLine><p>Cheers,<br/><NewLine>Vincent</p><NewLine></div>",https://discuss.pytorch.org/u/vpolflie,,vpolflie,"March 27, 2020,  1:00pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""74530"" data-username=""vpolflie""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/v/f1d935/40.png"" width=""20""/> vpolflie:</div><NewLine><blockquote><NewLine><p>Is there any easy way to feed a batch of weights to a functional conv</p><NewLine></blockquote><NewLine></aside><NewLine><p>Not sure if I understand the request clearly, it would be helpful if you could share some pseudo code.</p><NewLine><p>If all you need is scatter conv weights (one weight per sample) across different GPUs, looks like you can wrap that (samples + one conv layer per GPU) into a custom function? In the forward function, you can do sth like:</p><NewLine><pre><code class=""lang-python"">def forward(self, samples):<NewLine>    outputs = []<NewLine>    for sample in samples:<NewLine>        weight = generate_per_sample_weight(sample)<NewLine>        replace_conv_weight(self.conv, weight)<NewLine>        outputs.append(self.conv(sample))<NewLine>    return outputs<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry for the late response.</p><NewLine><p>The code you provide is the pseudo code I would give and very similar to the code I have in my code base, one small change is that the weights are generated by different samples.</p><NewLine><pre><code class=""lang-auto"">def forward(self, samples, reference_samples):<NewLine>    outputs = []<NewLine>    for sample in samples:<NewLine>        weight = generate_per_sample_weight(reference_samples)<NewLine>        replace_conv_weight(self.conv, weight)<NewLine>        outputs.append(self.conv(sample))<NewLine>    return outputs<NewLine></code></pre><NewLine><p>However, the main issue with this is that pytorch doesn’t distribute this over multiple GPU’s because of the for loop. These calculation are all located on the first main GPU (with the standard dataparallel package)</p><NewLine><p>I was wondering if there is an easy way to write something like this which still allows for data parallelisation:</p><NewLine><pre><code class=""lang-auto"">weights: Batch x # INPUT FILTERS x # OUTPUT FILTERS x FILTER WIDTH x FILTER HEIGHT<NewLine>samples: BATCH x CHANNELS x WIDTH x HEIGHT<NewLine>def forward(self, samples, weights):<NewLine>    outputs = self.conv(sample, weights)<NewLine>    return outputs<NewLine></code></pre><NewLine><p>So this self.conv function would then be one purely based on matrices like the original conv one, which should allow data parallelisation.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""74530"" data-username=""vpolflie""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/v/f1d935/40.png"" width=""20""/> vpolflie:</div><NewLine><blockquote><NewLine><p>I was wondering if there is an easy way to write something like this which still allows for data parallelisation:</p><NewLine></blockquote><NewLine></aside><NewLine><p>This should work, as <a href=""https://github.com/pytorch/pytorch/blob/df8d6eeb19423848b20cd727bc4a728337b73829/torch/nn/parallel/data_parallel.py#L151-L155"" rel=""nofollow noopener"">DataParallel</a> simply replicates model and scatters input. (assuming <code>self.conv</code> is a customized conv layer that replaces weight) So if you wrap that with DataParallel, different thread/replica should see samples/weights on a different device. Did you encounter any issue when doing this?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>At the moment I have the following psuedo code:</p><NewLine><pre><code class=""lang-auto"">def batch_conv(x, weight, bias=None, stride=1):<NewLine>    for i in range(x.size()[0]):<NewLine>        yi = F.conv_transpose2d(x[i:i+1], weight=weight[i], bias=bias[i,:weight.size(2)], padding=1, stride=int(1/stride), output_padding=1, groups=groups)<NewLine>        y = concat(y, yi)<NewLine>    return y<NewLine><NewLine>class AdaptiveConv2d(nn.Module):<NewLine>    def __init__(self, *args, **kwargs):<NewLine>        super().__init__()<NewLine><NewLine>    def forward(self, input, weight=None, bias=None, stride=1):<NewLine>        return batch_conv(input, weight, bias, stride)<NewLine><NewLine></code></pre><NewLine><p>However this doesn’t distribute properly and my assumption is that the data parallel isn’t able to handle the for loop in my code.</p><NewLine><p>I haven’t tried implementing a conv layer that takes a batch of weights instead of a single sample. Since I have only switched to pytorch recently and I am a bit out of depth with this.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vpolflie; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/vpolflie; <NewLine> ,"REPLY_DATE 1: March 27, 2020,  2:40pm; <NewLine> REPLY_DATE 2: April 3, 2020,  8:53am; <NewLine> REPLY_DATE 3: April 3, 2020,  2:23pm; <NewLine> REPLY_DATE 4: April 3, 2020,  2:53pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
75029,Distributed libtorch,2020-04-01T17:00:19.582Z,1,143,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Dear all,</p><NewLine><p>I see some interesting examples with python (<a href=""https://pytorch.org/tutorials/intermediate/dist_tuto.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/dist_tuto.html</a>).<br/><NewLine>I want to make a example with libtorch c++.<br/><NewLine>Please share with me the tutorial or similar functions with c++ to do that.</p><NewLine><p>Thank you so much,<br/><NewLine>Best regards,</p><NewLine></div>",https://discuss.pytorch.org/u/ph0123,(chau phuong),ph0123,"April 1, 2020,  5:00pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ph0123"">@ph0123</a> <code>DistributedDataParallel</code> API is not yet available in C++. We are working on that.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""75029"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p><code>DistributedDataParallel</code></p><NewLine></blockquote><NewLine></aside><NewLine><p>Thank you so much! Could you estimate when the API will release?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>We can aim for in 3 months, but cannot promise that. If we see more people requesting this, we can try to allocate more resources to it and bump up the priority.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ph0123; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: April 1, 2020,  6:11pm; <NewLine> REPLY_DATE 2: April 1, 2020,  6:34pm; <NewLine> REPLY_DATE 3: April 2, 2020,  3:26pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
75034,Controlling Epochs for distributed dataparallel,2020-04-01T19:30:10.778Z,0,104,"<div class=""post"" itemprop=""articleBody""><NewLine><p>While using dataparallel is it possible to run processes with different number of epochs. Say on machine one I would like to run the process for 20 epochs and sync with master however after 20 epochs I would want to run completely on master. Is there a workaround for this? I used one of the samples given in tutorials however in the event that epochs are varying the master waits to sync up though the process has completed on  another machine.</p><NewLine></div>",https://discuss.pytorch.org/u/aradhyamathur,(Aradhya Mathur),aradhyamathur,"April 1, 2020,  7:30pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>DDP instances need to all participate in the backward, otherwise it would hang. But there are work around. If you know that master would run say 100 epochs, and other nodes would run 80 epochs, you can call forward-backward on the DDP instance for 80 epochs. After that, you can delete the DDP instance, which will remove the DDP grad hooks accordingly. Then, you can run forward-backward on DDP.module (as DDP is deleted, you won’t be able to call DDP.module, but the program can still have a reference to the original local module separately) on master, and it will no longer trigger communications.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: April 2, 2020,  3:23pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
74866,DistributedDataParallel and SubsetRandomSampler,2020-03-31T03:36:49.134Z,0,80,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am currently using SubsetRandomSampler to enforce a train-val split on my custom dataset, which works well on my current single-GPU configuration. However, in anticipation of moving to training on multiple nodes and GPUs, I wanted to see if it’s possible to “wrap” the splits created by SubsetRandomSampler somehow such that within my train split, I can replicate the functionality of DistributedSampler.</p><NewLine><p>If not – what alternatives do I have for creating a train-val split?  Must I create separate Dataset objects for the train and the val set?</p><NewLine></div>",https://discuss.pytorch.org/u/tchainzzz,(Trenton Chang),tchainzzz,"March 31, 2020,  3:36am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/vincentqb"">@vincentqb</a> for dataloader question. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 31, 2020,  2:21pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
74840,Averaging Gradients in DistributedDataParallel,2020-03-30T20:38:36.218Z,0,259,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am a bit confused about averaging gradients in distributed data-parallel. It seems there are two examples from the PyTorch documentation that are different. In one example, you create the model and just pass it to the GPU available then create a separate function to average gradients.</p><NewLine><pre><code class=""lang-auto"">def average_gradients(model):<NewLine>    """""" Gradient averaging. """"""<NewLine>    size = float(dist.get_world_size())<NewLine>    for param in model.parameters():<NewLine>        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)<NewLine>        param.grad.data /= size<NewLine></code></pre><NewLine><p>and its executed as follows.</p><NewLine><pre><code class=""lang-auto"">        outputs = model(inputs)<NewLine>        loss = loss_function(outputs, labels)<NewLine>        loss.backward()<NewLine>        average_gradients(model)<NewLine>        optimizer.step()<NewLine><NewLine></code></pre><NewLine><p>The other approach I have seen doesn’t create a separate function and just calls DPP.</p><NewLine><pre><code class=""lang-auto"">    model = ToyModel().cuda(device_ids[0])<NewLine>    ddp_model = DDP(model, device_ids)<NewLine><NewLine>    loss_fn = nn.MSELoss()<NewLine>    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)<NewLine><NewLine>    optimizer.zero_grad()<NewLine>    outputs = ddp_model(torch.randn(20, 10))<NewLine>    labels = torch.randn(20, 5).to(device_ids[0])<NewLine>    loss_fn(outputs, labels).backward()<NewLine>    optimizer.step()<NewLine></code></pre><NewLine><p>I would like to know whats the difference between the two approaches and which one should one use for distributed training in a HPC cluster. I specifically want to use two nodes, each with 4 GPUs.</p><NewLine></div>",https://discuss.pytorch.org/u/ankahira,(Albert Njoroge Kahira),ankahira,"March 30, 2020,  8:38pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ankahira"">@ankahira</a>, usually, there are 4 steps in distributed data parallel training:</p><NewLine><ol><NewLine><li>local forward to compute loss</li><NewLine><li>local backward to compute local gradients</li><NewLine><li>allreduce (communication) to compute global gradients. This would be allreduce with SUM + divide by world size to calculate average</li><NewLine><li>optimizer step to use global gradients to update parameters</li><NewLine></ol><NewLine><p>Both examples you mentioned above conduct the same four steps and are mathematically equivalent. The difference is that DDP would allow step 2 (backward computation) and 3 (allreduce communication) to overlap and therefore DDP is expected to be faster than the <code>average_gradients</code> approach.</p><NewLine><p>More specifically, in the first example with <code>average_gradients</code>, there is a hard barrier between backward and allreduce, i.e., no comm can start before computation finishes. In the second example, DDP organizes gradients into buckets, and will launch comm as soon as a bucket of gradients are ready, so that computation and communication can run in parallel. <a href=""https://pytorch.org/docs/master/notes/ddp.html"" rel=""nofollow noopener"">This</a> would help explain that.</p><NewLine><p>I would recommend DDP. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 31, 2020,  2:38pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
73455,Load DDP model trained with 8 gpus on only 2 gpus?,2020-03-17T01:12:01.640Z,9,619,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have many <strong>Distributed Data Parallel models (NOT Data Parallel!)</strong> trained with 8 gpus on a cluster. I have no problem correctly restoring them with same number of gpus (8). But wait time to get 8 is too long. So I want to restore them with only two.</p><NewLine><p>I was wondering if it is even possible? if so what is the correct way to do it?</p><NewLine><p>The script below (test.py) works fine with 8 gpus but produces erroneous results with 2 gpus (in the latter case, the results are the same as a model just initialized with random weights). I use “python -m torch.distributed.launch --nproc_per_node=num_gpus test.py” to run it from terminal.</p><NewLine><pre><code class=""lang-auto"">import argparse<NewLine>from torchvision.models import resnet18<NewLine>from torch.nn.parallel import DistributedDataParallel as DDP<NewLine>import torch.distributed as dist<NewLine><NewLine>def cleanup():<NewLine>    dist.destroy_process_group()<NewLine><NewLine>def main():<NewLine>    torch.distributed.init_process_group(<NewLine>backend='nccl', init_method='env://')<NewLine>    torch.cuda.set_device(args.local_rank)<NewLine>    model = resnet18()<NewLine>    model = model.to([args.local_rank][0])<NewLine>    model = DDP(model, device_ids=[args.local_rank], <NewLine>output_device=[args.local_rank][0])<NewLine><NewLine>    # load the model<NewLine>    checkpoint = torch.load(load_path)<NewLine>    state_dict = checkpoint['model_state_dict']<NewLine>    model.load_state_dict(state_dict)<NewLine>    dist.barrier()<NewLine><NewLine>    cleanup()<NewLine><NewLine>if __name__ == '__main__':<NewLine>    parser = argparse.ArgumentParser(description=""blah"")<NewLine>    parser.add_argument(""--local_rank"", type=int)<NewLine>    args, _ = parser.parse_known_args()<NewLine>    main()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/kazem,(kazem safari),kazem,"March 17, 2020, 11:28am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This should be possible, there is a <code>map_location</code> argument in <code>torch.load</code>. Checkout <a href=""https://pytorch.org/tutorials/beginner/saving_loading_models.html"" rel=""nofollow noopener"">this</a>.</p><NewLine><p>The <code>map_location</code> can be a device, a function, a map etc. [<a href=""https://pytorch.org/docs/stable/torch.html#torch.load"" rel=""nofollow noopener"">API</a>]</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your answer. The documentation does not include a working example for DDP. I have already tried many ways using map function None of which have worked so far. If you could show me a simple working example with mnist dataset to map 8 gpus to 1 or 2 or 4 gpus, or cpu with DistributedData parallel I would greatly appreciate it.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>There are a few things to clarify.</p><NewLine><ol><NewLine><li>As you are using the <code>resnet18</code> from torchvision, the model only lives on a single GPU.</li><NewLine><li>The launcher script you use starts <code>num_gpus</code> processes, and each process has its own DDP instance, dataloader, and the model replica.</li><NewLine><li>With 1 and 2, your training scripts only need put the model to one GPU (you can use the rank as the device id), load the data into one GPU, and the DDP instance will handle the comm for you, and make sure that all model replicas are synchronized properly.</li><NewLine><li>With the above 3, the question then would be “how do I load a model to a specific GPU device?”. And the answer is use <code>map_local=torch.device(rank)</code>.</li><NewLine></ol><NewLine><p>The following code works for me with the launching cmd</p><NewLine><pre><code class=""lang-auto"">python -m torch.distributed.launch --nproc_per_node=2 test.py<NewLine></code></pre><NewLine><pre><code class=""lang-python"">import argparse<NewLine>from torchvision.models import resnet18<NewLine>from torch.nn.parallel import DistributedDataParallel as DDP<NewLine>import torch.distributed as dist<NewLine>import torch<NewLine><NewLine>def cleanup():<NewLine>    dist.destroy_process_group()<NewLine><NewLine>def main(args):<NewLine>    torch.distributed.init_process_group(backend='nccl', init_method='tcp://localhost:23456', rank=args.local_rank, world_size=2)<NewLine>    torch.cuda.set_device(args.local_rank)<NewLine>    model = resnet18()<NewLine><NewLine>    path = ""save_model.pt""<NewLine>    if args.local_rank == 0:<NewLine>        # save CPU model<NewLine>        torch.save(model, path)<NewLine><NewLine>    dist.barrier()<NewLine>    # local model to GPU<NewLine>    loaded_model = torch.load(path, map_location=torch.device(args.local_rank))<NewLine><NewLine>    model = DDP(loaded_model, device_ids=[args.local_rank])<NewLine>    print(f""Rank {args.local_rank} traning on device {list(model.parameters())[0].device}"")<NewLine><NewLine>    # create a dedicated data loader for each process<NewLine><NewLine>    cleanup()<NewLine><NewLine>if __name__ == '__main__':<NewLine>    parser = argparse.ArgumentParser(description=""blah"")<NewLine>    parser.add_argument(""--local_rank"", type=int)<NewLine>    args, _ = parser.parse_known_args()<NewLine>    main(args)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>@ mrshenli thanks for your reply. I tried your method after a few minor correction but it still gives me the same erroneous result. I use this <a href=""https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py"" rel=""nofollow noopener"">resnet</a> script to call the model.</p><NewLine><p>I trained it on a large dataset and decided to save it periodically during training. Due to testing slowing down the training I decided to test it later using the saved models. When I train DDP with 8 gpus and test DDP with 8 gpus later, there is no issue. However, when I train DDP with 8 gpus  and test DDP with 2 gpus later the problem occurs.</p><NewLine><p>Also I only want to save and load the state_dict and not the entire model since it takes a lot of space.</p><NewLine><p>I will create a working example for mnist shortly.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I tried your method after a few minor correction but it still gives me the same erroneous result. I use this <a href=""https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py"" rel=""nofollow noopener"">resnet</a> script to call the model.</p><NewLine></blockquote><NewLine><p>You mean you saw error by running the script as is? What error did you see and what fix did you applied?</p><NewLine><blockquote><NewLine><p>Also I only want to save and load the state_dict and not the entire model since it takes a lot of space.</p><NewLine></blockquote><NewLine><p>It should be doable by just modifying two lines (save and load).</p><NewLine><blockquote><NewLine><p>When I train DDP with 8 gpus and test DDP with 8 gpus later, there is no issue. However, when I train DDP with 8 gpus and test DDP with 2 gpus later the problem occurs.</p><NewLine></blockquote><NewLine><p>The resnet link you posted points to torchvision resnet, so the model only lives on a single device. How did you go from training on 8 gpus to testing on 2 gpus? Did you do the following?</p><NewLine><ol><NewLine><li>After training, use <strong>only</strong> rank 0 to save <code>ddp.module</code> to file.</li><NewLine><li>For testing, as you no longer need comm across models, you don’t need DDP. You can spawn two processes, each load the saved module from file to its dedicated device by setting <code>map_reduce</code>. And use sth like <code>all_gather</code> to collect loss/accuracy data to rank 0?</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>@ mrshenli thanks again. I will try to answer all your inquiries with more detail in a bit today.</p><NewLine><p>Unfortunately, I could not use your script as is because my already saved DDP (without “.module”) was already saved using a state_dict method.<br/><NewLine>So as for the minor changes, I did the following. :</p><NewLine><pre><code class=""lang-auto"">def main(args):<NewLine>    torch.distributed.init_process_group(backend='nccl', init_method='env://')<NewLine>    test_loader = DataLoader(<NewLine>        test_dataset,<NewLine>        batch_size=args.test_batch_size,<NewLine>        shuffle=False,<NewLine>        num_workers=args.num_workers,<NewLine>        pin_memory=True)<NewLine><NewLine>    model = get_model()<NewLine>#############################################################<NewLine>   # My changes<NewLine>    torch.cuda.set_device(args.local_rank)<NewLine>    model = model.to([args.local_rank][0])<NewLine>    model = DDP(model, device_ids=[args.local_rank], <NewLine>output_device=[args.local_rank][0])<NewLine>    checkpoint = torch.load(args.load_path)  # , map_location=map_location)<NewLine>    state_dict = checkpoint['model_state_dict']<NewLine>    model.load_state_dict(state_dict)<NewLine>##############################################################<NewLine>    dist.barrier()<NewLine>    test_function(model, test_loader, args.local_rank,args.load_path.with_suffix('.csv'))<NewLine></code></pre><NewLine><p>I trained resnet18 from scratch. I just copied and used the <a href=""https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py"" rel=""nofollow noopener"">resnet</a> script locally.</p><NewLine><p>As for your last two comments I did use just rank 0 to save the ddp, but I saved the state_dict() for ddp itself (without .module). That is why when I used your script I also had to remove the .module similar to this:<br/><NewLine><a href=""https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686"">[solved] KeyError: ‘unexpected key “module.encoder.embedding.weight” in state_dict’</a><br/><NewLine>Is it correctly to do so?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group quote-modified"" data-post=""7"" data-topic=""73455"" data-username=""kazem""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/kazem/40/20632_2.png"" width=""20""/> kazem:</div><NewLine><blockquote><NewLine><p>As for your last two comments I did use just rank 0 to save the ddp, but I saved the state_dict() for ddp itself (without .module). That is why when I used your script I also had to remove the. Is it correctly to do so?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, that is correct. The saved and loaded model type need to match.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""73455"" data-username=""kazem""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/kazem/40/20632_2.png"" width=""20""/> kazem:</div><NewLine><blockquote><NewLine><p>checkpoint = torch.load(args.load_path) # , map_location=map_location)</p><NewLine></blockquote><NewLine></aside><NewLine><p>This line might cause a problem if the model was saved from a device that is not available on the machine that loads the model. But it should be OK in your case, as the model was saved from rank 0 (i.e., “cuda:0”), whose device is available in both envs. However, without <code>map_location</code>, it means the two DDP processes in testing are operating on the same GPU? That could also cause problems.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://discuss.pytorch.org/u/mrshenli"">mrshenli</a> Sorry for the late reply. Say I want to train the DDP model on 4 gpus and restore it as DDP  on 2. I created an mnist example to illustrate my case while following your example. This whole script is borrowed from <a href=""https://github.com/pytorch/examples/blob/master/mnist/main.py"" rel=""nofollow noopener"">mnist</a>, modified and split into three scripts:</p><NewLine><ol start=""0""><NewLine><li>mnist_common.py</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.distributed as dist<NewLine>import argparse<NewLine>from torchvision import datasets, transforms<NewLine>from torch.utils.data.distributed import DistributedSampler<NewLine>from torch.utils.data import DataLoader<NewLine><NewLine><NewLine>def cleanup():<NewLine>    dist.destroy_process_group()<NewLine><NewLine><NewLine>class Net(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Net, self).__init__()<NewLine>        self.conv1 = nn.Conv2d(1, 32, 3, 1)<NewLine>        self.conv2 = nn.Conv2d(32, 64, 3, 1)<NewLine>        self.dropout1 = nn.Dropout2d(0.25)<NewLine>        self.dropout2 = nn.Dropout2d(0.5)<NewLine>        self.fc1 = nn.Linear(9216, 128)<NewLine>        self.fc2 = nn.Linear(128, 10)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.conv1(x)<NewLine>        x = F.relu(x)<NewLine>        x = self.conv2(x)<NewLine>        x = F.relu(x)<NewLine>        x = F.max_pool2d(x, 2)<NewLine>        x = self.dropout1(x)<NewLine>        x = torch.flatten(x, 1)<NewLine>        x = self.fc1(x)<NewLine>        x = F.relu(x)<NewLine>        x = self.dropout2(x)<NewLine>        x = self.fc2(x)<NewLine>        output = F.log_softmax(x, dim=1)<NewLine>        return output<NewLine><NewLine><NewLine>def train(args, model, device, train_loader, optimizer, epoch):<NewLine>    model.train()<NewLine>    for batch_idx, (data, target) in enumerate(train_loader):<NewLine>        data, target = data.to(device, non_blocking=True), \<NewLine>                       target.to(device, non_blocking=True)<NewLine>        optimizer.zero_grad()<NewLine>        output = model(data)<NewLine>        loss = F.nll_loss(output, target)<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine><NewLine><NewLine>def test(args, model, device, test_loader):<NewLine>    model.eval()<NewLine>    test_loss = 0<NewLine>    correct = 0<NewLine>    with torch.no_grad():<NewLine>        for data, target in test_loader:<NewLine>            data, target = data.to(device, non_blocking=True), \<NewLine>                           target.to(device, non_blocking=True)<NewLine>            output = model(data)<NewLine>            test_loss += F.nll_loss(<NewLine>                output,<NewLine>                target,<NewLine>                reduction='sum').item()<NewLine>            pred = output.argmax(dim=1, keepdim=True)<NewLine>            correct += pred.eq(target.view_as(pred)).sum().item()<NewLine><NewLine>    test_loss /= len(test_loader.dataset)<NewLine><NewLine>    if args.local_rank == 0:<NewLine>        print('Test set: Average loss: {:.4f},'<NewLine>              ' Accuracy: {}/{} ({:.2f}%)'.format(<NewLine>            test_loss, correct, len(test_loader.dataset),<NewLine>            100. * correct / len(test_loader.dataset)))<NewLine><NewLine><NewLine># Training settings<NewLine>parser = argparse.ArgumentParser(description='PyTorch MNIST Example')<NewLine>parser.add_argument('--batch-size',<NewLine>                    type=int,<NewLine>                    default=64,<NewLine>                    metavar='N',<NewLine>                    help='input batch size for training')<NewLine>parser.add_argument('--test-batch-size',<NewLine>                    type=int,<NewLine>                    default=1000,<NewLine>                    metavar='N',<NewLine>                    help='input batch size for testing')<NewLine>parser.add_argument('--epochs', type=int, default=14, metavar='N',<NewLine>                    help='number of epochs to train (default: 14)')<NewLine>parser.add_argument('--lr', type=float, default=1.0, metavar='LR',<NewLine>                    help='learning rate (default: 1.0)')<NewLine>parser.add_argument('--gamma', type=float, default=0.7, metavar='M',<NewLine>                    help='Learning rate step gamma (default: 0.7)')<NewLine>parser.add_argument('--seed', type=int, default=1, metavar='S',<NewLine>                    help='random seed (default: 1)')<NewLine>parser.add_argument('--local_rank', type=int)<NewLine>args = parser.parse_args()<NewLine><NewLine>train_dataset = datasets.MNIST(<NewLine>    '../data',<NewLine>    train=True,<NewLine>    download=False,<NewLine>    transform=transforms.Compose([<NewLine>       transforms.ToTensor(),<NewLine>       transforms.Normalize((0.1307,), (0.3081,))<NewLine>    ]))<NewLine>train_sampler = DistributedSampler(<NewLine>    train_dataset,<NewLine>    num_replicas=torch.cuda.device_count(),<NewLine>    rank=args.local_rank)<NewLine>train_loader = DataLoader(train_dataset,<NewLine>                          batch_size=args.batch_size,<NewLine>                          shuffle=(train_sampler is None),<NewLine>                          num_workers=0,<NewLine>                          pin_memory=True,<NewLine>                          sampler=train_sampler)<NewLine>test_loader = DataLoader(<NewLine>    datasets.MNIST(<NewLine>        '../data',<NewLine>        train=False,<NewLine>        transform=transforms.Compose([<NewLine>            transforms.ToTensor(),<NewLine>            transforms.Normalize((0.1307,), (0.3081,))<NewLine>        ])),<NewLine>    batch_size=args.test_batch_size,<NewLine>    shuffle=True,<NewLine>    num_workers=0,<NewLine>    pin_memory=True,)<NewLine></code></pre><NewLine><ol><NewLine><li>mnist_train.py</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">from __future__ import print_function<NewLine>import torch<NewLine>import torch.optim as optim<NewLine>import torch.distributed as dist<NewLine>import torch.backends.cudnn as cudnn<NewLine>from torch.optim.lr_scheduler import StepLR<NewLine>from torch.nn.parallel import DistributedDataParallel as DDP<NewLine>from mnist_common import args, Net, train_loader, train_sampler,\<NewLine>    test_loader, train, test, cleanup<NewLine><NewLine><NewLine>def main(args):<NewLine>    dist.init_process_group(backend='nccl',<NewLine>                            init_method='tcp://localhost:23456',<NewLine>                            rank=args.local_rank,<NewLine>                            world_size=torch.cuda.device_count())<NewLine>    torch.manual_seed(args.seed)<NewLine>    torch.cuda.set_device(args.local_rank)<NewLine>    cudnn.benchmark = True<NewLine><NewLine>    model = Net()<NewLine>    model = model.to([args.local_rank][0])  # distribute the model<NewLine>    # Should we set the output_device value in DPP?<NewLine>    model = DDP(model, device_ids=[args.local_rank])<NewLine>    # , output_device=[args.local_rank][0])<NewLine><NewLine>    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)<NewLine>    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)<NewLine>    for epoch in range(1, args.epochs + 1):<NewLine>        train_sampler.set_epoch(epoch)<NewLine>        train(args, model, args.local_rank,<NewLine>              train_loader, optimizer, epoch)<NewLine>        test(args, model, args.local_rank, test_loader)<NewLine>        scheduler.step(epoch)<NewLine><NewLine>    # I intend to save the model<NewLine>    # AFTER some training not, not before<NewLine>    if args.local_rank == 0:<NewLine>        torch.save(model, ""mnist_cnn.pt"")<NewLine>    dist.barrier()<NewLine>    cleanup()<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    main(args)<NewLine></code></pre><NewLine><p>Also I intend to test the model, only after training (sometimes up to a few days) has finished, by restoring the saved weights (or model).<br/><NewLine>2) mnist_test.py</p><NewLine><pre><code class=""lang-auto"">from __future__ import print_function<NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>from torch.nn.parallel import DistributedDataParallel as DDP<NewLine>from mnist_common import args, Net, test_loader, test, cleanup<NewLine><NewLine><NewLine>def main(args):<NewLine>    dist.init_process_group(backend='nccl',<NewLine>                            init_method='tcp://localhost:23456',<NewLine>                            rank=args.local_rank,<NewLine>                            world_size=2)<NewLine>    torch.manual_seed(args.seed)<NewLine>    torch.cuda.set_device(args.local_rank)<NewLine><NewLine>    model = torch.load(""mnist_cnn.pt"",<NewLine>                       map_location=torch.device(args.local_rank))<NewLine>    model = DDP(model, device_ids=[args.local_rank])<NewLine>    print(f""Rank {args.local_rank} ""<NewLine>          f""test on device {list(model.parameters())[0].device}"")<NewLine><NewLine>    test(args, model, args.local_rank, test_loader)<NewLine>    cleanup()<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    main(args)<NewLine></code></pre><NewLine><p>The mnist_train.py runs sucessfully using<br/><NewLine><code>python -m torch.distributed.launch nproc_per_node=4 (or 2) mnist_train.py</code>.<br/><NewLine>but when i run the test script using<br/><NewLine><code>python -m torch.distributed.launch nproc_per_node=2 mnist_test.py</code>.<br/><NewLine>I get the following:</p><NewLine><pre><code class=""lang-auto"">Rank 0 test on device cuda:0<NewLine>Rank 1 test on device cuda:1<NewLine>Test set: Average loss: 0.0274, Accuracy: 9913/10000 (99.13%)<NewLine><NewLine>RuntimeError: Expected tensor for argument #1 input to have <NewLine>the same device as tensor for argument #2 weight; <NewLine>but device 0 does not equal 1 <NewLine>(while checking arguments for cudnn_convolution)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Rank 0 test on device cuda:0<br/><NewLine>Rank 1 test on device cuda:1<br/><NewLine>Test set: Average loss: 0.0274, Accuracy: 9913/10000 (99.13%)</p><NewLine><p>RuntimeError: Expected tensor for argument <span class=""hashtag"">#1</span> input to have<br/><NewLine>the same device as tensor for argument <span class=""hashtag"">#2</span> weight;<br/><NewLine>but device 0 does not equal 1<br/><NewLine>(while checking arguments for cudnn_convolution)</p><NewLine></blockquote><NewLine><p>This means the first parameter of both models are placed onto the correct device. Can you do the same check for all parameters? i.e., making sure that all parameters are placed to the correct device.</p><NewLine><blockquote><NewLine><p><code>output = model(data)</code></p><NewLine></blockquote><NewLine><p>Before the line above in <code>test(...)</code>, can you print the device ids of the data as well? Looks like the model and data device does not match on rank 1.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see. But it should not be the case since both are moved to <code>args.local_rank</code>. Anyways, I did what you suggested and also changed the <code>test-batch-size</code> to <code>1024</code>. Here’s the outcome:</p><NewLine><pre><code class=""lang-auto"">Rank 0 test on device cuda:0<NewLine>Rank 1 test on device cuda:1<NewLine>after data=data.to(device,), before output=model(data) in test function,  batch_idx: 0 device: 1<NewLine>after data=data.to(device,), before output=model(data) in test function,  batch_idx: 0 device: 0<NewLine>after data=data.to(device,), before output=model(data) in test function,  batch_idx: 1 device: 0<NewLine>after data=data.to(device,), before output=model(data) in test function,  batch_idx: 2 device: 0<NewLine>after data=data.to(device,), before output=model(data) in test function,  batch_idx: 3 device: 0<NewLine>after data=data.to(device,), before output=model(data) in test function,  batch_idx: 4 device: 0<NewLine>after data=data.to(device,), before output=model(data) in test function,  batch_idx: 5 device: 0<NewLine>after data=data.to(device,), before output=model(data) in test function,  batch_idx: 6 device: 0<NewLine>after data=data.to(device,), before output=model(data) in test function,  batch_idx: 7 device: 0<NewLine>after data=data.to(device,), before output=model(data) in test function,  batch_idx: 8 device: 0<NewLine>after data=data.to(device,), before output=model(data) in test function,  batch_idx: 9 device: 0<NewLine>Test set: Average loss: 0.0275, Accuracy: 9913/10000 (99.13%)<NewLine><NewLine>RuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 0 does not equal 1 (while checking arguments for cudnn_convolution)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Rank 1 test on device cuda:1<br/><NewLine>after data=data.to(device,), before output=model(data) in test function,  batch_idx: 0 device: 1</p><NewLine></blockquote><NewLine><p>This is weird. This means all model parameters are on <code>cuda:1</code> and the input batch is also on <code>cuda:1</code>, but somehow one of the conv layers still throws device mismatch? I am not sure what happened here, but as the error suggests the mismatch occurs in <code>cudnn_convolution</code>, I would check if the input (<code>x</code>) of and the parameters the two conv layer (<code>self.conv1</code> and <code>self.conv2</code>) match in the <code>forward()</code> function during testing.</p><NewLine><p>BTW, two more comments on the script:</p><NewLine><ol><NewLine><li>As you are only doing forward during testing, it is not necessary to use DDP there, as all comm in DDP occurs during backward.</li><NewLine><li>I noticed you saving a DDP module and then load that DDP module and wrap it with another DDP module. Is this intentional? Shouldn’t <code>mnist_train.py</code> save <code>model.module</code> instead? (or use model.module to initialize DDP instances in testing)</li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kazem; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/kazem; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/kazem; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/kazem; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/kazem; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 19, 2020,  4:14pm; <NewLine> REPLY_DATE 2: March 20, 2020,  9:55pm; <NewLine> REPLY_DATE 3: March 23, 2020,  2:12am; <NewLine> REPLY_DATE 4: March 23, 2020,  6:06am; <NewLine> REPLY_DATE 5: March 23, 2020,  2:13pm; <NewLine> REPLY_DATE 6: March 23, 2020,  2:43pm; <NewLine> REPLY_DATE 7: March 23, 2020,  2:43pm; <NewLine> REPLY_DATE 8: March 23, 2020,  2:54pm; <NewLine> REPLY_DATE 9: March 29, 2020,  1:47am; <NewLine> REPLY_DATE 10: March 30, 2020,  8:34pm; <NewLine> REPLY_DATE 11: March 30, 2020, 11:21pm; <NewLine> REPLY_DATE 12: March 31, 2020, 12:26am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> 
74769,Pytorch distributed with gloo backend send float32 or float64,2020-03-30T09:37:33.056Z,1,70,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I am implementing MPI and compare to pytorch distributed,<br/><NewLine>it’s seems that there is mismatch between my implemetation and the one of pytorch with gloo.</p><NewLine><p>does pytorch distributed send float32 or float64 tensors?</p><NewLine></div>",https://discuss.pytorch.org/u/Liron_Mor_Yosef,(Liron Mor Yosef),Liron_Mor_Yosef,"March 30, 2020,  9:37am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""74769"" data-username=""Liron_Mor_Yosef""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/liron_mor_yosef/40/12953_2.png"" width=""20""/> Liron_Mor_Yosef:</div><NewLine><blockquote><NewLine><p>does pytorch distributed send float32 or float64 tensors?</p><NewLine></blockquote><NewLine></aside><NewLine><p>It depends on what the scalar type of the tensor you passed to the communication API. Check out <a href=""https://github.com/pytorch/pytorch/blob/a8ca340ad6faa8ebb51fc364b891985843b5fe14/torch/lib/c10d/ProcessGroupGloo.cpp#L36-L61"" rel=""nofollow noopener"">this</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 30, 2020,  6:05pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
74826,Pytorch multiprocessing dataloader worker_init_fn problem,2020-03-30T17:10:21.792Z,0,213,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a problem with this code piece</p><NewLine><pre><code class=""lang-auto"">import os<NewLine><NewLine>import torch<NewLine><NewLine><NewLine>class Dataset(torch.utils.data.Dataset):<NewLine>    arg = {'batch_size': 1}<NewLine>    def __init__(self, arg):<NewLine>        print('__init__')<NewLine>        self.arg.update(arg)<NewLine>        print(self.arg)<NewLine><NewLine>    def _worker_init_fn(self, *args):<NewLine>        print('worker init')<NewLine>        print(self.arg)<NewLine><NewLine>    def get_dataloader(self):<NewLine>        return torch.utils.data.DataLoader(self, batch_size=None,<NewLine>                                         num_workers=3,<NewLine>                                         worker_init_fn=self._worker_init_fn,<NewLine>                                         pin_memory=True,<NewLine>                                         multiprocessing_context='spawn')<NewLine><NewLine>    def __getitem__(self, idx):<NewLine>        return 0<NewLine><NewLine>    def __len__(self):<NewLine>        return 5<NewLine><NewLine><NewLine>def main():<NewLine>    dataloader = Dataset({'batch_size': 2}).get_dataloader()<NewLine>    for _ in dataloader:<NewLine>        pass<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    main()<NewLine></code></pre><NewLine><p>Basically I want the workers to have  <code>{'batch_size': 2}</code>, but actually they have <code>{'batch_size': 1}</code>. The code print the following:</p><NewLine><pre><code class=""lang-auto"">__init__<NewLine>{'batch_size': 2}<NewLine>worker init<NewLine>{'batch_size': 1}<NewLine>worker init<NewLine>{'batch_size': 1}<NewLine>worker init<NewLine>{'batch_size': 1}<NewLine></code></pre><NewLine><p>How can I make the workers to have the correct <code>batch_size</code>?</p><NewLine></div>",https://discuss.pytorch.org/u/chaonan99,(Haonan Chen),chaonan99,"March 30, 2020,  5:19pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""74826"" data-username=""chaonan99""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/chaonan99/40/22167_2.png"" width=""20""/> chaonan99:</div><NewLine><blockquote><NewLine><p>self.arg.update(arg)</p><NewLine></blockquote><NewLine></aside><NewLine><p>Solved. If I add <code>self.arg = self.arg</code> after this line it works.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/chaonan99; <NewLine> ,"REPLY_DATE 1: March 30, 2020,  5:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
68109,How to freeze feature extractor and train only classifier in DistributedDataParallel?,2020-01-30T09:43:56.695Z,4,242,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to train only the last fc layer in my pretrained CNN model with distributed data parallel module.</p><NewLine><p>I tried to make the whole model to eval mode and then change the fc layer to train.</p><NewLine><pre><code class=""lang-auto"">model.module.eval()<NewLine>model.module.fc.train()<NewLine></code></pre><NewLine><p>and I got following error msg,</p><NewLine><pre><code class=""lang-auto"">-- Process 0 terminated with the following error:<NewLine>Traceback (most recent call last):<NewLine>  File ""/usr/local/lib/python3.5/dist-packages/torch/multiprocessing/spawn.py"", line 19, in _wrap<NewLine>    fn(i, *args)<NewLine>  File ""/app/train_action_model_apex.py"", line 466, in main_worker<NewLine>    train_model(args, root_dir)<NewLine>  File ""/app/train_action_model_apex.py"", line 235, in train_model<NewLine>    trainer.train_epoch(epoch, use_amp=True)<NewLine>  File ""/app/trainers/action_model_trainer.py"", line 202, in train_epoch<NewLine>    self.optimize_model(loss_dict[self.update_loss_name], use_amp)<NewLine>  File ""/app/trainers/action_model_trainer.py"", line 68, in optimize_model<NewLine>    scaled_loss.backward()<NewLine>  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__<NewLine>    self.gen.throw(type, value, traceback)<NewLine>  File ""/usr/local/lib/python3.5/dist-packages/apex/amp/handle.py"", line 117, in scale_loss<NewLine>    yield (loss.float())*loss_scale<NewLine>  File ""/app/trainers/action_model_trainer.py"", line 68, in optimize_model<NewLine>    scaled_loss.backward()<NewLine>  File ""/usr/local/lib/python3.5/dist-packages/torch/tensor.py"", line 107, in backward<NewLine>    torch.autograd.backward(self, gradient, retain_graph, create_graph)<NewLine>  File ""/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py"", line 93, in backward<NewLine>    allow_unreachable=True)  # allow_unreachable flag<NewLine>RuntimeError: expected scalar type Half but found Float<NewLine></code></pre><NewLine><p>How can I properly fix the problem?</p><NewLine></div>",https://discuss.pytorch.org/u/kkjh0723,(Jinhyung Kim),kkjh0723,"January 30, 2020,  9:43am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems you are using some higher-level wrapper with <code>amp</code>?<br/><NewLine>Could you post a code snippet to reproduce this issue, please?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>, thanks for your reply.  I’m using <code>amp</code>.</p><NewLine><p>Here is a code snippet to reproduce the issue.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>from apex import amp<NewLine>import torch.distributed as dist<NewLine><NewLine><NewLine>class SomeModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(SomeModel, self).__init__()<NewLine>        self.conv = nn.Conv3d(<NewLine>            3,<NewLine>            16,<NewLine>            kernel_size=(1, 3, 3),<NewLine>            stride=1,<NewLine>            padding=(0, 1, 1),<NewLine>            bias=False)<NewLine>        self.bn1 = nn.BatchNorm3d(16)<NewLine>        self.relu = nn.ReLU(inplace=True)<NewLine>        self.avgpool = nn.AdaptiveAvgPool3d(1)<NewLine>        self.fc = nn.Linear(16, 3)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.conv(x)<NewLine>        x = self.bn1(x)<NewLine>        x = self.relu(x)<NewLine>        x = self.avgpool(x)<NewLine>        x = x.view(x.size(0), -1)<NewLine>        x = self.fc(x)<NewLine>        return x<NewLine><NewLine><NewLine><NewLine>print('init process group')<NewLine>dist.init_process_group(backend='nccl', init_method='tcp://127.0.0.1:7001',<NewLine>                            world_size=1, rank=0)<NewLine><NewLine>model = SomeModel().cuda()<NewLine>criterion = nn.CrossEntropyLoss().cuda()<NewLine>optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, )<NewLine>model, optimizer = amp.initialize(model, optimizer, opt_level='O2')<NewLine>print('ddp')<NewLine>model = torch.nn.parallel.DistributedDataParallel(model, find_unused_parameters=True)<NewLine><NewLine>print('model train')<NewLine># model.train() # works<NewLine><NewLine>model.eval()<NewLine>model.module.fc.train()<NewLine><NewLine>x = torch.randn((5, 3, 7, 7, 7), device='cuda')<NewLine>y = torch.ones((5, ), device='cuda').long()<NewLine><NewLine>print('model forward')<NewLine>outputs = model(x)<NewLine>print('calculate loss')<NewLine>loss = criterion(outputs, y)<NewLine>print('model backward')<NewLine>with amp.scale_loss(loss, optimizer) as scaled_loss:<NewLine>    scaled_loss.backward()<NewLine>print('optimizer step')<NewLine>optimizer.step()<NewLine></code></pre><NewLine><p>Also, while I make the code snippet, I found that BN cause the issue.<br/><NewLine>Without BN, no error raised, though I’m not sure it works properly as intended.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/kkjh0723"">@kkjh0723</a> I couldn’t get your original code to work since I kept running into this error</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Expected tensor for argument #2 'input' to have the same device as tensor for argument #3 'weight'; but device 1 does not equal 0 (while checking arguments for slow_conv_dilated_all_cuda_template)<NewLine></code></pre><NewLine><p>I added <code>device_ids=[0]</code> to the DistributedDataParallel constructor and the code seems to work fine now:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>from apex import amp<NewLine>import torch.distributed as dist<NewLine><NewLine><NewLine>class SomeModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(SomeModel, self).__init__()<NewLine>        self.conv = nn.Conv3d(<NewLine>            3,<NewLine>            16,<NewLine>            kernel_size=(1, 3, 3),<NewLine>            stride=1,<NewLine>            padding=(0, 1, 1),<NewLine>            bias=False)<NewLine>        self.bn1 = nn.BatchNorm3d(16)<NewLine>        self.relu = nn.ReLU(inplace=True)<NewLine>        self.avgpool = nn.AdaptiveAvgPool3d(1)<NewLine>        self.fc = nn.Linear(16, 3)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.conv(x)<NewLine>        x = self.bn1(x)<NewLine>        x = self.relu(x)<NewLine>        x = self.avgpool(x)<NewLine>        x = x.view(x.size(0), -1)<NewLine>        x = self.fc(x)<NewLine>        return x<NewLine><NewLine><NewLine><NewLine>print('init process group')<NewLine>dist.init_process_group(backend='nccl', init_method='tcp://127.0.0.1:7001',<NewLine>                            world_size=1, rank=0)<NewLine><NewLine>model = SomeModel().cuda()<NewLine>criterion = nn.CrossEntropyLoss().cuda()<NewLine>optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, )<NewLine>model, optimizer = amp.initialize(model, optimizer, opt_level='O2')<NewLine>print('ddp')<NewLine>model = torch.nn.parallel.DistributedDataParallel(model, find_unused_parameters=True, device_ids=[0])<NewLine><NewLine>print('model train')<NewLine># model.train() # works<NewLine><NewLine>model.eval()<NewLine>model.module.fc.train()<NewLine><NewLine>x = torch.randn((5, 3, 7, 7, 7), device='cuda')<NewLine>y = torch.ones((5, ), device='cuda').long()<NewLine><NewLine>print('model forward')<NewLine>outputs = model(x)<NewLine>print('calculate loss')<NewLine>loss = criterion(outputs, y)<NewLine>print('model backward')<NewLine>with amp.scale_loss(loss, optimizer) as scaled_loss:<NewLine>    scaled_loss.backward()<NewLine>print('optimizer step')<NewLine>optimizer.step()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pritamdamania87"">@pritamdamania87</a>, Thanks for answering.<br/><NewLine>I tried with only 1 visible GPU using <code>CUDA_VISIBLE_DEVICES=0</code> in my original code.<br/><NewLine>I also got the same error as you when multiple GPUs are visible.</p><NewLine><p>And I still got the following error when I add <code>device_ids=[0]</code><br/><NewLine><code>RuntimeError: expected scalar type Half but found Float</code></p><NewLine><p>I wonder if different version of pytorch might cause the problem?<br/><NewLine>I’m currently using 1.1.0.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I was using the latest PyTorch 1.4 release, will try to repro this with 1.1.0</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like I see the same issue with 1.1.0 and 1.2.0, although it seems to work 1.3 onwards. Could you try out a version &gt;= 1.3?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks! I found it works after updating to PyTorch 1.4.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kkjh0723; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/kkjh0723; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/kkjh0723; <NewLine> ,"REPLY_DATE 1: January 30, 2020,  6:39pm; <NewLine> REPLY_DATE 2: January 31, 2020,  1:53am; <NewLine> REPLY_DATE 3: February 5, 2020,  2:04am; <NewLine> REPLY_DATE 4: February 5, 2020,  5:11am; <NewLine> REPLY_DATE 5: February 5, 2020,  8:04pm; <NewLine> REPLY_DATE 6: March 30, 2020,  8:56am; <NewLine> REPLY_DATE 7: March 30, 2020,  8:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
74582,Module.cuda() not moving Module tensor?,2020-03-28T05:44:46.514Z,3,326,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">class ToyModule(torch.nn.Module):<NewLine>    def __init__(self) -&gt; None:<NewLine>        super(ToyModule, self).__init__()<NewLine>        self.layer = torch.nn.Linear(2, 2)<NewLine>        self.expected_moved_cuda_tensor = torch.tensor([0, 2, 3])<NewLine><NewLine>    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:<NewLine>        return self.layer(input)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">toy_module = ToyModule()<NewLine>toy_module.cuda()<NewLine></code></pre><NewLine><p>When we call  <code>.cuda()</code>  all the parameters and buffers of the module are moved to the GPU:</p><NewLine><pre><code class=""lang-auto"">next(toy_module.layer.parameters()).device<NewLine>&gt;&gt;&gt; device(type='cuda', index=0)<NewLine></code></pre><NewLine><p>But when we inspect the tensor attribute of toy_module, we see <code>device(type='cpu')</code>?</p><NewLine><pre><code class=""lang-auto"">toy_module.expected_moved_cuda_tensor.device<NewLine>&gt;&gt;&gt; device(type='cpu')<NewLine></code></pre><NewLine><p>Is this expected or am I missing anything? Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/danh,,danh,"March 29, 2020, 12:34am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""74582"" data-username=""danh""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/d/a9a28c/40.png"" width=""20""/> danh:</div><NewLine><blockquote><NewLine><p>When we call <code>.cuda()</code> all the parameters and buffers of the module are moved to the GPU:</p><NewLine></blockquote><NewLine></aside><NewLine><p><code>self.expected_moved_cuda_tensor</code> is neither a parameter nor a buffer, that’s why it’s device is unchanged. If you want to create a parameter and use it then you can do it as follows-</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine>class Model(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Model, self).__init__()<NewLine>        self.linear1 = nn.Linear(2, 1)<NewLine>        self.linear1.weight = torch.nn.Parameter(torch.ones(2, 1))<NewLine>        self.linear1.bias = torch.nn.Parameter(torch.zeros(1))<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.linear1(x)<NewLine>        return x<NewLine></code></pre><NewLine><p>You can even use those parameters in forward method like-</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine>class Model(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Model, self).__init__()<NewLine>        self.weight = torch.nn.Parameter(torch.ones(2, 1))<NewLine>        self.bias = torch.nn.Parameter(torch.zeros(1))<NewLine><NewLine>    def forward(self, x):<NewLine>        # linear regression completely from scratch,<NewLine>        # using parameters created in __init__<NewLine>        x = torch.mm(x, self.weight) + self.bias<NewLine>        return x<NewLine></code></pre><NewLine><p>And moving above model to <code>.cuda()</code> does move model parameters to cuda-</p><NewLine><pre><code class=""lang-python"">model = Model()<NewLine>model.cuda()<NewLine>print(model.weight.device) # prints device(type='cuda', index=0)<NewLine>print(model.bias.device) # prints device(type='cuda', index=0)</code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot!</p><NewLine><p>But isn’t it defeat the intuition of  <code>.cuda()</code>  if the Module tensor device stays unchanged?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Though <code>.cuda()</code> “should” do as you said, but I don’t think changing devices for all the <code>torch.tensor </code> attributes of a class inherited from <code>nn.Module</code> by default is good idea. In your use case it might be helpful, but in some case user may don’t that, so I think that’s the reason why it ain’t do that by default.</p><NewLine><p>One more thing if you want to create just a constant tensor (not a parameter) then you can do that as</p><NewLine><pre><code class=""lang-python"">self.a_constant_tensor = nn.Parameter(torch.ones(2, 1), requires_grad = False)<NewLine></code></pre><NewLine><p>and then use it in forward method.</p><NewLine><p><strong>OR</strong> you can use buffers, <strong>""which is recommended""</strong></p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine>class Model(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Model, self).__init__()<NewLine>        self.weight = torch.nn.Parameter(torch.zeros(2, 1))<NewLine>        self.bias = torch.nn.Parameter(torch.zeros(1))<NewLine>        self.register_buffer('a_constant_tensor', torch.tensor([0.5]))<NewLine><NewLine>    def forward(self, x):<NewLine>        # linear regression completely from scratch,<NewLine>        # using parameters created in __init__<NewLine>        x = torch.mm(x, self.weight) + self.bias + self.a_constant_tensor<NewLine>        return x<NewLine><NewLine><NewLine>model = Model().cuda()<NewLine></code></pre><NewLine><p>Doing this wouldn’t consider <code>self.a_constant_tensor</code> as a parameter, so printing parameters wouldn’t return <code>self.a_constant_tensor</code> -</p><NewLine><pre><code class=""lang-python"">for param in model.parameters():<NewLine>    print(param)<NewLine># Only prints about self.weight and self.bias<NewLine>'''<NewLine>Parameter containing:<NewLine>tensor([[0.],<NewLine>        [0.]], device='cuda:0', requires_grad=True)<NewLine>Parameter containing:<NewLine>tensor([0.], device='cuda:0', requires_grad=True)<NewLine>'''<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/braindotai; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/danh; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/braindotai; <NewLine> ,"REPLY_DATE 1: March 28, 2020,  6:28am; <NewLine> REPLY_DATE 2: March 28, 2020,  6:31am; <NewLine> REPLY_DATE 3: March 28, 2020,  6:50am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
74564,Proper DistributedDataParallel Usage,2020-03-27T20:28:30.203Z,0,194,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a couple of questions with regard to the proper usage of <code>DistributedDataParallel</code> that doesn’t seem to be covered anywhere.</p><NewLine><p>The questions are below inline with the code.</p><NewLine><pre><code class=""lang-python"">def train(device, num_epochs=10):<NewLine>    model = ToyModel().to(device)<NewLine>    # QUESTION: Suppose each process has a different random generator state, when<NewLine>    # `DistributedDataParallel` is initialized does each process need to have the same parameter<NewLine>    # values?<NewLine>    ddp_model = nn.DistributedDataParallel(model, device_ids=[device], output_device=device)<NewLine>    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)<NewLine>    loss_fn = nn.MSELoss()<NewLine><NewLine>    for i in range(num_epochs):<NewLine>        # Training<NewLine>        model.mode(train=True)<NewLine>        optimizer.zero_grad()<NewLine>        outputs = ddp_model(torch.randn(20, 10))<NewLine>        labels = torch.randn(20, 5).to(device)<NewLine>        loss_fn(outputs, labels).backward()<NewLine>        optimizer.step()<NewLine><NewLine>        # Evaluate on master<NewLine>        if torch.distributed.get_rank() == 0:<NewLine>            model.mode(train=False)<NewLine>            # QUESTION: In order to evaluate, on one GPU, can we use `ddp_model.module`?<NewLine>            # QUESTION: Can we use something like `EMA` to copy new parameters to `ddp_model.module`<NewLine>            # and then restore them after evaluation? Learn more:<NewLine>            # http://www.programmersought.com/article/28492072406/<NewLine>            outputs = ddp_model.module(torch.randn(20, 10))<NewLine>            labels = torch.randn(20, 5).to(device)<NewLine>            print(loss_fn(outputs, labels))<NewLine><NewLine>        # Save checkpoint on master<NewLine>        if torch.distributed.get_rank() == 0:<NewLine>            # QUESTION: In order to save the model, can we use `ddp_model.module`?<NewLine>            torch.save(ddp_model.module, 'checkpoint.pt')<NewLine><NewLine>        # QUESTION: Do we need to use `torch.distributed.barrier` so that the other processes<NewLine>        # don't continue training while the master evaluates?<NewLine></code></pre><NewLine><p>Thank you for the helpful tutorial <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html</a>. I reused it’s example code for this question.</p><NewLine></div>",https://discuss.pytorch.org/u/PetrochukM,(Michael Petrochuk),PetrochukM,"March 27, 2020,  8:52pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>QUESTION: Suppose each process has a different random generator state, when <code>DistributedDataParallel</code> is initialized does each process need to have the same parameter values?</p><NewLine></blockquote><NewLine><p>No. Rank 0 will broadcast model states to all other ranks when you construct DDP. Code for that is <a href=""https://github.com/pytorch/pytorch/blob/df8d6eeb19423848b20cd727bc4a728337b73829/torch/nn/parallel/distributed.py#L298-L303"" rel=""nofollow noopener"">here</a>.</p><NewLine><blockquote><NewLine><p>In order to evaluate, on one GPU, can we use <code>ddp_model.module</code>?</p><NewLine></blockquote><NewLine><p>Yes, this should work.</p><NewLine><blockquote><NewLine><p>Can we use something like <code>EMA</code> to copy new parameters to <code>ddp_model.module</code> and then restore them after evaluation?</p><NewLine></blockquote><NewLine><p>Yes, if you make sure you restored those model param values correctly. Otherwise, if this introduces inconsistency across param values across different processes, DDP will not fix that for you, as DDP only syncs grad instead of params. <a href=""https://pytorch.org/docs/master/notes/ddp.html"" rel=""nofollow noopener"">This</a> might be helpful to explain.</p><NewLine><blockquote><NewLine><p>In order to save the model, can we use <code>ddp_model.module</code></p><NewLine></blockquote><NewLine><p>Yes. And when you restore from the checkpoint, it’s better to reconstruct the DDP instance using the restored module to make sure that DDP starts from a clean state.</p><NewLine><blockquote><NewLine><p>Do we need to use <code>torch.distributed.barrier</code> so that the other processes don’t continue training while the master evaluates?</p><NewLine></blockquote><NewLine><p>It’s recommended this way. But if you are not consuming the checkpoint right away and not worried about timeout due to rank0 is doing more work, this is not necessary. Because the next DDP backward will launch allreduce comm ops, which will sync anyway. Some of this is also explained <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">here</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 27, 2020, 10:55pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> 
74560,Question about torch.distributed p2p communication,2020-03-27T19:30:05.931Z,0,66,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I have a question about the p2p communication in torch.distributed. Suppose we set up a group with 3 processes using command <em>init_process_group(backend=‘gloo’, init_method=“tcp://10.0.0.1:8888”, rank=args.rank, world_size=3)</em> on three different nodes with IP 10.0.0.1 to 10.0.0.3. When we are sending tensors from 10.0.0.2 to 10.0.0.3, how is the underlying network traffic routed? Is it directly from 10.0.0.2 to 10.0.0.3 or from 10.0.0.2 to 10.0.0.1 and then to 10.0.0.3? Probably the answer is obvious but I couldn’t find it based on the doc’s description. Thanks in advance!</p><NewLine><p>Yijing</p><NewLine></div>",https://discuss.pytorch.org/u/yijing,(Yijing),yijing,"March 27, 2020,  7:30pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/yijing"">@yijing</a></p><NewLine><p>The message will directly send from  10.0.0.2 to 10.0.0.3.</p><NewLine><p>In <code>init_process_group</code>, the <code>init_method=“tcp://10.0.0.1:8888”</code> is only for rendezvous, i.e., all process will use the same ip:port to find each other. After that communications don’t need to go through master.</p><NewLine><p>BTW, if you are using p2p comm, <a href=""https://pytorch.org/docs/master/rpc.html"" rel=""nofollow noopener"">torchrpc</a> might be useful too. <a href=""https://pytorch.org/tutorials/intermediate/rpc_tutorial.html"" rel=""nofollow noopener"">Here</a> is a tutoral.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 27, 2020,  9:16pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
74322,distrubutedDataParallel and dataParallel hangs in specified model,2020-03-25T09:14:37.993Z,2,327,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was trying to train my NLP model in multGPU with 2 <code>K80s</code>, each <code>K80</code> has 2 cores, my model works fine in CPU or single GPU with <code>DataParallel</code> or <code>distributedDataParallel</code>, but when I use 2 or more cores, embarrassing things happened, it hangs always,this is the symptom</p><NewLine><blockquote><NewLine><p>DataParallel</p><NewLine></blockquote><NewLine><pre><code class=""lang-python"">clothModel = myModel.cuda()<NewLine>clothModel = nn.DataParallel(clothModel) # &lt;-- it works fine<NewLine>······<NewLine>out, loss = clothModel(input) #  &lt;-- program always hang in this line, even I can't use ctrl+C to shut it down and I get this infomation by using VSCode debug <NewLine></code></pre><NewLine><p>when I turn to <code>nvidia-smi</code>, I found this</p><NewLine><pre><code class=""lang-auto"">+-----------------------------------------------------------------------------+<NewLine>| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |<NewLine>|-------------------------------+----------------------+----------------------+<NewLine>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<NewLine>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<NewLine>|===============================+======================+======================|<NewLine>|   0  Tesla K80           On   | 00000000:08:00.0 Off |                  Off |<NewLine>| N/A   45C    P0    70W / 149W |   2113MiB / 12206MiB |    100%   E. Process |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   1  Tesla K80           On   | 00000000:09:00.0 Off |                  Off |<NewLine>| N/A   35C    P0    70W / 149W |    322MiB / 12206MiB |      0%   E. Process |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   2  Tesla K80           On   | 00000000:86:00.0 Off |                  Off |<NewLine>| N/A   39C    P0    57W / 175W |    311MiB / 12206MiB |      0%   E. Process |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   3  Tesla K80           On   | 00000000:87:00.0 Off |                  Off |<NewLine>| N/A   31C    P0    71W / 175W |    320MiB / 12206MiB |      0%   E. Process |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine></code></pre><NewLine><p>after a night, it remains this still</p><NewLine><blockquote><NewLine><p>distributedDataParallel</p><NewLine></blockquote><NewLine><p>after failed to use DataParallel, I turned to distributedDataParallel which is recommanded, and it hangs in <code>clothModel = nn.parallel.DistributedDataParallel(clothModel)</code><br/><NewLine>I turn to <code>nvidia-smi</code>, it seems almost the same as when I use <code>nn.DataParallel</code><br/><NewLine>and this time I can use <code>ctrl + C</code> while the processes are still remain in cuda so that the only way to exit is <code>kill -9 PID</code><br/><NewLine>when I use <code>ctrl+c</code>, it displayed this</p><NewLine><pre><code class=""lang-auto"">  File ""/home/damaoooo/.conda/envs/test/lib/python3.6/runpy.py"", line 193, in _run_module_as_main<NewLine>    ""__main__"", mod_spec)<NewLine>  File ""/home/damaoooo/.conda/envs/test/lib/python3.6/runpy.py"", line 85, in _run_code<NewLine>    exec(code, run_globals)<NewLine>  File ""/home/damaoooo/.conda/envs/test/lib/python3.6/site-packages/torch/distributed/launch.py"", line 263, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""/home/damaoooo/.conda/envs/test/lib/python3.6/site-packages/torch/distributed/launch.py"", line 256, in main<NewLine>    process.wait()<NewLine>  File ""/home/damaoooo/.conda/envs/test/lib/python3.6/subprocess.py"", line 1477, in wait<NewLine>    (pid, sts) = self._try_wait(0)<NewLine>  File ""/home/damaoooo/.conda/envs/test/lib/python3.6/subprocess.py"", line 1424, in _try_wait<NewLine>    (pid, sts) = os.waitpid(self.pid, wait_flags)<NewLine><NewLine></code></pre><NewLine><p>and more interesting, I made a simply CNN for MNIST and turn to DataParallel or distributedDataParallel, it works perfect… I wonder is there something wrong with my <code>clothModel</code>？if there is, why I turn to single GPU, it works fine?<br/><NewLine>and how can I solve the confusing hang?</p><NewLine></div>",https://discuss.pytorch.org/u/damaoooo,(damaoooo),damaoooo,"March 25, 2020,  9:15am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""74322"" data-username=""damaoooo""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/damaoooo/40/22003_2.png"" width=""20""/> damaoooo:</div><NewLine><blockquote><NewLine><p>and it hangs in <code>clothModel = nn.parallel.DistributedDataParallel(clothModel)</code></p><NewLine></blockquote><NewLine></aside><NewLine><p>You mean DDP hangs at constructor? Can you attach the process to gdb and check the trace to see line is causing the hang?</p><NewLine><p>Have you set <code>CUDA_VISIBLE_DEVICES</code> or pass in <code>device_ids</code> arg properly for DDP? Each DDP process should exclusively work on one GPU.</p><NewLine><blockquote><NewLine><p>I wonder is there something wrong with my  <code>clothModel</code>?</p><NewLine></blockquote><NewLine><p>Given the trace, I assume you are using the launch script. With that, DDP should be constructed in the following way:</p><NewLine><pre><code class=""lang-python"">clothModel = DistributedDataParallel(clothModel, device_ids=[arg.local_rank], output_device=arg.local_rank)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks a lot! that’s the key to the question, after tried that, I successed to run, but How about the DataParallel hang in the first question?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not sure why DataParallel stuck. The source code is <a href=""https://github.com/pytorch/pytorch/blob/df8d6eeb19423848b20cd727bc4a728337b73829/torch/nn/parallel/data_parallel.py#L141-L156"" rel=""nofollow noopener"">here</a>. Can you attach the process/threads to GDB and backtrace the stack?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/damaoooo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 25, 2020,  2:22pm; <NewLine> REPLY_DATE 2: March 27, 2020,  3:45am; <NewLine> REPLY_DATE 3: March 27, 2020,  2:20pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
71444,Distributed GPU calculations and CUDA extensions,2020-02-28T14:13:15.337Z,4,409,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone,</p><NewLine><p>I’m building an app that makes calculations using CUDA (it makes some optimization based on <a href=""https://en.wikipedia.org/wiki/Simulated_annealing"" rel=""nofollow noopener""> Simulated annealing</a>). I successfully followed <a href=""https://pytorch.org/tutorials/advanced/cpp_extension.html#"" rel=""nofollow noopener"">Custom C++ and CUDA Extensions</a> tutorial and made stable version on simple GPU, so now I would like to use multiple GPUs (some tasks has huge amount of data, that could not be allocated a single GPU + I’d like to speed up my calculations).</p><NewLine><p>I have several tensors that I would like to split by dim=0 and make distributed calculations (all calculations based on map pattern, so all records by dim=0 are undependable). So best choice for me would be create my custom nn.Module class with forward method and use <code>DistributedDataParallel</code> module, but I have not any parameter that requires a gradient and module crushes. (Yeah it rises AssertionError: DistributedDataParallel is not needed when a module doesn’t have any parameter that requires a gradient.)</p><NewLine><p>Could you please recommend something how to solve this problem or some other modules/ways to have distributed calculations.</p><NewLine><p>Best regards, Demetry</p><NewLine></div>",https://discuss.pytorch.org/u/Demetry,(Demetry),Demetry,"February 28, 2020,  2:18pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Would splitting the data and sending each chunk to a specific device work?<br/><NewLine>Something like this could already solve your use case:</p><NewLine><pre><code class=""lang-python"">data = torch.randn(4, 100)<NewLine>chunks = data.chunk(4, 0)<NewLine><NewLine>res = []<NewLine>for idx, chunk in enumerate(chunks):<NewLine>    res.append(my_fun(chunk.to('cuda:{}'.format(idx))).to('cuda:0'))<NewLine>res = torch.stack(res)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you, ptrblck,</p><NewLine><p>As I understand your way will calculate consequentially?<br/><NewLine>I would like to calculate in parallel: my app calculates about million iterations and each one based on the previous, so should I use threading/multiprocessing/concurrent.futures or there is some better solutions?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>CUDA operations are asynchronous, so each device should operate on its own.<br/><NewLine>You could check the GPU utilization during the script execution, which should show that all devices are being used.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you, it really helped)</p><NewLine><p>So I’ve done something like that:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from concurrent.futures import ThreadPoolExecutor<NewLine><NewLine>import MyCustomCudaModule as my_module<NewLine><NewLine>class MyClass:<NewLine>    def __init__(self, data):<NewLine>        self.gpus = [0, 1]  #  set devices I'd like to use<NewLine><NewLine>        # Split some data to chunks and allocate on its own GPU<NewLine>        self.tensor0 = torch.tensor(data[0], dtype=torch.float64).chunk(len(self.gpus))<NewLine>        self.tensor0 = [self.tensor0[idx].to(f'cuda:{gpu}') for idx, gpu in enumerate(self.gpus)]<NewLine><NewLine>        self.tensor1 = torch.tensor(data[1], dtype=torch.float64).chunk(len(self.gpus))<NewLine>        self.tensor1 = [self.tensor1[idx].to(f'cuda:{gpu}') for idx, gpu in enumerate(self.gpus)]<NewLine><NewLine>    def calculate(self):<NewLine>        # Prepare input data to use my CUDA method<NewLine>        chunks = list()<NewLine>        for idx in range(len(self.gpus)):<NewLine>            chunk = [self.tensor0[idx], self.tensor1[idx]]<NewLine>            chunks.append(chunk)<NewLine><NewLine>        # Start my calculations asynced <NewLine>        futures = self.executor.map(lambda ch: my_module.calculate(*ch), chunks)<NewLine><NewLine>        total_result = 0.0<NewLine>        for result in futures:<NewLine>            total_result += result.item()  # return calculations result from GPU to CPU<NewLine><NewLine>        return result<NewLine></code></pre><NewLine><p>It splits my data between GPUs and correctly calculates but I have no speedup (the speed is the same as I use 1 GPU).<br/><NewLine>What should I do to calculate faster?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>How large is the data tensor? If it is not large enough, the GIL contention across threads and the extra overhead of setting this up could overshadow the speed up brought by using multiple GPUs. Another thing is how did you measure the delay? As the computation is done on CUDA, you might need to use CUDA events and <a href=""https://pytorch.org/docs/stable/cuda.html#torch.cuda.Event.elapsed_time"" rel=""nofollow noopener""><code>elapsed_time</code></a> to get the accurate measure.</p><NewLine><p>If <code>elapsed_time</code> still shows no improvement, can you try:</p><NewLine><ol><NewLine><li>increase chunk size.</li><NewLine><li>use multiple processes</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>In average tensors are about 2000 * 1000 * 100 elements, sometimes they are could be about 15000 * 8000 * 100 elements. I split on chunks by dim=0.</p><NewLine><p>Now I have a guess that they are calculating consequentially instead of in parallel (I thought that ThreadPoolExecutor.map will start first GPU thread, return to the main CPU thread, start second one GPU thread etc, then will await for results from any device that finished. But as I can see it waits until first GPU will finish and then start calculations on second one)</p><NewLine><p>So what is the best practice to start my calculations asynchronous? (I could not use asyncio)</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Multi-thread should work, and this is how <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/data_parallel.py"" rel=""nofollow noopener""><code>DataParallel</code></a> is implemented (search for <code>parallel_apply</code>). But if <code>my_module.calculate</code> is composed of many CPU ops, you might see sequential execution due to GIL.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Demetry; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Demetry; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Demetry; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 25, 2020,  8:36am; <NewLine> REPLY_DATE 2: March 2, 2020,  9:42am; <NewLine> REPLY_DATE 3: March 2, 2020,  3:04pm; <NewLine> REPLY_DATE 4: March 25, 2020,  9:00am; <NewLine> REPLY_DATE 5: March 25, 2020,  2:11pm; <NewLine> REPLY_DATE 6: March 25, 2020,  4:40pm; <NewLine> REPLY_DATE 7: March 25, 2020,  5:44pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
74150,Efficient implementation of Shuffle BN in MoCo?,2020-03-23T16:02:45.661Z,2,201,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Shuffle BN is an important trick proposed by MoCo (<a href=""https://arxiv.org/abs/1911.05722"" rel=""nofollow noopener"">Momentum Contrast for Unsupervised Visual Representation Learning</a>):</p><NewLine><blockquote><NewLine><p>We resolve this problem by shufﬂing BN. We train with multiple GPUs and perform BN on the samples independently for each GPU (as done in common practice). For the key encoder f k , we shufﬂe the sample order in the current mini-batch before distributing it among GPUs (and shufﬂe back after encoding); the sample order of the mini-batch for the query encoder f q is not altered. This ensures the batch statistics used to compute a query and its positive key come from two different subsets. This effectively tackles the cheating issue and allows training to beneﬁt from BN.</p><NewLine></blockquote><NewLine><p>Since the official code is not yet released, I tried to implement Shuffle BN as below (where the size of local tensor <code>data</code> is [32, 3, 224, 224]):</p><NewLine><pre><code class=""lang-auto"">def forward(self, data):<NewLine>    N = data.size(0)<NewLine>    if self.training and self.shuffle_bn:<NewLine>        global_data = distributed_concat_no_grad(data, 0)<NewLine>        shuffle_index = torch.randperm(global_data.size(0), device=data.device)<NewLine>        broadcast(shuffle_index, 0)<NewLine>        recover_index = shuffle_index.argsort()<NewLine>        beg = N * self.rank<NewLine>        end = beg + N<NewLine>        data = global_data[shuffle_index[beg: end]]<NewLine>    feature = self.some_feature_extracting_network(data)<NewLine>    feature = feature.view(N, -1)<NewLine>    if self.training and self.shuffle_bn:<NewLine>        global_feature = distributed_concat_with_grad(feature)<NewLine>        feature = global_feature[recover_index[beg: end]]<NewLine>    return feature<NewLine></code></pre><NewLine><p>However, the first call of allgather communication makes the training much slower (0.54s/iter -&gt; 0.84s/iter).</p><NewLine></div>",https://discuss.pytorch.org/u/WarBean,(WarBean),WarBean,"March 23, 2020,  4:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/warbean"">@WarBean</a></p><NewLine><ol><NewLine><li>Where is the allgather call? Do you mean the broadcast?</li><NewLine><li>Is this question about how to improve the efficiency?</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply.</p><NewLine><p>1.<code>distributed_concat_no_grad</code> allgather the data tensors on each GPUs.</p><NewLine><p>2.Yes.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like, if you can know the value of <code>global_data.size(0)</code> without communication, you then only need the real data from <code>global_data</code> at the end of the if statement. In this case, you can try launch an async allgather and only wait for it right before the shuffle, so that the comm can overlap with other steps in between.</p><NewLine><p>Another questions is why do you need to do the shuffle this way? Can you pre-shuffle the input data for multiple batches and then run multiple iterations without communication? If this is possible, you can both 1) consolidate smaller comm into larger ones and 2) launch multiple async comm and wait for all in one shot to saturate the bandwidth. Besides, looks like the comm only applies to input data, if so, you can even align one iteration with a previous comm, e.g., always let iteration i consume comm result from iteration i - 2. In this way, the comm i-2 might have already finished before kicking off iteration i.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/WarBean; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 23, 2020,  5:26pm; <NewLine> REPLY_DATE 2: March 24, 2020,  3:33am; <NewLine> REPLY_DATE 3: March 24, 2020,  2:06pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
64376,Network parameter sync in forward pass,2019-12-18T00:09:56.594Z,2,197,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, all</p><NewLine><p>I’m trying to use the distributed package for multi-gpu training. Because of the way the code is written, the master process does all the initialisations (creating model replicas, optimisers etc.). From pytorch source code, it seems like during forward pass, all model replicas will be synced with the one running in subprocess with rank 0. Does that mean I could just initialise one optimiser for subprocess 0 and only update the parameters of the first model replica?</p><NewLine><p>Thanks,</p><NewLine></div>",https://discuss.pytorch.org/u/DzReal,(Frederic ZHANG),DzReal,"December 18, 2019, 12:09am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/dzreal"">@DzReal</a></p><NewLine><blockquote><NewLine><p>From pytorch source code, it seems like during forward pass, all model replicas will be synced with the one running in subprocess with rank 0.</p><NewLine></blockquote><NewLine><p>If you are using DistributedDataParallel, above is actually not true. The distributed sync occurs during the backward pass, and it averages all gradients instead of parameters (check <code>torch/csrc/distributed/c10d/reducer.cpp</code>). So that when the optimizer consumes those gradients, they are already global gradients.</p><NewLine><p>The sync you mentioned in the forward pass might be <a href=""https://github.com/pytorch/pytorch/blob/46539eee0363e25ce5eb408c85cefd808cd6f878/torch/nn/parallel/distributed.py#L442"" rel=""nofollow noopener"">this</a>. This only does intra-node sync when you use one DDP process to work on multiple GPUs.</p><NewLine><blockquote><NewLine><p>Does that mean I could just initialise one optimiser for subprocess 0 and only update the parameters of the first model replica?</p><NewLine></blockquote><NewLine><p>No. Each DDP process should have its own local optimizer.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>So when running on a single node with multiple GPUs, could I only use one optimiser?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>So when running on a single node with multiple GPUs, could I only use one optimiser?</p><NewLine></blockquote><NewLine><p>You will need one optimizer per DDP process, regardless of where those DDP processes are. I hope the following note could help explain it:  <a href=""https://pytorch.org/docs/master/notes/ddp.html"" rel=""nofollow noopener"">https://pytorch.org/docs/master/notes/ddp.html</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/DzReal; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: December 26, 2019,  4:14pm; <NewLine> REPLY_DATE 2: January 11, 2020,  1:11am; <NewLine> REPLY_DATE 3: March 24, 2020,  2:28am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
74139,Parallelization of multiple cost functions,2020-03-23T14:20:57.323Z,0,38,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><p>I have a RNN and a set of time series data organized in n sequences as (y_1, u_1), (y_2, u_2), …, (y_n, u_n), where y_i is vector of data outputs and u_i is matrix where each column is a signal and each row a time sample of all signals.</p><NewLine><p>The cost function to be minimized is in the form of a sum</p><NewLine><p>min f_1(y_1, u_1) +  f_2(y_2, u_2) + f_3(y_3, u_3) + … + f_n(y_n, u_n)</p><NewLine><p>where each cost function f_i is different for each sequence i.</p><NewLine><p>I was wondering if someone has any experience, or can help me find where to start look how to make each term f_i(y_i, u_i) in the cost-function to be evaluated in parallel using multiple cpus/gpus?</p><NewLine></div>",https://discuss.pytorch.org/u/daner,,daner,"March 23, 2020,  2:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/daner"">@daner</a> if you are using a single machine with multiple GPUs, you can try scatter + parallel_apply + gather. The implementation of <code>DataParallel</code> can serve as an example. [<a href=""https://github.com/pytorch/pytorch/blob/df8d6eeb19423848b20cd727bc4a728337b73829/torch/nn/parallel/data_parallel.py#L151-L156"" rel=""nofollow noopener"">link</a>]</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 23, 2020,  2:39pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
73977,Parallelising models with variable size inputs across multiple GPUs,2020-03-21T19:33:18.796Z,0,67,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have written a module for small graphs that has a custom collate_batch function so that I have batches of 128 graphs at a time however the graphs are different sizes and so we have a variable number of nodes. I am using torch_scatter for some operations.</p><NewLine><p>My aim is to parallelise my code over multiple GPUs to reduce the real time for training. Naively using <code>nn.DataParallel(model)</code> gives me an error as I believe it requires the sub-tensors it splits up the tensor into to all be the same size which is not the case here as while each GPU gets 64 graphs the number of nodes varies. Is there a way to do this currently across multiple GPUs or do I need to just take the hit and train on one GPU?</p><NewLine></div>",https://discuss.pytorch.org/u/CompRhys,(Rhys),CompRhys,"March 21, 2020,  7:33pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""73977"" data-username=""CompRhys""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/comprhys/40/9863_2.png"" width=""20""/> CompRhys:</div><NewLine><blockquote><NewLine><p>Naively using <code>nn.DataParallel(model)</code> gives me an error as I believe it requires the sub-tensors it splits up the tensor into to all be the same size which is not the case here as while each GPU gets 64 graphs the number of nodes varies.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I am inclined to think this shouldn’t cause a problem for <code>DataParallel</code>. Because <code>DataParallel</code> would replicate your model and scatter your input on the first (batch) dimension. Even though different input element (graph) can contain different node, as long as your model forward function can handle it properly, it should not hit error.</p><NewLine><p>Could you please share a min repro for the error you saw?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 23, 2020,  2:18am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
72177,"Distributed Parallel, one machine multi gpu multi process?",2020-03-05T18:31:37.812Z,3,984,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,</p><NewLine><p>I am trying to train a model with one machine, but with multi gpus. Until now, I was using the nn.DataParallel which works well, but it seems a bit slow to me so I would like to use the DistributedDataParallel instead.</p><NewLine><p>However, I am not sure to understand clearly how to use this function (I have some weird results, the training takes 10x much more time than DataParallel).<br/><NewLine>In fact, I am not sure which gpus have to load the model/batch and compute the loss function ?<br/><NewLine>Moreover with the code below, my training is slower and I saw on nvidia-smi  a weird behavior. Instead of having ONE process on each gpu, I have two process for each gpu (I have two gpus, but I have 4 process) .</p><NewLine><p>My second issue is if I increase the number of workers in the dataloader, I have a dataloader pid killed error .<br/><NewLine>Am I doing something wrong ?</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.distributed as dist<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine>import torch.multiprocessing as mp<NewLine>from apex.parallel import DistributedDataParallel as DDP_apex<NewLine>from torch.nn.parallel import DistributedDataParallel as DDP<NewLine><NewLine>def run(gpu, args):<NewLine>	rank = gpu                   <NewLine>        dist.init_process_group(                                   <NewLine>    	        backend='nccl',                                         <NewLine>   		init_method='tcp://localhost:1088', #'env://',                                   <NewLine>    	        world_size=args.world_size,                              <NewLine>    	       rank=rank                                               <NewLine>              )      <NewLine>	trainset = ...<NewLine>	testset = ...<NewLine>	<NewLine>	################################################################<NewLine>        train_sampler = torch.utils.data.distributed.DistributedSampler(<NewLine>    	trainset,<NewLine>    	num_replicas=args.world_size,<NewLine>    	rank=rank<NewLine>    )<NewLine><NewLine>    ################################################################<NewLine>        test_sampler = torch.utils.data.distributed.DistributedSampler(<NewLine>    	testset,<NewLine>    	num_replicas=args.world_size,<NewLine>    	rank=rank<NewLine>    )<NewLine><NewLine>	trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True,                        num_workers=args.workers, pin_memory=False, drop_last= True )<NewLine>        testloader = torch.utils.data.DataLoader(testset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=False)<NewLine>	optim_params = list(filter(lambda p: p.requires_grad, net.parameters()))<NewLine>	optimizer = optim.Adam(optim_params, lr=args.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=args.weight_decay, amsgrad=True)<NewLine>	net = 2D_CNN()<NewLine>	net = net.to(args.gpus[0])<NewLine>	<NewLine>	net = DDP(net, device_ids=args.gpus)<NewLine>	<NewLine>	train(net, optimizer,  trainloader, testloader, args, gpu) # function which iterate accross the dataloader and do the forward/backward/step<NewLine><NewLine>if __name__ ==""__main__"":<NewLine>    args.nodes = 1 # one single machine<NewLine>    args.gpus = [0,1,2]<NewLine>    #########################################################<NewLine>    args.world_size = len(args.gpus) * args.nodes                #<NewLine>    os.environ['MASTER_ADDR'] = 'localhost'              #<NewLine>    os.environ['MASTER_PORT'] = '8888'                      #<NewLine>    print(args.gpus)<NewLine>    print(os.environ['MASTER_PORT'] )<NewLine>    mp.spawn(run, nprocs=len(args.gpus), args=(args,))         #<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Shiro,(Shiro),Shiro,"March 5, 2020,  6:33pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Related to my question, read <a href=""https://discuss.pytorch.org/t/distributed-data-parallel-slower-than-data-parallel/72052/4"">here</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>One requirement for DistributedDataParallel is that, you need to set <a href=""https://github.com/pytorch/pytorch/blob/v1.4.0/torch/nn/parallel/distributed.py#L171"" rel=""nofollow noopener""><code>device_ids</code></a> properly or use <code>CUDA_VISIBLE_DEVICES</code> env var to configure them properly to make sure that one process only works on one GPU. Otherwise, by default, each process will try to use all visible GPUs.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your replies.</p><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>  How can I configure the devices_ids if I want to have one process = one gpu ?<br/><NewLine>In my case, I have tried to change device_ids by the gpu number (which is equal to the rank value in my case) such as<code> device_ids = [gpu]</code> and call transfer the model in the first gpu <code>model.to(args.gpus[0])</code> .<br/><NewLine>But unfortunately, I got that error : <code>RuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 1 does not equal 0 (while checking arguments for cudnn_convolution)</code></p><NewLine><p>P.S : it seems to “work” now, I forgot to cast the loss function on the good gpu. But, when I increase the number of workers for the dataloader (&gt;0) it creates the error : <code>RuntimeError: DataLoader worker (pid(s) 16451) exited unexpectedly</code> . (and the training took above 24 hours). I will try with the apex distributed and then again compare with DataParallel.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please report back if you find that distributed is faster than data parallel (i.e. time it and lets us know). My intuition says that the claim is false but I haven’t seen anywhere on the docs a proper MWE on how to properly use distributed for each individual case in combination with <code>launch.py</code>.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/vincentqb"">@vincentqb</a> on RuntimeError from DataLoader</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://pytorch.org/docs/master/notes/ddp.html#example"" rel=""nofollow noopener"">This</a> is a minimum DDP example. Posting it here in case it is useful for future readers.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>That doesn’t cover <code>launch.py</code> examples.</p><NewLine><p>I would like to see examples using <code>launch.py</code> covering the following two separate use cases<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/042c3675a7e444492d1916d38c3e1fee479ff405"" href=""https://discuss.pytorch.org/uploads/default/original/3X/0/4/042c3675a7e444492d1916d38c3e1fee479ff405.png"" title=""image""><img alt=""image"" data-base62-sha1=""AUDnNSRjfyFUFGj0Uk6LgXLkUJ"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/4/042c3675a7e444492d1916d38c3e1fee479ff405_2_10x10.png"" height=""499"" src=""https://discuss.pytorch.org/uploads/default/original/3X/0/4/042c3675a7e444492d1916d38c3e1fee479ff405.png"" width=""479""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">583×608 24.1 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> Thanks for the link.</p><NewLine><p>I see that my training take exactly the same amount of time with 1 ou 2 gpus, so I was wondering, when we use Distributed process, do we have to divide the original number of iteration by the number of gpus used ?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/kirk86"">@kirk86</a>, could you please create an issue on GitHub for us to track this request? Thanks!</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/shiro"">@Shiro</a></p><NewLine><p>If each process runs the same amount of iterations with each iteration consuming the same amount of data, using 2GPUs might actually take longer, because there will be additional communication overhead between the two GPUs. But in this case, your model is actually trained using 2X number of batches.</p><NewLine><p>Reducing the number of iterations should work, or you can also reduce the batch size. One thing to note is that, this might also call for additional tuning on the learning rate or other configs. Some relevant discussions are available <a href=""https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769/8"">here</a>.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://github.com/pytorch/pytorch/issues/35160"" rel=""nofollow noopener"">Done</a>!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/kirk86; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Shiro; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/kirk86; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/kirk86; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Shiro; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/kirk86; <NewLine> ,"REPLY_DATE 1: March 5, 2020,  6:38pm; <NewLine> REPLY_DATE 2: March 6, 2020,  4:23pm; <NewLine> REPLY_DATE 3: March 10, 2020, 11:50am; <NewLine> REPLY_DATE 4: March 10, 2020, 12:15pm; <NewLine> REPLY_DATE 5: March 10, 2020,  3:10pm; <NewLine> REPLY_DATE 6: March 10, 2020,  3:11pm; <NewLine> REPLY_DATE 7: March 10, 2020,  5:11pm; <NewLine> REPLY_DATE 8: March 12, 2020,  1:28pm; <NewLine> REPLY_DATE 9: March 19, 2020,  5:47pm; <NewLine> REPLY_DATE 10: March 19, 2020,  5:52pm; <NewLine> REPLY_DATE 11: March 21, 2020,  5:21pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: 1 Like; <NewLine> 
73520,Adapt code with pytorch.distributed to only one gpu,2020-03-17T13:23:07.329Z,1,203,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I am running experiments on other’s people code. They used 16 gpus and the library torch.distributed. I just want to run the code with one gpu, I know it will be slow. Is there a simple way to adapt the code to one GPU without having to learn to use the library pytorch.distributed? at this moment my priority is to see if their code help us, if selected then I’ll focus on that library.</p><NewLine><ul><NewLine><li>Will the library pytorch.distributed automatically detect that I only have one GPU and work on it? No, because it is sending me errors.</li><NewLine></ul><NewLine><blockquote><NewLine><p>Use GPU: 0 for training<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “train.py”, line 97, in <br/><NewLine>main()<br/><NewLine>File “train.py”, line 29, in main<br/><NewLine>mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, idx_server, opt))<br/><NewLine>File “/home/ericd/anaconda/envs/myPytorch/lib/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 171, in spawn<br/><NewLine>while not spawn_context.join():<br/><NewLine>File “/home/ericd/anaconda/envs/myPytorch/lib/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 118, in join<br/><NewLine>raise Exception(msg)<br/><NewLine>Exception:</p><NewLine><p>– Process 0 terminated with the following error:<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “/home/ericd/anaconda/envs/myPytorch/lib/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 19, in _wrap<br/><NewLine>fn(i, *args)<br/><NewLine>File “/home/ericd/tests/CC-FPSE/train.py”, line 37, in main_worker<br/><NewLine>dist.init_process_group(backend=‘nccl’, init_method=opt.dist_url, world_size=world_size, rank=rank)<br/><NewLine>File “/home/ericd/anaconda/envs/myPytorch/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py”, line 397, in init_process_group<br/><NewLine>store, rank, world_size = next(rendezvous_iterator)<br/><NewLine>File “/home/ericd/anaconda/envs/myPytorch/lib/python3.6/site-packages/torch/distributed/rendezvous.py”, line 120, in _tcp_rendezvous_handler<br/><NewLine>store = TCPStore(result.hostname, result.port, world_size, start_daemon)<br/><NewLine>TypeError: <strong>init</strong>(): incompatible constructor arguments. The following argument types are supported:<br/><NewLine>1. torch.distributed.TCPStore(arg0: str, arg1: int, arg2: int, arg3: bool)</p><NewLine></blockquote><NewLine><p>line 37 is</p><NewLine><p><code>    torch.distributed.init_process_group(backend='nccl', init_method=opt.dist_url, world_size=world_size, rank=rank)</code></p><NewLine><ul><NewLine><li><NewLine><p>How can I adapt the code to my situation? Ideally there is some parameter that I can use and make things compatible.</p><NewLine></li><NewLine><li><NewLine><p>Do I need to find certain lines and modify them? I am afraid that may be the case but I don’t know where or which.</p><NewLine></li><NewLine></ul><NewLine><p>I am working with <a href=""https://github.com/xh-liu/CC-FPSE"" rel=""nofollow noopener"">https://github.com/xh-liu/CC-FPSE</a> but I am not sure if this helps.</p><NewLine></div>",https://discuss.pytorch.org/u/Rubiel1,(Eric Rubiel),Rubiel1,"March 17, 2020,  1:29pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It depends on how the code was written. If the model forward function has sth like:</p><NewLine><pre><code class=""lang-python"">def forward(input):<NewLine>    x1 = self.layer1(input.to(""cuda:0""))<NewLine>    x2 = self.layer2(input.to(""cuda:1""))<NewLine>    x3 = self.layer3(input.to(""cuda:2""))<NewLine>    return x3<NewLine></code></pre><NewLine><p>Then it would certainly fail as it cannot find the device.</p><NewLine><p>If there is nothing like that, you probably can get around by doing sth like:</p><NewLine><pre><code class=""lang-auto"">torch.distributed.init_process_group(backend='nccl', init_method=opt.dist_url, world_size=1, rank=0)<NewLine></code></pre><NewLine><p>You might also need to change <code>opt.dist_url</code> into sth like <code>""tcp://localhost:23456""</code></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It worked, thank you!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Rubiel1; <NewLine> ,"REPLY_DATE 1: March 20, 2020, 11:04pm; <NewLine> REPLY_DATE 2: March 20, 2020, 11:05pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
48981,Using multiple CPU cores for training,2019-06-26T05:55:36.511Z,5,7023,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <span class=""mention"">@all</span>,<br/><NewLine>I’m new to pytorch and currently trying my hands on an mnist model. I do not have a GPU but have 24 CPU cores and &gt;100GB RAM (using torch.get_num_threads()). However, I do not observe any significant improvement in training speed when I use torch.set_num_threads(10) - it seems to me that there isn’t any difference between setting the number of threads and not having at all.<br/><NewLine>I would like to know how I can take advantage of the multiple CPU cores available during model training. I have also tried setting num_workers of the data loader but to no avail.</p><NewLine></div>",https://discuss.pytorch.org/u/bbrighttaer,(Bbrighttaer),bbrighttaer,"June 26, 2019,  5:55am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Setting the number of threads will only make some individual operations faster (e.g. big matrix multiplication or convolution), if they work on big tensors. For your example, this might accelerate some of the big fully connected layers, if you use a batch size that’s big enough. Alternatively, you can explore running more processes, and using <code>torch.nn.parallel.DistributedDataParallel</code> to parallelize across processes.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your direction. I have tried using <code>torch.nn.parallel.DistributedDataParallelCPU</code> and the forward pass is able to utilize the number of processes I set (I assume that’s the same as cpu cores in my case). I followed the tutorial <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">here</a>. However, there’s a lengthy block, for what I think is the backward pass, before any forward pass is observed.<br/><NewLine>Any suggestion on how to address this?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>What do you mean with a “length block”?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, it’s a typo. I mean a ‘lengthy block’ of all forward pass ops before the spawned processes do the next forward pass.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you mean a lengthy block of time? That you observe upon starting the processes?</p><NewLine><p>It is possible the first forward pass take a bit longer than subsequent ones due to memory allocation and general initialization of all the operators/backends.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes please, lengthy block of time.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>If this only happens in the first iteration, it’s likely memory allocation and initialization stuff. If subsequent iterations also take longer than you expect, it is possible you have started too many processes and are overloading your system.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is torch.nn.parallel.DistributedDataParallel only applicable to GPU and not to CPU with multi cores?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>It works with CPUs with multi cores. From the <code>DistributedDataParallel</code> doc:</p><NewLine><blockquote><NewLine><p>For multi-device modules and CPU modules, device_ids must be None  or an empty list, and input data for the forward pass must be placed on the correct device.</p><NewLine></blockquote><NewLine><p>The thing is that as there is only one “cpu” device in PyTorch, you cannot specify which cores to run a DDP process using the <code>device_ids</code> arg in <code>DistributedDataParallel</code> constructor. However, you should still be able to set the CPU affinity for processes independently?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/bbrighttaer; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/bbrighttaer; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/bbrighttaer; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Murtaza_Basu; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: June 26, 2019,  6:49am; <NewLine> REPLY_DATE 2: June 26, 2019,  4:00pm; <NewLine> REPLY_DATE 3: June 27, 2019, 11:27am; <NewLine> REPLY_DATE 4: June 27, 2019, 12:58pm; <NewLine> REPLY_DATE 5: June 27, 2019,  1:29pm; <NewLine> REPLY_DATE 6: June 27, 2019,  1:49pm; <NewLine> REPLY_DATE 7: June 27, 2019,  2:09pm; <NewLine> REPLY_DATE 8: March 20, 2020, 11:27am; <NewLine> REPLY_DATE 9: March 20, 2020,  2:33pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> 
73043,Pytorch not compatible with React-native,2020-03-12T20:20:07.881Z,0,105,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve been using react native for one of my module.Recently we started using Pytorch . Pytorch along with react native is crashing.<br/><NewLine>Below is the error</p><NewLine><p>java.lang.UnsatisfiedLinkError: couldn’t find DSO to load: libpytorch_jni.so caused by: dlopen failed: cannot locate symbol “_ZN8facebook3jni10JByteOrder11nativeOrderEv” referenced by “/data/app/*****-4ZGDZJcLItKdhIsz3hj2rQ==/lib/arm/libpytorch_jni.so”.</p><NewLine><p>Please help me.I am in great urgency.</p><NewLine></div>",https://discuss.pytorch.org/u/Arun_Kumar1,(Arun Kumar),Arun_Kumar1,"March 12, 2020,  8:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/arun_kumar1"">@Arun_Kumar1</a></p><NewLine><p>Could you please share a snippet of the PyTorch code that causing this error? Why do you think this relates to <code>torch.distributed</code> or <code>torch.distributed.rpc</code>? Are you using these two packages? It seems to me this questions should be posted under mobile build and Java bindings tag?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>True .I didn’t notice .It should be tagged under mobile</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Arun_Kumar1; <NewLine> ,"REPLY_DATE 1: March 19, 2020,  5:44pm; <NewLine> REPLY_DATE 2: March 20, 2020,  5:23am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
73450,All_gather trigger unnecessary in-place change,2020-03-17T00:21:32.562Z,0,96,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi. For example, for the code snippet</p><NewLine><pre><code class=""lang-auto"">a = torch.rand(3).requires_grad_()<NewLine>l = a.sum()<NewLine>torch.distributed.all_gather([a,b,c], a)<NewLine></code></pre><NewLine><p><code>l.backward()</code> will trigger an error say</p><NewLine><blockquote><NewLine><p>one of the variables needed for gradient computation has been modified by an in-place operation</p><NewLine></blockquote><NewLine><p>I assume this is because all_gather do in-place change for all a,b,c.<br/><NewLine>But why? As a is emitted by the current process, it’s not necessary to change it and cause this issue. Is there any consideration behind this all_gather behavior?</p><NewLine><p>Thx</p><NewLine></div>",https://discuss.pytorch.org/u/boy2000_007man,(Xuenan Guo),boy2000_007man,"March 17, 2020, 12:21am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We probably can add a shortcut to avoid changing <code>a</code> in this case, but I am not sure if that is a good idea, because that will make all_gather have different behavior depending on underlying storage. Consider two cases.</p><NewLine><p>Case 1:</p><NewLine><pre><code class=""lang-auto"">x = empty_like(a)<NewLine>torch.distributed.all_gather([x,b,c], a)<NewLine></code></pre><NewLine><p>In this case, we would still need to write data from a to x, right?</p><NewLine><p>Case 2:</p><NewLine><pre><code class=""lang-auto"">x = a.view(...) # say change the stride<NewLine>torch.distributed.all_gather([x,b,c], a)<NewLine></code></pre><NewLine><p>In this case, <code>x</code> will share the storage with <code>a</code>, but using a different element layout, so we would need to write into x.</p><NewLine><p>To address the above problems, we probably can detect and only skip inplace write if <code>x</code> shares the same storage and meta with <code>a</code>. However, the concerns are 1) does the extra overhead worth it? 2) will the disparity in all_gather’s behavior confuse users?</p><NewLine><p>This <a href=""https://github.com/pytorch/pytorch/pull/33924"" rel=""nofollow noopener"">PR</a> might be relevant. It’s trying to avoid a flatten.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 19, 2020,  4:10pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
73177,How to use DataParallel with attention layers?,2020-03-14T00:35:02.412Z,0,94,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am looking for clarification on the best way to use DataParallel with attention layers. As an example, MultiheadAttention expects inputs which have shape (L,N,E) where L is the length of the sequence, N is the batchsize, and E is the embedding size. The fact that the batch size is NOT the first dimension leads to problem when using DataParallel. To work around this I am transposing the dimension, see example below:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine>class AttnParallel(nn.Module):<NewLine>        <NewLine>    def __init__(self, dim, num_heads):<NewLine>        super(AttnParallel,self).__init__()<NewLine>        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=0, bias=False)<NewLine>        <NewLine>    def forward(self, h, mask):<NewLine>        <NewLine>        print(""h has shape:"", h.shape)<NewLine>        print(""mask has shape"", mask.shape)<NewLine>        <NewLine>        h = h.transpose(0,1).contiguous()<NewLine>        h = self.attn(h,h,h, key_padding_mask=mask)[0]<NewLine>        h = h.transpose(0,1).contiguous()<NewLine>        return h<NewLine><NewLine># create model<NewLine>dim =4<NewLine>num_head=2<NewLine>device = torch.device(""cuda"")<NewLine>mod = AttnParallel(dim, num_head)<NewLine>mod = nn.DataParallel(mod.to(device))<NewLine><NewLine># create data<NewLine>bsz = 16<NewLine>L = 5<NewLine>h = torch.rand(bsz,L,dim)<NewLine>mask = torch.zeros(bsz,L).bool()<NewLine>mask[0,1] = True<NewLine>mask[2,4] = True<NewLine><NewLine># forward<NewLine>h = mod(h,mask)<NewLine></code></pre><NewLine><p>I have a few questions:</p><NewLine><ol><NewLine><li><NewLine><p>My understanding is that when using DataParralel, whatever tensors I feed to the forward() function will be chunked over the first dimension into 8 pieces and fed to 8 replica of my network (assuming 8 GPUs). So in this example, both the h and mask tensor will be chunked into 8 pieces. Eventually, the outputs of the 8 replica are concatenated over the first dimension. Am I understanding this correctly?</p><NewLine></li><NewLine><li><NewLine><p>Is transposing the input the recommended way of dealing with module that expect input whose first dimension is not the batch dimension. Is it recommended to use contiguous() to improve performance, or is that unnecessary?</p><NewLine></li><NewLine><li><NewLine><p>Should it be  nn.DataParallel(mod.to(device)) or nn.DataParallel(mod).to(device)? Both seem to work but the doc says: "" The parallelized  <code>module</code>  must have its parameters and buffers on  <code>device_ids[0]</code>  before running this DataParallel module.""  So I don’t understand how come nn.DataParallel(mod).to(device) work?</p><NewLine></li><NewLine></ol><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/tlaurent,(Thomas Laurent),tlaurent,"March 14, 2020, 12:55am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>your understand is correct, it will chunk data based on dim=0 in default</li><NewLine><li>I’m not sure transposing it recommended way, but contiguous() will cause copy</li><NewLine><li>I think nn.DataParallel(mod.to(device)) is better</li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Yanli_Zhao; <NewLine> ,"REPLY_DATE 1: March 16, 2020, 11:12pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
73354,"When I run model on multiple GPUs,register_hook is invalid",2020-03-16T04:21:33.607Z,0,88,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to save gradients of internal variables through register_hook() or retain_grad().<br/><NewLine>When I run model on single GPU, it works.<br/><NewLine>But when I run model on multiple GPUs through wrapping model into nn.DataParallel, I find that it doesn’t work.<br/><NewLine>Can anyone help me?</p><NewLine></div>",https://discuss.pytorch.org/u/Asuka-EVA,(Asuka Eva),Asuka-EVA,"March 16, 2020,  4:28am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>based on comments “In each forward, :attr:<code>module</code> is <strong>replicated</strong> on each device, so any<br/><NewLine>updates to the running module in <code>forward</code> will be lost. For example,<br/><NewLine>if :attr:<code>module</code> has a counter attribute that is incremented in each<br/><NewLine><code>forward</code>, it will always stay at the initial value because the update<br/><NewLine>is done on the replicas which are destroyed after <code>forward</code>. However,<br/><NewLine>:class:<code>~torch.nn.DataParallel</code> guarantees that the replica on<br/><NewLine><code>device[0]</code> will have its parameters and buffers sharing storage with<br/><NewLine>the base parallelized :attr:<code>module</code>. So <strong>in-place</strong> updates to the<br/><NewLine>parameters or buffers on <code>device[0]</code> will be recorded.”</p><NewLine><p>That means gradients of internal variables can not be updated in multiple GPUs, can only be updated in device[0]. if you want to sync buffers, you can try to use DistributedDataParallel package?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Yanli_Zhao; <NewLine> ,"REPLY_DATE 1: March 16, 2020,  8:46pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
72628,Runtime error: connection reset by peer in init_process_group,2020-03-09T17:54:41.916Z,1,345,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I am trying to implement distributed fashion training using torch.distributed package.<br/><NewLine>in torch.distributed.init_process_group(.) I am getting an error  with deadlock.<br/><NewLine>here is <strong>error</strong> ;<br/><NewLine>0 4<br/><NewLine>Process Process-4:<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “/usr/lib/python3.6/multiprocessing/process.py”, line 258, in _bootstrap<br/><NewLine>self.run()<br/><NewLine>File “/usr/lib/python3.6/multiprocessing/process.py”, line 93, in run<br/><NewLine>self._target(*self._args, **self._kwargs)<br/><NewLine>File “main.py”, line 17, in init_process<br/><NewLine>dist.init_process_group(backend, rank=rank, world_size=size)<br/><NewLine>File “/sdd1/amit/venv/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py”, line 407, in init_process_group<br/><NewLine>timeout=timeout)<br/><NewLine>File “/sdd1/amit/venv/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py”, line 475, in _new_process_group_helper<br/><NewLine>timeout=timeout)<br/><NewLine>RuntimeError: Connection reset by peer</p><NewLine><p>Here is <strong>code</strong><br/><NewLine>10 def run(size, rank):<br/><NewLine>11     print(size, rank)<br/><NewLine>12<br/><NewLine>13 def init_process(rank, size, fn, backend=‘gloo’):<br/><NewLine>14     “”"" Initialize the distributed environment. “”""<br/><NewLine>15     os.environ[‘MASTER_ADDR’] = ‘127.0.0.1’<br/><NewLine>16     os.environ[‘MASTER_PORT’] = ‘29500’<br/><NewLine>17     dist.init_process_group(backend, rank=rank, world_size=size)<br/><NewLine>18     fn(rank, size)<br/><NewLine>19<br/><NewLine>20<br/><NewLine>21 if <strong>name</strong> == “<strong>main</strong>”:<br/><NewLine>22     size = 4<br/><NewLine>23     processes = []<br/><NewLine>24     for rank in range(size):<br/><NewLine>25         p = Process(target=init_process, args=(rank, size, run))<br/><NewLine>26         p.start()<br/><NewLine>27         processes.append§<br/><NewLine>28<br/><NewLine>29     for p in processes:<br/><NewLine>30         p.join()</p><NewLine></div>",https://discuss.pytorch.org/u/Amit_Singh1,(Amit Singh),Amit_Singh1,"March 9, 2020,  5:54pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/amit_singh1"">@Amit_Singh1</a>, can you try if spawn mode works for you? I tried the following, and it works in my dev env.</p><NewLine><pre><code class=""lang-python"">import os<NewLine>import torch.distributed as dist<NewLine>import torch.multiprocessing as mp<NewLine><NewLine>def run(size, rank):<NewLine>    print(size, rank)<NewLine><NewLine>def init_process(rank, size, fn, backend=""gloo""):<NewLine>    """"""Initialize the distributed environment.""""""<NewLine>    os.environ[""MASTER_ADDR""] = ""127.0.0.1""<NewLine>    os.environ[""MASTER_PORT""] = ""29500""<NewLine>    dist.init_process_group(backend, rank=rank, world_size=size)<NewLine>    fn(rank, size)<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    size = 4<NewLine>    processes = []<NewLine>    mp.set_start_method(""spawn"")<NewLine>    for rank in range(size):<NewLine>        p = mp.Process(target=init_process, args=(rank, size, run))<NewLine>        p.start()<NewLine>        processes.append(p)<NewLine><NewLine>    for p in processes:<NewLine>        p.join()<NewLine></code></pre><NewLine><p><a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html"" rel=""nofollow noopener"">Here</a> are some best practices for multiprocessing.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Worked for me.<br/><NewLine>Thanks.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Amit_Singh1; <NewLine> ,"REPLY_DATE 1: March 15, 2020,  3:31am; <NewLine> REPLY_DATE 2: March 15, 2020,  3:32am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
73046,Batchsize with DDP v.s. without DDP,2020-03-12T20:37:04.057Z,0,255,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In distributeddataparallel, when local batch-size is 64 (i.e. <code>torch.utils.data.DataLoader(batch_size=64) and torch.utils.data.distributed.DistributedSampler() is used</code>), assume there are N processes totally in ddp (N processes distirbute in one node or more than one node). Is the forward-backward process in ddp similar to the forward-backward process in a single gpu using 64×N batch-size inputs?</p><NewLine></div>",https://discuss.pytorch.org/u/hhxx,(hhxx),hhxx,"March 12, 2020,  8:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>yes, distributed training using DDP is mathematically equivalent to local training</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Yanli_Zhao; <NewLine> ,"REPLY_DATE 1: March 13, 2020,  4:43am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
72769,Should we split batch_size according to ngpu_per_node when DistributedDataparallel,2020-03-10T19:41:49.045Z,5,776,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Assume we have two nodes: node-A and node-B, each has 4gpus(i.e. ngpu_per_node=4). We set args.batch_size = 256 on each node, means that we want each node process 256 images in each forward.</p><NewLine><p>(1) If we use DistributedDataparallel with 1gpu-per-process mode, shall we manually divide the batchsize by ngpu_per_node in torch.utils.data.DataLoader : <strong>torch.utils.data.DataLoader(batch_size = args.batch_size / 4)</strong>(the way used in <a href=""https://github.com/pytorch/examples/blob/5df464c46cf321ed1cc3df1e670358d7f5ae1887/imagenet/main.py#L42"" rel=""nofollow noopener"">pytorch-imagenet-official-example</a>). In my original opinion, I think <strong>DistributedSampler</strong> can handle such thing, because we have passed world_size and rank to DistributedSampler. . If I was wrong, please point it out, thanks!</p><NewLine><p>(2) If dividing the batchsize by ngpu_per_node is a correct way, I wonder what will happen if we do not that.</p><NewLine><ul><NewLine><li><NewLine><p>Does it means in each node, 4*batch_size images are processed per forward-process?</p><NewLine></li><NewLine><li><NewLine><p>Will 4*len(dataset)  images are processed in one epoch, or the forward frequency are four times less than usual(i.e. the total number images proceeded per epoch keep same)?</p><NewLine></li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/hhxx,(hhxx),hhxx,"March 10, 2020,  7:59pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""72769"" data-username=""hhxx""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/hhxx/40/21482_2.png"" width=""20""/> hhxx:</div><NewLine><blockquote><NewLine><p>(2) If dividing the batchsize by ngpu_per_node is a correct way, I wonder what will happen if we do not that.</p><NewLine><ul><NewLine><li>Does it means in each node, 4*batch_size images are processed per forward-process?</li><NewLine><li>Will 4*len(dataset) images are processed in one epoch, or the forward frequency are four times less than usual(i.e. the total number images proceeded per epoch keep same)?</li><NewLine></ul><NewLine></blockquote><NewLine></aside><NewLine><p>You are correct. Each DataLoader instance pairs with a DDP instances. If you do not divide the batch-size=256 by 4, then each DDP instance will process 256 images. As your env has 8-GPUs in total, there will be 8 DDP instances. So one iteration will process 256 * 8 images in total.</p><NewLine><p>However, DDP does divide the gradients by the world_size by default <a href=""https://github.com/pytorch/pytorch/blob/f326045b3757236aabe367dfca1894be14ce31ef/torch/csrc/distributed/c10d/reducer.cpp#L360"" rel=""nofollow noopener"">code</a>. So, when configuring learning rate, you only need to consider the batch_size for a single DDP instance.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Another question is if we do not divide batch-size by 8, the total images processed in <strong>one epoch</strong> will be the same as usual or eight times?</p><NewLine><p>As for learning rate, if we have 8-gpus in total, there wiil be 8 DDP instances. If the batch-size in each DDP distances is <strong>64</strong> (has been divides manually), then one iteration will process <strong>64×4=256</strong> images per node. Taking all gpu into account (2 nodes, 4gpus per node), then one iteration will process <strong>64×8=512</strong> images.  Assuming in one-gpu-one-node scenario, we set <strong>1×lr</strong> when batch-size=64, <strong>4×lr</strong> when batch-size=256 and <strong>8×lr</strong> when batch-size=512(a universal strategy that  increase learning rate with batch-size linearly). Let us back to DDP scenario (2 node, 4gpus per node), what learning rate shall we use？ 1×lr or 4×lr or 8×lr？</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""72769"" data-username=""hhxx""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/hhxx/40/21482_2.png"" width=""20""/> hhxx:</div><NewLine><blockquote><NewLine><p>Another question is if we do not divide batch-size by 8, the total images processed in <strong>one epoch</strong> will be the same as usual or eight times?</p><NewLine></blockquote><NewLine></aside><NewLine><p>The total number of images processed will be 8 times, because each DDP instance/process will process batch_size images.</p><NewLine><blockquote><NewLine><p>Let us back to DDP scenario (2 node, 4gpus per node), what learning rate shall we use？ 1×lr or 4×lr or 8×lr？</p><NewLine></blockquote><NewLine><p>It should be 1x lr. Because DDP calculates the average instead of sum of all local gradients. Let’s use some number to explain this. Assuming every image leads to a <code>torch.ones_like(param)</code> gradient for each parameter.</p><NewLine><ul><NewLine><li>For local training without DDP, if you set batch_size = 64, the gradient for each parameter will then be <code>torch.ones_like(param) * 64</code>.</li><NewLine><li>For 8-process DDP training, if you set batch_size = 64, the local gradient for each parameter will also be <code>torch.ones_like(param) * 64</code>. Then DDP use collective communication to calculate the sum of gradients across all DDP instances which will be <code>torch.ones_like(param) * 64 * 8</code>, and then DDP divides that value by 8. So the final gradient in <code>param.grad</code> field will still be  <code>torch.ones_like(param) * 64</code> (the code actually first divide and then do globally sum). So, when set lr, you only need to consider local batch_size.</li><NewLine></ul><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>According to the discuss in <a href=""https://discuss.pytorch.org/t/is-average-the-correct-way-for-the-gradient-in-distributeddataparallel-with-multi-nodes/34260/3"">Is average the correct way for the gradient in DistributedDataParallel</a>, I think we should  set 8×lr. I will state my reason under 1 node, 8gpus, local-batch=64(images processed by one gpu each iteration) scenario:<br/><NewLine>(1) Let us consider a batch images (batch-size=512), in DataParallel scenario, a complete forward-backforwad pipeline is:</p><NewLine><ol><NewLine><li><NewLine><p>the input data are split to 8 slices (each contains 64 images), each slice is feed to net to compute output</p><NewLine></li><NewLine><li><NewLine><p>outputs are concated  in master gpu (usually gpu 0) to form a [512, C] outputs</p><NewLine></li><NewLine><li><NewLine><p>compute the loss with groundtruth(same dimension: [512, C]) :   <code>loss = \frac{1}{512} \sum_{i=1}^512 mse(output[i], groundtruth[i])</code>( use mse loss as illustration)</p><NewLine></li><NewLine><li><NewLine><p>use loss.backward to compute gradients.</p><NewLine></li><NewLine></ol><NewLine><p>So the finally [512, C] outputs are the same as computed on one gpu. So the learning rate here shall be set as 8×lr to keep same as 512 batchsize in one-gpu-one-node scenarior.</p><NewLine><p>(2) Secondly, when DistributedDataparallel is used, the pipeline is</p><NewLine><ol><NewLine><li><NewLine><p>the input data are also split to 8 slices</p><NewLine></li><NewLine><li><NewLine><p>outputs are computed in each gpu to form a [64, C] outputs</p><NewLine></li><NewLine><li><NewLine><p>In each gpu, compute the loss  <code>loss = \frac{1}{64} \sum_{i=1}^64 mse(output[i], groundtruth[i])</code> and compute gradients <code>grad_k (k is the gpu number, k=0,1,...,7)</code>:  (this is different with Dataparallel, which need collect all outputs in master gpu)</p><NewLine></li><NewLine><li><NewLine><p>Average the gradients between all gpus: <code>avg_grad =\frac{1}{8} \sum_{k=1}^8 grad_k</code></p><NewLine></li><NewLine></ol><NewLine><p>By this way, the averaged gradients are also same as the gradients computed on one-gpu-one-node scenario. So I think learning rate here need to be set as 8×lr to keep same as 512 batchsize on one-gpu-one-node scenario.</p><NewLine><p>The main difference between you and me is that when local batch is set as 64, I think local gradients will be averaged over local samples, resulting in <code>torch.ones_like(param)*64/64</code>, but you think the local gradients will be summed over local samples, resulting in <code>torch.ones_like(param) * 64</code>. I think local gradients will be averaged mainly because the loss function in pytroch, like mse(), will compute the average loss over all input samples, so the gradients computed from such loss also should be averaged over all input samples.</p><NewLine><p>I do not know if I understand DistributedDataparallel in a right way. Please let me know if there has any wrong.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I agree with all your analysis on the magnitude of the gradients, and I agree that it depends on the loss function. But even with MSE loss fn, it can lead to different conclusions:</p><NewLine><ol><NewLine><li>If the fw-bw has processed 8X data, we should set lr to 8X, meaning that the model should take a larger step if it has processed more data as the gradient is more accurate. (IIUC, this is what you advocate for)</li><NewLine><li>If the gradient is of the same magnitude, we should use 1X lr, especially when approaching convergence. Otherwise, if we use 8X lr, it is more likely to overshoot and hurt converged model accuracy.</li><NewLine></ol><NewLine><p>After reading your analysis, I realized that, with MSE loss fn, the discussion is mostly irrelevant to DDP. The question would then be, if I increase batch size by k, how should I adjust the learning rate, which is an open question. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is it correct that when local batch-size is 64 (i.e. <code>torch.utils.data.DataLoader(batch_size=64) and torch.utils.data.distributed.DistributedSampler() is used</code>), and there are N processes totally in ddp (N processes distirbute in one node or more than one node), the forward-backward process is similar to the forward-backward process in 1-gpu-1-node using 64×N batch-size inputs?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hhxx; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/hhxx; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/hhxx; <NewLine> ,"REPLY_DATE 1: March 11, 2020,  3:06am; <NewLine> REPLY_DATE 2: March 11, 2020, 10:03am; <NewLine> REPLY_DATE 3: March 11, 2020,  2:11pm; <NewLine> REPLY_DATE 4: March 11, 2020,  8:14pm; <NewLine> REPLY_DATE 5: March 12, 2020,  8:29pm; <NewLine> REPLY_DATE 6: March 12, 2020,  5:38am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
72893,DataParallel with manual scatter,2020-03-11T15:21:16.432Z,0,109,"<div class=""post"" itemprop=""articleBody""><NewLine><p>With DataParallel, how can we assign exemples to GPUs manually while iterating on data loader?</p><NewLine><p>My dataset contains images of highly variable sizes and we chose to use a batchsize of 1. The automatic scatter in dataParallel will use the batchsize dimension to realize the scatter and will only assign to 1 GPU in this case.</p><NewLine><p>Is there a way to compute compute the backward in multi-GPU fashion in this context?</p><NewLine></div>",https://discuss.pytorch.org/u/Patrick_Dallaire,(Patrick Dallaire),Patrick_Dallaire,"March 11, 2020,  3:45pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you want to try to use DistributedDataParallel API, where you can spawn each process running on one GPU</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Yanli_Zhao; <NewLine> ,"REPLY_DATE 1: March 11, 2020,  6:30pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
72346,DistributedDataParallel hangs when there is autograd call in backward,2020-03-06T23:04:11.256Z,2,290,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I have a model which has a custom autograd Function call. The backward method of this call has mutliple torch.autograd.grad calls in a loop. Somewhat like this -</p><NewLine><pre><code class=""lang-auto"">class func1(Function):<NewLine>    @staticmethod<NewLine>    def forward(ctx, input1, input2, *args):<NewLine>        ctx.save_for_backward(input1, input2)<NewLine>        return input2<NewLine><NewLine>    @staticmethod<NewLine>    def backward(ctx, grad):<NewLine>        input1, input2 = ctx.saved_tensors<NewLine>        <NewLine>        for ii in range(10):<NewLine>            new = torch.autograd.grad(input2, input1, grad_outputs=grad,<NewLine>                                            retain_graph=True)<NewLine>        return (None, new)<NewLine><NewLine><NewLine>class MyModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(ToyModel, self).__init__()<NewLine>        self.net1 = torch.nn.Linear(10, 10)<NewLine>        self.relu = torch.nn.ReLU()<NewLine>        self.net2 = torch.nn.Linear(10, 5)<NewLine><NewLine>    def forward(self, x):<NewLine>        out = self.net1(x)<NewLine>        out = func1.apply(x, out)<NewLine>        out2 = self.relu(out)<NewLine>        out2 = func1.apply(x, out2)<NewLine>        out3 = self.net2(out2)<NewLine>        out3 = func1.apply(x, out3)<NewLine>        return  out3<NewLine></code></pre><NewLine><p>This works fine when I run with single GPU. But when I run with DDP it hangs in the loss.backward() call. I see that it hangs in the for loop in func1. Peculiarly all the workers hang in the same iteration of the for loop.</p><NewLine><p>Edit: CPU utlization and GPU utilization stays high (100%) when it hangs for all processes. Code run with torch.distributed.launch<br/><NewLine>Any help would be appreciated!</p><NewLine><p>Thanks!!</p><NewLine></div>",https://discuss.pytorch.org/u/alekhka,,alekhka,"March 6, 2020, 11:12pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>This is a known limitation of DDP I’m afraid. You can see the issue tracking this here: <a href=""https://github.com/pytorch/pytorch/issues/24005"">https://github.com/pytorch/pytorch/issues/24005</a></p><NewLine><p>We are planning on getting to this after the 1.5 release.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your response.</p><NewLine><p>After some testing, now I realised that the hang is due to syncBNs in the model - it works fine with normal BNs. The graph between input2 and input1 has syncBNs too and the many autograd.grad() calls give rise to many all_reduce calls in syncBNs’ backward which hang. I think this is what’s happening - <a href=""https://github.com/pytorch/pytorch/pull/14267#discussion_r257051495"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/14267#discussion_r257051495</a></p><NewLine><p>My model has many heads in parallel with syncBNs and those could be deadlocking too.</p><NewLine><p>Do you see a solution/workaround for this?</p><NewLine><p>Thanks again!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/alekhka"">@alekhka</a> as a temporary workaround, you could try doing allreduce manually after the backward pass. (see <a href=""https://github.com/pytorch/pytorch/issues/24005#issuecomment-523593196"" rel=""nofollow noopener"">this comment</a>). This will be slower as there will be no overlapping between computation and communication, but hopefully can avoid the hang problem.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>That should be same as setting <code>delay_reduce=True</code> in apex right? That doesn’t fix it either.</p><NewLine><p>I think the deadlock is between all_reduce calls in syncBNs’ <code>backward()</code> across the parallel heads and not between gradient all_reduce &amp; syncBN all_reduce.</p><NewLine><p>Removing the parallel head and having just 1 head works fine. Replacing syncBN with BN works fine for with parallel head model.</p><NewLine><p>Thank you!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/alekhka; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/alekhka; <NewLine> ,"REPLY_DATE 1: March 8, 2020,  9:31pm; <NewLine> REPLY_DATE 2: March 9, 2020,  9:43am; <NewLine> REPLY_DATE 3: March 10, 2020,  3:16pm; <NewLine> REPLY_DATE 4: March 11, 2020,  2:29pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
58288,Synchronization steps in distributed data parallel,2019-10-15T12:38:56.699Z,4,1257,"<div class=""post"" itemprop=""articleBody""><NewLine><p>As far as I understood, the <code>DistributedDataParallel</code> module performs gradient synchronization between different nodes automatically, one thing I don’t understand clearly is when this synchronization is done exactly?</p><NewLine><p>For example, the below snippet is from <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">GETTING STARTED WITH DISTRIBUTED DATA PARALLEL</a> PyTorch documentation with small change:</p><NewLine><pre><code class=""lang-auto"">def demo_basic(rank, world_size):<NewLine>    setup(rank, world_size)<NewLine><NewLine>    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and<NewLine>    # rank 2 uses GPUs [4, 5, 6, 7].<NewLine>    n = torch.cuda.device_count() // world_size<NewLine>    device_ids = list(range(rank * n, (rank + 1) * n))<NewLine><NewLine>    # create model and move it to device_ids[0]<NewLine>    model = ToyModel().to(device_ids[0])<NewLine>    # output_device defaults to device_ids[0]<NewLine>    ddp_model = DDP(model, device_ids=device_ids)<NewLine><NewLine>    loss_fn = nn.MSELoss()<NewLine>    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)<NewLine><NewLine>    optimizer.zero_grad()<NewLine>    outputs = ddp_model(torch.randn(20, 10))<NewLine>    labels = torch.randn(20, 5).to(device_ids[0])<NewLine><NewLine>    loss = loss_fn(outputs, labels)<NewLine>    loss.backward()<NewLine>#    loss_fn(outputs, labels).backward()<NewLine><NewLine>    optimizer.step()<NewLine><NewLine>    cleanup()<NewLine></code></pre><NewLine><p>In the above example, Is computed loss synchronized among all nodes? i.e., Does loss value represents only each node loss or it is averaged among all nodes?</p><NewLine></div>",https://discuss.pytorch.org/u/pooya_khandel,(pooya khandel),pooya_khandel,"October 15, 2019, 12:38pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The <code>DDP()</code> wrapper takes care of all the synchronizations and offer a <code>nn.Module</code> like api so that you can use it transparently.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, Do you know where the script of gradients synchronization during backward is in pytorch source code?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/meilu_zhu"">@meilu_zhu</a> The DDP wrapper creates a <code>c10d.Reducer</code> which is responsible for concatenating multiple gradients into larger buckets and reducing them. You can find the source code at <code>torch/csrc/distributed/c10d/reducer.cpp</code>.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, <a class=""mention"" href=""/u/pietern"">@pietern</a>. Thanks for your answer.  “DistributedDataParallel” automatically averages the gradient when calling <code>loss.backward()</code>, But I didn’t find the corresponding script about how calling <code>loss.backward()</code> triggers <code>torch/csrc/distributed/c10d/reducer.cpp</code> to concatenate multiple gradients in pytorch source code? Could you tell me where it is, please?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>In <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html</a>, the code <strong>demo_checkpoint</strong> really confused me. There are two main quetions:<br/><NewLine>(1) In <strong>demo_checkpoint</strong>, all processes need to be synchronized by loading the same checkponts saved by process-0. If it is necessary when we train model with  DistributedDataParallel.?<br/><NewLine>(2) If it is necessary to synchronize model across multi nodes and gpus by loading the same checkpoint, how can node-B load checkpoints saved in node-A?<br/><NewLine>I don’t know if I understand <strong>demo_checkpoint</strong> in a right way. Could you please help answer this question? Thanks !</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/hhxx"">@hhxx</a></p><NewLine><blockquote><NewLine><p>(1) In  <strong>demo_checkpoint</strong> , all processes need to be synchronized by loading the same checkponts saved by process-0. If it is necessary when we train model with DistributedDataParallel.?</p><NewLine></blockquote><NewLine><p>No, this is not necessary. This is only useful when your training job take very long and can crash in the middle. You can then use the checkpoint to recover instead of starting over from scratch.</p><NewLine><blockquote><NewLine><p>(2) If it is necessary to synchronize model across multi nodes and gpus by loading the same checkpoint, how can node-B load checkpoints saved in node-A?<br/><NewLine>I don’t know if I understand  <strong>demo_checkpoint</strong>  in a right way. Could you please help answer this question? Thanks !</p><NewLine></blockquote><NewLine><p>The recovery scheme should be application-dependent. That tutorial demonstrates single-machine multi-GPU DDP with checkpointing. So, all DDP processes can read from the same file. If you need checkpoint, and if your training spans multiple machines, you can load it from rank0 and then broadcast it to other ranks using <code>torch.distributed.braodcast</code>.</p><NewLine><p>BTW, we probably should call that tutorial “intermediate”, and use <a href=""https://pytorch.org/docs/master/notes/ddp.html#example"" rel=""nofollow noopener"">this one</a> as a starting example.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks very much! It is very clear.<br/><NewLine>Another question is that  when we train a model with DistributedDataparallel, DistributedSampler is suggested to use together.  If we need add <strong>sampler.set_epoch(epoch)</strong> before each epoch start according to <a href=""https://github.com/PyTorchLightning/pytorch-lightning/issues/224"" rel=""nofollow noopener"">set_epoch for DistributedSampler</a>.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/vincentqb"">@vincentqb</a> on <code>DistributedSampler</code> question <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/meilu_zhu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/meilu_zhu; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/hhxx; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/hhxx; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: October 15, 2019,  5:15pm; <NewLine> REPLY_DATE 2: November 26, 2019, 10:52am; <NewLine> REPLY_DATE 3: November 26, 2019,  3:29pm; <NewLine> REPLY_DATE 4: November 27, 2019,  5:56am; <NewLine> REPLY_DATE 5: March 9, 2020,  2:41pm; <NewLine> REPLY_DATE 6: March 10, 2020,  3:23pm; <NewLine> REPLY_DATE 7: March 10, 2020,  3:33pm; <NewLine> REPLY_DATE 8: March 10, 2020,  5:29pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
72472,Is it possible to do multi-machine single card training,2020-03-08T10:28:27.527Z,0,102,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am a deep learning beginner and running PyTorch’s Demo for study. I have several computers and laptops at home, the situation is different for each machine, some machines only have CPU, and some have powerful GPU, but I hope they all come together to speed up a training process.</p><NewLine><p>My implementation idea is the C / S structure. When the server is doing model input, if there is a request from the client, any number of samples are sent to the client through the socket, and the calculation of these sent samples is skipped. After the client completed, it sends back the results of the model forward, and the server merges the results. Finally, if the server completed an epoch, it blocks and waits for the results of all clients to return.</p><NewLine><p>Now, what I don’t know is how to merge the results of the model foward on the client into the server. I don’t know if this is correct …</p><NewLine><p>PS: Or, when the client’s model calls forward (), it does not make a classifier. Before the classifier is called, the data is sent back and the server model completes the classifier. Can the main calculation amount in the forward process be shared Arrived</p><NewLine></div>",https://discuss.pytorch.org/u/coollofty,,coollofty,"March 8, 2020, 10:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>IIUC, what you have in mind is the reverse structure of <a href=""https://pytorch.org/tutorials/intermediate/rpc_tutorial.html#distributed-reinforcement-learning-using-rpc-and-rref"" rel=""nofollow noopener"">this tutorial</a>. In the tutorial, there are multiple observers sending inputs to the same agent, while in your case, you would like to have the server sending inputs to different clients and run forward on the clients?</p><NewLine><p>The problem with the server-client structure is that, if forward is run on client, the autograd graph and activations will also be on client, meaning that the server cannot merge the output and run the backward locally.</p><NewLine><p>One possible alternative is that, instead of sending forward output from client to server, you can let the client finish forward-backward and then send the model param gradients to server. Then the server collects gradients from all clients and sum them, use the summed grads to update parameters, and then broadcast the updated model to all clients.</p><NewLine><p>Another alternative is to let the client finish forward-backward-optimizer, and then send the model params to the server. Then the server calculates the weighted average of all params and broadcast them back to the clients.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 10, 2020,  3:05pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
72239,RuntimeError: Tensors must be CUDA and dense,2020-03-06T06:37:59.437Z,2,1028,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I face this when using torch.nn.parallel.DistributedDataParallel(pytorch 1.4.0), and also using below</p><NewLine><p>device = torch.device(“cuda:0” if torch.cuda.is_available() else “cpu”)<br/><NewLine>tensor = torch.zeros(*shape, device=device).scatter_add(1, segment_ids, data)</p><NewLine><p>File “/home/gezi/mine/pikachu/utils/melt/eager/train.py”, line 1398, in train<br/><NewLine>loss.backward()<br/><NewLine>File “/home/gezi/env/anaconda3/lib/python3.6/site-packages/torch/tensor.py”, line 195, in backward<br/><NewLine>torch.autograd.backward(self, gradient, retain_graph, create_graph)<br/><NewLine>File “/home/gezi/env/anaconda3/lib/python3.6/site-packages/torch/autograd/<strong>init</strong>.py”, line 99, in backward<br/><NewLine>allow_unreachable=True)  # allow_unreachable flag<br/><NewLine>RuntimeError: Tensors must be CUDA and dense</p><NewLine><p>How to slove this ? I tried many such as<br/><NewLine>tensor = torch.zeros(*shape).cuda().scatter_add(1, segment_ids, data)<br/><NewLine>but this only works for DataParallel not DistributedDataParallel.</p><NewLine><p>Another problem of DIstributedDataParallel is each process using all gpus like below, is this by design ?<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/c64603e6055762df113d433817ea8cbc4b0ad5e5"" href=""https://discuss.pytorch.org/uploads/default/original/3X/c/6/c64603e6055762df113d433817ea8cbc4b0ad5e5.png"" title=""image""><img alt=""image"" data-base62-sha1=""si0xR9bIrnFcuPsWOBG66pJb5wV"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/6/c64603e6055762df113d433817ea8cbc4b0ad5e5_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/6/c64603e6055762df113d433817ea8cbc4b0ad5e5_2_352x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/6/c64603e6055762df113d433817ea8cbc4b0ad5e5_2_352x500.png, https://discuss.pytorch.org/uploads/default/optimized/3X/c/6/c64603e6055762df113d433817ea8cbc4b0ad5e5_2_528x750.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/c/6/c64603e6055762df113d433817ea8cbc4b0ad5e5_2_704x1000.png 2x"" width=""352""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1818×2576 467 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/Huige_Cheng,(Huige Cheng),Huige_Cheng,"March 6, 2020,  6:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""72239"" data-username=""Huige_Cheng""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/huige_cheng/40/13258_2.png"" width=""20""/> Huige_Cheng:</div><NewLine><blockquote><NewLine><p>RuntimeError: Tensors must be CUDA and dense</p><NewLine></blockquote><NewLine></aside><NewLine><p>Are you using sparse tensors?</p><NewLine><blockquote><NewLine><p>Another problem of DIstributedDataParallel is each process using all gpus like below, is this by design ?</p><NewLine></blockquote><NewLine><p>How did you construct DDP? You need to either set <a href=""https://github.com/pytorch/pytorch/blob/v1.4.0/torch/nn/parallel/distributed.py#L171"" rel=""nofollow noopener""><code>device_ids</code></a> arg properly or use the <code>CUDA_VISIBLE_DEVICES</code> env var to configure that, and make sure no DDP processes share the same GPU. Otherwise, each process will try to use all visible devices, and when two DDP process share a GPU, it could hang.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>Yes I’m using Embedding with arg sparse=True. But seems not ok to run DDP only if I using scatter_add later.</li><NewLine><li>If using 2 processes to run DDP. Then I set CUDA_VISIBLE_DEVICE=0,1 for each prcoess.<br/><NewLine>code like below<br/><NewLine>rank = dist.get_rank()<br/><NewLine>device = torch.device(‘cuda’, rank)<br/><NewLine>model = model.to(device)<br/><NewLine>model = torch.nn.parallel.DistributedDataParallel(model,device_ids=[rank],output_device=rank)<br/><NewLine>I tried to launch each process with CUDA_VISIBLE_DEVICE=0 CUDA_VISIBLE_DEVICE=1 but seems not work.</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> Well the second problem is due to I’m using tf dataset eager mode to read data first then convert to torch tensors, the problem has been solved.<br/><NewLine>For the first I find yes it is due to using sparse not related to scatter_add. So the problem is the same as <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/distributeddataparallel-sparse-embeddings/60410"">DistributedDataParallel Sparse Embeddings</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Huige_Cheng; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Huige_Cheng; <NewLine> ,"REPLY_DATE 1: March 6, 2020,  3:54pm; <NewLine> REPLY_DATE 2: March 7, 2020,  3:56am; <NewLine> REPLY_DATE 3: March 9, 2020,  7:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
34260,Is average the correct way for the gradient in DistributedDataParallel with multi nodes?,2019-01-09T13:37:58.948Z,14,3868,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I use DataParallel in one machine with two GPUs with 8 batch size(4 on each GPU), I get a satisfied training result. But, if I use DistributedDataParallel on two single GPU machines with 8 batch size(4 on each node), the training result is dissatisfied and convergence speed is slower than the DataParallel.</p><NewLine><p>After checking the doc of DataParallel and DistributedDataParallel, I noticed that DataParallel sum the gradient of each GPU, DistributedDataParallel average the gradient of each node(GPU under my condition).</p><NewLine><p>I think this difference is the reason for the different training results.</p><NewLine><p>Is average the correct way for the gradient in DistributedDataParallel with multi-node? Should I modify the DistributedDataParallel to sum the gradient of each node to reproduce the same training result in my exam?</p><NewLine></div>",https://discuss.pytorch.org/u/GeoffreyChen777,(Geo Ch),GeoffreyChen777,"January 9, 2019,  1:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, average across processes is the expected behavior here.</p><NewLine><p>Right now this behavior is not configurable.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/geoffreychen777"">@GeoffreyChen777</a> Yes, averaging is the correct way for gradient reduction among nodes.  The reason you are seeing DataParallel adds gradients together is the correct way too,</p><NewLine><p>The difference is that, DataParallel will split the batch size into sub-batches on each of the GPUs. When each GPU completes the computation, gradients are going to be reduced (added) onto the master GPU. Thinking about this as that: (1) this is a master-worker mode instead of true data parallel, since only the master GPU will scatter the batch and gather the results (2) we actually want to get the gradient of the total batch size, that’s why adding each worker’s gradient is the expected behavior. By comparison, Distributed Data Parallel goes completely parallel among distributed processes. And if the process itself has more than 1 GPU, the similar scatter and gather master worker mode will be employed similarly as DataParallel, and gradient will be added among worker GPU, and then averaged across distributed processes.  The bottomline here is, the gradient will be averaged across data-parallel workers (processes), not slave workers (within a single process).</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/teng-li"">@teng-li</a> Thank you!</p><NewLine><p>I am reproducing a huge network. The author use 16 batch size with 4GPU training. And they use DataParallel. I don’t have 4GPU machine. So I want to use 2 machine(2GPUs on each machine) to traing the network with batchsize 16. If the averaging is the default operation of DisributedDataParallel, there is no way to reproduce the training process. Right?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/geoffreychen777"">@GeoffreyChen777</a></p><NewLine><p>You can do this three ways:</p><NewLine><p>(1) If you can put 16 batch size on 2 GPUs<br/><NewLine>(2) If you cannot, use two nodes (two processes) DistributedDataParallel, each node(process) has a batch size of 8.  Here you should use the base LR for batch size of 8<br/><NewLine>(3) You can use four processes in two nodes with DistributedDataParallel, (this is the fastest way of doing distributed training), and each node will have two processes, and each process and DistributedDataParallel operates on one GPU (local rank, which is rank % number_of_gpu_per_node, here your rank is from 0 - 3, since you have four processes across two nodes).  But then you have to use the base LR for the batch size of 4.</p><NewLine><p>Hope this clarifies and helps</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/teng-li"">@teng-li</a> Thank you! If LR for batch size 16 is 0.01, LR for batch size 8 should be set to 0.02. Right?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/geoffreychen777"">@GeoffreyChen777</a> No, it should be 0.005</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/teng-li"">@teng-li</a> Thank you very much!</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply. After reading your answer, I have understood DataParallel but still confused by Distributed DataParallel. In my case, I have one machine with 4 GPUs. According to pytorch 1.0 tutorial , distributed dataparallel can also be used in single machine. Now I have 4 processes , each process has one GPU. So if batch_size is set to 128, that means each process (or single GPU) will be allocated batch_size 32 ? And some hyperparameters like LR should be set with batchsize 32 ?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/lausanne"">@Lausanne</a> I think you should keep the original learning rate.</p><NewLine><p>If you use the DistributedDataParallel, the gradient will be averaged between each process. DataParallel sum the gradient. It is equal to the DistributedDataParallel. The reason is that the the loss will be averaged by 128 batchsize and then backward to DataParallel model. The gradient will be reduced across each minibatch. And in DistributedDataParallel, the loss will be averaged by 32 and backward to each distributed model. So, we need to average the gradients between each distributed process.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/geoffreychen777"">@GeoffreyChen777</a><br/><NewLine>Really thank you! Briefly speaking, Dataparallel firstly sum and then average, because each GPU calculates part of the 128 batch, then they must send loss to master GPU to update parameters. Distributed Dataparallel has independent model and parameters in each GPU, so the loss calculated on one GPU has been the average of batch_size 32, then we should average loss between different GPU. That is in Distributed Dataparallel , we do average inner model and then average between different GPU. If I do not understand correctly, please let me known, thank you again!</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/lausanne"">@Lausanne</a></p><NewLine><p>You are right. I think the final gradient in DataParallel should be equal to the gradient of DistributedParallel. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/geoffreychen777"">@GeoffreyChen777</a><br/><NewLine>Thank you for your timly reply, best wish with you !</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Just to make sure I have understood correctly. If I train on one gpu with batchsize=16 and lr=0.01, what would be the correct lr if I train on two gpus in <code>torch.distributed</code> mode with the same batchsize=16(8 on each gpu)?</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/coincheung"">@coincheung</a> Your lr in torch.distributed mode should be 0.005</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, does this mean that in the distributed mode, the gradient of different gpus are summed up rather than meaned, thus I should reduce the lr to make remedy for the summation?</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>My guess. I think it depends on how you compute the loss and backward.<br/><NewLine>– DataParallel: if you merge 2 batch at the end then compute a single loss = average loss over each example, and then do loss.backward(). Then summing is mathematically correct. 1 or 2 or more GPUs, the gradient computed this way should be the same<br/><NewLine>– DistributedDataParallel: if using 2 separate losses for each GPU: loss1 = average over example in batch1, and loss2 = average over example in batch 2. To simulate loss = average over examples = (loss1+loss2)/2. You can loss1.backward(), loss2back.ward(), then average params gradient, that is equivalent to loss.backward()</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>According <a class=""mention"" href=""/u/geoffreychen777"">@GeoffreyChen777</a> 's answer, I think the learning rate should keep same, i.e. 0.01.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/teng-li; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/GeoffreyChen777; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/teng-li; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/GeoffreyChen777; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/teng-li; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/GeoffreyChen777; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Lausanne; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/GeoffreyChen777; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Lausanne; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/GeoffreyChen777; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/Lausanne; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/coincheung; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/Lausanne; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/coincheung; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/lugiavn; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/hhxx; <NewLine> ,"REPLY_DATE 1: January 9, 2019,  4:45pm; <NewLine> REPLY_DATE 2: January 9, 2019,  6:40pm; <NewLine> REPLY_DATE 3: January 9, 2019, 11:49pm; <NewLine> REPLY_DATE 4: January 17, 2019,  3:04am; <NewLine> REPLY_DATE 5: January 10, 2019,  1:29am; <NewLine> REPLY_DATE 6: January 10, 2019,  1:41am; <NewLine> REPLY_DATE 7: January 10, 2019,  1:51am; <NewLine> REPLY_DATE 8: January 16, 2019,  1:21pm; <NewLine> REPLY_DATE 9: January 17, 2019,  3:04am; <NewLine> REPLY_DATE 10: January 17, 2019,  2:44am; <NewLine> REPLY_DATE 11: January 17, 2019,  3:00am; <NewLine> REPLY_DATE 12: January 17, 2019,  3:02am; <NewLine> REPLY_DATE 13: February 1, 2019,  9:03am; <NewLine> REPLY_DATE 14: February 10, 2019,  2:56am; <NewLine> REPLY_DATE 15: February 13, 2019,  3:16am; <NewLine> REPLY_DATE 16: February 13, 2019,  3:45am; <NewLine> REPLY_DATE 17: March 8, 2020,  9:05pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 5 Likes; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> 
72293,Default collate_fn sending data to cuda:0,2020-03-06T14:09:53.691Z,2,319,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I used to have a single gpu, but since now I have two,<br/><NewLine>I tried to run my code in cuda:1, rather than cuda:0 which I normally use.</p><NewLine><p>However, I ran into the error of</p><NewLine><pre><code class=""lang-auto"">  File ""/Hard_3rd/harry/TOF_hj_0306/train/model_trainers/trainer_CU_MixRes_scale.py"", line 297, in _train_epoch<NewLine>    for step, data in data_loader:<NewLine>  File ""/home/user/anaconda3/envs/TOF/lib/python3.7/site-packages/tqdm/std.py"", line 1107, in __iter__<NewLine>    for obj in iterable:<NewLine>  File ""/home/user/anaconda3/envs/TOF/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 582, in __next__<NewLine>    return self._process_next_batch(batch)<NewLine>  File ""/home/user/anaconda3/envs/TOF/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 608, in _process_next_batch<NewLine>    raise batch.exc_type(batch.exc_msg)<NewLine>RuntimeError: Traceback (most recent call last):<NewLine>  File ""/home/user/anaconda3/envs/TOF/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py"", line 99, in _worker_loop<NewLine>    samples = collate_fn([dataset[i] for i in batch_indices])<NewLine>  File ""/home/user/anaconda3/envs/TOF/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py"", line 68, in default_collate<NewLine>    return [default_collate(samples) for samples in transposed]<NewLine>  File ""/home/user/anaconda3/envs/TOF/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py"", line 68, in &lt;listcomp&gt;<NewLine>    return [default_collate(samples) for samples in transposed]<NewLine>  File ""/home/user/anaconda3/envs/TOF/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py"", line 42, in default_collate<NewLine>    out = batch[0].new(storage)<NewLine>RuntimeError: Attempted to set the storage of a tensor on device ""cuda:1"" to a storage on different device ""cuda:0"".  This is no longer allowed; the devices must match.<NewLine></code></pre><NewLine><p>I guess the issues come from default <code>collate_fn</code> trying to send data to cuda:0, when it is already on cuda:1. How can I stop this from happening? Is there a way I can still use default <code>collate_fn</code> while running my code properly?</p><NewLine></div>",https://discuss.pytorch.org/u/Hyung_Jin_Chung,(Hyung Jin Chung),Hyung_Jin_Chung,"March 6, 2020,  2:09pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/vincentqb"">@vincentqb</a> for dataloader questions <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/vincentqb"">@vincentqb</a><br/><NewLine>Can I get some help here? <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you tried setting <code>CUDA_VISIBLE_DEVICES</code> env var before launching the process? It would be more clear if you share some minimum code snippet <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>As you mentioned, you can specify a custom <code>collate_fn</code>. Have you tried doing so? Can you provide a minimal code snippet that we could experiment to reproduce?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I didn’t realize I could do this, setting <code>CUDA_VISIBLE_DEVICES</code> to a single gpu. Thank you very much for your help!! <img alt="":grinning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/grinning.png?v=9"" title="":grinning:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Hyung_Jin_Chung; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/vincentqb; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Hyung_Jin_Chung; <NewLine> ,"REPLY_DATE 1: March 6, 2020,  3:48pm; <NewLine> REPLY_DATE 2: March 6, 2020,  6:50pm; <NewLine> REPLY_DATE 3: March 7, 2020,  1:00pm; <NewLine> REPLY_DATE 4: March 6, 2020, 10:13pm; <NewLine> REPLY_DATE 5: March 7, 2020,  1:01pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> 
72007,Loading of duplicated data in distributed train,2020-03-04T14:55:17.878Z,2,110,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I would like to pretrain BERT by using DDP.<br/><NewLine>I saved pretrain dataset(350GB of large corpus) as torch.tensor.</p><NewLine><p>When I run the code below, dataset is loaded in memory 8 times.<br/><NewLine>python -m torch.distributed.launch --nproc_per_node=8 train.py</p><NewLine><p>How can I prevent it?<br/><NewLine>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Innisfree,(Hyo Yong Jeong),Innisfree,"March 4, 2020,  2:55pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you store the complete dataset in a single tensor?<br/><NewLine>If so, I think you might need to load it once and store smaller chunks of the data (and load only certain chunks in each process) or load the data lazily from the beginning.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, I did<br/><NewLine>As you said, I stored smaller chunks of the data.</p><NewLine><p>Thanks for your reply.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I thought it is expected to have dedicated data loader in each process? So that 8 processes will have 8 dataloaders and 8 DDP instances?</p><NewLine><p>cc <a class=""mention"" href=""/u/vincentqb"">@vincentqb</a> please correct me if I am wrong.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Right, depending on the details in the code is organized, I would expect 8 processes/gpus getting different chunk of data as you said.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Innisfree; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/vincentqb; <NewLine> ,"REPLY_DATE 1: March 5, 2020, 12:08pm; <NewLine> REPLY_DATE 2: March 5, 2020, 12:07pm; <NewLine> REPLY_DATE 3: March 6, 2020,  4:26pm; <NewLine> REPLY_DATE 4: March 6, 2020, 10:25pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
71739,Send computation to a remote gpu,2020-03-02T16:52:35.745Z,3,517,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is there a way to define a remote GPU device inside our local code?</p><NewLine><p>For example:</p><NewLine><pre><code class=""lang-auto"">local_cpu = torch.device('cpu')<NewLine>remote_device = ... (?)<NewLine><NewLine>model = Model().to(remote_device)<NewLine>...<NewLine>inputs = inputs.to(remote_device)<NewLine>outputs = model(inputs)<NewLine>outputs = outputs.to(local_cpu)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/xerxex,,xerxex,"March 2, 2020,  4:52pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/xerxex"">@xerxex</a>, there is a <code>torch.distributed.rpc</code> package for this purpose. Please refer to the following docs:</p><NewLine><ol><NewLine><li>API doc: <a href=""https://pytorch.org/docs/master/rpc.html"" rel=""nofollow noopener"">https://pytorch.org/docs/master/rpc.html</a><NewLine></li><NewLine><li>Tutorial: <a href=""https://pytorch.org/tutorials/intermediate/rpc_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/rpc_tutorial.html</a><NewLine></li><NewLine></ol><NewLine><p>For now, we do not yet support creating remote device like <code>torch.device('worker1/cuda0')</code>, but this is on our roadmap and we plan to implement this as a sugar layer on top of RPC. Applications should be able to do the same thing using our raw RPC API.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>, thanks for the links. Unfortunately, I could not run the minimal example from the documentation. can you please correct me to run this?</p><NewLine><p>Let’s say I can not proceed without the return value so I need rpc_sync.<br/><NewLine>I created two python scripts. The first one is:</p><NewLine><pre><code class=""lang-auto""># On worker 0:<NewLine>import torch<NewLine>import torch.distributed.rpc as rpc<NewLine><NewLine>rpc.init_rpc(""worker0"", rank=0, world_size=2)<NewLine>ret = rpc.rpc_sync(""worker1"", torch.add, args=(torch.ones(2), 3))<NewLine>rpc.shutdown()<NewLine></code></pre><NewLine><p>and the second one is:</p><NewLine><pre><code class=""lang-auto""># On worker 1:<NewLine>import torch.distributed.rpc as rpc<NewLine>rpc.init_rpc(""worker1"", rank=1, world_size=2)<NewLine>rpc.shutdown()<NewLine></code></pre><NewLine><p>Then, when I execute the first script, I run into this error:</p><NewLine><pre><code class=""lang-auto"">  File ""process_0.py"", line 4, in &lt;module&gt;<NewLine>    rpc.init_rpc(""worker0"", rank=0, world_size=2)<NewLine>  File ""/data/anaconda3/lib/python3.7/site-packages/torch/distributed/rpc/__init__.py"", line 60, in init_rpc<NewLine>    init_method, rank=rank, world_size=world_size<NewLine>  File ""/data/anaconda3/lib/python3.7/site-packages/torch/distributed/rendezvous.py"", line 48, in rendezvous<NewLine>    raise RuntimeError(""`url` must be a string. {}: {}"".format(type(url), url))<NewLine>RuntimeError: `url` must be a string. &lt;class 'NoneType'&gt;: None<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you set the master address and port for Gloo ProcessGroup? Sth like:</p><NewLine><pre><code class=""lang-auto"">    os.environ['MASTER_ADDR'] = 'localhost'<NewLine>    os.environ['MASTER_PORT'] = '29500'<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group quote-modified"" data-post=""4"" data-topic=""71739"" data-username=""mrshenli""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/6f9a4e/40.png"" width=""20""/> mrshenli:</div><NewLine><blockquote><NewLine><p>os.environ[‘MASTER_ADDR’] = ‘localhost’ os.environ[‘MASTER_PORT’] = ‘29500’</p><NewLine></blockquote><NewLine></aside><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>, I added as you said, but still the same error.<br/><NewLine>The error comming from this line:</p><NewLine><pre><code class=""lang-auto"">rpc.init_rpc(""worker0"", rank=0, world_size=2)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/xerxex"">@xerxex</a>, which version of PyTorch are you using?</p><NewLine><pre><code class=""lang-auto"">  File ""/data/anaconda3/lib/python3.7/site-packages/torch/distributed/rpc/__init__.py"", line 60, in init_rpc<NewLine>    init_method, rank=rank, world_size=world_size<NewLine></code></pre><NewLine><p>Given this line above, it does seem to be <a href=""https://github.com/pytorch/pytorch/blob/v1.4.0/torch/distributed/rpc/__init__.py#L57-L67"" rel=""nofollow noopener"">v1.4</a> nor the current <a href=""https://github.com/pytorch/pytorch/blob/master/torch/distributed/rpc/__init__.py#L60-L77"" rel=""nofollow noopener"">master branch</a>.</p><NewLine><p>RPC is only available since v1.4.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>, my version is 1.4.0a0+a5272cb</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see, that’s the commit prior to the official v1.4.0 release, and that why the code from the error message looks different from v1.4.0.</p><NewLine><p>In <a href=""https://github.com/pytorch/pytorch/blob/a5272cb6433efe0bdaf76bff428ce961538daa25/torch/distributed/rpc/__init__.py#L60"" rel=""nofollow noopener"">the version you are using</a> (Nov 22nd, 2019), the <code>init_rpc</code> API takes an <code>init_method</code> arg, which you need to set. It is the same <code>init_method</code> as how you would call <code>init_process_group</code>.</p><NewLine><p>It will be easier if you switch to official v1.4 or the current master.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Honestly, we wouldn’t recommend using versions prior to v1.4.0, the API and behavior of RPC package are only officially announced as experimental in v1.4.0. So, even if you can get around <code>init_rpc</code> using your current PyTorch version by setting <code>init_method</code>, you might run into other issues later.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xerxex; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/xerxex; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/xerxex; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 3, 2020,  8:18pm; <NewLine> REPLY_DATE 2: March 4, 2020,  5:16pm; <NewLine> REPLY_DATE 3: March 4, 2020,  7:02pm; <NewLine> REPLY_DATE 4: March 4, 2020,  9:28pm; <NewLine> REPLY_DATE 5: March 6, 2020,  3:35pm; <NewLine> REPLY_DATE 6: March 6, 2020,  5:01pm; <NewLine> REPLY_DATE 7: March 6, 2020, 10:01pm; <NewLine> REPLY_DATE 8: March 6, 2020, 10:04pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
69322,Model Parallel Pipelining not working,2020-02-11T06:28:04.744Z,5,271,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I tried to reproduce the results by using the code provided in the tutorial on <a href=""https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html"" rel=""nofollow noopener"">Single Machine Model Parallel Best Practices</a>, however the results were a bit different.<br/><NewLine><img alt=""mp_vs_rn_vs_pp"" data-base62-sha1=""pyyw52k0hdIuvqoAyXWdyCmoL2Q"" height=""480"" src=""https://discuss.pytorch.org/uploads/default/original/3X/b/3/b31e5caa52b9c43a08ae2b05b14882d3730b8c4c.png"" width=""640""/><br/><NewLine>The model with pipelining is expected to perform better than than the rest two cases but it doesn’t. What can be the possible reasons for this?</p><NewLine></div>",https://discuss.pytorch.org/u/aniruddhadave,(Aniruddha Dave),aniruddhadave,"February 12, 2020, 12:52am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/aniruddhadave"">@aniruddhadave</a>, a lot of configurations could affect the performance, e.g., split size, hardware type, GPU interconnection bandwidth, model complexity, etc. And it does takes effort to get the best performance. One place to start with could be drawing the split size curve using your environment. I mean this figure blow. How does it look on your side?</p><NewLine><p><img alt=""image"" data-base62-sha1=""3l9xaTrBu887CYdnc45yQsVmLse"" height=""480"" src=""https://discuss.pytorch.org/uploads/default/original/3X/1/7/176aad2cccad5ce4d015797d0729dc44980d92f2.png"" width=""640""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a></p><NewLine><p>I also observed a similar result.</p><NewLine><p>This is the thread I created in the discussion.</p><NewLine><aside class=""quote quote-modified"" data-post=""1"" data-topic=""69380""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/vibhatha_abeykoon/40/17261_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/pytorch-model-parallel-best-practices-pipeline-stats/69380"">Pytorch Model Parallel Best Practices: Pipeline Stats</a> <a class=""badge-wrapper bullet"" href=""/c/distributed""><span class=""badge-category-bg"" style=""background-color: #0088CC;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""">distributed</span></a><NewLine></div><NewLine><blockquote><NewLine>    I am trying to replicate the model parallel best practices tutorial. <NewLine><a href=""https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html"" rel=""nofollow noopener"">Model Parallel Pytorch Docs</a> <NewLine>I use Tesla K80 GPUs for running the example. I didn’t plot graphs but I have the following stats. <NewLine>Single Node Time:  2.1659805027768018 <NewLine>Model Parallel Time:  2.23040875303559 <NewLine>Pipeline 20 Mean: 3.496733816713095 <NewLine>I don’t get the best results at this split size and it could be okay, depending on the hardware, software issues this can be possible. So I went for testing what is going on. Then I ran…<NewLine>  </blockquote><NewLine></aside><NewLine><p>I also benchmarked this using multiple configurations.</p><NewLine><p>I am not sure about the concurrent run of the code. So I changed it as follows and got a sort of fine result.</p><NewLine><pre><code class=""lang-python"">class PipelineParallelResNet50(ModelParallelResNet50):<NewLine>    def __init__(self, split_size=20, *args, **kwargs):<NewLine>        super(PipelineParallelResNet50, self).__init__(*args, **kwargs)<NewLine>        self.split_size = split_size<NewLine><NewLine>    def taskA(self, s_prev, ret):<NewLine>        s_prev = self.seq2(s_prev)<NewLine>        ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))<NewLine><NewLine>    def taskB(self, s_next):<NewLine>        s_prev = self.seq1(s_next).to('cuda:1')<NewLine>        return s_prev<NewLine><NewLine>    def forward(self, x):<NewLine>        splits = iter(x.split(self.split_size, dim=0))<NewLine>        s_next = next(splits)<NewLine>        s_prev = self.seq1(s_next).to('cuda:1')<NewLine>        ret = []<NewLine><NewLine>        for s_next in splits:<NewLine>            # A. s_prev runs on cuda:1<NewLine>            # self.taskA(s_prev=s_prev, ret=ret)<NewLine>            with concurrent.futures.ThreadPoolExecutor() as executor:<NewLine>                futureA = executor.submit(self.taskA, s_prev, ret)<NewLine>                futureA.result()<NewLine><NewLine>            # B. s_next runs on cuda:0, which can run concurrently with A<NewLine>            with concurrent.futures.ThreadPoolExecutor() as executor:<NewLine>                futureB = executor.submit(self.taskB, s_next)<NewLine>                s_prev = futureB.result()<NewLine><NewLine>        s_prev = self.seq2(s_prev)<NewLine>        ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))<NewLine><NewLine>        return torch.cat(ret)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/vibhatha_abeykoon"">@Vibhatha_Abeykoon</a></p><NewLine><p>Based on the numbers you posted in that thread, looks like you get the best performance with <code>split_size=60</code>, which gives you around 1.8s execution time, and it is a little faster than the 2.2 single node time?</p><NewLine><p>BTW are you using the same model as the tutorial?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, Thank you for the response. The split size was indeed the reason for the difference inperformance. On changing the split size I was able to reduce the time for pipelined model.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a></p><NewLine><p>I am exactly using the same code. I couldn’t replicate similar results.<br/><NewLine>Correct me if I have misunderstood the concept of running two micro-batches concurrently,</p><NewLine><pre><code class=""lang-python""> for s_next in splits:<NewLine>            # A. s_prev runs on cuda:1<NewLine>            s_prev = self.seq2(s_prev)<NewLine>            ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))<NewLine><NewLine>            # B. s_next runs on cuda:0, which can run concurrently with A<NewLine>            s_prev = self.seq1(s_next).to('cuda:1') <NewLine></code></pre><NewLine><p>Here part A, and part B as shown in the comments must run concurrently to get the pipeline performance. Am I correct/wrong?</p><NewLine><p>If so, as Pytorch eagerly executes the layers, it is not asynchronous? Am I following this right?</p><NewLine><p>If both these clauses are true, having threads is required? Isn’t it?</p><NewLine><p>My reasoning comes with the usage of the for loop. Here within the loop a concurrent execution could<br/><NewLine>happen if we use threads. Am I following this wrong?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""69322"" data-username=""Vibhatha_Abeykoon""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/vibhatha_abeykoon/40/17261_2.png"" width=""20""/> Vibhatha_Abeykoon:</div><NewLine><blockquote><NewLine><p>Here part A, and part B as shown in the comments must run concurrently to get the pipeline performance. Am I correct/wrong?</p><NewLine></blockquote><NewLine></aside><NewLine><p>No. Because CUDA operations run asynchronously from CPU’s point of view, unless you explicitly call <code>synchronize()</code> on CPU. And they are inserted into the same CUDA stream on each device in this example, which will guarantee ops on the same device run in order, but ops on different device can run in parallel.</p><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""69322"" data-username=""Vibhatha_Abeykoon""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/vibhatha_abeykoon/40/17261_2.png"" width=""20""/> Vibhatha_Abeykoon:</div><NewLine><blockquote><NewLine><p>If so, as Pytorch eagerly executes the layers, it is not asynchronous? Am I following this right?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, it launches the CUDA kernel right away, but the CUDA kernel execution can be asynchronous. And CUDA will actually queue the kernels in the stream and to coordinate the execution. So, “launch right away” does not mean it will wait for the kernel to finish, nor does it mean the the kernel will start on GPU immediately.</p><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""69322"" data-username=""Vibhatha_Abeykoon""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/vibhatha_abeykoon/40/17261_2.png"" width=""20""/> Vibhatha_Abeykoon:</div><NewLine><blockquote><NewLine><p>If both these clauses are true, having threads is required? Isn’t it?</p><NewLine></blockquote><NewLine></aside><NewLine><p>No, with multiple CUDA device + async CUDA kernel behavior, you could still get parallel execution without threads. And actually, even with thread, all Python code on CPU will still run in sequentially due to GIL.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a></p><NewLine><p>Yes, there is a part that GIL avoids. I was trying to use multiprocessing (torch version), it gave some memory issues. I understand your point.</p><NewLine><p>I was trying to run this on K80 GPUs. I didn’t get the graph as shown in the tutorial. Does old hardware could be an issue for not getting the expected graph?</p><NewLine><p>Could you share the script run command, I assumed it is just python scripy.py<br/><NewLine>do I have to use any specific flags or CUDA env variables that need to be set.</p><NewLine><p>I am just curious why the performance is not there as expected in the tutorial.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Could you share the script run command, I assumed it is just python scripy.py<br/><NewLine>do I have to use any specific flags or CUDA env variables that need to be set.</p><NewLine></blockquote><NewLine><p>I was actually using the exactly same script as shown in that tutorial. It is a <code>.rst</code> instead of a notebook because I don’t know what the hardware spec of our tutorial servers will be. I would like to avoid tuning parameters every time the underlying server hardware changes. I probably should have highlighted in the very beginning saying that the result will be different in different envs, and each env would require efforts to explore configuration space to get the best perf. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>I did that a while back, and my server env should be either this:</p><NewLine><pre><code class=""lang-auto"">+-----------------------------------------------------------------------------+<NewLine>| NVIDIA-SMI 396.69                 Driver Version: 396.69                    |<NewLine>|-------------------------------+----------------------+----------------------+<NewLine>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<NewLine>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<NewLine>|===============================+======================+======================|<NewLine>|   0  Tesla M40           On   | 00000000:0B:00.0 Off |                    0 |<NewLine>|  0%   27C    P8    18W / 250W |      0MiB / 11448MiB |      0%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   1  Tesla M40           On   | 00000000:0D:00.0 Off |                    0 |<NewLine>|  0%   25C    P8    18W / 250W |      0MiB / 11448MiB |      0%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>                                                                               <NewLine>+-----------------------------------------------------------------------------+<NewLine>| Processes:                                                       GPU Memory |<NewLine>|  GPU       PID   Type   Process name                             Usage      |<NewLine>|=============================================================================|<NewLine>|  No running processes found                                                 |<NewLine>+-----------------------------------------------------------------------------+<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">        GPU0    GPU1    CPU Affinity<NewLine>GPU0     X      PIX     0-11,24-35<NewLine>GPU1    PIX      X      0-11,24-35<NewLine></code></pre><NewLine><p>or this:</p><NewLine><pre><code class=""lang-auto"">+-----------------------------------------------------------------------------+<NewLine>| NVIDIA-SMI 418.116.00   Driver Version: 418.116.00   CUDA Version: 10.1     |<NewLine>|-------------------------------+----------------------+----------------------+<NewLine>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<NewLine>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<NewLine>|===============================+======================+======================|<NewLine>|   0  Quadro GP100        On   | 00000000:81:00.0 Off |                    0 |<NewLine>| 26%   32C    P0    29W / 235W |      1MiB / 16278MiB |      0%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>|   1  Quadro GP100        On   | 00000000:82:00.0 Off |                    0 |<NewLine>| 26%   33C    P0    30W / 235W |      1MiB / 16278MiB |      0%      Default |<NewLine>+-------------------------------+----------------------+----------------------+<NewLine>                                                                               <NewLine>+-----------------------------------------------------------------------------+<NewLine>| Processes:                                                       GPU Memory |<NewLine>|  GPU       PID   Type   Process name                             Usage      |<NewLine>|=============================================================================|<NewLine>|  No running processes found                                                 |<NewLine>+-----------------------------------------------------------------------------+<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">	GPU0	GPU1	CPU Affinity<NewLine>GPU0	 X 	NV4	12-23<NewLine>GPU1	NV4	 X 	12-23<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mrshenli"">@mrshenli</a></p><NewLine><p>Thanks a lot for finding the machine configs. I will see if I can run in modern hardware.</p><NewLine><p>For K80s that was the best I was able to produce. I did some micro-benchmarks as well to understand some of the bottlenecks. But for my experiments I got the optimum results close to end of the curve (end of the curve meaning the graphs +infinity direction).</p><NewLine><p>But I didn’t get a significant speed up from the code. That’s why I was trying different approaches.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Vibhatha_Abeykoon; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/aniruddhadave; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Vibhatha_Abeykoon; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Vibhatha_Abeykoon; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Vibhatha_Abeykoon; <NewLine> ,"REPLY_DATE 1: March 3, 2020,  8:26pm; <NewLine> REPLY_DATE 2: March 3, 2020, 10:53pm; <NewLine> REPLY_DATE 3: March 4, 2020,  7:34pm; <NewLine> REPLY_DATE 4: March 4, 2020, 11:04pm; <NewLine> REPLY_DATE 5: March 5, 2020,  2:27pm; <NewLine> REPLY_DATE 6: March 6, 2020,  3:46pm; <NewLine> REPLY_DATE 7: March 6, 2020,  3:56pm; <NewLine> REPLY_DATE 8: March 6, 2020,  4:19pm; <NewLine> REPLY_DATE 9: March 6, 2020,  4:40pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> 
72054,Does DDP tolerate heterogeneity?,2020-03-04T19:43:23.090Z,0,91,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m currently implementing a heterogeneity-aware distributed model.<br/><NewLine>My basic idea is to do all-reduce on a subset of the fast workers in the world group, however I noticed that in <code>torch.distributed.</code> <code>new_group</code>, it says:</p><NewLine><blockquote><NewLine><p>This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group.</p><NewLine></blockquote><NewLine><p>Does that mean that the fast workers should wait for the slow workers before they can move forward (i.e. the slower one will sync the faster one)?<br/><NewLine>If so, is there any other way to implement the model?</p><NewLine></div>",https://discuss.pytorch.org/u/Vikings,,Vikings,"March 5, 2020,  3:27am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Does that mean that the fast workers should wait for the slow workers before they can move forward (i.e. the slower one will sync the faster one)?</p><NewLine></blockquote><NewLine><p>No. It only requires all processes to call that function for rendezvous. After that, collective communications (e.g., allreduce) within a subgroup do not require non-member processes to join. So different subgroups can do allreduce independently.</p><NewLine><p>For implementation, you could create multiple DDP gangs, on the same model, with each gang spans different processes. But then, you will need to coordinate the communication because different DDP gangs will read from and write to the same set of <code>param.grad</code> field. Application needs to avoid the race there.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 6, 2020,  4:07pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
69380,Pytorch Model Parallel Best Practices: Pipeline Stats,2020-02-11T15:10:33.025Z,1,179,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to replicate the model parallel best practices tutorial.</p><NewLine><p><a href=""https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html"" rel=""nofollow noopener"">Model Parallel Pytorch Docs</a></p><NewLine><p>I use Tesla K80 GPUs for running the example. I didn’t plot graphs but I have the following stats.</p><NewLine><p>Single Node Time:  2.1659805027768018<br/><NewLine>Model Parallel Time:  2.23040875303559<br/><NewLine>Pipeline 20 Mean: 3.496733816713095</p><NewLine><p>I don’t get the best results at this split size and it could be okay, depending on the hardware, software issues this can be possible. So I went for testing what is going on. Then I ran the rest of the tutorial for different split sizes. These are the results I obtain for the given split sizes.</p><NewLine><p>Here is the graph I obtained,</p><NewLine><p><img alt=""resnet50_tutorial_v1"" data-base62-sha1=""hJda6IV0vdUj8gGnxCJYjPixpZZ"" height=""340"" src=""https://discuss.pytorch.org/uploads/default/original/3X/7/c/7c41530672a7150ea94ae4ea6224c93d7937d2df.png"" width=""605""/></p><NewLine><p>Stats for corresponding split sizes</p><NewLine><p>[11.667116858577355, 15.080700974399225, 3.556491438532248, 6.485900523653254, 2.4750063681043684, 4.956193731771782, 3.506740797869861, 1.6466765413060784, 1.5998394633177668]</p><NewLine><p>I don’t get a similar graph. Instead of having a local minima in the middle, I get the minimum value for split size 60.</p><NewLine><p>Then I investigated bit deeper by just logging the times for Forward prop, backward prop, label copy time and optimization time.</p><NewLine><p>I get something like this,</p><NewLine><p>MBi refers to i^th mini-batch<br/><NewLine>FW : forward time<br/><NewLine>BW: backward time<br/><NewLine>LBL_CP: label copy time<br/><NewLine>OPT : optimization time</p><NewLine><p><strong>For split size 20</strong></p><NewLine><p>MB-1: FW 0.12454676628112793, LBL_CP 0.5665407180786133, BW 0.25083422660827637, OPT 0.015613555908203125<br/><NewLine>MB-2: FW 0.31687474250793457, LBL_CP 0.5684511661529541, BW 0.26471543312072754, OPT 0.017733335494995117<br/><NewLine>MB-3: FW 0.3080329895019531, LBL_CP 0.571399450302124, BW 0.2626023292541504, OPT 0.018143177032470703</p><NewLine><p><em><strong>Split Size 1</strong></em></p><NewLine><p>MB1: FW 2.2466013431549072, LBL_CP 0.003688812255859375, BW 1.7002854347229004, OPT 0.0038182735443115234<br/><NewLine>MB2: FW 2.2562222480773926, LBL_CP 0.00019812583923339844, BW 1.6973598003387451, OPT 0.0039861202239990234<br/><NewLine>MB3: FW 2.2152814865112305, LBL_CP 0.0023992061614990234, BW 1.6881706714630127, OPT 0.004811525344848633</p><NewLine><p><em><strong>Split Size 3</strong></em></p><NewLine><p>MB1: FW 3.195209264755249, LBL_CP 0.7909142971038818, BW 0.9772884845733643, OPT 0.0038728713989257812<br/><NewLine>MB1: FW 3.122593402862549, LBL_CP 0.7815954685211182, BW 0.960608959197998, OPT 0.0037987232208251953<br/><NewLine>MB1: FW 3.2085180282592773, LBL_CP 0.7906265258789062, BW 0.9696476459503174, OPT 0.003855466842651367</p><NewLine><p><em><strong>Split Size 5</strong></em></p><NewLine><p>MB1: FW 0.5092735290527344, LBL_CP 0.003528594970703125, BW 0.6527245044708252, OPT 0.005049228668212891<br/><NewLine>MB2: FW 0.44788599014282227, LBL_CP 0.0061757564544677734, BW 0.6450486183166504, OPT 0.003782510757446289<br/><NewLine>MB3: FW 0.514885425567627, LBL_CP 0.003251314163208008, BW 0.6562778949737549, OPT 0.004816293716430664</p><NewLine><p>The label copy time fluctuates a lot, but It always copy the same size of array chunk, isn’ it?</p><NewLine><p>In addition the FW time also fluctuates in an unexpected way. I am trying to profile this and see what happens.</p><NewLine><p>But, I would like to get an insight, why this could be happening?<br/><NewLine>(Does this has something to do with NVLink?)</p><NewLine></div>",https://discuss.pytorch.org/u/Vibhatha_Abeykoon,(Vibhatha Abeykoon),Vibhatha_Abeykoon,"February 11, 2020,  3:10pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am facing a similar issue while trying to replicate the tutorial. In my case the pipelining time is higher than the model-parallel time which is not expected.<br/><NewLine><img alt=""mp_vs_rn_vs_pp"" data-base62-sha1=""pyyw52k0hdIuvqoAyXWdyCmoL2Q"" height=""480"" src=""https://discuss.pytorch.org/uploads/default/original/3X/b/3/b31e5caa52b9c43a08ae2b05b14882d3730b8c4c.png"" width=""640""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I also got the same problem. I got a similar graph like this. So I further continued.<br/><NewLine>It is not clear why it is happening?</p><NewLine><p>I am currently micro-benchmarking this. I still have no clear idea.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/vibhatha_abeykoon"">@Vibhatha_Abeykoon</a>, how did you log the time for fw, bw and opt? Do you use CUDA events and then call <code>elapsed_time</code>?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/aniruddhadave; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Vibhatha_Abeykoon; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: February 14, 2020,  1:16am; <NewLine> REPLY_DATE 2: February 15, 2020,  4:09am; <NewLine> REPLY_DATE 3: March 4, 2020,  7:23pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
72012,BatchNorm hangs with save and load state_dict while training with multi-processes,2020-03-04T15:12:02.051Z,0,70,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>In my implementation, I train the model by multiple processes and save its state dictionary by a concurrent process to evaluate/test it after the training is complete. During evaluation, I load the saved state dictionaries. I need to do this in order to compute some more info from my distributed/concurrent training algorithm. It also helps me in un-engaging the processors from test-computation-load while training.</p><NewLine><p>This implementation works perfectly for all the models that I worked with when it is done on a CPU. On a GPU, when the number of training processes is one, again it is fine for each of those models. Furthermore, it also works fine if there is no BatchNorm in the model and I train it using multiple processes over a GPU.</p><NewLine><p>However, with BatchNorm and multiple processes training on a GPU,  when I do a forward pass later during evaluation it hangs at Conv2d (it might be at some other place further inside, however, I could pin in my debugger up to Conv2d).</p><NewLine><p>What does exactly happen with the BatchNorm+multiprocessing combination?</p><NewLine></div>",https://discuss.pytorch.org/u/bapi,(Bapi Chatterjee),bapi,"March 4, 2020,  3:12pm",,,,,
71787,Can we compute nn.functional.linear with torch.chunks of data,2020-03-03T00:19:35.460Z,0,117,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I want to split my input and weight tensors to x number of torch.chunks, and then be able to evaluate the chunk-wise nn.functional.linear or nn.functional.conv_2d in parallel without having to use a for loop. Can I know if this possible in pytorch?</p><NewLine></div>",https://discuss.pytorch.org/u/Sai_Kiran,(Sai Kiran),Sai_Kiran,"March 3, 2020, 12:19am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/sai_kiran"">@Sai_Kiran</a>, are you referring to Mesh-TensorFlow-like parallelism? We currently don’t have the API for it yet. But the split, scatter, parallel_apply, and gather can be done in the application side.</p><NewLine><p>With parallel_apply, there won’t be a loop in the application code, but it internally uses a loop to launch multiple threads to process inputs in parallel. <a href=""https://github.com/pytorch/pytorch/blob/f62a0060972d594cc1c4ab99d44267373eee4ec6/torch/nn/parallel/data_parallel.py#L138-L153"" rel=""nofollow noopener"">Here</a> is an example usage of parallel_apply.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 3, 2020,  9:08pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
69416,DeepSpeed installation,2020-02-11T20:35:08.387Z,0,213,"<div class=""post"" itemprop=""articleBody""><NewLine><p>can someone help me installation of microsoft deepspeed library  for pytorch</p><NewLine></div>",https://discuss.pytorch.org/u/murali_perumalla,(murali perumalla),murali_perumalla,"February 11, 2020,  8:35pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you encounter any issue when installing that library? DeepSpeed authors would know better about the process. Will you consider routing this question to the <a href=""https://github.com/microsoft/DeepSpeed"" rel=""nofollow noopener"">DeepSpeed repo</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 3, 2020,  8:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
69315,Multiple Processes Per GPU?,2020-02-11T05:44:55.726Z,0,187,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am training a model that does not make full use of the GPU’s compute and memory. Training is carried out over two 2080ti GPUs using Distributed DataParallel.</p><NewLine><p>How can we concurrently train 2 models per GPU (each using different parameters), so that we can more fully utilize the GPs?</p><NewLine><p>The following code currently trains only 1 model across 2 GPUs.</p><NewLine><pre><code class=""lang-auto"">import torch.multiprocessing<NewLine>import torch.distributed<NewLine>import torch.nn as nn<NewLine><NewLine>def train(gpu, args):<NewLine>    distributed.init_process_group(<NewLine>        backend='nccl',<NewLine>        init_method='env://',<NewLine>        world_size=args['world_size'],<NewLine>        rank=args['nr']*args['gpu']+gpu<NewLine><NewLine>    ...<NewLine><NewLine>    torch.cuda.set_device(gpu)<NewLine>    model.cuda(gpu)<NewLine>    model = nn.parallel.DistributedDataParallel(model, device_ids=[gpu])<NewLine><NewLine>    # training loop<NewLine>    for epoch in range(num_epochs):<NewLine>        ...<NewLine><NewLine>if __name__ == '__main__':<NewLine>    multiprocessing.spawn(train, nprocs=2, args=(args,))<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/nyxynyx,,nyxynyx,"February 11, 2020,  5:45am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>One possibility is</p><NewLine><ol><NewLine><li>use the <a href=""https://github.com/pytorch/pytorch/blob/45c45195cd6526fbb64773abd316a24bbe49f50d/torch/distributed/distributed_c10d.py#L1495"" rel=""nofollow noopener""><code>new_group</code></a> API in <code>torch.distributed</code> to create a different process group for two different models,</li><NewLine><li>Create different DistributedDataParallel instances, one for each wrapper and pass the process group object explicitly to DistributedDataParallel constructor (<a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L187"" rel=""nofollow noopener"">process_group arg</a>) instead of using the default one.</li><NewLine></ol><NewLine><p>In this way, DistributedDataParallel’s allreduce operations will not collide.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 3, 2020,  8:31pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
70354,Multi-machine inference with PyTorch,2020-02-19T17:03:40.457Z,3,272,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m new to distributed computation on PyTorch.<br/><NewLine>I’m interested in perform a network partitioning so one piece of the network will run on the machine A and the other piece of the network will run on the machine B. The first thing I need to do is to send tensors from machine A to machine B.<br/><NewLine>So I thought about use the point-to-point communication as in <a href=""https://pytorch.org/tutorials/intermediate/dist_tuto.html"" rel=""nofollow noopener"">Writing Distributed Applications with PyTorch</a>. I’m trying to adapt this code to send messages between the machines A and B but I have not been well succeed. Can anyone explain the whole pipeline for this?<br/><NewLine>Any help would be appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/Luan_Goncalves,(Luan Gonçalves),Luan_Goncalves,"February 19, 2020,  5:03pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you want to use model sharding, <a href=""https://discuss.pytorch.org/t/split-single-model-in-multiple-gpus/13239/2"">this simple example</a> might be useful.<br/><NewLine>The linked tutorial explains a distributed setup, so let me know, if I misunderstood your use case.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>First of all, thanks for your attention.<br/><NewLine>It is not exactly what I need but it helped me in another point. So thanks again.<br/><NewLine>My issue is related to edge computing. Basically I need to run just a couple of layers in a Drone and the other layers will run in a machine equipped with a GPU.<br/><NewLine>So i thought that i could send messages from Drone to my machine.<br/><NewLine>It is possible to be made with PyTorch?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s a really interesting use case, but I’m not really sure, how well this would work.<br/><NewLine>You could most likely connect the drone and your workstation to the same network and use DDP indeed.<br/><NewLine>However, have you thought about the latency this would create?<br/><NewLine>How long can you wait for the response?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes! You are right.<br/><NewLine>Actually, I am interested on measuring this kind of problems because my research is concerned on 5G systems.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think what you may be looking for is our Distributed RPC framework (<a href=""https://pytorch.org/tutorials/intermediate/rpc_tutorial.html?highlight=rpc"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/rpc_tutorial.html?highlight=rpc</a>), which allows you to send messages and tensors between workers. Also see the Distributed Autograd Framework (<a href=""https://pytorch.org/docs/master/rpc.html#distributed-autograd-framework"" rel=""nofollow noopener"">https://pytorch.org/docs/master/rpc.html#distributed-autograd-framework</a>) for training models that are partitioned across machines. Lastly, here is an example of training an RNN using RPC/Distributed Autograd: <a href=""https://github.com/pytorch/examples/tree/master/distributed/rpc/rnn"" rel=""nofollow noopener"">https://github.com/pytorch/examples/tree/master/distributed/rpc/rnn</a>.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/osalpekar"">@osalpekar</a>! It helped me a lot.<br/><NewLine>Actually, I want to thank you both for your attention.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Luan_Goncalves; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Luan_Goncalves; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/osalpekar; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Luan_Goncalves; <NewLine> ,"REPLY_DATE 1: February 20, 2020,  2:28am; <NewLine> REPLY_DATE 2: February 20, 2020, 12:24pm; <NewLine> REPLY_DATE 3: February 21, 2020,  6:19am; <NewLine> REPLY_DATE 4: February 27, 2020, 12:47pm; <NewLine> REPLY_DATE 5: February 27, 2020,  6:29pm; <NewLine> REPLY_DATE 6: March 2, 2020, 12:15pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> 
70767,Performance degradation when GPU io and compute are parallel,2020-02-23T13:45:16.394Z,2,158,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using distributeddataparallel for multi-GPU training using pytorch, and a process uses one GPU.<br/><NewLine>When I follow the normal formular in pytorch, the inference time is OK, like this.</p><NewLine><pre><code class=""lang-auto"">x,y = next(train_loader)<NewLine>x = x.cuda(rank)<NewLine>y = y.cuda(rank)<NewLine>t0 = time.time()<NewLine>y1 = model(x)<NewLine>torch.cuda.synchronize()<NewLine>inference_time = time.time()-t0<NewLine></code></pre><NewLine><p>But when I get the data from another thread, which always read data from train_loader and input it to a queue. The code is as following.</p><NewLine><pre><code class=""lang-auto"">args.data_queue=queue.Queue()<NewLine>def load_data_queue(rank, dataloader, args):<NewLine>    n = 0<NewLine>    while True:<NewLine>        try:<NewLine>            x,y = next(dataloader)<NewLine>            x = x.cuda(rank)<NewLine>            y = y.cuda(rank)<NewLine>            args.data_queue.put([feature, label])<NewLine>        except StopIteration:<NewLine>            print('load queue quits normally')<NewLine>            return<NewLine>...<NewLine>t = threading.Thread(target=load_data_queue, args=(<NewLine>        rank, train_loader, args), daemon=True)<NewLine>t.start()<NewLine>...<NewLine>x,y = args.data_queue.get()<NewLine>t0 = time.time()<NewLine>y1 = model(x)<NewLine>#torch.cuda.synchronize()<NewLine>inference_time = time.time()-t0<NewLine></code></pre><NewLine><p>The inference_time will increase a lot.<br/><NewLine>To my understanding, GPU i/o should not influence GPU compute. What is causing this phenomenon？</p><NewLine></div>",https://discuss.pytorch.org/u/miyano,(chen xukun),miyano,"February 24, 2020,  1:35am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You might run into the GIL in Python.<br/><NewLine>Wouldn’t the first DDP approach work or what are the shortcomings you are facing that need the second approach?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply. We are doing some research which acquires GPU i/o and compute to run in parallel. And when we validated our idea, we observed this problem.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks！I change thread parallel to process parallel，and the train performance becomes normal.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/miyano; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/miyano; <NewLine> ,"REPLY_DATE 1: February 24, 2020,  2:55am; <NewLine> REPLY_DATE 2: February 24, 2020,  8:59am; <NewLine> REPLY_DATE 3: March 2, 2020, 10:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
63129,Computation graph optimization during training,2019-12-05T12:43:17.485Z,6,385,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, is it possible or neccessary to optimize the dynamic computation graph generated during training for a higher throughput?If it is, then what is the recommended way to achieve that? Thanks in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/Dale_Song,(Dale Song),Dale_Song,"December 5, 2019, 12:43pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>This is not necessary in general.<br/><NewLine>If you really want to try and get the best, you should use torchscript model with cpp inference to strip away the python interpreter.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for the reply. But my use case is to improve the training throughput, if I understand it right, torchscript can only improve performance for network inference rather than training(forward&amp;backward).And do you have any advice about how to improve pytorch forward&amp;backward efficiency?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can actually perform training with torchscript.<br/><NewLine>You can try to torchscript your python code during training.</p><NewLine><p>That being said, if your network is a regular architecture, we try to make sure that the performance for these are as good as possible out of the box.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Dale,have you slove this problem, and TorchScript works?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""63129"" data-username=""albanD""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/alband/40/215_2.png"" width=""20""/> albanD:</div><NewLine><blockquote><NewLine><p>torchscript</p><NewLine></blockquote><NewLine></aside><NewLine><p>Did torchscript can actually optimize computation graph during traing?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, touchscript does optimize the graph at train time. See :<br/><NewLine><a href=""https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/#writing-custom-rnns"" rel=""nofollow noopener"">https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/#writing-custom-rnns</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Dale_Song; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/111254; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/111254; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/G.M; <NewLine> ,"REPLY_DATE 1: December 5, 2019,  3:27pm; <NewLine> REPLY_DATE 2: December 6, 2019,  1:44am; <NewLine> REPLY_DATE 3: December 6, 2019,  3:50pm; <NewLine> REPLY_DATE 4: March 2, 2020,  9:18am; <NewLine> REPLY_DATE 5: March 2, 2020, 10:08am; <NewLine> REPLY_DATE 6: March 2, 2020, 10:36am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
71356,Torch Distributed Class Definitions,2020-02-27T21:38:57.218Z,0,180,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to locate ReduceOp definitions, referred in the following comment in the source.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/243af17d65fb147901bf9efef21b0cb3e4f25fee/torch/distributed/distributed_c10d.py#L92"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/243af17d65fb147901bf9efef21b0cb3e4f25fee/torch/distributed/distributed_c10d.py#L92"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/243af17d65fb147901bf9efef21b0cb3e4f25fee/torch/distributed/distributed_c10d.py#L92</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""82"" style=""counter-reset: li-counter 81 ;""><NewLine><li># TODO: remove them when users are ready to take a hard dependency on PyTorch 1.</li><NewLine><li>_backend = Backend.UNDEFINED</li><NewLine><li>dist_backend = Backend</li><NewLine><li><NewLine></li><NewLine><li><NewLine></li><NewLine><li>class reduce_op(object):</li><NewLine><li>    r""""""</li><NewLine><li>    Deprecated enum-like class for reduction operations: ``SUM``, ``PRODUCT``,</li><NewLine><li>    ``MIN``, and ``MAX``.</li><NewLine><li><NewLine></li><NewLine><li class=""selected"">    :class:`~torch.distributed.ReduceOp` is recommended to use instead.</li><NewLine><li>    """"""</li><NewLine><li><NewLine></li><NewLine><li>    def __init__(self):</li><NewLine><li>        # __members__ is a dict storing key-value pairs for enum classes</li><NewLine><li>        for k, v in ReduceOp.__members__.items():</li><NewLine><li>            setattr(self, k, v)</li><NewLine><li>        self.__members__ = ReduceOp.__members__</li><NewLine><li><NewLine></li><NewLine><li>    def __getattribute__(self, key):</li><NewLine><li>        warnings.warn(""torch.distributed.reduce_op is deprecated, please use ""</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>(I did search the git source and local installation both)</p><NewLine><p>I couldn’t find the class definition. Am I missing something here?</p><NewLine></div>",https://discuss.pytorch.org/u/Vibhatha_Abeykoon,(Vibhatha Abeykoon),Vibhatha_Abeykoon,"February 27, 2020,  9:38pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>ReduceOp is a C++ enum, and is exposed to the python interface using pybind (<a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/init.cpp#L145"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/init.cpp#L145</a>). That enum is defined here: <a href=""https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/Types.hpp#L8"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/Types.hpp#L8</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/osalpekar; <NewLine> ,"REPLY_DATE 1: February 27, 2020, 10:58pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
71043,Total number of processes and threads created using nn.distributed.parallel,2020-02-25T19:24:15.475Z,5,575,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Under to the context of training using python front end.</p><NewLine><p>Where could I find some information about the total number of processes and threads when using nn.distributed.parallel module ?</p><NewLine><p>If I have a simple neural network (eg. MNIST) and I do distributed data parallelism where I assign 1 process per GPU, and I have both training and eval going on and a dataloader with 1 worker, should I have only 3 processes per GPU: 1 main process (the training one) that spawns eval process and dataloader process (total of 3 processes). Then within the main process: a thread for scheduling work, a thread for forward, a thread for backward, a thread to deal with eval process, a thread to deal with dataloader, a thread for cache manager. That is 6 threads. When profiling I get to see several mode. Is there any document where I can get that info ? Also if BWD is consuming what FWD is producing, is there a way I can “merge” both threads of FWD and BWD in a single thread ? Is there also a way to not dealloc objects from the cache allocator if the number of objects (tensors, model) remains the same from iteration to iteration, so I can avoid the expensive mmap/munmap ?</p><NewLine><p>Thanks in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/joshua_mora,(Joshua Mora),joshua_mora,"February 25, 2020,  7:25pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>In terms of the total number of processes, the <code>num_workers</code> argument you pass to the <code>DataLoader</code> class determines the number of subprocess it uses (0 means the dataloader uses the main process). Here is some documentation: <a href=""https://pytorch.org/docs/1.1.0/_modules/torch/utils/data/dataloader.html"" rel=""nofollow noopener"">https://pytorch.org/docs/1.1.0/_modules/torch/utils/data/dataloader.html</a>.</p><NewLine><p>For the number of threads, this varies based on the communication backend you use (which is passed to <code>init_process_group</code>). For example the gloo backend uses 2 threads per device: <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/init.cpp#L565"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/init.cpp#L565</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the quick reply.</p><NewLine><p>I am using NCCL backend. I do not see how many threads are being passed to create the group. at <a href=""https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/ProcessGroupNCCL.cpp"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/ProcessGroupNCCL.cpp</a><br/><NewLine>Is there a knob to control that ?</p><NewLine><p>I am using dataloder with num_workers set to 1 (so main process spawns a separate process ?)<br/><NewLine>Based on the information on  <a href=""https://pytorch.org/docs/1.1.0/_modules/torch/utils/data/dataloader.html"" rel=""nofollow noopener"">https://pytorch.org/docs/1.1.0/_modules/torch/utils/data/dataloader.html</a>.<br/><NewLine>At each iteration the dataloader process is created and destroyed (if num_workers!=0) which has some overhead ?<br/><NewLine>Can we keep the processes (depending on how many samples within the batch you want to work concurrently) across iterations so we do not incur into that overhead ?</p><NewLine><p>I am basically trying to prune the number of processes and threads, while I understand I may restrict generality but I am trying to speed up the execution when I am CPU bound.</p><NewLine><p>Thanks in advance.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""3"" data-topic=""71043"" data-username=""joshua_mora""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/j/e19b73/40.png"" width=""20""/> joshua_mora:</div><NewLine><blockquote><NewLine><p>I am using NCCL backend. I do not see how many threads are being passed to create the group. at <a href=""https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/ProcessGroupNCCL.cpp"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/ProcessGroupNCCL.cpp</a><br/><NewLine>Is there a knob to control that ?</p><NewLine></blockquote><NewLine></aside><NewLine><p>The number of threads is currently not tunable by the user, but we’re considering making this possible in a future release.</p><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""3"" data-topic=""71043"" data-username=""joshua_mora""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/j/e19b73/40.png"" width=""20""/> joshua_mora:</div><NewLine><blockquote><NewLine><p>I am using dataloder with num_workers set to 1 (so main process spawns a separate process ?)<br/><NewLine>Based on the information on  <a href=""https://pytorch.org/docs/1.1.0/_modules/torch/utils/data/dataloader.html"" rel=""nofollow noopener"">https://pytorch.org/docs/1.1.0/_modules/torch/utils/data/dataloader.html</a>.<br/><NewLine>At each iteration the dataloader process is created and destroyed (if num_workers!=0) which has some overhead ?<br/><NewLine>Can we keep the processes (depending on how many samples within the batch you want to work concurrently) across iterations so we do not incur into that overhead ?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Right, <code>num_workers=1</code> would spawn a separate process. Here’s an issue tracking the discussion around keeping subprocesses alive across iterations (with a patch that should make this possible):<br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/15849"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/15849"" rel=""nofollow noopener"" target=""_blank"">DataLoader with option to re-use worker processes</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-01-09"" data-format=""ll"" data-time=""00:21:24"" data-timezone=""UTC"">12:21AM - 09 Jan 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/dashesy"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""dashesy"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/873905?v=4"" width=""20""/><NewLine>          dashesy<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">🚀 Feature<NewLine>Currently after an epoch is ended dataloader spawns a new process to read data. This means if processes have cached...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">feature</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">high priority</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">module: dataloader</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the pointer to this discussion on the dataloader.<br/><NewLine>With respect to the number of threads/processes, I still miss to understand all the other threads being generated.<br/><NewLine>Is it possible for example to use same thread for backward and forward if they deal with same model and batch, instead of having 2 threads that could assume using different models and samples ?<br/><NewLine>NCCL process group also accepts the option of size which I am not sure it refers also to the number of threads.<br/><NewLine>Are there any other hardcoded number of threads that I could reduce ? ( set_num_threads(1)/set_num_interop_threads(1) will not prevent from creating a bunch of threads, larger than 6 per process that deals with each GPU). I have very few cores available per GPU (~4) so I need to restrict the number of threads to what is necessary.</p><NewLine><p>Thanks, again.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""5"" data-topic=""71043"" data-username=""joshua_mora""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/j/e19b73/40.png"" width=""20""/> joshua_mora:</div><NewLine><blockquote><NewLine><p>Is it possible for example to use same thread for backward and forward if they deal with same model and batch, instead of having 2 threads that could assume using different models and samples ?</p><NewLine></blockquote><NewLine></aside><NewLine><p>I’m not sure of any way to coerce forward and backward into using the same thread.</p><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""5"" data-topic=""71043"" data-username=""joshua_mora""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/j/e19b73/40.png"" width=""20""/> joshua_mora:</div><NewLine><blockquote><NewLine><p>NCCL process group also accepts the option of size which I am not sure it refers also to the number of threads.</p><NewLine></blockquote><NewLine></aside><NewLine><p>That size actually refers to world_size, which is the total number of ranks in your job</p><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""5"" data-topic=""71043"" data-username=""joshua_mora""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/j/e19b73/40.png"" width=""20""/> joshua_mora:</div><NewLine><blockquote><NewLine><p>Are there any other hardcoded number of threads that I could reduce ? ( set_num_threads(1)/set_num_interop_threads(1) will not prevent from creating a bunch of threads, larger than 6 per process that deals with each GPU). I have very few cores available per GPU (~4) so I need to restrict the number of threads to what is necessary.</p><NewLine></blockquote><NewLine></aside><NewLine><p>This might provide some more insight into tuning the number of threads: <a href=""https://github.com/pytorch/pytorch/issues/16894"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/16894</a>. For example, the OMP_NUM_THREADS env var is used for controlling the number of OpenMP threads for CPU operations and MKL_NUM_THREADS for mkl.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/osalpekar"">@osalpekar</a>.<br/><NewLine>OpenMP environment variables dont make a difference if torch.set_num_threads is already set.<br/><NewLine>In fact I use GOMP_CPU_AFFINITY to enforce a particular set of openMP threads to run on specific cores. I did play also with OMP/MKL_DYNAMIC set to false.<br/><NewLine>I still do not understand though what torch.set_num_threads controls if I end up having 1 thread for FWD and 1 different thread for BWD. And several other threads.<br/><NewLine>I may think that there is a total amount of work and some “empirical” definition of how many threads to have for certain amount of work. the env var will overwrite that heuristic.<br/><NewLine>Just trying to find the code where that is precribed/defined.<br/><NewLine>Are you aware of a document where it is described the architecture in terms of thread/functionality ?<br/><NewLine>Why would I get &gt; 6 threads if I have torch.set_num_threads set to 1.</p><NewLine><p>Regards.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/osalpekar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/joshua_mora; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/osalpekar; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/joshua_mora; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/osalpekar; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/joshua_mora; <NewLine> ,"REPLY_DATE 1: February 25, 2020,  9:44pm; <NewLine> REPLY_DATE 2: February 25, 2020, 10:24pm; <NewLine> REPLY_DATE 3: February 25, 2020, 11:15pm; <NewLine> REPLY_DATE 4: February 26, 2020, 12:47am; <NewLine> REPLY_DATE 5: February 26, 2020,  9:53pm; <NewLine> REPLY_DATE 6: February 26, 2020, 11:13pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
47556,DataParallel doesn&rsquo;t work when calling model.module.some_attribute,2019-06-10T13:59:55.444Z,4,1874,"<div class=""post"" itemprop=""articleBody""><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/6cdde65ed7679ebfb8630f288f6dd57209fddc85"" href=""https://discuss.pytorch.org/uploads/default/original/2X/6/6cdde65ed7679ebfb8630f288f6dd57209fddc85.jpeg"" title=""Screenshot 2019-06-10 at 7.18.47 PM.jpg""><img alt=""47%20PM"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/6/6cdde65ed7679ebfb8630f288f6dd57209fddc85_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/6/6cdde65ed7679ebfb8630f288f6dd57209fddc85_2_680x500.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/6/6cdde65ed7679ebfb8630f288f6dd57209fddc85_2_680x500.jpeg, https://discuss.pytorch.org/uploads/default/optimized/2X/6/6cdde65ed7679ebfb8630f288f6dd57209fddc85_2_1020x750.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/6/6cdde65ed7679ebfb8630f288f6dd57209fddc85_2_1360x1000.jpeg 2x"" width=""680""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screenshot 2019-06-10 at 7.18.47 PM.jpg</span><span class=""informations"">1776×1304 281 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/0fa2b6a278f7ceeb6b2889d18c5ef1aa0d07e2e2"" href=""https://discuss.pytorch.org/uploads/default/original/2X/0/0fa2b6a278f7ceeb6b2889d18c5ef1aa0d07e2e2.jpeg"" title=""Screenshot 2019-06-10 at 7.19.26 PM.jpg""><img alt=""26%20PM"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/0/0fa2b6a278f7ceeb6b2889d18c5ef1aa0d07e2e2_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/0/0fa2b6a278f7ceeb6b2889d18c5ef1aa0d07e2e2_2_677x500.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/0/0fa2b6a278f7ceeb6b2889d18c5ef1aa0d07e2e2_2_677x500.jpeg, https://discuss.pytorch.org/uploads/default/optimized/2X/0/0fa2b6a278f7ceeb6b2889d18c5ef1aa0d07e2e2_2_1015x750.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/0/0fa2b6a278f7ceeb6b2889d18c5ef1aa0d07e2e2_2_1354x1000.jpeg 2x"" width=""677""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screenshot 2019-06-10 at 7.19.26 PM.jpg</span><span class=""informations"">1800×1328 327 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine>So in the two images there are two different models <code>model</code> and <code>model_p</code> both being wrapped under <code>nn.DataParallel</code>. But in <code>model</code> when calling some attribute <code>fit</code> using the <code>model.module</code> method, I’m unable utilize the two GPUs I originally wanted to parallelize my model upon.  i.e <code>model</code> doesn’t split the dim=0 batch_first dimension into two equal halves for putting it onto two devices as can be seen from the print statements.<br/><NewLine>Ps. I am very new to using DataParallel and wanted to use something like this. i.e What I actually want is, to call <code>model.module.fit</code> in my training loop with the args as the inputs from my dataloader and in this fit attribute ultimately will makes a call to the forward method of the class model.</p><NewLine><p>But this whole thing doesn’t seem to parallelize and utilize the two GPUs which the model_p could without any fit function and a direct call to forward internally.</p><NewLine><p>I’ve added the <a href=""https://drive.google.com/open?id=1yyipI2Mnud8X2Gc8jgoEj3V1v-AOJPIi"" rel=""nofollow noopener"">link</a> to the notebook which was run with CUDA_VISIBLE_DEVICES=0,1</p><NewLine><p>What should I change?<br/><NewLine>Thanks!</p><NewLine><pre><code class=""lang-auto"">class Model(nn.Module):<NewLine>    # Our model<NewLine><NewLine><NewLine>    def __init__(self, input_size, output_size):<NewLine>        super(Model, self).__init__()<NewLine>        self.fc = nn.Linear(input_size, output_size)<NewLine><NewLine>    def forward(self, input):<NewLine>        output = self.fc(input)<NewLine>        return output<NewLine>    <NewLine>    def fit(self, input):<NewLine>        output = self.forward(input)<NewLine>        print(""\tIn Model: input size"", input.size(),<NewLine>              ""output size"", output.size())<NewLine>        return output<NewLine><NewLine>model = Model(input_size, output_size)<NewLine>if torch.cuda.device_count() &gt; 1:<NewLine>  print(""Let's use"", torch.cuda.device_count(), ""GPUs!"")<NewLine>  # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs<NewLine>  model = nn.DataParallel(model)<NewLine><NewLine>model.to(device)<NewLine><NewLine>for data in rand_loader:<NewLine>    input = data.to(device)<NewLine>    output = model.module.fit(input)<NewLine>    print(""Outside: input size"", input.size(),""output_size"", output.size())<NewLine><NewLine>#############################CASE 2############################<NewLine>class ModelParallel(nn.Module):<NewLine>    # Our model<NewLine><NewLine>    def __init__(self, input_size, output_size):<NewLine>        super(ModelParallel, self).__init__()<NewLine>        self.fc = nn.Linear(input_size, output_size)<NewLine><NewLine>    def forward(self, input):<NewLine>        output = self.fc(input)<NewLine>        print(""\tIn Model: input size"", input.size(),<NewLine>              ""output size"", output.size())<NewLine><NewLine>        return output<NewLine>    <NewLine>model_p = ModelParallel(input_size, output_size)<NewLine>if torch.cuda.device_count() &gt; 1:<NewLine>  print(""Let's use"", torch.cuda.device_count(), ""GPUs!"")<NewLine>  # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs<NewLine>  model_p = nn.DataParallel(model_p)<NewLine>#   model.module.fit = nn.DataParallel(model.module.fit)<NewLine><NewLine>model_p.to(device)<NewLine><NewLine>for data in rand_loader:<NewLine>    input = data.to(device)<NewLine>    output = model_p(input)<NewLine>    print(""Outside: input size"", input.size(),""output_size"", output.size())<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/gollum,(Anirudh Dagar),gollum,"June 10, 2019,  2:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>DataParallel splits GPUs using its custom forward function and is implemented as a  wrapper rather than a subclass which overrides the model’s <code>forward</code>. When you’re calling fit, you’re calling the <code>forward()</code> associated with the model and not the one wrapped around <code>DataParallel</code>. Hence it will only use a single gpu, as the scatter gather in <code>DataParallel.forward(...)</code> is never called.</p><NewLine><p>From <a href=""https://pytorch.org/docs/stable/_modules/torch/nn/parallel/data_parallel.html"" rel=""nofollow noopener"">docs</a>:</p><NewLine><pre><code class=""lang-python"">    def forward(self, *inputs, **kwargs):<NewLine>        if not self.device_ids:<NewLine>            return self.module(*inputs, **kwargs)<NewLine><NewLine>        for t in chain(self.module.parameters(), self.module.buffers()):<NewLine>            if t.device != self.src_device_obj:<NewLine>                raise RuntimeError(""module must have its parameters and buffers ""<NewLine>                                   ""on device {} (device_ids[0]) but found one of ""<NewLine>                                   ""them on device: {}"".format(self.src_device_obj, t.device))<NewLine><NewLine>        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)<NewLine>        if len(self.device_ids) == 1:<NewLine>            return self.module(*inputs[0], **kwargs[0])<NewLine>        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])<NewLine>        outputs = self.parallel_apply(replicas, inputs, kwargs)<NewLine>        return self.gather(outputs, self.output_device)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the answer <a class=""mention"" href=""/u/jerinphilip"">@jerinphilip</a> ! Makes sense why it only uses one gpu in the case of <code>fit</code><br/><NewLine>So is there anything I can do which can help me in parallelizing the fit method? Or the only way to parallelize a model is to call the forward from dataparallel wrapped model itself?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can use a flag keyword argument inside forward, noticing that the two functions don’t differ by much. I tried to switch member functions using a flag and the following segment worked for me:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/jerinphilip/MaskGAN.pytorch/blob/cdb2a7aa87826464f79273976286e90fbd3845dc/mgan/modules/distributed_model.py#L58-L70"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/jerinphilip/MaskGAN.pytorch/blob/cdb2a7aa87826464f79273976286e90fbd3845dc/mgan/modules/distributed_model.py#L58-L70"" rel=""nofollow noopener"" target=""_blank"">jerinphilip/MaskGAN.pytorch/blob/cdb2a7aa87826464f79273976286e90fbd3845dc/mgan/modules/distributed_model.py#L58-L70</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""58"" style=""counter-reset: li-counter 57 ;""><NewLine><li>def forward(self, *args, **kwargs):</li><NewLine><li>    if 'ppl' not in kwargs:</li><NewLine><li>        kwargs['ppl'] = False</li><NewLine><li><NewLine></li><NewLine><li>    if kwargs['tag'] == 'g-step':</li><NewLine><li>        if self.pretrain:</li><NewLine><li>            return self._gstep_pretrain(*args, ppl_compute=kwargs['ppl'])</li><NewLine><li>        else:</li><NewLine><li>            return self._gstep(*args, ppl_compute=kwargs['ppl'])</li><NewLine><li>    elif kwargs['tag'] == 'c-step':</li><NewLine><li>        return self._cstep(*args)</li><NewLine><li><NewLine></li><NewLine><li>    return self._dstep(*args, real=kwargs['real'])</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>In my case I’m switching whether to use generator, discriminator or critic in an GAN-Actor Critic setup. I’m using <code>tag</code> here to control which sub-model’s forward is being called.</p><NewLine><p>You can see <code>scatter</code>'s source code below to understand how <code>args</code> and <code>kwargs</code> are replicated along workers, in case there’s any confusion, which at the time I had:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/4e3c97a0be5c1bba04928de6abbdad31169e62ee/torch/nn/parallel/scatter_gather.py#L5-L31"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/4e3c97a0be5c1bba04928de6abbdad31169e62ee/torch/nn/parallel/scatter_gather.py#L5-L31"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/4e3c97a0be5c1bba04928de6abbdad31169e62ee/torch/nn/parallel/scatter_gather.py#L5-L31</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""5"" style=""counter-reset: li-counter 4 ;""><NewLine><li>def scatter(inputs, target_gpus, dim=0):</li><NewLine><li>    r""""""</li><NewLine><li>    Slices tensors into approximately equal chunks and</li><NewLine><li>    distributes them across given GPUs. Duplicates</li><NewLine><li>    references to objects that are not tensors.</li><NewLine><li>    """"""</li><NewLine><li>    def scatter_map(obj):</li><NewLine><li>        if isinstance(obj, torch.Tensor):</li><NewLine><li>            return Scatter.apply(target_gpus, None, dim, obj)</li><NewLine><li>        if isinstance(obj, tuple) and len(obj) &gt; 0:</li><NewLine><li>            return list(zip(*map(scatter_map, obj)))</li><NewLine><li>        if isinstance(obj, list) and len(obj) &gt; 0:</li><NewLine><li>            return list(map(list, zip(*map(scatter_map, obj))))</li><NewLine><li>        if isinstance(obj, dict) and len(obj) &gt; 0:</li><NewLine><li>            return list(map(type(obj), zip(*map(scatter_map, obj.items()))))</li><NewLine><li>        return [obj for targets in target_gpus]</li><NewLine><li><NewLine></li><NewLine><li>    # After scatter_map is called, a scatter_map cell will exist. This cell</li><NewLine><li>    # has a reference to the actual function scatter_map, which has references</li><NewLine><li>    # to a closure that has a reference to the scatter_map cell (because the</li><NewLine></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/4e3c97a0be5c1bba04928de6abbdad31169e62ee/torch/nn/parallel/scatter_gather.py#L5-L31"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a tonne for helping me out! <img alt="":smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smile.png?v=9"" title="":smile:""/><br/><NewLine>I found a simple way to change my code and use the parallel functionality in my forward.<br/><NewLine>Everything working as expected now.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I met the same problem and want to use multi gpus even with model.module.predict, predict is a part defined in the model class. So, could you tell me the simple way you <a class=""mention"" href=""/u/gollum"">@gollum</a>  found? Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerinphilip; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/gollum; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerinphilip; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/gollum; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Sam_Sam; <NewLine> ,"REPLY_DATE 1: June 10, 2019,  8:47pm; <NewLine> REPLY_DATE 2: June 10, 2019,  9:14pm; <NewLine> REPLY_DATE 3: June 11, 2019,  4:20am; <NewLine> REPLY_DATE 4: June 11, 2019,  9:04am; <NewLine> REPLY_DATE 5: February 26, 2020,  9:35pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
70482,Multiple networks running in parallel on different CPUs,2020-02-20T13:39:40.989Z,1,348,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to have different neural networks run in parallel on different CPUs but am finding that it isn’t leading to any sort of speed up compared to running them sequentially.</p><NewLine><p>Below is my code that replicates the issue exactly. If you run this code it shows that with 2 processes it takes roughly twice as long as running it with 1 process but really it should take the same amount of time.</p><NewLine><pre><code class=""lang-auto"">import time<NewLine>import torch.multiprocessing as mp<NewLine>import gym<NewLine>import numpy as np<NewLine>import copy<NewLine>import torch.nn as nn<NewLine>import torch<NewLine><NewLine>class NN(nn.Module):<NewLine>    def __init__(self, output_dim):<NewLine>        nn.Module.__init__(self)<NewLine>        self.fc1 = nn.Linear(4, 50)<NewLine>        self.fc2 = nn.Linear(50, 500)<NewLine>        self.fc3 = nn.Linear(500, 5000)<NewLine>        self.fc4 = nn.Linear(5000, output_dim)<NewLine>        self.relu = nn.ReLU()<NewLine>    <NewLine>    def forward(self, x):<NewLine>        x = self.relu(self.fc1(x))<NewLine>        x = self.relu(self.fc2(x))<NewLine>        x = self.relu(self.fc3(x))<NewLine>        x = self.fc4(x)<NewLine>        return x<NewLine><NewLine>def Worker(ix):<NewLine>  print(""Starting training for worker "", ix)<NewLine>  env = gym.make('CartPole-v0')<NewLine>  model = NN(2)<NewLine>  for _ in range(2000):<NewLine>    model(torch.Tensor(env.reset()))<NewLine>  print(""Finishing training for worker "", ix)<NewLine><NewLine>def overall_process(num_workers):<NewLine>  workers = []<NewLine>  for ix in range(num_workers):<NewLine>    worker = mp.Process(target=Worker, args=(ix, ))<NewLine>    workers.append(worker)<NewLine>  [w.start() for w in workers]  <NewLine>  for worker in workers:<NewLine>    worker.join()<NewLine><NewLine>  print(""Finished Training"")  <NewLine>  print("" "")<NewLine><NewLine>start = time.time()<NewLine>overall_process(1)<NewLine>print(""Time taken: "", time.time() - start)<NewLine>print("" "")<NewLine><NewLine>start = time.time()<NewLine>overall_process(2)<NewLine>print(""Time taken: "", time.time() - start)<NewLine></code></pre><NewLine><p>Does anyone know why this might be happening and how to fix it?</p><NewLine><p>I thought that it is maybe because PyTorch networks automatically implement CPU parallelism in the background and so I tried adding the below 2 lines but it doesn’t always resolve the issue:</p><NewLine><pre><code class=""lang-auto"">torch.set_num_threads(1)<NewLine>torch.set_num_interop_threads(1)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Petros_Christodoulou,(Petros Christodoulou),Petros_Christodoulou,"February 20, 2020,  1:42pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/vitalyfedyunin"">@VitalyFedyunin</a> <a class=""mention"" href=""/u/alband"">@albanD</a> , do you know what could cause the seemingly sequential execution?</p><NewLine><p>I tried the script on my laptop and saw the same behavior. When I increase the # of iterations from 2000 to 20000, it becomes even worse.</p><NewLine><p>1 process<br/><NewLine>Time taken:  72.73709082603455</p><NewLine><p>2 processes<br/><NewLine>Time taken:  229.3490858078003</p><NewLine><p>Below is the CPU utilization graph. Looks like one process occupies half of the cores, and 2 process uses all of them. But still 2 processes execution time is worse than sequential.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/64f14a73e39548cbf99fa50ac66498b998a77154"" href=""https://discuss.pytorch.org/uploads/default/original/3X/6/4/64f14a73e39548cbf99fa50ac66498b998a77154.png"" title=""Screen Shot 2020-02-25 at 11.58.09 AM""><img alt=""Screen Shot 2020-02-25 at 11.58.09 AM"" data-base62-sha1=""eoYI2gdetIAWfrkAoGbNwzPybVq"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/6/4/64f14a73e39548cbf99fa50ac66498b998a77154_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/6/4/64f14a73e39548cbf99fa50ac66498b998a77154_2_242x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/6/4/64f14a73e39548cbf99fa50ac66498b998a77154_2_242x500.png, https://discuss.pytorch.org/uploads/default/optimized/3X/6/4/64f14a73e39548cbf99fa50ac66498b998a77154_2_363x750.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/6/4/64f14a73e39548cbf99fa50ac66498b998a77154_2_484x1000.png 2x"" width=""242""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screen Shot 2020-02-25 at 11.58.09 AM</span><span class=""informations"">706×1458 69.4 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Where do you set <code>torch.set_num_threads(1)</code> ?<br/><NewLine>You have to set it at the beginning of the <code>Worker()</code> function for it to have an effect on the newly created process.</p><NewLine><p>Checking with <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>, this seems to give the right behavior after setting this.</p><NewLine><p>In particular, by default, pytorch will use all the available cores to run computations on CPU. So if you launch two processes to do this at once, then they will fight for the CPU and most likely slow each other down.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks a lot. I was setting the number of threads in the parent process rather than the worker processes. It seems to resolve it if i set them in each of the worker processes</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Petros_Christodoulou; <NewLine> ,"REPLY_DATE 1: February 25, 2020,  5:01pm; <NewLine> REPLY_DATE 2: February 25, 2020,  5:56pm; <NewLine> REPLY_DATE 3: February 26, 2020, 10:23am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
51381,Gathering dictionaries of DistributedDataParallel,2019-07-23T12:20:19.815Z,3,542,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am training a model to segment 3D images in a slice by slice-fashion. To distribute over multiple GPUs I am using <code>DistributedDataParallel</code> and I use <code>DistributedSampler</code> to split the dataset across the GPUs.</p><NewLine><p>During prediction of new cases, I use a similar <code>Dataset</code> and <code>DataLoader</code> setup and I basically can gather a dictionary like: <code>{'filename': [(slice_no_1, slice_1_pred), (slice_no_2, slice_2_pred)], ...}</code> which I can subsequently sort on the first index to get an output. However, when I use <code>DistributedSampler</code> the <em>slices</em> are distributed along two GPUs, and I therefore end up with two dictionaries which most likely are both incomplete (one containing slices of the other and vice versa).</p><NewLine><p>How do I gather these two dictionaries? As I preferably cast the predictions to a numpy array, it might be most convenient to gather these in the CPU memory.</p><NewLine></div>",https://discuss.pytorch.org/u/jteuwen,(Jonas Teuwen),jteuwen,"July 23, 2019, 12:26pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/jteuwen"">@jteuwen</a>,</p><NewLine><p>I’m not sure I understand your issue, but I’ll give it a shot. You’re doing validation of your model and you’re using a distributed sampler on the validation set. This means you have a partial result on each process, and you’re looking to combine them into a single result, for the final accuracy numbers?</p><NewLine><p>Or… perhaps the sampler splits the array of slices for a single filename across GPUs and you want to keep the predictions for a single filename on a single process?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/pietern"">@pietern</a></p><NewLine><p>No, training and validation is done on a slice-by-slice basis while the data are 3D MRI images. My Dataset outputs a dictionary with the data including a key which says to which file the slice belongs, and what the index of the slice is. I use the same setup for predicting new cases. However, for that I would like to recombine data again into a 3D volume. When you have one GPU that is fine: when processing a complete dataset you can combine based on the dictionary key denoting the filename and the slice number.</p><NewLine><p>When doing this with a DistributedSampler, you have multiple python processing having a part of the dataset. In this case, even when the sampling is sequential, it can be that part of the slices end up in one process, and the others in another process. To recombine them I would need to have access in the process with rank 0 to <em>all</em> dictionaries of the other processes containing the slices.</p><NewLine><p>Solutions I have come up with now:</p><NewLine><ul><NewLine><li>Dump each slice to disk and when done, combine them in process rank 0</li><NewLine><li>Use a memorymap to do the same thing (but can do with pickled dictionaries)</li><NewLine><li>Use something such as Redis to store the results in. Extra bonus is that it would be easier to distribute as we already use Redis.</li><NewLine></ul><NewLine><p>However, that seems to be quite convoluted for a reasonably simple problem. I could change the Dataset classes and the sampler specifically for this purpose, but that has the disadvantage that (1) if I change something in the dataset / dataloader I would need to change in two places, and a source for bugs (2) also tricky to implement a multiGPU model which scales well across a cluster.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I understand, thanks for the clarification.</p><NewLine><p>You can use existing <code>torch.distributed</code> primitives <code>gather</code> or <code>all_gather</code> to get all results to a single or all processes, respectively. You say you’re outputting dictionaries, so you can’t do it with functions in core yet, and would need to serialize the dictionaries yourself. Coincidentally, <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> and I were talking about adding this to core yesterday, and he created an issue to track it: <a href=""https://github.com/pytorch/pytorch/issues/23232"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/23232</a>. This doesn’t solve your issue right now though.</p><NewLine><p>To solve this today, I would indeed write everything to a shared filesystem if you have one (with <code>torch.save</code>), probably named after the rank of the process, run a barrier to ensure all writes are done, and then <code>torch.load</code> all of them on the process where you want to collect the results.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply - that does seem like a good addition to the code base. By the way: if I would use the <code>torch.distributed</code> primitives, then since <code>nccl</code> is the backend, wouldn’t that transfer through the GPU memory (since <code>nccl</code> does not support CPU ipc)? That might be unconvenient, also for my use case.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s correct. It would require serialization on the CPU side, copy to GPU memory to perform the collective, copy back to CPU memory, and then deserialization. Alternatively, you can create a separate Gloo group and use that, with <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.new_group"" rel=""nofollow noopener""><code>torch.distributed.new_group</code></a>.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jteuwen"">@jteuwen</a> I have a question for you.  Are you loading a single 3D file that then gets sliced and passed into training a model?  If this is the case one question I have is that when doing multiGPU training via DDP, could you not run into a situation where multiple processes would get different slices that originate from the same 3D file and would therefore want to access the file at the same time?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jteuwen; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jteuwen; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/solarflarefx; <NewLine> ,"REPLY_DATE 1: July 24, 2019,  8:35am; <NewLine> REPLY_DATE 2: July 24, 2019,  8:46am; <NewLine> REPLY_DATE 3: July 24, 2019,  8:58am; <NewLine> REPLY_DATE 4: July 24, 2019,  9:15am; <NewLine> REPLY_DATE 5: July 24, 2019,  9:20am; <NewLine> REPLY_DATE 6: February 26, 2020,  4:47am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> 
70282,Deadlock with torch.distributed.rpc with num_workers &gt; 1,2020-02-19T06:22:40.139Z,0,278,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a large (93GB) .h5 file containing image features on my local system and my model is trained on SLURM ADA cluster which has a storage limit of 25GB.<br/><NewLine>I am trying to use <strong>torch.distributed.rpc</strong> framework for requesting image features in <strong>Dataset.getitem</strong> using remote call to rpc server on my local system.</p><NewLine><p>Code for initializing RPC server (local system):</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import torch.distributed.rpc as rpc<NewLine><NewLine>def run_worker(rank, world_size):<NewLine>    os.environ['MASTER_ADDR'] = 'XX.X.XX.XX'<NewLine>    os.environ['MASTER_PORT'] = 'XXXX'<NewLine>    <NewLine>    rpc.init_rpc(utils.SERVER_NAME,<NewLine>                 rank=rank, <NewLine>                 world_size=world_size)<NewLine>    print(""Server Initialized"", flush=True)<NewLine><NewLine>    rpc.shutdown()<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    world_size = 2<NewLine>    rank = 0<NewLine>    <NewLine>    run_worker(rank, world_size)<NewLine></code></pre><NewLine><p>Code for RPC server for requesting data from local system (On ADA),</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import torch.distributed.rpc as rpc<NewLine><NewLine>def run_worker(rank, world_size):<NewLine>    os.environ['MASTER_ADDR'] = 'XX.X.XX.XX'<NewLine>    os.environ['MASTER_PORT'] = 'XXXX'<NewLine><NewLine>    rpc.init_rpc(utils.CLIENT_NAME.format(rank), rank=rank, world_size=world_size)<NewLine>    print(""Client Initialized"", flush=True)<NewLine><NewLine>    main()<NewLine><NewLine>    rpc.shutdown()<NewLine><NewLine>if __name__ == '__main__':<NewLine>    world_size = 2<NewLine>    rank = 1<NewLine><NewLine>    run_worker(rank, world_size)<NewLine></code></pre><NewLine><p>In my <strong>data_loader</strong> I have specified <strong>num_worker=8</strong>,<br/><NewLine>Simplified code for dataset.getitem is (On ADA),</p><NewLine><pre><code class=""lang-auto"">def __getitem__(self, index):<NewLine>        ....<NewLine>        ....<NewLine><NewLine>        print(""fetching image for image_id {}, item {}"".format(image_id, item), flush=True)<NewLine>        v = utils._remote_method(utils.Server._load_image, self.server_ref, [index, self.coco_id_to_index])<NewLine><NewLine>        return v, ......<NewLine></code></pre><NewLine><p>Now in my training loop when I call <strong>enumerate(data_loader)</strong>, multi-process data loading is enabled and <strong>getitem</strong> function is called <strong>num_worker</strong> times and a deadlock is reached.<br/><NewLine>I am not sure why this deadlock is occuring because whenver <strong>getitem</strong> is called a remote call should be made to RPC server on my local system to request for data.</p><NewLine><p>How can I resolve the deadlock? Is there any other way to solve my problem if large file doesn’t fit in my ADA system, I don’t want to compromise on my latency.</p><NewLine><p>Edit: When I set num_worker=0 my code is working, but it is very slow 20 sec/iterations.</p><NewLine></div>",https://discuss.pytorch.org/u/Kanishk_Jain,(Kanishk Jain),Kanishk_Jain,"February 19, 2020,  2:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/kanishk_jain"">@Kanishk_Jain</a></p><NewLine><p>Thanks for trying out RPC.</p><NewLine><blockquote><NewLine><p>multi-process data loading is enabled and  <strong>getitem</strong>  function is called  <strong>num_worker</strong>  times and a deadlock is reached.</p><NewLine></blockquote><NewLine><p>It could be because it depleted the RPC threads in the thread pool. <code>num_send_recv_threads</code> by default is 4. Does it work if you bump up the number of threads? Sth like:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>from torch.distributed.rpc import ProcessGroupRpcBackendOptions<NewLine>from datetime import timedelta<NewLine>import os<NewLine>import torch.distributed.rpc as rpc<NewLine><NewLine>os.environ['MASTER_ADDR'] = 'localhost'<NewLine>os.environ['MASTER_PORT'] = '29500'<NewLine><NewLine>options = ProcessGroupRpcBackendOptions()<NewLine>options.rpc_timeout = timedelta(seconds=60)<NewLine>options.init_method = ""env://""<NewLine>options.num_send_recv_threads = 32<NewLine><NewLine>rpc.init_rpc(""client"", rank=0, world_size=2, rpc_backend_options=options)<NewLine>rpc.shutdown()<NewLine></code></pre><NewLine><p>The current <code>rpc_backend_options</code> API is too verbose and not well documented. We will improve that in the next release.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Regarding the concern on speed, hopefully, using more workers in the data loader will help to boost the throughput. We are also working on making the RPC comm layer more efficient by adding <a href=""https://github.com/pytorch/tensorpipe/tree/master/tensorpipe"" rel=""nofollow noopener"">TensorPipe</a> as a new backend, so that RPC does not have to do two round trips for each message as with <code>ProcessGroup</code> and TensorPipe would also allow using multiple comm media.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: February 25, 2020,  4:30pm; <NewLine> REPLY_DATE 2: February 25, 2020,  4:34pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
70633,Evaluate during training with distributed,2020-02-21T22:02:13.469Z,1,166,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using the distributed training package to train on multiple gpus. Training works fine but I would like to be able to evaluate during training either on one gpu or multiple gpus. If I directly call evaluate function during training, each model produces different results. How can I get evaluation results every certain steps while using the distributed package for training?</p><NewLine></div>",https://discuss.pytorch.org/u/maralm,(Maral),maralm,"February 21, 2020, 10:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you are using DDP, the model replica should be initialized in the same manner. Since DDP performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all processes in the same way, the model output should be the same.<br/><NewLine>Are you also observing different outputs during training?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>When I evaluate during training, it runs on all gpus and each one produce different results. I am actually running this script:<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/huggingface/transformers/blob/master/examples/run_glue.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/huggingface/transformers/blob/master/examples/run_glue.py"" rel=""nofollow noopener"" target=""_blank"">huggingface/transformers/blob/master/examples/run_glue.py</a></h4><NewLine><pre><code class=""lang-py""># coding=utf-8<NewLine># Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.<NewLine># Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.<NewLine>#<NewLine># Licensed under the Apache License, Version 2.0 (the ""License"");<NewLine># you may not use this file except in compliance with the License.<NewLine># You may obtain a copy of the License at<NewLine>#<NewLine>#     http://www.apache.org/licenses/LICENSE-2.0<NewLine>#<NewLine># Unless required by applicable law or agreed to in writing, software<NewLine># distributed under the License is distributed on an ""AS IS"" BASIS,<NewLine># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<NewLine># See the License for the specific language governing permissions and<NewLine># limitations under the License.<NewLine>"""""" Finetuning the library models for sequence classification on GLUE (Bert, XLM, XLNet, RoBERTa, Albert, XLM-RoBERTa).""""""<NewLine><NewLine><NewLine>import argparse<NewLine>import glob<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/huggingface/transformers/blob/master/examples/run_glue.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>On line 248, it is mentioned “Only evaluate when single GPU otherwise metrics may not average well”. I don’t understand why and how to change it to be able to evaluate correctly.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/maralm; <NewLine> ,"REPLY_DATE 1: February 22, 2020,  7:22am; <NewLine> REPLY_DATE 2: February 24, 2020,  5:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
70250,Multi GPU with Custom Backward and Attributes,2020-02-19T00:34:33.017Z,4,119,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I wrote a custom backward function for my model. I want to use the DataParallel package. However I have a problem. If I use</p><NewLine><pre><code class=""lang-auto"">model = torch.nn.DataParallel(model, device_ids=[0,1])<NewLine></code></pre><NewLine><p>I get the following error:</p><NewLine><p>“‘DataParallel’ object has no attribute ‘backward’”</p><NewLine><p>I know this can be solved by using model.module.backward, but then it will only use one gpu. Is there a way to use the torch.nn.DataParallel with custom backward and attributes?</p><NewLine></div>",https://discuss.pytorch.org/u/maralm,(Maral),maralm,"February 19, 2020, 12:35am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Would it be possible to return the outputs in your <code>forward</code> method and calculate the loss on the default device?<br/><NewLine>This would be the vanilla use case, while it seems you’ve implemented <code>backward</code> as a class function?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply. No the backward is not a separate class. It is a function inside the model class. Here is how I define it:</p><NewLine><pre><code class=""lang-auto"">Class myModel():<NewLine>    def __init__(self, config):<NewLine>         ....<NewLine>    def forward(...):<NewLine>        ....<NewLine>    def backward(...):<NewLine>        ....<NewLine></code></pre><NewLine><p>And I call it this way:</p><NewLine><pre><code class=""lang-auto"">outputs = model(....)<NewLine>loss = outputs[0]  <NewLine>if args.n_gpu &gt; 1:<NewLine>    loss = loss.mean()<NewLine>model = model.backward(...)<NewLine></code></pre><NewLine><p>but nn.DataParallel is not recognizing the backward and some other attributes without using module.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the information.</p><NewLine><p>What’s the design decision to put the <code>backward</code> call inside your model?<br/><NewLine>Are you using some internal parameters?<br/><NewLine>If so, how are these parameters updated/used inside the model?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I needed to access the activations and activation gradients in backward. I collect activations in forward pass and access to them in backward. I use autograd backward function to calculate each layer’s backward and make the changes that I want in the process.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried the distributed data parallel instead of data parallel and it is working.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/maralm; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/maralm; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/maralm; <NewLine> ,"REPLY_DATE 1: February 19, 2020,  7:27am; <NewLine> REPLY_DATE 2: February 19, 2020,  5:19pm; <NewLine> REPLY_DATE 3: February 20, 2020, 12:43am; <NewLine> REPLY_DATE 4: February 20, 2020,  9:22pm; <NewLine> REPLY_DATE 5: February 21, 2020, 10:02pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
42026,How to handle exception in DistributedDataParallel?,2019-04-09T02:31:01.766Z,2,711,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m using <code>DistributedDataParallel</code> to train my model. If one process met an exception and use the <code>try...except</code> block to catch the exception during <code>forward</code> then continue training with a new batch of data, all the process would hang (I guess that is because the fail of synchronization?). How can I handle exceptions in one process and continue training without hanging all the process? Thanks for the help!</p><NewLine></div>",https://discuss.pytorch.org/u/angshine,(Ang Shine),angshine,"April 9, 2019,  2:31am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>All communication done through <code>torch.distributed</code> is <em>collective</em>, meaning all processes expect all their peers to participate in all <em>collective calls</em> they execute. If a single processes ends up not participating, the others will time out or raise an exception. The only way out of this is to let all processes timeout or fail and to reinitialize the distributed module.</p><NewLine><p>You can use <code>torch.distributed.destroy_process_group</code> to deinitialize and then make another call to <code>torch.distributed.init_process_group</code> to reinitialize. This can only work if you’re using either the Gloo or the NCCL backend, <strong>and</strong> that the underlying initialization method can be reused. I believe this is the case for the the file initialization method as well as the TCP initialization method (see <a href=""https://pytorch.org/docs/stable/distributed.html#initialization"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/distributed.html#initialization</a> for more information on both).</p><NewLine><p>Good luck!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the help! I got the idea, but how can I “let all processes timeout or fail”. How can all the processes know that there is one process meeting an exception and all the processes should <code>destroy_process_group</code> and reinitialize? If one process meet an exception for one minibatch, can all the processes simply just jump the current minibatch and run the next minibatch?</p><NewLine><p>I find the following snippets in pytorch repo which might be helpful, but not sure how to implement the idea in detail.</p><NewLine><pre><code class=""lang-python"">def test_barrier_timeout_global(self):<NewLine>        dist.destroy_process_group()<NewLine><NewLine>        # Explicitly pass world size to the barrier because we've<NewLine>        # just destroyed any state in torch.distributed.<NewLine>        self._barrier(wait_for=int(WORLD_SIZE))<NewLine><NewLine>        # Reinitialize global process group<NewLine>        timeout = timedelta(seconds=0.2)<NewLine>        dist.init_process_group(<NewLine>            init_method=INIT_METHOD,<NewLine>            backend=BACKEND,<NewLine>            world_size=int(WORLD_SIZE),<NewLine>            rank=self.rank,<NewLine>            timeout=timeout,<NewLine>        )<NewLine>self._test_barrier_timeout(dist.group.WORLD, timeout)<NewLine></code></pre><NewLine><p>(from <a href=""https://github.com/pytorch/pytorch/blob/544783fa1d5eb2d787b1dd4de6460496d1c8a688/test/test_distributed.py"" rel=""nofollow noopener"">test_distributed.py</a>)</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>One way to do this is to use a smaller timeout. The default timeout for the distributed module is 30 minutes. You can override this by specifying the <code>timeout</code> keyword argument to <code>init_process_group</code> as a <code>timedelta</code> type (e.g. <code>datetime.timedelta(seconds=10)</code>). Then if one of the processes crashes, the others will time out. The problem with your proposed solution is that you’re not guaranteed that the crashed process will come back. Therefore you’ll have to rely on some out of band mechanism to figure out which processes are still alive, and only when you know for sure you have <code>WORLD_SIZE</code> machines (or after adjusting <code>WORLD_SIZE</code>), continue and reinitialize.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is there any clean way of accomplishing this now? I’m training on images with variable sizes and every ~30k iterations there’s an OOM error. I’m having trouble understanding how to synchronize the call to <code>init_process_group</code> between all the processes.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>We currently don’t support elasticity of workers, which means that if one of the processes crashes with an OOM error the user is currently responsible for spawning another process and re-initializing distributed communication if they want to continue with training.</p><NewLine><p>You may want to consider the use of the PyTorch elastic framework: <a href=""https://github.com/pytorch/elastic"" rel=""nofollow noopener"">https://github.com/pytorch/elastic</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/angshine; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Ali2500; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/rvarm1; <NewLine> ,"REPLY_DATE 1: April 9, 2019,  6:12am; <NewLine> REPLY_DATE 2: April 11, 2019,  7:08am; <NewLine> REPLY_DATE 3: April 15, 2019,  9:43pm; <NewLine> REPLY_DATE 4: February 14, 2020,  9:56am; <NewLine> REPLY_DATE 5: February 18, 2020, 10:02pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
56008,Some confusion about torch.multiprocessing.spawn in pytorch,2019-09-16T02:46:48.562Z,0,1168,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I really get confused when I use the function torch.multiprocessing.spawn. Consider the following code:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.multiprocessing as mp<NewLine><NewLine><NewLine>x = [1, 2]<NewLine><NewLine>def f(id, a):<NewLine>    print(x)<NewLine>    print(a)<NewLine><NewLine>if __name__ == '__main__':<NewLine>    x.append(3)<NewLine>    mp.spawn(f, nprocs=2, args=(x, ))<NewLine></code></pre><NewLine><p>For any process the <strong>main</strong> function spwans, it outputs the following:</p><NewLine><pre><code class=""lang-auto"">[1, 2]<NewLine>[1, 2, 3]<NewLine></code></pre><NewLine><p>I have the following questions:<br/><NewLine>(1) Why is the first line of output [1, 2]? I think x is a global variable, and fork new process will share the memory in linux, which follows this page: <a>https://stackoverflow.com/questions/5983159/python-multiprocessing-arguments-deep-copy</a><br/><NewLine>(2) Are the parameters in spawn deep copied to the new processes? Or just pass a reference?</p><NewLine><p>Thank you very much!</p><NewLine></div>",https://discuss.pytorch.org/u/zbh2047,,zbh2047,"September 16, 2019,  2:51am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have the exact same issue with torch.multiprocessing.spawn (mp.spawn) used for distributed parallel training.</p><NewLine><p>Since I have a large dataset of csv files which i convert to a shared multiprocessing numpy array object to avoid memory leak outside of my main. but mp.spwan It makes multiple copies of it anyways.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It looks like python’s <code>multiprocessing</code> module also copies the data if we use a spawn start_method:</p><NewLine><pre><code class=""lang-auto"">import multiprocessing as mp<NewLine>x = [1, 2]<NewLine>def foo(a):<NewLine>    print(x)<NewLine>    print(a)<NewLine><NewLine>if __name__ == '__main__':<NewLine>   mp.set_start_method(""spawn"")<NewLine>    x.append(3)<NewLine>    p = mp.Process(target=foo, args=(x,))<NewLine>    p.start()<NewLine>    p.join()<NewLine></code></pre><NewLine><p>To answer your question, there is a deepcopy, though shared memory will be used for <code>Tensor</code> data.</p><NewLine><p>From the pytorch multiprocessing “best practices” page (<a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/notes/multiprocessing.html</a>):</p><NewLine><pre><code class=""lang-auto"">We recommend using python:multiprocessing.Queue for passing all kinds of PyTorch objects between processes. It is possible to e.g. inherit the tensors and storages already in shared memory, when using the fork start method, however it is very bug prone and should be used with care, and only by advanced users. Queues, even though they’re sometimes a less elegant solution, will work properly in all cases.<NewLine><NewLine></code></pre><NewLine><p>You could thus use the <code>fork</code> start method with pytorch multiprocessing to avoid the copy, though as the docs mention this is not supported.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/kazem; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/rvarm1; <NewLine> ,"REPLY_DATE 1: February 17, 2020, 10:52am; <NewLine> REPLY_DATE 2: February 18, 2020,  9:20pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
70186,Tensor.pin_memory allocates memory on cuda:0,2020-02-18T14:11:40.600Z,2,83,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The following code always allocates memory on device cuda:0, but I want to use another GPU for training</p><NewLine><p>import torch<br/><NewLine>a = torch.randn([20000, 20000])<br/><NewLine>a.pin_memory() # &lt;- this operation allocates memory on “cuda:0”, but I want to leave it unused.<br/><NewLine>b = a.cuda(‘cuda:1’)</p><NewLine></div>",https://discuss.pytorch.org/u/Evgeny_Shalnov,(Evgeny Shalnov),Evgeny_Shalnov,"February 18, 2020,  2:12pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not sure why it allocates memory,<br/><NewLine>But I’d try using torch.set_device(‘cuda:1’)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This most likely happens because to get pinned memory, we need a cuda context. And so we initialize a cuda context on the current device when it is needed. Which would be cuda:0 here by default.<br/><NewLine>Changing the device will help.</p><NewLine><p>Also if you never want to touch cuda:0, a good practice is to use the <code>CUDA_VISIBLE_DEVICES=1</code> environment variable. This acts on the nvidia driver for the current process and hides the other GPUs. That way, you are sure that you never use them.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>torch.set_device(‘cuda:1’) helped</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/seliad; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Evgeny_Shalnov; <NewLine> ,"REPLY_DATE 1: February 18, 2020,  3:49pm; <NewLine> REPLY_DATE 2: February 18, 2020,  5:09pm; <NewLine> REPLY_DATE 3: February 18, 2020,  6:27pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
69858,Loss collection for outputs on multiple GPUs,2020-02-15T21:24:30.763Z,1,143,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I have the following problem. I am trying to propagate multiple outputs out of my network which are scalars, for example, latency or memory consumption of respective layers in addition to the output itself. These outputs I would then like to add to the main loss, let’s say cross-entropy.</p><NewLine><p>With a single GPU, I am using a <code>@dataclass</code> to accumulate the respective scalar layer outputs in an accumulator that I then add to the loss, which contents I add to the main loss. However, I do have multiple GPUs that I could utilise for training and I am not sure how to propage the respective scalars and combine them such that I could use <code>.backward()</code>. Any help is much appreciated. Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/martinferianc,(Martin Ferianc),martinferianc,"February 15, 2020,  9:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you are using <code>nn.DataParallel</code> the model will be replicated to each GPU and each model will get a chunk of your input batch.<br/><NewLine>The output will be gathered on the default device, so most likely you wouldn’t have to change anything.</p><NewLine><p>However, I’m not sure about the use case.<br/><NewLine>How are you calculating the memory consumption and is this operation differentiable?<br/><NewLine>I assume it’s not differentiable so that your accumulated loss will in fact just be the <code>nn.CrossEntropyLoss</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for getting back to me. I forgot to mention that the scalars are multiplied by a parameter that I would like to learn (I am experimenting with neural architecture search).</p><NewLine><p>When I did some small-scale experiments, I did not observe any errors so it has to be my implementation that is wrong, nevertheless, thank you for your clarification.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/martinferianc; <NewLine> ,"REPLY_DATE 1: February 16, 2020,  5:39pm; <NewLine> REPLY_DATE 2: February 16, 2020,  5:39pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
69572,Dataset doesn&rsquo;t work well for distributed training,2020-02-13T09:32:07.663Z,0,71,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have been working on implementing distributed training for NER. In the process I implemented a version using Horovod and one using DistributedDataParallel, because I initially thought my issues were related to my implementation. Both work as expected with a public dataset. I can scale the learning rate by the number of processes (or the batch size) and I get results that are very close to the non-distributed training, yet faster. With my private dataset, which served me for testing along the way, the behavior is different. The distributed training on e.g. 4 processes performs almost exactly like when I train on a single process on 1/4 of the data but using the scaled learning rate. Debugging showed that the different processes have different losses and that the gradients are correctly syncronized in the backward pass.</p><NewLine><p>The only two explanations I have for this: 1) There is still something wrong in my code. 2) The gradients computed are so similar for each process that there is not much or no gain in averaging them and the result is similar to working with 1/4 of the data and a scaled learning rate.</p><NewLine><p>This is my first experience with distributed training so I can’t tell if 2) is reasonable and I’d be keen to know more about your experience with this.</p><NewLine></div>",https://discuss.pytorch.org/u/marcelgwerder,(Marcel),marcelgwerder,"February 13, 2020,  9:32am",,,,,
40052,"Multi-processing training, GPU0 has more memory usage",2019-03-17T02:01:42.357Z,0,325,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I use the torch.distributed.launch module to multi-processing my training program. Everything seems fine but I don’t know why some process in 1-N gpu will has another memory usage in GPU 0.<br/><NewLine><img alt=""image"" height=""295"" src=""https://discuss.pytorch.org/uploads/default/original/2X/b/b8ccb1511cb8e9b83c89d38a2502250b52b6ccc6.png"" width=""179""/><br/><NewLine>As depicted in the picture, the process in gpu4,6 have something in gpu0, this two usage are about 700+M memory. And sometimes other processes will also have similar behavior, but not all the other process will have memory usage in gpu0.</p><NewLine><p>I don’t know why this thing happen? Since the memory unbalances, the training sometimes will be close due to 'out of memory error.</p><NewLine></div>",https://discuss.pytorch.org/u/zeal,(1156478780),zeal,"March 17, 2019,  2:01am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I agree this can be annoying. As you see not all processes initialize this context. Is there perhaps some path in your code that conditionally initialized some memory, or sets the CUDA device manually? We don’t have any facilities that I know of to point to the culprit here, aside from simply looking at code.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Maybe, it’s because that you doesn’t set your device to load when use distributeddataparallel.</p><NewLine><pre><code class=""lang-auto"">loc = 'cuda:{}'.format(args.gpu)<NewLine>checkpoint = torch.load(SAVE_PATH, map_location=loc)<NewLine></code></pre><NewLine><p>adding map_location option in your main_worker will solve your problem.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/11153; <NewLine> ,"REPLY_DATE 1: March 20, 2019, 11:37pm; <NewLine> REPLY_DATE 2: February 12, 2020,  9:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
69253,Debug on process 3 terminated with signal SIGTERM,2020-02-10T17:30:05.304Z,0,266,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Trying to train using ddp on 4 GPUs but I’m getting a: process 3 terminated with signal SIGTERM<br/><NewLine>Which happens most the way through validation for some reason. Does anyone have any idea why this might happen or how I can debug it easier?</p><NewLine><blockquote><NewLine><p>File “train_gpu.py”, line 210, in <br/><NewLine>main_local(hparam_trial)<br/><NewLine>File “train_gpu.py”, line 103, in main_local<br/><NewLine>trainer.fit(model)<br/><NewLine>File “/scratch/staff/brm512/anaconda3/envs/ln1/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py”, line 343, in fit<br/><NewLine>mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))<br/><NewLine>File “/scratch/staff/brm512/anaconda3/envs/ln1/lib/python3.7/site-packages/torch/multiprocessing/spawn.py”, line 171, in spawn<br/><NewLine>while not spawn_context.join():<br/><NewLine>File “/scratch/staff/brm512/anaconda3/envs/ln1/lib/python3.7/site-packages/torch/multiprocessing/spawn.py”, line 107, in join<br/><NewLine>(error_index, name)<br/><NewLine>Exception: process 3 terminated with signal SIGTERM</p><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/Bruce_Muller,,Bruce_Muller,"February 10, 2020,  5:30pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is the validation loop running correctly on a single device?<br/><NewLine>Usually the error messages might be better when disabling multi-GPU runs and multiprocessing.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: February 11, 2020,  4:52am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
67308,Best practice for uneven dataset sizes with DistributedDataParallel,2020-01-21T21:04:21.899Z,1,321,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am wondering about the recommended approach to balancing dataset sizes across different devices while training with DDP. I have split my dataset across four GPUs, but one of them receives a single extra batch, which causes training to hang and wait indefinitely for gradient synchronization with the other devices. I have thought of a few fixes but each seems like it has a drawback:</p><NewLine><p>1.Throw out the final batch to guarantee equal number of iterations<br/><NewLine>2. Use torch.cuda.no_sync() decorator on the final batch. This will cause one device to have different model weights.<br/><NewLine>3. Proceed to the next epoch on the other devices and allow the first batch of epoch 2 to synchronize with this final batch from epoch 1.</p><NewLine><p>I appreciate any suggestions you can give!</p><NewLine></div>",https://discuss.pytorch.org/u/jdvalencia,,jdvalencia,"January 21, 2020,  9:04pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I saw people doing option 1.</p><NewLine><p>People reporting this issue was usually because applications do not know how many batches each process will take prior to training. It seems in your case, you deterministically know what processes will take one more batch? In that case, I think we might be able to do better. For example,</p><NewLine><p>option 1. randomly skipping one batch in each of the processes that takes one more input batch<br/><NewLine>option 2. using no_sync on the <strong>first</strong> batch in each of the processes that takes one more input batch. no_sync won’t lead to parameter disparities, it will just accumulate the grad in param.grad. As long as you don’t run <code>optimizer.step()</code> in that iteration, it should be fine. The next forward-backward pass out of the no_sync context will accumulate more grad to param.grad and consume them together.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Would sth like <a href=""https://github.com/pytorch/pytorch/issues/33148#issuecomment-584400677"" rel=""nofollow noopener"">this</a> work for you? This can be implemented in the application using allreduce.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think that would accomplish it but I basically adopted approach 3 and it has been working fine.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jdvalencia; <NewLine> ,"REPLY_DATE 1: January 27, 2020,  7:17pm; <NewLine> REPLY_DATE 2: February 10, 2020, 11:05pm; <NewLine> REPLY_DATE 3: February 11, 2020, 12:18am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
68833,Strange behavior nn.Dataparallel,2020-02-06T08:11:23.365Z,11,201,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all! I have 4 GPUs 1080 Ti, and when I run training inception_v3 net on multiple GPU model have strange behavior. I didnt rewrite my code much from training on 1 GPU, just add:<br/><NewLine>model = nn.DataParallel(model, device_ids=[0,1,2,3]).cuda()<br/><NewLine>When I run script with device_ids=[0,1] GPUs full utilized and train much faster, when I run script with device_ids=[0,1,2] or device_ids=[0,1,2, 3] script starting (GPU full utilized in nvidia-smi, but reserved memory on card small: 1Gb on first card, and 500 Mb on other) and model dont train. Where I am wrong?  And I have 2 cores processor. Thanks for response. Sorry for my English</p><NewLine></div>",https://discuss.pytorch.org/u/111233,(Денис Милованов),111233,"February 6, 2020,  8:36am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Does your code just hangs using all 4 GPUs or does “model doesn’t train” mean that the training loss is worse than on a single device?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>With 1 or 2 GPUs model train, loss decreased, all good. With 3 or 4 GPUs sript run, but train dont work, I logged every epoch loss and accuracy. I run sript overnight, but nothing logged.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you try to run the code only on device 2 and 3, if 0 and 1 are working?<br/><NewLine>Set the device via <code>.to('cuda:2')</code> or <code>.to('cuda:3')</code>.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>If I set torch.device(“cuda:2”) or torch.device(“cuda:3”), I got error: tensors must be on the same device. If I set nn.DataParallel(model, device_ids=[1,2,3]).cuda(), on first GPU (with index 0) free memory decreased (same as I run training on it) and after that raise error: tensors must be on the same device. In training block of code batch of images send to GPU (input.to(device)). May be this happen because processor have only 2 cores?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you use device ids 0 and 1 in your script for <code>nn.DataParallel</code> and launch the script via:</p><NewLine><pre><code class=""lang-python"">CUDA_VISIBLE_DEVICES=2,3 python script.py args<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Running script with this parameters launch script, but used GPU memory on device 2 and 3: 1Gb and 500 Mb and model dont training.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the test.<br/><NewLine>Could you run the code on a single device now and check, if it’s working on GPU2 and GPU3?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, if set in terminal CUDA_VISIBLE_DEVICE=2, or 3 training is running ok.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>So you only see the hand when you are using <code>nn.DataParallel</code> with device 2 and 3?<br/><NewLine>Could you run the <code>p2pBandwidthLatencyTest</code> from the CUDA samples?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>p2pBandwidthLatencyTest works 24 hours and dont done.<br/><NewLine>P2P Connectivity Matrix<br/><NewLine>D\D     0     1     2     3<br/><NewLine>0       1     1     1     1<br/><NewLine>1       1     1     1     1<br/><NewLine>2       1     1     1     1<br/><NewLine>3       1     1     1     1<br/><NewLine>Unidirectional P2P=Disabled Bandwidth Matrix (GB/s)<br/><NewLine>D\D     0      1      2      3<br/><NewLine>0 352.55   0.41   0.41   0.41<br/><NewLine>1   0.39 216.53   0.39   0.39<br/><NewLine>2   0.39   0.39 349.40   0.39<br/><NewLine>3   0.39   0.39   0.39 350.65<br/><NewLine>Unidirectional P2P=Enabled Bandwidth Matrix (GB/s)<br/><NewLine>D\D     0      1      2      3<br/><NewLine>0 353.51   0.41   0.41   0.41<br/><NewLine>1   0.35 376.32   0.00   0.00<br/><NewLine>2   0.21   0.00 376.32   0.00<br/><NewLine>3   0.21   0.00   0.00 377.78<br/><NewLine>Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)<br/><NewLine>D\D     0      1      2      3<br/><NewLine>0 374.52   0.68   0.68   0.69<br/><NewLine>1   0.69 375.24   0.67   0.67<br/><NewLine>2   0.69   0.68 374.52   0.68<br/><NewLine>3   0.69   0.68   0.68 374.70<br/><NewLine>Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)<br/><NewLine>D\D     0      1      2      3<br/><NewLine>0 377.23   0.68   0.68   0.68<br/><NewLine>1   0.68 372.56   0.55   0.55<br/><NewLine>2   0.67   0.55 370.96   0.55<br/><NewLine>3   0.67   0.55   0.55 370.26<br/><NewLine>P2P=Disabled Latency Matrix (us)</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the information. This points towards some communication issues between the GPUs.<br/><NewLine>Could you run the PyTorch code using <code>NCCL_P2P_DISABLE=1</code> to use shared memory instead of p2p access?</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks! this option works for me.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/111233; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/111233; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/111233; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/111233; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/111233; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/111233; <NewLine> ,"REPLY_DATE 1: February 7, 2020, 12:56am; <NewLine> REPLY_DATE 2: February 7, 2020,  9:28am; <NewLine> REPLY_DATE 3: February 7, 2020,  7:05pm; <NewLine> REPLY_DATE 4: February 8, 2020,  7:55am; <NewLine> REPLY_DATE 5: February 8, 2020,  8:29am; <NewLine> REPLY_DATE 6: February 8, 2020,  7:44pm; <NewLine> REPLY_DATE 7: February 8, 2020,  8:14pm; <NewLine> REPLY_DATE 8: February 8, 2020,  9:52pm; <NewLine> REPLY_DATE 9: February 8, 2020, 10:01pm; <NewLine> REPLY_DATE 10: February 10, 2020,  5:52am; <NewLine> REPLY_DATE 11: February 10, 2020,  7:33am; <NewLine> REPLY_DATE 12: February 10, 2020,  7:34am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> 
47959,Unable to load WaveGlow checkpoint after training with multiple GPUs,2019-06-14T14:38:35.151Z,1,363,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve trained WaveGlow model from <a href=""https://github.com/NVIDIA/waveglow"" rel=""nofollow noopener"">here</a> with multiple GPU, but when I try to load the checkpoint to do inference (through inference.py), some checkpoints are loaded without any problem, but most of them raise the error below:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""inference.py"", line 105, in &lt;module&gt;<NewLine>    args.sampling_rate, args.is_fp16, args.denoiser_strength)<NewLine>  File ""inference.py"", line 46, in main<NewLine>    model_state_dict = torch.load(waveglow_path, map_location=""cuda:0"")['model'].state_dict()<NewLine>  File ""/home/anaconda3/envs/dl/lib/python3.6/site-packages/torch/serialization.py"", line 387, in load<NewLine>    return _load(f, map_location, pickle_module, **pickle_load_args)<NewLine>  File ""/home/anaconda3/envs/dl/lib/python3.6/site-packages/torch/serialization.py"", line 581, in _load<NewLine>    deserialized_objects[key]._set_from_file(f, offset, f_should_read_directly)<NewLine>RuntimeError: storage has wrong size: expected 3901634075968565895 got 512<NewLine></code></pre><NewLine><p>I changed the map_location to  “cpu” and “cuda” and also tried to load the checkpoint with the same number of GPUs used during training, but still get the same error.</p><NewLine><p>When I train the model with a single GPU, all checkpoints are loaded without any issue. This happens only after I run distributed training.</p><NewLine></div>",https://discuss.pytorch.org/u/Neuperc,(Neuperca),Neuperc,"June 14, 2019,  4:04pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This usually happens when multiple processes try to write to a single file.<br/><NewLine>However, this should be prevented with the if condition <code>if rank == 0:</code>.<br/><NewLine>Did you remove it or changed the save logic somehow?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, exactly! it was a simple mistake by me. I commented the original “save_checkpoint” section and only added “save_checkpoint” after the epoch loop without checking if rank==0. Now it works without any errors.<br/><NewLine>Thanks a lot for your help!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I was wondering in such a case is the checkpoints still salvageable or are they simply damaged?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>If multiple process have written to the same file, it’s most likely damaged and you won’t be able to restore it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Neuperc; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/justinliu; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: June 14, 2019,  3:59pm; <NewLine> REPLY_DATE 2: June 14, 2019,  4:03pm; <NewLine> REPLY_DATE 3: February 7, 2020,  7:09pm; <NewLine> REPLY_DATE 4: February 7, 2020, 11:57pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
68247,GPU memory accumulation in parallel_apply,2020-01-31T14:11:23.562Z,0,101,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My aim is to get a linear layer with large output dimension. To achieve this I store the weights of the linear layer in an embedding layer. Further I need to forward and backward only on some connections of the fully connected layer(hence the “shortlist”). Since the output size is large, I divide the embedding layer onto 2 GPUs.</p><NewLine><p>Relevant parts of the code:</p><NewLine><pre><code class=""lang-auto"">class SparseLinear(nn.Module):<NewLine>    def __init__(self, num_labels, hidden_size, device_embeddings):<NewLine>        super(SparseLinear, self).__init__()<NewLine>        <NewLine>        self.device_embeddings = device_embeddings<NewLine>        <NewLine>        self.input_size = hidden_size<NewLine>        self.output_size = num_labels<NewLine>        self.weight = Parameter(torch.Tensor(self.output_size, self.input_size))<NewLine>        if bias:<NewLine>            self.bias = Parameter(torch.Tensor(self.output_size, 1))<NewLine>        else:<NewLine>            self.register_parameter('bias', None)<NewLine>        self.reset_parameters()<NewLine>        self.sparse = True  # Required for optimizer<NewLine><NewLine>    def forward(self, embed, shortlist):<NewLine>        short_weights = F.embedding(shortlist,<NewLine>                                    self.weight,<NewLine>                                    sparse=self.sparse)<NewLine>        <NewLine>        out = torch.matmul(embed.unsqueeze(1), short_weights.permute(0, 2, 1))<NewLine>        <NewLine>        short_bias = F.embedding(shortlist,<NewLine>                                 self.bias,<NewLine>                                 sparse=self.sparse)<NewLine>        out = out + short_bias.permute(0, 2, 1)<NewLine>        del short_weights<NewLine>        return out.squeeze()<NewLine><NewLine>class DividedLinear(DeepXMLBase):<NewLine>    def __init__(self, &lt;params&gt;):<NewLine>    	# Say I have output size of 1000000, and I divide it into two 2 parts<NewLine>    	self.label_partition_lengths = [(500000, ""cuda:0""), (500000, ""cuda:1"")]<NewLine>    	self.classifier = [SparseLinear(num_labels, 300, torch.device(device_name)) for <NewLine>                num_labels, device_name in self.label_partition_lengths]<NewLine>        &lt;init other params&gt;<NewLine>    <NewLine>    def encode(self, batch_data):<NewLine>        return self.transform(batch_data[""doc_embeddings""].to(self.device_embeddings))  # is some network to transform embeddings<NewLine>    <NewLine>    <NewLine>    def forward_with_error_calc(self, batch_data, criterion):<NewLine>        print(""before"", torch.cuda.memory_allocated(1) / (1024 * 1024 * 1024), <NewLine>                      torch.cuda.memory_allocated(2) / (1024 * 1024 * 1024))<NewLine>        encoded = self.encode(batch_data)<NewLine>        device_embeddings = [torch.device(num_labels_device[1]) for num_labels_device in self.label_partition_lengths]<NewLine>        <NewLine>        shortlists = [x.to(device_embeddings[i]) for i, x in enumerate(batch_data[""shortlist""])]<NewLine>        encoded_replicate = [encoded.to(device_embeddings[i]) for i in range(len(device_embeddings))]<NewLine>        <NewLine>        outputs = nn.parallel.parallel_apply(self.classifier, list(zip(encoded_replicate, shortlists)))<NewLine>        <NewLine>        targets = [batch_data[""shortlist_weights""][i].to(device_embeddings[i]) for i in range(len(device_embeddings))]<NewLine>        errors = nn.parallel.parallel_apply(nn.parallel.replicate(criterion, device_embeddings), list(zip(outputs, targets)))<NewLine>        errors_gather = nn.parallel.gather(errors, target_device=device_embeddings[0])<NewLine>        <NewLine>        total_error = errors_gather.sum()<NewLine>        <NewLine>        print(""after"", torch.cuda.memory_allocated(1) / (1024 * 1024 * 1024), <NewLine>                      torch.cuda.memory_allocated(2) / (1024 * 1024 * 1024))<NewLine>        <NewLine>        for output in outputs:<NewLine>            del output<NewLine>        for target in targets:<NewLine>            del target<NewLine>        for x in shortlists:<NewLine>            del x<NewLine>        for x in encoded_replicate:<NewLine>            del x<NewLine>        torch.cuda.empty_cache()<NewLine>        print(""after del"", torch.cuda.memory_allocated(1) / (1024 * 1024 * 1024), <NewLine>                      torch.cuda.memory_allocated(2) / (1024 * 1024 * 1024))<NewLine><NewLine>        return total_error<NewLine></code></pre><NewLine><p>But the GPUs run out of memory after some batches. Particularly, I observe behavior like this:</p><NewLine><blockquote><NewLine><p>before 5.049468994140625 5.049465179443359<br/><NewLine>after 5.1367316246032715 5.1367268562316895<br/><NewLine>after del 5.1367316246032715 5.1367268562316895<br/><NewLine>before 5.136678695678711 5.1366729736328125<br/><NewLine>after 5.223941326141357 5.223934650421143<br/><NewLine>after del 5.223941326141357 5.223934650421143</p><NewLine></blockquote><NewLine><p>So these seems to be some leakage in the <code>forward_with_error_calc</code> function, but I can’t figure out what it is. Can someone please help me in figuring this out? TIA.</p><NewLine></div>",https://discuss.pytorch.org/u/DeepakSaini119,(Deepak Saini119),DeepakSaini119,"January 31, 2020,  3:15pm",,,,,
67778,Conditional gradient update in &ldquo;DistributedDataParallel&rdquo;,2020-01-27T13:57:55.323Z,2,190,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,<br/><NewLine>I want to update the weights if the <code>loss</code> value is less than some threshold. It works okay for the single-gpu case but gets halted (or sometimes throw gpu memory error) when using “DistributedDataParallel” on a single node.<br/><NewLine>Here is an example to reproduce the error. Can you folks help me to figure out this problem?</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>from datetime import datetime<NewLine>import argparse<NewLine>import torch.multiprocessing as mp<NewLine>import torchvision<NewLine>import torchvision.transforms as transforms<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.distributed as dist<NewLine><NewLine><NewLine>def main():<NewLine>    parser = argparse.ArgumentParser()<NewLine>    parser.add_argument(<NewLine>        ""-n"",<NewLine>        ""--nodes"",<NewLine>        default=1,<NewLine>        type=int,<NewLine>        metavar=""N"",<NewLine>        help=""number of data loading workers (default: 4)"",<NewLine>    )<NewLine>    parser.add_argument(<NewLine>        ""-g"", ""--gpus"", default=1, type=int, help=""number of gpus per node""<NewLine>    )<NewLine>    parser.add_argument(<NewLine>        ""-nr"", ""--nr"", default=0, type=int, help=""ranking within the nodes""<NewLine>    )<NewLine>    parser.add_argument(<NewLine>        ""--epochs"",<NewLine>        default=2,<NewLine>        type=int,<NewLine>        metavar=""N"",<NewLine>        help=""number of total epochs to run"",<NewLine>    )<NewLine>    args = parser.parse_args()<NewLine>    args.world_size = args.gpus * args.nodes<NewLine>    os.environ[""MASTER_ADDR""] = ""tcp://127.0.0.1""<NewLine>    os.environ[""MASTER_PORT""] = ""23456""<NewLine>    mp.spawn(train, nprocs=args.gpus, args=(args,))<NewLine><NewLine><NewLine>class ConvNet(nn.Module):<NewLine>    def __init__(self, num_classes=10):<NewLine>        super(ConvNet, self).__init__()<NewLine>        self.layer1 = nn.Sequential(<NewLine>            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),<NewLine>            nn.BatchNorm2d(16),<NewLine>            nn.ReLU(),<NewLine>            nn.MaxPool2d(kernel_size=2, stride=2),<NewLine>        )<NewLine>        self.layer2 = nn.Sequential(<NewLine>            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),<NewLine>            nn.BatchNorm2d(32),<NewLine>            nn.ReLU(),<NewLine>            nn.MaxPool2d(kernel_size=2, stride=2),<NewLine>        )<NewLine>        self.fc = nn.Linear(7 * 7 * 32, num_classes)<NewLine><NewLine>    def forward(self, x):<NewLine>        out = self.layer1(x)<NewLine>        out = self.layer2(out)<NewLine>        out = out.reshape(out.size(0), -1)<NewLine>        out = self.fc(out)<NewLine>        return out<NewLine><NewLine><NewLine>def train(gpu, args):<NewLine>    rank = args.nr * args.gpus + gpu<NewLine>    dist.init_process_group(<NewLine>        backend=""nccl"",<NewLine>        init_method=""tcp://127.0.0.1:23456"",<NewLine>        world_size=args.world_size,<NewLine>        rank=rank,<NewLine>    )<NewLine>    torch.manual_seed(0)<NewLine>    model = ConvNet()<NewLine>    torch.cuda.set_device(gpu)<NewLine>    model.cuda(gpu)<NewLine>    batch_size = 100<NewLine>    # define loss function (criterion) and optimizer<NewLine>    criterion = nn.CrossEntropyLoss().cuda(gpu)<NewLine>    optimizer = torch.optim.SGD(model.parameters(), 1e-4)<NewLine>    # Wrap the model<NewLine>    model = nn.parallel.DistributedDataParallel(model, device_ids=[gpu])<NewLine>    # Data loading code<NewLine>    train_dataset = torchvision.datasets.MNIST(<NewLine>        root=""./data"",<NewLine>        train=True,<NewLine>        transform=transforms.ToTensor(),<NewLine>        download=True,<NewLine>    )<NewLine>    train_sampler = torch.utils.data.distributed.DistributedSampler(<NewLine>        train_dataset, num_replicas=args.world_size, rank=rank<NewLine>    )<NewLine>    train_loader = torch.utils.data.DataLoader(<NewLine>        dataset=train_dataset,<NewLine>        batch_size=batch_size,<NewLine>        shuffle=False,<NewLine>        num_workers=0,<NewLine>        pin_memory=True,<NewLine>        sampler=train_sampler,<NewLine>    )<NewLine><NewLine>    start = datetime.now()<NewLine>    total_step = len(train_loader)<NewLine>    for epoch in range(args.epochs):<NewLine>        for i, (images, labels) in enumerate(train_loader):<NewLine>            images = images.cuda(non_blocking=True)<NewLine>            labels = labels.cuda(non_blocking=True)<NewLine>            # Forward pass<NewLine>            outputs = model(images)<NewLine>            loss = criterion(outputs, labels)<NewLine><NewLine>            # Backward and optimize<NewLine>            optimizer.zero_grad()<NewLine>            <NewLine>           # Get halts here<NewLine>            if loss.item() &gt; 1.8: <NewLine>                loss.backward()<NewLine>            else:<NewLine>                print(""skipping batch:"", loss.item())<NewLine>            optimizer.step()                <NewLine>            print(""GPU:{}, Epoch [{}/{}], Step [{}/{}], Loss: {}"".format(gpu,epoch + 1, args.epochs, i + 1, total_step, loss))<NewLine>    if gpu == 0:<NewLine>        print(""Training complete in: "" + str(datetime.now() - start))<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    main()<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/vinkle_srivastav,(Vinkle Srivastav),vinkle_srivastav,"January 27, 2020,  1:57pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/smth"">@smth</a> can you please help to solve this issue ?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-python"">if loss.item() &gt; 1.8: <NewLine>    loss.backward()<NewLine>else:<NewLine>    print(""skipping batch:"", loss.item())<NewLine></code></pre><NewLine><p>The above might be the cause of the problem. When using DistributedDataParallel, backward() pass will trigger gradient synchronization communication (all_reduce) across all processes, meaning that all processes need to agree on the number and order of all_reduce calls. However, the above code seems to skip the backward pass in some process but not guarantee to skip in other processes? If that is the case, then processes could run in to desync and cause hang.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the explanation.<br/><NewLine>But, I want if the <code>loss</code> in any process exceed some <code>threshold</code> then no process should should do the gradient update. Is is achievable when using <code>DistributedDataParallel</code> ?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>When using DistributedDataParallel (DDP), loss is a local var. DDP will not communicate loss across processes. In order to make this work, you can do the following on each process</p><NewLine><ol><NewLine><li>run forward on DDP model to calculate loss</li><NewLine><li>create tensor to represent whether the loss is larger than a threshold.</li><NewLine><li>use all_reduce or all_gather to collectively communicate this information to all processes.</li><NewLine><li>After 3, all processes will have the same view on whether they should launch backward+step or not, and hence they can avoid run into desync problems now.</li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vinkle_srivastav; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vinkle_srivastav; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: January 27, 2020,  5:23pm; <NewLine> REPLY_DATE 2: January 27, 2020,  6:58pm; <NewLine> REPLY_DATE 3: January 27, 2020, 10:45pm; <NewLine> REPLY_DATE 4: January 28, 2020,  3:46am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
67205,Multiprocessing - torch.multiprocessing.spawn,2020-01-20T20:08:29.529Z,0,122,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>Can somebody answer pls the following questions</p><NewLine><ul><NewLine><li>can I create in a model and custom data iterator inside the main_method</li><NewLine><li>will there be 4 data sets loaded into the RAM / CPU memory?</li><NewLine><li>will each “for batch_data in…” iterate independently</li><NewLine><li>will the model be updated e.g. every independed batch operation. Obviously I don’t want to have four independed models. What’s the process flow in this case… when gradients are updated etc?</li><NewLine></ul><NewLine><p>I have seen this solution but it uses DataLoader (not a custom iterator) and the model is instantiated before the train method is called. - <a href=""https://github.com/pytorch/examples/tree/master/mnist_hogwild"" rel=""nofollow noopener"">https://github.com/pytorch/examples/tree/master/mnist_hogwild</a></p><NewLine><pre><code class=""lang-auto"">class Net(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Net, self).__init__()<NewLine>        self.l1 = nn.Linear(100, 50)<NewLine>        self.l2 = nn.Linear(50, 2)<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.l2(self.l1(x))<NewLine><NewLine>class CustomDataClassIterator():<NewLine>    def __init__(self):<NewLine>        self.data = None<NewLine>        self.batch_size = 10<NewLine><NewLine>    def __iter__(self):<NewLine>        while True:<NewLine>            yield<NewLine><NewLine><NewLine>def main_method(i, args):<NewLine>    print(i, datetime.datetime.now())<NewLine>    model = Net()<NewLine>    data = CustomDataClassIterator()<NewLine>    for epoch in args.epoch_n:<NewLine>        for batch_data in data:<NewLine>            pass # some stuff<NewLine><NewLine>if __name__ == '__main__':<NewLine>    args = {'test': 10}<NewLine>    torch.multiprocessing.spawn(fn=main_method, args=(args), nprocs=4)<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/TommyLeeJones,(TommyLeeJones),TommyLeeJones,"January 20, 2020,  8:30pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>can I create in a model and custom data iterator inside the main_method?</p><NewLine></blockquote><NewLine><p>Given the above example, you created a generator to produce input data? If that is the case, yes, sure you can do that.</p><NewLine><blockquote><NewLine><p>will there be 4 data sets loaded into the RAM / CPU memory?</p><NewLine></blockquote><NewLine><p>I am assumingg <code>pass # some stuff</code> statement will be replaced by actual forward-backward-step functions? If that is the case, then the 4 date sets won’t be loaded into memory at the same time. Instead, each data set will no longer be needed and can be gc-ed at the end of every iteration.</p><NewLine><blockquote><NewLine><p>will each “for batch_data in…” iterate independently</p><NewLine></blockquote><NewLine><p>Yes. It will have it’s own forward pass (building autograd graph), backward pass (generating grads and sync them if necessary), and step function (updating params)</p><NewLine><blockquote><NewLine><p>will the model be updated e.g. every independed batch operation. Obviously I don’t want to have four independed models. What’s the process flow in this case… when gradients are updated etc?</p><NewLine></blockquote><NewLine><p>When you call backward, the gradient will be accumulated into <code>Tensor.grad</code>, and it is up to you regarding when to call <code>Optimizer.step()</code> to apply those grads into the parameter.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I saw you had a pointer to the hogwild training example. Could you please elaborate your use case? Are you looking for distributed data parallel training (like nn.parallel.DistributedDataParallel) or specifically asking for hogwild?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: January 27, 2020,  6:44pm; <NewLine> REPLY_DATE 2: January 27, 2020,  6:48pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
67516,Are optimizer per-parameter options supported with DistributedDataParallel?,2020-01-24T00:31:11.289Z,0,153,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi All,</p><NewLine><ol><NewLine><li>Lets suppose I have a model that I want to train using DistributedDataParallel, I wrap my model with DistributedDataParallel as follows:</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">ddp_model = DDP(model, device_ids=[device])<NewLine></code></pre><NewLine><ol start=""2""><NewLine><li>I init my optimizer as follows:</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">optim = optim.SGD(ddp_model.parameters(), lr=1e-2)<NewLine></code></pre><NewLine><p>Is there a way to modify step 2, to apply per parameter optimizer options? What does the following look like given the ddp model?</p><NewLine><pre><code class=""lang-auto"">optim.SGD([<NewLine>                {'params': model.base.parameters()},<NewLine>                {'params': model.classifier.parameters(), 'lr': 1e-3}<NewLine>            ], lr=1e-2, momentum=0.9)<NewLine></code></pre><NewLine><p>As on <a href=""https://pytorch.org/docs/stable/optim.html#per-parameter-options"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/optim.html#per-parameter-options</a></p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/nalapati,(Nishanth Alapati),nalapati,"January 24, 2020, 12:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I believe per-parameter options should be supported by DistributedDataParallel. Have you tried it out and seen any issues? If you do see issues/unexpected behavior feel free to open an issue on <a href=""https://github.com/pytorch/pytorch"" rel=""nofollow noopener"">github</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> ,"REPLY_DATE 1: January 25, 2020,  1:06am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
67452,Is it possible to run hyperparameter optimization on multiple GPUs in parallel?,2020-01-23T10:31:49.147Z,0,279,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Suppose I have a single node with 4 GPUs. I would like to do model selection w.r.t a specific dataset through random search. A manager will keep track of a grid of such hyperparams. At each iteration a distinct model with a specific setting will be created and trained on an assigned GPU. The training dataset is shared by all such models. A minimal example will be like following</p><NewLine><pre><code class=""lang-auto"">class HyperSearchManager:<NewLine>    def __init__(self, <NewLine>                 train_dataset: torch.utils.data.Dataset,<NewLine>                 valid_dataset: torch.utils.data.Dataset,<NewLine>                 test_dataset: torch.utils.data.Dataset,<NewLine>                 param_grid: Dict[str, List]):<NewLine>        self.train_dataset = train_dataset<NewLine>        self.valid_dataset = valid_dataset<NewLine>        self.test_dataset = test_dataset<NewLine>        self.param_grid = param_grid<NewLine><NewLine>        self.best = float('inf')<NewLine>        self.optimal_model = None<NewLine><NewLine>    def param_iter(self) -&gt; Dict:<NewLine>        ...<NewLine>        yield params<NewLine><NewLine>    def train_single_model(self, model: nn.Module, num_epoch: int, device: torch.device):<NewLine>        # copy model to the respective device<NewLine>        model = model.to(device)<NewLine>        # train loops for a single model<NewLine>        loader = torch.utils.data.DataLoader(self.train_dataset, batch_size, ...)<NewLine>        optimizer = torch.optim.Adam(model.parameters(), lr, ...)<NewLine>        for epoch in range(num_epoch):<NewLine>            for data in loader:<NewLine>                data = data.to(device)<NewLine>                train(model, data, optimizer)<NewLine>                ...<NewLine>            # Do validation with early stopping, etc.<NewLine>            valid_loss = validation(model, self.valid_dataset)<NewLine>        # update optimal model according to valid metrics<NewLine>        self.update(model.cpu(), valid_loss)<NewLine><NewLine>    def update(self, model, valid_loss):<NewLine>        # if valid_loss is minimal, keep current model<NewLine>        if valid_loss &lt; self.best:<NewLine>            self.optimal_model = model<NewLine><NewLine>    def search(self):<NewLine>        for _ in range(MAX_HYPER_OPT_ITER):<NewLine>            params = next(self.param_iter) # get next hyperparam combination<NewLine>            model = ModuleClass(**params) # create model for the specific hyperparam<NewLine><NewLine>            # if a free gpu is available, create a new subprocess to run the model on the allocated gpu<NewLine>            device = self.get_available_device()<NewLine>            proc = multiprocessing.Process(target=self.train_single_model, args=(model, num_epochs, device))<NewLine>            proc.start()<NewLine>            # else waiting...<NewLine><NewLine>        run_test(self.optimal_model, self.test_dataset)                <NewLine><NewLine></code></pre><NewLine><p>I wonder if it is possible to find a schedule to allocate idle gpu for a pending model. That is, at first 4 models are trained on 4 gpus, respectively. Once a training process is finished, a new model will be assigned to the released GPU.</p><NewLine><p>If that’s not straightforward, is there any easy implementation for such functionalities?</p><NewLine></div>",https://discuss.pytorch.org/u/XYJin,,XYJin,"January 23, 2020, 10:37am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>As far as I know PyTorch doesn’t provide a framework to do this automatically. You will have to build this scheduling mechanism in your application itself.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> ,"REPLY_DATE 1: January 25, 2020, 12:41am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
67518,Discrepancy in tensors shapes GRU,2020-01-24T02:37:25.199Z,0,114,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a GRU model which I am applying to time-series data , the class look like the following:</p><NewLine><pre><code class=""lang-auto"">class GRUNet(nn.Module):<NewLine>    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):<NewLine>        super(GRUNet, self).__init__()<NewLine>        self.hidden_dim = hidden_dim<NewLine>        self.n_layers = n_layers<NewLine>        <NewLine>        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)<NewLine>        self.fc = nn.Linear(hidden_dim, output_dim)<NewLine>        self.relu = nn.ReLU()<NewLine>        <NewLine>    def forward(self, x, h):<NewLine>        print('x inside forward {}'.format(x))<NewLine>        out, h = self.gru(x, h)<NewLine>        print('out shape :{}'.format(out.shape))<NewLine>        out = self.fc(self.relu(out[:,-1]))<NewLine>        return out, h<NewLine>    <NewLine>    def init_hidden(self, batch_size):<NewLine>        weight = next(self.parameters()).data<NewLine>        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)<NewLine>        return hidden<NewLine></code></pre><NewLine><p>and my training function is :</p><NewLine><pre><code class=""lang-auto"">def train(model, device, federated_train_loader, optimizer, epoch):<NewLine>    model.train()<NewLine>    # Iterate through each gateway's dataset<NewLine>    for idx, (seq, labels) in enumerate(federated_train_loader):<NewLine>        batch_idx = idx+1<NewLine>        # Send the model to the right gateway<NewLine>        model.send(seq.location)<NewLine>        # Move the data and target labels to the device (cpu/gpu) for computation<NewLine>        seq, labels = seq.to(device), labels.to(device)<NewLine>        h = model.init_hidden(BATCH_SIZE)<NewLine>        # Clear previous gradients (if they exist)<NewLine>        optimizer.zero_grad()<NewLine>        # Make a prediction<NewLine>        print('seq shape : {}'.format(seq.shape))<NewLine>        print('labels shape : {}'.format(labels.shape))<NewLine>        output, h = model(seq, h)<NewLine>        # Calculate huber loss for regression problems<NewLine>        #labels =labels.view(-1)<NewLine>        #seq = seq.view(-1)<NewLine>        #labels = labels.unsqueeze(1)<NewLine>        #labels = labels.float()<NewLine>        loss = loss_function(output, labels)<NewLine>        # Calculate the gradients<NewLine>        loss.backward()<NewLine>        # Update the model weights<NewLine>        optimizer.step()<NewLine>        # Get the model back from the gateway<NewLine>        #model.get()<NewLine>        if batch_idx==len(federated_train_loader) or (batch_idx!=0 and batch_idx % LOG_INTERVAL == 0):<NewLine>            # get the loss back<NewLine>            loss = loss.get()<NewLine>            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(<NewLine>                epoch, batch_idx * BATCH_SIZE, len(federated_train_loader) * BATCH_SIZE,<NewLine>                100. * batch_idx / len(federated_train_loader), loss.item()))<NewLine></code></pre><NewLine><p>I initiated and called the model and printed the shapes as follows :</p><NewLine><pre><code class=""lang-auto"">model = GRUNet(input_dim=1, hidden_dim=100, output_dim=1, n_layers=2)<NewLine><NewLine>GRUNet(<NewLine>  (gru): GRU(1, 100, num_layers=2, batch_first=True, dropout=0.2)<NewLine>  (fc): Linear(in_features=100, out_features=1, bias=True)<NewLine>  (relu): ReLU()<NewLine>)<NewLine>seq shape : torch.Size([1024, 1, 1])<NewLine>labels shape : torch.Size([1024, 1, 1])<NewLine>x inside forward (Wrapper)&gt;[PointerTensor | me:36457989435 -&gt; gatway1:28694227328]<NewLine></code></pre><NewLine><p>I got the following error at the end :</p><NewLine><pre><code class=""lang-auto"">RuntimeError                              Traceback (most recent call last)<NewLine>&lt;timed exec&gt; in &lt;module&gt;<NewLine><NewLine>&lt;ipython-input-30-8013666c5ed1&gt; in train(model, device, federated_train_loader, optimizer, epoch)<NewLine>     14         print('seq shape : {}'.format(seq.shape))<NewLine>     15         print('labels shape : {}'.format(labels.shape))<NewLine>---&gt; 16         output, h = model(seq, h)<NewLine>     17         # Calculate huber loss for regression problems<NewLine>     18         #labels =labels.view(-1)<NewLine><NewLine>~/anaconda3/envs/ftorch/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)<NewLine>    539             result = self._slow_forward(*input, **kwargs)<NewLine>    540         else:<NewLine>--&gt; 541             result = self.forward(*input, **kwargs)<NewLine>    542         for hook in self._forward_hooks.values():<NewLine>    543             hook_result = hook(self, input, result)<NewLine><NewLine>&lt;ipython-input-26-be5b95661398&gt; in forward(self, x, h)<NewLine>     11     def forward(self, x, h):<NewLine>     12         print('x inside forward {}'.format(x))<NewLine>---&gt; 13         out, h = self.gru(x, h)<NewLine>     14         print('out shape :{}'.format(out.shape))<NewLine>     15         out = self.fc(self.relu(out[:,-1]))<NewLine><NewLine>~/anaconda3/envs/ftorch/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)<NewLine>    539             result = self._slow_forward(*input, **kwargs)<NewLine>    540         else:<NewLine>--&gt; 541             result = self.forward(*input, **kwargs)<NewLine>    542         for hook in self._forward_hooks.values():<NewLine>    543             hook_result = hook(self, input, result)<NewLine><NewLine>~/anaconda3/envs/ftorch/lib/python3.7/site-packages/torch/nn/modules/rnn.py in forward(self, input, hx)<NewLine>    727             return self.forward_packed(input, hx)<NewLine>    728         else:<NewLine>--&gt; 729             return self.forward_tensor(input, hx)<NewLine>    730 <NewLine>    731 <NewLine><NewLine>~/anaconda3/envs/ftorch/lib/python3.7/site-packages/torch/nn/modules/rnn.py in forward_tensor(self, input, hx)<NewLine>    719         sorted_indices = None<NewLine>    720         unsorted_indices = None<NewLine>--&gt; 721         output, hidden = self.forward_impl(input, hx, batch_sizes, max_batch_size, sorted_indices)<NewLine>    722         return output, self.permute_hidden(hidden, unsorted_indices)<NewLine>    723 <NewLine><NewLine>~/anaconda3/envs/ftorch/lib/python3.7/site-packages/torch/nn/modules/rnn.py in forward_impl(self, input, hx, batch_sizes, max_batch_size, sorted_indices)<NewLine>    696             hx = self.permute_hidden(hx, sorted_indices)<NewLine>    697 <NewLine>--&gt; 698         self.check_forward_args(input, hx, batch_sizes)<NewLine>    699         result = self.run_impl(input, hx, batch_sizes)<NewLine>    700         output = result[0]<NewLine><NewLine>~/anaconda3/envs/ftorch/lib/python3.7/site-packages/torch/nn/modules/rnn.py in check_forward_args(self, input, hidden, batch_sizes)<NewLine>    168     def check_forward_args(self, input, hidden, batch_sizes):<NewLine>    169         # type: (Tensor, Tensor, Optional[Tensor]) -&gt; None<NewLine>--&gt; 170         self.check_input(input, batch_sizes)<NewLine>    171         expected_hidden_size = self.get_expected_hidden_size(input, batch_sizes)<NewLine>    172 <NewLine><NewLine>~/anaconda3/envs/ftorch/lib/python3.7/site-packages/torch/nn/modules/rnn.py in check_input(self, input, batch_sizes)<NewLine>    147             raise RuntimeError(<NewLine>    148                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(<NewLine>--&gt; 149                     self.input_size, input.size(-1)))<NewLine>    150 <NewLine>    151     def get_expected_hidden_size(self, input, batch_sizes):<NewLine><NewLine>RuntimeError: input.size(-1) must be equal to input_size. Expected 1, got 0<NewLine></code></pre><NewLine><p>A small note regarding this implementation, when I use The GRU alone and I generate the input using <code> torch.randn(1024, 1,1)</code> it works . But when I use it on my dataset through syft library for federated data it doesn’t work. Can it be the reason behind it. I also provided the shapes of my federated data but it is the same as the randomized tensor :</p><NewLine><pre><code class=""lang-auto"">train_inputs shape : torch.Size([815942, 1, 1])<NewLine>train_labels shape : torch.Size([815942, 1])<NewLine>test_inputs shape : torch.Size([149999, 1, 1])<NewLine>test_labels shape : torch.Size([149999, 1])<NewLine>gatway1_train_dataset : &lt;syft.frameworks.torch.fl.dataset.BaseDataset object at 0x7fd7023e42d0&gt;<NewLine>gatway2_train_dataset : &lt;syft.frameworks.torch.fl.dataset.BaseDataset object at 0x7fd6d0e4bf90&gt;<NewLine>federated_train_dataset : FederatedDataset<NewLine>    Distributed accross: gatway1, gatway2<NewLine>    Number of datapoints: 815942<NewLine><NewLine>federated_test_dataset : FederatedDataset<NewLine>    Distributed accross: gatway1, gatway2<NewLine>    Number of datapoints: 149999<NewLine></code></pre><NewLine><p>I have been stuck for a while now and I have seen other GRU models working properly on federated data. Any clue ? much appriciated!!</p><NewLine></div>",https://discuss.pytorch.org/u/jagoul,(jagoul smith),jagoul,"January 24, 2020,  2:37am",,,,,
66325,Training independent models simultaneously,2020-01-10T20:03:08.755Z,1,116,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to train multiple models simultaneously?<br/><NewLine>For instance, suppose by nettwork class is Net.</p><NewLine><p>net1 = Net()<br/><NewLine>net2 = Net()</p><NewLine><p>Is it possible to train net 1 and net 2 simultaneously?</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/sawal86,,sawal86,"January 10, 2020,  8:03pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I assume you need to do this because you want to use different training data for the models? In that case, yes, that should be possible. I think you can wrap them with a wrapper module, sth like:</p><NewLine><pre><code class=""lang-python"">class WrapperModule(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(WrapperModule, self).__init__()<NewLine>        self.net0 = Net()<NewLine>        self.net1 = Net()<NewLine><NewLine>    def forward(inputs):<NewLine>        return [self.net0(inputs[0]), self.net1(inputs[1])]<NewLine><NewLine>net = WrapperModule()<NewLine>opt = SomeOptimizer(net.parameters())<NewLine>ddp = DistributedDataParallel(net)<NewLine>ddp.forward(inputs).backward()<NewLine>opt.step()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, just realized you didn’t mention DistributedDataParallel in the question. Is this for distributed training? Could you please provide more contexts?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can pass each of your model to a different GPU. See <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/how-to-deploy-different-scripts-on-different-gpus/61011/3"">How to deploy different scripts on different GPUs?</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/TinfoilHat0; <NewLine> ,"REPLY_DATE 1: January 16, 2020,  5:23pm; <NewLine> REPLY_DATE 2: January 16, 2020,  5:24pm; <NewLine> REPLY_DATE 3: January 18, 2020, 12:29am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
59971,PyTorch+Windows: is parallelization over multiple GPUs now possible?,2019-11-04T17:02:37.691Z,1,934,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am planning to add a new GPU on my computer. Using Pytorch on Windows, I wonder if it will be possible for me to use parallelism.</p><NewLine><p>In 2017 it wasn’t possible(<a href=""https://github.com/pytorch/pytorch/issues/4391"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/4391</a>).<br/><NewLine>Has it changed? Will I have to switch to Linux? And is there a guide of how to install Pytorch for DataParallelism?</p><NewLine><p>Thank you for all the library!</p><NewLine></div>",https://discuss.pytorch.org/u/SmoothPQ,(SmoothPQ),SmoothPQ,"November 4, 2019,  5:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am not aware of a NCCL binary from nvidia that supports windows, so parallelization over multiple GPUs on windows is still not possible.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much for the answer. Is nccl installed automatically when installing CUDA on Linux, or do I need to add something else?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Im also on a Windows system.  I was able to use dataparallel on my model without any apparent errors.  However, the performance was actually worse;  which makes me think that it’s not actually using multiple gpus.  Why am I able to use multiple gpus in tensorflow on a windows system, but not pytorch?  There must be some hack to get be able to do this.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""3"" data-topic=""59971""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/7ba0ec/40.png"" width=""20""/> SmoothPQ:</div><NewLine><blockquote><NewLine><p>Thank you very much for the answer. Is nccl installed automatically when installing CUDA on Linux, or do I need to add something else?</p><NewLine></blockquote><NewLine></aside><NewLine><p>On Linux, NCCL and torch.distributed will be enabled by default. On MacOs, with PyTorch 1.3.1+, you need to conda install libuv and pkg-config explicitly set USE_DISTRIBUTED=1 when compiling from source. For Windows, torch.distributed is not enabled yet.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I was able to use dataparallel on my model without any apparent errors. However, the performance was actually worse; which makes me think that it’s not actually using multiple gpus.</p><NewLine></blockquote><NewLine><p>DataParallel is single-process-multi-thread data parallelism, and it replicates the input module in every forward pass, which is expected to be slow, but this is a very convenient entry point for enabling parallelism. You don’t need to do anything to enable that, and it should work fine if the batch size is large enough (to shadow the model replicating overhead)</p><NewLine><blockquote><NewLine><p>Why am I able to use multiple gpus in tensorflow on a windows system</p><NewLine></blockquote><NewLine><p>We are working on using libuv to enable that, as <a class=""mention"" href=""/u/pietern"">@pietern</a> did for Windows, but timeline is TBD</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/rvarm1; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/SmoothPQ; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/MLnut; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: November 5, 2019,  6:14pm; <NewLine> REPLY_DATE 2: November 6, 2019,  3:19pm; <NewLine> REPLY_DATE 3: January 17, 2020,  1:02am; <NewLine> REPLY_DATE 4: January 17, 2020,  8:12pm; <NewLine> REPLY_DATE 5: January 17, 2020,  8:15pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
65611,Dist.init_process_group works but TCPStore failed,2020-01-02T17:59:18.948Z,0,125,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I am trying init dist and get stuck.<br/><NewLine>I have 2 nodes: master and slave, both pytorch 1.3.1 installed by anaconda<br/><NewLine>It works on both when:</p><NewLine><pre><code class=""lang-auto"">    dist.init_process_group(<NewLine>            backend =""NCCL"",<NewLine>            world_size = 2,<NewLine>            rank = 0,# 0 for master and 1 for slave<NewLine>            init_method=""tcp://192.168.1.102:23458""#master addr and port<NewLine>            )<NewLine></code></pre><NewLine><p>It is hung up on both when:</p><NewLine><pre><code class=""lang-auto"">store = dist.TCPStore(""192.168.1.102"", 23458, 2, 0)<NewLine></code></pre><NewLine><p>Could somebody help?<br/><NewLine>Thanks in advance and Happy New Year.</p><NewLine></div>",https://discuss.pytorch.org/u/forhonourlx,(Forhonourlx),forhonourlx,"January 2, 2020,  5:59pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://github.com/pytorch/pytorch/blob/20e5c90d8246aac61f70e84c78f0dd9670b76987/torch/distributed/rendezvous.py#L125"" rel=""nofollow noopener"">This</a> is how TCPStore is initialized when you call <code>init_process_group</code>. You can print out the args on both process to check which one went wrong.</p><NewLine><p>For <code>start_daemon</code>, did you pass in 1 for rank0 and 0 for rank1?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: January 16, 2020,  5:54pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
65573,Deterministic problem when infer same input,2020-01-02T08:09:27.182Z,0,132,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When i infer the same input, the output is not deterministic sometimes, and the code is as below.<br/><NewLine>Debug found that the posterior is same, but the sample is different for same input sometimes.</p><NewLine><pre><code class=""lang-auto"">                    posterior = F.softmax(logits, dim=1)                  <NewLine>                    distrib = torch.distributions.Categorical(posterior)<NewLine>                    sample = distrib.sample().float()<NewLine>                    <NewLine></code></pre><NewLine><p>I execute model.eval() and have set the seed at the begining. Do you have any suggestions? Thank you.</p><NewLine><pre><code class=""lang-auto"">seed = 1234<NewLine>torch.manual_seed(seed)<NewLine>torch.cuda.manual_seed(seed)<NewLine>torch.cuda.manual_seed_all(seed)<NewLine>np.random.seed(seed)<NewLine>random.seed(seed)<NewLine>torch.backends.cudnn.benchmark = False<NewLine>torch.backends.cudnn.deterministic = True<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/xinzheshen,,xinzheshen,"January 2, 2020,  8:09am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you ask this question with Uncategorized label. It seems there is some randomness somewhere, but <code>torch.distributed</code> (not <code>torch.distributions</code>) is not involved here.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: January 16, 2020,  5:48pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
65438,Using Pytorch&rsquo;s multiprocessing along with distributed package,2019-12-31T06:59:03.861Z,0,1065,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to spawn a couple of process using pytorch’s multiprocessing module within a openmpi distributed back-end. What I have is the following code:</p><NewLine><pre><code>def run(rank_local, rank, world_size, maingp):<NewLine>    print(""I WAS SPAWNED "", rank_local, "" OF "", rank)<NewLine>    <NewLine>    tensor = torch.zeros(1)<NewLine>    tensor += 1<NewLine>    <NewLine>    if rank == 0:<NewLine>        tensor += 100<NewLine>        dist.send(tensor, dst=1)<NewLine>    else:<NewLine>        print(""I am spawn: "", rank, ""and my tensor value before receive: "", tensor[0])<NewLine>        dist.recv(tensor, src=0)<NewLine>        print(""I am spawn: "", rank, ""and my tensor value after  receive: "", tensor[0])<NewLine>    <NewLine><NewLine>if __name__ == '__main__':<NewLine>    <NewLine>    # Initialize Process Group<NewLine>    dist.init_process_group(backend=""mpi"", group_name=""main"")<NewLine>    maingp = None #torch.distributed.new_group([0,1])<NewLine>    mp.set_start_method('spawn')    <NewLine>    <NewLine>    # get current process information<NewLine>    world_size = dist.get_world_size()<NewLine>    rank = dist.get_rank()<NewLine>    <NewLine>    # Establish Local Rank and set device on this node<NewLine>    mp.spawn(run, args=(rank, world_size, maingp), nprocs=1)<NewLine></code></pre><NewLine><p>I run this code using the openmpi as follows:</p><NewLine><pre><code>mpirun -n 2 python code.py<NewLine></code></pre><NewLine><p>So my understanding is that mpirun creates two process with ranks [0, 1], each of these process spawn new process with their local rank as 0. Now if I want to communicate between these two sub-processes of the main process I get some Traceback and following error:</p><NewLine><pre><code>-- Process 0 terminated with the following error:<NewLine>Traceback (most recent call last):<NewLine>  File ""/home/usama/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 19, in _wrap<NewLine>    fn(i, *args)<NewLine>  File ""/home/usama/code/test/code.py"", line 19, in run<NewLine>    dist.send(tensor, dst=1)<NewLine>  File ""/home/usama/anaconda3/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 666, in send<NewLine>    _check_default_pg()<NewLine>  File ""/home/usama/anaconda3/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 191, in _check_default_pg<NewLine>    ""Default process group is not initialized""<NewLine>AssertionError: Default process group is not initialized<NewLine></code></pre><NewLine><p>My question is how do I make these sub-processes to be able to communicate i.e the [0, 0] process sending something to [1, 0] process. Any ideas?</p><NewLine><p>I have asked this on stack-overflow as well but to no avail!!!<br/><NewLine><aside class=""onebox stackexchange""><NewLine><header class=""source""><NewLine><a href=""https://stackoverflow.com/questions/56054164/using-pytorchs-multiprocessing-along-with-distributed"" rel=""nofollow noopener"" target=""_blank"">stackoverflow.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://stackoverflow.com/users/2555668/usama-zafar"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""Usama Zafar"" class=""thumbnail onebox-avatar"" height=""128"" src=""https://i.stack.imgur.com/048Fx.jpg?s=128&amp;g=1"" width=""128""/><NewLine></a><NewLine><h4><NewLine><a href=""https://stackoverflow.com/questions/56054164/using-pytorchs-multiprocessing-along-with-distributed"" rel=""nofollow noopener"" target=""_blank"">Using Pytorch's Multiprocessing along with Distributed</a><NewLine></h4><NewLine><div class=""tags""><NewLine><strong>python, pytorch, openmpi</strong><NewLine></div><NewLine><div class=""date""><NewLine>  asked by<NewLine>  <NewLine>  <a href=""https://stackoverflow.com/users/2555668/usama-zafar"" rel=""nofollow noopener"" target=""_blank""><NewLine>    Usama Zafar<NewLine>  </a><NewLine>  on <a href=""https://stackoverflow.com/questions/56054164/using-pytorchs-multiprocessing-along-with-distributed"" rel=""nofollow noopener"" target=""_blank"">07:22AM - 09 May 19 UTC</a><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>",https://discuss.pytorch.org/u/Usama-Zafar,(Usama Zafar),Usama-Zafar,"December 31, 2019,  7:01am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""65438""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/usama-zafar/40/11955_2.png"" width=""20""/> Usama-Zafar:</div><NewLine><blockquote><NewLine><p>AssertionError: Default process group is not initialized</p><NewLine></blockquote><NewLine></aside><NewLine><p>above suggests the <code>init_process_group</code> method is not called on the process that tries to use the distributed package. I think the follow line needs to be moved to the <code>run</code> method, and it is the entry point for the spawned process:</p><NewLine><pre><code class=""lang-auto""># Initialize Process Group<NewLine>dist.init_process_group(backend=""mpi"", group_name=""main"")<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group quote-modified"" data-post=""1"" data-topic=""65438""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/usama-zafar/40/11955_2.png"" width=""20""/> Usama-Zafar:</div><NewLine><blockquote><NewLine><p>dist.init_process_group(backend=“mpi”, group_name=“main”)</p><NewLine></blockquote><NewLine></aside><NewLine><p>BTW, the way you call <code>init_process_group</code> does not look correct to me. You will need to either provide rank+world_size or provide an initialized store. The former will be easier.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: January 16, 2020,  5:39pm; <NewLine> REPLY_DATE 2: January 16, 2020,  5:41pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
65410,Using custom method in distributed model,2019-12-30T21:14:34.981Z,0,168,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Not sure if this is the right way to do it. I’m wrapping my model in nn.DataParallel for multi gpu training. There is an LSTM module as part of this model. This LSTM module has a custom method that resets the hidden states, called after each time a forward pass is done during training. This is the only custom method that’s used.</p><NewLine><p>To access this reset method in the parallel model, I do<br/><NewLine><code>model.module.lstm.reset_hidden_state()</code><br/><NewLine>Whereas if my model is <strong>not</strong> wrapped in DataParallel, it would just be<br/><NewLine><code>model.lstm.reset_hidden_state()</code></p><NewLine><p>Is this right, or do I have to write a custom DataParallel wrapper that has scatter, gather, etc methods? If so, how would I do it?</p><NewLine><p>This is the lstm module:</p><NewLine><pre><code class=""lang-auto"">class LSTM(nn.Module):<NewLine>    def __init__(self, latent_dim, num_layers, hidden_dim):<NewLine>        super().__init__()<NewLine>        self.lstm = nn.LSTM(input_size=latent_dim, num_layers=num_layers, hidden_size=hidden_dim, batch_first=True, dropout=0.0)<NewLine>        self.hidden_state = None<NewLine>    def reset_hidden_state(self):<NewLine>        self.hidden_state = None<NewLine>    def forward(self,X):<NewLine>        self.lstm.flatten_parameters()<NewLine>        X, self.hidden_state = self.lstm(X, self.hidden_state)<NewLine>        return X<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/bigyeet,,bigyeet,"December 30, 2019,  9:17pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""65410""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/b/dfb087/40.png"" width=""20""/> bigyeet:</div><NewLine><blockquote><NewLine><p>Is this right, or do I have to write a custom DataParallel wrapper that has scatter, gather, etc methods? If so, how would I do it?</p><NewLine></blockquote><NewLine></aside><NewLine><p>It depends on what you expected <code>reset_hidden_state</code> to achieve. Below is what happens in EVERY forward pass when you use DataParallel.</p><NewLine><ol><NewLine><li>split input data</li><NewLine><li>replicate model to all devices</li><NewLine><li>feed input data splits to all model replicas</li><NewLine><li>gather outputs from all replicas</li><NewLine><li>done with forward</li><NewLine></ol><NewLine><p>After the forward pass, the autograd graph actually contains multiple model replicas. It looks sth like</p><NewLine><p>original model &lt;- scatter &lt;- model replicas &lt;- replica output &lt;- gather &lt;- final output.</p><NewLine><p>So in your above use case, if <code>reset_hidden_state</code> has any side effect that you would like to apply to the backward pass, it will only apply to the original model, not to model replicas. But if you are only trying to clear some states for the next forward pass, it should work.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: February 17, 2020,  7:38am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
66682,Data splitting in DistributedDataParallel,2020-01-15T00:05:53.328Z,0,570,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m trying to use DistributedDataParallel on a CPU-only machine with multiple cores.</p><NewLine><p>The documentation for DDP (<a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py</a>) states: ""For multi-device modules and CPU modules, device_ids must be None or an empty list, and <strong>input data for the forward pass must be placed on the correct device</strong>. (default: all devices for single-device modules).</p><NewLine><p>I want to parallelize training across CPU processes in a single machine. My dataset is an in-memory numpy array.</p><NewLine><p>Would I have to manually separate this dataset into different subsets, and load each subset for each CPU process? Does splitting the input along the batch dimension work for CPU modules as well? I am using torch’s multiprocessing module to spawn processes to use with my DDP model.</p><NewLine><p>Thank you.</p><NewLine><p>PS. What’s the best practice for sharing an in-memory array across torch processes? That would be helpful as well.</p><NewLine><hr/><NewLine><p>Some additional observations:</p><NewLine><p>When I print the loss in each process, the loss value is the same. If the data was being split properly by DDP, wouldn’t each process have a different loss value?</p><NewLine></div>",https://discuss.pytorch.org/u/juniper,,juniper,"January 15, 2020, 12:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>From my experiments, it appears that for DDP using CPU processes, there is no splitting of data across the batch dimension across processes.</p><NewLine><p>In the source code as well, if the model’s device_ids is None, then scattering is not performed in the forward() pass of the model.</p><NewLine><p>Can someone more authoritative confirm this behavior?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>In the source code as well, if the model’s device_ids is None, then scattering is not performed in the forward() pass of the model.</p><NewLine></blockquote><NewLine><p>Yes, this is correct.</p><NewLine><p>Input data split only occurs in two situations:</p><NewLine><ol><NewLine><li>When using DataParallel (single-process multi-thread)</li><NewLine><li>Using DistributedDataParallel (DDP), and provide a <code>device_ids</code> list of multiple CUDA devices. In this case, each DDP process will operate on multiple devices and multiple model replicas, and hence need to split the input data. (This is not recommended, as this could be slow)</li><NewLine></ol><NewLine><p>For the recommended use case of DDP (one device/replica per DDP process), DDP will NOT split input or distributed them into multiple processes. Each DDP process needs to read its own input data independently. You could try manually splitting those data (say on rank0) and pass them across processes though, if they are on the same machine. Or, I also saw many people using the <a href=""https://pytorch.org/cppdocs/api/classtorch_1_1data_1_1samplers_1_1_distributed_sampler.html"" rel=""nofollow noopener"">DistributedSampler</a> to load input data</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/juniper; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: January 15, 2020,  3:04pm; <NewLine> REPLY_DATE 2: January 16, 2020,  5:10pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
65644,DistributedDataParralled not support cuda?,2020-01-03T07:00:43.507Z,0,109,"<div class=""post"" itemprop=""articleBody""><NewLine><p>there was not proper synchronization with the CUDA events that recored copies into this contents tensor before bucket contents tensor allreduce. it seems like not supporting cuda?</p><NewLine><p>web link:<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/reducer.cpp#L404"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/reducer.cpp#L404"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/reducer.cpp#L404</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""394"" style=""counter-reset: li-counter 393 ;""><NewLine><li><NewLine></li><NewLine><li>// Keep going, until we either:</li><NewLine><li>// - have kicked off reduction for all buckets, or</li><NewLine><li>// - found a bucket that's not yet ready for reduction.</li><NewLine><li>for (; next_bucket_ &lt; buckets_.size() &amp;&amp; buckets_[next_bucket_].pending == 0;</li><NewLine><li>     next_bucket_++) {</li><NewLine><li>  auto&amp; bucket = buckets_[next_bucket_];</li><NewLine><li>  std::vector&lt;at::Tensor&gt; tensors;</li><NewLine><li>  tensors.reserve(bucket.replicas.size());</li><NewLine><li>  for (const auto&amp; replica : bucket.replicas) {</li><NewLine><li class=""selected"">    // TODO(@pietern): Ensure proper synchronization with the CUDA events</li><NewLine><li>    // that recorded copies into this contents tensor. If these copies are</li><NewLine><li>    // executed on non-default streams, the current stream for the device</li><NewLine><li>    // that holds the contents tensor must wait on these events.</li><NewLine><li>    //</li><NewLine><li>    // As long as autograd uses the default stream for every device,</li><NewLine><li>    // these operations are implicitly sequenced, and we don't need to</li><NewLine><li>    // do any extra synchronization here.</li><NewLine><li>    //</li><NewLine><li>    tensors.push_back(replica.contents);</li><NewLine><li>  }</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>",https://discuss.pytorch.org/u/liubanglan,(Liubanglan),liubanglan,"January 3, 2020,  7:00am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>DistributedDataParallel (DDP) does supports CUDA. The comment suggests extra care might be necessary when backward run on non-default stream. Actually, even if backward occurs on non-default streams it should be fine for most use cases. Below is why:</p><NewLine><p>background: I learned from <a class=""mention"" href=""/u/alband"">@albanD</a> that autograd engine will use the same stream as the forward pass.</p><NewLine><p>Let’s take a look at what could go wrong for the code you quoted.</p><NewLine><p>1: the tensor is not ready when launching the allreduce operation<br/><NewLine>2: the tensor was destroyed too soon before the allreduce finishes.</p><NewLine><p>We can rule out 2 for now, as all_reduce does recordStream() properly to prevent CUDA blocks to be freed too early.</p><NewLine><p>Then the only thing left is 1. The operation on that tensor before allreduce is <code>bucket_view.copy_(grad.view({-1}), /* non_blocking */ true);</code> in <code>mark_variable_ready_dense</code>. The copy here happens on the same device (replica.contents and grad). And Reducer itself does not switch streams in between. So the only case that could hit race condition is when the application used different streams for different operators during the forward pass, and grads associated with those operators fall into the same bucket in reducer.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: February 7, 2020,  1:56am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
65999,"Use 4 gpu to train model, loss batch_size = batch_size * 4",2020-01-07T08:02:08.148Z,1,121,"<div class=""post"" itemprop=""articleBody""><NewLine><p>my code:</p><NewLine><pre><code class=""lang-auto"">batch_size = 64<NewLine>model = nn.DataParallel(model, device_ids=[0,1,2,3], dim=0)<NewLine>model.cuda()<NewLine>criterion = nn.BCEWithLogitsLoss()<NewLine>criterion = cuda()<NewLine><NewLine>for s, t in loader:<NewLine>    logits = model(s, t)<NewLine>    loss = model.module.compute_loss(logits, tgt, criterion)<NewLine></code></pre><NewLine><p>when I compute_loss, raise ValueError, say logits.shape is (256, num_classes), but t.shape is (64, num_classes), I want to know why</p><NewLine></div>",https://discuss.pytorch.org/u/TripleTry,(Triple Try),TripleTry,"January 7, 2020,  8:02am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I think your code is missing some important bits.<br/><NewLine>Could you give a small code sample that we can run that shows the issue please?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much <img alt="":grinning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/grinning.png?v=9"" title="":grinning:""/>, I have solve this problem.<br/><NewLine>There is something wrong with my dataloader function, when I load data, I use padding to process my data, but I forgot to turn list into tensor, as a result nn.Dataparallel to split data wrong in batch dim. <img alt="":sweat_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/sweat_smile.png?v=9"" title="":sweat_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/TripleTry; <NewLine> ,"REPLY_DATE 1: January 9, 2020,  3:44am; <NewLine> REPLY_DATE 2: January 9, 2020,  3:44am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
63664,Distributed training gives nan loss but single GPU training is fine,2019-12-10T21:30:58.561Z,3,446,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I train my network with a single GPU, the training process terminates successfully after 120 epochs. However, if I use two GPUs, I get nan loss after a dozen epochs. The only thing I change is the batch size. For single GPU I use a batch size of 2 and for 2 GPUs I use a batch size of 1 for each GPU. The other parameters are exactly the same. I also replace every batchnorm2d layer with a syncbatchnorm layer. Strangely, syncbatchnorm gives higher loss. What could be the possible reasons?</p><NewLine></div>",https://discuss.pytorch.org/u/Beinan_Wang,(Beinan Wang),Beinan_Wang,"December 10, 2019,  9:34pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you please paste a code snippet to reproduce? Are you using DataParallel or DistributedDataParallel?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I use DDP. I enabled anomaly detection. Below is the message I get</p><NewLine><p>/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:57: UserWarning: Traceback of forward call that caused the error:<br/><NewLine>File “”, line 1, in <br/><NewLine>File “/usr/lib/python3.6/multiprocessing/spawn.py”, line 105, in spawn_main<br/><NewLine>exitcode = _main(fd)<br/><NewLine>File “/usr/lib/python3.6/multiprocessing/spawn.py”, line 118, in _main<br/><NewLine>return self._bootstrap()<br/><NewLine>File “/usr/lib/python3.6/multiprocessing/process.py”, line 258, in _bootstrap<br/><NewLine>self.run()<br/><NewLine>File “/usr/lib/python3.6/multiprocessing/process.py”, line 93, in run<br/><NewLine>self._target(*self._args, **self._kwargs)<br/><NewLine>File “/home/beinan/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 19, in _wrap<br/><NewLine>fn(i, *args)<br/><NewLine>File “/home/beinan/Desktop/pytorch-bcn/jupyter/train.py”, line 153, in main_worker<br/><NewLine>train(train_loader, model, criterion, optimizer, epoch, args)<br/><NewLine>File “/home/beinan/Desktop/pytorch-bcn/jupyter/train.py”, line 199, in train<br/><NewLine>output = model(images)<br/><NewLine>File “/home/beinan/.local/lib/python3.6/site-packages/torch/nn/modules/module.py”, line 541, in <strong>call</strong><br/><NewLine>result = self.forward(*input, **kwargs)<br/><NewLine>File “/home/beinan/.local/lib/python3.6/site-packages/apex/parallel/distributed.py”, line 560, in forward<br/><NewLine>result = self.module(*inputs, **kwargs)<br/><NewLine>File “/home/beinan/.local/lib/python3.6/site-packages/torch/nn/modules/module.py”, line 541, in <strong>call</strong><br/><NewLine>result = self.forward(*input, **kwargs)<br/><NewLine>File “…/bcn/models/semantic/resnet34.py”, line 295, in forward<br/><NewLine>x, encoder_features, encoder_feature = self.encoder(x)<br/><NewLine>File “/home/beinan/.local/lib/python3.6/site-packages/torch/nn/modules/module.py”, line 541, in <strong>call</strong><br/><NewLine>result = self.forward(*input, **kwargs)<br/><NewLine>File “…/bcn/models/semantic/resnet34.py”, line 224, in forward<br/><NewLine>x = self.bn(self.conv(x))<br/><NewLine>File “/home/beinan/.local/lib/python3.6/site-packages/torch/nn/modules/module.py”, line 541, in <strong>call</strong><br/><NewLine>result = self.forward(*input, **kwargs)<br/><NewLine>File “…/bcn/layers/conv.py”, line 38, in forward<br/><NewLine>groups=self.groups<br/><NewLine>File “/home/beinan/.local/lib/python3.6/site-packages/apex/amp/wrap.py”, line 28, in wrapper<br/><NewLine>return orig_fn(*new_args, **kwargs)</p><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “train.py”, line 314, in <br/><NewLine>main()<br/><NewLine>File “train.py”, line 66, in main<br/><NewLine>mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))<br/><NewLine>File “/home/beinan/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 171, in spawn<br/><NewLine>while not spawn_context.join():<br/><NewLine>File “/home/beinan/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 118, in join<br/><NewLine>raise Exception(msg)<br/><NewLine>Exception:</p><NewLine><p>– Process 1 terminated with the following error:<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “/home/beinan/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 19, in _wrap<br/><NewLine>fn(i, *args)<br/><NewLine>File “/home/beinan/Desktop/pytorch-bcn/jupyter/train.py”, line 153, in main_worker<br/><NewLine>train(train_loader, model, criterion, optimizer, epoch, args)<br/><NewLine>File “/home/beinan/Desktop/pytorch-bcn/jupyter/train.py”, line 208, in train<br/><NewLine>scaled_loss.backward()<br/><NewLine>File “/home/beinan/.local/lib/python3.6/site-packages/torch/tensor.py”, line 166, in backward<br/><NewLine>torch.autograd.backward(self, gradient, retain_graph, create_graph)<br/><NewLine>File “/home/beinan/.local/lib/python3.6/site-packages/torch/autograd/<strong>init</strong>.py”, line 99, in backward<br/><NewLine>allow_unreachable=True)  # allow_unreachable flag<br/><NewLine>RuntimeError: Function ‘CudnnConvolutionBackward’ returned nan values in its 1th output.</p><NewLine><p>This consistently happens after 90+ epochs but only if I use DDP. Single GPU single node does not have this problem. BTW, I train with fp16 precision. Is it possible that fp16 + DDP + SyncBatchNorm somehow leads to this?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible for the transformation below to cause any problem?</p><NewLine><pre><code class=""lang-auto"">class RandomResizeCrop(object):<NewLine><NewLine>    def __init__(self, min_scale, max_scale, scale_step, output_size):<NewLine><NewLine>        self.scales = np.arange(min_scale, max_scale, scale_step)<NewLine>        self.output_height, self.output_width = output_size<NewLine><NewLine>    def __call__(self, image, annotation):<NewLine><NewLine>        scale = np.random.choice(self.scales)<NewLine>        <NewLine>        image = cv2.resize(image, (0,0), fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)<NewLine>        annotation = cv2.resize(annotation, (0,0), fx=scale, fy=scale, interpolation=cv2.INTER_NEAREST)<NewLine><NewLine>        input_height, input_width = image.shape[:2]<NewLine><NewLine>        row_pads = max(self.output_height - input_height, 0)<NewLine>        col_pads = max(self.output_width - input_width, 0)<NewLine><NewLine>        top_pads = randint(0, row_pads)<NewLine>        bot_pads = row_pads - top_pads<NewLine><NewLine>        left_pads = randint(0, col_pads)<NewLine>        right_pads = col_pads - left_pads<NewLine><NewLine>        image = np.pad(image, ((top_pads,bot_pads),(left_pads,right_pads),(0,0)), mode='constant', constant_values=0)<NewLine>        annotation = np.pad(annotation, ((top_pads,bot_pads),(left_pads,right_pads)), mode='constant', constant_values=255)<NewLine><NewLine>        y1 = randint(0, max(input_height - self.output_height, 0))<NewLine>        y2 = y1 + self.output_height<NewLine><NewLine>        x1 = randint(0, max(input_width - self.output_width, 0))<NewLine>        x2 = x1 + self.output_width<NewLine><NewLine>        return image[y1:y2,x1:x2], annotation[y1:y2,x1:x2]</code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>It looks like the first convolution (operation) in resnet is causing nan. Is there any way some values of an image become nan after transformation?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Beinan_Wang; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Beinan_Wang; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Beinan_Wang; <NewLine> ,"REPLY_DATE 1: December 26, 2019,  4:16pm; <NewLine> REPLY_DATE 2: January 8, 2020,  3:30pm; <NewLine> REPLY_DATE 3: January 8, 2020,  3:33pm; <NewLine> REPLY_DATE 4: January 8, 2020,  5:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
58153,SyncBatchNorm.convert_sync_batchnorm() causes ValueError: expected at least 3D input (got 2D input),2019-10-14T08:00:24.268Z,7,1083,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Not sure if something is missing but isn’t <code>SyncBatchnorm.convert_sync_batchnorm()</code> supposed to convert the module transparently?<br/><NewLine>However, the following code segment produce <code>ValueError: expected at least 3D input (got 2D input) </code>.<br/><NewLine>Without the conversion, the forward goes as expected.<br/><NewLine>Any ideas?</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import torch<NewLine>from torch import nn<NewLine><NewLine>module = torch.nn.Sequential(<NewLine>           torch.nn.Linear(20, 100),<NewLine>           torch.nn.BatchNorm1d(100)<NewLine>         ).cuda()<NewLine><NewLine># creating process group (optional)<NewLine># process_ids is a list of int identifying rank ids.<NewLine>os.environ['RANK'] = '0' <NewLine>os.environ['WORLD_SIZE'] = '1' <NewLine>os.environ['MASTER_ADDR'] = 'localhost'<NewLine>os.environ['MASTER_PORT'] = '25791'<NewLine><NewLine>process_group = torch.distributed.init_process_group(backend='nccl')<NewLine>module = nn.SyncBatchNorm.convert_sync_batchnorm(module, process_group)<NewLine><NewLine>input = torch.randn(2, 20).cuda()<NewLine>output = module(input)<NewLine>print(output.shape)<NewLine></code></pre><NewLine><p>The output:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""syncBN.py"", line 21, in &lt;module&gt;<NewLine>    output = module(input)<NewLine>  File ""/home/ml/farleylai/miniconda3/envs/sinet37/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/ml/farleylai/miniconda3/envs/sinet37/lib/python3.7/site-packages/torch/nn/modules/container.py"", line 92, in forward<NewLine>    input = module(input)<NewLine>  File ""/home/ml/farleylai/miniconda3/envs/sinet37/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/ml/farleylai/miniconda3/envs/sinet37/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py"", line 429, in forward<NewLine>    self._check_input_dim(input)<NewLine>  File ""/home/ml/farleylai/miniconda3/envs/sinet37/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py"", line 417, in _check_input_dim<NewLine>    .format(input.dim()))<NewLine>ValueError: expected at least 3D input (got 2D input)<NewLine></code></pre><NewLine><p>Expected output as w/o conversion:</p><NewLine><pre><code class=""lang-auto"">torch.Size([2, 100])<NewLine></code></pre><NewLine><p>Ubuntu 16.04 with PyTorch 1.3 installed through conda.</p><NewLine></div>",https://discuss.pytorch.org/u/farleylai,(Farley Lai),farleylai,"October 14, 2019,  2:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>The original modules like <code>BatchNorm1d</code> or <code>BatchNorm2d</code> support not having a batch size, so they handle respectively 2d/3d inputs and 3d/4d inputs.<br/><NewLine>The sync batchnorm has no specialized functions and works for all. But to know which version to use, it must use the number of dimensions of the input (otherwise as you see above, 3d input could be either a batched 1d or an unbatched 2d). And so it only allows having a batch dimension.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/alband"">@albanD</a><br/><NewLine>I looked into the code and found this restriction is imposed by <code>SyncBatchNorm</code>:</p><NewLine><pre><code class=""lang-auto"">def _check_input_dim(self, input):<NewLine>        if input.dim() &lt;= 2:<NewLine>            raise ValueError('expected at least 3D input (got {}D input)'<NewLine>                             .format(input.dim()))<NewLine></code></pre><NewLine><p>This is completely different from the original <code>BatchNorm1d</code> to be wrapped:</p><NewLine><pre><code class=""lang-auto"">    def _check_input_dim(self, input):<NewLine>        if input.dim() != 2 and input.dim() != 3:<NewLine>            raise ValueError('expected 2D or 3D input (got {}D input)'<NewLine>                             .format(input.dim()))<NewLine></code></pre><NewLine><p>I got confused with the code segment that is actually from your API document of <code>SyncBatchNorm</code> using <code>BatchNorm1d</code> for <code>convert_sync_batchnorm</code>.<br/><NewLine>Why doesn’t <code>SyncBatchNorm</code> explicitly check whether the module to wrap is <code>BatchNorm1d</code> or <code>BatchNorm2d</code> instead of the general <code>_BatchNorm</code> in <code>convert_sync_batchnorm</code>?<br/><NewLine>If this is not going to work, what is the right way to use <code>convert_sync_batchnorm</code> for those models with <code>BatchNorm1d</code>?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>If this is not going to work, what is the right way to use  <code>convert_sync_batchnorm</code>  for those models with  <code>BatchNorm1d</code> ?</p><NewLine></blockquote><NewLine><p>I think the fix here is to ensure you always have a batch dimension. Potentially adding an <code>.unsqueeze(0)</code> to your input.</p><NewLine><blockquote><NewLine><p>Then why doesn’t  <code>SyncBatchNorm</code>  explicitly check whether the module to wrap is  <code>BatchNorm1d</code>  or  <code>BatchNorm2d</code>  instead of the general  <code>_BatchNorm</code>  in  <code>convert_sync_batchnorm</code> ?</p><NewLine></blockquote><NewLine><p>This would be a nice addition, we would be happy to merge a PR that adds this feature!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I think the fix here is to ensure you always have a batch dimension. Potentially adding an  <code>.unsqueeze(0)</code>  to your input.</p><NewLine></blockquote><NewLine><p>The example input (2, 20) already contains a batch dim, indicating a batch of two 1D examples.<br/><NewLine>If we fake the input with <code>unsqueeze(0)</code>, how could it work when there are other modules before <code>BatchNorm1d</code> in the model that may assume the 0th dim must be the batch dim?<br/><NewLine>After all, the layer(s) before <code>BatchNorm1d</code> can be anything else in general, right?</p><NewLine><p>BTW, I tried to make it of size (1, 2, 20) but it still complains something wrong with the running_mean size:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""syncBN.py"", line 29, in &lt;module&gt;<NewLine>    output = module(input)<NewLine>  File ""/home/ml/farleylai/miniconda3/envs/sinet37/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/ml/farleylai/miniconda3/envs/sinet37/lib/python3.7/site-packages/torch/nn/modules/container.py"", line 92, in forward<NewLine>    input = module(input)<NewLine>  File ""/home/ml/farleylai/miniconda3/envs/sinet37/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/ml/farleylai/miniconda3/envs/sinet37/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py"", line 459, in forward<NewLine>    exponential_average_factor, self.eps)<NewLine>  File ""/home/ml/farleylai/miniconda3/envs/sinet37/lib/python3.7/site-packages/torch/nn/functional.py"", line 1670, in batch_norm<NewLine>    training, momentum, eps, torch.backends.cudnn.enabled<NewLine>RuntimeError: running_mean should contain 2 elements not 100<NewLine></code></pre><NewLine><blockquote><NewLine><p>This would be a nice addition, we would be happy to merge a PR that adds this feature!</p><NewLine></blockquote><NewLine><p>So you are suggesting it is not intentional but something that could be completed?<br/><NewLine>If that is the case, it seems like creating an issue on the GitHub repo makes more sense and I will look into the details under the hood.</p><NewLine><p>Therefore to sum up, there is likely no luck for those with <code>BatchNorm1d</code> already to be converted to SyncBN transparently by default.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>The example input (2, 20) already contains a batch dim, indicating a batch of two 1D examples.</p><NewLine></blockquote><NewLine><p>That is not how batchnorm 1d works. Batchnorm 1d assumes an optional first batch dimension, then a channel dimension then an actual dimension. So the input is 2d without batch and 3d with batch.</p><NewLine><blockquote><NewLine><p>BTW, I tried to make it of size (1, 2, 20) but it still complains something wrong with the running_mean size:</p><NewLine></blockquote><NewLine><p>This is because you define your batchnorm as having <code>100</code> channels, but what you give as input has <code>2</code>.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am expiriencing the same problem as you <a class=""mention"" href=""/u/farleylai"">@farleylai</a> exactly now.</p><NewLine><p>I am trying to run a model with ResNet backbone, which has only BatchNorm2d and a head network that have exactly ONE BatchNorm1d and that is exactly what causes problem.</p><NewLine><p>The input to the BatchNorm1d in the forward function of the model is [64,2048].<br/><NewLine>As suggested by <a class=""mention"" href=""/u/alband"">@albanD</a> I unsqueezed it in the forward function) so that the input shape is now [64, 1, 2048]. Next module is a Linear classifier, so I squeezed the output of the BatchNorm1d to again have [64, 2048] input to Linear layer. This helped in the sense that the forward pass is working, but in the backward pass I am getting now an error:</p><NewLine><blockquote><NewLine><p>RuntimeError: Function SyncBatchNormBackward returned an invalid gradient at index 1 - got [1] but expected shape compatible with [2048]</p><NewLine></blockquote><NewLine><p>Any suggestions <a class=""mention"" href=""/u/alband"">@albanD</a> ?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you have a small code sample so that I can reproduce that locally?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not really sure what you mean by ‘small code sample’. So I will try:</p><NewLine><pre><code class=""lang-auto"">class Net(nn.Module):<NewLine>    in_planes = 2048<NewLine><NewLine>    def __init__(self, num_classes, model_path,  model_name):<NewLine>        super(Net, self).__init__()<NewLine>        self.base = ResNet(block=Bottleneck,<NewLine>                           layers=[3, 4, 6, 3])<NewLine><NewLine>        self.gap = nn.AdaptiveAvgPool2d(1)<NewLine>        self.num_classes = num_classes<NewLine>        self.bottleneck = nn.BatchNorm1d(self.in_planes)<NewLine>        self.bottleneck.bias.requires_grad_(False)  # no shift<NewLine>        self.classifier = nn.Linear(self.in_planes, self.num_classes, bias=False)<NewLine><NewLine>    def forward(self, x):<NewLine><NewLine>        global_feat = self.gap(self.base(x))  # (b, 2048, 1, 1)<NewLine>        global_feat = global_feat.view(global_feat.shape[0], -1)  # flatten to (bs, 2048)<NewLine><NewLine>        feat = self.bottleneck(global_feat.unsqueeze(1))  ### To allow SyncBatchnorm<NewLine><NewLine>        cls_score = self.classifier(feat.squeeze())   ### To adjust for Linear layer input<NewLine><NewLine>        return cls_score, global_feat <NewLine><NewLine><NewLine>def train():<NewLine>        model.train()<NewLine>        optimizer.zero_grad()<NewLine>        img, target = batch<NewLine>        img = img.to(device)<NewLine>        target = target.to(device)t<NewLine>        score, feat = model(img)<NewLine>        LOSS = loss_fn(score, feat, target)<NewLine>        LOSS.backward()<NewLine>        optimizer.step()<NewLine></code></pre><NewLine><p>I shortened the code as much as possible to get the most important parts I think. The img size (input in train function) is [batch_size, 3, W, H] so standard for images.</p><NewLine><p>EDIT: format and making a little clearer code</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think I got it right now.</p><NewLine><p>According to docs <a href=""https://pytorch.org/docs/stable/nn.html#batchnorm1d"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/nn.html#batchnorm1d</a> and quoting:</p><NewLine><blockquote><NewLine><p>Parameters</p><NewLine><ul><NewLine><li><NewLine><strong>num_features</strong> – C from an expected input of size (N,C,L) or L from input of size (N,L)</li><NewLine></ul><NewLine></blockquote><NewLine><p>So I figured out that as we need 3d input and BatchNorm1d uses C as the num_features in three dimensional input, the singular dimension should be the last one.<br/><NewLine>So instead of</p><NewLine><p><code>feat = self.bottleneck(global_feat.unsqueeze(1))   # Which gives [bs, 1, 2048] </code><br/><NewLine>I just did:<br/><NewLine><code>feat = self.bottleneck(global_feat.unsqueeze(-1))   # Which gives [bs, 2048, 1]</code></p><NewLine><p>No more errors and training seems to run smoothly with SyncBatchNorm as well. Hope, this helps someone.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>That is not how batchnorm 1d works. Batchnorm 1d assumes an optional first batch dimension, then a channel dimension then an actual dimension. So the input is 2d without batch and 3d with batch.</p><NewLine></blockquote><NewLine><p>As defined by the <code>BatchNorm1d</code>, the Input is expected to be of size <code>(N, L)</code> or <code>(N, C, L)</code> with batch dim first. What is optional is the additional channel dimension for <code>BatchNorm1d</code> from the documentation.</p><NewLine><blockquote><NewLine><p>This is because you define your batchnorm as having  <code>100</code>  channels, but what you give as input has  <code>2</code> .</p><NewLine></blockquote><NewLine><p><code>(1, 2, 20)</code> is due to the suggestion <code>adding .unsqueeze(0) to your input</code> but the resulting shape is not originally intended. By definition, whether the <code>100</code> is C or L in the previous example, <code>BatchNorm1d</code> produces the same results given <code>(N, 100)</code> or <code>(N, 100, 1)</code>. <code>(2, 100)</code> is already a batch input with <code>2</code> 1D features and matches the input accepted by <code>BatchNorm1d</code>. This has to be on the same page.</p><NewLine><p>Now, get back to the issue with <code>SyncBatchNorm</code> conversion. Two questions:</p><NewLine><ol><NewLine><li>Does <code>SyncBatchNorm</code> wrapped <code>BatchNorm1d</code> behave as expected as before the conversion?</li><NewLine></ol><NewLine><p>The original <code>BatchNorm1d</code> takes both (N, L) or (N, C, L) and produces the same results as the following revised code segment shows. However, after converted to <code>SyncBatchNorm</code> which CHANGES the interface to ONLY accepts input of size (N, C, L). This conversion unlikely works transparently with existing models using <code>BatchNorm1d</code> to accept input of size <code>(N, L)</code>.</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import copy<NewLine>import torch<NewLine>from torch import nn<NewLine><NewLine>with torch.no_grad():<NewLine>    inputNL = torch.randn(2, 20).cuda()<NewLine>    module = torch.nn.Sequential(<NewLine>               torch.nn.Linear(20, 100),<NewLine>               torch.nn.BatchNorm1d(100)<NewLine>             ).cuda()<NewLine>    moduleC = copy.deepcopy(module).cuda()<NewLine>    moduleL = copy.deepcopy(module).cuda()<NewLine>    moduleC.eval()<NewLine>    moduleL.eval()<NewLine><NewLine>    # XXX: BatchNorm1d accepts (N, C, L) <NewLine>    outputNL = moduleC[0](inputNL)<NewLine>    outputNCL = moduleC[1](outputNL.unsqueeze(-1))<NewLine>    print('BatchNorm1d NCL:', outputNCL.shape, round(outputNCL.mean().item(), 7))<NewLine><NewLine>    # XXX: BatchNorm1d accepts (N, L) too<NewLine>    outputNL = moduleL[0](inputNL)<NewLine>    outputNL = moduleL[1](outputNL)<NewLine>    print('BatchNorm1d NL:', outputNL.shape, round(outputNL.mean().item(), 7))<NewLine><NewLine>    os.environ['RANK'] = '0'<NewLine>    os.environ['WORLD_SIZE'] = '1'<NewLine>    os.environ['MASTER_ADDR'] = 'localhost'<NewLine>    os.environ['MASTER_PORT'] = '25791'<NewLine>    torch.distributed.init_process_group(backend='nccl')<NewLine><NewLine>    moduleC = copy.deepcopy(module)<NewLine>    moduleL = copy.deepcopy(module)<NewLine>    moduleC = nn.SyncBatchNorm.convert_sync_batchnorm(moduleC)<NewLine>    moduleL = nn.SyncBatchNorm.convert_sync_batchnorm(moduleL)<NewLine>    moduleC.eval()<NewLine>    moduleL.eval()<NewLine><NewLine>    # XXX: converted BatchNorm1d ONLY accepts (N, C, L) <NewLine>    outputNL = moduleC[0](inputNL)<NewLine>    outputNCL = moduleC[1](outputNL.unsqueeze(-1))<NewLine>    print('SyncBatchNorm NCL:', outputNCL.shape, round(outputNCL.mean().item(), 7))<NewLine><NewLine>    # FIXME: Converted BatchNorm1d never accepts (N, L)<NewLine>    outputNL = moduleL[0](inputNL)<NewLine>    outputNL = moduleL[1](outputNL)<NewLine>    print('SyncBatchNorm NL:', outputNL.shape, round(outputNL.mean().item(), 7))<NewLine></code></pre><NewLine><p>Sample output:</p><NewLine><pre><code class=""lang-auto"">BatchNorm1d NCL: torch.Size([2, 100, 1]) 0.0683341<NewLine>BatchNorm1d NL: torch.Size([2, 100]) 0.0683341<NewLine>SyncBatchNorm NCL: torch.Size([2, 100, 1]) 0.0683341<NewLine>Traceback (most recent call last):<NewLine>  File ""syncBN.py"", line 45, in &lt;module&gt;<NewLine>    outputNL = moduleL[1](outputNL)<NewLine>  File ""/home/ml/farleylai/miniconda3/envs/sinet37/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/ml/farleylai/miniconda3/envs/sinet37/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py"", line 429, in forward<NewLine>    self._check_input_dim(input)<NewLine>  File ""/home/ml/farleylai/miniconda3/envs/sinet37/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py"", line 417, in _check_input_dim<NewLine>    .format(input.dim()))<NewLine>ValueError: expected at least 3D input (got 2D input)<NewLine></code></pre><NewLine><ol start=""2""><NewLine><li>If not, what is the justification or workaround that does not require changing the existing model to wrap?</li><NewLine></ol><NewLine><p>One workaround is to reshape/unsqueeze(-1) the immediate input of size (N, L) to (N, C=L, L=1) before the converted <code>BatchNorm1d</code> as demonstrated by <a class=""mention"" href=""/u/bonzogondo"">@bonzogondo</a>. Unfortunately, this may not be scalable if the uses of <code>BatchNorm1d</code> are all over the place in existing models. There is no reshape layers in PyTorch to automate the <code>unsqeeze</code>. An alternative could be to identify whether the <code>BatchNorm</code> to wrap is 1D or not so that the SyncBatchNorm._check_input_dim(…) checks the same criteria as <code>BatchNorm1d</code> as sketched in the following. There may be some other exceptions but the goal should be to wrap existing models transparently.</p><NewLine><pre><code class=""lang-auto"">class SyncBatchNorm(nn.SyncBatchNorm):<NewLine>    def _check_input_dim(self, input):<NewLine>        if self._1d:<NewLine>            if input.dim() != 2 and input.dim() != 3:<NewLine>                raise ValueError('expected 2D or 3D input (got {}D input)'<NewLine>                                    .format(input.dim()))<NewLine>        elif input.dim() &lt;= 2:<NewLine>            raise ValueError('expected at least 3D input (got {}D input)'<NewLine>                             .format(input.dim()))<NewLine><NewLine>    @classmethod<NewLine>    def convert_sync_batchnorm(cls, module, process_group=None):<NewLine>        ...<NewLine>        if isinstance(module, nn.modules.batchnorm._BatchNorm):<NewLine>            module_output = SyncBatchNorm(module.num_features,<NewLine>                                              module.eps, module.momentum,<NewLine>                                              module.affine,<NewLine>                                              module.track_running_stats,<NewLine>                                              process_group)<NewLine>            module_output._1d = isinstance(module, nn.modules.batchnorm.BatchNorm1d)<NewLine>            ...<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>I faced the same problem. Any update?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/farleylai; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/farleylai; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/bonzogondo; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/bonzogondo; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/bonzogondo; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/farleylai; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/Kirayue; <NewLine> ,"REPLY_DATE 1: October 14, 2019,  2:50pm; <NewLine> REPLY_DATE 2: October 14, 2019,  4:28pm; <NewLine> REPLY_DATE 3: October 14, 2019,  4:28pm; <NewLine> REPLY_DATE 4: October 14, 2019,  6:28pm; <NewLine> REPLY_DATE 5: October 14, 2019,  6:31pm; <NewLine> REPLY_DATE 6: October 14, 2019,  7:08pm; <NewLine> REPLY_DATE 7: October 14, 2019,  7:11pm; <NewLine> REPLY_DATE 8: October 14, 2019,  7:34pm; <NewLine> REPLY_DATE 9: October 14, 2019,  8:17pm; <NewLine> REPLY_DATE 10: October 15, 2019,  3:08am; <NewLine> REPLY_DATE 11: January 8, 2020,  6:50am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: 2 Likes; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> REPLY 11 LIKES: ; <NewLine> 
65724,Matplotlib doesn&rsquo;t work in distributed training,2020-01-04T04:54:19.025Z,0,90,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am training a model in a 4-GPUs machine with torch.distributed,and I want the ONLY rank_0 process be responsible for plotting,so I wrote code like this:</p><NewLine><pre><code class=""lang-auto""> if is_distributed() and distributed.get_rank()!=0:<NewLine>             print('Only rank_0 will do plotting,this is rank_{}'.format(distributed.get_rank()))<NewLine>             return# in parallel context,single plot is enough<NewLine>print('this is rank_0 and  will do plotting')<NewLine>plotAccuracyAndLoss()<NewLine> .....<NewLine></code></pre><NewLine><p>So,if the process rank is not 0,it should print out:</p><NewLine><pre><code class=""lang-auto"">    Only rank_0 will do plotting,this is rank_x<NewLine></code></pre><NewLine><p><strong>and I do get 3 printings of this type</strong><br/><NewLine>if the process rank is 0,it should print out:</p><NewLine><pre><code class=""lang-auto"">      this is rank_0 and  will do plotting<NewLine></code></pre><NewLine><p><strong>and I never got this type of printing,and meanwhile all processes hanging and no exception got thrown out</strong></p><NewLine><p><code>watch -n0.1 nvidia-smi</code> tell that before these code all, all GPU will have memory usage &gt; 10341MB,when hitting these lines,the first GPU’s memory usage drops to 2387MB,others remain,More strangely,if changing code to<br/><NewLine><code> if is_distributed() and distributed.get_rank()!=1:</code><br/><NewLine>which let the second GPU to be responsible for plotting,when comes to these lines,1st,3rd,4th GPU’s memory usage still &gt; 10341MB,but the 2nd GPU’s memory usage drop to 1073MB,training hangs,no exception got thrown out.<br/><NewLine>With same code in non-distributed training,the plotting works fine,would you please tell me how to make plotting work?</p><NewLine></div>",https://discuss.pytorch.org/u/Alex_Luya,(Alex Luya),Alex_Luya,"January 4, 2020,  4:54am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>After adding:<br/><NewLine><code>distributed.barriere()</code><br/><NewLine>before any rank_x specific operation,everything goes fine,silly me</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Alex_Luya; <NewLine> ,"REPLY_DATE 1: January 7, 2020,  1:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
65891,Training hangs if any specific rank process start an other process to do anything,2020-01-06T09:48:39.922Z,0,80,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,a similar question has been asked here:<a href=""https://discuss.pytorch.org/t/matplotlib-doesnt-work-in-distributed-training/65724,but"">https://discuss.pytorch.org/t/matplotlib-doesnt-work-in-distributed-training/65724,but</a> no answer,Question can be briefed blow:</p><NewLine><p>In a 4-GPU machine,all gpus are used for training,there is some some code like this:</p><NewLine><pre><code class=""lang-auto""> if is_distributed() and distributed.get_rank()!=0:<NewLine>             print('Only rank_0 will do plotting,this is rank_{}'.format(distributed.get_rank()))<NewLine>             return# in parallel context,single plot is enough<NewLine>print('this is rank_0 and  it will do plotting')<NewLine>plotAccuracyAndLoss()<NewLine></code></pre><NewLine><p>When comes to these code,three:<br/><NewLine><code> Only rank_0 will do plotting,this is rank_x</code></p><NewLine><p>got printed out,but</p><NewLine><p><code>print('this is rank_0 and  will do plotting')</code></p><NewLine><p>never got printed out,and all 4 processes hanged and NO exception got thrown out</p><NewLine><p><code>watch -n0.1 nvidia-smi</code>  tell that</p><NewLine><pre><code class=""lang-auto"">before these code all, all GPU will have memory usage &gt; 10341MB,<NewLine>when hitting these lines,the first GPU’s memory usage drops to 2387MB,others remain<NewLine></code></pre><NewLine><p>previously,I thought that it is the matplotlib which caused this hanging,but right now I found any rank_0-only operation(ploting/checkpointing…) will cause hanging,further more,any rank_x-only operation will cause hanging,so,How to solve this problem?</p><NewLine></div>",https://discuss.pytorch.org/u/Alex_Luya,(Alex Luya),Alex_Luya,"January 6, 2020,  9:48am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>After adding:<br/><NewLine><code>distributed.barriere()</code><br/><NewLine>before any rank_x specific operation,everything goes fine.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Alex_Luya; <NewLine> ,"REPLY_DATE 1: January 7, 2020,  1:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
65564,Pytorch multiprocessing CUDA Initialization error,2020-01-02T06:10:49.854Z,0,164,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to run multiple threads in pytorch with GPU enabled. In each thread, I am trying to create a CUDA tensor from numpy array using the following code:</p><NewLine><p>tensor = torch.from_numpy(array).cuda().float()</p><NewLine><p>this triggers the following error report:</p><NewLine><p>RuntimeError: CUDA error: initialization error</p><NewLine><p>Any help would be greatly appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/Peter_Ham,(Peter Ham),Peter_Ham,"January 2, 2020,  6:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>sorry I just solved this problem by using</p><NewLine><pre><code class=""lang-auto"">    mp.set_start_method('spawn')<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Peter_Ham; <NewLine> ,"REPLY_DATE 1: January 2, 2020,  6:15am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
65511,How to use spawn function in torch.multiprocessing module,2020-01-01T12:28:29.011Z,0,240,"<div class=""post"" itemprop=""articleBody""><NewLine><p>RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the ‘spawn’ start method</p><NewLine><p>This is the error that I get when I want to use mutiple gpus to evaluate different models using multiprocessing.</p><NewLine></div>",https://discuss.pytorch.org/u/Neel_S,(Neel S),Neel_S,"January 1, 2020, 12:28pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Try <code>mp.set_start_method('spawn', force=True)</code> at your main; like the following:</p><NewLine><pre><code class=""lang-auto"">if __name__ == '__main__':<NewLine><NewLine>    mp.set_start_method('spawn', force=True)<NewLine><NewLine>    main()<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/bapi; <NewLine> ,"REPLY_DATE 1: January 1, 2020,  1:54pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
63358,DDP on 8 gpu work much worse then on single,2019-12-08T00:25:39.001Z,6,552,"<div class=""post"" itemprop=""articleBody""><NewLine><p>So i read this thread and some other, googled and etc<br/><NewLine><aside class=""quote quote-modified"" data-post=""1"" data-topic=""47152""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/jimfan/40/1296_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/training-performance-degrades-with-distributeddataparallel/47152"">Training performance degrades with DistributedDataParallel</a> <a class=""badge-wrapper bullet"" href=""/c/distributed""><span class=""badge-category-bg"" style=""background-color: #0088CC;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""">distributed</span></a><NewLine></div><NewLine><blockquote><NewLine>    I’m training a conv model using DataParallel (DP) and DistributedDataParallel (DDP) modes. For DDP, I only use it on a single node and each process is one GPU. <NewLine>My model has many BatchNorm2d layers. Given all other things the same, I observe that DP trains better than DDP (in classification accuracy). Even if I add SyncBN from pytorch 1.1, I still observe that DP &gt; DDP+SyncBN &gt; DDP without SyncBN in test accuracy. <NewLine>I’m aware of the difference between DP and DDP’s handling of averaging/sum: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/is-average-the-correct-way-for-the-gradient-in-distributeddataparallel-with-multi-nodes/34260/16"">Is av…</a><NewLine></blockquote><NewLine></aside><NewLine></p><NewLine><p>My model is seq to seq variable len input (tacotron 2).</p><NewLine><p>This is how looks like loss<br/><NewLine><a class=""onebox"" href=""https://i.imgur.com/NAm5Woe.png"" rel=""nofollow noopener"" target=""_blank""><NewLine><img height=""243"" src=""https://i.imgur.com/NAm5Woe.png"" width=""690""/><NewLine></a><NewLine></p><NewLine><p>Any suggestions? Any way how to debug it?</p><NewLine><p>Also if i remove all batchnorm from model, it raise unused variable error, how it work?</p><NewLine></div>",https://discuss.pytorch.org/u/hadaev8,(Had),hadaev8,"December 8, 2019, 12:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you using any reference implementation as your code base or did you write the complete code yourself?<br/><NewLine>Could you compare your implementation to <a href=""https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/Tacotron2"">this one</a> written by NVIDIA?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Im using fork of this repo<br/><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicon.ico"" width=""16""/><NewLine><a href=""https://github.com/NVIDIA/tacotron2"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars0.githubusercontent.com/u/1728152?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/NVIDIA/tacotron2"" rel=""nofollow noopener"" target=""_blank"">NVIDIA/tacotron2</a></h3><NewLine><p>Tacotron 2 - PyTorch implementation with faster-than-realtime inference - NVIDIA/tacotron2</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>(looks like same with repo you linked)<br/><NewLine>But i have a lot of changes</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>In that case I would focus on the error (about the unused variable) by reverting the changes and make sure your model still trains.</p><NewLine><p>I assume the error you mentioned claims that no parameters were found which require gradients during the backward pass.<br/><NewLine>If so, it’s usually a sign, that all parameters were frozen or that the computation graph was detached at some point.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>So, this problem happen then i comment out batchnorm layers.<br/><NewLine>Then i return it back, it runs (still worse then single gpu).</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Another observation: upgraded torch from 1.2 to 1.3.1 and now i have OOM error on same setup.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>If the original code base works and you see this unwanted behavior, I would still recommend to triage the bug by removing your changed parts.<br/><NewLine>The error points to a (partly) frozen model. I’m not familiar with the use case, so it might be a red herring. Anyway, it might be a good starter for debugging.</p><NewLine><p>We are aware of the higher memory usage due to some added functionalities in a tensor method and are thinking about different approaches to fix this.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Most major difference between my model and nvidia’s is this modification.<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/bfs18/tacotron2/blob/master/train.py#L251"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/bfs18/tacotron2/blob/master/train.py#L251"" rel=""nofollow noopener"" target=""_blank"">bfs18/tacotron2/blob/master/train.py#L251</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""241"" style=""counter-reset: li-counter 240 ;""><NewLine><li>x, y = model.parse_batch(batch)</li><NewLine><li>y_pred = model(x)</li><NewLine><li><NewLine></li><NewLine><li>loss = criterion(y_pred, y)</li><NewLine><li>if model.mi is not None:</li><NewLine><li>    # transpose to [b, T, dim]</li><NewLine><li>    decoder_outputs = y_pred[0].transpose(2, 1)</li><NewLine><li>    ctc_text, ctc_text_lengths, aco_lengths = x[-2], x[-1], x[4]</li><NewLine><li>    taco_loss = loss</li><NewLine><li>    mi_loss = model.mi(decoder_outputs, ctc_text, aco_lengths, ctc_text_lengths)</li><NewLine><li class=""selected"">    if hparams.use_gaf:</li><NewLine><li>        if i % gradient_adaptive_factor.UPDATE_GAF_EVERY_N_STEP == 0:</li><NewLine><li>            safe_loss = 0. * sum([x.sum() for x in model.parameters()])</li><NewLine><li>            gaf = gradient_adaptive_factor.calc_grad_adapt_factor(</li><NewLine><li>                taco_loss + safe_loss, mi_loss + safe_loss, model.parameters(), optimizer)</li><NewLine><li>            gaf = min(gaf, hparams.max_gaf)</li><NewLine><li>    else:</li><NewLine><li>        gaf = 1.0</li><NewLine><li>    loss = loss + gaf * mi_loss</li><NewLine><li>else:</li><NewLine><li>    taco_loss = loss</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>Without gradient balancing lost curve much better (red)<br/><NewLine><a class=""onebox"" href=""https://i.imgur.com/uONtHJj.png"" rel=""nofollow noopener"" target=""_blank""><NewLine><img height=""271"" src=""https://i.imgur.com/uONtHJj.png"" width=""690""/><NewLine></a><NewLine></p><NewLine><p>Does it matter if loss calculated in forward or outside?<br/><NewLine>Also is it good idea in general to balance loss value like this?<br/><NewLine>Tacotron by its own have 2 losses<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/bfs18/tacotron2/blob/master/loss_function.py#L19"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/bfs18/tacotron2/blob/master/loss_function.py#L19"" rel=""nofollow noopener"" target=""_blank"">bfs18/tacotron2/blob/master/loss_function.py#L19</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""9"" style=""counter-reset: li-counter 8 ;""><NewLine><li>mel_target, gate_target = targets[0], targets[1]</li><NewLine><li>mel_target.requires_grad = False</li><NewLine><li>gate_target.requires_grad = False</li><NewLine><li>gate_target = gate_target.view(-1, 1)</li><NewLine><li><NewLine></li><NewLine><li>_, mel_out, mel_out_postnet, gate_out, _ = model_output</li><NewLine><li>gate_out = gate_out.view(-1, 1)</li><NewLine><li>mel_loss = nn.MSELoss()(mel_out, mel_target) + \</li><NewLine><li>    nn.MSELoss()(mel_out_postnet, mel_target)</li><NewLine><li>gate_loss = nn.BCEWithLogitsLoss()(gate_out, gate_target)</li><NewLine><li class=""selected"">return mel_loss + gate_loss</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>Do you think it should be balanced too?</p><NewLine><p>About batchnorm thing, i don’t understand how batchnorm affect params.<br/><NewLine>I mean it change only existed variables, do not freeze layers or something.<br/><NewLine>It work with batchnorm layers and raise error without.<br/><NewLine><a class=""onebox"" href=""https://i.imgur.com/Tc3yACY.png"" rel=""nofollow noopener"" target=""_blank""><NewLine><img height=""345"" src=""https://i.imgur.com/Tc3yACY.png"" width=""525""/><NewLine></a><NewLine><br/><NewLine>May be because without batchnorm i have only one layer in module?<br/><NewLine>Or it doesnt matter?</p><NewLine><p>I made another thread about memory usage (it is not only distributed thing) with example, hope it helps.<br/><NewLine><aside class=""quote"" data-post=""1"" data-topic=""63398""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/hadaev8/40/16280_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/50-vram-used-on-torch-1-3-compared-to-1-2/63398"">+ 50% VRAM used on torch 1.3 compared to 1.2</a><NewLine></div><NewLine><blockquote><NewLine>    1.3 <NewLine><a class=""onebox"" href=""https://colab.research.google.com/drive/104LtQ1zIioIOMQEPgVve77m5Rd4Gm0wU"" rel=""nofollow noopener"" target=""_blank"">https://colab.research.google.com/drive/104LtQ1zIioIOMQEPgVve77m5Rd4Gm0wU</a> <NewLine>1.2 <NewLine><a class=""onebox"" href=""https://colab.research.google.com/drive/1y4LF1a90PYfKvFgQw6fCMYKtIFK07mVr"" rel=""nofollow noopener"" target=""_blank"">https://colab.research.google.com/drive/1y4LF1a90PYfKvFgQw6fCMYKtIFK07mVr</a> <NewLine>Everything same except torch version. <NewLine>At the end nvidia-smi show 11gb and 7 gb memory used.<NewLine>  </blockquote><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is here any pytorch options to debug this situation?<br/><NewLine>I have made another test, gap seems to be too bad.<br/><NewLine><a class=""onebox"" href=""https://i.imgur.com/ldW0uQw.png"" rel=""nofollow noopener"" target=""_blank""><NewLine><img height=""313"" src=""https://i.imgur.com/ldW0uQw.png"" width=""690""/><NewLine></a><NewLine></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hadaev8; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/hadaev8; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/hadaev8; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/hadaev8; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/hadaev8; <NewLine> ,"REPLY_DATE 1: December 8, 2019,  1:05am; <NewLine> REPLY_DATE 2: December 8, 2019,  1:08am; <NewLine> REPLY_DATE 3: December 8, 2019,  1:10am; <NewLine> REPLY_DATE 4: December 8, 2019,  1:12am; <NewLine> REPLY_DATE 5: December 8, 2019, 10:54am; <NewLine> REPLY_DATE 6: December 8, 2019,  4:45pm; <NewLine> REPLY_DATE 7: December 8, 2019,  5:39pm; <NewLine> REPLY_DATE 8: December 28, 2019,  9:31pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
65054,Race condition in Isend,2019-12-26T12:08:37.486Z,0,116,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I get some bug during torch.distributed.isend and irecv: I think that there is a race condition there, but not sure how to debug this.<br/><NewLine>I’m using MPI, so the buffers have to stay untouched until the send/recv is over.<br/><NewLine>I see crazy “spikes” in error, which I get only with a certain degree of parallelism.</p><NewLine><p>I wonder if pytorch/python garbage collector touches my buffers.<br/><NewLine>I saved them in a list, just in case.<br/><NewLine>Where can I check this in code? can I “guard” the buffers somehow?</p><NewLine><p>I see pytorch tests barely check the Isend/Irecv,  and want to verify that the bug is not internal…</p><NewLine></div>",https://discuss.pytorch.org/u/seliad,(Saar Eliad),seliad,"December 26, 2019, 12:08pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/seliad"">@seliad</a></p><NewLine><p>The code for MPI-based torch.distributed.isend is here: <a href=""https://github.com/pytorch/pytorch/blob/cc16819028c325e2543d45752a875bd3c5e09b32/torch/lib/c10d/ProcessGroupMPI.cpp#L591"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/cc16819028c325e2543d45752a875bd3c5e09b32/torch/lib/c10d/ProcessGroupMPI.cpp#L591</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I looked at the code and did not see something suspicious.<br/><NewLine>However, the data corruption is still there.</p><NewLine><p>I found that if I do <code>torch.distributed.synchronize(device)</code> explicitly before Isends the problem is mitigated and can be mistaken to “solved”, but I don’t like this solution at all.<br/><NewLine>I don’t see any rational reason to do so, there is probably some bug there.</p><NewLine><p>Reading the warnings <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html"" rel=""nofollow noopener"">here</a> and <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html#cuda-in-multiprocessing"" rel=""nofollow noopener"">here</a> makes me believe that the MPI/distributed API probably does not do many stuff necessary for sharing tensors<br/><NewLine>like handling references counts, using mutex to guard stuff and etc.<br/><NewLine>I use CUDA-aware with openMPI, I thought it is supported.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/seliad; <NewLine> ,"REPLY_DATE 1: December 26, 2019,  4:06pm; <NewLine> REPLY_DATE 2: December 28, 2019,  6:13pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
64660,Using 2 GPUs for Different Parts of the Model,2019-12-20T18:38:48.773Z,2,322,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!</p><NewLine><p>I have 2 of the same GPU and I want to achieve faster processing by utilizing both of them. However, I am doing this in a different way, imitating the idea of <a href=""https://arxiv.org/pdf/1806.03863.pdf"" rel=""nofollow noopener"">Massively Parallel Video Networks</a>:</p><NewLine><p>I have divided my model into two sub-models. I want to run them concurrently, one part processing the input video frame by frame, and the other processing the output of the first one. However, there is a catch. When the first sub-model returns an output, it passes it to the second sub-model and starts processing the next frame of the input. By utilizing both the GPUs the authors of the paper achieve faster processing. Any idea on how to do this? The figure shows the idea: (the network is unrolled over time)</p><NewLine><p><img alt=""Screenshot%20from%202019-12-20%2021-37-25"" data-base62-sha1=""6kJWKc98HpySaVORzLwtuVgszPt"" height=""489"" src=""https://discuss.pytorch.org/uploads/default/original/3X/2/c/2c65416531119c70ae45c2590b22acd76711ca03.png"" width=""217""/></p><NewLine><p>The idea is not the same as <code>nn.DataParallel()</code>. I have tried <code>torch.multiprocessing</code>, <code>DistributedDataParallel()</code> but I am having trouble understanding how to do this.</p><NewLine><p>If anyone have some answer, I would be glad.</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/uertenli,(U Ertenli),uertenli,"December 20, 2019,  6:38pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>One approach…</p><NewLine><p>Start 2 python programs, in separate interpreters to avoid the dreaded GIL lock.</p><NewLine><p>Processor 1</p><NewLine><ol><NewLine><li>Put tensor on cuda:0, get the output.</li><NewLine><li>Serialize and push the output to shared redis database</li><NewLine></ol><NewLine><p>Processor 2</p><NewLine><ol><NewLine><li>Consumer picks up from database, pushes to cuda:1</li><NewLine><li>Consumer runs the next step of calculation.</li><NewLine></ol><NewLine><p>If you need to send gradients for backprop you can store and reload them also.</p><NewLine><aside class=""quote quote-modified"" data-post=""1"" data-topic=""34398""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/duanenielsen/40/4158_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/bit-of-fun-with-gradients/34398"">Bit of fun with gradients</a> <a class=""badge-wrapper bullet"" href=""/c/autograd""><span class=""badge-category-bg"" style=""background-color: #AB9364;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""A category of posts relating to the autograd engine itself."">autograd</span></a><NewLine></div><NewLine><blockquote><NewLine>    Thought I’d share this code.  Learned a few things about autograd doing this. <NewLine>import torch<NewLine><NewLine># unbroken gradient, backward goes all the way to x<NewLine>x = torch.ones(2, 2, requires_grad=True)<NewLine>y = 2 * x + 2<NewLine>z = y * y * 3<NewLine>out = z.mean()<NewLine>out.backward()<NewLine>print(x.grad)<NewLine>baseline_x = x.grad<NewLine><NewLine># broken gradient, ends at _y<NewLine>x = torch.ones(2, 2, requires_grad=True)<NewLine>y = 2 * x + 2<NewLine><NewLine>_y = torch.tensor(y.detach(), requires_grad=True)<NewLine>z = _y * _y * 3<NewLine>out = z.mean()<NewLine>out.backward()<NewLine>print(x.grad)<NewLine>print(_y.grad)<NewLine><NewLine># we can …<NewLine>  </blockquote><NewLine></aside><NewLine><p>That’s one way…  Not easy though.  I spent easy a month just trying to distribute calculations over multiple processors.</p><NewLine><p>If you can pull it off… then it’s an awesome skill.</p><NewLine><p>Also, there is the Ray project <a href=""https://github.com/ray-project/ray"" rel=""nofollow noopener"">https://github.com/ray-project/ray</a></p><NewLine><p>I tried using it.  It had great promise, but ended up being a bit too new at the time.  It might be a bit more mature now.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply and sorry for my late reply. I will look into these methods. I am only doing this for the test phase, so I will only have to transfer one tensor per input frame to processor 2.</p><NewLine><p>If anybody else has some further suggestions, I will be happy to hear them!</p><NewLine><p>Thanks.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Will this tutorial be helpful? <a href=""https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you. I have seen this tutorial previously, however, the model parallel part is not what I want. For the pipelining part, I am having trouble how that part is getting executed. If you can further clarify that part for me, that would be great.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/DuaneNielsen; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/uertenli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/uertenli; <NewLine> ,"REPLY_DATE 1: December 21, 2019,  4:25am; <NewLine> REPLY_DATE 2: December 22, 2019,  8:05pm; <NewLine> REPLY_DATE 3: December 27, 2019,  3:32pm; <NewLine> REPLY_DATE 4: December 28, 2019, 11:33am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
37899,Setup models on different gpus and use dataparallel,2019-02-21T21:18:21.532Z,0,160,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Lets say I have 2 models A() and B() and 2 gpus. Outputs of A will be fed to B as inputs</p><NewLine><p>Because 2 models are too big to fit on the same gpus so I have to manually instantiate A on gpu 0 and B on gpu 1. Hence, I have to manually change the device of A’s output to feed to B.</p><NewLine><p>Sometimes, my batchsize is too large to run A() on gpu 0, but if I theoretically can utilize gpu1, I can still use that batch size without reducing it.</p><NewLine><p>The question is can my models be placed on different gpus but still run in dataparallel mode?</p><NewLine><p>Update:<br/><NewLine>I saw a post mentioning about this:</p><NewLine><pre><code class=""lang-auto"">model1 = nn.DataParallel(model1).cuda(device=0)<NewLine>model1_feat = model1(input_image)<NewLine><NewLine>model2 = nn.DataParallel(model2).cuda(device=1)<NewLine>model2_feat = model2(model1_feat, input_feat)<NewLine></code></pre><NewLine><p>My question is does that mean model1 is replicated on both gpu?</p><NewLine></div>",https://discuss.pytorch.org/u/Hung_Nguyen,(Hung Nguyen),Hung_Nguyen,"February 21, 2019,  9:27pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""37899""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/hung_nguyen/40/8273_2.png"" width=""20""/> Hung_Nguyen:</div><NewLine><blockquote><NewLine><p>My question is does that mean model1 is replicated on both gpu?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, <code>DataParallel </code> will replicate the model, scatter inputs, and gather outputs in every iteration. So, for the above code snippet, everytime you run <code>model1_feat = model1(input_image)</code>, <code>model1</code> is replicated to all devices in the forward pass.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: December 27, 2019,  6:53pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
57983,How to combine data parallelism with model parallelism for multiple nodes?,2019-10-11T19:30:17.478Z,0,311,"<div class=""post"" itemprop=""articleBody""><NewLine><p>For example, I have 4 nodes and every node has two gpus. I want to devide one model into four parts, every node run part of the model and  use data parallelism on its two gpus.</p><NewLine><p>I use hook to get the gradients and use “dist.send” to send them to other node, it’s effective for<br/><NewLine>model parallelism.</p><NewLine><blockquote><NewLine><h1>on node 1:</h1><NewLine><p>dist.init_process_group(backend=“gloo”, init_method=‘tcp://172.22.4.11:28456’, rank=0,<br/><NewLine>world_size=2)</p><NewLine><h1>outputs is the result of node 1</h1><NewLine><p>dist.send(tensor=outputs.to(‘cpu’), dst=1, tag=0)</p><NewLine><h1>rec is the gradients send from node 2</h1><NewLine><p>dist.recv(rec, src=1)<br/><NewLine>outputs.backward(rec.cuda())</p><NewLine><h1>on node 2:</h1><NewLine><p>dist.init_process_group(backend=“gloo”, init_method=‘tcp://172.22.4.11:28456’, rank=1,<br/><NewLine>world_size=2)</p><NewLine><h1>rec is the result of node 1</h1><NewLine><p>dist.recv(tensor=rec, src=0, tag=0)<br/><NewLine>outputs2 = net2(rec)</p><NewLine><h1>feta[0] is the gradients of node 2</h1><NewLine><p>dist.send(tensor=feat[0].to(‘cpu’), dst=0)</p><NewLine></blockquote><NewLine><p>But when I try to combine data parallelism with model parallelism, it failed. I choose “torch.nn.parallel.DistributedDataParallel” to achieve data parallelism, but node2 can’t receive the  gradients from node1.</p><NewLine><p>Question:<br/><NewLine>So how to combine data parallelism with model parallelism for multiple nodes?</p><NewLine></div>",https://discuss.pytorch.org/u/Sword-Song,(Sword Song),Sword-Song,"October 11, 2019,  7:31pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It might be easier to run model parallel on multiple GPUs in the same machine and distributed data parallel across machines. Checkout <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#combine-ddp-with-model-parallelism"" rel=""nofollow noopener"">this section</a> for more details.</p><NewLine><p>For your above use case, you will need to create multiple process groups. Given the above configuration, 4 nodes, and 2 GPUs per node, I assume you will have 8 processes, one process per GPU. Then you can create:</p><NewLine><ol><NewLine><li>Two process groups of world size 4, which will be responsible for send/recv outputs/gradients across machines.</li><NewLine><li>One process group of world size 2 on EACH machine, which will do the distributed data parallel on the machine.</li><NewLine></ol><NewLine><p>The reason you need the above setting is because DistributedDataParallel would expect all processes in the same group are training the same model in a synchronized fashion. It won’t work if you only use 2 processes in the same group of size 8.</p><NewLine><p>See the <a href=""https://github.com/pytorch/pytorch/blob/46539eee0363e25ce5eb408c85cefd808cd6f878/torch/distributed/distributed_c10d.py#L1434"" rel=""nofollow noopener""><code>new_group</code></a> API.</p><NewLine><p>BTW, the <a href=""https://pytorch.org/docs/master/rpc.html"" rel=""nofollow noopener""><code>torch.distributed.rpc</code></a> API might make send/recv outputs/grads easier for you, and it also supports distributed autograd.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: December 27, 2019,  6:44pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
33930,Distributed training of multiple models on multiple nodes (CPU only),2019-01-06T10:45:31.175Z,1,432,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I have been trying to figure out how to train a population of models on multiple nodes (which do not have GPUs, but that’s not the main point; I’m happy with training on CPUs). Ideally, I would like a single process per model running on a separate CPU. I can request hundreds or thousands of CPUs, and each model is <em>fully contained</em>, meaning that I don’t want to share any parameters from one model across nodes; rather, I want each model to train on its own CPU.</p><NewLine><p>I have tried using a worker pool from <code>torch.multiprocessing</code> and passing models to the training function. I train each model for one epoch, then I perform some processing in the main process and then I map them again to the worker pool to train them for another epoch, and so on. That works fine if I run the models on a single machine, but it doesn’t scale up to a multi-node scenario because <code>torch.multiprocessing</code> is not aware of the additional nodes (I requested 256 CPUs on the cluster, which translates to 8 nodes with 16 CPUs each, but 7 of those remained idle).</p><NewLine><p>As far as I can tell, all examples I found (for example, using <code>torch.distributed</code> <a href=""https://pytorch.org/tutorials/intermediate/dist_tuto.html"" rel=""nofollow noopener"">here</a>) assume that you have a single large model and you want to spread the training of one model across multiple workers. This is not my case - my models are small and I’d like to train them in parallel but independently of each other. They <em>are</em>, however, being trained on the same task using the same data, in case that’s relevant.</p><NewLine><p>Any help would be appreciated! Apologies if I’m missing something obvious.</p><NewLine></div>",https://discuss.pytorch.org/u/gaussian,,gaussian,"January 6, 2019, 10:46am",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>IIUC, DistributedDataParallel does not fit in this use case because you have a population of independent models to train on the same set of data instead one big model on different splits of input data. It looks like the experimental <a href=""https://pytorch.org/docs/master/rpc.html"" rel=""nofollow noopener""><code>torch.distributed.rpc</code></a> might be helpful here, it would at least help you take care of the communication. But you would still need to write code to dispatch models to the workers in the pool.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: December 27, 2019,  6:26pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
36303,Torch.distributed does not support part of the model training?,2019-02-03T15:01:10.971Z,0,638,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, recently I tried to use torch.distributed package to train my model. In my case, I used one architecture like encoder-decoder. For accelerating my code, I pre-computed features of input data, but when I only trained decoder with calling model.module.decoder (input), it gave me some bug info like the following<br/><NewLine><strong>TypeError: _queue_reduction(): incompatible function arguments. The following argument types are supported:</strong><br/><NewLine><strong>1. (process_group: torch.distributed.ProcessGroup, grads_batch:List[List[at::Tensor]], devices: List[int]) -&gt; Tuple[torch.distributed.Work, at::Tensor]</strong><br/><NewLine>I wonder if someone can give me some suggestions? Does torch.distributed package could not work normally if part of the model doesn’t participate in computing process?</p><NewLine></div>",https://discuss.pytorch.org/u/Lausanne,,Lausanne,"February 3, 2019,  3:01pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>In v1.1, we added a new <a href=""https://github.com/pytorch/pytorch/blob/46539eee0363e25ce5eb408c85cefd808cd6f878/torch/nn/parallel/distributed.py#L196"" rel=""nofollow noopener""><code>find_unused_parameters</code></a> arg to DistributedDataParallel. If some of the model params are not involved in the forward pass, you can set  <code>find_unused_parameters</code> to <code>True</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: December 27, 2019,  6:14pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
58465,How to reduce the execution time of &ldquo;forward pass&rdquo; on GPU,2019-10-17T10:29:39.747Z,4,540,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I already had a pre-train model. I only use it to extract the feature (only use “forward pass”).<br/><NewLine>Now, I only load it into single GPU and use with   <code>torch.no_grad()</code><br/><NewLine>My question is that: I have 2 GPUs on my computer and how to reduce the execution time with 2 GPUs.<br/><NewLine>P/s: Input of my model have a size as <code>(8,3,256,128)</code></p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/dat_pham_thanh,(Dat Pham Thanh),dat_pham_thanh,"October 17, 2019,  3:22pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"" rel=""nofollow noopener"">DataParallel</a> and <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L33"" rel=""nofollow noopener"">DistributedDataParallel</a> would help here.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for replaying!!</p><NewLine><p>However, when I read the document of <a href=""https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"" rel=""nofollow noopener"">DataParallel</a> and <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L33"" rel=""nofollow noopener"">DistributedDataParallel </a>, I think it would not help me to reduce the execution time because I do not need the backward pass.</p><NewLine><pre><code class=""lang-auto"">&gt; assert any((p.requires_grad for p in module.parameters())), (<NewLine>&gt;             ""DistributedDataParallel is not needed when a module ""<NewLine>&gt;             ""doesn't have any parameter that requires a gradient.""<NewLine>&gt;         )<NewLine></code></pre><NewLine><p>I will try with it and tell u the result.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>How to use DataParallel:<br/><NewLine>model = DataParallel(model, dim=<em>your batch dim in input</em>, device_ids=[main_id, other_ids …], output_device=main_id)<br/><NewLine>Note that main_id (GPU that store the original model parameter) should be the first in the list of device_ids.</p><NewLine><p>You can use DataParallel since it’s easier to setup and test, but remember to:</p><NewLine><ol><NewLine><li>Set the batch dimension of DataParallel. The default is dim=0 but sometime you might want to apply another dimension.<br/><NewLine>(e.g. my input size is (Time, Batch, Dim_Data), and the model require a fully time series. In this scenario I will apply dim=1 instead of dim=0, because If I choose dim=0, the time serie will be split into multiple fragments.)</li><NewLine><li>DataParallel return a wrapped model, so use the wrapped model to forward instead of original one, and remember to…</li><NewLine><li>…handle the state_dict of wrapped model before save the state_dict into files. DataParallel will append prefix “module.” to each key of the original state_dict.keys(), you have to remove the prefix before saving the state_dict.</li><NewLine><li>Set the GPU with larger memory as output_device , and also pass model parameter and your input to this GPU. output_device need to store both data and model parameters, so larger GPU memory is favorable.</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here is my code with DataParallel</p><NewLine><pre><code class=""lang-auto"">import time<NewLine>import torchvision.models as models<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>model = models.resnet50(num_classes=1000).to('cuda:0')<NewLine><NewLine>model = nn.DataParallel(model, device_ids=[0,1], output_device=0)<NewLine>#####<NewLine>batch_size = 8<NewLine>image_w = 128<NewLine>image_h = 128<NewLine><NewLine>#####<NewLine>#warm up GPU<NewLine>input = torch.randn(batch_size,3,image_w, image_h).to('cuda:0')<NewLine>model.eval()<NewLine><NewLine>listTime = []<NewLine>for i in range(20):<NewLine>    with torch.no_grad():<NewLine>        startTime = time.time()<NewLine>        input = torch.randn(batch_size,3,image_w, image_h).to('cuda:0')<NewLine>        out = model(input)<NewLine>        esl = time.time() - startTime<NewLine>        listTime.append(esl)<NewLine>        print(""Total time of loop {} :: {}"".format(i, esl))<NewLine><NewLine>meanTime = torch.mean(torch.tensor(listTime[9:]))<NewLine>print(meanTime)<NewLine></code></pre><NewLine><p>I test with resnet50(). The size of input is (8,3,128,128).<br/><NewLine>I run the forward() pass in 20 steps and choice the last 10 steps to find the mean of execution time.</p><NewLine><p>Without Dataparallel, <code>meanTime = 0.0064s</code> (run with single GPU)<br/><NewLine>and with Dataparalle, <code>meanTime = 0.0396s</code>(run with 2 GPUs)<br/><NewLine>P/s: I have 2 GPUs as below image.<br/><NewLine>Do you have any solution for my problem ?<br/><NewLine>Thanks</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/4ef855adac77bba3401ad880399e3e69120deab8"" href=""https://discuss.pytorch.org/uploads/default/original/3X/4/e/4ef855adac77bba3401ad880399e3e69120deab8.png"" title=""nvida.png""><img alt=""nvida"" data-base62-sha1=""bgBiqNW9i7ZtBck63zHEWumetPW"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/e/4ef855adac77bba3401ad880399e3e69120deab8_2_10x10.png"" height=""200"" src=""https://discuss.pytorch.org/uploads/default/original/3X/4/e/4ef855adac77bba3401ad880399e3e69120deab8.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">nvida.png</span><span class=""informations"">710×206 26.4 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/dat_pham_thanh"">@dat_pham_thanh</a> Can you benchmark using at least 1000 iterations and also track throughput instead (images/s)? Mean time can be misleading since a single outlier could change the mean quite a bit.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/dat_pham_thanh"">@dat_pham_thanh</a></p><NewLine><p>DataParallel would replicate the model, scatter the input, and gather outputs in every iteration. So, if the input size is too small, the overhead of replicating the model might overshadow the benefits of parallelizing the computation. Besides what <a class=""mention"" href=""/u/pritamdamania87"">@pritamdamania87</a> suggested above, could you please also try with large batch size?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank for your reply!!<br/><NewLine>I think you are correct. I can not increase the batch size because it is fixed ( batch size always equals 8) for each iteration.<br/><NewLine>I use the ONNX model to solve my problem!!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dat_pham_thanh; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/wangwwno1; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/dat_pham_thanh; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/dat_pham_thanh; <NewLine> ,"REPLY_DATE 1: October 17, 2019,  5:40pm; <NewLine> REPLY_DATE 2: October 18, 2019,  8:53am; <NewLine> REPLY_DATE 3: October 18, 2019,  9:28am; <NewLine> REPLY_DATE 4: October 18, 2019, 10:52am; <NewLine> REPLY_DATE 5: October 21, 2019, 10:02pm; <NewLine> REPLY_DATE 6: December 27, 2019,  3:30pm; <NewLine> REPLY_DATE 7: December 27, 2019,  4:01pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
54503,Multiple replicas of the model on same GPU?,2019-08-28T00:02:05.008Z,1,155,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am a newbee for pytorch distributed.<br/><NewLine>My model is only a small component of a much more complicated problem.<br/><NewLine>I noticed that if I train it using single-GPU, then it takes at most one quarter of the GPU memory and utility.<br/><NewLine>So I wonder if it is possible to distribute four replicas of the model on the same GPU so that hopefully I can get 4x speedup.</p><NewLine><p>I read the documents and there are many example of multi-gpu, but none of them is using fractional gpu like this. Anyone have ideas? Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Joshua_Lian,(Joshua Lian),Joshua_Lian,"August 28, 2019, 12:02am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It really depends. Even if 4 replicas of your model can fit into the memory of one GPU, they still need to compete for the same set of streaming multiprocessors and other shared resources on that GPU. You can try if using multiple streams would help, e.g.:</p><NewLine><pre><code class=""lang-python"">s0 = torch.cuda.Stream()<NewLine>s1 = torch.cuda.Stream()<NewLine>with torch.cuda.stream(s0):<NewLine>    output0 = model_replica0(input0)<NewLine><NewLine>with torch.cuda.stream(s1):<NewLine>    output1 = model_replica1(input1)<NewLine><NewLine>s0.synchronize()<NewLine>s1.synchronize()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, Shen Li, Hi, the “DistributedDataParallel” automatically average the gradient when calling “loss.backward()”,<br/><NewLine>But I didn’t find the corresponding script in pytorch source code, Do you know where it is ?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/meilu_zhu"">@meilu_zhu</a></p><NewLine><p>Sorry about the delay. The grad averaging algorithm is implemented in the <a href=""https://github.com/pytorch/pytorch/blob/70013415c7acc109985a86f42d835fbbcfa45bf3/torch/csrc/distributed/c10d/reducer.cpp"" rel=""nofollow noopener"">reducer</a>. Each DistributedDataParallel creates its reducer instance <a href=""https://github.com/pytorch/pytorch/blob/70013415c7acc109985a86f42d835fbbcfa45bf3/torch/nn/parallel/distributed.py#L378"" rel=""nofollow noopener"">in the constructor</a>. More specifically, allreduce is invoked <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/reducer.cpp#L415"" rel=""nofollow noopener"">here</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/meilu_zhu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: October 16, 2019,  2:10am; <NewLine> REPLY_DATE 2: November 26, 2019,  2:44am; <NewLine> REPLY_DATE 3: December 27, 2019,  3:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
64753,RuntimeError: address family mismatch when use &lsquo;gloo&rsquo; backend,2019-12-22T09:59:15.515Z,0,168,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!<br/><NewLine>I have four machines, and each machine has one GPU device. I want to train my model use four GPU devices but failed.<br/><NewLine>Below are the information of my machines.(node name with ip)</p><NewLine><pre><code class=""lang-auto"">n100: 172.22.99.10<NewLine>n101: 172.22.99.11<NewLine>n102: 172.22.99.12<NewLine>n104: 172.22.99.14<NewLine></code></pre><NewLine><p>In my program, I use the Gloo backend. If I run the program with 3 nodes: n100, n101, n102, the program works well. But when I use all the nodes, I get the following error:</p><NewLine><pre><code class=""lang-bash"">fanxp@n100:~/vscode/pytorch_test$ ./parallel_deepAR.py --rank 0 --world-size 4<NewLine>Traceback (most recent call last):<NewLine>  File ""./parallel_deepAR.py"", line 472, in &lt;module&gt;<NewLine>    init_process(args.rank, args.world_size, run, 'gloo', args.ip, args.port)<NewLine>  File ""./parallel_deepAR.py"", line 313, in init_process<NewLine>    init_method='tcp://{}:{}'.format(ip, port), rank=rank, world_size=size)<NewLine>  File ""/simm/home/fanxp/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 410, in init_process_group<NewLine>    timeout=timeout)<NewLine>  File ""/simm/home/fanxp/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 478, in _new_process_group_helper<NewLine>    timeout=timeout)<NewLine>RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:207] address family mismatch<NewLine></code></pre><NewLine><p>I think the node <code>n104</code> may have a different address family, which cause the error. But I don’t know how to solve this.<br/><NewLine><strong>Some additional informations</strong></p><NewLine><ul><NewLine><li>the <code>ifconfig</code> output of the internet interface on each node</li><NewLine></ul><NewLine><pre><code class=""lang-bash"">fanxp@n100:~/vscode/pytorch_test$ ifconfig eth5<NewLine>eth5: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<NewLine>        inet 172.22.99.10  netmask 255.255.255.0  broadcast 172.22.99.255<NewLine>        inet6 fe80::1602:ecff:fe69:ef5d  prefixlen 64  scopeid 0x20&lt;link&gt;<NewLine>        inet6 2400:dd02:100c:3199:1602:ecff:fe69:ef5d  prefixlen 64  scopeid 0x0&lt;global&gt;<NewLine>        ether 14:02:ec:69:ef:5d  txqueuelen 1000  (Ethernet)<NewLine>        RX packets 472256109  bytes 701421415319 (701.4 GB)<NewLine>        RX errors 0  dropped 5470  overruns 0  frame 0<NewLine>        TX packets 553043129  bytes 818712088574 (818.7 GB)<NewLine>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<NewLine>fanxp@n101:~/vscode/pytorch_test$ ifconfig eth5<NewLine>eth5: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<NewLine>        inet 172.22.99.11  netmask 255.255.255.0  broadcast 172.22.99.255<NewLine>        inet6 fe80::211:aff:fe6c:2345  prefixlen 64  scopeid 0x20&lt;link&gt;<NewLine>        inet6 2400:dd02:100c:3199:211:aff:fe6c:2345  prefixlen 64  scopeid 0x0&lt;global&gt;<NewLine>        ether 00:11:0a:6c:23:45  txqueuelen 1000  (Ethernet)<NewLine>        RX packets 373027705  bytes 535914116118 (535.9 GB)<NewLine>        RX errors 0  dropped 1720  overruns 0  frame 0<NewLine>        TX packets 87419537  bytes 80820382770 (80.8 GB)<NewLine>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<NewLine>fanxp@n102:~/vscode/pytorch_test$ ifconfig eth5<NewLine>eth5: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<NewLine>        inet 172.22.99.12  netmask 255.255.255.0  broadcast 172.22.99.255<NewLine>        inet6 fe80::211:aff:fe6c:2325  prefixlen 64  scopeid 0x20&lt;link&gt;<NewLine>        inet6 2400:dd02:100c:3199:211:aff:fe6c:2325  prefixlen 64  scopeid 0x0&lt;global&gt;<NewLine>        ether 00:11:0a:6c:23:25  txqueuelen 1000  (Ethernet)<NewLine>        RX packets 9676903  bytes 10243508657 (10.2 GB)<NewLine>        RX errors 0  dropped 0  overruns 0  frame 0<NewLine>        TX packets 8458287  bytes 7559359606 (7.5 GB)<NewLine>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<NewLine>fanxp@n104:~/vscode/pytorch_test$ ifconfig ens1f1<NewLine>ens1f1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<NewLine>        inet 172.22.99.14  netmask 255.255.255.0  broadcast 172.22.99.255<NewLine>        inet6 2400:dd02:100c:3199:1602:ecff:fe72:8ae8  prefixlen 64  scopeid 0x0&lt;global&gt;<NewLine>        inet6 fe80::1602:ecff:fe72:8ae8  prefixlen 64  scopeid 0x20&lt;link&gt;<NewLine>        ether 14:02:ec:72:8a:e8  txqueuelen 1000  (Ethernet)<NewLine>        RX packets 6220778  bytes 5698014724 (5.6 GB)<NewLine>        RX errors 0  dropped 1166  overruns 0  frame 0<NewLine>        TX packets 12621081  bytes 14816572590 (14.8 GB)<NewLine>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<NewLine></code></pre><NewLine><ul><NewLine><li>all the network interface use the infiniband</li><NewLine><li>the source code is too complex, so it’s not good to provide the code. I think the code to initialize process may be helpful</li><NewLine></ul><NewLine><pre><code class=""lang-python"">def init_process(rank, size, fn, backend='gloo', ip=None, port=None):<NewLine>    """""" Initialize the distributed environment. """"""<NewLine>    # os.environ['MASTER_ADDR'] = '172.22.99.10'<NewLine>    # os.environ['MASTER_PORT'] = '29500'<NewLine>    # dist.init_process_group(backend, rank=rank, world_size=size)<NewLine>    dist.init_process_group(<NewLine>        backend=backend,<NewLine>        init_method='tcp://{}:{}'.format(ip, port), rank=rank, world_size=size)<NewLine>    fn(rank, size)<NewLine></code></pre><NewLine><ul><NewLine><li>The master used in Gloo backend<br/><NewLine>address: 172.22.99.10 port: 20000</li><NewLine><li>pytorch version<br/><NewLine>PyTorch version: 1.3.0a0+ee77ccb<br/><NewLine>Is debug build: No<br/><NewLine>CUDA used to build PyTorch: 10.1.243<br/><NewLine>OS: Ubuntu 18.04.1 LTS<br/><NewLine>GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0<br/><NewLine>CMake version: version 3.10.2<br/><NewLine>Python version: 3.6<br/><NewLine>Is CUDA available: Yes<br/><NewLine>CUDA runtime version: 10.1.243<br/><NewLine>GPU models and configuration: GPU 0: Tesla K40c<br/><NewLine>Nvidia driver version: 440.33.01<br/><NewLine>cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.2<br/><NewLine>Versions of relevant libraries:<br/><NewLine>[pip] numpy==1.14.5<br/><NewLine>[conda] Could not collect</li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/xinping_fan,(xinping fan),xinping_fan,"December 22, 2019, 10:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you try specifying <code>GLOO_SOCKET_IFNAME</code> to select the appropriate interface on each node as described here: <a href=""https://pytorch.org/docs/stable/distributed.html#choosing-the-network-interface-to-use"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/distributed.html#choosing-the-network-interface-to-use</a>?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> ,"REPLY_DATE 1: December 27, 2019,  6:02am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
63648,DDP for model parallelism,2019-12-10T17:55:42.752Z,0,200,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to implement the main idea in <a href=""https://arxiv.org/abs/1806.03863"" rel=""nofollow noopener"">Massively Parallel Video Networks</a>. In the paper, the authors implement model parallelism for training video networks by giving the outputs of each layer to the next layer as usual but to the next time step. This way, they can process layers independently on different GPUs. The following figure shows the most basic case:</p><NewLine><p><img alt=""Screenshot%20from%202019-12-10%2020-45-48"" data-base62-sha1=""1DOKTfFaMJu2vyFHqtBDzhhQbsv"" height=""203"" src=""https://discuss.pytorch.org/uploads/default/original/3X/0/b/0b827525be2bf106b4aee341c9c8681dcfbfd1a3.png"" width=""142""/></p><NewLine><p>This photo shows the network unrolled over time. At every time instant, for this case, we have 4 different layers and each of these 4 layers can be processed independently on separate GPUs.</p><NewLine><p>To do this I want to use hopefully, something simple. However, I am having trouble understanding if I can use DDP for this. Essentially, I want to divide my model into independent blocks and pass the gradient in the direction of the arrows. Can I use DDP with model parallel as given <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#combine-ddp-with-model-parallelism"" rel=""nofollow noopener"">here</a>. If so, how to do this for a big model where I can choose how I define my sub-blocks.</p><NewLine><p>As a further question: In the paper, they also implement the same idea on a CPU. Is there some function in PyTorch which can achieve this task.</p><NewLine><p>Note: We cannot use <code>nn.DataParallel()</code> because the training setting is an online training and we want to process frames one by one as they come.</p><NewLine><p>Thanks in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/uertenli,(U Ertenli),uertenli,"December 10, 2019,  5:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>IIUC, what we’re looking for here is pipeline parallelism. PyTorch currently doesn’t have native support for pipeline parallelism. There are a few projects that have built something similar on top of PyTorch: <a href=""https://github.com/kakaobrain/torchgpipe"" rel=""nofollow noopener"">https://github.com/kakaobrain/torchgpipe</a> and <a href=""https://github.com/msr-fiddle/pipedream"" rel=""nofollow noopener"">https://github.com/msr-fiddle/pipedream</a>. You could also use the <a href=""https://pytorch.org/docs/master/rpc.html"" rel=""nofollow noopener"">Distributed RPC Framework</a> to build something like this.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> ,"REPLY_DATE 1: December 26, 2019, 11:35pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
63854,ParameterList assigned to 1 GPU only (?),2019-12-12T10:45:29.184Z,0,147,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey folks,</p><NewLine><p>I am new to pytorch and I am trying to parallelize my network. Using <code>nn.DataParallel</code> seems to work as expected for the <code>nn.modules</code> living inside my class, however, it looks like the <code>nn.ParameterLists</code> that I’m defining as class members are listed as sitting in <code>(GPU 0)</code> only, when I print out the module’s parameters:</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/4ab6504d922e1f0c172c7a21cf004209d1263d8a"" href=""https://discuss.pytorch.org/uploads/default/original/3X/4/a/4ab6504d922e1f0c172c7a21cf004209d1263d8a.png"" title=""image""><img alt=""image"" data-base62-sha1=""aEVW9Yu8TNMqkNc49YzJ6Uvup0C"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/a/4ab6504d922e1f0c172c7a21cf004209d1263d8a_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/a/4ab6504d922e1f0c172c7a21cf004209d1263d8a_2_688x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/a/4ab6504d922e1f0c172c7a21cf004209d1263d8a_2_688x500.png, https://discuss.pytorch.org/uploads/default/optimized/3X/4/a/4ab6504d922e1f0c172c7a21cf004209d1263d8a_2_1032x750.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/4/a/4ab6504d922e1f0c172c7a21cf004209d1263d8a.png 2x"" width=""688""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1182×859 127 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>Is this expected behaviour and why are they not listed on both of the GPUs I’m using? Could somebody please explain what is going on here?</p><NewLine><hr/><NewLine><ul><NewLine><li><NewLine><code>torch.cuda.device_count</code> returns <code>2</code> as expected.</li><NewLine></ul><NewLine><p>My code looks something like the following:</p><NewLine><pre><code class=""lang-auto"">class Network(nn.Module):<NewLine>    def __init__(self):<NewLine>        ...<NewLine>        self.templates = nn.ModuleList([nn.ParameterList([nn.Parameter(template_init, requires_grad=True) for i in range(n)]) for n in self.num_t])<NewLine><NewLine>...<NewLine><NewLine>self.Network = nn.DataParallel(self.Network)<NewLine>self.Network.to(self.device)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/ortho-stice,,ortho-stice,"December 12, 2019, 10:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ortho-stice"">@ortho-stice</a></p><NewLine><p>This is expected behavior. Here is the source code of DataParallel: <a href=""https://github.com/pytorch/pytorch/blob/46539eee0363e25ce5eb408c85cefd808cd6f878/torch/nn/parallel/data_parallel.py#L148-L153"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/46539eee0363e25ce5eb408c85cefd808cd6f878/torch/nn/parallel/data_parallel.py#L148-L153</a></p><NewLine><p>What happens is that, in every forward pass, DataParallel will</p><NewLine><ol><NewLine><li>scatters the input to all GPUs</li><NewLine><li>replicate the model to all GPUs</li><NewLine><li>launch parallel_apply so that every GPU will run its own forward pass using its input data split in parallel.</li><NewLine><li>gather all outputs to the output device</li><NewLine></ol><NewLine><p>So the model replication only occurs in the forward pass, and hence you won’t see those model replicas outside the forward function.</p><NewLine><p>BTW, we do recommend using DistributedDataParallel which only replicates the model once in constructor instead of in every forward invocation.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: December 26, 2019,  4:21pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
63790,How does DistributedDataParallel handle ignore classes when averaging gradients?,2019-12-11T21:35:15.355Z,0,226,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How does gradient averaging work in DistributedDataParallel training? I am particularly interested in what happens when the batches have masked or ignored data, e.g. with semantic segmentation.</p><NewLine><p>For example: let’s say I have 4 GPUs and I am training a semantic segmentation network with a dataset with an ignore class. As I understand it, in the DataParallel setting, the outputs are aggregated on GPU0, the loss computed, and then the gradient is backpropagated back through each GPU’s model. In the DistributedDataParallel case,  L0, L1, L2, L3 are each computed for each GPU’s share of the batch, the losses are backpropagated back through their respective GPU’s model, and the gradients along the way are averaged.</p><NewLine><p>Using DataParallel, the presence of an ignore class makes no difference. Even if one GPU’s mini-batch has a lopsided amount of ignore pixels, the loss is computed as the weighted average. However, what happens when you have a lopsided distribution of ignore pixels on one GPU using DistributedDataParallel? There does not seem to be any mechanism for weighting the average of the gradients. Yet in this case, L0, L1, L2, and L3 ought to have their contributions weighted by the ratio of valid pixels when averaging gradients during backpropagation.</p><NewLine><p>Is there some way to handle this ignore class imbalance during distributed training?</p><NewLine></div>",https://discuss.pytorch.org/u/marcman411,(Marc),marcman411,"December 22, 2019,  4:01pm",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>How does gradient averaging work in DistributedDataParallel training?</p><NewLine></blockquote><NewLine><ol start=""0""><NewLine><li>Every DDP instance will have its own copy of the local model, and DDP will setup post autograd hooks on every parameter (i.e., DDP hooks).</li><NewLine><li>In every forward pass, DDP feeds the input data to its own local model, and returns the local output to the application.</li><NewLine><li>The application uses the local output to compute the local loss, and calls backward on the local loss, which kicks off the local autograd engine to compute gradient for all parameters. When one local gradient becomes ready, that will trigger the corresponding DDP hook. The DDP hook will run allreduce on the given gradients, and write the averaged grads back to the parameter.grad field.</li><NewLine><li>When backward is done, parameter.grad should all be globally averaged gradients. Optimizer can then consume that grad to update parameters.</li><NewLine></ol><NewLine><blockquote><NewLine><p>Is there some way to handle this ignore class imbalance during distributed training?</p><NewLine></blockquote><NewLine><p>DDP simply averages (sum and then divide by the number of DDP world size) all local gradients. So, it should work as long as the ignored data do not contribute to the local gradients.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: December 26, 2019,  4:04pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
51204,How to use SyncBatchNorm in nn.parallel.DistributedDataParallel with v1.1.0?,2019-07-21T14:18:03.162Z,1,2946,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Could you please post a short code to introduce the instructions of it?<br/><NewLine>I have a machine with two GPUs, which means I want to use single process multi gpus.<br/><NewLine>I tried to use SyncBatchNorm, but failed, sadly like this …</p><NewLine><p>It raise a “ValueError: SyncBatchNorm is only supported for DDP with single GPU per process”…!<br/><NewLine>But in <a href=""https://pytorch.org/docs/stable/nn.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel"" rel=""nofollow noopener"">docs of DDP</a>, it says single-process multi-gpus.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>class net(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(net, self).__init__()<NewLine>        self.convBlock = nn.Sequential(<NewLine>            nn.Conv2d(3, 128, 3, 1, 1),<NewLine>            nn.SyncBatchNorm(128),<NewLine>            nn.ReLU(),<NewLine>            nn.Conv2d(128, 512, 3, 1, 1),<NewLine>            nn.SyncBatchNorm(512),<NewLine>            nn.ReLU(),<NewLine>            nn.Conv2d(512, 1, 3, 1, 1),<NewLine>            nn.SyncBatchNorm(1),<NewLine>            nn.ReLU()<NewLine>        )<NewLine>    def forward(self, x):<NewLine>        x = self.convBlock(x)<NewLine>        return x<NewLine>torch.distributed.init_process_group(backend='nccl', init_method='tcp://127.0.0.1:12345', world_size=1, rank=0)<NewLine><NewLine>model = net().cuda()<NewLine>model = nn.parallel.DistributedDataParallel(model, device_ids=[0, 1], output_device=0)<NewLine>model = model<NewLine> optimizer = torch.optim.Adam(model.parameters())<NewLine>mseloss = torch.nn.L1Loss()<NewLine>for i in range(1000):<NewLine>    x = torch.rand(10, 3, 224, 224)<NewLine>    y = torch.rand(10, 1, 224, 224)<NewLine>    x = x.cuda()<NewLine>    y = y.cuda()<NewLine>    out = model(x)<NewLine>    optimizer.zero_grad()<NewLine>    loss = mseloss(out, y)<NewLine>    print(i, loss)<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/cb_zhang,(Cb Zhang),cb_zhang,"July 21, 2019,  2:43pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is expected.</p><NewLine><p>While DDP supports using multiple GPUs from a single process, <code>nn.SyncBatchNorm</code> does not and requires you to use a single GPU per process. Also see the docs for <a href=""https://pytorch.org/docs/master/nn.html#torch.nn.SyncBatchNorm"" rel=""nofollow noopener""><code>torch.nn.SyncBatchNorm</code></a>:</p><NewLine><blockquote><NewLine><p>Currently SyncBatchNorm only supports DistributedDataParallel with single GPU per process. Use torch.nn.SyncBatchNorm.convert_sync_batchnorm() to convert BatchNorm layer to SyncBatchNorm before wrapping Network with DDP.</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think this is worth fixing. Distributed data parallel uses a lot of CPU threads. This is okay for expensive servers used by industry, but a lot of us have a limited number of CPU cores at our disposal.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tstandley; <NewLine> ,"REPLY_DATE 1: July 24, 2019,  8:41am; <NewLine> REPLY_DATE 2: December 25, 2019,  8:05am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
59291,DistributedDataParallel modify gradient before averaging,2019-10-26T18:45:11.685Z,4,619,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all! I think the “DistributedDataParallel” automatically average the gradient when calling “loss.backward()”. But is it possible to first compute the local gradient of the parameters, then do some modification to the local gradient, and finally average the gradient among the workers?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/anxu,(An Xu),anxu,"October 26, 2019,  6:45pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/anxu"">@anxu</a> tensor.register_hook(customHook) may work for your case, you need to write customHook to modify grad of the tensor</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Yanli,</p><NewLine><p>I am not sure whether tensor.register_hook will work, but the documentation mentioned that,</p><NewLine><blockquote><NewLine><p>Forward and backward hooks defined on  <code>module</code>  and its submodules won’t be invoked anymore, unless the hooks are initialized in the  <code>forward()</code>  method.</p><NewLine></blockquote><NewLine><p>Besides I need to first collect the whole gradient and then do some modification. Now I am turning to torch.distributed.all_reduce, but it will be easier if there is a way to do this via DistributedDataParallel.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, anxu <a class=""mention"" href=""/u/anxu"">@anxu</a> , the “DistributedDataParallel” automatically average the gradient when calling “loss.backward()”,<br/><NewLine>But I didn’t find the corresponding script in pytorch source code, Do you know where it is ?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, Yanli <a class=""mention"" href=""/u/yanli_zhao"">@Yanli_Zhao</a> , the “DistributedDataParallel” automatically average the gradient when calling “loss.backward()”,<br/><NewLine>But I didn’t find the corresponding script in pytorch source code, Do you know where it is ?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>DDP averages gradients by all-reducing them across participating processes (see <a href=""https://pytorch.org/docs/stable/_modules/torch/distributed/distributed_c10d.html#all_reduce"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/_modules/torch/distributed/distributed_c10d.html#all_reduce</a>). Some specific bits that include gradient averaging can be found in the <code>allReduce</code> calls here: <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/reducer.cpp"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/reducer.cpp</a></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/yanli_zhao"">@Yanli_Zhao</a>’s solution works great. You can  register the hook either before or after DDP’ing the model.  Though the docs say that hooks are removed, that’s either not actually the case or it doesn’t apply to hooks on the tensors themselves.</p><NewLine><p>Here’s some demo code:</p><NewLine><pre><code class=""lang-auto"">from torch.nn.parallel import DistributedDataParallel as DDP<NewLine>from torch import nn<NewLine>import torch<NewLine>import os<NewLine>import torch.distributed as dist<NewLine>import torch.multiprocessing as mp<NewLine><NewLine>def setup(rank, world_size):<NewLine>    """"""Setup code comes directly from the docs:  <NewLine><NewLine>    https://pytorch.org/tutorials/intermediate/ddp_tutorial.html<NewLine>    """"""<NewLine>    os.environ['MASTER_ADDR'] = '127.0.0.1'<NewLine>    os.environ['MASTER_PORT'] = '29500'<NewLine><NewLine>    dist.init_process_group(""nccl"", rank=rank, world_size=world_size)<NewLine>    torch.manual_seed(42)<NewLine><NewLine>def cleanup():<NewLine>    dist.destroy_process_group()<NewLine><NewLine>def pre_average(g):<NewLine>    print(f'Pre-DDP hook ({g.device}): {g[0, 0]}')<NewLine><NewLine>def post_average(g):<NewLine>    print(f'Post-DDP hook ({g.device}): {g[0, 0]}')<NewLine><NewLine>def worker(rank, world_size):<NewLine>    # Set up multiprocessing stuff<NewLine>    setup(rank, world_size)<NewLine><NewLine>    # Create a trivial model <NewLine>    model = nn.Linear(1, 1, bias=False).to(rank)<NewLine>    torch.nn.init.constant_(model.weight, 1.)<NewLine><NewLine>    # Create some trivial data. <NewLine>    # Gradients for x = (1, 2) should be (2, 8)<NewLine>    x = torch.tensor([rank+1]).float().to(rank)<NewLine><NewLine>    # Register a hook before and after DDP'ing the model<NewLine>    model.weight.register_hook(pre_average)<NewLine>    model = DDP(model, device_ids=[rank])<NewLine>    model.module.weight.register_hook(post_average)<NewLine><NewLine>    # Backprop!<NewLine>    l = model(x).pow(2).sum()<NewLine>    l.backward()<NewLine><NewLine>    # Check what's left in the gradient tensors<NewLine>    print(f'Final ({x.device}): {model.module.weight.grad[0, 0]}')<NewLine><NewLine>    cleanup()<NewLine><NewLine>if  __name__ == '__main__':<NewLine>    world_size = 2<NewLine>    mp.spawn(worker,<NewLine>                args=(world_size,),<NewLine>                nprocs=world_size,<NewLine>                join=True) <NewLine></code></pre><NewLine><p>Run from the terminal, this should print</p><NewLine><pre><code class=""lang-nohighlight"">Pre-DDP hook  (cuda:0): 2.0<NewLine>Post-DDP hook (cuda:0): 2.0<NewLine>Pre-DDP hook  (cuda:1): 8.0<NewLine>Post-DDP hook (cuda:1): 8.0<NewLine>Final value   (cuda:0): 5.0<NewLine>Final value   (cuda:1): 5.0<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Yanli_Zhao; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/anxu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/meilu_zhu; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/meilu_zhu; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/rvarm1; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/andyljones; <NewLine> ,"REPLY_DATE 1: October 29, 2019,  5:42am; <NewLine> REPLY_DATE 2: October 29, 2019,  1:21pm; <NewLine> REPLY_DATE 3: November 26, 2019,  2:52am; <NewLine> REPLY_DATE 4: November 26, 2019,  2:52am; <NewLine> REPLY_DATE 5: December 2, 2019,  1:06am; <NewLine> REPLY_DATE 6: December 24, 2019,  7:59pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
64865,Error when using DistributedDataParallel on single-GPU machine,2019-12-24T05:04:52.192Z,5,786,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m using a 4 GPUs machine with torch.distributed for training, and I want to do the inference with the trained model on another mahcine with only one GPU. But when I run the code like this:</p><NewLine><blockquote><NewLine><p>python -m torch.distributed.launch --nproc_per_node=1 visualizer_distributed.py.py</p><NewLine></blockquote><NewLine><p>I got an error</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""visualizer_distributed.py"", line 21, in &lt;module&gt;<NewLine>    model = torch.nn.parallel.DistributedDataParallel(model)<NewLine>  File ""H:\anaconda3\lib\site-packages\torch\nn\parallel\distributed.py"", line 259, in __init__<NewLine>    self.process_group = _get_default_group()<NewLine>NameError: name '_get_default_group' is not defined<NewLine>Traceback (most recent call last):<NewLine>  File ""H:\anaconda3\lib\runpy.py"", line 193, in _run_module_as_main<NewLine>    ""__main__"", mod_spec)<NewLine>  File ""H:\anaconda3\lib\runpy.py"", line 85, in _run_code<NewLine>    exec(code, run_globals)<NewLine>  File ""H:\anaconda3\lib\site-packages\torch\distributed\launch.py"", line 235, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""H:\anaconda3\lib\site-packages\torch\distributed\launch.py"", line 231, in main<NewLine>    cmd=process.args)<NewLine>subprocess.CalledProcessError: Command '['H:\\anaconda3\\python.exe', '-u', 'visualizer_distributed.py', '--local_rank=0']' returned non-zero exit status 1.<NewLine></code></pre><NewLine><p>Here is the snippet of code</p><NewLine><pre><code class=""lang-auto"">model = ...<NewLine>model = torch.nn.parallel.DistributedDataParallel(model)<NewLine>model.load_state_dict(torch.load(model_params))<NewLine>model.cuda()<NewLine></code></pre><NewLine><p>When I set the  device_ids like</p><NewLine><pre><code class=""lang-auto"">model = ...<NewLine>model = torch.nn.parallel.DistributedDataParallel(model, device_ids=0)<NewLine>torch.cuda.set_device(0)<NewLine>model.load_state_dict(torch.load(model_params))<NewLine>model.cuda()<NewLine></code></pre><NewLine><p>I got:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""visualizer_distributed.py"", line 21, in &lt;module&gt;<NewLine>    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=0)<NewLine>  File ""H:\anaconda3\lib\site-packages\torch\nn\parallel\distributed.py"", line 259, in __init__<NewLine>    self.process_group = _get_default_group()<NewLine>NameError: name '_get_default_group' is not defined<NewLine>Traceback (most recent call last):<NewLine>  File ""H:\anaconda3\lib\runpy.py"", line 193, in _run_module_as_main<NewLine>    ""__main__"", mod_spec)<NewLine>  File ""H:\anaconda3\lib\runpy.py"", line 85, in _run_code<NewLine>    exec(code, run_globals)<NewLine>  File ""H:\anaconda3\lib\site-packages\torch\distributed\launch.py"", line 235, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""H:\anaconda3\lib\site-packages\torch\distributed\launch.py"", line 231, in main<NewLine>    cmd=process.args)<NewLine>subprocess.CalledProcessError: Command '['H:\\anaconda3\\python.exe', '-u', 'visualizer_distributed.py', '--local_rank=0']' returned non-zero exit status 1.<NewLine></code></pre><NewLine><p>Does anyone know why the problem occurs and how to use DistributedDataParallel for inference on a single-GPU machine?</p><NewLine><p>Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/fan_percy,(Fan Percy),fan_percy,"December 24, 2019,  5:04am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Based on the error message it looks like you are using a Windows machine.<br/><NewLine>I’m not familiar with Windows, but I thought it doesn’t support distributed applications.<br/><NewLine>Were you also using Windows on the first machine?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes that’s right, I’m using Windows for inference and Ubuntu for training.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>In that case I think you cannot use a distributed setup. <img alt="":confused:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=9"" title="":confused:""/><br/><NewLine>However, since you have a single GPU on your Windows system, you won’t get any benefits anyway. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>So what should I do if I want to use the distributed model for inference in the single-GPU windows?</p><NewLine><p>I was using nn.DataParallel in these two machines, I must call nn.DataParallel(model) before loading the  model. I’m now just trying to do the same thing with DistributedDataParallel but got the problem. <img alt="":sweat_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/sweat_smile.png?v=9"" title="":sweat_smile:""/></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think the easiest way would be to store the <code>state_dict</code> without the <code>nn.DataParallel</code> <code>.module</code> attribute (I assume you are stuck there) as described <a href=""https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-torch-nn-dataparallel-models"">here</a>.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see…</p><NewLine><p>I will try to fix it, thank you for the help!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/fan_percy; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/fan_percy; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/fan_percy; <NewLine> ,"REPLY_DATE 1: December 24, 2019,  6:03am; <NewLine> REPLY_DATE 2: December 24, 2019,  6:37am; <NewLine> REPLY_DATE 3: December 24, 2019,  6:39am; <NewLine> REPLY_DATE 4: December 24, 2019,  6:46am; <NewLine> REPLY_DATE 5: December 24, 2019,  6:48am; <NewLine> REPLY_DATE 6: December 24, 2019,  6:49am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
64206,How to handle criterion with trainable params in DDP setup?,2019-12-16T11:18:00.269Z,1,167,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Related thread<br/><NewLine><aside class=""quote"" data-post=""31"" data-topic=""39681""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v4/letter/r/ce7236/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/how-to-learn-the-weights-between-two-losses/39681/31"">How to learn the weights between two losses?</a><NewLine></div><NewLine><blockquote><NewLine>    Hi, <NewLine>You mentioned the usage as: <NewLine>usage<NewLine>is_regression = torch.Tensor([True, True, False]) # True: Regression/MeanSquaredErrorLoss, False: Classification/CrossEntropyLoss<NewLine><NewLine>multitaskloss_instance = MultiTaskLoss(is_regression)<NewLine><NewLine>So in case of classification  problem I should put <NewLine>is_regression = False <NewLine>can clarify it a bit ?<NewLine>  </blockquote><NewLine></aside><NewLine><br/><NewLine>I have this loss:</p><NewLine><pre><code>class Tacotron2Loss(nn.Module):<NewLine>	def __init__(self, hparams):<NewLine>		super(Tacotron2Loss, self).__init__()<NewLine>		self.gate_loss_fn = nn.BCEWithLogitsLoss()<NewLine>		self.emotion_loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-1)<NewLine><NewLine>		num_losses = 3<NewLine>		self.use_mmi = hparams.use_mmi<NewLine>		if self.use_mmi:<NewLine>			self.ctc_loss_fn = torch.nn.CTCLoss(<NewLine>				blank=len(ctc_symbols), reduction='none')<NewLine>			num_losses += 1<NewLine><NewLine>		# loss weights<NewLine>		self.eta = nn.Parameter(torch.ones(num_losses, dtype=torch.float32))<NewLine><NewLine>	@staticmethod<NewLine>	def masked_l2_loss(out, target, lengths):<NewLine>		num_not_padded = lengths.sum() * out.size(1)<NewLine>		loss = F.mse_loss(out, target, reduction=""sum"")<NewLine>		loss = loss / num_not_padded<NewLine>		return loss<NewLine><NewLine>	def forward(self, y_pred, y, output_lengths):<NewLine>		mel_target, gate_target, ctc_text, ctc_text_lengths, emotion_label = y<NewLine>		# mel_target.requires_grad = False<NewLine>		# gate_target.requires_grad = False<NewLine>		gate_target = gate_target.view(-1, 1)<NewLine><NewLine>		_, mel_out, mel_out_postnet, gate_out, _, log_probs, emotion_weights = y_pred<NewLine><NewLine>		gate_out = gate_out.view(-1, 1)<NewLine><NewLine>		losses = []<NewLine><NewLine>		mel_loss = self.masked_l2_loss(mel_out, mel_target, output_lengths) + \<NewLine>			self.masked_l2_loss(mel_out_postnet, mel_target, output_lengths)<NewLine>		losses.append(mel_loss)<NewLine><NewLine>		gate_loss = self.gate_loss_fn(gate_out, gate_target)<NewLine>		losses.append(gate_loss)<NewLine><NewLine>		emotiom_loss = self.emotion_loss_fn(emotion_weights, emotion_label)<NewLine>		losses.append(emotiom_loss)<NewLine><NewLine>		if self.use_mmi:<NewLine>			ctc_loss = (self.ctc_loss_fn(log_probs, ctc_text, output_lengths, ctc_text_lengths) /<NewLine>						output_lengths.float()).mean()<NewLine>			losses.append(ctc_loss)<NewLine><NewLine>		total_loss = torch.stack(losses) * torch.exp(-self.eta) + self.eta<NewLine>		return total_loss.sum(), losses, self.eta<NewLine></code></pre><NewLine><p>Then i pu it in optimizer like this:</p><NewLine><pre><code>optimizer = torch.optim.AdamW(list(<NewLine>		model.parameters()) + list(criterion.parameters()), lr=hparams.learning_rate)<NewLine></code></pre><NewLine><p>So, what is right way to use it in DDP setup?<br/><NewLine>Should i put criterion in main model’s forwars function as submodule or use DDP wrapped on criterion, or something else?</p><NewLine></div>",https://discuss.pytorch.org/u/hadaev8,(Had),hadaev8,"December 16, 2019, 11:18am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>IMHO, adding trainable parameters to the loss function makes it part of the network to be trained. We need to think out of the box a bit here. So what I reckon that you could do is to wrap the criterion into your network. This is going to require a bit of change to how people usually write foward() method. Here is an example.</p><NewLine><pre><code class=""lang-auto"">def forward(self, x, y=None):<NewLine>    # Regular forward pass<NewLine>    output = self.model(x)<NewLine>    # Insert your criterion here<NewLine>    if self.training:<NewLine>        assert y is not None, ""Target should be passed during training.""<NewLine>        loss = self.criterion(output, y)<NewLine>        return loss<NewLine>    return output<NewLine><NewLine></code></pre><NewLine><p>Basically, the code snippet above includes the computation of loss as part of model’s forward pass. And because your criterion is part of your network, you don’t have to explicitly add its parameters to the optimiser anymore. And <code>torch.nn.parallel.DistributedDataParallel</code> will make sure even the parameters of the criterion are synced across GPUs during forward pass.</p><NewLine><p>Hope this helps.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is here any difference between adding criterion to model or having it separate wrapped to ddp?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/DzReal; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hadaev8; <NewLine> ,"REPLY_DATE 1: December 18, 2019,  4:45am; <NewLine> REPLY_DATE 2: December 23, 2019,  2:42pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
64758,P2P Cuda Aware MPI problem,2019-12-22T11:08:16.940Z,0,100,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I’m trying to use Isends with cuda aware openmpi.</p><NewLine><p>I found that I need to explicitly call torch.cuda.synchronize(device) before every Isend (otherwise training error collapses). I get that problem even when I stash the sent tensor (so it will have a reference and therefore won’t be freed and overwritten).</p><NewLine><p>I have tried it with several different settings:</p><NewLine><ol><NewLine><li>with P2P enabled GPU (GTX1080)</li><NewLine><li>and without P2P enabled GPUs (RTX2080ti). (in the latter case the sends must go through the host).</li><NewLine></ol><NewLine><p>I wonder, what could be happening there?<br/><NewLine>(I am using a single thread with async operations)</p><NewLine></div>",https://discuss.pytorch.org/u/seliad,(Saar Eliad),seliad,"December 22, 2019, 11:09am",,,,,
64659,Module Buffers not updating in DataParrallel,2019-12-20T18:26:58.957Z,0,114,"<div class=""post"" itemprop=""articleBody""><NewLine><p>If I use the DataParrallel utility as such,</p><NewLine><pre><code class=""lang-auto""> model = torch.nn.DataParallel(model)<NewLine></code></pre><NewLine><p>my custom batchNorm module does not update it’s buffers, running_avg_mean and running_avg_std. It does update if I run the model without DataParrallel on a single GPU. How can I get the buffers to update in DataParrallel?</p><NewLine><pre><code class=""lang-auto""># Batch Renormalization for convolutional neural nets (2D) implementation based<NewLine># on https://arxiv.org/abs/1702.03275<NewLine><NewLine>from torch.nn import Module<NewLine>import torch<NewLine><NewLine>class BatchRenormalization2D(Module):<NewLine>    '''Batch renorm from https://arxiv.org/pdf/1702.03275.pdf'''<NewLine><NewLine>    def __init__(self, num_features, eps=1e-05, momentum=0.01, r_d_max_inc_step=0.0001):<NewLine>        super(BatchRenormalization2D, self).__init__()<NewLine><NewLine>        self.eps = eps<NewLine>        self.momentum = momentum<NewLine><NewLine>        self.gamma = torch.nn.Parameter(torch.ones((1, num_features, 1, 1)), requires_grad=True)<NewLine>        self.beta = torch.nn.Parameter(torch.zeros((1, num_features, 1, 1)), requires_grad=True)<NewLine><NewLine>        self.register_buffer('running_avg_mean', torch.zeros((1, num_features, 1, 1)))<NewLine>        self.register_buffer('running_avg_std', torch.ones((1, num_features, 1, 1)))<NewLine><NewLine>        self.max_r_max = 3.0<NewLine>        self.max_d_max = 5.0<NewLine><NewLine>        self.r_max_inc_step = r_d_max_inc_step<NewLine>        self.d_max_inc_step = r_d_max_inc_step<NewLine><NewLine>        self.r_max = 1.0<NewLine>        self.d_max = 0.0<NewLine><NewLine>    def forward(self, x):<NewLine><NewLine>        batch_ch_mean = torch.mean(x, dim=(0, 2, 3), keepdim=True)<NewLine>        batch_ch_std = torch.clamp(torch.std(x, dim=(0, 2, 3), keepdim=True), self.eps, 1e10)<NewLine><NewLine><NewLine>        if self.training:<NewLine><NewLine>            r = torch.clamp(batch_ch_std / self.running_avg_std, 1.0 / self.r_max, self.r_max).data<NewLine>            d = torch.clamp((batch_ch_mean - self.running_avg_mean) / self.running_avg_std, -self.d_max, self.d_max).data<NewLine><NewLine>            x = ((x - batch_ch_mean) * r )/ batch_ch_std + d<NewLine>            x = self.gamma * x + self.beta<NewLine><NewLine>            if self.r_max &lt; self.max_r_max:<NewLine>                self.r_max += self.r_max_inc_step * x.shape[0]<NewLine><NewLine>            if self.d_max &lt; self.max_d_max:<NewLine>                self.d_max += self.d_max_inc_step * x.shape[0]<NewLine><NewLine>            self.running_avg_mean = self.running_avg_mean + self.momentum * (batch_ch_mean.detach() - self.running_avg_mean)<NewLine>            self.running_avg_std = self.running_avg_std + self.momentum * (batch_ch_std.detach() - self.running_avg_std)<NewLine><NewLine>        else:<NewLine><NewLine>            x = (x - self.running_avg_mean) / self.running_avg_std<NewLine>            x = self.gamma * x + self.beta<NewLine><NewLine>        return x<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Kendall_Reid,(Kendall Reid),Kendall_Reid,"December 20, 2019,  6:26pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>update, was able to make it work using the .lerp() function. Why does what I did not work?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""64659""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/kendall_reid/40/18984_2.png"" width=""20""/> Kendall_Reid:</div><NewLine><blockquote><NewLine><p>self.running_avg_mean = self.running_avg_mean + self.momentum * (batch_ch_mean.detach() - self.running_avg_mean)</p><NewLine></blockquote><NewLine></aside><NewLine><p>Because in DP, the python module object is replicated to run on each GPU in a different thread. However, this setattr assigns the updated the buffer to the replica, which is lost right afterwards. Instead, inplace updates to the buffer works because buffers in the replica on the first GPU share memory with the original one.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Kendall_Reid; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/SimonW; <NewLine> ,"REPLY_DATE 1: December 20, 2019,  7:16pm; <NewLine> REPLY_DATE 2: December 20, 2019, 11:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
64234,Checkpointing for a dataparallel model,2019-12-16T16:16:36.111Z,1,227,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Does it make a difference if you checkpoint your model for retraining after model.eval() or model.train() loop?</p><NewLine></div>",https://discuss.pytorch.org/u/Shubhankar,,Shubhankar,"December 16, 2019,  4:16pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It shouldn’t make any difference, as long as you don’t update the parameters in your validation loop.</p><NewLine><p>This question seems to be unrelated to the topic, so do you have any issues using <code>DataParallel</code>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to save a Dataparallel model but getting</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-17-e0020f404c69&gt; in &lt;module&gt;()<NewLine>     69             optimizer.zero_grad() # Zero gradients<NewLine>     70             loss.backward() # Calculate gradients<NewLine>---&gt; 71             optimizer.step() # Update weights<NewLine>     72             m.track_loss(loss)<NewLine>     73             m.track_num_correct(preds, labels)<NewLine><NewLine>~/pytorch-1.0-p3/anaconda3/lib/python3.6/site-packages/torch/optim/adamw.py in step(self, closure)<NewLine>     98 <NewLine>     99                 # Decay the first and second moment running average coefficient<NewLine>--&gt; 100                 exp_avg.mul_(beta1).add_(1 - beta1, grad)<NewLine>    101                 exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)<NewLine>    102                 if amsgrad:<NewLine><NewLine>RuntimeError: expected device cpu but got device cuda:0<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The stack trace points to <code>optimizer.step()</code>, which is unrelated to saving the <code>state_dict</code>.</p><NewLine><p>How did you pass the parameters to the optimizer?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>So this worked for me which is little weird of a workflow:</p><NewLine><pre><code class=""lang-auto""># While saving checkpoint i.e. comment out while loading checkpoint<NewLine>    if torch.cuda.device_count() &gt; 1:<NewLine>        print(""Let's use"", torch.cuda.device_count(), ""GPUs!"")<NewLine>        network = nn.DataParallel(network)<NewLine>    <NewLine>    <NewLine>    network.to(device)<NewLine>    optimizer = optim.AdamW(network.parameters(), lr=run.lr, weight_decay=run.weight_decay)<NewLine>    <NewLine>#     try:<NewLine>    with open('check-point.pth', 'rb') as f:<NewLine>        print('file opened')<NewLine>        checkpoint = torch.load(f)<NewLine>        print('file loaded')<NewLine>        network.load_state_dict(checkpoint['model_state_dict'])<NewLine>        print('network loaded')<NewLine>        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])<NewLine>        print('optimizer loaded')<NewLine>        epoc = checkpoint['epoch']<NewLine>        print(f'blah epoc: {epoc}')    <NewLine>    # While loading checkpoint<NewLine>        if torch.cuda.device_count() &gt; 1:<NewLine>            print(""Let's use"", torch.cuda.device_count(), ""GPUs!"")<NewLine>            network = nn.DataParallel(network)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Shubhankar; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Shubhankar; <NewLine> ,"REPLY_DATE 1: December 16, 2019,  7:33pm; <NewLine> REPLY_DATE 2: December 17, 2019,  1:58am; <NewLine> REPLY_DATE 3: December 17, 2019,  2:19am; <NewLine> REPLY_DATE 4: December 18, 2019,  3:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
63799,Memory cost of nn.SyncBatchNorm,2019-12-11T23:46:27.350Z,0,114,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I use nn.parallel.DistributedDataParallel for multi-gpu training in a single node, I use the nn.SyncBatchNorm to work as batch normalization across GPUs. However, I found the gpu memory cost increased a lot, at least 1gb for one gpu. When I use the SyncBatchNorm provided by apex (but I cannot successfully compile apex in this server), the gpu memory cost is normal. Can anyone help with it?</p><NewLine></div>",https://discuss.pytorch.org/u/ginobilinie,(No Name),ginobilinie,"December 11, 2019, 11:46pm",,,,,
63768,unhandled cuda error,2019-12-11T17:40:27.361Z,0,261,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When trying out just 2 instances with 1 gpu each attached to test distributed training this error occurred:</p><NewLine><blockquote><NewLine><p>(base) ubuntu@ip-172-31-11-131:~/detectron2$ NCCL_SOCKET_IFNAME=ens3 NCCL_IB_DISABLE=1 python tools/train_net.py --num-gpus 1 --num-machines 2 --machine-rank 1 --dist-url tcp://32.273.122.180:3000 --config-file configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml<br/><NewLine>Command Line Args: Namespace(config_file=‘configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml’, dist_url=‘tcp://34.253.142.180:3000’, eval_only=False, machine_rank=1, num_gpus=1, num_machines=2, opts=[], resume=False)<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “tools/train_net.py”, line 161, in <br/><NewLine>args=(args,),<br/><NewLine>File “/home/ubuntu/detectron2/detectron2/engine/launch.py”, line 49, in launch<br/><NewLine>daemon=False,<br/><NewLine>File “/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py”, line 171, in spawn<br/><NewLine>while not spawn_context.join():<br/><NewLine>File “/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py”, line 118, in join<br/><NewLine>raise Exception(msg)<br/><NewLine>Exception:</p><NewLine><p>– Process 0 terminated with the following error:<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py”, line 19, in _wrap<br/><NewLine>fn(i, *args)<br/><NewLine>File “/home/ubuntu/detectron2/detectron2/engine/launch.py”, line 70, in _distributed_worker<br/><NewLine>comm.synchronize()<br/><NewLine>File “/home/ubuntu/detectron2/detectron2/utils/comm.py”, line 79, in synchronize<br/><NewLine>dist.barrier()<br/><NewLine>File “/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py”, line 1424, in barrier<br/><NewLine>work = _default_pg.barrier()<br/><NewLine>RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1573049306803/work/torch/lib/c10d/ProcessGroupNCCL.cpp:400, unhandled cuda error</p><NewLine></blockquote><NewLine><p>Exporting NCCL_SOCKET_IFNAME and NCCL_IB_DISABLE didn’t help, also other fixes as discussed in github issues and everything I found in the net on this topic.<br/><NewLine>Maybe I forgot an argument. In AWS, “ens3” seems to be the ethernet connection, at least <code>ifconfig</code> does not reveal eth0 as usual.</p><NewLine><p>What to do?</p><NewLine></div>",https://discuss.pytorch.org/u/1seck,(Thorsten Seckert),1seck,"December 11, 2019,  5:40pm",,,,,
62939,Gloo backend default device,2019-12-04T01:51:14.670Z,1,356,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi PyTorch experts,</p><NewLine><p>I am trying to use torch.distributed package for my distributed training. The backend I am using is gloo.</p><NewLine><p>Based on this doc: <a href=""https://pytorch.org/docs/stable/distributed.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/distributed.html</a>, gloo supports all_reduce on both CPU and GPU, but it seems there is no specific way to chose one over the other.</p><NewLine><p>I am wondering, during training, does gloo perform all_reduce automatically based on the tensor’s device type? Like if the tensors are on GPU, then perform all_reduce on GPU; if the tensors are on CPU, perform it on CPU?</p><NewLine><p>Also, when all_reduce is performed on GPU, does gloo fallback using nccl?</p><NewLine><p>Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/xzhu1900,,xzhu1900,"December 4, 2019,  1:51am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I am wondering, during training, does gloo perform all_reduce automatically based on the tensor’s device type?</p><NewLine></blockquote><NewLine><p>Yes, see <a href=""https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/ProcessGroupGloo.cpp#L720"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/ProcessGroupGloo.cpp#L720</a>. Essentially, we check the input’s device type, and run the appropriate operation based on that.</p><NewLine><blockquote><NewLine><p>Also, when all_reduce is performed on GPU, does gloo fallback using nccl?</p><NewLine></blockquote><NewLine><p>This doesn’t happen, the GLOO backend can be built with CUDA and supports GPU operations (<a href=""https://github.com/facebookincubator/gloo/blob/master/docs/cuda.md"" rel=""nofollow noopener"">https://github.com/facebookincubator/gloo/blob/master/docs/cuda.md</a>)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/rvarm1"">@rvarm1</a>. This helps a lot!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/rvarm1; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xzhu1900; <NewLine> ,"REPLY_DATE 1: December 4, 2019,  6:40am; <NewLine> REPLY_DATE 2: December 9, 2019, 10:43pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
63524,Runtime error while multiprocessing,2019-12-09T19:23:23.197Z,0,135,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m having trouble with multiple processes working on the same GPU. I wrote minimal error-reproducing example.<br/><NewLine>I ran the example code successfully on my local machine, using CUDA 10.2 and pytorch 1.2.0.<br/><NewLine>While this works just fine, it fails to run on a cluster with CUDA 10.1 and pytorch 1.2.0.</p><NewLine><p>Does anybody know why or how to overcome this? Thanks a ton.</p><NewLine><p><strong>CODE EXAMPLE</strong></p><NewLine><pre><code class=""lang-auto"">import torch.multiprocessing as _mp<NewLine>import torch<NewLine>import os<NewLine>import time<NewLine>import numpy as np<NewLine><NewLine>mp = _mp.get_context('spawn')<NewLine><NewLine>class Process(mp.Process):<NewLine>    def __init__(self, id):<NewLine>        super().__init__()<NewLine>        print(""Init Process"")<NewLine>        self.id = id<NewLine>        return<NewLine><NewLine>    def run(self):<NewLine>        os.environ['CUDA_VISIBLE_DEVICES'] = '0'<NewLine>        for i in range(3):<NewLine>            with torch.cuda.device(0):<NewLine>                x = torch.Tensor(10).to(0)<NewLine>                x.to('cpu')<NewLine>                del x<NewLine>            time.sleep(np.random.random())<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    num_processes = 2<NewLine>    os.environ['CUDA_VISIBLE_DEVICES'] = '0'<NewLine>    processes = [Process(i) for i in range(num_processes)]<NewLine>    [p.start() for p in processes]<NewLine>    [p.join() for p in processes]<NewLine></code></pre><NewLine><p><strong>ERROR</strong></p><NewLine><pre><code class=""lang-auto"">Process Process-2:<NewLine>Traceback (most recent call last):<NewLine>  File ""/cluster/home/marksm/software/anaconda/envs/test/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap<NewLine>    self.run()<NewLine>  File ""/cluster/home/marksm/mp_demonstration.py"", line 20, in run<NewLine>    x = torch.Tensor(10).to(0)<NewLine>RuntimeError: CUDA error: all CUDA-capable devices are busy or unavailable<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/damaggu,(Damaggu),damaggu,"December 9, 2019,  7:23pm",1 Like,,,,
63161,Memory issue of using nn.DataParallel,2019-12-05T19:32:41.140Z,1,155,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi guys,</p><NewLine><p>I’m currently using nn.DataParallel for mutli-gpu (8-gpu) training in a single node. However, if I put the data and model to devices[0], I found the memory on GPU 0 will be huge and make the program exits (cuda out of memory) at the begining of training. Can anyone help?</p><NewLine><p>BTW, I find  if I use DistributedDataParallel, the memory is fine.</p><NewLine><p>Environment:<br/><NewLine>pytorch 1.0.1<br/><NewLine>cuda9.0</p><NewLine></div>",https://discuss.pytorch.org/u/ginobilinie,(No Name),ginobilinie,"December 5, 2019,  7:32pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This effect is described by <a class=""mention"" href=""/u/thomas_wolf"">@Thomas_Wolf</a> in <a href=""https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255"">this blog post</a>.</p><NewLine><p>We generally recommend using DDP. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks. Which syncBatchNormalization do you recommend when using DDP? I’m not sure if the default nn.BatchNorm2d considers multi-gpu ops?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ginobilinie; <NewLine> ,"REPLY_DATE 1: December 7, 2019, 12:21am; <NewLine> REPLY_DATE 2: December 7, 2019, 12:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
63030,PyTorch Distributed Gloo Backend,2019-12-04T15:28:59.373Z,2,178,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it a good practice to create a new group on every training iteration?</p><NewLine><p>Using dist.new_group requires every process to pass through the function even if they are not a part of the distributed training process. Sometimes it hangs up on this function for a reason that I am not aware of. I was wondering has anybody come up with a better solution for this?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/Tabrizian,(Iman Tabrizian),Tabrizian,"December 4, 2019,  3:29pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>iiuc, it is not good practice to create a new group every training iteration, it is non trivial to initialize it, there are communication costs.</p><NewLine><p>why do you need to create new group every iteration?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the answer. Because I need to run allreduce on a subset of nodes in each iteration. Do you have any recommendation about how to do it differently?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>do you want to create these sub groups before training loop, and each iteration just uses corresponding created sub groups? instead of creating the same sub groups repeatedly inside the loop?</p><NewLine><p>also, add time out if it hangs?</p><NewLine><p>lastly, maybe need to figure out why it hangs, starting with debugging using a small number of training iterations?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the suggestion! I originally thought about this approach. But because I would like to create this for large number of machines (around 100) the number of groups can become quite large (combination of 10 out of 100 =17310309456440!!).</p><NewLine><p>I debugged it and it seems like the limit for the number of cgroups exceeds the current user limit. I guess this may be related to the dist.new_group and repeated creation of groups.</p><NewLine><p>The error printed in the kernel messages is printed below:</p><NewLine><pre><code class=""lang-auto"">cgroup: fork rejected by pids controller in /user.slice/user-1000.slice/session-3.scope<NewLine></code></pre><NewLine><p>I wish there was a method to delete the groups in order to avoid this problem.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>you can call dist.destroy_process_group()</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Yanli_Zhao; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Tabrizian; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Yanli_Zhao; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Tabrizian; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Yanli_Zhao; <NewLine> ,"REPLY_DATE 1: December 4, 2019, 10:25pm; <NewLine> REPLY_DATE 2: December 4, 2019, 11:40pm; <NewLine> REPLY_DATE 3: December 5, 2019, 12:59am; <NewLine> REPLY_DATE 4: December 5, 2019,  1:31am; <NewLine> REPLY_DATE 5: December 5, 2019,  7:28am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
62367,How to choose learning rate when using Mixed Precision Training,2019-11-28T04:09:09.027Z,1,280,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m training ImageNet on ResNet-50 architecture using Fastai v1 1.0.60dev (Pytorch 1.3.0). There are substantial speed gains using Mixed Precision Training since I can effectively use 4 times the batch size (= 512) thanks to reduction in VRAM consumption and using smaller size of 224. The problem is I am unable to select a good learning rate. I am using fastai’s lr_finder with SGD but the suggested lr causes huge overfitting. Diving the suggested lr by 100 just overfits a bit while lr divided by 512 seems to be okish but slow.</p><NewLine><p>While these guesses work, I’m not sure how to choose a good learning rate in general. I thought about using APEX but the dynamic loss scaling seems to be integrated in the learn.to_fp16(). Training is done using learn.fit_one_cycle() ie 1-cycle policy. I think everything else is working fine.<br/><NewLine>Below is the image for lr divided by 100<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/e40d7b7085a9dbd78575a9772808951a9cd01458"" href=""https://discuss.pytorch.org/uploads/default/original/3X/e/4/e40d7b7085a9dbd78575a9772808951a9cd01458.png"" title=""Screenshot%20at%202019-11-28%2008-55-56""><img alt=""Screenshot%20at%202019-11-28%2008-55-56"" data-base62-sha1=""wxrKq32xekJjCBC5RiP9CsDBCkw"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/4/e40d7b7085a9dbd78575a9772808951a9cd01458_2_10x10.png"" height=""382"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/4/e40d7b7085a9dbd78575a9772808951a9cd01458_2_690x382.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/4/e40d7b7085a9dbd78575a9772808951a9cd01458_2_690x382.png, https://discuss.pytorch.org/uploads/default/original/3X/e/4/e40d7b7085a9dbd78575a9772808951a9cd01458.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/e/4/e40d7b7085a9dbd78575a9772808951a9cd01458.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screenshot%20at%202019-11-28%2008-55-56</span><span class=""informations"">955×529 25.6 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>And this is for lr divided by 512<br/><NewLine><img alt=""Screenshot%20at%202019-11-28%2008-57-26"" data-base62-sha1=""zgjDghVXMws9mg1iTyfRFQd4Lo3"" height=""366"" src=""https://discuss.pytorch.org/uploads/default/original/3X/f/7/f72443eb5fcd41ca29d0c7d39a71b00d24648c7f.png"" width=""669""/></p><NewLine></div>",https://discuss.pytorch.org/u/numbpy,(Abhinav Raj),numbpy,"November 28, 2019,  4:11am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you potentially try a learning rate in between <code>lr/100</code> and <code>lr/512</code>, or potentially stick with <code>lr/100</code> and decay the learning rate over time? Also, this might be a better question for the fastai forums, since it uses fastai’s library: <a href=""https://forums.fast.ai/c/fastai-users"" rel=""nofollow noopener"">https://forums.fast.ai/c/fastai-users</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I posted the question there but haven’t gotten a reply. Will try varying the learning rates as suggested.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/rvarm1; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/numbpy; <NewLine> ,"REPLY_DATE 1: December 2, 2019,  1:32am; <NewLine> REPLY_DATE 2: December 2, 2019,  8:25am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
62656,Single machine multi-GPUs: arguments are located on different GPUs,2019-12-01T15:14:15.998Z,1,223,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, all.</p><NewLine><p>I encountered a very confusing problem.</p><NewLine><p>It’s ok when I run model in single GPU. But it’s not work when I use multi-GPUs (single machine multi-GPUs)  by  model = torch.nn.DataParallel(model, device_ids=device_ids).</p><NewLine><p>The puzzling thing is that the code is executable at the beginning. But after executing 5 batches (batch-szie = 100), an error occurs.</p><NewLine><pre><code class=""lang-auto"">2019-12-01 16:17:15,151:Traceback (most recent call last):<NewLine>2019-12-01 16:17:15,151:  File ""exp\GSTEG.py"", line 25, in &lt;module&gt;<NewLine>2019-12-01 16:17:15,151:    main()<NewLine>2019-12-01 16:17:15,151:  File "".\main.py"", line 51, in main<NewLine>2019-12-01 16:17:15,151:    s_top1,s_top5,o_top1,o_top5,v_top1,v_top5, sov_top1 = trainer.train(train_loader, base_model, logits_model, criterion, base_optimizer, logits_optimizer, epoch, opt)<NewLine>2019-12-01 16:17:15,151:  File "".\train.py"", line 172, in train<NewLine>2019-12-01 16:17:15,151:    # s_output, o_output, v_output, loss = criterion(*((s, o, v, so, ov, vs, ss, oo, vv, so_t, ov_t, vs_t, os_t, vo_t, sv_t) + (s_target_var, o_target_var, v_target_var, meta)))<NewLine>2019-12-01 16:17:15,151:  File ""C:\Users\gorvinchen\Miniconda3\envs\rcenet\lib\site-packages\torch\nn\modules\module.py"", line 489, in __call__<NewLine>2019-12-01 16:17:15,151:    result = self.forward(*input, **kwargs)<NewLine>2019-12-01 16:17:15,151:  File ""C:\Users\gorvinchen\Miniconda3\envs\rcenet\lib\site-packages\torch\nn\parallel\data_parallel.py"", line 143, in forward<NewLine>2019-12-01 16:17:15,151:    outputs = self.parallel_apply(replicas, inputs, kwargs)<NewLine>2019-12-01 16:17:15,151:  File ""C:\Users\gorvinchen\Miniconda3\envs\rcenet\lib\site-packages\torch\nn\parallel\data_parallel.py"", line 153, in parallel_apply<NewLine>2019-12-01 16:17:15,151:    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])<NewLine>2019-12-01 16:17:15,151:  File ""C:\Users\gorvinchen\Miniconda3\envs\rcenet\lib\site-packages\torch\nn\parallel\parallel_apply.py"", line 83, in parallel_apply<NewLine>2019-12-01 16:17:15,151:    raise output<NewLine>2019-12-01 16:17:15,151:  File ""C:\Users\gorvinchen\Miniconda3\envs\rcenet\lib\site-packages\torch\nn\parallel\parallel_apply.py"", line 59, in _worker<NewLine>2019-12-01 16:17:15,151:    output = module(*input, **kwargs)<NewLine>2019-12-01 16:17:15,151:  File ""C:\Users\gorvinchen\Miniconda3\envs\rcenet\lib\site-packages\torch\nn\modules\module.py"", line 489, in __call__<NewLine>2019-12-01 16:17:15,151:    result = self.forward(*input, **kwargs)<NewLine>2019-12-01 16:17:15,151:  File "".\models\layers\AsyncTFCriterion.py"", line 244, in forward<NewLine>2019-12-01 16:17:15,151:    s_msg, o_msg, v_msg  = self.get_msg(idtime, 'past')<NewLine>2019-12-01 16:17:15,151:  File "".\models\layers\AsyncTFCriterion.py"", line 147, in get_msg<NewLine>2019-12-01 16:17:15,151:    return self.mget(idtime, self.ns, self.no, self.nv, s_storage, o_storage, v_storage, cond, kernel)<NewLine>2019-12-01 16:17:15,151:  File "".\models\layers\AsyncTFCriterion.py"", line 127, in mget<NewLine>2019-12-01 16:17:15,151:    s_out = [meta(ids, time, s_size, s_storage) for ids, time in idtime]<NewLine>2019-12-01 16:17:15,151:  File "".\models\layers\AsyncTFCriterion.py"", line 127, in &lt;listcomp&gt;<NewLine>2019-12-01 16:17:15,151:    s_out = [meta(ids, time, s_size, s_storage) for ids, time in idtime]<NewLine>2019-12-01 16:17:15,151:  File "".\models\layers\AsyncTFCriterion.py"", line 124, in meta<NewLine>2019-12-01 16:17:15,151:    if cond(t, t0)), 1. / self.decay)<NewLine>2019-12-01 16:17:15,151:  File "".\models\layers\AsyncTFCriterion.py"", line 43, in avg<NewLine>2019-12-01 16:17:15,167:    item, w = next(iterator)<NewLine>2019-12-01 16:17:15,167:  File "".\models\layers\AsyncTFCriterion.py"", line 124, in &lt;genexpr&gt;<NewLine>2019-12-01 16:17:15,167:    if cond(t, t0)), 1. / self.decay)<NewLine>2019-12-01 16:17:15,167:  File "".\models\layers\AsyncTFCriterion.py"", line 145, in &lt;lambda&gt;<NewLine>2019-12-01 16:17:15,167:    cond = lambda t, t0: t &lt; t0 if time == 'past' else t &gt; t0<NewLine>2019-12-01 16:17:15,167:RuntimeError: arguments are located on different GPUs at c:\a\w\1\s\windows\pytorch\aten\src\thc\generic/THCTensorMathCompareT.cu:7<NewLine></code></pre><NewLine><p>My code is here</p><NewLine><pre><code class=""lang-auto"">def avg(iterator, weight=1.):<NewLine>    # compounding weight<NewLine>    item, w = next(iterator)<NewLine>    total = item.clone() * w<NewLine>    n = 1.<NewLine>    for i, (item, w) in enumerate(iterator):<NewLine>        w1 = 1. * weight**(i + 1)<NewLine>        total += item * w1 * w<NewLine>        n += w1<NewLine>    return total / n<NewLine><NewLine>class MessagePassing(object):<NewLine>    # Class for keeping track of messages across frames<NewLine>    def __init__(self, maxsize, w_temporal, w_spatio, decay, sigma, ns, no, nv):<NewLine>        super(MessagePassing, self).__init__()<NewLine>        self.maxsize = maxsize<NewLine>        self.w_temporal = w_temporal<NewLine>        self.w_spatio = w_spatio<NewLine>        self.decay = decay<NewLine>        self.sigma = sigma<NewLine>        self.s_storage = {}<NewLine>        self.s_storage_gt = {}<NewLine>        self.o_storage = {}<NewLine>        self.o_storage_gt = {}<NewLine>        self.v_storage = {}<NewLine>        self.v_storage_gt = {}<NewLine>        self.training = self.training if hasattr(self, 'training') else True<NewLine>        self.ns = ns<NewLine>        self.no = no<NewLine>        self.nv = nv<NewLine><NewLine>    def mget(self, idtime, s_size, o_size, v_size, s_storage, o_storage, v_storage, cond=lambda t, t0: True, kernel=lambda t, t0: 1):<NewLine>        # get message using condition on the timestamps<NewLine>        def meta(ids, t0, size, storage):<NewLine>            try:<NewLine>                return avg(((y, kernel(t, t0)) for t, y in storage[ids]<NewLine>                            if cond(t, t0)), 1. / self.decay)<NewLine>            except (StopIteration, KeyError):<NewLine>                return torch.zeros(size)<NewLine>        s_out = [meta(ids, time, s_size, s_storage) for ids, time in idtime]<NewLine>        o_out = [meta(ids, time, o_size, o_storage) for ids, time in idtime]<NewLine>        v_out = [meta(ids, time, v_size, v_storage) for ids, time in idtime]<NewLine>        return Variable(torch.stack(s_out, 0).cuda()), Variable(torch.stack(o_out, 0).cuda()), Variable(torch.stack(v_out, 0).cuda())<NewLine><NewLine>    def get_msg(self, idtime, time='past', s_storage=None, o_storage=None, v_storage=None):<NewLine>        s_storage = self.s_storage if s_storage is None else s_storage<NewLine>        o_storage = self.o_storage if o_storage is None else o_storage<NewLine>        v_storage = self.v_storage if v_storage is None else v_storage<NewLine>        cond = lambda t, t0: t &lt; t0 if time == 'past' else t &gt; t0<NewLine>        kernel = lambda t, t0: math.exp(-float(t - t0)**2 / (2 * self.sigma**2))<NewLine>        return self.mget(idtime, self.ns, self.no, self.nv, s_storage, o_storage, v_storage, cond, kernel) <NewLine><NewLine>    def get_gt_msg(self, idtime, time='past'):<NewLine>        return self.get_msg(idtime, time, self.s_storage_gt, self.o_storage_gt, self.v_storage_gt)<NewLine><NewLine>    def mset(self, s_msg, o_msg, v_msg, idtime, s_storage, o_storage, v_storage):<NewLine>        # keep a queue of size maxsize for each id<NewLine>        # messages are stored in normal space<NewLine>        # queue for each id is stored in the order in which the messages were stored<NewLine>        for s_m, o_m, v_m, (ids, time) in sorted(zip(s_msg, o_msg, v_msg, idtime), key=lambda x: random()):<NewLine>            if ids not in s_storage:<NewLine>                s_storage[ids] = []<NewLine>            if ids not in o_storage:<NewLine>                o_storage[ids] = []<NewLine>            if ids not in v_storage:<NewLine>                v_storage[ids] = []<NewLine>            <NewLine>            s_data = s_m if type(s_m) is not torch.Tensor else s_m.data.cpu()<NewLine>            o_data = o_m if type(o_m) is not torch.Tensor else o_m.data.cpu()<NewLine>            v_data = v_m if type(v_m) is not torch.Tensor else v_m.data.cpu()<NewLine>            <NewLine>            s_storage[ids].append((time, s_data))<NewLine>            o_storage[ids].append((time, o_data))<NewLine>            v_storage[ids].append((time, v_data))<NewLine>            <NewLine>            if len(s_storage[ids]) &gt; self.maxsize:<NewLine>                del s_storage[ids][0]<NewLine>            if len(o_storage[ids]) &gt; self.maxsize:<NewLine>                del o_storage[ids][0]<NewLine>            if len(v_storage[ids]) &gt; self.maxsize:<NewLine>                del v_storage[ids][0]<NewLine><NewLine>    def set_msg(self, qs, qo, qv, idtime):<NewLine>        self.mset(qs, qo, qv, idtime, self.s_storage, self.o_storage, self.v_storage)<NewLine><NewLine>    def set_gt_msg(self, s_target, o_target, v_target, idtime):<NewLine>        s_x = s_target.data.cpu()<NewLine>        o_x = o_target.data.cpu()<NewLine>        v_x = v_target.data.cpu()<NewLine>        self.mset(s_x, o_x, v_x, idtime, self.s_storage_gt, self.o_storage_gt, self.v_storage_gt)<NewLine><NewLine><NewLine>class AsyncTFCriterion(nn.Module, MessagePassing):<NewLine>    def __init__(self, args):<NewLine>        memory_size = 20<NewLine>        w_temporal = 0.1<NewLine>        w_spatio = 0.1<NewLine>        memory_decay = 1.0<NewLine>        sigma = 300<NewLine>        MessagePassing.__init__(self, memory_size, w_temporal, w_spatio, memory_decay, sigma, args.s_class, args.o_class, args.v_class)<NewLine>        nn.Module.__init__(self)<NewLine>        self.msg_n = 5<NewLine>        <NewLine>        self.cross_loss = nn.CrossEntropyLoss() # for s<NewLine>        self.bce_loss = nn.BCEWithLogitsLoss() # for c, o, v <NewLine>        <NewLine>        self.BalanceLabels = BalanceLabels()<NewLine>        self.winsmooth = 1<NewLine><NewLine>    def forward(self, s, o, v, so, ov, vs, ss, oo, vv, so_t, ov_t, vs_t, os_t, vo_t, sv_t, s_target, o_target, v_target, id_time, n=1, synchronous=False):<NewLine>        if o_target.dim() == 1:<NewLine>            print('converting Nx1 target to NxC')<NewLine>            o_target = Variable(gtmat(o.shape, o_target.data.long()))<NewLine>        if v_target.dim() == 1:<NewLine>            print('converting Nx1 target to NxC')<NewLine>            v_target = Variable(gtmat(v.shape, v_target.data.long()))<NewLine>        o_target = o_target.float()<NewLine>        v_target = v_target.float()<NewLine>        idtime = list(zip(id_time['id'], id_time['time']))<NewLine><NewLine>        <NewLine>        s_msg, o_msg, v_msg  = self.get_msg(idtime, 'past')<NewLine>        s_fmsg, o_fmsg, v_fmsg  = self.get_msg(idtime, 'future')<NewLine><NewLine>        s_loss = self.cross_loss(s, s_target)<NewLine>        _qs = torch.nn.Softmax(dim = 1)(s)<NewLine>        o_loss = self.bce_loss(o, o_target) <NewLine>        _qo = torch.nn.Sigmoid()(o)<NewLine>        v_loss = self.bce_loss(v, v_target)<NewLine>        _qv = torch.nn.Sigmoid()(v)<NewLine>        <NewLine>        qs_before_softmax = s.clone()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/LeonardoYueG,(Leonardo Yue G),LeonardoYueG,"December 1, 2019,  3:14pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I can’t quite reproduce the error since the main training loop hasn’t been posted. Though, with <code>nn.DataParallel</code>, a replica of your model will be created on each device (passed in via <code>device_ids</code>). In your forward function, it looks like you conditionally do things such as <code>.cuda()</code>, <code>.cpu()</code> , which can force tensors to conditionally be on a different device, which can result in this error. If possible, you could try not making your <code>forward</code> function rely on tensors being on specific devices, and instead do this logic in the main training loop.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much for your reply.</p><NewLine><p>Useing multi-gpus by torch.nn.DataParallel.  Input data and models are placed on the master device.  In forward function,   will this result of <code>.cuda(), .cpu()</code> be placed on the corresponding device?  I’m not very sure.  I will use the solution you mentioned above.</p><NewLine><p>And, the incomprehensible thing is that I only get errors after executing several batches. The first few batch programs will execute normally.</p><NewLine><p>This code is open sourced on github. I changed it from distributed to DataParallel.  The URL of code: <a href=""https://github.com/yaohungt/Gated-Spatio-Temporal-Energy-Graph"" rel=""nofollow noopener"">https://github.com/yaohungt/Gated-Spatio-Temporal-Energy-Graph</a></p><NewLine><p>Major changes：</p><NewLine><pre><code class=""lang-auto"">    def train(self, loader, base_model, logits_model, criterion, base_optimizer, logits_optimizer, epoch, args):<NewLine>        adjust_learning_rate(args.lr, args.lr_decay_rate, base_optimizer, epoch)<NewLine>        adjust_learning_rate(args.lr, args.lr_decay_rate, logits_optimizer, epoch)<NewLine>        batch_time = AverageMeter()<NewLine>        data_time = AverageMeter()<NewLine>        losses = AverageMeter()<NewLine>        s_top1 = AverageMeter()<NewLine>        s_top5 = AverageMeter()<NewLine>        o_top1 = AverageMeter()<NewLine>        o_top5 = AverageMeter()<NewLine>        v_top1 = AverageMeter()<NewLine>        v_top5 = AverageMeter()    <NewLine>        sov_top1 = AverageMeter()<NewLine><NewLine>        # switch to train mode<NewLine>        base_model.train()<NewLine>        logits_model.train()<NewLine>        criterion.train()<NewLine>        base_optimizer.zero_grad()<NewLine>        logits_optimizer.zero_grad()<NewLine><NewLine>        def part(x): return itertools.islice(x, int(len(x)*args.train_size))<NewLine>        end = time.time()<NewLine>        for i, (input, s_target, o_target, v_target, meta) in enumerate(part(loader)):<NewLine>            gc.collect()<NewLine>            data_time.update(time.time() - end)<NewLine>            meta['epoch'] = epoch<NewLine><NewLine>            print(""meta = {}"".format(meta))<NewLine><NewLine>            # s_target = s_target.long().cuda(async=True)<NewLine>            # o_target = o_target.long().cuda(async=True)<NewLine>            # v_target = v_target.long().cuda(async=True)<NewLine>            # input_var = torch.autograd.Variable(input.cuda())<NewLine>            s_target = s_target.long().cuda(device=device_ids[0])<NewLine>            o_target = o_target.long().cuda(device=device_ids[0])<NewLine>            v_target = v_target.long().cuda(device=device_ids[0])<NewLine>            input_var = torch.autograd.Variable(input.cuda(device=device_ids[0]))<NewLine>            s_target_var = torch.autograd.Variable(s_target)<NewLine>            o_target_var = torch.autograd.Variable(o_target)<NewLine>            v_target_var = torch.autograd.Variable(v_target)<NewLine>            <NewLine>            feat = base_model(input_var)<NewLine><NewLine>            feat = feat.cuda(device=device_ids[0])<NewLine><NewLine>            s, o, v, so, ov, vs, ss, oo, vv, so_t, ov_t, vs_t, os_t, vo_t, sv_t = logits_model(feat)<NewLine><NewLine>            s = s.cuda(device=device_ids[0])<NewLine>            o = o.cuda(device=device_ids[0])<NewLine>            v = v.cuda(device=device_ids[0])<NewLine>            so = so.cuda(device=device_ids[0])<NewLine>            ov = ov.cuda(device=device_ids[0])<NewLine>            vs = vs.cuda(device=device_ids[0])<NewLine>            ss = ss.cuda(device=device_ids[0])<NewLine>            oo = oo.cuda(device=device_ids[0])<NewLine>            vv = vv.cuda(device=device_ids[0])<NewLine>            so_t = so_t.cuda(device=device_ids[0])<NewLine>            ov_t = ov_t.cuda(device=device_ids[0])<NewLine>            vs_t = vs_t.cuda(device=device_ids[0])<NewLine>            os_t = os_t.cuda(device=device_ids[0])<NewLine>            vo_t = vo_t.cuda(device=device_ids[0])<NewLine>            sv_t = sv_t.cuda(device=device_ids[0])<NewLine>            s_target_var = s_target_var.cuda(device=device_ids[0])<NewLine>            o_target_var = o_target_var.cuda(device=device_ids[0])<NewLine>            v_target_var = v_target_var.cuda(device=device_ids[0])<NewLine>            meta['ids'] = meta['ids'].cuda(device=device_ids[0])<NewLine>            meta['time'] = meta['time'] .cuda(device=device_ids[0])<NewLine><NewLine><NewLine>            <NewLine>            s_output, o_output, v_output, loss = criterion(*((s, o, v, so, ov, vs, ss, oo, vv, so_t, ov_t, vs_t, os_t, vo_t, sv_t) + (s_target_var, o_target_var, v_target_var, meta)))<NewLine>            <NewLine>            s_prec1, s_prec5, s_prec1_output = accuracy_s(s_output.data, s_target, topk=(1, 5))<NewLine>            o_prec1, o_prec5, o_prec1_output = accuracy(o_output.data, o_target, topk=(1, 5))<NewLine>            v_prec1, v_prec5, v_prec1_output = accuracy(v_output.data, v_target, topk=(1, 5))<NewLine><NewLine>            sov_prec1 = s_prec1_output.cpu() * o_prec1_output * v_prec1_output<NewLine>            sov_prec1 = sov_prec1.sum(0, keepdim=True)<NewLine>            sov_prec1 = sov_prec1.mul_(100.0 / input.size(0))<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/rvarm1; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/LeonardoYueG; <NewLine> ,"REPLY_DATE 1: December 2, 2019,  1:27am; <NewLine> REPLY_DATE 2: December 2, 2019,  2:37am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
62451,Evaluate multiple models on multiple GPUs,2019-11-28T17:52:44.422Z,1,359,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello guys,</p><NewLine><p>I would like to do parallel evaluation of my models on multiple GPUs.  I don’t have much experience using python and pytorch this way. Here is a pseudocode of what I’m trying to do:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.multiprocessing as mp<NewLine>from mycnn import CNN<NewLine>from data_parser import parser<NewLine>from fitness import get_fitness  # this also runs on GPU<NewLine><NewLine>def run_model(outputs, model, device_id, input):<NewLine>    out = model(input)<NewLine>    f = get_fitness(out)    # due to this I cannot just run: model(input, non_blocking=True)<NewLine>    outputs[device_id] = f.cpu()<NewLine><NewLine>if __name__ == '__main__':<NewLine><NewLine>batch = parser.get_batch()<NewLine>model = CNN()<NewLine>GPU_NUM = 2<NewLine>outputs = torch.zeros(GPU_NUM, dtype=torch.double)  # collect outputs here<NewLine>outputs.share_memory_()     # I guess this is not enough to make it work<NewLine>mp.set_start_method('spawn')<NewLine>processes = []<NewLine><NewLine>for dev_id in (range(GPU_NUM)):<NewLine>    device = torch.device(""cuda:"" + str(dev_id))<NewLine>    dev_model = model.to(device)<NewLine>    dev_batch = batch.to(device)<NewLine>    p = mp.Process(target=run_model, args=(outputs, dev_model, dev_id, dev_batch))<NewLine>    p.start()<NewLine>    processes.append(p)<NewLine>for p in processes:<NewLine>    p.join()<NewLine><NewLine>print(outputs)<NewLine></code></pre><NewLine><p>Sadly this doesn’t work at all and I’m probably doing it completely wrong. I’m getting this error:</p><NewLine><pre><code class=""lang-auto"">OSError: [Errno 12] Cannot allocate memory<NewLine></code></pre><NewLine><p>For some reason it drains all memory on my server, even when GPU_NUM = 1. When I run code synchronously I get no errors. Could you please tell me what is the right way to do something like this or give me link to some examples, which would help me?</p><NewLine></div>",https://discuss.pytorch.org/u/Zdeeno,(Zdenek R.),Zdeeno,"November 28, 2019,  6:01pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you have multiple GPUs and want to evaluate each model on a single dedicated GPU independently, you could just push each model to a GPU via:</p><NewLine><pre><code class=""lang-python"">modelA = modelA.to('cuda:0')<NewLine>modelB = modelB.to('cuda:1')<NewLine>...<NewLine></code></pre><NewLine><p>and evaluate each model separately.<br/><NewLine>Since CUDA calls are asynchronous, the GPUs shouldn’t block each other, if you don’t synchronize them manually.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""62451""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/zdeeno/40/18243_2.png"" width=""20""/> Zdeeno:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">OSError: [Errno 12] Cannot allocate memory<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>If the cannot allocate memory issues still persist, you might try allocating additional swap space on your machine (see <a href=""https://github.com/pytorch/pytorch/issues/4387"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/4387</a> for more details).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/rvarm1; <NewLine> ,"REPLY_DATE 1: November 30, 2019,  7:25am; <NewLine> REPLY_DATE 2: December 2, 2019,  1:09am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
61631,DistributedDataParallel do not work with custom function in model,2019-11-20T16:35:41.948Z,1,174,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Im trying to use this model<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/NVIDIA/tacotron2/blob/master/model.py#L487"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/NVIDIA/tacotron2/blob/master/model.py#L487"" rel=""nofollow noopener"" target=""_blank"">NVIDIA/tacotron2/blob/master/model.py#L487</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""477"" style=""counter-reset: li-counter 476 ;""><NewLine><li>    input_lengths = to_gpu(input_lengths).long()</li><NewLine><li>    max_len = torch.max(input_lengths.data).item()</li><NewLine><li>    mel_padded = to_gpu(mel_padded).float()</li><NewLine><li>    gate_padded = to_gpu(gate_padded).float()</li><NewLine><li>    output_lengths = to_gpu(output_lengths).long()</li><NewLine><li><NewLine></li><NewLine><li>    return (</li><NewLine><li>        (text_padded, input_lengths, mel_padded, max_len, output_lengths),</li><NewLine><li>        (mel_padded, gate_padded))</li><NewLine><li><NewLine></li><NewLine><li class=""selected"">def parse_output(self, outputs, output_lengths=None):</li><NewLine><li>    if self.mask_padding and output_lengths is not None:</li><NewLine><li>        mask = ~get_mask_from_lengths(output_lengths)</li><NewLine><li>        mask = mask.expand(self.n_mel_channels, mask.size(0), mask.size(1))</li><NewLine><li>        mask = mask.permute(1, 0, 2)</li><NewLine><li><NewLine></li><NewLine><li>        outputs[0].data.masked_fill_(mask, 0.0)</li><NewLine><li>        outputs[1].data.masked_fill_(mask, 0.0)</li><NewLine><li>        outputs[2].data.masked_fill_(mask[:, 0, :], 1e3)  # gate energies</li><NewLine><li><NewLine></li><NewLine><li>    return outputs</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>But getting this error</p><NewLine><blockquote><NewLine><p>File “/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py”, line 60, in <em>worker<br/><NewLine>output = module(*input, **kwargs)<br/><NewLine>File “/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py”, line 547, in <strong>call</strong><br/><NewLine>result = self.forward(*input, **kwargs)<br/><NewLine>File “/home/ec2-user/SageMaker/tacotron2/model/model.py”, line 480, in forward<br/><NewLine>output_lengths)<br/><NewLine>File “/home/ec2-user/SageMaker/tacotron2/model/model.py”, line 452, in parse_output<br/><NewLine>outputs[0].data.masked_fill</em>(mask, 0.0)<br/><NewLine>RuntimeError: The expanded size of the tensor (1079) must match the existing size (836) at non-singleton dimension 2.  Target sizes: [4, 80, 1079].  Tensor sizes: [4, 80, 836]</p><NewLine></blockquote><NewLine><p>How to solve it?</p><NewLine></div>",https://discuss.pytorch.org/u/hadaev8,(Had),hadaev8,"November 20, 2019,  4:35pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you provide your DDP code to reproduce the issue? Also, does the model work properly without DDP?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""onebox"" href=""https://colab.research.google.com/drive/104LtQ1zIioIOMQEPgVve77m5Rd4Gm0wU"" rel=""nofollow noopener"" target=""_blank"">https://colab.research.google.com/drive/104LtQ1zIioIOMQEPgVve77m5Rd4Gm0wU</a><br/><NewLine>Yes, it is works fine, also same code works fine on single gpu colab instance.<br/><NewLine>Tested on 8 v100 instance from amazon.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hadaev8; <NewLine> ,"REPLY_DATE 1: November 26, 2019,  8:06pm; <NewLine> REPLY_DATE 2: November 29, 2019,  3:21pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
61940,What is the difference between rank and local-rank?,2019-11-23T08:45:06.100Z,2,1888,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The doc confuses me quite a lot,would you please tell me:</p><NewLine><ol><NewLine><li>What is the difference of two?</li><NewLine><li>When must use rank and when must use local-rank?</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/AlexLuya,(Alex Luya),AlexLuya,"November 23, 2019,  8:45am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/alexluya"">@AlexLuya</a>,</p><NewLine><p>In the context of multi-node training, you have:</p><NewLine><ul><NewLine><li><NewLine><code>local_rank</code>, the rank of the process on the local machine.</li><NewLine><li><NewLine><code>rank</code>, the rank of the process in the network.</li><NewLine></ul><NewLine><p>To illustrate that, let;s say you have 2 nodes (machines) with 2 GPU each, you will have a total of 4 processes (p1…p4):</p><NewLine><pre><code class=""lang-auto"">            |    Node1  |   Node2    |<NewLine>____________| p1 |  p2  |  p3  |  p4 |<NewLine>local_rank  | 0  |   1  |  0   |   1 |<NewLine>rank        | 0  |   1  |  2   |   4 |<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/spanev"">@spanev</a>,thanks,if p3 wants to send sth to p4,<br/><NewLine>1,it can use either local_rank or rank<br/><NewLine>2,but for performance,it should use local_rank<br/><NewLine>Am I right about above two?<br/><NewLine>3,why not just all use rank,and let lib to decide to which seed method(cross process or cross node) to use,what is the case that developer must use local_rank?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m assuming you’re referring to <code>local_rank</code> mentioned here: <a href=""https://github.com/pytorch/pytorch/blob/master/torch/distributed/launch.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/distributed/launch.py</a></p><NewLine><ol><NewLine><li>You should use <code>rank</code> and not <code>local_rank</code> when using torch.distributed primitives (send/recv etc). local_rank is passed to the training script only to indicate which GPU device the training script is supposed to use.</li><NewLine><li>You should always use <code>rank</code>.</li><NewLine><li><NewLine><code>local_rank</code> is supplied to the developer to indicate that a particular instance of the training script should use the “local_rank” GPU device. For illustration, in the example above provided by <a class=""mention"" href=""/u/spanev"">@spanev</a>, p1 is passed local_rank 0 indicating it should use GPU device id 0.</li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/spanev; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/AlexLuya; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> ,"REPLY_DATE 1: November 23, 2019, 10:36am; <NewLine> REPLY_DATE 2: November 24, 2019,  3:38am; <NewLine> REPLY_DATE 3: November 26, 2019,  9:33pm; <NewLine> ",REPLY 1 LIKES: 3 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
49767,DistributedDataParallel gradient print,2019-07-05T02:11:30.395Z,1,446,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all! New to pytorch and i am using pytorch to do distributed training. I knew that ‘DistributedDataParallel’ averages gradients between each process. I want to know that whether i could print the gradient before average for every process ?</p><NewLine></div>",https://discuss.pytorch.org/u/sherdencooper,,sherdencooper,"July 5, 2019,  2:11am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! This is possible. DDP relies on <code>torch.autograd.backward</code> to accumulate the gradients into the <code>grad</code> tensor of the model parameters. There is a functional alternative in <code>torch.autograd.grad</code> that doesn’t accumulate at all. If you’re interested in the local gradients, instead of running <code>loss.backward()</code>, you can run <code>torch.autograd.grad(loss, model.parameters())</code> and get back a list of gradient tensors, one for every model parameter. This doesn’t accumulate them into the <code>grad</code> tensor of the model parameter, so it doesn’t kick off DDP. If you want to run DDP afterwards anyway, make sure to pass the <code>retain_graph=True</code> kwarg to <code>torch.autograd.grad</code>. I haven’t tried any of this out, but in theory it should work.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot ! Your advice is really helpful ! I got the local gradients and it seems that DDP is not affected at all ! Its very cool that i can get professional guidance at pytorch’s forum.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m glad you were able to continue! <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, pietern <a class=""mention"" href=""/u/pietern"">@pietern</a> , the “DistributedDataParallel” automatically average the gradient when calling “loss.backward()”,<br/><NewLine>But I didn’t find the corresponding script about how to get all the gradient of nodes and average the gradients during backward in pytorch source code, Do you know where it is ?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/sherdencooper; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/meilu_zhu; <NewLine> ,"REPLY_DATE 1: July 5, 2019, 10:36am; <NewLine> REPLY_DATE 2: July 6, 2019, 12:26pm; <NewLine> REPLY_DATE 3: July 7, 2019,  5:58pm; <NewLine> REPLY_DATE 4: November 26, 2019,  2:50am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
62102,_pickle.UnpicklingError: pickle data was truncated,2019-11-25T11:06:12.765Z,0,1171,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve installed pytorch 1.0 on ubuntu18.04.<br/><NewLine>When I try to use webcam demo provided by  maskrcnn-benchmark. An error occured:<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “webcam.py”, line 80, in <br/><NewLine>main()<br/><NewLine>File “webcam.py”, line 64, in main<br/><NewLine>min_image_size=args.min_image_size,<br/><NewLine>File “/home/aisen/github/maskrcnn-benchmark/demo/predictor.py”, line 149, in <strong>init</strong><br/><NewLine>_ = checkpointer.load(cfg.MODEL.WEIGHT)<br/><NewLine>File “/home/aisen/github/maskrcnn-benchmark/maskrcnn_benchmark/utils/checkpoint.py”, line 61, in load<br/><NewLine>checkpoint = self._load_file(f)<br/><NewLine>File “/home/aisen/github/maskrcnn-benchmark/maskrcnn_benchmark/utils/checkpoint.py”, line 134, in _load_file<br/><NewLine>return load_c2_format(self.cfg, f)<br/><NewLine>File “/home/aisen/github/maskrcnn-benchmark/maskrcnn_benchmark/utils/c2_model_loading.py”, line 206, in load_c2_format<br/><NewLine>return C2_FORMAT_LOADER[cfg.MODEL.BACKBONE.CONV_BODY](cfg, f)<br/><NewLine>File “/home/aisen/github/maskrcnn-benchmark/maskrcnn_benchmark/utils/c2_model_loading.py”, line 192, in load_resnet_c2_format<br/><NewLine>state_dict = _load_c2_pickled_weights(f)<br/><NewLine>File “/home/aisen/github/maskrcnn-benchmark/maskrcnn_benchmark/utils/c2_model_loading.py”, line 136, in _load_c2_pickled_weights<br/><NewLine>data = pickle.load(f, encoding=“latin1”)<br/><NewLine>_pickle.UnpicklingError: pickle data was truncated</p><NewLine></div>",https://discuss.pytorch.org/u/Aisen,(FengyanZhong),Aisen,"November 25, 2019, 11:06am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/aisen"">@Aisen</a>, can you post the code which dumped the pickle file which you are trying to load</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/anantguptadbl; <NewLine> ,"REPLY_DATE 1: November 25, 2019, 11:55am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
44291,Distributed loss function,2019-05-03T15:17:23.082Z,0,459,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is there any example of how to calculate the loss within multiple GPU and merge all of them later after the calculation?</p><NewLine><p>Currently, we could calculate the output from a network by using DistributedDataParalel. However, the result from DistributedDataParallel is collected in device 0. Therefore, the calculation was done in 1 GPU only instead of multi-GPU.</p><NewLine></div>",https://discuss.pytorch.org/u/norm_inf,,norm_inf,"May 3, 2019,  3:17pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What do you mean exactly here? Are you looking to compute only a single loss value for a model that gets executed on multiple processes? Or just on multiple GPUs from a single process?</p><NewLine><p>Even though the result is collected in GPU 0, the gradients will propagate back through the activations on all GPUs that contributed in computing the final loss value. The gradients that are computed for every replica are averaged automatically by <code>torch.nn.parallel.DistributedDataParallel</code>, so all replicas contribute to the gradient that is later used by the optimizer to update your model weights.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am facing the same problem, have you find any proper solution?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Zhilin_Lu; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  9:39am; <NewLine> REPLY_DATE 2: November 20, 2019,  8:46am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
60619,Send and receive tensor with indefinite length,2019-11-11T11:02:13.577Z,0,100,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I am doing some work about gradient sparsification and compression in model training. After compress, tensor size may be different among workers. Now I have to send tensor size to others at first and then send the compressed tensor. However it looks ugly. I want to send and receive tensor with indefinite length in one communication, any ideas?</p><NewLine></div>",https://discuss.pytorch.org/u/lecoan,(lecoan),lecoan,"November 11, 2019, 11:02am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Unfortunately, with ProcessGroup send/recv, this needs to be done in two comms (size and data) for now. We are building a new <a href=""https://github.com/pytorch/pytorch/issues/23110"" rel=""nofollow noopener"">RPC API</a> which could simplifies it a bit at the API level, but internally it still uses ProcessGroup send/recv currently until we have a better comm primitive for that.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: November 19, 2019,  5:49pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
61257,Saving distributed models,2019-11-17T09:40:47.130Z,1,153,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m trying to train multi-agent reinforcement learning. To do so, each agent has distributed (separate) network model. Therefore, the distributed model has a number of nn.Module for each separate model.</p><NewLine><p>I want to save the entire networks’ parameters to evaluate the trained model. How can I save the entire parameters?</p><NewLine><p>As I know, using</p><NewLine><blockquote><NewLine><p>d=model.state_dict()</p><NewLine></blockquote><NewLine><p>and</p><NewLine><blockquote><NewLine><p>torch.save(d, path)</p><NewLine></blockquote><NewLine><p>is appropriate. But the <em>model</em> used in the above command seems to be linked for only one network model (not entire separate model).</p><NewLine><p>How can I save all separate model?</p><NewLine><p>Thank you</p><NewLine></div>",https://discuss.pytorch.org/u/iwkim,,iwkim,"November 18, 2019,  6:27am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not sure what “distributed” means in your use case.<br/><NewLine>Are you working with different models? If so, you could just save the <code>state_dict</code> of each model using a separate file.<br/><NewLine>Or are you working in a distributed setup, where the models are scattered and gathers using different nodes?<br/><NewLine>In that case, you could most likely want to reduce the model to the main node and just store this <code>state_dict</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply.<br/><NewLine>Specifically, I have a number of agents and each agent has own policy network.<br/><NewLine>So I said it as ‘distributed’ but it was quite vague… sorry for the inconvenience.</p><NewLine><p>I think the first one you gave me is applicable, right?<br/><NewLine>Because I generated a number of policy networks (models) and it is necessary to store separately.<br/><NewLine>Am I right?</p><NewLine><p>Thank you</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/iwkim; <NewLine> ,"REPLY_DATE 1: November 18, 2019,  6:35am; <NewLine> REPLY_DATE 2: November 18, 2019,  7:12am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
61011,How to deploy different scripts on different GPUs?,2019-11-14T17:26:12.901Z,0,113,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everybody,</p><NewLine><p>I am trying to deploy different pytorch-based training scripts on different GPUs. However, the information I could find is about training a model on multiple GPUs.</p><NewLine><p>Could some tell me how to do this?<br/><NewLine>I tried the ‘spawn’ trick, ‘.cuda(0)/.cuda(1)’ trick… But they were not working.</p><NewLine><p>Sorry if this is a bad question</p><NewLine><p>Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/Huimin_ZENG,(Huimin ZENG),Huimin_ZENG,"November 14, 2019,  5:26pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>just need to pass CUDA_VISIBLE_DEVICES=0 python script.py<br/><NewLine>replace 0 by the gpu you want</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could pass the device you want to train on as an argument to the script.</p><NewLine><p>For example, ‘cuda:0’ corresponds to the 1st GPU in your system, ‘cuda:1’ corresponds to the 2nd GPU and so on.</p><NewLine><p>Then assuming you store the passed argument in a variable named <code>device</code>, all you have to do is to call <code>.to(device)</code> on your tensors etc.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/JuanFMontesinos; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/TinfoilHat0; <NewLine> ,"REPLY_DATE 1: November 14, 2019, 10:11pm; <NewLine> REPLY_DATE 2: November 14, 2019, 10:11pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
58544,[BUG?] DistributedDataParallel cannot be destroyed,2019-10-18T03:24:23.490Z,2,242,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My code involves two stage, like<br/><NewLine>model_a -&gt; do something and model_b -&gt; do something<br/><NewLine>And I use DistributedDataParallel(DDP) to accelerate them rather than DP. So I do something like that:</p><NewLine><pre><code class=""lang-auto"">model_a -&gt; model_a = setup DDP and DDP(model_a) -&gt; model_b -&gt; setup DDP and model_b = DDP(model_b)<NewLine></code></pre><NewLine><p><strong>This will cause problem because you cannot open start two DDP in one process.</strong></p><NewLine><p>So I use dist.destroy_process_group(). But when I use this function, <strong>the program will wait for endless time.</strong></p><NewLine><p>Here are my start DDP and destroy DDP codes:</p><NewLine><pre><code class=""lang-python""> def setup(rank, world_size):<NewLine>     os.environ['MASTER_ADDR'] = 'localhost'<NewLine>     os.environ['MASTER_PORT'] = '12355'<NewLine><NewLine>     # initialize the process group<NewLine>     dist.init_process_group(""gloo"", rank=rank, world_size=world_size)<NewLine>     #dist.init_process_group(""nccl"", rank=rank, world_size=world_size)<NewLine><NewLine>     # Explicitly setting seed to make sure that models created in two processes<NewLine>     # start from same random weights and biases.<NewLine>     torch.manual_seed(42)<NewLine><NewLine><NewLine> def cleanup():<NewLine>     dist.destroy_process_group()<NewLine></code></pre><NewLine><p>I’ve tried either gloo backend or nccl backend</p><NewLine><p>For Pytorch 1.1, python 3.6, <strong>CUDA 9.0</strong></p><NewLine><p>Thank you</p><NewLine></div>",https://discuss.pytorch.org/u/CuriousCat-7,(Neo),CuriousCat-7,"October 18, 2019,  3:39am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">model_a -&gt; model_a = setup DDP and DDP(model_a) -&gt; model_b -&gt; setup DDP and model_b = DDP(model_b)<NewLine></code></pre><NewLine><p>I’m not sure I follow this completely, does model_b use the output of model_a? Could you share some code about how model_a and model_b are initialized and trained using DDP?</p><NewLine><p>Is it possible to create a single model with model_a and model_b as submodules and then use that as part of DDP?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry for late, I mean I setup DDP for model_a and setup DDP for model_b in the same time:<br/><NewLine>Looks like:</p><NewLine><pre><code class=""lang-python"">setup()<NewLine>model_a = DDP(model_a)<NewLine>setup()<NewLine>model_b = DDP(model_b)<NewLine></code></pre><NewLine><p>This will cause an error.</p><NewLine><p>So I need to change like that:</p><NewLine><pre><code class=""lang-python"">setup()<NewLine>model_a = DDP(model_a)<NewLine>cleanup()<NewLine>setup()<NewLine>model_b = DDP(model_b)<NewLine></code></pre><NewLine><p>but, after the cleanup, the program will be blocked.<img alt="":joy:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/joy.png?v=9"" title="":joy:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried the following program (which is a bit similar to what you were doing), but couldn’t reproduce the issue:</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import tempfile<NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine>import torch.multiprocessing as mp<NewLine><NewLine>from torch.nn.parallel import DistributedDataParallel as DDP<NewLine><NewLine><NewLine>def setup(rank, world_size):<NewLine>    os.environ['MASTER_ADDR'] = 'localhost'<NewLine>    os.environ['MASTER_PORT'] = '12355'<NewLine><NewLine>    # initialize the process group<NewLine>    dist.init_process_group(""gloo"", rank=rank, world_size=world_size)<NewLine><NewLine>    # Explicitly setting seed to make sure that models created in two processes<NewLine>    # start from same random weights and biases.<NewLine>    torch.manual_seed(42)<NewLine><NewLine><NewLine>def cleanup():<NewLine>    dist.destroy_process_group()<NewLine><NewLine>class ToyModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(ToyModel, self).__init__()<NewLine>        self.net1 = nn.Linear(10, 10)<NewLine>        self.relu = nn.ReLU()<NewLine>        self.net2 = nn.Linear(10, 5)<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.net2(self.relu(self.net1(x)))<NewLine><NewLine>setup(0, 1)<NewLine>print (""F1"")<NewLine>model_a = DDP(ToyModel())<NewLine>print (""F2"")<NewLine>cleanup()<NewLine>print (""F3"")<NewLine>setup(0, 1)<NewLine>print (""F4"")<NewLine>model_b = DDP(ToyModel())<NewLine>print (""F5"")<NewLine></code></pre><NewLine><p>The program prints:</p><NewLine><pre><code class=""lang-auto"">F1<NewLine>F2<NewLine>F3<NewLine>F4<NewLine>F5<NewLine></code></pre><NewLine><p>Could you share more details about your environment (OS, python version etc)? Also, do you know at which line after the <code>cleanup</code> the program blocks on?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/CuriousCat-7; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> ,"REPLY_DATE 1: October 22, 2019, 12:37am; <NewLine> REPLY_DATE 2: November 13, 2019,  1:08pm; <NewLine> REPLY_DATE 3: November 13, 2019,  9:26pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
60700,DataParallel output differs from its module,2019-11-12T05:54:29.493Z,2,91,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to do multi-task learning with two classification layers from single shared representation.<br/><NewLine>I believe, having two nn.Linear layers would not differ from having one nn.Linear layer then split,<br/><NewLine>but it occurs to me they work differently when used with nn.DataParallel.</p><NewLine><p>Please see the code at the bottom.<br/><NewLine>&lt;class ‘Split’&gt; has one nn.Linear then splits the output.<br/><NewLine>&lt;class ‘TwoHeads’&gt; has two nn.Linear layers.</p><NewLine><p>What I do in the main code is to compare the outputs of</p><NewLine><blockquote><NewLine><p>nn.DataParallel(net)</p><NewLine></blockquote><NewLine><p>to</p><NewLine><blockquote><NewLine><p>nn.DataParallel(net).module.</p><NewLine></blockquote><NewLine><p>For Split the output coincides.<br/><NewLine>For TwoHeads the output differs.</p><NewLine><p>And the code here.</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine>class Split(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Split, self).__init__()<NewLine>        self.linear = nn.Linear(10, 5)<NewLine><NewLine>    def forward(self, x):<NewLine>        out = self.linear(x)<NewLine>        out1, out2 = torch.split(out, 3, dim=1)<NewLine>        return out1, out2<NewLine><NewLine>class TwoHeads(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(TwoHeads, self).__init__()<NewLine>        self.linear1 = nn.Linear(10, 3)<NewLine>        self.linear2 = nn.Linear(10, 2)<NewLine><NewLine>    def forward(self, x):<NewLine>        out1 = self.linear1(x)<NewLine>        out2 = self.linear2(x)<NewLine>        return out1, out2<NewLine><NewLine>if __name__ == '__main__':<NewLine>    net1 = Split()<NewLine>    net2 = TwoHeads()<NewLine><NewLine>    for net in [net1, net2]:<NewLine>        net.cuda()<NewLine>        net = nn.DataParallel(net, list(range(4)))<NewLine><NewLine>        with torch.no_grad():<NewLine>            data = torch.randn(500, 10).cuda()<NewLine><NewLine>            out1, out2 = net(data)<NewLine>            mod1, mod2 = net.module(data)<NewLine><NewLine>            print(int(not torch.equal(out1, mod1)), end=' ')<NewLine>            print(int(not torch.equal(out2, mod2)), end=' ')<NewLine><NewLine>            print((out1 - mod1).abs().max(), (out2 - mod2).abs().max())<NewLine></code></pre><NewLine><p>For me, the output looks like</p><NewLine><blockquote><NewLine><p>0 0 tensor(0., device=‘cuda:0’) tensor(0., device=‘cuda:0’)<br/><NewLine>0 1 tensor(0., device=‘cuda:0’) tensor(2.3842e-07, device=‘cuda:0’)</p><NewLine></blockquote><NewLine><p>My questions are:</p><NewLine><ul><NewLine><li>Is it intended?</li><NewLine><li>If it is because the computation graph uses the same variable twice, shall I always avoid ‘branching’ the computation graph?</li><NewLine><li>What would be the correct way to do multi-task learning with DataParallel?</li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/seongmin,,seongmin,"November 12, 2019,  6:22am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The error of <code>1e-7</code> is most likely due to floating point precision (usually you expect the error for FP32 to be in ~1e-6), so it seems your code is working fine.</p><NewLine><pre><code class=""lang-python"">x = torch.randn(10, 10, 10)<NewLine>sum1 = x.sum()<NewLine>sum2 = x.sum(0).sum(0).sum(0)<NewLine>print(sum1 - sum2)<NewLine>&gt; tensor(-3.8147e-06)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you so much for your answer and the code.</p><NewLine><p>I firstly thought of it too, and I repeated the same experiment many times and on another machine but<br/><NewLine>the error still occurs for &lt;class ‘TwoHeads’&gt; at the second output only.</p><NewLine><p>Would it be still most likely the floating point error?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Since the error is that low, I would still assume it’s still due to floating point precision.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/seongmin; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: November 12, 2019,  9:38pm; <NewLine> REPLY_DATE 2: November 13, 2019,  2:28am; <NewLine> REPLY_DATE 3: November 13, 2019,  3:05am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
60481,Split and distribute a large tensor like what it does in torch.utils.data.DistributedSampler,2019-11-09T17:30:52.518Z,1,193,"<div class=""post"" itemprop=""articleBody""><NewLine><p>AFAIK, the simplest way to do the distributed training (multiple modes) with Pytorch is something like:</p><NewLine><pre><code>sampler = torch.utils.data.distributed.DistributedSampler(train_data)<NewLine>data_loader = torch.utils.data.DataLoader(dataset, sampler=sampler)<NewLine>model = torch.nn.DataParallel(model).cuda()<NewLine><NewLine>for data, target in data_loader:<NewLine>    out = model(data)<NewLine>    ...<NewLine></code></pre><NewLine><p>But what if I already have a large tensor <code>data</code> in hand and would like to split and distribute it and get the same output as the above snippet? Specifically,</p><NewLine><pre><code> model = torch.nn.DataParallel(model).cuda()<NewLine> data = do_sth_fuct(data)<NewLine> out = model(data)<NewLine></code></pre><NewLine><p>Is there a PyTorch API to do so? Otherwise, what is the best way to achieve it? Thank you in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/pytorchdope,,pytorchdope,"November 10, 2019,  1:32pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>But what if I already have a large tensor  <code>data</code>  in hand and would like to split and distribute it and get the same output as the above snippet</p><NewLine></blockquote><NewLine><p>It really depends on where this large tensor is stored and how it is loaded. Is this large tensor stored in memory of one of the nodes? It might be helpful if you describe your system a bit more in detail and especially how the large tensor is computed/retrieved.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, thanks for your reply!</p><NewLine><p>The tensor is stored in one of the nodes. More spefcifically, I have, say, two nodes and each of them have 8 gpus. I have a text dataset <code>train.txt</code>. I have written a function that convert the text data to  large tensor <code>X</code>.</p><NewLine><p>If I used <code>torch.nn.parallel.DistributedDataParallel</code> in the following way:</p><NewLine><pre><code>model = torch.nn.parallel.DistributedDataParallel(mode.cuda())<NewLine></code></pre><NewLine><p>Would <code>model(X)</code> do what I want, that is, split the X and distribute the pieces to 16 gpus?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pytorchdope; <NewLine> ,"REPLY_DATE 1: November 12, 2019,  2:38am; <NewLine> REPLY_DATE 2: November 12, 2019,  2:55am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
60459,Question about loading the model that was trained using 4GPU with distributed dataparallel to only 1 GPU job,2019-11-09T12:46:08.350Z,2,693,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/>  , I’m having trouble loading the distributed dataparallel model to just 1 GPU. And I want to know how to load the model (trained by 4 GPU with distributed dataparallel) to another job using only 1 GPU.</p><NewLine><p>I have trained a model using 4 GPU and distributed dataparallel, and I saved it as the tutorial:<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#save-and-load-checkpoints"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#save-and-load-checkpoints</a><br/><NewLine>However, I don’t know how to load it using just 1 GPU for some simple job like validation test.</p><NewLine><pre><code class=""lang-auto"">if rank == 0:<NewLine>   torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)<NewLine>dist.barrier()<NewLine></code></pre><NewLine><p>I’m now using this method:</p><NewLine><pre><code class=""lang-auto"">    # initialize<NewLine>    torch.distributed.init_process_group(backend=""nccl"")<NewLine>    local_rank = torch.distributed.get_rank()<NewLine>    print(local_rank)<NewLine>    torch.cuda.set_device(local_rank)<NewLine>    device = torch.device(""cuda"", local_rank)<NewLine>    print(device)<NewLine>    <NewLine>    # only gpu with rank0 can remain running<NewLine><NewLine>    model = resnet50()<NewLine>    model.to(device)<NewLine>    model = torch.nn.parallel.DistributedDataParallel(model, <NewLine>                                                        device_ids=[local_rank],<NewLine>                                                        output_device=local_rank)<NewLine>        <NewLine>    model.load_state_dict(torch.load(cfg.MODEL.pretrained_model_path))<NewLine>    model.eval()<NewLine>    if local_rank == 0:<NewLine>        acc, acc_std, th = lfw_test(model, cfg.TEST.lfw_root, cfg.TEST.lfw_test_list)<NewLine></code></pre><NewLine><p>and the command code:</p><NewLine><pre><code class=""lang-auto"">CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 test.py<NewLine></code></pre><NewLine><p>This method works, and through <code>nvidia-smi</code> I saw that only GPU0 is working, but when I run another test process using GPU device 1 (when the previous one is still running):</p><NewLine><pre><code class=""lang-auto"">CUDA_VISIBLE_DEVICES=1,2,3 python -m torch.distributed.launch --nproc_per_node=3 test.py<NewLine></code></pre><NewLine><p>The previous process throw a runtime error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1549633347309/work/torch/lib/c10d/ProcessGroupNCCL.cpp:260, unhandled system error<NewLine></code></pre><NewLine><p>There are 2 reasons for me to load the model using 1GPU:</p><NewLine><ol><NewLine><li>Some jobs have file-writing part and distributed parallel may cause wrong order.</li><NewLine><li>Running 4 tiny experiment with 1 GPU per process is more efficient for me to test my idea and finding bugs.</li><NewLine></ol><NewLine><p>So is there a way that I can load the model like the common ways :<code>torch.load_state_dict(torch.load()).to(torch.device(""cuda:0))</code>? <img alt="":smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smile.png?v=9"" title="":smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/zhengrchan,(Zhengrchan),zhengrchan,"November 9, 2019, 12:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh… the first method is not work, neither. I find that using:</p><NewLine><pre><code class=""lang-auto"">if local_rank == 0:<NewLine>    output = model(input)<NewLine></code></pre><NewLine><p>the <code>model(input)</code> will never output. And the code is just blocked there.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not sure to understand the use case.<br/><NewLine>It seems you would like to load the <code>state_dict</code> to a single GPU machine, but in your code you are wrapping the model again in DDP.</p><NewLine><p>Would creating the model, loading the <code>state_dict</code>, and pushing the model to the single GPU not work?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It works! Thank you <img alt="":smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smile.png?v=9"" title="":smile:""/></p><NewLine><p>I used:</p><NewLine><pre><code class=""lang-auto"">model = resnet50()<NewLine>model.to(device)<NewLine>model.load_state_dict(torch.load(cfg.MODEL.pretrained_model_path))<NewLine></code></pre><NewLine><p>and I got a Runtime error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Error(s) in loading state_dict for Resnet:<NewLine>    Missing key(s) in state_dict: ""conv1.weight"", ""bn1.weight"" ... ...<NewLine>    Unexpected key(s) in state_dict: ""module.conv1.weight"", ""module.bn1.weight"" ... ...<NewLine></code></pre><NewLine><p>Seems the Distributed DataParallel save the model in <code>module</code>. Then I find a <strong>solution</strong> there:<br/><NewLine><aside class=""quote"" data-post=""4"" data-topic=""1686""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/fmassa/40/58_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/4"">[solved] KeyError: 'unexpected key ""module.encoder.embedding.weight"" in state_dict'</a><NewLine></div><NewLine><blockquote><NewLine>    I was thinking about something like the following: <NewLine># original saved file with DataParallel<NewLine>state_dict = torch.load('myfile.pth.tar')<NewLine># create new OrderedDict that does not contain `module.`<NewLine>from collections import OrderedDict<NewLine>new_state_dict = OrderedDict()<NewLine>for k, v in state_dict.items():<NewLine>    name = k[7:] # remove `module.`<NewLine>    new_state_dict[name] = v<NewLine># load params<NewLine>model.load_state_dict(new_state_dict)<NewLine>  </blockquote><NewLine></aside><NewLine></p><NewLine><pre><code class=""lang-auto"">    state_dict = torch.load(cfg.MODEL.pretrained_model_path)<NewLine>    new_state_dict = OrderedDict()<NewLine>    for k, v in state_dict.items():<NewLine>        name = k[7:]<NewLine>        new_state_dict[name] = v<NewLine>    model.load_state_dict(new_state_dict)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zhengrchan; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/zhengrchan; <NewLine> ,"REPLY_DATE 1: November 9, 2019,  1:05pm; <NewLine> REPLY_DATE 2: November 11, 2019,  8:38am; <NewLine> REPLY_DATE 3: November 11, 2019,  8:37am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
60454,Using two DataParallel in one Architecture,2019-11-09T11:25:58.062Z,1,101,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using 4 GPUs, let’s say device0, device1, device2, and device3. And the model is:</p><NewLine><ul><NewLine><li>Inside the <strong>init</strong> function:</li><NewLine></ul><NewLine><pre><code class=""lang-auto"">sub_model1.to(device0)<NewLine>sub_model1 = torch.nn.DataParallel(sub_model1, device_ids=[device0, device1])<NewLine>sub_model2.to(device2)<NewLine>sub_model2 = torch.nn.DataParallel(sub_model2, device_ids=[device2, device3])<NewLine></code></pre><NewLine><ul><NewLine><li>Inside the <strong>forward</strong> function:</li><NewLine></ul><NewLine><pre><code class=""lang-auto"">y = sub_model1(x)<NewLine>#y = y.to(device2) # we dont need this as we can put data in any device when using DataParallel<NewLine>out = sub_model2(y)<NewLine></code></pre><NewLine><p>This gives me out of memory problems after running perfectly for some epochs. The error is written like <code> (RuntimeError: Caught RuntimeError in replica 1 on device 3.).</code></p><NewLine><p>Am I doing it correctly? In my case, if I don’t use the DataParallel, it works perfectly with 2GPUs (I simply double the <code>batch_size</code> when using this DataParallel setting, 4 GPUs)</p><NewLine></div>",https://discuss.pytorch.org/u/Fajri_Koto,(Fajri Koto),Fajri_Koto,"November 9, 2019, 12:13pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The error message doesn’t sound like an OOM error, but a <code>RuntimeError</code>.<br/><NewLine>I assume you are not seeing this error using a single GPU?</p><NewLine><p>Do you get any more information from the stack trace?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi thanks for your reply</p><NewLine><p>This is the complete error:</p><NewLine><pre><code class=""lang-auto"">2019-11-10 10:56:51,904 - Parser - Current learning rate: 0.002000<NewLine>2019-11-10 10:56:59,308 - Parser - Epoch 0, Batch 1, AvgCost: 2.10, CorrectSpan: 0.49, CorrectNuclear: 0.37, CorrectRelation: 0.03 - 0 mins 7 secs<NewLine>2019-11-10 10:57:00,668 - Parser - Epoch 0, Batch 2, AvgCost: 2.08, CorrectSpan: 0.51, CorrectNuclear: 0.36, CorrectRelation: 0.02 - 0 mins 8 secs<NewLine>2019-11-10 10:57:03,495 - Parser - Epoch 0, Batch 3, AvgCost: 2.11, CorrectSpan: 0.51, CorrectNuclear: 0.35, CorrectRelation: 0.03 - 0 mins 11 secs<NewLine>2019-11-10 10:57:04,270 - Parser - Epoch 0, Batch 4, AvgCost: 2.10, CorrectSpan: 0.52, CorrectNuclear: 0.36, CorrectRelation: 0.03 - 0 mins 12 secs<NewLine>2019-11-10 10:57:04,866 - Parser - Epoch 0, Batch 5, AvgCost: 2.04, CorrectSpan: 0.52, CorrectNuclear: 0.34, CorrectRelation: 0.03 - 0 mins 12 secs<NewLine>2019-11-10 10:57:09,912 - Parser - Epoch 0, Batch 6, AvgCost: 2.05, CorrectSpan: 0.53, CorrectNuclear: 0.35, CorrectRelation: 0.05 - 0 mins 18 secs<NewLine>2019-11-10 10:57:12,131 - Parser - Epoch 0, Batch 7, AvgCost: 2.07, CorrectSpan: 0.52, CorrectNuclear: 0.37, CorrectRelation: 0.04 - 0 mins 20 secs<NewLine>2019-11-10 10:57:12,906 - Parser - Epoch 0, Batch 8, AvgCost: 2.06, CorrectSpan: 0.53, CorrectNuclear: 0.37, CorrectRelation: 0.04 - 0 mins 21 secs<NewLine>2019-11-10 10:57:13,351 - Parser - Epoch 0, Batch 9, AvgCost: 2.06, CorrectSpan: 0.53, CorrectNuclear: 0.37, CorrectRelation: 0.04 - 0 mins 21 secs<NewLine>2019-11-10 10:57:13,651 - Parser - Epoch 0, Batch 10, AvgCost: 2.08, CorrectSpan: 0.53, CorrectNuclear: 0.37, CorrectRelation: 0.04 - 0 mins 21 secs<NewLine>Traceback (most recent call last):<NewLine>  File ""train.py"", line 319, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""train.py"", line 238, in main<NewLine>    cost, cost_val = network.loss(subset_data, gold_subtrees, epoch=epoch)<NewLine>  File ""/home/ffajri/Workspace/neural_project/models/architecture.py"", line 497, in loss<NewLine>    cost = self.decode(encoder_output, gold_nuclear, gold_relation, gold_segmentation, span, len_golds)<NewLine>  File ""/home/ffajri/Workspace/neural_project/models/architecture.py"", line 381, in decode<NewLine>    segment_output, transformer_output = self.run_transformer_segmentation(hidden_state1, segment_mask) #output in cuda-2<NewLine>  File ""/home/ffajri/Workspace/neural_project/models/architecture.py"", line 98, in run_transformer_segmentation<NewLine>    edus_score, edus_vec  = self.transformer_segmenter(segmented_encoder, segment_mask.int())<NewLine>  File ""/home/ffajri/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/ffajri/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 152, in forward<NewLine>    outputs = self.parallel_apply(replicas, inputs, kwargs)<NewLine>  File ""/home/ffajri/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 162, in parallel_apply<NewLine>    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])<NewLine>  File ""/home/ffajri/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 85, in parallel_apply<NewLine>    output.reraise()<NewLine>  File ""/home/ffajri/anaconda3/envs/py3/lib/python3.7/site-packages/torch/_utils.py"", line 385, in reraise<NewLine>    raise self.exc_type(msg)<NewLine>RuntimeError: Caught RuntimeError in replica 1 on device 3.<NewLine>Original Traceback (most recent call last):<NewLine>  File ""/home/ffajri/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 60, in _worker<NewLine>    output = module(*input, **kwargs)<NewLine>  File ""/home/ffajri/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/ffajri/Workspace/neural_project/modules/encoder.py"", line 95, in forward<NewLine>    x = self.transformer_inter[i](i, x, x, mask!=1)  # all_sents * max_tokens * dim<NewLine>  File ""/home/ffajri/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/ffajri/Workspace/neural_project/modules/encoder.py"", line 68, in forward<NewLine>    mask=mask)<NewLine>  File ""/home/ffajri/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/ffajri/Workspace/neural_project/modules/neural.py"", line 410, in forward<NewLine>    query = query / math.sqrt(dim_per_head)<NewLine>RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 3; 15.75 GiB total capacity; 11.64 GiB already allocated; 2.12 MiB free; 2.97 GiB cached)<NewLine></code></pre><NewLine><p>Without DataParallel, it requires me to run it with at least two GPUs, and it works perfectly.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>For now, I think the problem is because I am calling the <code>sub_model2</code> within a loop.</p><NewLine><pre><code class=""lang-auto"">y = sub_model1(x)<NewLine>while (not_finished()):<NewLine>     y1 = pick_some_index(y)<NewLine>     out = sub_model2(y1)<NewLine></code></pre><NewLine><p>Based on this <a href=""https://erickguan.me/2019/pytorch-parallel-model"" rel=""nofollow noopener"">https://erickguan.me/2019/pytorch-parallel-model</a>, DataParallel copy all model and mini-batch for each device. I guess the copied model in each device might not be deleted (nor handled wisely by Pytorch?) for each iteration in the loop (CMIIW).</p><NewLine><p>I change my training stage by eradicating these loops. It works now with <code>2 * batch_size</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Fajri_Koto; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Fajri_Koto; <NewLine> ,"REPLY_DATE 1: November 9, 2019,  9:52pm; <NewLine> REPLY_DATE 2: November 10, 2019,  5:43am; <NewLine> REPLY_DATE 3: November 10, 2019,  6:21pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
46506,Why do we need &ldquo;flatten_parameters&rdquo; when using RNN with DataParallel,2019-05-29T06:50:34.561Z,1,4316,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I got the following warning message when I use LSTM with nn.DataParallel.</p><NewLine><pre><code class=""lang-bash"">RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory.<NewLine>This means they need to be compacted at every call, possibly greatly increasing memory usage.<NewLine>To compact weights again call flatten_parameters().<NewLine></code></pre><NewLine><p>I found the error is gone when I put <code>self.lstm.flatten_parameters()</code> at the top of <code>forward</code> function, but I wonder why do we need it.</p><NewLine><p>Why is the weight of RNN non-contiguous on memory when we use nn.DataParallel?</p><NewLine><p>And also I found the error would be gone if we replace DataParallel with DistributedDataParallel, then why isn’t the weight non-contiguous in latter case?</p><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/7092"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://github.com/erogol"" rel=""nofollow noopener""><NewLine><img class=""thumbnail onebox-avatar"" height=""96"" src=""https://avatars1.githubusercontent.com/u/1402048?v=2&amp;s=96"" width=""96""/><NewLine></a><NewLine><h4><a href=""https://github.com/pytorch/pytorch/issues/7092"" rel=""nofollow noopener"" target=""_blank"">Issue: Multi-GPU autograd error with Pytorch 0.4</a></h4><NewLine><div class=""date"" style=""margin-top:10px;""><NewLine><div class=""user"" style=""margin-top:10px;""><NewLine>	opened by <a href=""https://github.com/erogol"" rel=""nofollow noopener"" target=""_blank"">erogol</a><NewLine>	on <a href=""https://github.com/pytorch/pytorch/issues/7092"" rel=""nofollow noopener"" target=""_blank"">2018-04-30</a><NewLine></div><NewLine><div class=""user""><NewLine></div><NewLine></div><NewLine><pre class=""content"" style=""white-space: pre-wrap;"">After updating pytorch 0.4 I am getting the following error when I try to train my model here: https://github.com/mozilla/TTS with multi-gpus....</pre><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">todo</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>I found some similar questions but none of them had the answer.<br/><NewLine><aside class=""quote"" data-post=""1"" data-topic=""29186""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v4/letter/b/76d3ee/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/what-does-flatten-parameters-do/29186"">What does flatten_parameters() do?</a><NewLine></div><NewLine><blockquote><NewLine>    I saw many Pytorch examples using flatten_parameters in the forward function of the RNN <NewLine>self.rnn.flatten_parameters()<NewLine><NewLine>I saw this <a href=""https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html"" rel=""nofollow noopener"">RNNBase</a> and it is written that it <NewLine><NewLine>Resets parameter data pointer so that they can use faster code paths <NewLine><NewLine>What does that mean?<NewLine>  </blockquote><NewLine></aside><NewLine><br/><NewLine><aside class=""quote"" data-post=""1"" data-topic=""18585""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v4/letter/k/67e7ee/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/do-we-need-to-call-flatten-parameters-in-lstm-only-if-we-are-using-multi-gpus/18585"">Do we need to call flatten_parameters() in LSTM only if we are using Multi-GPUs?</a> <a class=""badge-wrapper bullet"" href=""/c/nlp""><span class=""badge-category-bg"" style=""background-color: #AB9364;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""">nlp</span></a><NewLine></div><NewLine><blockquote><NewLine>    I am trying to understand the use case for flatten_parameters(), when do we use it and what does it do? Is it used only if we are running our model on multiple GPUs?<NewLine>  </blockquote><NewLine></aside><NewLine></p><NewLine></div>",https://discuss.pytorch.org/u/wwiiiii,(Jeong TaeYeong),wwiiiii,"May 29, 2019,  7:50am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>After reading some related codes, I think I almost get it but still have few questions.<br/><NewLine>So what I understand is</p><NewLine><hr/><NewLine><p>Everytime when we make new RNN module instance, it <a href=""https://github.com/pytorch/pytorch/blob/c611630b9df4a25007dd299676b2bb88bf692784/torch/nn/modules/rnn.py#L72"" rel=""nofollow noopener"">allocates</a> new <code>w_ih, w_hh, b_ih, b_hh</code> tensors and register them as <code>Parameter</code> for each layer, direction.</p><NewLine><p>But it’s not guranteed that new tensors are contiguous on GPU memory, performance can be dropped due to the fragmentation. So we call <code>flatten_parameters</code> function at the end of constructor to <a href=""https://github.com/pytorch/pytorch/blob/c611630b9df4a25007dd299676b2bb88bf692784/torch/nn/modules/rnn.py#L90"" rel=""nofollow noopener"">aggregate all the weight tensors into continuous space of GPU memory</a>.</p><NewLine><p>This task is done as</p><NewLine><ol><NewLine><li><NewLine><a href=""https://github.com/pytorch/pytorch/blob/d1623f4cc9334ade7376b3685bb91d3071e3b418/aten/src/ATen/native/cudnn/RNN.cpp#L647"" rel=""nofollow noopener"">Allocate</a> one big buffer tensor called <code>weight_buf</code><NewLine></li><NewLine><li><NewLine><a>Copy</a> values of weight tensor into <code>weight_buf</code><NewLine></li><NewLine><li>Make each weight tensor’s internal data pointer <a href=""https://github.com/pytorch/pytorch/blob/d1623f4cc9334ade7376b3685bb91d3071e3b418/aten/src/ATen/native/cudnn/RNN.cpp#L469"" rel=""nofollow noopener"">indicating</a> <code>weight_buf + offset</code><NewLine></li><NewLine></ol><NewLine><p>(The real execution steps are 1-&gt;3-&gt;2 in real code)</p><NewLine><p>But when we use nn.DataParallel, it <a>replicates</a> original module(which is allocated only on certain GPU device) to every GPU it uses, then weight tensors are fragmented again since there’s no gurantee that replicated tensors are still contiguous on memory space.</p><NewLine><p>Therefore we should <code>flatten_parameters</code> again everytime the module is replicated to another GPU, and the best place to put function call would be the head of <code>forward</code> function (of nn.Module), because <code>forward</code> function of <code>nn.Module</code> on each GPU is called only one time when <code>forward</code> of <code>nn.DataParallel</code> is called.</p><NewLine><p>Although I never used <code>nn.DistributedDataParallel</code>, I guess that the reason why it doesn’t need the <code>flatten_parameters</code> call is because when it allocates new instance of RNN module, <code>flatten_parameters</code> are automatically called, then it doesn’t move internal data position on memory unlike <code>nn.DataParallel</code>, but it only copies some values into it.</p><NewLine><hr/><NewLine><p>And questions are</p><NewLine><ol><NewLine><li><NewLine><p>Do I understand right? Is there any misunderstood point?</p><NewLine></li><NewLine><li><NewLine><p>When we do the step 3 of aggregation(=Make each weight tensor’s internal data pointer indicating <code>weight_buf + offset</code>), we call the <a>get_parameters</a> function and it</p><NewLine><ul><NewLine><li>calls <a href=""https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnGetRNNLinLayerMatrixParams"" rel=""nofollow noopener"">cudnnGetRNNLinLayerMatrixParams</a> so that <code>matrix_pointer</code> indicates the GPU memory position of original, un-aggregated weight tensor,</li><NewLine><li><NewLine><a href=""https://github.com/pytorch/pytorch/blob/d1623f4cc9334ade7376b3685bb91d3071e3b418/aten/src/ATen/native/cudnn/RNN.cpp#L454"" rel=""nofollow noopener"">sets offset</a> as the difference of <code>matrix_pointer</code> and start of <code>weight_buf</code>,</li><NewLine><li>make internal data pointer of weight tensor <a href=""https://github.com/pytorch/pytorch/blob/d1623f4cc9334ade7376b3685bb91d3071e3b418/aten/src/ATen/native/cudnn/RNN.cpp#L469"" rel=""nofollow noopener"">indicating</a> <code>weight_buf + offset</code><NewLine></li><NewLine></ul><NewLine><p>Then isn’t it indicating <code>matrix_pointer</code> again?    Why don’t we replcate</p><NewLine><p><code>Tensor param = at::empty({0}, weight_buf.options()).set_(weight_buf.storage(), offset, size);</code><br/><NewLine>with</p><NewLine><p><code>Tensor param = at::empty({0}, weight_buf.options()).set_(weight_buf.storage(), cumsum, size); cumsum += size;</code><br/><NewLine>?<br/><NewLine>Or does that function calculate expected position of given component with respect to the given (start) data pointer?</p><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""46506""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/wwiiiii/40/6613_2.png"" width=""20""/> wwiiiii:</div><NewLine><blockquote><NewLine><p>Therefore we should <code>flatten_parameters</code> again everytime the module is replicated to another GPU, and the best place to put function call would be the head of <code>forward</code> function (of nn.Module), because <code>forward</code> function of <code>nn.Module</code> on each GPU is called only one time when <code>forward</code> of <code>nn.DataParallel</code> is called.</p><NewLine></blockquote><NewLine></aside><NewLine><p>That’s the conclusion I came to as well, except that I actually observe a larger VRAM usage and loss compute time when I put <code>flatten_parameters</code> in the forward pass (and I get no warning)  vs. putting it in the <code>__init__</code> function of the model (and then I get the warning only when using DataParallel).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wwiiiii; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Valiox; <NewLine> ,"REPLY_DATE 1: May 30, 2019,  7:16am; <NewLine> REPLY_DATE 2: November 6, 2019,  2:36pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
60097,Parallelizing a loss function over CPU cores,2019-11-05T18:32:40.353Z,0,458,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone,</p><NewLine><p>I’m training a sequence-based model on a single machine with 1 GPU and 16 CPU cores. My loss function is computationally expensive and performs best on the CPU. The reason it performs well on a CPU is that computing this particular loss is a sequential process and, while it can’t be parallelized/GPU optimized for a single data point, it can be parallelized across a batch of samples. The rest of the model works well on a GPU.</p><NewLine><p><strong>I’d like to know how to parallelize the loss of my model over a batch of samples and CPU cores while keeping the rest of my model on the GPU.</strong> I know this will involve transferring the model’s predictions from GPU - &gt; CPU, but I believe the performance increase by using CPUs for the loss will outweigh this fact.</p><NewLine><p>I’ve tried <code>torch.multiprocessing</code> as follows (where <code>single_item_loss</code> is the loss function I described above, that takes a single model prediction from the batch of predictions and returns a single Torch float tensor representing the loss):</p><NewLine><pre><code class=""lang-python"">    with multiprocessing.Pool(multiprocessing.cpu_count()) as p:<NewLine>        results = p.map(single_item_loss, predictions)<NewLine></code></pre><NewLine><p>but this yields the error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue).<NewLine></code></pre><NewLine><p>I’ve also tried using the <code>joblib</code> library:</p><NewLine><pre><code class=""lang-auto"">results = Parallel(n_jobs=multiprocessing.cpu_count(), <NewLine>            backend=""loky"")(delayed(single_item_loss)(pred) for pred in predictions)<NewLine></code></pre><NewLine><p>but this must sever each of the idividual losses from the computation graph, because calling <code>torch.mean(torch.stack(results)).backward()</code> (and later <code>optimizer.step()</code>) has no effect and the model does not train.</p><NewLine><p>I’ve also tried calling <code>loss.backward()</code> before returning <code>loss</code> from my <code>single_item_loss</code> function, but this also does not work.</p><NewLine><p>I’m eager to hear any feedback and work to solve this problem. Evidently this is not a very common situation as GPUs are almost always preferred, but in this case, I believe the custom loss function warrants this special treatment. Thank you very much for your help!</p><NewLine></div>",https://discuss.pytorch.org/u/jonathanking,(Jonathan King),jonathanking,"November 5, 2019,  6:33pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>So currently this isn’t supported.</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""60097""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/jonathanking/40/16256_2.png"" width=""20""/> jonathanking:</div><NewLine><blockquote><NewLine><p>RuntimeError: Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries. If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue).</p><NewLine></blockquote><NewLine></aside><NewLine><p>You could implement forward and backward separately and stick it in a autograd.Function. Then inside the forward, you don’t have grad-requiring things.<br/><NewLine>For a loss function in particular, you could compute the backward inside the forward as well (you can turn also autograd back on inside the forward with <code>torch.enable_grad</code>).<br/><NewLine>Then in the backward you just hand back the pre-computed result. (This is what CuDNN’s CTC Loss implementation does.)</p><NewLine><p>It might be more efficient in terms of development time as well as runtime to drop into C++ and use PyTorch’s <code>parallel_for</code>, though.</p><NewLine><p>Note that PyTorch tries to use intra-op parallelism, you would not want to “overbook” your cores.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: November 6, 2019,  8:16am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
59636,DistributedDataParallel imbalanced GPU memory usage,2019-10-31T02:02:03.448Z,3,863,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there,<br/><NewLine>I’m trying to train my network wrapped with DistributedDataParallel on a single machine with 4 GPUs. It went smoothly until the 43rd epoch. The training process was interrupted by CUDA out of memory error on GPU 2.</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""train_ddp.py"", line 247, in &lt;module&gt;<NewLine>    trainer.training(epoch)<NewLine>  File ""train_ddp.py"", line 171, in training<NewLine>    iter_loss.backward()<NewLine>  File ""/scratch/workspace/zsding/anaconda3/lib/python3.6/site-packages/torch/tensor.py"", line 107, in backward<NewLine>    torch.autograd.backward(self, gradient, retain_graph, create_graph)<NewLine>  File ""/scratch/workspace/zsding/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 93, in backward<NewLine>    allow_unreachable=True)  # allow_unreachable flag<NewLine>RuntimeError: CUDA out of memory. Tried to allocate 752.00 MiB (GPU 2; 15.77 GiB total capacity; 10.24 GiB already allocated; 518.25 MiB free; 785.63 MiB cached)<NewLine></code></pre><NewLine><p>Then I shrank the input size and resumed from my previous weight to try to debug the memory footprint. The chart below shows that there were three extra python threads running and occupying 1080 mib<br/><NewLine>on GPU 2. And I find that they shared same PID with the threads on other GPUs.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/7e91cf550013579d65d7ee80dd1ef4d7b87210c7"" href=""https://discuss.pytorch.org/uploads/default/original/3X/7/e/7e91cf550013579d65d7ee80dd1ef4d7b87210c7.png"" title=""Capture.PNG""><img alt=""Capture"" data-base62-sha1=""i3Gyz8eJqCbMyB2rycyBsZmKR0z"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/7/e/7e91cf550013579d65d7ee80dd1ef4d7b87210c7_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/original/3X/7/e/7e91cf550013579d65d7ee80dd1ef4d7b87210c7.png"" width=""588""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Capture.PNG</span><span class=""informations"">954×811 32.6 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine>And of course, each GPU has only one thread during the first training epoch. No GPU specific operation (like <code>.to(2)</code>) used in my train script, but I applied <code>SyncBatchNorm</code> on my model (can it be the reason?).<br/><NewLine>How can I figure out what are those three threads? Could you provide some solutions to solve this problem?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/txfs1926,(Jiang),txfs1926,"October 31, 2019,  2:33am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, Is it possible for you to provide a snippet of your code/a way to reproduce the issue that you are seeing? Similar to <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/dataparallel-imbalanced-memory-usage/22551"">DataParallel imbalanced memory usage</a>, it could be the case that the outputs of your <code>forward</code> pass are being gathered onto a single GPU (GPU 2 in your case), causing it to OOM.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, thanks for replying.<br/><NewLine>Here is the forward pass part. I don’t know if it is helpful.</p><NewLine><pre><code class=""lang-auto"">    def training(self, epoch):<NewLine>        train_loss = 0.0<NewLine>        self.model.train()<NewLine>        tbar = tqdm(self.trainloader)<NewLine>        for i, (image, target) in enumerate(tbar):<NewLine>            image, target = image.to(self.device), target.to(self.device)<NewLine>            self.scheduler(self.optimizer, i, epoch, self.best_pred)<NewLine>            self.optimizer.zero_grad()<NewLine>            outputs = self.model(image)<NewLine><NewLine>            # multi-scale training<NewLine>            iter_loss = 0<NewLine>            for logit in outputs:<NewLine>                _, _, H, W = logit.shape<NewLine>                labels_ = utils.resize_labels(target, size=(H, W))<NewLine>                iter_loss += self.criterion(logit.cuda(), labels_.cuda())<NewLine><NewLine>            torch.cuda.empty_cache()<NewLine>            iter_loss.backward()<NewLine>            self.optimizer.step()<NewLine>            train_loss += iter_loss.item()<NewLine>            tbar.set_description('Train loss: %.3f' % (train_loss / (i + 1)))<NewLine></code></pre><NewLine><p>And there is no <code>gather</code> operation explicitly used in my code.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, thanks for your reply!<br/><NewLine>Here is the forward pass part in the script. I don’t know whether it is helpful.</p><NewLine><pre><code class=""lang-auto"">def training(self, epoch):<NewLine>    train_loss = 0.0<NewLine>    self.model.train()<NewLine>    tbar = tqdm(self.trainloader)<NewLine>    for i, (image, target) in enumerate(tbar):<NewLine>        image, target = image.to(self.device), target.to(self.device)<NewLine>        self.scheduler(self.optimizer, i, epoch, self.best_pred)<NewLine>        self.optimizer.zero_grad()<NewLine>        outputs = self.model(image)<NewLine><NewLine>        # Multi-size training<NewLine>        iter_loss = 0<NewLine>        for logit in outputs:<NewLine>            _, _, H, W = logit.shape<NewLine>            labels_ = utils.resize_labels(target, size=(H, W))<NewLine>            iter_loss += self.criterion(logit.cuda(), labels_.cuda())<NewLine><NewLine>        torch.cuda.empty_cache()<NewLine>        iter_loss.backward()<NewLine>        self.optimizer.step()<NewLine>        train_loss += iter_loss.item()<NewLine>        tbar.set_description('Train loss: %.3f' % (train_loss / (i + 1)))<NewLine></code></pre><NewLine><p>And I’ve not applied <code>gather</code> function (like <code>torch.nn.parallel.scatter_gather.gather</code>) explicitly.</p><NewLine><p>thanks!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry for the late reply here. Just to confirm, are you spawning a single process per device (gpu)?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/rvarm1; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/txfs1926; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/txfs1926; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/rvarm1; <NewLine> ,"REPLY_DATE 1: October 31, 2019,  2:32am; <NewLine> REPLY_DATE 2: October 31, 2019,  2:41am; <NewLine> REPLY_DATE 3: October 31, 2019,  6:14am; <NewLine> REPLY_DATE 4: November 5, 2019,  9:48pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
59905,Distributed Pytorch with Existing MPI Processes,2019-11-03T23:45:36.869Z,0,106,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi All,</p><NewLine><p>I have already running MPI program in the distributed model. I want to use those processes and run a PyTorch model distributedly. Is there a straightforward way to do this?</p><NewLine><p>For instance, MPI_INIT() is already called from my programme, and I have a world_size of 4.<br/><NewLine>Is it possible to start a PyTorch distributed programme from this model?</p><NewLine><p>Adding more information:</p><NewLine><p>How to use the following option with Pytorch DistributedDataParallel model?</p><NewLine><p>store(Store, optional): Key/value store accessible to all workers, used<br/><NewLine>to exchange connection/address information.<br/><NewLine>Mutually exclusive with <code>init_method</code>.</p><NewLine><p>I am trying to see how to map my existing MPI processes to launch a distributed data-parallel model in Pytorch using existing MPI instances.</p><NewLine><p>Is there a way such that Pytorch distributed mode can consume existing MPI processes?</p><NewLine></div>",https://discuss.pytorch.org/u/Vibhatha_Abeykoon,(Vibhatha Abeykoon),Vibhatha_Abeykoon,"November 4, 2019, 11:47am",,,,,
59744,Accessing tensors present on different GPUs,2019-11-01T06:24:35.795Z,2,165,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey Folks,</p><NewLine><p>I am running my model on multiple gpus.And I have a tensor, which will be present on each Gpus, which I want to access. Now, I am looking to get hold of all these tensor, on all the gpus, and do some operation, in a synchronous fashion, and then broadcast the result on all the gpus, to be used in the next step.</p><NewLine><p>For Example: the tensor we are talking is <code>T</code>, with <code>T</code> being present in all cuda 0-3(for 4 gpus). Now, I need to get hold of this <code>T</code> tensor(which has different values at different gpu) and get some stat out of this, and then send back this stat to all gpus.</p><NewLine><p>Please suggest me how this can be achieved.</p><NewLine></div>",https://discuss.pytorch.org/u/nbansal90,(Nitin Kumar Bansal),nbansal90,"November 1, 2019,  6:24am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>The simple way I see to do this is the following:<br/><NewLine>You will have to first send all these Tensors to a common GPU. agregate the restults and compute your update. Then send the result back to each GPU.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks Alan! I was wondering how do I send all the tensors to one gpu, and perform operations, which I want to perform, before the value of any of the tensor changes in any of their respective gpus.</p><NewLine><p>Meaning, I don’t want the values of these tensor present on different gpus to change, before I complete my operation and send them back to respective gpu.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Nitin,</p><NewLine><p>To send all tensors to one GPU, you’d want to use <code>dist.gather</code>, which will gather all of the tensors onto one gpu (this is assuming you have one process running per gpu). If your tensor is <code>t</code>, then your call would look like:</p><NewLine><pre><code class=""lang-auto"">t = your_tensor(size)<NewLine>if rank == 0:<NewLine># rank 0 is the node all tensors will be gathered on<NewLine>    gathered_tensors = [torch.zeros(size) for _ in range(WORLD_SIZE)]<NewLine>    dist.gather(t, gathered_tensors, dst=0)<NewLine>else:<NewLine>    dist.gather(t, dst=0)<NewLine>    <NewLine></code></pre><NewLine><p>Then, you can compute what you want and send the result back wrapped in a tensor via a <code>dist.scatter</code>. To make sure the tensors don’t change on the gpu before they are gathered, ensure all nodes call into <code>gather</code> at the same time when all nodes have the desired value. You could also use <code>torch.distributed.barrier</code> if you need additional synchronization. Check out the docs at <a href=""https://pytorch.org/docs/stable/distributed.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/distributed.html</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you Rohan for exhaustive explanation!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/nbansal90; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/rvarm1; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/nbansal90; <NewLine> ,"REPLY_DATE 1: November 1, 2019,  3:18pm; <NewLine> REPLY_DATE 2: November 1, 2019,  6:45pm; <NewLine> REPLY_DATE 3: November 2, 2019,  3:09am; <NewLine> REPLY_DATE 4: November 2, 2019,  3:09am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
59780,Training independent networks in parallel with reproducibility,2019-11-01T13:20:56.415Z,2,128,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I would like to train, say, 10 independent neural networks on 5 GPUs in parallel (by training two on each GPU assuming there are no memory constraints). Also, I would like the code to be reproducible. Therefore, I have been training each network by re-running the same Python script, changing only the GPU device. I specify <code>torch.manual_seed</code> for reproducibility.</p><NewLine><p>Is there anyway to do this in a single Python script where PyTorch does the work of distributing the networks to the GPUs <strong>while maintaining reproducibility</strong>? I am aware of threading as one way to do this (something similar to <a href=""https://github.com/waitwaitforget/modelparallel_pytorch"" rel=""nofollow noopener"">ModelParallel</a>), but I am worried about reproducibility of my code when using threading.</p><NewLine><p>Thank you for the help!</p><NewLine></div>",https://discuss.pytorch.org/u/pgf,,pgf,"November 1, 2019,  1:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Even with the same code, if you use different hardware/software versions, we don’t guarantee reproducibility. See the doc about this <a href=""https://pytorch.org/docs/stable/notes/randomness.html"" rel=""nofollow noopener"">here</a>.</p><NewLine><p>If you want to ensure that the scripts are independent. I would recommend using a simple batch script that launch all the jobs <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply! I have seen the documentation on reproducibility.</p><NewLine><p>In this case, to begin with, I am only looking for reproducibility for my particular set-up (hardware and software). I am indeed using a shell script to launch the jobs, but I thought it would be neater if I could do this all within one script with PyTorch responsible for the distribution.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The level of “independence” between the run would be much smaller if you run the whole think in a single process. They will share the same python interpreter, the same cuda allocator, same memory space.</p><NewLine><p>The ModelParallel tool that you linked seems interesting. But I don’t know of any other work along those lines…</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pgf; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: November 1, 2019,  3:14pm; <NewLine> REPLY_DATE 2: November 1, 2019,  3:55pm; <NewLine> REPLY_DATE 3: November 1, 2019,  4:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
59342,Gradient scaling in federated learning,2019-10-28T02:38:13.915Z,2,215,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am using torch.distributed to do federated learning. What should I do if I want to scale some workers’ gradient since their data are more valuable than others? Thx a lot in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/sherdencooper,,sherdencooper,"October 28, 2019,  2:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>tensor.register_hook(customHook) may work, you need to write customHook to modify grad of the tensor.</p><NewLine><p>but as far as I know customHook should be a function of grad only. For your case, you want to make customHook to be a function of grad and workerRank as well?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I only want to make customHook to be a function of grad and I haven’t used this before. Does <em>hook</em> only influence the backward process?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>yes, only for backward pass</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Yanli_Zhao; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/sherdencooper; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Yanli_Zhao; <NewLine> ,"REPLY_DATE 1: November 1, 2019, 11:12am; <NewLine> REPLY_DATE 2: October 31, 2019,  7:58am; <NewLine> REPLY_DATE 3: October 31, 2019,  5:04pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
59304,Problem with model accuracy (after restore) on TPU,2019-10-27T08:21:18.130Z,1,488,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m training GPT-2 from huggingface/transformers on TPU. It’s training well. At the end of a training I’ve got loss around 4.36. When I save and restore the model - the loss skyrockets somewhere to 9.75.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/afe9a6e9e511e8bedd41a133ecd80a65ac4c233e"" href=""https://discuss.pytorch.org/uploads/default/original/3X/a/f/afe9a6e9e511e8bedd41a133ecd80a65ac4c233e.png"" title=""1.png""><img alt=""1"" data-base62-sha1=""p6c9zGfFB03RanSGCsxqltJTstg"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/f/afe9a6e9e511e8bedd41a133ecd80a65ac4c233e_2_10x10.png"" height=""282"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/f/afe9a6e9e511e8bedd41a133ecd80a65ac4c233e_2_690x282.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/f/afe9a6e9e511e8bedd41a133ecd80a65ac4c233e_2_690x282.png, https://discuss.pytorch.org/uploads/default/optimized/3X/a/f/afe9a6e9e511e8bedd41a133ecd80a65ac4c233e_2_1035x423.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/a/f/afe9a6e9e511e8bedd41a133ecd80a65ac4c233e_2_1380x564.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">1.png</span><span class=""informations"">1631×667 59.1 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/c2a852fcc83b91294cad92a24e167fcca219885e"" href=""https://discuss.pytorch.org/uploads/default/original/3X/c/2/c2a852fcc83b91294cad92a24e167fcca219885e.png"" title=""2.png""><img alt=""2"" data-base62-sha1=""rM1gfEFQQhBUIYFBdom1KnxVj6S"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/2/c2a852fcc83b91294cad92a24e167fcca219885e_2_10x10.png"" height=""124"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/2/c2a852fcc83b91294cad92a24e167fcca219885e_2_690x124.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/2/c2a852fcc83b91294cad92a24e167fcca219885e_2_690x124.png, https://discuss.pytorch.org/uploads/default/optimized/3X/c/2/c2a852fcc83b91294cad92a24e167fcca219885e_2_1035x186.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/c/2/c2a852fcc83b91294cad92a24e167fcca219885e_2_1380x248.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">2.png</span><span class=""informations"">1722×310 33.5 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>I’ve got no similar issues with saving and loading on GPU with that code.</p><NewLine><p>The code what is used to save is just this</p><NewLine><p><code>xm.save(model_to_save.state_dict(), output_model_file)</code></p><NewLine><p><code>xm.save</code> is a convinience what moves tensors from TPU to CPU before saving.</p><NewLine><p>The whole code is here <a href=""https://github.com/mgrankin/ru_transformers/blob/master/tpu_lm_finetuning.py"" rel=""nofollow noopener"">https://github.com/mgrankin/ru_transformers/blob/master/tpu_lm_finetuning.py</a></p><NewLine><p>I’ve tried to do the following.</p><NewLine><ol><NewLine><li>I’ve tried to do save and load right after the training</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">    results = evaluate(args, model, tokenizer, ""checkpoint-0"", False)<NewLine>    log_info(f""Eval1 {results}"")<NewLine>    <NewLine>    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config)<NewLine>    model.to(args.device)<NewLine>    results = evaluate(args, model, tokenizer, ""checkpoint-0"", False)<NewLine>    log_info(f""Eval2 {results}"")<NewLine></code></pre><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/a57d4e8daba636ba8ac639b4887a92c5d4282ab3"" href=""https://discuss.pytorch.org/uploads/default/original/3X/a/5/a57d4e8daba636ba8ac639b4887a92c5d4282ab3.png"" title=""3.png""><img alt=""3"" data-base62-sha1=""nBZfA2iFPIVfjL7w6XtxWSlxCr9"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/5/a57d4e8daba636ba8ac639b4887a92c5d4282ab3_2_10x10.png"" height=""300"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/5/a57d4e8daba636ba8ac639b4887a92c5d4282ab3_2_690x300.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/5/a57d4e8daba636ba8ac639b4887a92c5d4282ab3_2_690x300.png, https://discuss.pytorch.org/uploads/default/optimized/3X/a/5/a57d4e8daba636ba8ac639b4887a92c5d4282ab3_2_1035x450.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/a/5/a57d4e8daba636ba8ac639b4887a92c5d4282ab3_2_1380x600.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">3.png</span><span class=""informations"">1670×728 88.3 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>Eval2 is much bigger that Eval1</p><NewLine><ol start=""2""><NewLine><li>I also tried not to recreate the model, but to replace model state_dict with saved state_dict</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">    results = evaluate(args, model, tokenizer, ""checkpoint-0"", False)<NewLine>    log_info(f""Eval1 {results}"")<NewLine>    <NewLine>    model.load_state_dict(torch.load('output/classic_s/pytorch_model.bin'))<NewLine>    model.to(args.device)<NewLine>    results = evaluate(args, model, tokenizer, ""checkpoint-0"", False)<NewLine>    log_info(f""Eval2 {results}"")<NewLine></code></pre><NewLine><p>In that case Eval2 is equal to Eval1</p><NewLine><p>So, there is something that isn’t in a state_dict, but it affects the model perfomance. What can that be?</p><NewLine></div>",https://discuss.pytorch.org/u/Grankin,(Mikhail Grankin),Grankin,"October 27, 2019,  8:21am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Skimming through your code, it looks like you are using <code>AdamW</code> as your optimizer, which uses internal states. To be able to properly resume your training, you should also store/restore the optimizer’s <code>state_dict</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for a valuable advice on saving the state of AdamW. But that is not the root problem here. I don’t run train after save/load and it’s performing way worse right after loading.</p><NewLine><p>Davide Libenzi is trying hard to help me with the issue here <a href=""https://github.com/pytorch/xla/issues/1245"" rel=""nofollow noopener"">https://github.com/pytorch/xla/issues/1245</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The issue is mostly resolved here<br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/xla/issues/1245#issuecomment-547777404"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/xla</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/xla/issues/1245#issuecomment-547777404"" rel=""nofollow noopener"" target=""_blank"">Problem with model accuracy (after restore) on TPU</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-10-27"" data-format=""ll"" data-time=""09:05:48"" data-timezone=""UTC"">09:05AM - 27 Oct 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/mgrankin"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""mgrankin"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars0.githubusercontent.com/u/3540879?v=4"" width=""20""/><NewLine>          mgrankin<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">🐛 Bug<NewLine>I’m training GPT-2 from huggingface/transformers on TPU. It’s training well. At the end of a training I’ve got loss around...</p><NewLine></div><NewLine><div class=""labels""><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Grankin; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Grankin; <NewLine> ,"REPLY_DATE 1: October 27, 2019,  3:02pm; <NewLine> REPLY_DATE 2: October 28, 2019,  6:20pm; <NewLine> REPLY_DATE 3: October 30, 2019,  7:52am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
59367,Parallel For Loop for parallelized sub computation in a gradient step,2019-10-28T10:06:29.317Z,0,110,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I have a simple problem (hopefully!) regarding parallelizing a part of my model. I have looked around but cannot seem to find a definitive answer on how to approach this although similar questions seem to have been asked.</p><NewLine><p>TLDR: How do you do a <em>parallel for loop</em> across multiple CPUs or GPUs in the same computer in the middle of a gradient step?</p><NewLine><p>What I have is multiple additional computations which I know are embarassingly parallelizable, but compute bound. Currently, I am calculating them sequentially in a for loop within a .fit() function. These results are accumulated and then combined to produce the final loss.</p><NewLine><p>A code sketch is as follows:</p><NewLine><pre><code class=""lang-auto"">for i in range(epochs):  <NewLine>     self.optimizer.zero_grad()<NewLine>     loss = self.fit_get_loss()<NewLine>     loss.backward(retain_graph=False)<NewLine><NewLine>def fit_get_loss():<NewLine>     # This is the for loop to parallelize<NewLine>     total_loss = 0<NewLine>     for j in range(self.N_extra_models):<NewLine>         m1 = self.extra_models_1[j]<NewLine>         m2 = self.extra_models_2[j]<NewLine>         loss1 = m1.fit_get_loss(with_grad=True)<NewLine>         loss2 = m2.fit_get_loss(with_grad=True)<NewLine>         total_loss = total_loss + loss1 + loss2<NewLine>     return total_loss<NewLine><NewLine></code></pre><NewLine><p>I would like to distribute the computation across many CPUs (e.g. a workstation with 20 cores) such that , for example, i have 20 of those j iterations occuring in parallel and I just accumulate the loss value. This is preferrable to do on CPU given the hardware I currently have available. However I am also keen to know how to apply the sample problem to multiple GPUs.</p><NewLine><p>Actually, the  real problem I have is more complicated than the above, and I need to re-use the extra_models but I think the above is the simplest form of what i’m trying to achieve. The extension of the problem is that I would like to access the updated fitted values for each of those extra_models within the main optimization loop call.</p><NewLine><p>thanks again</p><NewLine></div>",https://discuss.pytorch.org/u/MushroomHunting,,MushroomHunting,"October 28, 2019, 10:18am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>You can use python’s native threading to achieve this. I am not very familiar with it <img alt="":confused:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=9"" title="":confused:""/></p><NewLine><p>Keep in mind though that if you perform big enough operations in pytorch’s operations, it will use multiple threads automatically.<br/><NewLine>If you just have many many python code to run. multiple threads won’t help you, because you can only run one thread at a time in python (you can google for the GIL).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: October 28, 2019,  2:10pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
42687,How to create multiple DistributedDataParallel tasks on a single node,2019-04-16T04:16:49.816Z,2,2299,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m trying to start 2 training tasks on a single node and I want each task to occupy 2 GPUs respectively. Everything is fine with task 1. However, when I try to launch task 2, I encounter the following error:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""dist_test.py"", line 93, in &lt;module&gt;<NewLine>    train()<NewLine>  File ""dist_test.py"", line 62, in train<NewLine>    init_method='env://',<NewLine>  File ""/data0/whr/anaconda3/envs/torch1.0/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 354, in init_process_group<NewLine>    store, rank, world_size = next(rendezvous(url))<NewLine>  File ""/data0/whr/anaconda3/envs/torch1.0/lib/python3.6/site-packages/torch/distributed/rendezvous.py"", line 143, in _env_rendezvous_handler<NewLine>    store = TCPStore(master_addr, master_port, start_daemon)<NewLine>RuntimeError: Address already in use<NewLine>Traceback (most recent call last):<NewLine>  File ""dist_test.py"", line 93, in &lt;module&gt;<NewLine>    train()<NewLine>  File ""dist_test.py"", line 69, in train<NewLine>    output_device = args.local_rank<NewLine>  File ""/data0/whr/anaconda3/envs/torch1.0/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 215, in __init__<NewLine>    self.broadcast_bucket_size)<NewLine>  File ""/data0/whr/anaconda3/envs/torch1.0/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 377, in _dist_broadcast_coalesced<NewLine>    dist._dist_broadcast_coalesced(self.process_group, tensors, buffer_size, False)<NewLine>RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1544081127912/work/torch/lib/c10d/ProcessGroupNCCL.cpp:260, unhandled system error<NewLine></code></pre><NewLine><p>Because I want to use sync batch norm which is only supported with DistributedDataParallel, so unfortunatelly I cannot switch to DataParallel. It’ll be best if somebody could tell me whether it’s possible to run 2 DistributedDataParallel tasks on a single node at the same time.</p><NewLine><p>Here is an example code snippet for reproducing this problem:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import time<NewLine><NewLine>import argparse<NewLine>import os<NewLine><NewLine>def parse_args():<NewLine>    parse = argparse.ArgumentParser()<NewLine>    parse.add_argument(<NewLine>            '--local_rank',<NewLine>            dest = 'local_rank',<NewLine>            type = int,<NewLine>            default = 0,<NewLine>            )<NewLine>    parse.add_argument(""--gpu"", type=str, default='None',<NewLine>                        help=""choose gpu device."")<NewLine>    return parse.parse_args()<NewLine><NewLine><NewLine>class Net(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Net, self).__init__()<NewLine>        self.conv1 = nn.Conv2d(3,<NewLine>            64,<NewLine>            kernel_size = 3,<NewLine>            stride = 2,<NewLine>            padding = 1,)<NewLine>        self.conv2 = nn.Conv2d(64,<NewLine>            256,<NewLine>            kernel_size = 3,<NewLine>            stride = 2,<NewLine>            padding = 1,)<NewLine>        self.conv3 = nn.Conv2d(256,<NewLine>            512,<NewLine>            kernel_size = 3,<NewLine>            stride = 2,<NewLine>            padding = 1,)<NewLine>        #self.linear = nn.Linear(512, 10)<NewLine><NewLine>    def forward(self, x):<NewLine>        H, W = x.size()[2:]<NewLine>        x = self.conv1(x)<NewLine>        x = self.conv2(x)<NewLine>        logits = self.conv3(x)<NewLine>        logits = F.interpolate(logits, (H, W), mode='bilinear')<NewLine>        return logits<NewLine><NewLine><NewLine>def train():<NewLine>    args = parse_args()<NewLine>    if not args.gpu == 'None':<NewLine>        device = torch.device(""cuda"")<NewLine>        os.environ[""CUDA_VISIBLE_DEVICES""]=args.gpu<NewLine>    else:<NewLine>        device = torch.device(""cpu"")<NewLine><NewLine>    torch.cuda.set_device(args.local_rank)<NewLine>    torch.distributed.init_process_group(<NewLine>                backend='nccl',<NewLine>                init_method='env://',<NewLine>                )<NewLine>    net = Net()<NewLine>    <NewLine>    net = net.to(device)<NewLine>    net = nn.parallel.DistributedDataParallel(net,<NewLine>            device_ids = [args.local_rank, ],<NewLine>            output_device = args.local_rank<NewLine>            )<NewLine>    net.train()<NewLine><NewLine>    optim = torch.optim.SGD(<NewLine>            net.parameters(),<NewLine>            lr = 1e-3,<NewLine>            momentum = 0.9,<NewLine>            weight_decay = 5e-4)<NewLine>    criteria = nn.CrossEntropyLoss()<NewLine><NewLine>    for i in range(10000):<NewLine>        img = torch.randn(2, 3, 128, 128).cuda()<NewLine>        lb = torch.randint(0, 19, [2,128,128]).cuda()<NewLine>        optim.zero_grad()<NewLine>        out = net(img)<NewLine>        loss = criteria(out, lb)<NewLine>        loss.backward()<NewLine>        loss_val = loss.item()<NewLine>        optim.step()<NewLine>        print(loss_val)<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    train()<NewLine></code></pre><NewLine><p>I run the following command for task 1 :</p><NewLine><pre><code class=""lang-auto"">python -m torch.distributed.launch --nproc_per_node 2 dist_test.py --gpu 0,1<NewLine></code></pre><NewLine><p>and the command for task 2:</p><NewLine><pre><code class=""lang-auto"">python -m torch.distributed.launch --nproc_per_node 2 dist_test.py --gpu 2,3<NewLine></code></pre><NewLine><p>Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/krumo,(Haoran Wang),krumo,"April 16, 2019,  4:16am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!</p><NewLine><p>See <a href=""https://pytorch.org/docs/stable/distributed.html#environment-variable-initialization"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/distributed.html#environment-variable-initialization</a> for an overview of the <code>env</code> initialization method. Also see the help output of the launch utility (run with <code>--help</code>). You’ll find that you’re silently trying to use the same port on localhost for the processes in a single task to find each other. Specify a different port for each task and it will work.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot for your answer! It totally solved my problem!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have set --master_port for each task, but RuntimeError raise up</p><NewLine><pre><code class=""lang-auto""> Expected to have finished reduction in the prior iteration before starting a new one<NewLine>RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing its output (the return value of `forward`). You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`. If you already have this argument set, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable). (prepare_for_backward at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:408)<NewLine></code></pre><NewLine><p><code>find_unused_parameters=True</code> don’t work</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you already have this argument set, then the distributed data parallel module wasn’t able to locate the output tensors in the return value of your module’s <code>forward</code> function. Please include the structure of the return value of <code>forward</code> of your module when reporting this issue (e.g. list, dict, iterable).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/krumo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/4e3c81574af4c3e6560b; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Yanli_Zhao; <NewLine> ,"REPLY_DATE 1: April 17, 2019,  1:59am; <NewLine> REPLY_DATE 2: April 17, 2019,  2:00am; <NewLine> REPLY_DATE 3: October 24, 2019, 11:34am; <NewLine> REPLY_DATE 4: October 24, 2019, 11:07pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
59027,Model parallel issue that disappears with CUDA_LAUNCH_BLOCKING=1,2019-10-23T12:33:14.324Z,0,220,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><p>I’m trying to parallelize a somewhat large encoder-decoder model. I have the input data on GPU 0 fed into the encoder, then I transfer the latent code to GPU 1 and feed it to the decoder, then compute the losses on GPU 1.</p><NewLine><p>One particular loss is implemented as a <code>torch.autograd.Function</code> and something triggers a device-side assert with out of bound indices in that snippet:</p><NewLine><pre><code class=""lang-auto"">batchV = V.view((-1, 3))<NewLine><NewLine># Compute half cotangents and double the triangle areas<NewLine>C, TwoA = half_cotangent(V, faces)<NewLine><NewLine>batchC = C.view((-1, 3))<NewLine><NewLine># Adjust face indices to stack:<NewLine>offset = torch.arange(V.shape[0], device=V.device).view((-1, 1, 1)) * V.shape[1]<NewLine><NewLine># Add the offset to the faces passed as parameters and save in a different tensor<NewLine>F = faces + offset<NewLine>batchF = F.view((-1, 3))<NewLine><NewLine># import ipdb; ipdb.set_trace()<NewLine># Fails here if not run with CUDA_LAUNCH_BLOCKING=1<NewLine>rows = batchF[:, [1, 2, 0]].view(<NewLine>    1, -1<NewLine>)  # 1,2,0 i.e to vertex 2-3 associate cot(23)<NewLine>cols = batchF[:, [2, 0, 1]].view(<NewLine>    1, -1<NewLine>)<NewLine></code></pre><NewLine><p>The code runs fine for 2 samples, then on the third I get the device-side assert. The dataloader is shuffling the samples, yet it consistently fails on the third sample.</p><NewLine><p>Debugging in pdb gives me the following traceback:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""train.py"", line 593, in &lt;module&gt;<NewLine>    exp_flag,<NewLine>  File ""train.py"", line 524, in compute_losses_real<NewLine>    exp_real_and,<NewLine>  File ""&lt;thefile&gt;.py"", line 23, in __call__<NewLine>    Lx = self.laplacian.apply(V, self.F[mask])<NewLine>  File ""&lt;thefile&gt;.py"", line 64, in forward<NewLine>    rows = batchF[:, [1, 2, 0]].view(<NewLine>RuntimeError: size is inconsistent with indices: for dim 0, size is 7380 but found index 4684893058448109737<NewLine></code></pre><NewLine><p>I then ran the same exact code with CUDA_LAUNCH_BLOCKING=1 and the code doesn’t crash, and the loss decreases.</p><NewLine><p>What I already tried:</p><NewLine><ul><NewLine><li><NewLine><p>In case this might be related, I disabled pinned memory and non-blocking data transfers from host to GPU, but the problem persists.</p><NewLine></li><NewLine><li><NewLine><p>I added <code>torch.cuda.synchronize()</code> right above <code>rows = batchF[:, [1, 2, 0]].view(</code> with no success.</p><NewLine></li><NewLine><li><NewLine><p>This code works fine when the model is on a single GPU.</p><NewLine></li><NewLine></ul><NewLine><p>Any help would be much appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/mbahri,(Mehdi Bahri),mbahri,"October 23, 2019, 12:34pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>It does look like we’re missing a synchronization point…<br/><NewLine>Could you provide a small code sample that triggers this issue so that we can reproduce locally please? <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, would you mind providing your complete code? from the snippet you provided, it is hard to say the root cause.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Yanli_Zhao; <NewLine> ,"REPLY_DATE 1: October 23, 2019,  4:16pm; <NewLine> REPLY_DATE 2: October 24, 2019, 10:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
58802,torch.nn.DataParallel problem with new server,2019-10-21T11:57:50.614Z,3,1618,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I have a program leverage torch.nn.DataParallel to run on multiple GPUs. I tested it on a system with 3 GPUs==1080ti using pytorch==1.2 and cuda==10.0. Everything is perfect, program will run and leverage all 3 GPUs.</p><NewLine><p>Now, I’m going to run it on a new server with 3GPUs==2080ti and the same config of pytorch and cuda. I got the following error:</p><NewLine><pre><code class=""lang-auto"">File ""/nfs/brm/main.py"", line 384, in &lt;module&gt;<NewLine>    train_loss = model.fit(interactions=ds_train, verbose=True)<NewLine>  File ""/nfs/brm/implicit.py"", line 255, in fit<NewLine>    positive_prediction = self._net(batch_user, batch_item)<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 547, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py"", line 146, in forward<NewLine>    ""them on device: {}"".format(self.src_device_obj, t.device))<NewLine>RuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cuda:1<NewLine></code></pre><NewLine><p>The error is clear, it seems that some part of the model or inputs are in another GPU. But it’s not the case as it runs on another server perfectly. This is the way that I’m using DataParallel:</p><NewLine><pre><code class=""lang-auto"">        self.device = torch.device(<NewLine>            ""cuda"" if torch.cuda.is_available() else ""cpu"")<NewLine>        self._net.to(self.device) #_net is my model<NewLine>        self._net = torch.nn.DataParallel(self._net)<NewLine><NewLine></code></pre><NewLine><p>Also I’m using the same way to move model’s input into GPUs (.to(self.device)).</p><NewLine><p>The program on the <strong>new server</strong> is run if I ask for only one GPU. But it fails when I ask for multiple (e.g.3 GPUs).</p><NewLine><p>Do you have any idea to investigate the problem?</p><NewLine></div>",https://discuss.pytorch.org/u/Amirhj,(Amir),Amirhj,"October 21, 2019, 11:57am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group quote-modified"" data-post=""1"" data-topic=""58802""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/amirhj/40/16888_2.png"" width=""20""/> Amirhj:</div><NewLine><blockquote><NewLine><p>self.device = torch.device( “cuda” if torch.cuda.is_available() else “cpu”)</p><NewLine></blockquote><NewLine></aside><NewLine><p>Can you try this instead? This would ensure we always allocate parameters on device 0</p><NewLine><pre><code class=""lang-auto"">self.device = torch.device(<NewLine>            ""cuda:0"" if torch.cuda.is_available() else ""cpu"")<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your answer. It changes the error message to another one:</p><NewLine><pre><code class=""lang-auto"">File ""/nfs/brm/main.py"", line 385, in &lt;module&gt;<NewLine>    train_loss = model.fit(interactions=ds_train, verbose=True)<NewLine>  File ""/nfs/brm/implicit.py"", line 255, in fit<NewLine>    positive_prediction = self._net(batch_user, batch_item)<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 547, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py"", line 152, in forward<NewLine>    outputs = self.parallel_apply(replicas, inputs, kwargs)<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py"", line 162, in parallel_apply<NewLine>    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/parallel_apply.py"", line 85, in parallel_apply<NewLine>    output.reraise()<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/torch/_utils.py"", line 369, in reraise<NewLine>    raise self.exc_type(msg)<NewLine>RuntimeError: Caught RuntimeError in replica 1 on device 1.<NewLine>Original Traceback (most recent call last):<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/parallel_apply.py"", line 60, in _worker<NewLine>    output = module(*input, **kwargs)<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 547, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/nfs/brm/representations.py"", line 95, in forward<NewLine>    attention_mask=input_mask)[0]<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 547, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/transformers/modeling_distilbert.py"", line 592, in forward<NewLine>    head_mask=head_mask)<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 547, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/transformers/modeling_distilbert.py"", line 461, in forward<NewLine>    embedding_output = self.embeddings(input_ids)   # (bs, seq_length, dim)<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 547, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/transformers/modeling_distilbert.py"", line 92, in forward<NewLine>    word_embeddings = self.word_embeddings(input_ids)                   # (bs, max_seq_length, dim)<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 547, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py"", line 114, in forward<NewLine>    self.norm_type, self.scale_grad_by_freq, self.sparse)<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py"", line 1467, in embedding<NewLine>    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)<NewLine>RuntimeError: arguments are located on different GPUs at /pytorch/aten/src/THC/generic/THCTensorIndex.cu:397<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you share the code you’re running so that I can try to repro this locally and see what the issue might be?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Unfortunately, the source code depends on different modules and large data, it’s not useful for debugging purpose. In addition, the code perfectly run on other server when I connect to it via ssh and directly run my python.<br/><NewLine>The new server is based on Kubernetes and OpenShift and my code is deployed via a docker container. I think that it causes the misidentification of GPUs by DataParallel.<br/><NewLine>Did you have any related experiment?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are the inputs you feed to the model on the same device (cuda:0) when you run the training loop?</p><NewLine><p>Also, would it be possible for you to come up with a small example that reproduces the problem you’re seeing? It would be easier to debug the issue that way.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>It was a bug in CUDA10, just upgrading to CUDA 10.1 solved the problem.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Amirhj; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Amirhj; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Amirhj; <NewLine> ,"REPLY_DATE 1: October 21, 2019,  9:45pm; <NewLine> REPLY_DATE 2: October 21, 2019, 10:24pm; <NewLine> REPLY_DATE 3: October 22, 2019, 12:49am; <NewLine> REPLY_DATE 4: October 22, 2019,  8:55am; <NewLine> REPLY_DATE 5: October 22, 2019,  7:03pm; <NewLine> REPLY_DATE 6: October 24, 2019,  8:12am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
58032,DistributedDataParallel consumes much more gpu memory,2019-10-12T10:36:26.419Z,2,232,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Would DistributedDataParallel wrapper cost much GPU memory? In my case, the model cost around 7300MB when loaded into a GPU. However, when wrapped in DistributedDataParallel and run in the distributed mode, it costs 22000MB GPU momery.<br/><NewLine>Is it caused by the DistributedDataParallel wrapper? Are there any methods to save memory usage? Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/turboxin,,turboxin,"October 12, 2019, 10:36am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s a huge increase in memory. Which model and batch sizes are you using?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am using pointpillars from this repo <a href=""https://github.com/traveller59/second.pytorch"" rel=""nofollow noopener"">https://github.com/traveller59/second.pytorch</a>, with DataParallel changed to DistributedDataParallel, the batch size is 2 per gpu.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>How many nodes are you using and how many GPUs per node? Also, which communication backend are you using?</p><NewLine><p>Also, it might be helpful to debug if you could share the code you’re using to initialize and train using DistributedDataParallel?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/turboxin; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> ,"REPLY_DATE 1: October 16, 2019,  1:28am; <NewLine> REPLY_DATE 2: October 17, 2019,  8:36am; <NewLine> REPLY_DATE 3: October 22, 2019,  1:29am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
58603,Num_workers&gt;0 creates memory error in SLURM?,2019-10-18T20:23:10.102Z,0,215,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m trying to run my code on a SLURM cluster with the following configuration - 1 node, 2 Nvidia 1080Ti GPUs, 8 CPUs and 8GBs of RAM per CPU.</p><NewLine><p>I’m implementing ResNeXt, with a dataset that contains about 1million 32x32 images. When I try running this code with torchvision.datasets.ImageFolder and num_workers = 4-8, it throws an “exceeded virtual memory” error by requesting for 341GB of data! That seems a little absurd. This error is thrown at the first trainLoader loop as it is preparing the first batch for the code.</p><NewLine><p>Initially, I assumed this was an error with my program, but my program works just fine with num_workers = 8 on Google Colab. My program only works when I set num_workers=0. At num_workers=2, it works for 2 epochs before throwing the same error. Any solution for this would really be appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/Vedant_Sanil,(Vedant Sanil),Vedant_Sanil,"October 18, 2019,  8:23pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you using DistributedDataParallel or DataParallel for this? It seems like your question is more related to torchvision or the PyTorch dataset/dataloader (the ‘distributed’ tag is not appropriate for this). Maybe tag this with ‘vision’?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pritamdamania87; <NewLine> ,"REPLY_DATE 1: October 21, 2019,  9:55pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
58558,RuntimeError: Broken pipe using NVIDIA Megatron-LM,2019-10-18T08:23:30.379Z,3,278,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!<br/><NewLine>I’m experimenting with distributed training using NVIDIA <a href=""https://github.com/NVIDIA/Megatron-LM"" rel=""nofollow noopener"">Megatron-LM project</a>. And I get an error when running the script bash <a href=""https://github.com/NVIDIA/Megatron-LM/blob/master/scripts/pretrain_gpt2_model_parallel.sh"" rel=""nofollow noopener"">scripts/pretrain_gpt2_model_parallel.sh</a></p><NewLine><p>Traceback looks like</p><NewLine><pre><code class=""lang-auto"">File ""pretrain_gpt2.py"", line 625, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""pretrain_gpt2.py"", line 569, in main<NewLine>    args.eod_token = get_train_val_test_data(args)<NewLine>  File ""pretrain_gpt2.py"", line 536, in get_train_val_test_data<NewLine>    group=mpu.get_model_parallel_group())<NewLine>  File ""/home/ubuntu/Env/ml/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 810, in broadcast<NewLine>    work = group.broadcast([tensor], opts)<NewLine>RuntimeError: Broken pipe<NewLine>Traceback (most recent call last):<NewLine>  File ""pretrain_gpt2.py"", line 625, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""pretrain_gpt2.py"", line 569, in main<NewLine>    args.eod_token = get_train_val_test_data(args)<NewLine>  File ""pretrain_gpt2.py"", line 536, in get_train_val_test_data<NewLine>    group=mpu.get_model_parallel_group())<NewLine>  File ""/home/ubuntu/Env/ml/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 810, in broadcast<NewLine>    work = group.broadcast([tensor], opts)<NewLine>RuntimeError: Broken pipe<NewLine>Traceback (most recent call last):<NewLine>  File ""pretrain_gpt2.py"", line 625, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""pretrain_gpt2.py"", line 569, in main<NewLine>    args.eod_token = get_train_val_test_data(args)<NewLine>  File ""pretrain_gpt2.py"", line 536, in get_train_val_test_data<NewLine>    group=mpu.get_model_parallel_group())<NewLine>  File ""/home/ubuntu/Env/ml/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 810, in broadcast<NewLine>    work = group.broadcast([tensor], opts)<NewLine>RuntimeError: Broken pipe<NewLine>Traceback (most recent call last):<NewLine>  File ""pretrain_gpt2.py"", line 625, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""pretrain_gpt2.py"", line 569, in main<NewLine>    args.eod_token = get_train_val_test_data(args)<NewLine>  File ""pretrain_gpt2.py"", line 536, in get_train_val_test_data<NewLine>    group=mpu.get_model_parallel_group())<NewLine>  File ""/home/ubuntu/Env/ml/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 810, in broadcast<NewLine>    work = group.broadcast([tensor], opts)<NewLine>RuntimeError: Broken pipe<NewLine></code></pre><NewLine><p>The error occurs in the file  <a href=""https://github.com/NVIDIA/Megatron-LM/blob/master/pretrain_gpt2.py"" rel=""nofollow noopener"">pretrain_gpt2.py</a></p><NewLine><p>Could anybody help me with this issue?</p><NewLine></div>",https://discuss.pytorch.org/u/Jakson,(Evgeny Lepilov),Jakson,"October 18, 2019,  8:26am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I would recommend to create an issue in the GitHub repository directly, as the authors of the code might help there.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I did, but unfortunately I didn’t get an answer.<br/><NewLine>The error traceback refers to <code>lib/python3.6/site-packages/torch/distributed/distributed_c10d.py</code> . That’s why I thought I might be able to get a hint here.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I assume you’ve created <a href=""https://github.com/NVIDIA/Megatron-LM/issues/15"" rel=""nofollow noopener"">this issue</a>?<br/><NewLine>It looks like your datasets are empty and the actual error message is:</p><NewLine><pre><code class=""lang-python"">TypeError: iteration over a 0-d array<NewLine></code></pre><NewLine><p>when calculating the dataset lengths.<br/><NewLine>I’ll also post in the issue directly.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> That’s a great idea, I’ll check it out . Thank you</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Jakson; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Jakson; <NewLine> ,"REPLY_DATE 1: October 21, 2019, 12:11pm; <NewLine> REPLY_DATE 2: October 21, 2019, 12:23pm; <NewLine> REPLY_DATE 3: October 21, 2019, 12:43pm; <NewLine> REPLY_DATE 4: October 21, 2019, 12:56pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
57876,How to run network with multiple independent inputs in parallel in Pytorch?,2019-10-10T13:02:54.563Z,0,336,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a neural network with two separate vectors as inputs, similar to <a href=""https://stackoverflow.com/questions/51700729/how-to-construct-a-network-with-two-inputs-in-pytorch"" rel=""nofollow noopener"">this question</a>. Both inputs are encoded and then processed further. But until then, the encoding of those inputs is completely independent. How can I parallelize the encoding phase in pytorch? A minimal example of my code:</p><NewLine><pre><code class=""lang-auto"">class MyModel(nn.Module):<NewLine>    def __init__(params):<NewLine>        super(MyModel self).__init__()<NewLine><NewLine>        self.encoder1 = Encoder1()<NewLine>        self.encoder2 = Encoder2()<NewLine><NewLine>    def forward(x1, x2):<NewLine>        # how to calculate both encodings in parallel?<NewLine>        enc1 = self.encoder1(x1)<NewLine>        enc2 = self.encoder2(x2)<NewLine>        return some_func(enc1, enc2)<NewLine></code></pre><NewLine><p>I checked pytorch forums and learned a bit about DataParallel, but i am not sure how to fit it to my case and on only 1 GPU.</p><NewLine></div>",https://discuss.pytorch.org/u/Jan_Vainer,(Jan Vainer),Jan_Vainer,"October 10, 2019,  1:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>DataParallel won’t help here if there is only one GPU, but you could use multiple CUDA streams. For example,</p><NewLine><pre><code class=""lang-auto"">s0 = torch.cuda.Stream()<NewLine>s1 = torch.cuda.Stream()<NewLine>with torch.cuda.stream(s0):<NewLine>    enc1 = self.encoder1(x1)<NewLine><NewLine>with torch.cuda.stream(s1):<NewLine>    enc2 = self.encoder2(x2)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much! <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Jan_Vainer; <NewLine> ,"REPLY_DATE 1: October 11, 2019,  3:23am; <NewLine> REPLY_DATE 2: October 13, 2019,  6:42pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
57962,Best way to debug DistributedDataParallel code?,2019-10-11T12:22:15.161Z,0,652,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey,</p><NewLine><p>I’m having an issue that my code randomly hangs when using DistributedDataParallel.<br/><NewLine>It is completely random when this occurs, and it does not always occur.<br/><NewLine>I suspect that it has something to do with the DistributedDataParallel as out of the 4 gpu’s I’m using, 3 are reporting to be using 100% of that gpu and 1 is completely idle.</p><NewLine><p>What is the best way for me to debug what is going on as I’m getting no errors?</p><NewLine></div>",https://discuss.pytorch.org/u/Mxbonn,(Maxim),Mxbonn,"October 11, 2019, 12:22pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like the program somehow desynchronized (different processes wants to sync different amount of parameters or run different numbers of iterations). Unfortunately, DDP does not have a debug mode for now. Can you share some code snippet? Does your code tries to handle any errors in backward pass on its own (say catch OOM and rertry)? Does all processes see exactly the same amount of input data?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve had to deal with similar issues, you should feel lucky you only have 4 processes and not 64 <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>The fact that 3 are in 100% utilization means that are inside nccl sync operation like the one at the end of backward .backward(), while the 4th one is doing something non-GPU related, like waiting for user input.</p><NewLine><p>The general strategy is to look at stack trace of the “odd one out” process. You can get C++ stack trace by doing ""gdb -p "" and “bt” or “thread apply all bt”</p><NewLine><p>With a bit more work, you can get Python stack. This requires modifying client code. For instance, I run <a href=""https://gist.github.com/yaroslavvb/5de8ad332f2e37aa542e69e208476433"" rel=""nofollow noopener"">install_pdb_hander</a> on init of my processes. It allows me to break into PDB on CTRL+\ and look what the current process is doing.</p><NewLine><p>When using distributed launcher, this will only send CTRL+\ to the launcher process, so for this to get you stack trace of arbitrary worker you could need to modify your launching procedure to run in 4 worker in 4 different tmux windows</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>related issue <a href=""https://github.com/pytorch/pytorch/issues/27757"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/27757</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Yaroslav_Bulatov; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Yaroslav_Bulatov; <NewLine> ,"REPLY_DATE 1: October 11, 2019,  5:39pm; <NewLine> REPLY_DATE 2: October 11, 2019,  5:57pm; <NewLine> REPLY_DATE 3: October 11, 2019,  6:03pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
56369,Synchronizing/pausing all processes but one with DistributedDataParallel,2019-09-19T13:58:01.424Z,0,381,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I’m working with the code example from the <a href=""https://github.com/pytorch/examples/tree/master/imagenet#multi-processing-distributed-data-parallel-training"" rel=""nofollow noopener"">official ImagNet distributed tutorial</a>.<br/><NewLine>Basically, the code uses <code>torch.multiprocessing.spawn(main_worker)</code> to run a copy of the “main worker” function on each GPU.<br/><NewLine>Then, in each worker the <code>dist.init_process_group</code> command is run and both the model and dataset/dataloader are created and cast into <code>torch.nn.parallel.DistributedDataParallel(model)</code> and <code>torch.utils.data.distributed.DistributedSampler(dataset)</code>.</p><NewLine><p>My problem is that after every epoch I want to modify and rewrite all the data in the dataset, I thought the most straightforward way was to run this shuffling in only one of the nodes inside of an “if” like <code>if args.multiprocessing_distributed and args.rank == 1:</code> so not all the nodes would perform the shuffling simultaneously, similarly to <a href=""https://github.com/pytorch/examples/blob/master/imagenet/main.py#L252"" rel=""nofollow noopener"">the 252 line of the code</a>.</p><NewLine><p>The operation of rewriting the dataset takes long time (&gt;10 minutes). This created a problem where processes which didn’t perform the shuffling were trying to access data while it’s being rewritten by the node that modifies the data.</p><NewLine><p>Is there any suggested way of making the rest of the processes wait for the one process to finish the modification and writing of the data before continuing the training?</p><NewLine><p>I found <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.barrier"" rel=""nofollow noopener"">this “barrier” method</a> from the pytorch distributed package.<br/><NewLine>And also this <a href=""https://docs.python.org/3/library/multiprocessing.html#synchronization-primitives"" rel=""nofollow noopener"">section about synchronization</a> in python’s documentation.<br/><NewLine>But because the error I get isn’t 100% reproducible (in some runs it appears 10 minutes after start in some runs it appears hours into the run) I can’t really test the those implementations and I couldn’t find any easy to follow examples.</p><NewLine><p>Any suggestions would be appreciated <img alt="":sweat_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/sweat_smile.png?v=9"" title="":sweat_smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/Art,(Art),Art,"September 19, 2019,  1:58pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>dist.barrier()</code> should help to block all processes in the group until everyone has reached the same barrier. What error did you see after adding a barrier?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: October 11, 2019,  5:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
57883,Is it possible to access the model replicas in `torch.nn.DataParallel()` between `forward()` and `.backward()` call?,2019-10-10T14:06:54.811Z,0,96,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I am trying to design backward hooks that depend on the loss.<br/><NewLine>In the single GPU version, i modify the hook between the forward and backward pass depending on the loss value.<br/><NewLine>I would like to scale to multi-gpu and modify each replicated hook depending on the loss of each GPU.<br/><NewLine>To do so, Is it possible to access the model replicas in <code>torch.nn.DataParallel()</code> between <code>forward()</code> and  <code>.backward()</code> call?<br/><NewLine>Thanks for helping <img alt="":slightly_smiling_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slightly_smiling_face.png?v=9"" title="":slightly_smiling_face:""/></p><NewLine></div>",https://discuss.pytorch.org/u/ThibaultGROUEIX,(Thibault Groueix),ThibaultGROUEIX,"October 10, 2019,  2:06pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>nn.DataParallel does not expose replicas, but you could make some changes to it locally or copy the data_parallel.py code to make replicas a member field (<a href=""https://github.com/mrshenli/pytorch/blob/0471b1c270cac2aa301f47994d4141d9c7612fc6/torch/nn/parallel/data_parallel.py#L151"" rel=""nofollow noopener"">this line</a>)</p><NewLine><p>It might be easier to install hooks with DistributedDataParallel (DDP) though, as DDP is not making new copies of models in every iteration. So that whatever hooks installed to the original module should still be valid with DDP.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ll take a look at DistributedDataParallel first, it looks the cleanest thing to do.<br/><NewLine>Thanks for your help <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> !</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ThibaultGROUEIX; <NewLine> ,"REPLY_DATE 1: October 11, 2019,  3:17am; <NewLine> REPLY_DATE 2: October 11, 2019,  8:21am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
57432,DataParallel and manually modifying parameters,2019-10-04T02:34:00.736Z,0,858,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I’d like to train my model on multiple GPUs but unfortunately I’m getting a massive validation error (but not so when only doing one gpu) after even 1 epoch.</p><NewLine><p>I think the reason is that I manually modify some of the model parameters, after the optimization step:</p><NewLine><pre><code class=""lang-python"">torch.cuda.set_device(args.local_rank)<NewLine>torch.distributed.init_process_group(backend='nccl', init_method='env://')<NewLine><NewLine>...<NewLine><NewLine>model = nn.Sequential(...).cuda()<NewLine>dmodel = nn.DataParallel(model)<NewLine><NewLine>...<NewLine>    loss = criterion(dmodel(x),y)<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine>    with torch.no_grad():<NewLine>        deterministic_modify(model[17]) # I need to manually modify some weights.<NewLine></code></pre><NewLine><p>I’m guessing there is a sync problem, because if I do this on a single gpu, things work as expected. But on multiple gpu’s I get terrible validation error.</p><NewLine><p>The way I understand nn.DataParallel works (but please correct me if I’m wrong) is that it’s a wrapper, and each gpu has a copy of the model, and nn.DataParallel splits the batch into two, gives each gpu half, computes the gradients, and then, somehow, sync’s the model in both gpus (how?).</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/mraggi,(Miguel Raggi),mraggi,"October 4, 2019,  2:34am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""57432""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mraggi/40/7200_2.png"" width=""20""/> mraggi:</div><NewLine><blockquote><NewLine><p>The way I understand nn.DataParallel works (but please correct me if I’m wrong) is that it’s a wrapper, and each gpu has a copy of the model, and nn.DataParallel splits the batch into two, gives each gpu half, computes the gradients, and then, somehow, sync’s the model in both gpus (how?).</p><NewLine></blockquote><NewLine></aside><NewLine><p>Above is correct except the model sync part. In the forward pass, it replicates the model to all devices, creates one thread per device, scatters (uses broadcast) the input to all devices (so that each thread exclusively works on one model replica with a input split), and finally gathers the output from all threads, and uses that as the return value of the forward function.</p><NewLine><p>So after the forward pass, the autograd graph will contain multiple replicas of the original model, which all points back to the same original model. In the backward pass, each model replica will compute there own gradients, then, as they all have autograd edges point back to the original model, those gradients on different replicates will also accumulate into the same original module. So, it is not synchronizing across replicas, instead all gradients from all replicates are accumulated into the original module.</p><NewLine><p>I noticed that the code snippet above calls init_process_group, which is required for DistributedDataParallel but not necessary for DataParallel. And DistributedDataParallel indeed does the gradient sync across multiple processes, and which should be faster than DataParallel.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: October 10, 2019,  4:34am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
57438,World_size and rank torch.distributed.init_process_group(),2019-10-04T06:44:02.001Z,0,523,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there,</p><NewLine><p>To read the official doc, it totally confuse me.</p><NewLine><p>What’s the definition of ‘world_size’ and ‘rank’ torch.distributed.init_process_group()?</p><NewLine><p>Regarding the argument ‘world_size’, is it cross machine total device count? Or just total machine count?</p><NewLine><p>Regarding the argument ‘rank’, is it an index for each machine, or an index for each devices?</p><NewLine><p>For instance, if I have 2 machines and there are 4 GPUs per machine. What’s the value of ‘world_size’ and ‘rank’ I have to set when I call torch.distributed.init_process_group()?</p><NewLine></div>",https://discuss.pytorch.org/u/nexgus,,nexgus,"October 4, 2019,  6:44am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The concepts of world_size and rank are defined on processes (hence the name process_group). If you would like to create 8 processes, then the world_size should be 8, and the ranks for them should range from 0 to 7. It is up to the application to determine how to place processes to machines. In the above cluster (2 machines, and 4 GPUs each), the best setup would be creating 4 processes on each machine, with each exclusively working on a different GPU.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: October 18, 2019, 12:13am; <NewLine> ",REPLY 1 LIKES: 4 Likes; <NewLine> 
57673,How to get deterministic behavior with DistributedDataParallel?,2019-10-08T05:12:57.125Z,0,213,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, my code has deterministic behavior without DistributedDataParallel, however, not deterministic with DistributedDataParallel.</p><NewLine><p>My code for deterministic behavior is:</p><NewLine><p>cudnn.benchmark = False<br/><NewLine>cudnn.deterministic = True<br/><NewLine>random.seed(123)<br/><NewLine>torch.manual_seed(123)<br/><NewLine>torch.cuda.manual_seed_all(123)<br/><NewLine>torch.utils.data.DataLoader(…, worker_init_fn=random.seed)</p><NewLine><p>And my launch command:<br/><NewLine>python - torch.distributed.launch <br/><NewLine>–nproc_per_node=4 <br/><NewLine>–master_ports=$((RANDOM + 10000)) <br/><NewLine>train.py</p><NewLine><p>Does the DistributedDataParallel need more tricks to get deterministic behavior?</p><NewLine></div>",https://discuss.pytorch.org/u/wzn0828,(Wang Zhennan),wzn0828,"October 8, 2019,  5:12am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>DistributedDataParallel should be deterministic. All it does is applying allreduce to sync gradients across processes. Can you check if the data loader is providing deterministic inputs for you?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: October 10, 2019,  4:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
57762,TCPStore permission denied,2019-10-09T08:03:42.274Z,0,271,"<div class=""post"" itemprop=""articleBody""><NewLine><p>While trying to start a multi-gpu process;</p><NewLine><pre><code class=""lang-auto""> store = TCPStore(result.hostname, result.port, world_size, start_daemon)<NewLine>RuntimeError: Permission denied<NewLine></code></pre><NewLine><p>Has anyone met with this before?</p><NewLine></div>",https://discuss.pytorch.org/u/Shirley_Han,(Shirley Han),Shirley_Han,"October 9, 2019,  8:04am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What port number you are using? and does it work if using sudo?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: October 10, 2019,  3:57am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
39671,Libtorch C++ MPI example,2019-03-12T19:49:08.701Z,1,537,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>Do you guys have a C++ example similar to the python sample here:<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/tutorials/intermediate/dist_tuto.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/intermediate/dist_tuto.html</a><br/><NewLine>From looking at the source code, I only see python support in source code; for example here …\torch\csrc\multiprocessing\init.cpp</p><NewLine><p>Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/kais,,kais,"March 12, 2019,  7:49pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you mean an example of distributed training using the C++ frontend? We don’t have one combining the two unfortunately. Also, there is not yet a <code>torch.nn.parallel.DistributedDataParallel</code> equivalent for the C++ frontend. That said, it is possible to use the distributed <em>primitives</em> from C++. See <code>torch/lib/c10d</code> for the source code.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yep, that’s what I meant. I will take a look at torch/lib/c10d and try to build one myself.<br/><NewLine>Thanks for the reply.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/kais"">@kais</a> I am looking for an example for distributed training with C++ frontend. If you managed to build one, can you please share?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kais; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/soumyadipghosh; <NewLine> ,"REPLY_DATE 1: March 13, 2019,  4:40pm; <NewLine> REPLY_DATE 2: March 13, 2019,  8:40pm; <NewLine> REPLY_DATE 3: October 10, 2019,  3:44am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
56864,Distributed Training by Pytorch,2019-09-26T07:37:35.335Z,0,338,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am going to train my model on multi-server (N servers), each of which includes 8 GPUs. It means that I want to train my model with 8*N GPUs.</p><NewLine><p>I have checked the code provided by a tutorial, which is a code that uses distributed training to train a model on ImageNet. ( <a href=""https://github.com/pytorch/examples/tree/master/imagenet"" rel=""nofollow noopener"">https://github.com/pytorch/examples/tree/master/imagenet</a> )</p><NewLine><p>I found that I need to run the training code on different server seperately just as the guide introduce :</p><NewLine><h3>Multiple nodes:</h3><NewLine><p>Node 0:</p><NewLine><pre><code class=""lang-auto"">python main.py -a resnet50 --dist-url 'tcp://IP_OF_NODE0:FREEPORT' --dist-backend 'nccl' --multiprocessing-distributed --world-size 2 --rank 0 [imagenet-folder with train and val folders]<NewLine></code></pre><NewLine><p>Node 1:</p><NewLine><pre><code class=""lang-auto"">python main.py -a resnet50 --dist-url 'tcp://IP_OF_NODE0:FREEPORT' --dist-backend 'nccl' --multiprocessing-distributed --world-size 2 --rank 1 [imagenet-folder with train and val folders]<NewLine></code></pre><NewLine><p>I am wondering is it possible to input the command on one server and then run the codes on different servers simultaneously and automatically?</p><NewLine><p>Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/Junwu_Weng,(TechWuere),Junwu_Weng,"September 26, 2019,  7:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Any help would be appreciated !</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Maybe you can try something like <a href=""https://www.ansible.com/"" rel=""nofollow noopener"">ansible</a> to deploy your application to multiple machines.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>This depends on your cluster/cloud.</p><NewLine><p>For AWS, I have an example here: <a href=""https://github.com/cybertronai/pytorch-aws/"" rel=""nofollow noopener"">https://github.com/cybertronai/pytorch-aws/</a><br/><NewLine>Basically you set your AWS credentials and then do</p><NewLine><p><code>python mnist_distributed.py --mode=remote --nnodes=2</code></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Junwu_Weng; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/nexgus; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Yaroslav_Bulatov; <NewLine> ,"REPLY_DATE 1: September 26, 2019, 11:28am; <NewLine> REPLY_DATE 2: October 4, 2019,  7:23am; <NewLine> REPLY_DATE 3: October 4, 2019,  8:35pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
57333,Pytorch with Horovod,2019-10-02T15:18:52.401Z,0,217,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to use Pytorch with Horovod. I am trying to run one of the example. I am getting the following error:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""pytorch_imagenet_resnet50.py"", line 3, in &lt;module&gt;<NewLine>    import torch<NewLine>  File ""/ccs/home/amalik/.conda/envs/py366/lib/python3.6/site-packages/torch/__init__.py"", line 84, in &lt;module&gt;<NewLine>    from torch._C import *<NewLine>ImportError: /ccs/home/amalik/.conda/envs/py366/lib/python3.6/site-packages/torch/lib/libtorch_python.so: undefined symbol: MPIX_Query_cuda_support<NewLine></code></pre><NewLine><p>However, when I import torch on simple python command, I don’t get the error.</p><NewLine><pre><code class=""lang-auto"">a&gt;&gt; python<NewLine>&gt;&gt;&gt; import torch<NewLine>&gt;&gt;&gt;<NewLine></code></pre><NewLine><p>no error message</p><NewLine></div>",https://discuss.pytorch.org/u/ammalikwaterloo,(Abid Malik),ammalikwaterloo,"October 2, 2019,  3:18pm",,,,,
57089,Something wrong when i tried to use two gpus in a node,2019-09-29T13:05:28.440Z,0,955,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I use the  command line below to run the script(<a href=""https://github.com/NVIDIA/DALI/blob/master/docs/examples/pytorch/resnet50/main.py"" rel=""nofollow noopener"">pytorch example of dali</a>)</p><NewLine><pre><code class=""lang-auto"">python -m torch.distributed.launch --nproc_per_node=1 train_imagenet_with_dali.py -t<NewLine></code></pre><NewLine><p>when the nproc_per_node=1,it work. But when nproc_per_node=2,there is a error.</p><NewLine><pre><code class=""lang-auto"">*****************************************<NewLine>Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.<NewLine>*****************************************<NewLine>=&gt; creating model 'resnet50'<NewLine>=&gt; creating model 'resnet50'<NewLine>Traceback (most recent call last):<NewLine>  File ""/home/zyy/anaconda3/envs/python36/lib/python3.6/runpy.py"", line 193, in _run_module_as_main<NewLine>    ""__main__"", mod_spec)<NewLine>  File ""/home/zyy/anaconda3/envs/python36/lib/python3.6/runpy.py"", line 85, in _run_code<NewLine>    exec(code, run_globals)<NewLine>  File ""/home/zyy/anaconda3/envs/python36/lib/python3.6/site-packages/torch/distributed/launch.py"", line 246, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""/home/zyy/anaconda3/envs/python36/lib/python3.6/site-packages/torch/distributed/launch.py"", line 242, in main<NewLine>    cmd=cmd)<NewLine>subprocess.CalledProcessError: Command '['/home/zyy/anaconda3/envs/python36/bin/python', '-u', 'train_imagenet_with_dali.py', '--local_rank=1', '-t']' died with &lt;Signals.SIGSEGV: 11&gt;.<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/zyyupup,(zyyupup),zyyupup,"September 29, 2019,  1:05pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/zyyupup"">@zyyupup</a>,</p><NewLine><p>Can you try this please:</p><NewLine><pre><code class=""lang-auto"">python -m torch.distributed.launch --nproc_per_node=2 main.py -a resnet50 --dali_cpu --fp16 --b 32 --static-loss-scale 128.0 --workers 4 --lr=0.4 ./ 2&gt;&amp;1<NewLine></code></pre><NewLine><p>And give us full repro step and your environment (Anaconda, PyTorch, DALI versions)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply. I tried the commamd  and the same error accured again. My environment is:</p><NewLine><ul><NewLine><li>Anaconda(python):3.6.8</li><NewLine><li>Pytorch:1.2.0+cuda9.2</li><NewLine><li>DALI:0.13.0</li><NewLine><li>System:ubuntu16.04 with 2 gpus</li><NewLine></ul><NewLine><p>I only changed the path of the dataset in the code and then run it.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>DALI example is based on an older version of PyTorch APEX example - <a href=""https://github.com/NVIDIA/apex/tree/master/examples/imagenet"" rel=""nofollow noopener"">https://github.com/NVIDIA/apex/tree/master/examples/imagenet</a>. You can try to run it as well to check if this may be the DALI fault.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/spanev; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/zyyupup; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/JanuszL; <NewLine> ,"REPLY_DATE 1: September 29, 2019, 10:45pm; <NewLine> REPLY_DATE 2: October 1, 2019, 10:58am; <NewLine> REPLY_DATE 3: October 1, 2019, 11:43am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
57199,Install Pytorch Error,2019-09-30T20:34:44.045Z,0,532,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I download pytorch from the official website. However, when I imported pytorch on my local windows computer, I got the following error as below. It seems torchvision suffered malfunction. What does it mean? Can someone know how to fix it? Thank you.</p><NewLine><hr/><NewLine><p>ImportError                               Traceback (most recent call last)<br/><NewLine> in ()<br/><NewLine>3 import torch<br/><NewLine>4 import torch.nn as nn<br/><NewLine>----&gt; 5 import torchvision<br/><NewLine>6 import torchvision.datasets as dataset<br/><NewLine>7 import torchvision.transforms as transforms</p><NewLine><p>C:\ProgramData\Anaconda3\lib\site-packages\torchvision_<em>init</em>_.py in ()<br/><NewLine>1 from torchvision import models<br/><NewLine>----&gt; 2 from torchvision import datasets<br/><NewLine>3 from torchvision import ops<br/><NewLine>4 from torchvision import transforms<br/><NewLine>5 from torchvision import utils</p><NewLine><p>C:\ProgramData\Anaconda3\lib\site-packages\torchvision\datasets_<em>init</em>_.py in ()<br/><NewLine>----&gt; 1 from .lsun import LSUN, LSUNClass<br/><NewLine>2 from .folder import ImageFolder, DatasetFolder<br/><NewLine>3 from .coco import CocoCaptions, CocoDetection<br/><NewLine>4 from .cifar import CIFAR10, CIFAR100<br/><NewLine>5 from .stl10 import STL10</p><NewLine><p>C:\ProgramData\Anaconda3\lib\site-packages\torchvision\datasets\lsun.py in ()<br/><NewLine>----&gt; 1 from .vision import VisionDataset<br/><NewLine>2 from PIL import Image<br/><NewLine>3 import os<br/><NewLine>4 import os.path<br/><NewLine>5 import six</p><NewLine><p>C:\ProgramData\Anaconda3\lib\site-packages\torchvision\datasets\vision.py in ()<br/><NewLine>1 import os<br/><NewLine>2 import torch<br/><NewLine>----&gt; 3 import torch.utils.data as data<br/><NewLine>4<br/><NewLine>5</p><NewLine><p>C:\ProgramData\Anaconda3\lib\site-packages\torch\utils\data_<em>init</em>_.py in ()<br/><NewLine>2 from .distributed import DistributedSampler  # noqa: F401<br/><NewLine>3 from .dataset import Dataset, IterableDataset, TensorDataset, ConcatDataset, ChainDataset, Subset, random_split  # noqa: F401<br/><NewLine>----&gt; 4 from .dataloader import DataLoader, _DatasetKind, get_worker_info  # noqa: F401</p><NewLine><p>C:\ProgramData\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py in ()<br/><NewLine>10 import torch.multiprocessing as multiprocessing<br/><NewLine>11 from . import IterableDataset, Sampler, SequentialSampler, RandomSampler, BatchSampler<br/><NewLine>—&gt; 12 from . import _utils<br/><NewLine>13 from torch._utils import ExceptionWrapper<br/><NewLine>14 import threading</p><NewLine><p>C:\ProgramData\Anaconda3\lib\site-packages\torch\utils\data_utils_<em>init</em>_.py in ()<br/><NewLine>12<br/><NewLine>13 # old private location of the ExceptionWrapper that some users rely on:<br/><NewLine>—&gt; 14 from torch._utils import ExceptionWrapper  # noqa: F401<br/><NewLine>15<br/><NewLine>16</p><NewLine><p>ImportError: cannot import name ‘ExceptionWrapper’</p><NewLine></div>",https://discuss.pytorch.org/u/siriushou,,siriushou,"September 30, 2019,  8:34pm",,,,,
57124,The Gather problem in DataParallel: dimension are not matched,2019-09-30T04:21:01.042Z,0,155,"<div class=""post"" itemprop=""articleBody""><NewLine><h2>Problem</h2><NewLine><p>During my application, a strange bug is that my model works well with single GPU but fails in multi-GPUs by:<br/><NewLine><strong>RuntimeError: Gather got an input of invalid size: got [24, 10, 448，448], but expected [24, 11, 448，448] (gather at /pytorch/torch/csrc/cuda/comm.cpp:239</strong><br/><NewLine>My input size is [48,3,448,448], and two GPUs are used. Thus, it is ok to split 24 images into each gpu, but it is strange why exists 10 and 11 channels?</p><NewLine><p>After debug, the problem is found out:<br/><NewLine><img alt=""image"" data-base62-sha1=""d4xtJnFxxC2AJxq1GQjEhYdP8fF"" height=""91"" src=""https://discuss.pytorch.org/uploads/default/original/3X/5/b/5b9fa427028107de72bbaf580dbeb1fca29affe3.png"" width=""477""/><br/><NewLine>The self.embed_arr is an input semantic labels, whose size is [21,300].<br/><NewLine>self.embed_arr will affect the image feature channels.<br/><NewLine>Under single GPU setting, each image will meet a same self.embed_arr, thereby having the same image channels.<br/><NewLine>However, under multi-GPUs settings, self.embed_arr will be split into multi-parts, e.g., [10,30] and [11,30], thereby leading to different image channels and a bug during feature gathering.<br/><NewLine>( If self.embed_arr has a size of [20,300], this problem will not appear, and I may think that the bad performance is attributed to my algorithm, which is terrible!)</p><NewLine><h2>Solution</h2><NewLine><p>An alternative solution is to duplicate the input so that the scattered inputs in each GPU is the same by:<br/><NewLine>self.embed_arr = self.embed_arr.repeat(len(range(torch.cuda.device_count())),1)</p><NewLine><h2>Suggest</h2><NewLine><p>So, I suggest pytorch to add a function that can ontrol which inputs should be scattered to different GPUs. Or, is there any better solutions?</p><NewLine></div>",https://discuss.pytorch.org/u/Shaobo_Min,(Shaobo Min),Shaobo_Min,"September 30, 2019,  4:21am",,,,,
57040,Gathering dictionaries with NCCL for hard example mining,2019-09-28T17:30:58.044Z,0,220,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When hard example mining, it is important to keep track of the data indices to be able to set the proper weights in either the loss function or the sampler. For this purpose, my Dataset outputs a dictionary including the index that outputs the sample, e.g., <code>{'image_index': idx, 'image': image, 'target': target}</code>.</p><NewLine><p>The collate function then merges the indexes to a <code>Double</code> tensor, so far so good. When I am now evaluating the training set in multi gpu setting, I store the loss and the indices in two dictionaries (as data types are different) and attempt to merge these dictionaries across these GPUs to GPU 0 which I can then use to compute proper weights for the next epoch.</p><NewLine><p>However, NCCL does not seem to support <code>gather</code>. I get <code>RuntimeError: ProcessGroupNCCL does not support gather</code> I could copy the data to the CPU before gathering and use a different process group with gloo, but preferable I would want to keep these tensors on the GPU and only copy to the CPU when  the complete evaluation is done. Is there a way around this so I can gather anyway (or another approach)?</p><NewLine></div>",https://discuss.pytorch.org/u/jteuwen,(Jonas Teuwen),jteuwen,"September 28, 2019,  5:30pm",,,,,
56937,Problem with speedup DataParallel,2019-09-27T00:44:11.148Z,3,472,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I am trying to understand how DataParallel works. Now I am testing simple code to see speedup on 2 GPUs:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>from torch.utils.data import Dataset, DataLoader<NewLine><NewLine># Parameters and DataLoaders<NewLine>input_size = 5<NewLine>output_size = 2<NewLine><NewLine>batch_size = 10000<NewLine>data_size = 1000000<NewLine><NewLine>device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")<NewLine><NewLine>class RandomDataset(Dataset):<NewLine><NewLine>    def __init__(self, size, length):<NewLine>        self.len = length<NewLine>        self.data = torch.randn(length, size)<NewLine><NewLine>    def __getitem__(self, index):<NewLine>        return self.data[index]<NewLine><NewLine>    def __len__(self):<NewLine>        return self.len<NewLine><NewLine>rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),<NewLine>                         batch_size=batch_size, shuffle=True)<NewLine><NewLine>class Model(nn.Module):<NewLine>    # Our model<NewLine><NewLine>    def __init__(self, input_size, output_size):<NewLine>        super(Model, self).__init__()<NewLine>        self.fc = nn.Linear(input_size, output_size)<NewLine><NewLine>    def forward(self, input):<NewLine>        output = self.fc(input)<NewLine>        print(""\tIn Model: input size"", input.size(),<NewLine>              ""output size"", output.size())<NewLine><NewLine>        return output<NewLine><NewLine>model = Model(input_size, output_size)<NewLine>if torch.cuda.device_count() &gt; 1:<NewLine>  print(""Let's use"", torch.cuda.device_count(), ""GPUs!"")<NewLine>  # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs<NewLine>  model = nn.DataParallel(model)<NewLine>model.to(device)<NewLine><NewLine>for data in rand_loader:<NewLine>  input = data.to(device)<NewLine>  output = model(input)<NewLine>  print(""Outside: input size"", data.size(),<NewLine>        ""output_size"", output.size())<NewLine></code></pre><NewLine><p>I measure time by using ‘time’ utility. Execution on 1 GPU takes 3m26.624s, and execution on 2 GPUs takes approximately the same time (±5 seconds). What could be the problem?</p><NewLine></div>",https://discuss.pytorch.org/u/goaltender,(Yuri Konkov),goaltender,"September 27, 2019, 12:44am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you sure you’re using both GPUs with <code>nn.DataParallel</code>? The line:</p><NewLine><pre><code class=""lang-python"">device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")<NewLine></code></pre><NewLine><p>makes me think that it’s running on a single gpu. You could try changing this to:</p><NewLine><pre><code class=""lang-python"">device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")<NewLine></code></pre><NewLine><p>In addition, one of the main benefits of data parallelism is that you can use larger batch sizes to speed up iterating through the dataset, so you can try:</p><NewLine><pre><code class=""lang-python"">batch_size = 10000 * torch.cuda.device_count()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for answer. I changed “cuda:0” to “cuda” as you said, and didn’t see any changes. If I change “batch_size” depending on the “torch.cuda.device_count()”, I will get different sizes of batch and time comparisons will not be honest between single GPU and multi GPU.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I think the problem is that your model is so small that your task is cpu bound. So using more GPUs won’t help.<br/><NewLine>You can do the same experiment with a resnet from torchvision for example (and lower batch_size) to make sure you get a GPU bound task.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I change a little bit code to use it with resnet18</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>from torch.utils.data import Dataset, DataLoader<NewLine>import torchvision.models as models<NewLine><NewLine># Parameters and DataLoaders<NewLine>input_size = 224<NewLine>output_size = 1000<NewLine><NewLine>batch_size = 256<NewLine>data_size = 10000<NewLine><NewLine>device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")<NewLine><NewLine>class RandomDataset(Dataset):<NewLine><NewLine>    def __init__(self, size, length):<NewLine>        self.len = length<NewLine>        self.data = torch.randn(length, 3 * size * size).view(length, 3, size, size)<NewLine><NewLine>    def __getitem__(self, index):<NewLine>        return self.data[index]<NewLine><NewLine>    def __len__(self):<NewLine>        return self.len<NewLine><NewLine>rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),<NewLine>                         batch_size=batch_size, shuffle=True)<NewLine><NewLine>model = models.resnet18(pretrained=True)<NewLine>if torch.cuda.device_count() &gt; 1:<NewLine>  print(""Let's use"", torch.cuda.device_count(), ""GPUs!"")<NewLine>  model = nn.DataParallel(model)<NewLine>model.to(device)<NewLine><NewLine>for data in rand_loader:<NewLine>  input = data.to(device)<NewLine>  output = model(input)<NewLine>  print(""Outside: input size"", data.size(),<NewLine>        ""output_size"", output.size())<NewLine></code></pre><NewLine><p>However, using “batch_size=256” on single and multiple GPUs I see the same time (2m6.530s).</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>And what is the usage of the GPU when you run on a single one? When you run on two?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your replies. The problem was with heavy RandomDataset, that generates dataset. When I reduced “data_size” and add 200 epochs, I have seen speedup on two GPUs. So problem was CPU bound.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Swarchal; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/goaltender; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/goaltender; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/goaltender; <NewLine> ,"REPLY_DATE 1: September 27, 2019,  2:39pm; <NewLine> REPLY_DATE 2: September 27, 2019,  3:02pm; <NewLine> REPLY_DATE 3: September 27, 2019,  8:45pm; <NewLine> REPLY_DATE 4: September 27, 2019,  4:20pm; <NewLine> REPLY_DATE 5: September 27, 2019,  6:29pm; <NewLine> REPLY_DATE 6: September 27, 2019,  8:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 2 Likes; <NewLine> 
31872,Distributed error. module &lsquo;torch.distributed&rsquo; has no attribute &lsquo;is_initialized&rsquo;,2018-12-11T14:57:51.105Z,0,2047,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve installed pytorch 1.0 on windows.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/321d3c2d53b59092e2330b46a65d29171bbb7508"" href=""https://discuss.pytorch.org/uploads/default/original/2X/3/321d3c2d53b59092e2330b46a65d29171bbb7508.png"" title=""TIM截图20181211225234.png""><img alt=""TIM%E6%88%AA%E5%9B%BE20181211225234"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/3/321d3c2d53b59092e2330b46a65d29171bbb7508_2_10x10.png"" height=""88"" src=""https://discuss.pytorch.org/uploads/default/original/2X/3/321d3c2d53b59092e2330b46a65d29171bbb7508.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">TIM截图20181211225234.png</span><span class=""informations"">1287×165 8.67 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine>When I try to use webcam demo provided by <a href=""https://github.com/facebookresearch/maskrcnn-benchmark"" rel=""nofollow noopener"">maskrcnn-benchmark</a>. An error occured:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""webcam.py"", line 80, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""webcam.py"", line 64, in main<NewLine>    min_image_size=args.min_image_size,<NewLine>  File ""G:\PyProjects\maskrcnn-benchmark\demo\predictor.py"", line 115, in __init__<NewLine>    _ = checkpointer.load(cfg.MODEL.WEIGHT)<NewLine>  File ""g:\pyprojects\maskrcnn-benchmark\maskrcnn_benchmark\utils\checkpoint.py"", line 61, in load<NewLine>    checkpoint = self._load_file(f)<NewLine>  File ""g:\pyprojects\maskrcnn-benchmark\maskrcnn_benchmark\utils\checkpoint.py"", line 128, in _load_file<NewLine>    cached_f = cache_url(f)<NewLine>  File ""g:\pyprojects\maskrcnn-benchmark\maskrcnn_benchmark\utils\model_zoo.py"", line 44, in cache_url<NewLine>    if not os.path.exists(cached_file) and is_main_process():<NewLine>  File ""g:\pyprojects\maskrcnn-benchmark\maskrcnn_benchmark\utils\comm.py"", line 28, in is_main_process<NewLine>    if not torch.distributed.is_initialized():<NewLine>AttributeError: module 'torch.distributed' has no attribute 'is_initialized'<NewLine></code></pre><NewLine><p>But when I checked the pytorch 1.0 document, the <code>torch.distributed</code> module does have the <code>is_initialized()</code> method.<br/><NewLine>How to solve this problem</p><NewLine></div>",https://discuss.pytorch.org/u/congve1,(Cong Luwen),congve1,"December 31, 2018,  7:03am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Same issue herehyrtjedtujtuktu</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>We don’t support Windows for torch.distributed, unfortunately.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jinfagang; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: September 27, 2019,  2:24am; <NewLine> REPLY_DATE 2: September 27, 2019,  8:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
56469,Tensor Inverse in parallel over mutliple GPUs,2019-09-21T00:48:23.069Z,1,153,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to run over multiple GPUs in parallel torch.inverse(). I saw this post <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/matmul-on-multiple-gpus/33122"">Matmul on multiple GPUs</a>. Which shows that if you have multiple tensors allocated to each GPU matmul will be run in parallel. I was able to replicate this behavior for matmul but when I try to do the same thing for torch.inverse() it seems to run sequentially when I watch nvidia-smi. Any ideas?</p><NewLine></div>",https://discuss.pytorch.org/u/ozdi,(omd),ozdi,"September 21, 2019, 12:48am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you post your code snippet so that we could have a look?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for the quick reply. As you can see it greatly mirrors the other post.</p><NewLine><pre><code class=""lang-python"">import torch<NewLine><NewLine>ngpu = torch.cuda.device_count()<NewLine># This is the allocation to each GPU.<NewLine>lis = []<NewLine>for i in range(ngpu):<NewLine>    lis.append(torch.rand(5000,5000,device = 'cuda:'+ str(i)))<NewLine><NewLine># per the matmul on multiple GPUs post this should already be in parallel to my understanding<NewLine># but doesnt seem to be based on watch nvidia-smi<NewLine>C_ = []<NewLine>for i in range(ngpu):<NewLine>    C_.append(torch.inverse(lis[i]))<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ozdi; <NewLine> ,"REPLY_DATE 1: September 21, 2019, 12:46pm; <NewLine> REPLY_DATE 2: September 22, 2019,  3:13am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
55973,Distributed launch utility unstable,2019-09-15T14:39:34.126Z,6,271,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The distributed launch utility seems like unstable in usage.<br/><NewLine>Executing the same program once with the following command</p><NewLine><p><code>python -m torch.distributed.launch --nproc_per_node=3 --nnodes=1 --node_rank=0 --master_addr=""127.0.0.1"" --master_port=62123 main.py</code></p><NewLine><p>Works fine:</p><NewLine><pre><code class=""lang-auto"">1.0, 0.05, 2.1814, 0.1697, 2.0053, 0.2154<NewLine>1.0, 0.05, 2.1804, 0.1674, 1.9767, 0.2406<NewLine>1.0, 0.05, 2.1823, 0.1703, 1.9799, 0.2352<NewLine>2.0, 0.05, 2.1526, 0.1779, 2.1166, 0.1908<NewLine>2.0, 0.05, 2.1562, 0.1812, 2.0868, 0.2076<NewLine>2.0, 0.05, 2.1593, 0.1741, 2.0935, 0.192<NewLine>3.0, 0.05, 1.9386, 0.2413, 1.8037, 0.3017<NewLine>3.0, 0.05, 1.9319, 0.2473, 1.8041, 0.2903<NewLine>3.0, 0.05, 1.9286, 0.2443, 1.815, 0.2939<NewLine>4.0, 0.05, 1.7522, 0.3153, 1.828, 0.3131<NewLine>4.0, 0.05, 1.7504, 0.3207, 1.7613, 0.3245<NewLine></code></pre><NewLine><p>After the program is finished executing again the same command i.e., calling launch with the same arguments results in an error</p><NewLine><pre><code class=""lang-auto"">File ""/home/kirk/miniconda3/envs/torch/lib/python3.6/site-packages/torch/serialization.py"", line 386, in load<NewLine>    return _load(f, map_location, pickle_module, **pickle_load_args)<NewLine>  File ""/home/kirk/miniconda3/envs/torch/lib/python3.6/site-packages/torch/serialization.py"", line 580, in _load<NewLine>    deserialized_objects[key]._set_from_file(f, offset, f_should_read_directly)<NewLine>RuntimeError: storage has wrong size: expected 4333514340733757174 got 256<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/kirk86,(Kirk86),kirk86,"September 15, 2019,  2:39pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The error point to a broken checkpoint, not the distributed launch.<br/><NewLine>It seems you’ve saved some checkpoints in the previous runs without making sure only a single process (e.g. rank0) writes to the files.<br/><NewLine>This might yield to multiple processes writing to the same checkpoint file and thus breaking it.<br/><NewLine>Could this be the case?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply. That could be the case but in all the examples I’ve seen using distributed launch didn’t show how to properly save the checkpoint. When I save the checkpoint I was just using <code>torch.save</code>. Should I be using something else?</p><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""55973""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ptrblck/40/1823_2.png"" width=""20""/> ptrblck:</div><NewLine><blockquote><NewLine><p>It seems you’ve saved some checkpoints in the previous runs without making sure only a single process (e.g. rank0) writes to the files.</p><NewLine></blockquote><NewLine></aside><NewLine><p>How do I do that? Any pointers or examples where to look would be much appreciated!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> So I did  try your suggestion in the following way:</p><NewLine><pre><code class=""lang-auto"">if torch.distributed.is_available() and torch.distributed.is_initialized():<NewLine>   if os.environ['RANK'] == 0:<NewLine>      torch.save(checkpoint)<NewLine>else:<NewLine>      torch.save(checkpoint)<NewLine></code></pre><NewLine><p>But still I’m getting the same error:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/f7311958668856883d949b75e5995aded56cb56c"" href=""https://discuss.pytorch.org/uploads/default/original/2X/f/f7311958668856883d949b75e5995aded56cb56c.png"" title=""image.png""><img alt=""image"" data-base62-sha1=""zgL819YACn8ZV3v2QEiWOs8q86E"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/f/f7311958668856883d949b75e5995aded56cb56c_2_10x10.png"" height=""62"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/f/f7311958668856883d949b75e5995aded56cb56c_2_690x62.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/f/f7311958668856883d949b75e5995aded56cb56c_2_690x62.png, https://discuss.pytorch.org/uploads/default/optimized/2X/f/f7311958668856883d949b75e5995aded56cb56c_2_1035x93.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/f/f7311958668856883d949b75e5995aded56cb56c.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">1111×100 36.1 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not sure if the <code>RANK</code> env variable is useful at this point. In the <a href=""https://github.com/pytorch/examples/blob/ee964a2eeb41e1712fe719b83645c79bcbd0ba1a/imagenet/main.py#L252"" rel=""nofollow noopener"">ImageNet example</a> the <code>args.rank</code> variable is used. Could you try that?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> Thanks for the pointer. My scenario is single node multi-gpu. Considering that case <code>rank=0</code> and <code>world_size=2</code>. According to the imagenet example it says save the checkpoint if torch distributed is running or if torch distributed is running and the rank is equal to the num_gpus.</p><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""55973""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ptrblck/40/1823_2.png"" width=""20""/> ptrblck:</div><NewLine><blockquote><NewLine><p>I’m not sure if the <code>RANK</code> env variable is useful at this point.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Why is not useful, my understanding is that it is set dynamically from the <code>launch</code>  utility and it will contain whichever rank is currently running?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>The environment variable is the legacy approach and is just set, if <code>use_env</code> was specified as seen <a href=""https://github.com/pytorch/pytorch/blob/2b52c1d9826e8e8c83f1ddafd4888bdb3bd00af9/torch/distributed/launch.py#L179"" rel=""nofollow noopener"">here</a>.<br/><NewLine>From the docs:</p><NewLine><pre><code class=""lang-auto"">5. Another way to pass ``local_rank`` to the subprocesses via environment variable<NewLine>``LOCAL_RANK``. This behavior is enabled when you launch the script with<NewLine>``--use_env=True``. You must adjust the subprocess example above to replace<NewLine>``args.local_rank`` with ``os.environ['LOCAL_RANK']``; the launcher<NewLine>will not pass ``--local_rank`` when you specify this flag.<NewLine>.. warning::<NewLine>    ``local_rank`` is NOT globally unique: it is only unique per process<NewLine>    on a machine.  Thus, don't use it to decide if you should, e.g.,<NewLine>    write to a networked filesystem.  See<NewLine>    https://github.com/pytorch/pytorch/issues/12042 for an example of<NewLine>    how things can go wrong if you don't do this correctly.<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the clarifications, reading through the github issues it seems that:</p><NewLine><ol><NewLine><li><NewLine><p><code>local_rank</code>  is actually the ID  <em>within</em>  a worker; multiple workers have a  <code>local_rank</code>  of  <code>0</code> , so they’re probably trampling each other’s checkpoints.</p><NewLine></li><NewLine><li><NewLine><p>added a  <code>--global_rank</code>  command line argument as well. (solution)</p><NewLine></li><NewLine><li><NewLine><p>someone else comments, <code>torch.distributed.launch</code> <a href=""https://github.com/pytorch/pytorch/blob/929bffe020be508cd269b50b372a56c397073160/torch/distributed/launch.py"" rel=""nofollow noopener"">sets up a  <code>RANK</code>  environment variable</a> which can be used to detect if you are on the master process (with  <code>os.environ['RANK'] == '0'</code>  from python</p><NewLine></li><NewLine><li><NewLine><p>you can use  <code>torch.distributed.get_rank()</code>  to get the global rank. (I suppose this might be the most appropriate way to do it?)</p><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kirk86; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/kirk86; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/kirk86; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/kirk86; <NewLine> ,"REPLY_DATE 1: September 15, 2019, 11:13pm; <NewLine> REPLY_DATE 2: September 16, 2019,  3:47am; <NewLine> REPLY_DATE 3: September 16, 2019,  9:15am; <NewLine> REPLY_DATE 4: September 16, 2019, 10:17am; <NewLine> REPLY_DATE 5: September 16, 2019, 11:05pm; <NewLine> REPLY_DATE 6: September 16, 2019, 11:16pm; <NewLine> REPLY_DATE 7: September 16, 2019, 11:56pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> 
54714,Slow distributed training,2019-08-29T19:19:31.105Z,0,254,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My network is 1 Gbit ethernet and i am trying to use pytorch distributed training on two 8-gpu servers. Training procedure is simple classification objective with feed-forward network. I experience significant slowdown in comparison with single 8-gpu server training. Also “nload” tool shows full bandwidth usage even for small model (resnet18).</p><NewLine><p>Is my network too slow for distributed training? If it is, what bandwidth (in Gbit/s) do I need to train heavy models like resnet101?</p><NewLine></div>",https://discuss.pytorch.org/u/Boris_Lestsov,(Boris Lestsov),Boris_Lestsov,"August 29, 2019,  7:19pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can figure it out from your gradient size (could just take size of your model checkpoint) + step time. For instance Resnet50 160ms per batch, 50MB checkpoint, therefore each worker needs to send and receive 50/.16 = 312 MB per second, means you need &gt;=2.5 Gbps</p><NewLine><p>What matters here is the ratio of compute time to parameter size. If you double computation + double parameter size, the network requirement is unaffected. Conv nets have good ratio of compute/bandwidth, transformers will need more bandwidth because of matmuls</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Yaroslav_Bulatov; <NewLine> ,"REPLY_DATE 1: September 15, 2019, 10:17pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
55887,Distributed training prints log messages twice,2019-09-13T15:56:29.517Z,0,183,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How can I prevent my log messages to not get printed multiple times when I use distributed training?</p><NewLine><p><img alt=""image"" data-base62-sha1=""9dQTwZedV1qskaljP1EzZ2d4M0s"" height=""120"" src=""https://discuss.pytorch.org/uploads/default/original/2X/4/40a473ce455cbf84b6a026fd493911ac39cb250c.png"" width=""450""/><br/><NewLine>Any ideas how to resolve this or where to took at?</p><NewLine><p>Also, I keep getting an error every time I set <code>OMP_NUM_THREADS &gt; 1</code><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/b186211275a78d4339aa67fd40450c112c71677d"" href=""https://discuss.pytorch.org/uploads/default/original/2X/b/b186211275a78d4339aa67fd40450c112c71677d.png"" title=""image.png""><img alt=""image"" data-base62-sha1=""pkrSJd6d8csfpxmLB7PaknD7ThH"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/b/b186211275a78d4339aa67fd40450c112c71677d_2_10x10.png"" height=""244"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/b/b186211275a78d4339aa67fd40450c112c71677d_2_690x244.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/b/b186211275a78d4339aa67fd40450c112c71677d_2_690x244.png, https://discuss.pytorch.org/uploads/default/optimized/2X/b/b186211275a78d4339aa67fd40450c112c71677d_2_1035x366.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/b/b186211275a78d4339aa67fd40450c112c71677d.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">1147×407 134 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>Any thoughts into what might have gone wrong here?</p><NewLine></div>",https://discuss.pytorch.org/u/kirk86,(Kirk86),kirk86,"September 13, 2019,  4:06pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I managed to solve the error I was getting when using <code>OMP_NUM_THREADS &gt; 1</code><br/><NewLine>Basically looking in my script I had to add <code>init_method=""env://""</code> in the call to the <code>process_group</code></p><NewLine><p><code>torch.distributed.init_process_group(backend='nccl', init_method='env://')</code></p><NewLine><p>The other thing that I was missing is that when calling the <code>launch</code> utility you have to pass a random port otherwise I would get the above error</p><NewLine><p><code>python -m torch.distributed.launch --nproc_per_node=number of gpus --master_port=some random high number port main.py</code></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/kirk86; <NewLine> ,"REPLY_DATE 1: September 13, 2019, 10:02pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
55806,Limit process to single GPU,2019-09-12T14:50:46.520Z,1,277,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I have a setup with 4 GPUs. When now working with multiple processes in PyTorch, Is there a way to enforce that a process only accesses a given, single gpu, therefore limiting the CUDA driver context to be present only once per process?</p><NewLine><p>Thanks in advance for your help,</p><NewLine><p>Benjamin</p><NewLine></div>",https://discuss.pytorch.org/u/besterma,(Benjamin Estermann),besterma,"September 12, 2019,  2:50pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/besterma"">@besterma</a>,</p><NewLine><p>Sure, you can do it with the env variable <code>CUDA_VISIBLE_DEVICES</code>.</p><NewLine><p>E.g. to use GPU 0 and 2:</p><NewLine><pre><code class=""lang-auto"">CUDA_VISIBLE_DEVICES=0,2 python pytorch_script.py<NewLine></code></pre><NewLine><p>and in your case you have to give a different env variable to each process.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <a class=""mention"" href=""/u/spanev"">@spanev</a>!</p><NewLine><p>In case anyone is wondering, here is how to set process specific env variables:</p><NewLine><pre><code class=""lang-python"">import torch.multiprocessing as _mp<NewLine>import torch<NewLine>import os<NewLine><NewLine>mp = _mp.get_context('fork')<NewLine><NewLine><NewLine>class Process(mp.Process):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        print(""Init Process"")<NewLine>        return<NewLine><NewLine>    def run(self):<NewLine>        print(""Hello World!"")<NewLine>        os.environ['CUDA_VISIBLE_DEVICES'] = '1'<NewLine>        print(torch.cuda.device_count())<NewLine>        print(torch.cuda.current_device())<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    num_processes = 1<NewLine>    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'<NewLine>    processes = [Process() for i in range(num_processes)]<NewLine>    [p.start() for p in processes]<NewLine>    print(""main: "" + os.environ['CUDA_VISIBLE_DEVICES'])<NewLine>    [p.join() for p in processes]<NewLine></code></pre><NewLine><p>It is important to set it in the run method of the process, as the <strong>init</strong> method is still called in the main process, therefore setting the env vars of the main process when set there.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/spanev; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/besterma; <NewLine> ,"REPLY_DATE 1: September 13, 2019,  8:20am; <NewLine> REPLY_DATE 2: September 13, 2019,  8:12am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
55547,Weird learning stagnation when using DataParallel,2019-09-09T13:40:51.869Z,6,180,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m currently facing a weird behaviour which I cannot explain. I’m training a vgg16 on svhn and training on 1 gpu with SGD and fixed hypeparams I get the following nice results:<br/><NewLine><img alt=""image"" data-base62-sha1=""sLCkwWtU5VRzkZb7CUbKSDBkFPj"" height=""458"" src=""https://discuss.pytorch.org/uploads/default/original/2X/c/c99edc9763b359be5f3f11a5c43ec1899ffb1261.png"" width=""560""/></p><NewLine><p>Now trying to train the same model with same optimizer and hyperparams as before but using <code>DataParallel</code> it exhibits the following behaviour where the learning process actually stagnates and it doesn’t learn anything.<br/><NewLine><img alt=""image"" data-base62-sha1=""6ysxzR0p8J84KfHp0DmveNgRVMS"" height=""474"" src=""https://discuss.pytorch.org/uploads/default/original/2X/2/2df24471697b8c16ff439d2cb49c7e4919d963f2.png"" width=""559""/></p><NewLine><p>Even more weird is the fact that if I swap vgg16 for resnet50 it starts learning again.</p><NewLine><p>Anyone has any insights on this, or what it might be going on?</p><NewLine><p>I would expect that if a model <code>M</code> trained on one device with fixed optimizer and hyperams exhibiting good learning behaviour to have the same behaviour when trained with <code>DataParallel</code> using same optimizer and hyperparams.</p><NewLine></div>",https://discuss.pytorch.org/u/kirk86,(Kirk86),kirk86,"September 9, 2019, 11:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you post a minimal working example to reproduce these results? It could be, e.g. that you don’t pass the DataParallel’s parameters to the <code>params</code> argument of the optimizer</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/dsuess"">@dsuess</a> Thanks for the response!<br/><NewLine>I’ve actually followed the example <a href=""https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html?highlight=data%20parallel"" rel=""nofollow noopener"">here</a> for <code>DataParallel</code>.</p><NewLine><p>Here’s a MWE (trying to avoid putting lot’s of code here)</p><NewLine><pre><code class=""lang-auto"">model = VGG16<NewLine> dataset = CIFAR10<NewLine><NewLine>def main():<NewLine>    model = create_model()<NewLine>    train_loader = torch.utils.data.DataLoader(...CIFAR10...)<NewLine>    optimizer = torch.optim.SGD(model.parameters(), lr=0.05, weight_decay=5e-4, momentum=0.9)<NewLine>     if torch.cuda.device_count() &gt; 1:<NewLine>         model = torch.nn.parallel.DataParallel(model).to(device)<NewLine>     else:<NewLine>         model.to(device)<NewLine>     train()<NewLine>     test()<NewLine></code></pre><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""55547""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/dsuess/40/9946_2.png"" width=""20""/> dsuess:</div><NewLine><blockquote><NewLine><p>It could be, e.g. that you don’t pass the DataParallel’s parameters to the <code>params</code> argument of the optimizer</p><NewLine></blockquote><NewLine></aside><NewLine><p>I think that might be the case since in the MWE I’m creating the optimizer before putting the model on DataParallel?</p><NewLine><p>But, it doesn’t explain why the same code works when everything else is constant in the MWE and just swap VGG16 for ResNet50?</p><NewLine><p>Let me give it a try and rearrange the optimizer order after model is sent to DataParallel.</p><NewLine><p>Last question, do you by any chance have any insight on <a href=""https://discuss.pytorch.org/t/hanging-distributed-data-parallel-on-interactive-example/55439"">this problem</a>?</p><NewLine><p>Thanks!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It looks like a dimension problem to me. Have you check that the batch sizes are at the same position?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""55547""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/n/eb9ed0/40.png"" width=""20""/> nooblyh:</div><NewLine><blockquote><NewLine><p>Have you check that the batch sizes are at the same position?</p><NewLine></blockquote><NewLine></aside><NewLine><p>What do you mean by at the same position?<br/><NewLine>My understanding is that <code>DataParallel</code> takes data of batch size <code>m</code> and splits them as <code>int(m/num_of_devices)</code> sending to each device it’s own split and a copy of the model?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>OK, just double checked and your order seems to be correct. I think it’s best if you post a full working example, otherwise we’re just guessing</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, I mean have you rule out the possibility of shape mismatch?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok so I did check and reordered the code as below:</p><NewLine><pre><code class=""lang-auto"">model = VGG16<NewLine>dataset = CIFAR10<NewLine><NewLine>def main():<NewLine>    model = create_model()<NewLine>    train_loader = torch.utils.data.DataLoader(...CIFAR10...)<NewLine>     if torch.cuda.device_count() &gt; 1:<NewLine>         model = torch.nn.parallel.DataParallel(model)<NewLine>     optimizer = torch.optim.SGD(model.parameters(), lr=0.05, weight_decay=5e-4, momentum=0.9)<NewLine>     model.to(device)<NewLine>     train()<NewLine>     test()<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dsuess; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kirk86; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/nooblyh; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/kirk86; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/dsuess; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/nooblyh; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/kirk86; <NewLine> ,"REPLY_DATE 1: September 11, 2019, 11:22am; <NewLine> REPLY_DATE 2: September 10, 2019, 12:07pm; <NewLine> REPLY_DATE 3: September 10, 2019, 12:24pm; <NewLine> REPLY_DATE 4: September 10, 2019, 12:35pm; <NewLine> REPLY_DATE 5: September 10, 2019, 11:56pm; <NewLine> REPLY_DATE 6: September 11, 2019,  1:31am; <NewLine> REPLY_DATE 7: September 11, 2019, 11:27am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
51439,Distributed.init_process_group failure,2019-07-24T00:44:41.298Z,4,313,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am trying to get started with torch.distributed with the following toy example, on a multi-gpu cluster :</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/narumiruna/pytorch-distributed-example/blob/master/toy/main.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/narumiruna/pytorch-distributed-example/blob/master/toy/main.py"" rel=""nofollow noopener"" target=""_blank"">narumiruna/pytorch-distributed-example/blob/master/toy/main.py</a></h4><NewLine><pre><code class=""lang-py"">import argparse<NewLine>from random import randint<NewLine>from time import sleep<NewLine><NewLine>import torch<NewLine>import torch.distributed as dist<NewLine><NewLine><NewLine>def run(world_size, rank, steps):<NewLine>    for step in range(1, steps + 1):<NewLine>        # get random int<NewLine>        value = randint(0, 10)<NewLine><NewLine>        # group all ranks<NewLine>        ranks = list(range(world_size))<NewLine>        group = dist.new_group(ranks=ranks)<NewLine><NewLine>        # compute reduced sum<NewLine>        tensor = torch.tensor(value, dtype=torch.int)<NewLine>        dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group)<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/narumiruna/pytorch-distributed-example/blob/master/toy/main.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>After running the program with the following command :</p><NewLine><pre><code class=""lang-auto"">python3 main.py --init-method tcp://127.0.0.1:23456 --rank 0 --world-size 2<NewLine></code></pre><NewLine><p>The program gets stuck in an the  dist.init_process_group on line 42. I am not really sure about the reason as no message gets displayed.</p><NewLine><p>Thanks,</p><NewLine></div>",https://discuss.pytorch.org/u/MehdiA,,MehdiA,"July 24, 2019, 12:44am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>it’s waiting for both ranks to reach that line to actually initialize the proc group.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Also see the docs for the <a href=""https://pytorch.org/docs/stable/distributed.html#launch-utility"" rel=""nofollow noopener""><code>torch.distributed.launch</code></a> tool.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have launched all the node, but the program still gets stuck in the init_process_group.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>have solved. it is the problem about communication between nodes.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>How did you solve this?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/alchemi5t"">@alchemi5t</a> If you’re running processes on two machines, they won’t be able to talk if you’re using localhost (127.0.0.1) for the address of rank 0 in the initialization method. It must be an IP that’s reachable from all other ranks. In the example here, rank 1 was trying to connect to rank 0 over 127.0.0.1.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/pietern"">@pietern</a>,</p><NewLine><p>I was running it on one machine with 4 cards in it( trying to train only on 2). I fixed my problem by installing and using nvidia Apex(apex.parallel.multiproc).</p><NewLine><p>Not sure why I had to do this, because I’ve seen people use the same script without any hacks like this.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Very odd. Especially since Apex also uses torch.distributed under the hood.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SimonW; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/meilu_zhu; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/meilu_zhu; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/alchemi5t; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/alchemi5t; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: July 25, 2019, 12:28am; <NewLine> REPLY_DATE 2: July 24, 2019,  8:50am; <NewLine> REPLY_DATE 3: August 19, 2019, 12:33pm; <NewLine> REPLY_DATE 4: August 19, 2019,  1:12pm; <NewLine> REPLY_DATE 5: September 6, 2019, 12:00pm; <NewLine> REPLY_DATE 6: September 10, 2019,  1:27pm; <NewLine> REPLY_DATE 7: September 10, 2019, 12:47pm; <NewLine> REPLY_DATE 8: September 10, 2019,  1:18pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
55245,Socket Timeout when wrapping model parallel with DDP,2019-09-05T10:08:20.512Z,0,453,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi guys,</p><NewLine><p>I was trying to wrap my model with DistributedDataParallel. My model is separated into 2 parts, each parnt runs on one GPU. Thus I followed the <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#combine-ddp-with-model-parallelism"" rel=""nofollow noopener"">Combine DDP with Model Parallelism</a> in official tutorial, but after that I encountered with <code>RuntimeError: Socket Timeout</code>.</p><NewLine><p>My codebase is basically like this:</p><NewLine><pre><code class=""lang-python""># fire tasks on SLURM cluster...<NewLine>os.environ[""MASTER_PORT""] = str(port)<NewLine>os.environ[""MASTER_ADDR""] = str(master_ip)<NewLine>os.environ[""WORLD_SIZE""] = str(n_tasks)<NewLine>os.environ[""RANK""] = str(proc_id)<NewLine>dist.init_process_group(backend=dist.Backend.NCCL, timeout=timedelta(seconds=30))<NewLine><NewLine># ...<NewLine>class MyModel(nn.Module)<NewLine>    def __init__(self, ..., device0, device1):<NewLine>        # ...<NewLine>        self.part_1.to(device0)<NewLine>        self.part_2.to(device1)<NewLine><NewLine># task0 get GPU{0, 1}, task1 get GPU(2, 3)...<NewLine>d0 = torch.device(f""cuda:{rank * 2}"")<NewLine>d1 = torch.device(f""cuda:{rank * 2 + 1}"")<NewLine>model = MyModel(..., d0, d1)<NewLine># not all parameters are used in each iteration<NewLine>ddp_model = DistributedDataParallel(model, , find_unused_parameters=True)<NewLine><NewLine># ...<NewLine></code></pre><NewLine><p>Invoking DDP did not raise any error, however after the <code>timeout</code> (30s in my setting), I encountered with following error:</p><NewLine><pre><code class=""lang-bash"">Traceback (most recent call last):<NewLine>  File ""../tools/train_val_classifier.py"", line 332, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""../tools/train_val_classifier.py"", line 103, in main<NewLine>    model, model_without_ddp = get_ddp_model(model, devices=(fp_device, q_device))<NewLine>  File "".../quant_prob/utils/distributed.py"", line 120, in get_ddp_model<NewLine>    ddp_model = DistributedDataParallel(model, device_ids=devices, find_unused_parameters=True)<NewLine>  File ""/envs/r0.3.0/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 286, in __init__<NewLine>    self.broadcast_bucket_size)<NewLine>  File ""/envs/r0.3.0/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 410, in _dist_broadcast_coalesced<NewLine>    dist._dist_broadcast_coalesced(self.process_group, tensors, buffer_size, False)<NewLine>RuntimeError: Socket Timeout<NewLine></code></pre><NewLine><p>Seems that this error came from DDP implementation. I denifitely sure that I followed the official tutorial, and GPUs assiged to each tasks did not overlap. How can I fix this? Thank you so much~</p><NewLine></div>",https://discuss.pytorch.org/u/CrazyRundong,(Rundong Li),CrazyRundong,"September 5, 2019, 10:08am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is a dup of <a href=""https://github.com/pytorch/pytorch/issues/25767"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/25767</a>. Cross linking for posterity.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: September 10, 2019, 11:58am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
55605,"Distributed send/recv (CUDA, MPI backend)",2019-09-10T10:51:06.192Z,0,328,"<div class=""post"" itemprop=""articleBody""><NewLine><p>We’ve built PyTorch from source and tried to call send/recv, but failed. Could you tell me what I am doing wrong? A toy program is here:</p><NewLine><pre><code class=""lang-auto"">  1 import os<NewLine>  2 import socket<NewLine>  3 import torch<NewLine>  4 import torch.distributed as dist<NewLine>  5 from torch.multiprocessing import Process<NewLine>  6 <NewLine>  7 <NewLine>  8 def run(rank, size, hostname):<NewLine>  9     print(f""I am {rank} of {size} in {hostname}"")<NewLine> 10     tensor = torch.zeros(1, device=torch.device('cuda:{}'.format(rank)))<NewLine> 11     if rank == 0:<NewLine> 12         tensor += 1<NewLine> 13         # Send the tensor to process 1<NewLine> 14         dist.send(tensor=tensor, dst=1)<NewLine> 15     else:<NewLine> 16         # Receive tensor from process 0<NewLine> 17         dist.recv(tensor=tensor, src=0)<NewLine> 18     print('Rank ', rank, ' has data ', tensor[0])<NewLine> 19     <NewLine> 20     <NewLine> 21 def init_processes(rank, size, hostname, fn, backend='tcp'):<NewLine> 22     """""" Initialize the distributed environment. """"""<NewLine> 23     dist.init_process_group(backend, rank=rank, world_size=size)<NewLine> 24     fn(rank, size, hostname)<NewLine> 25     <NewLine> 26     <NewLine> 27 if __name__ == ""__main__"":<NewLine> 28     world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])<NewLine> 29     world_rank = int(os.environ['OMPI_COMM_WORLD_RANK'])<NewLine> 30     hostname = socket.gethostname()<NewLine> 31     init_processes(world_rank, world_size, hostname, run, backend='mpi')<NewLine></code></pre><NewLine><p>The cluster which I use is managed using slurm. Here is a list of loaded modules:</p><NewLine><pre><code class=""lang-auto"">  1) /gpu/cuda-10.0          2) /mpi/hpcx-v2.4.0        3) /python/python-3.6.8    4) /python/pytorch-1.3.0,<NewLine></code></pre><NewLine><p>where pytorch-1.3.0 is installed from source.</p><NewLine><p>And this is how I call this script:</p><NewLine><pre><code class=""lang-auto"">mpirun -np 2 python3 pytorch_distributed.py<NewLine></code></pre><NewLine><p>The error looks as follows</p><NewLine><pre><code class=""lang-auto"">I am 1 of 2 in gn10.zhores<NewLine>I am 0 of 2 in gn10.zhores<NewLine>--------------------------------------------------------------------------<NewLine>Primary job  terminated normally, but 1 process returned<NewLine>a non-zero exit code. Per user-direction, the job has been aborted.<NewLine>--------------------------------------------------------------------------<NewLine>--------------------------------------------------------------------------<NewLine>mpirun noticed that process rank 0 with PID 11502 on node gn10 exited on signal 11 (Segmentation fault).<NewLine>--------------------------------------------------------------------------<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/spaceinvader,(Space Invader),spaceinvader,"September 10, 2019, 12:54pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t know what’s wrong, but I do know it’s likely going wrong somewhere in the MPI code.</p><NewLine><p>PyTorch performs a runtime check for CUDA awareness of the MPI distribution before running any collective (including send/recv) with a CUDA tensor. You’re passing CUDA tensors, and not seeing this error, so the runtime check must be successful. What happens beyond that, in the MPI code, is beyond my purview. To be sure there is nothing wrong with your code you can try running it with CPU side tensors. If that passes, and it only fails with CUDA tensors, I’d try and run MPI in some kind of debug mode. Good luck!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: September 10, 2019, 11:46am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
55090,Can&rsquo;t figure out what i&rsquo;m doing wrong,2019-09-03T23:26:18.051Z,4,846,"<div class=""post"" itemprop=""articleBody""><NewLine><p>EDIT: skip to the bottom</p><NewLine><p>i’m training SRGAN on VOC2012 using NVIDIA DALI and experimenting between DataParallel and DistributedDataParallel (I was using apex too but I’ve removed it in order to figure out what’s going wrong).</p><NewLine><p>here is my <code>run.sh</code></p><NewLine><pre><code class=""lang-bash"">python -m torch.distributed.launch \<NewLine>  --nproc_per_node=1 \<NewLine>  train_srgan_dali.py \<NewLine>  --train-mx-path=/home/maksim/data/VOC2012/voc_train.rec \<NewLine>  --train-mx-index-path=/home/maksim/data/VOC2012/voc_train.idx \<NewLine>  --val-mx-path=/home/maksim/data/VOC2012/voc_val.rec \<NewLine>  --val-mx-index-path=/home/maksim/data/VOC2012/voc_val.idx \<NewLine>  --checkpoint-dir=/home/maksim/dev_projects/atlas_sr/checkpoints \<NewLine>  --experiment-name=srgan_dali_pascal_3_channel_icnr_dp \<NewLine>  --batch-size=64 \<NewLine>  --lr=1e-3 \<NewLine>  --crop-size=88 \<NewLine>  --upscale-factor=2 \<NewLine>  --epochs=100 \<NewLine>  --workers=1 \<NewLine>  --channels=3<NewLine></code></pre><NewLine><p><a href=""https://github.com/makslevental/atlas_sr/blob/master/models/SRGAN.py"" rel=""nofollow noopener"">here</a> is my model and here is my train script</p><NewLine><pre><code class=""lang-python"">import argparse<NewLine>import os<NewLine>import time<NewLine>from math import log10<NewLine><NewLine>import pandas as pd<NewLine>import torch<NewLine>import torch.backends.cudnn<NewLine>import torch.distributed<NewLine>from nvidia.dali import types<NewLine>from torch import nn<NewLine><NewLine>from data_utils.dali import StupidDALIIterator, SRGANMXNetPipeline<NewLine>from metrics.metrics import AverageMeter<NewLine>from metrics.ssim import ssim<NewLine>from models.SRGAN import (<NewLine>    Generator,<NewLine>    Discriminator,<NewLine>    GeneratorLoss,<NewLine>)<NewLine>from util.util import monkey_patch_bn<NewLine><NewLine>monkey_patch_bn()<NewLine><NewLine>parser = argparse.ArgumentParser()<NewLine>parser.add_argument(""--local_rank"", default=0, type=int)<NewLine>parser.add_argument(""--channels"", type=int, default=3)<NewLine>parser.add_argument(""--experiment-name"", type=str, default=""test"")<NewLine>parser.add_argument(<NewLine>    ""--train-mx-path"", default=""/home/maksim/data/VOC2012/voc_train.rec""<NewLine>)<NewLine>parser.add_argument(<NewLine>    ""--train-mx-index-path"", default=""/home/maksim/data/VOC2012/voc_train.idx""<NewLine>)<NewLine>parser.add_argument(""--val-mx-path"", default=""/home/maksim/data/VOC2012/voc_val.rec"")<NewLine>parser.add_argument(<NewLine>    ""--val-mx-index-path"", default=""/home/maksim/data/VOC2012/voc_val.idx""<NewLine>)<NewLine>parser.add_argument(""--checkpoint-dir"", default=""/home/maksim/data/checkpoints"")<NewLine>parser.add_argument(""--upscale-factor"", type=int, default=2)<NewLine>parser.add_argument(""--epochs"", type=int, default=100)<NewLine>parser.add_argument(""--batch-size"", type=int, default=64)<NewLine>parser.add_argument(""--prof"", action=""store_true"", default=False)<NewLine>parser.add_argument(""--lr"", type=float, default=1e-3)<NewLine>parser.add_argument(""--crop-size"", type=int, default=88)<NewLine>parser.add_argument(""--workers"", type=int, default=4)<NewLine><NewLine>args = parser.parse_args()<NewLine>local_rank = args.local_rank<NewLine>train_mx_path = args.train_mx_path<NewLine>train_mx_index_path = args.train_mx_index_path<NewLine>val_mx_path = args.val_mx_path<NewLine>val_mx_index_path = args.val_mx_index_path<NewLine>experiment_name = args.experiment_name<NewLine>checkpoint_dir = args.checkpoint_dir<NewLine>upscale_factor = args.upscale_factor<NewLine>epochs = args.epochs<NewLine>batch_size = args.batch_size<NewLine>crop_size = args.crop_size<NewLine>prof = args.prof<NewLine>workers = args.workers<NewLine>lr = args.lr<NewLine>channels = args.channels<NewLine>print_freq = 10<NewLine><NewLine>assert os.path.exists(train_mx_path)<NewLine>assert os.path.exists(train_mx_index_path)<NewLine>assert os.path.exists(val_mx_path)<NewLine>assert os.path.exists(val_mx_index_path)<NewLine>assert experiment_name<NewLine>assert os.path.exists(checkpoint_dir)<NewLine><NewLine>distributed = False<NewLine>world_size = 1<NewLine><NewLine>if local_rank == 0:<NewLine>    checkpoint_dir = os.path.join(checkpoint_dir, experiment_name)<NewLine>    if not os.path.exists(checkpoint_dir):<NewLine>        os.mkdir(checkpoint_dir)<NewLine><NewLine>if ""WORLD_SIZE"" in os.environ:<NewLine>    world_size = int(os.environ[""WORLD_SIZE""])<NewLine>    distributed = world_size &gt; 1<NewLine><NewLine>netG = Generator(scale_factor=upscale_factor, in_channels=channels)<NewLine>netD = Discriminator(in_channels=channels)<NewLine>g = GeneratorLoss()<NewLine>if distributed:<NewLine>    gpu = local_rank % torch.cuda.device_count()<NewLine>    torch.cuda.set_device(gpu)<NewLine>    torch.distributed.init_process_group(backend=""nccl"", init_method=""env://"")<NewLine>    assert world_size == torch.distributed.get_world_size()<NewLine>    netG = nn.SyncBatchNorm.convert_sync_batchnorm(netG)<NewLine>    netD = nn.SyncBatchNorm.convert_sync_batchnorm(netD)<NewLine>    netG.cuda(gpu)<NewLine>    netD.cuda(gpu)<NewLine>    g.cuda(gpu)<NewLine>    netG = nn.parallel.DistributedDataParallel(netG, device_ids=[gpu])<NewLine>    netD = nn.parallel.DistributedDataParallel(netD, device_ids=[gpu])<NewLine>    lr /= world_size<NewLine>else:<NewLine>    netG = Generator(scale_factor=upscale_factor, in_channels=channels)<NewLine>    netD = Discriminator(in_channels=channels)<NewLine>    netG = nn.DataParallel(netG)<NewLine>    netD = nn.DataParallel(netD)<NewLine>    netG = netG.cuda()<NewLine>    netD = netD.cuda()<NewLine>    g = g.cuda()<NewLine><NewLine># because vgg excepts 3 channels<NewLine>if channels == 1:<NewLine>    generator_loss = lambda fake_out, fake_img, hr_image: g(<NewLine>        fake_out,<NewLine>        torch.cat([fake_img, fake_img, fake_img], dim=1),<NewLine>        torch.cat([hr_image, hr_image, hr_image], dim=1),<NewLine>    )<NewLine>else:<NewLine>    generator_loss = g<NewLine><NewLine>optimizerG = torch.optim.Adam(netG.parameters(), lr=lr)<NewLine>optimizerD = torch.optim.Adam(netD.parameters(), lr=lr)<NewLine><NewLine>train_pipe = SRGANMXNetPipeline(<NewLine>    batch_size=batch_size,<NewLine>    num_gpus=world_size,<NewLine>    num_threads=workers,<NewLine>    device_id=local_rank,<NewLine>    crop=crop_size,<NewLine>    mx_path=train_mx_path,<NewLine>    mx_index_path=train_mx_index_path,<NewLine>    upscale_factor=upscale_factor,<NewLine>    image_type=types.DALIImageType.RGB,<NewLine>)<NewLine>train_pipe.build()<NewLine>train_loader = StupidDALIIterator(<NewLine>    pipelines=[train_pipe],<NewLine>    output_map=[""lr_image"", ""hr_image""],<NewLine>    size=int(train_pipe.epoch_size(""Reader"") / world_size),<NewLine>    auto_reset=False,<NewLine>)<NewLine>val_pipe = SRGANMXNetPipeline(<NewLine>    batch_size=batch_size,<NewLine>    num_gpus=world_size,<NewLine>    num_threads=workers,<NewLine>    device_id=local_rank,<NewLine>    crop=crop_size,<NewLine>    mx_path=val_mx_path,<NewLine>    mx_index_path=val_mx_index_path,<NewLine>    upscale_factor=upscale_factor,<NewLine>    random_shuffle=False,<NewLine>    image_type=types.DALIImageType.RGB,<NewLine>)<NewLine>val_pipe.build()<NewLine>val_loader = StupidDALIIterator(<NewLine>    pipelines=[val_pipe],<NewLine>    output_map=[""lr_image"", ""hr_image""],<NewLine>    size=int(val_pipe.epoch_size(""Reader"") / world_size),<NewLine>    auto_reset=False,<NewLine>)<NewLine><NewLine>g_loss_meter = AverageMeter(""g_loss"")<NewLine>d_loss_meter = AverageMeter(""d_loss"")<NewLine>sample_speed_meter = AverageMeter(""sample_speed"")<NewLine><NewLine><NewLine>def train(epoch):<NewLine>    g_loss_meter.reset()<NewLine>    d_loss_meter.reset()<NewLine>    sample_speed_meter.reset()<NewLine>    netG.train()<NewLine>    netD.train()<NewLine><NewLine>    for i, (lr_image, hr_image) in enumerate(train_loader):<NewLine>        start = time.time()<NewLine>        batch_size = lr_image.shape[0]<NewLine><NewLine>        if prof and i &gt; 10:<NewLine>            break<NewLine><NewLine>        ############################<NewLine>        # (1) Update D network: maximize D(x)-1-D(G(z))<NewLine>        ##########################<NewLine>        fake_img = netG(lr_image)<NewLine><NewLine>        netD.zero_grad()<NewLine>        real_out = netD(hr_image).mean()<NewLine>        fake_out = netD(fake_img).mean()<NewLine>        d_loss = 1 - real_out + fake_out<NewLine>        d_loss_meter.update(d_loss.item())<NewLine>        d_loss.backward(retain_graph=True)<NewLine>        optimizerD.step()<NewLine><NewLine>        ############################<NewLine>        # (2) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss<NewLine>        ###########################<NewLine>        netG.zero_grad()<NewLine>        g_loss = generator_loss(fake_out, fake_img, hr_image)<NewLine>        g_loss_meter.update(g_loss.item())<NewLine>        g_loss.backward()<NewLine>        optimizerG.step()<NewLine><NewLine>        sample_speed_meter.update(world_size * batch_size / (time.time() - start))<NewLine><NewLine>        if local_rank == 0 and i % print_freq == 0:<NewLine>            print(<NewLine>                ""\t"".join(<NewLine>                    [<NewLine>                        f""epoch {epoch}"",<NewLine>                        f""step {i + 1}/{train_loader.size // batch_size}"",<NewLine>                        str(sample_speed_meter),<NewLine>                        str(d_loss_meter),<NewLine>                        str(g_loss_meter),<NewLine>                    ]<NewLine>                )<NewLine>            )<NewLine><NewLine><NewLine>mse_meter = AverageMeter(""mse"")<NewLine>ssim_meter = AverageMeter(""ssim"")<NewLine>psnr_meter = AverageMeter(""psnr"")<NewLine><NewLine><NewLine>def validate(epoch):<NewLine>    mse_meter.reset()<NewLine>    ssim_meter.reset()<NewLine>    psnr_meter.reset()<NewLine>    netG.eval()<NewLine>    for i, (lr_image, hr_image) in enumerate(val_loader):<NewLine>        batch_size = lr_image.shape[0]<NewLine>        if prof and i &gt; 10:<NewLine>            break<NewLine><NewLine>        with torch.no_grad():<NewLine>            sr_image = netG(lr_image)<NewLine><NewLine>        batch_mse = ((sr_image - hr_image) ** 2).mean()<NewLine>        batch_ssim = ssim(sr_image, hr_image)<NewLine><NewLine>        mse_meter.update(batch_mse.item(), batch_size)<NewLine>        ssim_meter.update(batch_ssim.item(), batch_size)<NewLine><NewLine>    psnr_meter.update(10 * log10(1 / mse_meter.avg))<NewLine><NewLine>    if local_rank == 0:<NewLine>        print(<NewLine>            ""\t"".join(<NewLine>                [<NewLine>                    ""\033[1;31m"" f""epoch {epoch}"",<NewLine>                    str(mse_meter),<NewLine>                    str(ssim_meter),<NewLine>                    str(psnr_meter),<NewLine>                    ""\033[1;0m"",<NewLine>                ]<NewLine>            )<NewLine>        )<NewLine><NewLine><NewLine>epoch_time_meter = AverageMeter(""epoch"")<NewLine><NewLine>running_meters = {<NewLine>    ""g_loss"": [],<NewLine>    ""d_loss"": [],<NewLine>    ""sample_speed"": [],<NewLine>    ""mse"": [],<NewLine>    ""ssim"": [],<NewLine>    ""psnr"": [],<NewLine>    ""epoch_time"": [],<NewLine>}<NewLine><NewLine><NewLine>def update_running_meters():<NewLine>    global running_meters<NewLine>    running_meters[""g_loss""].append(g_loss_meter.avg)<NewLine>    running_meters[""d_loss""].append(d_loss_meter.avg)<NewLine>    running_meters[""sample_speed""].append(sample_speed_meter.avg)<NewLine>    running_meters[""mse""].append(mse_meter.avg)<NewLine>    running_meters[""ssim""].append(ssim_meter.avg)<NewLine>    running_meters[""psnr""].append(psnr_meter.avg)<NewLine>    running_meters[""epoch_time""].append(epoch_time_meter.val)<NewLine><NewLine><NewLine>def main():<NewLine>    for epoch in range(epochs):<NewLine>        start = time.time()<NewLine>        train(epoch)<NewLine>        validate(epoch)<NewLine>        if local_rank == 0:<NewLine>            torch.save(<NewLine>                netG.state_dict(),<NewLine>                f""{checkpoint_dir}/netG_epoch_{upscale_factor}_{epoch}.pth"",<NewLine>            )<NewLine>            torch.save(<NewLine>                netD.state_dict(),<NewLine>                f""{checkpoint_dir}/netD_epoch_{upscale_factor}_{epoch}.pth"",<NewLine>            )<NewLine>            epoch_time_meter.update(time.time() - start)<NewLine>            update_running_meters()<NewLine>            if epoch != 0 and not prof:<NewLine>                data_frame = pd.DataFrame(data=running_meters)<NewLine>                data_frame.to_csv(<NewLine>                    os.path.join(checkpoint_dir, ""metrics.csv""), index_label=""Epoch""<NewLine>                )<NewLine><NewLine>        val_loader.reset()<NewLine>        train_loader.reset()<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    main()<NewLine></code></pre><NewLine><p>when switching between using <code>DataParallel</code> and <code>DistributedDataParallel</code> I get drastically different <code>psnr</code> performance. I’ve found <a href=""https://discuss.pytorch.org/t/training-performance-degrades-with-distributeddataparallel/47152"">this</a> post but none of the solutions seems to work. That’s one matter (the difference between averaging and summing gradients). The other matter, the thing that I can’t for the life of me figure out, is the difference in how my losses behave in both cases. Here is a trace of my losses (for one epoch) if I train using <code>DataParallel</code></p><NewLine><pre><code class=""lang-auto"">epoch 7 step 1/241        d_loss 0.999999      g_loss 0.005589 <NewLine>epoch 7 step 11/241       d_loss 0.999999      g_loss 0.005433 <NewLine>epoch 7 step 21/241       d_loss 0.999998      g_loss 0.004887 <NewLine>epoch 7 step 31/241       d_loss 1.000002      g_loss 0.004837 <NewLine>epoch 7 step 41/241       d_loss 1.000000      g_loss 0.004958 <NewLine>epoch 7 step 51/241       d_loss 1.000000      g_loss 0.004784 <NewLine>epoch 7 step 61/241       d_loss 1.000000      g_loss 0.005808 <NewLine>epoch 7 step 71/241       d_loss 0.999979      g_loss 0.005283 <NewLine>epoch 7 step 81/241       d_loss 1.000003      g_loss 0.005585 <NewLine>epoch 7 step 91/241       d_loss 0.999999      g_loss 0.004718 <NewLine>epoch 7 step 101/241      d_loss 0.999999      g_loss 0.006046 <NewLine>epoch 7 step 111/241      d_loss 0.999978      g_loss 0.005157 <NewLine>epoch 7 step 121/241      d_loss 1.000007      g_loss 0.006780 <NewLine>epoch 7 step 131/241      d_loss 1.000001      g_loss 0.005851 <NewLine>epoch 7 step 141/241      d_loss 1.000000      g_loss 0.005644 <NewLine>epoch 7 step 151/241      d_loss 0.999986      g_loss 0.005973 <NewLine>epoch 7 step 161/241      d_loss 1.000002      g_loss 0.005687 <NewLine>epoch 7 step 171/241      d_loss 1.000012      g_loss 0.006535 <NewLine>epoch 7 step 181/241      d_loss 0.999999      g_loss 0.005457 <NewLine>epoch 7 step 191/241      d_loss 0.999999      g_loss 0.005313 <NewLine>epoch 7 step 201/241      d_loss 1.000000      g_loss 0.006094 <NewLine>epoch 7 step 211/241      d_loss 1.000000      g_loss 0.006187 <NewLine>epoch 7 step 221/241      d_loss 1.000116      g_loss 0.005385 <NewLine>epoch 7 step 231/241      d_loss 0.999931      g_loss 0.005718 <NewLine>epoch 7 step 241/241      d_loss 0.999774      g_loss 0.005635 <NewLine><NewLine></code></pre><NewLine><p>From my understanding (and by watching the psnr) this is how the losses should trend for SRGAN.</p><NewLine><p>Now here are my losses when using <code>DistributedDataParallel</code> (across several epochs to show the trend)</p><NewLine><pre><code class=""lang-auto"">epoch 0 step 1/60       d_loss 1.000204 (1.000204)      g_loss 0.153849 (0.153849)<NewLine>epoch 0 step 11/60      d_loss 0.974728 (0.965737)      g_loss 0.019822 (0.058211)<NewLine>epoch 0 step 21/60      d_loss 0.468546 (0.831723)      g_loss 0.015897 (0.038876)<NewLine>epoch 0 step 31/60      d_loss 0.230158 (0.677370)      g_loss 0.014611 (0.031437)<NewLine>epoch 0 step 41/60      d_loss 0.077666 (0.544439)      g_loss 0.014681 (0.027434)<NewLine>epoch 0 step 51/60      d_loss 0.020034 (0.447585)      g_loss 0.011524 (0.024474)<NewLine>epoch 0 step 61/60      d_loss 0.013507 (0.378103)      g_loss 0.011936 (0.022396)<NewLine>epoch 0 mse 0.006693 (0.007545) ssim 0.661945 (0.645649)        psnr 21.223185 (21.223185)<NewLine>epoch 1 step 1/60       d_loss 0.019439 (0.019439)      g_loss 0.010366 (0.010366)<NewLine>epoch 1 step 11/60      d_loss 0.009224 (0.010984)      g_loss 0.009906 (0.010792)<NewLine>epoch 1 step 21/60      d_loss 0.003987 (0.008465)      g_loss 0.011732 (0.010643)<NewLine>epoch 1 step 31/60      d_loss 0.007867 (0.007535)      g_loss 0.009154 (0.010312)<NewLine>epoch 1 step 41/60      d_loss 0.003442 (0.006837)      g_loss 0.010357 (0.010266)<NewLine>epoch 1 step 51/60      d_loss 0.003987 (0.005997)      g_loss 0.010241 (0.010080)<NewLine>epoch 1 mse 0.004144 (0.004839) ssim 0.746634 (0.726122)        psnr 23.152690 (23.152690)<NewLine>epoch 2 step 1/60       d_loss 0.006586 (0.006586)      g_loss 0.009223 (0.009223)<NewLine>epoch 2 step 11/60      d_loss 0.859120 (0.566964)      g_loss 0.008221 (0.008524)<NewLine>epoch 2 step 21/60      d_loss 0.876267 (0.731556)      g_loss 0.008248 (0.008669)<NewLine>epoch 2 step 31/60      d_loss 0.665335 (0.739961)      g_loss 0.010071 (0.008873)<NewLine>epoch 2 step 41/60      d_loss 0.508060 (0.741789)      g_loss 0.007758 (0.009077)<NewLine>epoch 2 step 51/60      d_loss 0.533404 (0.670928)      g_loss 0.007410 (0.008923)<NewLine>epoch 2 mse 0.004435 (0.004207) ssim 0.733819 (0.747270)        psnr 23.760117 (23.760117)<NewLine>epoch 3 step 1/60       d_loss 0.976557 (0.976557)      g_loss 0.008353 (0.008353)<NewLine>epoch 3 step 11/60      d_loss 0.873007 (0.948327)      g_loss 0.010379 (0.008218)<NewLine>epoch 3 step 21/60      d_loss 0.688478 (0.868267)      g_loss 0.006677 (0.008104)<NewLine>epoch 3 step 31/60      d_loss 0.256862 (0.726863)      g_loss 0.007438 (0.008090)<NewLine>epoch 3 step 41/60      d_loss 0.101930 (0.586502)      g_loss 0.008943 (0.007990)<NewLine>epoch 3 step 51/60      d_loss 0.073482 (0.483037)      g_loss 0.009807 (0.007858)<NewLine>epoch 3 mse 0.003936 (0.003998) ssim 0.749274 (0.763466)        psnr 23.981862 (23.981862)<NewLine></code></pre><NewLine><p>Notice that in this case <code>d_loss</code> goes to zero rather than to 1.</p><NewLine><p>I’ve been wrestling with it for several days and I can’t for the life of me figure out what is I’m doing wrong in switching from <code>DataParallel</code> to <code>DistributedDataParallel</code> that causes this kind of behavior.</p><NewLine><p>EDIT:</p><NewLine><p>life lesson: this is what happens when you copy paste code without understanding completely. i copied this code from <a href=""https://github.com/NVIDIA/DALI/blob/master/docs/examples/pytorch/resnet50/main.py"" rel=""nofollow noopener"">https://github.com/NVIDIA/DALI/blob/master/docs/examples/pytorch/resnet50/main.py</a></p><NewLine><p>and adapted it for my needs. the problem turned out to be that in the original code</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/NVIDIA/DALI/blob/master/docs/examples/pytorch/resnet50/main.py#L329"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/NVIDIA/DALI/blob/master/docs/examples/pytorch/resnet50/main.py#L329"" rel=""nofollow noopener"" target=""_blank"">NVIDIA/DALI/blob/master/docs/examples/pytorch/resnet50/main.py#L329</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""319"" style=""counter-reset: li-counter 318 ;""><NewLine><li>input_var = Variable(input)</li><NewLine><li>target_var = Variable(target)</li><NewLine><li><NewLine></li><NewLine><li># compute output</li><NewLine><li>output = model(input_var)</li><NewLine><li>loss = criterion(output, target_var)</li><NewLine><li><NewLine></li><NewLine><li># measure accuracy and record loss</li><NewLine><li>prec1, prec5 = accuracy(output.data, target, topk=(1, 5))</li><NewLine><li><NewLine></li><NewLine><li class=""selected"">if args.distributed:</li><NewLine><li>    reduced_loss = reduce_tensor(loss.data)</li><NewLine><li>    prec1 = reduce_tensor(prec1)</li><NewLine><li>    prec5 = reduce_tensor(prec5)</li><NewLine><li>else:</li><NewLine><li>    reduced_loss = loss.data</li><NewLine><li><NewLine></li><NewLine><li>losses.update(to_python_float(reduced_loss), input.size(0))</li><NewLine><li>top1.update(to_python_float(prec1), input.size(0))</li><NewLine><li>top5.update(to_python_float(prec5), input.size(0))</li><NewLine><li><NewLine></li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>there’s a reduce when logging the metrics. i misread and misinterpreted simultaneously: i missed that the backwards pass is actually run on the unreduced loss further down and misinterpreted <code>delay_allreduce</code><br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/NVIDIA/DALI/blob/master/docs/examples/pytorch/resnet50/main.py#L210"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/NVIDIA/DALI/blob/master/docs/examples/pytorch/resnet50/main.py#L210"" rel=""nofollow noopener"" target=""_blank"">NVIDIA/DALI/blob/master/docs/examples/pytorch/resnet50/main.py#L210</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""200"" style=""counter-reset: li-counter 199 ;""><NewLine><li>else:</li><NewLine><li>    print(""=&gt; creating model '{}'"".format(args.arch))</li><NewLine><li>    model = models.__dict__[args.arch]()</li><NewLine><li><NewLine></li><NewLine><li>model = model.cuda()</li><NewLine><li>if args.fp16:</li><NewLine><li>    model = network_to_half(model)</li><NewLine><li>if args.distributed:</li><NewLine><li>    # shared param/delay all reduce turns off bucketing in DDP, for lower latency runs this can improve perf</li><NewLine><li>    # for the older version of APEX please use shared_param, for newer one it is delay_allreduce</li><NewLine><li class=""selected"">    model = DDP(model, delay_allreduce=True)</li><NewLine><li><NewLine></li><NewLine><li># define loss function (criterion) and optimizer</li><NewLine><li>criterion = nn.CrossEntropyLoss().cuda()</li><NewLine><li><NewLine></li><NewLine><li>optimizer = torch.optim.SGD(model.parameters(), args.lr,</li><NewLine><li>                            momentum=args.momentum,</li><NewLine><li>                            weight_decay=args.weight_decay)</li><NewLine><li>if args.fp16:</li><NewLine><li>    optimizer = FP16_Optimizer(optimizer,</li><NewLine><li>                               static_loss_scale=args.static_loss_scale,</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>to mean that <code>apex.DistributedDataParallel</code> wouldn’t be doing any reduce at all and that the <code>reduce_tensor</code> call was necessarily done by hand.</p><NewLine><p>so in summary i was dividing my loss by <code>world_size</code> unnecessarily.</p><NewLine></div>",https://discuss.pytorch.org/u/makslevental,(Maksim Levental),makslevental,"September 5, 2019,  5:35pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/makslevental"">@makslevental</a>,</p><NewLine><p>I’m glad that you solved it! <img alt="":slightly_smiling_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slightly_smiling_face.png?v=9"" title="":slightly_smiling_face:""/></p><NewLine><p>I am curious, which Reader are you using in DALI to load VOC2012? Are you using <a href=""https://docs.nvidia.com/deeplearning/sdk/dali-developer-guide/docs/supported_ops.html#nvidia.dali.ops.ExternalSource"" rel=""nofollow noopener"">ExternalSource</a> or a custom operator?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/spanev"">@spanev</a></p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/makslevental/atlas_sr/blob/master/data_utils/dali.py#L186"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/makslevental/atlas_sr/blob/master/data_utils/dali.py#L186"" rel=""nofollow noopener"" target=""_blank"">makslevental/atlas_sr/blob/master/data_utils/dali.py#L186</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""176"" style=""counter-reset: li-counter 175 ;""><NewLine><li>    ):</li><NewLine><li>        super(SRGANMXNetPipeline, self).__init__(</li><NewLine><li>            batch_size,</li><NewLine><li>            num_threads,</li><NewLine><li>            device_id,</li><NewLine><li>            crop,</li><NewLine><li>            upscale_factor,</li><NewLine><li>            image_type,</li><NewLine><li>            dali_cpu,</li><NewLine><li>        )</li><NewLine><li class=""selected"">        self.input = ops.MXNetReader(</li><NewLine><li>            path=[mx_path],</li><NewLine><li>            index_path=[mx_index_path],</li><NewLine><li>            random_shuffle=random_shuffle,</li><NewLine><li>            shard_id=device_id,</li><NewLine><li>            num_shards=num_gpus,</li><NewLine><li>        )</li><NewLine><li><NewLine></li><NewLine><li><NewLine></li><NewLine><li>class SRGANFilePipeline(SRGANPipeline):</li><NewLine><li>    def __init__(</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p><a class=""mention"" href=""/u/spanev"">@spanev</a> since it looks like you work for on DALI: can you tell me why the operators aren’t more orthogonal? for example I really would like to be able to normalize without cropping, to change imagetypes outside of the decoder, or resize but after crop (i.e. I’d like to use decoder, crop, and resize independently of one another but I can’t because resize expects uint8). I’m not complaining (thanks for the toolkit!) I’m just wondering if it’s something having to do with how the cuda kernels are compiled or it’s just a design choice.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It is great to see that you are able to use it to accelerate your training. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>DALI is still under active development (and technically still in <a href=""https://docs.nvidia.com/deeplearning/sdk/dali-release-notes/index.html"" rel=""nofollow noopener"">beta</a>).<br/><NewLine>The team is currently reworking the whole architecture and working on some major missing features (such as pointwise operations).</p><NewLine><p>A few notes about the operators you mentioned:</p><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""55090""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/makslevental/40/15605_2.png"" width=""20""/> makslevental:</div><NewLine><blockquote><NewLine><p>for example I really would like to be able to normalize without cropping</p><NewLine></blockquote><NewLine></aside><NewLine><p>I know it may seem a little bit counter-intuitive but you should be using <a href=""https://docs.nvidia.com/deeplearning/sdk/dali-release-notes/index.html"" rel=""nofollow noopener"">CropMirrorNormalize</a> (minus the crop and mirror options). The DALI and CUDA kernels have compile time mechanisms to reduce the runtime overhead (to none).</p><NewLine><p>We may add an <code>Normalize</code> op in the future but it would still use the same kernels under the hood.</p><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""55090""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/makslevental/40/15605_2.png"" width=""20""/> makslevental:</div><NewLine><blockquote><NewLine><p>to change imagetypes outside of the decoder</p><NewLine></blockquote><NewLine></aside><NewLine><p>You can actually do it with the <a href=""https://docs.nvidia.com/deeplearning/sdk/dali-developer-guide/docs/supported_ops.html#nvidia.dali.ops.Cast"" rel=""nofollow noopener"">Cast</a> operator.</p><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""55090""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/makslevental/40/15605_2.png"" width=""20""/> makslevental:</div><NewLine><blockquote><NewLine><p>or resize but after crop</p><NewLine></blockquote><NewLine></aside><NewLine><p>I guess you mean Crop after Resize.</p><NewLine><p>This limitation comes from the fact that Crop and Resize operations are (sorta) commutative, but performance-wise it is better to first crop first to not apply the interpolation algorithm on a data that will be cropped anyway.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I know it may seem a little bit counter-intuitive but you should be using <a href=""https://docs.nvidia.com/deeplearning/sdk/dali-release-notes/index.html"" rel=""nofollow noopener"">CropMirrorNormalize</a> (minus the crop and mirror options). The DALI and CUDA kernels have compile time mechanisms to reduce the runtime overhead (to none).</p><NewLine></blockquote><NewLine><p>how can you use CropMirrorNormalize without crop? not setting any values for any of the crop parameters simply crops to half because of <code>default = 0.5</code> for both of <code>crop_pos_x</code> and <code>crop_pos_y</code>.</p><NewLine><blockquote><NewLine><p>You can actually do it with the <a href=""https://docs.nvidia.com/deeplearning/sdk/dali-developer-guide/docs/supported_ops.html#nvidia.dali.ops.Cast"" rel=""nofollow noopener"">Cast</a> operator.</p><NewLine></blockquote><NewLine><p>how? i tried to cast prior to resize (because resize expects uint8) but i just … a cast i.e. my images were all black because i got truncation of the floats the came out of the decoder.</p><NewLine><blockquote><NewLine><p>I guess you mean Crop after Resize.</p><NewLine></blockquote><NewLine><p>sorry yes you’re right.</p><NewLine><blockquote><NewLine><p>but performance-wise it is better to first crop first to not apply the interpolation algorithm on a data that will be cropped anyway.</p><NewLine></blockquote><NewLine><p>that makes sense but i feel like i should be able to choose to take the penalty. just so we’re concrete: i would like to crop a high resolution image randomly, then resize to half (or quarter scale) in order to produce a low resolution image (i’m working on super resolution networks). right now i do using decodercrop then resize and i have to normalize in just plain pytorch</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/makslevental/atlas_sr/blob/master/data_utils/dali.py#L96"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/makslevental/atlas_sr/blob/master/data_utils/dali.py#L96"" rel=""nofollow noopener"" target=""_blank"">makslevental/atlas_sr/blob/master/data_utils/dali.py#L96</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""86"" style=""counter-reset: li-counter 85 ;""><NewLine><li># https://github.com/NVIDIA/DALI/issues/1227</li><NewLine><li>def __init__(self, *args, **kwargs):</li><NewLine><li>    self.dali_iter = DALIGenericIterator(*args, **kwargs)</li><NewLine><li><NewLine></li><NewLine><li>def __iter__(self):</li><NewLine><li>    return self</li><NewLine><li><NewLine></li><NewLine><li>def __next__(self):</li><NewLine><li>    n = next(self.dali_iter)</li><NewLine><li>    lr_image, hr_image = n[0][""lr_image""], n[0][""hr_image""]</li><NewLine><li class=""selected"">    hr_image = hr_image.to(torch.float).div(255)</li><NewLine><li>    lr_image = lr_image.to(torch.float).div(255)</li><NewLine><li>    hr_image = hr_image.permute(0, 3, 1, 2)</li><NewLine><li>    lr_image = lr_image.permute(0, 3, 1, 2)</li><NewLine><li>    return lr_image, hr_image</li><NewLine><li><NewLine></li><NewLine><li>@property</li><NewLine><li>def size(self):</li><NewLine><li>    return self.dali_iter._size</li><NewLine><li><NewLine></li><NewLine><li># hack to make lr_finder work</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>not ideal. i want to compose decoder -&gt; cropmirrornormalize -&gt; resize but i can’t because of data width mismatches (if i recall correctly resize complains about not getting uint8s for this composition).</p><NewLine><blockquote><NewLine><p>It is great to see that you are able to use it to accelerate your training.</p><NewLine></blockquote><NewLine><p>yes it’s definitely great - i’m able to saturate my gpus (sometimes a little too much and they get hot <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/> .</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/spanev; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/makslevental; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/spanev; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/makslevental; <NewLine> ,"REPLY_DATE 1: September 5, 2019, 10:26pm; <NewLine> REPLY_DATE 2: September 5, 2019, 11:50pm; <NewLine> REPLY_DATE 3: September 6, 2019, 11:37am; <NewLine> REPLY_DATE 4: September 6, 2019,  9:22pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
54947,Clueless halt while GPU is still running! ,2019-09-02T10:19:55.759Z,1,113,"<div class=""post"" itemprop=""articleBody""><NewLine><p>While I use the distributed training, the training process encounters clueless halt. The GPU-Memory and GPU-Util (90%-100%) of all GPUs are normal with no pid being killed!!!</p><NewLine></div>",https://discuss.pytorch.org/u/youansheng,(Donny You),youansheng,"September 2, 2019,  1:18pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>In my case, it shows like a 100% GPU utility occupation there on the GPU. And some cores of CPU are taken as well but it’s weird that actually nothing is ongoing and the training program seems just like “dead”. We use the same repo here: <a href=""https://github.com/donnyyou/torchcv"" rel=""nofollow noopener"">TorchCV</a>.</p><NewLine><p>I tried to stop the program with Ctrl+C or simply kill the program. I was expecting some error info but nothing was printed.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Does anyone encounter this kind of problem before? It would be also great if anybody has some techniques to localize the problem or at least print something.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jessemelpolio; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jessemelpolio; <NewLine> ,"REPLY_DATE 1: September 2, 2019,  1:23pm; <NewLine> REPLY_DATE 2: September 2, 2019,  1:27pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
49343,How to preserve backward grad_fn after distributed operations,2019-06-30T14:40:51.982Z,10,1300,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to implement model parallelism in a distributed cluster setting.</p><NewLine><p>Let’s say I have a tensor <code>tensor</code> in each process and a number of operations have been performed on it (in each process independently). The tensor has a <code>.grad_fn</code> attached to it. Now I want to perform an <code>all_gather</code>. so that I create a list <code>[tensor_1, tensor_2...tensor_n]</code>. Then I can concatenate all those tensors using <code>torch.cat</code>. All the tensors in the list will lose the <code>grad_fn</code> property. My expectation is that process i will maintain the <code>grad_fn</code> for <code>tensor_i</code> in the list. It’s ok if all the others are lost. I want to be able to backward() after <code>torch.cat</code> in each process i through <code>tensor_i</code>. How can I achieve that? Any help is appreciated!</p><NewLine><p>EDIT: I think I can just do <code>tensor_list[dist.get_rank()] = tensor</code> after the all_gather operation but I am not sure if there is a better way. Help?</p><NewLine></div>",https://discuss.pytorch.org/u/Andreas_Georgiou,(Andreas Georgiou),Andreas_Georgiou,"June 30, 2019,  4:34pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Would it be possible to manually assign the <code>grad_fn</code> back to <code>tensor_i</code>?</p><NewLine><p>I don’t think it’s a good idea to retain gradient functions on output tensors of collective functions. If anything, it would give an expectation of this working well out of the box, which is not the case. I think a better solution would be to stitch things together with <code>torch.autograd.grad</code> yourself, before and after the collectives.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you have any idea? Do you want to calculate tensor_i in different process but accumulate between the processes so the loss will be attained by all the tensor_i?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve built this package that does this automatically now: <a href=""https://github.com/ag14774/diffdist"" rel=""nofollow noopener"">https://github.com/ag14774/diffdist</a>. So this question can be marked as solved</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s really cool! Thanks for your sharing!<br/><NewLine>While I am not sure how the package works and whether it can be applied to such problem:</p><NewLine><pre><code class=""lang-auto"">for iteration, data0, data1 in enumerate(data_loader, start_iter):<NewLine>    tensor = model(data0)<NewLine>    synchronize()<NewLine>    tensors = dist.all_gather(tensor)<NewLine>    loss = model(data1, tensors)<NewLine></code></pre><NewLine><p>So in each process different data0 will generates a tensor, and the gathered tensors will be used for further training. Since ‘all_gather’ cannot preserve the ‘grad_fn’, can you give me some advice to solve it?<br/><NewLine>Thanks a lot.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes the package can do that. However, <code>tensor</code> needs to be of same shape and size in all processes. Then you can do something like:</p><NewLine><pre><code class=""lang-auto"">for iteration, data0, data1 in enumerate(data_loader, start_iter):<NewLine>    tensor = model(data0)<NewLine>    synchronize()  # You probably do not need this since all_gather will force a sync<NewLine>    gather_list = [torch.empty_like(tensor) for i in range(dist.get_world_size())]<NewLine>    gather_list = diffdist.functional.all_gather(gather_list, tensor)<NewLine>    loss = model(data1, gather_list)<NewLine></code></pre><NewLine><p>Keep in mind though that <code>all_gather</code> is not very fast because its backprop involves running <code>dist.reduce</code> multiple times. When pytorch adds support for <code>reduce_scatter</code>, I will update the package to speed up the backprop.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you so much for your help. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/><br/><NewLine>I tried the code, but the gather_list after <code>diffdist.functional.all_gather(gather_list, tensor)</code> also doesn’t contain each tensor’s grad_fn.</p><NewLine><p>I found there is a parameter <code>self.next_backprop</code> in your code, do I need to set it? Sorry to bother you again.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""49343""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/11123/40/15404_2.png"" width=""20""/> 11123:</div><NewLine><blockquote><NewLine><p>I found there is a parameter <code>self.next_backprop</code> in your code, do I need to set it? Sorry to bother you again.</p><NewLine></blockquote><NewLine></aside><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""49343""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/andreas_georgiou/40/11871_2.png"" width=""20""/> Andreas_Georgiou:</div><NewLine><blockquote><NewLine><p>diffdist.functional.all_gather(gather_list, tensor)</p><NewLine></blockquote><NewLine></aside><NewLine><p>Apologies, the line should be</p><NewLine><pre><code class=""lang-auto"">gather_list = diffdist.functional.all_gather(gather_list, tensor)`<NewLine></code></pre><NewLine><p>If you get any errors try setting <code>inplace=False</code>. No need to use <code>next_backprop</code></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you again. one final question, I write a simple example  to see how the <code>grad_fn</code> works:</p><NewLine><pre><code class=""lang-auto"">    # in each process:<NewLine>    a = torch.tensor([1.0, 3.0], requires_grad=True).cuda()<NewLine>    b = a + 2 * dist.get_rank()<NewLine>    # gather<NewLine>    bs = [torch.empty_like(b) for i in range(dist.get_world_size())]<NewLine>    bs = diffdist.functional.all_gather(bs, b)<NewLine>    # loss backward<NewLine>    loss = (torch.cat(bs) * torch.cat(bs)).mean()<NewLine>    loss.backward()<NewLine>    print(a.grad)<NewLine></code></pre><NewLine><p>I think <code>a</code> should has its gradient?  But currently it is <code>None</code>. I am a little bit lost.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>You are right it seems to be working for CPU but not for CUDA for some reason. I will investigate a bit more. Feel free to open a pull request if you find the problem</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found the problem. The package is working fine. The problem is that when you set <code>requires_grad=True</code> you set it on the CPU version of <code>a</code>. Then you called <code>cuda()</code> which created another node in the graph. Gradient will pass through the GPU tensor <code>a</code> and then be accumulated to the CPU version of the tensor since that is the one that has <code>requires_grad</code> set to true. What you should do is <code>torch.tensor([1.0, 3.0], requires_grad=True, device='cuda')</code>. In a realistic scenario with normal training this won’t be a problem.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry for my late reply.</p><NewLine><p>I tried your advice and then applied to my own model, it works! Thank you for your help. Actually I don’t know how do you implement your model parallelism, here I use <code>distributeddataparallel</code> in pytorch to distribute the model to different gpus of one node. So based on my experiment, I think maybe your work can also solve the distributed gpu grad_fn gathering problem? like in <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/will-dist-all-gather-break-the-auto-gradient-graph/47350"">Will ""dist.all_gather"" break the auto gradient graph?</a>. Thank you again.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Glad it works!</p><NewLine><aside class=""quote no-group quote-modified"" data-post=""12"" data-topic=""49343""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/11123/40/15404_2.png"" width=""20""/> 11123:</div><NewLine><blockquote><NewLine><p>So based on my experiment, I think maybe your work can also solve the distributed gpu grad_fn gathering problem? like in <a href=""https://discuss.pytorch.org/t/will-dist-all-gather-break-the-auto-gradient-graph/47350"">Will “dist.all_gather” break the auto gradient graph?</a>.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes it seems that <code>diffdist</code> can handle that case. Of course different processes will have different computational graphs but with <code>diffdist</code> some nodes are inserted in the graph that will cause them to sync and communicate with each other. For example, doing a <code>Send</code> operation will cause a <code>Recv</code> to be called during backward in order to receive the gradient.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/11123; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Andreas_Georgiou; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/11123; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Andreas_Georgiou; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/11123; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Andreas_Georgiou; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/11123; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Andreas_Georgiou; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Andreas_Georgiou; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/11123; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/Andreas_Georgiou; <NewLine> ,"REPLY_DATE 1: July 1, 2019,  8:40am; <NewLine> REPLY_DATE 2: August 27, 2019,  3:04am; <NewLine> REPLY_DATE 3: August 28, 2019, 11:43am; <NewLine> REPLY_DATE 4: August 29, 2019,  8:50am; <NewLine> REPLY_DATE 5: August 30, 2019,  7:59am; <NewLine> REPLY_DATE 6: August 30, 2019,  7:44am; <NewLine> REPLY_DATE 7: August 30, 2019,  8:12am; <NewLine> REPLY_DATE 8: August 30, 2019,  8:42am; <NewLine> REPLY_DATE 9: August 30, 2019,  9:07am; <NewLine> REPLY_DATE 10: August 30, 2019,  9:35am; <NewLine> REPLY_DATE 11: September 2, 2019,  2:34am; <NewLine> REPLY_DATE 12: September 2, 2019,  7:38am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> 
54862,My GPU is dead while using Nvidia Apex,2019-09-01T07:11:57.455Z,3,693,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was training my model with 3 Nvidia 2080 Ti on Ubuntu 16.04.<br/><NewLine>I used Nvidia Apex to use the full capacity of the gpus.<br/><NewLine>However, my pytorch training code hung up after a few epochs.<br/><NewLine>It worked well for one or two trainings.<br/><NewLine>I terminated the program and check the gpus with ‘nvidia-smi’.<br/><NewLine>It showed only two gpus (and it was really slow).</p><NewLine><p>I found out that one of my gpus were dead.<br/><NewLine>My computer did not properly boot with that dead gpu (GUI didn’t show up).<br/><NewLine>I reinstalled OS of my computer to Ubuntu 18.04, reinstall drivers, but the problem still existed.<br/><NewLine>When I plugged that GPU on a Windows machine, it showed a 43 error code.</p><NewLine><p>I was wondering if this problem is caused by Apex or did my graphics card had a problem.<br/><NewLine>Is there anyone who had a similar issue with Apex?</p><NewLine></div>",https://discuss.pytorch.org/u/keunwoo,(Keunwoo Park),keunwoo,"September 1, 2019, 10:30am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve never seen this issue raised by using apex and suspect the GPU might have some hardware issues.<br/><NewLine>Depending on the <code>opt_level</code> you are using in <code>apex.amp</code>, we are e.g. patching some PyTorch methods to use FP16 instead of FP32 (whitelist/blacklist style) or transform the model’s parameters to FP16 and use master parameters (master gradients) etc.<br/><NewLine><code>apex</code> does not manipulate the hardware in any way and just uses CUDA and PyTorch for e.g. mixed precision training.</p><NewLine><p>How old is the GPU and how long was it working fine?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just bought the gpus.<br/><NewLine>I’ve been using the gpu about two weeks and it ran fine for a few train sessions.</p><NewLine><p>I’m not sure it is relevant, but there were some weird incidents while using Apex.<br/><NewLine>When I terminated a python script with ctrl-c, sometimes Apex did not fully terminate and some sub-processes existed. Those sub-processes kept held GPU memories, so I had to kill them manually. These incidents made me suspect Apex.</p><NewLine><p>I agree that it might be a hardware problem, but I wonder if a high GPU utilization might harm a gpu.<br/><NewLine>Is it possible that utilizing GPU 99% for a long time can affect the hardware (e.g. overheat)?</p><NewLine><p>Moreover, is there any other way to utilize gpu up to 90% instead of using Apex?<br/><NewLine>When I tested some codes, gpu utilization was around 50~70% and I’m not sure whether it is normal.<br/><NewLine>I wanted to increase it, so I ended up with Apex.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""54862""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/keunwoo/40/13192_2.png"" width=""20""/> keunwoo:</div><NewLine><blockquote><NewLine><p>I’m not sure it is relevant, but there were some weird incidents while using Apex.<br/><NewLine>When I terminated a python script with ctrl-c, sometimes Apex did not fully terminate and some sub-processes existed. Those sub-processes kept held GPU memories, so I had to kill them manually. These incidents made me suspect Apex.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I’m not sure, if this is caused by <code>apex</code> or PyTorch, as I’ve seen this behavior using plain PyTorch. If I’m not mistaken, this should be fixed in the latest stable release.</p><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""54862""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/keunwoo/40/13192_2.png"" width=""20""/> keunwoo:</div><NewLine><blockquote><NewLine><p>I agree that it might be a hardware problem, but I wonder if a high GPU utilization might harm a gpu.<br/><NewLine>Is it possible that utilizing GPU 99% for a long time can affect the hardware (e.g. overheat)?</p><NewLine></blockquote><NewLine></aside><NewLine><p>If you didn’t overclocked the GPU, it should be fine. In case your device overheats, e.g. if your GPUs are packed tightly into the case, it should reduce its clock and shutdown as the last step.</p><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""54862""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/keunwoo/40/13192_2.png"" width=""20""/> keunwoo:</div><NewLine><blockquote><NewLine><p>Moreover, is there any other way to utilize gpu up to 90% instead of using Apex?<br/><NewLine>When I tested some codes, gpu utilization was around 50~70% and I’m not sure whether it is normal.<br/><NewLine>I wanted to increase it, so I ended up with Apex.</p><NewLine></blockquote><NewLine></aside><NewLine><p>It depends on your code and e.g. you might have a data loading bottleneck.<br/><NewLine><a href=""https://discuss.pytorch.org/t/how-to-prefetch-data-when-processing-with-gpu/548/19"">This post</a> explains some workarounds.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much! Your answers really helped me.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/keunwoo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/keunwoo; <NewLine> ,"REPLY_DATE 1: September 1, 2019, 10:51am; <NewLine> REPLY_DATE 2: September 1, 2019, 11:51am; <NewLine> REPLY_DATE 3: September 1, 2019, 12:01pm; <NewLine> REPLY_DATE 4: September 1, 2019, 12:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
54733,Connection reset by peer from torch.distributed.recv,2019-08-30T00:57:55.827Z,0,389,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am using torch distributed with gloo backend (because I need peer to peer communication). While running my test script, I got a ‘Connection reset by peer’ error while dist.recv is called. Any clue on what causes this?</p><NewLine><p>I am using mpi to launch 2 processes and my script is pasted as below:</p><NewLine><blockquote><NewLine><p>def run(rank, size):<br/><NewLine>tensor = torch.zeros(1).cuda()<br/><NewLine>if rank == 0:<br/><NewLine>tensor += 1<br/><NewLine># Send the tensor to process 1<br/><NewLine>dist.send(tensor=tensor, dst=1)<br/><NewLine>else:<br/><NewLine># Receive tensor from process 0<br/><NewLine>dist.recv(tensor=tensor, src=0)<br/><NewLine>print('Rank ', rank, ’ has data ', tensor[0])<br/><NewLine>def init_processes(rank, size, addr, fn, backend):<br/><NewLine>os.environ[‘MASTER_ADDR’] = addr<br/><NewLine>os.environ[‘MASTER_PORT’] = ‘12345’<br/><NewLine>my_rank = os.environ[‘OMPI_COMM_WORLD_RANK’]<br/><NewLine>dist.init_process_group(backend, rank=my_rank, world_size=size)<br/><NewLine>fn(rank, size)<br/><NewLine>if <strong>name</strong> == “<strong>main</strong>”:<br/><NewLine>hostname = socket.gethostname()<br/><NewLine>addr = socket.gethostbyname(hostname)<br/><NewLine>size = 2<br/><NewLine>my_rank = os.environ[‘OMPI_COMM_WORLD_RANK’]<br/><NewLine>init_processes(my_rank, size, addr, run, ‘gloo’)</p><NewLine></blockquote><NewLine><p>The error I got is as below:</p><NewLine><blockquote><NewLine><p>File “/home/xzhu1900/anaconda3/envs/test_py37/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py”, line 712, in recv<br/><NewLine>pg.recv([tensor], src, tag).wait()<br/><NewLine>RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:563] Read error [127.0.1.1]:48028: Connection reset by peer</p><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/xzhu1900,,xzhu1900,"August 30, 2019, 12:57am",,,,,
54492,Machine reboot when running model in torch.nn.DataParallel,2019-08-27T20:07:38.736Z,6,357,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi folks, I’ve noticed that whenever I run multi-gpu training on my machine it suddenly reboots. There are 2 gpus on the machine a 1080Ti and a Titan X. 1080Ti is gpu:0  and is also used to power 2 monitors for video display. Usually never had issues before with multi-gpu training using tensorflow. Any ideas what might be causing this?</p><NewLine></div>",https://discuss.pytorch.org/u/kirk86,(Kirk86),kirk86,"August 27, 2019,  8:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This sounds like your PSU might be too weak. What kind of PSU are you using at the moment?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think that’s the case I have an 850W <a href=""https://www.lc-power.com/en/product/pc-power-supply-units/metatron-gaming-series/lc8850iii-v23-arkangel-3/"" rel=""nofollow noopener"">PSU</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think this might be indeed the problem, as the recommended system power for the <a href=""https://www.geforce.com/hardware/desktop-gpus/geforce-gtx-titan-x/specifications"" rel=""nofollow noopener"">Titan X</a> are 600W and the <a href=""https://www.geforce.com/hardware/desktop-gpus/geforce-gtx-1080-ti/specifications"" rel=""nofollow noopener"">1080 Ti</a> might need another 250W at max. performance. Your PSU might be maxed out with these two GPUs (of course it also depends on other hardware and its power consumption).</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>My understanding of reading the linked paged to Titan X is that the actual card requires 250W and it’s recommended to have an overall system with 600W in which you’re gonna use the card? 600W for Titan X seems ridiculously high to me? If I max out the TX then it barely goes above 260W. In the worst case scenario both of them maxed out would have gotten 600W still not explains the reboots, also I’ve trained multi-gpu using tensorflow and maxing out both cards didn’t notice any of these issues?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>The info for the Titan X states 600W are recommended for this single GPU and the whole system.<br/><NewLine>So if that’s the max. power consumption of a single Titan X + rest of the system, the additional 1080Ti might need 250W extra as stated on the other page.</p><NewLine><p>Maybe you were lucky in TF as the GPUs might not have been at their peak power consumption.<br/><NewLine>You could try to set the power limits lower using <code>nvidia-smi</code>, maybe even create artificial bottlenecks so that your GPUs will only have a short burst of power consumption or test another PSU if available.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""54492""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ptrblck/40/1823_2.png"" width=""20""/> ptrblck:</div><NewLine><blockquote><NewLine><p>set the power limits lower using <code>nvidia-smi</code></p><NewLine></blockquote><NewLine></aside><NewLine><p>Oh nice, I didn’t know you could do that! Let me search around and see how to test those things. Thanks for the tip!</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>,<br/><NewLine>so I did a breakdown of the components and Watts consumption required on my setup.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/73bd3581bf17a89d6348dc877bc3686de94d7ad8"" href=""https://discuss.pytorch.org/uploads/default/original/2X/7/73bd3581bf17a89d6348dc877bc3686de94d7ad8.png"" title=""image.png""><img alt=""image"" data-base62-sha1=""gvShPioFSV6dAXufsZPzZ7kHnrG"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/7/73bd3581bf17a89d6348dc877bc3686de94d7ad8_2_10x10.png"" height=""429"" src=""https://discuss.pytorch.org/uploads/default/original/2X/7/73bd3581bf17a89d6348dc877bc3686de94d7ad8.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">702×437 47.9 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>And it seems that an 850W PSU should be able to handle that no?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kirk86; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/kirk86; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/kirk86; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/kirk86; <NewLine> ,"REPLY_DATE 1: August 27, 2019,  9:20pm; <NewLine> REPLY_DATE 2: August 27, 2019,  9:26pm; <NewLine> REPLY_DATE 3: August 27, 2019,  9:42pm; <NewLine> REPLY_DATE 4: August 27, 2019, 10:16pm; <NewLine> REPLY_DATE 5: August 27, 2019, 10:22pm; <NewLine> REPLY_DATE 6: August 27, 2019, 10:25pm; <NewLine> REPLY_DATE 7: August 28, 2019,  2:03pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
36643,Issue with dataloader using pin_memory = True,2019-02-07T22:27:04.815Z,2,1206,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I’m seeing an odd issue with using the pin_memory = true flag with the dataloader. I’m measuring the time taken to transfer data from the host RAM to GPU memory as follows:</p><NewLine><pre><code>    transfer_time_start = time.time()<NewLine>    input = input.cuda(args.gpu, non_blocking=False)<NewLine>    target = target.cuda(args.gpu, non_blocking=False)<NewLine>    torch.cuda.synchronize()<NewLine>    transfer_time.update(time.time()-transfer_time_start)<NewLine></code></pre><NewLine><p>with pin_memory = True in the dataloader, this gives me a transfer time of 0.03 sec, which for a batch size of 256, translates into 256<em>224</em>224<em>3</em>4/0.03 = 5.1GB, which is a bit low for my CPU-GPU interconnect (x16, PCIe3) which should deliver ~12GB.</p><NewLine><p>I then tried calling pin_memory() manually on the tensor returned by the enumerate call, as shown below:</p><NewLine><p>for i, (input, target) in enumerate(train_loader):<br/><NewLine>input = input.pin_memory()<br/><NewLine># measure data loading time<br/><NewLine>data_time.update(time.time() - end)<br/><NewLine>transfer_time_start = time.time()</p><NewLine><pre><code>    input = input.cuda(args.gpu, non_blocking=False)<NewLine>    target = target.cuda(args.gpu, non_blocking=False)<NewLine>    torch.cuda.synchronize()<NewLine>    transfer_time.update(time.time()-transfer_time_start)<NewLine></code></pre><NewLine><p>Now the transfer time dropped to 0.014, which translates to ~11GB, which is as expected. Anyone has any ideas why setting pin_memory = True in the data loader may not return a tensor already in pinned memory?</p><NewLine><p>Also attached below are two plots showing the transfer time (green plot) from host memory to the GPU.<br/><NewLine>This plot shows the transfer time when I call pin_memory manually<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/1030a1bf0654ce5ea04d5ffb376e2196181a852c"" href=""https://discuss.pytorch.org/uploads/default/original/2X/1/1030a1bf0654ce5ea04d5ffb376e2196181a852c.png"" title=""image.png""><img alt=""image"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/1/1030a1bf0654ce5ea04d5ffb376e2196181a852c_2_10x10.png"" height=""208"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/1/1030a1bf0654ce5ea04d5ffb376e2196181a852c_2_690x208.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/1/1030a1bf0654ce5ea04d5ffb376e2196181a852c_2_690x208.png, https://discuss.pytorch.org/uploads/default/optimized/2X/1/1030a1bf0654ce5ea04d5ffb376e2196181a852c_2_1035x312.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/1/1030a1bf0654ce5ea04d5ffb376e2196181a852c_2_1380x416.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">1585×478 111 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine>You can see that the transfer time stays consistently low.</p><NewLine><p>Whereas this one shows the transfer time without calling pin_memory manually. Now the transfer time is highly variable and averages to around 0.03 sec<br/><NewLine><img alt=""image"" height=""247"" src=""https://discuss.pytorch.org/uploads/default/original/2X/0/0637d60222fd949df0229830bbdf1c86eaa467f3.png"" width=""624""/></p><NewLine></div>",https://discuss.pytorch.org/u/ankur6ue,(Ankur Mohan),ankur6ue,"February 8, 2019,  6:16pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I can not speak much about the manual approach as I haven’t tried it, but regarding <code>pin_memory=True</code> I observe in practice that it slows done the training at about 2x (compared to <code>False</code>) – tested it in PyTorch 0.4.1 and 1.0 and on two independent machines (one with 1080Ti’s and one with Titan V’s). So, in practice, I abandoned using that. I remember there was a thread where someone mentioned similar observations.</p><NewLine><p>So, it may well be that there’s a bug with <code>pin_memory = True</code>, esp. since you observe that the manual approach results in the expected speed up.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply. As my experiments confirm, transfer to GPU is significantly faster for data in pinned memory, so it is worth doing it. Issue is that transfer to pinned memory itself costs time, and only saves time overall if it can be parallelized. The data loader seems to be doing this - it spins up a separate thread for transfer to pinned memory when pin_memory flag is set to True. When you call enumerate or next(iter), dataloader waits until a batch is available in pinned_memory so if the processing time on GPU is sufficiently long, then the latency of pinned memory transfer should be hidden, at least partly.</p><NewLine><p>The question is why is transfer to the GPU still slow even though the batch is in pinned memory? One difference between the manual approach and the regular approach (not calling input.pin_memory() manually) is that in the manual approach, the transfer is done over the main thread, while in the regular approach, it is being done on another thread. Does this make a difference?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I can also verify this since I have the same observations were using <code>pin_memory=True</code> and <code>num_workers=1</code> I see the gpu utilization at ~40% through all the training period but with <code>pin_memory=False</code> and <code>num_workers=4</code> the gpu utilization is at ~90%. Plus I see my cpu at full utilization since the fans kick in but without pin_memory everything seems fine.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/rasbt; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ankur6ue; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/kirk86; <NewLine> ,"REPLY_DATE 1: February 9, 2019,  4:35am; <NewLine> REPLY_DATE 2: February 9, 2019,  3:43pm; <NewLine> REPLY_DATE 3: August 27, 2019, 10:55am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
54047,Assigning every instance of siamese network to separate GPU,2019-08-22T12:10:38.727Z,2,202,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a Siamese Network with a triplet loss function at the end. E.g.:</p><NewLine><pre><code class=""lang-auto"">&gt; class Siamese(nn.Module):<NewLine>&gt;     def __init__(self, ae_net):<NewLine>&gt;         super(Siamese, self).__init__()<NewLine>&gt;         self.ae_net = ae_net<NewLine>&gt; <NewLine>&gt;     def forward(self, x1, x2, x3, hidden):<NewLine>&gt;         <NewLine>&gt;         a = self.ae_net(x1, hidden)<NewLine>&gt;         b = self.ae_net(x2, hidden)<NewLine>&gt;         c = self.ae_net(x3, hidden)<NewLine>&gt;         return a, b, c<NewLine></code></pre><NewLine><p>The network that repeats itself (i.e., <code>self.ae_net</code>) is an LSTM with inputs of varying lengths, so I’m not sure I can use <code>nn.DataParallel</code>.  I was wondering if there was a way to assign the implementation of every instance of <code>self.ae_net</code> to a different GPU, so that they would be calculated in parallel.</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/Tzeviya,,Tzeviya,"August 22, 2019, 12:12pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>In your example, you use the hidden state sequentially for each network one after the other. Is that what you want? Because if you want that, then you cannot really run them in parallel on different GPUs as they will need to wait for the previous one to finish.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, thanks for your answer! The hidden state that is used for <code>a</code>, <code>b</code> and <code>c</code> is the same one, i.e., it’s just an initialized tensor used three times separately (isn’t it?), so I don’t think it’s a problem to use them in parallel.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ho right it is, my bad.</p><NewLine><p>It is a bit tricky for Siamese network as you need to accumulate the gradients for all tree runs.<br/><NewLine>One simple way to do this is to make three copies of your network, one on each device, then send each copy to it’s respective device.<br/><NewLine>After each backward, you will need to accumulate the gradients by hand then share the new values by hand.<br/><NewLine>This is going to be tricky to do very efficiently, and you might not get a large improvement for using multiple gpus because of the synchronisation needed between devices.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Tzeviya; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: August 22, 2019,  6:24pm; <NewLine> REPLY_DATE 2: August 23, 2019,  1:06pm; <NewLine> REPLY_DATE 3: August 26, 2019,  3:22pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
54071,Running nn.DataParallel model with occasionally missing losses,2019-08-22T18:02:37.577Z,0,93,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I am trying to implement a model that runs with multigpu training and DataParallel. The problem is that due to the nature of my model, occasionally there will be forward paths where losses are not produced.</p><NewLine><p>Problem:<br/><NewLine>Without data parallel, I simply set these missing losses to None or 0, and only add losses that are not None or 0 to my total loss. However, moving on to multigpu, there are instances where 1 process on GPU 0 will produce an actual loss, while the process on GPU1 will <em>not</em> produce that particular loss.</p><NewLine><p>Hence, when DataParallel module gathers these losses, bad things happen – it cannot combine the losses from two GPU together.</p><NewLine><p>Things I tried:<br/><NewLine>I tried making the missing loss 0, a float tensor of 0, or setting it to None. None of these methods work – they each produce an error in torch/nn/parallel/scatter_gather.py. For instance, setting the missing loss to 0, which is not iterable, will give “TypeError: zip argument <span class=""hashtag"">#1</span> must support iteration” error.</p><NewLine><p>Question:<br/><NewLine>I am wondering if there is a particular thing in DataParallel that ignores missing entries when combining losses from multiple GPUs? Or is there a possible solution to this problem without altering pytorch code?</p><NewLine><p>Any help is greatly appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/broccolizzy,,broccolizzy,"August 22, 2019,  6:08pm",,,,,
53897,"Multi workers specified by num_workers load samples to form a batch, or each worker load a batch respectively in DataLoader?",2019-08-21T06:40:46.986Z,3,399,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I mean, supposed batch_size=4 and num_workers=2, which of the following may match the runtime case?</p><NewLine><p><span class=""hashtag"">#1</span><br/><NewLine>worker1: load sample1, sample2<br/><NewLine>worker2: load sample3, sample4<br/><NewLine>batch1 contains sample1, sample2, sample3, sample4</p><NewLine><p><span class=""hashtag"">#2</span><br/><NewLine>worker1: load batch1<br/><NewLine>worker2: load batch2</p><NewLine></div>",https://discuss.pytorch.org/u/yangwenhuan,(Yangwenhuan),yangwenhuan,"August 21, 2019,  6:40am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Apparently, it would be case <span class=""hashtag"">#2</span>. See <a href=""https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/2"">this answer</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have read the mentioned post, but how can I proof it? or any PyTorch source code show the fact?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The source code for the DataLoader logic is too complex for me, but I see two reasons why <span class=""hashtag"">#2</span> would be the approach used in practice:</p><NewLine><ol><NewLine><li><NewLine><a class=""mention"" href=""/u/apaszke"">@apaszke</a> said it,</li><NewLine><li>creating a batch from the two workers would require waiting for them to finish their task (getting data) before returning the batch, which is inefficient and (I guess depending on the architecture, data structure, storage etc…) useless compared to a single process loading sample1, sample2, sample3 and sample4 and then returning the batch.</li><NewLine></ol><NewLine><p>What is your use case?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>for your first point, <a class=""mention"" href=""/u/apaszke"">@apaszke</a> only showed the result, but I want to know the reason.<br/><NewLine>for the second, multi workers handle a batch may not be slower than single worker.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alex.veuthey; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/yangwenhuan; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/alex.veuthey; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/yangwenhuan; <NewLine> ,"REPLY_DATE 1: August 21, 2019,  7:10am; <NewLine> REPLY_DATE 2: August 21, 2019,  8:09am; <NewLine> REPLY_DATE 3: August 21, 2019,  8:45am; <NewLine> REPLY_DATE 4: August 21, 2019,  8:56am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
53690,Bug in Data Parallel?,2019-08-19T02:38:33.426Z,5,1593,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I have a network  having two components (X, Y) which takes two inputs: A and B.</p><NewLine><p>I wrap the network in data parallel. X returns full batch size output (how?) while Y returns 1/4th batch size output (this is expected).</p><NewLine><p>Should I not nest subclasses of nn.Module while using Data parallel wrapper? Since, data parallel does support multiple inputs so this is unexpected (<a href=""https://github.com/pytorch/pytorch/pull/794"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/794</a>).</p><NewLine><p>I even tried separating the X, Y modules and wrapping them in data parallel separately, I still get the same error.</p><NewLine><p>Any hints on what might be wrong?</p><NewLine></div>",https://discuss.pytorch.org/u/Rafael_R,(jean),Rafael_R,"August 19, 2019,  3:46am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you post a minimal code snippet to see, how you are using submodules inside your model and how you are applying <code>nn.DataParallel</code> to it?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am getting the same error in two ways:</p><NewLine><pre><code class=""lang-auto"">&gt; class EncoderCNN(nn.Module):<NewLine>&gt;     def __init__(self, embed_size, pre_trained_emb_size=2048):<NewLine>&gt;         """"""Load the pretrained ResNet Features""""""<NewLine>&gt;         super(EncoderCNN, self).__init__()<NewLine>&gt;         self.linear = nn.Linear(pre_trained_emb_size, embed_size)<NewLine>&gt;         self.relu = nn.ReLU(embed_size)<NewLine>&gt;         self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)<NewLine>&gt; <NewLine>&gt;     def forward(self, images):<NewLine>&gt;         """"""Extracted feature vectors form input""""""<NewLine>&gt;     <NewLine>&gt;         features = self.bn(self.relu(self.linear(images)))<NewLine>&gt;         return features<NewLine>&gt; <NewLine>&gt; class TransformerEncoder(nn.Module):<NewLine>&gt;     def __init__(self, vocab_size, embed_size, d_model=300, nhead=6, dim_feedforward=512, dropout=0.1):<NewLine>&gt;         super(TransformerEncoder, self).__init__()<NewLine>&gt;         self.input_size = vocab_size<NewLine>&gt;         self.embed_size = embed_size<NewLine>&gt;         self.embedding = nn.Embedding(self.input_size, self.embed_size)<NewLine>&gt;         # Encoder CNN<NewLine>&gt;         self.encoder_cnn = EncoderCNN(embed_size=self.embed_size)<NewLine>&gt;         self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)<NewLine>&gt; <NewLine>&gt;     def forward(self, input, input_lengths, images):<NewLine>&gt;         input = input.reshape((input.shape[1], input.shape[0]))<NewLine>&gt;         embedded = self.embedding(input)<NewLine>&gt;         output = self.transformer_encoder_layer(embedded)<NewLine>&gt;         image_encodings = self.encoder_cnn(images)<NewLine>&gt;         return output, image_encodings<NewLine></code></pre><NewLine><p>Even if I take the encoder cnn module out and wrap it in data parallel separately, I get the same error. the final returned tensors output, image_encodings have different batch sizes <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not sure, why you are reshaping the input in <code>TransformerEncoder</code>'s <code>forward</code> method.<br/><NewLine>Could you explain this work flow as I think it might be related to this issue?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><ol><NewLine><li><NewLine><p>I am sending max_len x batch size x embedding_size input to transformer layer. This is the same api as in LSTM (without batch_first=True). Since, data parallel module expects the module’s input to have batch in first dimension, I am passing batch as first dimension and then reshaping it before passing it to transformer.</p><NewLine></li><NewLine><li><NewLine><p>Same for images, batch_size x embedding_size input is passed to the module.</p><NewLine></li><NewLine></ol><NewLine><p>Do you see anything wrong here?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the information.<br/><NewLine>In that case you might want to use <code>.permute</code>, as <code>.reshape</code> might interleave the data:</p><NewLine><pre><code class=""lang-python"">x = torch.tensor([[0., 0.],<NewLine>                  [1., 1.],<NewLine>                  [2., 2.],<NewLine>                  [3., 3.]])<NewLine><NewLine>print(x.reshape(x.size(1), x.size(0)))<NewLine>&gt; tensor([[0., 0., 1., 1.],<NewLine>          [2., 2., 3., 3.]])<NewLine>print(x.permute(1, 0))<NewLine>&gt; tensor([[0., 1., 2., 3.],<NewLine>          [0., 1., 2., 3.]])<NewLine></code></pre><NewLine><p>I don’t see any obvious errors. Could you add print statements inside the <code>forward</code> method, which will print the shape as well as the device of the input, output and each intermediate tensor?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks,</p><NewLine><p>The print statements and the output on the console is:</p><NewLine><pre><code class=""lang-auto"">    def forward(self, input, input_lengths, images):<NewLine>        ptvsd.break_into_debugger()<NewLine>        print(""Input"", input.shape, input.get_device())<NewLine>        input = input.permute((1, 0))  #input.reshape((input.shape[1], input.shape[0]))<NewLine>        print(""Permuted Input"", input.shape, input.get_device())<NewLine>        embedded = self.embedding(input)<NewLine>        print(""Embedded"", embedded.shape, embedded.get_device())<NewLine>        #packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)<NewLine>        output = self.transformer_encoder_layer(embedded)<NewLine>        print(""Output"", output.shape, output.get_device())<NewLine>        #output, _ = torch.nn.utils.rnn.pad_packed_sequence(output)<NewLine>        image_encodings = self.encoder_cnn(images)<NewLine>        print(""Image Encodings"", image_encodings.shape, image_encodings.get_device())<NewLine>        return output, image_encodings<NewLine></code></pre><NewLine><p>And the output is:</p><NewLine><pre><code class=""lang-auto"">Input torch.Size([64, 30]) 0<NewLine>Input torch.Size([64, 30]) 1<NewLine>Permuted Input torch.Size([30, 64]) 0<NewLine>Input torch.Size([64, 30]) 2<NewLine>Permuted Input torch.Size([30, 64]) 1<NewLine>Permuted Input torch.Size([30, 64]) 2<NewLine>Input torch.Size([64, 30]) 3<NewLine>Embedded torch.Size([30, 64, 300]) 0<NewLine>Embedded torch.Size([30, 64, 300]) 2<NewLine>Embedded torch.Size([30, 64, 300]) 1<NewLine>Output torch.Size([30, 64, 300]) 1<NewLine>Output torch.Size([30, 64, 300]) 0<NewLine>Output torch.Size([30, 64, 300]) 2<NewLine>Permuted Input torch.Size([30, 64]) 3<NewLine>Image Encodings torch.Size([64, 300]) 1<NewLine>Image Encodings Image Encodingstorch.Size([64, 300]) 2<NewLine> torch.Size([64, 300]) 0<NewLine>Embedded torch.Size([30, 64, 300]) 3<NewLine>Output torch.Size([30, 64, 300]) 3<NewLine>Image Encodings torch.Size([64, 300]) 3<NewLine></code></pre><NewLine><p>And the returned tensors have the shape:</p><NewLine><pre><code class=""lang-auto"">shape:torch.Size([120, 64, 300])<NewLine>device:device(type='cuda', index=0)<NewLine></code></pre><NewLine><p>and</p><NewLine><pre><code class=""lang-auto"">shape:torch.Size([256, 300])<NewLine>device:device(type='cuda', index=0)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>It looks like you would need to permute the output of the <code>TransformerEncoderLayer</code> again, as its output has the shape <code>[T, N, E]</code>  (batch dimension in dim1).</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks, the data parallel issue is solved it seems and the code is working. however, it is very slow (slower than a single gpu ) and i am getting the error here: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/how-to-flatten-parameters/53799"">How to flatten parameters?</a></p><NewLine><p>any hints on how to fix the issue, i assume flatten lstm must be called after each call but then even if i do it, i still get the linked warning and the code is very slow</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Rafael_R; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Rafael_R; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Rafael_R; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Rafael_R; <NewLine> ,"REPLY_DATE 1: August 19, 2019, 11:09am; <NewLine> REPLY_DATE 2: August 19, 2019,  7:42pm; <NewLine> REPLY_DATE 3: August 19, 2019, 10:30pm; <NewLine> REPLY_DATE 4: August 19, 2019, 10:45pm; <NewLine> REPLY_DATE 5: August 19, 2019, 10:58pm; <NewLine> REPLY_DATE 6: August 19, 2019, 11:14pm; <NewLine> REPLY_DATE 7: August 19, 2019, 11:21pm; <NewLine> REPLY_DATE 8: August 20, 2019,  9:20pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> 
53558,Building a Modular Model: State dict vs Chaining Models,2019-08-16T18:44:43.385Z,0,74,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I’m working on a distributed learning system where I’m splitting a big model into smaller parts.<br/><NewLine>I call this big network a chain consisting of smaller networks called links.<br/><NewLine>The use case is that the links can be a client in a distributed learning system.</p><NewLine><p>Currently I’m initializing separate models, and forwarding the output of each link to the next link in the chain, this works well.</p><NewLine><p>When I would like to replace a link in the chain for another, I’m currently simply calling a different model. For implementation sake, it would be easier to just replace the state dict of several layers in the chain by the state dict of another link.</p><NewLine><p><strong>My question: Does the state dict of layers encompass all the layers properties/attributes/links?</strong><br/><NewLine>(I’m familiar with most, weights, biases, gradients, but I’ve switched to PyTorch quite recently, and would not know what other information might stay linked.)</p><NewLine></div>",https://discuss.pytorch.org/u/mgpoirot,(Maarten),mgpoirot,"August 16, 2019,  6:44pm",,,,,
52958,Model declaration distributed data parallel,2019-08-09T09:37:32.927Z,4,254,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, guys. I’m trying to build a simple distributed data parallel training program with 1 GPU per process. Firstly I followed <a href=""https://pytorch.org/tutorials/intermediate/dist_tuto.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/dist_tuto.html</a> and added some modification.</p><NewLine><pre><code class=""lang-auto"">def run(rank, size):<NewLine>    ...<NewLine>    model = Net()<NewLine>    ...<NewLine><NewLine>if __name__ == '__main__':<NewLine>    ...<NewLine>    for rank in range(size):<NewLine>        p = Process(target=init_process, args=(rank, size, run))<NewLine>        ...<NewLine></code></pre><NewLine><p>However, after reading the example <a href=""https://github.com/pytorch/examples/tree/master/mnist_hogwild"" rel=""nofollow noopener"">https://github.com/pytorch/examples/tree/master/mnist_hogwild</a>, I found model is one of the arguments of a process:</p><NewLine><pre><code class=""lang-auto"">if __name__ == '__main__':<NewLine>    ...<NewLine>    model = Net().to(device)<NewLine>    ...<NewLine>    for rank in range(args.num_process):<NewLine>        p = mp.Process(target=train, args=(rank, args, model, device, dataloader_kwargs))<NewLine>        ...<NewLine></code></pre><NewLine><p>So I just wonder: where should the model be declared, inside or outside these processes? When and how do the gradients get synchronized?</p><NewLine></div>",https://discuss.pytorch.org/u/haowxu,(Haowen Xu),haowxu,"August 9, 2019,  3:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you don’t care for doing hogwild, the second example you list is not applicable to you. Everything is declared in the primary process because the model weights are to be shared (physically shared through shared memory) between the worker processes.</p><NewLine><p>If you want to simply use a single process per GPU and don’t care for physical weight sharing, then you should declare everything in the subprocesses. Think of it as if you were running this on different machines instead of multiple processes on a single machine. Then you’d also have to declare everything in the processes that you launch on those machine instead of a single launcher processes.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Big thanks to your reply! So hogwild is not a must for distributed data parallel, even if the models are declared in different processes, they can synchronize their parameters and gradients in some way. Is it right?<br/><NewLine>I tried saving the models at the end of these processes using the following code, but I found the parameters of the models are different. Does this mean that I made something wrong?</p><NewLine><pre><code class=""lang-auto"">def run(rank, size):<NewLine>    ...<NewLine>    model = Net()<NewLine>    ...<NewLine>    for epoch in range(args.epochs):<NewLine>        for batch_idx, (input, target) in enumerate(train_loader):<NewLine>            output = model(input)<NewLine>            loss = criterion(output, target)<NewLine>            optimizer.zero_grad()<NewLine>            loss.backward()<NewLine>            optimizer.step()<NewLine><NewLine>    ...<NewLine>    model.save({...}, f'checkpoint-process-{rank}.pth')<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You need to wrap your model with <code>nn.DistributedDataParallel</code>, see <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html</a> for a tutorial.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Surely I did this. Sorry for omitting this in the code above. It’s like:</p><NewLine><pre><code class=""lang-auto"">...<NewLine>torch.cuda.set_device(rank)<NewLine>...<NewLine>model = Net().cuda()<NewLine>model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank], output_device=rank)<NewLine>...<NewLine></code></pre><NewLine><p>However, the parameters I got still differ.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried saving the model just after the forward, the parameters of the saved models are the same.</p><NewLine><pre><code class=""lang-auto"">def run(rank, size):<NewLine>    ...<NewLine>    for epoch in range(args.epochs):<NewLine>        output = model(input)<NewLine>        loss = criterion(output, target)<NewLine><NewLine>        torch.save({...}, f'checkpoint-process-{rank}-epoch-{epoch}.pth')<NewLine><NewLine>        ...<NewLine><NewLine>    ...<NewLine></code></pre><NewLine><p>Still checking the sync of parameters and gradients in the source code.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Update: Just refering the ImageNet example can help sync the parameters. I was using the wrong way to check the parameters of the model.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/haowxu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/haowxu; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/haowxu; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/haowxu; <NewLine> ,"REPLY_DATE 1: August 13, 2019, 10:15pm; <NewLine> REPLY_DATE 2: August 13, 2019,  6:50am; <NewLine> REPLY_DATE 3: August 13, 2019,  7:08am; <NewLine> REPLY_DATE 4: August 13, 2019,  9:35am; <NewLine> REPLY_DATE 5: August 14, 2019,  8:59am; <NewLine> REPLY_DATE 6: August 16, 2019, 10:19am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
41202,MPI Allreduce Queue,2019-03-29T11:32:10.099Z,1,246,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>What is the function/reason to <em>enqueue(std::move(entry));</em> in ProcessGroupMPI Allreduce? I don’t understand this part of the code</p><NewLine></div>",https://discuss.pytorch.org/u/hpc-unex,(SergioUnex),hpc-unex,"March 29, 2019, 11:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is where the operation is queued to be executed by a background thread. All collective calls execute on a separate thread so you can block on their completion only when you need the result. This allows for overlapping of gradient reduction with gradient computation.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>When you talk about “overlapping of gradient reduction with gradient computation” you mean the overlapping between forward and backward propagation?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, it is overlapped with gradient computation (backward propagation) only. As more and more gradients have been computed, they are ready to be reduced. There is no need to wait with reducing them until you have computed all of them.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pietern"">@pietern</a> Hi,could you tell ne why ProcessGroupNCCL not use enqueue(xxx) function？Does NCCL implement multiple nodes gradients calculation synchronization?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hpc-unex; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/qiuyang; <NewLine> ,"REPLY_DATE 1: March 29, 2019,  4:40pm; <NewLine> REPLY_DATE 2: April 2, 2019,  9:43am; <NewLine> REPLY_DATE 3: April 2, 2019,  5:21pm; <NewLine> REPLY_DATE 4: August 16, 2019,  7:54am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> 
53405,Using grid_sample on multiple GPUs,2019-08-14T18:11:18.628Z,6,438,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I have a model with a <a href=""https://pytorch.org/docs/stable/nn.functional.html?highlight=grid_sample#torch.nn.functional.grid_sample"" rel=""nofollow noopener"">grid_sample</a> layer, I tried to train my model on multiple GPUs, but got the following error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: grid_sampler(): expected input and grid to be on same device, but input is on cuda:1 and grid is on cuda:0<NewLine></code></pre><NewLine><p>Is there anyway to use this layer on multiple GPUs? Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/sunshineatnoon,(Sunshine At Noon ),sunshineatnoon,"August 14, 2019,  6:11pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The <code>input</code> and <code>grid</code> should be on the same device.<br/><NewLine>If you are creating one of these tensors manually in the <code>forward</code> or pass it to the <code>forward</code> method, make sure to transfer it to the same device, e.g. by using:</p><NewLine><pre><code class=""lang-python"">grid = grid.to(x.device)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply. I got a <code>segmentation default</code> when moving either of the grid or the input to the same device by <code>input = input.to(grid.get_device())</code>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>My grid is actually the same for all inputs, so I stored it using <code>self.grid = grid</code> and using <code>grid_sample(input, self.grid)</code>. Do you think this causes the problem? But I think it’s inefficient to pass the grid every forward.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Try to use <code>grid.device</code> instead.</p><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""53405""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/sunshineatnoon/40/13446_2.png"" width=""20""/> sunshineatnoon:</div><NewLine><blockquote><NewLine><p>But I think it’s inefficient to pass the grid every forward.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Might be and you should stick to your work flow, as I was just using it as an example. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine><p>You could also try to register <code>self.grid</code> as a <code>buffer</code> using <code>self.register_buffer</code>, which would move the tensor automatically using <code>model.to()</code>.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>register_buffer</code> solves my problem. The <code>segmentation default</code> actually comes from other parts. It seems when training on multiple GPUs, we cannot call <code>.cuda()</code> during the forward path, so everything should be registered in the buffer.</p><NewLine><p>Thanks so much for your help!</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could call <code>.cuda()</code> or <code>to()</code>, but should specify the right device to push the tensor to.<br/><NewLine>E.g. if you would like to create some tensors inside the <code>forward</code> method, you could use the device of some buffers/parameters or the incoming tensor to create the new one.</p><NewLine><p>However, if <code>self.grid</code> is treated as an attribute of the model, registering it as a buffer is the cleaner and better approach. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Right, that makes so much sense now. Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/sunshineatnoon; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/sunshineatnoon; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/sunshineatnoon; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/sunshineatnoon; <NewLine> ,"REPLY_DATE 1: August 14, 2019,  6:44pm; <NewLine> REPLY_DATE 2: August 14, 2019,  7:06pm; <NewLine> REPLY_DATE 3: August 14, 2019,  7:07pm; <NewLine> REPLY_DATE 4: August 14, 2019,  8:39pm; <NewLine> REPLY_DATE 5: August 14, 2019,  8:38pm; <NewLine> REPLY_DATE 6: August 14, 2019, 11:26pm; <NewLine> REPLY_DATE 7: August 15, 2019, 11:07pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
53032,Get local size in distributed training,2019-08-09T21:12:57.441Z,2,146,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Anyway to get number of procedures per node in distributed training?<br/><NewLine>In horovod we could use <code>hvd.local_size()</code>, but I found no alternative in distributed module.<br/><NewLine>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/meijieru,(梅杰儒),meijieru,"August 9, 2019,  9:12pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There is no concept of local processes and remote processes; only the total number of processes through <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.get_world_size"" rel=""nofollow noopener""><code>torch.distributed.get_world_size()</code></a>.</p><NewLine><p>To understand if we need add this: what do you need this for?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>For example, I want to fully use all CPU for data loading without overhead.<br/><NewLine>Thus I want to divide the <code>number_workers</code> by local size.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pietern"">@pietern</a> Any solution for this? Thanks.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/meijieru; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/meijieru; <NewLine> ,"REPLY_DATE 1: August 12, 2019, 11:19am; <NewLine> REPLY_DATE 2: August 12, 2019,  5:26pm; <NewLine> REPLY_DATE 3: August 14, 2019, 11:12pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
43532,Pytorch fft on multiple gpus,2019-04-25T05:10:43.967Z,0,558,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using pytorch function torch.rfft() and torch.irfft() inside the forward path of a model. It runs fine on single GPU. However, when I train the model on multiple GPUs, it fails and gave the error:</p><NewLine><p>RuntimeError: cuFFT error: CUFFT_INTERNAL_ERROR</p><NewLine><p>Does anybody has the intuition why this is the case? Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/Tim_Zhang,,Tim_Zhang,"April 25, 2019,  5:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/tim_zhang"">@Tim_Zhang</a> – are you using <code>torch.nn.DataParallel</code> for training on multiple GPUs? If so, this could be some sort of initialization bug where cuFFT is initialized on the first device only and not others.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have the same problem here, when using DataParallel with torch.fft() or torch.rfft(), it generates error without error message. I.e. the visual studio pops up saying"" An unhandled win32 exception ocurred in python.exe""</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I managed to reproduce this issue and reported it at <a href=""https://github.com/pytorch/pytorch/issues/24176"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/24176</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/chenzhuo1011; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019, 11:00am; <NewLine> REPLY_DATE 2: August 8, 2019, 10:57pm; <NewLine> REPLY_DATE 3: August 12, 2019, 12:10pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
52996,(Distributed) Why all gpus are giving the same output?,2019-08-09T13:58:51.680Z,0,395,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am a new comer to PyTorch and I’m confused when I am running the official example of <code>torch.distributed</code><br/><NewLine>at <a href=""https://github.com/pytorch/examples/blob/d587b53f3604b029764f8c864b6831d0ab269008/imagenet/main.py#L304"" rel=""nofollow noopener"">PyTorch ImageNet main.py L304</a>.</p><NewLine><p>I have made some small modification on the evaluation part of the source code like below:</p><NewLine><pre><code class=""lang-auto"">model.eval()<NewLine>    with torch.no_grad():<NewLine>        end = time.time()<NewLine>        for i, (images, target, image_ids) in enumerate(val_loader):<NewLine>            if args.gpu is not None:<NewLine>                images = images.cuda(args.gpu, non_blocking=True)<NewLine><NewLine>            target = target.cuda(args.gpu, non_blocking=True)<NewLine>            image_ids = image_ids.data.cpu().numpy()<NewLine>            output = model(images)<NewLine>            loss = criterion(output, target)<NewLine><NewLine>            # Get acc1, acc5 and update<NewLine>            acc1, acc5 = accuracy(output, target, topk=(1, 5))<NewLine>            losses.update(loss.item(), images.size(0))<NewLine>            top1.update(acc1[0], images.size(0))<NewLine>            top1.update(acc1[0], images.size(0))<NewLine>            top5.update(acc5[0], images.size(0))<NewLine>            <NewLine>            # print at i-th batch of images only<NewLine>            dist.barrier()<NewLine>            if i==0:<NewLine>                if args.gpu==0:<NewLine>                    print(""gpu 0"",acc1,output.shape)<NewLine>                if args.gpu==1:<NewLine>                    print(""gpu 1"",acc1,output.shape)<NewLine>                if args.gpu==2:<NewLine>                    print(""gpu 2"",acc1,output.shape)<NewLine>                if args.gpu==3:<NewLine>                    print(""gpu 3"",acc1,output.shape)<NewLine></code></pre><NewLine><p>And above code gives the following output:</p><NewLine><pre><code class=""lang-auto"">Use GPU: 0 for training<NewLine>Use GPU: 1 for training<NewLine>Use GPU: 3 for training<NewLine>Use GPU: 2 for training<NewLine>=&gt; loading checkpoint model_best.pth.tar'<NewLine>...<NewLine>gpu 3 tensor([75.], device='cuda:3') torch.Size([32, 200])<NewLine>gpu 2 tensor([75.], device='cuda:2') torch.Size([32, 200])<NewLine>gpu 1 tensor([75.], device='cuda:1') torch.Size([32, 200])<NewLine>gpu 0 tensor([75.], device='cuda:0') torch.Size([32, 200])<NewLine></code></pre><NewLine><p>As I am using <strong>4 GPU</strong> with a batch size of <strong>128</strong>, I think 128 images have been divided and fed into 4 GPU respectively. So all the four GPU have <code>output.shape[0]=32</code><em>(where 200 is num_classes).</em></p><NewLine><p>But what has really confused me is that, all the 4 GPU are showing the same <code>acc1</code>. In my understanding, as 4 GPUs are taking different input portion (32 images respectively), they should also give different output and accuracy corresponding to their input respectively. However, in my print test, these GPU are showing the same output and accuracy. And I don’t know why, shouldn’t they be different ?</p><NewLine><p>Looking for help. Thank you in advance !</p><NewLine></div>",https://discuss.pytorch.org/u/Wayne_Mai,(Wayne Mai),Wayne_Mai,"August 9, 2019,  1:58pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Okay, I think that maybe I have found the answer at its Github issues.<a href=""https://github.com/pytorch/examples/issues/461"" rel=""nofollow noopener"">distributed eval to be done</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s correct. Without a distributed sampler for the evaluation dataset, the different processes end up processing the same evaluation inputs, and correspondingly give the same accuracy.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Wayne_Mai; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: August 10, 2019,  7:32am; <NewLine> REPLY_DATE 2: August 12, 2019, 11:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
53050,How to launch two distributed programs on single machine?,2019-08-10T08:20:28.845Z,0,119,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I try to train ImageNet on 8-gpu server. However, <strong>I have enough memory</strong> after I start to train resnet50 using distributed training. So I want to launch another training, but the error shows that the address has been used. <strong>Can we achieve two distributed training in a single machine</strong> ?</p><NewLine></div>",https://discuss.pytorch.org/u/lxtGH,(Latou),lxtGH,"August 10, 2019,  8:20am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>How are you launching the distributed training processes?</p><NewLine><p>If manually using <code>torch.distributed</code>, try setting the <code>master_port</code> setting. The way to do this differs. Refer <a href=""https://pytorch.org/docs/stable/distributed.html#tcp-initialization"" rel=""nofollow noopener"">here</a>.</p><NewLine><p>If you’re doing it using <code>torch.distributed.launch</code> utility, then try setting the <code>--master_port</code> flag.</p><NewLine><p>Finally, it isn’t always all about the GPU memory left unused: Check if your data loader is fast enough? You could do this by checking your CPU usage. If all your CPUs are maxing out, or waiting on data io or whatever the case: if your GPU is idle waiting on data, then launching another training will just load the CPUs more, thus using even lesser GPU since the GPU would be idle most of the time waiting for data loading.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/shubhvachher; <NewLine> ,"REPLY_DATE 1: August 12, 2019,  9:19am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
52291,Simultaneously change parameters on all the GPUs,2019-08-01T09:56:22.473Z,2,355,"<div class=""post"" itemprop=""articleBody""><NewLine><p>during training the network on 8 GPUs in parallel, I am going to manually change the parameters in the network by the following code,</p><NewLine><pre><code class=""lang-python"">for param in model.parameters():<NewLine>  param.data.fill_(other parameter)<NewLine></code></pre><NewLine><p>I am wondering if this would change all the parameters on different GPU or just GPU:0?</p><NewLine></div>",https://discuss.pytorch.org/u/yangzhh,,yangzhh,"August 1, 2019,  9:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It depends what you’re using for parallelism.</p><NewLine><p>If you use <code>nn.DataParallel</code> you should be able to do this, as the model is replicated to the other GPUs in every iteration. This means you only need to modify the parameters of the root module. This is also where you’d run the optimizer, for example.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The model is replicated to the other GPUs in every iteration means the state_dicts are copied to other GPUs every iteration? So the mean and var in BN are also copied to other GPUs from the GPU:0?</p><NewLine><p>Is there any document explain this process elaborately? I am really curious about the parallel mechanism utilized in PyTorch, for I always conduct experiments on multi-gpu environment.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, that’s correct. The documentation covers this (the replication bit), see <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel"" rel=""nofollow noopener"">torch.nn.DataParallel</a>. Note that this is <strong>not</strong> how the distributed version works. There, every process runs forward/backward/optimizer against a single copy of the model, so its parameters are equivalent already. Not by replicating the values, but by executing the exact same optimizer step.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot! I know what you mean. So, in summarize, the multi-gpu environment works like following:</p><NewLine><ol><NewLine><li>Scatter the model and the state dict from GPU:0 to all the GPUs.</li><NewLine><li>Split the data, and seperately forward them on different GPUs.</li><NewLine><li>Gather output from GPUs to GPU:0</li><NewLine><li>Calculate Loss by using outputs and targets on GPU:0</li><NewLine><li>Backward Loss to GPUs and seperately calculate gradients</li><NewLine><li>Gather gradients from GPUs to GPU:0</li><NewLine><li>Update parameters on GPU:0</li><NewLine><li>GoTo Step1.</li><NewLine></ol><NewLine><p>So, the only thing that not fully synchronized is the mean and var of BN, because it does not gather to GPU:0 during backward. All the other parameters are fully synchronized because of the gather-scatter mechanism.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/yangzhh; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/yangzhh; <NewLine> ,"REPLY_DATE 1: August 1, 2019, 11:40am; <NewLine> REPLY_DATE 2: August 3, 2019,  6:11pm; <NewLine> REPLY_DATE 3: August 5, 2019,  5:14am; <NewLine> REPLY_DATE 4: August 5, 2019,  7:15am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
52288,Try to use Docker Cluster without GPU to run distributed training，but connect refused,2019-08-01T09:18:45.290Z,1,717,"<div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li><NewLine><p>2 nodes ，1 container/node</p><NewLine></li><NewLine><li><NewLine><p>only cpu</p><NewLine></li><NewLine><li><NewLine><p>code run in container</p><NewLine></li><NewLine><li><NewLine><p>connect by tcp</p><NewLine></li><NewLine><li><NewLine><pre><code class=""lang-bash"">docker run -it  run --rm -it --ipc=host --network=host xxx<NewLine></code></pre><NewLine></li><NewLine><li><NewLine><pre><code class=""lang-bash"">python mnist.py --init-method tcp://ip:port --rank 0 --world-size 2<NewLine>python mnist.py --init-method tcp://ip:port --rank 1 --world-size 2<NewLine></code></pre><NewLine></li><NewLine></ol><NewLine><p>my code is here</p><NewLine><pre><code class=""lang-auto"">from __future__ import print_function<NewLine>import argparse<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import time<NewLine><NewLine>import torch.nn.parallel<NewLine>import torch.nn.functional as F<NewLine>import torch.backends.cudnn as cudnn<NewLine>import torch.distributed as dist<NewLine>import torch.utils.data<NewLine>import torch.utils.data.distributed<NewLine>import torch.optim as optim<NewLine>from torchvision import datasets, transforms<NewLine>from torch.autograd import Variable<NewLine><NewLine># Training settings<NewLine>parser = argparse.ArgumentParser(description='PyTorch MNIST Example')<NewLine>parser.add_argument('--batch-size', type=int, default=1024, metavar='N',<NewLine>                    help='input batch size for training (default: 64)')<NewLine>parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',<NewLine>                    help='input batch size for testing (default: 1000)')<NewLine>parser.add_argument('--epochs', type=int, default=20, metavar='N',<NewLine>                    help='number of epochs to train (default: 10)')<NewLine>parser.add_argument('--lr', type=float, default=0.01, metavar='LR',<NewLine>                    help='learning rate (default: 0.01)')<NewLine>parser.add_argument('--momentum', type=float, default=0.5, metavar='M',<NewLine>                    help='SGD momentum (default: 0.5)')<NewLine>parser.add_argument('--no-cuda', action='store_false', default=False,<NewLine>                    help='disables CUDA training')<NewLine>parser.add_argument('--seed', type=int, default=1, metavar='S',<NewLine>                    help='random seed (default: 1)')<NewLine>parser.add_argument('--log-interval', type=int, default=10, metavar='N',<NewLine>                    help='how many batches to wait before logging training status')<NewLine>parser.add_argument('--init-method', type=str, default='tcp://127.0.0.1:23456')<NewLine>parser.add_argument('--rank', type=int)<NewLine>parser.add_argument('--world-size',type=int)<NewLine><NewLine>args = parser.parse_args()<NewLine>args.cuda = not args.no_cuda and torch.cuda.is_available()<NewLine><NewLine><NewLine>dist.init_process_group(init_method=args.init_method,backend=""gloo"",world_size=args.world_size,rank=args.rank,group_name=""pytorch_test"")<NewLine><NewLine>torch.manual_seed(args.seed)<NewLine>if args.cuda:<NewLine>    torch.cuda.manual_seed(args.seed)<NewLine><NewLine>train_dataset=datasets.MNIST('data', train=True, download=True,<NewLine>               transform=transforms.Compose([<NewLine>                   transforms.ToTensor(),<NewLine>                   transforms.Normalize((0.1307,), (0.3081,))<NewLine>               ]))<NewLine><NewLine>train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)<NewLine><NewLine>kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}<NewLine><NewLine>train_loader = torch.utils.data.DataLoader(train_dataset,<NewLine>    batch_size=args.batch_size, shuffle=True, **kwargs,sampler=train_sampler)<NewLine>test_loader = torch.utils.data.DataLoader(<NewLine>    datasets.MNIST('data', train=False, transform=transforms.Compose([<NewLine>                       transforms.ToTensor(),<NewLine>                       transforms.Normalize((0.1307,), (0.3081,))<NewLine>                   ])),<NewLine>    batch_size=args.test_batch_size, shuffle=True, **kwargs)<NewLine><NewLine><NewLine>class Net(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Net, self).__init__()<NewLine>        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)<NewLine>        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)<NewLine>        self.conv2_drop = nn.Dropout2d()<NewLine>        self.fc1 = nn.Linear(320, 50)<NewLine>        self.fc2 = nn.Linear(50, 10)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = F.relu(F.max_pool2d(self.conv1(x), 2))<NewLine>        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))<NewLine>        x = x.view(-1, 320)<NewLine>        x = F.relu(self.fc1(x))<NewLine>        x = F.dropout(x, training=self.training)<NewLine>        x = self.fc2(x)<NewLine>        return F.log_softmax(x)<NewLine><NewLine>model = Net()<NewLine><NewLine>model = torch.nn.parallel.DistributedDataParallelCPU(model)<NewLine>optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)<NewLine><NewLine>def train(epoch):<NewLine>    model.train()<NewLine>    for batch_idx, (data, target) in enumerate(train_loader):<NewLine>        if args.cuda:<NewLine>            data, target = data.cuda(), target.cuda()<NewLine>        data, target = Variable(data), Variable(target)<NewLine>        optimizer.zero_grad()<NewLine>        output = model(data)<NewLine>        loss = F.nll_loss(output, target)<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine>        if batch_idx % args.log_interval == 0:<NewLine>            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(<NewLine>                epoch, batch_idx * len(data), len(train_loader.dataset),<NewLine>                100. * batch_idx / len(train_loader), loss.item()))<NewLine><NewLine>def test():<NewLine>    model.eval()<NewLine>    test_loss = 0<NewLine>    correct = 0<NewLine>    for data, target in test_loader:<NewLine>        if args.cuda:<NewLine>            data, target = data.cuda(), target.cuda()<NewLine>        data, target = Variable(data, volatile=True), Variable(target)<NewLine>        output = model(data)<NewLine>        test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss<NewLine>        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability<NewLine>        correct += pred.eq(target.data.view_as(pred)).cpu().sum()<NewLine><NewLine>    test_loss /= len(test_loader.dataset)<NewLine>    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(<NewLine>        test_loss, correct, len(test_loader.dataset),<NewLine>        100. * correct / len(test_loader.dataset)))<NewLine><NewLine>tot_time=0;<NewLine><NewLine>for epoch in range(1, args.epochs + 1):<NewLine><NewLine>    train_sampler.set_epoch(epoch)<NewLine>    start_cpu_secs = time.time()<NewLine>    #long running<NewLine>    train(epoch)<NewLine>    end_cpu_secs = time.time()<NewLine>    print(""Epoch {} of {} took {:.3f}s"".format(<NewLine>        epoch , args.epochs , end_cpu_secs - start_cpu_secs))<NewLine>    tot_time+=end_cpu_secs - start_cpu_secs<NewLine>    test()<NewLine><NewLine>print(""Total time= {:.3f}s"".format(tot_time))<NewLine></code></pre><NewLine><p>and then i got problem</p><NewLine><pre><code class=""lang-auto""><NewLine>  File ""mnsit.py"", line 43, in &lt;module&gt;<NewLine>    dist.init_process_group(init_method=args.init_method,backend=""gloo"",world_size=args.world_size,rank=args.rank,group_name=""pytorch_test"")<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 416, in init_process_group<NewLine>    timeout=timeout)<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 484, in _new_process_group_helper<NewLine>    timeout=timeout)<NewLine>RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:760] connect [127.0.1.1]:10129: Connection refused<NewLine>root@pcl2-2288H-V5:/workspace/recommendation# python mnsit.py --init-method tcp://10.10.16.62:45795 --rank 0 --world-size 2<NewLine>Traceback (most recent call last):<NewLine>  File ""mnsit.py"", line 43, in &lt;module&gt;<NewLine>    dist.init_process_group(init_method=args.init_method,backend=""gloo"",world_size=args.world_size,rank=args.rank,group_name=""pytorch_test"")<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 416, in init_process_group<NewLine>    timeout=timeout)<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 484, in _new_process_group_helper<NewLine>    timeout=timeout)<NewLine>RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:760] connect [127.0.1.1]:39850: Connection refused<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/molybdenum,(molybdenumyz),molybdenum,"August 1, 2019, 10:01am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The Gloo backend tries to resolve each process’ IP address by looking at the host name. This likely resolves to the loopback address for you, looking at the error message.</p><NewLine><p>You can set <code>GLOO_SOCKET_IFNAME</code> to the network interface name you want to use for communication and it will resolve the right IP address.</p><NewLine><p>Also see the <a href=""https://pytorch.org/docs/stable/distributed.html#choosing-the-network-interface-to-use"" rel=""nofollow noopener""><code>torch.distributed</code> documentation</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks,it fixed,too many network interface the machine have,i tried all of then.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/molybdenum; <NewLine> ,"REPLY_DATE 1: August 1, 2019, 11:35am; <NewLine> REPLY_DATE 2: August 2, 2019,  1:38am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
52164,Can&rsquo;t pickle local object &lsquo;generate_dataset.&lt;locals&gt;.&lt;lambda&gt;&rsquo;,2019-07-31T09:09:52.850Z,1,4212,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I got the following after I change my code to use multiprocessing and distributed dataparallel module.<br/><NewLine>Additionally I also started to use <a href=""https://github.com/NVIDIA/apex"" rel=""nofollow noopener"">apex</a> package for mixed precision computation.</p><NewLine><pre><code class=""lang-auto"">-- Process 0 terminated with the following error:<NewLine><NewLine>Traceback (most recent call last):<NewLine><NewLine>File ""/usr/local/lib/python3.5/dist-packages/torch/multiprocessing/spawn.py"", line 19, in _wrap<NewLine><NewLine>fn(i, *args)<NewLine><NewLine>File ""/app/train_action_model_apex.py"", line 385, in main_worker<NewLine><NewLine>evaluate_model(args, root_dir)<NewLine><NewLine>File ""/app/train_action_model_apex.py"", line 343, in evaluate_model<NewLine><NewLine>dict_results = evaluator.inference()<NewLine><NewLine>File ""/app/evaluators/action_model_evaluator.py"", line 224, in inference<NewLine><NewLine>for data in self.dataloader:<NewLine><NewLine>File ""/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py"", line 193, in __iter__<NewLine><NewLine>return _DataLoaderIter(self)<NewLine><NewLine>File ""/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py"", line 469, in __init__<NewLine><NewLine>w.start()<NewLine><NewLine>File ""/usr/lib/python3.5/multiprocessing/process.py"", line 105, in start<NewLine>self._popen = self._Popen(self)<NewLine><NewLine>File ""/usr/lib/python3.5/multiprocessing/context.py"", line 212, in _Popen<NewLine><NewLine>return _default_context.get_context().Process._Popen(process_obj)<NewLine><NewLine>File ""/usr/lib/python3.5/multiprocessing/context.py"", line 274, in _Popen<NewLine><NewLine>return Popen(process_obj)<NewLine><NewLine>File ""/usr/lib/python3.5/multiprocessing/popen_spawn_posix.py"", line 33, in __init__<NewLine><NewLine>super().__init__(process_obj)<NewLine><NewLine>File ""/usr/lib/python3.5/multiprocessing/popen_fork.py"", line 20, in __init__<NewLine><NewLine>self._launch(process_obj)<NewLine><NewLine>File ""/usr/lib/python3.5/multiprocessing/popen_spawn_posix.py"", line 48, in _launch<NewLine><NewLine>reduction.dump(process_obj, fp)<NewLine><NewLine>File ""/usr/lib/python3.5/multiprocessing/reduction.py"", line 59, in dump<NewLine><NewLine>ForkingPickler(file, protocol).dump(obj)<NewLine><NewLine>AttributeError: Can't pickle local object 'generate_dataset.&lt;locals&gt;.&lt;lambda&gt;'<NewLine></code></pre><NewLine><p>It seems that the error is caused by <code>lambda</code> used for <code>ThreeCrop</code> transformation which mimics <code>FiveCrop</code> from pytorch. I follow the example code in the <a href=""https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.FiveCrop"" rel=""nofollow noopener"">link</a>.<br/><NewLine>I found that without <code>ThreeCrop</code> the error was not happened.<br/><NewLine>And before I used multiprocessing dataparallel, the error was not occured.</p><NewLine><p>The code was tested on linux environment.</p><NewLine></div>",https://discuss.pytorch.org/u/kkjh0723,(Jinhyung Kim),kkjh0723,"July 31, 2019,  9:12am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you try to write a transform class and replace the lambda method with it?<br/><NewLine>As far as I know there are some limitation in Python regarding pickling lambdas, which is apparently the case here.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your help. Maybe I should write a new transform class as you suggested.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kkjh0723; <NewLine> ,"REPLY_DATE 1: October 28, 2019,  5:57am; <NewLine> REPLY_DATE 2: August 1, 2019,  2:20am; <NewLine> ",REPLY 1 LIKES: 3 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> 
41660,Loss explodes in validation. Takes a few training steps to recover. Only when using DistributedDataParallel,2019-04-04T09:17:04.125Z,4,1019,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I train my network on multiple machines (using DistributedDataParallel) I observe my loss exploding when I switch my network to evaluation using <code>model.eval()</code> and <code>torch.no_grad()</code>.</p><NewLine><p>When outside the <code>torch.no_grad()</code> context, I switch to <code>model.train()</code> and observe a loss that is way worse than what I was observing at the end of the epoch.</p><NewLine><p>This only happens when using DistributedDataParallel.</p><NewLine><p><strong>Training</strong><br/><NewLine><img alt=""train"" height=""480"" src=""https://discuss.pytorch.org/uploads/default/original/2X/8/8d504778f024d56dde5ad32ee8c3b9cd80929a54.png"" width=""640""/></p><NewLine><p>the spikes appear at the beginning of the epoch, just after the validation step. The loss at that moment is close to what I observe in validation. Has anyone an idea about what could be causing that ?</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/Milas,,Milas,"April 4, 2019,  9:25am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Milas! This looks very odd. Is it possible that you’re seeing some data contamination between your training and validation datasets? The fact that you run with <code>no_grad</code> during evaluation mode, as well as setting <code>model.eval()</code> all sounds perfectly normal.</p><NewLine><p>You say that if you run without <code>DistributedDataParallel</code> you don’t observe this issue? Does this happen for any number of processes &gt; 1?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Pieter.</p><NewLine><p>Thanks for your answer. This looks odd indeed. There is no contamination between the sets. Everything is in separated folders and the dataset class only gets the right folder.</p><NewLine><p>I did try many things.<br/><NewLine>Even when I set to train, never switch to eval and only do training steps without ever going to the validation dataset this still occurs at the beginning of the epochs. I tried with either <code>torch.utils.data.distributed.DistributedSampler</code> or simply with random sampling without splitting the dataset between machines.</p><NewLine><ul><NewLine><li>I am using filesystem synchronisation.</li><NewLine><li>My code is fairly based on the imagenet example.</li><NewLine><li>I am using batchnorm and have seen that it does not get properly synchronized but it should not be a problem I guess since the distribution of my samples should stay the same.</li><NewLine><li>I do have some custom initialisations in the __init__ of my modules.</li><NewLine></ul><NewLine><p>If I find something I will keep this thread updated.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I was looking at <code>def train</code> in <code>DistributedDataParallel</code>, more specifically <a href=""https://github.com/pytorch/pytorch/blob/bb15580e88cc963df22eab291fbe182f42426ad5/torch/nn/parallel/distributed.py#L370-L374"" rel=""nofollow noopener"">these lines</a> and am worried the slicing of replicas may be causing this. Looking at <code>def train</code> in <code>nn.Module</code> I don’t see how the train mode would be set on <code>self.module</code>. Can you try removing the slicing to ensure the train mode is set on every module replica (even if there aren’t any) and see what happens? Since the train mode controls whether or not you’re accumulating running stats in batch norm layers, this could explain both the regression <strong>and</strong> the recovery after a couple of iterations.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please disregard my previous suggestion – this is working as intended.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes that was also my thought at first but the <code>super(DistributedDataParallel, self).train(mode)</code> should take care of <code>model_copies[0]</code></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/milas"">@Milas</a> There was a bug in DDP that didn’t take into account evaluation mode, but this was only introduced in <a href=""https://github.com/pytorch/pytorch/pull/18953"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/18953</a> which was merged 2 weeks ago. Not 25 days ago when you first started noticing this issue. This issue was fixed last night with <a href=""https://github.com/pytorch/pytorch/pull/19897"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/19897</a> and <a href=""https://github.com/pytorch/pytorch/pull/19901"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/19901</a>.</p><NewLine><p>Can you confirm this this still an issue with the current nightly build?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pietern"">@pietern</a> unfortunately it still occurs with the nightly. Tried to remove as many custom things as I could but it still happens. Perhaps having non homogeneous  gpus is the issue ? Since I don’t control the machines the job is spawned on I usually end up on a mix of kepler and pascal architectures.</p><NewLine><p>The train / eval modes probably have nothing to do with this since I tried switching to train at the very beginning and never switching again and it still occurs. Currently I am not using DDP but if I find where it goes wrong I will definitely update.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/milas"">@Milas</a> Thank you for the update. I doubt that heterogeneous GPUs is the issue… I would expect the numerical behavior to be largely identical across architectures. If you also mix library versions (e.g. cuDNN) this is more likely though.</p><NewLine><p>Any more information sure is appreciated!</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>The bug was on my side. The order of the data loaded was not deterministic. The problem disappeared once I fixed that.</p><NewLine><p>Thanks for your time and for the help.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/milas"">@Milas</a>, I’m running into the same kind of issues. Could you elaborate as to why the order of the data loaded has to be deterministic ?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>torch.utils.data.distributed.DistributedSampler</code> uses the epoch number as a seed to deterministically randomize the data to create coherent bigger batches (i.e. a different subsample of the dataset is seen each time through training and the samples do not overlap between the different training instances.)</p><NewLine><p>Also do you scale the learning rate according to the number of instances you spawn ?</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much for your help. Turns out for me the problem was in my custom loss function.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Milas; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Milas; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Milas; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Milas; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/laurentdillard; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/Milas; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/laurentdillard; <NewLine> ,"REPLY_DATE 1: April 15, 2019, 10:02pm; <NewLine> REPLY_DATE 2: April 16, 2019, 10:43am; <NewLine> REPLY_DATE 3: April 16, 2019, 10:41pm; <NewLine> REPLY_DATE 4: April 17, 2019,  3:52am; <NewLine> REPLY_DATE 5: April 17, 2019, 10:22am; <NewLine> REPLY_DATE 6: April 29, 2019,  5:27pm; <NewLine> REPLY_DATE 7: May 1, 2019,  7:59am; <NewLine> REPLY_DATE 8: May 1, 2019,  3:51pm; <NewLine> REPLY_DATE 9: May 24, 2019, 12:30pm; <NewLine> REPLY_DATE 10: July 22, 2019,  3:42pm; <NewLine> REPLY_DATE 11: July 23, 2019,  8:18am; <NewLine> REPLY_DATE 12: July 29, 2019,  5:11pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> 
36725,Distributed.all_reduce bandwidth expectations,2019-02-08T19:28:02.537Z,0,447,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to benchmark how quickly PyTorch with the Gloo backend is able to all-reduce all-gather a model synchronously. To do so, I’ve written the following script [2] working with the latest Gloo backend / PyTorch. I start it on N machines, and then they together all-reduce it fine. However, the bandwidth that I see, irrespective of N, is 0.5 * link_bandwidth. The N machines are all connected to a 100 Mbps per-port switch. This is expected with a large N, as the documentation does state that it uses a ring all-reduce/all-gather to perform the distributed.all_reduce behind that scenes.</p><NewLine><p>However, for i.e. N=2 I would expect it to perform the all-reduce at 1.0 * link_bandwidth, as one node only needs to send to one other node its full model. My experiments show however 0.5 * link_bandwidth [3]. I would expect the bandwidth of the all-reduce for any N to be [1]:</p><NewLine><p>Amount of data to send in all-reduce: (N - 1) / N<br/><NewLine>Amount of data to send in all-gather: (N - 1) / N<br/><NewLine>Total data for each node to send: 2 * (N - 1) / N<br/><NewLine>I.e., at N=2 we only need to send 2 * 1/2 = 1x model, and at N-&gt;inf we have to send 2x model. Translating into a all-reduce/all-gather bandwidth of 1.0 * link_bandwidth (N=2) and 0.5 * link_bandwidth (N-&gt;inf).</p><NewLine><p>I am not sure where my calculation is wrong, or where I am misunderstanding the all-reduce ring method.</p><NewLine><p>[1] <a href=""https://github.com/zhangruiskyline/DeepLearning/blob/master/doc/system.md#allreduce-in-practice"" rel=""nofollow noopener"">https://github.com/zhangruiskyline/DeepLearning/blob/master/doc/system.md#allreduce-in-practice</a></p><NewLine><p>[2] allreduce.py –  execute for N=2 on each of the two machines: <code>python allreduce.py 0 2 192.168.0.1 10000</code> and <code>python allreduce.py 1 2 192.168.0.1 10000</code></p><NewLine><pre><code class=""lang-auto"">#!/usr/bin/env python<NewLine><NewLine>import os<NewLine>import sys<NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>from torch.multiprocessing import Process<NewLine>import time<NewLine><NewLine># Values are 4 bytes each, so 2 * 1000 * 1000 * 32 * 4 = 256 MB = 2048 Mbit<NewLine>MODEL_SIZE_VALUES = 2 * 1000 * 1000 * 32<NewLine>BIT_PER_VALUE = 4 * 8<NewLine>BITS_PER_MBIT = 1000 * 1000<NewLine><NewLine><NewLine>def current_time_in_ms():<NewLine>    return int(round(time.time() * 1000))<NewLine><NewLine><NewLine>def run(rank, size):<NewLine>    group = dist.new_group(list(range(size)))<NewLine>    tensor = torch.ones(MODEL_SIZE_VALUES, dtype=torch.float32)<NewLine>    print(""Performing allreduce..."")<NewLine>    print(""   &gt; Data to send: %d Mbit"" % ((MODEL_SIZE_VALUES * BIT_PER_VALUE) / float(BITS_PER_MBIT)))<NewLine>    start = current_time_in_ms()<NewLine>    dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group)<NewLine>    elapsed_ms = current_time_in_ms() - start<NewLine>    print(""   &gt; Finished."")<NewLine>    print(""   &gt; Time: %.2f s"" % (elapsed_ms / 1000.0))<NewLine>    print(""   &gt; Speed: %.2f Mbit/s"" % ((MODEL_SIZE_VALUES * BIT_PER_VALUE / BITS_PER_MBIT) / float(elapsed_ms / 1000.0)))<NewLine>    print('   &gt; Result: Rank ', rank, ' has data ', str(tensor), '.\n')<NewLine><NewLine><NewLine>def init_process(my_rank, size, master_address, master_port, fn, backend='gloo'):<NewLine>    # Initialize the distributed environment<NewLine>    os.environ['MASTER_ADDR'] = master_address<NewLine>    os.environ['MASTER_PORT'] = master_port<NewLine><NewLine>    # Initialize process group<NewLine>    print(""Initializing process group..."")<NewLine>    dist.init_process_group(backend, rank=my_rank, world_size=size)<NewLine>    print(""   &gt; Initialized."")<NewLine>    print("""")<NewLine>    fn(my_rank, size)<NewLine><NewLine><NewLine>def main(my_rank, size, master_address, master_port):<NewLine>    p = Process(target=init_process, args=(my_rank, size, master_address, master_port, run))<NewLine>    p.start()<NewLine>    p.join()<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    args = sys.argv[1:]<NewLine>    if len(args) != 4:<NewLine>        print(""Usage: python allreduce.py &lt;my rank&gt; &lt;size&gt; &lt;master address&gt; &lt;master port&gt;"")<NewLine>        exit(1)<NewLine>    else:<NewLine>        main(int(args[0]), int(args[1]), str(args[2]), str(args[3]))<NewLine></code></pre><NewLine><p>[3] Output of machine 1 and 2 with N=2:</p><NewLine><p>Machine 1:</p><NewLine><pre><code class=""lang-auto"">Initializing process group...<NewLine>   &gt; Initialized.<NewLine><NewLine>Performing allreduce...<NewLine>   &gt; Data to send: 2048 Mbit<NewLine>   &gt; Finished.<NewLine>   &gt; Time: 44.79 s<NewLine>   &gt; Speed: 45.72 Mbit/s<NewLine>   &gt; Result: Rank  0  has data  tensor([2., 2., 2.,  ..., 2., 2., 2.]) .<NewLine></code></pre><NewLine><p>Machine 2:</p><NewLine><pre><code class=""lang-auto"">Initializing process group...<NewLine>   &gt; Initialized.<NewLine><NewLine>Performing allreduce...<NewLine>   &gt; Data to send: 2048 Mbit<NewLine>   &gt; Finished.<NewLine>   &gt; Time: 44.79 s<NewLine>   &gt; Speed: 45.72 Mbit/s<NewLine>   &gt; Result: Rank  1  has data  tensor([2., 2., 2.,  ..., 2., 2., 2.]) .<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/98f9a893,,98f9a893,"February 8, 2019,  7:30pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for posting the question. Your analysis is correct, it should be equal to link bandwidth for N=2 (provided the input dimensions are large enough for the runtime to be dominated by the bandwidth). The implementation in Gloo ensures there is always 1 chunk in flight and 1 chunk being reduced, so it is possible there is something wrong there for the N=2 case.</p><NewLine><p>I’ll investigate and post back here.</p><NewLine><p>edit: I added <a href=""https://github.com/facebookincubator/gloo/issues/169"" rel=""nofollow noopener"">https://github.com/facebookincubator/gloo/issues/169</a> to not lose track of it.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I got around to running a test with the Gloo benchmark tool and confirmed the issue.</p><NewLine><p>For a larger explanation of the issue and the fix, see <a href=""https://github.com/facebookincubator/gloo/pull/192"" rel=""nofollow noopener"">https://github.com/facebookincubator/gloo/pull/192</a>.</p><NewLine><p>Once this is merged and bumped in PyTorch, I expect you’ll be able to run the same test and find the bandwidth to be very close to link speed.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: February 13, 2019,  5:50pm; <NewLine> REPLY_DATE 2: July 26, 2019,  2:07pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
50833,Deadlock using DistributedDataParallel,2019-07-17T10:02:34.492Z,0,301,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m facing a problem. When using <code>DistributedDataParallel</code> with NCCL, my training will meet a deadlock. According to the pytorch doc, I try to set the <code>set_start_method</code> to <code>spawn</code> and <code>forkserver</code>, but an error that <code>address already in use</code> occurs.</p><NewLine></div>",https://discuss.pytorch.org/u/111110,(wb),111110,"July 17, 2019, 10:02am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Faced a similar error - solved it by initializing the process group first, and then setting the model cuda device (as opposed to the other way around, which led to the same kind of deadlock you describe)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vyom_Kavishwar; <NewLine> ,"REPLY_DATE 1: July 24, 2019,  2:53pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
50878,MPI_Gatherv and MPI_Scatterv ( Feature request/idea ),2019-07-17T23:16:49.614Z,0,185,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In the documentation of torch.distributed it is not made clear that all tensors in gather() need to be of the same size. After reading the code I noticed that in MPI c++ code the function used is MPI_Gather which imposes that restriction. I was thinking that maybe it would make more sense to use MPI_Gatherv instead so that tensors of variable size can be accepted. I would try to implement it myself but my C++ skills are not that good. If anyone is interested and wants to create a pull request, it would be great!. This is the file I am referring to: <a href=""https://github.com/pytorch/pytorch/blob/eb76b7a564121c5fede749ad7d0a36f2b61a0a95/torch/lib/c10d/ProcessGroupMPI.cpp#L430"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/eb76b7a564121c5fede749ad7d0a36f2b61a0a95/torch/lib/c10d/ProcessGroupMPI.cpp#L430</a></p><NewLine><p>At least if someone can provide a guide on how to make the change for MPI_Gatherv I can make the change for MPI_Scatterv myself.</p><NewLine><p>For MPI_Gatherv we need to provide a list of integers instead of a single integer. This should be easy by calling numel() on each tensor in the gather_list. We also need to provide displacements(see here: <a href=""https://www.mpich.org/static/docs/v3.1/www3/MPI_Gatherv.html"" rel=""nofollow noopener"">https://www.mpich.org/static/docs/v3.1/www3/MPI_Gatherv.html</a>). We could set this to always be the previous displacement plus the last tensor’s numel() and by setting the first displacement to 0. This is if we want to modify the built-in gather. This way it accepts tensors of any size so it covers the case of equal sized tensors. Alternatively, we could create a new torch.distributed.gatherv()</p><NewLine><p>Any help would be appreciated</p><NewLine></div>",https://discuss.pytorch.org/u/Andreas_Georgiou,(Andreas Georgiou),Andreas_Georgiou,"July 17, 2019, 11:16pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is a great idea. I agree having both gather and allgather for variable size is very useful. Also, seeing as the displacement is an implementation detail, the underlying C++ implementation can take care of sharing each process’ contribution, allocating the required memory, and returning the list of tensors.</p><NewLine><p>I created <a href=""https://github.com/pytorch/pytorch/issues/23299"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/23299</a> to track the feature.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: July 24, 2019,  9:30am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
49523,DataParallel scoring multi-GPU utilization,2019-07-02T20:09:12.446Z,2,675,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I followed the online DataParallel tutorial and I can’t get the model to split compute evenly among different GPUs at score-time (forward pass of trained model). On 3 GPUs, I get something like this:<br/><NewLine>±----------------------------------------------------------------------------+<br/><NewLine>| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |<br/><NewLine>|-------------------------------±---------------------±---------------------+<br/><NewLine>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br/><NewLine>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br/><NewLine>|===============================+======================+======================|<br/><NewLine>|   0  Tesla V100-SXM2…  On   | 00001961:00:00.0 Off |                    0 |<br/><NewLine>| N/A   53C    P0   224W / 300W |  15248MiB / 16130MiB |     93%      Default |<br/><NewLine>±------------------------------±---------------------±---------------------+<br/><NewLine>|   1  Tesla V100-SXM2…  On   | 00003673:00:00.0 Off |                    0 |<br/><NewLine>| N/A   49C    P0    86W / 300W |   7004MiB / 16130MiB |      6%      Default |<br/><NewLine>±------------------------------±---------------------±---------------------+<br/><NewLine>|   2  Tesla V100-SXM2…  On   | 00005A1F:00:00.0 Off |                    0 |<br/><NewLine>| N/A   54C    P0    76W / 300W |   6996MiB / 16130MiB |     85%      Default |<br/><NewLine>±------------------------------±---------------------±---------------------+</p><NewLine><p>So usually GPU 0 and 2 are loaded and 1 is underutilized. Also I get a very large lag in-between batches, almost 1-2 seconds of idle time when all three GPUs are at 0%, then they do some compute, then go to 0% again.</p><NewLine><p>My guess is that syncing on GPU 0 is the culprit - is there a way I can run distributed operation on multiple GPUs for scoring in pyTorch to obtain even memory usage and compute across multiple GPUs? Notice how this is different from training as I’m not computing the loss and aggregating gradients.</p><NewLine><p>The code is here: <a href=""https://github.com/waldeland/CNN-for-ASI/blob/master/test_parallel.py"" rel=""nofollow noopener"">https://github.com/waldeland/CNN-for-ASI/blob/master/test_parallel.py</a> and I already tried calling .to(device) before DataParallel and specifying “device_ids” - nothing seems to work. Another option would be to use DistributedDataParallel I suppose, but I want to understand why this isn’t working first.</p><NewLine></div>",https://discuss.pytorch.org/u/prokyon,,prokyon,"July 2, 2019,  8:09pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What is the batch size of the input you pass to the <code>nn.DataParallel</code> wrapped model?</p><NewLine><p>The input is split as evenly as possible over the devices you want to use. But if the split is odd, if you’re splitting over 3 GPUs, then it is possible for a subset of GPUs to have suboptimal performance. For example, a batch size of 11 is going to be much worse than a batch of 8. If you don’t have enough data for an even split, where every GPU gets a power-of-two sized batch, you can always fill it back up with garbage tensors, since you’re only doing inference.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s in the code: 2^12=4096. The model we’re using has a fairly small memory footprint and we want to use large batches to maximize GPU memory utilization for bulk scoring.</p><NewLine><p>I get this behavior on 2-8 GPUs, not just 3, so the odd number of GPUs shouldn’t be a factor. Do you think I should make batch size a multiple of the number of GPUs?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you tried running a profiler (like nvprof) to see if there is anything preventing the GPUs from making forward progress? This would show you if there is any imbalance between the work the GPUs perform.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>The problem is that although one can distribute forward-pass and not have it collect on one GPU, there is no way to distribute data across GPUs evenly in DataParallel: the batch goes on GPU0 (or one GPU of your choice), and then that batch get split into further minibatches on other GPUs; as a result GPU0 becomes the memory bottleneck - this article explains it well <a href=""https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255"" rel=""nofollow noopener"">https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255</a></p><NewLine><p>This behavior of DataParallel isn’t an issue for large models because size(model)&gt;size(batch), but in our case size(model)&lt;&lt;size(batch).</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see – perhaps the better approach then is to create your own version of <code>nn.DataParallel</code> that scatters straight from CPU to the right destination device. Then you don’t pay the cost of first going to GPU 0 and then scatter from there to the other GPUs.</p><NewLine><p>edit: It looks like <code>nn.DataParallel</code> already supports this if you just keep your input on CPU.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/prokyon; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/prokyon; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: July 4, 2019,  5:39pm; <NewLine> REPLY_DATE 2: July 8, 2019,  8:39pm; <NewLine> REPLY_DATE 3: July 17, 2019,  1:58pm; <NewLine> REPLY_DATE 4: July 17, 2019,  7:05pm; <NewLine> REPLY_DATE 5: July 24, 2019,  9:12am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
51486,"Multiple isend, multiple irecv between two processes",2019-07-24T08:58:44.998Z,0,127,"<div class=""post"" itemprop=""articleBody""><NewLine><p>If I send multiple tensors to another process, does the receiving process receive tensors by the order irecv is called?</p><NewLine></div>",https://discuss.pytorch.org/u/urw7rs,,urw7rs,"July 24, 2019,  8:58am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, if you use the same tag for the isend and irecv.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: July 24, 2019,  9:01am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
51438,Manually specifying ip addresses and ports of all nodes for distributed data parallel,2019-07-23T23:21:51.064Z,0,198,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I noticed that for distributed data parallel, you only need to specify the ip address and port of rank0 node, and then during initialization all nodes discover each other through rank0 node. But due to certain firewall restrictions, I want to manually specify the ip address and port of each node via which they should communicate for all reduce operations. Is there a way to do that? I am open to make changes in the pytorch source code.</p><NewLine></div>",https://discuss.pytorch.org/u/ruppesh,(Ruppesh Nalwaya),ruppesh,"July 23, 2019, 11:21pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There is no way to do this today. It also depends on which distributed backend you’re using whether this would be possible in the first place. If you’re using Gloo, it might be possible, but it’s quite a bit of work. If you’re using NCCL, it’s up to NVIDIA. If you’re using MPI, I don’t know.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: July 24, 2019,  8:49am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
51151,Shared file-system is a file?,2019-07-20T12:59:02.970Z,0,119,"<div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://pytorch.org/docs/stable/distributed.html#shared-file-system-initialization"" rel=""nofollow noopener"">link to doc</a><br/><NewLine>According to the doc page, this is how you initialize a shared file-system.</p><NewLine><pre><code class=""lang-auto"">dist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',<NewLine>                        world_size=4, rank=args.rank)<NewLine></code></pre><NewLine><p>And in the warning box above the code in the doc page, it says this code creates a file. Does this mean that I can’t use it as a directory?</p><NewLine></div>",https://discuss.pytorch.org/u/urw7rs,,urw7rs,"July 20, 2019, 12:59pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, the path has to be a file. If all your processes gracefully terminate the file will be removed. If one of your processes crashes, it may not be deleted and you’ll have to delete it yourself.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: July 24, 2019,  8:38am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
40749,Huge loss with DataParallel,2019-03-24T08:50:18.149Z,2,2164,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m training a pretty standard WGAN-GP on MNIST, and I’m trying out much larger batch sizes since that seems to be the standard wisdom now. When I parallelize across multiple GPUs I get enormous losses compared to using just one GPU. If I initialize my networks with <code>nn.DataParallel(model, [0])</code> then I get pretty normal functionality:</p><NewLine><p>ITER  0: D cost 1.200, G cost 2.729<br/><NewLine>ITER  200: D cost -3.931, G cost 2.298</p><NewLine><p>But if I use <code>nn.DataParallel(model, [0, 1, 2])</code>  to run across more GPUs, I get absurd numbers:</p><NewLine><p>ITER  200: D cost 112856899584.0, G cost 456269.437</p><NewLine><p>I’ve used dataparallel successfully before with classifiers and the tutorial DCGAN, so I have no idea what the issue is here. I’m not parallelizing the loss or doing anything different than initializing the networks in this way. Are there some caveats here (like with FP16) that I’m not aware of with DataParallel?</p><NewLine><p>I can post a full code example, but its a hundred lines or so and I’d rather not start off the post like that.</p><NewLine></div>",https://discuss.pytorch.org/u/neale,(neale ratzlaff),neale,"March 24, 2019,  8:53am",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The DataParallel module is pretty straightforward: it splits the input up into N chunks (typically across the first dimension), runs the same forward pass on N replicas of your model, and gathers the output back into a single tensor (across the same dimension as the input was split). Gradients are always accumulated in the source model (not the replicas).</p><NewLine><p>It looks like updates to buffers in these replicas don’t propagate back to the source model, as the model replicas are tossed after every forward pass. Perhaps this is a starting point for your investigation?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pietern"">@pietern</a> even if the gradients didn’t accumulate properly, it should at worst look as if I only have one GPU. I’ve verified that everything works well with up to 4 GPUs with pytorch <code>0.4.1</code>. But I can’t seem to nail down the actual problem in <code>1.0</code></p><NewLine><p>I’m not even sure how to diagnose this, but I’ve been able to replicate this behavior with dataparallel on 2 other popular WGAN repos from github.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I had the same problem with with wp-gan. For some reasons, the gradient penalty increases quickly until it reaches inf.</p><NewLine><p>Also, this happens for other gan too, e.g Self supervised gan. The behavior of loss function is also completely different when training with multi gpus.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/hung_nguyen"">@Hung_Nguyen</a> Right.<br/><NewLine>Remove the gradient penalty and the loss should still endlessly increase.<br/><NewLine>The official(?) Wasserstein GAN code doesn’t suffer from this weird behavior with parallelization, so that could be a starting point. It uses <code>nn.parallel.data_parallel</code> - the functional version of <code>nn.DataParallel</code>, but I don’t know if there’s an interesting difference there.<br/><NewLine>Converting any WGAN-GP repo into a regular WGAN mitigates the behavior somewhat, but weight clipping has its drawbacks.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Currently also experiencing this issue. The gradient penalty eventually goes to the millions before blowing up completely. <a class=""mention"" href=""/u/neale"">@neale</a>, I have tried to reproduce this issue with various popular WGAN-GP repos as well, and they also suffer from this.</p><NewLine><p>Pytorch 1.0.1 on CUDA 10.1.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/neale"">@neale</a> With 0.4 working and 1.0 not working this is clearly a regression. But we haven’t significantly modified (if at all) the data parallel wrapper between these versions. Can you check if the regression happened in 1.0.0 or 1.0.1? I have created <a href=""https://github.com/pytorch/pytorch/issues/19024"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/19024</a> to track this.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/neale"">@neale</a> Could you also try reproducing with the nightly build? There have been some changes recently related to CUDA stream synchronization that may have fixed this, per <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a>.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Running into this problem as well, using a WGAN-GP and it works perfectly on 1 GPU but the loss explodes when running on multiple GPUs.</p><NewLine><p>Using CUDA 9.2, PyTorch 1.0.1. Working on installing the nightly build to see if there is any difference.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pietern"">@pietern</a> Sorry this took quite some time.<br/><NewLine>I can confirm that the issue persists into version 1.1</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems like the issue is <a href=""https://github.com/pytorch/pytorch/issues/16433"" rel=""nofollow noopener"">#16433</a>.</p><NewLine><p>A workaround would be to calculate the gradient-penalty directly (without calling a function to do so) and calling backward in the same scope.</p><NewLine><p>For example the following code will explode on CUDA with multi-gpu:</p><NewLine><pre><code class=""lang-auto"">gp = calc_grad_penalty(network, real_target, fake_target)<NewLine>gp.backwards(retain_graph=True)<NewLine></code></pre><NewLine><p>While the following does not:</p><NewLine><pre><code class=""lang-auto"">### gp += torch.autograd.grad()<NewLine>### etc. etc. code to calcualte GP<NewLine>gp.backwards(retain_graph=True)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, <a class=""mention"" href=""/u/aluo-x"">@aluo-x</a>.</p><NewLine><p>It is also the same as <a href=""https://github.com/pytorch/pytorch/issues/16532"" rel=""nofollow noopener"">#16532</a> and there has been an attempt at a fix. The problem lies somewhere deep in the guts of autograd. This has surfaced a couple of times and there should be a fix soon (and it should be included in the next stable release).</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pietern"">@pietern</a> you mentioned that gradient accumulation only occurs in the source model, but according to <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/data_parallel.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/data_parallel.py</a> “In the forward<br/><NewLine>pass, the module is replicated on each device, and each replica handles a<br/><NewLine>portion of the input. During the backwards pass, gradients from each replica<br/><NewLine>are summed into the original module.” this implies that gradients are accumulated in the leaf nodes of each of the replicas.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>FYI, excellent debugging from <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a> and <a class=""mention"" href=""/u/ezyang"">@ezyang</a> deep in the guts of autograd led to <a href=""https://github.com/pytorch/pytorch/pull/22983"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/22983</a> and this was merged yesterday. Please give the latest nightly builds a try to see if fixed the issue.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/neale; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Hung_Nguyen; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/neale; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/aluo-x; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Nurble; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/neale; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/aluo-x; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/kiddyboots216; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: March 25, 2019,  4:56pm; <NewLine> REPLY_DATE 2: March 26, 2019, 12:30am; <NewLine> REPLY_DATE 3: March 28, 2019,  1:03am; <NewLine> REPLY_DATE 4: March 28, 2019,  1:34am; <NewLine> REPLY_DATE 5: April 7, 2019, 12:09am; <NewLine> REPLY_DATE 6: April 8, 2019,  5:06pm; <NewLine> REPLY_DATE 7: April 8, 2019,  5:11pm; <NewLine> REPLY_DATE 8: April 9, 2019, 12:45pm; <NewLine> REPLY_DATE 9: May 1, 2019,  7:07pm; <NewLine> REPLY_DATE 10: July 2, 2019,  1:15am; <NewLine> REPLY_DATE 11: July 17, 2019,  2:01pm; <NewLine> REPLY_DATE 12: July 18, 2019,  2:25am; <NewLine> REPLY_DATE 13: July 24, 2019,  8:28am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: 3 Likes; <NewLine> 
51275,Async/Parallel Evaluation of model,2019-07-22T14:17:06.557Z,1,344,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a machine with 10 GPUs and my utilization is quite bad (&lt;50% spent training) because of the computation of the metrics I have to track each epoch (not everything can be easily parallelized across 10 GPUs and they are quite involved). I have already reduced the resolution for many metrics. They are very important to me and I can’t reduce them further. For some metrics, I generate matplotlib-plots, which is also quite costly.</p><NewLine><p>I am thinking that switching from a purely sequential (train -&gt; eval -&gt; train -&gt; eval …) to a parallel setup would greatly speed up my model (So for example 8-9 GPUs are constantly occupied with training and 1-2 with evaluating my metrics). Is this possible with pytorch? The only examples I’ve found are about parallelizing the training, but this is already working.</p><NewLine><p>I would have to clone my model and push it to my evaluation-workers.</p><NewLine></div>",https://discuss.pytorch.org/u/LeanderK,(Leander Kurscheidt),LeanderK,"July 22, 2019,  2:31pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you push a copy of the current model to GPU9 and execute the evaluation method, it should run asynchronously while your other GPUs are training.<br/><NewLine>Note that your data loading might become a bottleneck, if not multiple <code>DataLoaders</code> are trying to read from your drive.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok, I’ll test it.</p><NewLine><blockquote><NewLine><p>If you push a copy of the current model to GPU9 and execute the evaluation method, it should run asynchronously while your other GPUs are training.</p><NewLine></blockquote><NewLine><p>Could this be implemented using torch.multiprocessing or what can you recommend?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>CUDA calls should run asynchronously by default.<br/><NewLine>Let me know, if you encounter an issue.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/LeanderK; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 22, 2019,  6:48pm; <NewLine> REPLY_DATE 2: July 23, 2019, 11:59am; <NewLine> REPLY_DATE 3: July 23, 2019, 10:11pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
50384,Optimize Training Speed for Recommendation System Using Subnetworks,2019-07-11T17:10:41.734Z,0,118,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am building a recommendation system inspired by YouTube’s “Deep Neural Networks for YouTube Recommendations” paper. I will need to execute recommendations in real time so I structured it with low latency predictions in mind. The structure is the following</p><NewLine><pre><code class=""lang-auto"">      |User Features|                         |Item Features|<NewLine>               |                                   |<NewLine>|Fully Connected NN_user|              |Fully Connected NN_item|<NewLine>                \                        /<NewLine>              |Concatenated output of both NNs|<NewLine>                              |<NewLine>                     |Fully Connected NN|<NewLine>                              |<NewLine>                           |output|<NewLine></code></pre><NewLine><p>This is all one network built using two sub-networks.</p><NewLine><p>The reason I did it this way is to create rich embeddings for the user and item based on their features which I could then store. At prediction time, I can retrieve the stored embeddings, then only the top NN needs to be executed and is therefore very fast. In testing, the model gives good results.</p><NewLine><p>My question is about decreasing the time it takes to train this model. Is there a way for Pytorch to execute the sub-networks in parallel? Using DataParallel splits that data and trains it in parallel, but I think that the two sub-NN are trained one after the other, even though they don’t need to be. The forward section of the model has the following structure:</p><NewLine><pre><code class=""lang-auto"">def sub-network(features, **params):<NewLine>      ....<NewLine><NewLine>def forward(user_features, item_features):<NewLine>      user_embedding = sub-network(user_features)<NewLine>      item_embedding = sub-network(item_features)<NewLine>      x = torch.cat([user_embedding, item_embedding],1)<NewLine>      ...<NewLine></code></pre><NewLine><p>What is a good strategy for parallelizing the execution of the sub-network functions?</p><NewLine></div>",https://discuss.pytorch.org/u/Misael_Manjarres,(Misael Manjarres),Misael_Manjarres,"July 18, 2019,  3:41pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You do have several GPUs to make this worthwile, right?<br/><NewLine>Given the asynchronous nature of of GPU computation, you can just move one network and inputs to the second GPU. Then it will be queued serially, but executed in parallel. Just be sure to not introduce sync points.<br/><NewLine>Or you could look at the <a href=""https://pytorch.org/docs/stable/notes/multiprocessing.html"" rel=""nofollow noopener"">multiprocessing best practices</a> for advice.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: July 19, 2019,  8:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
49836,How to use Distributed Data Parallel properly,2019-07-05T20:53:27.792Z,0,594,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m relatively new to pytorch, but have good experience with Keras &amp; Tensor flow. I’ve followed this article:  <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">DistributedDataParallel</a> to use DDP on my own training script. But for some reason, I always end up getting <code>process 0 terminated with exit status 1</code>.</p><NewLine><p>Here’s how my functions related to DDP look like:</p><NewLine><pre><code class=""lang-auto"">def setup(rank, world_size):<NewLine>    os.environ['MASTER_ADDR'] = 'localhost'<NewLine>    os.environ['MASTER_PORT'] = '12355'<NewLine><NewLine>    # initialize the process group<NewLine>    dist.init_process_group(""gloo"", rank=rank, world_size=world_size)<NewLine><NewLine>    # Explicitly setting seed to make sure that models created in two processes<NewLine>    # start from same random weights and biases.<NewLine>    torch.manual_seed(42)<NewLine><NewLine><NewLine>def cleanup():<NewLine>    dist.destroy_process_group()<NewLine>    <NewLine>    <NewLine>    <NewLine>def run_demo(fn, *args):<NewLine>    mp.spawn(fn,<NewLine>             <NewLine>             args = (args[0],args[1], args[2], args[3], args[4]),<NewLine><NewLine>             nprocs = 1 # Also tried 2 , but no difference<NewLine>             join = True<NewLine>             )<NewLine></code></pre><NewLine><p>And here’s how my train function looks like:</p><NewLine><pre><code class=""lang-auto""><NewLine>def train(model, X, batch_size = 32, epochs = 75, gradient_acc = 0):<NewLine>  <NewLine>  <NewLine>    setup(1, 2)<NewLine>    <NewLine>    <NewLine>    <NewLine>    device = model.get_default_device()<NewLine>    <NewLine>    <NewLine>    <NewLine>    model = model.to(device, non_blocking = True)<NewLine>    <NewLine>    <NewLine>    ddp_model = DDP(model, device_ids = [0]) # Only one GPU<NewLine>    <NewLine>..<NewLine>..<NewLine>..<NewLine>..<NewLine><NewLine>    <NewLine>    <NewLine>    ddp_model.hidden_enc = ddp_model.init_hidden_enc()<NewLine>    <NewLine>    <NewLine>    ddp_model.hidden_dec = ddp_model.init_hidden_dec()<NewLine>    <NewLine>    <NewLine>    ddp_model.train()<NewLine>    <NewLine>    for ep in range(epochs):<NewLine>        <NewLine>     <NewLine>        loss_br = 0; nb_batch_steps = 0<NewLine><NewLine>        <NewLine>        for step, batch in enumerate( data_loader ):<NewLine>          <NewLine>            batch = batch.to(device, non_blocking = True)<NewLine>            <NewLine>      <NewLine>            nb_batch_steps += 1<NewLine>            <NewLine>            loss = ddp_model(batch)<NewLine>            <NewLine>            ..<NewLine>             ..<NewLine>             ..<NewLine>    cleanup()<NewLine>      <NewLine><NewLine><NewLine></code></pre><NewLine><p>I’m calling the <code>run_demo</code> function in this way:</p><NewLine><pre><code class=""lang-auto"">if __name__ == ""__main__"":<NewLine>    <NewLine>    run_demo(train, model, <NewLine>         holder[:], 32, <NewLine>         75,3 )<NewLine><NewLine></code></pre><NewLine><p>I can make out that some process in the system is failing and that’s the reason why <code>spawn.py</code> is raising that error. But, I’m not sure how to rectify that issue. If I call my <code>train</code> function directly without intervention of <code>run_demo</code> the code never executes and programs seems to go in infinite loop.</p><NewLine><p>I’m on Google Colab, with single GPU.</p><NewLine><p>P.S: My <code>lscpu</code> command results in:</p><NewLine><pre><code class=""lang-auto"">Architecture:        x86_64<NewLine>CPU op-mode(s):      32-bit, 64-bit<NewLine>Byte Order:          Little Endian<NewLine>CPU(s):              2<NewLine>On-line CPU(s) list: 0,1<NewLine>Thread(s) per core:  2<NewLine>Core(s) per socket:  1<NewLine>Socket(s):           1<NewLine>NUMA node(s):        1<NewLine>Vendor ID:           GenuineIntel<NewLine>CPU family:          6<NewLine>Model:               63<NewLine>Model name:          Intel(R) Xeon(R) CPU @ 2.30GHz<NewLine>Stepping:            0<NewLine>CPU MHz:             2300.000<NewLine>BogoMIPS:            4600.00<NewLine>Hypervisor vendor:   KVM<NewLine>Virtualization type: full<NewLine>L1d cache:           32K<NewLine>L1i cache:           32K<NewLine>L2 cache:            256K<NewLine>L3 cache:            46080K<NewLine>NUMA node0 CPU(s):   0,1<NewLine>Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat arch_capabilities<NewLine></code></pre><NewLine><p>Any help is highly appreciated. Thanks !</p><NewLine></div>",https://discuss.pytorch.org/u/Amith_Adiraju,(Amith Adiraju),Amith_Adiraju,"July 8, 2019,  4:10pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!</p><NewLine><p>There are at least 2 things off in this example:</p><NewLine><ul><NewLine><li><NewLine><code>mp.spawn</code> calls the specified function with the local rank as first argument. This means that the arguments you pass are off by 1 (this is likely what causes the first error).</li><NewLine><li>You’re calling <code>setup(1, 2)</code> even if you run with a single process. This will cause a hang followed by a timeout after 30 minutes.</li><NewLine></ul><NewLine><p>Good luck!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: July 12, 2019,  1:11pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
50408,nn.Parallel wraps the whole model?,2019-07-12T01:44:20.746Z,0,285,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m rather confused after reading both official tutorials on multi-GPU and data-parallelism. Can I wrap the whole model in <code>nn.Parallel</code>, instead of one layer at a time?</p><NewLine><p>e.g. is the following code block legitimate?</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine><NewLine>class DataParallelModel(nn.Module):<NewLine><NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.block1 = nn.Linear(10, 20)<NewLine>        self.block2 = nn.Linear(20, 20)<NewLine>        self.block3 = nn.Linear(20, 20)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.block1(x)<NewLine>        x = self.block2(x)<NewLine>        x = self.block3(x)<NewLine>        return x<NewLine><NewLine>model = DataParallelModel()<NewLine>model = nn.DataParallel(model)<NewLine></code></pre><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/yuqli,(Yuqli),yuqli,"July 12, 2019,  1:44am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/yuqli"">@yuqli</a>! Yep, running the code as you have it works perfectly.  If you print out your model, you can see that the whole thing is wrapped in a <code>DataParallel</code> wrapper.</p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; model<NewLine><NewLine>DataParallel(<NewLine>  (module): DataParallelModel(<NewLine>    (block1): Linear(in_features=10, out_features=20, bias=True)<NewLine>    (block2): Linear(in_features=20, out_features=20, bias=True)<NewLine>    (block3): Linear(in_features=20, out_features=20, bias=True)<NewLine>  )<NewLine>)<NewLine></code></pre><NewLine><p>There’s some more info on <code>DataParallel</code> and how it works in <a href=""https://discuss.pytorch.org/t/how-pytorchs-parallel-method-and-distributed-method-works/30349"">this forum post</a>, where <a class=""mention"" href=""/u/rasbt"">@rasbt</a> gives a good diagram.</p><NewLine><p>Hope that answers your question!</p><NewLine><p>(If you’re curious as to how the inner workings of <code>DataParallel</code> function, <a class=""mention"" href=""/u/fantasticfears"">@fantasticfears</a> has a great <a href=""https://erickguan.me/2019/pytorch-parallel-model"" rel=""nofollow noopener"">post on their blog</a>).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/akernel; <NewLine> ,"REPLY_DATE 1: July 12, 2019,  2:20am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
50299,Differentiable communication - Distributed Model Parallel,2019-07-10T22:21:24.946Z,0,201,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone, I created this pip package that includes differentiable versions of scatter/gather/send/recv so that pytorch’s autograd can backpropagate through those. I thought I should share. I haven’t thoroughly tested it so apologies if something breaks. Contributions are welcome!</p><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://github.githubassets.com/favicon.ico"" width=""32""/><NewLine><a href=""https://github.com/ag14774/diffdist"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""420"" src=""https://avatars2.githubusercontent.com/u/10788242?s=400&amp;amp;v=4"" width=""420""/><NewLine><h3><a href=""https://github.com/ag14774/diffdist"" rel=""nofollow noopener"" target=""_blank"">ag14774/diffdist</a></h3><NewLine><p>Contribute to ag14774/diffdist development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>There is some example code here: <a href=""https://github.com/ag14774/diffdist/blob/master/diffdist/testing.py"" rel=""nofollow noopener"">https://github.com/ag14774/diffdist/blob/master/diffdist/testing.py</a></p><NewLine></div>",https://discuss.pytorch.org/u/Andreas_Georgiou,(Andreas Georgiou),Andreas_Georgiou,"July 10, 2019, 11:30pm",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Very nice, thank you for sharing!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: July 11, 2019, 10:49am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
43516,What does net.to(device) do in nn.DataParallel,2019-04-25T01:36:34.592Z,2,1351,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The following code from the tutorial to <a href=""https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"" rel=""nofollow noopener"">pytorch data paraleelism</a> reads strange to me:</p><NewLine><pre><code class=""lang-auto"">device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")<NewLine><NewLine>model = Model(input_size, output_size)<NewLine>if torch.cuda.device_count() &gt; 1:<NewLine>  print(""Let's use"", torch.cuda.device_count(), ""GPUs!"")<NewLine>  # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs<NewLine>  model = nn.DataParallel(model)<NewLine><NewLine>model.to(device)<NewLine></code></pre><NewLine><p>According to my best knowledge, <code>mode.to(device)</code> copy the data to GPU.</p><NewLine><blockquote><NewLine><p>DataParallel splits your data automatically and sends job orders to multiple models on several GPUs. After each model finishes their job, DataParallel collects and merges the results before returning it to you.</p><NewLine></blockquote><NewLine><p>If the <code>DataParallel</code> does the job of copying, what does the <code>to(device)</code> do here?</p><NewLine></div>",https://discuss.pytorch.org/u/tengerye,(Tengerye),tengerye,"April 25, 2019,  2:04am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It moves the model weights to GPU.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>If so, what does <code>nn.DataParallel(model)</code> do then?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>On calling <code>forward</code> it splits the input into multiple chunks (one chunk per GPU), replicates the underlying model to multiple GPUs, runs <code>forward</code> on each of them, and gathers the outputs.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you. I think I need to read more core code of pytorch to fully understand.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tengerye; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/tengerye; <NewLine> ,"REPLY_DATE 1: June 24, 2019, 11:10am; <NewLine> REPLY_DATE 2: July 2, 2019,  7:30am; <NewLine> REPLY_DATE 3: July 3, 2019,  2:20am; <NewLine> REPLY_DATE 4: July 3, 2019,  2:21am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
46630,What will parallel model do when calling the forward function,2019-05-30T10:40:23.638Z,0,167,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I encountered a problem that it seems the data movement operation takes long time in parallel forward, but I can’t find the reason. The main codes are as follow,</p><NewLine><pre><code class=""lang-python"">def main():<NewLine>    model = nn.DataParallel(MyModel())<NewLine>    model = model.cuda()<NewLine><NewLine>    before_time = time.time()<NewLine>    logits = model(input)<NewLine><NewLine><NewLine>class MyModel(nn.Module):<NewLine>   def forward(self, input):<NewLine>      start_time = time.time()<NewLine>      xxx<NewLine>      end_time = time.time()<NewLine>      return out<NewLine></code></pre><NewLine><p>I use 4 V100 GPU in parallel, and num_workers set to 0.</p><NewLine><p>The <em>start_time - before_time</em> takes 0.6s, and the actual forward time <em>end_time-start_time</em> only takes 0.15s. Data batch is (512 * 3 * 224 * 224).</p><NewLine><p>I am curious that the actual forward time is only 0.15 seconds, but what operations takes 0.6 seconds during before_time and start_time? Is it the data movement operation? I don’t think that such few data would cost 0.6 seconds because 0.6s is too long for moving the 77M data.<img alt="":thinking:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/thinking.png?v=9"" title="":thinking:""/></p><NewLine></div>",https://discuss.pytorch.org/u/yangzhh,,yangzhh,"May 30, 2019, 10:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I suspect that calling <code>model.cuda()</code> on <code>nn.DataParallel</code> is causing trouble. Can you try creating the model first, then calling <code>model.cuda()</code>, and <em>then</em> wrapping it in <code>nn.DataParallel</code>? I think that in this example the entire model is copied to GPU every time you call <code>forward</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you! I will try it and then reply. The original style is inspired from <a href=""https://github.com/quark0/darts/blob/master/cnn/train_imagenet.py#L92"" rel=""nofollow noopener"">DARTS</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/yangzhh; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  6:49am; <NewLine> REPLY_DATE 2: July 2, 2019,  5:34pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
49251,Specifying GPU to use with DataParallel,2019-06-29T00:38:22.715Z,3,658,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a <code>DataParallel</code> model that has been sent to the GPUs via <code>.to('cuda')</code>. I also have some processes calling this model in parallel at various points. It seems like because these are forward passes of batch size 1, they are automatically allocated to CUDA:0, which results in disproportionately high GPU utilization on that device.</p><NewLine><p>How do I specify which GPU is used in a forward pass? I don’t want to have to do any sending of parameters / state dicts. Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/hyperdo,,hyperdo,"June 29, 2019,  5:33pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I do not think there is any option/parameter to tell <code>DataParallel</code> which GPUs to use for inference/forward.  If you are only doing inference, won’t it be easy to maintain models in each GPU manually (<code>mode.to(device)</code>) instead of <code>DataParallel</code>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, I’m only doing inference. I thought that if we had <code>model_new=model.to('cuda:1')</code>, then after an update to <code>model</code>, the parameters wouldn’t be synced. Is that not right? Thanks.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you are only doing inference in <code>.eval()</code> (not <code>.train()</code> mode), there is no need for parameter sync. Isn’t it?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I thought that if you made a model <code>model_new = model.to('cuda:1')</code>, and then updated the parameters of <code>model</code> with <code>model_optimizer.step()</code>, then the parameters of <code>model_new</code> would be out of sync / differ from <code>model</code>?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>According to <a href=""https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"" rel=""nofollow noopener"">the tutorial</a>, DataParallel splits “data” automatically across available GPUs. I’m pretty sure it only works on batches, so you need batches of more than 1 sample, otherwise it might (a) make no sense to split data, (b) be very inefficient due to synchronization…</p><NewLine><p>Do you even have any usage on other GPUs than the first one? If you have batch sizes of 1, nothing would be split across GPUs and only CUDA:0 would be used.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Indeed, batch size 1 with <code>DataParallel</code> goes to first specified device (or defaults to <code>cuda:0</code>).</p><NewLine><p>If you want to do inference with batch size 1 there is no need to use <code>nn.DataParallel</code>. This would be useful only if you have a much larger batch that you want to automatically split and automatically run on multiple GPUs. If you want to manually balance batches of size 1 you’re going to have to copy the model yourself and round robin over it. You’re right that the weights are not automatically updated if the source model is updated, because they are different tensors at that point. You’ll have to re-initialize the per-device models every time after running an optimizer on a single source model. In fact, this is exactly how <code>nn.DataParallel</code> works under the covers. On every call to <code>forward</code> it replicates a single module over N devices, scatters the input, runs <code>forward</code> on every one of them, and gathers the outputs. This repeats for every iteration.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/InnovArul; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hyperdo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/InnovArul; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/hyperdo; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/alex.veuthey; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 29, 2019,  6:16pm; <NewLine> REPLY_DATE 2: June 29, 2019, 10:17pm; <NewLine> REPLY_DATE 3: June 30, 2019,  2:39am; <NewLine> REPLY_DATE 4: June 30, 2019, 11:54pm; <NewLine> REPLY_DATE 5: July 1, 2019,  6:50am; <NewLine> REPLY_DATE 6: July 1, 2019,  8:36am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
49191,How to synchronize the image size when using DistributedDataParallel?,2019-06-28T05:54:45.092Z,0,148,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Since I want to adopt multi-scale training for object detection, the image sizes would be changed per fixed frequency. When using dataparallel, the image sizes trained on each GPU can be synchronized.</p><NewLine><p>To speed up the training phase, I want to use DistributedDataParallel, but I don’t know how to synchronize the input image size. Any suggestions, please?</p><NewLine></div>",https://discuss.pytorch.org/u/CgYoung,,CgYoung,"June 28, 2019,  5:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!</p><NewLine><p>One way to do it is to resize your batches in a custom collate function that you send to your dataloader. This collate function would make sure that all the images within one batch have the same dimension.</p><NewLine><p>I would try to get this working before you start with the DistributedDataParallel. Actually, depending on your multi-scale schedule (fixed-frequency), the DistributedDataParallel might not offer any difficulties once you got the collate function up and running.</p><NewLine><p>Google + search at these forums on how to implement a custom collate function for the dataloader and give me a poke if you want to talk something over.</p><NewLine><p>Good luck <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>Edit: Since it’s object detection you also need to transform your bounding boxes. I recommend using <a href=""https://imgaug.readthedocs.io/en/latest/source/examples_bounding_boxes.html"" rel=""nofollow noopener"">imgaug</a> for this, so much easier</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Oli; <NewLine> ,"REPLY_DATE 1: June 28, 2019,  8:31am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
49036,Saving and loading optimizers in Distributed Data Parallel situations,2019-06-26T16:16:40.582Z,1,973,"<div class=""post"" itemprop=""articleBody""><NewLine><p>As is given <a href=""https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-torch-nn-dataparallel-models"" rel=""nofollow noopener"">here</a>:</p><NewLine><blockquote><NewLine><p><code>torch.nn.DataParallel</code> is a model wrapper that enables parallel GPU utilization. To save a <code>DataParallel</code> model generically, save the <code>model.module.state_dict()</code> . This way, you have the flexibility to load the model any way you want to any device you want.</p><NewLine></blockquote><NewLine><p>Considering the discussion just above this, of saving GPU models and loading on CPU etc. , I’m guessing this line refers to the data distributed models being on any one of the available GPUs and the <code>model.module</code> being the underlying module part that somehow will be device agnostic. (Please correct me here)</p><NewLine><p>That being said, what happens to the optimizer internal state variables (mentioned <a href=""https://discuss.pytorch.org/t/clarification-on-re-initializing-optimizer-in-every-epoch/48997/4"">here</a>). They too would be on whichever GPU rank they were saved from. Is this issue just solved by using the <code>map_location</code> argument to <code>torch.load()</code>? If so, why the special treatment for the DistributedDataModel (<code>model.module.state_dict()</code> used while saving)?</p><NewLine></div>",https://discuss.pytorch.org/u/shubhvachher,(Shubh Vachher),shubhvachher,"June 26, 2019,  4:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""49036""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/e480ec/40.png"" width=""20""/> shubhvachher:</div><NewLine><blockquote><NewLine><p>Considering the discussion just above this, of saving GPU models and loading on CPU etc. , I’m guessing this line refers to the data distributed models being on any one of the available GPUs and the <code>model.module</code> being the underlying module part that somehow will be device agnostic. (Please correct me here)</p><NewLine></blockquote><NewLine></aside><NewLine><p>That’s correct. If you try to serialize the <code>nn.DataParallel</code> module itself then it contains the list of devices you parallelize for, the dimension to split the input batch on, etc. When you serialize the inner module then none of that is included and is up to you to do again (or not) after you load it.</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""49036""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/e480ec/40.png"" width=""20""/> shubhvachher:</div><NewLine><blockquote><NewLine><p>That being said, what happens to the optimizer internal state variables (mentioned <a href=""https://discuss.pytorch.org/t/clarification-on-re-initializing-optimizer-in-every-epoch/48997/4"">here </a>). They too would be on whichever GPU rank they were saved from. Is this issue just solved by using the <code>map_location</code> argument to <code>torch.load()</code> ? If so, why the special treatment for the DistributedDataModel ( <code>model.module.state_dict()</code> used while saving)?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Any optimizer state will likely include the devices that those state variables live on. You’re correct to say you can use <code>map_location</code> to remap at load time. Alternatively, you can copy the optimizer state to CPU first, then serialize, and then not worry about it at load time. What special treatment exactly are you talking about?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""49036""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/pietern/40/4981_2.png"" width=""20""/> pietern:</div><NewLine><blockquote><NewLine><p>the <code>nn.DataParallel</code> module itself then it contains the list of devices you parallelize for, the dimension to split the input batch on, etc. When you serialize the inner module then none of that is included and is up to you to do again (or not) after you load it.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Nothing more! My model is training but the error doesn’t seem to be coming down… I was just exploring the possibilities… This clears it up! Thanks <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Glad to help. Good luck figuring it out.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/shubhvachher; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 27, 2019, 11:43am; <NewLine> REPLY_DATE 2: June 27, 2019, 11:43am; <NewLine> REPLY_DATE 3: June 27, 2019,  1:27pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
43390,CUDA initialization error when DataLoader with CUDA Tensor,2019-04-24T01:59:34.041Z,7,7730,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My dataset is small, and I want to load all my dataset into GPU memory when a dataset is created. Meanwhile, I still want to use <code>torch.utils.data.DataLoader</code> because of compatibility with other situations where I load my data on the fly.</p><NewLine><p>My short working example is as follows.</p><NewLine><pre><code class=""lang-auto"">import numpy as np<NewLine>from torch.utils.data import TensorDataset, DataLoader<NewLine>import torch<NewLine><NewLine>data = np.array([[1,2,3], [4,5,6]])<NewLine># I move dataset to GPU first<NewLine>ds = TensorDataset(torch.Tensor(data).cuda())                                 <NewLine>dl = DataLoader(ds, batch_size=1, num_workers=1, shuffle=True) <NewLine>for x in dl:<NewLine>    print(x)<NewLine></code></pre><NewLine><p>However, it crashes.</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-10-11b3bb8f6574&gt; in &lt;module&gt;<NewLine>      6 ds = TensorDataset(torch.Tensor(data).cuda())<NewLine>      7 dl = DataLoader(ds, batch_size=1, num_workers=1, shuffle=True)<NewLine>----&gt; 8 for x in dl:<NewLine>      9     print(x)<NewLine><NewLine>~/.conda/envs/ml/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)<NewLine>    635                 self.reorder_dict[idx] = batch<NewLine>    636                 continue<NewLine>--&gt; 637             return self._process_next_batch(batch)<NewLine>    638 <NewLine>    639     next = __next__  # Python 2 compatibility<NewLine><NewLine>~/.conda/envs/ml/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _process_next_batch(self, batch)<NewLine>    656         self._put_indices()<NewLine>    657         if isinstance(batch, ExceptionWrapper):<NewLine>--&gt; 658             raise batch.exc_type(batch.exc_msg)<NewLine>    659         return batch<NewLine>    660 <NewLine><NewLine>RuntimeError: Traceback (most recent call last):<NewLine>  File ""/home/swyoon/.conda/envs/ml/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 138, in _worker_loop<NewLine>    samples = collate_fn([dataset[i] for i in batch_indices])<NewLine>  File ""/home/swyoon/.conda/envs/ml/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 138, in &lt;listcomp&gt;<NewLine>    samples = collate_fn([dataset[i] for i in batch_indices])<NewLine>  File ""/home/swyoon/.conda/envs/ml/lib/python3.6/site-packages/torch/utils/data/dataset.py"", line 40, in __getitem__<NewLine>    return tuple(tensor[index] for tensor in self.tensors)<NewLine>  File ""/home/swyoon/.conda/envs/ml/lib/python3.6/site-packages/torch/utils/data/dataset.py"", line 40, in &lt;genexpr&gt;<NewLine>    return tuple(tensor[index] for tensor in self.tensors)<NewLine>RuntimeError: CUDA error: initialization error<NewLine></code></pre><NewLine><p>Is using a DataLoader when all my data is loaded on GPU?<br/><NewLine>Is the error an intended feature of Pytorch?</p><NewLine></div>",https://discuss.pytorch.org/u/SangWoong_Yoon,(Sang Woong Yoon),SangWoong_Yoon,"April 24, 2019,  2:00am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>When I write a custom data loader which simply batches through the <code>TensorDatasest</code>, everything is fine.<br/><NewLine>Then I guess the problem is the multiprocessing, so I tried <code>num_workers=0</code> which disables multiprocessing in the original <code>DataLoader</code>.<br/><NewLine>Now it works.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you would like to use multiple workers in your <code>DataLoader</code>, pass the <code>data</code> tensor as a CPU tensor to <code>TensorDataset</code> and push each batch to the GPU using:</p><NewLine><pre><code class=""lang-python"">ds = TensorDataset(torch.from_numpy(data))<NewLine>dl = DataLoader(ds, batch_size=1, num_workers=1, shuffle=True)<NewLine>for x in dl:<NewLine>    x = x.to('cuda', non_blocking=True)<NewLine></code></pre><NewLine><p>Otherwise multiple CUDA contexts will be initialized yielding your error.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> Thanks for the reply.</p><NewLine><ol><NewLine><li><NewLine><p>Which way would you recommend in terms of performance? The reason why I’m putting all my data to GPU first is to increase the speed.</p><NewLine></li><NewLine><li><NewLine><p>When using <code>non_blocking=True</code>, is it okay not to use <code>pin_memory=True</code> in <code>DataLoader</code>? The <a href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cuda"" rel=""nofollow noopener"">torch.Tensor.cuda() doc</a> says <code>non_blocking</code> is effective when the data is in pin memory.</p><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li><NewLine><p>If your data is already on the GPU, you won’t really need multiple workers, since most likely you are just slicing the data and passing the the model (or passing all the data at once). However, this approach uses some of your limited GPU memory, which might be used for e.g. a bigger model etc.</p><NewLine></li><NewLine><li><NewLine><p>The op will be non_blocking, if <code>pin_memory</code> was set to <code>True</code>, so you should do it. I’ve missed that part in my code snippet so thanks for pointing it out. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> Now I think everything is clear. Thank you very much!</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>The above is exactly the same issue I’m running into now.  I’ve got a working version that loads the data into gpu memory without cuda calls (or at least without causing this issue)  via pyarrow.read_parquet and it’s 2.3x faster with 4 workers than it is with 0, despite all of the data being in pinned gpu memory.</p><NewLine><p>Following the <a href=""https://pytorch.org/docs/master/notes/multiprocessing.html#sharing-cuda-tensors"" rel=""nofollow noopener"">instructions for multiprocessing best practices</a> I’ve tried to set torch’s multiprocessing start method to spawn (I’ve also tried forkserver) but when I do this I run into invalid device pointer: 0x7f8a8c000000 at /pytorch/aten/src/THC/THCCachingAllocator.cpp:301 exceptions.</p><NewLine><p>If the data is already on GPU I’d expect 0 workers would be fast, but there’s a big performance hit, so much so that it’s not really worth doing from what I can see.  I’m relatively new to multiprocessing so there’s a good chance I’m doing something wrong, and it may just make sense to stick to CPU dataloading to keep the GPU memory available for the model, but I’d love to figure out a more robust solution, particularly in light of in GPU preprocessing options like RAPIDS.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""43390""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/even_oldridge/40/11504_2.png"" width=""20""/> Even_Oldridge:</div><NewLine><blockquote><NewLine><p>s not really worth doing from what I can see. I’m relatively new to multiprocessing so there’s a good chance I’m doing something wrong, and it may just make sense to stick to CPU dataloading to keep the GPU memory available for the model, but I’d love to figure out a more robust solution, particularly in light of in GPU prepro</p><NewLine></blockquote><NewLine></aside><NewLine><p>It doesn’t seem to me that the data will be pre-loaded to the GPU. Also if the next operation depends on the data this doesn’t really gain any performance. Also if we’re moving data back to the CPU the data-loading isn’t overlapped. Is there any way to pre-load the data in the main CUDA context?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>The easiest way to preload all data on GPU is by simply copying it there (<code>Tensor.cuda()</code>) and maintaining a Python list with all samples that you want to process. Then, instead of iterating over a dataset, you iterate over a Python list with pre-existing CUDA tensors. The reasons these multiprocessing data loaders exist are 1) datasets are typically much larger than a single GPU can hold resident in memory, 2) a single CPU cannot preprocess enough examples per second to saturate GPU throughput.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SangWoong_Yoon; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/SangWoong_Yoon; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/SangWoong_Yoon; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Even_Oldridge; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Andreas_Bloch; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: April 24, 2019,  2:27am; <NewLine> REPLY_DATE 2: April 24, 2019, 11:01am; <NewLine> REPLY_DATE 3: April 24, 2019, 11:44am; <NewLine> REPLY_DATE 4: April 24, 2019, 11:53am; <NewLine> REPLY_DATE 5: April 25, 2019,  4:28am; <NewLine> REPLY_DATE 6: May 3, 2019,  3:06am; <NewLine> REPLY_DATE 7: June 26, 2019,  2:50pm; <NewLine> REPLY_DATE 8: June 27, 2019, 11:26am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 3 Likes; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 2 Likes; <NewLine> 
47129,Save model for distributeddataparallel,2019-06-05T11:55:23.941Z,6,1498,"<div class=""post"" itemprop=""articleBody""><NewLine><p>hi, i am new to distributeddataparallel, but i just find almost all the example code show in pytorch save the rank 0 model, i just want to know do we need to save all the model  if we do not sync bn parameters in our model ? so, each rank seems to have different model, if bn parameters is not sync. but we often use all the rank for inference. so, if we want to get the sample inference result as we use all the gpu for inference , should we save all the model of each rank and load all the model then inference ? hope help! thanks very much !!!</p><NewLine></div>",https://discuss.pytorch.org/u/weiwei,(weiwei),weiwei,"June 5, 2019, 11:55am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, that’s correct. Either you only run inference on the model on rank 0, or you explicitly replicate the BN stats from rank 0 to the other ranks prior to inference. The model weights should be identical across processes and only the BN stats should be different.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>i found that only use the rank 0 model trained with distributeddataparallel to inference on val dataset , performance is not as good as use all the model  trained with distributeddataparallel to inference on val dataset, only use rank 0 model usually get 0.5% to 1% accuracy slower in classification task. so do we need to allreduce the model in the training , for example, we can allreduce the model begin or after every epoch? hope for your detailed kind reply . Thanks very much.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>How do you do validation with a model wrapped with <code>DistributedDataParallel</code>? I imagine you call <code>.eval()</code> everywhere and have every worker validate a subset of your validation set, followed by allreduce of the accuracies. Is this correct?</p><NewLine><p>Another piece of information that might be useful here: if you construct <code>DistributedDataParallel</code> with <code>broadcast_buffers=True</code> then all ranks will receive a copy of the registered buffers prior to executing a forward pass. With respect to batch normalization this means they all use the running stats from rank 0.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, i use every rank to validate a subset of my validation set, it is a default setting in distributed distributeddataparallel when we define the dataset loader.  Anyhow it is not an important point.<br/><NewLine>As the default setting of broadcast_buffers is True. so batch normalization is only calculated in rank 0 and every other rank shared the same batch normalization statistic in rank 0.<br/><NewLine>but in fact accuracy is different. (1)when i only save the  model in rank 0 and all the other ranks load rank0’s model to do inference accuracy is 75%. (2) each rank save its own model and load its own model to do inference accuracy can be 75.8%.<br/><NewLine>since model.eval() will freeze the bn parameter.<br/><NewLine>i think the most difference might be : (1) each rank’s model is not same. they share different model, such as bn parameters at least.<br/><NewLine>(2)Furthermore, in the backward process, different bn parameters might cause the gradient calculated different in every layer, i am not sure whether the gradient is calculated in every each rank or they only broadcast the gradient calculated in rank 0.<br/><NewLine>so, what is the truth ?  Thank you very much.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>The only difference I can imagine is that the BN parameters don’t get replicated from rank 0 to the other ranks after the final call to backward, since they are frozen for evaluation after that. I highly doubt that can be the cause of a 0.8% accuracy difference though. What is your approach for aggregating the accuracy numbers?</p><NewLine><p>Regarding (2): the gradient is computed in every process and then averaged across them. This means that if you start with identically initialized model weights and use the same optimizer in all processes, the optimizer will take the same step everywhere.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your kind reply.</p><NewLine><p>(1)Yes, i am sure that get 0.8% accuracy difference, if you run the pytorch / example code for imagenet, you will get the same truth, that difference definitely  exists, some time the difference is small  0.1%, some time you may get 0.3% or larger difference between accuracy. Difference seed and num_worker might cause different gap between the two method i am not sure for the truth.</p><NewLine><p>(2)By the way, i am not able to solve the reproducible problem for pytorch with distributeddataparallel. i have try to follow the setting with cudnn.benchmark=False cudnn.deterministic=True, and set the  torch seed and cuda seed for each rank and dataloader worker_init_fn ( numpy seed and random.seed ) for dataloader and fix the num_workers  to a fixed number every time runing my code, but result always different. how can we reproduce our own experiments with the same setting.</p><NewLine><p>(3)As for the accuracy, each rank save the index for error file and write to a json file. at last i summary all the json file to get the accuracy. this is an accurate method.  i have also follow the dist.all_reduce method to get a similar result. in both case, difference quite exists and some time quite be unexpected.<br/><NewLine>so, i do really think sync bn parameters is quite important for some tasks, when we can not shared a big batch size.</p><NewLine><p>(4) As for the difference between our model in each rank , should we all_reduce all model for each rank  right after each epoch, so bn parameter get all_reduce for average. this may not cause too much time when we run in multi-node.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Regarding the random seed, do you also call <code>torch.manual_seed()</code>? There are a few that you have to initialize unfortunately.</p><NewLine><p>Regarding the BN stats sync. If you use DDP with <code>broadcast_buffers=True</code> then it will replicate the BN statistics before every forward pass and make sure all ranks have a copy of the stats used by rank 0. The models must be identical after this step happens. You could try to confirm this by passing an identical input to the model and verify they produce an identical output.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>so, how can i make sure each rank share the same model when i use the DDP.<br/><NewLine>to be specific, do i need to set the torch seed to be same for each rank, so the initialized model will be same for each rank at the beginning?<br/><NewLine>it seems that each rank display different output .</p><NewLine><p>another problem is how can i make the experiments reproducible ? thanks very much.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>The initialized model is broadcast from rank 0 to the other processes from the DDP constructor, so the initialized parameters will be identical when you first call <code>forward</code>.</p><NewLine><p>Numerical reproducibility depends on determinism on the inputs/transforms, deterministic partitioning of the dataset, as well as deterministic execution of your model (e.g. there are modes where cuDNN does not execute deterministically). If at any of these you have some non-determinism, numerical reproducibility is impossible.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey all,<br/><NewLine>Why don’t you take a look at <a href=""https://github.com/psyec1/Lipreading-PyTorch/issues/7"" rel=""nofollow noopener"">SyncBatchNorm</a>.</p><NewLine><p>From the link: For a <code>network</code>, already initialized, that has any BatchNormalization layers you can do :<br/><NewLine><code>sync_bn_network = torch.nn.utils.convert_sync_batchnorm(network, process_group)</code></p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""11"" data-topic=""47129""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/e480ec/40.png"" width=""20""/> shubhvachher:</div><NewLine><blockquote><NewLine><p>twork = torch.nn.utils.convert_sync_batchnorm(networ</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thanks a lot, since this new layer in pytorch1.1 operation is quite slow when apply to multi-node distributed training, so i do not plan to do so. speed can slow down to 2 times longer when i training on 4 node with each node 4 gpus. multi-node gpu communication is an bottleneck in my case.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>oh! Thats interesting. I’m using the SyncBatchNorm layer currently in single node 8GPU training.</p><NewLine><p>Did you try to implement your own BN synchronization code? I see that you had an idea here</p><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""47129""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/w/f9ae1b/40.png"" width=""20""/> weiwei:</div><NewLine><blockquote><NewLine><p>so do we need to allreduce the model in the training</p><NewLine></blockquote><NewLine></aside><NewLine><p>Did you implement synchronization? Also, were you successful in getting your accuracy up?</p><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""47129""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/w/f9ae1b/40.png"" width=""20""/> weiwei:</div><NewLine><blockquote><NewLine><p>(2)Furthermore, in the backward process, different bn parameters might cause the gradient calculated different in every layer, i am not sure whether the gradient is calculated in every each rank or they only broadcast the gradient calculated in rank 0.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Also, did you test the above? If you did, what were the results?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/weiwei; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/weiwei; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/weiwei; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/weiwei; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/shubhvachher; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/weiwei; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/shubhvachher; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  6:12am; <NewLine> REPLY_DATE 2: June 24, 2019,  8:33am; <NewLine> REPLY_DATE 3: June 24, 2019, 11:09am; <NewLine> REPLY_DATE 4: June 24, 2019, 11:36am; <NewLine> REPLY_DATE 5: June 24, 2019,  2:49pm; <NewLine> REPLY_DATE 6: June 24, 2019,  5:18pm; <NewLine> REPLY_DATE 7: June 25, 2019,  9:51am; <NewLine> REPLY_DATE 8: June 26, 2019,  2:14am; <NewLine> REPLY_DATE 9: June 26, 2019,  6:46am; <NewLine> REPLY_DATE 10: June 26, 2019, 12:19pm; <NewLine> REPLY_DATE 11: June 26, 2019,  1:50pm; <NewLine> REPLY_DATE 12: June 26, 2019,  2:19pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> 
46616,A GPipe implementation in PyTorch,2019-05-30T06:50:11.736Z,5,767,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Kakao Brain announces <a href=""https://github.com/kakaobrain/torchgpipe"" rel=""nofollow noopener"">torchgpipe</a>, an implementation of <a href=""https://arxiv.org/abs/1811.06965"" rel=""nofollow noopener"">GPipe</a> in PyTorch as a handy library.</p><NewLine><pre><code class=""lang-python"">from torchgpipe import GPipe<NewLine>model = nn.Sequential(a, b, c, d)<NewLine>model = GPipe(model, balance=[1, 1, 1, 1], chunks=8)<NewLine>output = model(input)<NewLine></code></pre><NewLine><p>GPipe is a scalable pipeline parallelism library published by Google Brain. It leverages the training of a giant model which requires much memory. For instance, Google trained AmoebaNet-B with 557M parameters over GPipe.</p><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://github.githubassets.com/favicon.ico"" width=""32""/><NewLine><a href=""https://github.com/kakaobrain/torchgpipe"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""400"" src=""https://avatars0.githubusercontent.com/u/25736994?s=400&amp;amp;v=4"" width=""400""/><NewLine><h3><a href=""https://github.com/kakaobrain/torchgpipe"" rel=""nofollow noopener"" target=""_blank"">kakaobrain/torchgpipe</a></h3><NewLine><p>A GPipe implementation in PyTorch. Contribute to kakaobrain/torchgpipe development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>",https://discuss.pytorch.org/u/sublee,(Heungsub Lee),sublee,"May 30, 2019,  6:50am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m trying to use torchgpipe on some other models. But the training time increased with GPipe. And I can’t reproduce the paper’s result with torchgpipe’s example resnet101. I think I might measure the training time in a wrong way. How did you measure the training time of resnet101 on GPipe? Thanks ahead!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I would expect the training time to take a hit, because you’re moving much more data around compared to a direct forward/backward. All of that overhead will come at a performance penalty. If I understand correctly, pipelining with this approach is best suited for allowing extremely large models to train if you have limited memory available.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, thank you for your question.</p><NewLine><p>There are some conditions to optimize a model with GPipe:</p><NewLine><ul><NewLine><li>The model requires a large amount of memory.</li><NewLine><li>The original batch size is not so small. Because we need a micro-batch which is not too small. If a micro-batch is too small, GPU wouldn’t be utilized.</li><NewLine><li>Well balanced. The imbalance between partitions makes GPipe underutilized.</li><NewLine></ul><NewLine><p>I just published <a href=""https://github.com/kakaobrain/torchgpipe/tree/master/examples/resnet101_performance_benchmark"" rel=""nofollow noopener"">my ResNet-101 performance benchmark</a>. If you have the same environment with me, I expect you get the same result. Even you don’t have the same environment, the code will be helpful. Especially, you can check the balance of ResNet-101 what I’ve used, and how I measured the training time.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah, I totally agreed. I guess this approach only suits some models. It’s just in the paper they achieved really great result on AmoebaNet, in terms of both memory usage and training time, which made me doubt my result.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot for your expaination! My GPU memory is too small for a large batch size which limits my tests. I guess the main purpose of GPipe is to enable us to train extremely large model not to accelerate the training process.</p><NewLine><p>I noticed that in the your experiments, pipeline methods use different batch size. Isn’t this going to affect the model performance?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Good question. Yes, it affects. My experiment reproduces a performance benchmark in the original paper. The benchmark also uses adjusted batch size to maximize throughput regardless of the model accuracy.</p><NewLine><blockquote><NewLine><p>In our experiments, we mea- sured the effects of pipeline parallelism and recomputation on the model throughput of ResNet-101 and AmoebaNet-D (4, 512). We fixed the image size at 224 × 224. <em>We adjusted the mini-batch size to maximize the throughput.</em></p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see, thank you so muck!</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just released v0.0.2 of torchgpipe with <a href=""https://torchgpipe.readthedocs.io/en/v0.0.2/"" rel=""nofollow noopener"">the detailed documentation</a>. This version includes the automatic balancing.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ddd; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/sublee; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ddd; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ddd; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/sublee; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ddd; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/sublee; <NewLine> ,"REPLY_DATE 1: June 21, 2019,  7:49pm; <NewLine> REPLY_DATE 2: June 24, 2019,  4:44am; <NewLine> REPLY_DATE 3: June 24, 2019,  5:24am; <NewLine> REPLY_DATE 4: June 24, 2019,  6:18pm; <NewLine> REPLY_DATE 5: June 24, 2019,  9:45pm; <NewLine> REPLY_DATE 6: June 25, 2019,  3:02am; <NewLine> REPLY_DATE 7: June 25, 2019, 10:51pm; <NewLine> REPLY_DATE 8: June 26, 2019,  9:07am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> 
38055,Calling DistributedDataParallel on multiple Modules?,2019-02-23T22:54:10.760Z,1,655,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m wondering if anyone has some insight into the effects of calling DDP twice for one process group initialization? Good example of this would be a GAN where there are two distinct models. Can they both safely be wrapped in DDP? I suppose a dummy container module could be made that encases both models and only requires a single DDP wrapper. I looked around but didn’t see anything posted about this.</p><NewLine></div>",https://discuss.pytorch.org/u/mdlockyer,(Michael Lockyer),mdlockyer,"February 23, 2019, 10:54pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m also curious about this question. Have you solved it?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Using the same process group for multiple DDP wrapped modules <strong>may</strong> work, only if they are independently used, and a call to backwards doesn’t involve both models. For GANs this may be the case, where you alternate between training the discriminator and the generator. That said, if a single call to backward involves gradient accumulation for more than 1 DDP wrapped module, then you’ll have to use a different process group for each of them to avoid interference.</p><NewLine><p>You can do this as follows:</p><NewLine><pre><code class=""lang-python"">pg1 = torch.distributed.new_group(range(torch.distributed.get_world_size()))<NewLine>model1 = DistributedDataParallel(<NewLine>    create_model1(),<NewLine>    device_ids=[args.local_rank],<NewLine>    process_group=pg1)<NewLine><NewLine>pg2 = torch.distributed.new_group(range(torch.distributed.get_world_size()))<NewLine>model2 = DistributedDataParallel(<NewLine>    create_model2(),<NewLine>    device_ids=[args.local_rank],<NewLine>    process_group=pg2)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is interesting. With most GAN architectures, the backward pass of the generator does indeed include both G and D. Generally you will get some prediction from the discriminator using the fake samples and then call backward on that, propagating that output through D, then G. It may actually be a requirement then that GANs use separate process groups for each model. Granted, in this architecture, weight updates only occur for one model at a time, but gradients should be accumulated for both D and G during the G backward/update stage.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>If it is indeed the case that GANs need separate process groups for G and D, then that is something that definitely needs to be in the docs. I’ve had some strange training results while using DDP and this may be the cause.</p><NewLine><p><a href=""/u/pietern"">@pietern</a> Do you know if the interference you speak of would cause an exception, or just produce incorrect gradients?</p><NewLine><p>I want to put together a test bed for this and see if there are indeed different gradients when using 1 vs 2 PGs.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mdlockyer"">@mdlockyer</a> I think bad behavior would result in crashes or hangs. Calls to allreduce that are mixed and matched with different dimensionality across processes is a recipe for out of bound memory access and undefined behavior.</p><NewLine><p>Would it be possible to use <code>autograd.grad</code> for the discriminator part and <code>autograd.backward</code> for the generator part? This would avoid gradient accumulation and reduction for the discriminator. You’d still have to stitch them together at the boundary of course.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/pietern"">@pietern</a> that’s an interesting idea! Only <code>backward()</code> will trigger the reducer right? Off the top of your head, what would the stitch look like between <code>grad</code> and <code>backward</code>?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, only <code>backward()</code> interacts with the reducer.</p><NewLine><p>I imagine combining <code>grad</code> and <code>backward</code> like this (but YMMV and I don’t know if this works).</p><NewLine><pre><code class=""lang-auto"">G_out_grad = torch.autograd.grad(D_loss, G_out)<NewLine>torch.autograd.backward(G_out, G_out_grad)<NewLine></code></pre><NewLine><p>This wouldn’t trigger any reduction for the discriminator and only do so for the generator.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Very cool. When I get a second I’m going to implement this and test it out.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/krumo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mdlockyer; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mdlockyer; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mdlockyer; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mdlockyer; <NewLine> ,"REPLY_DATE 1: April 23, 2019, 10:51am; <NewLine> REPLY_DATE 2: April 23, 2019,  4:44pm; <NewLine> REPLY_DATE 3: April 23, 2019,  6:49pm; <NewLine> REPLY_DATE 4: April 23, 2019,  6:53pm; <NewLine> REPLY_DATE 5: June 24, 2019, 11:55am; <NewLine> REPLY_DATE 6: June 24, 2019,  3:58pm; <NewLine> REPLY_DATE 7: June 25, 2019,  7:17pm; <NewLine> REPLY_DATE 8: June 25, 2019,  7:22pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> 
48822,How to finetune models trained by distributed data parallel(ddp),2019-06-24T10:42:04.960Z,2,343,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to finetuning model by replacing only the last linear layer.<br/><NewLine>I could do that when I used DataParallel module as below</p><NewLine><pre><code>model = nn.DataParallel(model)<NewLine><NewLine>...<NewLine>model.load_state_dict(checkpoint['state_dict'])<NewLine>...    <NewLine><NewLine>model.module.fc = torch.nn.Linear(model.module.fc.in_features,<NewLine>                                    opt.n_finetune_classes)<NewLine>model.module.fc = model.module.fc.cuda()<NewLine></code></pre><NewLine><p>In case of DDP, how can I do that?<br/><NewLine>according to the warning in the <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel"" rel=""nofollow noopener"">document</a>,  it seems that I cannot change parameters after I load the checkpoint.</p><NewLine><pre><code>This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers.<NewLine></code></pre><NewLine><p>Should I change the linear layer first and load the parameters without the linear layer?<br/><NewLine>Is that a right way?</p><NewLine></div>",https://discuss.pytorch.org/u/kkjh0723,(Jinhyung Kim),kkjh0723,"June 24, 2019, 10:42am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, try modifying the module first, and once you’re done, wrapping it in <code>nn.DataParallel</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/pietern"">@pietern</a>. How can I load some part of parameters after wrapping with <code>nn.parallel.DistributedDataParallel</code>?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It is easier to load the parameters prior to wrapping with DDP. You can save/load the wrapped model as well, but then you can no longer use it <strong>without</strong> DDP.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kkjh0723; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019, 11:12am; <NewLine> REPLY_DATE 2: June 25, 2019,  1:43am; <NewLine> REPLY_DATE 3: June 25, 2019,  7:28am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
41385,Interactive debug in distributed.launch,2019-04-01T12:24:51.189Z,0,445,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, is there any interactive way to debugin distributed launch?<br/><NewLine>In pytorch 0.4.1, I use pdb in dataparallel, however, it seems that distributed launch would split the process into multiple copies. When I type sth in pdb, my input would also be split to different copies of processes, that’s not what I expected.</p><NewLine><p>So I’m wondering if there is any method that can help me do interactive debugging in distributed launch?</p><NewLine></div>",https://discuss.pytorch.org/u/tree33,,tree33,"April 1, 2019, 12:24pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This happens because all processes share the same input file descriptor. When you type a character, the first process who reads it will get it. This makes interactive debugging almost impossible. What you can try, in lieu of a a proper solution, is close the input descriptor by running <code>sys.stdin.close()</code> on the ranks where you don’t want to run pdb.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019, 12:09pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
41401,Loading optimizer in a distributed setting,2019-04-01T15:29:35.773Z,0,136,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to load snapshot from a file on one of the machines running in a distributed setting. From what I see, optimizers aren’t broadcast among machines in such a case. Is there any easy way to do it?</p><NewLine></div>",https://discuss.pytorch.org/u/Konstantin_Solomatov,(Konstantin Solomatov),Konstantin_Solomatov,"April 1, 2019,  3:29pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There is no common way to expose optimizer state AFAIK. If you know how you can access the state of your optimizer then you’ll be able to synchronize it by using <code>torch.distributed</code> collectives directly, e.g. <code>broadcast</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019, 12:03pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
43323,What is the best practice for running distributed adversarial training?,2019-04-23T11:30:53.491Z,1,479,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I’m just a newbie to PyTorch and struggling for PyTorch distributed training. Currently, I’m trying to implement a GAN like training strategy. The training consists of two stages:</p><NewLine><ol><NewLine><li><NewLine><p>Fix task network, train discrinmator, my workflow is as following:<br/><NewLine>src_data -&gt; T() -&gt;detach()-&gt; D() -&gt; loss(src_pred, src_label)<br/><NewLine>tgt_data -&gt; T()-&gt;detach()-&gt;D()-&gt;loss(tgt_pred, tgt_label)</p><NewLine></li><NewLine><li><NewLine><p>Fix discrinmator, train task network, my workflow is as following:<br/><NewLine>src_data-&gt;T()-&gt;supervised_loss<br/><NewLine>tgt_data-&gt;T()-&gt;D()-&gt;-1*loss(tgt_pred, tgt_label)<br/><NewLine>The task network T() and discriminator network D() are both wrapped in DDP and they are placed in different process group. The task network is trained with supervised loss with labeled data and finetuned by the adversarial loss with unlabeled data.</p><NewLine></li><NewLine></ol><NewLine><p>For this setting I have 2 questions:</p><NewLine><ol><NewLine><li>Is it the correct way to combine two DDP models? Or do I have to warp them into one single module first and then place them under DDP?</li><NewLine><li>During training process of task network, I have to fix discriminator’s parameters. Now I just set the requires_grad of all parameters in discrinmator as False and turn them back to True after the loss.backward() is called. Is there anything else to be changed? I found DDP doesn’t allow unused parameters now, but it seems okay to use a module which doesn’t require gradients entirely. Do I do it in a correct way?</li><NewLine></ol><NewLine><p>I’ll appreciate if there’s somebody could tell me what’s the best practice of implementing multiple models for adversarial training. Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/krumo,(Haoran Wang),krumo,"April 23, 2019, 11:30am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have opened a discussion here about a similar question regarding two DDP modules in a GAN setting <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/calling-distributeddataparallel-on-multiple-modules/38055/5"">Calling DistributedDataParallel on multiple Modules?</a>. I’m still trying to determine if one process group can suffice, but it seems like the safest course of action is to use separate groups for G and D.</p><NewLine><p>Regarding setting requires_grad to False on D while back propagating G’s loss, I have been meaning to implement that same thing but never got around to it. It seems like the logical approach, as it is just wasting compute time calculating gradients for D when they are going to be discarded.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""43323""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/m/3bc359/40.png"" width=""20""/> mdlockyer:</div><NewLine><blockquote><NewLine><p>Regarding setting requires_grad to False on D while back propagating G’s loss, I have been meaning to implement that same thing but never got around to it. It seems like the logical approach, as it is just wasting compute time calculating gradients for D when they are going to be discarded.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I think that doing exactly that would make it work with a single process group. Because you no longer race the allreduce calls from the two models. Also, I think you could put the discriminator in eval mode when doing this, which side steps some of the synchronization code paths in <code>DistributedDataParallel</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mdlockyer; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: April 23, 2019,  7:08pm; <NewLine> REPLY_DATE 2: June 24, 2019, 11:57am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
41523,DistributedDataParallel - Master starts without workers,2019-04-02T20:46:30.435Z,0,155,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!</p><NewLine><p>I’m implementing DistributedDataParallel in my code. However, when I start it, if I use PyTorch’s lanch module, one task will start training before the others have begun. This is different from without using PyTorch’s launch module, when I’ll see the processes wait on each other before starting the next epoch, etc.</p><NewLine><p>I’m using an implementation that mirrors <a href=""https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255"" rel=""nofollow noopener"">this Medium article</a>. I’ve been struggling with this issue for two days now, so any help would be extremely appreciated!</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/Moin_Nadeem,(Moin Nadeem),Moin_Nadeem,"April 2, 2019,  8:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>When <code>torch.nn.parallel.DistributedDataParallel</code> is initialized with the right distributed context then every iteration should happen in lock step between all processes. If a single process starts going by itself, I think there is something missing in initialization.</p><NewLine><p>Can you share a code snippet how you initialize all of them?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019, 11:39am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
48494,Heterogeneous GPUs but same computation time,2019-06-20T08:03:14.849Z,2,150,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I’m using a RTX 2080 ti and a GTX 1050 ti in a two node cluster using pytorch. The problem cames when I execute it (distributed) but both of them take the same time solving MNIST. There are no sync points. Can anyone help me?</p><NewLine></div>",https://discuss.pytorch.org/u/hpc-unex,(SergioUnex),hpc-unex,"June 20, 2019,  8:03am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>My cuda version is 10.0 in RTX and 9.2 in GTX. Im using pytorch 1.2 with mpi 3.1</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you using DDP?<br/><NewLine>If so, the slower card might sync the faster one.</p><NewLine><p>Or are you profiling the cards separately? If so, your code might have other bottlenecks (e.g. data loading). Have you profiled it?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>We don’t guarantee compatibility between different versions of PyTorch. You say you have one version compiled against CUDA 10 and another against CUDA 9.2. This <em>might</em> work, but YMMV.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, im using DPP, but im using asynchronous all_reduce to average gradients, so theres no synchronization if I dont make explicit a.wait() (which im not doing just to test), right? In that case, training times still the same which makes no sense for me. Am I losing anything?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hpc-unex; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/hpc-unex; <NewLine> ,"REPLY_DATE 1: June 20, 2019,  8:07am; <NewLine> REPLY_DATE 2: June 22, 2019,  7:57pm; <NewLine> REPLY_DATE 3: June 24, 2019,  4:41am; <NewLine> REPLY_DATE 4: June 24, 2019, 11:09am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
44105,DistributedDataParallel with autograd.grad,2019-05-01T18:36:49.405Z,0,248,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am using a loss function that contains the gradient of the output w.r.t. to the input of the network, which I obtained with <code>autograd.grad</code>.</p><NewLine><p>I am interested in training my model in parallel using the <code>DistributedDataParallel</code> container. However, as one WARNING in the doc page mentions, <code>DistributedDataParallel</code> does not support <code>autograd.grad</code>. If I understand correctly, this is because local model parameters (not the one averaged across devices) will be used if I use <code>autograd.grad</code> after the <code>forward</code> call. Of course, this is incorrect.</p><NewLine><p>Looking into the implementation of <code>DistributedDataParallel</code>, I found the method <a href=""https://github.com/pytorch/pytorch/blob/de582e2f89d154ef7579caf8fe8aac80c2d8a569/torch/nn/parallel/distributed.py#L412"" rel=""nofollow noopener"">_sync_params</a> is called at the beginning of the <code>forward</code> method to sync params across devices. My question is:</p><NewLine><p>Is it OK for me to call <a href=""https://github.com/pytorch/pytorch/blob/de582e2f89d154ef7579caf8fe8aac80c2d8a569/torch/nn/parallel/distributed.py#L412"" rel=""nofollow noopener"">_sync_params</a> once more before I use <code>autograd.grad</code> to compute the gradient of the output w.r.t. to the input and then use it in my loss function? In such, the gradient computation will use the averaged parameters. Is there any caveats?</p><NewLine></div>",https://discuss.pytorch.org/u/wenxx151,,wenxx151,"May 1, 2019,  6:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The problem here is that <code>DistributedDataParallel</code> performs gradient averaging across processes by hooking into the <a href=""https://github.com/pytorch/pytorch/blob/v1.1.0/torch/csrc/autograd/functions/accumulate_grad.cpp"" rel=""nofollow noopener""><code>AccumulateGrad</code></a> function. This allows for performing averaging for the last most gradients while autograd is still running.</p><NewLine><p>Would it be possible for you to first compute the initial loss, call <code>autograd.backward</code> instead of <code>autograd.grad</code>, and have it accumulate the first order gradients in the model parameters? Then you could detach those and compute something else before letting the optimizer do its thing. If not, then you’ll have to perform your own averaging, I think.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019, 10:00am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
44175,Gradients update,2019-05-02T11:16:51.959Z,0,122,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>As I see in the code, there is a Queue used by background threads in order to communicate the parameters with Allreduce. My question is how this parameters are updated. Allreduce is a blocking collective so the background threads will be waiting until all of them enqueue their parameters. So, maybe there is the possibility that parameters arent updated so a process isnt taking into account sometimes the parameters of the others process which have different data. Am I right? Does this affect to the precision ? How the parameter update work then? Whats the point of using a Queue?</p><NewLine></div>",https://discuss.pytorch.org/u/hpc-unex,(SergioUnex),hpc-unex,"May 2, 2019, 11:16am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Which code were you looking at?</p><NewLine><p>In version 1.0 the distributed backend has been updated such that all collectives run asynchronously w.r.t. the main thread, even if they are blocking. For MPI collectives this means they are run on a single background thread. The queue approach was taken in PyTorch before version 1.0. In 1.1 we introduced a new C++ based gradient reduction mechanism (see <a href=""https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/reducer.cpp"" rel=""nofollow noopener"">reducer.cpp</a>) that concatenates multiple gradients into a large bucket and performs <code>allreduce</code> against those buckets instead of individual parameters.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  9:45am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
44462,Asynchronous Allreduce gradients,2019-05-05T18:35:27.589Z,0,296,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I want to know how the MPI_Allreduce works in asynchronous mode when the gradients are calculated. Suppose we have 3 processes. If the first epoch is finished and only one process have update the gradients, when it takes the gradients from a shared buffer it takes NaN in the SUM of the process that havent finished ?? Im pretty lost here because Allreduce is a blocking primitive but the training doesnt stop for it.</p><NewLine></div>",https://discuss.pytorch.org/u/hpc-unex,(SergioUnex),hpc-unex,"May 5, 2019,  6:35pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What do you mean by “training doesn’t stop”?</p><NewLine><p>Also, how do you run allreduce in asynchronous mode? The synchronization done by <code>torch.nn.parallel.DistributedDataParallel</code> is done implicitly, when you make autograd compute gradients for your model parameters. It doesn’t return until all the allreduce calls have finished (or in the case of CUDA tensors, until all the NCCL allreduce kernels have been queued).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  9:30am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
44554,Automatic rank assignment in init_process_group,2019-05-06T23:56:21.644Z,0,168,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to setup distributed training and encountered some problems with initialization of process group.<br/><NewLine>Since I have a shared file-system between nodes, I chose initialization with <code>file://</code>. But I got this error:</p><NewLine><blockquote><NewLine><p>ValueError: Error initializing torch.distributed using file:// rendezvous: rank parameter missing</p><NewLine></blockquote><NewLine><p>Then I found in documentation, that “automatic rank assignment is not supported anymore”, although  documentation for <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group"" rel=""nofollow noopener"">init_process_group</a> imply otherwise.</p><NewLine><p>Is there a way to not tell <code>init_process_group</code> rank explicitly. And what is the point of <code>init_process_group</code> if I have to pass rank explicitly.</p><NewLine></div>",https://discuss.pytorch.org/u/starphoenix,(Alexander V.),starphoenix,"May 6, 2019, 11:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Good point.</p><NewLine><p>This used to be possible and was not reinstated when we moved to c10d for PyTorch 1.0. I created an issue on GitHub to bring this functionality back: <a href=""https://github.com/pytorch/pytorch/issues/22128"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/22128</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  9:27am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
44526,Parallel processing samples that can&rsquo;t be orgnized as batches,2019-05-06T15:09:10.643Z,0,126,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a task that every sample have different sizes (or different modules of network to forward), so I can’t put them in batches. But train the samples one by one is very inefficient. How can I paralleling the process?</p><NewLine><p>I think <code>torch.multiprocessing</code> might be one solution. But I’m still not sure how to use it after reading the docs.</p><NewLine></div>",https://discuss.pytorch.org/u/zhangyuygss,(张宇),zhangyuygss,"May 7, 2019,  5:07am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>A common approach is to pad the inputs to the biggest shape. You still have to make sure that your model works well with padded inputs of course.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  9:01am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
45511,Transfer data to GPU doubled in distributed training,2019-05-17T14:24:28.106Z,0,239,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I test execution time for these two lines in <a href=""https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/engine/trainer.py#L64"" rel=""nofollow noopener"">engine/trainer.py#L64</a>. I called it as todevice time</p><NewLine><pre><code class=""lang-python"">        images = images.to(device)<NewLine>        targets = [target.to(device) for target in targets]<NewLine></code></pre><NewLine><p>I used 2 nodes, each node has 8 GPUs. Each GPU processes 2 images. I run cammand on first host (second host just replace with --node_rank=1):</p><NewLine><pre><code class=""lang-auto"">export NGPUS=8<NewLine>python -m torch.distributed.launch --nproc_per_node=$NGPUS \<NewLine>--nnodes=2 --node_rank=0 --master_addr=""172.17.61.2"" --master_port=22876 \<NewLine>tools/train_net.py --config-file ""configs/e2e_faster_rcnn_R_50_FPN_1x.yaml"" MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN 2000 SOLVER.IMS_PER_BATCH 32 SOLVER.BASE_LR 0.04 SOLVER.STEPS ""(30000, 40000)"" SOLVER.MAX_ITER 50000 TEST.IMS_PER_BATCH 16 OUTPUT_DIR models/tmp-2n8g<NewLine></code></pre><NewLine><p>todevice time in 2 nodes(each 8GPUs) is doubled compared to 1 node 8GPUs. I also test other time such as data_time, backbone time, rpn time, backward time, step() update params time. All these time is so closed to one node with 8 GPUs.</p><NewLine><p>I also test 2 nodes with 16GPUs on each. It’s the same that todevice time is twice than one node with 16GPUs, each GPU processes 2 images.</p><NewLine><p>I am very confused. Each gpu processes the same number of images in both 2 situations. But time increased in distributed mode.</p><NewLine></div>",https://discuss.pytorch.org/u/Antsypc,(Pengcheng Young),Antsypc,"May 19, 2019,  5:12am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>That is weird indeed. Can you isolate the problem to data loading (i.e. don’t train a model, just iterate over the data set)? Due to asynchronous nature of CUDA, wall clock time that attribute to data transfer is in fact caused by asynchronous execution of for example autograd, the optimizer, etc.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  7:34am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
45184,Is my code the correct way using DistributedDataParallel in single node multi GPUs?,2019-05-14T06:19:41.794Z,0,2170,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I use <strong>torch.nn.parallel.DistributedDataParallel</strong> API in PyTorch1.1 to spawn my multi-card model (2 GPUs) to 8 GPUs.  According to official tutorial <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"">GETTING STARTED WITH DISTRIBUTED DATA PARALLEL</a>, DistributedDataParallel is the recommanded way to parallel one’s model. I am not confident about my implementation and I can’t find other valuable tutorials, so come here for help.</p><NewLine><p>My ideas are simply as follow:</p><NewLine><ol><NewLine><li>split my 3D CNN model into 2 GPUs (simply called dev_in and dev_out),</li><NewLine><li>use DistributedDataParallel() to spawn my 2-GPUs model to 4 <strong>Processes</strong>, each model replica using same random seed to initialize weights, and each <strong>Process</strong> don’t share GPUs with other <strong>Processes</strong>.</li><NewLine><li>wrap my dataset with Dataset() and DataLoader() api, and manually separate one batch’s data equally in batch-dim, so each <strong>Process</strong> (with 2 GPUs) will process different data with <strong>SAME weights</strong>.</li><NewLine><li>after forward propagate in each <strong>Process</strong>, collect loss value in each <strong>Process</strong> and average them, then using this averaged loss value to get gradients and update <strong>All 4 models in 4 Processes</strong>,</li><NewLine><li>after each epoch of training and validation, calculate ACC and AUC scores for training dataset and validation dataset respectively.</li><NewLine><li>after one epoch of training dataset, using validation dataset to validate my my model, <strong>currently I use ONE PROCESS model (before warpped by DistributedDataParallel() API to start my validation, because there is something wrong I can’t deal with when I used model after DistributedDataParallel())</strong><NewLine></li><NewLine></ol><NewLine><p>Currently, here is my code related:</p><NewLine><pre><code class=""lang-auto""># sample/train.py<NewLine><NewLine>import tempfile<NewLine>import torch.distributed as dist<NewLine>import torch.nn as nn<NewLine>from torch import optim<NewLine>import torch.multiprocessing as mp<NewLine>from torch.nn.parallel import DistributedDataParallel<NewLine>from torch.distributed import Backend<NewLine>from time import time<NewLine>from sample.networks.XXXXNet import XXXXNet<NewLine>from sample.data import XXXXDataSet<NewLine>from torch.utils.data import DataLoader<NewLine>import yaml<NewLine>import torch<NewLine>import os<NewLine>from yaml import CLoader as Loader<NewLine>from sklearn.metrics import accuracy_score, auc, roc_curve<NewLine>import numpy as np<NewLine>from torch.utils.tensorboard import SummaryWriter<NewLine><NewLine>cfg = yaml.load(<NewLine>    open(os.path.join(os.path.abspath(<NewLine>        os.path.join(os.path.dirname(__file__), ""../config/config.yml"")))),<NewLine>    Loader=Loader<NewLine>)[""DATASET""][0]<NewLine><NewLine>np.random.seed(cfg[""SEED""])<NewLine>torch.random.manual_seed(cfg[""SEED""])<NewLine>tempfile.tempdir = os.path.abspath(""~/tmp"")<NewLine>NAME = ""%dGPUs_1e-6"" % cfg[""WORLD_SIZE""]<NewLine>writer = SummaryWriter(log_dir=os.path.join(os.path.dirname(__file__), ""logs/tb_logs/%s"" % NAME))<NewLine><NewLine><NewLine>def setup_env(rank, world_size):<NewLine>    """"""<NewLine>    Initialize the distributed environment.<NewLine><NewLine>    :param rank: Rank of the current process.<NewLine>    :param world_size: Number of processes participating in the job.<NewLine>    :return:<NewLine>    """"""<NewLine>    assert isinstance(world_size, int) and world_size &gt; 0<NewLine>    assert isinstance(rank, int) and 0 &lt;= rank &lt; world_size<NewLine><NewLine>    os.environ['MASTER_ADDR'] = cfg[""MASTER_ADDR""]<NewLine>    os.environ['MASTER_PORT'] = cfg[""MASTER_PORT""]<NewLine><NewLine>    # Initialize the process group<NewLine>    dist.init_process_group(Backend.NCCL, rank=rank, world_size=world_size)<NewLine><NewLine>    # Explicitly setting seed to make sure that models created in two processes<NewLine>    # start from same random weights and biases.<NewLine>    torch.manual_seed(cfg[""SEED""])<NewLine><NewLine><NewLine>def cleanup_env():<NewLine>    """"""<NewLine>    Destroy the default process group.<NewLine><NewLine>    :return:<NewLine>    """"""<NewLine>    dist.destroy_process_group()<NewLine><NewLine><NewLine>def train_model(rank, world_size, offset=1, ):<NewLine>    """"""<NewLine>    Training model.<NewLine><NewLine>    :param rank: Rank of the current process.<NewLine>    :param world_size: The number of processes in the current process group.<NewLine>    :param offset: The index of first GPU to use.<NewLine>    :return:<NewLine>    """"""<NewLine>    assert isinstance(world_size, int) and world_size &gt; 0<NewLine>    assert isinstance(rank, int) and 0 &lt;= rank &lt; world_size<NewLine>    assert isinstance(offset, int) and offset &gt;= 0<NewLine><NewLine>    setup_env(rank, world_size)<NewLine><NewLine>    # Setup mp_model and devices for this process<NewLine>    dev_in = rank * 2 + offset<NewLine>    dev_out = rank * 2 + 1 + offset<NewLine>    mp_model = XXXXNet(dev_in=dev_in, dev_out=dev_out)<NewLine>    ddp_mp_model = DistributedDataParallel(mp_model)<NewLine><NewLine>    loss_fn = nn.CrossEntropyLoss()<NewLine>    optimizer = optim.Adam(<NewLine>        ddp_mp_model.parameters(),<NewLine>        lr=float(cfg[""LEARNING_RATE""]) * world_size,<NewLine>        weight_decay=float(cfg[""L2""]),<NewLine>    )<NewLine>    old_lr = float(cfg[""LEARNING_RATE""]) * world_size<NewLine>    batch_size = world_size * cfg[""BATCH_SIZE_PER_CARD""]<NewLine><NewLine>    # Training dataset<NewLine>    dataset_train = XXXXDataSet(val=False, shape=cfg[""CUBE_SIZE""][1:])<NewLine>    data_loader_train = DataLoader(dataset_train, batch_size=batch_size, num_workers=cfg[""NUM_WORKERS""])<NewLine>    # Validation dataset<NewLine>    dataset_val = XXXXDataSet(val=True, shape=cfg[""CUBE_SIZE""][1:])<NewLine>    data_loader_val = DataLoader(dataset_val, batch_size=cfg[""BATCH_SIZE_PER_CARD""], num_workers=cfg[""NUM_WORKERS""])<NewLine><NewLine>    with open(os.path.join(os.path.dirname(__file__), ""logs/"" + NAME + "".log""), ""w"") as log_file:<NewLine>        def _print(string, file=log_file, target_rank=0):<NewLine>            """"""<NewLine>            Print to cmd and log file simultaneously.<NewLine><NewLine>            :param string: Content need to print.<NewLine>            :param file: Log file object.<NewLine>            :return:<NewLine>            """"""<NewLine>            if target_rank == -1:<NewLine>                print(string, file=file)<NewLine>                print(string)<NewLine>                file.flush()<NewLine>            elif target_rank == rank:<NewLine>                print(string, file=file)<NewLine>                print(string)<NewLine>                file.flush()<NewLine><NewLine>        _print(str(cfg))<NewLine>        no_optim = 0<NewLine>        total_epoch = cfg[""EPOCHS""]<NewLine>        epoch_best_loss_train = 100.<NewLine>        epoch_best_loss_val = 100.<NewLine><NewLine>        for epoch in range(1, total_epoch + 1):<NewLine>            # ==TRAINING====TRAINING====TRAINING====TRAINING====TRAINING====TRAINING==<NewLine>            tic_train = time()<NewLine>            # =====ACC&amp;AUC start=====<NewLine>            prods_train, gts_train = [], []<NewLine>            # ======ACC&amp;AUC end======<NewLine>            data_loader_iter_train = iter(data_loader_train)<NewLine>            train_epoch_loss = 0<NewLine>            for img, label in data_loader_iter_train:<NewLine>                inp = img[rank * cfg[""BATCH_SIZE_PER_CARD""]: (rank + 1) * cfg[""BATCH_SIZE_PER_CARD""]]<NewLine>                label = label[rank * cfg[""BATCH_SIZE_PER_CARD""]: (rank + 1) * cfg[""BATCH_SIZE_PER_CARD""]].to(dev_out)<NewLine>                # Calculate loss<NewLine>                if inp.size()[0] &lt; 2:<NewLine>                    _print(""inp is None!!!!!!!!!!!!"", target_rank=-1)<NewLine>                    train_loss = torch.tensor(0.)<NewLine>                else:<NewLine>                    optimizer.zero_grad()<NewLine>                    pred = ddp_mp_model(inp)<NewLine>                    train_loss = loss_fn(pred, label)<NewLine>                train_loss_lst = [torch.zeros_like(train_loss)] * world_size<NewLine>                prods_train_lst = [torch.zeros_like(pred)] * world_size<NewLine>                label_train_lst = [torch.zeros_like(label)] * world_size<NewLine><NewLine>                dist.all_gather(prods_train_lst, pred)  # Sync between all processes<NewLine>                dist.all_gather(label_train_lst, label)  # Sync between all processes<NewLine>                dist.all_gather(train_loss_lst, train_loss)  # Sync between all processes<NewLine>                dist.all_reduce(train_loss, op=dist.ReduceOp.SUM)  # Sync between all processes<NewLine>                train_loss /= torch.tensor(train_loss_lst).nonzero().size(0)<NewLine>                # Backward propagate and update weights<NewLine>                train_loss.backward()<NewLine>                optimizer.step()<NewLine>                train_epoch_loss += train_loss.item()<NewLine><NewLine>                # =====ACC&amp;AUC start=====<NewLine>                prods_train.append(torch.cat(prods_train_lst, dim=0).cpu().detach().numpy())<NewLine>                gts_train.append(torch.cat(label_train_lst, dim=0).cpu().numpy())<NewLine><NewLine>            prods_train = np.concatenate(tuple(prods_train))<NewLine>            gts_train = np.concatenate(tuple(gts_train))<NewLine>            prods_train = prods_train[:, 1]<NewLine>            prods_01 = np.where(prods_train &gt; 0.5, 1, 0)  # Turn probability to 0-1 binary output<NewLine>            acc_NN = accuracy_score(gts_train, prods_01)<NewLine>            false_positive_rate, recall, thresholds = roc_curve(gts_train, prods_train, pos_label=1)<NewLine>            roc_auc = auc(false_positive_rate, recall)<NewLine>            # ======ACC&amp;AUC end======<NewLine>            train_epoch_loss /= len(data_loader_iter_train)<NewLine>            _print(""******************************"")<NewLine>            _print(""epoch[%03d/%03d], time: %02dm:%02ds"" %<NewLine>                   (epoch, cfg[""EPOCHS""], int(time() - tic_train) // 60, int(time() - tic_train) % 60))<NewLine>            _print(""train loss = %6.4f"" % train_epoch_loss)<NewLine>            _print(""CUBE_SIZE: %s"" % str(cfg[""CUBE_SIZE""]))<NewLine>            _print(""ACC = %6.4f, AUC = %6.4f"" % (acc_NN, roc_auc))<NewLine><NewLine>            # ==Validation====Validation====Validation====Validation====Validation====Validation==<NewLine>            _print(""------------------------------"")<NewLine>            mp_model.eval()<NewLine>            tic_val = time()<NewLine>            # =====code for ACC&amp;AUC start=====<NewLine>            prods_val = []<NewLine>            gts_val = []<NewLine>            # ======code for ACC&amp;AUC end======<NewLine>            data_loader_iter_val = iter(data_loader_val)<NewLine>            val_epoch_loss = 0<NewLine>            with torch.no_grad():<NewLine>                for val_img, val_label in data_loader_iter_val:<NewLine>                    val_label = val_label.to(dev_out)<NewLine>                    # Calculate predicts and loss<NewLine>                    val_pred = ddp_mp_model(val_img)<NewLine>                    val_loss = loss_fn(val_pred, val_label)<NewLine>                    val_epoch_loss += val_loss.item()<NewLine>                    # =====code for ACC&amp;AUC start=====<NewLine>                    val_pred = val_pred.cpu().detach().numpy()<NewLine>                    val_label = val_label.cpu().numpy()<NewLine>                    prods_val.append(val_pred)<NewLine>                    gts_val.append(val_label)<NewLine>            prods_val = np.concatenate(tuple(prods_val))<NewLine>            gts_val = np.concatenate(tuple(gts_val))<NewLine><NewLine>            prods_val = prods_val[:, 1]<NewLine>            prods_01_val = np.where(prods_val &gt; 0.5, 1, 0)  # Turn probability to 0-1 binary output<NewLine>            acc_NN_val = accuracy_score(gts_val, prods_01_val)<NewLine>            false_positive_rate_val, recall_val, thresholds_val = roc_curve(gts_val, prods_val, pos_label=1)<NewLine>            roc_auc_val = auc(false_positive_rate_val, recall_val)<NewLine>            # ======code for ACC&amp;AUC end======<NewLine>            val_epoch_loss /= len(data_loader_iter_val)<NewLine>            _print(""validation time: %02dm:%02ds"" % (int(time() - tic_val) // 60, int(time() - tic_val) % 60))<NewLine>            _print(""validation loss = %6.4f"" % val_epoch_loss)<NewLine>            _print(""validation ACC = %6.4f, validation AUC = %6.4f"" % (acc_NN_val, roc_auc_val))<NewLine>            if rank == 0:<NewLine>                writer.add_scalars(main_tag=""lr"", tag_scalar_dict={""train"": old_lr}, global_step=epoch)<NewLine>                writer.add_scalars(main_tag=""time"",<NewLine>                                   tag_scalar_dict={""train"": time() - tic_train,<NewLine>                                                    ""val"": time() - tic_val}, global_step=epoch)<NewLine>                writer.add_scalars(main_tag=""loss"",<NewLine>                                   tag_scalar_dict={""train"": train_epoch_loss,<NewLine>                                                    ""val"": val_epoch_loss}, global_step=epoch)<NewLine>                writer.add_scalars(main_tag=""ACC"",<NewLine>                                   tag_scalar_dict={""train"": acc_NN,<NewLine>                                                    ""val"": acc_NN_val}, global_step=epoch)<NewLine>                writer.add_scalars(main_tag=""AUC"",<NewLine>                                   tag_scalar_dict={""train"": roc_auc,<NewLine>                                                    ""val"": roc_auc_val}, global_step=epoch)<NewLine>            mp_model.train()<NewLine>            # ==Validation End====Validation End====Validation End====Validation End====Validation End==<NewLine><NewLine>            if train_epoch_loss &gt;= epoch_best_loss_train:<NewLine>                no_optim += 1<NewLine>            else:<NewLine>                no_optim = 0<NewLine>                epoch_best_loss_train = train_epoch_loss<NewLine>                torch.save(ddp_mp_model.state_dict(),<NewLine>                           os.path.join(os.path.dirname(__file__), ""weights/"" + NAME + "".th""))<NewLine>            if no_optim &gt; 6:<NewLine>                _print(""early stop at [%03d] epoch"" % epoch)<NewLine>                break<NewLine>            if no_optim &gt; 3:<NewLine>                if old_lr &lt; 5e-7:<NewLine>                    break<NewLine>                ddp_mp_model.load_state_dict(torch.load(<NewLine>                    os.path.join(os.path.dirname(__file__), ""weights/"" + NAME + "".th"")))<NewLine>                new_lr = old_lr / 5.0<NewLine>                for param_group in optimizer.param_groups:<NewLine>                    param_group['lr'] = new_lr<NewLine>                _print(""update learning rate: %f -&gt; %f"" % (old_lr, new_lr))<NewLine>                old_lr = new_lr<NewLine>            _print(""******************************"")<NewLine>        _print(""Finish!"")<NewLine><NewLine>    cleanup_env()<NewLine><NewLine><NewLine>def ddp_train(demo_fn, world_size):<NewLine>    """"""<NewLine><NewLine>    :param demo_fn: Function.<NewLine>    :param world_size: The number of processes in the current process group.<NewLine>    :return:<NewLine>    """"""<NewLine>    mp.spawn(demo_fn,<NewLine>             args=(world_size,),<NewLine>             nprocs=world_size,<NewLine>             join=True)<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    ddp_train(<NewLine>        train_model,<NewLine>        world_size=cfg[""WORLD_SIZE""],<NewLine>    )<NewLine>    writer.close()<NewLine></code></pre><NewLine><p>And here is another .py file</p><NewLine><pre><code class=""lang-auto""># sample/networks/XXXXNet.py<NewLine><NewLine>import yaml<NewLine>from yaml import CLoader as Loader<NewLine>import torch.nn.functional as F<NewLine>import os<NewLine>import torch<NewLine>import torch.distributed as dist<NewLine>import torch.nn as nn<NewLine>from torch.distributed import Backend<NewLine><NewLine>cfg = yaml.load(<NewLine>    open(os.path.join(os.path.abspath(<NewLine>        os.path.join(os.path.dirname(__file__), ""../../config/config.yml"")))),<NewLine>    Loader=Loader<NewLine>)[""DATASET""][0]<NewLine><NewLine>non_linearity = nn.LeakyReLU<NewLine><NewLine><NewLine>class FireModule3D(nn.Module):<NewLine>    """"""<NewLine>    FireModule3D module<NewLine><NewLine>    (Tested 5.10)<NewLine>    """"""<NewLine><NewLine>    def __init__(self, in_channels, out_channels, kernel_size=3,<NewLine>                 dilation=1, bias=False,<NewLine>                 squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                 use_bn=True, momentum=0.1, use_dp=False, use_bypass=True):<NewLine>        """"""<NewLine>        Init function<NewLine><NewLine>        :param in_channels: Number of input channels.<NewLine>        :param out_channels: Number of output channels.<NewLine>        :param kernel_size: Kernel size.<NewLine>        :param dilation: Dilation rate of dilated convolution.<NewLine>        :param bias: Whether to use bias.<NewLine>        :param squeeze_ratio: Squeeze ratio of Fire Module.<NewLine>        :param pct_3x3: Percent of 3x3 convolution in expand layer.<NewLine>        :param activation: Activation function.<NewLine>        :param use_bn: Whether to use batch normalization.<NewLine>        :param momentum: The value used for the running_mean and running_var computation.<NewLine>        :param use_dp: Whether to use dropout.<NewLine>        :param use_bypass: Whether to use bypass connection.<NewLine>        """"""<NewLine>        super(FireModule3D, self).__init__()<NewLine>        self.use_bn = use_bn<NewLine>        self.use_dp = use_dp<NewLine>        self.use_bypass = use_bypass<NewLine><NewLine>        e_i = out_channels<NewLine>        s_1x1 = int(squeeze_ratio * e_i)  # number of channels in squeeze 1x1 layer<NewLine>        e_3x3 = int(pct_3x3 * e_i)  # number of channels in expand 3x3 layer<NewLine>        e_1x1 = e_i - e_3x3<NewLine><NewLine>        self.activation = activation(inplace=True)<NewLine>        self.squeeze1x1 = nn.Conv3d(in_channels=in_channels, out_channels=s_1x1,<NewLine>                                    kernel_size=1, dilation=1, groups=1, bias=bias)<NewLine>        self.expand1x1 = nn.Conv3d(in_channels=s_1x1, out_channels=e_1x1,<NewLine>                                   kernel_size=1, dilation=1, groups=1, bias=bias)<NewLine>        self.expand3x3 = nn.Conv3d(in_channels=s_1x1, out_channels=e_3x3, kernel_size=kernel_size,<NewLine>                                   padding=1, dilation=dilation, bias=bias)<NewLine><NewLine>        # Bypass connection<NewLine>        if self.use_bypass:<NewLine>            if in_channels != out_channels:<NewLine>                self.bypass = nn.Conv3d(in_channels=in_channels, out_channels=out_channels,<NewLine>                                        kernel_size=1, bias=bias)<NewLine>            else:<NewLine>                self.bypass = None<NewLine><NewLine>        if self.use_bn:<NewLine>            self.bn_s1x1 = nn.BatchNorm3d(num_features=s_1x1, momentum=momentum)<NewLine>            self.bn_e1x1 = nn.BatchNorm3d(num_features=e_1x1, momentum=momentum)<NewLine>            self.bn_e3x3 = nn.BatchNorm3d(num_features=e_3x3, momentum=momentum)<NewLine>        if self.use_dp:<NewLine>            self.dp = nn.Dropout2d(0.5)<NewLine><NewLine>    def forward(self, x):<NewLine>        """"""<NewLine>        Forward computation function.<NewLine><NewLine>        :param x: Input tensor.<NewLine>        :return: Result tensor.<NewLine>        """"""<NewLine>        # Squeeze 1x1 layer<NewLine>        squeeze = self.squeeze1x1(x)<NewLine>        if self.use_bn:<NewLine>            squeeze = self.bn_s1x1(squeeze)<NewLine>        squeeze = self.activation(squeeze)<NewLine><NewLine>        # Expand 1x1 layer<NewLine>        expand1x1 = self.expand1x1(squeeze)<NewLine>        if self.use_dp:<NewLine>            expand1x1 = self.dp(expand1x1)<NewLine>        if self.use_bn:<NewLine>            expand1x1 = self.bn_e1x1(expand1x1)<NewLine><NewLine>        # Expand 3x3 layer<NewLine>        expand3x3 = self.expand3x3(squeeze)<NewLine>        if self.use_dp:<NewLine>            expand3x3 = self.dp(expand3x3)<NewLine>        if self.use_bn:<NewLine>            expand3x3 = self.bn_e3x3(expand3x3)<NewLine><NewLine>        merge = self.activation(torch.cat([expand1x1, expand3x3], dim=1))<NewLine><NewLine>        if self.use_bypass:  # Bypass connection<NewLine>            if self.bypass is not None:<NewLine>                x = self.bypass(x)<NewLine>            merge = merge + x<NewLine>        return merge<NewLine><NewLine><NewLine>class XXXXNet(nn.Module):<NewLine>    def __init__(self, nb_class=2, dev_in=None, dev_out=None):<NewLine>        super(XXXXNet, self).__init__()<NewLine>        self.device1 = dev_in<NewLine>        self.device2 = dev_out<NewLine><NewLine>        self.conv0 = nn.Sequential(nn.Conv3d(1, 8, kernel_size=7, stride=2, padding=3, bias=False),<NewLine>                                   non_linearity(inplace=True)).to(self.device1)<NewLine>        self.conv1 = FireModule3D(in_channels=8, out_channels=8, kernel_size=3,<NewLine>                                  dilation=1, bias=False,<NewLine>                                  squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                  use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device1)<NewLine>        self.conv2 = FireModule3D(in_channels=8, out_channels=8, kernel_size=3,<NewLine>                                  dilation=1, bias=False,<NewLine>                                  squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                  use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device1)<NewLine>        self.conv3 = FireModule3D(in_channels=8, out_channels=8, kernel_size=3,<NewLine>                                  dilation=1, bias=False,<NewLine>                                  squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                  use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device1)<NewLine>        self.conv4 = FireModule3D(in_channels=8, out_channels=16, kernel_size=3,<NewLine>                                  dilation=1, bias=False,<NewLine>                                  squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                  use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device2)<NewLine>        self.mp1 = nn.MaxPool3d(kernel_size=2, stride=2).to(self.device2)<NewLine><NewLine>        self.conv5 = FireModule3D(in_channels=16, out_channels=16, kernel_size=3,<NewLine>                                  dilation=1, bias=False,<NewLine>                                  squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                  use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device2)<NewLine>        self.conv6 = FireModule3D(in_channels=16, out_channels=16, kernel_size=3,<NewLine>                                  dilation=1, bias=False,<NewLine>                                  squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                  use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device2)<NewLine>        self.conv7 = FireModule3D(in_channels=16, out_channels=16, kernel_size=3,<NewLine>                                  dilation=1, bias=False,<NewLine>                                  squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                  use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device2)<NewLine>        self.conv8 = FireModule3D(in_channels=16, out_channels=32, kernel_size=3,<NewLine>                                  dilation=1, bias=False,<NewLine>                                  squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                  use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device2)<NewLine>        self.mp2 = nn.MaxPool3d(kernel_size=2, stride=2).to(self.device2)<NewLine><NewLine>        self.conv9 = FireModule3D(in_channels=32, out_channels=32, kernel_size=3,<NewLine>                                  dilation=1, bias=False,<NewLine>                                  squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                  use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device2)<NewLine>        self.conv10 = FireModule3D(in_channels=32, out_channels=32, kernel_size=3,<NewLine>                                   dilation=1, bias=False,<NewLine>                                   squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                   use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device2)<NewLine>        self.conv11 = FireModule3D(in_channels=32, out_channels=32, kernel_size=3,<NewLine>                                   dilation=1, bias=False,<NewLine>                                   squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                   use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device2)<NewLine>        self.conv12 = FireModule3D(in_channels=32, out_channels=64, kernel_size=3,<NewLine>                                   dilation=1, bias=False,<NewLine>                                   squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                   use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device2)<NewLine>        self.mp3 = nn.MaxPool3d(kernel_size=2, stride=2).to(self.device2)<NewLine><NewLine>        self.conv13 = FireModule3D(in_channels=64, out_channels=64, kernel_size=3,<NewLine>                                   dilation=1, bias=False,<NewLine>                                   squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                   use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device2)<NewLine>        self.conv14 = FireModule3D(in_channels=64, out_channels=64, kernel_size=3,<NewLine>                                   dilation=1, bias=False,<NewLine>                                   squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                   use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device2)<NewLine>        self.conv15 = FireModule3D(in_channels=64, out_channels=64, kernel_size=3,<NewLine>                                   dilation=1, bias=False,<NewLine>                                   squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                   use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device2)<NewLine>        self.conv16 = FireModule3D(in_channels=64, out_channels=128, kernel_size=3,<NewLine>                                   dilation=1, bias=False,<NewLine>                                   squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                   use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device2)<NewLine>        self.mp4 = nn.MaxPool3d(kernel_size=2, stride=2).to(self.device2)<NewLine><NewLine>        self.conv17 = FireModule3D(in_channels=128, out_channels=128, kernel_size=3,<NewLine>                                   dilation=1, bias=False,<NewLine>                                   squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                   use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device2)<NewLine>        self.conv18 = FireModule3D(in_channels=128, out_channels=128, kernel_size=3,<NewLine>                                   dilation=1, bias=False,<NewLine>                                   squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                   use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device2)<NewLine>        self.conv19 = FireModule3D(in_channels=128, out_channels=128, kernel_size=3,<NewLine>                                   dilation=1, bias=False,<NewLine>                                   squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                   use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device2)<NewLine>        self.conv20 = FireModule3D(in_channels=128, out_channels=256, kernel_size=3,<NewLine>                                   dilation=1, bias=False,<NewLine>                                   squeeze_ratio=0.125, pct_3x3=0.5, activation=non_linearity,<NewLine>                                   use_bn=True, momentum=0.1, use_dp=False, use_bypass=True).to(self.device2)<NewLine>        self.mp5 = nn.MaxPool3d(kernel_size=2, stride=2).to(self.device2)<NewLine><NewLine>        self.fc1 = nn.Sequential(<NewLine>            nn.Linear(256 * 7 * 3 * 5, 256, bias=False),<NewLine>            non_linearity(inplace=True),<NewLine>            nn.Dropout2d(p=0.5),<NewLine>        ).to(self.device2)<NewLine>        self.fc3 = nn.Linear(256, nb_class, bias=False).to(self.device2)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = x.to(self.device1)<NewLine>        x = self.conv0(x)<NewLine><NewLine>        x = self.conv3(self.conv2(self.conv1(x)))<NewLine>        x = x.to(self.device2)<NewLine>        x = self.mp1(self.conv4(x))<NewLine><NewLine>        x = self.mp2(self.conv8(self.conv7(self.conv6(self.conv5(x)))))<NewLine>        x = self.mp3(self.conv12(self.conv11(self.conv10(self.conv9(x)))))<NewLine>        x = self.mp4(self.conv16(self.conv15(self.conv14(self.conv13(x)))))<NewLine>        x = self.mp5(self.conv20(self.conv19(self.conv18(self.conv17(x)))))<NewLine><NewLine>        # flatten<NewLine>        x = torch.flatten(x, start_dim=1)<NewLine>        x = self.fc1(x)<NewLine>        x = F.softmax(self.fc3(x), dim=1)  # .squeeze().contiguous()<NewLine>        return x<NewLine></code></pre><NewLine><p>And my config/config.yml file is</p><NewLine><pre><code class=""lang-auto"">DATASET:<NewLine>  - NAME: ""XXXX""<NewLine>    ROOT: ""/home/aaa/organized_data""# ""/Users/aaa/fsdownload"" #<NewLine>    IMAGE_FOLDER: ""raw_scans""<NewLine>    NPY_FOLDER: ""pre_result""<NewLine>    NPY_FOLDER2: ""my_npy"" #<NewLine>    TRAIN_CSV: ""train.csv""<NewLine>    VAL_CSV: ""val.csv""<NewLine>    SEED: 1<NewLine>    BATCH_SIZE_PER_CARD: 2<NewLine>    NUM_WORKERS: 4<NewLine>    MOMENTUM: 0.01<NewLine>    LEARNING_RATE: 1e-6<NewLine>    NUM_CLASSES: 2<NewLine>    WORLD_SIZE: 4<NewLine>    CUBE_SIZE: [1, 450, 220, 325] #(C, D, H, W)<NewLine>    EPOCHS: 100<NewLine>    MASTER_ADDR: ""localhost""<NewLine>    MASTER_PORT: ""12355""<NewLine>    VIS_PORT: 8097<NewLine>    L2: 5e-3<NewLine></code></pre><NewLine><p>In my case:<br/><NewLine>Process0 using GPU1 and GPU2<br/><NewLine>Process1 using GPU3 and GPU4<br/><NewLine>Process2 using GPU5 and GPU6<br/><NewLine>Process3 using GPU7 and GPU8<br/><NewLine>my server has 10 GPUs and I didn’t use GPU0 and 9.</p><NewLine><p>When I monitored the running process of the program using nvidia-smi, I found that <strong>GPU 2, 4, 6, 8 are often unable to complete tasks at the same time, and the GPUs that completed calculation first would wait for the straggler, so my GPU overall usage is low.</strong></p><NewLine><p>I think there are a lot of things in my code that can be improved, so where should I start optimizing my code? Looking forward to any suggestions.<img alt="":grinning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/grinning.png?v=9"" title="":grinning:""/></p><NewLine></div>",https://discuss.pytorch.org/u/StuChen,,StuChen,"May 14, 2019,  6:55am",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You don’t need to average the loss before calling <code>loss.backward()</code>. The gradients that are computed on each process are reduced across processes, and upon returning from <code>loss.backward()</code> each process has identical gradients for their model parameters.</p><NewLine><p>Regarding the utilization, check out <a href=""https://torchgpipe.readthedocs.io/en/latest/"" rel=""nofollow noopener"">torchgpipe</a>. It might be useful here.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  7:30am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
45618,How to calculate meters in Pytorch1.1 &amp; DistributedDataParallel()?,2019-05-19T07:36:11.714Z,0,259,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to use model parallel and data parallel at the same time, and have read many docs and tutorials from official website.<br/><NewLine>One confusing problem I faced is how to collect all kinds of meter values in each Process?</p><NewLine><p><strong>Question1</strong>: In the <a href=""https://github.com/pytorch/examples/blob/5b1f45057dc14a5e2132b45233c258a1dc2a0aab/imagenet/main.py#L286"" rel=""nofollow noopener"">official tutorial</a>, they just record meters value in each Process.<br/><NewLine>But in my code, I print loss value in each process, they are different. So, I think the value of other meters are also different.<br/><NewLine><strong>Is that tutorial wrong?</strong> In my opinion, I think the right way should synchronize loss, acc and other meters first, then all processes maintain the same values, after that I just need to print meters information in one Process.</p><NewLine><p><strong>Question2</strong>: In the <a href=""https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html#initialize-model"" rel=""nofollow noopener"">official tutorial</a>, they say ‘the DistributedDataParallel module also handles the averaging of gradients across the world, so we do not have to explicitly average the gradients in the training step’.<br/><NewLine>But, because of question1, <strong>does the API actually work as what the tutorial said?</strong>  Because each of the processes has a different loss value, although they start from the same init weights, <strong>will model weights in each process be optimized in different directions?</strong></p><NewLine></div>",https://discuss.pytorch.org/u/StuChen,,StuChen,"May 20, 2019,  4:53am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/stuchen"">@StuChen</a>,</p><NewLine><ol><NewLine><li>The losses are different because different processes see different inputs with different labels and therefore produce different losses. If you’re looking for a global top1 or top5, you can use distributed primitives from <code>torch.distributed</code> to average them.</li><NewLine><li>The model in each process will be optimized in the same way, because after calling <code>loss.backward()</code> the resulting gradients are identical across processes. In combination with the initial weights being identical, the resulting weights after optimization are also identical.</li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  7:25am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
48006,Index_add_ doesn&rsquo;t work with DataParallel,2019-06-15T07:19:11.792Z,1,131,"<div class=""post"" itemprop=""articleBody""><NewLine><h2><NewLine><img alt="":bug:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/bug.png?v=9"" title="":bug:""/> Bug</h2><NewLine><p><code>index_add_</code> (and probably other similar indexing functions like <code>index_copy_</code>, Ps. Not tested) give wrong results when used inside a model which has been wrapped with <code>DataParallel</code>.<br/><NewLine>Even with <code>DataParallel</code> wrapped model, the forward function which may be using <code>index_add_</code> for some kind of calculations should work normally as in the case for single GPU.<br/><NewLine>Refer to the log attached which illustrates the problem.</p><NewLine><h2>To Reproduce</h2><NewLine><p>Steps to reproduce the behaviour:</p><NewLine><ol><NewLine><li>Run the below dummy code snippet.</li><NewLine><li>Use 2 GPUs for running(export CUDA_VISIBLE_DEVICES=0,1).</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine>idx = torch.arange(0,40, device=torch.device('cuda:0'), dtype=torch.long).reshape(4,10)<NewLine>print(""index:"", idx.shape)<NewLine><NewLine>emb = torch.arange(10,130, dtype=torch.int, device=torch.device('cuda:0')).reshape(4,10,3)<NewLine>print(""t"", emb.shape)<NewLine><NewLine>print(""\n"")<NewLine><NewLine>class Index_Add_Checker(torch.nn.Module):<NewLine>    def __init__(self, index, t):<NewLine>        super().__init__()<NewLine>        <NewLine>    def forward(self, index, t):<NewLine>        index.view(-1)<NewLine>        pooled = torch.zeros(40, 3, dtype=torch.int).cuda()<NewLine>        print(""index:"", index.shape)<NewLine>        print(""t:"", t.shape)<NewLine>        pooled.index_add_(0, index.view(-1), t.view(-1,3))<NewLine>        return pooled<NewLine><NewLine>model_dp = Index_Add_Checker(idx, emb)<NewLine>model_dp = torch.nn.DataParallel(model_dp).cuda()<NewLine><NewLine>ans_dp = model_dp(idx, emb)<NewLine>print(""ans_dp shape:"", ans_dp.shape)<NewLine>print(""ans_dp:"", ans_dp)<NewLine><NewLine>print(""\n=====================================================================\n"")<NewLine><NewLine>model_without_dp = Index_Add_Checker(idx, emb)<NewLine>ans = model_without_dp(idx, emb)<NewLine>print(""ans shape:"", ans.shape)<NewLine>print(""ans:"", ans)<NewLine></code></pre><NewLine><h2>Expected behaviour</h2><NewLine><p>Basically, the <code>ans</code> and <code>ans_dp</code> should be same, but <code>ans_dp</code> i.e ans in case of data parallel model doesn’t seem to be correct and something which is not expected out of <code>index_add_</code>.<br/><NewLine>This is probably happening because <code>DataParallel</code> splits the <code>index</code> and <code>t</code> along <code>batch_first=0</code> dimension. And when they are used for index_add_ the indices do not line up as expected and hence the problem.</p><NewLine><p>Output Log:</p><NewLine><pre><code class=""lang-auto"">index: torch.Size([4, 10])<NewLine>t torch.Size([4, 10, 3])<NewLine><NewLine><NewLine>index: torch.Size([2, 10])<NewLine>t: torch.Size([2, 10, 3])<NewLine>index: torch.Size([2, 10])<NewLine>t: torch.Size([2, 10, 3])<NewLine>ans_dp shape: torch.Size([80, 3])<NewLine>ans_dp: tensor([[ 10,  11,  12],<NewLine>        [ 13,  14,  15],<NewLine>        [ 16,  17,  18],<NewLine>        [ 19,  20,  21],<NewLine>        [ 22,  23,  24],<NewLine>        [ 25,  26,  27],<NewLine>        [ 28,  29,  30],<NewLine>        [ 31,  32,  33],<NewLine>        [ 34,  35,  36],<NewLine>        [ 37,  38,  39],<NewLine>        [ 40,  41,  42],<NewLine>        [ 43,  44,  45],<NewLine>        [ 46,  47,  48],<NewLine>        [ 49,  50,  51],<NewLine>        [ 52,  53,  54],<NewLine>        [ 55,  56,  57],<NewLine>        [ 58,  59,  60],<NewLine>        [ 61,  62,  63],<NewLine>        [ 64,  65,  66],<NewLine>        [ 67,  68,  69],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [  0,   0,   0],<NewLine>        [ 70,  71,  72],<NewLine>        [ 73,  74,  75],<NewLine>        [ 76,  77,  78],<NewLine>        [ 79,  80,  81],<NewLine>        [ 82,  83,  84],<NewLine>        [ 85,  86,  87],<NewLine>        [ 88,  89,  90],<NewLine>        [ 91,  92,  93],<NewLine>        [ 94,  95,  96],<NewLine>        [ 97,  98,  99],<NewLine>        [100, 101, 102],<NewLine>        [103, 104, 105],<NewLine>        [106, 107, 108],<NewLine>        [109, 110, 111],<NewLine>        [112, 113, 114],<NewLine>        [115, 116, 117],<NewLine>        [118, 119, 120],<NewLine>        [121, 122, 123],<NewLine>        [124, 125, 126],<NewLine>        [127, 128, 129]], device='cuda:0', dtype=torch.int32)<NewLine><NewLine>=====================================================================<NewLine><NewLine>index: torch.Size([4, 10])<NewLine>t: torch.Size([4, 10, 3])<NewLine>ans shape: torch.Size([40, 3])<NewLine>ans: tensor([[ 10,  11,  12],<NewLine>        [ 13,  14,  15],<NewLine>        [ 16,  17,  18],<NewLine>        [ 19,  20,  21],<NewLine>        [ 22,  23,  24],<NewLine>        [ 25,  26,  27],<NewLine>        [ 28,  29,  30],<NewLine>        [ 31,  32,  33],<NewLine>        [ 34,  35,  36],<NewLine>        [ 37,  38,  39],<NewLine>        [ 40,  41,  42],<NewLine>        [ 43,  44,  45],<NewLine>        [ 46,  47,  48],<NewLine>        [ 49,  50,  51],<NewLine>        [ 52,  53,  54],<NewLine>        [ 55,  56,  57],<NewLine>        [ 58,  59,  60],<NewLine>        [ 61,  62,  63],<NewLine>        [ 64,  65,  66],<NewLine>        [ 67,  68,  69],<NewLine>        [ 70,  71,  72],<NewLine>        [ 73,  74,  75],<NewLine>        [ 76,  77,  78],<NewLine>        [ 79,  80,  81],<NewLine>        [ 82,  83,  84],<NewLine>        [ 85,  86,  87],<NewLine>        [ 88,  89,  90],<NewLine>        [ 91,  92,  93],<NewLine>        [ 94,  95,  96],<NewLine>        [ 97,  98,  99],<NewLine>        [100, 101, 102],<NewLine>        [103, 104, 105],<NewLine>        [106, 107, 108],<NewLine>        [109, 110, 111],<NewLine>        [112, 113, 114],<NewLine>        [115, 116, 117],<NewLine>        [118, 119, 120],<NewLine>        [121, 122, 123],<NewLine>        [124, 125, 126],<NewLine>        [127, 128, 129]], device='cuda:0', dtype=torch.int32)<NewLine><NewLine></code></pre><NewLine><h2>Environment</h2><NewLine><pre><code class=""lang-auto"">PyTorch version: 1.1.0<NewLine>Is debug build: No<NewLine>CUDA used to build PyTorch: 9.0.176<NewLine><NewLine>OS: Ubuntu 16.04.3 LTS<NewLine>GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609<NewLine>CMake version: version 3.5.1<NewLine><NewLine>Python version: 3.6<NewLine>Is CUDA available: Yes<NewLine>CUDA runtime version: 7.5.17<NewLine>GPU models and configuration:<NewLine>GPU 0: GeForce GTX 1080 Ti<NewLine>GPU 1: GeForce GTX 1080 Ti<NewLine>GPU 2: GeForce GTX 1080 Ti<NewLine>GPU 3: GeForce GTX 1080 Ti<NewLine>GPU 4: GeForce GTX 1080 Ti<NewLine>GPU 5: GeForce GTX 1080 Ti<NewLine>GPU 6: GeForce GTX 1080 Ti<NewLine>GPU 7: GeForce GTX 1080 Ti<NewLine><NewLine>Nvidia driver version: 418.39<NewLine>cuDNN version: Probably one of the following:<NewLine>/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21<NewLine>/usr/lib/x86_64-linux-gnu/libcudnn.so.7.5.0<NewLine>/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6<NewLine>/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7<NewLine>/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudnn.so.7.0.5<NewLine><NewLine>Versions of relevant libraries:<NewLine>[pip3] numpy==1.14.0<NewLine>[pip3] numpydoc==0.7.0<NewLine>[pip3] torch==1.0.1.post2<NewLine>[pip3] torchvision==0.2.2.post3<NewLine>[conda] torch                     1.1.0                    pypi_0    pypi<NewLine>[conda] torch-cluster             1.3.0                    pypi_0    pypi<NewLine>[conda] torch-geometric           1.2.0                    pypi_0    pypi<NewLine>[conda] torch-scatter             1.2.0                    pypi_0    pypi<NewLine>[conda] torch-sparse              0.4.0                    pypi_0    pypi<NewLine>[conda] torch-spline-conv         1.1.0                    pypi_0    pypi<NewLine>[conda] torchvision               0.2.2.post3              pypi_0    pypi<NewLine>[conda] torchviz                  0.0.1                    pypi_0    pypi<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/gollum,(Anirudh Dagar),gollum,"June 15, 2019,  7:19am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is a cross-post of <a href=""https://github.com/pytorch/pytorch/issues/21810"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/21810</a>. If this is a proper issue please continue on GitHub.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! I made a post here because I didn’t get a reply on GitHub issue tracker. 'm new to PyTorch so just wanted to be sure if my post is actually right.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/gollum; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  5:30am; <NewLine> REPLY_DATE 2: June 24, 2019,  7:24am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
45497,Loading data by pinning and DataParallel not working,2019-05-17T11:37:08.670Z,1,496,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone,</p><NewLine><p>Could you guys have a look on my problem.</p><NewLine><p>I have two problems: data loading and DataParallel are not working.</p><NewLine><p>To train densenet121  on 4 GPUs (Tesla V100) I use DataParallel. The code I use for these tests is from <a href=""https://github.com/pytorch/examples/blob/master/imagenet/main.py#L68"" rel=""nofollow noopener"">here</a> as suggested  <a href=""https://discuss.pytorch.org/t/debugging-dataparallel-no-speedup-and-uneven-memory-allocation/1100/14"">here</a>. I just customised this code. I even tried with smaller batches as suggested in <a href=""https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html"" rel=""nofollow noopener"">this</a> tutorial.</p><NewLine><p>Here are my results for parallel training:</p><NewLine><ul><NewLine><li>with 4 GPUs, 80 batch and 10 epochs: 7m 24s</li><NewLine><li>with 1 GPU, 20 batch and 10 epochs: 5m 26s</li><NewLine></ul><NewLine><p>Concerning pinning the loaded data to CPU, there is also no changes. In DataLoader I set ‘pin_memory = True’ and in cuda ‘non_blocking = True’.</p><NewLine><p>Thank you for your precious considered time.</p><NewLine><p>Here is my code:</p><NewLine><pre><code class=""lang-auto"">#!/usr/bin/env python3<NewLine># -*- coding: utf-8 -*-<NewLine>""""""<NewLine>Created on Thu May 16 15:11:33 2019<NewLine><NewLine>""""""<NewLine>import os<NewLine>import shutil<NewLine>import time<NewLine><NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.parallel<NewLine>import torch.optim<NewLine>import torch.utils.data<NewLine>import torch.utils.data.distributed<NewLine>import torchvision.transforms as transforms<NewLine>import torchvision.datasets as datasets<NewLine>import torchvision.models as models<NewLine><NewLine><NewLine>def set_parameter_requires_grad(model, feature_extracting):<NewLine>    if feature_extracting:<NewLine>        for param in model.parameters():<NewLine>            param.requires_grad = False<NewLine><NewLine>def cuda_managment(device_count, text = str):<NewLine>    print('\n')<NewLine>    print(text)<NewLine>    for d in range(device_count):<NewLine>        #print('GPU {} allocated memory {}'.format(d, cuda.memory_allocated(d)/1e+9))<NewLine>        print('GPU {} cached memory {}'.format(d, torch.cuda.max_memory_cached(d)/1e+9))<NewLine>    print('\n')<NewLine>    <NewLine>     <NewLine>          <NewLine>def img_loader(train_or_test, transformers, batch_size, shuffle_data = True):<NewLine>    # create dataset<NewLine>    data = []<NewLine>    for t in transformers:<NewLine>        data.append(datasets.ImageFolder(os.path.join(data_dir, train_or_test), t))<NewLine>    data = torch.utils.data.ConcatDataset(data)<NewLine><NewLine>    # create dataloader<NewLine>    loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=shuffle_data, <NewLine>                                         num_workers=20,<NewLine>                                         pin_memory = True<NewLine>                                         )<NewLine>    <NewLine>    return loader<NewLine><NewLine><NewLine>def main_worker(num_classes, datadir, batch_size, train_loader, val_loader, <NewLine>                n_workers = 3, evaluate = False, epochs = 100, feature_extract = False):<NewLine>    <NewLine>    <NewLine>    since = time.time()<NewLine>    best_acc1 = 0.0<NewLine>    <NewLine>    # create model    <NewLine>    model = models.densenet121(pretrained=False)<NewLine>    set_parameter_requires_grad(model, feature_extract)<NewLine>    num_ftrs = model.classifier.in_features<NewLine>    model.classifier = nn.Linear(num_ftrs, num_classes)<NewLine>    <NewLine>    <NewLine>    #no parallel traing<NewLine>#    model = model.cuda(0)<NewLine>    <NewLine>    # parallel training<NewLine>    model = nn.DataParallel(model).cuda()<NewLine><NewLine>    # define loss function (criterion) and optimizer<NewLine>    criterion = nn.CrossEntropyLoss().cuda(0)<NewLine>    optimizer = torch.optim.SGD(model.parameters(), <NewLine>                                lr = 0.001,<NewLine>                                momentum=0.9,<NewLine>                               )<NewLine>    <NewLine>    if evaluate:<NewLine>        validate(val_loader, model, criterion)<NewLine>        return<NewLine><NewLine>    for epoch in range(epochs):<NewLine>        cuda_managment(torch.cuda.device_count(), 'CUDA STATE')<NewLine>        adjust_learning_rate(optimizer, epoch)<NewLine><NewLine>        # train for one epoch<NewLine>        train(train_loader, model, criterion, optimizer, epoch)<NewLine><NewLine>        # evaluate on validation set<NewLine>        acc1 = validate(val_loader, model, criterion)<NewLine><NewLine>        # remember best acc@1 and save checkpoint<NewLine>        is_best = acc1 &gt; best_acc1<NewLine>        best_acc1 = max(acc1, best_acc1)<NewLine><NewLine>       <NewLine>        save_checkpoint({<NewLine>            'epoch': epoch + 1,<NewLine>            'state_dict': model.state_dict(),<NewLine>            'best_acc1': best_acc1,<NewLine>            'optimizer' : optimizer.state_dict(),<NewLine>        }, is_best)<NewLine>    <NewLine>    <NewLine>    time_elapsed = time.time() - since<NewLine>    print('training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))<NewLine>    <NewLine>    <NewLine>def train(train_loader, model, criterion, optimizer, epoch):<NewLine>    batch_time = AverageMeter('Time', ':6.3f')<NewLine>    data_time = AverageMeter('Data', ':6.3f')<NewLine>    losses = AverageMeter('Loss', ':.4e')<NewLine>    top1 = AverageMeter('Acc@1', ':6.2f')<NewLine>    top5 = AverageMeter('Acc@5', ':6.2f')<NewLine>    progress = ProgressMeter(len(train_loader), batch_time, data_time, losses, top1,<NewLine>                             top5, prefix=""Epoch: [{}]"".format(epoch))<NewLine><NewLine>    # switch to train mode<NewLine>    model.train()<NewLine>    end = time.time()<NewLine>    <NewLine>    for i, (input, target) in enumerate(train_loader):<NewLine>        # measure data loading time<NewLine>        data_time.update(time.time() - end)<NewLine>        <NewLine>        input = input.cuda(0, non_blocking = True)<NewLine>        target = target.cuda(0, non_blocking=True)<NewLine>#        input = input.cuda(0)<NewLine>#        target = target.cuda(0)<NewLine><NewLine>        # compute output<NewLine>        output = model(input)<NewLine>        loss = criterion(output, target)<NewLine><NewLine>        # measure accuracy and record loss<NewLine>        acc1, acc5 = accuracy(output, target, topk=(1, 2))<NewLine>        losses.update(loss.item(), input.size(0))<NewLine>        top1.update(acc1[0], input.size(0))<NewLine>        top5.update(acc5[0], input.size(0))<NewLine><NewLine>        # compute gradient and do SGD step<NewLine>        optimizer.zero_grad()<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine><NewLine>        # measure elapsed time<NewLine>        batch_time.update(time.time() - end)<NewLine>        end = time.time()<NewLine><NewLine>        progress.print(i)<NewLine><NewLine><NewLine>def validate(val_loader, model, criterion):<NewLine>    batch_time = AverageMeter('Time', ':6.3f')<NewLine>    losses = AverageMeter('Loss', ':.4e')<NewLine>    top1 = AverageMeter('Acc@1', ':6.2f')<NewLine>    top5 = AverageMeter('Acc@5', ':6.2f')<NewLine>    progress = ProgressMeter(len(val_loader), batch_time, losses, top1, top5,<NewLine>                             prefix='Test: ')<NewLine><NewLine>    # switch to evaluate mode<NewLine>    model.eval()<NewLine><NewLine>    with torch.no_grad():<NewLine>        end = time.time()<NewLine>        for i, (input, target) in enumerate(val_loader):<NewLine>            input = input.cuda(0, non_blocking=True)<NewLine>            target = target.cuda(0, non_blocking=True)<NewLine>#            input = input.cuda(0)<NewLine>#            target = target.cuda(0)<NewLine>            <NewLine>            # compute output<NewLine>            output = model(input)<NewLine>            loss = criterion(output, target)<NewLine><NewLine>            # measure accuracy and record loss<NewLine>            acc1, acc5 = accuracy(output, target, topk=(1, 2))<NewLine>            losses.update(loss.item(), input.size(0))<NewLine>            top1.update(acc1[0], input.size(0))<NewLine>            top5.update(acc5[0], input.size(0))<NewLine><NewLine>            # measure elapsed time<NewLine>            batch_time.update(time.time() - end)<NewLine>            end = time.time()<NewLine><NewLine>            progress.print(i)<NewLine><NewLine>        # TODO: this should also be done with the ProgressMeter<NewLine>        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'<NewLine>              .format(top1=top1, top5=top5))<NewLine><NewLine>    return top1.avg<NewLine><NewLine><NewLine><NewLine><NewLine>def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):<NewLine>    torch.save(state, filename)<NewLine>    if is_best:<NewLine>        shutil.copyfile(filename, 'model_best.pth.tar')<NewLine><NewLine><NewLine>class AverageMeter(object):<NewLine>    """"""Computes and stores the average and current value""""""<NewLine>    def __init__(self, name, fmt=':f'):<NewLine>        self.name = name<NewLine>        self.fmt = fmt<NewLine>        self.reset()<NewLine><NewLine>    def reset(self):<NewLine>        self.val = 0<NewLine>        self.avg = 0<NewLine>        self.sum = 0<NewLine>        self.count = 0<NewLine><NewLine>    def update(self, val, n=1):<NewLine>        self.val = val<NewLine>        self.sum += val * n<NewLine>        self.count += n<NewLine>        self.avg = self.sum / self.count<NewLine><NewLine>    def __str__(self):<NewLine>        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'<NewLine>        return fmtstr.format(**self.__dict__)<NewLine><NewLine><NewLine>class ProgressMeter(object):<NewLine>    def __init__(self, num_batches, *meters, prefix=""""):<NewLine>        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)<NewLine>        self.meters = meters<NewLine>        self.prefix = prefix<NewLine><NewLine>    def print(self, batch):<NewLine>        entries = [self.prefix + self.batch_fmtstr.format(batch)]<NewLine>        entries += [str(meter) for meter in self.meters]<NewLine>        print('\t'.join(entries))<NewLine><NewLine>    def _get_batch_fmtstr(self, num_batches):<NewLine>        num_digits = len(str(num_batches // 1))<NewLine>        fmt = '{:' + str(num_digits) + 'd}'<NewLine>        return '[' + fmt + '/' + fmt.format(num_batches) + ']'<NewLine><NewLine><NewLine>def adjust_learning_rate(optimizer, epoch):<NewLine>    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""<NewLine>    lr = 0.001 * (0.1 ** (epoch // 30))<NewLine>    for param_group in optimizer.param_groups:<NewLine>        param_group['lr'] = lr<NewLine><NewLine><NewLine>def accuracy(output, target, topk=(1,)):<NewLine>    """"""Computes the accuracy over the k top predictions for the specified values of k""""""<NewLine>    with torch.no_grad():<NewLine>        maxk = max(topk)<NewLine>        batch_size = target.size(0)<NewLine><NewLine>        _, pred = output.topk(maxk, 1, True, True)<NewLine>        pred = pred.t()<NewLine>        correct = pred.eq(target.view(1, -1).expand_as(pred))<NewLine><NewLine>        res = []<NewLine>        for k in topk:<NewLine>            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)<NewLine>            res.append(correct_k.mul_(100.0 / batch_size))<NewLine>        return res<NewLine><NewLine><NewLine><NewLine><NewLine><NewLine><NewLine><NewLine>data_dir = '/home/bgv/Desktop/cresus ia/data/train_test'<NewLine>batch_size = 20<NewLine>num_epochs = 50<NewLine>n_classes = 4<NewLine>h = 224<NewLine>w = 224<NewLine>input_size = h, w<NewLine><NewLine><NewLine># normalisation<NewLine>normalisation = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])<NewLine><NewLine># color<NewLine>color_jitter_scb = transforms.ColorJitter(saturation = (0.2, 2), contrast= (0.2, 2), brightness = (0.3, 2))<NewLine>color_jitter_s = transforms.ColorJitter(saturation = (0.2, 2))<NewLine>color_jitter_c = transforms.ColorJitter(contrast= (0.2, 2))<NewLine>color_jitter_b = transforms.ColorJitter(brightness = (0.3, 2))<NewLine>color_jitter_random = transforms.RandomChoice([color_jitter_scb, color_jitter_s, color_jitter_c, color_jitter_b])<NewLine><NewLine># rotation<NewLine>rotation = transforms.RandomRotation(degrees = (-10,10), expand=False)<NewLine>rotation_expand = transforms.RandomRotation(degrees = (-10,10), expand=True)<NewLine>rotation_down = transforms.RandomRotation(degrees = (-179,-180), expand=False)<NewLine>rotation_random = transforms.RandomChoice([rotation, rotation_expand])<NewLine><NewLine>original = transforms.Compose([transforms.Resize(input_size), transforms.ToTensor(), normalisation])<NewLine>color_rotation = transforms.Compose([<NewLine>                                    rotation_random,<NewLine>                                    color_jitter_random,<NewLine>                                    transforms.Resize(input_size),<NewLine>                                    transforms.ToTensor(),<NewLine>                                    normalisation<NewLine>                                    ])<NewLine><NewLine># train<NewLine>transforms_ = [original, color_rotation]<NewLine><NewLine>img_count = 0<NewLine>load_train = img_loader('train', transforms_, batch_size)<NewLine>load_test = img_loader('test', transforms_, batch_size, shuffle_data = False)<NewLine>img_loaders = {'train': load_train, 'test': load_test}<NewLine><NewLine>main_worker(n_classes, data_dir, batch_size, load_train, load_test, n_workers = 20, evaluate = False, epochs = num_epochs)<NewLine><NewLine><NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/DrOrstxo,,DrOrstxo,"May 20, 2019, 11:24am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The code looks generally alright.<br/><NewLine>Have you tried to reduce the number of workers? 20 seems to be quite high (of course it’s depending on your system setup).</p><NewLine><p>Also, could you check if <a href=""https://pytorch.org/docs/stable/nn.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel"" rel=""nofollow noopener"">DistributedDataParallel</a> speeds up your training?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for you response.</p><NewLine><p>I already tested with less number of workers, and to be sure I tested once again after your post. It takes more time with less workers (I set it to 10). My machine has 20 physical cores, it is NVIDIA DGX Station.</p><NewLine><p>I tried:</p><NewLine><blockquote><NewLine><p>torch.distributed.init_process_group(backend=“nccl”)<br/><NewLine>model = torch.nn.parallel.DistributedDataParallel(model)</p><NewLine></blockquote><NewLine><p>but I got the following error:</p><NewLine><blockquote><NewLine><p>ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set</p><NewLine></blockquote><NewLine><p>If I understand it well, <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel"" rel=""nofollow noopener"">DataParallel</a> does the same stuff or it is wrong?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>They do the same, but with <code>DataParallel</code> you drive N devices from a single process, whereas with <code>DistributedDataParallel</code> you drive N devices from N processes. For the latter, those devices may be located in different machines, hence the distributed part. You’ll have to launch those N processes though, you cannot start a single process and have it work out of the box. You can start multiple processes manually (and set the <code>RANK</code> environment variable accordingly, per the error message you’re seeing), or use the <a href=""https://pytorch.org/docs/stable/distributed.html#launch-utility"" rel=""nofollow noopener""><code>torch.distributed.launch</code></a> utility to launch processes for you.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/DrOrstxo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: May 21, 2019, 12:43pm; <NewLine> REPLY_DATE 2: May 21, 2019,  3:01pm; <NewLine> REPLY_DATE 3: June 24, 2019,  7:20am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
45876,MultiGPU Dataloader numpy to gpu and tensor to gpu different on CPU usage,2019-05-22T05:51:02.578Z,0,270,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I encounter a strange problem:</p><NewLine><p>Previously, I define my collat_batch function to collect preprocessed data in numpy format, and set pin_memory = Ture; In the training loop, I iter a batch, then move the batch from numpy to gpu device; This way couldn’t make full use of cpu so that even GPU is waiting for data, cpu isn’t running fully.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/7c594ccad2256058173f197084212c9fd5211a40"" href=""https://discuss.pytorch.org/uploads/default/original/2X/7/7c594ccad2256058173f197084212c9fd5211a40.png"" title=""Screen Shot 2019-05-22 at 1.46.30 PM.png""><img alt=""30%20PM"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/7/7c594ccad2256058173f197084212c9fd5211a40_2_10x10.png"" height=""281"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/7/7c594ccad2256058173f197084212c9fd5211a40_2_690x281.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/7/7c594ccad2256058173f197084212c9fd5211a40_2_690x281.png, https://discuss.pytorch.org/uploads/default/original/2X/7/7c594ccad2256058173f197084212c9fd5211a40.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/7/7c594ccad2256058173f197084212c9fd5211a40.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screen Shot 2019-05-22 at 1.46.30 PM.png</span><span class=""informations"">958×391 144 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>Then I change the above process, I define a collate_batch_torch function to collect preprocessed data in numpy format then convert it to torch.tensor, set pin_memory = True; In the training loop, I iter a batch, then move the batch from torch.tensor to device; This can make cpu running fully, but the step time is much slower; so it seems that this modification make cpu the bottelneck.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/9c0f9cfbbd62ab71610dfd4ed314ccba838beee8"" href=""https://discuss.pytorch.org/uploads/default/original/2X/9/9c0f9cfbbd62ab71610dfd4ed314ccba838beee8.png"" title=""Screen Shot 2019-05-22 at 1.53.39 PM.png""><img alt=""39%20PM"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/9/9c0f9cfbbd62ab71610dfd4ed314ccba838beee8_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/9/9c0f9cfbbd62ab71610dfd4ed314ccba838beee8_2_453x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/9/9c0f9cfbbd62ab71610dfd4ed314ccba838beee8_2_453x500.png, https://discuss.pytorch.org/uploads/default/optimized/2X/9/9c0f9cfbbd62ab71610dfd4ed314ccba838beee8_2_679x750.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/9/9c0f9cfbbd62ab71610dfd4ed314ccba838beee8_2_906x1000.png 2x"" width=""453""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screen Shot 2019-05-22 at 1.53.39 PM.png</span><span class=""informations"">958×1056 383 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>When I test it using one gpu or two gpus, the accelerate rate is linear. for example, using one gpu, time cost per iteration is 1s, two gpu is also 1s, but when I use 8 gpus, time could be 8s.</p><NewLine><p>What’s wrong?</p><NewLine></div>",https://discuss.pytorch.org/u/poodarchu,(poodarchu),poodarchu,"May 22, 2019,  6:09am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The bottom image shows a TON of system time= and high system load. At 8 processes you may be spawning a large multiple of data workers that end up overloading your machine. Make sure to tune this to the available cores in your system.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  7:07am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
46064,Best practice for training &amp; validating on multiple GPUs?,2019-05-24T01:56:43.832Z,0,305,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Several configuration I could think of:</p><NewLine><ol><NewLine><li>Train and validate on all possible same GPUs (not able to set different batch_size for train/validate)</li><NewLine><li>Train and validate on different GPUs (can set different batch_size)</li><NewLine><li>Train on all GPUs and save the model per epoch, later run the model on validation data. (not able to use early stopping on validation loss)</li><NewLine></ol><NewLine><p>What is the best practice?<br/><NewLine>Any other thoughts and suggestions will be appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/TDHTTTT,,TDHTTTT,"May 24, 2019, 10:41pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>All depends on your goals. If you want to maximize validation throughput, you’ll want to use as as many devices as you can. If you don’t care and want to keep your code simple, you can choose to use just one.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  7:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
45163,PyTorch &ldquo;NCCL error: unhandled system error&rdquo; during backprop,2019-05-13T23:04:21.392Z,1,769,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to do distributed training with PyTorch and encountered a problem.<br/><NewLine>This runtime error occurs during first backwards pass (initially error occurred<br/><NewLine>on model initialization).</p><NewLine><pre><code class=""lang-auto"">  File ""/home/user/anaconda3/lib/python3.7/runpy.py"", line 193, in _run_module_as_main<NewLine>    ""__main__"", mod_spec)<NewLine>  File ""/home/user/anaconda3/lib/python3.7/runpy.py"", line 85, in _run_code<NewLine>    exec(code, run_globals)<NewLine>  File ""/home/user/anaconda3/lib/python3.7/site-packages/mpi4py/__main__.py"", line 7, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""/home/user/anaconda3/lib/python3.7/site-packages/mpi4py/run.py"", line 196, in main<NewLine>    run_command_line(args)<NewLine>  File ""/home/user/anaconda3/lib/python3.7/site-packages/mpi4py/run.py"", line 47, in run_command_line<NewLine>    run_path(sys.argv[0], run_name='__main__')<NewLine>  File ""/home/user/anaconda3/lib/python3.7/runpy.py"", line 263, in run_path<NewLine>    pkg_name=pkg_name, script_name=fname)<NewLine>  File ""/home/user/anaconda3/lib/python3.7/runpy.py"", line 96, in _run_module_code<NewLine>    mod_name, mod_spec, pkg_name, script_name)<NewLine>  File ""/home/user/anaconda3/lib/python3.7/runpy.py"", line 85, in _run_code<NewLine>    exec(code, run_globals)<NewLine>  File ""project/main.py"", line 115, in &lt;module&gt;<NewLine>    trainer.run(config[""epochs""])<NewLine>  File ""/home/user/project/trainer/trainer.py"", line 107, in run<NewLine>    self.run_epoch()<NewLine>  File ""/home/user/project/trainer/trainer.py"", line 70, in run_epoch<NewLine>    loss.backward()<NewLine>  File ""/home/user/anaconda3/lib/python3.7/site-packages/torch/tensor.py"", line 107, in backward<NewLine>    torch.autograd.backward(self, gradient, retain_graph, create_graph)<NewLine>  File ""/home/user/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 93, in backward<NewLine>    allow_unreachable=True)  # allow_unreachable flag<NewLine>RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:272, unhandled system error<NewLine></code></pre><NewLine><p>Error occurs always.<br/><NewLine>I use MPI for automatic rank assignment and NCCL as main back-end.<br/><NewLine>Initialization is done through file on a shared file system.<br/><NewLine>Each process uses 2 GPUs, processes run on different nodes.<br/><NewLine>Environment variable <code>NCCL_SOCKET_IFNAME</code> is set.</p><NewLine><p>Does anyone know why this error may occur? Thanks in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/starphoenix,(Alexander V.),starphoenix,"May 13, 2019, 11:04pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The NCCL errors can be notoriously cryptic. Can you reproduce the issue as well when you run 2 processes per machine and 4 in total (so you use just a single GPU per process)?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, in case of one process per each  gpu NCCL error  doesn’t reproduce. But another problem arises: all processes freeze during DistributedDataParallel initialization.</p><NewLine><pre><code class=""lang-auto"">model = DistributedDataParallel(<NewLine>    model,<NewLine>    device_ids=[device],<NewLine>    output_device=device,<NewLine>)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can set the environment variable <code>NCCL_DEBUG=INFO</code> to make it output logs.</p><NewLine><p>Also see:</p><NewLine><ul><NewLine><li><a href=""https://pytorch.org/docs/stable/distributed.html#other-nccl-environment-variables"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/distributed.html#other-nccl-environment-variables</a></li><NewLine><li><a href=""https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html"" rel=""nofollow noopener"">https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html</a></li><NewLine></ul><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/starphoenix; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: May 21, 2019,  3:56pm; <NewLine> REPLY_DATE 2: May 26, 2019,  2:52pm; <NewLine> REPLY_DATE 3: June 24, 2019,  7:02am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
46304,How can Pytorch share memory among several processes?,2019-05-27T09:31:48.332Z,0,169,"<div class=""post"" itemprop=""articleBody""><NewLine><p>According to <a href=""https://stackoverflow.com/a/3044626/7238787"" rel=""nofollow noopener"">this</a>, ‘processes have separate memory’. But Pytorch can somehow share memory among several processes, according to this <a href=""https://pytorch.org/docs/stable/multiprocessing.html"" rel=""nofollow noopener"">link</a>: <strong>‘Once the tensor/storage is moved to shared_memory (see share_memory_()), it will be possible to send it to other processes without making any copies.’</strong> Why is it possible to share memory among separate memory? Doesn’t it sound like a paradox?</p><NewLine></div>",https://discuss.pytorch.org/u/jabberwoo,,jabberwoo,"May 27, 2019,  9:31am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It uses <em>shared memory</em>. Multiple processes can map the same shared memory segment into their own private memory space. The same segment may have a different address in each process, but maps to the same underlying physical memory. Also see <a href=""https://en.wikipedia.org/wiki/Shared_memory"" rel=""nofollow noopener"">https://en.wikipedia.org/wiki/Shared_memory</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  6:57am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
46947,Training becomes slower when using nn.DataParallel with custom convolution layer,2019-06-03T09:49:51.416Z,1,721,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In my layer class, there is a value tensor, an index tensor, and a kernel tensor. In the forward function I use scatter_add to add the value to kernel according to the index. Then the kernel is used as the convolution kernel to perform convolution, my layer class looks like this:</p><NewLine><pre><code class=""lang-auto"">class MyLayer(nn.Module):<NewLine>  def __init__(self, C_in, C_out):<NewLine>    super(MyLayer, self).__init__()<NewLine><NewLine>    self.C_in = C_in<NewLine>    self.C_out = C_out<NewLine><NewLine>    self.value = nn.Parameter(...)<NewLine>    self.register_buffer('inds', ...)<NewLine>    self.register_buffer('kernel', torch.zeros(self.C_in * self.C_out * 1 * 1))<NewLine><NewLine>  def forward(self, x, p):<NewLine>    value = self.value * p<NewLine>    kernel = self.kernel.scatter_add(0, self.inds, value)<NewLine>    kernel = kernel.view(self.C_in, self.C_out, 1, 1)<NewLine><NewLine>    out = F.conv2d(x, kernel, stride=1)<NewLine><NewLine>    return out<NewLine></code></pre><NewLine><p>However, when I wrap my network with nn.DataParallel and training on 2 GPUs, I observe a doubled forward time compared with single GPU.  Could someone tell me why my layer becomes even slower with multi-GPU, and how to modify it to work with nn.DataParallel?</p><NewLine></div>",https://discuss.pytorch.org/u/Mr.Z,,Mr.Z,"June 3, 2019, 10:34am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Nothing in the code you pasted looks particularly slow. Perhaps the larger model you’re using contains many small layers/kernels? The <code>nn.DataParallel</code> wrapper replicates a module to N devices and runs <code>forward</code> on each of them. This overhead can dominate the runtime if your model is very small or has many very small kernels.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your advice. Yes, there are many small kernels in my network, actually I found it extremely slow when the batch size is small. Looks like with small batch size the runtime of scatter_add becomes the bottleneck (which can not be accelerated by nn.DataParallel), and when the batch size increases the nn.DataParallel begins to speed up training.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Mr.Z; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  6:21am; <NewLine> REPLY_DATE 2: June 24, 2019,  6:53am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
46431,Accumulate gradients in DDP,2019-05-28T14:32:19.761Z,0,264,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey,</p><NewLine><p>Is there any easy way to accumulate gradients in a DistributedDataParallel model?<br/><NewLine>From what I see the only way to do this would be to copy gradients to a separate buffer before the next forward/backward?</p><NewLine><p>Any plans on adding functionality for this to Pytorch? DataParallel gives too much overhead for me otherwise I would use that.</p><NewLine></div>",https://discuss.pytorch.org/u/tetratrio,(Adrian Sahlman),tetratrio,"May 28, 2019,  2:32pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This was merged very recently in <a href=""https://github.com/pytorch/pytorch/pull/21736"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/21736</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  6:51am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
44289,Hogwild on MultiGPU,2019-05-03T14:42:23.936Z,0,458,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I need to use multiple GPUs available in the machine in a way that each of the processes uses exactly one GPU. I modified the mnist_hogwild code <a href=""https://github.com/pytorch/examples/blob/master/mnist_hogwild/main.py"" rel=""nofollow noopener"">https://github.com/pytorch/examples/blob/master/mnist_hogwild/main.py</a> as the following:</p><NewLine><pre><code class=""lang-auto"">dataloader_kwargs = {'pin_memory': True} if use_cuda else {}<NewLine>    dcount = torch.cuda.device_count()<NewLine>    devices = []<NewLine>    model = Net()<NewLine>    for i in range(dcount):<NewLine>        devices.append(torch.device(""cuda:""+str(i)))<NewLine>    torch.manual_seed(args.seed)<NewLine>    mp.set_start_method('spawn')<NewLine><NewLine>    # model = Net().to(device)<NewLine>    for i in range(dcount):<NewLine>        model.to(devices[i])<NewLine>    model.share_memory() # gradients are allocated lazily, so they are not shared here<NewLine><NewLine>    processes = []<NewLine>    for rank in range(args.num_processes):<NewLine>        p = mp.Process(target=train, args=(rank, args, model, devices[int(rank%dcount)], dataloader_kwargs))<NewLine>        # We first train the model across `num_processes` processes<NewLine>        p.start()<NewLine>        processes.append(p)<NewLine>    for p in processes:<NewLine>        p.join()<NewLine><NewLine></code></pre><NewLine><p>However, while running this code with num_processes = 2, as there are two GPUs in my machine, I can see only one of them engaged. Can you please suggest what exactly I need in the code here?</p><NewLine></div>",https://discuss.pytorch.org/u/bapi,(Bapi Chatterjee),bapi,"May 3, 2019,  2:43pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please review my version.<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/aurotripathy/menace/blob/master/mnist_multigpu_hogwild/main.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/aurotripathy/menace/blob/master/mnist_multigpu_hogwild/main.py"" rel=""nofollow noopener"" target=""_blank"">aurotripathy/menace/blob/master/mnist_multigpu_hogwild/main.py</a></h4><NewLine><pre><code class=""lang-py"">""""""<NewLine>Adding multi-gpu support to mnist w/hogwild<NewLine>""""""<NewLine>from __future__ import print_function<NewLine>import argparse<NewLine>import torch<NewLine>import torch.multiprocessing as mp<NewLine>from model import Net<NewLine>from train import train, test<NewLine>from shared_optim import SharedAdam<NewLine><NewLine><NewLine># Training settings<NewLine>parser = argparse.ArgumentParser(description='PyTorch MNIST Example')<NewLine>parser.add_argument('--batch-size', type=int, default=64, metavar='N',<NewLine>                    help='input batch size for training (default: 64)')<NewLine>parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',<NewLine>                    help='input batch size for testing (default: 1000)')<NewLine>parser.add_argument('--epochs', type=int, default=10, metavar='N',<NewLine>                    help='number of epochs to train (default: 10)')<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/aurotripathy/menace/blob/master/mnist_multigpu_hogwild/main.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>I’m happy to fix issues, improve readability.</p><NewLine><p>This really is derived from an RL implementation by <a class=""mention"" href=""/u/dgriff"">@dgriff</a> available <a href=""https://github.com/dgriff777/rl_a3c_pytorch"" rel=""nofollow noopener"">here</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""44289""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/bapi/40/10124_2.png"" width=""20""/> bapi:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">for i in range(dcount):<NewLine>  model.to(devices[i])<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>This snippet will first move the model to device 0 and then to device 1. If you don’t explicitly move the model in the functions you’re running through multiprocessing, then you’ll have to make this dependent on the rank of the target process. As is, I assume you’re only using process 1.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/auro; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 3, 2019,  6:38pm; <NewLine> REPLY_DATE 2: June 24, 2019,  6:47am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
45753,Safely removing a Module from DDP,2019-05-20T23:27:06.235Z,0,209,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Does anyone have any thoughts on the safest way to remove the DistributedDataParallel wrapper from a Module? Currently I’m just doing something like:</p><NewLine><pre><code class=""lang-auto""># Model in this case has already been wrapped in DDP<NewLine>model = model.module<NewLine></code></pre><NewLine><p>In the docs for DDP, it mentions hooks that are being registered in the module’s params:</p><NewLine><blockquote><NewLine><p>when wrapping up your model with DistributedDataParallel, the constructor of<br/><NewLine>DistributedDataParallel will register the additional gradient<br/><NewLine>reduction functions on all the parameters of the model itself at the<br/><NewLine>time of construction</p><NewLine></blockquote><NewLine><p>I take it those hooks are still there if I just grab the module attribute from the DDP instance right?</p><NewLine></div>",https://discuss.pytorch.org/u/mdlockyer,(Michael Lockyer),mdlockyer,"May 20, 2019, 11:54pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Based on a <a href=""https://github.com/pytorch/pytorch/issues/21344"" rel=""nofollow noopener"">recent issue</a> opened for PyTorch, it is in fact the case currently (v1.1.0) that the module will retain the reduction functions and new ones will be added each time the model is wrapped in DDP</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This has been fixed and will be available in PyTorch 1.2 (and is already available in the nightly builds).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mdlockyer; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 7, 2019,  1:41am; <NewLine> REPLY_DATE 2: June 24, 2019,  6:01am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
47481,Why do workers end up with different loss values?,2019-06-09T15:48:55.388Z,0,182,"<div class=""post"" itemprop=""articleBody""><NewLine><p>From <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel"" rel=""nofollow noopener"">docs</a>,</p><NewLine><blockquote><NewLine><p>Constructor, forward method, and differentiation of the output (or a function of the output of this module) is a distributed synchronization point. Take that into account in case different processes might be executing different code.</p><NewLine></blockquote><NewLine><p>I’m trying to print loss from each worker, and I’m getting the following output:</p><NewLine><pre><code class=""lang-auto"">| distributed init (rank 2): tcp://localhost:1947<NewLine>| distributed init (rank 3): tcp://localhost:1947<NewLine>| distributed init (rank 0): tcp://localhost:1947<NewLine>| distributed init (rank 1): tcp://localhost:1947<NewLine>| initialized host gnode03 as rank 3<NewLine>| initialized host gnode03 as rank 1<NewLine>| initialized host gnode03 as rank 2<NewLine>| initialized host gnode03 as rank 0<NewLine>rank 0 loss 920.7410278320312<NewLine>rank 1 loss 1102.2825927734375<NewLine>rank 3 loss 765.515869140625<NewLine>rank 2 loss 642.1211547851562<NewLine>rank 2 loss 950.1659545898438<NewLine>rank 1 loss 863.4507446289062<NewLine>rank 3 loss 1053.586669921875<NewLine>rank 0 loss 551.5623168945312<NewLine>rank 0 loss 679.0967407226562<NewLine>rank 2 loss 970.89892578125<NewLine>rank 1 loss 1246.443359375<NewLine>rank 3 loss 1169.9415283203125<NewLine>rank 0 loss 798.79833984375<NewLine></code></pre><NewLine><p>Does this mean I have to explicitly aggregate and average the total loss by total batch size? Or is this handled internally? The segment which prints the above looks like this:</p><NewLine><pre><code class=""lang-auto"">        self.model.train()<NewLine>        self._optimizer.zero_grad()<NewLine>        sample = move_to(sample, self.device)<NewLine>        loss, logging_outputs = self.model(sample)<NewLine>        loss.backward()<NewLine>        clip_grad_norm_(self._model.parameters(), args.max_grad_norm)<NewLine>        self._optimizer.step()<NewLine>        return loss.item()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/jerinphilip,(Jerin Philip),jerinphilip,"June 9, 2019,  6:18pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah! This is indeed what you meant in <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/distributeddataparallel-loss-compute-and-backpropogation/47205"">DistributedDataParallel loss compute and backpropogation?</a>. Pasting my answer there here as well for posterity and the indexers.</p><NewLine><p>Each process computes its own output, using its own input, with its own activations, and computes its own loss. Then on <code>loss.backward()</code> all processes reduce their <em>gradients</em>. As <code>loss.backward()</code> returns, the gradients of your model parameters will be the same, and the optimizer in each process will perform the exact same update to the model parameters.</p><NewLine><p>Note that this is <strong>only</strong> the case if you use <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel"" rel=""nofollow noopener""><code>torch.nn.parallel.DistributedDataParallel</code></a>. If you don’t, you’ll need to take care of gradient synchronization yourself.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  5:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
47873,Torch.distributed: Send and Gather on GPU,2019-06-13T14:23:29.686Z,0,283,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!</p><NewLine><p>I want to write a distributed program and run it on a cluster with several multi-GPU nodes which is managed using slurm.</p><NewLine><p>The program should have one master process, which sends (equal to MPI_Send / MPI_Recv) different data to other processes and then collect the results (equal to MPI_Gather).</p><NewLine><p>Could you please tell me if my task can be solved using torch.distributed? In the official docs (<a href=""https://pytorch.org/docs/stable/distributed.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/distributed.html</a>) I found only question marks for send/recv MPI operations for GPU.</p><NewLine><p>I also tried Horovod but found no wrappers around send/recv functions.</p><NewLine></div>",https://discuss.pytorch.org/u/spaceinvader,(Space Invader),spaceinvader,"June 13, 2019,  2:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The question marks mean that it depends whether or not your MPI distribution is compiled with CUDA support or not. If it is, send/recv of GPU tensors works. If it doesn’t, you’ll have to copy GPU tensors to CPU before you can pass them to send/recv. (see <a href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cpu"" rel=""nofollow noopener""><code>torch.Tensor.cpu</code></a>).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: September 10, 2019, 10:45am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
47544,FusedAdam optimizer in Nvidia AMP package,2019-06-10T12:05:26.798Z,0,907,"<div class=""post"" itemprop=""articleBody""><NewLine><p>What is the difference between FusedAdam optimizer in Nvidia AMP package with the Adam optimizer in Pytorch?</p><NewLine></div>",https://discuss.pytorch.org/u/jia_lee,(Jia Lee),jia_lee,"June 10, 2019, 12:05pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The Adam optimizer in Pytorch (like all Pytorch optimizers) carries out optimizer.step() by looping over parameters, and launching a series of kernels for each parameter.  This can require hundreds of small launches that are mostly bound by CPU-side Python looping and kernel launch overhead, resulting in poor device utilization.  Currently, the FusedAdam implementation in Apex flattens the parameters for the optimization step, then carries out the optimization step itself via a fused kernel that combines all the Adam operations.  In this way, the loop over parameters as well as the internal series of Adam operations for each parameter are fused such that optimizer.step() requires only a few kernel launches.</p><NewLine><p>The current implementation (in Apex master) is brittle and only works with Amp opt_level O2.  I’ve got a WIP branch to make it work for any opt_level (<a href=""https://github.com/NVIDIA/apex/pull/351"" rel=""nofollow noopener"">https://github.com/NVIDIA/apex/pull/351</a>).  I recommend waiting until this is merged then trying it.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just wondering, wouldn’t it be possible to use pytorch multiprocessing to parallelise the Adam loop?  Or CUDA streams?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/sbsky"">@sbsky</a> Either technique comes with its own overhead. If the time it takes to launch one of these kernels is &gt;&gt; the time it takes to execute it, you’ll have to optimize the launch itself. If you decide to do this with multiprocessing, you’ll need to move the references to those tensors between processes, which isn’t free. The alternative is to launch fewer kernels, which is what <a class=""mention"" href=""/u/mcarilli"">@mcarilli</a> did in AMP.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mcarilli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/sbsky; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 10, 2019,  9:16pm; <NewLine> REPLY_DATE 2: June 16, 2019, 12:48pm; <NewLine> REPLY_DATE 3: June 24, 2019,  5:27am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
48417,torch.nn.DataParallel(model) fails with different batch size,2019-06-19T16:02:56.440Z,1,797,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am facing a very starnge problem with <code>torch.nn.DataParallel()</code>. I have a system with 8 GPUs and I want to use multiple GPUs for training my model. Now when I wrap the model with nn.DataParallel, it works only for batch_size 10! This is very odd because for any batch size other than 10 ( even smaller), the execution just gets stuck. When I am not using parallelism and running on single GPU, it is working properly. But for batch_size more than 16, cuda is running out of memory because my input vectors are very large and model is very big. So I am unable to take advantage of multiple GPUs. Any soltuion out there? Thank you in advance… <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/ricky123,(Ricky),ricky123,"June 19, 2019,  4:02pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Batch size 10 is odd indeed. I would have expected it to only work with a multiple of 8, if you’re using 8 devices. What kind of model are you trying to parallelize?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>A transformer model. It has 12 layers of encoder blocks.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If the batches are asymmetric in size then it is possible that some devices can handle 2 examples where others can’t. Not much that can be done about this, save for memory profiling to prove that this is what’s happening.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ricky123; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 19, 2019,  6:06pm; <NewLine> REPLY_DATE 2: June 20, 2019,  6:30am; <NewLine> REPLY_DATE 3: June 24, 2019,  5:19am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
48541,Unequal GPU amount in nodes for distributed training,2019-06-20T19:39:24.541Z,0,116,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hypothetically, if I have 2 GPUs in node 0 and 3 GPUs in node 1, how would I configure it to support that? All the examples in the documentation as well as example codes perform <code>word_size = gpus_per_node * args.world_size</code>, which assumes from <code>gpus_per_node</code> that there is an equivalent amount of GPUs per node.</p><NewLine></div>",https://discuss.pytorch.org/u/briankosw,(Brian Ko),briankosw,"June 20, 2019,  7:39pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>That expectation is built into the <code>torch.distributed.launch</code> utility but not elsewhere. You can start 5 processes (1 per GPU) and use <code>world_size=5</code> where you have 2 processes on one machine and 3 processes on the other machine. It’s not very common to have this situation, so I’m not surprised most of the examples you see assume a symmetric contribution across machines. That said, you can still make it work, but will have to adapt those examples or start from scratch with <code>torch.nn.parallel.DistributedDataParallel</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  4:47am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
48691,How to learn distributed data parellel?,2019-06-22T06:30:37.333Z,0,97,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I didn’t learn basic knowledge of computer before, can’t understand communication, concurrent, multiprocessing, what do I need to learn for understanding DDP?</p><NewLine></div>",https://discuss.pytorch.org/u/CHENCHAO0526,(Chenchao0526),CHENCHAO0526,"June 22, 2019,  6:30am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can start with the tutorial. I suggest you study and research the concepts you don’t know thoroughly, if that’s what your goal is. The documentation for <a href=""https://docs.python.org/3.7/library/multiprocessing.html"" rel=""nofollow noopener"">Python multiprocessing</a> is very thorough, for starters.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 24, 2019,  4:39am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
48707,What is multi-node in &rdquo;Multi-Node multi-process distributed training mean?,2019-06-22T13:26:06.908Z,2,160,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In the DDP tutorial, the author user 'multi-node ’ In one computer, I don’t understand why using it, I want to know when should I use multi-node?</p><NewLine></div>",https://discuss.pytorch.org/u/CHENCHAO0526,(Chenchao0526),CHENCHAO0526,"June 22, 2019,  1:26pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you post a link to the tutorial?<br/><NewLine>A node usually refers to a host, so that I’m not sure what multi-node on a single computer means.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""onebox"" href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html</a><br/><NewLine>I guess that the author just want to show how to use multi-node, and let us generize to another enviroment?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can replace it with “multi-process” and it will still be valid. It’s common to use a single PyTorch process per GPU device in your system. Running 8 processes across 8 machines won’t be different from running 8 processes on a single machine (provided it has 8 GPUs), except for performance.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/CHENCHAO0526; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 22, 2019,  2:48pm; <NewLine> REPLY_DATE 2: June 22, 2019,  3:20pm; <NewLine> REPLY_DATE 3: June 24, 2019,  4:36am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
48415,Using streams to seperate conv branches on same gpu,2019-06-19T15:58:47.351Z,0,198,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I was trying to use streams to speed up calling multiple Conv2d modules on the same GPU.</p><NewLine><p>My code is below.</p><NewLine><p>It doesn’t appear to run any quicker . There was a previous question asked last year about using streams. There was a suggestion that all ops should be run on non-default streams. I tried to accomplish this but my attempts don’t seem to have helped.</p><NewLine><p>Is there any obvious problem with my approach?</p><NewLine><p>Thanks</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine><NewLine>class ParallelDilatedConv(nn.Module):<NewLine>    def __init__(self, num_dilations, num_streams):<NewLine>        super(ParallelDilatedConv, self).__init__()<NewLine>        self.m = num_dilations<NewLine>        self.streams = [torch.cuda.Stream() for i in range(num_streams)] <NewLine>        self.module = nn.ModuleList([nn.Conv2d(1, 1, (3, 3), dilation=2**i, padding=2**i) for i in range(num_dilations)])<NewLine>                <NewLine>    def forward(self, input):<NewLine>        res = []<NewLine>        for i in range(self.m):<NewLine>            with torch.cuda.stream(self.streams[i%len(self.streams)]):<NewLine>                res.append(self.module[i](input))<NewLine>        return torch.cat(res)<NewLine><NewLine><NewLine>class DilatedConv(nn.Module):<NewLine>    def __init__(self, num_dilations):<NewLine>        super(DilatedConv, self).__init__()<NewLine>        self.m = num_dilations<NewLine>        self.module = nn.ModuleList([nn.Conv2d(1, 1, (3, 3), dilation=2**i, padding=2**i) for i in range(self.m)])<NewLine><NewLine>    def forward(self, input):<NewLine>        res = []<NewLine>        for i in range(self.m):<NewLine>            res.append(self.module[i](input))<NewLine>        return torch.cat(res)<NewLine><NewLine><NewLine>def time_loop(mod, num_iter, outstr):<NewLine>    start = time.time()<NewLine>    for i in range(num_iter):<NewLine>        mod(im).cpu()<NewLine>    end = time.time()<NewLine>    print(outstr.format(end-start))<NewLine><NewLine>if __name__ == '__main__':<NewLine>    import time<NewLine>    num_iter = 10<NewLine>    num_conv = 6<NewLine>    device = 'cuda:0'<NewLine>    num_streams = 6<NewLine>    s = torch.cuda.Stream()<NewLine>    with torch.cuda.stream(s):<NewLine>        im = torch.rand(1000, 1, 200, 200).to(device, non_blocking=True)<NewLine><NewLine><NewLine>        mod = DilatedConv(num_conv).to(device, non_blocking=True).share_memory()<NewLine>        time_loop(mod, num_iter, 'Sequential took {}')<NewLine>        <NewLine>        mod = ParallelDilatedConv(num_conv, num_streams).to(device, non_blocking=True).share_memory()<NewLine>        time_loop(mod, num_iter, 'Parallel took {}')<NewLine>    <NewLine>    <NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/mellorjc,(Mellorjc),mellorjc,"June 19, 2019,  3:58pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s likely that the convs you’re launching are big enough to occupy the entire GPU. When you launch a number of big kernels on different streams, they end up being executed sequentially. Smaller kernels, that use only a small slice of GPU resources, can be parallelized by using multiple streams.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: June 19, 2019,  5:19pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> 
48379,DataParallel error when applied to simplest possible architecture: &ldquo;Module must have its parameters and buffers on device&rdquo;,2019-06-19T08:54:12.241Z,0,2715,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve been having a lot of problems with DataParallel. I’ve tried the simplest possible version of DataParallel I can think of, and it still errors out. Any help or advice would be greatly appreciated! This is running on a server with two P100 GPUs.</p><NewLine><pre><code class=""lang-auto"">In [4]: mlp = nn.DataParallel(nn.Linear(100, 200))<NewLine>In [5]: mlp(torch.zeros((32, 100)))<NewLine>---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>~/code/projects/mve_sac/core.py in &lt;module&gt;<NewLine>----&gt; 1 mlp(torch.zeros((32, 100)))<NewLine>/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)<NewLine>    491             result = self._slow_forward(*input, **kwargs)<NewLine>    492         else:<NewLine>--&gt; 493             result = self.forward(*input, **kwargs)<NewLine>    494         for hook in self._forward_hooks.values():<NewLine>    495             hook_result = hook(self, input, result)<NewLine>/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py in forward(self, *inputs, **kwargs)<NewLine>    144                 raise RuntimeError(""module must have its parameters and buffers ""<NewLine>    145                                    ""on device {} (device_ids[0]) but found one of ""<NewLine>--&gt; 146                                    ""them on device: {}"".format(self.src_device_obj, t.device))<NewLine>    147<NewLine>    148         inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)<NewLine>RuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/hyperdo,,hyperdo,"June 19, 2019,  9:01am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You need to move the parameters of <code>nn.Linear</code> to <code>cuda</code>. As the error message says, they are currently on <code>cpu</code>.</p><NewLine><pre><code class=""lang-auto"">linear = nn.Linear(100, 200).cuda()<NewLine>mlp = nn.DataParallel(linear)<NewLine>mlp(torch.zeros((32, 100)))</code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vabh; <NewLine> ,"REPLY_DATE 1: June 19, 2019,  9:24am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
47675,Grad values are None when tensors undergo shallow copy,2019-06-12T02:12:41.558Z,2,192,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi. I have a model who’s forward method performs a shallow copy of the tensors into a dictionary before returning like so -</p><NewLine><pre><code class=""lang-auto"">def forward(self, input):<NewLine>    block0 = self.block0(input)<NewLine>    block1 = self.block1(block0)<NewLine>    self.end_points = {}<NewLine>    self.end_points['block0'] = (block0, 0)<NewLine>    self.end_points['block1'] = (block1, 1)<NewLine>    return block0 <NewLine></code></pre><NewLine><p>Where <strong>self.block0</strong> and <strong>self.block1</strong> are <strong>nn.Conv2d</strong> layers followed by <strong>batch norm</strong> and <strong>leaky relu</strong></p><NewLine><p>If now I do -</p><NewLine><pre><code class=""lang-auto"">output = model(input)<NewLine>loss = output.mean()<NewLine>loss.backward()<NewLine>print (model.block0.conv.bias.grad) #block0 is an nn.Module with contains a class attribute conv which is nn.Conv2d<NewLine></code></pre><NewLine><p>The grad value is None. There is a similar outcome if I return just return <strong>self.end_points</strong> dict.</p><NewLine><p>One the other hand with the following forward function -</p><NewLine><pre><code class=""lang-auto"">def forward(self, input):<NewLine>    block0 = self.block0(input)<NewLine>    block1 = self.block1(block0)<NewLine>    self.end_points = {}<NewLine>    self.end_points['block0'] = (block0, 0)<NewLine>    return block0<NewLine>    self.end_points['block1'] = (block1, 1) <NewLine></code></pre><NewLine><p>The grad attribute of <strong>model.block0</strong> gets accumulated with the correct gradient.</p><NewLine><p>I have this problem when I wrap the module in nn.DataParallel only. I’m using the following workaround since I have some custom functions.</p><NewLine><pre><code class=""lang-auto"">class MyDataParallel(torch.nn.DataParallel):<NewLine>    """"""<NewLine>    Allow nn.DataParallel to call model's attributes.<NewLine>    """"""<NewLine>    def __getattr__(self, name):<NewLine>        try:<NewLine>            return super().__getattr__(name)<NewLine>        except AttributeError:<NewLine>            return getattr(self.module, name)<NewLine></code></pre><NewLine><p>I am not able to understand why this is the case. Please help! Thank you in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/Siddhesh_Mhatre,(Siddhesh Mhatre),Siddhesh_Mhatre,"June 12, 2019,  6:25pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you post the definition of <code>self.block0</code> and <code>self.block1</code>?<br/><NewLine>Based on your description, I would assume you’ve defined them as <code>nn.Sequential</code> modules, but then you would get an error calling <code>model.block0.grad</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>My apologies. I did not mention earlier that I encounter this problem only when I wrap my module inside nn.DataParallel. I have updated the description above.</p><NewLine><p>As to your question both of those blocks are nn.Modules which have an instance of nn.Conv2d, nn.LeakyRelu and nn.BatchNorm2d which are called on the input in the forward method in that order.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The problem is fixed by not setting end_points as a class variable and returning the entire dict. I suspect the problem is along the lines of <a href=""https://github.com/pytorch/pytorch/issues/16532"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/16532</a>. Not sure though where the tensor is going out of scope and triggering a recursive deletion of the rest of the graph though.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Siddhesh_Mhatre; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Siddhesh_Mhatre; <NewLine> ,"REPLY_DATE 1: June 12, 2019, 10:51am; <NewLine> REPLY_DATE 2: June 12, 2019,  3:54pm; <NewLine> REPLY_DATE 3: June 13, 2019,  6:05pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
36008,Surviving OOM events in distributed training,2019-01-30T20:00:56.293Z,4,580,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Dealing with varying input size, I catch OOM exceptions during training (in my setting roughly 1 in few hundred minibatches). Due to domain specific reasons, I prefer not to crop/resize inputs to a constant size. Also, there is not a clear way to know in advance which input sizes will cause an OOM.<br/><NewLine>This is generally fine by me as long as I can recover from the OOM events and continue training.</p><NewLine><p>If I detect an OOM event, I’m “cleaning up” using <code>torch.cuda.empty_cache()</code>, zero gradients, and then continue training as usual. This works great in a non-distributed setup, but creates problems in a distributed setting.</p><NewLine><p>note - I am following the suggested way to deal with OOM as mentioned here:<br/><NewLine><aside class=""quote"" data-post=""1"" data-topic=""12754""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v2/letter/j/cab0a1/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/is-it-safe-to-recover-from-cuda-oom/12754"">Is it safe to recover from CUDA OOM?</a><NewLine></div><NewLine><blockquote><NewLine>    I’m looking at trying to improve the robustness of our trainer code.  If I select batch parameters that are not tight enough, I may run out of memory.  Is it safe to catch the OOM, reduce the batch size, and try again? <NewLine>Thanks <NewLine>Jerry<NewLine>  </blockquote><NewLine></aside><NewLine></p><NewLine><p>To deal with OOM in a distributed setting, I do something like this:</p><NewLine><pre><code class=""lang-auto"">            if problems_occured:<NewLine>                success_tens = torch.ones(0)<NewLine>            else:<NewLine>                success_tens = torch.ones(1)<NewLine><NewLine>            dist.all_reduce(success_tens, op=dist.reduce_op.SUM)  ###error happens here<NewLine></code></pre><NewLine><p>and then, only if success_tens reached the size of the world_size, I do an additional all_reduce over the gradients to sum them.<br/><NewLine>This is to make sure that all workers succeeded in calculating their own gradient before combining the gradient.</p><NewLine><p>However, after I catch the OOM event in the worker that caught this OOM event I get the following error:</p><NewLine><pre><code class=""lang-auto"">miniconda3/envs/py36torch/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 838, in all_reduce<NewLine>    work.wait()<NewLine>RuntimeError: [enforce fail at /opt/conda/conda-bld/pytorch_1544174967633/work/third_party/gloo/gloo/allreduce.cc:29] opts.elements &gt; 0<NewLine></code></pre><NewLine><p>note: as can be seen in the error message - I’m currently using gloo as the distributed backend.</p><NewLine><p>Any suggestions on how to solve this are very welcome <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/></p><NewLine><p>Note - while I’m currently using a simple syncronized distributed gradients calculation, I’m open to any suggestions, as long as they help survive occasional (relatively rare) OOM events.</p><NewLine></div>",https://discuss.pytorch.org/u/yoelshoshan,,yoelshoshan,"January 30, 2019,  8:04pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> any idea? the tl;dr is that I can survive OOM events in a none distributed setting (just discarding this training minibatch, cleaning up memory and continuing), but using distributed setting I can’t.<br/><NewLine>This is important for me as I’m in medical imaging setting in which resolution is important, and cropping is too destructive.<br/><NewLine>Any suggestions and/or pointing me to relevant people if necessary is very welcome <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, I’m not really familiar with distributed training and gloo, so I can’t give any useful input to fix this issue. <img alt="":confused:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=6"" title="":confused:""/></p><NewLine><p>However, have you thought about other approaches to avoid the OOM issues?<br/><NewLine><a href=""https://pytorch.org/docs/stable/checkpoint.html"" rel=""nofollow noopener"">torch.utils.checkpoint</a> might be worth a try (although I’m not sure how it behaves in a distributed setup) or <a href=""https://github.com/NVIDIA/apex"" rel=""nofollow noopener"">NVIDIA’s apex - mixed precision training</a>.</p><NewLine><p>Let me know, this would be an option.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks! I’m already using both checkpointing and mixed precision, which helped to make the OOM events pretty rare, but they still exist here and there. Perhaps it’s reasonable to consider this “a bug” or feature request and just report it on the github channel.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/yoelshoshan"">@yoelshoshan</a>! The error message indicates that the tensor that you’re passing to allreduce is empty. Is it possible that the “success_tens” itself is somehow empty? Not sure this is possible, but since you’re already dealing with an OOM…</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! first of all thanks for trying to assist <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=6"" title="":slight_smile:""/><br/><NewLine>success_tens is not empty.</p><NewLine><p>I managed to build NCCL and when using it as the backend for the distributed functions, this issue does not happen, so I believe that this is gloo specific problem.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>OK, I just realized what’s going on here. I misunderstood the code snippet you list in the original post. If you see an OOM, you create an <strong>empty tensor</strong>. This is why the error triggers. Instead of <code>torch.ones(0)</code>, you’ll want to use <code>torch.zeros(1)</code>.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Like you suggested, using torch.tensor(0.0) or torch.tensor(1.0) does not trigger that issue.</p><NewLine><p>Thanks for the help! &lt;3</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/yoelshoshan; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/yoelshoshan; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/yoelshoshan; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/yoelshoshan; <NewLine> ,"REPLY_DATE 1: February 4, 2019,  7:05am; <NewLine> REPLY_DATE 2: February 4, 2019, 11:32pm; <NewLine> REPLY_DATE 3: February 5, 2019,  4:20am; <NewLine> REPLY_DATE 4: February 20, 2019,  7:19pm; <NewLine> REPLY_DATE 5: February 24, 2019,  2:18pm; <NewLine> REPLY_DATE 6: April 16, 2019, 12:32am; <NewLine> REPLY_DATE 7: June 4, 2019,  6:17pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> 
46682,Run `torch.distributed.launch` with `-m` option for the script,2019-05-30T20:15:21.249Z,3,333,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi I’m using <code>DistributedDataParallel</code> to run my model across multi-GPU with sync BN. However, my script uses relative imports and is supposed to be run with <code>-m</code> option. How can I do this when launching it via <code>torch.distributed.launch</code>?</p><NewLine><p>Example (does not work, but I’d like to do this):<br/><NewLine><code>python -m torch.distributed.launch --nproc_per_node 2 -m detector.train --arg1 --arg2</code></p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/arc144,(Eduardo Rocha De Andrade),arc144,"May 30, 2019,  8:15pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Take a look at <a href=""https://discuss.pytorch.org/t/how-to-use-multigpu-pytorch/46290/2"">this snippet</a>, it could help.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply but I think you misunderstood. The issue is not running <code>torch.distributed.launch</code> with <code>-m</code> option. The problem is that my <strong>script</strong> uses relative imports and it is supposed to be run with <code>-m</code> option. I reckon that when <code>torch.distributed.launch</code> spawns the script it uses the more natural approach <code>python detector/script.py</code>, whereas I’d like it to call like <code>python -m detector.script</code></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can create a copy of <a href=""https://github.com/pytorch/pytorch/blob/master/torch/distributed/launch.py"" rel=""nofollow noopener"">this file</a> and customize it the way you want. <a href=""https://github.com/pytorch/pytorch/blob/master/torch/distributed/launch.py#L224"" rel=""nofollow noopener"">Here</a> this module  will spawn parallel processes according to their rank. You could arrange your script so that the <code>cmd</code> could look like <code>cmd = python -m detector.script --local_rank --arg1 --arg2 ...</code>.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>It is unfortunate that I have to make a copy and alter it but I guess it works! Thanks a lot =]</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/LeviViana; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/arc144; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/LeviViana; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/arc144; <NewLine> ,"REPLY_DATE 1: May 31, 2019, 12:05pm; <NewLine> REPLY_DATE 2: May 31, 2019,  1:20pm; <NewLine> REPLY_DATE 3: May 31, 2019,  3:47pm; <NewLine> REPLY_DATE 4: May 31, 2019,  3:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
46697,Has anyone used Ray for distributed learning?,2019-05-30T23:48:08.522Z,0,219,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Does it make sense to use Ray instead of torch.multiprocessing, if used only on a single computer?<br/><NewLine>Has anyone used it for multiple clusters?</p><NewLine><p>What are the advantages/disadvantages?<br/><NewLine>Anything to be cautious of?</p><NewLine><p>Thank you</p><NewLine></div>",https://discuss.pytorch.org/u/Muppet,(Muppet),Muppet,"May 30, 2019, 11:48pm",,,,,
46666,Torch.cuda.is_available() is returning false,2019-05-30T16:45:41.317Z,1,223,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am facing a very strange issue. I am working with cuda 9.0 version of PyTorch and yesterday it was working properly in my system ( ubuntu 14.04). Today when I tried to run my model, I noticed that it is using CPU. When I looked into it, I found that <code>torch.cuda.is_available()</code> is returning <code>false</code> even though the cuda driver is available. How is it possible? It was working properly till yesterday. How can it change syddenly?</p><NewLine></div>",https://discuss.pytorch.org/u/ricky123,(Ricky),ricky123,"May 30, 2019,  4:45pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you update and NVIDIA drivers etc.?<br/><NewLine>Could you try to restart your machine and check, if it’s working again?<br/><NewLine>I had similar issues after Ubuntu updated some drivers.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""46666""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ptrblck/40/1823_2.png"" width=""20""/> ptrblck:</div><NewLine><blockquote><NewLine><p>if it’s working again?</p><NewLine></blockquote><NewLine></aside><NewLine><p>I was working on a remote machine. It will take some time to reboot and check. I will inform after checking. NVIDIA drivers are up to date</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ricky123; <NewLine> ,"REPLY_DATE 1: May 30, 2019,  5:07pm; <NewLine> REPLY_DATE 2: May 30, 2019,  5:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
45176,Is it inefficient to apply nn.DataParallel to nn.Module which is composed of sub-module accelerated with nn.DataParallel?,2019-05-14T04:16:40.631Z,0,286,"<div class=""post"" itemprop=""articleBody""><NewLine><p>We sometimes reuse existent models to build loss module like perceptual loss and GAN’s adversarial loss. I would like to know that if the existent models are accelerated with nn.DataParallel, it is inefficient to use nn.DataParallel one more time for the loss Module which use the existent models.</p><NewLine><p>For example, model1 and model2 compose cycle loss module as follows. In that case, is the code, “cycle_loss = nn.DataParallel(CycleLoss(model1, model2)).cuda()”, inefficient?</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine><NewLine>class Model(nn.Module):<NewLine>	def __init__(self):<NewLine>		super().__init__()<NewLine>		self.conv = nn.Conv2d(1,1,3,1,1)<NewLine><NewLine>	def forward(input):<NewLine>		return F.relu(self.conv(input))<NewLine><NewLine>class CycleLoss(nn.Module):<NewLine>	def __init__(self, model1, model2):<NewLine>		super().__init__()<NewLine>		self.model1 = model1<NewLine>		self.model2 = model2<NewLine>		self.l1 = nn.L1Loss()<NewLine><NewLine>	def forward(input1, input2):<NewLine>		loss = self.l1(self.model2(self.model1(input1)), input1)<NewLine>		loss += self.l1(self.model1(self.model2(input2)), input2)<NewLine>		return loss[None] # expand dim=0 to concatnate<NewLine><NewLine># make model<NewLine>model1 = nn.DataParallel(Model()).cuda()<NewLine>model2 = nn.DataParallel(Model()).cuda()<NewLine><NewLine># loss module<NewLine>cycle_loss = nn.DataParallel(CycleLoss(model1, model2)).cuda()<NewLine><NewLine># opt<NewLine>opt = torch.optim.Adam(list(model1.parameters()) + list(model2.parameters()))<NewLine><NewLine># train<NewLine>for input1, input2 in data_loader:<NewLine>	opt.zero_grad()<NewLine>	loss = cycle_loss(input1, input2)<NewLine>	loss = loss.mean()<NewLine>	loss.backward()<NewLine>	opt.step()<NewLine></code></pre><NewLine><p>I am afraid that nesting nn.DataParallel makes the code perform scatter and gather at each sub-module needlessly.</p><NewLine><p>Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/toracorp,(toracorp),toracorp,"May 14, 2019,  4:17am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, this is not great. The outer <code>nn.DataParallel</code> module will replicate N times. The inner modules will also be replicated N times. I think you’ll end up with N^2 replicas instead of just N. You can add some logging to the forward functions to confirm what really ends up happening.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: May 21, 2019,  3:59pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
44971,Can infiniband accelerate distributed training without GPUDirect?,2019-05-11T09:37:31.239Z,0,251,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have two 4x2080ti machines. I want to train my model by NCCL distributed backend. But the training is slow because these two machines are connected by a 1000M ethernet card.<br/><NewLine>So I want to use two infiniband cards to connect these two machines.<br/><NewLine>But my GPU is a GeForce not a Tesla. The question is, can infiniband accelerate the training if the GPU don’t support GPUDirect?</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/GeoffreyChen777,(Geo Ch),GeoffreyChen777,"May 11, 2019,  9:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>In theory, yes. As long as you get cards with a higher bandwidth than your Ethernet setup it should result in an improvement. But since NCCL is built for using GPUDirect, I’m not sure if it will work with NCCL out of the box. If it doesn’t, you could try and experiment with IPoIB and fall back to using NCCL’s TCP transport.</p><NewLine><p>Good luck!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: May 23, 2019,  2:47am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
44182,Does all_reduce_multigpu work with shared list (created by multiprocess.Module.list())?,2019-05-02T13:17:15.949Z,0,105,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello.</p><NewLine><p>I’m trying to use “all_reduce_multigpu” function with shared memory (list created by multiprocess module). However, it does not work with the shared list object (only working with a normal list).</p><NewLine><p>I want to use the shared list to get return values from the child processes. Is there any alternative way?<br/><NewLine>Thank you in advance <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/yangyang,(Eunju Yang),yangyang,"May 2, 2019,  1:17pm",,,,,
44101,Make cross validation parallelized,2019-05-01T16:31:16.312Z,0,173,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a cuda9-docker with tensorflow and pytorch installed, I am doing cross validation on an image dataset. Currently I am using a for loop to do the cross validation. Something like</p><NewLine><pre><code>for data_train, data_test in sklearn.kfold(5, all_data):<NewLine>  train(data_train)<NewLine>  test(data_test)<NewLine></code></pre><NewLine><p>But the for loop takes too long, will the following code work to parallelize the for loop? Maybe there is already a solution. But this is not Data Parallelization.</p><NewLine><pre><code>from multiprocessing import Pool<NewLine><NewLine>def f(trainset, testset):<NewLine>    train_result = train(trainset)<NewLine>    test_result = test(testset)<NewLine>    save_train_result()<NewLine>    save_test_result()<NewLine><NewLine>if __name__ == '__main__':<NewLine>    with Pool(5) as p:<NewLine>        print(p.map(f, sklearn.cvfold(5, all_data)))<NewLine></code></pre><NewLine><p>I am not sure if the multiprocessing will only paralize the cpu or both cpu and gpu? This might be easiler than doing parallel in side  a model i guess like <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/parallelize-simple-for-loop-for-single-gpu/33701"">Parallelize simple for-loop for single GPU</a><br/><NewLine>since in my case, there is no need to communicate across each process?</p><NewLine></div>",https://discuss.pytorch.org/u/udo,(Xudong Sun),udo,"May 1, 2019,  4:31pm",,,,,
42437,Errors in GLOO backend in init_process_group after system updates,2019-04-12T22:53:20.675Z,3,558,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My Pytorch distributed code (similar to ImageNet example, but without multiprocessing) was working on distributed nodes, using the GLOO backend (Python 3.7, Pytorch 1.0.1). There was some system updates, and now all of a sudden I have intermittent success with the code, with the init_process_group throwing various errors often. I can usually get it work on 2 nodes (4 GPU’s each node), but when I try &gt; 2 compute nodes, almost always there is an error thrown in the init_process_group call (see below for some of the errors).<br/><NewLine>I put “python -X faulthandler”, and it tells me for the Segmentation faults, that the error is in ProcessGroupGloo in distributed_c10d.py (line 360).<br/><NewLine>I’m a bit at a loss of how to debug this, and what to check. I have reinstalled the Pytorch packages after the system updates, and tried going to Python 3.6 also, but no luck. I haven’t tried compiling from source yet. I started looking at the GLOO repo, and saw there are some tests, not sure if they would help pinpoint the cause.</p><NewLine><p>Error <span class=""hashtag"">#1:</span></p><NewLine><blockquote><NewLine><p>srun: error: tiger-i21g6: task 0: Segmentation fault</p><NewLine></blockquote><NewLine><p>Error <span class=""hashtag"">#2:</span></p><NewLine><blockquote><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “disruptcnn/main.py”, line 595, in &lt;module&gt;<br/><NewLine>main()<br/><NewLine>File “disruptcnn/main.py”, line 161, in main<br/><NewLine>main_worker(args.gpu, ngpus_per_node, args)<br/><NewLine>File “disruptcnn/main.py”, line 177, in main_worker<br/><NewLine>world_size=args.world_size, rank=args.rank)<br/><NewLine>File “~/.conda/envs/python3a/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py”, line 360, in init_process_group<br/><NewLine>timeout=timeout)<br/><NewLine>RuntimeError: read: Bad address<br/><NewLine>terminate called after throwing an instance of 'std::system_error’<br/><NewLine>what():  read: Bad address</p><NewLine></blockquote><NewLine><p>Error <span class=""hashtag"">#3:</span></p><NewLine><blockquote><NewLine><p>*** Error in `~/.conda/envs/python36/bin/python’: free(): invalid next size (fast): 0x000055b8f75ea9b0 ***</p><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/churchillmic,(Michael),churchillmic,"April 12, 2019, 10:53pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!</p><NewLine><p>Can you share how you’re calling <code>init_process_group</code>, which initialization method you’re using, etc? If you get the first error on one machine and the second error on another, it is possible that the second error is caused by the first process crashing.</p><NewLine><p>I’m asking because the <code>read: Bad address</code> error makes me think there is something going on with the TCP store. If you’re using the TCP initialization method, and try to use an IP that is no longer valid, for example, this is the type of error that <em>could</em> happen.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m using the file method, on a parallel file system:</p><NewLine><blockquote><NewLine><pre><code>    jobid = os.environ['SLURM_JOB_ID']<NewLine>    world_size = int(os.environ['SLURM_NTASKS'])<NewLine>    rank = int(os.environ['SLURM_PROCID'])<NewLine>    dist.init_process_group(backend='gloo', init_method='file:///scratch/gpfs/me/main_'+jobid+'.txt',<NewLine>                            world_size=world_size, rank=rank)<NewLine></code></pre><NewLine></blockquote><NewLine><p>The errors don’t occur one on each machine, rather if I try running this various times, one of those errors will be thrown (but not both at the same time).</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>When you run this multiple times, do those runs use the same <code>SLURM_JOB_ID</code>? PyTorch makes an attempt to remove the file that is used for initialization at exit, but if any of the processes crashes it may stick around, and cause problems. You can “fix” this by force removing the file before starting a new run.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, the SLURM system gives you a unique SLURM_JOB_ID for each run that you do (which is why I’m using it, to ensure the file is unique for each run).</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I noticed the note on fcntl, is there some test I should run on the parallel file system to ensure there are no issues with correct locking? I think GPFS should be fine, but perhaps theres some edge case, and a newer driver or something caused things to mess up.</p><NewLine><p>I was able to try out NCCL, and this appears to be working for the &gt; 2 nodes runs, so its not as urgent, but I’d still be interested in figuring this out</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""5"" data-topic=""42437""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v3/letter/c/e495f1/40.png"" width=""20""/> churchillmic:</div><NewLine><blockquote><NewLine><p>No, the SLURM system gives you a unique SLURM_JOB_ID for each run that you do (which is why I’m using it, to ensure the file is unique for each run).</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thanks, that rules out clobbering the same file from multiple runs.</p><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""42437""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v3/letter/c/e495f1/40.png"" width=""20""/> churchillmic:</div><NewLine><blockquote><NewLine><p>I noticed the note on fcntl, is there some test I should run on the parallel file system to ensure there are no issues with correct locking?</p><NewLine></blockquote><NewLine></aside><NewLine><p>There is. We have a test for the file store that’s built by default if you compile from source and will be located at <code>build/bin/FileStoreTest</code>. This test automatically creates some files in <code>TMPDIR</code>, which you can override yourself to force it to use the GPFS path. This doesn’t fully simulate the scenario you have with multiple machines, but at least hammers the file system with multiple processes from the same machine. It could uncover something, so it’s definitely worth a try.</p><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""42437""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v3/letter/c/e495f1/40.png"" width=""20""/> churchillmic:</div><NewLine><blockquote><NewLine><p>I was able to try out NCCL, and this appears to be working for the &gt; 2 nodes runs, so its not as urgent, but I’d still be interested in figuring this out</p><NewLine></blockquote><NewLine></aside><NewLine><p>The use of this store when using the NCCL backend is very light. Only a single process writes to the file and all others read from it. When using the Gloo backend, everybody both writes to and reads from the file, causing a lot more contention.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there,</p><NewLine><p>I’m using GPFS filesystem for file init, and I also got the <code>read(): bad address</code> problem. When I change the file location to local <code>/tmp</code>, it’s fine.</p><NewLine><p>FYI: the mandatory file lock of Gluster and some known issues <a href=""https://docs.gluster.org/en/v3/Administrator%20Guide/Mandatory%20Locks/"" rel=""nofollow noopener"">https://docs.gluster.org/en/v3/Administrator%20Guide/Mandatory%20Locks/</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/churchillmic; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/churchillmic; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/churchillmic; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Stone; <NewLine> ,"REPLY_DATE 1: April 15, 2019,  9:34pm; <NewLine> REPLY_DATE 2: April 16, 2019,  2:52am; <NewLine> REPLY_DATE 3: April 16, 2019, 10:57pm; <NewLine> REPLY_DATE 4: April 16, 2019, 11:03pm; <NewLine> REPLY_DATE 5: April 16, 2019, 11:06pm; <NewLine> REPLY_DATE 6: April 17, 2019,  3:07am; <NewLine> REPLY_DATE 7: April 22, 2019,  5:49pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> 
40887,How to perform data parallelism for model parallelism,2019-03-26T03:29:37.339Z,1,363,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I try to make data parallelism compatible with model parallelism, but I encounter <code>RuntimeError: all tensors must be on devices[0]</code> during this process. Below is a simplified example of my code (my torch version is <code>1.0.1.post2</code>):</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine><NewLine><NewLine>class MyModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.fc1 = nn.Linear(784, 512)<NewLine>        self.fc2 = nn.Linear(512, 10)<NewLine><NewLine>    def forward(self, x):<NewLine>        first_device = x.device<NewLine>        x = self.fc1(x.to(self.fc1.weight.device))<NewLine>        x = F.relu(x)<NewLine>        x = self.fc2(x.to(self.fc2.weight.device))<NewLine>        x = F.softmax(x).to(first_device)<NewLine>        return x<NewLine>    <NewLine>    def model_parallel(self, start):<NewLine>        self.fc1.cuda(start)<NewLine>        self.fc2.cuda(start + 1)<NewLine><NewLine><NewLine>def run(rank, device_id, world_size):<NewLine>    torch.distributed.init_process_group(<NewLine>        backend='nccl',<NewLine>        init_method='tcp://localhost:10000', <NewLine>        world_size=world_size, <NewLine>        rank=rank, <NewLine>    )<NewLine>    model = MyModel()<NewLine>    model.model_parallel(device_id)<NewLine>    model = nn.parallel.DistributedDataParallel(<NewLine>        module=model,<NewLine>        device_ids=list(range(device_id, device_id + world_size)),<NewLine>        output_device=device_id,<NewLine>        broadcast_buffers=False,<NewLine>    )<NewLine>    model(torch.randn(1, 784).cuda(device_id))<NewLine><NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    mp = torch.multiprocessing.get_context('spawn')<NewLine>    <NewLine>    world_size = 2<NewLine>    model_size = 2<NewLine>    procs = []<NewLine>    for i in range(world_size):<NewLine>        rank = i<NewLine>        device_id = i * model_size<NewLine>        procs.append(mp.Process(target=run, args=(rank, device_id, world_size, ), daemon=True))<NewLine>        procs[i].start()<NewLine>    for p in procs:<NewLine>        p.join()<NewLine></code></pre><NewLine><p>The full traceback is:</p><NewLine><pre><code class=""lang-auto"">Process SpawnProcess-1:<NewLine>Traceback (most recent call last):<NewLine>  File ""/home/user/lib/python3.6/multiprocessing/process.py"", line 249, in _bootstrap<NewLine>    self.run()<NewLine>  File ""/home/user/lib/python3.6/multiprocessing/process.py"", line 93, in run<NewLine>    self._target(*self._args, **self._kwargs)<NewLine>  File ""/home/user/nmt-research/example.py"", line 34, in run<NewLine>    broadcast_buffers=False,<NewLine>  File ""/home/user/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 217, in __init__<NewLine>    self._ddp_init_helper()<NewLine>  File ""/home/user/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 232, in _ddp_init_helper<NewLine>    self._module_copies = replicate(self.module, self.device_ids, detach=True)<NewLine>  File ""/home/user/lib/python3.6/site-packages/torch/nn/parallel/replicate.py"", line 13, in replicate<NewLine>    param_copies = Broadcast.apply(devices, *params)<NewLine>  File ""/home/user/lib/python3.6/site-packages/torch/nn/parallel/_functions.py"", line 21, in forward<NewLine>    outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)<NewLine>  File ""/home/user/lib/python3.6/site-packages/torch/cuda/comm.py"", line 40, in broadcast_coalesced<NewLine>    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)<NewLine>RuntimeError: all tensors must be on devices[0]<NewLine>Process SpawnProcess-2:<NewLine>Traceback (most recent call last):<NewLine>  File ""/home/user/lib/python3.6/multiprocessing/process.py"", line 249, in _bootstrap<NewLine>    self.run()<NewLine>  File ""/home/user/lib/python3.6/multiprocessing/process.py"", line 93, in run<NewLine>    self._target(*self._args, **self._kwargs)<NewLine>  File ""/home/user/nmt-research/example.py"", line 34, in run<NewLine>    broadcast_buffers=False,<NewLine>  File ""/home/user/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 217, in __init__<NewLine>    self._ddp_init_helper()<NewLine>  File ""/home/user/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 232, in _ddp_init_helper<NewLine>    self._module_copies = replicate(self.module, self.device_ids, detach=True)<NewLine>  File ""/home/user/lib/python3.6/site-packages/torch/nn/parallel/replicate.py"", line 13, in replicate<NewLine>    param_copies = Broadcast.apply(devices, *params)<NewLine>  File ""/home/user/lib/python3.6/site-packages/torch/nn/parallel/_functions.py"", line 21, in forward<NewLine>    outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)<NewLine>  File ""/home/user/lib/python3.6/site-packages/torch/cuda/comm.py"", line 40, in broadcast_coalesced<NewLine>    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)<NewLine>RuntimeError: all tensors must be on devices[0]<NewLine></code></pre><NewLine><p>I want to know how to perform data parallelism together with model parallelism correctly. Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/lyy1994,,lyy1994,"March 26, 2019,  3:36am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There are assumptions in <code>torch.nn.parallel.DistributedDataParallel</code> today that prevent you from doing this unfortunately. We’re working on some changes to DDP to make this possible. Stay tuned.</p><NewLine><p>cc <a class=""mention"" href=""/u/mrshenli"">@mrshenli</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Added a link to this post <a href=""https://github.com/pytorch/pytorch/issues/17757"" rel=""nofollow noopener"">in this GitHub issue</a> so we don’t forget about it.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, pietern! Hope this upgrade will be available soon!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>This should be possible after <a href=""https://github.com/pytorch/pytorch/pull/19271"" rel=""nofollow noopener"">#19271</a>. Here is a <a href=""https://github.com/pytorch/tutorials/pull/478"" rel=""nofollow noopener"">tutorial</a> <a class=""mention"" href=""/u/lyy1994"">@lyy1994</a> could you please help to verify if that works for you?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/lyy1994; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mrshenli; <NewLine> ,"REPLY_DATE 1: March 26, 2019,  3:46pm; <NewLine> REPLY_DATE 2: March 26, 2019,  3:47pm; <NewLine> REPLY_DATE 3: March 27, 2019,  6:28am; <NewLine> REPLY_DATE 4: April 18, 2019,  4:04pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
42370,GPUs allowed in PyTorch,2019-04-12T07:19:14.073Z,2,172,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, is there any table where can I check which GPUs are compatible with pyTorch?<br/><NewLine>For example, geforce gtx/rtx models?</p><NewLine></div>",https://discuss.pytorch.org/u/hpc-unex,(SergioUnex),hpc-unex,"April 12, 2019,  7:19am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think pytorch can use on every GPU which  can work with CUDA, you may check the website for <a href=""https://developer.nvidia.com/cuda-legacy-gpus"" rel=""nofollow noopener"">reference</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, but im talking about a Distributed Neural Network. I tried to execute it on Tesla series with &lt; 3.x compute capabilite and it doesnt work. So, I want to make sure if any GPU with &gt; 3.x will work.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>PyTorch works with compute cability 3.5 and higher. This is the Tesla Kepler series (K20, K40, K80).</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>What about RTX and GTX Series? It doesnt work?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes they do. You can check on <a href=""https://developer.nvidia.com/cuda-gpus"" rel=""nofollow noopener"">this website</a> for the compute capability of your GPU and it works, if your GPU has a capability of 3.5 or above.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Hansama; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hpc-unex; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/hpc-unex; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/justusschock; <NewLine> ,"REPLY_DATE 1: April 12, 2019,  8:14am; <NewLine> REPLY_DATE 2: April 12, 2019,  8:57am; <NewLine> REPLY_DATE 3: April 15, 2019,  9:38pm; <NewLine> REPLY_DATE 4: April 18, 2019, 10:36am; <NewLine> REPLY_DATE 5: April 18, 2019, 10:57am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
38274,(shared) Memory leak on Pytorch 1.0,2019-02-26T08:25:00.918Z,1,454,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’d like to report a weird behaviour in 1.0 that I could resolve by only going back to 0.4. I am training a network on quite a big data set (35 GB) and use 4 gpus by applying the command<br/><NewLine>torch.nn.DataParallel(model).cuda()<br/><NewLine>Further I am having a big batch size (&gt;1000) which makes the command<br/><NewLine>torch.multiprocessing.set_sharing_strategy(‘file_system’). I have num_workers=16 in the dataloader<br/><NewLine>necessary. Now the trouble begins: every epoch my /dev/shm increases by ca. 3GB. At some point it is full and my process crashes. I tried 1.0.0 and 1.0.1 but both showed this bahaviour. Pytorch 0.4 does not have this problem, /dev/shm is never above 1GB.<br/><NewLine>Is this a bug?</p><NewLine></div>",https://discuss.pytorch.org/u/lev,,lev,"February 26, 2019,  8:25am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looking at the <a href=""https://pytorch.org/docs/stable/multiprocessing.html#sharing-strategies"" rel=""nofollow noopener"">available sharing strategies</a>, the <code>file_system</code> one is clearly prone to leaks. If the data loader ends up allocating new shared tensors for every epoch, this would explain your leak. Did you try using the <code>file_descriptor</code> sharing strategy?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>file_descriptor is the default setting. I tried it of course but could not use for other reasons.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you can (have sudo privilege), increase the file descriptor limit of your system and use the <code>file_descriptor</code> sharing strategy.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/lev; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/NeoZ; <NewLine> ,"REPLY_DATE 1: March 12, 2019,  5:23pm; <NewLine> REPLY_DATE 2: March 13, 2019,  8:54am; <NewLine> REPLY_DATE 3: April 17, 2019,  8:14am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
42656,Multiprocessing on distributed Multi-nodes shutdown error: &lsquo;spawn&rsquo; on slave node leads to semaphore_tracker leaked,2019-04-15T18:49:25.945Z,1,558,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Masters,</p><NewLine><p>I am trying the following code on 2 nodes with diff num of CPU/GPU devices, running one parameter server (ps) process and diff num of worker process on each node.(e.g. global_ranks:[[0(ps),2(worker),3(worker)],[1(ps),4(worker)]])</p><NewLine><p>For CUDA init reasons, I turned <code>mp.set_start_method('spawn', force=True)</code> on slave node and leads to the following crash:(NOT warning)<br/><NewLine>/home/simon/anaconda3/lib/python3.6/multiprocessing/semaphore_tracker.py:146: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown<br/><NewLine>len(cache))</p><NewLine><p>Could somebody help? Thanks in advance.</p><NewLine><h2>Results:</h2><NewLine><pre><code class=""lang-auto"">$Slave Node<NewLine>&lt;Process(Process-1, started)&gt;  is started...<NewLine>&lt;Process(Process-2, started)&gt;  is started...<NewLine>0 test_run_worker() on global_rank: 4 ,global_c: 1 ,counter_list: [tensor([1.]), tensor([0.]), tensor([1.])]<NewLine>0 test_run_ps() on global_rank: 1 , global_c= 0 ,counter_list: [tensor([1.]), tensor([0.]), tensor([1.])]<NewLine>1 test_run_worker() on global_rank: 4 ,global_c: 4 ,counter_list: [tensor([2.]), tensor([1.]), tensor([2.])]<NewLine>2 test_run_worker() on global_rank: 4 ,global_c: 6 ,counter_list: [tensor([2.]), tensor([1.]), tensor([3.])]<NewLine>1 test_run_ps() on global_rank: 1 , global_c= 3 ,counter_list: [tensor([2.]), tensor([1.]), tensor([2.])]<NewLine>3 test_run_worker() on global_rank: 4 ,global_c: 9 ,counter_list: [tensor([3.]), tensor([2.]), tensor([4.])]<NewLine>4 test_run_worker() on global_rank: 4 ,global_c: 10 ,counter_list: [tensor([3.]), tensor([2.]), tensor([5.])]<NewLine>2 test_run_ps() on global_rank: 1 , global_c= 6 ,counter_list: [tensor([3.]), tensor([2.]), tensor([3.])]<NewLine>3 test_run_ps() on global_rank: 1 , global_c= 9 ,counter_list: [tensor([4.]), tensor([3.]), tensor([4.])]<NewLine>4 test_run_ps() on global_rank: 1 , global_c= 12 ,counter_list: [tensor([5.]), tensor([4.]), tensor([5.])]<NewLine>/home/simon/anaconda3/lib/python3.6/multiprocessing/semaphore_tracker.py:146: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown<NewLine>  len(cache))<NewLine>/home/simon/anaconda3/lib/python3.6/multiprocessing/semaphore_tracker.py:146: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown<NewLine>  len(cache))<NewLine></code></pre><NewLine><h2>Some Codes:</h2><NewLine><pre><code class=""lang-auto"">#Master Node:<NewLine>#GLOO_SOCKET_IFNAME=enp0s31f6 python -m torch.distributed.launch torch_dist_test2.py --local_rank=0<NewLine>#Slave Node:<NewLine>#GLOO_SOCKET_IFNAME=enp7s0 python -m torch.distributed.launch torch_dist_test2.py --local_rank=1<NewLine><NewLine>def init_dist_multi_process(world_size, global_rank, backend='gloo'):#'nccl'<NewLine>    dist.init_process_group(backend=backend,<NewLine>                        init_method='tcp://192.168.1.12:23457',<NewLine>                        world_size=world_size,<NewLine>                        rank=global_rank)<NewLine><NewLine>def dist_broadcast(src, dic=None, tensor=None, async_op=True, p_device=torch.device('cpu')):<NewLine>    if not dic == None:<NewLine>        for key, value in dic.items():<NewLine>            dist.broadcast(tensor=torch.Tensor(value).to(p_device), src=src, async_op=async_op)<NewLine>    else:<NewLine>        dist.broadcast(tensor=tensor.to(p_device), src=src, async_op=async_op)<NewLine><NewLine>def test_run_ps(shared_coms):<NewLine>    init_dist_multi_process(world_size=shared_coms.world_size, global_rank=shared_coms.server_rank, backend='gloo')<NewLine><NewLine>    counter_list = [torch.Tensor([0]) for _ in shared_coms.global_worker_rank_flatten_list]<NewLine>    for _ in range(5):<NewLine>        time.sleep(0.5)<NewLine>        for r, gr in enumerate(shared_coms.global_worker_rank_flatten_list):##<NewLine>            dist_broadcast(src=gr,tensor=counter_list[r])<NewLine>        global_c = sum([int(x) for x in counter_list])<NewLine>        print(_,'test_run_ps() on global_rank:',shared_coms.server_rank,', global_c=',global_c,',counter_list:',counter_list)  <NewLine><NewLine>    print('test_run_ps() time up')<NewLine>    time.sleep(5)<NewLine><NewLine>def test_run_worker(shared_coms, device_r):<NewLine>    init_dist_multi_process(world_size=shared_coms.world_size, global_rank=shared_coms.worker_rank_list[device_r], backend='gloo')<NewLine><NewLine>    c = 0<NewLine>    counter_list = [torch.Tensor([0]) for _ in shared_coms.global_worker_rank_flatten_list]<NewLine><NewLine>    for _ in range(5):<NewLine>        time.sleep(0.25*(1+device_r))<NewLine>        c+=1<NewLine>        i=0<NewLine>        for r, gr in enumerate(shared_coms.global_worker_rank_flatten_list):##<NewLine>            if gr == shared_coms.global_worker_rank_list[shared_coms.server_rank][device_r]:<NewLine>                counter_list[r] = torch.Tensor([c])<NewLine>            dist_broadcast(src=gr,tensor=counter_list[r])<NewLine>        global_c = sum([int(x) for x in counter_list])<NewLine>        print(_,'test_run_worker() on global_rank:',shared_coms.worker_rank_list[device_r],',global_c:',global_c,',counter_list:',counter_list)<NewLine>    <NewLine>    print('test_run_worker() time up')<NewLine>    time.sleep(5)<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    parser = argparse.ArgumentParser()<NewLine>    parser.add_argument(""--local_rank"", type=int, default=0)<NewLine>    parser.add_argument(""--global_rank_list"", type=list, default=[[0,2,3],[1,4]])<NewLine>    parser.add_argument(""--n_worker"", type=int, default=16)<NewLine>    args = parser.parse_args()<NewLine><NewLine><NewLine>    server_rank = args.local_rank<NewLine>    global_rank_list = args.global_rank_list<NewLine>    world_size = sum([1 for y in global_rank_list for x in y])<NewLine>    n_proc = len(global_rank_list[server_rank])-1<NewLine>    global_n_proc = world_size - len(global_rank_list)<NewLine>    n_server = len(global_rank_list)<NewLine>    worker_rank_list = global_rank_list[server_rank][1:]<NewLine>    global_worker_rank_list = [global_rank_list[x][1:] for x in range(len(global_rank_list))]<NewLine>    global_worker_rank_flatten_list = []<NewLine>    for x in global_worker_rank_list: global_worker_rank_flatten_list+=x<NewLine><NewLine>    n_workers_per_slave = args.n_worker<NewLine>    game = 'BreakoutNoFrameskip-v4'<NewLine>    process_list = []<NewLine><NewLine>    shared_coms = SharedComponents(game, server_rank, global_rank_list, p_device=torch.device('cuda'))<NewLine><NewLine>    mp.set_start_method('spawn', force=True)<NewLine><NewLine>    p = mp.Process(target=test_run_ps, args=(shared_coms, ))#, args=(None))<NewLine>    process_list.append(p)<NewLine><NewLine>    for device_r in range(n_proc):##<NewLine>        p = mp.Process(target=test_run_worker, args=(shared_coms, device_r))<NewLine>        process_list.append(p)<NewLine><NewLine>    for p in process_list:<NewLine>        p.start()<NewLine>        print(p,' is started...')<NewLine><NewLine>    for p in process_list:<NewLine>        p.join()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/forhonourlx,(Forhonourlx),forhonourlx,"April 15, 2019,  6:49pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! Hard to tell where this is going wrong. The warning from <code>multiprocessing</code> happens in the parent process (I think) and doesn’t pinpoint where the crash is happening. It looks like neither process gets to log the <code>time up</code> message, so do they even break out of their loops?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Pieter,</p><NewLine><p>You are right, they do break out of their loops or may be only warning messages after crashes.<br/><NewLine>Any way to debug? Thank you in advance.</p><NewLine><p>I add <code>logger = multiprocessing.log_to_stderr()</code>, <code>logger.setLevel(multiprocessing.SUBDEBUG)</code> to the demo, getting the following info:</p><NewLine><pre><code class=""lang-auto"">[INFO/Process-2] process shutting down<NewLine>[DEBUG/Process-2] running all ""atexit"" finalizers with priority &gt;= 0<NewLine>[DEBUG/Process-2] running the remaining ""atexit"" finalizers<NewLine>[INFO/Process-2] process exiting with exitcode 0<NewLine>/home/simon/anaconda3/lib/python3.6/multiprocessing/semaphore_tracker.py:146: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown<NewLine>  len(cache))<NewLine>[INFO/Process-1] process shutting down<NewLine>[DEBUG/Process-1] running all ""atexit"" finalizers with priority &gt;= 0<NewLine>[DEBUG/Process-1] running the remaining ""atexit"" finalizers<NewLine>[INFO/Process-1] process exiting with exitcode 0<NewLine>/home/simon/anaconda3/lib/python3.6/multiprocessing/semaphore_tracker.py:146: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown<NewLine>  len(cache))<NewLine>[INFO/MainProcess] process shutting down<NewLine>[DEBUG/MainProcess] running all ""atexit"" finalizers with priority &gt;= 0<NewLine>[DEBUG/MainProcess] running the remaining ""atexit"" finalizers<NewLine>[Level 5/MainProcess] calling &lt;Finalize object, dead&gt;<NewLine>[Level 5/MainProcess] finalizer calling &lt;function rmtree at 0x7fab0b4bfea0&gt; with args ['/tmp/pymp-hfadelk_'] and kwargs {}<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hard to say. I did a quick search and came up with <a href=""https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming"" rel=""nofollow noopener"">https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming</a>, which might be useful here. It’s a warning that comes from deep in the guts of multiprocessing, so I’d start there.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/forhonourlx; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: April 15, 2019,  9:30pm; <NewLine> REPLY_DATE 2: April 15, 2019, 11:50pm; <NewLine> REPLY_DATE 3: April 15, 2019, 11:55pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
42140,Do nn.BatchNorm in distributed training default to be synchronized?,2019-04-10T03:12:59.329Z,0,887,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When using <code>torch.nn.parallel.DistributedDataParallel</code> to parallelize the network on multiple GPUs, do <code>nn.BatchNorm</code> become synchronized among GPUs ?<br/><NewLine>I suppose it is, because there is a <code>broadcast_buffers</code> flag in <code>DistributedDataParallel</code> defaulted to <code>True</code>.<br/><NewLine>Do anyone has any thoughts or confirmation on this ?</p><NewLine></div>",https://discuss.pytorch.org/u/roytseng,(Roy Tseng),roytseng,"April 10, 2019,  6:19am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The buffers in batch norm are synchronized between processes if <code>broadcast_buffers=True</code>, yes. This means that all processes get a <strong>copy</strong> of the buffers on process with rank 0. If you want to use a synchronized batch norm, check out <a href=""https://pytorch.org/docs/master/nn.html#torch.nn.SyncBatchNorm"" rel=""nofollow noopener""><code>nn.SyncBatchNorm</code></a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: April 15, 2019,  9:47pm; <NewLine> ",REPLY 1 LIKES: 3 Likes; <NewLine> 
41850,Torch.distributed.isend / irecv cannot transfer multiple tensors asyncly,2019-04-06T23:06:47.426Z,0,220,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I am testing p2p communication of torch.distributed.<br/><NewLine>I have 2 nodes, with gloo backend.<br/><NewLine>When I isend / irecv multiple tensors with diff ‘tag’, it doesn’t show the expected result.<br/><NewLine>Could somebody help me of the async p2p?</p><NewLine><h2>Node0:</h2><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.distributed as dist<NewLine>if __name__ == ""__main__"":<NewLine><NewLine>    rank = 0<NewLine><NewLine>    dist.init_process_group(backend=""gloo"",<NewLine>                            init_method='tcp://192.168.1.12:23457',<NewLine>                            world_size=2,<NewLine>                            rank=rank)<NewLine>                        <NewLine>    grads={'T0_grad':torch.zeros(2,2),'T1_grad':torch.zeros(2,2),'T2_grad':torch.zeros(2,2),'T3_grad':torch.zeros(2,2),'T4_grad':torch.zeros(2,2)}<NewLine>    if rank ==1:<NewLine>        tmp_tensor = torch.ones(2,2)<NewLine>        req = dist.isend(tmp_tensor,dst=0,tag=0)<NewLine>        print('rank',rank,' dist.isend(tmp_tensor):\n',tmp_tensor)<NewLine>        tmp_tensor2 = torch.ones(2,2)*4<NewLine>        req = dist.isend(tmp_tensor2,dst=0,tag=1)<NewLine>        print('rank',rank,' dist.isend(tmp_tensor2):\n',tmp_tensor2)<NewLine>        time.sleep(6)<NewLine>        <NewLine>    elif rank==0:<NewLine>        time.sleep(1)<NewLine>        i = 3<NewLine>        req = dist.irecv(grads['T'+str(i)+'_grad'],src=1,tag=0)<NewLine>        print('rank',rank,' dist.irecv(grads[T'+str(i)+'_grad]):\n',grads['T'+str(i)+'_grad'])<NewLine>        i = 4<NewLine>        req = dist.irecv(grads['T'+str(i)+'_grad'],src=1,tag=1)<NewLine>        print('rank',rank,' dist.irecv(grads[T'+str(i)+'_grad]):\n',grads['T'+str(i)+'_grad'])<NewLine></code></pre><NewLine><h2>Node1:</h2><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.distributed as dist<NewLine>if __name__ == ""__main__"":<NewLine><NewLine>    rank = 1<NewLine>    #... All else equal...<NewLine></code></pre><NewLine><h2>Result:</h2><NewLine><pre><code class=""lang-auto"">rank 1  dist.isend(tmp_tensor):<NewLine> tensor([[1., 1.],<NewLine>        [1., 1.]])<NewLine>rank 1  dist.isend(tmp_tensor2):<NewLine> tensor([[4., 4.],<NewLine>        [4., 4.]])<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">rank 0  dist.irecv(grads[T3_grad]):<NewLine> tensor([[0., 0.],<NewLine>        [0., 0.]])<NewLine>rank 0  dist.irecv(grads[T4_grad]):<NewLine> tensor([[0., 0.],<NewLine>        [0., 0.]])<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/forhonourlx,(Forhonourlx),forhonourlx,"April 6, 2019, 11:06pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You’re kicking off the send/recv operations, but don’t synchronize on completion. You’ll have to add calls to <code>req.wait()</code> to ensure the send/recv operations have actually completed. If you’re looking for <strong>synchronous</strong> send/recv instead, replace calls to <code>isend/irecv</code> by calls to <code>send/recv</code> (without the <code>i</code>).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: April 8, 2019,  4:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
41276,NCCL backend hangs when training over infiniband,2019-03-30T18:25:08.438Z,1,339,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I train my model across several machines. I have two machines which have gpus and infiniband cards. The networks is 1Gbit, Infiniband is 2x40Gbit. When I remove cards, and start training everything works, though slower than on one machine. When I run with infiniband setup, the system just hangs. There’s 100% GPU utilisation, wattage is 1/2 of maximum, and there’s very little network activity.</p><NewLine><p>Do you have any hints on how to proceed with finding out what’s wrong with training?</p><NewLine></div>",https://discuss.pytorch.org/u/Konstantin_Solomatov,(Konstantin Solomatov),Konstantin_Solomatov,"March 30, 2019,  6:25pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It might be a bios issue. I have a similar issue with 4Xv100 GPU that runs too slow. It turns out to be a bios setup problem. There is a bios adjustments which needs to be arranged as performance mode.<br/><NewLine>[<a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/v100-is-too-slow-for-training/34842/2"">V100 is too slow for training</a>](<a href=""http://v100"" rel=""nofollow noopener"">http://v100</a> topic)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/otutay"">@otutay</a> In your case it’s just too slow. In my case it plainly hangs.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/otutay; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Konstantin_Solomatov; <NewLine> ,"REPLY_DATE 1: March 30, 2019, 10:03pm; <NewLine> REPLY_DATE 2: March 30, 2019, 10:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
41277,DataParrallel Invalid Device Ids,2019-03-30T18:32:05.120Z,0,578,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I have access to 4 GPUs, and would like to use 2 for my task training with DataParrallel. From what I understand, instructing torch which GPUs to use is done by:</p><NewLine><p>task = nn.DataParrallel(model, device_ids=[1, 2]) to use GPUs 1 and 2.</p><NewLine><p>However, this yields the following assertion error: “raise AssertionError(“Invalid device id”)”.</p><NewLine><p>From my looking at previous answers (apologize I’m not sure how to link them, first post), this error indicates that I do not have GPUs 1 or 2 available to me. However, this does not appear to be the case.</p><NewLine><p>For instance, extracting the device properties in the python interpreter (same environment) works without issue:</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/f00ffa966fd13636d585cfba27c0062d71473386"" href=""https://discuss.pytorch.org/uploads/default/original/2X/f/f00ffa966fd13636d585cfba27c0062d71473386.png"" title=""Screen Shot 2019-03-30 at 2.29.56 PM.png""><img alt=""56%20PM"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/f/f00ffa966fd13636d585cfba27c0062d71473386_2_10x10.png"" height=""21"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/f/f00ffa966fd13636d585cfba27c0062d71473386_2_690x21.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/f/f00ffa966fd13636d585cfba27c0062d71473386_2_690x21.png, https://discuss.pytorch.org/uploads/default/optimized/2X/f/f00ffa966fd13636d585cfba27c0062d71473386_2_1035x31.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/f/f00ffa966fd13636d585cfba27c0062d71473386_2_1380x42.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screen Shot 2019-03-30 at 2.29.56 PM.png</span><span class=""informations"">1517×47 4.85 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>Is there something I’m missing with regards to assigning the model to specific device ids? Or recommendations on other methods through which I can try accomplish this task?</p><NewLine><p>Thank you very much for your time!</p><NewLine></div>",https://discuss.pytorch.org/u/gstoica,(George Stoica),gstoica,"March 30, 2019,  6:32pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you able to create tensors on each device?<br/><NewLine>Could you run this code and check the device ids?</p><NewLine><pre><code class=""lang-python"">x = torch.randn(10)<NewLine>for id in range(4):<NewLine>    y = x.to(id)<NewLine>    print(y.device)<NewLine></code></pre><NewLine><p>Also, note that the device ids start with 0, in case you would like to use the “first two” GPUs.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: March 30, 2019,  8:31pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
41149,CUDA Capability,2019-03-28T18:07:11.642Z,0,144,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!!</p><NewLine><p>Its possible to run pytorch distributed NN using GPUs with CUDA Capability &lt; 2.0 ??</p><NewLine><p>If its possible. How??</p><NewLine><p>Thanks <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=7"" title="":slight_smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/hpc-unex,(SergioUnex),hpc-unex,"March 28, 2019,  6:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! The lowest supported compute capability is 3.5 (the Tesla Kepler series).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: March 28, 2019,  6:36pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
40915,Question about init_method in distributed.init_process_group,2019-03-26T09:59:45.956Z,2,164,"<div class=""post"" itemprop=""articleBody""><NewLine><p>the default of init_method is <code>init_method='env://'</code> (using <code>nccl</code> backend)，if I want to run another code, what kind of url can I use?  thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/paul_c,,paul_c,"March 26, 2019,  9:59am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What do you mean with “run another code”? Do you want to use another distributed backend or different initialization method?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>For example: (ignore the export NGPUS)</p><NewLine><pre><code class=""lang-auto"">python -m torch.distributed.launch --nproc_per_node=$NGPUS run1.py<NewLine>python -m torch.distributed.launch --nproc_per_node=$NGPUS run2.py<NewLine></code></pre><NewLine><p>After try. just using the form like “env://tmp” can work. thank you.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see. The <code>env://</code> initialization method pulls all information it needs from the environment, so it will be isolated to a single run. If you use the <code>file://</code> initialization method, and any of the processes crashes, it may leave a stale file that prevents you from running something else until you delete it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/paul_c; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: March 26, 2019,  3:35pm; <NewLine> REPLY_DATE 2: March 27, 2019, 10:11am; <NewLine> REPLY_DATE 3: March 28, 2019,  2:33am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
40884,Data is stored in another server,2019-03-26T03:12:44.306Z,0,109,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My institution uses one server to store many different datasets while another one to conduct computation. How should we load the data from the data server, such that the loading of data will not become the bottleneck? Does Dataloader have considered this occasion?</p><NewLine></div>",https://discuss.pytorch.org/u/tangbohu,(Nolan),tangbohu,"March 26, 2019,  3:12am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Welcome!</p><NewLine><p>Check out the documentation for the <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" rel=""nofollow noopener""><code>torch.utils.data.DataLoader</code></a>. There is a keyword argument for the number of worker processes to spawn to load and preprocess data. You can use this to try and fix any data loading bottlenecks.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: March 26, 2019,  3:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
39304,Training hangs for a second at the beginning of each epoch,2019-03-08T15:15:56.290Z,1,230,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I used the command <code>watch -n0.1 nvidia-smi</code> to check the behavior of GPUs and<br/><NewLine>I found the utilization of GPU becomes 0% at the beginning of each epoch for a short period. I am just wondering if it is common that the training hangs for a second before the beginning of each epoch? Maybe the reason is that dataloader has to re-prepare data for the beginning of each epoch?</p><NewLine></div>",https://discuss.pytorch.org/u/musicpiece,,musicpiece,"March 8, 2019,  3:15pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s likely, yes. You can add some timing code in the body of your trainer to confirm this. For example, printing some output as soon as you get the first batch of input data can be used to prove/disprove the data loader being the cause of the slow start.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks Pieter. I believe it has some slow down at the beginning. I am just wondering what’s the reason causing such delay.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/musicpiece; <NewLine> ,"REPLY_DATE 1: March 12, 2019,  5:15pm; <NewLine> REPLY_DATE 2: March 22, 2019,  1:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
39739,Basic operations on multiple GPU-s,2019-03-13T12:09:28.917Z,4,335,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hy I just switched from tf and Im loving pytorch. Im would like to use parallel GPU computations on basic operation like matmul and torch.randn (im doing evolution strategies) is there any way to implement it in pytorch. Ive only seen examples that involve using the nn.DataParallel wrapper on models.</p><NewLine></div>",https://discuss.pytorch.org/u/Pataki_Marton,(Pataki Márton),Pataki_Marton,"March 13, 2019, 12:09pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you explain your use case a bit more?</p><NewLine><p>If you have separate computations, you could use each GPU to perform a single op:</p><NewLine><pre><code class=""lang-python"">res0 = torch.matmul(x.to('cuda:0'), y.to('cuda:0')<NewLine>res1 = torch.matmul(x.to('cuda:1'), y.to('cuda:1')<NewLine></code></pre><NewLine><p>Another approach would be to use <code>nn.DataParallel</code> in case you would like to send data chunks to each GPU and perform the same operations.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Most of my operations are sequential, what I would like to do is splitting up my arrays by the population dimension along multiple GPU-s. Can I use nn.DataParallel without using a Model class ?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""39739""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/pataki_marton/40/9948_2.png"" width=""20""/> Pataki_Marton:</div><NewLine><blockquote><NewLine><p>Can I use nn.DataParallel without using a Model class ?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes you can <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=7"" title="":slight_smile:""/> See <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/uneven-gpu-utilization-during-training-backpropagation/36117/14"">Uneven GPU utilization during training backpropagation</a> for an example wrapping the loss function with DataParallel</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Im using evolution methods, I dont have backpropagation and my loss function is not differentiabe. I want to parallelize sequential basic operations like torch.matmul.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>then you don’t need to call .backward()</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you elaborate please ?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Can you elaborate please ?</p><NewLine></blockquote><NewLine><p>I linked a post describing how to wrap an arbitrary loss function via DataParallel so that you can compute it via scattering the dataset onto multiple GPUs. Then you responded</p><NewLine><blockquote><NewLine><p>Im using evolution methods, I dont have backpropagation and my loss function is not differentiabe. I want to parallelize sequential basic operations like torch.matmul.</p><NewLine></blockquote><NewLine><p>and I mentioned that in that case, if you don’t want to use backpropagation, you don’t need to call <code>.backward()</code> on your loss function. In other words, I also wasn’t sure how your answer was related to wrapping a loss function in DataParallel. I mean, you can use it whether or not you want to do backpropagation, because DataParallel is not tied to backpropagation as far as I know.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Pataki_Marton; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/rasbt; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Pataki_Marton; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/rasbt; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Pataki_Marton; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/rasbt; <NewLine> ,"REPLY_DATE 1: March 13, 2019,  4:08pm; <NewLine> REPLY_DATE 2: March 19, 2019,  1:13pm; <NewLine> REPLY_DATE 3: March 19, 2019,  6:24pm; <NewLine> REPLY_DATE 4: March 20, 2019,  8:33am; <NewLine> REPLY_DATE 5: March 20, 2019,  3:44pm; <NewLine> REPLY_DATE 6: March 21, 2019, 11:45am; <NewLine> REPLY_DATE 7: March 21, 2019,  6:36pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> 
40075,Unable to use DataParallel + LSTM + batch_first=False + packed_sequence,2019-03-17T12:36:57.220Z,0,435,"<div class=""post"" itemprop=""articleBody""><NewLine><p>So the minimal example to produce the error:</p><NewLine><pre><code class=""lang-auto"">device = torch.device('cuda')<NewLine>lstm = nn.DataParallel(nn.LSTM(1, 5, batch_first=False), dim=1).to(device)<NewLine>batch_size = 30<NewLine>max_length = 20<NewLine>lengths=torch.tensor([max_length]*batch_size, device=device) <NewLine>inputs = torch.zeros(max_length, batch_size, 1, device=device)<NewLine>inputs_pack = torch.nn.utils.rnn.pack_padded_sequence(inputs, lengths, batch_first=False)<NewLine>outputs, hidden = lstm(inputs_pack)<NewLine></code></pre><NewLine><p>which ends up with an exception:</p><NewLine><blockquote><NewLine><p>Dimension out of range (expected to be in range of [-1, 0], but got 1)</p><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/shaform,(Yong Siang Shih),shaform,"March 17, 2019, 12:39pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you include the trace of the exception?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, the trace is quite long.</p><NewLine><p>But now I kind of know what the problem is. Basically, the <code>packed sequence</code> could not be parallelized because it could not be divided along a batch dimension.</p><NewLine><p>The solution is to put LSTM inside another module and pack the sequence inside the forward function of that module.</p><NewLine><pre><code class=""lang-auto"">RuntimeError                              Traceback (most recent call last)                                                                                   <NewLine>&lt;ipython-input-5-4bf0d856e1ee&gt; in &lt;module&gt;                                                                                                                    <NewLine>      6 inputs = torch.zeros(max_length, batch_size, 1, device=device)                                                                                        <NewLine>      7 inputs_pack = torch.nn.utils.rnn.pack_padded_sequence(inputs, lengths, batch_first=False)                                                             <NewLine>----&gt; 8 outputs, hidden = lstm(inputs_pack)                                                                                                                   <NewLine>                                                                                                                                                              <NewLine>/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)                               <NewLine>    487             result = self._slow_forward(*input, **kwargs)                                                                                             <NewLine>    488         else:                                                                                                                                         <NewLine>--&gt; 489             result = self.forward(*input, **kwargs)                                                                                                   <NewLine>    490         for hook in self._forward_hooks.values():                                                                                                     <NewLine>    491             hook_result = hook(self, input, result)                                                                                                   <NewLine>                                                                                                                                                              <NewLine>/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py in forward(self, *inputs, **kwargs)                       <NewLine>    137         if not self.device_ids:<NewLine>    138             return self.module(*inputs, **kwargs)<NewLine>--&gt; 139         inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)<NewLine>    140         if len(self.device_ids) == 1:<NewLine>    141             return self.module(*inputs[0], **kwargs[0])<NewLine><NewLine>/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py in scatter(self, inputs, kwargs, device_ids)<NewLine>    148<NewLine>    149     def scatter(self, inputs, kwargs, device_ids):<NewLine>--&gt; 150         return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)<NewLine>    151<NewLine>    152     def parallel_apply(self, replicas, inputs, kwargs):<NewLine><NewLine>/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py in scatter_kwargs(inputs, kwargs, target_gpus, dim)<NewLine>     33 def scatter_kwargs(inputs, kwargs, target_gpus, dim=0):<NewLine>     34     r""""""Scatter with support for kwargs dictionary""""""<NewLine>---&gt; 35     inputs = scatter(inputs, target_gpus, dim) if inputs else []<NewLine>     36     kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []<NewLine>     37     if len(inputs) &lt; len(kwargs):                                                                                                 <NewLine>                                                                                                               <NewLine>/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py in scatter(inputs, target_gpus, dim)<NewLine>     26     # None, clearing the cell                                                                          <NewLine>     27     try:                                                                                             <NewLine>---&gt; 28         return scatter_map(inputs)                                                                  <NewLine>     29     finally:                                                                                        <NewLine>     30         scatter_map = None                                                                                                                            <NewLine>                                                                                                                <NewLine>/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py in scatter_map(obj)                                      <NewLine>     13             return Scatter.apply(target_gpus, None, dim, obj) <NewLine>---&gt; 15             return list(zip(*map(scatter_map, obj)))                                                <NewLine>     16         if isinstance(obj, list) and len(obj) &gt; 0:                                                                                                    <NewLine>     17             return list(map(list, zip(*map(scatter_map, obj))))                               <NewLine>                                                                                                                                                              <NewLine>/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py in scatter_map(obj)<NewLine>     13             return Scatter.apply(target_gpus, None, dim, obj)                                                                                         <NewLine>     14         if isinstance(obj, tuple) and len(obj) &gt; 0:                                                 <NewLine>---&gt; 15             return list(zip(*map(scatter_map, obj)))                                                                                                  <NewLine>     16         if isinstance(obj, list) and len(obj) &gt; 0:                                                                                                    <NewLine>     17             return list(map(list, zip(*map(scatter_map, obj))))                                     <NewLine>                                                                                                                                                         <NewLine>/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py in scatter_map(obj)                                 <NewLine>     11     def scatter_map(obj):                                                                              <NewLine>     12         if isinstance(obj, torch.Tensor):                                                           <NewLine>---&gt; 13             return Scatter.apply(target_gpus, None, dim, obj)                                          <NewLine>     14         if isinstance(obj, tuple) and len(obj) &gt; 0:                                                     <NewLine>     15             return list(zip(*map(scatter_map, obj)))                                                <NewLine>                                                                                                            <NewLine>/lib/python3.6/site-packages/torch/nn/parallel/_functions.py in forward(ctx, target_gpus, chunk_sizes, dim, input)<NewLine>     87             # Perform CPU to GPU copies in a background stream                                      <NewLine>     88             streams = [_get_stream(device) for device in target_gpus]                                  <NewLine>---&gt; 89         outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)                    <NewLine>     90         # Synchronize with the copy stream                                                                                                            <NewLine>     91         if streams is not None:                                                            <NewLine>                                                                                                                <NewLine>/lib/python3.6/site-packages/torch/cuda/comm.py in scatter(tensor, devices, chunk_sizes, dim, streams)<NewLine>    146         ``devices``.                                                                                   <NewLine>    147     """"""                                                                                              <NewLine>--&gt; 148     return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))                        <NewLine>    149                                                                                                      <NewLine>    150                                                                                                     <NewLine>                                                                                                            <NewLine>RuntimeError: Dimension out of range (expected to be in range of [-1, 0], but got 1)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/shaform; <NewLine> ,"REPLY_DATE 1: March 19, 2019,  4:28pm; <NewLine> REPLY_DATE 2: March 19, 2019, 11:35pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
40133,Error occurred when executing loss.backward() in pytorch with distributed training,2019-03-18T06:33:22.009Z,0,341,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""./pretraining/run_pretraining.py"", line 440, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""./pretraining/run_pretraining.py"", line 384, in main<NewLine>    loss.backward()<NewLine>  File ""/data/anaconda/envs/bzheng_env/lib/python3.6/site-packages/torch/tensor.py"", line 102, in backward<NewLine>    torch.autograd.backward(self, gradient, retain_graph, create_graph)<NewLine>  File ""/data/anaconda/envs/bzheng_env/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 90, in backward<NewLine>    allow_unreachable=True)  # allow_unreachable flag<NewLine>  File ""/data/anaconda/envs/bzheng_env/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/apex/parallel/distributed.py"", line 310, in overlapping_backward_epilogue<NewLine>    ""This probably indicates some buckets were not allreduced."")<NewLine>RuntimeError: ('In epilogue, next_bucket (0) != num_buckets (1).  ', 'This probably indicates some buckets were not allreduced.')<NewLine></code></pre><NewLine><p>This error occurred when executing loss.backward() in pytorch with distributed training.</p><NewLine><p>It occurs even using only a single gpu while the program runs normally without distributed training.</p><NewLine><p>Have anyone met the same problem with me?</p><NewLine><p>The command I use to start the program is shown follows:</p><NewLine><pre><code class=""lang-auto"">python -u -m torch.distributed.launch --nproc_per_node=1 ./pretraining/run_pretraining.py ******<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/11116,(郑博),11116,"March 18, 2019,  6:33am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you distributedDataParallel(model) in your program?<br/><NewLine>You should change some code in your program when using torch.distributed.launch</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/11116"">@11116</a> I see you’re using Apex. The error message in the epilogue means that not all learnable parameters in your model had their gradients computed (i.e. they didn’t participate in the forward pass). This is possible if you use any type of control flow in your forward pass that excludes use of certain parameters. You can fix this in Apex by delaying all reduction until the very end of the backwards pass by using the <code>delay_allreduce</code> option. See <a href=""https://nvidia.github.io/apex/parallel.html"" rel=""nofollow noopener"">https://nvidia.github.io/apex/parallel.html</a> for more details.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zeal; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: March 19, 2019,  2:17am; <NewLine> REPLY_DATE 2: March 19, 2019,  4:23pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
39485,nn.DataParallel with input as a list not a tensor,2019-03-11T13:12:39.025Z,2,909,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,<br/><NewLine>I’m trying to train the  <a href=""https://github.com/yikang-li/FactorizableNet"" rel=""nofollow noopener"">FactorizableNet</a> using multi-GPUs.  My GPUs are Titan Xp.<br/><NewLine>This network can be trained with the batch size equal to the number of GPUs. As you can see in the below code snip, the main input of the model is a list of images of diffirent sizes, not a tensor. The dataloader and the collate function just return a list of tuples.</p><NewLine><pre><code class=""lang-auto"">for i, sample in enumerate(loader): # (im_data, im_info, gt_objects, gt_relationships)<NewLine>        # measure the data loading time<NewLine>        batch_size = len(sample['visual'])<NewLine>        # measure data loading time<NewLine>        meters['data_time'].update(time.time() - end, n=batch_size)<NewLine><NewLine>        input_visual = [item for item in sample['visual']]<NewLine>        target_objects = sample['objects']<NewLine>        target_relations = sample['relations']<NewLine>        image_info = sample['image_info']<NewLine>        # RPN targets<NewLine>        rpn_anchor_targets_obj = [[<NewLine>                np_to_variable(item[0],is_cuda=False, dtype=torch.LongTensor),<NewLine>                np_to_variable(item[1],is_cuda=False),<NewLine>                np_to_variable(item[2],is_cuda=False),<NewLine>                np_to_variable(item[3],is_cuda=False)<NewLine>                ] for item in sample['rpn_targets']['object']]<NewLine><NewLine>        # compute output<NewLine>        try:<NewLine>            raw_losses = model(<NewLine>                        im_data=input_visual, <NewLine>                        im_info=image_info,<NewLine>                        gt_objects=target_objects,<NewLine>                        gt_relationships=target_relations,<NewLine>                       rpn_anchor_targets_obj=rpn_anchor_targets_obj)<NewLine>.....<NewLine></code></pre><NewLine><p>The problem is that the time to complete a batch with batch size 1 is 1/8 of time for a batch of size 8. This means the training time is not reduced at all. Memory on the The GPU volatile utilization of all GPUs are very low.<br/><NewLine>I have asked the author, but it seems that he does not have time to answer questions.<br/><NewLine>What can I do now to reduce training time?<br/><NewLine>Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/caonv,(Cao Nguyen-Van),caonv,"March 11, 2019,  1:12pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Cao,</p><NewLine><p>With low memory utilization on all GPUs and a batch of 1 per GPU, you should try and increase the batch size per GPU. I took a brief look at the underlying code and it looks like it is explicitly hard coded to 1 per GPU (see <a href=""https://github.com/yikang-li/FactorizableNet/commit/65e050fb8fe51220ccee9ac2809f59c99cf540c2#diff-2c6533e02b5a87a414f8e7e602040c34R116"" rel=""nofollow noopener"">this commit</a>). If this can be modified to allow for &gt;1 then you’re likely to see some speedups.</p><NewLine><p>Good luck.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank for your reply and sorry for my bad writing as well.<br/><NewLine>The main point here is processing time for batch size 1 (1 GPUs) is 0.48s, and that for batch 8 (8 GPUs) is 3.7s. There is no parallel processing at all.</p><NewLine><p>I think feeding a list into the model is the main reason, then I would like to get help from you and other Pytorch experts.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It depends what the underlying implementation does. If it wraps nn.DataParallel, you should see a speedup. If it just processes the examples serially then not. When you run this, do you see GPU utilization on all the GPUs you expect to be participating (e.g. with nvidia-smi)?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I used <code>gpustat</code> to view the GPUs utilization every 2 second.<br/><NewLine>Most of running time, the GPUs utilization is very low. Sometime it jumps to a high value for a moment then go back to a idle.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>After carefully inspecting the code, I have found that the author didn’t use the nn.DataParallel but their own DataParallel.<br/><NewLine>Code for the DataParallel is below:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>from torch.nn import DataParallel as DataParallel_raw<NewLine>import numpy as np<NewLine><NewLine><NewLine>class DataParallel(DataParallel_raw):<NewLine>    """"""<NewLine>    we do the scatter outside of the DataPrallel.<NewLine>    input: Scattered Inputs without kwargs.<NewLine>    """"""<NewLine><NewLine>    def __init__(self, module):<NewLine>        # Disable all the other parameters<NewLine>        super(DataParallel, self).__init__(module)<NewLine><NewLine><NewLine>    def forward(self, *inputs, **kwargs):<NewLine>        assert len(inputs) == 0, ""Only support arguments like [variable_name = xxx]""<NewLine>        new_inputs = [{} for _ in self.device_ids]<NewLine>        for key in kwargs:<NewLine>            if key == 'im_data':<NewLine>                for i, device in enumerate(self.device_ids):<NewLine>                    new_inputs[i][key] = kwargs[key][i].to(device)<NewLine>            elif key.startswith(""rpn_anchor_targets""):<NewLine>                for i, device in enumerate(self.device_ids):<NewLine>                    new_inputs[i][key] = [item.to(device) for item in kwargs[key][i]]<NewLine>                <NewLine>            else:<NewLine>                assert isinstance(kwargs[key], list)<NewLine>                for i in range(len(self.device_ids)):<NewLine>                    new_inputs[i][key] = [kwargs[key][i], ]<NewLine>        nones = [[] for _ in self.device_ids]<NewLine>        replicas = self.replicate(self.module, self.device_ids)<NewLine>        outputs = self.parallel_apply(replicas, nones, new_inputs)<NewLine>return self.gather(outputs, self.output_device)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could add some timing information to those paths. For example, if all time is spent in <code>parallel_apply</code> you know there is something inside the model that’s causing this, instead of this custom <code>nn.DataParallel</code> wrapper. Alternatively, wait for the author to have time to debug this with you.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much,<br/><NewLine>I have just give working on that code.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/caonv; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/caonv; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/caonv; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/caonv; <NewLine> ,"REPLY_DATE 1: March 11, 2019,  4:00pm; <NewLine> REPLY_DATE 2: March 11, 2019,  4:33pm; <NewLine> REPLY_DATE 3: March 11, 2019,  5:48pm; <NewLine> REPLY_DATE 4: March 12, 2019,  3:47am; <NewLine> REPLY_DATE 5: March 12, 2019,  4:28am; <NewLine> REPLY_DATE 6: March 12, 2019,  4:34pm; <NewLine> REPLY_DATE 7: March 14, 2019, 10:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
39642,Optimizing CPU-GPU data transfer with nn.DataParallel,2019-03-12T15:08:37.178Z,1,1697,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I wrap my model in <code>nn.DataParallel</code>, it requires me to move the model to the GPU (via <code>.cuda()</code>). However, it also requires moving the inputs to the forward pass to CUDA (e.g., via <code>to(torch.device('cuda'))</code>). I saw <a href=""https://discuss.pytorch.org/t/cuda-vs-dataparallel-why-the-difference/4062/3?u=alsuhr"">this post</a> from 2017 mentioning DataParallel allows CPU inputs, but I’m running into issues when passing CPU tensors to the parallelized model (specifically, it’s saying it’s expecting cuda tensors but didn’t get cuda tensors). Maybe things changed since then.</p><NewLine><p>I want to avoid a case where I put my input on one GPU, then DataParallel has to take it off that GPU and distribute it on the rest, making it really slow.</p><NewLine><p>Are there any optimizations I can do to ensure that I’m not doing any unnecessary transfer between GPU and CPU? And is it correct that I have to pass cuda tensors to a parallelized module?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/alsuhr,,alsuhr,"March 12, 2019,  3:11pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you share an example snippet that shows this problem? Looking at <a href=""https://github.com/pytorch/pytorch/blob/743fdbdb192dd0f07d51d6f94f8b47984484b54f/torch/nn/parallel/_functions.py#L81"" rel=""nofollow noopener"">the code</a> that gets called there is an explicit mention of copying one big input tensor to all GPU devices you want to use.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>My network is a seq2seq net with three inputs:</p><NewLine><ul><NewLine><li>Word embeddings for the input sequence</li><NewLine><li>Word embeddings for the output sequence</li><NewLine><li>Image-like inputs for each token in the output sequence</li><NewLine></ul><NewLine><p>When I try to keep any of these three on CPU before the forward pass with DataParallel it complains that the tensors aren’t cuda tensors.</p><NewLine><p>E.g., here’s a snippet of my input encoder:</p><NewLine><pre><code class=""lang-auto""># Tensor containing sequence word type indices<NewLine>torch_indices: torch.Tensor = torch.zeros((len(examples), max(seq_lens)), dtype=torch.long)<NewLine>for idx, (sequence, sequence_length) in enumerate(zip(batch_indices, seq_lens)):<NewLine>    torch_indices[idx, :sequence_length] = torch.tensor(sequence, dtype=torch.long)<NewLine><NewLine># Now embed things<NewLine>batch_embeddings: torch.Tensor = self._embedder(torch_indices.to(DEVICE))<NewLine></code></pre><NewLine><p><code>len(examples)</code> gets my my batch size, <code>max(seq_lens)</code> gets me the maximum sequence length in the batch, and I iterate over indexed sequences (<code>batch_indices</code>) and modify values of the indices tensor indicating the indices of word types. I then put it on the device (in the case of single-GPU, this will be <code>DEVICE=torch.device('cuda')</code>; in the case of CPU, this will be <code>DEVICE=torch.device('cpu')</code>; and when I have more than one GPU this is by default also <code>DEVICE=torch.device('cpu')</code>. Perhaps I shouldn’t use <code>to</code> (and explicitly place on CPU) at all when using DataParallel?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just tested removing the call to <code>to(DEVICE)</code> in the snippet above, and it still gives an error that it’s expecting a cuda tensor (e.g., in the call to the embedder,</p><NewLine><p><code>RuntimeError: Expected object of type torch.cuda.LongTensor but found type torch.LongTensor for argument #3 'index'</code></p><NewLine><p><code>self._embedder</code> is an object of a class which extends <code>nn.Module</code>, and has an attribute of type <code>nn.Embedding</code>, which is on the GPU when I make this call to it.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I may be running into these problems given how I set my code up. I wanted the code to be adaptable for zero, one, or multiple GPUs, so I have a <code>ModelWrapper</code> class which keeps track of whether it’s being parallelized or not.</p><NewLine><p>Internal to that I have a member <code>model</code> which is the actual <code>nn.Module</code> being parallelized. It extends both <code>nn.Module</code> and an abstract model class (I use an abstract model class so that I can have multiple kinds of model architectures, but the assumptions are that all models in my project have both an encoder and a decoder, and also implement <code>forward</code> as they are modules).</p><NewLine><p>When initializing the <code>ModelWrapper</code>, I first create the <code>model</code> module. This object has attributes for the encoder and decoder (which are objects also extending <code>nn.Module</code>), and these attributes have attributes which are also modules, e.g., an embedding module, and so on. Once I create the <code>model</code>, if I have more than one GPU, I first wrap it in <code>nn.DataParallel</code>, and then put it on the GPU by calling <code>model.cuda()</code>.</p><NewLine><p>When I want to use the model during training, e.g., to compute the loss, I just call <code>model(inputs)</code> (do a forward pass), which returns a tensor.</p><NewLine><p>Perhaps the call to <code>nn.DataParallel</code> is not actually distributing the model parameters on the GPU correctly, given how I wrapped everything in classes?</p><NewLine><p>I did verify that all parameters in my model are on the GPU, and during training all three GPUs are being used by the process.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sounds like this error is expected here.</p><NewLine><p>The input encoder you posted earlier will always run on CPU, since you don’t pass a device kwarg to <code>torch.zeroes</code>. I’m assuming you’re calling the encoder from <em>within</em> the forward function. If you want inputs to be distributed to all GPUs, you need to call the wrapped module (the resulting model after wrapping it with <code>nn.DataParallel</code>) with the CPU side inputs, and <code>nn.DataParallel</code> will make sure the inputs are distributed accordingly. If you generate the encoded input from within the forward function, there is no place where <code>nn.DataParallel</code> could hook into and move them around.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I’m assuming you’re calling the encoder from  <em>within</em>  the forward function.</p><NewLine></blockquote><NewLine><p>Yes, this code is all in the <code>forward</code> function for the instruction encoder Module (the Module object is an attribute of another Module who is wrapped in <code>DataParallel</code>, and its forward call is called during the top-level forward call. It is very modular code!). The forward call for this model takes as input a list of string vectors <code>seqs (List[List[str]])</code>, and just before the call I posted, I convert them in to lists of ints:</p><NewLine><pre><code class=""lang-auto"">batch_indices: List[List[int]] = [[self._embedder.get_index(tok) for tok in seq] for seq in seqs]<NewLine>seq_lens: List[int] = [len(instruction) for instruction in instructions]<NewLine></code></pre><NewLine><p>I think I know what the issue is – does DataParallel require that the input to the forward calls be tensors so it can distribute them?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Another issue with assembling the batch in the forward function is that you end up doing the same work multiple times, depending on the number of GPUs you are using (the forward function is called N times).</p><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""39642""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/alsuhr/40/811_2.png"" width=""20""/> alsuhr:</div><NewLine><blockquote><NewLine><p>I think I know what the issue is – does DataParallel require that the input to the forward calls be tensors so it can distribute them?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes. If you pass the input batch (as a tensor) to <code>nn.DataParallel</code>, it will split along the batch dimension and distribute the smaller batches to participating GPUs.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/alsuhr; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/alsuhr; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/alsuhr; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/alsuhr; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: March 12, 2019,  4:24pm; <NewLine> REPLY_DATE 2: March 12, 2019,  8:23pm; <NewLine> REPLY_DATE 3: March 12, 2019,  8:28pm; <NewLine> REPLY_DATE 4: March 12, 2019,  8:41pm; <NewLine> REPLY_DATE 5: March 13, 2019, 10:18pm; <NewLine> REPLY_DATE 6: March 13, 2019, 10:31pm; <NewLine> REPLY_DATE 7: March 13, 2019, 10:40pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> 
39162,Calling all_reduce in forward of Module,2019-03-07T01:08:42.688Z,2,176,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a question regarding the use of collective functions such as all_reduce().</p><NewLine><p>I am working on a layer in which I would like to synchronize a value across processes. I have seen an <a href=""https://github.com/NVIDIA/apex/tree/master/apex/parallel"" rel=""nofollow noopener"">implementation</a> of synchronized batch norm that essentially does what I am looking for. In those layers, it seems that all_reduce is called from forward, but there is also an autograd function that defines backward behavior as well.</p><NewLine><p>Is that approach necessary for a Module that does <strong>not</strong> have trainable parameters? Can I just call all_reduce in the forward method of a Module or do I need to define it in an autograd function?</p><NewLine><p>Btw, the layer I’m working on looks like this:</p><NewLine><pre><code class=""lang-auto"">class BatchStandardDeviation(Module):<NewLine><NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine><NewLine>    def forward(self, x):<NewLine>        batch_size, _, height, width = x.size()<NewLine>        out = x - x.mean(dim=0, keepdim=True)  # Shape: B, C, H, W<NewLine>        out = torch.sqrt(out.pow(2.0).mean(dim=0, keepdim=False) + 1e-8)  # Shape: 1, C, H, W<NewLine>        out = out.mean().view(1, 1, 1, 1)<NewLine>        out = out.repeat(batch_size, 1, height, width)  # Shape: B, 1, H, W<NewLine>        return torch.cat([x, out], dim=1)<NewLine></code></pre><NewLine><p>It concats mini-batch statistics to each feature map. I would like to get those statistics from batches across all processes rather than just the local batch in one processes.</p><NewLine></div>",https://discuss.pytorch.org/u/mdlockyer,(Michael Lockyer),mdlockyer,"March 7, 2019,  1:12am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can call allreduce in the forward pass, but beware that if you have multiple of these layers, or other layers that need to call any <code>torch.distributed</code> functions, that the order they are called in needs to be identical across all workers. If you end up with any learnable parameters, consider the concerns I expressed on <a href=""https://github.com/pytorch/pytorch/pull/14267#pullrequestreview-204017347"" rel=""nofollow noopener"">this PR adding sync batch norm</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply! After reading over the PR for synced batch norm (really excited to see this functionality being baked into PyTorch btw, I think it’s a must have for proper distributed training), you seem to point out a potential deadlock when everything is running on the same process group. Is this a factor for my layer since no learnable parameters are used? I can always spin up a new process group for this layer to use if so.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mdlockyer; <NewLine> ,"REPLY_DATE 1: March 12, 2019,  4:47pm; <NewLine> REPLY_DATE 2: March 12, 2019, 10:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
39381,How to have a process wait for the other with `DistributedDataParallel`?,2019-03-09T22:49:38.275Z,1,618,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The first time my training scripts are run, the dataset is ‘compiled’ in the <code>Dataset</code> class. For instance, I do work often with medical data and then it is possible I add another dataset, and I want to cut this data into tiles of a specified shape. These files are then cached to the SSD, so next time the compilation phase is skipped.</p><NewLine><p>However, when I use <code>DistributedDataParallel</code> all processes will do this. I could check the rank of the process, and only allow rank == 0 to execute this, but then the other processes will crash because they will find an empty dataset. Is there a way I can tell the second process to wait before it starts training?</p><NewLine></div>",https://discuss.pytorch.org/u/jteuwen,(Jonas Teuwen),jteuwen,"March 9, 2019, 10:49pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could do one of two things:</p><NewLine><ol><NewLine><li>Segment your input dataset into <code>WORLD_SIZE</code> chunks and let every process preprocess its own subset of the dataset (this also gets you parallelization of the preprocessing). You can call <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.get_rank"" rel=""nofollow noopener""><code>torch.distributed.get_rank</code></a> to get the rank of the current process and use this to index into your input dataset.</li><NewLine><li>Like you say, force <code>rank == 0</code> to perform all preprocessing, and make all other workers wait for completion. You can do this with a call to <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.barrier"" rel=""nofollow noopener""><code>torch.distributed.barrier</code></a>. The default timeout for this call is 30 minutes. If preprocessing takes more time, you can tune the timeout through the <code>timeout</code> kwarg to <a href=""https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group"" rel=""nofollow noopener""><code>torch.distributed.init_process_group</code></a>.</li><NewLine></ol><NewLine><p>Are you running on a single machine or multiple machines?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Currently I am running on one machine, but, ideally, the solution would also be useful for both single and multiple machines.</p><NewLine><p>Thanks you for the suggestions, both seem reasonable, but it might be unclear beforehand how long the processing will take (also depends on the network speed and such) so, that would require a bit of tweaking for the timeout parameter. I also do not known beforehand how large the dataset will be precisely, as it can happen new samples have been added so it would be tricky to write a class which effectively splits the dataset into smaller ones, as that would require me to know how large it.</p><NewLine><p>So, perhaps, a combination of 1 and 2 would also work: I can make a rough split across <code>WORLD_SIZE</code>, and base this on <code>get_rank</code>. So, it can happen that some processes are finished earlier than the others. If I then call <code>torch.distributed.barrier()</code> at the end of the processing of the  dataset, this would have the effect that the preprocessing will be split  along the processes and all will wait until each one is done with their part. Do I understand this correctly?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, that’s correct. If the split is a rough split, you’ll still have to synchronize on the actual dataset size once preprocessing has completed. Distributed data parallel expects all workers to be called with an equal number of equally sized batches.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jteuwen; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: March 12, 2019,  5:00pm; <NewLine> REPLY_DATE 2: March 12, 2019,  6:23pm; <NewLine> REPLY_DATE 3: March 12, 2019,  6:27pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
39655,DataParallel prediction,2019-03-12T17:32:08.760Z,0,250,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I have an 8-GPU machine and successfully used DataParallel to train my network. At the end of the epoch I’m attempting to evaluate the model on a fairly large dev set, and I’d like to parallelize that operation.  For some reason only one GPU is utilized during my prediction operations.  Does setting model.eval() have some impact on how DataParallel works?</p><NewLine></div>",https://discuss.pytorch.org/u/Robert_Sim,(Robert Sim),Robert_Sim,"March 12, 2019,  5:32pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It looks like my issue is that the sample code I’m working with sets a new varialble mnetwork=DataParallel(network), and the training code operates on mnetwork but prediction code operates on network. Verifying the fix now.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Robert_Sim; <NewLine> ,"REPLY_DATE 1: March 12, 2019,  6:11pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> 
38134,Pytorch multiprocessing question,2019-02-25T00:50:10.791Z,0,141,"<div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li><NewLine><p>Is there a code example on sharing tensor among multiple processes using process pool? The doc above does not specify how to other than saying we need to move tensor to “shared memory” and use “queue”.</p><NewLine></li><NewLine><li><NewLine><p>Also, if I want all the processes to have access at the same time because it is garunteed that the processes only do read and not write, using semaphores and queue seems to be unnecessary. Is there a workaround to disable semaphore and queue when sharing tensors?</p><NewLine></li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/0xFFFFFFFF,(Joong Kun Lee),0xFFFFFFFF,"February 25, 2019,  1:07am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Don’t know of some example code. But if you use a queue per process in the pool and just send the same shared tensor to every single queue, all processes should have an identical tensor with the storage mapped to the same memory, so they’ll all see all writes to that storage. This should also address the second question you post. If you don’t want coordination between processes because the tensor is read only, you just send it, and access it, and not do anything else (or I’m missing something here, but this is my understanding here).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: March 12, 2019,  6:04pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
38276,How to create new `CUcontext` for different threads of the same process,2019-02-26T08:45:16.094Z,0,208,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve noticed the, when multiple threads run GPU pytorch code, the operations happen in serial ( i.e. the speed of having 10 threads of the same process each perform a GPU task is the same as having 1 thread perform 10 tasks ). However, when I spanw multiple processes, there is a significant and linear speedup for upto 3 processes.</p><NewLine><ol><NewLine><li><NewLine><p>Why do you think this could be? I hypothesize: by default, same <code>CUcontext</code> is used among multiple threads of the same process, where as different processes use different <code>CUcontext</code>.</p><NewLine></li><NewLine><li><NewLine><p>If my hypothesis in (1) is correct, then how can I manually create <code>CUcontext</code> for each thread in pytorch?</p><NewLine></li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/0xFFFFFFFF,(Joong Kun Lee),0xFFFFFFFF,"February 26, 2019,  8:45am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you using different CUDA streams for every thread?</p><NewLine><p>By default they will all use the same CUDA stream, which will serialize all operations.</p><NewLine><p>See <a href=""https://github.com/pytorch/pytorch/blob/master/c10/core/Stream.h"" rel=""nofollow noopener""><code>c10::Stream</code></a> if you’re developing against master, or <a href=""https://github.com/pytorch/pytorch/blob/v1.0.1/c10/Stream.h"" rel=""nofollow noopener"">this one</a> if you’re developing on 1.0.1.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: March 12, 2019,  5:19pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
39278,How to solve &ldquo;RuntimeError: Address already in use&rdquo; in pytorch distributed training?,2019-03-08T08:46:08.469Z,0,4222,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In the pytorch distributed training, I met a RuntimeError as following:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""visual/distribution_train.py"", line 387, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""visual/distribution_train.py"", line 67, in main<NewLine>    spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))<NewLine>  File ""/public/home/fengm/.conda/envs/fm_pytorch_env/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 167, in spawn<NewLine>    while not spawn_context.join():<NewLine>  File ""/public/home/fengm/.conda/envs/fm_pytorch_env/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 114, in join<NewLine>    raise Exception(msg)<NewLine>Exception:<NewLine><NewLine>-- Process 0 terminated with the following error:<NewLine>Traceback (most recent call last):<NewLine>  File ""/public/home/fengm/.conda/envs/fm_pytorch_env/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 19, in _wrap<NewLine>    fn(i, *args)<NewLine>  File ""/public/home/fengm/vehicle_reid/pytorch-pose-master/visual/distribution_train.py"", line 74, in main_worker<NewLine>    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,world_size=args.world_size, rank=args.rank)<NewLine>  File ""/public/home/fengm/.conda/envs/fm_pytorch_env/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 354, in init_process_group<NewLine>    store, rank, world_size = next(rendezvous(url))<NewLine>  File ""/public/home/fengm/.conda/envs/fm_pytorch_env/lib/python3.6/site-packages/torch/distributed/rendezvous.py"", line 95, in _tcp_rendezvous_handler<NewLine>    store = TCPStore(result.hostname, result.port, world_size, start_daemon)<NewLine>RuntimeError: Address already in use<NewLine></code></pre><NewLine><p>pytorch distributed initial setting is</p><NewLine><pre><code class=""lang-auto"">torch.multiprocessing.spawn(main_worker, nprocs=8, args=(8, args))<NewLine>torch.distributed.init_process_group(backend='nccl', init_method='tcp://110.2.1.101:8900',world_size=4, rank=0)<NewLine></code></pre><NewLine><p>There are 10 nodes with gpu mounted under the master node. The master node doesn’t have GPU. I used the <strong>slurm system</strong> to submit my task and my task is randomly assigned to worker node. ‘110.2.1.101’ in init_method is the master IP.  I don’t kown whether is the init_method wrong?<br/><NewLine>Is there anyone have met it before ? Who can help me to fix this bug?</p><NewLine></div>",https://discuss.pytorch.org/u/keira_feng,(Keira Feng),keira_feng,"March 8, 2019,  8:51am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What do you run in <code>main_worker</code> and where do the <code>world_size=4</code> and <code>rank=0</code> arguments to <code>init_process_group</code> come from? Are they hard coded, or do you list a single example?</p><NewLine><p>The error itself means that multiple processes try to bind to the address and port, so I assume you are trying to run multiple processes with <code>rank=0</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: March 12, 2019,  5:13pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
39379,Reducing metrics with DistributedDataParallel,2019-03-09T22:21:09.822Z,1,532,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In replacing <code>DataParallel</code> with <code>DistributedDataParallel</code>, I encountered that each epoch compute a number of metrics. While using <code>DataParallel</code> this still was fine, as everything was running in the same process.</p><NewLine><p>However, I would like to collect the metrics on the first GPU as well. There metric could either be computed on the GPU or CPU. As I understand, with <code>DistributedDataParallel</code> in case there are 2 GPUs,  3 processes are started (one to collect things). Then, <code>loss.backward()</code> should work as expected. But how do I do this for my metrics?</p><NewLine></div>",https://discuss.pytorch.org/u/jteuwen,(Jonas Teuwen),jteuwen,"March 9, 2019, 10:21pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Let me answer my own question. I think I got it right now. So I store my metrics in a dictionary <code>{'batch_metric_0': etc}</code>. Initially these were numpy arrays, but I converted the code to torch, I assume if this is not possible, you could otherwise dump to a pickle and use <code>ByteTensor</code>.</p><NewLine><p>Then you can collect them together by iterating over the dictionary like (you can use <code>torch.no_grad()</code>):</p><NewLine><pre><code class=""lang-auto"">for k in sorted(tensors_dict.keys()):<NewLine>    tensor_names.append(k)<NewLine>    all_tensors.append(tensors_dict[k])<NewLine>all_tensors = torch.stack(all_tensors, dim=0)<NewLine></code></pre><NewLine><p>Then, <code>torch.distributed.reduce(all_tensors, dst=0)</code> collects everything on device <code>0</code>. Then you need to divide by <code>WORLD_SIZE</code> only for device <code>0</code> (as the other processes are not collected, and we are not interested in those.</p><NewLine><p>Hope this is helpful to someone.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""39379""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/jteuwen/40/2230_2.png"" width=""20""/> jteuwen:</div><NewLine><blockquote><NewLine><p>As I understand, with <code>DistributedDataParallel</code> in case there are 2 GPUs, 3 processes are started (one to collect things). Then, <code>loss.backward()</code> should work as expected. But how do I do this for my metrics?</p><NewLine></blockquote><NewLine></aside><NewLine><p>There is no separate process to collect things. You are responsible for starting all processes, either through running them yourself from a terminal, through <code>torch.distributed.launch</code>, or with some other runner such as <code>mpirun</code>. The typical mode of execution is to use 1 GPU per process.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Great solution! Thanks for reporting back and sharing it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jteuwen; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pietern; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pietern; <NewLine> ,"REPLY_DATE 1: March 10, 2019,  8:16pm; <NewLine> REPLY_DATE 2: March 12, 2019,  4:50pm; <NewLine> REPLY_DATE 3: March 12, 2019,  4:51pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
