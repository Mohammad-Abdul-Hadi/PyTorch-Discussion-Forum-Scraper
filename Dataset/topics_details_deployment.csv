id,title,created_at,reply_count,views,description,creator_link,creator_name,creator_alias,post_date,post_likes,replies,repliers_links,reply_dates,reply_likes
41856,About the deployment category,2019-04-07T03:52:12.345Z,0,430,"<div class=""post"" itemprop=""articleBody""><NewLine><p>A category of posts focused on production usage of PyTorch. Mobile deployment is out of scope for this category (for now… <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/> )</p><NewLine><p>You should care if you are looking for:</p><NewLine><ul><NewLine><li>a serving solution to ship PyTorch models to production</li><NewLine><li>tools to manage models, work in an end to end managed workflow</li><NewLine><li>job orchestration tooling</li><NewLine><li>monitoring, debugging or visualization</li><NewLine><li>optimization</li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/jspisak,"(Facebook AI, Product Manager)",jspisak,"April 10, 2019, 11:29pm",5 Likes,,,,
97423,How to load an older model in PyTorch 1.6?,2020-09-24T22:34:47.177Z,2,51,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have some models trained in PyTorch 1.5 and saved by <code>torch.save(filename, model)</code>, instead of <code>torch.save(filename, model.state_dict())</code>.</p><NewLine><p>Now, I am not able to load them in PyTorch 1.6.<br/><NewLine>I get <code>torch.nn.modules.module.ModuleAttributeError: 'BatchNorm2d' object has no attribute '_non_persistent_buffers_set'</code></p><NewLine><p>What is the correct way to load such models?</p><NewLine><p>Following <strong><a href=""https://github.com/ultralytics/yolov5/issues/58"" rel=""nofollow noopener"">https://github.com/ultralytics/yolov5/issues/58</a></strong>, I tried this on the model and seemed to load without problem. But is this the right way?</p><NewLine><pre><code class=""lang-auto"">model = ModelDenseNet()<NewLine>model1 = torch.load(model_file)<NewLine>for k, m in model1.named_modules():<NewLine>   m._non_persistent_buffers_set = set()  # pytorch 1.6.0 compatability<NewLine>model.load_state_dict(model1.state_dict())<NewLine><NewLine></code></pre><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/blackberry,(Blackberry),blackberry,"September 24, 2020, 10:34pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I would consider this a workaround and would try to make sure your model is still working as intended.<br/><NewLine>E.g. you could create some tests using predefined data and compare the outputs, gradients, losses etc.<br/><NewLine>The recommended way would be to store the <code>state_dict</code> instead of the model directly.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The workaround seems to work (gives the same accuracy on the test set). I was wondering if there is a better way.</p><NewLine><p>Would be great if PyTorch provided and option to <code>torch.save/torch.load</code> to save load either the whole model or the state_dict.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""97423"" data-username=""blackberry""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/b/ecd19e/40.png"" width=""20""/> blackberry:</div><NewLine><blockquote><NewLine><p>Would be great if PyTorch provided and option to <code>torch.save/torch.load</code> to save load either the whole model or the state_dict.</p><NewLine></blockquote><NewLine></aside><NewLine><p>The recommended way of storing the <code>state_dict</code> should be working fine or are you seeing any issues with it? As explained in the <a href=""https://pytorch.org/docs/stable/notes/serialization.html"">Serialization semantics</a>, storing and loading the model directly could be breaking in various ways due to the way Python pickles the model.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/blackberry; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 26, 2020,  8:14am; <NewLine> REPLY_DATE 2: September 26, 2020,  7:47pm; <NewLine> REPLY_DATE 3: September 27, 2020,  1:58am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
60198,Unable to convert Model with Onnx,2019-11-06T17:42:48.910Z,0,278,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi i’m trying to convert this model to onnx.</p><NewLine><pre><code class=""lang-auto"">class onnx_Autoencoder(nn.Module):<NewLine> def __init__(self):<NewLine>        super(onnx_Autoencoder, self).__init__()<NewLine><NewLine>        self.Cov1=nn.Conv2d(1,32,3)<NewLine>        self.batchNorm1=nn.BatchNorm2d(32)<NewLine>        self.relu1 =nn.ReLU()<NewLine><NewLine>        self.Cov2 = nn.Conv2d(32, 64, 3)<NewLine>        self.batchNorm2 = nn.BatchNorm2d(64)<NewLine>        self.relu2 = nn.ReLU()<NewLine><NewLine><NewLine><NewLine>        #<NewLine>        self.Transpose1= nn.ConvTranspose2d(64, 32, 3)<NewLine>        self.batchNorm1Tra=nn.BatchNorm2d(32)<NewLine>        self.relu1Tra=nn.ReLU(inplace=True)<NewLine><NewLine>        self.Transpose2 = nn.ConvTranspose2d(32, 1, 3)<NewLine></code></pre><NewLine><p>Everything is working fine except when i add track_running_stats=False in batch norm layer</p><NewLine><pre><code class=""lang-auto""><NewLine>self.batchNorm1=nn.BatchNorm2d(32,track_running_stats=False)<NewLine></code></pre><NewLine><p>When i check the exported onnx model i get the following errror</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""onnx_check_model.py"", line 5, in &lt;module&gt;<NewLine>    onnx.checker.check_model(onnx_model)<NewLine>  File ""/home/velab/anaconda3/lib/python3.7/site-packages/onnx/checker.py"", line 86, in check_model<NewLine>    C.check_model(model.SerializeToString())<NewLine>onnx.onnx_cpp2py_export.checker.ValidationError: Node ()'s input 3 is marked single but has an empty string in the graph<NewLine><NewLine>==&gt; Context: Bad node spec: input: ""27"" input: ""batchNorm1Tra.weight"" input: ""batchNorm1Tra.bias"" input: """" input: """" output: ""30"" output: ""31"" output: ""32"" output: ""batch_norm_dead_output-72"" output: ""batch_norm_dead_output-73"" op_type: ""BatchNormalization"" attribute { name: ""epsilon"" f: 1e-05 type: FLOAT } attribute { name: ""momentum"" f: 0.9 type: FLOAT }<NewLine></code></pre><NewLine><p>Could someone provide me an idea on how to solve this?<br/><NewLine>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/erict,(eric),erict,"November 6, 2019,  5:45pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have the same issue. Any new findings to solve this problem?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve filed a bug report about this error: <a href=""https://github.com/pytorch/pytorch/issues/45333"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/45333</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Zeleni9; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jacek9; <NewLine> ,"REPLY_DATE 1: May 20, 2020,  9:00am; <NewLine> REPLY_DATE 2: September 25, 2020, 11:40am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
92781,How to do ONNX to TensorRT in INT8 mode?,2020-08-14T08:47:29.341Z,0,74,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello.</p><NewLine><p>I am working with the subject, PyTorch to TensorRT.</p><NewLine><p>With a tutorial, I could simply finish the process PyTorch to ONNX.<br/><NewLine>And, I also completed ONNX to TensorRT in fp16 mode.</p><NewLine><p>However, I couldn’t take a step for ONNX to TensorRT in <strong>int8 mode</strong>.<br/><NewLine>Debugger always say that `You need to do calibration for int8*.*</p><NewLine><p>Does anyone know how to do convert ONNX model to TensorRT int8 mode?</p><NewLine><p>Thank you in adavance</p><NewLine></div>",https://discuss.pytorch.org/u/GB_K,(GyeongBong),GB_K,"August 14, 2020,  8:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This thread says exporting quantized pytorch models to onnx is not yet supported.<br/><NewLine><aside class=""quote"" data-post=""14"" data-topic=""76884""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v4/letter/s/db5fbb/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/onnx-export-of-quantized-model/76884/14"">ONNX export of quantized model</a> <a class=""badge-wrapper bullet"" href=""/c/quantization/17""><span class=""badge-category-bg"" style=""background-color: #0088CC;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""This category is for questions, discussion and issues related to PyTorch’s quantization feature."">quantization</span></a><NewLine></div><NewLine><blockquote><NewLine>    General export of quantized models to ONNX isn’t currently supported. We only support conversion to ONNX for Caffe2 backend. This thread has additional context on what we currently support - <a href=""https://discuss.pytorch.org/t/onnx-export-of-quantized-model/76884/8"">ONNX export of quantized model</a><NewLine></blockquote><NewLine></aside><NewLine></p><NewLine><p>You can try quantizing after you export pytorch model to onnx by using <a href=""https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/README.md"" rel=""nofollow noopener"">onnxruntime</a>.</p><NewLine><p>Otherwise, you may want to check out if direct export from pytorch to tensorrt supports quantized models.<br/><NewLine></p><aside class=""onebox allowlistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""16""/><NewLine><a href=""https://github.com/NVIDIA-AI-IOT/torch2trt"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars1.githubusercontent.com/u/29582968?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/NVIDIA-AI-IOT/torch2trt"" rel=""nofollow noopener"" target=""_blank"">NVIDIA-AI-IOT/torch2trt</a></h3><NewLine><p>An easy to use PyTorch to TensorRT converter. Contribute to NVIDIA-AI-IOT/torch2trt development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/seungjun; <NewLine> ,"REPLY_DATE 1: September 25, 2020,  8:07am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
97359,How to set QNNPack parallelism?,2020-09-24T10:55:54.532Z,0,24,"<div class=""post"" itemprop=""articleBody""><NewLine><p>For our usecase we need to do the job on one thread (preferable the main one). For all other workflows we just call <code>torch.set_num_threads(1)</code> and this gets the job done <img alt="":slightly_smiling_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slightly_smiling_face.png?v=9"" title="":slightly_smiling_face:""/>.<br/><NewLine>But QNNPack (more specifically the quantized Convolution) doesn’t respect this setting,  since it uses <a href=""https://github.com/pytorch/pytorch/blob/v1.6.0/aten/src/ATen/native/quantized/cpu/qconv.cpp#L606"" rel=""nofollow noopener""><code>caffe2::pthreadpool_()</code></a>.<br/><NewLine>In our tests we get good results by making that function return <code>null</code>.</p><NewLine><p>Is there a way to set the <code>num_threads</code> of that pool without recompiling pytorch?<br/><NewLine>If not, what’s the minimal acceptable refactoring to make that configurable or respect the <code>torch.set_num_threads</code>?</p><NewLine></div>",https://discuss.pytorch.org/u/dbalchev,(Daniel Balchev),dbalchev,"September 24, 2020, 10:55am",,,,,
96853,CXXABI_* and GLIBCXX_* not found after building pre-abi Libtorch from source using gcc 5.4.0,2020-09-19T16:22:55.670Z,0,55,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>After building the pre-abi libtorch from source using gcc 5.4.0,<br/><NewLine>using the following steps</p><NewLine><pre><code class=""lang-auto"">conda activate pytorch-build<NewLine><NewLine># Setup compiler gcc 5.4.0 <NewLine>export CC=~/gcc-5.4.0/bin/gcc<NewLine>export CXX=~/gcc-5.4.0/bin/g++<NewLine>export LD_LIBRARY_PATH_BACKUP=$LD_LIBRARY_PATH<NewLine>export LD_LIBRARY_PATH=~/gcc-5.4.0/lib64:$LD_LIBRARY_PATH<NewLine><NewLine># Pytorch env variables<NewLine>export BUILD_CAFFE2_MOBILE=OFF BUILD_TEST=OFF USE_OPENMP=0 USE_CUDA=0 USE_CUDNN=0 USE_MKLDNN=0 USE_MKL=0 USE_NNPACK=0 USE_QNNPACK=0 USE_DISTRIBUTED=0 USE_FFMPEG=0 USE_FBGEMM=0 USE_PYTORCH_QNNPACK=0<NewLine>export PYTORCH_BUILD_VERSION=1.6.0 PYTORCH_BUILD_NUMBER=0<NewLine>export _GLIBCXX_USE_CXX11_ABI=0<NewLine><NewLine># Build libtorch<NewLine>cd ~/Repos/pytorch<NewLine>git checkout v1.6.0<NewLine>python setup.py clean<NewLine>mkdir -p build_libtorch &amp;&amp; cd build_libtorch<NewLine>python ../tools/build_libtorch.py<NewLine><NewLine># Disable gcc 5.4.0 to use 4.8.2<NewLine>unset CC &amp;&amp; unset CXX &amp;&amp; export LD_LIBRARY_PATH=$LD_LIBRARY_PATH_BACKUP<NewLine></code></pre><NewLine><p>Then after switching to an older gcc 4.8.2, the command <code>ldd libtorch.so</code> throws some errors :</p><NewLine><pre><code class=""lang-auto"">build/lib/libtorch.so: /.../gcc/4.8.2/lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /.../pytorch/build_libtorch/build/lib/libtorch_cpu.so)<NewLine>build/lib/libtorch.so: /.../gcc/4.8.2/lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /.../pytorch/build_libtorch/build/lib/libtorch_cpu.so)<NewLine>build/lib/libtorch.so: /.../gcc/4.8.2/lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by /.../pytorch/build_libtorch/build/lib/libtorch_cpu.so)<NewLine>build/lib/libtorch.so: /.../gcc/4.8.2/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /.../pytorch/build_libtorch/build/lib/libtorch_cpu.so)<NewLine>build/lib/libtorch.so: /.../gcc/4.8.2/lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /.../pytorch/build_libtorch/build/lib/libc10.so)<NewLine>build/lib/libtorch.so: /.../gcc/4.8.2/lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by /.../pytorch/build_libtorch/build/lib/libc10.so)<NewLine>build/lib/libtorch.so: /.../gcc/4.8.2/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /.../pytorch/build_libtorch/build/lib/libc10.so)<NewLine></code></pre><NewLine><p>I also tried with no success to hardcode the value of D_GLIBCXX_USE_CXX11_ABI to 0 in the <code>pytorch/CMakeLists.txt</code>:</p><NewLine><pre><code class=""lang-auto"">set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -D_GLIBCXX_USE_CXX11_ABI=0"")<NewLine># or<NewLine>add_definitions(-D_GLIBCXX_USE_CXX11_ABI=0)<NewLine></code></pre><NewLine><p>OS used: Centos 7</p><NewLine><p>The distribution pre-abi provided by pytorch: <a href=""https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.6.0%2Bcpu.zip"" rel=""nofollow noopener"">https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.6.0%2Bcpu.zip</a> doesn’t have this issue.</p><NewLine><p>Any idea how to solve this ?</p><NewLine><p>Regards,</p><NewLine></div>",https://discuss.pytorch.org/u/Marouane_Maatouk,(Marouane Maatouk),Marouane_Maatouk,"September 20, 2020,  9:55am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Your compiler is too old my friend. You need to bump down to v1.3.0 maximum as that’s the last one with only c++11 support. Above that it uses c++14 which your compiler does not support</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Charles, thanks for the reply.<br/><NewLine>But i’m not sure why i have to, since with my gcc 4.8.2 i don’t see the <code>CXXABI_* and GLIBCXX_* not found</code> error using the Pre-cxx11 ABI libtorch distribution <em>(v1.6.0)</em> <a href=""https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.6.0%2Bcpu.zip"" rel=""nofollow noopener"">libtorch-shared-with-deps-1.6.0%2Bcpu.zip</a> on Pytorch website which is compiled with gcc 7.3.1</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Solution: <a href=""https://github.com/pytorch/pytorch/issues/45064#issuecomment-696466277"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/45064#issuecomment-696466277</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/copythatpasta; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Marouane_Maatouk; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Marouane_Maatouk; <NewLine> ,"REPLY_DATE 1: September 20, 2020, 12:17pm; <NewLine> REPLY_DATE 2: September 20, 2020, 12:55pm; <NewLine> REPLY_DATE 3: September 23, 2020, 10:29am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
96803,Why can&rsquo;t self defined layers run on GPUs,2020-09-19T04:02:16.754Z,0,39,"<div class=""post"" itemprop=""articleBody""><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/291afe1d9bea7473a91e220edd457588a2b1432a"" href=""https://discuss.pytorch.org/uploads/default/original/3X/2/9/291afe1d9bea7473a91e220edd457588a2b1432a.png"" title=""20200919_084028""><img alt=""20200919_084028"" data-base62-sha1=""5RDphozze4tmc8N5Jxg7HbRFL2a"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/9/291afe1d9bea7473a91e220edd457588a2b1432a_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/9/291afe1d9bea7473a91e220edd457588a2b1432a_2_640x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/9/291afe1d9bea7473a91e220edd457588a2b1432a_2_640x500.png, https://discuss.pytorch.org/uploads/default/optimized/3X/2/9/291afe1d9bea7473a91e220edd457588a2b1432a_2_960x750.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/2/9/291afe1d9bea7473a91e220edd457588a2b1432a.png 2x"" width=""640""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">20200919_084028</span><span class=""informations"">1148×896 77.9 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/567194e62c59706fae2359e7188f75c309feabb7"" href=""https://discuss.pytorch.org/uploads/default/original/3X/5/6/567194e62c59706fae2359e7188f75c309feabb7.png"" title=""20200919_084106""><img alt=""20200919_084106"" data-base62-sha1=""ckIpq12b8KlG5yg9MAftueNcsGH"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/5/6/567194e62c59706fae2359e7188f75c309feabb7_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/5/6/567194e62c59706fae2359e7188f75c309feabb7_2_348x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/5/6/567194e62c59706fae2359e7188f75c309feabb7_2_348x500.png, https://discuss.pytorch.org/uploads/default/optimized/3X/5/6/567194e62c59706fae2359e7188f75c309feabb7_2_522x750.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/5/6/567194e62c59706fae2359e7188f75c309feabb7.png 2x"" width=""348""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">20200919_084106</span><span class=""informations"">593×852 64.4 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine>def forward( self, x ):<br/><NewLine>start = time.time()<br/><NewLine>out = self.Rconv2d( x )<br/><NewLine>cost = time.time()-start<br/><NewLine>out = self.maxpool1( out )<br/><NewLine>out = self.block1( out )<br/><NewLine>out = self.block2( out )<br/><NewLine>out = self.block3( out )<br/><NewLine>out = F.avg_pool2d( out, 7 )<br/><NewLine>out = out.view( out.size(0), -1 )<br/><NewLine>out = self.out_layer( out )<br/><NewLine>total_cost = time.time()-start</p><NewLine><p>print(cost/total_cost)<br/><NewLine>return out</p><NewLine><p>最后使用time模块查看该层执行效率，发现这个变换层模块占了模型99.9%的时间</p><NewLine></div>",https://discuss.pytorch.org/u/Mudou,(Mudou),Mudou,"September 19, 2020,  4:02am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>From Google translate:</p><NewLine><blockquote><NewLine><p>Finally, use the time module to check the execution efficiency of this layer and find that this transformation layer module accounts for 99.9% of the model’s time</p><NewLine></blockquote><NewLine><p>Could you please use an online translate service, so that we could support you? <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>Custom layers can run on the GPU. However, since CUDA operations are asynchronous, you would have to synchronize the code via <code>torch.cuda.synchronize()</code> before starting and stopping the timer.</p><NewLine><p>Also, please post code snippets by wrapping them into three backticks ```, which makes debugging easier.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks for your reply. l will check my code</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Mudou; <NewLine> ,"REPLY_DATE 1: September 19, 2020,  8:51am; <NewLine> REPLY_DATE 2: September 22, 2020,  5:49am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
84433,Object detection model deployment on CPU? Real-life solutions,2020-06-06T15:02:21.560Z,6,652,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am currently working on a small toy-project that involves object detection as one of the steps.<br/><NewLine>Currently, I amusing a pre-trained Faster-RCNN from <a href=""https://github.com/facebookresearch/detectron2"" rel=""nofollow noopener"">Detectron2</a> with ResNet-101 backbone.</p><NewLine><p>I wanted to make an MVP and show it to my colleagues, so I thought of deploying my model on a CPU machine. Detectron2 can be easily converted to Caffe2 (<a href=""https://detectron2.readthedocs.io/tutorials/deployment.html#use-the-model-in-c-python"" rel=""nofollow noopener"">DOCS</a>) for the deployment.</p><NewLine><p>I measured the inference times for GPU and the CPU mode. The inference time of the original Detectron2 model using PyTorch and GPU is around 90ms on my RTX2080 Ti.<br/><NewLine>The converted model on CPU (i9 9940X) and using Caffe2 API took 2.4s. I read that the Caffe2 is optimized for CPU inference, so I am quite surprised by the inference time on CPU. I asked about this situation on the Detectron2 GitHub and I got an answer like: „Expected inference time of R-50-FPN Faster R-CNN on a 8 core CPU is around 1.9s. Usually, ResNets are not used on CPUs.”</p><NewLine><p>There is my question, how such deep learning solutions for Computer Vision should be deployed in the real-world? I read somewhere that Facebook does use Caffe2 for their production models as CPUs are super cheap compared to GPUs (of course they are), but the difference in the running time is really huge. Using CPU for object detection seems useless for any real-time application.</p><NewLine><p>Should I use some other architecture, which does not include ResNet or Faster-RCNN (like YOLO v4/v3, SSD, etc.)? Or maybe the original GPU-trained model should be converted to ONNX and then used in other more CPU-optimized frameworks such as OpenVINO? Or there are some other tweaks such as quantization, pruning, etc. that are necessary to boost the CPU-inference efficiency of production models?</p><NewLine><p>I know that this is just a toy-project (for now at least), I can use GPU for inference (quite costly in real applications?) or just use other architecture (but sacrifice performance). I am just wondering what is the go-to solution for real-world systems.</p><NewLine><p>Thanks in advance for sharing your knowledge and experience. I will be grateful for any hints!</p><NewLine></div>",https://discuss.pytorch.org/u/bonzogondo,,bonzogondo,"June 6, 2020,  3:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am not a big expert in this area but just for letting know what I have discovered.</p><NewLine><p>Converting  PyTorch models to TorchScripts models is a good idea. You get a file that contains the arch class and weights. In the first run of the model gets optimized and after that is pretty fast. I am getting around <strong>1.98 s ± 42.1 ms</strong> in my Ryzen 1700 with Mask-RCNN Resnet 50 (the implementation of torchvision).</p><NewLine><p>I have also discovered <a href=""https://pytorch.org/docs/stable/quantization.html"" rel=""nofollow noopener"">quantization</a> . It optimizes models and make them able to run on a mobile or arm devices like Raspberry. However, after quantization the model losses some precision. Here you have an image explaining the process:</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/203846267f409c47172ee5a4228624792e7ac660"" href=""https://discuss.pytorch.org/uploads/default/original/3X/2/0/203846267f409c47172ee5a4228624792e7ac660.png"" title=""image""><img alt=""image"" data-base62-sha1=""4B1Qqtn7SJ1oTbmL7lN6IeCRhWo"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/0/203846267f409c47172ee5a4228624792e7ac660_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/0/203846267f409c47172ee5a4228624792e7ac660_2_443x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/0/203846267f409c47172ee5a4228624792e7ac660_2_443x500.png, https://discuss.pytorch.org/uploads/default/optimized/3X/2/0/203846267f409c47172ee5a4228624792e7ac660_2_664x750.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/2/0/203846267f409c47172ee5a4228624792e7ac660_2_886x1000.png 2x"" width=""443""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1396×1574 113 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>You could run this models on your computer not just mobile devices</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for sharing this.<br/><NewLine>I am about to check the quantization, but I did not think about using TorchScript. Thanks!</p><NewLine><p>Even though your running time of Mask-RCNN is better than my detector I feel still dissatisfied with inference time around 2s. I would be glad if the detector could run in around 1s; that would be acceptable for my settings, but I am not sure if this is feasible for Faster-RCNN &amp; CPU.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried something similar as you did and also began with a RCNN and also had to realize that the RCNN is  indeed slow. In my experience SSD and in particular SSDLite is accurate and fast.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, I think the RCNN is a dead-end for CPU inference. However, Facebook does use object detection algorithms on CPUs (that’s what I found on their GitHub and on some articles), but I am not sure if the use RCNN actually.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, have you successfully converted the MaskrCNN model in Torchvion to TorcScript?which version of pytorch did you use for it?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, you can find it in my repo:</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/WaterKnight1998/Deep-Tumour-Spheroid/blob/develop/notebooks/Mask-RCNN.ipynb"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/WaterKnight1998/Deep-Tumour-Spheroid/blob/develop/notebooks/Mask-RCNN.ipynb"" rel=""nofollow noopener"" target=""_blank"">WaterKnight1998/Deep-Tumour-Spheroid/blob/develop/notebooks/Mask-RCNN.ipynb</a></h4><NewLine><pre><code class=""lang-ipynb"">{<NewLine> ""cells"": [<NewLine>  {<NewLine>   ""cell_type"": ""code"",<NewLine>   ""execution_count"": 1,<NewLine>   ""metadata"": {},<NewLine>   ""outputs"": [<NewLine>    {<NewLine>     ""name"": ""stderr"",<NewLine>     ""output_type"": ""stream"",<NewLine>     ""text"": [<NewLine>      ""/home/david/anaconda3/envs/Deep-Tumour-Spheroid/lib/python3.7/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n"",<NewLine>      ""  warnings.warn(warning.format(ret))\n""<NewLine>     ]<NewLine>    }<NewLine>   ],<NewLine>   ""source"": [<NewLine>    ""from fastai.basics import *\n"",<NewLine>    ""from fastai.vision import models\n"",<NewLine>    ""from fastai.vision.all import *\n"",<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/WaterKnight1998/Deep-Tumour-Spheroid/blob/develop/notebooks/Mask-RCNN.ipynb"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/bonzogondo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/FabianSchuetze; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/bonzogondo; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/yang-gis; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/WaterKnight; <NewLine> ,"REPLY_DATE 1: June 6, 2020,  3:15pm; <NewLine> REPLY_DATE 2: June 6, 2020,  3:56pm; <NewLine> REPLY_DATE 3: June 18, 2020, 10:40am; <NewLine> REPLY_DATE 4: June 18, 2020,  1:59pm; <NewLine> REPLY_DATE 5: September 17, 2020,  6:47am; <NewLine> REPLY_DATE 6: September 19, 2020,  3:30pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> 
67594,Densenet in the browser,2020-01-25T00:38:24.855Z,0,144,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I have a densenet model and I want to run it in the browser. Does anyone know any solutions I can use?</p><NewLine><p>I tried onnx.js and I get the following error:</p><NewLine><pre><code class=""lang-auto"">Uncaught (in promise) TypeError: cannot resolve operator 'Pad' with opsets: ai.onnx v9<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/SharanSMenon,(Sharan S Menon),SharanSMenon,"January 25, 2020, 12:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am converted my ConvNet model using opset v11 which I think is the latest but still I get the same error. It would be great if someone can help.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/iamharshit; <NewLine> ,"REPLY_DATE 1: September 19, 2020,  2:40am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
96298,Implement about code,2020-09-15T03:04:58.609Z,0,33,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello</p><NewLine><p>I’m a student who studies deep learning with Pytorch.</p><NewLine><p>I have a question, but I want to learn only a specific character dataset (for example, alphabet V) when learning EMNIST dataset with Pytorch, but I want to know how to implement this with code.</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/Walter_sh,(고승형 ­),Walter_sh,"September 15, 2020,  3:04am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could manipulate the targets such that your target class uses the class index <code>1</code> while all other classes are assigned the class index <code>0</code>.<br/><NewLine>Afterwards you can treat it as a binary classification and could use e.g. <code>nn.BCEWithLogitsLoss</code> as your criterion.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 17, 2020,  7:39am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
95550,CUDA is not available,2020-09-08T12:06:15.478Z,1,59,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I installed CUDA 10.1.105 and reinstall NVIDIA driver 430.64 , cuDnn 7.6.5 even Anaconda because of no available</p><NewLine><pre><code class=""lang-auto"">python -m torch.utils.collect_env<NewLine>Collecting environment information...<NewLine>PyTorch version: 1.3.1<NewLine>Is debug build: No<NewLine>CUDA used to build PyTorch: Could not collect<NewLine><NewLine>OS: Ubuntu 18.04.4 LTS<NewLine>GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0<NewLine>CMake version: version 3.10.2<NewLine><NewLine>Python version: 3.7<NewLine>Is CUDA available: No<NewLine>CUDA runtime version: 10.1.105<NewLine>GPU models and configuration: GPU 0: GeForce RTX 2080 Ti<NewLine>Nvidia driver version: 430.64<NewLine>cuDNN version: Probably one of the following:<NewLine>/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5<NewLine>/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7.6.5<NewLine><NewLine>Versions of relevant libraries:<NewLine>[pip3] numpy==1.19.1<NewLine>[pip3] torch==1.3.1<NewLine>[pip3] torchvision==0.4.2<NewLine><NewLine></code></pre><NewLine><p>But till</p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; import torch<NewLine>&gt;&gt;&gt; print(torch.cuda.is_available())<NewLine>False<NewLine>&gt;&gt;&gt; exit()<NewLine></code></pre><NewLine><p>Please, I need help ,  solution or sth</p><NewLine></div>",https://discuss.pytorch.org/u/ImportYang_Howard,(ImportYang Howard),ImportYang_Howard,"September 8, 2020, 12:06pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You don’t need local CUDA and cudnn installations, if you are using the conda binary.<br/><NewLine>Only the NVIDIA driver will be used.<br/><NewLine>Based on the output you are seeing, I assume you’ve installed the CPU binary.<br/><NewLine>Could you reinstall PyTorch with the desired CUDA version using the install command from <a href=""https://pytorch.org/get-started/locally/"">here</a> and post the install log, if it’s still not working?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thnaks. It’s working. I find this problem occur because of  uncorrelation of  Pytorch , cuda and cudnn.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ImportYang_Howard; <NewLine> ,"REPLY_DATE 1: September 17, 2020,  1:11am; <NewLine> REPLY_DATE 2: September 17, 2020, 12:58am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
93981,Dynamic Input for ONNX.js using a Pytorch trained model,2020-08-25T00:38:41.121Z,0,54,"<div class=""post"" itemprop=""articleBody""><NewLine><p>So I’ve got this autoencoder that I’ve trained and now I wanna deploy it to a website.<br/><NewLine>I came across ONNX.js and liked it because it can run natively on the browser(Saving me my much needed Student cloud credits <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/>)</p><NewLine><p>However I noticed that onnx requires a dummy input so that it can trace the graph and this requires a fixed input size.</p><NewLine><pre><code class=""lang-auto"">dummy = torch.randn(1, 3, 1920, 1080)<NewLine>torch.onnx.export(transformer, dummy, save_path, opset_version=11)<NewLine></code></pre><NewLine><p>I’m almost certain that on the ONNX.js side I need the input to be the same shape.<br/><NewLine>Is there anyway around this, because I need dynamic shape inputs.<br/><NewLine>Or should I just used Flask and host the model?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/Ashraf_Ali,(Ashraf Ali),Ashraf_Ali,"August 25, 2020, 12:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m by no means an expert, but I think you can use the <code>dynamic_axes</code> optional argument to <code>onnx.export</code></p><NewLine><p>In the tutorial <a href=""https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html#running-the-model-on-an-image-using-onnx-runtime"" rel=""nofollow noopener"">here</a> (about a quarter of the way down) the example uses the dynamic_axes argument to have a dynamic batch size:</p><NewLine><pre><code class=""lang-auto"">                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes<NewLine>                                'output' : {0 : 'batch_size'}})<NewLine></code></pre><NewLine><p>I <em>assume</em> that can also be done for the other axes, but I haven’t tried it - my use case has images which are guaranteed to be the same size, so I don’t need it.</p><NewLine><p>I have seen some discussion that would suggest this is not the case though, so maybe someone with more experience can weigh in?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/afg1; <NewLine> ,"REPLY_DATE 1: September 15, 2020,  3:49pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
96125,Use Torchtext model in Flaskapp,2020-09-13T13:50:39.288Z,0,22,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello Pytorch Community</p><NewLine><p>I am new to this whole framework and decided to do research on different models for Sentiment Analysis and their accuracy. For my pytorch models, I used this github repo: <a href=""https://github.com/bentrevett/pytorch-sentiment-analysis"" rel=""nofollow noopener"">https://github.com/bentrevett/pytorch-sentiment-analysis</a>.</p><NewLine><p>I followed the <a href=""https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb"" rel=""nofollow noopener"">LSTM</a>, <a href=""https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb"" rel=""nofollow noopener"">CNN</a> and <a href=""https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb"" rel=""nofollow noopener"">Transformer</a> tutorial.</p><NewLine><p>I managed to benchmark the models and got my research done but I also want to build a flask app to allow people to test them out and compare them one on one. But I could not find good resources for deployment of torchtext models that utilize embeddings, vocab and so on.  As I am not very experienced with Pytorch, it is also hard for me to transfer tutorials that deploy normal pytorch models to my torchtext model.</p><NewLine><p>Can anybody help me out with this?</p><NewLine></div>",https://discuss.pytorch.org/u/EmreTokyuez,(Emre Tokyuez),EmreTokyuez,"September 13, 2020,  1:50pm",,,,,
95954,Torch.rfft() with onnx,2020-09-11T17:23:16.288Z,1,28,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Question about onnx exporting a model which uses torch.rfft() in the forward function. How would this work as ONNX opset doesn’t support FFT yet.</p><NewLine><p>Do let me know. Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/hashbrown,,hashbrown,"September 11, 2020,  5:23pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think you’ll just get a funny-looking operator (aten::…) in ONNX.<br/><NewLine>The other question is, of course, which backend would than execute your operator.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Would use x86_64 CPU to run this using onnxruntime.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>From a cursory look, it seems to be a contributed operator shown as <code>com.microsoft.rfft</code> there. Maybe you can pretend that aten::rfft is a custom op and <a href=""https://pytorch.org/docs/stable/onnx.html#custom-operators"" rel=""nofollow noopener"">register a custom ONNX operator for it</a>.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hashbrown; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: September 11, 2020,  6:36pm; <NewLine> REPLY_DATE 2: September 11, 2020,  6:59pm; <NewLine> REPLY_DATE 3: September 11, 2020,  7:14pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
94751,Raspberry Arm64 binaries,2020-09-01T02:02:25.408Z,4,106,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I have had a few failures, trials and errors with conda and the compile and just wondered if there are any completed wheels for Pi-OS-64</p><NewLine><p>Its available even if Beta in desktop &amp; lite form <a href=""https://downloads.raspberrypi.org/raspios_lite_arm64/images/raspios_lite_arm64-2020-08-24/"" rel=""nofollow noopener"">https://downloads.raspberrypi.org/raspios_lite_arm64/images/raspios_lite_arm64-2020-08-24/</a></p><NewLine><p>Anyone got any links, tutorials or even an official method as that would be so sweet.</p><NewLine><p>I have failed on debian Arm64 &amp; Ubuntu following some gists I found and the length of time it takes is killing my resolve.</p><NewLine><p>Are there any binaries anywhere or would anyone post me to a recent and known working install for raspios_arm64</p><NewLine><p>Thanks all</p><NewLine><p><img alt="":slight_smile:"" class=""emoji only-emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/rolyan_trauts,(rolyan trauts),rolyan_trauts,"September 1, 2020,  2:02am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Heya, I have a rather old PyTorch build for Debian 64 bit (<a href=""http://mathinf.com/pytorch/arm64/"" rel=""nofollow noopener"">http://mathinf.com/pytorch/arm64/</a>), but I’ve ordered a 8GB Raspberry Pi to ease some of the pain of building it and will see to having PiOs-64 builds.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I should actually grab my pi4 (2gb) as why I am trying to build on a Pi3A+ is a mystery to be honest and is the definition of pain <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>There is a lite version now of the 64bit of raspios <a href=""https://downloads.raspberrypi.org/raspios_lite_arm64/images/raspios_lite_arm64-2020-08-24/"" rel=""nofollow noopener"">https://downloads.raspberrypi.org/raspios_lite_arm64/images/raspios_lite_arm64-2020-08-24/</a> even if still a beta its likely to end up a datum distro for Pis.</p><NewLine><p>Debian is fine but guess you will be building 1.6 so might wait.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well, so for the packages there, I used a 4GB Raspberry. Crucially, I did not set up swap. (For good reason!)<br/><NewLine>I just ran <code>python3 setup.py bdist_wheel</code> as usual. This builds on 4 cores. When compiling the extremely large autogenerated files (the ones wrapping ATen into autograd and Python), it runs out of memory and gets killed. Then I ran cmake manually for a bit with -j1 (1 process at a time) and when I got bored switched to -j4 again. Maybe with the 8GB I can just run -j2 throughout or something.</p><NewLine><p>I’ll look at building with the Raspberry OS (64 bit Raspbian wasn’t yet available a year ago). Thank you for handing me an excuse to get the 8GB one. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/> It is scheduled to arrive tomorrow, so we’ll see if I have packages over the weekend.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I usually install my zram-swap utility but go shy on the compression ratio and then add a backup decent swap drive and get most done in memory.</p><NewLine><aside class=""onebox allowlistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""32""/><NewLine><a href=""https://github.com/StuartIanNaylor/zram-config"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""400"" src=""https://avatars0.githubusercontent.com/u/2593098?s=400&amp;v=4"" width=""400""/><NewLine><h3><a href=""https://github.com/StuartIanNaylor/zram-config"" rel=""nofollow noopener"" target=""_blank"">StuartIanNaylor/zram-config</a></h3><NewLine><p>Complete zram config utility for swap, directory &amp; log. Usefull for IoT / maker projects for reducing SD, Nand and Emmc block wear via log operations. Uses Zram to minimise precious memory foot...</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine><aside class=""onebox allowlistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""32""/><NewLine><a href=""https://github.com/StuartIanNaylor/zram-swap-config"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""400"" src=""https://avatars0.githubusercontent.com/u/2593098?s=400&amp;v=4"" width=""400""/><NewLine><h3><a href=""https://github.com/StuartIanNaylor/zram-swap-config"" rel=""nofollow noopener"" target=""_blank"">StuartIanNaylor/zram-swap-config</a></h3><NewLine><p>Replacement for broken zram-config-0.5 package. Contribute to StuartIanNaylor/zram-swap-config development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>They where just a bit of a protest about the big distros actually getting a decent utility available but seem to of been popular.<br/><NewLine>But its all personal choice and opinion but seen as you got a 8gb I will wait <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/><br/><NewLine>Did the official lite of RaspiOS 64 tempt you?</p><NewLine><p>It would be amazing if we could get an official wheel for RaspiOS 64 though.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I now have <a href=""http://mathinf.com/pytorch/arm64/"" rel=""nofollow noopener"">PyToch 1.6 / TorchVision 0.7 for 64 bit Raspberry OS</a> posted on my page linked above. Compiling PyTorch on the 8GB Raspberry Pi worked out of the box (no swap needed, no memory problems) and took a bit less than 4 hours (the <code>.git</code> checkout and the <code>whl</code> have timestamps exactly 4 hours apart, but I did the installation of requirements in between, too).<br/><NewLine>I hope you enjoy it.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Cheers Tom.</p><NewLine><p>Many thanks.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/rolyan_trauts; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/rolyan_trauts; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/rolyan_trauts; <NewLine> ,"REPLY_DATE 1: September 2, 2020, 10:28am; <NewLine> REPLY_DATE 2: September 3, 2020,  7:19am; <NewLine> REPLY_DATE 3: September 3, 2020, 10:03am; <NewLine> REPLY_DATE 4: September 4, 2020,  1:34am; <NewLine> REPLY_DATE 5: September 7, 2020, 12:00pm; <NewLine> REPLY_DATE 6: September 8, 2020, 12:58pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> 
95476,Pytorch inference is slow inside Flask API,2020-09-07T17:41:38.744Z,5,75,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I encounter a problem with the inference speed between inference in main thread and inside Flask API enpoint (which is inside another thread). The inference speed is about 10 times slower.<br/><NewLine>Do you have any idea about this problem and any solution?</p><NewLine></div>",https://discuss.pytorch.org/u/voqtuyen,(Voqtuyen),voqtuyen,"September 7, 2020,  5:41pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Do you know if flask is reducing the priority of these worker threads maybe?</p><NewLine><p>Another possibility is that these worker threads don’t use multithreading to process your model?<br/><NewLine>Or that so many of these threads use multithreading that you over-use the CPU which leads to slow down?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have tested it with a single API running pytorch model inference. The speed is also lower than that running in main thread, but much less worse (2 times slower). That is, multiple threads running could take up CPU usage. However, in the case of a single API, i did not figure out how to improve it</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you run your PyTorch model in Python or have you JITed it to TorchScript?<br/><NewLine>We discuss this a bit in <a href=""https://pytorch.org/deep-learning-with-pytorch"" rel=""nofollow noopener"">Chapter 15 of our book</a>, if you run your model through the JIT - even from Python - you avoid the infamous Python GIL for all intermediates, which may give you an edge in multithreaded deployment scenarios.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I run Pytorch model in Python</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I would suspect that JITing the model would already help, then.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your help, i have tried JIT the model and it solved the inference speed problem. I am currently verifying the performance to see if there is any degradation?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/voqtuyen; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/voqtuyen; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/voqtuyen; <NewLine> ,"REPLY_DATE 1: September 7, 2020,  8:08pm; <NewLine> REPLY_DATE 2: September 8, 2020,  3:56am; <NewLine> REPLY_DATE 3: September 8, 2020,  9:54am; <NewLine> REPLY_DATE 4: September 8, 2020,  6:43am; <NewLine> REPLY_DATE 5: September 8, 2020,  9:51am; <NewLine> REPLY_DATE 6: September 8, 2020,  9:53am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
95356,Model loading from Colab,2020-09-06T13:48:37.561Z,0,56,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi it’s a small problem I am facing with respect to model saving and loading.<br/><NewLine>I am training a simple classification model in Google Colab. As per the instructions from <a href=""https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html</a><br/><NewLine>I am saving the model as</p><NewLine><pre><code class=""lang-auto"">torch.save(net.cpu().state_dict(),""/folder/model.pt"")<NewLine></code></pre><NewLine><p>After this, I am downloading the saved file from colab. While trying to load it(<strong>ONLY FOR INFERENCE</strong>)in my local machine as</p><NewLine><pre><code class=""lang-auto"">model = NetClass()<NewLine>model.load_state_dict(torch.load(""/local_folder/model.pt"")<NewLine></code></pre><NewLine><p>It does not load the model but <strong>returns</strong> an error as</p><NewLine><pre><code class=""lang-auto"">RuntimeError: ./mymodel/model.pt is a zip archive (did you mean to use torch.jit.load()?)<NewLine></code></pre><NewLine><p>I tried unzipping the downloaded file manually and it has a directory structure which is expected. Now I don’t know where to proceed.</p><NewLine><p>I tried the same method in the same colab file itself and <strong>IT WORKED</strong>. But in local file it is <strong>NOT WORKING</strong><br/><NewLine>Any kind of help and suggestions would be highly appreciated!<br/><NewLine>Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/Ajinkya_Ambatwar,(Ajinkya Ambatwar),Ajinkya_Ambatwar,"September 6, 2020,  1:52pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you using different PyTorch versions in Colab and on your local machine?<br/><NewLine>If so, I guess your local installation might be older so could you update it?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 25, 2020, 10:35am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
87967,Pytorch inference on Raspberry pi?,2020-07-04T18:20:23.871Z,2,320,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m working on an object detection model to be deployed on Raspberry pi.<br/><NewLine>I usually stick to tflite for this use case. But I really want to use PyTorch if possible.</p><NewLine><p>I’ll be running inference on Raspberry pi. Currently, what is the best way to go about doing this?</p><NewLine><p>Will torchscript help?<br/><NewLine>Most discussions in the forums give an arm_whl file to setup PyTorch in raspberry pi, is this the best method  - <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/installing-pytorch-on-raspberry-pi-3/25215/21"">Installing pytorch on raspberry pi 3</a>?</p><NewLine><p>Any resource on this topic will be very much appreciated. Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Gautham_Kumaran,(Gautham Kumaran),Gautham_Kumaran,"July 4, 2020,  6:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Gautham,</p><NewLine><p>in my opinion it is the best to install via whl file, otherwise it takes ages to be installed.</p><NewLine><p>Might be a good idea to install pytorch &gt;= 1.5.0, because you (already mentioned) torchscript. If it is not possible on raspi3 you can also go for flask and a wdsgi as backend.</p><NewLine><p>A great pretrained transformer is the DETR object detector and you can already download it from facebooks github for pytorch.<br/><NewLine>Others like faster r-cnn might be a little to huge for a raspi3.</p><NewLine><p>Hope it helps &amp; have fun!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/maltequast"">@maltequast</a> , I’ll proceed with the whl file and pytorch &gt;= 1.5.0.</p><NewLine><p>Regarding performance, is there any benchmarking data for pytorch vs tensorflow lite on raspberry pi? or in your opinion is the performance comparable, given that tflite was purpose-built for this?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Haven’t used tflite, but I know since pytorch 1.3 you can use pytorch on mobile devices / arm architectures for experimental usage.<br/><NewLine>In Pytorch 1.5 you can use dynamic quanization for better latency.<br/><NewLine><a href=""https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html"" rel=""nofollow noopener"">Here</a> is the tutorial.<br/><NewLine>Also huge advantage of detr object detector is the model size. Check it out <a href=""https://github.com/facebookresearch/detr"" rel=""nofollow noopener"">here</a>.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I put up <a href=""http://mathinf.com/pytorch/arm64/"" rel=""nofollow noopener"">PyTorch 1.6/TorchVision 0.7 wheels for Raspberry Pi OS 64bit</a> in the hope that they’ll be useful.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/maltequast; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Gautham_Kumaran; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/maltequast; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: July 4, 2020,  6:43pm; <NewLine> REPLY_DATE 2: July 4, 2020,  6:57pm; <NewLine> REPLY_DATE 3: July 9, 2020,  8:38pm; <NewLine> REPLY_DATE 4: September 7, 2020,  1:33pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 2 Likes; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
95413,Backend worker monitoring thread interrupted or backend worker process died,2020-09-07T09:24:12.257Z,0,51,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m testing torchserve using resnet-18 tutorial in this link: <a href=""https://github.com/pytorch/serve/tree/master/examples/image_classifier/resnet_18"" rel=""nofollow noopener"">https://github.com/pytorch/serve/tree/master/examples/image_classifier/resnet_18</a></p><NewLine><p>The tutorial works perfectly fine, but when I moved resnet.py from torchvision, index_to_name.json, resnet18-5c106cde.pth, model.py into the same folder, and I changed model.py into:</p><NewLine><pre><code class=""lang-auto"">from resnet import ResNet, BasicBlock<NewLine><NewLine>class ImageClassifier(ResNet):<NewLine>    def __init__(self):<NewLine>        super(ImageClassifier, self).__init__(BasicBlock, [2, 2, 2, 2])<NewLine></code></pre><NewLine><p>then I use the following command to archive the model and start torchserve:</p><NewLine><blockquote><NewLine><p>torch-model-archiver --model-name resnet-18 --version 1.0 --model-file model.py --serialized-file resnet18-5c106cde.pth --handler image_classifier --extra-files index_to_name.json</p><NewLine></blockquote><NewLine><blockquote><NewLine><p>mkdir model_store</p><NewLine></blockquote><NewLine><blockquote><NewLine><p>mv resnet-18.mar model_store/</p><NewLine></blockquote><NewLine><blockquote><NewLine><p>torchserve --start --model-store model_store --models resnet-18=resnet-18.mar</p><NewLine></blockquote><NewLine><p>here is the log after torchserve is executed:</p><NewLine><p><strong>&gt; 2020-09-07 15:49:36,082 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /tmp/.ts.sock.9000</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,082 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]18279</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,082 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,082 [DEBUG] W-9000-resnet-18_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-resnet-18_1.0 State change WORKER_STOPPED -&gt; WORKER_STARTED</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,082 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.9</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,082 [INFO ] W-9000-resnet-18_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,083 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /tmp/.ts.sock.9000.</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,546 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,546 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,546 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File “/home/hoangminhq/.local/lib/python3.6/site-packages/ts/model_service_worker.py”, line 176, in </strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,546 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     worker.run_server()</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,546 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File “/home/hoangminhq/.local/lib/python3.6/site-packages/ts/model_service_worker.py”, line 148, in run_server</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,546 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.handle_connection(cl_socket)</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,546 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File “/home/hoangminhq/.local/lib/python3.6/site-packages/ts/model_service_worker.py”, line 112, in handle_connection</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,546 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File “/home/hoangminhq/.local/lib/python3.6/site-packages/ts/model_service_worker.py”, line 85, in load_model</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size)</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File “/home/hoangminhq/.local/lib/python3.6/site-packages/ts/model_loader.py”, line 117, in load</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     model_service.initialize(service.context)</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File “/home/hoangminhq/.local/lib/python3.6/site-packages/ts/torch_handler/base_handler.py”, line 50, in initialize</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     self.model = self._load_pickled_model(model_dir, model_file, model_pt_path)</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File “/home/hoangminhq/.local/lib/python3.6/site-packages/ts/torch_handler/base_handler.py”, line 74, in _load_pickled_model</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     module = importlib.import_module(model_file.split(""."")[0])</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File “/usr/lib/python3.6/importlib/<strong>init</strong>.py”, line 126, in import_module</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     return _bootstrap._gcd_import(name[level:], package, level)</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File “”, line 994, in _gcd_import</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File “”, line 971, in _find_and_load</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File “”, line 955, in _find_and_load_unlocked</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File “”, line 665, in _load_unlocked</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File “”, line 678, in exec_module</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File “”, line 219, in _call_with_frames_removed</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File “/tmp/models/492344f70f0f41beb13b7ed4335b3a18/model.py”, line 1, in </strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     from resnet import ResNet, BasicBlock</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,547 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ModuleNotFoundError: No module named 'resnet’</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,548 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,551 [DEBUG] W-9000-resnet-18_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,552 [DEBUG] W-9000-resnet-18_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.</strong><br/><NewLine><strong>&gt; java.lang.InterruptedException</strong><br/><NewLine><strong>&gt; 	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)</strong><br/><NewLine><strong>&gt; 	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)</strong><br/><NewLine><strong>&gt; 	at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)</strong><br/><NewLine><strong>&gt; 	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:129)</strong><br/><NewLine><strong>&gt; 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)</strong><br/><NewLine><strong>&gt; 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)</strong><br/><NewLine><strong>&gt; 	at java.base/java.lang.Thread.run(Thread.java:834)</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,552 [WARN ] W-9000-resnet-18_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: resnet-18, error: Worker died.</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,552 [DEBUG] W-9000-resnet-18_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-resnet-18_1.0 State change WORKER_STARTED -&gt; WORKER_STOPPED</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,552 [WARN ] W-9000-resnet-18_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-resnet-18_1.0-stderr</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,552 [WARN ] W-9000-resnet-18_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-resnet-18_1.0-stdout</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,553 [INFO ] W-9000-resnet-18_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,565 [INFO ] W-9000-resnet-18_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-resnet-18_1.0-stderr</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:36,565 [INFO ] W-9000-resnet-18_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-resnet-18_1.0-stdout</strong><br/><NewLine><strong>&gt; 2020-09-07 15:49:38,081 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.</strong></p><NewLine><p>I’m using:</p><NewLine><ul><NewLine><li>torchserve version: 0.2.0</li><NewLine><li>torch version: 1.6.0</li><NewLine><li>java version: 11.0.8</li><NewLine><li>Operating System and version: Ubuntu 18.04.5</li><NewLine></ul><NewLine><p>it seems like the model cannot be loaded, what did I do wrong and how can I fix this?</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/LiThiLi,(Li Thi Li),LiThiLi,"September 7, 2020,  9:25am",,,,,
95348,TorchServe for Partially Trained Models,2020-09-06T10:42:43.738Z,0,27,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am reading into torchserve and from my understanding it provides a system for clients to send inference requests to a trained model.</p><NewLine><p>Is it possible to use a model which needs to perform gradient ascent to predict values with torchserve, using a partially trained model?</p><NewLine><p>If not, what alternatives could one use?</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/A-F-V,(A F V),A-F-V,"September 6, 2020, 10:47am",,,,,
94768,Share weights within the model in dynamic way,2020-09-01T05:31:21.873Z,0,35,"<div class=""post"" itemprop=""articleBody""><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/ba9589127930dfc58629bb1483f29f359095c99b"" href=""https://discuss.pytorch.org/uploads/default/original/3X/b/a/ba9589127930dfc58629bb1483f29f359095c99b.png"" title=""ADAIN""><img alt=""ADAIN"" data-base62-sha1=""qCBbDwG4IXcQ6zXFI1WjuzvyGXF"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/a/ba9589127930dfc58629bb1483f29f359095c99b_2_10x10.png"" height=""413"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/a/ba9589127930dfc58629bb1483f29f359095c99b_2_690x413.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/a/ba9589127930dfc58629bb1483f29f359095c99b_2_690x413.png, https://discuss.pytorch.org/uploads/default/optimized/3X/b/a/ba9589127930dfc58629bb1483f29f359095c99b_2_1035x619.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/b/a/ba9589127930dfc58629bb1483f29f359095c99b.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">ADAIN</span><span class=""informations"">1069×640 81.4 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine>I’m trying to deploy the following model in PyTorch.</p><NewLine><p>Consider the right part of the network. I have data related to each station from 1 to K where K is dynamic. layers on the right should share weights for all stations.</p><NewLine><p>Can someone tell me how can I share weights for this kind of application?</p><NewLine></div>",https://discuss.pytorch.org/u/zeeel_patel,(zeeel patel),zeeel_patel,"September 1, 2020,  5:31am",,,,,
67353,How to add pytorch as a requirement for a windows python package,2020-01-22T09:31:30.450Z,6,274,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a python package which depends on pytorch and which I’d like windows users to be able to install via pip (the specific package is: <a href=""https://github.com/mindsdb/lightwood"" rel=""nofollow noopener"">https://github.com/mindsdb/lightwood</a>, but I don’t think this is very relevant to my question).</p><NewLine><p>What are the best practices for going about this ?</p><NewLine><p>Are there some project I could use as examples ?</p><NewLine><p>It seems like the pypi hosted version of torch &amp; torchvision aren’t windows compatible and the “getting started” section suggests installing from the custom pytorch repository, but beyond that I’m not sure what the ideal solution would be to incorporate this as part of a setup script.</p><NewLine></div>",https://discuss.pytorch.org/u/George3d6,(George),George3d6,"January 22, 2020,  9:31am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Pytorch is available on windows. did you check <a href=""http://Pytorch.org"" rel=""nofollow noopener"">Pytorch.org</a>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hmh, maybe I didn’t phrase my question correctly.</p><NewLine><p>Could you tell me what you thought I was asking :?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>It seems like the pypi hosted version of torch &amp; torchvision aren’t windows compatible and the “getting started” section suggests installing from the custom pytorch repository, but beyond that I’m not sure what the ideal solution would be to incorporate this as part of a setup script.</p><NewLine></blockquote><NewLine><p>Reading  this, I thought you dont know about <code>Pytorch.org</code> that provides 100% windows compatible pip packages which you can use!<br/><NewLine>You can find all the packages here is as well <a href=""https://download.pytorch.org/whl/torch_stable.html"" rel=""nofollow noopener"">https://download.pytorch.org/whl/torch_stable.html</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, the issue with that is that you can’t list them as a dependency, since pip will complain about downloading stuff from sources other than pypi and the version that pytroch has on pypi doesn’t seem to work on windows.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please see my answers here: <a href=""https://github.com/pytorch/pytorch/issues/32485#issuecomment-577113812"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/32485#issuecomment-577113812</a>.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I did see the answer, problem being that I can’t do that for a pypi package, setup.py doesn’t support find-links anymore and if I use the <code>@</code> syntax or some other syntax to set the repository it will complain about “Can’t depend on packages not hosted on pypi”.</p><NewLine><p>The workaround I have right now is a manual install on windows as part of the setup script by calling a pip subprocess and trying the various versions: <a href=""https://github.com/mindsdb/lightwood/blob/master/setup.py#L41"" rel=""nofollow noopener"">https://github.com/mindsdb/lightwood/blob/master/setup.py#L41</a></p><NewLine><p>But it doesn’t look like it’s particularly best practiy to me.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, did you find a better solution?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/George3d6; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/George3d6; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/peterjc123; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/George3d6; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/WaterKnight; <NewLine> ,"REPLY_DATE 1: January 22, 2020,  9:41am; <NewLine> REPLY_DATE 2: January 22, 2020,  9:48am; <NewLine> REPLY_DATE 3: January 25, 2020, 11:54am; <NewLine> REPLY_DATE 4: January 25, 2020,  9:27pm; <NewLine> REPLY_DATE 5: January 26, 2020,  7:54am; <NewLine> REPLY_DATE 6: February 1, 2020, 11:04am; <NewLine> REPLY_DATE 7: August 28, 2020, 11:58am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> 
94354,Java JDK dependencies for TorchServe,2020-08-27T19:43:22.685Z,0,45,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Current the doc says:<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/serve/install.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/serve/install.html</a><br/><NewLine><strong>java 8</strong> : Required. TorchServe use java to serve HTTP requests. You must install java 8 (or later) and make sure java is on available in $PATH environment variable  <em>before</em>  installing TorchServe. If you have multiple java installed, you can use $JAVA_HOME environment variable to control which java to use.</p><NewLine><p>But the instructions refer to Java 11 JDK. Which I think is correct (I tried with just Java8 and it did not work) So the docs need an update. But my real question is… I’m not sure if one needs full JDK or JRE?<br/><NewLine>From a security perspective JDK is a larger attach surface, having dependency reduced to JRE makes TorchServe better from a production standpoint. Would love some thoughts on this topic from the devs.</p><NewLine><p>Best,<br/><NewLine>Dhruv</p><NewLine></div>",https://discuss.pytorch.org/u/dhruvsakalley,(Dhruv Sakalley),dhruvsakalley,"August 27, 2020,  7:45pm",,,,,
94339,Redistributing PyTorch,2020-08-27T18:20:48.737Z,0,53,"<div class=""post"" itemprop=""articleBody""><NewLine><p>First of all, thanks a lot for PyTorch. I have been using PyTorch in my research and I love it!</p><NewLine><p>I want to package precompiled <code>libtorch</code> in a Linux distribution. I know that PyTorch itself is licensed under the BSD license. However, the binary builds include parts of Intel MKL and also parts of the CUDA toolkit. I wanted to ask if the resulting binaries are redistributable. In other words, is it OK to take the libtorch [1] archive, repackage it in the distribution’s native package format and provide it for download?</p><NewLine><p>[1] E.g. <a href=""https://download.pytorch.org/libtorch/cu102/libtorch-cxx11-abi-shared-with-deps-1.6.0.zip"" rel=""nofollow noopener"">https://download.pytorch.org/libtorch/cu102/libtorch-cxx11-abi-shared-with-deps-1.6.0.zip</a></p><NewLine></div>",https://discuss.pytorch.org/u/danieldk,(Daniël de Kok),danieldk,"August 27, 2020,  6:23pm",1 Like,,,,
94023,"Open source project to easily manage datasets, models and optimizers in pytorch",2020-08-25T08:43:14.848Z,0,47,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, pytorch forum! How are you managing your dataset or models? Introducing a project called matorage that can easily solve these problems.</p><NewLine><ul><NewLine><li><NewLine><p>Github : <a href=""https://github.com/graykode/matorage"" rel=""nofollow noopener"">https://github.com/graykode/matorage</a></p><NewLine></li><NewLine><li><NewLine><p>Document : <a href=""https://matorage.readthedocs.io"" rel=""nofollow noopener"">https://matorage.readthedocs.io</a></p><NewLine></li><NewLine><li><NewLine><p>Tutorial: <a href=""https://github.com/graykode/matorage#quick-start-with-pytorch-example"" rel=""nofollow noopener"">https://github.com/graykode/matorage#quick-start-with-pytorch-example</a></p><NewLine></li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/graykode,(Tae Hwan Jung),graykode,"August 25, 2020,  8:43am",,,,,
93862,Unable to load the model_weights from S3 Bucket,2020-08-24T06:31:42.088Z,0,39,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Im working on S3 , I ve my model saved in S3 as PTH file , but when i try to load it  I get the following error</p><NewLine><pre><code class=""lang-auto"">TypeError: new(): invalid data type 'bytes'<NewLine></code></pre><NewLine><p>My code is:</p><NewLine><pre><code class=""lang-auto"">s3 = boto3.resource('s3')<NewLine>bucket_name = 'bucket_name'<NewLine>key = 'network/model/model.pth'<NewLine>obj = s3.Object(bucket_name,key)<NewLine>body = obj.get()['Body'].read()<NewLine><NewLine>state = torch.as_tensor(body)<NewLine><NewLine></code></pre><NewLine><p>I am assuming torch cant load model as bytes …I am new to AWS , so anyone can help me with a solution how to load the model weights from S3 bucket</p><NewLine></div>",https://discuss.pytorch.org/u/Shakir_Khurshid,,Shakir_Khurshid,"August 24, 2020,  6:32am",,,,,
64951,UserWarning: Exporting a model to ONNX with a batch_size other than 1,2019-12-25T07:07:55.601Z,0,745,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I met this warning when converting CRNN to ONNX model, my code is as follows:</p><NewLine><pre><code class=""lang-auto"">from torch import nn,onnx<NewLine>import torch<NewLine><NewLine>class BidirectionalLSTM(nn.Module):<NewLine><NewLine>    def __init__(self, nIn, nHidden, nOut):<NewLine>        super(BidirectionalLSTM, self).__init__()<NewLine><NewLine>        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)<NewLine>        self.embedding = nn.Linear(nHidden * 2, nOut)<NewLine><NewLine>    def forward(self, input):<NewLine>        recurrent, _ = self.rnn(input)<NewLine>        T, b, h = recurrent.size()<NewLine>        t_rec = recurrent.view(T * b, h)<NewLine>        output = self.embedding(t_rec)  # [T * b, nOut]<NewLine>        output = output.view(T, b, -1)<NewLine>        return output<NewLine><NewLine>class CRNN(nn.Module):<NewLine><NewLine>    def __init__(self, imgH, nc, nclass, nh, n_rnn=2, leakyRelu=False, lstmFlag=True):<NewLine>        """"""<NewLine>        是否加入lstm特征层<NewLine>        """"""<NewLine>        super(CRNN, self).__init__()<NewLine>        assert imgH % 16 == 0, 'imgH has to be a multiple of 16'<NewLine><NewLine>        ks = [3, 3, 3, 3, 3, 3, 2]<NewLine>        ps = [1, 1, 1, 1, 1, 1, 0]<NewLine>        ss = [1, 1, 1, 1, 1, 1, 1]<NewLine>        nm = [64, 128, 256, 256, 512, 512, 512]<NewLine>        self.lstmFlag = lstmFlag<NewLine><NewLine>        cnn = nn.Sequential()<NewLine><NewLine>        def convRelu(i, batchNormalization=False):<NewLine>            nIn = nc if i == 0 else nm[i - 1]<NewLine>            nOut = nm[i]<NewLine>            cnn.add_module('conv{0}'.format(i),<NewLine>                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))<NewLine>            if batchNormalization:<NewLine>                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))<NewLine>            if leakyRelu:<NewLine>                cnn.add_module('relu{0}'.format(i),<NewLine>                               nn.LeakyReLU(0.2, inplace=True))<NewLine>            else:<NewLine>                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))<NewLine><NewLine>        convRelu(0)<NewLine>        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64<NewLine>        convRelu(1)<NewLine>        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32<NewLine>        convRelu(2, True)<NewLine>        convRelu(3)<NewLine>        cnn.add_module('pooling{0}'.format(2),<NewLine>                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16<NewLine>        convRelu(4, True)<NewLine>        convRelu(5)<NewLine>        cnn.add_module('pooling{0}'.format(3),<NewLine>                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16<NewLine>        convRelu(6, True)  # 512x1x16<NewLine><NewLine>        self.cnn = cnn<NewLine>        if self.lstmFlag:<NewLine>            self.rnn = nn.Sequential(<NewLine>                BidirectionalLSTM(512, nh, nh),<NewLine>                BidirectionalLSTM(nh, nh, nclass))<NewLine>        else:<NewLine>            self.linear = nn.Linear(nh * 2, nclass)<NewLine><NewLine>    def forward(self, input):<NewLine>        # conv features<NewLine>        conv = self.cnn(input)<NewLine>        b, c, h, w = conv.size()<NewLine>        conv = conv.squeeze(2)<NewLine>        conv = conv.permute(2, 0, 1)  # [w, b, c]<NewLine>        output = self.rnn(conv)<NewLine>        return output<NewLine><NewLine><NewLine>model = CRNN(32, 1, 5530, 256,n_rnn=2,leakyRelu=False,lstmFlag=True)<NewLine>x = torch.rand(1,1,32,100)<NewLine><NewLine>onnx.export(<NewLine>    model,x,""crnn.onnx"",export_params=True,opset_version=11,# verbose=True,<NewLine>    do_constant_folding=True,input_names=[""input""],output_names=[""output""],<NewLine>    dynamic_axes={'input' : {0 : 'batch_size'},'output' : {1 : 'batch_size'}}<NewLine>)<NewLine></code></pre><NewLine><p>The full information of warning is :</p><NewLine><pre><code class=""lang-auto"">/home/dai/py36env/lib/python3.6/site-packages/torch/onnx/symbolic_opset9.py:1377: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable lenght with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model.<NewLine>  ""or define the initial states (h0/c0) as inputs of the model. "")<NewLine></code></pre><NewLine><p>How can I avoid this warning or how to define the initial states(h0/c0)？</p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"December 25, 2019,  7:07am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi,Have you found a solution for this problem?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Same problem over here. Any lead on solving this?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Save problem.Anyone solve it? I convert the crnn pytorch model to onnx and then convert into a openvino model, but the inference  output shape in openvino is wrong.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Zrufy; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/HMonsarrat; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ZhangJ; <NewLine> ,"REPLY_DATE 1: May 8, 2020,  3:15pm; <NewLine> REPLY_DATE 2: May 22, 2020,  9:33pm; <NewLine> REPLY_DATE 3: August 20, 2020,  2:48am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
93223,"First time deployment questions about TorchScript, C++, and prototyping",2020-08-18T12:40:51.982Z,0,40,"<div class=""post"" itemprop=""articleBody""><NewLine><p>So I finally got my model working well enough to consider moving to a prototype and realized it isn’t as easy as I had initially thought. I just want to make sure I understand the steps of the process before I put in a lot of time learning TorchScript and C++. The model is a binary(possibly more classes later) semantic segmentation of cracked roads. Also I know OpenCV has C++ capabilities but are there similar libraries like numpy, pandas. Any recommendations on hardware to use?<br/><NewLine>Steps</p><NewLine><ol><NewLine><li>Use TorchScript to map/compile just the Pytorch model to C++</li><NewLine><li>Then use CodeBlocks to write a C++ program that will call the model</li><NewLine><li>Use OpenCV and other addons in program for video filtering and matrix operations</li><NewLine><li>Install the program on a hardware device. Any recommendations?</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/RyanPatterson,(Ryan),RyanPatterson,"August 18, 2020, 12:40pm",,,,,
81347,Fail to &ldquo;import torch&rdquo; on Windows debug build,2020-05-15T10:07:29.249Z,3,476,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I try to build pytorch from source on Windows, using Anaconda3, Ninja, and “DEBUG=1”. “python setup.py install”. It failed with fatal error LNK1104 “LNK1104: ╬▐╖¿┤≥┐¬╬─╝■í░python37_d.libí▒”. I found the  “python37_d.lib” is not in Anacoda. So in order to get “python37_d.lib” available, I installed python3.7 from <a href=""http://www.python.org"" rel=""nofollow noopener"">www.python.org</a> and select “Download debug binaries”. Then I copied the whole newly installed python folder to Anaconda3\envs\gt37 and replace existing old files.<br/><NewLine>After seting enviroment variable “LIB” to the Anaconda3\envs\gt37\libs, I successully built the debug version.<br/><NewLine>However, when I execute “import torch” in python, it shows the following error:</p><NewLine><blockquote><NewLine><p>(gt37) C:\Users\tegao\mygit\v0512\pytorch&gt;python<br/><NewLine>Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)] on win32<br/><NewLine>Type “help”, “copyright”, “credits” or “license” for more information.<br/><NewLine>&gt;&gt;&gt;\ import torch<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “”, line 1, in <br/><NewLine>File “C:\Users\tegao\mygit\v0512\pytorch\torch_<em>init</em>_.py”, line 247, in <br/><NewLine>from .random import set_rng_state, get_rng_state, manual_seed, initial_seed, seed<br/><NewLine>File “C:\Users\tegao\mygit\v0512\pytorch\torch\random.py”, line 4, in <br/><NewLine>from torch._C import default_generator<br/><NewLine>ImportError: cannot import name ‘default_generator’ from ‘torch._C’ (unknown location)</p><NewLine></blockquote><NewLine><p>Then I tried using python3.8, still failed with same error.<br/><NewLine>Then I tried using “python setup.py develop” and set the following enviroments：</p><NewLine><p>set DEBUG=1<br/><NewLine>set USE_DISTRIBUTED=0<br/><NewLine>set USE_MKLDNN=0<br/><NewLine>set USE_CUDA=0<br/><NewLine>set BUILD_TEST=0<br/><NewLine>set USE_FBGEMM=0<br/><NewLine>set USE_NNPACK=0<br/><NewLine>set USE_QNNPACK=0<br/><NewLine>set USE_XNNPACK=0<br/><NewLine>set LIB=C:\Users\tegao\Anaconda3\envs\gt38\libs</p><NewLine><p>This time the error changed when “import torch”:</p><NewLine><blockquote><NewLine><p>(gt38) C:\Users\tegao\mygit\pytorch&gt;python<br/><NewLine>Python 3.8.3 (tags/v3.8.3:6f8c832, May 13 2020, 22:37:02) [MSC v.1924 64 bit (AMD64)] on win32<br/><NewLine>Type “help”, “copyright”, “credits” or “license” for more information.<br/><NewLine>&gt;&gt;&gt; import torch<br/><NewLine>Fatal Python error: _PyInterpreterState_Get(): no current thread state<br/><NewLine>Python runtime state: unknown</p><NewLine></blockquote><NewLine><p>I make sure I have run “python setup.py clean” before each try.<br/><NewLine>But it could succeed on Ununtu. Building release version of pytorch on Windows could also succeed.<br/><NewLine>Is my method wrong(using self-installed python files to replace the ones under Anaconda in order to have pytorch37_d.lib)? How to successfully build debug version of pytorch on windows?</p><NewLine></div>",https://discuss.pytorch.org/u/teng,(teng),teng,"May 15, 2020, 10:07am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I had this issue before.</p><NewLine><p>I uninstalled the anaconda</p><NewLine><p>I installed again. But this time i selected the pip and specifically <strong>if you don’t have cuda select NONE</strong></p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/2c1426e9509bc1c3f6cb7645553e980f579b7a06"" href=""https://discuss.pytorch.org/uploads/default/original/3X/2/c/2c1426e9509bc1c3f6cb7645553e980f579b7a06.png"" title=""Screen Shot 2020-05-15 at 1.56.33 PM""><img alt=""Screen Shot 2020-05-15 at 1.56.33 PM"" data-base62-sha1=""6hWbpTpws66wweriEG7fMc4dIz4"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/c/2c1426e9509bc1c3f6cb7645553e980f579b7a06_2_10x10.png"" height=""493"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/c/2c1426e9509bc1c3f6cb7645553e980f579b7a06_2_690x493.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/c/2c1426e9509bc1c3f6cb7645553e980f579b7a06_2_690x493.png, https://discuss.pytorch.org/uploads/default/optimized/3X/2/c/2c1426e9509bc1c3f6cb7645553e980f579b7a06_2_1035x739.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/2/c/2c1426e9509bc1c3f6cb7645553e980f579b7a06_2_1380x986.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screen Shot 2020-05-15 at 1.56.33 PM</span><span class=""informations"">1862×1332 216 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot! But install from pip can’t debug into the code. I want to step into the pytorch to learn the calling stack of some operators or functions. So I have to build from source.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I guess you can try <code>set REL_WITH_DEB_INFO=1</code> instead of <code>set DEBUG=1</code>. In that way, you could also step into it and get the stack trace.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I have the same problem. Have you solved it?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jmandivarapu1; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/teng; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/peterjc123; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ayl; <NewLine> ,"REPLY_DATE 1: May 15, 2020,  5:57pm; <NewLine> REPLY_DATE 2: May 16, 2020, 12:31pm; <NewLine> REPLY_DATE 3: May 16, 2020,  3:49pm; <NewLine> REPLY_DATE 4: August 16, 2020,  1:56am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
91523,Cannot load Sentiment model for prediction,2020-08-03T20:48:28.587Z,1,85,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a Sentiment model trained using glove vectors refer <a href=""https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/"" rel=""nofollow noopener"">https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/</a>.<br/><NewLine>I save the trained mode and dictionary<br/><NewLine>torch.save(trainedmodel, ‘./trained_lstm.pt’)<br/><NewLine>torch.save(trainedmodel.state_dict(), ‘./state_dict.pt’)</p><NewLine><p>I try to recreate the model for inference<br/><NewLine>model = torch.load(‘trained_lstm.pt’, map_location=torch.device(“cpu”))<br/><NewLine>but keep getting error cannot get attribute SentimentNet in module main.  If I just load the state dictionary i get error on embedding and weight sizes.<br/><NewLine>I included the model class in the same file while saving and reloading. Any help ?</p><NewLine></div>",https://discuss.pytorch.org/u/Mukta_Mohindra,(Mukta Mohindra),Mukta_Mohindra,"August 3, 2020,  8:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I would recommend to stick to the second approach (save and load the <code>state_dict</code>), as the former approach can break in various ways.</p><NewLine><p>That being said, could you post the error message for the shape mismatch and check, what might have changed in the model?<br/><NewLine>E.g. did you store the <code>state_dict</code> and changed the embedding layer afterwards somehow?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The issue is model was trained using a glove vector and the embedding dimension was dynamically set using that. That is what I was trying to avoid by reloading the complete model. I was getting error -<br/><NewLine>RuntimeError: Error(s) in loading state_dict for SentimentNet:<br/><NewLine>size mismatch for embedding.weight: copying a param with shape torch.Size([1259, 300]) from checkpoint, the shape in current model is torch.Size([1000, 400]).<br/><NewLine>size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([2048, 300]) from checkpoint, the shape in current model is torch.Size([2048, 400]).<br/><NewLine>I retried setting the initialization parameters -<br/><NewLine>model_params = {‘weight_matrix’: None,<br/><NewLine>‘output_size’: 1,<br/><NewLine>‘hidden_dim’: 512,<br/><NewLine>‘n_layers’: 2,<br/><NewLine>‘dropout_prob’: 0.001,<br/><NewLine>‘vocab_size’ :1259,<br/><NewLine>‘embedding_dim’: 300<br/><NewLine>}<br/><NewLine>model = SentimentNet(**model_params)<br/><NewLine>model = model.load_state_dict(torch.load(‘state_dict.pt’, map_location=device))</p><NewLine><pre><code>model.eval()<NewLine></code></pre><NewLine><p>But now I get this error -<br/><NewLine>model.eval()<br/><NewLine>AttributeError: ‘_IncompatibleKeys’ object has no attribute ‘eval’</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>model.load_state_dict()</code> does not return the model, but the information about incompatible keys, so you should remove the assignment and rerun the code:</p><NewLine><pre><code class=""lang-python"">model_params = {'weight_matrix': None,<NewLine>'output_size': 1,<NewLine>'hidden_dim': 512,<NewLine>'n_layers': 2,<NewLine>'dropout_prob': 0.001,<NewLine>'vocab_size' :1259,<NewLine>'embedding_dim': 300<NewLine>}<NewLine>model = SentimentNet(**model_params)<NewLine>model.load_state_dict(torch.load('state_dict.pt', map_location=device))<NewLine><NewLine>model.eval()<NewLine></code></pre><NewLine><p>PS: you can add code snippets by wrapping them into three backticks ```, which makes debugging easier <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Mukta_Mohindra; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: August 4, 2020, 10:15am; <NewLine> REPLY_DATE 2: August 4, 2020,  3:12pm; <NewLine> REPLY_DATE 3: August 7, 2020,  5:11am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
89513,Deploying a pytorch model in meteor + react js,2020-07-16T22:46:49.169Z,0,121,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have gone through a couple of pages looking at my options.I intend to build a Pytorch model, I already have a web app all written in meteor+react. I am not sure if there is a way to combine flask and meteor. Else, is how easy is it to deploy my model for both torch and TF, then use Tensorflow.js  with my react +meteor app.</p><NewLine><p>Would appreciate any advice. i have most of my front end build already so that stack can’t change now.</p><NewLine></div>",https://discuss.pytorch.org/u/DeepsMoseli,(Moseli Motsoehli),DeepsMoseli,"July 16, 2020, 10:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Moseli!</p><NewLine><p>Hope you’re going good.</p><NewLine><p>Given you already have a reactjs frontend, you probably want to maintain the separation of Backend and frontend. I think in your case, it would make sense in your case to build a Flask API, and have your reactjs app simply call your external API?</p><NewLine><p>I wrote an <a href=""https://discuss.pytorch.org/t/i-want-to-serve-my-chatbot/54586"">answer back here, in Sep 2019</a> pointing to links. Since then, PyTorch has implemented TorchServing which could be of interest - <a href=""https://pytorch.org/blog/model-serving-in-pyorch/"" rel=""nofollow noopener"">blog here</a>, <a href=""https://pytorch.org/serve/"" rel=""nofollow noopener"">docs here.</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/JamesTrick; <NewLine> ,"REPLY_DATE 1: August 5, 2020, 12:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
91553,Can i make my pytorch model to docker image?,2020-08-04T03:34:53.547Z,0,51,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to pack my model to docker image.<br/><NewLine>Cause i’m beginner in domain of docker, i just know one solution - make a REST API server with pytorch model and make it as docker image. Then, when use this docker server, send request to train or test model.<br/><NewLine>Is there other options??</p><NewLine><p>Thank you</p><NewLine></div>",https://discuss.pytorch.org/u/111335,(동희 윤),111335,"August 4, 2020,  3:34am",,,,,
91430,What&rsquo;s the official high performance serving solution for PyTorch?,2020-08-03T03:00:35.659Z,0,72,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, all. Currently I’m looking for an official PyTorch model serving solution with high performance. From the <a href=""https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html"" rel=""nofollow noopener"">docs</a> about PyTorch model serving, there are several options for model deployment. I’m afraid that none of the solutions can meet all the requirement at the same time.</p><NewLine><ul><NewLine><li>Deploy model using Flask: Python have GIL, there may be performance issue if the traffic is heavy.</li><NewLine><li>TorchServe: The key logic (preprocess, inference, postprocess) is in the <a href=""https://github.com/pytorch/serve/tree/master/ts/torch_handler"" rel=""nofollow noopener"">torch_handlers</a>. The handlers are also Python modules. There may be performance issue because of the same reason as the option above.</li><NewLine><li>Load TorchScript using LibTorch: It provides the LibTorch library instead of a model server. And the solution doesn’t contain preprocess and postprocess.</li><NewLine></ul><NewLine><p>Look forward to more suggestions.</p><NewLine></div>",https://discuss.pytorch.org/u/Bright,,Bright,"August 4, 2020,  1:17am",,,,,
76401,Huge gpu performance difference between using NVIDIA RXT 2070 and GTX 1080 GPUs,2020-04-12T07:13:06.639Z,11,344,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Even though these two GPUs are somewhat close in terms of compute specifications. the GTX 1080 being slightly better. When I train my model on a GTX 1080 GPU powered machine, it takes 0.5 seconds of GPU processing time for a single batch. Whereas an RTX 2070 powered machine takes 9 seconds in average for the same operation. This happens for a variety of models I have trained including pure CNN, CNN+LSTM, CNN+RNN. I am also using the cudnn.benchmark = True and pin_memory = True, for both machines. Both machines have roughly the same CPU and memory capabilities. Any thoughts as to why the RTX machine could be 18x longer for the same operation?</p><NewLine></div>",https://discuss.pytorch.org/u/pipemon,(Felipe Moreno),pipemon,"April 12, 2020,  7:14am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you make sure to synchronize the code properly before starting and stopping the timer?<br/><NewLine>Since CUDA operations are asynchronous, you’ll get wrong profiling results without synchronizations.<br/><NewLine>Also, are you using the same PyTorch, CUDA and cudnn versions?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Interesting, after investigating the torch versions, it turns out that the GTX machine had torch version == 0.4.1 and the RTX had version == 1.4.0. When I upgraded the GTX version upwards to 1.4.0, I was able to reproduce the bad performance of the RTX.</p><NewLine><p>Why is it the case?</p><NewLine><p>In terms of synchronizing the times, I’m just measuring the time it takes to complete a forward iteration for a batch and  also measuring the time that it takes the dataloaders to get the data. So the difference between the two is the time it takes to perform a forward pass in the data. In any case, upgrading the pytorch version makes my epochs go from 5 minutes to complete to 80 minutes.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually, I did some more analysis of the timing. It turns out, most of the time spent in the routine is in the loss.backward() operation, not when computing the forward nor when loading the data with .cuda().  Here is a snippet of the code inside my training loop.</p><NewLine><pre><code class=""lang-auto"">for data in dataloader:<NewLine>    data = data.cuda()<NewLine>    data = torch.torch.autograd(data, requires_grad=True)<NewLine>    output = model(data)<NewLine>    loss  = criterion(output, labels)<NewLine>    predictions = (output.detach() &gt; 0).int()<NewLine>    accuracy = (labels.detach().int() == predictions).sum().item()/batch_size<NewLine>    optimizer.zero_grad()<NewLine>    loss.backward() ### backward takes 100x longer than forward in torch 1.4.0<NewLine>    optimizer.step()<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Again, if you don’t synchronize the code via <code>torch.cuda.synchronize()</code>, the next “blocking” operation will accumulate the time from all asynchronous calls.<br/><NewLine>Please synchronize the code before starting and stopping the timers and feel free to post the results.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, if I didn’t make it clear. I do am using the I am using the synchronize functionality, I just didn’t wanted the code to look like spaghetti. I insert it in between every line shown as follows. I also use an Averager object to access the steady state averages of each time measure.</p><NewLine><pre><code class=""lang-auto"">#Initialize averager objects to accumulate time measures<NewLine>batch_time = Averager()<NewLine>data_time = Averager()<NewLine>cuda_time = Averager()<NewLine>grad_time = Averager()<NewLine>forward_time =Averager()<NewLine>loss_time  = Averager()<NewLine>prediction_time = Averager()<NewLine>accuracy_time = Averager()<NewLine>optimizer_time = Averager()<NewLine>backward_time = Averager()<NewLine>#Initialize per epoch timer<NewLine>end_time = time.time()<NewLine>for data in dataloader:<NewLine>    #data loading<NewLine>    data_time.update(time.time() - end_time)<NewLine>    #load gpu<NewLine>    data = data.cuda(async=False)<NewLine>    torch.cuda.synchronize()<NewLine>    cuda_time.update(time.time() - end_time)<NewLine>    #set gradient<NewLine>    data = torch.torch.autograd(data, requires_grad=True)<NewLine>    torch.cuda.synchronize()<NewLine>    grad_time.update(time.time() - end_time)<NewLine>    #forward call<NewLine>    output = model(data)<NewLine>    torch.cuda.synchronize()<NewLine>    forward_time.update(time.time() - end_time)<NewLine>    #compute loss<NewLine>    loss  = criterion(output, labels)<NewLine>    torch.cuda.synchronize()<NewLine>    loss_time.update(time.time() - end_time)<NewLine>    #make predictions<NewLine>    predictions = (output.detach() &gt; 0).int()<NewLine>    torch.cuda.synchronize()<NewLine>    prediction_time.update(time.time() - end_time)<NewLine>    #calculate accuracy<NewLine>    accuracy = (labels.detach().int() == predictions).sum().item()/batch_size<NewLine>    torch.cuda.synchronize()<NewLine>    accuracy_time.update(time.time() - end_time)<NewLine>    #optimize<NewLine>    optimizer.zero_grad()<NewLine>    torch.cuda.synchronize()<NewLine>    optimizer_time.update(time.time() - end_time)<NewLine>    loss.backward() ### backward takes 100x longer than forward in torch 1.4.0<NewLine>    torch.cuda.synchronize()<NewLine>    backward_time.update(time.time() - end_time)<NewLine>    optimizer.step()<NewLine>    torch.cuda.synchronize()<NewLine>    batch_time.update(time.time() - end_time)<NewLine>    #restart the timer<NewLine>    end_time = time.time()<NewLine><NewLine>#Then print the average measures for the epoch<NewLine>print(<NewLine>    Time ({batch_time.avg:.3f})\t'<NewLine>    'Backward  ({backward_time.avg:.3f})\t'<NewLine>    'Optimizer  ({optimizer_time.avg:.3f})\t'<NewLine>    'Accuracy  ({accuracy_time.avg:.3f})\t'<NewLine>    'Loss  ({loss_time.avg:3f})\t'<NewLine>    'Forward  ({forward_time.avg:3f})\t'<NewLine>    'Grad  ({grad_time.avg:3f})\t'<NewLine>    'Cuda  ({cuda_time.avg:.3f})\t'<NewLine>    'Data  ({data_time.avg:.3f})\t''.format(<NewLine>    backward_time=backward_time,<NewLine>    optimizer_time=optimizer_time,<NewLine>    forward_time=forward_time,<NewLine>    accuracy_time=accuracy_time,<NewLine>    loss_time=loss_time,<NewLine>    grad_time=grad_time,<NewLine>    batch_time=batch_time,<NewLine>    cuda_time=cuda_time,<NewLine>    data_time=data_time))<NewLine></code></pre><NewLine><p>These are my recorded measures when running on torch 1.4.0:<br/><NewLine>Time (16.885),   Backward (16.885), Optimizer (0.476), Accuracy (0.476),  Loss (0.476), Forward (0.457),  Grad (0.348), Cuda (0.348),  Data (0.277)</p><NewLine><p>Then when run on the same machine using torch 0.4.1:<br/><NewLine>Time (0.524) , Backward (0.520),  Optimizer (0.510), Accuracy (0.510),  Loss (0.508), Forward (0.419565), Grad (0.405319), Cuda (0.405), Data (0.229)</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>To add some more information about the system:<br/><NewLine>GPU GTX 1080. Cuda 10.2, NVCC 9.2. I also get similar benchmarks when running on<br/><NewLine>GPU RTX 2070, CUDA 10.2, NVCC 10.2.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the code. Could you post your model and the input shapes, so that we could take a look, why the backward is 100x slower in <code>1.4.0</code>?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <span class=""mention"">@prtblck</span> , I’m somewhat conflicted about publicly sharing my source code. Is there a way I could share it only with Pytorch developers? I personally want to contribute to the community, but would not like to expose intellectual property.</p><NewLine><p>Inspecting my code, something that might be impacting the computation of the gradients is that my model’s forward method contains conditional statements, assertions and try/except blocks. I’m guessing some of these are might be impacting the performance.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""9"" data-topic=""76401"" data-username=""pipemon""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/pipemon/40/18142_2.png"" width=""20""/> pipemon:</div><NewLine><blockquote><NewLine><p>I personally want to contribute to the community, but would not like to expose intellectual property.</p><NewLine></blockquote><NewLine></aside><NewLine><p>That is understandable.<br/><NewLine>Would it be possible to “simulate” your approach, e.g. by adding some conditions using just random numbers?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t really know how, and the model is ~300 lines of code. I’m do am planning to release the code with the publication. Shall we put a hold to this till then?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sure. Make sure you publish the paper and ping me here again once the code is public. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine><p>PS: Not sure, if you’re already using it, but add <code>torch.backends.cudnn.benchmark = True</code> at the beginning of your script. This will run benchmarks for each new input shape and select the fastest cudnn kernel for your workload. Note that the first iteration will be slower due to benchmarking (and each “first” iteration for new input shapes).</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>,<br/><NewLine>Yes, I’m using the <code>torch.backends.cudnn.benchmark = True</code> and also replaced all the try/except with conditionals and that made no difference in the performance.</p><NewLine><p>Another piece of information that might be useful to determine the sink in performance is that the model I’m using is a transfer learning model which was used trained on pytorch 0.4.1. Maybe there could be a performance issue with using weights trained with a previous pytorch version?</p><NewLine><p>I still haven’t been able to publicly release the code, but I can grant you read access.Do you think that might help resolve this issue?</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""13"" data-topic=""76401"" data-username=""pipemon""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/pipemon/40/18142_2.png"" width=""20""/> pipemon:</div><NewLine><blockquote><NewLine><p>Maybe there could be a performance issue with using weights trained with a previous pytorch version?</p><NewLine></blockquote><NewLine></aside><NewLine><p>That shouldn’t be the case, as the <code>state_dict</code> would only hold the parameters and no operations etc.</p><NewLine><aside class=""quote no-group"" data-post=""13"" data-topic=""76401"" data-username=""pipemon""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/pipemon/40/18142_2.png"" width=""20""/> pipemon:</div><NewLine><blockquote><NewLine><p>I still haven’t been able to publicly release the code, but I can grant you read access.Do you think that might help resolve this issue?</p><NewLine></blockquote><NewLine></aside><NewLine><p>No, please don’t share the code with “strangers” without a proper agreement. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/><br/><NewLine>If you could write a “fake model” with a similar architecture without releasing any of your research, feel free to post it here. Otherwise, I’ll just wait until you are able to release the code to have a look at it.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>. I actually figured out a way to share the model without disclosing my custom architecture. I’m basing my model on the ITracker model (<a href=""https://github.com/CSAILVision/GazeCapture/blob/master/pytorch/ITrackerModel.py"" rel=""nofollow noopener"">https://github.com/CSAILVision/GazeCapture/blob/master/pytorch/ITrackerModel.py</a>) which is publicly available on the repo. <a href=""https://github.com/CSAILVision/GazeCapture/tree/master/pytorch"" rel=""nofollow noopener"">https://github.com/CSAILVision/GazeCapture/tree/master/pytorch</a>. This model suffers from the same issues as mine.</p><NewLine><p>I was able to pin-point that the change which introduced the performance issues was introduced in the torch version 1.3.0.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>, I understand if this is not a pressing issue to resolve currently by the development team, but I wonder if there has been any progress in understanding the root cause of the issue and if I can help somehow. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pipemon; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pipemon; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/pipemon; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/pipemon; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/pipemon; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/pipemon; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/pipemon; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/pipemon; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/pipemon; <NewLine> ,"REPLY_DATE 1: April 12, 2020,  7:36am; <NewLine> REPLY_DATE 2: April 12, 2020,  9:37pm; <NewLine> REPLY_DATE 3: April 12, 2020, 11:21pm; <NewLine> REPLY_DATE 4: April 13, 2020,  4:03am; <NewLine> REPLY_DATE 5: April 13, 2020,  6:50am; <NewLine> REPLY_DATE 6: April 14, 2020,  4:37pm; <NewLine> REPLY_DATE 7: April 15, 2020,  4:13am; <NewLine> REPLY_DATE 8: April 17, 2020,  6:39am; <NewLine> REPLY_DATE 9: April 17, 2020,  6:46am; <NewLine> REPLY_DATE 10: July 8, 2020,  1:44am; <NewLine> REPLY_DATE 11: April 17, 2020,  7:20am; <NewLine> REPLY_DATE 12: July 7, 2020, 12:08am; <NewLine> REPLY_DATE 13: July 7, 2020,  3:38am; <NewLine> REPLY_DATE 14: July 9, 2020, 11:13pm; <NewLine> REPLY_DATE 15: August 2, 2020,  8:23pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> 
91357,Conversion torch model to onnx model ignoring slice operation,2020-08-02T05:04:34.989Z,0,68,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m converting a pytorch model to onnx model. in this model there an assignment of tensor to a slice of another tensor.<br/><NewLine>when i’m running the converted model with onnxruntime he crashes when trying to assign the small tensor to the big tensor and ignoring the slice operation.</p><NewLine><p>i isolated the problem to this forward function:</p><NewLine><pre><code class=""lang-auto"">def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:<NewLine>    x[:y.size(0), 0, :] = y<NewLine>    return x<NewLine></code></pre><NewLine><p>onnxruntime returns this error where shape of x is (56,6,256) ans shape of y is (22,256):<br/><NewLine><strong>The input tensor cannot be reshaped to the requested shape. Input shape:{22,256}, requested shape:{56,1,256}</strong></p><NewLine><p>my env is:</p><NewLine><ul><NewLine><li>PyTorch Version: 1.5.1</li><NewLine><li>OS (e.g., Linux): Linux ubuntu 16.04</li><NewLine><li>Python version: 3.6.8</li><NewLine><li>onnxruntime verion: 1.4.0</li><NewLine></ul><NewLine><p>to reproduce this bug just ran this code:</p><NewLine><p>``import torch<br/><NewLine>import onnx<br/><NewLine>import onnxruntime</p><NewLine><p>class Test(torch.nn.Module):<br/><NewLine>def  <strong>init</strong> (self):<br/><NewLine>super(). <strong>init</strong> ()</p><NewLine><p>def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:<br/><NewLine>x[:y.size(0), 0, :] = y<br/><NewLine>return x</p><NewLine><p>x = torch.zeros((56,6,256))<br/><NewLine>y = torch.rand((22,256))</p><NewLine><p>m = Test()<br/><NewLine>torch_outputs = m(x, y)</p><NewLine><p>path = “/tmp/m.onnx”<br/><NewLine>torch.onnx.export(m, (x, y), path,<br/><NewLine>do_constant_folding=True,<br/><NewLine>opset_version=12,<br/><NewLine>input_names=[“x”, “y”],<br/><NewLine>output_names=[“z”])</p><NewLine><p>onnx_model = onnx.load_model(path)<br/><NewLine>onnx.checker.check_model(onnx_model)<br/><NewLine>inferred_model = onnx.shape_inference.infer_shapes(onnx_model)<br/><NewLine>onnx.save(inferred_model, path)</p><NewLine><p>ort_model = onnxruntime.InferenceSession(path)</p><NewLine><p>ort_outputs = ort_model.run([“z”], {“x”: x.numpy(), “y”: y.numpy()})``</p><NewLine></div>",https://discuss.pytorch.org/u/natan,(natan gold),natan,"August 2, 2020,  5:04am",,,,,
51927,&ldquo;AttributeError: &lsquo;ReLU&rsquo; object has no attribute &lsquo;threshold&rsquo; &rdquo;,2019-07-29T10:56:27.934Z,0,845,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, All.</p><NewLine><p>I have met this error when run pytorch inference in Jetson nano.</p><NewLine><p>""AttributeError: ‘ReLU’ object has no attribute ‘threshold’ "".</p><NewLine><p>What wrong to me?</p><NewLine><p>Jetson nano, Ubuntu 18.04LTS aarch64, Pytorch 1.1.0, Torchvision 0.3.0</p><NewLine><p>Thanks.<br/><NewLine><span class=""mention"">@bemoregt</span>.</p><NewLine></div>",https://discuss.pytorch.org/u/bemorept,(Wonwoo Park),bemorept,"July 29, 2019, 10:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you post the code snippet which raises this error?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you solved it?I also ran into it when I called the model</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I ran into the issue when I am running the following code using the export.pkl model I had saved in my directory:</p><NewLine><pre><code class=""lang-auto"">import fastai<NewLine>from fastai.vision import *<NewLine><NewLine>def main():<NewLine><NewLine>    classes = ['Potato___Late_blight',<NewLine>               'Cherry_(including_sour)___healthy',<NewLine>               'Apple___Black_rot',<NewLine>               'Apple___Apple_scab',<NewLine>               'Cherry_(including_sour)___Powdery_mildew',<NewLine>               'Potato___healthy',<NewLine>               'Apple___Cedar_apple_rust',<NewLine>               'Potato___Early_blight',<NewLine>               'Apple___healthy']<NewLine><NewLine>    #load predictor<NewLine>    predictor = load_learner('.')<NewLine>    img = open_image('apple_black_rot1.jpg')<NewLine>    pred,pred_idx,probs = predictor.predict(img)<NewLine><NewLine>    print(classes[pred_idx])<NewLine><NewLine>if __name__==""__main__"":<NewLine>    main()<NewLine></code></pre><NewLine><p>The same error pops up along with various warnings. Is there any fix to this yet?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/lianzhang132; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/arnabsinha; <NewLine> ,"REPLY_DATE 1: July 29, 2019, 11:06am; <NewLine> REPLY_DATE 2: July 3, 2020,  9:09am; <NewLine> REPLY_DATE 3: July 31, 2020, 10:18am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
89314,Combining two pretrained models for Training,2020-07-15T11:58:04.642Z,1,90,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to use resnet18 and densenet121 as the pretrained models and have added 1 FC layer at the end of each network two change dimensions to 512 of both network and then have concatenated the outputs and passed to two final FC layers.</p><NewLine><p>this can be seen here:</p><NewLine><pre><code class=""lang-auto"">class classifier(nn.Module):<NewLine>  def __init__(self,num_classes):<NewLine>    super(classifier,self).__init__()<NewLine><NewLine>    self.resnet=models.resnet18(pretrained=True)<NewLine><NewLine>    self.rfc1=nn.Linear(512,512)<NewLine>    <NewLine>    self.densenet=models.densenet121(pretrained=True)<NewLine>    self.dfc1=nn.Linear(1024,512)<NewLine><NewLine>    self.final_fc1=nn.Linear(1024,512)<NewLine>    self.final_fc2=nn.Linear(512,num_classes)<NewLine>    self.dropout=nn.Dropout(0.2)<NewLine><NewLine>  def forward(self,x):<NewLine>    y=x.detach().clone()<NewLine><NewLine>    x=self.resnet.conv1(x)<NewLine>    x=self.resnet.bn1(x)<NewLine>    x=self.resnet.relu(x)<NewLine>    x=self.resnet.maxpool(x)<NewLine>    x=self.resnet.layer1(x)<NewLine>    x=self.resnet.layer2(x)<NewLine>    x=self.resnet.layer3(x)<NewLine>    x=self.resnet.layer4(x)<NewLine>    x=self.resnet.avgpool(x)<NewLine>    x=x.view(x.size(0),-1)<NewLine>    x=nn.functional.relu(self.rfc1(x))<NewLine><NewLine>    y=self.densenet.features(y)<NewLine>    y=y.view(y.size(0),-1)<NewLine>    y=nn.functional.relu(self.dfc1(y))<NewLine><NewLine>    x=torch.cat((x,y),0)<NewLine>    x=nn.functional.relu(self.final_fc1(x))<NewLine>    x=self.dropout(x)<NewLine>    x=self.final_fc2(x)<NewLine><NewLine>    return x<NewLine></code></pre><NewLine><p>Error:</p><NewLine><pre><code class=""lang-auto"">size mismatch, m1: [1048576 x 1], m2: [1024 x 512] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:283<NewLine></code></pre><NewLine><p>but as i could see the densenet outputs 1024 feature to the last layer</p><NewLine><p>Questions:</p><NewLine><ol><NewLine><li>Is the implementation correct ?</li><NewLine><li>Can this implementation work as a Ensemble</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/deadwalker7539,(Aakash kaushik),deadwalker7539,"July 15, 2020, 12:00pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Your current implementation misses the <code>relu</code> as well as adaptive pooling layer from the <code>DenseNet</code> implementation as used <a href=""https://github.com/pytorch/vision/blob/03b1d38ba3c67703e648fb067570eeb1a1e61265/torchvision/models/densenet.py#L193-L194"">here</a>, which would create a tensor of <code>[batch_size, 1024, 7, 7]</code> for the output of <code>self.densenet.features</code>.<br/><NewLine>Also, you might want to concatenate the activation tensors in <code>dim1</code> (feature dimension).</p><NewLine><p>These changes should work:</p><NewLine><pre><code class=""lang-python"">...<NewLine>    y = self.densenet.features(y)<NewLine>    y = F.relu(y)<NewLine>    y = F.adaptive_avg_pool2d(y, (1, 1))<NewLine>    y = y.view(y.size(0), -1)<NewLine>    # y = F.relu(self.dfc1(y)) remove this here, if you are using the first relu<NewLine><NewLine>    x = torch.cat((x,y), 1)<NewLine>...<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, This Worked.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/deadwalker7539; <NewLine> ,"REPLY_DATE 1: July 26, 2020,  6:08pm; <NewLine> REPLY_DATE 2: July 26, 2020,  6:09pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> 
90541,Unexpected cuda behaviour when registering buffers,2020-07-25T21:21:24.594Z,0,48,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello all,<br/><NewLine>I’ve encountered an issue while trying to register buffers from another module to a custom made module while doing the same thing with parameters (registering parameters) the method behaved as expected.</p><NewLine><p>Here is a smaple of my code</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torchvision.models<NewLine><NewLine>class MyModule(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super(MyModule, self).__init__()<NewLine><NewLine>resnet18_model = torchvision.models.resnet18()<NewLine>my_model = MyModule()<NewLine><NewLine>print(""Checking Buffers"")<NewLine><NewLine>for buffer_name, buffer in list(resnet18_model.named_buffers()):<NewLine>    my_model.register_buffer(buffer_name.replace('.', '_'), buffer)<NewLine>resnet18_model.cuda()<NewLine>for buffer in list(my_model.buffers()):<NewLine>    print(buffer.is_cuda)<NewLine><NewLine>print(""Checking Parameters"")<NewLine><NewLine>for param_name, param in list(resnet18_model.named_parameters()):<NewLine>    my_model.register_parameter(param_name.replace('.', '_'), param)<NewLine>resnet18_model.cuda()<NewLine>for param in list(my_model.parameters()):<NewLine>    print(param.is_cuda)<NewLine><NewLine></code></pre><NewLine><p>My expected behaviour of this code was as follows :<br/><NewLine>after loading the resnet18 module into cuda, and after registering its parameters and buffers into my module I expected both of them to be in cuda, as you can see i’ve checked if is_coda flag is on and my results where as follows:</p><NewLine><pre><code class=""lang-auto"">Checking Buffers<NewLine>False <NewLine>False<NewLine>.<NewLine>.<NewLine>.<NewLine>Checking Parameters<NewLine>True<NewLine>True<NewLine>.<NewLine>.<NewLine>.<NewLine></code></pre><NewLine><p>it seems as if the <code>register_buffer</code> uses some sort of  “copying” the input buffers and not copying its address (reference) to the memory, while <code>register_parameters</code> is acting as expected.</p><NewLine><p>Is it a bug? or i’m missing something?<br/><NewLine>Is there a different way of registering the buffers pointers into other modules?</p><NewLine></div>",https://discuss.pytorch.org/u/Omer_B,(Omer B),Omer_B,"July 25, 2020,  9:21pm",,,,,
89998,Binning tensor values,2020-07-21T08:22:56.894Z,0,57,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I am looking for a way to bin tensor values.<br/><NewLine>Say my tensor has values between -0.255 and +0.256, I am looking for a way to represent this will “less numbers”.<br/><NewLine>For example I would like to bin this range into 32 values so I will only have the values -0.255, -0.239, -0.223 and so on… (the bins are in jumps of 16 because 512/32=16)<br/><NewLine>I found torch.histc but it gives me only a histogram, this way I lose “which index was mapped to each bin”.<br/><NewLine>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/erap129,(Elad Rapaport),erap129,"July 21, 2020,  8:22am",,,,,
89970,Export LSTM to ONNX with sequence length and dynamic batch size,2020-07-21T02:21:40.598Z,0,76,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I use LSTM to modeling text with the following code, the shape of <code>inputs</code> is [batch_size, max_seq_len, embedding_size], the shape of <code>input_lens</code> is [batch_size]. <code>rnn</code> is simply a bidirectional LSTM defined as follows:</p><NewLine><p><code>self.rnn = nn.LSTM(self.input_dim, self.hidden_size, self.num_layers,                                bidirectional=self.bidirectional, dropout=self.dropout, batch_first=True)</code></p><NewLine><pre><code>       `padded_seq_len = inputs.shape[1]<NewLine><NewLine>       # Sort by length (keep idx)<NewLine>        _, idx_sort = torch.sort(input_lens, dim=0, descending=True)<NewLine>        _, idx_unsort = torch.sort(idx_sort, dim=0)<NewLine><NewLine>        inputs = inputs.index_select(0, idx_sort)<NewLine>        input_lens = list(input_lens[idx_sort])<NewLine><NewLine>        # Handling padding in Recurrent Networks<NewLine>        inputs_packed = nn.utils.rnn.pack_padded_sequence(inputs, input_lens, batch_first=True)<NewLine>        self.rnn.flatten_parameters()<NewLine>        if str(self.cell).lower() == 'lstm':<NewLine>            outputs, (hn, cn) = self.rnn(inputs_packed)<NewLine>        elif str(self.cell).lower() == 'gru':<NewLine>            outputs, hn = self.rnn(inputs_packed)<NewLine><NewLine>        outputs = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True, total_length=padded_seq_len)[0]<NewLine><NewLine>        # Un-sort by length<NewLine>        outputs = outputs.index_select(0, idx_unsort)<NewLine>        hn = hn.index_select(1, idx_unsort)`<NewLine></code></pre><NewLine><p>However, when I try to export this model to ONNX with dynamic batch_size, I cannot get the parity result with the original pytorch checkpoint. Anyone has such experience?</p><NewLine></div>",https://discuss.pytorch.org/u/zhujiangang,(Jiangang Zhu),zhujiangang,"July 21, 2020,  2:21am",,,,,
89525,Pytorch mobile deployment,2020-07-17T02:29:20.572Z,0,74,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi. I am using pytorch 1.5.0. I have put gradle dependency as</p><NewLine><pre><code class=""lang-auto"">dependencies {<NewLine>    implementation fileTree(dir: ""libs"", include: [""*.jar""])<NewLine>    implementation 'androidx.appcompat:appcompat:1.1.0'<NewLine>    implementation 'androidx.constraintlayout:constraintlayout:1.1.3'<NewLine>    testImplementation 'junit:junit:4.12'<NewLine>    androidTestImplementation 'androidx.test.ext:junit:1.1.1'<NewLine>    androidTestImplementation 'androidx.test.espresso:espresso-core:3.2.0'<NewLine>    implementation 'org.pytorch:pytorch_android:1.4.0'<NewLine>    implementation 'org.pytorch:pytorch_android_torchvision:1.4.0'<NewLine><NewLine>}<NewLine></code></pre><NewLine><p>I trained my model on CPU and used this script to export</p><NewLine><pre><code class=""lang-auto"">model_ft.eval()<NewLine>example = torch.rand(1, 3, 224, 224)<NewLine>traced_script_module = torch.jit.trace(model_ft, example)<NewLine>traced_script_module.save(""model_res.pt"")<NewLine></code></pre><NewLine><p>When I run the detection in android, it shows</p><NewLine><pre><code class=""lang-auto"">Process: com.example.pytorchmobile, PID: 7756<NewLine>    com.facebook.jni.CppException: empty_strided not implemented for DispatchKeySet(BackendSelect, CUDATensorId) (empty_strided at aten/src/ATen/Functions.h:4051)<NewLine>    (no backtrace available)<NewLine></code></pre><NewLine><p>Please HELP ME.</p><NewLine></div>",https://discuss.pytorch.org/u/zeus,(pyzeus),zeus,"July 17, 2020,  2:30am",,,,,
89305,Torchserve - how to log custom metrics?,2020-07-15T10:55:24.878Z,0,57,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was going through the TorchServe <a href=""https://github.com/pytorch/serve/blob/master/docs/metrics.md"" rel=""nofollow noopener"">documentation</a> for logging custom metrics but I’m not sure I fully understand it, especially the use of <code>Dimension</code> class. What exactly is its purpose?</p><NewLine><p>Can someone please explain with a short toy example? Let’s say I’m serving a classification example and I want to log 2 things -</p><NewLine><ol><NewLine><li>Probability of the predicted class</li><NewLine><li>Probability of the top 5 predicted classes.</li><NewLine></ol><NewLine><p>How can I do it? Thanks a lot!</p><NewLine></div>",https://discuss.pytorch.org/u/vishalagarwal,(Vishal Agarwal),vishalagarwal,"July 15, 2020, 10:55am",,,,,
89192,How to compile pytorch in an executable?,2020-07-14T16:35:44.970Z,0,74,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, Im trying to compile a game using a pytorch model for part of the game. I want it in an executable format. For some context, I am doing this using cx_freeze on windows</p><NewLine><p>But im getting a compilation error as shown in the image below. (Sorry, was unable to extract the text efficiently)<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/a24298e958c4b56cd70536ca97a6afdaf8a464f1"" href=""https://discuss.pytorch.org/uploads/default/original/3X/a/2/a24298e958c4b56cd70536ca97a6afdaf8a464f1.jpeg"" title=""photo_2020-07-15_00-22-46""><img alt=""photo_2020-07-15_00-22-46"" data-base62-sha1=""n9q27OTrpB9Xv7gLeJi6dYOXFGp"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/2/a24298e958c4b56cd70536ca97a6afdaf8a464f1_2_10x10.png"" height=""499"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/2/a24298e958c4b56cd70536ca97a6afdaf8a464f1_2_214x499.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/2/a24298e958c4b56cd70536ca97a6afdaf8a464f1_2_214x499.jpeg, https://discuss.pytorch.org/uploads/default/optimized/3X/a/2/a24298e958c4b56cd70536ca97a6afdaf8a464f1_2_321x748.jpeg 1.5x"" width=""214""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">photo_2020-07-15_00-22-46</span><span class=""informations"">414×965 91.6 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine>I dug around the errors a bit, and saw that the issue was related to how _VF is importing torch from within torch itself? I am unsure how to amend this though.</p><NewLine><p>Any suggestions as to how I can troubleshoot this? The only thing i need the model to be able to do in compilation after training the weights before all this is to feedforward an image/stack of images and come to a predicted action.</p><NewLine><p>Any advise or suggestion is much appreciated! Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/blackonyyx,(Stephen Tan),blackonyyx,"July 14, 2020,  4:35pm",,,,,
74340,Export object detection model to ONNX:empty output by ONNX inference,2020-03-25T12:20:17.960Z,0,470,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I try to convert my PyTorch object detection model (Faster R-CNN) to ONNX. I have two setups. The first one is working correctly but I want to use the second one for deployment reasons. The difference lies in the example image which I use for the export of the function <em>torch.onnx.export()</em>.</p><NewLine><p>In the first setup I use a real image as input for the ONNX export. But in a official <a href=""https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html"" rel=""nofollow noopener"">tutorial</a> they say that I can use a dummy input, which should have the same size as the model expects the input. So I created a tensor with the same shape but with random values. The export in both setups is working correctly. But the second setup does not deliver the desired results after inference with the ONNX runtime. The code and the exemplary output can be found below.</p><NewLine><p><strong>Setup 1</strong></p><NewLine><pre><code>model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)<NewLine>...<NewLine>checkpoint = torch.load(model_state_dict_path)<NewLine>model.load_state_dict(checkpoint['model_state_dict'])<NewLine>model.eval()<NewLine><NewLine>to_tensor = transforms.ToTensor()<NewLine>img_rgb = Image.open(image_path_model).convert('RGB')<NewLine>img_rgb = to_tensor(img_rgb)<NewLine>img_rgb.unsqueeze_(0)    <NewLine><NewLine>torch.onnx.export(model, img_rgb, ""detection.onnx"", opset_version=11) <NewLine></code></pre><NewLine><p>I get no error and the export works. Afterwards I run the model with the ONNX runtime and I get the following output:</p><NewLine><pre><code>[array([[704.0696  , 535.19556 , 944.8986  , 786.1619  ],<NewLine>         ...], dtype=float32),<NewLine>array([2, 2, 2, 2, 2, 1, 1], dtype=int64),<NewLine>array([0.9994363 , 0.9984769 , 0.99816966, ...], dtype=float32)]<NewLine></code></pre><NewLine><p>The output is as I expect it to be (Bounding boxes, object classes and probabilities).</p><NewLine><p><strong>Setup 2</strong></p><NewLine><pre><code>model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)<NewLine>...<NewLine>checkpoint = torch.load(model_state_dict_path)<NewLine>model.load_state_dict(checkpoint['model_state_dict'])<NewLine>model.eval()<NewLine><NewLine>img_rgb = torch.randn(1, 3, 1024, 1024)   <NewLine><NewLine>torch.onnx.export(model, img_rgb, ""detection.onnx"", opset_version=11) <NewLine></code></pre><NewLine><p>Like in setup 1 I get no error and the export works. Afterwards I run the model with the ONNX runtime and with the same image as in setup 1 and I get the following output:</p><NewLine><pre><code>[array([], shape=(0, 4), dtype=float32),<NewLine>array([], dtype=int64),<NewLine>array([], dtype=float32)]<NewLine></code></pre><NewLine><p>It is just an empty array.</p><NewLine><p>What is wrong with the second setup? I am new to ONNX. The export runs the model. Do I have to provide an input on which the model also recognizes objects and therefore the dummy input with random values does not work? Is the statement <em>“The values in this can be random as long as it is the right type and size.”</em> only valid for the provided tutorial?</p><NewLine></div>",https://discuss.pytorch.org/u/tominator,,tominator,"March 25, 2020, 12:20pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Related: <a href=""https://github.com/pytorch/vision/issues/1706#issuecomment-658198204"" rel=""nofollow noopener"">https://github.com/pytorch/vision/issues/1706#issuecomment-658198204</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/danilopeixoto; <NewLine> ,"REPLY_DATE 1: July 14, 2020,  2:06pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
89157,Commercial licensing of pytorch with CUDA,2020-07-14T13:35:08.043Z,0,55,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Not sure if this is the right place to ask but I guess experience is around. Is there any problem in packaging a commercial application with pytorch + CUDA for GPU support as long as everything is correctly cited and documented ? I have search online but couldn’t find any clear answer to this.</p><NewLine></div>",https://discuss.pytorch.org/u/Jaap,,Jaap,"July 14, 2020,  1:35pm",,,,,
88883,Workaround for index_add / masked_scatter,2020-07-12T15:18:29.049Z,0,34,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m currently trying to export a model to ONNX and run it using an external library that parses ONNX. Unfortunately my model uses <a href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor.index_add_"" rel=""nofollow noopener"">index_add</a> and <a href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor.masked_scatter_"" rel=""nofollow noopener"">masked_scatter</a> and the external library can’t parse the ONNX operations these two are transformed into (I believe index_add doesn’t even have an ONNX equivalent for opset 11 which is what I’m using). Is there any way I can rewrite these operations into “simpler” ONNX compatible operations, without a major dip in performance (i. e. without having to write explicit for loops in python)?</p><NewLine></div>",https://discuss.pytorch.org/u/IgnacioPickering,(Ignacio Pickering),IgnacioPickering,"July 12, 2020,  3:24pm",,,,,
50257,Basic operations do not work in 1.1.0 with uWSGI+Flask,2019-07-10T13:21:13.864Z,3,740,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Came across the problem, that in pytorch <strong>1.1.0</strong> with uwsgi+flask on cpu even <code>torch.cat</code> does not work (everything just <em>freezes without errors</em>). I determined that problem is in <strong>uwsgi/flask</strong>, because in the same environment I was able to do the same operations without any issues. There was <strong>no</strong> such problem in previous versions of pytorch. As far as I understand, I am dealing with <em>multiprocessing</em> artifacts…<br/><NewLine>Nevertheless I <strong>found</strong> the solution and it was simple:</p><NewLine><pre><code class=""lang-python"">app = flask.Flask(__name__)<NewLine>segmentator = None<NewLine><NewLine>@app.before_first_request<NewLine>def load_segmentator():<NewLine>    global segmentator<NewLine>    segmentator = Segmentator()<NewLine></code></pre><NewLine><p>where <code>Segmentator</code> is a class with pytorch’s <code>nn.Module</code>, which loads weights in <code>__init__</code><br/><NewLine>Hope, it will help somebody.</p><NewLine><p>P.S. If someone explains what is going on here, will be grateful.</p><NewLine></div>",https://discuss.pytorch.org/u/lebionick,(Nikolay Maslovich),lebionick,"July 10, 2019,  1:22pm",3 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m facing the same issue. i tried a similar solution but still doesn’t seem to work. Any more information about this would be appreciated</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>To be honest I’m not really understand why it worked and cannot say much more in terms of code.<br/><NewLine>Here is my “code” inside <code>Segmentator</code>:</p><NewLine><pre><code class=""lang-auto"">class Segmentator:<NewLine>    def __init__(self, path):<NewLine>        print('loading model')<NewLine>        loaded_model = UNetWithResnet50Encoder(9)<NewLine>        print('loading weights')<NewLine>        checkpoint = torch.load(path, map_location='cpu')<NewLine>        print('loading checkpoint')<NewLine>        loaded_model.load_state_dict(checkpoint['model'])<NewLine>        loaded_model.eval()<NewLine>        print('loaded!')<NewLine>        self.model = loaded_model<NewLine><NewLine>    def process(img_url):<NewLine>        # loads image into RAM, preprocesses it,<NewLine>        # passes trough self.model, and postprocesses output<NewLine>        pass<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/lebionick"">@lebionick</a> thanks for the info. I actually have the exact same project structure as your code. <code>Segmentor</code> just named differently <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/> I had another solution for another app but I changed the solution to be exactly like yours with the <code>before_first_request</code> still to no success. I will report back if I find out more about this.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>ok it seems <code>load_state_dict</code> and other operations in <code>__init__</code> when run after <code>flask.Flask(__name__)</code> cause pytorch operations done in requests to hang forever. Not sure about the cause but maybe this info could help someone</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Try  <a href=""https://github.com/craigsidcarlson/PytorchFlaskApp/blob/master/app/application.py"" rel=""nofollow noopener"">this</a></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Don’t you running the flask app with uWSGI’s “preforking worker mode” that is a basic config?<br/><NewLine>Try <code>lazy-apps</code> mode that each worker will load the trained model for them self and not share with others.<br/><NewLine>It works in my environment even I load model in the global.</p><NewLine><p>command<br/><NewLine><code>uwsgi --lazy-app </code></p><NewLine><p>or ini file</p><NewLine><pre><code class=""lang-auto"">[uwsgi]<NewLine>lazy-apps = true<NewLine></code></pre><NewLine><p>minimum reproduction<br/><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""32""/><NewLine><a href=""https://github.com/keng000/uwsgi-flask-ml"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""400"" src=""https://avatars2.githubusercontent.com/u/21191288?s=400&amp;v=4"" width=""400""/><NewLine><h3><a href=""https://github.com/keng000/uwsgi-flask-ml"" rel=""nofollow noopener"" target=""_blank"">keng000/uwsgi-flask-ml</a></h3><NewLine><p>Contribute to keng000/uwsgi-flask-ml development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>By setting lazy-apps to be true, it solves the problem I had with flask+uwsgi+pytorch deployment. Thx.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ijnf; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/lebionick; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ijnf; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ijnf; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Khan1; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/keng000; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/randomwalk10; <NewLine> ,"REPLY_DATE 1: August 2, 2019, 11:28am; <NewLine> REPLY_DATE 2: August 2, 2019, 11:54am; <NewLine> REPLY_DATE 3: August 2, 2019, 12:14pm; <NewLine> REPLY_DATE 4: August 2, 2019,  3:18pm; <NewLine> REPLY_DATE 5: August 2, 2019,  4:08pm; <NewLine> REPLY_DATE 6: May 5, 2020,  1:49am; <NewLine> REPLY_DATE 7: July 12, 2020, 12:29pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> REPLY 3 LIKES: 2 Likes; <NewLine> REPLY 4 LIKES: 3 Likes; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> 
88713,Deploying action recognition model,2020-07-10T17:20:28.254Z,0,65,"<div class=""post"" itemprop=""articleBody""><NewLine><p>If performance is the priority here, would it be okay to deploy a heavy model for action recognition natively on mobile (using pytorch mobile)?</p><NewLine><p>Or should I go with something like rest api, hosted on other remote server (amazon, azure, etc).</p><NewLine></div>",https://discuss.pytorch.org/u/braindotai,,braindotai,"July 10, 2020,  5:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Having it on remote is more logical unless you have very bad internet connection.<br/><NewLine>its not customary to place the actual model on the device, as it puts everything into risk.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> ,"REPLY_DATE 1: July 11, 2020,  9:24am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
88531,How to convert checkpoint file (.ckpt) to state dict file (.pth)?,2020-07-09T09:24:34.472Z,0,166,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! New PyTorch user here <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/><br/><NewLine>I am trained my model using Pytorch Lighting and ModelCheckpoint with parameter <code>save_top_k=1</code>, so only the best checkpoint is saved. . After the training is finished I saved the model as usual with <code>torch.save(model.state_dict())</code>.<br/><NewLine>Now I want to deploy my model for inference. My <code>epoch=42.ckpt</code> file contains a model with better performance than the final model, so I want to use this checkpoint file. But the checkpoint file is three times larger than the normal model file (.pth).<br/><NewLine>How do I convert a .ckpt file to a .pth file?</p><NewLine></div>",https://discuss.pytorch.org/u/davic11915,,davic11915,"July 9, 2020,  9:24am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>CC <a class=""mention"" href=""/u/williamfalcon"">@williamFalcon</a> and <a class=""mention"" href=""/u/justusschock"">@justusschock</a> for Lightning questions <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 11, 2020,  8:11am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
88443,Error with XNNPACK,2020-07-08T21:26:23.158Z,0,74,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When trying to build pytorch 1.5 on centos 6.7, I get the following error from XNNPACK:</p><NewLine><p>In file included from …/third_party/XNNPACK/include/xnnpack.h:15:0,<br/><NewLine>from …/third_party/XNNPACK/src/xnnpack/params.h:15,<br/><NewLine>from …/third_party/XNNPACK/src/xnnpack/raddextexp.h:11,<br/><NewLine>from …/third_party/XNNPACK/src/f32-raddextexp/gen/avx2-p5-x80-acc5.c:15:<br/><NewLine>…/third_party/pthreadpool/include/pthreadpool.h:183:2: warning: ‘pthreadpool_function_1d_t’ is deprecated [-Wdeprecated-declarations]<br/><NewLine>pthreadpool_function_1d_t function,<br/><NewLine>^~~~~~~~~~~~~~~~~~~~~~~~~<br/><NewLine>…/third_party/pthreadpool/include/pthreadpool.h:189:2: warning: ‘pthreadpool_function_1d_tiled_t’ is deprecated [-Wdeprecated-declarations]<br/><NewLine>pthreadpool_function_1d_tiled_t function,<br/><NewLine>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/><NewLine>…/third_party/pthreadpool/include/pthreadpool.h:196:2: warning: ‘pthreadpool_function_2d_t’ is deprecated [-Wdeprecated-declarations]<br/><NewLine>pthreadpool_function_2d_t function,<br/><NewLine>^~~~~~~~~~~~~~~~~~~~~~~~~<br/><NewLine>…/third_party/pthreadpool/include/pthreadpool.h:203:2: warning: ‘pthreadpool_function_2d_tiled_t’ is deprecated [-Wdeprecated-declarations]<br/><NewLine>pthreadpool_function_2d_tiled_t function,<br/><NewLine>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/><NewLine>…/third_party/pthreadpool/include/pthreadpool.h:212:2: warning: ‘pthreadpool_function_3d_tiled_t’ is deprecated [-Wdeprecated-declarations]<br/><NewLine>pthreadpool_function_3d_tiled_t function,<br/><NewLine>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/><NewLine>…/third_party/pthreadpool/include/pthreadpool.h:223:2: warning: ‘pthreadpool_function_4d_tiled_t’ is deprecated [-Wdeprecated-declarations]<br/><NewLine>pthreadpool_function_4d_tiled_t function,<br/><NewLine>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/><NewLine>/tmp/ccQoCO7m.s: Assembler messages:<br/><NewLine>/tmp/ccQoCO7m.s:303: Error: suffix or operands invalid for <code>vpslld' /tmp/ccQoCO7m.s:307: Error: suffix or operands invalid for </code>vpslld’<br/><NewLine>/tmp/ccQoCO7m.s:311: Error: suffix or operands invalid for <code>vpslld' /tmp/ccQoCO7m.s:314: Error: suffix or operands invalid for </code>vpslld’<br/><NewLine>/tmp/ccQoCO7m.s:317: Error: suffix or operands invalid for <code>vpslld' /tmp/ccQoCO7m.s:320: Error: suffix or operands invalid for </code>vpslld’<br/><NewLine>/tmp/ccQoCO7m.s:323: Error: suffix or operands invalid for <code>vpslld' /tmp/ccQoCO7m.s:326: Error: suffix or operands invalid for </code>vpslld’<br/><NewLine>/tmp/ccQoCO7m.s:331: Error: suffix or operands invalid for <code>vpslld' /tmp/ccQoCO7m.s:336: Error: suffix or operands invalid for </code>vpslld’<br/><NewLine>/tmp/ccQoCO7m.s:341: Error: suffix or operands invalid for <code>vpslld' /tmp/ccQoCO7m.s:345: Error: suffix or operands invalid for </code>vpslld’<br/><NewLine>/tmp/ccQoCO7m.s:348: Error: suffix or operands invalid for <code>vpslld' /tmp/ccQoCO7m.s:353: Error: suffix or operands invalid for </code>vpslld’<br/><NewLine>/tmp/ccQoCO7m.s:359: Error: suffix or operands invalid for <code>vpslld' /tmp/ccQoCO7m.s:391: Error: suffix or operands invalid for </code>vpslld’<br/><NewLine>/tmp/ccQoCO7m.s:396: Error: suffix or operands invalid for <code>vpslld' /tmp/ccQoCO7m.s:400: Error: suffix or operands invalid for </code>vpslld’<br/><NewLine>/tmp/ccQoCO7m.s:404: Error: suffix or operands invalid for <code>vpslld' /tmp/ccQoCO7m.s:405: Error: suffix or operands invalid for </code>vpslld’<br/><NewLine>/tmp/ccQoCO7m.s:448: Error: suffix or operands invalid for <code>vpslld' /tmp/ccQoCO7m.s:451: Error: suffix or operands invalid for </code>vpslld’<br/><NewLine>/tmp/ccQoCO7m.s:468: Error: suffix or operands invalid for <code>vpslld' /tmp/ccQoCO7m.s:516: Error: no such instruction: </code>vinserti128 $0x1,16(%rax),%ymm1,%ymm5’<br/><NewLine>/tmp/ccQoCO7m.s:538: Error: suffix or operands invalid for <code>vpslld' /tmp/ccQoCO7m.s:541: Error: suffix or operands invalid for </code>vpslld’</p><NewLine><p>I used gcc 7.4 for this build. Any ideas how to resolve this issue? Is there any way to disable this module, and if yes, what is the side effect on speed?</p><NewLine></div>",https://discuss.pytorch.org/u/Mohammadreza_Nazari,(Mohammadreza Nazari),Mohammadreza_Nazari,"July 8, 2020,  9:26pm",,,,,
88348,Torch custom model to onnx conversion,2020-07-08T08:45:33.418Z,0,48,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Can we convert our model whose layers are defined in seperate custom classes into onnx form for deloyment. Model is defined in the class, and <em>init</em> of this class is instanciating layer (object) of other classes.</p><NewLine></div>",https://discuss.pytorch.org/u/jvarshney,(jatin Varshney),jvarshney,"July 8, 2020,  8:45am",,,,,
87983,Error when Load GPU-trained Model to CPU,2020-07-05T02:41:37.513Z,0,77,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>Here are the piece of code when I try to load a gpu-trained model to cpu (and want to use CPU for evaluation):</p><NewLine><pre><code class=""lang-python"">model_conv.load_state_dict(torch.load(model_file, map_location='cpu'))<NewLine>model_conv = model_conv.cpu()<NewLine></code></pre><NewLine><p>and the error message is:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""prediction.py"", line 269, in &lt;module&gt;<NewLine>    model_conv.load_state_dict(torch.load(resume_file, map_location='cpu'))<NewLine>  File ""/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/serialization.py"", line 229, in load<NewLine>    return _load(f, map_location, pickle_module)<NewLine>  File ""/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/serialization.py"", line 377, in _load<NewLine>    result = unpickler.load()<NewLine>  File ""/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/serialization.py"", line 348, in persistent_load<NewLine>    data_type(size), location)<NewLine>  File ""/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/serialization.py"", line 246, in restore_location<NewLine>    result = map_location(storage, location)<NewLine>TypeError: 'str' object is not callable<NewLine></code></pre><NewLine><p>My pytorch version is <code>0.1.12_1</code>. Any idea how to fix this? I have checked <a href=""https://stackoverflow.com/questions/55511857/how-to-load-the-gpu-trained-model-into-the-cpu"" rel=""nofollow noopener"">https://stackoverflow.com/questions/55511857/how-to-load-the-gpu-trained-model-into-the-cpu</a> but the solution seems not work in my case.</p><NewLine><p>Without <code>map_location</code> argument, the error msg is</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""prediction.py"", line 268, in &lt;module&gt;<NewLine>    model_conv.load_state_dict(torch.load(resume_file))<NewLine>  File ""/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/serialization.py"", line 229, in load<NewLine>    return _load(f, map_location, pickle_module)<NewLine>  File ""/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/serialization.py"", line 377, in _load<NewLine>    result = unpickler.load()<NewLine>  File ""/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/serialization.py"", line 348, in persistent_load<NewLine>    data_type(size), location)<NewLine>  File ""/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/serialization.py"", line 85, in default_restore_location<NewLine>    result = fn(storage, location)<NewLine>  File ""/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/serialization.py"", line 67, in _cuda_deserialize<NewLine>    return obj.cuda(device_id)<NewLine>  File ""/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/_utils.py"", line 57, in _cuda<NewLine>    with torch.cuda.device(device):<NewLine>  File ""/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.py"", line 124, in __enter__<NewLine>    _lazy_init()<NewLine>  File ""/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.py"", line 84, in _lazy_init<NewLine>    _check_driver()<NewLine>  File ""/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.py"", line 58, in _check_driver<NewLine>    http://www.nvidia.com/Download/index.aspx"""""")<NewLine>AssertionError: <NewLine>Found no NVIDIA driver on your system. Please check that you<NewLine>have an NVIDIA GPU and installed a driver from<NewLine>http://www.nvidia.com/Download/index.aspx<NewLine></code></pre><NewLine><p>Any idea how to fix this problem? Any suggestion is appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/t326wang,,t326wang,"July 5, 2020,  2:41am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>PyTorch <code>0.1.12_1</code> is one of the earliest releases, so I would strongly recommend to update to the latest stable release (<code>1.5.1</code>).</p><NewLine><p>In <code>0.1.12</code>, you had to pass a function to <code>map_location</code> via:</p><NewLine><pre><code class=""lang-python"">torch.load(..., map_location=lambda storage, location: 'cpu')<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 6, 2020,  6:29am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
87950,Easier method for model inference and deployment,2020-07-04T15:01:07.136Z,1,85,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have been looking into ways to deploy and use models after they are trained. I originally started machine/deep learning using Keras/Tensorflow, and one of the options with them is the ability to save the model architecture as a <code>.json</code> file and loading the weights into that. Is there any plans on doing something like with with PyTorch? For a lot of use cases, it isn’t ideal to use the same development project to rebuild the model architecture to load the weights into in the deployment use-case.</p><NewLine></div>",https://discuss.pytorch.org/u/kleingeo,(Geoff Klein),kleingeo,"July 4, 2020,  3:01pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you tried using Torchscript or exporting the model to onnx? They are self contained with weights and model structure.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I haven’t looked too much into Onnx, but that is on my radar. Not sure what Torchscript is, I should look into that.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/RicCu; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kleingeo; <NewLine> ,"REPLY_DATE 1: July 4, 2020,  5:12pm; <NewLine> REPLY_DATE 2: July 5, 2020, 12:27am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
87495,Not all model parameters are transfered to GPU,2020-06-30T18:08:17.920Z,0,72,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m noticing that if I run <code>model.to(device)</code>, the tensors used in the <code>forward</code> are not necessarily transferred over the GPU. Below is a minimal example of this</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>from torch.distributions import MultivariateNormal<NewLine><NewLine>class SimpleModel(torch.nn.Module):<NewLine>    def __init__(self, q):<NewLine>        super(SimpleModel, self).__init__()<NewLine>        self._zero_mean = torch.zeros(2 * q)<NewLine>        self._eye_covar = torch.eye(2 * q)<NewLine>        self.mvn = MultivariateNormal(self._zero_mean, self._eye_covar)<NewLine>    def reparameterize(self, mu, logv):<NewLine>        eps = torch.randn_like(mu)  <NewLine>        z = mu + eps * torch.exp(logv)  <NewLine>        logp = self.mvn.log_prob(eps) <NewLine>        return z, logp        <NewLine>    def forward(self, x):<NewLine>        return self.reparameterize(x, x)<NewLine><NewLine>model = SimpleModel(10).to('cuda')<NewLine>x = torch.ones(20).to('cuda')<NewLine>model(x)<NewLine></code></pre><NewLine><p>This gives the following error</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-15-bd0200007a4a&gt; in &lt;module&gt;<NewLine>----&gt; 1 model(x)<NewLine><NewLine>~/miniconda3/envs/mavi/lib/python3.8/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)<NewLine>    548             result = self._slow_forward(*input, **kwargs)<NewLine>    549         else:<NewLine>--&gt; 550             result = self.forward(*input, **kwargs)<NewLine>    551         for hook in self._forward_hooks.values():<NewLine>    552             hook_result = hook(self, input, result)<NewLine><NewLine>&lt;ipython-input-10-7cdded9571ea&gt; in forward(self, x)<NewLine>     14         return z, logp<NewLine>     15     def forward(self, x):<NewLine>---&gt; 16         return self.reparameterize(x, x)<NewLine><NewLine>&lt;ipython-input-10-7cdded9571ea&gt; in reparameterize(self, mu, logv)<NewLine>     11         eps = torch.randn_like(mu)<NewLine>     12         z = mu + eps * torch.exp(logv)<NewLine>---&gt; 13         logp = self.mvn.log_prob(eps)<NewLine>     14         return z, logp<NewLine>     15     def forward(self, x):<NewLine><NewLine>~/miniconda3/envs/mavi/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py in log_prob(self, value)<NewLine>    205         if self._validate_args:<NewLine>    206             self._validate_sample(value)<NewLine>--&gt; 207         diff = value - self.loc<NewLine>    208         M = _batch_mahalanobis(self._unbroadcasted_scale_tril, diff)<NewLine>    209         half_log_det = self._unbroadcasted_scale_tril.diagonal(dim1=-2, dim2=-1).log().sum(-1)<NewLine><NewLine>RuntimeError: expected device cuda:0 but got device cpu<NewLine></code></pre><NewLine><p>I know that I can manually override the <code>to()</code> and <code>cuda()</code> methods, so there is a solution. But I’m curious why the <code>to()</code> and <code>cuda()</code> can’t already handle these sorts of edge cases (or if there is something that I am overlooking).</p><NewLine></div>",https://discuss.pytorch.org/u/mortonjt,(Jamie Morton),mortonjt,"June 30, 2020,  6:08pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><p>Yes I don’t think the distribution objects are moved around by the nn.Modules.<br/><NewLine>But If you want <code>_zero_mean</code> and <code>_eye_covar</code> to be moved around, you need to register them as buffers in the nn.Module.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: July 15, 2020,  9:56pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
84383,Several models loaded but just one at a time in GPU,2020-06-06T08:39:53.644Z,1,94,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey guys,</p><NewLine><p>I am doing a website for comparing mask outputs of several models. However, this models are very big and just one model fits in my GPU at a time?</p><NewLine><p>I am passing the image throught several models sequently.</p><NewLine><p>Is there a way of moving to GPU a model, make inference and after that move model again to GPU?</p><NewLine></div>",https://discuss.pytorch.org/u/WaterKnight,(David Lacalle),WaterKnight,"June 6, 2020,  8:39am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>What comes to me mind (and I have done sth similar) is to prepare a dict that would look like:</p><NewLine><blockquote><NewLine><p>dict = {‘model1’: Model_1, ‘model2’: Model_2, … }</p><NewLine></blockquote><NewLine><p>where Model_X is a PyTorch model with loaded weights and sitting on CPU.</p><NewLine><p>During the run I would retrieve models sequentially by just getting one at a time from the dict. Move it to GPU, run inference, move back to CPU, clear GPU cache, load model2, …, etc.</p><NewLine><p>The problem is definitely the time of loading/unloading models to/from GPUs. That would take a while definitely.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have find a better approach <a class=""mention"" href=""/u/bonzogondo"">@bonzogondo</a> :</p><NewLine><aside class=""quote"" data-post=""3"" data-topic=""83948""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v4/letter/w/9d8465/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/api-rest-with-several-models-loaded-using-gpu-but-not-at-same-time/83948/3"">API Rest with several models loaded using GPU but not at same time</a><NewLine></div><NewLine><blockquote><NewLine>    Thank you for comming to this issue <img alt=""sweat_smile"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/sweat_smile.png?v=9"" title=""sweat_smile""/> <NewLine>I have tried that approach, the problem that I have seen is that if I don’t delete pred variable after using, it keeps memory in use and when I load other model cuda memory error is thrown!<NewLine>  </blockquote><NewLine></aside><NewLine><pre><code class=""lang-auto"">models = [...] # define a list of all models on the CPU<NewLine><NewLine>input = ... # get your input<NewLine>for model in models:<NewLine>    model.to('cuda')<NewLine>    pred = model(input)<NewLine>    #make something with pred<NewLine>    del pred<NewLine>    model.to('cpu')<NewLine></code></pre><NewLine><p>They key fact is that you need to delete predictions variable if you don’t wank cuda memory issues</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>This will be a good use case for TorchServe which supports multiple models. Please give that a try and provide your feedback.</p><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""16""/><NewLine><a href=""https://github.com/pytorch/serve"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars2.githubusercontent.com/u/21003710?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/pytorch/serve"" rel=""nofollow noopener"" target=""_blank"">pytorch/serve</a></h3><NewLine><p>Model Serving on PyTorch. Contribute to pytorch/serve development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/bonzogondo; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/WaterKnight; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/gchauhan; <NewLine> ,"REPLY_DATE 1: June 6, 2020,  4:04pm; <NewLine> REPLY_DATE 2: June 6, 2020,  4:22pm; <NewLine> REPLY_DATE 3: June 27, 2020,  8:20pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
68828,Batch Permutation op problem when converting caffe2 to onnx,2020-02-06T07:49:12.767Z,0,249,"<div class=""post"" itemprop=""articleBody""><NewLine><p>As the title suggests, I have a caffe2 model (model_init.pb and model.pb) and wanted to convert it into onnx via the following code snippet.</p><NewLine><pre><code class=""lang-auto"">import onnx<NewLine>import caffe2.python.onnx.frontend<NewLine>from caffe2.proto import caffe2_pb2<NewLine>from PIL import Image<NewLine>import numpy as np<NewLine><NewLine># We need to provide type and shape of the model inputs, <NewLine># see above Note section for explanation<NewLine>data_type = onnx.TensorProto.FLOAT<NewLine><NewLine>IMAGE_LOCATION = ""/home/an1/caffe2_model/input.jpg""<NewLine>INIT_NET = ""/home/an1/caffe2_model/model_init.pb""<NewLine>PREDICT_NET = ""/home/an1/caffe2_model/model.pb""<NewLine><NewLine># Read single image<NewLine>img = Image.open(IMAGE_LOCATION)<NewLine>img = np.array(img)<NewLine><NewLine># Convert HWC -&gt; CHW<NewLine>img = img.swapaxes(1, 2).swapaxes(0, 1)<NewLine><NewLine># Convert CHW -&gt; NCHW<NewLine>img = np.array([img])<NewLine><NewLine># Im info N x 3 tensor of (height, width, scale)<NewLine>im_info = np.reshape(np.array([img.shape[2], img.shape[3], 1.0]), (1,-1))<NewLine><NewLine>value_info = {<NewLine>    'data': (data_type, img.shape),<NewLine>    'im_info': (data_type, (1,3))<NewLine>}<NewLine><NewLine>predict_net = caffe2_pb2.NetDef()<NewLine>with open(PREDICT_NET, 'rb') as f:<NewLine>    predict_net.ParseFromString(f.read())<NewLine><NewLine>init_net = caffe2_pb2.NetDef()<NewLine>with open(INIT_NET, 'rb') as f:<NewLine>    init_net.ParseFromString(f.read())<NewLine><NewLine>onnx_model = caffe2.python.onnx.frontend.caffe2_net_to_onnx_model(<NewLine>    predict_net,<NewLine>    init_net,<NewLine>    value_info,<NewLine>)<NewLine><NewLine>onnx.checker.check_model(onnx_model)<NewLine></code></pre><NewLine><p>However, I’m encountering an error with the Batch Permutation op:<br/><NewLine><code>RuntimeError: [enforce fail at batch_permutation_op.cc:64] X.dim32(0) &gt; 0. 0 vs 0</code></p><NewLine><p>Does anyone have an idea what could be causing this?</p><NewLine></div>",https://discuss.pytorch.org/u/ash95,(Ash),ash95,"February 6, 2020,  7:51am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to do the same thing, and getting the same error. Did you ever fix this?</p><NewLine><p>EDIT: The solution is to use a larger image input size, I suspect because otherwise the successive pooling operations will reduce it to have a negative size or something. Also it must be a multiple of 32.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did anyone find the solution for this? I am still facing it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/9thDimension; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/GJoshi27; <NewLine> ,"REPLY_DATE 1: May 12, 2020,  3:25pm; <NewLine> REPLY_DATE 2: June 25, 2020,  4:54pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
86695,Performance drop of trained model in production,2020-06-24T10:04:31.604Z,1,97,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone,</p><NewLine><p>I am pretty sure this question has been asked hundreds of times. However, I failed to find an answer to my problem.</p><NewLine><p>Context: I’ve trained the ResNet18 model on 2 classes. In training 2700/3100 images per class, on validation: 680/770. The model trained well, I ended up with the accuracy of around 99% and loss around 0.01 - 0.02. Fine tuning, pretrained.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/d706925fd7bafb1b85e36ae35f45c373226d4487"" href=""https://discuss.pytorch.org/uploads/default/original/3X/d/7/d706925fd7bafb1b85e36ae35f45c373226d4487.jpeg"" title=""Training3""><img alt=""Training3"" data-base62-sha1=""uGcJDBudKILt7Xi8hOD18xL4Nn1"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/7/d706925fd7bafb1b85e36ae35f45c373226d4487_2_10x10.png"" height=""350"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/7/d706925fd7bafb1b85e36ae35f45c373226d4487_2_690x350.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/7/d706925fd7bafb1b85e36ae35f45c373226d4487_2_690x350.jpeg, https://discuss.pytorch.org/uploads/default/optimized/3X/d/7/d706925fd7bafb1b85e36ae35f45c373226d4487_2_1035x525.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/d/7/d706925fd7bafb1b85e36ae35f45c373226d4487_2_1380x700.jpeg 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Training3</span><span class=""informations"">1558×791 80 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>However, when we integrated the model in our system we quickly noticed it is half not as good as on validation data even though the data we’ve been feeding to the model is pretty much the same. I am 99% sure the problem is in preprocessing, however I am unable to find where I made a mistake. I’ve been staring in the code for too long, if anyone could have a quick glance please. Probably you could spot what I did wrong.</p><NewLine><p>On step 1, we read N frames of a video (or just 1 photo) and load it to GPU. No preprocessing done. Just original images:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/466f0e086e0a63a526c24d0cbad8774e2bcf65b4"" href=""https://discuss.pytorch.org/uploads/default/original/3X/4/6/466f0e086e0a63a526c24d0cbad8774e2bcf65b4.png"" title=""image""><img alt=""image"" data-base62-sha1=""a35m7s6sf98CS34vr39wzXXe404"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/6/466f0e086e0a63a526c24d0cbad8774e2bcf65b4_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/6/466f0e086e0a63a526c24d0cbad8774e2bcf65b4_2_662x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/6/466f0e086e0a63a526c24d0cbad8774e2bcf65b4_2_662x500.png, https://discuss.pytorch.org/uploads/default/original/3X/4/6/466f0e086e0a63a526c24d0cbad8774e2bcf65b4.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/4/6/466f0e086e0a63a526c24d0cbad8774e2bcf65b4.png 2x"" width=""662""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">887×669 83.4 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>Then we have the YOLO net that finds objects for us. Using the coordinates of detected objects, we slice torch tensors on GPU and accumulate them in a list and send to our classification ResNet model. The model will need to do all necessary preprocessing because uploaded to GPU images underwent none.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/07e0260eb09448b41cfb2eca07ad7dbfe5b5bb0a"" href=""https://discuss.pytorch.org/uploads/default/original/3X/0/7/07e0260eb09448b41cfb2eca07ad7dbfe5b5bb0a.png"" title=""image""><img alt=""image"" data-base62-sha1=""17FzS5dDrS819z1927cMjUyCggW"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/7/07e0260eb09448b41cfb2eca07ad7dbfe5b5bb0a_2_10x10.png"" height=""494"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/7/07e0260eb09448b41cfb2eca07ad7dbfe5b5bb0a_2_690x494.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/7/07e0260eb09448b41cfb2eca07ad7dbfe5b5bb0a_2_690x494.png, https://discuss.pytorch.org/uploads/default/original/3X/0/7/07e0260eb09448b41cfb2eca07ad7dbfe5b5bb0a.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/0/7/07e0260eb09448b41cfb2eca07ad7dbfe5b5bb0a.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">997×715 82.2 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine>The normalization function:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/47d08f2b1d3295e16dae324c784dc312258d3ae0"" href=""https://discuss.pytorch.org/uploads/default/original/3X/4/7/47d08f2b1d3295e16dae324c784dc312258d3ae0.png"" title=""image""><img alt=""image"" data-base62-sha1=""afiJFfymClBGdUJnj7U1u4UMK6Q"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/7/47d08f2b1d3295e16dae324c784dc312258d3ae0_2_10x10.png"" height=""128"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/7/47d08f2b1d3295e16dae324c784dc312258d3ae0_2_690x128.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/7/47d08f2b1d3295e16dae324c784dc312258d3ae0_2_690x128.png, https://discuss.pytorch.org/uploads/default/original/3X/4/7/47d08f2b1d3295e16dae324c784dc312258d3ae0.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/4/7/47d08f2b1d3295e16dae324c784dc312258d3ae0.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">860×160 17.6 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>I do understand that I must have failed to follow the preprocessing I used during training. Which is here:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/b9b2f8d653627f95c617b0dc463eab06b2dfb708"" href=""https://discuss.pytorch.org/uploads/default/original/3X/b/9/b9b2f8d653627f95c617b0dc463eab06b2dfb708.png"" title=""image""><img alt=""image"" data-base62-sha1=""quLMdVRIrwnQPkfyI6RMZJoyUBy"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/9/b9b2f8d653627f95c617b0dc463eab06b2dfb708_2_10x10.png"" height=""183"" src=""https://discuss.pytorch.org/uploads/default/original/3X/b/9/b9b2f8d653627f95c617b0dc463eab06b2dfb708.png"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">697×185 18 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>If anyone could please have a look and let me know what I did wrong because I cannot find the problem.</p><NewLine><p>I’ve studied what happens with validation images under the hood in Resize(), ToTensor() and Normalize() steps. What I’m doing seems to be identical, however I am getting much more errors than on validation data, which is very very similar to what the model’s been dealing with after integration.</p><NewLine><p>Thank you very much in advance.<br/><NewLine>Regards,<br/><NewLine>Eugene</p><NewLine></div>",https://discuss.pytorch.org/u/evgeniititov,(Eugene),evgeniititov,"June 24, 2020, 10:26am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Fun fact - when I do image preprocessing and make sure my torch tensor is RGB and not BGR (predict function, beginning of the for loop where I do premute), the model works even slightly worse. I’d expect it to work better though because it was training with RGB images. Thanks ColorJitter().</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>This is my guess. During training process, both dimensions of image are same. Width and height parameters of resize function were set to ‘input_size’. But, during testing, interpolation function was used with ‘size = image_size’. In this case, the smallest dimension of image is set to this value. The bigger dimension will be scaled accordingly to maintain aspect ratio. During training phase, aspect ratio of image is not maintained.</p><NewLine><p>Thanks<br/><NewLine>VGS</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/vgsprasad"">@vgsprasad</a>,</p><NewLine><p>Thanks for your reply. Sorry, the screenshot I attached is a bit misleading. The value of image size is actually (224, 224). So, it is the same as during training <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/></p><NewLine><p>Regards,<br/><NewLine>Eugene</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/evgeniititov; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vgsprasad; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/evgeniititov; <NewLine> ,"REPLY_DATE 1: June 24, 2020,  1:26pm; <NewLine> REPLY_DATE 2: June 25, 2020,  4:11am; <NewLine> REPLY_DATE 3: June 25, 2020,  7:09am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
82546,Applying a semantic segmentation classifier to a large image,2020-05-22T23:06:52.628Z,0,134,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I have trained a semantic segmentation classifier (UNet-variant) and now need to apply the classifier to a to a large stack of climate data. Given the dimensions of the climate data over my study area are much larger than than the classifier input size.</p><NewLine><p>I know I need to break the larger image into smaller chunks and feed them through the classifier. I also know that including some overlap during inference is generally good practice to reduce edge effects in the predictions.</p><NewLine><p>Has someone produced a script that illustrates an efficient way of doing this?</p><NewLine><p>Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/John_Kilbride,(John K.),John_Kilbride,"May 22, 2020, 11:06pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I would try to use <code>nn.Unfold</code> to create the smaller patches and <code>nn.Fold</code> to recreate the bigger output.<br/><NewLine>Note that <code>nn.Fold</code> will accumulate the values in the overlapping regions, so you might want to use a manual approach with permutations and some <code>view</code> operations.</p><NewLine><p>Let me know, if that would work for you. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the suggestion! Could you please help me fill out an example of how to use unfold and fold in conjunction.</p><NewLine><p>For simplicity sake, let’s say we have an 3x2048x2048 image (my actual images are a lot larger) and a semantic segmentation model that has an input resolution of 256x256.</p><NewLine><p>The following code for example doesn’t produce a tensor of 256x256 windows like I’d expect…which I assume means I am using the unfold function incorrectly:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn.functional as f<NewLine><NewLine><NewLine># Load in a big image<NewLine>big_image = torch.rand(1,3,2048,2048)<NewLine><NewLine># Unfold the image into 3 x 256 x 256<NewLine>windows = f.unfold(big_image, kernel_size=256)<NewLine><NewLine>print(windows.shape)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Your approach works, but you should enter a stride, otherwise you get a very large output!</p><NewLine><pre><code class=""lang-auto""># load in a big image<NewLine>big_image = torch.randn(1, 3, 2048, 2048)<NewLine><NewLine># unfold the image into (b_size, 3 x 256 x 256, num_of_folds)<NewLine>small_flat_imgs = f.unfold(big_image, kernel_size=256, stride=64)<NewLine>print(small_flat_imgs.shape)<NewLine>&gt;&gt; torch.Size([1, 196608, 841])<NewLine><NewLine><NewLine># test if upper-left corner are identical<NewLine>print(torch.equal(small_flat_imgs[:,:,0].view(-1, 3, 256, 256), big_image[:, :, :256, :256]))<NewLine>&gt;&gt; True<NewLine><NewLine>small_imgs = small_flat_imgs.view(-1, 3, 256, 256)<NewLine>print(small_imgs.shape)<NewLine>&gt;&gt; torch.Size([841, 3, 256, 256])<NewLine></code></pre><NewLine><p>For F.fold I’m to stupid and would be happy about a answer by ptrblck!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I feel like I should comment to avoid the leaving an incomplete thread.</p><NewLine><p>Ultimately, I just used the <a href=""https://rasterio.readthedocs.io/en/latest/topics/windowed-rw.html"" rel=""nofollow noopener"">windowed reading</a> functionality of Rasterio.</p><NewLine><p>I then glued the windowed outputs back together  using np.block(). Probably not the most efficient method but it works well enough for my needs.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/John_Kilbride; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Caruso; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/John_Kilbride; <NewLine> ,"REPLY_DATE 1: May 24, 2020,  8:16am; <NewLine> REPLY_DATE 2: May 25, 2020,  7:02pm; <NewLine> REPLY_DATE 3: May 25, 2020,  7:38pm; <NewLine> REPLY_DATE 4: June 22, 2020,  3:19am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
43317,What is the official release build options?,2019-04-23T09:34:53.160Z,0,171,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was wondering what is the build options for release package? it seems different than what is said on GitHub build from source section. At least for _C.cpython-36m-x86_64-linux-gnu.so, the official build seems static linking cudnn/nccl etc. but what I got from custom build / docker build, it always dynamic linking cudnn / nccl. I known there’s USE_STATIC_CUDNN / USE_STATIC_NCCL which may works for static linking, but still wondering the exactly build options for release build.</p><NewLine></div>",https://discuss.pytorch.org/u/Weichao_Luo,(Weichao Luo),Weichao_Luo,"April 23, 2019,  9:34am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am also interested in this information.<br/><NewLine>Can anyone share the official release build options?</p><NewLine><p>Thanks.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/seanprime7; <NewLine> ,"REPLY_DATE 1: June 16, 2020,  5:04pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
85385,Comparsion through torch.eq,2020-06-14T05:15:56.680Z,0,73,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, all.</p><NewLine><p>While checking the values of torch tensor, I have found a weird boolean on torch.eq. Distinctly, tensor values between a, b on the captured photo are all the same, but, the booleans are not all the same. How come these the result was coming?</p><NewLine><blockquote><NewLine><p>import torch<br/><NewLine>import torch.nn as nn</p><NewLine><p>h_src = torch.Tensor(10,3,5).uniform_(0,1)<br/><NewLine>h_t_tgt = torch.Tensor(10,1,5).uniform_(0,1)</p><NewLine><p>model = nn.Linear(5,5, bias=None)<br/><NewLine>a = model(h_t_tgt)<br/><NewLine>b = torch.einsum(‘lk,ijk-&gt;ijl’, [model.weight, h_t_tgt])</p><NewLine><p>print(torch.eq(a.data, b.data))<br/><NewLine>print(a)<br/><NewLine>print(b)</p><NewLine></blockquote><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/0a6a0d23cb1bee39cb24488392c9df0e0f7f4958"" href=""https://discuss.pytorch.org/uploads/default/original/3X/0/a/0a6a0d23cb1bee39cb24488392c9df0e0f7f4958.png"" title=""제목 없음""><img alt=""제목 없음"" data-base62-sha1=""1u7Zih9G9vAyR0Ygle28REgoHSo"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/a/0a6a0d23cb1bee39cb24488392c9df0e0f7f4958_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/original/3X/0/a/0a6a0d23cb1bee39cb24488392c9df0e0f7f4958.png"" width=""470""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">제목 없음</span><span class=""informations"">1340×1423 11.6 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>I understood this problem is related with a floating-point calculation. Then, is it hard for torch.eq to be precisely used?</p><NewLine></div>",https://discuss.pytorch.org/u/userdyk,(Dongmyeong Lee),userdyk,"June 14, 2020,  8:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>To compare floating point numbers, which might have small errors due to the limited floating point precision, you should use <code>torch.isclose</code> or <code>torch.allclose</code> instead of a direct comparison.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: June 14, 2020, 10:15am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
74845,Torch CUDA is not available,2020-03-30T21:15:03.688Z,5,1274,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Running following returns false:<br/><NewLine>import torch<br/><NewLine>torch.cuda.is_available()</p><NewLine><p>nvidia-smi output: (driver version seems compatible with CUDA version)<br/><NewLine>±----------------------------------------------------------------------------+<br/><NewLine>| NVIDIA-SMI 442.19       Driver Version: 442.19       CUDA Version: 10.2     |<br/><NewLine>|-------------------------------±---------------------±---------------------+<br/><NewLine>| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |<br/><NewLine>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br/><NewLine>|===============================+======================+======================|<br/><NewLine>|   0  GeForce GTX 1650   WDDM  | 00000000:01:00.0 Off |                  N/A |<br/><NewLine>| N/A   49C    P8     5W /  N/A |    132MiB /  4096MiB |      0%      Default |<br/><NewLine>±------------------------------±---------------------±---------------------+</p><NewLine><p>Using Python 3.7.6<br/><NewLine>Windows 10 laptop with NVIDIA GeForce  GTX 1650<br/><NewLine>Torch version 1.4.0</p><NewLine><p>Running deviceQuery and bandwidthTest returns all tests PASS successfull</p><NewLine><p>torch.version.cuda returns none.</p><NewLine><p>torch.backends.cudnn.enabled returns true.</p><NewLine><p>Could not find a solution, tried re-installing Conda, CUDA, drivers, Pytorch - did not help.</p><NewLine><p>Solution found: conda remove cpuonly</p><NewLine></div>",https://discuss.pytorch.org/u/Aleksas,(Aleksas),Aleksas,"March 31, 2020,  6:43pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you do as the following?</p><NewLine><pre><code class=""lang-auto"">conda install pytorch torchvision cudatoolkit=10.1 -c pytorch<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, tried this exact command first and the same one with 10.2 as well</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please uninstall <code>cpuonly</code> in your conda environment. If torch.version.cuda returns none, then it means that you are using a CPU only binary.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>What is the command line that unstalls cpuonly?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>conda uninstall cpuonly</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Tony-Y; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Aleksas; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/peterjc123; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/complexfilter; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/peterjc123; <NewLine> ,"REPLY_DATE 1: March 31, 2020,  5:35am; <NewLine> REPLY_DATE 2: March 31, 2020,  7:17am; <NewLine> REPLY_DATE 3: March 31, 2020,  6:41pm; <NewLine> REPLY_DATE 4: June 13, 2020,  7:46pm; <NewLine> REPLY_DATE 5: June 14, 2020,  2:02am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
85151,Complexity of &lsquo;==&rsquo;,2020-06-12T05:22:14.702Z,0,70,"<div class=""post"" itemprop=""articleBody""><NewLine><p>What is the complexity of the following when the length of <code>vector</code> is <code>n</code> :</p><NewLine><blockquote><NewLine><p>import torch<br/><NewLine>vector = torch.tensor([5, 8, 5, 8, 5, 9, 1, 3, 2, 5, 3, 1])<br/><NewLine>f1 = vector.unsqueeze(1)<br/><NewLine>f2 = vector.unsqueeze(0)<br/><NewLine>matches = (f1 == f2)<br/><NewLine>matches<br/><NewLine>tensor([[1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0],<br/><NewLine>[0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],<br/><NewLine>[1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0],<br/><NewLine>[0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],<br/><NewLine>[1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0],<br/><NewLine>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],<br/><NewLine>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],<br/><NewLine>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],<br/><NewLine>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],<br/><NewLine>[1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0],<br/><NewLine>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],<br/><NewLine>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]], dtype=torch.uint8)</p><NewLine></blockquote><NewLine><p>I thought the complexity of this command is <code>O(n^2)</code> due to the <code>==</code>. But the speed of the code is way faster than I thought.</p><NewLine></div>",https://discuss.pytorch.org/u/DDavid,,DDavid,"June 12, 2020,  5:23am",,,,,
85081,How to deploy model in Java ？,2020-06-11T16:14:09.205Z,0,130,"<div class=""post"" itemprop=""articleBody""><NewLine><p>我在Ubuntu系统使用java调用model，总是提示""No<br/><NewLine>suchMethod org.pytorch.NativePeer.forward "", 以下是我的代码。请不吝指教。非常谢谢</p><NewLine><p>I use java to call model in Ubuntu system, always prompt “no”</p><NewLine><p>suchMethod org.pytorch.NativePeer . forward "", here is my code. Please don’t be stingy. Thank you very much！</p><NewLine><p><img alt=""image"" data-base62-sha1=""3KSEN0rzsfOoOCDo2z60vQ6i9XJ"" height=""252"" src=""https://discuss.pytorch.org/uploads/default/original/3X/1/a/1a5331d373b6e15f58eb5100defdea37aac614ab.png"" width=""529""/></p><NewLine><p><img alt=""image"" data-base62-sha1=""dIr7xsKB8euaHbLLJdycCoAy1SZ"" height=""399"" src=""https://discuss.pytorch.org/uploads/default/original/3X/6/0/6022348c58f6be2e1b9a7b6e0dcc26c06a4e7805.png"" width=""623""/></p><NewLine><p><img alt=""image"" data-base62-sha1=""9j3B4tWIAwJEe11ZNV3B8RIprzF"" height=""388"" src=""https://discuss.pytorch.org/uploads/default/original/3X/4/1/413b1292d672112067c80e62abc5432558bb04e7.png"" width=""604""/></p><NewLine></div>",https://discuss.pytorch.org/u/yonxin3344520,,yonxin3344520,"June 11, 2020,  4:14pm",,,,,
85032,pytorch java demp,2020-06-11T10:15:24.224Z,0,80,"<div class=""post"" itemprop=""articleBody""><NewLine><p><strong>pytorch</strong> : 1.5 statble<br/><NewLine><strong>libtorch</strong> : <a href=""https://download.pytorch.org/libtorch/cu102/libtorch-shared-with-deps-1.5.0.zip"" rel=""nofollow noopener"">https://download.pytorch.org/libtorch/cu102/libtorch-shared-with-deps-1.5.0.zip</a></p><NewLine><p><strong>run command</strong> : ./gradlew run --stacktrace --debug</p><NewLine><h3>stop at:</h3><NewLine><p>&lt;=========----&gt; 75% EXECUTING [1m 37s]</p><NewLine><blockquote><NewLine><p>:run</p><NewLine></blockquote><NewLine><h3>debug infomation:</h3><NewLine><p>2020-06-10T13:43:15.052+0800 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Waiting to acquire shared lock on daemon addresses registry.<br/><NewLine>2020-06-10T13:43:15.052+0800 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Lock acquired on daemon addresses registry.<br/><NewLine>2020-06-10T13:43:15.053+0800 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Releasing lock on daemon addresses registry.</p><NewLine><p>2020-06-10T10:45:56.962+0800 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: STARTED<br/><NewLine>2020-06-10T10:45:56.962+0800 [INFO] [org.gradle.process.internal.DefaultExecHandle] Successfully started process ‘command ‘/home/ks/software/jdk-9.0.4/bin/java’’<br/><NewLine>2020-06-10T10:45:56.962+0800 [DEBUG] [org.gradle.process.internal.ExecHandleRunner] waiting until streams are handled…<br/><NewLine>2020-06-10T10:45:57.206+0800 [ERROR] [system.err] Exception in thread “main” java.lang.NoSuchMethodError: nativeNewTensor<br/><NewLine>2020-06-10T10:45:57.207+0800 [ERROR] [system.err] at org.pytorch.NativePeer.forward(Native Method)<br/><NewLine>2020-06-10T10:45:57.207+0800 [ERROR] [system.err] at org.pytorch.Module.forward(Module.java:37)<br/><NewLine>2020-06-10T10:45:57.207+0800 [ERROR] [system.err] at demo.App.main(App.java:17)<br/><NewLine>2020-06-10T10:46:02.923+0800 [LIFECYCLE] [org.gradle.cache.internal.DefaultFileLockManager]<br/><NewLine>2020-06-10T10:46:02.923+0800 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Waiting to acquire shared lock on daemon addresses registry.</p><NewLine></div>",https://discuss.pytorch.org/u/aaronlyt,(aaronlyt),aaronlyt,"June 11, 2020, 10:15am",,,,,
84640,"What is the difference between the .t7 format and .pth format for saving a model, and more specifically, which is better?",2020-06-08T11:26:16.473Z,0,143,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi guys,<br/><NewLine>I am training a model these days, and I find that there is another format of <code>.t7</code> for saving a model.<br/><NewLine>So, I am wondering what is the difference between the <code>.t7</code> format and <code>.pth</code> format for saving a model, and more specifically, which would be a better choice?</p><NewLine><p>Your answer and idea will be appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/songyuc,,songyuc,"June 8, 2020, 11:27am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>.t7</code> was used in Torch7 and is not used in PyTorch. If I’m not mistaken the file extension does not change the behavior of <code>torch.save</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: June 9, 2020,  5:14am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
54279,Can I use the pretrained models for commercial use,2019-08-25T12:33:03.239Z,0,634,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to use a pretrained model that I downloaded from torchvision.models, train it on my own dataset, and use it for commercial purpose, is that legal??</p><NewLine></div>",https://discuss.pytorch.org/u/Hassan_Saeed,(Hassan Saeed),Hassan_Saeed,"August 25, 2019, 12:33pm",4 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am interested in this topic as well.</p><NewLine><p>Currently, is it right to assume that these pre-trained models offered by PyTorch fall under the same BSD permissive license as stated on its GitHub <strong><a href=""https://github.com/pytorch/pytorch#license"" rel=""nofollow noopener"">readme.md</a></strong>?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/cardboardcode; <NewLine> ,"REPLY_DATE 1: June 5, 2020,  6:18am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
83976,Recommended way to replace several values in a tensor,2020-06-03T02:13:20.111Z,0,63,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is there a batch way to replace several particular values? for example,  <code>old_new_value = [[2,22],[3,33]]</code> , which means 2 should be replaced by 22, and 3 should be replaced by 33.<br/><NewLine>Can I have an efficient way to achieve this?<br/><NewLine>Example:<br/><NewLine>old_values = torch.Tensor([1, 2, 3, 4, 5])<br/><NewLine>old_new_value = [[2,22], [3,33]]<br/><NewLine>end_result = torch.Tensor([1, 22, 33, 4, 5])</p><NewLine></div>",https://discuss.pytorch.org/u/aghtaal,,aghtaal,"June 3, 2020,  2:13am",,,,,
83800,Unifying Preprocessing Pipelines for Machine Learning on Time Series Data (High Throughput Batch &amp; Low Latency Streaming),2020-06-02T03:33:41.186Z,0,99,"<div class=""post"" itemprop=""articleBody""><NewLine><h1>The Question/TL;DR</h1><NewLine><p>What frameworks, design patterns, systems, etc. exist for unifying the preprocessing of time series data in Python such that high throughput is achieved on the retrospective batch data for training machine learning models while also allowing for easy model deployment with low latency on real time streaming data for inference?</p><NewLine><h1>System Details</h1><NewLine><p>The data is raw, high frequency (50-500Hz) ICU bedside monitor signals which are recorded into a bespoke time-series database containing over 3 trillion data points which come from a RabbitMQ stream producing on the order of tens of thousands of new points per second. The database is accessible via a rather slow REST API backed in PHP or SDKs based in various languages. Researchers are really only familiar with Python based tooling (<code>numpy</code>, <code>scipy</code>, <code>pandas</code>, <code>biosppy</code>, PyTorch, Tensorflow, various GitHub repos of domain specific code from research papers, etc) while also not being interested or skilled in engineering custom deployment code for each of their models. We are needing high throughput while preparing large subsets (up to a couple terabytes raw) of the database for efficiently and easily developing models from retrospective data, while also having a very low latency (&lt;500ms) on real time streaming data.</p><NewLine><h1>Options</h1><NewLine><ol><NewLine><li><strong>Traditional ML Research Approach</strong></li><NewLine></ol><NewLine><ul><NewLine><li>Relevant retrospective data is pulled to disk, processed sequentially (usually inefficiently), and eventually a decent performing model is made.</li><NewLine><li>This works, but is completely incompatible with the real time systems meaning deployment requires significant custom engineering. Or worse, this processing is simply inefficient and takes days to process the dataset and/or extra engineering work to parallelize. Therefore, this is not a great approach due to how the research phase is slowed down and the excessive technical work requirements for optimizing the processing as well as deploying to real time data.</li><NewLine></ul><NewLine><ol start=""2""><NewLine><li><strong>Custom Python Framework</strong></li><NewLine></ol><NewLine><ul><NewLine><li>I had started on developing a Python based framework which tried to minimize the amount of systems knowledge and code required from researchers while still allowing them freedom to perform their work however they saw fit via familiar tools (ie, Python). This ended up being quite the undertaking and has been very difficult. Worries of bugs, dealing with edge cases, performance, maintenance, adaptability, … have me looking for a better approach. Many of the problems I discovered along the way seem to be addressed by the streaming frameworks.</li><NewLine></ul><NewLine><ol start=""3""><NewLine><li><strong>Streaming Frameworks</strong></li><NewLine></ol><NewLine><ul><NewLine><li>It appears that streaming frameworks are the current leading way of feeding real time machine learning models. In particular, Spark Streaming, but also Storm, Flink, Samza, Trill, …</li><NewLine><li><NewLine><a href=""https://beam.apache.org/"" rel=""nofollow noopener"">Apache Beam</a> is currently the most interesting though I am uncertain this is the best option<NewLine><ul><NewLine><li>Unified: batch and streaming can be processed in the same way<NewLine><ul><NewLine><li>I recently learned Flink offers similar functionality</li><NewLine></ul><NewLine></li><NewLine><li>Extensible: allows for new SDKs, IO connectors, etc<NewLine><ul><NewLine><li>This is important for interfacing with our bespoke database.</li><NewLine><li>Issue: The Python SDK of Apache Beam does not have existing IO for RabbitMQ but Java SDK does</li><NewLine><li>Allows for the execution of any Python code on the data, not just simplistic min/max/windowing and such of other streaming engines</li><NewLine></ul><NewLine></li><NewLine><li>Language: Allows for everything to be written in Python<NewLine><ul><NewLine><li>Only other all Python offering I have found is <a href=""https://streamz.readthedocs.io/en/latest/index.html"" rel=""nofollow noopener"">Streamz</a><NewLine></li><NewLine></ul><NewLine></li><NewLine></ul><NewLine></li><NewLine></ul><NewLine><p>Thank you for your time. If there are better venues or ways to ask this question, please let me know! Especially if I have misused terminology, please let me know as that will allow me to research these issues better on my own.</p><NewLine></div>",https://discuss.pytorch.org/u/carsonmclean,(Carson McLean),carsonmclean,"June 2, 2020,  3:33am",1 Like,,,,
66841,How to convert layer_norm layer to ONNX?,2020-01-16T10:14:34.704Z,0,325,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to convert my model to ONNX format for further deployment in TensorRT. Here is a sample code to illustrate my problem in <strong>layer_norm</strong> here.</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>from torch import nn<NewLine><NewLine>class ExportModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine><NewLine>    def forward(self, x):<NewLine>        # n, c, h, w = x.shape<NewLine>        # y = nn.functional.layer_norm(x, [c, h, w]])       # not working<NewLine>        # y = nn.functional.layer_norm(x, x.size()[1:])     # not working<NewLine>        y = nn.functional.layer_norm(x, [16, 32, 128])<NewLine><NewLine>        return y<NewLine><NewLine>def main():<NewLine>    model = ExportModel()<NewLine><NewLine>    dummy_input = torch.randn(64, 16, 32, 128)<NewLine>    input_names = [ ""input"" ]<NewLine>    output_names = [ ""output"" ]<NewLine><NewLine>    with torch.no_grad():<NewLine>        torch.onnx.export(<NewLine>            model, dummy_input, ""sample.onnx"", verbose=True,<NewLine>            input_names=input_names, output_names=output_names<NewLine>        )<NewLine>    return<NewLine><NewLine>if __name__ == '__main__':<NewLine>    main()<NewLine></code></pre><NewLine><p>It could only work when the parameter of layer_norm is constant number. If not, the following error will occur.</p><NewLine><pre><code class=""lang-auto""><NewLine>Traceback (most recent call last):<NewLine>  File ""sample.py"", line 31, in &lt;module&gt;<NewLine>    main()<NewLine>  File ""sample.py"", line 26, in main<NewLine>    verbose=True, input_names=input_names, output_names=output_names<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/onnx/__init__.py"", line 148, in export<NewLine>    strip_doc_string, dynamic_axes, keep_initializers_as_inputs)<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py"", line 66, in export<NewLine>    dynamic_axes=dynamic_axes, keep_initializers_as_inputs=keep_initializers_as_inputs)<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py"", line 409, in _export<NewLine>    fixed_batch_size=fixed_batch_size)<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py"", line 289, in _model_to_graph<NewLine>    fixed_batch_size=fixed_batch_size)<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py"", line 132, in _optimize_graph<NewLine>    graph = torch._C._jit_pass_onnx(graph, operator_export_type)<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/onnx/__init__.py"", line 179, in _run_symbolic_function<NewLine>    return utils._run_symbolic_function(*args, **kwargs)<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py"", line 647, in _run_symbolic_function<NewLine>    return op_fn(g, *inputs, **attrs)<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/onnx/symbolic_helper.py"", line 128, in wrapper<NewLine>    args = [_parse_arg(arg, arg_desc) for arg, arg_desc in zip(args, arg_descriptors)]<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/onnx/symbolic_helper.py"", line 128, in &lt;listcomp&gt;<NewLine>    args = [_parse_arg(arg, arg_desc) for arg, arg_desc in zip(args, arg_descriptors)]<NewLine>  File ""/opt/conda/lib/python3.6/site-packages/torch/onnx/symbolic_helper.py"", line 81, in _parse_arg<NewLine>    ""', since it's not constant, please try to make ""<NewLine>RuntimeError: Failed to export an ONNX attribute 'onnx::Gather', since it's not constant, please try to make things (e.g., kernel size) static if possible<NewLine></code></pre><NewLine><p>I have few code blocks in my model have layer_norm op. It would turn into some ugly code if I explicitly mark all parameters constant number. Is there any “best practice” of how to use dynamic shape for this kind of use case?</p><NewLine><p>Thanks in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/rtrobin,(rtrobin),rtrobin,"January 16, 2020, 10:14am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Try this codes. it converts tensor variables to integer ones.<br/><NewLine>I confirmed that it works for your example.<br/><NewLine>n, c, h, w = x.shape<br/><NewLine>c1 = c.item()<br/><NewLine>h1 = h.item()<br/><NewLine>w1 = w.item()<br/><NewLine>y = nn.functional.layer_norm(x, [c1, h1, w1])</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/111325; <NewLine> ,"REPLY_DATE 1: May 27, 2020,  2:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
81312,TorchScript vs C++ Frontend,2020-05-15T04:02:32.126Z,3,264,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>What is the recommended way to use PyTorch models in C++ for deployment?</p><NewLine><p>I have read different articles on the forum but could not find any conclusion. Assuming that development effort is not an issue, will the pure C++ model be faster during training and inference compared to a Python model converted using TorchScript and then loaded in C++ application?</p><NewLine><p>Are there any limitations to the Python -&gt; TorchScript -&gt; C++ approach compared to pure C++ models?</p><NewLine><p>Also, what is the long term support plan by PyTorch team for TorchScript vs C++ frontend (to better understand the recommended approach)?</p><NewLine></div>",https://discuss.pytorch.org/u/sahni,(Anjandeep Sahni),sahni,"May 15, 2020,  4:02am",3 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>, <a class=""mention"" href=""/u/yf225"">@yf225</a>, <a class=""mention"" href=""/u/alband"">@albanD</a>, <a class=""mention"" href=""/u/michael_suo"">@Michael_Suo</a>, saw your comments on other similar posts so I thought perhaps you could provide some valuable input.</p><NewLine><p>I also reviewed the <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/pytorch-c-deployment-story-2019/58199"">PyTorch C++ Deployment Story: 2019</a> post. But I am not clear on difference on speed between TorchScript and pure C++. Also, I could use some input on limitations of TorchScript. I read on some posts that LSTM and maybe some conditional constructs if/loops are not supported (or require extra effort that I am not completely clear on).</p><NewLine><p>I would appreciate any help.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Writing directly in C++ is needed, for deployment, if your network topology is dynamic (depend on the inputs such as Tree) or you need to train in an environment that can’t have Python runtime. In most other cases, TorchScript (if the model has control flow or loop) or Trace is the way to go. Speed wise, it should be pretty close, script might be faster in some cases due to the optimization passes (i.e. OP fusion, constant folding…etc).</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ebarsoum"">@ebarsoum</a>, thanks for your feedback. I was also curious as to wether we can use Tensorboard with the C++ frontend. Any idea?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/tensorflow/tensorboard/issues/2636"" rel=""nofollow noopener"" target=""_blank"">github.com/tensorflow/tensorboard</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/tensorflow/tensorboard/issues/2636"" rel=""nofollow noopener"" target=""_blank"">Feature Request: C++ logging from non-TensorFlow frameworks</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-09-11"" data-format=""ll"" data-time=""16:06:10"" data-timezone=""UTC"">04:06PM - 11 Sep 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/orionr"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""orionr"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars3.githubusercontent.com/u/79994?v=4"" width=""20""/><NewLine>          orionr<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">TensorBoard logging can be done from a variety of ML frameworks, including PyTorch with torch.utils.tensorboard. The FileWriter and protos exposed from...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">stat:awaiting tensorflower</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">type:feature</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/sahni; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ebarsoum; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/sahni; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ebarsoum; <NewLine> ,"REPLY_DATE 1: May 24, 2020, 12:23am; <NewLine> REPLY_DATE 2: May 27, 2020,  5:59am; <NewLine> REPLY_DATE 3: May 27, 2020,  4:51am; <NewLine> REPLY_DATE 4: May 27, 2020,  5:16am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
82521,Why would model loading takes that long?(usually takes 2 seconds to complete),2020-05-22T19:07:29.726Z,0,109,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there,<br/><NewLine>I am curious about why the first time job launch cost so much time in PyTorch.<br/><NewLine>e.g. load the model into CPU, move the model to GPU.</p><NewLine><p>Here is the code snippet to reproduce:</p><NewLine><pre><code class=""lang-auto"">import torch.multiprocessing as mp<NewLine>from torchvision import models<NewLine>import torch<NewLine>import time<NewLine><NewLine>def only_import():<NewLine>    """"""""""""<NewLine>    t1 = time.time()<NewLine><NewLine>    model = torch.hub.load('pytorch/vision:v0.4.2',<NewLine>                           ""resnet152"",<NewLine>                           pretrained=True)<NewLine>    # util.set_fullname(model, MODEL_NAME)<NewLine>    t2 = time.time()<NewLine><NewLine>    model = model.to('cuda')<NewLine>    t3 = time.time()<NewLine><NewLine>    print(""import model takes"", (t2 - t1) * 1e3, 'ms')<NewLine>    print(""move model to cuda"", (t3 - t2) * 1e3, 'ms')<NewLine><NewLine>def main():<NewLine>    """"""""""""<NewLine><NewLine>    for _ in range(10):<NewLine>        proc = mp.Process(target=only_import)<NewLine>        proc.start()<NewLine>        proc.join()<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    mp.set_start_method(""spawn"")<NewLine>    main()<NewLine></code></pre><NewLine><p>Base on the log, it usually takes 1.5s to 2.2s to load the model to CPU. and over 2 seconds to load the model to cuda.</p><NewLine><p>Is this because PyTorch need to dynamically load libraries from disk, at the first time?</p><NewLine><p>(experiment setup: aws-ec2-p3.2xlarge intance, PyTorch 1.3.0)</p><NewLine></div>",https://discuss.pytorch.org/u/zarzen,(Zhang Zhen),zarzen,"May 22, 2020,  7:13pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The first time to use cuda, cudnn has to allot some cache and that takes time. If you pass a dummy input to GPU like <code>torch.randn(4).to(device)</code> then after that you will normal transfer speed.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Kushaj; <NewLine> ,"REPLY_DATE 1: May 23, 2020,  1:01pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
82480,Why deployment is favored in Tensorflow?,2020-05-22T12:55:37.135Z,0,145,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Helloo,<br/><NewLine>I have a question of bit general. I am wondering why people say TensorFlow is better than PyTorch  in deployment? Because I see a lot of companies integrating their deep learning models in their base code using Pytorch!<br/><NewLine>Because at the end all you need is an input, and you shall give back an output for further processing. Botch frameworks support multi-gpo/cpu computing. So the question is why, or what do people actually mean by deployment? it would be better If someone gives a concrete example with technical details.<br/><NewLine>Thank you</p><NewLine></div>",https://discuss.pytorch.org/u/ilyes,(ilyes),ilyes,"May 22, 2020, 12:55pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>A few reasons that I am aware of:</p><NewLine><ol><NewLine><li>pytorch <a href=""https://pytorch.org/docs/stable/quantization.html"" rel=""nofollow noopener"">APIs for optimizing</a> and otherwise preparing a model for production are still fairly new and rough around the edges</li><NewLine><li>FB described themselves as <a href=""https://research.fb.com/wp-content/uploads/2017/12/hpca-2018-facebook.pdf"" rel=""nofollow noopener"">relying on caffe for production inference</a> – but caffe seems to have been “merged” into pytorch in 2018 and the result for us the end users seems to be a somewhat confused mix of possible runtimes (or “backends”): python, JIT traced, JIT scripted, ONNX. If you quantize a model you might not be able to export it to ONNX. Or even save it. And so on – things just don’t feel properly cooked yet.</li><NewLine></ol><NewLine><blockquote><NewLine><p>Because at the end all you need is an input, and you shall give back an output for further processing.</p><NewLine></blockquote><NewLine><p>Hmm, a lot of people want more. Can you run the above on mobile? How about a production environment that doesn’t have python at all?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vladium; <NewLine> ,"REPLY_DATE 1: May 22, 2020,  7:49pm; <NewLine> ",REPLY 1 LIKES: 3 Likes; <NewLine> 
60270,State_dict optimizer params,2019-11-07T11:58:45.376Z,0,157,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there,</p><NewLine><p>when saving an optimizier during training with optimizer.state_dict()<br/><NewLine>and later loading it via torch.load() I get a dictionary containing “state” and “param_groups”<br/><NewLine>as described in the documentation <a href=""https://pytorch.org/docs/stable/optim.html?highlight=state_dict#torch.optim.Optimizer.load_state_dict"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/optim.html?highlight=state_dict#torch.optim.Optimizer.load_state_dict</a>.</p><NewLine><p>In “params_group” I have an entry for “params”: what exactly are these values?<br/><NewLine>Are these the last gradient values that have been used to update my NN?</p><NewLine><p>Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/MGud,,MGud,"November 7, 2019, 11:58am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>see answer <a href=""https://discuss.pytorch.org/t/optimizer-params-not-in-the-optimizer-state-dict/42957/2"">here</a></p><NewLine><p>These numbers represent the ids of the parameters you’ve passed to the optimizer.</p><NewLine><p>Thank <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>  very much!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Alpha; <NewLine> ,"REPLY_DATE 1: May 21, 2020,  1:29am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
82137,Does torch.onnx support if control flow?,2020-05-20T08:38:20.641Z,0,105,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I want to export my RNN Transducer model using torch.onnx.export. However, there is an “if” in my network forward. I have checked the documents and one way to solve the control flow problem is mixing tracing and scripting.</p><NewLine><p><a class=""onebox"" href=""https://pytorch.org/docs/stable/onnx.html#tracing-vs-scripting"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/stable/onnx.html#tracing-vs-scripting</a></p><NewLine><p>I have tried the example and it works well for “for loop”, but there will be errors as following when I use “if”. So I want to know if ""if "" is supported.</p><NewLine><p>My pytorch version is 1.4.0</p><NewLine><blockquote><NewLine><p>graph(%input : Long(2, 3),<br/><NewLine>%loop : Long(),<br/><NewLine>%d : Long()):<br/><NewLine>%3 : Long() = onnx::Constant<a>value={3}</a><br/><NewLine>%4 : Long() = onnx::Greater(%d, %3) # test2.py:8:7<br/><NewLine>%5 : Long(2, 3) = onnx::If(%4) # test2.py:8:4<br/><NewLine>block0():<br/><NewLine>%6 : LongTensor = onnx::Add(%input, %d) # test2.py:10:12<br/><NewLine>-&gt; (%6)<br/><NewLine>block1():<br/><NewLine>-&gt; (%input)<br/><NewLine>return (%5)</p><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “test2.py”, line 31, in <br/><NewLine>ort_sess = ort.InferenceSession(‘loop.onnx’)<br/><NewLine>File “/ceph/sz_ts80_new/siningsun/pytorch/espnet/tools/venv/lib/python3.7/site-packages/onnxruntime/capi/session.py”, line 158, in <strong>init</strong><br/><NewLine>self._load_model(providers)<br/><NewLine>File “/ceph/sz_ts80_new/siningsun/pytorch/espnet/tools/venv/lib/python3.7/site-packages/onnxruntime/capi/session.py”, line 177, in _load_model<br/><NewLine>self._sess.load_model(providers)<br/><NewLine>onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /onnxruntime_src/onnxruntime/core/graph/graph.cc:912 void onnxruntime::Graph::InitializeStateFromModelFileGraphProto() This is an invalid model. Graph output (input) does not exist in the graph.</p><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/snsun,(Sining Sun),snsun,"May 20, 2020,  9:26am",,,,,
81405,Object Detection with Pytorch,2020-05-15T16:17:17.540Z,4,377,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello<br/><NewLine>I’m a beginner in DNN. I’m using Jetson Nano.<br/><NewLine>I managed to do transfer learning on a ResNet-18 model with my custom dataset for object detection. It seems quite straight forward with Pytorch.</p><NewLine><p>What I’m struggling with is the deployment of my model.</p><NewLine><p>My question is simple: Is it possible to deploy the model that I trained in Pytorch and run object detection inference on it? Or do I absolutely need to export it to some other format such as Caffe?</p><NewLine><p>It it is possible to do it in Pytorch, and if so, is there a sample code in Python?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/Jimmy_2times,(Jimmy 2times),Jimmy_2times,"May 15, 2020,  4:17pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>if you are using Jetson Nano, you can directly run the model trained in PyTorch on the Nano.</p><NewLine><p>If you want to speed the model up on Nano, you can use something like <a href=""https://github.com/NVIDIA-AI-IOT/torch2trt"">https://github.com/NVIDIA-AI-IOT/torch2trt</a> to offload part of the model to TensorRT</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>here are links to pytorch binaries on Jetson platforms: <a href=""https://github.com/pytorch/pytorch#nvidia-jetson-platforms"">https://github.com/pytorch/pytorch#nvidia-jetson-platforms</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>hey thanks for a quick reply smth.<br/><NewLine>When you say “if you are using Jetson Nano, you can directly run the model trained in PyTorch on the Nano.” what libraries/modules shall I use?<br/><NewLine>Do you mean I have to use the .pth file without converting it to ONNX? Could you be a bit more specific?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>just use pytorch directly, without converting to ONNX. it runs on jetson nano.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s great to know that it’s possible!!<br/><NewLine>Could you direct me to a specific snippet, piece of code or something written in Python so I can take it from there?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""81405"" data-username=""Jimmy_2times""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/jimmy_2times/40/24286_2.png"" width=""20""/> Jimmy_2times:</div><NewLine><blockquote><NewLine><p>I managed to do transfer learning on a ResNet-18 model with my custom dataset for object detection. It seems quite straight forward with Pytorch.</p><NewLine></blockquote><NewLine></aside><NewLine><p>seems like you already have some code?</p><NewLine><p>either ways, feel free to use something like <a href=""https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/"">https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/</a></p><NewLine><p>there might be some samples at <a href=""https://github.com/NVIDIA-AI-IOT"">https://github.com/NVIDIA-AI-IOT</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I took the code from Jetson Inference examples, which was meant for transfer learning and used it to train on my dataset.<br/><NewLine>But I didn’t have any code to run the inference.<br/><NewLine>I will look into the code you sent me. I really appreciate it!!</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Smth. I tried DEEPLABV3-RESNET101. It works very well!<br/><NewLine>Except this is segmentation, and I don’t know how to build a bounding box around the drawn mask over the detected object.</p><NewLine><p>Could you please show me an example that actually draws the bounding boxes around the detected object?</p><NewLine><p>Thanks again!!</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just realized that Resnet18 is not for object detection but for image classification.<br/><NewLine>Since I need Object Detection, I will have to write a new script to train for instance Faster RCNN.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Jimmy_2times; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Jimmy_2times; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Jimmy_2times; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Jimmy_2times; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Jimmy_2times; <NewLine> ,"REPLY_DATE 1: May 15, 2020,  6:49pm; <NewLine> REPLY_DATE 2: May 15, 2020,  6:50pm; <NewLine> REPLY_DATE 3: May 15, 2020,  7:00pm; <NewLine> REPLY_DATE 4: May 15, 2020,  7:13pm; <NewLine> REPLY_DATE 5: May 15, 2020,  7:15pm; <NewLine> REPLY_DATE 6: May 15, 2020,  7:26pm; <NewLine> REPLY_DATE 7: May 15, 2020,  7:37pm; <NewLine> REPLY_DATE 8: May 16, 2020, 12:15am; <NewLine> REPLY_DATE 9: May 19, 2020,  5:53am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> 
81922,Is there a backend difference when running a scripted model in python vs c++?,2020-05-19T03:31:58.460Z,0,65,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Greetings. A couple of questions regarding x86 inference backend(s):</p><NewLine><ol><NewLine><li>Assume that I produced a model via torch.jit.script(). Is it correct to think that essentially the same JIT runtime + BLAS implementation are used regardless of whether this model is subsequently evaluated via the python or the c++ interface? (assuming that the c++ app is compiled and linked against lib{torch, torch_cpu, c10}.so that are part of the pytorch distribution)</li><NewLine></ol><NewLine><p>Put differently, other than being able to run in a python-less environment, are there benefits to doing model <em>inference</em> via the c++ API?</p><NewLine><ol start=""2""><NewLine><li>I read everything I could find about fbgemm but remain confused as to when it actually kicks in. Does it get used only if the model contains fused or quantized ops?</li><NewLine></ol><NewLine><p>Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/vladium,,vladium,"May 19, 2020,  3:31am",,,,,
81890,Local Inference of saved PyTorch models,2020-05-18T21:07:47.010Z,0,135,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am interested in performing local (no server or cloud) inference of saved PyTorch models that I can “deploy” (for example, using PyInstaller) to machines that do not have any dependencies. Can someone point me in the correct direction for this?</p><NewLine><p>My goal is to be able create a command-line executable that I can share with collaborators who do not have any idea about Python/programming/Docker/containers and would simply like to infer on the models I send (with the correct data, of course).</p><NewLine><p>Requirements from end product:</p><NewLine><ul><NewLine><li>No “cloud” or remote processing - data privacy is a critical requirement and processing needs to be local on Windows and/or Linux machines using CPU (GPU would be a huge plus but not required)</li><NewLine><li>No need for user to install anything (no Python/Docker engine/server/etc.) as this needs to be catered to a non-tech audience</li><NewLine></ul><NewLine><p>Please comment if further clarifications are needed.</p><NewLine><p>Thanks in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/sarthakpati,(Sarthak Pati),sarthakpati,"May 18, 2020,  9:07pm",,,,,
81490,Reduce number of operations in this PyTorch code,2020-05-16T08:36:43.503Z,0,85,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a input variable ‘fm’ which needs to be transformed into ‘fm1’ and ‘fm3’ to be processed by the system. However, in doing so I create a temporary variable ‘fm2’ which I do not require at all.</p><NewLine><p>Since i want to reduce time and memory of my deep model and make it as fast as possible I suspect it should be possible to directly obtain ‘fm3’ from ‘fm1’ with less number of ‘view()’ and ‘permutations()’ and ‘fm2’ is not really required. Can someone please help me on this?</p><NewLine><p>Alternatively obtaining ‘fm3’ directly from ‘fm’ will also work.</p><NewLine><p>Here is my code:</p><NewLine><pre><code class=""lang-auto"">h=16 # any number expressible as a power of 2<NewLine>w=32 # any number expressible as a power of 2<NewLine>fm = torch.arange(1*1*h*w).view(1,1,h,w)<NewLine>print(fm)<NewLine><NewLine>b,c,h,w = fm.size()<NewLine>R=2 <NewLine>out_channel = c*(R**2)<NewLine>out_h = h//R<NewLine>out_w = w//R<NewLine>fm1 = fm.view(b, c, out_h, R, out_w, R).permute(0,1,3,5,2,4).contiguous().view(b,out_channel, out_h, out_w)<NewLine>print(fm1)<NewLine><NewLine>b,c,h,w = fm1.size()<NewLine>r=4 <NewLine>out_channel = c*(r**2)<NewLine>out_h = h//r<NewLine>out_w = w//r<NewLine>fm2 = fm1.view(b, c, out_h, r, out_w, r).permute(0,1,3,5,2,4).contiguous().view(b,out_channel, out_h, out_w)<NewLine># This fm2 is temporary and is not required<NewLine><NewLine>b,c,h,w = fm2.size()<NewLine>G=R**2<NewLine>fm3 = fm2.view(b, G, c // G, h, w).permute(0, 2, 1, 3, 4).contiguous().view(b, c, h, w)<NewLine>print(fm3)<NewLine></code></pre><NewLine><p>The output is as follows (for brevity here h,w=8; R=2 and r=2),</p><NewLine><pre><code class=""lang-auto"">fm: tensor([[[[ 0,  1,  2,  3,  4,  5,  6,  7],<NewLine>          [ 8,  9, 10, 11, 12, 13, 14, 15],<NewLine>          [16, 17, 18, 19, 20, 21, 22, 23],<NewLine>          [24, 25, 26, 27, 28, 29, 30, 31],<NewLine>          [32, 33, 34, 35, 36, 37, 38, 39],<NewLine>          [40, 41, 42, 43, 44, 45, 46, 47],<NewLine>          [48, 49, 50, 51, 52, 53, 54, 55],<NewLine>          [56, 57, 58, 59, 60, 61, 62, 63]]]])<NewLine><NewLine> fm1: tensor([[[[ 0,  2,  4,  6],<NewLine>          [16, 18, 20, 22],<NewLine>          [32, 34, 36, 38],<NewLine>          [48, 50, 52, 54]],<NewLine><NewLine>         [[ 1,  3,  5,  7],<NewLine>          [17, 19, 21, 23],<NewLine>          [33, 35, 37, 39],<NewLine>          [49, 51, 53, 55]],<NewLine><NewLine>         [[ 8, 10, 12, 14],<NewLine>          [24, 26, 28, 30],<NewLine>          [40, 42, 44, 46],<NewLine>          [56, 58, 60, 62]],<NewLine><NewLine>         [[ 9, 11, 13, 15],<NewLine>          [25, 27, 29, 31],<NewLine>          [41, 43, 45, 47],<NewLine>          [57, 59, 61, 63]]]])<NewLine><NewLine> fm3: tensor([[[[ 0,  4],<NewLine>          [32, 36]],<NewLine><NewLine>         [[ 1,  5],<NewLine>          [33, 37]],<NewLine><NewLine>         [[ 8, 12],<NewLine>          [40, 44]],<NewLine><NewLine>         [[ 9, 13],<NewLine>          [41, 45]],<NewLine><NewLine>         [[ 2,  6],<NewLine>          [34, 38]],<NewLine><NewLine>         [[ 3,  7],<NewLine>          [35, 39]],<NewLine><NewLine>         [[10, 14],<NewLine>          [42, 46]],<NewLine><NewLine>         [[11, 15],<NewLine>          [43, 47]],<NewLine><NewLine>         [[16, 20],<NewLine>          [48, 52]],<NewLine><NewLine>         [[17, 21],<NewLine>          [49, 53]],<NewLine><NewLine>         [[24, 28],<NewLine>          [56, 60]],<NewLine><NewLine>         [[25, 29],<NewLine>          [57, 61]],<NewLine><NewLine>         [[18, 22],<NewLine>          [50, 54]],<NewLine><NewLine>         [[19, 23],<NewLine>          [51, 55]],<NewLine><NewLine>         [[26, 30],<NewLine>          [58, 62]],<NewLine><NewLine>         [[27, 31],<NewLine>          [59, 63]]]])<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/mohit117,(Mohit Lamba),mohit117,"May 16, 2020,  1:08pm",,,,,
81127,Fail to build debug version on windows,2020-05-14T03:11:57.576Z,3,439,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I try to build pytorch from source on Windows, using Anaconda3, and “DEBUG=1”. It failed with fatal error LNK1104 “LNK1104: ╬▐╖¿┤≥┐¬╬─╝■í░python37_d.libí▒”.</p><NewLine><p>Then I found that “python37_d.lib” didn’t exist on this computer(Anaconda doesn’t have it by default). Then I installed python3.7 from <a href=""http://www.python.org"" rel=""nofollow noopener"">www.python.org</a> and select “Download debug binaries”. Then I copied the whole newly installed python folder to Anaconda3\envs\MyEnv and replace existing old files. When I built pytorch again, it shows “LNK1104: ╬▐╖¿┤≥┐¬╬─╝■í░python37.libí▒”. I checked and made sure both “python37_d.lib” and “python37.lib” are under Anaconda3\envs\MyEnv\libs.</p><NewLine><p>I tried both Visual Studio compiler and Ninja, both failed with the same error.<br/><NewLine>But it could succeed on Ununtu. Building release version of pytorch on Windows could also succeed.</p><NewLine><p>Is my method wrong(using self-installed python files to replace the ones under Anaconda in order to have pytorch37_d.lib)? How to successfully build debug version of pytorch on windows?</p><NewLine><p>Error message when using Visual Studio compiler: (After failure, I checked the caffe2_pybind11_state.vcxproj and all the path with “python37.lib” and “python37_d.lib” are correct)</p><NewLine><blockquote><NewLine><p>pybind_state_dlpack.cc<br/><NewLine>cl : ??? warning D9025: ???""/W1""(?""/w"") [C:\Users\tegao\mygit\v0512\pytorch\build\caffe2\caffe2_pybind11_state.vcxproj<br/><NewLine>]<br/><NewLine>pybind_state_nomni.cc<br/><NewLine>pybind_state_registry.cc<br/><NewLine>pybind_state_int8.cc<br/><NewLine>pybind_state_ideep.cc<br/><NewLine>LINK : fatal error LNK1104: ╬▐╖¿┤≥┐¬╬─╝■í░python37.libí▒ [C:\Users\tegao\mygit\v0512\pytorch\build\caffe2\caffe2_pybind<br/><NewLine>11_state.vcxproj]<br/><NewLine>Building Custom Rule C:/Users/tegao/mygit/v0512/pytorch/third_party/ideep/mkl-dnn/src/CMakeLists.txt<br/><NewLine>Generating <strong>init</strong>.py<br/><NewLine>Generating contrib/<strong>init</strong>.py</p><NewLine></blockquote><NewLine><p>Error message when using Ninja:</p><NewLine><blockquote><NewLine><p>[2704/2922] Linking CXX executable bin\extension_backend_test.exe<br/><NewLine>╒²╘┌┤┤╜¿┐Γ lib\extension_backend_test.lib ║═╢╘╧≤ lib\extension_backend_test.exp<br/><NewLine>[2706/2922] Linking CXX shared module caffe2\python\caffe2_pybind11_state.cp37-win_amd64.pyd<br/><NewLine>FAILED: caffe2/python/caffe2_pybind11_state.cp37-win_amd64.pyd<br/><NewLine>cmd.exe /C “cd . &amp;&amp; C:\Users\tegao\Anaconda3\envs\gt37\Library\bin\cmake.exe -E vs_link_dll --intdir=caffe2\CMakeFiles\caffe2_pybind11_state.dir --rc=C:\PROGRA~2\WI3CF2~1\10\bin\100177~1.0\x64\rc.exe --mt=C:\PROGRA~2\WI3CF2~1\10\bin\100177~1.0\x64\mt.exe --manifests  – C:\PROGRA~2\MIB055~1\2017\PROFES~1\VC\Tools\MSVC\1416~1.270\bin\Hostx64\x64\link.exe /nologo caffe2\CMakeFiles\caffe2_pybind11_state.dir\python\pybind_state.cc.obj caffe2\CMakeFiles\caffe2_pybind11_state.dir\python\pybind_state_dlpack.cc.obj caffe2\CMakeFiles\caffe2_pybind11_state.dir\python\pybind_state_nomni.cc.obj caffe2\CMakeFiles\caffe2_pybind11_state.dir\python\pybind_state_registry.cc.obj caffe2\CMakeFiles\caffe2_pybind11_state.dir\python\pybind_state_int8.cc.obj caffe2\CMakeFiles\caffe2_pybind11_state.dir\python\pybind_state_ideep.cc.obj  /out:caffe2\python\caffe2_pybind11_state.cp37-win_amd64.pyd /implib:lib\caffe2_pybind11_state.lib /pdb:caffe2\python\caffe2_pybind11_state.pdb /dll /version:0.0 /machine:x64 /ignore:4049 /ignore:4217 /debug /INCREMENTAL:NO /machine:x64 /ignore:4049 /ignore:4217  C:\Users\tegao\Anaconda3\envs\gt37\libs\python37_d.lib  lib\onnx_proto.lib  lib\torch.lib  lib\torch_cpu.lib  lib\c10.lib  lib\dnnl.lib  lib\libprotobufd.lib  kernel32.lib user32.lib gdi32.lib winspool.lib shell32.lib ole32.lib oleaut32.lib uuid.lib comdlg32.lib advapi32.lib  &amp;&amp; cd .”<br/><NewLine>LINK: command “C:\PROGRA~2\MIB055~1\2017\PROFES~1\VC\Tools\MSVC\1416~1.270\bin\Hostx64\x64\link.exe /nologo caffe2\CMakeFiles\caffe2_pybind11_state.dir\python\pybind_state.cc.obj caffe2\CMakeFiles\caffe2_pybind11_state.dir\python\pybind_state_dlpack.cc.obj caffe2\CMakeFiles\caffe2_pybind11_state.dir\python\pybind_state_nomni.cc.obj caffe2\CMakeFiles\caffe2_pybind11_state.dir\python\pybind_state_registry.cc.obj caffe2\CMakeFiles\caffe2_pybind11_state.dir\python\pybind_state_int8.cc.obj caffe2\CMakeFiles\caffe2_pybind11_state.dir\python\pybind_state_ideep.cc.obj /out:caffe2\python\caffe2_pybind11_state.cp37-win_amd64.pyd /implib:lib\caffe2_pybind11_state.lib /pdb:caffe2\python\caffe2_pybind11_state.pdb /dll /version:0.0 /machine:x64 /ignore:4049 /ignore:4217 /debug /INCREMENTAL:NO /machine:x64 /ignore:4049 /ignore:4217 C:\Users\tegao\Anaconda3\envs\gt37\libs\python37_d.lib lib\onnx_proto.lib lib\torch.lib lib\torch_cpu.lib lib\c10.lib lib\dnnl.lib lib\libprotobufd.lib kernel32.lib user32.lib gdi32.lib winspool.lib shell32.lib ole32.lib oleaut32.lib uuid.lib comdlg32.lib advapi32.lib /MANIFEST /MANIFESTFILE:caffe2\python\caffe2_pybind11_state.cp37-win_amd64.pyd.manifest” failed (exit code 1104) with the following output:<br/><NewLine>LINK : fatal error LNK1104: ╬▐╖¿┤≥┐¬╬─╝■í░python37.libí▒<br/><NewLine>[2708/2922] Linking CXX executable bin\pow_test.exe<br/><NewLine>╒²╘┌┤┤╜¿┐Γ lib\pow_test.lib ║═╢╘╧≤ lib\pow_test.exp<br/><NewLine>[2709/2922] Building CXX object caffe2\torch\CMakeFiles\torch_python.dir\csrc\jit\backends\backend_init.cpp.obj<br/><NewLine>ninja: build stopped: subcommand failed.<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “setup.py”, line 740, in <br/><NewLine>build_deps()<br/><NewLine>File “setup.py”, line 323, in build_deps<br/><NewLine>cmake=cmake)<br/><NewLine>File “C:\Users\tegao\mygit\v0512\pytorch\tools\build_pytorch_libs.py”, line 62, in build_caffe2<br/><NewLine>cmake.build(my_env)<br/><NewLine>File “C:\Users\tegao\mygit\v0512\pytorch\tools\setup_helpers\cmake.py”, line 345, in build<br/><NewLine>self.run(build_args, my_env)<br/><NewLine>File “C:\Users\tegao\mygit\v0512\pytorch\tools\setup_helpers\cmake.py”, line 141, in run<br/><NewLine>check_call(command, cwd=self.build_dir, env=env)<br/><NewLine>File “C:\Users\tegao\Anaconda3\envs\gt37\lib\subprocess.py”, line 363, in check_call<br/><NewLine>raise CalledProcessError(retcode, cmd)<br/><NewLine>subprocess.CalledProcessError: Command ‘[‘cmake’, ‘–build’, ‘.’, ‘–target’, ‘install’, ‘–config’, ‘Debug’, ‘–’, ‘-j’, ‘4’]’ returned non-zero exit status 1.</p><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/teng,(teng),teng,"May 14, 2020,  3:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually, you’ll need to append the directory of <code>python37_d.lib</code> to the environment variable <code>LIB</code>. And I vaguely remember that it will then fail with something else that I cannot resolve.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much! I works when set LIB.<br/><NewLine>But after building, it fails to “import torch”. Is it the same problem as yours?</p><NewLine><blockquote><NewLine><p>(gt37) C:\Users\tegao\mygit\v0512\pytorch&gt;python<br/><NewLine>Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)] on win32<br/><NewLine>Type “help”, “copyright”, “credits” or “license” for more information.<br/><NewLine>&gt;&gt;&gt; import torch<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “”, line 1, in <br/><NewLine>File “C:\Users\tegao\mygit\v0512\pytorch\torch_<em>init</em>_.py”, line 247, in <br/><NewLine>from .random import set_rng_state, get_rng_state, manual_seed, initial_seed, seed<br/><NewLine>File “C:\Users\tegao\mygit\v0512\pytorch\torch\random.py”, line 4, in <br/><NewLine>from torch._C import default_generator<br/><NewLine>ImportError: cannot import name ‘default_generator’ from ‘torch._C’ (unknown location)</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""81127"" data-username=""teng""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/t/b782af/40.png"" width=""20""/> teng:</div><NewLine><blockquote><NewLine><p>ImportError: cannot import name ‘default_generator’ from ‘torch._C’ (unknown location)</p><NewLine></blockquote><NewLine></aside><NewLine><p>Nope, mine is a different one. As for your problem, if you have a previous installation, please remember to remove that first.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot!<br/><NewLine>I move to a new computer and fail to “import torch” again. Because the initial failing to build problem is solved by your kindly help “set LIB=XXX\Anaconda3\envs\myenv\libs”, I close this disscussion and create a new one <a href=""https://discuss.pytorch.org/t/fail-to-import-torch-on-windows-debug-build/81347"">here</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/peterjc123; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/teng; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/peterjc123; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/teng; <NewLine> ,"REPLY_DATE 1: May 15, 2020, 10:11am; <NewLine> REPLY_DATE 2: May 14, 2020, 10:34am; <NewLine> REPLY_DATE 3: May 14, 2020, 12:29pm; <NewLine> REPLY_DATE 4: May 15, 2020, 10:10am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
80843,PyTorch generic analog of F.interpolate?,2020-05-12T11:43:34.242Z,1,171,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am trying to convert my model to TFlite and getting troubles because of F.interpolate? some how it has control flow in it and so TFlite converter dies. I also tried equivalent repeat_interleave it works fine but pytorch refuses to convert that n ONNX model. Could you please offer me some analog of F.interpolate based on very basic pytorch functions, speed is actually has secondary priority.</p><NewLine></div>",https://discuss.pytorch.org/u/Marat,(Закиров Марат),Marat,"May 12, 2020, 11:43am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do you get an error message when the converter dies?<br/><NewLine>If so, could you post it please?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>!</p><NewLine><p>I am converting this to ONNX then to *.pb (GraphDef using onnx-tensorflow project, which is OK in execution) and then to TFlite which converter dies.<br/><NewLine>I am using CUDA_VISIBLE_DEVICES=-1 and <span class=""hashtag"">#613</span> fix for onnx_tensorflow project (for Dilation2d issue)<br/><NewLine>ONNX : 1.6.0<br/><NewLine>TF: 2.1.0<br/><NewLine>troch: 1.5.0</p><NewLine><pre><code class=""lang-auto"">class SimpleNet(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super(SimpleNet, self).__init__()<NewLine><NewLine>    def forward(self, x):<NewLine>        x = F.interpolate(x, scale_factor=2, mode='nearest')#x.view(1, 3, 672, 672)<NewLine>        return x<NewLine></code></pre><NewLine><p>Most interesting part of output:</p><NewLine><pre><code class=""lang-auto"">2020-05-13 10:47:51.019359: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: If<NewLine>2020-05-13 10:47:51.019406: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)<NewLine>Fatal Python error: Aborted<NewLine></code></pre><NewLine><p>Whole output:</p><NewLine><pre><code class=""lang-auto"">tensorflow.lite.python.convert.ConverterError: See console for info.<NewLine>2020-05-13 10:47:50.562338: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/home/marat/anaconda3/lib<NewLine>2020-05-13 10:47:50.562423: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/home/marat/anaconda3/lib<NewLine>2020-05-13 10:47:50.562430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.<NewLine>2020-05-13 10:47:51.003834: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA<NewLine>2020-05-13 10:47:51.009672: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3407930000 Hz<NewLine>2020-05-13 10:47:51.010213: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a180ac8c00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:<NewLine>2020-05-13 10:47:51.010228: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version<NewLine>2020-05-13 10:47:51.011847: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1<NewLine>2020-05-13 10:47:51.014197: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected<NewLine>2020-05-13 10:47:51.014232: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: saturn<NewLine>2020-05-13 10:47:51.014237: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: saturn<NewLine>2020-05-13 10:47:51.014283: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 440.64.0<NewLine>2020-05-13 10:47:51.014316: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 440.64.0<NewLine>2020-05-13 10:47:51.014321: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 440.64.0<NewLine>2020-05-13 10:47:51.019359: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: If<NewLine>2020-05-13 10:47:51.019406: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)<NewLine>Fatal Python error: Aborted<NewLine><NewLine>Current thread 0x00007f68c1c05700 (most recent call first):<NewLine>  File ""/home/marat/anaconda3/envs/cexp/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 56 in execute<NewLine>  File ""/home/marat/anaconda3/envs/cexp/lib/python3.7/site-packages/absl/app.py"", line 250 in _run_main<NewLine>  File ""/home/marat/anaconda3/envs/cexp/lib/python3.7/site-packages/absl/app.py"", line 299 in run<NewLine>  File ""/home/marat/anaconda3/envs/cexp/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40 in run<NewLine>  File ""/home/marat/anaconda3/envs/cexp/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 93 in main<NewLine>  File ""/home/marat/anaconda3/envs/cexp/bin/toco_from_protos"", line 8 in &lt;module&gt;<NewLine>Aborted (core dumped)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the update. This seems to be a TensorFlow issue, and I’m not experienced enough with this framework, so you might get a better answer of StackOverflow.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Marat; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: May 13, 2020,  2:24am; <NewLine> REPLY_DATE 2: May 13, 2020,  7:53am; <NewLine> REPLY_DATE 3: May 14, 2020, 12:35am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
64596,Fft: ATen not compiled with MKL support,2019-12-20T07:54:22.176Z,1,209,"<div class=""post"" itemprop=""articleBody""><NewLine><p>May I know what should I do prior to PyTorch installation to get rid of this error:</p><NewLine><pre><code class=""lang-auto"">  File ""main.py"", line 15, in &lt;module&gt;<NewLine>    specgram = torchaudio.transforms.Spectrogram()(waveform)<NewLine>  File ""/home/mnaderan/.local/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""build/bdist.linux-x86_64/egg/torchaudio/transforms.py"", line 70, in forward<NewLine>  File ""build/bdist.linux-x86_64/egg/torchaudio/functional.py"", line 259, in spectrogram<NewLine>  File ""build/bdist.linux-x86_64/egg/torchaudio/functional.py"", line 52, in _stft<NewLine>  File ""&lt;__torch_function__ internals&gt;"", line 6, in stft<NewLine>  File ""/home/mnaderan/.local/lib/python2.7/site-packages/torch/_overrides.py"", line 140, in _implement_torch_function<NewLine>    return implementation(*args, **kwargs)<NewLine>  File ""/home/mnaderan/.local/lib/python2.7/site-packages/torch/functional.py"", line 427, in stft<NewLine>    return torch._C._VariableFunctions.stft(input, n_fft, hop_length, win_length, window, normalized, onesided)<NewLine>RuntimeError: fft: ATen not compiled with MKL support<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/mahmoodn,(Mahmood Naderan),mahmoodn,"December 20, 2019,  7:54am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""64596"" data-username=""mahmoodn""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mahmoodn/40/13302_2.png"" width=""20""/> mahmoodn:</div><NewLine><blockquote><NewLine><p>:</p><NewLine></blockquote><NewLine></aside><NewLine><p>bump! any solution to this problem?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>for anyone coming across this, the issue seems to be a non intel cpu or you haven’t compiled with MKL support</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/miken; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/miken; <NewLine> ,"REPLY_DATE 1: May 12, 2020,  9:04pm; <NewLine> REPLY_DATE 2: May 13, 2020,  4:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
79971,"RuntimeError: shape &lsquo;[1, 573, 447, 1]&rsquo; is invalid for input of size 768393",2020-05-06T10:39:48.118Z,1,104,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to train a fully convolutional network with input images that all have a different size.</p><NewLine><p>Model FCN8s</p><NewLine><pre><code class=""lang-auto"">        super(FCN8s, self).__init__()<NewLine>        # conv1<NewLine>        self.conv1_1 = nn.Conv2d(3, 64, 3, padding=100)<NewLine>        self.relu1_1 = nn.ReLU(inplace=True)<NewLine>        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)<NewLine>        self.relu1_2 = nn.ReLU(inplace=True)<NewLine>        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/2<NewLine><NewLine>        # conv2<NewLine>        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)<NewLine>        self.relu2_1 = nn.ReLU(inplace=True)<NewLine>        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)<NewLine>        self.relu2_2 = nn.ReLU(inplace=True)<NewLine>        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/4<NewLine><NewLine>        # conv3<NewLine>        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)<NewLine>        self.relu3_1 = nn.ReLU(inplace=True)<NewLine>        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)<NewLine>        self.relu3_2 = nn.ReLU(inplace=True)<NewLine>        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)<NewLine>        self.relu3_3 = nn.ReLU(inplace=True)<NewLine>        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/8<NewLine><NewLine>        # conv4<NewLine>        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)<NewLine>        self.relu4_1 = nn.ReLU(inplace=True)<NewLine>        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)<NewLine>        self.relu4_2 = nn.ReLU(inplace=True)<NewLine>        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)<NewLine>        self.relu4_3 = nn.ReLU(inplace=True)<NewLine>        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/16<NewLine><NewLine>        # conv5<NewLine>        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)<NewLine>        self.relu5_1 = nn.ReLU(inplace=True)<NewLine>        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)<NewLine>        self.relu5_2 = nn.ReLU(inplace=True)<NewLine>        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)<NewLine>        self.relu5_3 = nn.ReLU(inplace=True)<NewLine>        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/32<NewLine><NewLine>        # fc6<NewLine>        self.fc6 = nn.Conv2d(512, 4096, 7)<NewLine>        self.relu6 = nn.ReLU(inplace=True)<NewLine>        self.drop6 = nn.Dropout2d()<NewLine><NewLine>        # fc7<NewLine>        self.fc7 = nn.Conv2d(4096, 4096, 1)<NewLine>        self.relu7 = nn.ReLU(inplace=True)<NewLine>        self.drop7 = nn.Dropout2d()<NewLine><NewLine>        self.score_fr = nn.Conv2d(4096, n_class, 1)<NewLine>        self.score_pool3 = nn.Conv2d(256, n_class, 1)<NewLine>        self.score_pool4 = nn.Conv2d(512, n_class, 1)<NewLine><NewLine>        self.upscore2 = nn.ConvTranspose2d(<NewLine>            n_class, n_class, 4, stride=2, bias=False)<NewLine>        self.upscore8 = nn.ConvTranspose2d(<NewLine>            n_class, n_class, 16, stride=8, bias=False)<NewLine>        self.upscore_pool4 = nn.ConvTranspose2d(<NewLine>            n_class, n_class, 4, stride=2, bias=False)<NewLine></code></pre><NewLine><p>I get the following error, and don’t really understand how to fix it:</p><NewLine><blockquote><NewLine><p>RuntimeError: shape ‘[1, 573, 447, 1]’ is invalid for input of size 768393</p><NewLine></blockquote><NewLine><p>This is on line 32 of trainer.py file hosted on GitHub: <a href=""https://github.com/wkentaro/pytorch-fcn/blob/master/torchfcn/trainer.py"" rel=""nofollow noopener"">torchfcn</a></p><NewLine><blockquote><NewLine><p>log_p = log_p[target.view(n, h, w, 1).repeat(1, 1, 1, c) &gt;= 0]</p><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/fredpeertje,(Fred),fredpeertje,"May 6, 2020,  1:24pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The failing operation tries to reshape the <code>target</code> to <code>target.view(n, h, w, 1)</code>, where <code>n, h, w</code> are coming from the <code>input</code>: <code>n, c, h, w = input.size()</code>.<br/><NewLine>The expected shapes are given as:</p><NewLine><pre><code class=""lang-python""># input: (n, c, h, w), target: (n, h, w)<NewLine></code></pre><NewLine><p>which doesn’t seem to be the case for your work flow.<br/><NewLine>Could you check the model output shape as well as the target shape and make sure that the batch dimension as well as the spatial dimensions are equal?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply! Hopefully this provides enough information.</p><NewLine><p>The input and target:</p><NewLine><pre><code class=""lang-auto"">train_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=True, **kwargs)<NewLine>for data, target in train_loader: break<NewLine>print(data.shape)<NewLine>print(target.shape)<NewLine></code></pre><NewLine><p>Print result:</p><NewLine><pre><code class=""lang-auto"">torch.Size([1, 3, 539, 456])<NewLine>torch.Size([1, 539, 456, 3])<NewLine></code></pre><NewLine><p>The output shape of each layer (I used the following code line):</p><NewLine><blockquote><NewLine><p>summary(model, (3, 539, 456))):</p><NewLine></blockquote><NewLine><pre><code class=""lang-auto"">[/root/data/models/pytorch/vgg16_from_caffe.pth] Checking md5 (aa75b158f4181e7f6230029eb96c1b13)<NewLine>Train:   0%|                                            | 0/250 [00:00&lt;?, ?it/s]----------------------------------------------------------------<NewLine>        Layer (type)               Output Shape         Param #<NewLine>================================================================<NewLine>            Conv2d-1         [-1, 64, 745, 478]           1,792<NewLine>              ReLU-2         [-1, 64, 745, 478]               0<NewLine>            Conv2d-3         [-1, 64, 745, 478]          36,928<NewLine>              ReLU-4         [-1, 64, 745, 478]               0<NewLine>         MaxPool2d-5         [-1, 64, 373, 239]               0<NewLine>            Conv2d-6        [-1, 128, 373, 239]          73,856<NewLine>              ReLU-7        [-1, 128, 373, 239]               0<NewLine>            Conv2d-8        [-1, 128, 373, 239]         147,584<NewLine>              ReLU-9        [-1, 128, 373, 239]               0<NewLine>        MaxPool2d-10        [-1, 128, 187, 120]               0<NewLine>           Conv2d-11        [-1, 256, 187, 120]         295,168<NewLine>             ReLU-12        [-1, 256, 187, 120]               0<NewLine>           Conv2d-13        [-1, 256, 187, 120]         590,080<NewLine>             ReLU-14        [-1, 256, 187, 120]               0<NewLine>           Conv2d-15        [-1, 256, 187, 120]         590,080<NewLine>             ReLU-16        [-1, 256, 187, 120]               0<NewLine>        MaxPool2d-17          [-1, 256, 94, 60]               0<NewLine>           Conv2d-18          [-1, 512, 94, 60]       1,180,160<NewLine>             ReLU-19          [-1, 512, 94, 60]               0<NewLine>           Conv2d-20          [-1, 512, 94, 60]       2,359,808<NewLine>             ReLU-21          [-1, 512, 94, 60]               0<NewLine>           Conv2d-22          [-1, 512, 94, 60]       2,359,808<NewLine>             ReLU-23          [-1, 512, 94, 60]               0<NewLine>        MaxPool2d-24          [-1, 512, 47, 30]               0<NewLine>           Conv2d-25          [-1, 512, 47, 30]       2,359,808<NewLine>             ReLU-26          [-1, 512, 47, 30]               0<NewLine>           Conv2d-27          [-1, 512, 47, 30]       2,359,808<NewLine>             ReLU-28          [-1, 512, 47, 30]               0<NewLine>           Conv2d-29          [-1, 512, 47, 30]       2,359,808<NewLine>             ReLU-30          [-1, 512, 47, 30]               0<NewLine>        MaxPool2d-31          [-1, 512, 24, 15]               0<NewLine>           Conv2d-32          [-1, 4096, 18, 9]     102,764,544<NewLine>             ReLU-33          [-1, 4096, 18, 9]               0<NewLine>        Dropout2d-34          [-1, 4096, 18, 9]               0<NewLine>           Conv2d-35          [-1, 4096, 18, 9]      16,781,312<NewLine>             ReLU-36          [-1, 4096, 18, 9]               0<NewLine>        Dropout2d-37          [-1, 4096, 18, 9]               0<NewLine>           Conv2d-38             [-1, 8, 18, 9]          32,776<NewLine>  ConvTranspose2d-39            [-1, 8, 38, 20]           1,024<NewLine>           Conv2d-40            [-1, 8, 47, 30]           4,104<NewLine>  ConvTranspose2d-41            [-1, 8, 78, 42]           1,024<NewLine>           Conv2d-42            [-1, 8, 94, 60]           2,056<NewLine>  ConvTranspose2d-43          [-1, 8, 632, 344]          16,384<NewLine>================================================================<NewLine>Total params: 134,317,912<NewLine>Trainable params: 134,317,912<NewLine>Non-trainable params: 0<NewLine>----------------------------------------------------------------<NewLine>Input size (MB): 1.75<NewLine>Forward/backward pass size (MB): 1599.66<NewLine>Params size (MB): 512.38<NewLine>Estimated Total Size (MB): 2113.80<NewLine>----------------------------------------------------------------<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> I still havent figured this out… I’m using a batch size of 1 because of the varying input image sizes.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>The output of your model seems to have the shape <code>[-1, 8, 632, 344]</code> based on your model summary.<br/><NewLine>However, neither the printed target shape not the error message fit this shape, so I’m unsure how the model is supposed to be used.</p><NewLine><p>The script assumes that the model output and target shape match, while the currently posted shapes don’t really match.</p><NewLine><p>Also, your target seems to use 3 channels in a channels-last format.<br/><NewLine>What does dim3 represent?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/fredpeertje; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/fredpeertje; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: May 7, 2020,  5:16am; <NewLine> REPLY_DATE 2: May 7, 2020,  9:57am; <NewLine> REPLY_DATE 3: May 12, 2020,  8:42pm; <NewLine> REPLY_DATE 4: May 13, 2020,  1:39am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
80218,Using PyTorch with Large Amount of High Frequency Waveforms,2020-05-07T18:46:04.918Z,0,80,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all, first post here. I’m going to apologise in advance for the broadness of this post and if there are better places to ask, please let me know! <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>I am a Machine Learning for Health masters student. The ICU unit I work with has been archiving the high frequency (500Hz) bedside monitor data to a self-built database. My original goal was to predict a physiological event a few minutes in advance from ~3 of these waveforms. Unfortunately, I ran into significant performance problems. The amount of data is huge (~900GB, billions of data points, months of wall time) and preprocessing couldn’t be done on the fly in the Dataset/Dataloader fast enough to put much load if any on the GPUs (only have 6 CPU cores) or else I would have to let the preprocessing run for days, only to find I want to change one thing and have to run it all again. Epochs were taking hours to days. My initial strategy was to just build a DIY Python pipeline with (<code>multiprocessing</code>, <code>numpy</code>, etc) and try to make it as parallel as possible with caching at each step along the way. I should note that our DIY database is fairly slow and can currently only return a couple hours of data in JSON via an API call, so no streaming, etc. I have since pivoted to tackling the larger issue of deploying these models into “production”.</p><NewLine><p>My project is the largest (in terms of data, etc) that our group has done so far. Before, data was downsampled to 0.2 Hz and was just looking at more specific situations, so far less data was relevant (&gt;30GB). This meant it could be kept in CSVs, naively processed, and ran through PyTorch. However, in order to see how these same models behave in real time (eg: displayed at the bedside), they need to consume streaming data from RabbitMQ, which also feeds to waveform database.</p><NewLine><p>Now, instead of trying to parallelize the preprocessing, I have been trying to use PyTorch <code>IterableDataset</code> after reading this article: <a href=""https://medium.com/speechmatics/how-to-build-a-streaming-dataloader-with-pytorch-a66dd891d9dd"" rel=""nofollow noopener"">https://medium.com/speechmatics/how-to-build-a-streaming-dataloader-with-pytorch-a66dd891d9dd</a>  I’ve been building a framework which approaches this in an entirely streaming fashion instead. Without getting too deep into the details, the framework requires researchers provide a minimal amount of config (RabMQ login, relevant data in our database, …) along a variety of Python callables/functions for: preprocessing, assigning a label, … which all operate on a standardised <code>Interval</code> class. Therefore, one can toggle between running the offline pipeline to build up a gigantic processed cache which is then artificially streamed to their PyTorch model, or actually consuming the real streaming data, with just the flip of a config variable.</p><NewLine><p>Of course, then there is the whole other open source side of solutions. Hadoop, Spark, … Things I am not as familiar with. I know these of course work, at massive scales, at tech companies worldwide. If there’s something out there that would allow for me to instead focus back on the Healthcare ML questions and not this data pipe-lining, I’d be extremely grateful to know.</p><NewLine><p>So, I think my <strong>questions</strong> are as follows:</p><NewLine><ol><NewLine><li>How does one work with data that isn’t atomic?<NewLine><ul><NewLine><li>With images, you have a certain number of photos, so indexing is easy. Augmentation is also very understandable. Here, we might want to alter window size, overlap, frequency, … and if we cut everything up into the samples that actually go into our model, that means rerunning everything.</li><NewLine></ul><NewLine></li><NewLine><li>What are some standard data science design patterns for these types of problems?<NewLine><ul><NewLine><li>I’m certainly far from the first person that wants to strap a neural net onto some waveform data and do outlier detection/event prediction. Obviously places like Google, etc take in far more data per second from say their clusters and want to predict failures, etc. That is highly similar to predicting a heart attach from ECG and such.</li><NewLine></ul><NewLine></li><NewLine><li>Does cooking up a custom framework for my lab as I described above make sense?</li><NewLine></ol><NewLine><p>If you made it through or even just did a quick skim, thank you very much! This has been a gigantic blocker on my research work for months now so any help is appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/carsonmclean,(Carson McLean),carsonmclean,"May 7, 2020,  6:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m actually working on a similar problem. To reduce the size of the processed the WAV using Mel-frequency cepstrum, it applies a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency, by applying that i was able to reduce the size of my data from 300GB to just over 20GB.</p><NewLine><p>I’ve also split my data into 3 segments: development, training and testing. The development set is much smaller than the training ~ 5% of the training set size, and it’s purpose is for quick experimentation. With this approach you can quickly catch errors, and you can move to the larger training sample once you are satisfied with the results.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dachosen1; <NewLine> ,"REPLY_DATE 1: May 7, 2020,  7:18pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
79892,Deployment in FP16? Calling model.half() only reduces memory by 7%,2020-05-05T21:02:16.273Z,0,104,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m using the standard ResNet50 from torchvision model with an added 512x128 FC layer. Calling <code>model.half()</code> on it only bring it down from 940mb to 870mb GPU usage. Shouldn’t there be a more significant reduction in memory usage?<br/><NewLine>I call <code>torch.cuda.empty_cache()</code> after initializing the model. I’ve set <code>torch.backends.cudnn.benchmark = True</code> as well.</p><NewLine><p>Same behavior on Tesla T4 and 2080Ti.</p><NewLine><p>model:</p><NewLine><pre><code class=""lang-python"">model = torchvision.models.resnet50(<NewLine>                pretrained=False, progress=False)<NewLine>model.fc = torch.nn.Sequential(<NewLine>                torch.nn.Linear(self.model.fc.in_features, 512),<NewLine>                torch.nn.Linear(512, 128),<NewLine>                torch.nn.Linear(128, 1)<NewLine>            )<NewLine>## section to load weights ## <NewLine>if use_fp16:<NewLine>    model.half()<NewLine>model.to(compute_device)<NewLine>model.eval()<NewLine>torch.cuda.empty_cache()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/IsCoelacanth,(Anurag Malyala ),IsCoelacanth,"May 5, 2020,  9:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>As far as I know, you should use tensorRT to get the real speedup and reduce gpu memory.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/cyanM; <NewLine> ,"REPLY_DATE 1: May 6, 2020,  3:25am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
78991,Reducing docker size with PyTorch model,2020-04-29T15:58:39.225Z,1,525,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’d like to deploy four of my models with a total size of ~100mb when the state saved on disk. It would be great if the docker could take as small space as possible, no more than 700 mb.</p><NewLine><p>Now I’m creating docker and install a few dependencies. Suddenly it takes 2.8 GB on disk, where PyTorch and related libraries take at least 800 MB in conda.</p><NewLine><p>Therefore I’m looking for a simple way to deploy these models. As far as I understand I could use jit and e able to run models with small library libtorch. However, I’d like to run the model in Python, not C++. Is it possible?</p><NewLine><p>And the second thing I don’t understand is whether I should use onnx or jit? What is the difference?</p><NewLine><p>I’m looking for a simple guide with the steps to do that: export with jit and load with some lib I don’t know in Python? Or export to ONNX and then to TensorFlow? Or maybe get rid of conda and just install some PyTorch version and it should be no more than 80 MB on disk?</p><NewLine><p>Any help appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/Dawid_S,(Dawid S),Dawid_S,"April 29, 2020,  3:58pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>if you are deploying to a CPU inference, instead of GPU-based, then you can save a lot of space by installing PyTorch with CPU-only capabilities. That significantly reduces the docker image size (the pytorch component is ~128MB compressed.</p><NewLine><p>To install CPU-only, go to <a href=""https://pytorch.org"">https://pytorch.org</a> and in the Install selector, select the option <code>CUDA</code> to be <code>None</code> and you will get the right set of commands.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/smth"">@smth</a> hank you for the help!<br/><NewLine>unfortunately, having empty <code>virtualenv</code> of size 4.8 mb, after the command:</p><NewLine><p><code>pip install https://download.pytorch.org/whl/cpu/torch-1.5.0-cp37-none-macosx_10_9_x86_64.whl</code><br/><NewLine>or command<br/><NewLine><code>pip install torch</code></p><NewLine><p>venv grows to 375 mb.</p><NewLine><p>I’m using Python 3.7 and macOS Catalina 10.15.4</p><NewLine><p>My lib catalog in venv looks like this:</p><NewLine><blockquote><NewLine><p>16M caffe2<br/><NewLine>4,0K easy-install.pth<br/><NewLine>3,0M future<br/><NewLine>56K future-0.18.2.dist-info<br/><NewLine>292K libfuturize<br/><NewLine>236K libpasteurize<br/><NewLine>86M numpy<br/><NewLine>132K numpy-1.18.3.dist-info<br/><NewLine>116K past<br/><NewLine>7,3M pip-19.0.3-py3.7.egg<br/><NewLine>560K setuptools-40.8.0-py3.7.egg<br/><NewLine>4,0K setuptools.pth<br/><NewLine>256M torch<br/><NewLine>352K torch-1.5.0.dist-info</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>correct, that is uncompressed. when you compress it, it should be about half of that.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not familiar with any way of using it as compressed package. I mean, when I want to infer, I need to uncompress it anyway?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I assume the best way is to convert model and use it with torchlib or export it to onnx and try there.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Dawid_S; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Dawid_S; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Dawid_S; <NewLine> ,"REPLY_DATE 1: April 29, 2020,  4:37pm; <NewLine> REPLY_DATE 2: April 29, 2020,  4:56pm; <NewLine> REPLY_DATE 3: April 29, 2020,  5:18pm; <NewLine> REPLY_DATE 4: April 29, 2020,  5:33pm; <NewLine> REPLY_DATE 5: May 2, 2020,  8:13pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
79209,Deploying Pytorch model to production in Mobile,2020-05-01T04:08:12.719Z,0,130,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there, I’m completely new to the mobile app development and would like to deploy my model trained using pytorch to both android and ios. Now I know one way for this is to use the <strong>Pytorch Mobile</strong>, but for that, I’d have to build my app with different code bases for both android and ios (which I’m not worried that much though). I’m just wondering is there a way for deploying pytorch models for both android and ios using the same codebase.</p><NewLine><p>Hence I’m really interested in Flutter, but there’s no as such official way of deploying a pytorch model to flutter. (Some packages are there but they aren’t updated since like last year)</p><NewLine><p>One way I can think of is by deploying my model using Flask or Django as a REST API, and then use the JSON file to in order to connect it to Flutter. One con of this approach is that model wouldn’t run on the device, so I’d have to deploy the API to the server, which of course could cost money for better inference.</p><NewLine><p>So is there any better way for the same. Again, I’ve no problem for learning different things for different os, just wondering if such an approach exists.</p><NewLine></div>",https://discuss.pytorch.org/u/braindotai,,braindotai,"May 1, 2020,  4:10am",,,,,
78980,Error with java class org.pytorch.serve.ModelServer when starting TorchServe,2020-04-29T14:27:44.279Z,1,226,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I have recently installed TorchServe on Windows 10 using pip:</p><NewLine><p>pip install -f <a href=""https://download.pytorch.org/whl/torch_stable.html"" rel=""nofollow noopener"">https://download.pytorch.org/whl/torch_stable.html</a> torchserve torch-model-archiver</p><NewLine><p>I had no problems archiving the sample model, it generated the output file densenet161.mar:</p><NewLine><p>torch-model-archiver --model-name densenet161 --version 1.0 --model-file serve/examples/image_classifier/densenet_161/model.py --serialized-file model_store/densenet161-8d451a50.pth --extra-files serve/examples/image_classifier/index_to_name.json --handler image_classifier</p><NewLine><p>But when I try to start the server with following line (model_store subdirectory is inside current directory):</p><NewLine><p>torchserve --start --model-store model_store --models model_store/densenet161.mar</p><NewLine><p>Then I got following error:</p><NewLine><p>Error: Could not find or load main class org.pytorch.serve.ModelServer<br/><NewLine>Caused by: java.lang.ClassNotFoundException: org.pytorch.serve.ModelServer</p><NewLine><p>Java version:</p><NewLine><p>java –version<br/><NewLine>java version “11” 2018-09-25<br/><NewLine>Java™ SE Runtime Environment 18.9 (build 11+28)<br/><NewLine>Java HotSpot™ 64-Bit Server VM 18.9 (build 11+28, mixed mode)</p><NewLine><p>Python version:</p><NewLine><p>Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)] on win32</p><NewLine><p>PyTorch version:</p><NewLine><blockquote><NewLine><blockquote><NewLine><blockquote><NewLine><p>print(torch.<strong>version</strong>)<br/><NewLine>1.5.0+cpu</p><NewLine></blockquote><NewLine></blockquote><NewLine></blockquote><NewLine><p>I appreciate any help. Thanks in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/mike_deepx,(Mike),mike_deepx,"April 29, 2020,  2:27pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/32516#issuecomment-621054762"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/32516#issuecomment-621054762"" rel=""nofollow noopener"" target=""_blank"">Java bindings for Windows</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-01-23"" data-format=""ll"" data-time=""00:36:08"" data-timezone=""UTC"">12:36AM - 23 Jan 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/dreiss"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""dreiss"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars0.githubusercontent.com/u/4121?v=4"" width=""20""/><NewLine>          dreiss<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">🚀 Feature<NewLine>It would be cool if the Java bindings worked on Windows. They don't right now. If you are interested in...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">feature</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">module: java</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">module: windows</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your answer. I see now that this a previously reported issue. Thanks again.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/peterjc123; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mike_deepx; <NewLine> ,"REPLY_DATE 1: April 30, 2020,  2:25am; <NewLine> REPLY_DATE 2: May 1, 2020,  2:38am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
75281,PyTorch model to Onnx to TensorRT engine = no speed up for inference?,2020-04-03T23:15:07.027Z,0,1358,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey everyone,</p><NewLine><p>I’m working with a Jetson Nano device, TRT 6 (the latest version that can be used on the Nano), PyTorch 1.2.0 (compatible with TRT6), and Torchvision 0.4.0 (compatible with PyTorch 1.2.0).</p><NewLine><p>I have a Torchvision Mobilenetv2 model I exported to Onnx with the built-in function:</p><NewLine><pre><code class=""lang-auto"">    torch.onnx.export(pt_model, dummy_input, out_path, verbose=True)<NewLine></code></pre><NewLine><p>I then built a TensorRt engine with this Onnx model:</p><NewLine><pre><code class=""lang-auto"">with trt.Builder(TRT_LOGGER) as builder, \<NewLine>      builder.create_network(*EXPLICIT_BATCH) as network, \<NewLine>      trt.OnnxParser(network, TRT_LOGGER) as parser:<NewLine><NewLine>      builder.max_workspace_size = 1 &lt;&lt; 28<NewLine>      builder.max_batch_size = 1<NewLine>      builder.fp16_mode = True<NewLine>      # ... <NewLine>      engine = builder.build_cuda_engine(network)<NewLine></code></pre><NewLine><p>Then I run inference on this new engine on my Jetson Nano device and can get a latency of about 0.045 seconds (22.2 fps). Running inference on the PyTorch version of this model also has almost the exact same latency of 0.045 seconds.</p><NewLine><p>I also tried to change the mode to INT8 mode when building the TensorRT engine and get the error: <code>Builder failed while configuring INT8 mode</code>.</p><NewLine><p>Anyone have experience with optimizing Torch models with TensorRT? Am I missing something fundamental when building the TensorRT engine or should I expect a speed-up?</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/jdev,(Jeremy),jdev,"April 3, 2020, 11:27pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I haven’t tried to export Mobilenet yet, but could see a speedup for FasterRCNN.<br/><NewLine>How are you currently measuring the the throughput?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for this thread. I am also in the same boat, trying to figure out optimizing networks for jetson. <a class=""mention"" href=""/u/jdev"">@jdev</a> Have you tried the int8 quantization pipeline, first calibrating images to int8 and then doing inference on it? I believe you will run into difficulties for operations that are not supported.</p><NewLine><p>Nvidia’s retinanet-examples repo gives a working pipeline but it’s obviously much more useful to have this going for our own custom networks.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kakrafoon; <NewLine> ,"REPLY_DATE 1: April 4, 2020,  5:27am; <NewLine> REPLY_DATE 2: April 29, 2020,  6:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
78771,Is there a slimmed down pytorch for computation?,2020-04-28T06:12:25.149Z,4,560,"<div class=""post"" itemprop=""articleBody""><NewLine><p>We developed a neural network in PyTorch and when we try to deploy we had to include PyTorch in the docker image. Backend team complains that PyTorch is “too heavy”. Is there a fast pkg out there that can carry out the computation of a neural network developed by Pytorch?</p><NewLine></div>",https://discuss.pytorch.org/u/xiaodai,(evalparse),xiaodai,"April 28, 2020,  6:12am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>we provide a <code>cpuonly</code> version of PyTorch without the CUDA functionality, and it is significantly smaller in size (about 5x smaller).</p><NewLine><p>To install it go to <a href=""https://pytorch.org"">https://pytorch.org</a> and in the Install Selector, select <code>CUDA</code> to be <code>None</code>.</p><NewLine><p>Hopefully your backend team is happy with this version.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much! Do you see a possibility of separating out the functionality to run a NN into a separate tiny package that only runs the models? That would be really awesome for deployment.</p><NewLine><p>Thanks again for your help?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>we actually provide something like that (a runtime that is ~5MB or lesser), but for Mobile only at the moment, i.e. for iOS / Android: <a href=""https://pytorch.org/mobile/home/"">https://pytorch.org/mobile/home/</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>part of the challenge when you run on Desktop / Server is not so much the size of PyTorch, but the size of depedencies that make deployment also efficient.</p><NewLine><p>On x86 CPU, you will want Intel MKL and MKLDNN (which PyTorch install provides) which themselves are ~70MB and ~50+MB respectively. They are used for fast matrix multiplications and neural network operations.</p><NewLine><p>on GPU, you will want CUDA and CuDNN, which take about 2GB</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Someone on twitter suggested the onnxruntime might be worth looking into</p><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""16""/><NewLine><a href=""https://github.com/microsoft/onnxruntime"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars2.githubusercontent.com/u/6154722?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/microsoft/onnxruntime"" rel=""nofollow noopener"" target=""_blank"">microsoft/onnxruntime</a></h3><NewLine><p>ONNX Runtime: cross-platform, high performance scoring engine for ML models - microsoft/onnxruntime</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>including here for ppl stumbling upon this.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/smth"">@smth</a> PyTorch could consider shipping with custom builds of MKL libraries that strip out unwanted features. If that happens, most don’t have to go search for an optimized inference engine</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>in the pytorch CPU-only wheels, we actually do statically link MKL and MKLDNN so that the unneeded symbols are stripped out, hence their respective sizes.</p><NewLine><p>The reality is, whether you use ONNX Runtime or PyTorch wheel, if you want MKL and MKLDNN, you will pay the cost of the size of the routines they ship, and their BLAS routines and their Convolution routines are large (they ship a lot of precompiled codegen).</p><NewLine><p>The PyTorch CPU-only wheel today ships at a size of 127.3MB for PyTorch 1.5.0</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Transitioning from MKLDNN to a Glow CPU backend would be great!</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>then we have to include precompiled Glow code?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xiaodai; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/xiaodai; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/suryaprakaz; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/smth; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/suryaprakaz; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/smth; <NewLine> ,"REPLY_DATE 1: April 28, 2020,  6:17am; <NewLine> REPLY_DATE 2: April 28, 2020,  6:31am; <NewLine> REPLY_DATE 3: April 28, 2020,  6:32am; <NewLine> REPLY_DATE 4: April 28, 2020,  6:35am; <NewLine> REPLY_DATE 5: April 28, 2020,  6:47am; <NewLine> REPLY_DATE 6: April 28, 2020,  6:51am; <NewLine> REPLY_DATE 7: April 28, 2020,  4:45pm; <NewLine> REPLY_DATE 8: April 28, 2020,  5:04pm; <NewLine> REPLY_DATE 9: April 28, 2020,  5:39pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> 
78763,Assigning Module to Device,2020-04-28T04:44:51.249Z,0,52,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was wondering if someone could walk me through what happens when<br/><NewLine><code>model = model.to('cuda:0')</code> is called.</p><NewLine><p>In particular how do parameters and buffers get moved onto the GPU</p><NewLine></div>",https://discuss.pytorch.org/u/almeetb,,almeetb,"April 28, 2020,  4:44am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>[Here[(<a href=""https://github.com/pytorch/pytorch/blob/ebcacd5e878f3f8e84c941d6b9567c88f50a2f10/torch/nn/modules/module.py#L367"">https://github.com/pytorch/pytorch/blob/ebcacd5e878f3f8e84c941d6b9567c88f50a2f10/torch/nn/modules/module.py#L367</a>) you can find the <code>module.to()</code> implementation, which calls <a href=""https://github.com/pytorch/pytorch/blob/ebcacd5e878f3f8e84c941d6b9567c88f50a2f10/torch/nn/modules/module.py#L445-L448"">convert</a> recursively on all parameters and buffers.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: April 28, 2020,  7:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
64662,PyTorch on MicroControllers,2019-12-20T18:51:14.817Z,1,624,"<div class=""post"" itemprop=""articleBody""><NewLine><p>We are working on deploying PyTorch on Micro Controllers with different sensors.</p><NewLine><p>We are looking for optimization of exported model with quantization, fusion and other memory reduction techniques before exporting the model to onnx.</p><NewLine><p>I’ve searched PyTorch Docs, Tutorials and Mobile Demos without luck.</p><NewLine><p>If you’ve worked, planning to work or have interest, please leave guidance, comment and other helpful resources.</p><NewLine><p>Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/srohit0,(Rohit Sharma),srohit0,"December 20, 2019,  6:51pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Has there been any update on this?<br/><NewLine>I’m interested in trying this too.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/anandijain"">@anandijain</a>, nice to connect with you here. We’re putting effort behind this to get it working. Check out <a href=""http://bit.ly/deep-C"" rel=""nofollow noopener"">bit.ly/deep-C</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/anandijain; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/srohit0; <NewLine> ,"REPLY_DATE 1: March 6, 2020,  6:06am; <NewLine> REPLY_DATE 2: April 25, 2020,  3:00pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
77970,Docker build error,2020-04-23T04:39:28.831Z,0,309,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I tried building the pytorch/docker/pytorch/dockerfile using two ways</p><NewLine><ol><NewLine><li>in the docker/pytorch/ directory</li><NewLine></ol><NewLine><p>docker build -t pytorch_build:1.0 .</p><NewLine><p>i got the following error<br/><NewLine>Package numpy conflicts for:<br/><NewLine>numpy<br/><NewLine>scipy -&gt; numpy[version=’&gt;=1.11.3,&lt;2.0a0|&gt;=1.14.6,&lt;2.0a0|&gt;=1.15.1,&lt;2.0a0|&gt;=1.9.3,&lt;2.0a0’]</p><NewLine><p>Package wheel conflicts for:<br/><NewLine>pip -&gt; wheel<br/><NewLine>python=3.8 -&gt; pip -&gt; wheel<br/><NewLine>wheel</p><NewLine><p>Package pycparser conflicts for:<br/><NewLine>cffi -&gt; pycparser<br/><NewLine>pycparser<br/><NewLine>cryptography -&gt; cffi[version=’&gt;=1.8’] -&gt; pycparser</p><NewLine><p>The command ‘/bin/sh -c curl -o ~/miniconda.sh <a href=""https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"" rel=""nofollow noopener"">https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</a> &amp;&amp;      chmod +x ~/miniconda.sh &amp;&amp;      ~/miniconda.sh -b -p /opt/conda &amp;&amp;      rm ~/miniconda.sh &amp;&amp;      /opt/conda/bin/conda install -y python=$PYTHON_VERSION numpy pyyaml scipy ipython mkl mkl-include ninja cython typing &amp;&amp;      /opt/conda/bin/conda install -y -c pytorch magma-cuda100 &amp;&amp;      /opt/conda/bin/conda clean -ya’ returned a non-zero code: 1</p><NewLine><ol start=""2""><NewLine><li>using the Makefile</li><NewLine></ol><NewLine><p>i used             $ sudo make -f docker.Makefile</p><NewLine><p>got the following error:</p><NewLine><h2>=&gt; ERROR [conda 1/1] RUN curl -v -o ~/miniconda.sh -O  https:// repo.continuum .io/miniconda/Miniconda3-latest-Linux-x86_64.sh  &amp;&amp;     chmod +x ~/miniconda.sh &amp;&amp;     ~/miniconda.sh -b -p /opt/conda   3.8s<br/><NewLine>=&gt; CANCELED [submodule-update 2/3] COPY . .                                                                                                                                                           3.9s</h2><NewLine><blockquote><NewLine><p>[conda 1/1] RUN curl -v -o ~/miniconda.sh -O  <a href=""https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh"" rel=""nofollow noopener"">https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh</a>  &amp;&amp;     chmod +x ~/miniconda.sh &amp;&amp;     ~/miniconda.sh -b -p /opt/conda &amp;&amp;     rm ~/miniconda.sh &amp;&amp;     /opt/conda/bin/conda install -y python=${PYTHON_VERSION} conda-build pyyaml numpy ipython&amp;&amp;     /opt/conda/bin/conda clean -ya:<br/><NewLine><span class=""hashtag"">#11</span> 0.590   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current<br/><NewLine><span class=""hashtag"">#11</span> 0.590                                  Dload  Upload   Total   Spent    Left  Speed<br/><NewLine>0     0    0     0    0     0      0      0 --:–:-- --:–:-- --:–:--     0*   Trying 104.18.200.79…<br/><NewLine><span class=""hashtag"">#11</span> 0.843 * TCP_NODELAY set<br/><NewLine><span class=""hashtag"">#11</span> 0.871 * Connected to repo.continuum .io (104.18.200.79) port 443 (<span class=""hashtag"">#0</span>)<br/><NewLine><span class=""hashtag"">#11</span> 0.875 * ALPN, offering h2<br/><NewLine><span class=""hashtag"">#11</span> 0.875 * ALPN, offering http/1.1<br/><NewLine><span class=""hashtag"">#11</span> 0.894 * successfully set certificate verify locations:<br/><NewLine><span class=""hashtag"">#11</span> 0.894 *   CAfile: /etc/ssl/certs/ca-certificates.crt<br/><NewLine><span class=""hashtag"">#11</span> 0.894   CApath: /etc/ssl/certs<br/><NewLine><span class=""hashtag"">#11</span> 0.895 } [5 bytes data]<br/><NewLine><span class=""hashtag"">#11</span> 0.895 * TLSv1.3 (OUT), TLS handshake, Client hello (1):<br/><NewLine><span class=""hashtag"">#11</span> 0.895 } [512 bytes data]<br/><NewLine><span class=""hashtag"">#11</span> 0.930 * TLSv1.3 (IN), TLS handshake, Server hello (2):<br/><NewLine><span class=""hashtag"">#11</span> 0.930 { [104 bytes data]<br/><NewLine><span class=""hashtag"">#11</span> 0.930 * TLSv1.2 (IN), TLS handshake, Certificate (11):<br/><NewLine><span class=""hashtag"">#11</span> 0.930 { [2206 bytes data]<br/><NewLine><span class=""hashtag"">#11</span> 0.932 * TLSv1.2 (IN), TLS handshake, Server key exchange (12):<br/><NewLine><span class=""hashtag"">#11</span> 0.932 { [115 bytes data]<br/><NewLine><span class=""hashtag"">#11</span> 0.932 * TLSv1.2 (IN), TLS handshake, Server finished (14):<br/><NewLine><span class=""hashtag"">#11</span> 0.932 { [4 bytes data]<br/><NewLine><span class=""hashtag"">#11</span> 0.932 * TLSv1.2 (OUT), TLS handshake, Client key exchange (16):<br/><NewLine><span class=""hashtag"">#11</span> 0.932 } [37 bytes data]<br/><NewLine><span class=""hashtag"">#11</span> 0.933 * TLSv1.2 (OUT), TLS change cipher, Client hello (1):<br/><NewLine><span class=""hashtag"">#11</span> 0.933 } [1 bytes data]<br/><NewLine><span class=""hashtag"">#11</span> 0.933 * TLSv1.2 (OUT), TLS handshake, Finished (20):<br/><NewLine><span class=""hashtag"">#11</span> 0.933 } [16 bytes data]<br/><NewLine><span class=""hashtag"">#11</span> 0.961 * TLSv1.2 (IN), TLS handshake, Finished (20):<br/><NewLine><span class=""hashtag"">#11</span> 0.961 { [16 bytes data]<br/><NewLine><span class=""hashtag"">#11</span> 0.961 * SSL connection using TLSv1.2 / ECDHE-ECDSA-CHACHA20-POLY1305<br/><NewLine><span class=""hashtag"">#11</span> 0.961 * ALPN, server accepted to use h2<br/><NewLine><span class=""hashtag"">#11</span> 0.961 * Server certificate:<br/><NewLine><span class=""hashtag"">#11</span> 0.961 *  subject: C=US; ST=CA; L=San Francisco; O=Cloudflare, Inc.; CN=sni.cloudflaressl .com<br/><NewLine><span class=""hashtag"">#11</span> 0.961 *  start date: Apr  6 00:00:00 2020 GMT<br/><NewLine><span class=""hashtag"">#11</span> 0.961 *  expire date: Oct  9 12:00:00 2020 GMT<br/><NewLine><span class=""hashtag"">#11</span> 0.962 *  subjectAltName: host repo.continuum.i o matched cert’s “*.continuum.io”<br/><NewLine><span class=""hashtag"">#11</span> 0.962 *  issuer: C=US; ST=CA; L=San Francisco; O=CloudFlare, Inc.; CN=CloudFlare Inc ECC CA-2<br/><NewLine><span class=""hashtag"">#11</span> 0.962 *  SSL certificate verify ok.<br/><NewLine><span class=""hashtag"">#11</span> 0.962 * Using HTTP2, server supports multi-use<br/><NewLine><span class=""hashtag"">#11</span> 0.962 * Connection state changed (HTTP/2 confirmed)<br/><NewLine><span class=""hashtag"">#11</span> 0.962 * Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0<br/><NewLine><span class=""hashtag"">#11</span> 0.962 } [5 bytes data]<br/><NewLine><span class=""hashtag"">#11</span> 0.962 * Using Stream ID: 1 (easy handle 0x5594f99cf580)<br/><NewLine><span class=""hashtag"">#11</span> 0.962 } [5 bytes data]<br/><NewLine><span class=""hashtag"">#11</span> 0.963 &gt; GET /miniconda/Miniconda3-latest-Linux-x86_64.sh HTTP/2<br/><NewLine><span class=""hashtag"">#11</span> 0.963 &gt; Host: repo.continuum.i o<br/><NewLine><span class=""hashtag"">#11</span> 0.963 &gt; User-Agent: curl/7.58.0<br/><NewLine><span class=""hashtag"">#11</span> 0.963 &gt; Accept: <em>/</em><br/><NewLine><span class=""hashtag"">#11</span> 0.963 &gt;<br/><NewLine><span class=""hashtag"">#11</span> 0.963 { [5 bytes data]<br/><NewLine><span class=""hashtag"">#11</span> 0.963 * Connection state changed (MAX_CONCURRENT_STREAMS updated)!<br/><NewLine><span class=""hashtag"">#11</span> 0.963 } [5 bytes data]<br/><NewLine><span class=""hashtag"">#11</span> 1.022 &lt; HTTP/2 301<br/><NewLine><span class=""hashtag"">#11</span> 1.022 &lt; date: Thu, 23 Apr 2020 03:42:56 GMT<br/><NewLine><span class=""hashtag"">#11</span> 1.022 &lt; cache-control: max-age=3600<br/><NewLine><span class=""hashtag"">#11</span> 1.022 &lt; expires: Thu, 23 Apr 2020 04:42:56 GMT<br/><NewLine><span class=""hashtag"">#11</span> 1.022 &lt; location: https:/ /repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh<br/><NewLine><span class=""hashtag"">#11</span> 1.022 &lt; expect-ct: max-age=604800, report-uri=“https:/ /report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct”<br/><NewLine><span class=""hashtag"">#11</span> 1.022 &lt; server: cloudflare<br/><NewLine><span class=""hashtag"">#11</span> 1.022 &lt; cf-ray: 588493d40fa9df63-BOM<br/><NewLine><span class=""hashtag"">#11</span> 1.022 &lt; cf-request-id: 0246bab8800000df6320900200000001<br/><NewLine><span class=""hashtag"">#11</span> 1.022 &lt;<br/><NewLine><span class=""hashtag"">#11</span> 1.022 { [0 bytes data]<br/><NewLine>0     0    0     0    0     0      0      0 --:–:-- --:–:-- --:–:--     0<br/><NewLine><span class=""hashtag"">#11</span> 1.022 * Connection <span class=""hashtag"">#0</span> to host repo.continuum .io left intact<br/><NewLine><span class=""hashtag"">#11</span> 1.037 /bin/sh: 1: /opt/conda/bin/conda: not found</p><NewLine></blockquote><NewLine><hr/><NewLine><p>failed to solve with frontend dockerfile.v0: failed to solve with frontend gateway.v0: rpc error: code = Unknown desc = failed to build LLB: executor failed running [/bin/sh -c curl -v -o ~/miniconda.sh -O  h ttps://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh  &amp;&amp;     chmod +x ~/miniconda.sh &amp;&amp;     ~/miniconda.sh -b -p /opt/conda &amp;&amp;     rm ~/miniconda.sh &amp;&amp;     /opt/conda/bin/conda install -y python=${PYTHON_VERSION} conda-build pyyaml numpy ipython&amp;&amp;     /opt/conda/bin/conda clean -ya]: runc did not terminate sucessfully<br/><NewLine>docker.Makefile:32: recipe for target ‘devel-image’ failed<br/><NewLine>make: *** [devel-image] Error 1</p><NewLine><p>Im using ubuntu 18.04.4 with geforce 940mx<br/><NewLine>docker version Docker version 19.03.8<br/><NewLine>and nvidia-docker2 installed alongside</p><NewLine></div>",https://discuss.pytorch.org/u/VIGNESHinZONE,(Vignes Hin Zone),VIGNESHinZONE,"April 23, 2020,  4:39am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you try to use <code>wget</code> instead of <code>curl</code> or <code>curl -Ssl</code>, as the download of miniconda seems to fail?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><strong>here is the dockerfile which i had used</strong> <div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/a6f590efc597446b7afd7c98bde11fd84c6cb743"" href=""https://discuss.pytorch.org/uploads/default/original/3X/a/6/a6f590efc597446b7afd7c98bde11fd84c6cb743.png"" title=""Screenshot from 2020-04-23 15-26-00""><img alt=""Screenshot from 2020-04-23 15-26-00"" data-base62-sha1=""nOZnKSsu57AmbPPRjSVKb9XHpkf"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/6/a6f590efc597446b7afd7c98bde11fd84c6cb743_2_10x10.png"" height=""388"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/6/a6f590efc597446b7afd7c98bde11fd84c6cb743_2_690x388.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/6/a6f590efc597446b7afd7c98bde11fd84c6cb743_2_690x388.png, https://discuss.pytorch.org/uploads/default/optimized/3X/a/6/a6f590efc597446b7afd7c98bde11fd84c6cb743_2_1035x582.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/a/6/a6f590efc597446b7afd7c98bde11fd84c6cb743_2_1380x776.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screenshot from 2020-04-23 15-26-00</span><span class=""informations"">1920×1080 226 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p><strong>im still getting and error seems to be in package conflicts</strong></p><NewLine><p>Package cryptography conflicts for:<br/><NewLine>cryptography<br/><NewLine>urllib3 -&gt; cryptography[version=’&gt;=1.3.4’]<br/><NewLine>urllib3 -&gt; pyopenssl[version=’&gt;=0.14’] -&gt; cryptography[version=’&gt;=1.9|&gt;=2.1.4|&gt;=2.2.1|&gt;=2.8’]<br/><NewLine>conda[version=’&gt;=4.8.2’] -&gt; pyopenssl[version=’&gt;=16.2.0’] -&gt; cryptography[version=’&gt;=1.9|&gt;=2.1.4|&gt;=2.2.1|&gt;=2.8’]<br/><NewLine>requests -&gt; urllib3[version=’&gt;=1.21.1,&lt;1.26,!=1.25.0,!=1.25.1’] -&gt; cryptography[version=’&gt;=1.3.4’]<br/><NewLine>pyopenssl -&gt; cryptography[version=’&gt;=1.9|&gt;=2.1.4|&gt;=2.2.1|&gt;=2.8’]</p><NewLine><p>Package pycosat conflicts for:<br/><NewLine>pycosat<br/><NewLine>conda[version=’&gt;=4.8.2’] -&gt; pycosat[version=’&gt;=0.6.3’]</p><NewLine><p>Package idna conflicts for:<br/><NewLine>conda[version=’&gt;=4.8.2’] -&gt; requests[version=’&gt;=2.18.4,&lt;3’] -&gt; idna[version=’&gt;=2.5,&lt;2.7|&gt;=2.5,&lt;2.8|&gt;=2.5,&lt;2.9|&gt;=2.5,&lt;3’]<br/><NewLine>urllib3 -&gt; idna[version=’&gt;=2.0.0’]<br/><NewLine>requests -&gt; idna[version=’&gt;=2.5,&lt;2.7|&gt;=2.5,&lt;2.8|&gt;=2.5,&lt;2.9|&gt;=2.5,&lt;3’]<br/><NewLine>idna<br/><NewLine>urllib3 -&gt; cryptography[version=’&gt;=1.3.4’] -&gt; idna[version=’&gt;=2.1’]<br/><NewLine>cryptography -&gt; idna[version=’&gt;=2.1’]<br/><NewLine>requests -&gt; urllib3[version=’&gt;=1.21.1,&lt;1.26,!=1.25.0,!=1.25.1’] -&gt; idna[version=’&gt;=2.0.0’]<br/><NewLine>pyopenssl -&gt; cryptography[version=’&gt;=2.8’] -&gt; idna[version=’&gt;=2.1’]</p><NewLine><p>Package pysocks conflicts for:<br/><NewLine>urllib3 -&gt; pysocks[version=’&gt;=1.5.6,&lt;2.0,!=1.5.7’]<br/><NewLine>requests -&gt; urllib3[version=’&gt;=1.21.1,&lt;1.26,!=1.25.0,!=1.25.1’] -&gt; pysocks[version=’&gt;=1.5.6,&lt;2.0,!=1.5.7’]<br/><NewLine>pysocks</p><NewLine><p>The command ‘/bin/sh -c wget ht tps://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh &amp;&amp;     chmod +x Miniconda3-latest-Linux-x86_64.sh &amp;&amp;      bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /opt/conda &amp;&amp;      /opt/conda/bin/conda install -y python=$PYTHON_VERSION numpy pyyaml scipy ipython mkl mkl-include ninja cython typing &amp;&amp;      /opt/conda/bin/conda install -y -c pytorch magma-cuda100 &amp;&amp;      /opt/conda/bin/conda clean -ya’ returned a non-zero code: 1</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!</p><NewLine><p>Thanks for posting.</p><NewLine><p>I don’t believe this is actually related to any issues with pytorch.</p><NewLine><p>It would appear as though there are some conflicting packages out of all of the dependency packages you are trying to install.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <a class=""mention"" href=""/u/seemethere"">@seemethere</a> , but isnt this the official pytorch docker build image. can you tell me list of things i could change to make it work. Its also not working on nvidia dgx and my friend had the same troubles</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/VIGNESHinZONE; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/seemethere; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/VIGNESHinZONE; <NewLine> ,"REPLY_DATE 1: April 23, 2020,  7:36am; <NewLine> REPLY_DATE 2: April 23, 2020,  9:58am; <NewLine> REPLY_DATE 3: April 23, 2020,  9:27pm; <NewLine> REPLY_DATE 4: April 23, 2020, 11:59pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
77831,Dynamic graphs on mobile,2020-04-22T09:48:29.144Z,0,64,"<div class=""post"" itemprop=""articleBody""><NewLine><p>is it possible to keep a dynamic computational graph on mobile?<br/><NewLine>i see an opportunity for RNN, if you process arbitrary-length input, you cannot do the whole computation at once on your neural accelerator (think a video stream for example).</p><NewLine><p>When porting to ONNX a RNN, i managed to port it Feed-Forward style meaning the RNN do 1/ N steps with hidden states and give back output and updated hidden states. The bottleneck of this approach is that you need to transfer a lot of data between host &amp; accelerator (for instance when using ConvLSTM for large video)</p><NewLine><p>Is there a way to simply run pytorch code on mobile side without passing by this feed-forward hack?</p><NewLine></div>",https://discuss.pytorch.org/u/Etienne_Perot,(Etienne Perot),Etienne_Perot,"April 22, 2020,  9:48am",1 Like,,,,
76976,Example with gRPC,2020-04-16T03:02:28.168Z,0,202,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m looking for serving pytorch model with grpc with python.<br/><NewLine>But cannot. find any source…</p><NewLine><p>Is there any guide serving pytorch model with grpc?</p><NewLine></div>",https://discuss.pytorch.org/u/juhyung,(손주형),juhyung,"April 16, 2020,  3:02am",,,,,
76220,Predict same image but get different probability,2020-04-10T15:02:59.463Z,1,90,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Today, I find one strange thing that if I put one or many images into the model, but when check the outputs in different layers of the same images, the values of outputs are different.</p><NewLine><ol><NewLine><li>The variable images contains 100 pictures, which have been normalized. Then the resnet18 model made a prediction, and I print the output of last layer. We can see the values of the last picture as follows:<br/><NewLine>tensor([[ 0.3845428824, -0.5180321932, 0.2798461318,-0.5726045370,0.1901987493, -0.2272544652, -0.8677281737, 0.4837094247,-0.0999206007,-0.1482645720]])</li><NewLine><li>I only chose the last picture as input, and we can the the output is different from the values above<br/><NewLine>tensor([[ 0.3845427036, -0.5180319548,  0.2798460722,-0.5726044774,0.1901988089, -0.2272544056, -0.8677282333, 0.4837094545,-0.0999205410,-0.1482645869]])</li><NewLine><li>I duplicate the last picture 100 times as input, and find the result is the same as result of 1).<br/><NewLine>[ 0.3845428824, -0.5180321932, 0.2798461318,-0.5726045370,0.1901987493,-0.2272544652,-0.8677281737, 0.4837094247,-0.0999206007,-0.1482645720]</li><NewLine><li>Put the last three pictures into the net, the output is:<br/><NewLine>[0.3845428526,-0.5180321336,0.2798461318,-0.5726043582,0.1901988238,-0.2272545248,-0.8677282333,0.4837094247,-0.0999206007,-0.1482645124]<br/><NewLine>It seems that the size of input is he key reason of the output difference.  I can’t understand this problem, espicially when I do the same things on tensorflow, there are no difference, in the condition of same batch_size.<br/><NewLine>Why it happens? Please help me to find out which caused this on pytorch, thank you!</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/ljlkxj,,ljlkxj,"April 10, 2020,  3:02pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Make sure to call <code>model.eval()</code> before running your test.<br/><NewLine>This will make sure to disable dropout layers and use the running estimates of batchnorm layers.<br/><NewLine>During training, dropout will of course alter your output and batchnorm layers will normalize the input batch using the current batch statistics and will also update the running estimates.</p><NewLine><p>I’m not sure how TensorFlow handles it.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>As you suggest, I add model.eval() and then run it, the condition changes but still get different probability. But this time, 2 and 3 become same:  [-3.5615596771, -3.3041067123,  0.2262650430, -0.5413500071,<br/><NewLine>5.4653391838,  3.9844801426, -4.7832412720, 14.4310092926,<br/><NewLine>-6.6198024750, -6.1641607285].<br/><NewLine>while 1: [-3.5615587234, -3.3041062355,  0.2262653410, -0.5413498878,<br/><NewLine>5.4653387070,  3.9844801426, -4.7832412720, 14.4310083389,<br/><NewLine>-6.6198034286, -6.1641597748]<br/><NewLine>4 : [-3.5615592003, -3.3041067123,  0.2262655199, -0.5413499475,<br/><NewLine>5.4653387070,  3.9844801426, -4.7832412720, 14.4310073853,<br/><NewLine>-6.6198034286, -6.1641602516]]<br/><NewLine>1 and 2 and 4 are different, which means that the data in same batch influence the output.<br/><NewLine>Are there some mechanism in pytorch which bring the small changes? Thank you again!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It depends on the layers you are using in your model as well as e.g. if you set <code>cudnn</code> to use deterministic algorithms as described <a href=""https://pytorch.org/docs/stable/notes/randomness.html"">here</a>.<br/><NewLine>If the absolute difference is in the range ~1e-6, it’ll be most likely due to the limited floating point precision.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ljlkxj; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: April 11, 2020,  4:59am; <NewLine> REPLY_DATE 2: April 14, 2020,  3:51pm; <NewLine> REPLY_DATE 3: April 15, 2020,  4:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
76507,Type conversion in libtorch,2020-04-13T06:25:28.921Z,0,49,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi，<br/><NewLine>Say we have two tensor priors, and loc_cat,<br/><NewLine>I want to set the type of priors same as loc_cat,<br/><NewLine>in python, we can use the following code:</p><NewLine><p>priors.type(loc_cat.data.type)</p><NewLine><p>So how to do that in cpp libtorch?<br/><NewLine>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Edwardmark,(Edwardmark),Edwardmark,"April 13, 2020,  6:25am",,,,,
76056,Deserialization error,2020-04-09T14:52:38.637Z,0,83,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I saved the object of my trainer class using the torch save method. However, when loading the saved file, the depickle process can’t find “__iterator”, which is a private method inside my “Trainer” class. Because of that, the whole loading process fails. Any idea how to solve or bypass it? I just need to recover the data saved into the attributes of that class.</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File "".conda/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3331, in run_code<NewLine>    exec(code_obj, self.user_global_ns, self.user_ns)<NewLine>  File ""&lt;ipython-input-1-a5666d77c70f&gt;"", line 1, in &lt;module&gt;<NewLine>    torch.load(""snapshots/model.pth"", map_location='cpu')<NewLine>  File "".conda/envs/py37/lib/python3.7/site-packages/torch/serialization.py"", line 529, in load<NewLine>    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)<NewLine>  File "".conda/envs/py37/lib/python3.7/site-packages/torch/serialization.py"", line 702, in _legacy_load<NewLine>    result = unpickler.load()<NewLine>AttributeError: 'Trainer' object has no attribute '__iterator'<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/gsouza7,,gsouza7,"April 9, 2020,  2:54pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you store the <code>Trainer</code> instance directly? Also, could you post its definition?</p><NewLine><p>As explained in the <a href=""https://pytorch.org/docs/stable/notes/serialization.html"">serialization docs</a> it’s recommended to store the <code>state_dict</code>s instead of the objects, as changes in the source files might yield to such errors and you would also have to make sure to recreate the same folder structure.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Unfortunately, I saved the entire object, and that is what caused the whole problem. Here there is a piece of code that will generate the problem I’m facing right now.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine>class Test:<NewLine>	def __init__(self):<NewLine>		self.a = min<NewLine>		self.b = max<NewLine>		self.c = self.__private  # buggy<NewLine><NewLine>	def __private(self):<NewLine>		return None<NewLine><NewLine>test = Test()<NewLine><NewLine>torch.save({""test"": test}, ""file.pkl"")<NewLine>torch.load(""file.pkl"")<NewLine></code></pre><NewLine><p>However, if you remove the private attribute from the method, you won’t get any error.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine>class Test:<NewLine>	def __init__(self):<NewLine>		self.a = min<NewLine>		self.b = max<NewLine>		self.c = self.private  # not buggy<NewLine><NewLine>	def private(self):<NewLine>		return None<NewLine><NewLine>test = Test()<NewLine><NewLine>torch.save({""test"": test}, ""file.pkl"")<NewLine>torch.load(""file.pkl"")<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/gsouza7; <NewLine> ,"REPLY_DATE 1: April 10, 2020,  3:53am; <NewLine> REPLY_DATE 2: April 11, 2020,  1:07pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
75302,How to load using torch.load without source class (using which model was created)?,2020-04-04T04:45:53.268Z,1,238,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there, in first file I’m defining the model class as “Classifier” and training the model and then saving it using <code>torch.save(model, 'model.pt', dill)</code>. Code in first script looks like-</p><NewLine><pre><code class=""lang-python"">class Classifier(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Classifier, self).__init__()<NewLine>        # a toy example model<NewLine>        self.model = nn.Sequential(<NewLine>            nn.Linear(10, 5),<NewLine>            nn.LeakyReLU(0.2),<NewLine>            nn.Linear(5, 10))<NewLine>    <NewLine>    def forward(self, x):<NewLine>        return self.model(x)<NewLine><NewLine>    def a_use_full_function(self):<NewLine>        """"""<NewLine>        A function I'm using while training the model,<NewLine>        so I wanna save and load this as well so that I can use it<NewLine>        for continuing training my model in the second script<NewLine>        """"""<NewLine><NewLine>model = Classifier()<NewLine>model.cuda()<NewLine># training the model....<NewLine># evaluating and testing it....<NewLine># and saving as-<NewLine><NewLine>import dill<NewLine>torch.save(model, 'model.pt', dill)<NewLine></code></pre><NewLine><p>In the second script I’m trying to loading the model without any <code>Classifier</code> definition, so code in second script looks exactly like-</p><NewLine><pre><code class=""lang-python"">from torch import nn<NewLine>import dill<NewLine>model = torch.load('model.pt', 'cuda', dill)<NewLine></code></pre><NewLine><p>Running it is showing error as “<code>AttributeError: Can't get attribute 'Classifier' on </code>”.<br/><NewLine>I understood this error so when I copy and paste the <code>Classifier</code> class in this second script, then it works, but of course, this is not loading model without model definition.</p><NewLine><p>So my question is how can I save and load my model so that I don’t have to deal with the original source code of the model definition.</p><NewLine><p><strong>One note</strong>- when I tried saving and loading the model build just using <code>torch.nn.Sequential</code> then it got loaded in the second scripts without any error.</p><NewLine></div>",https://discuss.pytorch.org/u/braindotai,,braindotai,"April 4, 2020,  5:00am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could save the jitted model, which can then be loaded in other applications.<br/><NewLine>As far as I know, there is no way of storing the model directly without recreating the file structure.<br/><NewLine>Generally this workflow is also not recommended and you should store and load the <code>state_dict</code> instead.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, yeah as you said I now am able to save the model-<br/><NewLine><code>torch.jit.save(torch.jit.trace(model, (x)), ""model.pth"")</code><br/><NewLine>and load it like-<br/><NewLine><code>loaded_model = torch.jit.load(""model.pth"")</code>.</p><NewLine><p>Though one trick I came up so that while loading I don’t have to deal with <code>Classifier</code> definition, by defining a <code>load_model</code> function inside the <code>Classifier</code> class, then have a three scripts structure like-</p><NewLine><ul><NewLine><li>model.py (defining the <code>Classifier</code> only)</li><NewLine><li>train.py (building and training model by importing <code>Classifier</code> from model.py)</li><NewLine><li>load.py, and here loading model by importing <code>Classifier</code> from model.py and defining model like-<pre><code class=""lang-python"">from model import Classifier<NewLine>loaded_model = Classifier.load_model('model.pth')<NewLine></code></pre><NewLine></li><NewLine></ul><NewLine><p>No bad actually <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/braindotai; <NewLine> ,"REPLY_DATE 1: April 4, 2020,  6:19am; <NewLine> REPLY_DATE 2: April 4, 2020,  6:29am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
75128,Export to onnx has warning messages,2020-04-02T16:38:42.155Z,0,97,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Trying to export the encoder model based on <a href=""https://github.com/nianticlabs/monodepth2"" rel=""nofollow noopener"">this project</a>, since the onnx export function do works well with list output, I change the forward function of the <a href=""https://github.com/nianticlabs/monodepth2/blob/master/networks/resnet_encoder.py"" rel=""nofollow noopener"">encoder</a> from</p><NewLine><pre><code class=""lang-auto"">def forward(self, input_image):<NewLine>        self.features = []<NewLine>        x = (input_image - 0.45) / 0.225<NewLine>        x = self.encoder.conv1(x)<NewLine>        x = self.encoder.bn1(x)<NewLine>        self.features.append(self.encoder.relu(x))<NewLine>        self.features.append(self.encoder.layer1(self.encoder.maxpool(self.features[-1])))<NewLine>        self.features.append(self.encoder.layer2(self.features[-1]))<NewLine>        self.features.append(self.encoder.layer3(self.features[-1]))<NewLine>        self.features.append(self.encoder.layer4(self.features[-1]))<NewLine><NewLine>        return self.features<NewLine></code></pre><NewLine><p>To</p><NewLine><pre><code class=""lang-auto"">def forward(self, input_image):        <NewLine>        x = (input_image - 0.45) / 0.225<NewLine>        x = self.encoder.conv1(x)<NewLine>        x = self.encoder.bn1(x)		<NewLine>        output1 = self.encoder.relu(x)<NewLine>        output2 = self.encoder.layer1(self.encoder.maxpool(output1))<NewLine>        output3 = self.encoder.layer2(output2)<NewLine>        output4 = self.encoder.layer3(output3)<NewLine>        output5 = self.encoder.layer4(output4)<NewLine>		<NewLine>        output_tensor = torch.ones(2887680)<NewLine>        #guess I do not need to call clone?<NewLine>        output_tensor[0:1966080] = output1.flatten()<NewLine>        output_tensor[1966080:2457600] = output2.flatten()<NewLine>        output_tensor[2457600:2703360] = output3.flatten()<NewLine>        output_tensor[2703360:2826240] = output4.flatten()<NewLine>        output_tensor[2826240:] = output5.flatten()<NewLine><NewLine>        return output_tensor<NewLine></code></pre><NewLine><p>It show me warning</p><NewLine><p><strong>TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.</strong></p><NewLine><p>Is this safe?</p><NewLine></div>",https://discuss.pytorch.org/u/ngap_wei_Tham,(Ngap Wei Tham),ngap_wei_Tham,"April 2, 2020,  4:38pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Verify with the output values, this don’t work. What is the proper way to copy the data of different layers into a big tensor if you want to export it to onnx model?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found the solution, use cat.</p><NewLine><p><code>return torch.cat((output1.flatten(), output2.flatten(), output3.flatten(), output4.flatten(), output5.flatten()))</code></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ngap_wei_Tham; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ngap_wei_Tham; <NewLine> ,"REPLY_DATE 1: April 3, 2020,  5:45am; <NewLine> REPLY_DATE 2: April 3, 2020,  6:36am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
49227,DataLoader batch first,2019-06-28T14:00:16.813Z,0,1068,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey everyone,</p><NewLine><p>I am gonna use torch.DataLoader for RNNs but it forms batches as batch first: <em>[batch x seq x data]</em>. However I would like to have the data in default pytorch RNN format: <em>[seq x batch x data].</em><br/><NewLine>I could not see any easy way to do like <em>batch_first=False</em></p><NewLine><p>Could you please help me with?</p><NewLine></div>",https://discuss.pytorch.org/u/denis,(Denis),denis,"June 28, 2019,  2:00pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The dataloader is just a utility used to work with the dataset class (passed as the first argument to the dataloader) Check out the dataset class that you pass to the dataloader. That would be where you’d have coded in the dimensions of the data returned by dataset class and thus your dataloader.</p><NewLine><p>If for whatever reason this dataset class isn’t accessible to you; check out the collate function : <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/how-to-create-a-dataloader-with-variable-size-input/8278/3"">How to create a dataloader with variable-size input</a> and <code>input.permute(1, 0, 2)</code> your batches there.</p><NewLine><p>Check out this useful answers on using RNNs with PyTorch <a href=""https://stackoverflow.com/questions/49466894/how-to-correctly-give-inputs-to-embedding-lstm-and-linear-layers-in-pytorch/49473068#49473068"" rel=""nofollow noopener"">https://stackoverflow.com/questions/49466894/how-to-correctly-give-inputs-to-embedding-lstm-and-linear-layers-in-pytorch/49473068#49473068</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I ran into a similar issue and made a mistake of using <code>torch.reshape(x,(seq,batch,data))</code>. But this resulted in messing the order of my time series data. The way I fixed was using <code>torch.permute(x,(1,0,2))</code></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/shubhvachher; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vpn; <NewLine> ,"REPLY_DATE 1: June 28, 2019,  3:01pm; <NewLine> REPLY_DATE 2: April 1, 2020,  9:31am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
74551,GPU forward pass is ~40 times slower than CPU,2020-03-27T17:06:42.010Z,1,86,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a <code>ModuleDict</code> model like the one shown below:</p><NewLine><pre><code class=""lang-auto"">NeuralNetwork(<NewLine>  (linears): ModuleDict(<NewLine>    (C): Sequential(<NewLine>      (0): Linear(in_features=384, out_features=192, bias=True)<NewLine>      (1): ReLU()<NewLine>      (2): Linear(in_features=192, out_features=192, bias=True)<NewLine>      (3): ReLU()<NewLine>      (4): Linear(in_features=192, out_features=96, bias=True)<NewLine>      (5): ReLU()<NewLine>      (6): Linear(in_features=96, out_features=48, bias=True)<NewLine>      (7): ReLU()<NewLine>      (8): Linear(in_features=48, out_features=1, bias=True)<NewLine>    )<NewLine>    (H): Sequential(<NewLine>      (0): Linear(in_features=384, out_features=192, bias=True)<NewLine>      (1): ReLU()<NewLine>      (2): Linear(in_features=192, out_features=192, bias=True)<NewLine>      (3): ReLU()<NewLine>      (4): Linear(in_features=192, out_features=96, bias=True)<NewLine>      (5): ReLU()<NewLine>      (6): Linear(in_features=96, out_features=48, bias=True)<NewLine>      (7): ReLU()<NewLine>      (8): Linear(in_features=48, out_features=1, bias=True)<NewLine>    )<NewLine>    (N): Sequential(<NewLine>      (0): Linear(in_features=384, out_features=192, bias=True)<NewLine>      (1): ReLU()<NewLine>      (2): Linear(in_features=192, out_features=192, bias=True)<NewLine>      (3): ReLU()<NewLine>      (4): Linear(in_features=192, out_features=96, bias=True)<NewLine>      (5): ReLU()<NewLine>      (6): Linear(in_features=96, out_features=48, bias=True)<NewLine>      (7): ReLU()<NewLine>      (8): Linear(in_features=48, out_features=1, bias=True)<NewLine>    )<NewLine>    (O): Sequential(<NewLine>      (0): Linear(in_features=384, out_features=192, bias=True)<NewLine>      (1): ReLU()<NewLine>      (2): Linear(in_features=192, out_features=192, bias=True)<NewLine>      (3): ReLU()<NewLine>      (4): Linear(in_features=192, out_features=96, bias=True)<NewLine>      (5): ReLU()<NewLine>      (6): Linear(in_features=96, out_features=48, bias=True)<NewLine>      (7): ReLU()<NewLine>      (8): Linear(in_features=48, out_features=1, bias=True)<NewLine>    )<NewLine>    (P): Sequential(<NewLine>      (0): Linear(in_features=384, out_features=192, bias=True)<NewLine>      (1): ReLU()<NewLine>      (2): Linear(in_features=192, out_features=192, bias=True)<NewLine>      (3): ReLU()<NewLine>      (4): Linear(in_features=192, out_features=96, bias=True)<NewLine>      (5): ReLU()<NewLine>      (6): Linear(in_features=96, out_features=48, bias=True)<NewLine>      (7): ReLU()<NewLine>      (8): Linear(in_features=48, out_features=1, bias=True)<NewLine>    )<NewLine>    (S): Sequential(<NewLine>      (0): Linear(in_features=384, out_features=192, bias=True)<NewLine>      (1): ReLU()<NewLine>      (2): Linear(in_features=192, out_features=192, bias=True)<NewLine>      (3): ReLU()<NewLine>      (4): Linear(in_features=192, out_features=96, bias=True)<NewLine>      (5): ReLU()<NewLine>      (6): Linear(in_features=96, out_features=48, bias=True)<NewLine>      (7): ReLU()<NewLine>      (8): Linear(in_features=48, out_features=1, bias=True)<NewLine>    )<NewLine>  )<NewLine>)<NewLine></code></pre><NewLine><p>The forward that use to train it looks like this:</p><NewLine><pre><code class=""lang-auto"">    def forward(self, X, device=None):<NewLine>        """"""Forward propagation<NewLine><NewLine>        This is forward propagation and it returns the atomic energy.<NewLine><NewLine>        Parameters<NewLine>        ----------<NewLine>        X : dict<NewLine>            Dictionary of inputs in the feature space.<NewLine><NewLine>        Returns<NewLine>        -------<NewLine>        outputs : tensor<NewLine>            A list of tensors with energies per image.<NewLine>        """"""<NewLine><NewLine>        outputs = []<NewLine><NewLine>        for hash in X:<NewLine>            image = X[hash]<NewLine>            atomic_energies = []<NewLine><NewLine>            for symbol, x in image:<NewLine>                if isinstance(symbol, bytes):<NewLine>                    symbol = symbol.decode(""utf-8"")<NewLine>                try:<NewLine>                    x = self.linears[symbol](x)<NewLine>                except RuntimeError:<NewLine>                    x = self.linears[symbol](x.to(device))<NewLine><NewLine>                intercept_name = ""intercept_"" + symbol<NewLine>                slope_name = ""slope_"" + symbol<NewLine>                slope = getattr(self, slope_name)<NewLine>                intercept = getattr(self, intercept_name)<NewLine><NewLine>                x = (slope * x) + intercept<NewLine>                atomic_energies.append(x)<NewLine><NewLine>            atomic_energies = torch.cat(atomic_energies)<NewLine>            image_energy = torch.sum(atomic_energies)<NewLine>            outputs.append(image_energy)<NewLine>        outputs = torch.stack(outputs)<NewLine>        return outputs<NewLine></code></pre><NewLine><p>Running that <code>forward()</code>  in a single  CPU takes 1.74 seconds and in a single GPU 38.33 seconds. Why is this difference? I have some hypothesis:</p><NewLine><ol><NewLine><li>The structure of <code>X</code> should be changed to improve efficiency and avoid the <code>for</code> loop.</li><NewLine><li>Movement of tensors to CUDA device should be avoided inside the <code>forward()</code> function and instead done only once before calling forward.</li><NewLine></ol><NewLine><p>I would appreciate any advice. Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/muammar,(Muammar El Khatib),muammar,"March 27, 2020,  5:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You two hypothesis are very good candidates. Keep in mind that GPU are really good at doing a single large op but very bad at doing many small ones.  So removing the for-loop will definitely help.<br/><NewLine>Moving Tensors between cpu and gpu is quite expensive as well and should be avoided yes.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply. This will represent my first GPU experiment <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/> I will try to keep this thread updated with my findings so that people also getting started could find something helpful.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>First step forward. It turns out that doing <code>.to(device)</code> to all data first to GPU instead of inside the <code>forward</code> pass reduces GPU time to 2.74 from  38.33. Still, the CPU is slightly faster. Next try is vectorization to avoid the <code>for</code> loop and I will report back.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/muammar; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/muammar; <NewLine> ,"REPLY_DATE 1: March 27, 2020,  5:17pm; <NewLine> REPLY_DATE 2: March 27, 2020,  5:29pm; <NewLine> REPLY_DATE 3: March 27, 2020,  8:04pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
74442,Conda packaging with CUDA enabled / CPU only,2020-03-26T11:58:50.608Z,0,76,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there,</p><NewLine><p>I am working on a project that heavily relies on PyTorch which I now want to deploy as a conda package.<br/><NewLine>I have some custom C++ and CUDA (equivalent but faster) code which I also want to include in my conda build.</p><NewLine><p>At the moment everything works so far, but I haven’t been able to implement the cuda / cpuonly logic which pytorch uses. So my idea would be to just reuse the pytorch logic</p><NewLine><pre><code class=""lang-auto"">conda install [myownbuild] cudatoolkit=10.1 -c [mychannel]<NewLine>conda install [myownbuild] cpuonly -c [mychannel]<NewLine></code></pre><NewLine><p>such that when pytorch is installed with the respective cudatoolkit I would want to use the cuda version of my own build and when the cpu only flag is used, then I would want to use the CPU only version of my build as well.</p><NewLine><p>But how can I handle this in the conda meta.yaml?</p><NewLine><p>Currently, I have a simple Linux switch that looks like</p><NewLine><pre><code class=""lang-auto"">...<NewLine>  host:<NewLine>    - python {{ python }}<NewLine>    - numpy {{ numpy }}<NewLine>    - pybind11 &gt;=2.4<NewLine>    - cudatoolkit-dev # [linux]<NewLine>...<NewLine></code></pre><NewLine><p>and kind of assumes that on the linux machine there is a cuda card. Is it possible to use the cpuonly / cudatoolkit flag somewhere there? Or could some tell me how this is done in pytorch?</p><NewLine><p>Cheers,<br/><NewLine>Lucas</p><NewLine></div>",https://discuss.pytorch.org/u/Haydnspass,(Lucas Müller),Haydnspass,"March 26, 2020, 11:58am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Check out <a href=""https://github.com/pytorch/builder"">this repository</a>, which is also used to build the binaries.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: March 27, 2020,  5:34am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
73978,How to estimate necessary (GPU) memory for a model,2020-03-21T20:06:16.875Z,1,247,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a model that throws <code>not enough memory on GPU</code> error upon training (coming from <code>.forward()</code>).</p><NewLine><pre><code class=""lang-auto"">File ""/home/.../.../run/train.py"", line 131, in main<NewLine>  output = model(data)<NewLine>File ""/home/.../anaconda3/envs/.../lib/python3.7/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>  result = self.forward(*input, **kwargs)<NewLine>File ""/home/.../.../model/resnet.py"", line 47, in forward<NewLine>  x = self.layers[f""conv_{i}""](x)<NewLine>File ""/home/.../anaconda3/envs/.../lib/python3.7/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>  result = self.forward(*input, **kwargs)<NewLine>File ""/home/.../anaconda3/envs/.../lib/python3.7/site-packages/torch/nn/modules/conv.py"", line 345, in forward<NewLine>  return self.conv2d_forward(input, self.weight)<NewLine>File ""/home/.../anaconda3/envs/.../lib/python3.7/site-packages/torch/nn/modules/conv.py"", line 342, in conv2d_forward<NewLine>  self.padding, self.dilation, self.groups)<NewLine>RuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 7.93 GiB total capacity; 7.35 GiB already allocated; 12.75 MiB free; 87.83 MiB cached)<NewLine></code></pre><NewLine><p>When I checked the model size based on parameters it definitely fits into the memory and each batch size is also quite small that these cannot be the source of the exception.</p><NewLine><p>(I was told from my friend that it might be coming from the activation tensors that are stored for the backward pass.)</p><NewLine><p>In this post, I would like to ask the following questions.</p><NewLine><ol><NewLine><li>What does PyTorch allocate memory for other than model and data (especially during the training process)? I would like to know the exact cause of the exception.</li><NewLine><li>Is there any way of estimating how much memory the model requires “prior to training” and “programmatically”?</li><NewLine></ol><NewLine><p>When I use term <em>memory</em>, it can simply be the number of float (tensor) because I can always estimate one metric from the other</p><NewLine><p>Thank you for your time</p><NewLine></div>",https://discuss.pytorch.org/u/ljj7975,(Brandon Lee),ljj7975,"March 21, 2020,  8:10pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am not sure about the inbuilt memory usage by pytorch library. There is one library called <a href=""https://pypi.org/project/pytorch-memlab/"" rel=""nofollow noopener"">pytorch_memlab</a>, which can be used to inspect the GPU memory usage by each line of your code.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It looks like I have to call backward() to check the usage though…</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/divyesh_rajpura; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ljj7975; <NewLine> ,"REPLY_DATE 1: March 21, 2020,  8:39pm; <NewLine> REPLY_DATE 2: March 27, 2020,  2:48am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
74294,What is the best way to deploy object detection and segmentation model to the cloud,2020-03-25T00:25:56.618Z,0,76,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi guys,<br/><NewLine>I have a model written in Pytorch, and I wonder what is the best way to deploy it on the cloud with simple GUI allows user to use it simply. If you have some resources, please send it my way.<br/><NewLine>Please help,<br/><NewLine>Thank you!<br/><NewLine>Bill</p><NewLine></div>",https://discuss.pytorch.org/u/Bill_CX,(Bill CX),Bill_CX,"March 25, 2020, 12:25am",,,,,
74155,Speed up dataloader when complex operations are needed in the __getitem__ function,2020-03-23T17:40:13.103Z,0,73,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to write a customized data loader to process some graph-structured data, in the <strong>getitem</strong> function, I am trying to permute the edges in a graph indexed by the “index” parameter, and then sample different groups of the edges from it, (actually in the flavor of meta-learning), however, the complex operations in it makes the data loader works very slowly, (actually during training, the volatile GPU-util is always 0). Here is the sample code for the customized data loader.</p><NewLine><pre><code class=""lang-auto"">class DataGenerator(Dataset):<NewLine>    """"""docstring for DataGenerator""""""<NewLine><NewLine>    def __init__(self, args, graph_group, status):<NewLine>        super(DataGenerator, self).__init__()<NewLine>        self.args = args<NewLine>        self.graphs = graph_group<NewLine>        self.graph_nx = []<NewLine>        self.row = []<NewLine>        self.col = []<NewLine>        # self.g = torch.Generator()<NewLine>        # self.g.manual_seed(args.seed)<NewLine>        # random.seed(args.seed)<NewLine>        self.status = status<NewLine>        self.rng_permutation_train = np.random.RandomState(args.seed)<NewLine>        self.rng_choose_edge_train = np.random.RandomState(args.seed)<NewLine>        self.rng_permutation_test = np.random.RandomState(args.seed)<NewLine>        self.rng_choose_edge_test = np.random.RandomState(args.seed)<NewLine>        if args.use_cross_graph_meta:<NewLine>            self.rng_choose_graph_train_cross = np.random.RandomState(<NewLine>                args.seed)<NewLine>            self.rng_permutation_train_cross = np.random.RandomState(args.seed)<NewLine>            self.rng_choose_edge_train_cross = np.random.RandomState(args.seed)<NewLine>        if status == 'train':<NewLine>            self.graphs_in_use = self.graphs[:-args.test_graph_number]<NewLine>        elif status == 'test':<NewLine>            self.graphs_in_use = self.graphs[-args.test_graph_number:]<NewLine>        for graph in self.graphs_in_use:<NewLine>            graph_temp = nx.read_gpickle(graph)<NewLine>            self.graph_nx.append(graph_temp)<NewLine>            self.row.append(torch.IntTensor(<NewLine>                [e[0] for e in graph_temp.edges()]))<NewLine>            self.col.append(torch.IntTensor(<NewLine>                [e[1] for e in graph_temp.edges()]))<NewLine><NewLine>    def __getitem__(self, index):<NewLine>      <NewLine>            pos_train_edges, neg_train_edges, pos_test_edges, neg_test_edges\<NewLine>                = self.sampling_function_for_task(<NewLine>                    index, self.args,<NewLine>                    self.graph_nx[index].number_of_nodes())<NewLine>            return pos_train_edges, neg_train_edges,\<NewLine>                pos_test_edges, neg_test_edges<NewLine><NewLine>    def __len__(self):<NewLine>        return len(self.graphs_in_use)<NewLine><NewLine>    def sampling_function_for_task(self, index, args, num_nodes):<NewLine>        row = self.row[index] <NewLine>        col = self.col[index]  <NewLine><NewLine>        n_v = int(math.floor(args.sample_ratio * row.size(0)))<NewLine>        n_t = int(math.floor(args.query_ratio * row.size(0)))<NewLine><NewLine>        # Positive edges.<NewLine>        perm = self.rng_permutation_train.permutation(row.size(0))<NewLine>        row, col = row[perm], col[perm]<NewLine><NewLine>        r, c = row[:n_v], col[:n_v]<NewLine>        sample_pos_edge_index = torch.stack([r, c], dim=0)<NewLine>        r, c = row[n_v:n_v + n_t], col[n_v:n_v + n_t]<NewLine>        query_pos_edge_index = torch.stack([r, c], dim=0)<NewLine><NewLine>        # Negative edges.<NewLine>        neg_adj_mask = torch.ones(num_nodes, num_nodes, dtype=torch.uint8)<NewLine>        neg_adj_mask = neg_adj_mask.triu(diagonal=1)<NewLine>        row = row.to(torch.long)<NewLine>        col = row.to(torch.long)<NewLine>        neg_adj_mask[row, col] = 0<NewLine><NewLine>        neg_row, neg_col = neg_adj_mask.nonzero().t()<NewLine>        # note random.sample doesn't allow replacement.<NewLine>        perm = torch.tensor(self.rng_choose_edge_train.choice(<NewLine>            range(neg_row.size(0)), int(args.neg_pos_ratio) * (n_t + n_v),<NewLine>            replace=False))<NewLine>        perm = perm.to(torch.long)<NewLine>        neg_row, neg_col = neg_row[perm], neg_col[perm]<NewLine><NewLine>        neg_adj_mask[neg_row, neg_col] = 0<NewLine>        # train_neg_adj_mask = neg_adj_mask<NewLine><NewLine>        row, col = neg_row[:int(args.neg_pos_ratio) *<NewLine>                           n_v], neg_col[:int(args.neg_pos_ratio) * n_v]<NewLine>        sample_neg_edge_index = torch.stack([row, col], dim=0)<NewLine><NewLine>        row, col = neg_row[<NewLine>            int(args.neg_pos_ratio) * n_v:<NewLine>            int(args.neg_pos_ratio) * (n_v + n_t)], \<NewLine>            neg_col[int(args.neg_pos_ratio) *<NewLine>                    n_v:int(args.neg_pos_ratio) * (n_v + n_t)]<NewLine>        query_neg_edge_index = torch.stack([row, col], dim=0)<NewLine><NewLine>        return sample_pos_edge_index, sample_neg_edge_index, \<NewLine>            query_pos_edge_index, query_neg_edge_index<NewLine></code></pre><NewLine><p>I have tried some speeding techniques like adding prefetcher but it still works slowly. Anybody have suggestions on how to refactor the code block to make it more efficient?</p><NewLine><p>For easier reading, here the ''graph_group"" is the address list of the input graphs, and the graph is composed of edges, so the <code>graph_temp.edges</code> is like [(1,12),…(12345,34567)], which means node 1 and 12 are connected. They form an edge.</p><NewLine></div>",https://discuss.pytorch.org/u/Xuefeng_Du,(Xuefeng Du),Xuefeng_Du,"March 23, 2020,  5:51pm",,,,,
73518,Importing TensorFlow model for Inference in Torchscript,2020-03-17T13:10:30.742Z,0,137,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I would like to use a trained model I have in TensorFlow for inference in TorchScript.  I would like to compare the performance between the two.</p><NewLine><p>What is the best method to do this?</p><NewLine><p>I believe it would be the following:<br/><NewLine>Convert TensorFlow model to frozen graph (.pb) --&gt; Convert to ONNX (.onnx) --&gt; Convert to PyTorch model (.pt)</p><NewLine><p>Is this correct?  Has anyone tried this approach before?  Is there another approach that is recommended?</p><NewLine></div>",https://discuss.pytorch.org/u/solarflarefx,,solarflarefx,"March 17, 2020,  1:10pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Or is there a way to create a TorchScript from an onnx model?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/solarflarefx; <NewLine> ,"REPLY_DATE 1: March 17, 2020,  8:59pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
72594,"How to optimize variable input, variable output network?",2020-03-09T14:08:51.134Z,0,90,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Current optimization techniques, that I’m aware of, are not applicable to the wide range of networks that are built for image-to-sequence or sequence-to-sequence problems where both image dimensions and output sequences can vary. Some applications that can utilize such models are:</p><NewLine><ol><NewLine><li>image captioning</li><NewLine><li>speech-to-text and vice versa</li><NewLine><li>OCR</li><NewLine><li>image-to-speech (e.g. describing photos to blind or almost blind people)</li><NewLine></ol><NewLine><p>Such models can for example be built from fully convolutional backend (spatial or temporal - it doesn’t really matter), after which some attention and RNN decoder are applied. Additional complexity with variable sized input comes from the fact that real user data cannot be efficiently batched all the time due to large difference in input shapes (huge padding introduces too much wasted computation).</p><NewLine><p>As specified here: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936"">What does torch.backends.cudnn.benchmark do?</a>, cudnn benchmark which does wonders for some class of models doesn’t work. Switching to float16 inference might give minimal boost on Volta+ architectures, but TensorCores require some strict shape restrictions to be properly utilized and that might be problematic with variable batch dimension. Exporting and converting to TensorRT network is again problematic for variable in/out shape networks.</p><NewLine><p>Am I missing something or there are no good general strategies for optimizing these and alike models?</p><NewLine></div>",https://discuss.pytorch.org/u/Ognjen_Kocic1,(Ognjen Kocic),Ognjen_Kocic1,"March 9, 2020,  2:08pm",,,,,
72204,Optimizing simultaneous inference for two distinct models,2020-03-05T23:23:37.526Z,0,78,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m working with two independent autoregressive models for inference. One takes queries (sequential data) and yields an intermediate sequential output which is piped to the second model to produce the final output (which is sequential data as well).</p><NewLine><p>Both these models are rather heavy, and inference takes from 1 to 10 seconds for each, depending on the sequence length. In order to optimize query scheduling and inference time, I figured that for both models I could operate in a step by step fashion, i.e. by using RNN Cells instead of RNN Layers and by repeatedly calling a step() function for inference. When batching queries, this allows to start a new query as soon as one is finished, rather than having to wait for the entire batch to complete.</p><NewLine><p>Inference for a single query then works as follows:</p><NewLine><pre><code class=""lang-auto"">query: [a_1, a_2, ..., a_N]<NewLine>pipeline: a_i -&gt; [model 1] -&gt; b_i -&gt; [model 2] -&gt; c_i<NewLine>output: [c_1, c_2, ..., c_N]<NewLine></code></pre><NewLine><p>So I’m in a situation where I need to alternate between two models back and forth. From an engineering perspective, it is optimal to run the first pipeline <code>a_i -&gt; b_i</code> and the second pipeline <code>b_i -&gt; c_i</code> in separate processes, with a shared buffer for <code>b_i</code>. This is trivial when using 2 GPUs (although I haven’t implemented it yet), and assuming the operation time to complete pipineline 1 alone is N, and M for pipeline 2, you get a running time of max(N, M).</p><NewLine><p>But what if I am using a single GPU? Operating on the same GPU in a blocking fashion would amount, in the simplest case, to doing something like this:</p><NewLine><pre><code class=""lang-auto"">b1 = model1.step(a1)<NewLine>c1 = model2.step(b1)<NewLine>b2 = model1.step(a2)<NewLine>c2 = model2.step(b2)<NewLine>b3 = model1.step(a3)<NewLine>...<NewLine></code></pre><NewLine><p>But then you do not leverage the independence of both models, and you end up with a N + M runtime. Indeed, lines 2 and 3, as well as line 4 and 5 could be executed in parallel on 2 GPUs. Is there any way to achieve this on a single GPU? I’m aware that cuda operations are async, but does that mean that different operations can run in parallel on a GPU? If yes, is this trivial to implement (i.e. torch manages the parallelism for independent tasks automatically) or do I need to write async/threaded code?</p><NewLine></div>",https://discuss.pytorch.org/u/Valiox,,Valiox,"March 5, 2020, 11:23pm",,,,,
72179,Speeding up inference on CPU with a complex and dense model,2020-03-05T18:48:19.413Z,0,99,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The model I am using is extremely complex and does not just pass data between different pytorch models. In between various math operators are performed and reshapes of the data. When I applied the pytorch pruning (1.4) I was able to maintain acceptable accuracy at like 60% sparsity - unfortunately model pruning isn’t much farther than the research phase. I tried quantization but the support wasn’t good enough and would require me to quant and dequant very frequently.</p><NewLine><p>Are there any other techniques for speeding up my model inference time on CPU that I could try? Or is it better to just re-architect the entire thing to flow the data better with quantization in mind?</p><NewLine></div>",https://discuss.pytorch.org/u/Duck,,Duck,"March 5, 2020,  6:48pm",,,,,
71176,RuntimeError: expected device cuda:0 but got device cpu,2020-02-26T17:33:52.895Z,1,2378,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>While developing my GNNs, I finally faced a problem that I can’t understand without help.<br/><NewLine>The problem is</p><NewLine><p>/opt/conda/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py in l1_loss(input, target, size_average, reduce, reduction)<br/><NewLine>2189     else:<br/><NewLine>2190         expanded_input, expanded_target = torch.broadcast_tensors(input, target)<br/><NewLine>-&gt; 2191         ret = torch._C._nn.l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))<br/><NewLine>2192     return ret<br/><NewLine>2193</p><NewLine><p>RuntimeError: expected device cuda:1 but got device cpu.</p><NewLine><p>I checked that cuda is available and there are three GTX Titan V on a remote server. ( I’m working on this server)<br/><NewLine>I’m using pytorch 1.4.0, CUDA toolkit 10.1 cuDNN 7603.</p><NewLine><p>There aren’t any errors before that line.<br/><NewLine>Thank you for your help in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/mia-minyoung,(Mia Minyoung),mia-minyoung,"February 26, 2020,  5:33pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I guess you have to pass tensor to CUDA using <code>input = input.cuda()</code> and <code>target=target.cuda()</code> in your training script</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>You are right! Thank you so much!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/simaiden; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mia-minyoung; <NewLine> ,"REPLY_DATE 1: February 27, 2020,  9:15am; <NewLine> REPLY_DATE 2: February 27, 2020,  9:15am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
70998,How to serialize torch::jit::IValue or torch::Tensor to protobuffer?,2020-02-25T13:05:55.219Z,0,286,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to develop a torch-serving backend with libtorch.<br/><NewLine>as the libtorch c++ frontend example shows:</p><NewLine><pre><code class=""lang-auto"">torch::jit::script::Module module;<NewLine>  try {<NewLine>    // Deserialize the ScriptModule from a file using torch::jit::load().<NewLine>    module = torch::jit::load(argv[1]);<NewLine>  }<NewLine>  catch (const c10::Error&amp; e) {<NewLine>    std::cerr &lt;&lt; ""error loading the model\n"";<NewLine>    return -1;<NewLine>  }<NewLine>  std::cout &lt;&lt; ""ok\n"";<NewLine><NewLine>  std::vector&lt;torch::jit::IValue&gt; inputs;<NewLine>  inputs.push_back(torch::ones({1, 3, 224, 224}));<NewLine>  // Execute the model and turn its output into a tensor.<NewLine>  at::Tensor output = module.forward(inputs).toTensor();<NewLine>  std::cout &lt;&lt; output.slice(/*dim=*/1, /*start=*/0, /*end=*/5) &lt;&lt; '\n';<NewLine></code></pre><NewLine><p>now I can load traced model with libtorch , and call module.forward to do predict.<br/><NewLine>however the predict requests with inputs are coming from network by another client like python/java/c++ &amp;.<br/><NewLine>so I need serialize the request from the client, and deserialize the request at the server backend, now I choose protobuffer to do the thing.<br/><NewLine>but I’m confused about which protodef file I can use to define the   torch::jit::IValue or torch::Tensor ??</p><NewLine><p>Is there anyone can help me ???</p><NewLine></div>",https://discuss.pytorch.org/u/sigmoid,,sigmoid,"February 26, 2020,  3:17am",,,,,
70964,Using Pytorch model for processing a dataframe,2020-02-25T08:57:40.204Z,0,73,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to use my trained model to classify images whose file-names are stored in a column of a pandas dataframe. I would like a new column which contains the classification of the images.<br/><NewLine>I have not been able to find any examples which do not produce a web-service as the deployment. Most of those are toys which classify a single image.<br/><NewLine>I have my data, and a trained classifier model. I want to process  tens of thousands of images at once. Eventually I’d like to do some stream processing.<br/><NewLine>— Cheers…</p><NewLine></div>",https://discuss.pytorch.org/u/banksiaboy,(Peter Goodall),banksiaboy,"February 25, 2020,  8:57am",,,,,
70511,Is it possible to bake custom torchscript classes into scripted models?,2020-02-20T16:17:16.958Z,0,74,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Suppose we have a torch model like this</p><NewLine><pre><code class=""lang-auto"">from script_class import CustomClass<NewLine>class Model(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Model, self).__init___()<NewLine>        self.object = CustomClass()<NewLine>   def forward(self):<NewLine>       print('hello')<NewLine></code></pre><NewLine><p>Then i script the model and save it as ‘model.pt’<br/><NewLine>Next when I load it without import CustomClass and run it, it will crash with unreadable c++ errors</p><NewLine><p>How do I bake the class into the scripted model, something like torch.jit.export but for data structures?</p><NewLine></div>",https://discuss.pytorch.org/u/Chenchao_Zhao,(Chenchao Zhao),Chenchao_Zhao,"February 20, 2020,  4:17pm",,,,,
70432,How to reduce the deployment footprint,2020-02-20T07:22:52.692Z,0,119,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi All,</p><NewLine><p>This is the first time I am going to deploy a model in production(AWS), so need some help. The AWS server will not have a GPU and our inference code will be in python - we are not worried about speed right now. The main requirement is to keep the size of the instance minimal. Considering this, can someone please tell me how to install pytorch (CPU only) without having to install all the extra packages that come with anaconda? Any other tips will be very appreciated; I am new to this space.</p><NewLine><p>Sincerely,<br/><NewLine>Vishal</p><NewLine></div>",https://discuss.pytorch.org/u/Vishal_Ahuja,(Vishal Ahuja),Vishal_Ahuja,"February 20, 2020,  7:22am",,,,,
70339,Anaconda3-2019.1 does not load pytorch using recommended conda install pytorch torchvision cudatoolkit=10.1 -c pytorch,2020-02-19T15:09:59.056Z,1,87,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Pytorch load via conda gives error</p><NewLine></div>",https://discuss.pytorch.org/u/hadfield75,(anthony Hadfield),hadfield75,"February 19, 2020,  3:09pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>What error are you seeing?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I solved the problem by getting the Github repository of Pytorch and putting it into the Anaconda 3 packages directory.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hadfield75; <NewLine> ,"REPLY_DATE 1: February 20, 2020,  2:12am; <NewLine> REPLY_DATE 2: February 20, 2020,  5:13am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
70219,Recommended way of PyTorch model behind a high traffic REST API?,2020-02-18T18:56:32.901Z,0,233,"<div class=""post"" itemprop=""articleBody""><NewLine><p>As title suggests, I am wondering what’s the right way to expose a PyTorch model that thousands of requests a second?</p><NewLine><p>Would one write a REST API in C++ and serve model using TorchScript?<br/><NewLine>Or can one be lazy and serve through Python based Flask app (or something more resilient) and use PyTorch as is?</p><NewLine><p>Or can one take the extra step and try to serve using GoLang based service and somehow use a model saved as TorchScript?  (If so, how would one leverage GPU based inference?</p><NewLine><p>What are the pros cons of the above methods?</p><NewLine></div>",https://discuss.pytorch.org/u/praateekmahajan,(Praateek),praateekmahajan,"February 18, 2020,  6:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Technically, it depends on the size of the model you are using… But there’s a reason people use C++ in production: python just isn’t fast enough for that kind of traffic.</p><NewLine><p>I’ve done a classification demo website in the past, python/pytorch-only, with Flask or Bottle or something like that, and requests took a couple of seconds to be processed (it was a ResNet-34 model, with input size close to ImageNet’s, run on a 1080Ti).</p><NewLine><p>I’d recommend going by iteration: starting with python-only code, then switch to C++ if it’s too slow. Don’t know about GoLang, though!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alex.veuthey; <NewLine> ,"REPLY_DATE 1: February 19, 2020,  7:32am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
57281,Trying to load a torch model via Dropbox,2019-10-02T00:28:03.384Z,4,318,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to find a way to download a saved pytorch model hosted on Dropbox and load it into pytorch.  I can’t save or write a file to the server that is running pytorch.  I tried to download the torch model with the code below and got a attribute error.  Any other ideas do load a model without writing to a file?</p><NewLine><pre><code class=""lang-auto"">        dropbox_url_act = ""https://www.dropbox.com/path_here?&amp;dl=1""<NewLine>        dropbox_url_cri = ""https://www.dropbox.com/path_here?&amp;dl=1""<NewLine>        <NewLine>        req = requests.get(dropbox_url_act)<NewLine>        actor = pickle.loads(req.content)<NewLine>        <NewLine>        req = requests.get(dropbox_url_cri)<NewLine>        critic = pickle.loads(req.content)<NewLine>        <NewLine>        agent.actor_local.load_state_dict(torch.load(actor))<NewLine>        agent.critic_local.load_state_dict(torch.load(critic))<NewLine></code></pre><NewLine><p>AttributeError : ‘int’ object has no attribute ‘seek’. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.</p><NewLine></div>",https://discuss.pytorch.org/u/Bastulli,(Joe Bastulli),Bastulli,"October 2, 2019, 12:28am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Also, when trying to use torch.as_tensor, I get the following error.</p><NewLine><p>During the algorithm initialization, the following exception has occurred: RuntimeError : Overflow when</p><NewLine><pre><code class=""lang-auto"">unpacking long<NewLine>  at Initialize in main.py:line 26<NewLine> :: self.agent.actor_local.load_state_dict(torch.as_tensor(actor))<NewLine> RuntimeError : Overflow when unpacking long<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found a solution using BytesIO with torch.load()</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, how exactly did you solve it? <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code>    from io import BytesIO<NewLine>    <NewLine>    '''Self.Download is a function that allows me to download from dropbox""""""<NewLine>    act_b64_str = self.Download(""enter_url_here"")<NewLine>    cri_b64_str = self.Download(""enter_url_here"")<NewLine>    <NewLine>    # String Encode to bytes<NewLine>    act_b = act_b64_str.encode(""UTF-8"")<NewLine>    <NewLine>    # Decoding the Base64 bytes<NewLine>    act_d = base64.b64decode(act_b)<NewLine><NewLine>    # String Encode to bytes<NewLine>    cri_b = cri_b64_str.encode(""UTF-8"")<NewLine>    <NewLine>    # Decoding the Base64 bytes<NewLine>    cri_d = base64.b64decode(cri_b)<NewLine><NewLine>    self.agent.actor_local.load_state_dict(torch.load(BytesIO(act_d), map_location=lambda storage, loc: storage))<NewLine>    self.agent.critic_local.load_state_dict(torch.load(BytesIO(cri_d), map_location=lambda storage, loc: storage))</code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Its also worth noting that the Pytorch model was converted to Base64 before uploaded on dropbox</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Bastulli; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Bastulli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Sarah_W; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Bastulli; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Bastulli; <NewLine> ,"REPLY_DATE 1: October 2, 2019, 12:37am; <NewLine> REPLY_DATE 2: October 2, 2019,  2:00am; <NewLine> REPLY_DATE 3: February 5, 2020,  2:58pm; <NewLine> REPLY_DATE 4: February 19, 2020,  3:16am; <NewLine> REPLY_DATE 5: February 19, 2020,  3:18am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> 
70263,Target value of regression with CNN are almost same,2020-02-19T03:00:55.351Z,0,125,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t know why the prediction result is so bad,<br/><NewLine>My model wants to predict the 4 target values at the same time<br/><NewLine>but the prediction results are almost the same. it looks that their shape are the same but with different magnitude.<br/><NewLine>the following figure shows the prediction results. the top 4 lines are the target values and the bottom 4 lines are the predicted ones.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/262dfb8de5ba8e9d68bd85986eec4eab3cf2a9e9"" href=""https://discuss.pytorch.org/uploads/default/original/3X/2/6/262dfb8de5ba8e9d68bd85986eec4eab3cf2a9e9.png"" title=""image""><img alt=""image"" data-base62-sha1=""5rKEZIgp2Pevng3jP3KOvd7c3fb"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/6/262dfb8de5ba8e9d68bd85986eec4eab3cf2a9e9_2_10x10.png"" height=""464"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/6/262dfb8de5ba8e9d68bd85986eec4eab3cf2a9e9_2_690x464.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/2/6/262dfb8de5ba8e9d68bd85986eec4eab3cf2a9e9_2_690x464.png, https://discuss.pytorch.org/uploads/default/original/3X/2/6/262dfb8de5ba8e9d68bd85986eec4eab3cf2a9e9.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/2/6/262dfb8de5ba8e9d68bd85986eec4eab3cf2a9e9.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">802×540 144 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>Here I add the main model and the training codes.</p><NewLine><p>I worry if I make any mistake.</p><NewLine><pre><code class=""lang-auto""><NewLine>class Net(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Net, self).__init__()<NewLine><NewLine>        self.conv_stem = nn.Conv2d(1, 8, 5, 2)<NewLine>        self.bn_stem = nn.BatchNorm2d(8)<NewLine>        self.relu = nn.ReLU(inplace=True)<NewLine>        self.conv_1 = nn.Conv2d(2, 6, 3, 2)<NewLine>        self.conv_2 = nn.Conv2d(24, 3, 3, 1)<NewLine>        self.conv_3 = nn.Conv2d(12, 3, 3, 2)<NewLine><NewLine>        self.bn1 = nn.BatchNorm2d(6)<NewLine>        self.bn2 = nn.BatchNorm2d(3)<NewLine><NewLine>        self.fc1 = nn.Linear(3 * 3 * 3 + 2, fc_num)<NewLine>        self.fc2 = nn.Linear(fc_num, 1)<NewLine>        self.dropout = nn.Dropout(p=0.5, inplace=False)<NewLine><NewLine>    def forward(self, x_image, x_continuous):<NewLine>        x_image = self.conv_stem(x_image)<NewLine>        if with_attention:<NewLine>            x_image, att_stem = self.attn_stem(x_image)  # attention<NewLine>        x_image = self.bn_stem(x_image)<NewLine>        x_image = self.relu(x_image)<NewLine><NewLine>        x1 = x_image[:, 0:2, :, :]<NewLine>        x2 = x_image[:, 2:4, :, :]<NewLine>        x3 = x_image[:, 4:6, :, :]<NewLine>        x4 = x_image[:, 6:8, :, :]<NewLine><NewLine>        x1 = self.conv_1(x1)<NewLine>        x1 = self.bn1(x1)<NewLine>        x1 = self.relu(x1)<NewLine><NewLine>        x2 = self.conv_1(x2)<NewLine>        x2 = self.bn1(x2)<NewLine>        x2 = self.relu(x2)<NewLine><NewLine>        x3 = self.conv_1(x3)<NewLine>        x3 = self.bn1(x3)<NewLine>        x3 = self.relu(x3)<NewLine><NewLine>        x4 = self.conv_1(x4)<NewLine>        x4 = self.bn1(x4)<NewLine>        x4 = self.relu(x4)<NewLine><NewLine>        merge_actv1 = torch.cat((x1, x2, x3, x4), 1)<NewLine><NewLine>        x1 = self.conv_2(merge_actv1)<NewLine>        x1 = self.bn2(x1)<NewLine>        x1 = self.relu(x1)<NewLine><NewLine>        x2 = self.conv_2(merge_actv1)<NewLine>        x2 = self.bn2(x2)<NewLine>        x2 = self.relu(x2)<NewLine><NewLine>        x3 = self.conv_2(merge_actv1)<NewLine>        x3 = self.bn2(x3)<NewLine>        x3 = self.relu(x3)<NewLine><NewLine>        x4 = self.conv_2(merge_actv1)<NewLine>        x4 = self.bn2(x4)<NewLine>        x4 = self.relu(x4)<NewLine><NewLine>        merge_actv2 = torch.cat((x1, x2, x3, x4), 1)<NewLine><NewLine>        x1 = self.conv_3(merge_actv2)<NewLine>        x1 = self.bn2(x1)<NewLine>        x1 = self.relu(x1)<NewLine><NewLine>        x2 = self.conv_3(merge_actv2)<NewLine>        x2 = self.bn2(x2)<NewLine>        x2 = self.relu(x2)<NewLine><NewLine>        x3 = self.conv_3(merge_actv2)<NewLine>        x3 = self.bn2(x3)<NewLine>        x3 = self.relu(x3)<NewLine><NewLine>        x4 = self.conv_3(merge_actv2)<NewLine>        x4 = self.bn2(x4)<NewLine>        x4 = self.relu(x4)<NewLine><NewLine>        x1_shape0, x1_shape1, x1_shape2, x1_shape3 = x1.shape<NewLine>        flatten_length = x1_shape1 * x1_shape2 * x1_shape3<NewLine>        # print('after_conv_layer3', x1.shape)  # (batch_size,3,3,3)<NewLine><NewLine>        x1 = x1.view(-1, flatten_length)<NewLine>        x2 = x2.view(-1, flatten_length)<NewLine>        x3 = x3.view(-1, flatten_length)<NewLine>        x4 = x4.view(-1, flatten_length)<NewLine>        # print('after_flatten', x1.shape)  # (batch_size,27)<NewLine><NewLine>        # print(x_continuous.shape)  # (batch_size,5)<NewLine>        x_distance = x_continuous[:, 0]<NewLine>        angle1 = x_continuous[:, 1]<NewLine>        angle2 = x_continuous[:, 2]<NewLine>        angle3 = x_continuous[:, 3]<NewLine>        angle4 = x_continuous[:, 4]<NewLine>        # print('distance shape', angle1.shape)  # (batch_size,)<NewLine><NewLine>        x_distance = x_distance.view(-1, 1)<NewLine>        angle1 = angle1.view(-1, 1)<NewLine>        angle2 = angle2.view(-1, 1)<NewLine>        angle3 = angle3.view(-1, 1)<NewLine>        angle4 = angle4.view(-1, 1)<NewLine><NewLine>        # print('distance shape', angle1.shape)  # (batch_size,1)<NewLine><NewLine>        x1 = torch.cat((x1, x_distance, angle1), 1)<NewLine>        x2 = torch.cat((x2, x_distance, angle2), 1)<NewLine>        x3 = torch.cat((x3, x_distance, angle3), 1)<NewLine>        x4 = torch.cat((x4, x_distance, angle4), 1)<NewLine>        # print('after_concat', x1.shape)  # (batch_size,29)<NewLine><NewLine>        x1 = self.fc1(x1)<NewLine>        x1 = self.relu(x1)<NewLine>        # x1 = x1.view(x1.size(0), -1)<NewLine>        x1 = self.dropout(x1)<NewLine>        x1 = self.fc2(x1)<NewLine><NewLine>        x2 = self.fc1(x2)<NewLine>        x2 = self.relu(x2)<NewLine>        # x2 = x2.view(x2.size(0), -1)<NewLine>        x2 = self.dropout(x2)<NewLine>        x2 = self.fc2(x2)<NewLine><NewLine>        x3 = self.fc1(x3)<NewLine>        x3 = self.relu(x3)<NewLine>        # x3 = x3.view(x3.size(0), -1)<NewLine>        x3 = self.dropout(x3)<NewLine>        x3 = self.fc2(x3)<NewLine><NewLine>        x4 = self.fc1(x4)<NewLine>        x4 = self.relu(x4)<NewLine>        # x4 = x4.view(x4.size(0), -1)<NewLine>        x4 = self.dropout(x4)<NewLine>        x4 = self.fc2(x4)<NewLine><NewLine>        # print('final_predited PL', x1.shape)  # (batch_size,1)<NewLine><NewLine>        out = torch.cat((x1, x2, x3, x4), 1)<NewLine>        # print('after_concat', x1.shape)  # (batch_size,4)<NewLine>        return out<NewLine><NewLine><NewLine>net = Net()<NewLine>net = net.to(device)<NewLine><NewLine># print(net)<NewLine>if isSGD:<NewLine>    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)<NewLine>else:<NewLine>    optimizer = optim.Adam(net.parameters(), lr=0.001)<NewLine><NewLine><NewLine>def RMSELoss(yhat, y):<NewLine>    # print(torch.sqrt(torch.mean((yhat-y)**2),0))<NewLine>    # print(torch.sqrt(torch.mean((yhat-y)**2),1))<NewLine>    return torch.sqrt(torch.mean((yhat - y) ** 2))<NewLine><NewLine><NewLine>criterion = RMSELoss<NewLine># criterion = nn.MSELoss()  # nn.CrossEntropyLoss()<NewLine># optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)<NewLine><NewLine>losses = []<NewLine>val_losses = []<NewLine>running_loss = 0.0<NewLine>print_time = 0<NewLine>accmulated_amount_sample = 0<NewLine><NewLine><NewLine>for epoch in range(num_epoch):<NewLine>    training_dataloader_batch_size = training_dataloader.batch_size<NewLine><NewLine>    for i, data in enumerate(training_dataloader, 0):<NewLine>        inputs, labels, tr_angles, positions = data['image'], data['pls'], data['tr_angles'], data['positions']<NewLine>        inputs, labels, tr_angles, positions = inputs.to(device, dtype=torch.float), \<NewLine>                                               labels.to(device, dtype=torch.float), \<NewLine>                                               tr_angles.to(device, dtype=torch.float), \<NewLine>                                               positions.to(device, dtype=torch.float)<NewLine>        optimizer.zero_grad()<NewLine><NewLine>        outputs = net(inputs, tr_angles)<NewLine><NewLine>        # loss = torch.sqrt(criterion(outputs, labels))<NewLine>        loss = criterion(outputs, labels)<NewLine>        # print(loss)  # tensor(107.8965, device='cuda:0', grad_fn=&lt;SqrtBackward&gt;)<NewLine><NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine>        loss_value = loss.item()<NewLine><NewLine>        # print('val_loss_value: ', loss_value)<NewLine>        running_loss += loss_value<NewLine>        losses.append(loss_value)<NewLine>        # break<NewLine>    epoch_loss_value = running_loss / (i + 1)  # len_training = accmulated_amount_sample<NewLine>    print('[training epoch %d] loss= %7.6f' %<NewLine>          (epoch + 1, epoch_loss_value))<NewLine>    writer.add_scalar('Loss/Training', epoch_loss_value, epoch)<NewLine>    running_loss = 0.0<NewLine>    accmulated_amount_sample = 0<NewLine>    print_time += 1<NewLine><NewLine>    if val_print_show_time == print_time:<NewLine>        print_time = 0<NewLine>        # val_running_loss = 0.0<NewLine>        with torch.no_grad():<NewLine>            for i, data in enumerate(validation_dataloader, 0):<NewLine>                val_inputs, val_labels, \<NewLine>                val_tr_angles, val_positions = data['image'], data['pls'], \<NewLine>                                               data['tr_angles'], data['positions']<NewLine>                val_inputs, val_labels, \<NewLine>                val_tr_angles, val_positions = val_inputs.to(device, dtype=torch.float), \<NewLine>                                               val_labels.to(device, dtype=torch.float), \<NewLine>                                               val_tr_angles.to(device, dtype=torch.float), \<NewLine>                                               val_positions.to(device, dtype=torch.float)<NewLine><NewLine>                net.eval()<NewLine><NewLine>                val_outputs = net(val_inputs, val_tr_angles)<NewLine>                val_loss = criterion(val_outputs, val_labels)<NewLine>                val_loss_value = val_loss.item()<NewLine>                writer.add_scalar('Loss/Validation', val_loss_value, epoch)<NewLine>                writer.add_scalars('Loss/Training&amp;Validation',<NewLine>                                   {'Training': epoch_loss_value, 'Validation': val_loss_value}, epoch)<NewLine>                # val_running_loss += val_loss_value<NewLine>                # print('val_loss_value: ', val_loss_value)<NewLine>                val_losses.append(val_loss_value)<NewLine>        # print(validation_dataloader.batch_size)<NewLine>        print('[test epoch %d] val loss: %7.6f' %<NewLine>              (epoch + 1, val_loss_value))  # / validation_dataloader.batch_size<NewLine>        # val_running_loss = 0<NewLine>        parameters = net.named_parameters()<NewLine>        for name, param in parameters:<NewLine>            if 'bn' not in name and 'attn' not in name:<NewLine>                writer.add_histogram(name, param, epoch)<NewLine><NewLine>print('Finished Training')<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Hong_Cheng,(Hong Cheng),Hong_Cheng,"February 19, 2020,  3:00am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>A related question:</p><NewLine><h1><a href=""https://discuss.pytorch.org/t/possible-reasons-for-regression-problem-with-almost-same-prediction-result/66596"">Possible reasons for regression problem with almost same prediction result</a></h1><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Hong_Cheng; <NewLine> ,"REPLY_DATE 1: February 19, 2020,  3:02am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
69770,PyTorch in action,2020-02-14T18:32:09.291Z,0,95,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi<br/><NewLine>I would like to know if there is any list that mention open source or commercial AI projects that use PyTorch as the infrastructure.<br/><NewLine>I haven’t found such a thing.</p><NewLine></div>",https://discuss.pytorch.org/u/mahmoodn,(Mahmood Naderan),mahmoodn,"February 14, 2020,  6:32pm",,,,,
69706,LSTM variable perdiction performance,2020-02-14T08:16:53.127Z,0,94,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m running an LSTM model on a AWS t2.medium instance (2,4GHZ 2 cores).<br/><NewLine>I use sequence length of 120 and have 16 features.</p><NewLine><p>The time it takes to predict and cpu utilization seems very variable. prediction takes around 50 ms most of the time but all of a sudden it can take 5 seconds for one predictions while cpu usage goes above 100%<br/><NewLine>See the model below, any ideas?</p><NewLine><pre><code class=""lang-auto"">class LSTMmodel(nn.Module):<NewLine><NewLine>    def __init__(self, output_size, hidden_dim, n_layers, features, drop_prob=0.3):<NewLine>        """"""<NewLine>        Initialize the model by setting up the layers.<NewLine>        """"""<NewLine>        super().__init__()<NewLine><NewLine>        self.output_size = output_size<NewLine>        self.n_layers = n_layers<NewLine>        self.hidden_dim = hidden_dim<NewLine>        self.features = features<NewLine><NewLine>     <NewLine>        self.lstm = nn.LSTM(self.features, hidden_dim, n_layers,<NewLine>                            dropout=drop_prob)<NewLine><NewLine>        # dropout layer<NewLine>        self.dropout = nn.Dropout(0.3)<NewLine><NewLine>        self.fc = nn.Linear(hidden_dim, output_size)<NewLine><NewLine>    def forward(self, x, hidden):<NewLine><NewLine>        batch_size = x.size(1)<NewLine>        lstm_out, hidden = self.lstm(x, hidden)<NewLine><NewLine>        # stack up lstm outputs<NewLine>        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)<NewLine><NewLine>        # dropout and fully-connected layer<NewLine>        out = self.dropout(lstm_out)<NewLine>        out = self.fc(out)<NewLine><NewLine>        # reshape to be batch_size first<NewLine>        out = out.view(batch_size, -1, self.output_size)<NewLine>        out = out[:, -1] # get last batch of labels<NewLine><NewLine>        return out, hidden<NewLine><NewLine><NewLine><NewLine>    def init_hidden(self, batch_size, train_on_gpu=False):<NewLine>        ''' Initializes hidden state '''<NewLine><NewLine>        weight = next(self.parameters()).data<NewLine><NewLine>        if (train_on_gpu):<NewLine>            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),<NewLine>                  weight.new(self.n_layers, batch_size,  self.hidden_dim).zero_().cuda())<NewLine>        else:<NewLine>            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),<NewLine>                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())<NewLine><NewLine>        return hidden<NewLine><NewLine><NewLine>    def predict(self, inp):<NewLine><NewLine>        self.eval()<NewLine>        h = self.init_hidden(1)<NewLine>        inp = torch.FloatTensor(inp)<NewLine>        with torch.no_grad():<NewLine>                outputs, h = self(inp, h)<NewLine>                _, prediction = torch.max(outputs, 1)<NewLine>        return F.softmax(outputs, dim=1).numpy()[0], prediction.numpy()[0]<NewLine><NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Roaldb86,(Roald Brønstad),Roaldb86,"February 14, 2020,  8:24am",,,,,
69662,Need help with Go binding,2020-02-13T22:17:34.015Z,0,375,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey,</p><NewLine><p>My colleague and I have been working on binding the most updated Torchlib to Go for our PyTorch production model. I attached the link below.</p><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicon.ico"" width=""16""/><NewLine><a href=""https://github.com/nextcaller/go-torch"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars2.githubusercontent.com/u/5240414?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/nextcaller/go-torch"" rel=""nofollow noopener"" target=""_blank"">nextcaller/go-torch</a></h3><NewLine><p>LibTorch (PyTorch) bindings for Golang. Contribute to nextcaller/go-torch development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>We reached the bottleneck where we tried to convert the Compilation unit to Module, and the compilation unit is a new thing. I wonder if we can get any help from the official PyTorch developers to help explain some C++ functions and classes.</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/Yitao_Sun,(Yitao Sun),Yitao_Sun,"February 13, 2020, 10:17pm",1 Like,,,,
67684,How to customize build torchscript model to be used in another code,2020-01-26T10:22:16.445Z,2,180,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to compile my model to be executed in the Python script running on our customers computers.<br/><NewLine>Currently my resnet model size is ~100MB but it needs torch, which requires 1.5GB of space.<br/><NewLine>Currently i am doing this series of commands:</p><NewLine><pre><code class=""lang-auto"">model = torchvision.models.resnet50(pretrained=True)<NewLine>model.eval()<NewLine>example = torch.ones(1, 3, 224, 224)<NewLine>traced_model = torch.jit.trace(model, example)<NewLine>ops = torch.jit.export_opnames(model)<NewLine>traced_model.save('traced_model.pt')<NewLine>with open('model_ops.yaml', 'w') as output:<NewLine>    yaml.dump(ops, output)<NewLine></code></pre><NewLine><p>The question is how to continue from here in order to build a model i can use in another python/c script without the need to load the entire torch or libtorch packages, but only what is needed based on the model operations.</p><NewLine></div>",https://discuss.pytorch.org/u/danmalowany-allegro,,danmalowany-allegro,"January 26, 2020, 12:41pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We currently don’t support this. This sounds like a reasonable feature request that is important for deployment. Please open a feature request over at <a href=""https://github.com/pytorch/pytorch"">https://github.com/pytorch/pytorch</a> (or voice your support on a current issue if it exists) !</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This sounds like it overlaps with some of the concerns of the Mobile userbase (e.g. building with only the used ops to reduce total size). Cc <a class=""mention"" href=""/u/david_reiss"">@David_Reiss</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jeff_smith"">@Jeff_Smith</a> - I agree. This is relevant for mobile, as well as for other end devices. However, considering the resources in modern smartphones, the need in other end devices (not mobile) is much more prominent. This is a major issue for deployment.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/richard"">@richard</a> - I saw some discussions about it for mobile deployment (<a href=""https://pytorch.org/mobile/ios/#custom-build"" rel=""nofollow noopener"">https://pytorch.org/mobile/ios/#custom-build</a>). However, as i answered to <a class=""mention"" href=""/u/jeff_smith"">@Jeff_Smith</a> , the need for such a solution for deployment on any end device is even greater than for modern smartphones.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Moved discussion to <a href=""https://github.com/pytorch/pytorch/issues/32690#"">https://github.com/pytorch/pytorch/issues/32690#</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/richard; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Jeff_Smith; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/danmalowany-allegro; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/danmalowany-allegro; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: January 27, 2020,  6:44pm; <NewLine> REPLY_DATE 2: January 27, 2020,  8:02pm; <NewLine> REPLY_DATE 3: January 28, 2020,  9:43am; <NewLine> REPLY_DATE 4: January 28, 2020,  9:51am; <NewLine> REPLY_DATE 5: February 5, 2020,  6:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
68138,Why is MKLDNN slower than normal cpu mode,2020-01-30T14:40:32.817Z,1,275,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello Everyone, Hope you are having a great day.<br/><NewLine>For the past several weeks, I have been trying to get our models to run faster either using the MKLDNN or the Quantization. However, both of them seems to be failing for some reasons.<br/><NewLine>Today, I found out MKLDNN also doesnt work! it seems not to have any effect on runtime performance. Its even some times, slower than normal cpu mode!<br/><NewLine>The issue exists both on 1.3.1 and 1.4.0.<br/><NewLine>You may try the following snippet and see how it performs for you.<br/><NewLine>Running this on my local machine and also Google Colab, resulted the same and they were both slow as snail , in some cases slower than the normal (default) cpu mode!</p><NewLine><p>Run this on a supporting platform such as Linux (e.g. ubuntu 18.04)</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>print(f'Pytorch version : {torch.__version__}')<NewLine>print(*torch.__config__.show().split(""\n""), sep=""\n"")<NewLine><NewLine>from torchvision import models<NewLine>from torch.utils import mkldnn as mkldnn_utils<NewLine>import time <NewLine><NewLine>def forward(net, use_mkldnn=False, iteration=1, batch_size=10):<NewLine>  net.eval()<NewLine>  batch = torch.rand(batch_size, 3,224,224)<NewLine>  if use_mkldnn:<NewLine>    net = mkldnn_utils.to_mkldnn(net)<NewLine>    batch = batch.to_mkldnn()<NewLine><NewLine>  start_time = time.time()<NewLine>  for i in range(iteration):<NewLine>      net(batch)<NewLine>  return time.time() - start_time<NewLine><NewLine>net = models.resnet18(False)<NewLine>iter_cnt = 100<NewLine>batch_size = 1<NewLine><NewLine>no_mkldnn   = forward(net, False, iter_cnt, batch_size)<NewLine>with_mkldnn = forward(net, True,  iter_cnt, batch_size)<NewLine><NewLine>print(f""time-normal: {no_mkldnn:.4f}s"")<NewLine>print(f""time-mkldnn: {with_mkldnn:.4f}s"")<NewLine>print(f""mkldnn is {with_mkldnn/no_mkldnn:.2f}x slower!"")<NewLine><NewLine></code></pre><NewLine><p>Any help is greatly appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/Shisho_Sama,(A curious guy here!),Shisho_Sama,"January 30, 2020,  2:40pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It might depend on your architecture, but I see a speedup:</p><NewLine><pre><code class=""lang-python"">time-normal: 2.7962s<NewLine>time-mkldnn: 1.5869s<NewLine>mkldnn is 0.57x slower!<NewLine></code></pre><NewLine><p>for an <code>Intel(R) Core(TM) i7-5820K CPU @ 3.30GHz</code>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Compared to results from <a href=""https://discuss.pytorch.org/t/use-mkldnn-in-pytorch/54943/15"">here</a> : its really nothing ! I ran this some time before and got around 600x faster inference on mkldnn version!<br/><NewLine>I remember running it couple of times as well!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> ,"REPLY_DATE 1: January 31, 2020, 12:04am; <NewLine> REPLY_DATE 2: January 31, 2020,  6:30am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
67769,Pipeline batches during inference,2020-01-27T11:32:22.807Z,0,175,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>Google published a paper where they “pipeline” the inference (and backprop) stage (each later at a time).<br/><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://static.arxiv.org/static/browse/0.2.7/images/icons/favicon.ico"" width=""16""/><NewLine><a href=""https://arxiv.org/abs/2001.06232"" rel=""nofollow noopener"" target=""_blank"">arXiv.org</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail"" height=""16"" src="""" width=""16""/><NewLine><h3><a href=""https://arxiv.org/abs/2001.06232"" rel=""nofollow noopener"" target=""_blank"">Sideways: Depth-Parallel Training of Video Models</a></h3><NewLine><p>We propose Sideways, an approximate backpropagation scheme for training video<NewLine>models. In standard backpropagation, the gradients and activations at every<NewLine>computation step through the model are temporally synchronized. The forward<NewLine>activations need to...</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>I thought that most frameworks do this during the forward pass when inferencing, since you don’t need to wait for the batch to finish calculating all the outputs before loading a new batch.<br/><NewLine>If that’s not the case, is any work on this planned for pytorch?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/adizhol,(Adizhol),adizhol,"January 27, 2020, 11:32am",,,,,
67515,Deploying models to Google App Engine Flexible,2020-01-24T00:15:47.615Z,0,378,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to deploy a Torch/Transformers model on Google App Engine Flexible.  During deploy, I get this error:</p><NewLine><pre><code class=""lang-auto"">Updating service [default] (this may take several minutes)...failed.                                                                      <NewLine>ERROR: (gcloud.app.deploy) Error Response: [4] Your deployment has failed to become healthy in the allotted time and therefore was rolled back. If you believe this was an error, try adjusting the 'app_start_timeout_sec' setting in the 'readiness_check' section.<NewLine></code></pre><NewLine><p>I’ve done the recommended change and also cranked up the instance memory to 10GB but I still get the same error.  I’m copying my <code>app.yaml</code> below.</p><NewLine><p>Has anyone successfully deployed a model like this to GAE flex?</p><NewLine><p>I’m able to deploy to AWS but need it on GAE flex…</p><NewLine><p>=====</p><NewLine><pre><code class=""lang-auto"">runtime: python<NewLine>env: flex<NewLine><NewLine>entrypoint: gunicorn -b :$PORT main:app<NewLine><NewLine>runtime_config:<NewLine>  python_version: 3<NewLine><NewLine>manual_scaling:<NewLine>  instances: 2<NewLine><NewLine>resources:<NewLine>  cpu: 1<NewLine>  memory_gb: 10<NewLine>  disk_size_gb: 10<NewLine><NewLine>network:<NewLine>  name: default<NewLine><NewLine>readiness_check:<NewLine>  app_start_timeout_sec: 1800<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/JeffO,,JeffO,"January 24, 2020, 12:15am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Increasing the disk size to 20gb fixed this for me.  There was actually an error message in the deploy logs but it is easy to miss because of the sheer volume of log messages.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/JeffO; <NewLine> ,"REPLY_DATE 1: January 26, 2020,  5:37pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
67007,Why cannot the defined Variable be computed by nueral network?,2020-01-18T00:41:18.904Z,1,127,"<div class=""post"" itemprop=""articleBody""><NewLine><p><strong>the  neural network is defined as</strong><br/><NewLine>class networka(nn.Module):<br/><NewLine>def <strong>init</strong>(self):<br/><NewLine>super(networka,self).<strong>init</strong>()<br/><NewLine>self.network=nn.Sequential(<br/><NewLine>nn.Linear(19,11),<br/><NewLine>nn.ReLU(True),<br/><NewLine>nn.Linear(11,3),<br/><NewLine>nn.Softmax()<br/><NewLine>)<br/><NewLine>def forward(self, x):<br/><NewLine>x=self.network(x)<br/><NewLine>return x<br/><NewLine>A=networka()</p><NewLine><p><strong>This code works</strong></p><NewLine><p>z = Variable(torch.randn(19,19))<br/><NewLine>A(z)</p><NewLine><p><strong>But this doesn’t</strong></p><NewLine><p>X_data=(np.random.rand(19,19))<br/><NewLine>X=Variable(torch.from_numpy(X_data))<br/><NewLine>A(X)</p><NewLine><p>Could anyone tell me why?</p><NewLine></div>",https://discuss.pytorch.org/u/999999999,,999999999,"January 18, 2020, 12:41am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You should get an error message:</p><NewLine><pre><code class=""lang-python"">RuntimeError: Expected object of scalar type Float but got scalar type Double for argument #2 'mat1' in call to _th_addmm<NewLine></code></pre><NewLine><p>which points to a type mismatch in an operation.</p><NewLine><p>Numpy uses float64 by default, while PyTorch uses float32.<br/><NewLine>You could either transform the data to float32 via <code>X = X.float()</code>, which would be the usual use case or alternatively you could transform the model parameters to float64 via <code>model = model.double</code>.</p><NewLine><p>Also, <code>Variables</code> are deprecated since PyTorch <code>0.4.0</code>, so don’t use them.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much for your help!</p><NewLine><p>ptrblck via PyTorch Forums <a href=""mailto:noreply@discuss.pytorch.org"">noreply@discuss.pytorch.org</a>于2020年1月19日 周日01:58写道：</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/999999999; <NewLine> ,"REPLY_DATE 1: January 19, 2020,  9:48am; <NewLine> REPLY_DATE 2: January 19, 2020,  4:34pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
66827,Is there any pytorch resources for Click Though Rate?,2020-01-16T07:58:39.018Z,0,77,"<div class=""post"" itemprop=""articleBody""><NewLine><p>It seems that the resources about CTR(Click-Though Rate) is rare.</p><NewLine></div>",https://discuss.pytorch.org/u/kailing_ye,(kailing ye),kailing_ye,"January 16, 2020,  7:58am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m not sure what you mean by this? You mean a Dataset for training?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: January 16, 2020,  3:20pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
66626,Should I one_hot the category feature before send the feature to Embedding layer?,2020-01-14T11:12:56.545Z,0,79,"<div class=""post"" itemprop=""articleBody""><NewLine><p>For example, I have the category feature named fruit that contains apple, banana, peach, pear. Should I do one hot encoding before sending to the Embedding layer?</p><NewLine></div>",https://discuss.pytorch.org/u/kailing_ye,(kailing ye),kailing_ye,"January 14, 2020, 11:12am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>You usually associate an index to each word. The Embedding layer takes the indices as input, not one-hot encoding of these indices.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: January 15, 2020,  6:17am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
66470,Is there any official implement to rollback checkpoint and continue to training,2020-01-13T03:08:32.371Z,0,76,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I write my own coding to <strong>save checkpoint</strong> and do the <strong>rollback by early stopping</strong>.<br/><NewLine>I’d like to know if anyone have better implementation to share.</p><NewLine><p>Best regard,<br/><NewLine>Hong</p><NewLine></div>",https://discuss.pytorch.org/u/Hong_Cheng,(Hong Cheng),Hong_Cheng,"January 13, 2020,  3:08am",,,,,
64223,Unable to export gru model in onnx accepting variable length sequence,2019-12-16T14:36:30.408Z,0,265,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I am trying to export in onnx a Module using a simple Gru and some linear layers on top of it. The module is trained using packed sequences so there is no fixed length sequence at training time.</p><NewLine><p>I can export the model my_model using onnx</p><NewLine><pre><code class=""lang-auto"">input_names = [ ""time series"" ] + [ f""learned_{i}"" for i, p in enumerate(my_model.parameters()) ]<NewLine>output_names = [ ""log probability"" ]<NewLine><NewLine>dummy_input = torch.randn(100, device='cuda').view(-1, 1, 1)<NewLine>dummy_output = torch.randn(100, 3, device='cuda').view(-1, 1, 3)<NewLine><NewLine>my_model_jit= torch.jit.script(my_model)<NewLine>torch.onnx.export(my_model_jit, dummy_input, ""my_model.onnx"", verbose=True, input_names=input_names, output_names=output_names, example_outputs=dummy_output)<NewLine></code></pre><NewLine><p>The issue issue is that when I load my exported model then I can only do inference for fixed length series of length 100 (which is the size I randomly picked for the example)</p><NewLine><pre><code class=""lang-auto"">import onnxruntime as rt<NewLine>sess = rt.InferenceSession(r""my_model.onnx"")<NewLine><NewLine>input_name = sess.get_inputs()[0].name<NewLine><NewLine># this is working fine with length = 100<NewLine>ts = np.random.randn(100).cumsum().astype(np.float32).reshape( -1, 1, 1)<NewLine>pred_onx = sess.run(None, {input_name: ts})<NewLine><NewLine># this is failing with length != 100<NewLine>ts = np.random.randn(110).cumsum().astype(np.float32).reshape( -1, 1, 1)<NewLine>pred_onx = sess.run(None, {input_name: ts})<NewLine><NewLine>ouput :<NewLine>---------------------------------------------------------------------------<NewLine>InvalidArgument                           Traceback (most recent call last)<NewLine>&lt;ipython-input-162-da0f71c171b2&gt; in &lt;module&gt;()<NewLine>      1 ts = np.random.randn(110).cumsum().astype(np.float32).reshape( -1, 1, 1)<NewLine>----&gt; 2 pred_onx = sess.run(None, {input_name: ts})<NewLine><NewLine>c:\homeware\lib\site-packages\onnxruntime\capi\session.py in run(self, output_names, input_feed, run_options)<NewLine>    134             output_names = [output.name for output in self._outputs_meta]<NewLine>    135         try:<NewLine>--&gt; 136             return self._sess.run(output_names, input_feed, run_options)<NewLine>    137         except C.EPFail as err:<NewLine>    138             if self._enable_fallback:<NewLine><NewLine>InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: time series for the following indices<NewLine> index: 0 Got: 110 Expected: 100<NewLine> Please fix either the inputs or the model.<NewLine><NewLine></code></pre><NewLine><p>Is there a way to have a model without this limitation ie working like in pytorch ?</p><NewLine><p>Thanks a lot</p><NewLine></div>",https://discuss.pytorch.org/u/alexm,(miot),alexm,"December 16, 2019,  2:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have found how to do it and I was basically over-complicating something simple. You just need to specify dynamic_axes in your torch.onnx.export function. In particular, there is no need to jit the model and trace based export works fine.<br/><NewLine>In my example I should have done this (making sure that the batch_first in the GRU init is set to False</p><NewLine><pre><code class=""lang-auto"">dummy_input = torch.randn(100).reshape(1, -1, 1)<NewLine>torch.onnx.export(my_model_LEFT_AS_NN_MODULE,<NewLine>                    (dummy_input, ),<NewLine>                    'test_rnn.onnx',<NewLine>                    verbose=True,<NewLine>                    input_names=['input'],<NewLine>                    output_names=['output',],<NewLine>                    dynamic_axes={'input': {1: 'sequence'}, 'output': {1: 'sequence'} } )<NewLine></code></pre><NewLine><p>And then to use it simply do</p><NewLine><pre><code class=""lang-auto"">import onnxruntime<NewLine>import numpy as np<NewLine>sess = onnxruntime.InferenceSession(""test_rnn.onnx"")<NewLine>test_input = np.random.randn(100).astype(np.float32).reshape( 1, -1, 1)  <NewLine>sess.run([""output""], {'input': test_input }))<NewLine>test_input_different_size = np.random.randn(10).astype(np.float32).reshape( 1, -1, 1)  <NewLine>sess.run([""output""], {'input': test_input_different_size }))<NewLine></code></pre><NewLine><p>I should have read the doc ; )<br/><NewLine>Hope it helps if anybody stumbles upon the same issue</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alexm; <NewLine> ,"REPLY_DATE 1: January 6, 2020,  3:54pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
61058,Get the matmul operations in a net?,2019-11-15T07:29:18.876Z,1,172,"<div class=""post"" itemprop=""articleBody""><NewLine><p>There’s code such as [1] which walks through a net’s nn.Modules, checks their type, and calculates their FLOPs.</p><NewLine><p>However, some functions in pytorch are defined only in forward() and not in <strong>init</strong>(). For example, using the methodology of [1], we would get the FLOPs of torch.nn.Linear, but we would miss any instances of torch.matmul. Does anyone have an idea for how to write a function that takes a net and figures out the FLOPs used by torch.matmul operations?</p><NewLine><p>[1] <a href=""https://github.com/Lyken17/pytorch-OpCounter"" rel=""nofollow noopener"">https://github.com/Lyken17/pytorch-OpCounter</a></p><NewLine></div>",https://discuss.pytorch.org/u/solvingPuzzles,,solvingPuzzles,"November 15, 2019,  7:29am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>…I ended up making nn.Module wrappers for anonymous functions such as MatMul for my net. Not the most elegant solution, but it worked for my use-case.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I had seen a code aiming to convert PyTorch to Caffe. It looks like</p><NewLine><pre><code class=""lang-python"">raw_mul = torch.Tensor.__mul__<NewLine>def mul(input, *args):<NewLine>    x = raw_mul(input, *args)<NewLine>    do something<NewLine>torch.Tensor.__mul__ = mul<NewLine></code></pre><NewLine><p>Not the most elegant solution.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>That makes sense too.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/solvingPuzzles; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Eta_C; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/solvingPuzzles; <NewLine> ,"REPLY_DATE 1: January 6, 2020,  2:31am; <NewLine> REPLY_DATE 2: January 6, 2020,  1:48am; <NewLine> REPLY_DATE 3: January 6, 2020,  2:31am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
65747,Can pytorch1.0.0 in win10 with cuda9.0 use tensorboard？,2020-01-04T14:22:49.978Z,3,970,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Now I am using pytorch1.0 in win10 with cuda9.0. I’d like to use tensorboard to visualize the training progress and check the network structure.</p><NewLine><p>However, I find pytorch1.3 have the official support for tensorboard.<br/><NewLine>But pytorch1.3 need cuda9.2.</p><NewLine><p>What should I do?</p><NewLine></div>",https://discuss.pytorch.org/u/Hong_Cheng,(Hong Cheng),Hong_Cheng,"January 4, 2020,  2:22pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you have the driver version &gt;= 398.26, then you could install pytorch 1.3. cuda 9.2 here doesn’t refer to the cuda version at the user side, but it means the version of cuda used during build.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><img alt=""image"" data-base62-sha1=""8hrqjqQ2jxzZBry9gxEi7CqVk0b"" height=""334"" src=""https://discuss.pytorch.org/uploads/default/original/3X/3/a/3a0a2496f70209d28897701180cdca250a3ec133.png"" width=""673""/><br/><NewLine>God, I use search tool in win10, and I find the nvidia-smi!<br/><NewLine>Here the driver version is 432, I’ll try to install pytorch1.3 in my PC.</p><NewLine><p>Thanks a lot</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""65747""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/hong_cheng/40/10639_2.png"" width=""20""/> Hong_Cheng:</div><NewLine><blockquote><NewLine><p>0, and I find the nvidia-smi!<br/><NewLine>Here the driver version is 432, I’ll try to install pytorch1.3 in my PC.</p><NewLine><p>Thanks a lot</p><NewLine></blockquote><NewLine></aside><NewLine><p>It works.<br/><NewLine>I install pytorch1.3.1 with cuda10 and cudnn7.5 in the GTX1080<br/><NewLine>And we need to install the tensorboard with version higher 1.14.0</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/peterjc123; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Hong_Cheng; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Hong_Cheng; <NewLine> ,"REPLY_DATE 1: January 5, 2020,  2:19pm; <NewLine> REPLY_DATE 2: January 4, 2020,  3:11pm; <NewLine> REPLY_DATE 3: January 5, 2020,  2:19pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
65648,Normalization for images and other input data,2020-01-03T07:37:13.583Z,0,72,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Here is my custom DATASET</p><NewLine><p>In each sample, I have the image data and other data (float number)<br/><NewLine>I hope I can do the normalization for <code>image</code>, <code>tr_angles</code> and <code>pls</code>.</p><NewLine><p>Thanks a lot.</p><NewLine><pre><code class=""lang-auto""><NewLine>class PLDataset(Dataset):<NewLine>    """"""Face Landmarks dataset.""""""<NewLine><NewLine>    def __init__(self, csv_file, root_dir, transform=None):<NewLine>        """"""<NewLine>        Args:<NewLine>            csv_file (string): Path to the csv file with annotations.<NewLine>            root_dir (string): Directory with all the images.<NewLine>            transform (callable, optional): Optional transform to be applied<NewLine>                on a sample.<NewLine>        """"""<NewLine>        self.landmarks_frame = pd.read_excel(csv_file)<NewLine>        self.root_dir = root_dir<NewLine>        self.transform = transform<NewLine><NewLine>    def __len__(self):<NewLine>        return len(self.landmarks_frame)<NewLine><NewLine>    def __getitem__(self, idx):<NewLine>        if torch.is_tensor(idx):<NewLine>            idx = idx.tolist()<NewLine><NewLine>        img_names = os.path.join(self.root_dir,<NewLine>                                 self.landmarks_frame.iloc[idx, 0])<NewLine>        image = io.imread(img_names)<NewLine><NewLine>        pls = self.landmarks_frame.iloc[idx, 5:9]  # not [:, 5:9]<NewLine>        tr_angles = self.landmarks_frame.iloc[idx, 9:14]<NewLine>        positions = self.landmarks_frame.iloc[idx, 3:5]<NewLine><NewLine>        image = image.astype('float')  # .values<NewLine>        pls = pls.values.astype('float')<NewLine>        tr_angles = tr_angles.values.astype('float')<NewLine>        positions = positions.values.astype('float')<NewLine><NewLine>        sample = {'image': image,<NewLine>                  'pls': pls,<NewLine>                  'tr_angles': tr_angles,<NewLine>                  'positions': positions}<NewLine><NewLine>        if self.transform:<NewLine>            sample = self.transform(sample)<NewLine><NewLine>        return sample<NewLine><NewLine></code></pre><NewLine><p>Now I just have my own <code>ToTensor()</code> function,</p><NewLine><pre><code class=""lang-auto"">class ToTensor(object):<NewLine>    """"""Convert ndarrays in sample to Tensors.""""""<NewLine><NewLine>    def __call__(self, sample):<NewLine>        image, pls, tr_angles, positions = \<NewLine>            sample['image'], sample['pls'], sample['tr_angles'], sample['positions']<NewLine>        image = image[np.newaxis, :, :]<NewLine>        # swap color axis because<NewLine>        # numpy image: H x W x C<NewLine>        # torch image: C X H X W<NewLine>        # image = image.transpose((2, 0, 1)) # single channel<NewLine>        return {'image': torch.from_numpy(image),<NewLine>                'pls': torch.from_numpy(pls),<NewLine>                'tr_angles': torch.from_numpy(tr_angles),<NewLine>                'positions': torch.from_numpy(positions)<NewLine>                }<NewLine><NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Hong_Cheng,(Hong Cheng),Hong_Cheng,"January 3, 2020,  7:37am",,,,,
65523,Examples utilizing tensor cores,2020-01-01T16:11:31.685Z,0,81,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi<br/><NewLine>I ran some tutorials such as <a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""nofollow noopener"">seq2seq</a>, <a href=""https://pytorch.org/tutorials/advanced/neural_style_tutorial.html"" rel=""nofollow noopener"">neural_style</a>, <a href=""https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html"" rel=""nofollow noopener"">spatial_transformer</a>, and others.</p><NewLine><p>Using the default code, no FP16 has been observed and tensor unit is idle according to the profiler’s report.<br/><NewLine>I would like to know what should I do in order to use them? All FP instructions are 32-bit.</p><NewLine></div>",https://discuss.pytorch.org/u/mahmoodn,(Mahmood Naderan),mahmoodn,"January 1, 2020,  4:11pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>To use TensorCores, you would have to apply some operations in FP16. You could transform (some) operations manually via <code>.half()</code> or I would recommend to use our mixed-precision approach in <a href=""https://github.com/NVIDIA/apex"">apex</a>.</p><NewLine><p>We are currently working on a direct PyTorch integration.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: January 1, 2020,  8:19pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
64158,Unable to convert PyTorch model to onnx,2019-12-16T06:15:54.508Z,2,705,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello. I have some troubles in converting my yolo model to onnx format.<br/><NewLine>I have next errors:</p><NewLine><pre><code class=""lang-auto"">C:\Users\1\PycharmProjects\untitled\my_utils.py:145: TracerWarning: There are 3 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.<NewLine>  pred[:, :, 0] = torch.sigmoid(pred[:, :, 0])<NewLine>C:\Users\1\PycharmProjects\untitled\my_utils.py:146: TracerWarning: There are 3 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.<NewLine>  pred[:, :, 1] = torch.sigmoid(pred[:, :, 1])<NewLine>C:\Users\1\PycharmProjects\untitled\my_utils.py:147: TracerWarning: There are 3 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.<NewLine>  pred[:, :, 4] = torch.sigmoid(pred[:, :, 4])<NewLine>C:\Users\1\PycharmProjects\untitled\my_utils.py:149: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<NewLine>  grid = np.arange(grid_size)<NewLine>C:\Users\1\PycharmProjects\untitled\my_utils.py:149: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<NewLine>  grid = np.arange(grid_size)<NewLine>C:\Users\1\PycharmProjects\untitled\my_utils.py:160: TracerWarning: There are 3 live references to the data region being modified when tracing in-place operator add_. This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.<NewLine>  pred[:, :, :2] += x_y_offset<NewLine>C:\Users\1\PycharmProjects\untitled\my_utils.py:160: TracerWarning: There are 5 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.<NewLine>  pred[:, :, :2] += x_y_offset<NewLine>C:\Users\1\PycharmProjects\untitled\my_utils.py:162: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<NewLine>  anchors = torch.FloatTensor(anchors)<NewLine>C:\Users\1\PycharmProjects\untitled\my_utils.py:167: TracerWarning: There are 3 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.<NewLine>  pred[:, :, 2:4] = torch.exp(pred[:, :, 2:4]) * anchors<NewLine>C:\Users\1\PycharmProjects\untitled\my_utils.py:169: TracerWarning: There are 3 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.<NewLine>  pred[:, :, 5:5 + num_classes] = torch.sigmoid(pred[:, :, 5:5 + num_classes])<NewLine>C:\Users\1\PycharmProjects\untitled\my_utils.py:170: TracerWarning: There are 3 live references to the data region being modified when tracing in-place operator mul_. This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.<NewLine>  pred[:, :, :4] *= stride<NewLine>C:\Users\1\PycharmProjects\untitled\my_utils.py:170: TracerWarning: There are 5 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.<NewLine>  pred[:, :, :4] *= stride<NewLine></code></pre><NewLine><p>What should I do to fix this problems. Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/thearheus,(Artem Zhukov),thearheus,"December 16, 2019,  6:15am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There cannot be any in-place assignment, like:</p><NewLine><pre><code class=""lang-auto"">pred[:, :, 0] = torch.sigmoid(pred[:, :, 0])<NewLine></code></pre><NewLine><p>I think that you need to modify it as:</p><NewLine><pre><code class=""lang-auto"">pred = torch.cat((torch.sigmoid(pred[:, :, 0:1]), pred[:, :, 1:]), dim=2)<NewLine></code></pre><NewLine><p>Anyway, slicing cannot occur on the left side of “=”</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, I still not good at pytorch. Thank you for your answer.<br/><NewLine>I’m interested, why did you change indexing in cat function from <code>[:, :, 0]</code> to <code>[:, :, 0:1]</code>, don’t understand that.</p><NewLine><p>I understand, how to fix most of this warnings, but still, don’t understand, how to fix this warning:</p><NewLine><pre><code class=""lang-auto"">C:\Users\1\PycharmProjects\untitled\util.py:84: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!<NewLine>  anchors = torch.FloatTensor(anchors)<NewLine></code></pre><NewLine><p>But anchors variable is just pythonic list. Thank you</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If pred.shape is (a, b, c), pred[:, :, 0].shape is (a, b), while pred[:, :, 0:1].shape is (a, b, 1). To keep the dimensions consistent in the concatenation, I think that pred[:, :, 0:1] should be used.</p><NewLine><p>The following code converts the Tensor to a Python float.</p><NewLine><pre><code class=""lang-auto"">anchors = torch.FloatTensor(anchors)<NewLine></code></pre><NewLine><p>As a result, the variable “anchors” will be treated as a constant when you use the ONNX model. If this is not your intention, you need to avoid this conversion, and always use Torch Tensor in computation.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/HeavyWeapon; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/thearheus; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/HeavyWeapon; <NewLine> ,"REPLY_DATE 1: December 27, 2019,  3:05am; <NewLine> REPLY_DATE 2: December 27, 2019,  7:45am; <NewLine> REPLY_DATE 3: December 30, 2019,  2:57am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
65294,Elementwise kernel,2019-12-29T17:11:00.373Z,0,167,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Using profiler I see that this kernel is in the top important kernels affecting gpu time.</p><NewLine><pre><code class=""lang-auto"">void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}&gt;(at::TensorIterator&amp;, at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1} const&amp;)::{lambda(int)#2}&gt;(int, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}&gt;(at::TensorIterator&amp;, at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1} const&amp;)::{lambda(int)#2})<NewLine></code></pre><NewLine><p>Looking at the code I see,</p><NewLine><pre><code class=""lang-auto"">template&lt;int nt, int vt, typename func_t&gt;<NewLine>C10_LAUNCH_BOUNDS_2(nt, launch_bound2)<NewLine>__global__ void elementwise_kernel(int N, func_t f) {<NewLine>  int tid = threadIdx.x;<NewLine>  int nv = nt * vt;<NewLine>  int idx = nv * blockIdx.x + tid;<NewLine>  #pragma unroll<NewLine>  for (int i = 0; i &lt; vt; i++) {<NewLine>    if (idx &lt; N) {<NewLine>      f(idx);<NewLine>      idx += nt;<NewLine>    }<NewLine>  }<NewLine>}<NewLine></code></pre><NewLine><p>So what is important there is the f(). I think it is <code>add_kernel_cuda</code>. Am I right?</p><NewLine></div>",https://discuss.pytorch.org/u/mahmoodn,(Mahmood Naderan),mahmoodn,"December 29, 2019,  5:11pm",,,,,
65163,I want to study Pytorch from scratch!,2019-12-27T15:08:39.431Z,0,121,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, everyone!</p><NewLine><p>When we learn or research deep learning and the other things related to ML or DL we use PyTorch…</p><NewLine><p>I’m also a user of Pytorch.</p><NewLine><p>Using PyTorch, I suddenly think that I want to study PyTorch from scratch!</p><NewLine><p>I’ve wondered how the tensor is defined and how the values are back-propagated.</p><NewLine><p>So, I’ve decided that I study Pytorch which version of either c++ or python.</p><NewLine><p>but, I don’t know where to start.</p><NewLine><p>Could you give me some help to learn Pytorch?<br/><NewLine>What is the first thing that I have to do?</p><NewLine><p>It is my future goal to join in PyTorch open-source development project in Github.</p><NewLine></div>",https://discuss.pytorch.org/u/sunshower76,(sunwoo Kim),sunshower76,"December 27, 2019,  5:01pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I would suggest to pick an easy looking issue on GitHub, post that you are interested and new, and either someone will guide you through the PR or could suggest another one, if your pick is not beginner friendly.<br/><NewLine>You will learn the internals of PyTorch while working on a real fix instead of staring at code, which sounds like more fun to me. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: December 28, 2019,  1:35am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
65130,How to convert F.interpolate to onnx?,2019-12-27T08:04:01.952Z,0,440,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve converted an east model into ONNX，but when checking model by <code>onnx.checker.check_model()</code>, I got the following error:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""east_onnx_demo.py"", line 223, in &lt;module&gt;<NewLine>    onnx.checker.check_model(net)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/onnx/checker.py"", line 86, in check_model<NewLine>    C.check_model(model.SerializeToString())<NewLine>onnx.onnx_cpp2py_export.checker.ValidationError: Node () has input size 4 not in range [min=2, max=2].<NewLine><NewLine>==&gt; Context: Bad node spec: input: ""901"" input: ""924"" input: ""924"" input: ""923"" output: ""925"" op_type: ""Resize"" attribute { name: ""coordinate_transformation_mode"" s: ""pytorch_half_pixel"" type: STRING } attribute { name: ""cubic_coeff_a"" f: -0.75 type: FLOAT } attribute { name: ""mode"" s: ""linear"" type: STRING } attribute { name: ""nearest_mode"" s: ""floor"" type: STRING }<NewLine></code></pre><NewLine><p>The problem could come from the following code:</p><NewLine><pre><code class=""lang-auto"">def forward(self, x1,x2,x3,x4):<NewLine>        y = F.interpolate(<NewLine>            x4, scale_factor=2.0, mode=""bilinear"", align_corners=False<NewLine>        )<NewLine>        y = torch.cat((y, x3), 1)<NewLine>        y = self.relu1(self.bn1(self.conv1(y)))<NewLine>        y = self.relu2(self.bn2(self.conv2(y)))<NewLine><NewLine>        y = F.interpolate(<NewLine>            y, scale_factor=2.0, mode=""bilinear"", align_corners=False<NewLine>        )<NewLine>        y = torch.cat((y, x2), 1)<NewLine>        y = self.relu3(self.bn3(self.conv3(y)))<NewLine>        y = self.relu4(self.bn4(self.conv4(y)))<NewLine><NewLine>        y = F.interpolate(<NewLine>            y, scale_factor=2.0, mode=""bilinear"", align_corners=False<NewLine>        )<NewLine>        y = torch.cat((y, x1), 1)<NewLine>        y = self.relu5(self.bn5(self.conv5(y)))<NewLine>        y = self.relu6(self.bn6(self.conv6(y)))<NewLine><NewLine>        y = self.relu7(self.bn7(self.conv7(y)))<NewLine>        return y<NewLine></code></pre><NewLine><p>Environment: Pytorch1.3.1, ONNX1.5.0</p><NewLine><p>How can I solve this problem?</p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"December 27, 2019,  8:04am",1 Like,,,,
65028,Strange GPU memory consumption in case of multiple models on one GPU,2019-12-26T08:11:38.073Z,0,92,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to deploy multiple models on one GPU in a single service. When I add one model on a GPU, the GPU memory consumption is 819M; when I add another model on the same GPU,  the GPU memory consumption increases just 84M, to 903M. Is it a normal phenomenon? Why?</p><NewLine><p>Below is the demo code, I use Pytorch1.0, Ubuntu 18.04.</p><NewLine><pre><code class=""lang-auto"">import PIL<NewLine>import time<NewLine>import torch<NewLine>import torchvision.models as models<NewLine>import torchvision.transforms as transforms<NewLine><NewLine><NewLine>class Classifier(object):<NewLine>    def __init__(self, gpuid):<NewLine>        self.device = torch.device(""cuda:{}"".format(gpuid))<NewLine>        self.model = models.densenet201(pretrained=True)<NewLine>        self.model.to(self.device)<NewLine>        self.model.eval()<NewLine><NewLine>        ### forward a fake sample to allocate GPU memory<NewLine>        fake = torch.zeros((1, 3, 224, 224), dtype=torch.float32)<NewLine>        self.model(fake.to(self.device)).cpu()<NewLine><NewLine><NewLine>c1 = Classifier(0)  # add one classifier, GPU memory: 819M<NewLine>c2 = Classifier(0)  # add another classifier, GPU memory: 903M<NewLine><NewLine>time.sleep(100)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Chen_Li,(Chen Li),Chen_Li,"December 26, 2019,  8:12am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Most likely the main part of the memory allocation after initializing the first model will be grabbed by the CUDA context for this device.<br/><NewLine>The second model will thus just use the necessary space for all parameters.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: December 26, 2019,  8:26am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
64936,How to get the first dimension size of tensor with onnx?,2019-12-25T01:10:20.539Z,0,211,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to check if the size of tensor <code>x</code>.<br/><NewLine>But when I used <code>len(x)</code>, I got error:</p><NewLine><pre><code class=""lang-auto"">ONNX export failed on ATen operator len because torch.onnx.symbolic_opset11.len does not exist<NewLine></code></pre><NewLine><p>and</p><NewLine><pre><code class=""lang-auto"">KeyError: 'len'<NewLine></code></pre><NewLine><p>When I used <code>x.size(0)</code>, I got error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: ONNX export failed on ne, which is not implemented for opset 11. Try exporting with other opset versions<NewLine></code></pre><NewLine><p>When I used <code>x.shape[0]</code>, I got error:</p><NewLine><pre><code class=""lang-auto"">UserWarning: ONNX export failed on ATen operator __getitem_ because torch.onnx.symbolic_opset11.__getitem_ does not exist<NewLine></code></pre><NewLine><p>and</p><NewLine><pre><code class=""lang-auto"">KeyError: '__getitem_'<NewLine></code></pre><NewLine><p>What can I do to solve this problem? Looking forward to your reply.</p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"December 25, 2019,  1:12am",,,,,
64858,Register onnx op failed,2019-12-24T02:29:19.767Z,0,291,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m registering custom ops from TorchScript to ONNX refer to <a href=""https://pytorch.org/docs/stable/onnx.html#custom-operators"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/onnx.html#custom-operators</a>, But I met the following error:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""onnx_demo.py"", line 23, in &lt;module&gt;<NewLine>    print(torch.ops.custom_ops.intersection)<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/_ops.py"", line 61, in __getattr__<NewLine>    op = torch._C._jit_get_operation(qualified_op_name)<NewLine>RuntimeError: No such operator custom_ops::intersection<NewLine></code></pre><NewLine><p>Here is my code:</p><NewLine><pre><code class=""lang-auto"">torch.ops.load_library(""build/libexample.so"")<NewLine><NewLine>def intersection(g,p):<NewLine>    return torch.ops.my_ops.iou(g[:8],p[:8])<NewLine><NewLine>print(torch.ops.my_ops.iou)<NewLine>register_custom_op_symbolic('custom_ops::intersection', intersection, 11)<NewLine># register_custom_op_symbolic('custom_ops::intersection', torch.ops.my_ops.iou, 11)<NewLine>print(torch.ops.custom_ops.intersection)<NewLine></code></pre><NewLine><p>libexample.so is the custom op file of TorchScript defined in C++. How can I convert it to ONNX op?</p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"December 24, 2019,  2:29am",,,,,
64786,How to use submodule in onnx?,2019-12-23T02:26:31.194Z,0,73,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m converting an ocr model to onnx through script-based onnx exporter like <a href=""https://pytorch.org/docs/stable/onnx.html#tracing-vs-scripting"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/onnx.html#tracing-vs-scripting</a></p><NewLine><p>But I met this error:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""ocr.py"", line 52, in &lt;module&gt;<NewLine>    class OCR(nn.Module):<NewLine>  File ""ocr.py"", line 59, in OCR<NewLine>    @torch.jit.script<NewLine>  File ""/home/dai/py36env/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1226, in script<NewLine>    fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))<NewLine>RuntimeError:<NewLine>Unknown builtin op: aten::east.<NewLine>Here are some suggestions:<NewLine>        aten::hash<NewLine>        aten::wait<NewLine>        aten::dist<NewLine>        aten::list<NewLine>        aten::cat<NewLine><NewLine>The original call is:<NewLine>at ocr.py:75:16<NewLine>        TGT_WIDTH = 800<NewLine>        if w_raw &gt; TGT_WIDTH:<NewLine>            ratio = w_raw / TGT_WIDTH<NewLine>            img_tensor = torch.ops.my_ops.resize(img_tensor.float(),TGT_WIDTH, int(h_raw / ratio),3)<NewLine>        img_tensor = resize_img(img_tensor)<NewLine>        # self.show(img_tensor)<NewLine><NewLine>        img_tensor_ = (img_tensor - 0.5) / 0.5<NewLine>        img_tensor_ = img_tensor_.permute(2,0,1).unsqueeze(0) # --&gt; n x c x h x w<NewLine>        boxes = self.east(img_tensor_)<NewLine>                ~~~~~~~~~ &lt;--- HERE<NewLine>        # self.show(img_tensor,boxes)<NewLine>        new_boxes = []<NewLine>        pass_list:List[int] = []<NewLine>        for i in range(boxes.size(0)):<NewLine>            for j in range(i+1,boxes.size(0)):<NewLine>                if i in pass_list or j in pass_list:<NewLine>                    continue<NewLine>                cx1,cy1,angle1 = box_analyse(boxes[i])<NewLine>                cx2,cy2,angle2 = box_analyse(boxes[j])<NewLine></code></pre><NewLine><p>self.east is a detection model, how can I export the whole ocr model(detection and recognition model) to onnx?</p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"December 23, 2019,  2:26am",,,,,
64465,Elementwise_kernel kernel,2019-12-18T20:26:37.341Z,0,90,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have profiled a gan network and see that the following kernel is the most important one:</p><NewLine><pre><code class=""lang-auto"">_ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_EEvS4_RKT_EUliE0_EEviT1_<NewLine></code></pre><NewLine><p>Using <a href=""http://demangler.com"" rel=""nofollow noopener"">demangler.com</a>, the real name is</p><NewLine><pre><code class=""lang-auto"">void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}&gt;(at::TensorIterator&amp;, at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1} const&amp;)::{lambda(int)#2}&gt;(int, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}&gt;(at::TensorIterator&amp;, at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1} const&amp;)::{lambda(int)#2})<NewLine></code></pre><NewLine><p>When I grep <code>elementwise_kernel</code>, the implementation is in <code>./aten/src/ATen/native/cuda/Loops.cuh</code> as below</p><NewLine><pre><code class=""lang-auto"">template&lt;int nt, int vt, typename func_t&gt;<NewLine>C10_LAUNCH_BOUNDS_2(nt, launch_bound2)<NewLine>__global__ void elementwise_kernel(int N, func_t f) {<NewLine>  int tid = threadIdx.x;<NewLine>  int nv = nt * vt;<NewLine>  int idx = nv * blockIdx.x + tid;<NewLine>  #pragma unroll<NewLine>  for (int i = 0; i &lt; vt; i++) {<NewLine>    if (idx &lt; N) {<NewLine>      f(idx);<NewLine>      idx += nt;<NewLine>    }<NewLine>  }<NewLine>}<NewLine></code></pre><NewLine><p>So, the important thing is <code>func_t f</code>. From the demangler, I am confused with the function name. which one is correct?<br/><NewLine>gpu_kernel_impl<br/><NewLine>or<br/><NewLine>add_kernel_cuda<br/><NewLine>?</p><NewLine><p>Any thought?</p><NewLine></div>",https://discuss.pytorch.org/u/mahmoodn,(Mahmood Naderan),mahmoodn,"December 18, 2019,  8:26pm",,,,,
64129,High memory usage while building PyTorch from source,2019-12-15T18:38:41.337Z,1,117,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How can I reduce the RAM usage of compilation from source via <code>python setup.py install</code> command? It automatically spawns many threads and I don’t want that!</p><NewLine></div>",https://discuss.pytorch.org/u/mahmoodn,(Mahmood Naderan),mahmoodn,"December 15, 2019,  6:38pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Try to set the <code>MAX_JOBS</code> argument to your desired max. number of jobs.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is that a shell variable?<br/><NewLine>export MAX_JOBS=2<br/><NewLine>?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes. You could also set it in front of the install command: <code>MAX_JOBS=2 python setup.py install</code>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mahmoodn; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: December 15, 2019,  7:11pm; <NewLine> REPLY_DATE 2: December 16, 2019,  7:48pm; <NewLine> REPLY_DATE 3: December 17, 2019,  8:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
64246,Undefined symbol: _Py_ZeroStruct,2019-12-16T19:55:45.268Z,0,761,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Although I have successfully installed pytorch with both python2.7 and python3, when I run a program, I get this error:</p><NewLine><pre><code class=""lang-auto"">$ python3 main.py<NewLine>Traceback (most recent call last):<NewLine>  File ""main.py"", line 3, in &lt;module&gt;<NewLine>    import torch<NewLine>  File ""/home/mnaderan/.local/lib/python3.6/site-packages/torch/__init__.py"", line 81, in &lt;module&gt;<NewLine>    from torch._C import *<NewLine>ImportError: /home/mnaderan/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so: undefined symbol: _Py_ZeroStruct<NewLine>Segmentation fault (core dumped)<NewLine></code></pre><NewLine><p>The output of <code>python3 setup.py install --user</code> is <a href=""https://pastebin.com/MzrGE5G5"" rel=""nofollow noopener"">here</a> and <a href=""https://pastebin.com/Py43ra2P"" rel=""nofollow noopener"">here</a>.</p><NewLine></div>",https://discuss.pytorch.org/u/mahmoodn,(Mahmood Naderan),mahmoodn,"December 16, 2019,  7:55pm",,,,,
63997,How do you save your model&rsquo;s code?,2019-12-13T18:25:05.988Z,1,116,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,<br/><NewLine>Coming from Keras (TensorFlow), I used to have a somewhat robust way to save/load models trained in previous development iterations. Now using PyTorch, I learned that it’s preferable to save only the model parameters contained in the model.state_dict(). I would like to be able to edit my model code, retrain the model and then perform various experiments on this last model and also its predecessors. I can only think of two naive approaches:</p><NewLine><ol><NewLine><li>Have a model definition function that supports a version argument. Save the model version with the model state dict and, during loading, use it to build the good model.</li><NewLine></ol><NewLine><p>pros: explicit<br/><NewLine>cons: would get messy pretty quickly, would require lots of discipline to keep the old code unchanged</p><NewLine><ol start=""2""><NewLine><li>Save the code of the model with the model state dict. During loading, dynamically use this code to build the model.</li><NewLine></ol><NewLine><p>pros: less messy<br/><NewLine>cons: would probably get complex if the model definition function and its dependencies don’t fit in a single file (which, excluding libraries, is not my case for now)</p><NewLine><p>Cleary both approaches could still break if I change my python environment (update the project dependencies) but I guess I can live with that. Do you think these approaches are viable? Do you know better?</p><NewLine></div>",https://discuss.pytorch.org/u/MLaurenceFournier,(M Laurence Fournier),MLaurenceFournier,"December 13, 2019,  7:53pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think it is possible to save raw written code. Torchscript can be used, where the model is converted to torchscript and that can be saved.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you. I will definitely look into torchscript eventually. For now, I just save the code of my model building function as a string (obtains with the inspect module). When loading, I use exec. This is probably not a really clean way to serialize a function but it works for my needs. The dill package might be a better way. I also realized my question is really specific to the way I work, where each revision only defines one version of the model.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Kushaj; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/MLaurenceFournier; <NewLine> ,"REPLY_DATE 1: December 14, 2019,  3:44pm; <NewLine> REPLY_DATE 2: December 16, 2019,  5:37pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
62906,How to save memory by doing op like x = conv(x) while inferencing,2019-12-03T17:45:45.248Z,3,211,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am trying to run my code in “torch.no_grad” and “eval” model. I want to save memory via operations like x = conv(x). But while I running the following code:</p><NewLine><pre><code class=""lang-auto"">x = self.conv1(x)<NewLine>x = self.conv2(x)<NewLine></code></pre><NewLine><p>I found it would  consume 3 times memory as much as x, it seems that the first x is not be replaced. But Ideally, it could be reduced to 2 times (since conv operations need extra memory). I have finished the training process, how could I free the memory of the first x after ops like x = conv(x)?<br/><NewLine>thx!</p><NewLine></div>",https://discuss.pytorch.org/u/111197,(非接触无线供电轨道试验车系统 张军磊),111197,"December 3, 2019,  5:45pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I doubt you can save more memory than is already saved by wrapping the code in a <code>with torch.no_grad()</code> block. The output of <code>conv1</code> has to be allocated at one place during the forward pass.<br/><NewLine>How many output channels are you using?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, I didn’t make myself clear. The code is like this:</p><NewLine><pre><code class=""lang-auto"">def forward(self, x):<NewLine>    x = conv1(x)<NewLine>    x = conv2(x)<NewLine>  <NewLine></code></pre><NewLine><p>After calculating the conv1, the memory of the very first x should be free since it is useless anynore. But pytorch dose not free it. Is any solution to free the input x after the conv1.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If <code>x</code> is not needed anymore, PyTorch will free it and reuse the memory.<br/><NewLine>In case you doubt it, you could try to assign the result of <code>conv1</code> to another variable and delete <code>x</code> manually.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually I test it based on a resnet architecture.<br/><NewLine>The whole code would like this:</p><NewLine><pre><code class=""lang-auto"">Class BasicBlock(nn.Module):<NewLine>    def __init__():<NewLine>        super(BasicBlock, self).__init__()<NewLine>        self.conv1 = nn.Conv2d(64, 64, 3)<NewLine>        self.conv2 = nn.Conv2d(64, 64, 3)<NewLine>    def forward(self, x):<NewLine>        out = self.conv1(x)<NewLine>        out = self.conv2(x)<NewLine>    return x<NewLine>class WholeModel(nn.Module):<NewLine>    def __init__():<NewLine>        super(WholeModel, self).__init__()<NewLine>        self.block1 = BasicBlock()<NewLine>        self.block2 = BasicBlock()<NewLine>    def forward(self, x):<NewLine>        out = self.block1(x)<NewLine>        out = self.block2(out)<NewLine></code></pre><NewLine><p>While runing the “out = self.block1(x)”, it saves x. And after the first conv in block1: “out = self.conv1(x)”. x is useless anymore. But the input x will not be freed until “out = self.block1(x)” is finished. I think it could be freed earlier since it is useless anymore after the conv1 in block1 is finished. I tried to “del x” after the “out = self.conv1(x)”, but it is obviously useless. Do you have any ideas about how to free the  memory just after the “out  = self.conv1(x)”? Thank youx</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/111197; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/111197; <NewLine> ,"REPLY_DATE 1: December 4, 2019,  5:49am; <NewLine> REPLY_DATE 2: December 9, 2019,  4:28am; <NewLine> REPLY_DATE 3: December 9, 2019,  4:30am; <NewLine> REPLY_DATE 4: December 9, 2019,  6:17am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
63046,How to save Pytorch model (graph + weights) in a single file,2019-12-04T17:16:33.726Z,1,520,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have saved the model using the torch.save(_model, PATH1) function and weights in torch.save(‘model_state_dict’: _model.state_dict(), PATH2).</p><NewLine><p>I want to load the model from another system.</p><NewLine><p>When I load the “_model” I get the error saying that the model architecture is not defined (Can’t get attribute ‘Net’) and had to instantiate the class that creates the model and then load the weights from the checkpoint _model.state_dict in path2.</p><NewLine><p>Is there a way to save both the weights and architecture in a file so that I need not depend on the class definition file, to first create the model object and then load the weights from state dict?</p><NewLine><p>I am looking for a solution similar to what Lua+Torch implementation of torch.save(model, path) does.</p><NewLine><p><a class=""mention"" href=""/u/alband"">@albanD</a> <a class=""mention"" href=""/u/smth"">@smth</a> <a class=""mention"" href=""/u/apaszke"">@apaszke</a> <a class=""mention"" href=""/u/colesbury"">@colesbury</a></p><NewLine></div>",https://discuss.pytorch.org/u/sujay,(sujay),sujay,"December 4, 2019,  5:26pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Unfortunately this is a python limitation. You cannot save the code easily and in a portable way.</p><NewLine><p>You can try to save a torchscript model that you get from the jit if your model can be handled by torchscript.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks for clarifying</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/sujay; <NewLine> ,"REPLY_DATE 1: December 6, 2019,  5:23am; <NewLine> REPLY_DATE 2: December 5, 2019,  5:05am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
63076,Multithreaded inference,2019-12-05T03:46:12.100Z,0,166,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a deployment scenario where it is required that multiple python threads should be using the same model for inference.<br/><NewLine>How should I modify/structure the inference code so that multiple threads can use the same mode?</p><NewLine><p>What I am observing is that some of the threads are failing with the following error<br/><NewLine><strong>The expanded size of the tensor (100) must match the existing size (3) at non-singleton dimension 1.</strong><br/><NewLine>This error does not happen when the code is run in a sequential manner.</p><NewLine></div>",https://discuss.pytorch.org/u/MRK,,MRK,"December 5, 2019,  3:46am",,,,,
62681,How to reshape last layer of pytorch CNN model while doing transfer learning,2019-12-01T21:14:32.161Z,2,1054,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Actually i am trying to replicate keras structure to pytorch.(new in pytorch).<br/><NewLine>Here is keras architecture</p><NewLine><pre><code>base_model = InceptionV3(weights='imagenet', include_top=False)<NewLine>x = base_model.output<NewLine>x = Dense(512, activation='relu')(x)<NewLine>predictions = Dense(49*6,activation='sigmoid')(x)<NewLine>reshape=Reshape((49,6))(predictions)<NewLine>model = Model(inputs=base_model.input, outputs=reshape)<NewLine>for layer in base_model.layers:<NewLine>    layer.trainable = False<NewLine></code></pre><NewLine><p>I want to reshape last layer of my netwrok. I have implemented transfer learning.</p><NewLine><pre><code>model =  models.inception_v3(pretrained=True)<NewLine><NewLine>for param in model.parameters():<NewLine>    param.requires_grad = False<NewLine>    <NewLine>num_ftrs = model.fc.in_features<NewLine></code></pre><NewLine><p>I believe if i can attach last layer of resnet with my following architecture, problem can be solved. But i dont know how i can attach them</p><NewLine><pre><code>class CNN(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(CNN, self).__init__()<NewLine>        self.fc1 = nn.Linear(num_ftrs, 512)<NewLine>        self.fc2 = nn.Linear(512, 49*6)<NewLine><NewLine>    def forward(self, x):<NewLine>        print (x.shape)<NewLine>        x = x.view(-1,num_ftrs)<NewLine>        #print (x.shape)<NewLine>        x = F.relu(self.fc1(x))<NewLine>        x = self.fc2(x)<NewLine>        x=torch.sigmoid(x.view(49,6))<NewLine>        return x<NewLine></code></pre><NewLine><p>Any idea, how this problem can be resolved</p><NewLine></div>",https://discuss.pytorch.org/u/talhaanwarch,(Talha Anwar),talhaanwarch,"December 1, 2019,  9:14pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could add your custom linear layers to <code>model.fc</code> as shown here:</p><NewLine><pre><code class=""lang-python"">model =  models.inception_v3()<NewLine>fc_feat = model.fc.in_features<NewLine>model.fc = nn.Sequential(<NewLine>    nn.Linear(fc_feat, 512),<NewLine>    nn.ReLU(),<NewLine>    nn.Linear(512, 49*6),<NewLine>    nn.Sigmoid(),<NewLine>)<NewLine></code></pre><NewLine><p>Let me know, if that would work for your use case.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>there is no reshaping of last layer output, i want 49,6 instead of 49*6</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can reshape the output tensor of shape <code>(49*6, )</code> to <code>(49, 6)</code>.<br/><NewLine>Simply do:</p><NewLine><pre><code class=""lang-auto"">output = model(input)<NewLine>output = output.view(-1, 49, 6)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/talhaanwarch; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mailcorahul; <NewLine> ,"REPLY_DATE 1: December 2, 2019,  3:55am; <NewLine> REPLY_DATE 2: December 2, 2019,  1:09pm; <NewLine> REPLY_DATE 3: December 2, 2019,  2:03pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
62557,Unbale to solve RuntimeError: Expected object of backend CPU but got backend CUDA for argument #2 &lsquo;weight&rsquo;,2019-11-30T09:47:15.508Z,0,99,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Greetings pytoch community. I am getting an error Expected object of backend CPU but got backend CUDA for argument <span class=""hashtag"">#2</span> ‘weight’. As I searched on internet i have added <code>to(device) </code> to my model and my variables <code>var_X_batch </code> aand <code>var_y_batch </code>. But still i am getting error. Any help ?</p><NewLine><pre><code class=""lang-auto"">import numpy as np<NewLine>    X=[]<NewLine>    Y=[]<NewLine>    for i in range(700):<NewLine>        X.append(np.load('X_{}.npy'.format(i)))<NewLine>        Y.append(np.load('Y_{}.npy'.format(i)))<NewLine>    <NewLine>    X=np.array(X)<NewLine>    Y=np.array(Y)<NewLine><NewLine>#X.shape is (700,448,448,3)<NewLine>#y.shape is (700,49,5)<NewLine><NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.utils.data<NewLine>from torch.autograd import Variable<NewLine><NewLine>device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")<NewLine>print(device)<NewLine><NewLine>def custom_loss(y_pred,y_true):<NewLine><NewLine><NewLine>    for t,p in zip(y_true,y_pred):<NewLine>        t=t.view(49,5)<NewLine>        p=p.view(49,5)<NewLine>        idx=torch.unique(t.nonzero()[:,0])<NewLine><NewLine>        xt=t[idx,0]<NewLine>        xp=p[idx,0]<NewLine>        yt=t[idx,1]<NewLine>        yp=p[idx,1]<NewLine>        eq1=torch.sum((xt-xp)**2 +(yt-yp)**2,dim=0  )<NewLine>        wt=t[idx,2]<NewLine>        wp=p[idx,2]<NewLine>        ht=t[idx,3]<NewLine>        hp=p[idx,3]<NewLine>        eq2=torch.sum((torch.sqrt(torch.abs(wt))-torch.sqrt(torch.abs(wp))**2) + (torch.sqrt(torch.abs(ht))-torch.sqrt(torch.abs(hp))**2))<NewLine><NewLine>        ct=t[idx,4]<NewLine>        cp=p[idx,4]<NewLine>        eq3=torch.sum((ct-cp)**2)<NewLine>        res=eq1+eq2+10*eq3<NewLine>        print('res',res)<NewLine>    return res<NewLine><NewLine><NewLine><NewLine><NewLine><NewLine>BATCH_SIZE = 5<NewLine><NewLine>torch_X = torch.from_numpy(X)<NewLine>torch_X = torch_X.permute(0, 3, 1, 2)<NewLine><NewLine>y=np.reshape(Y,(Y.shape[0],-1))<NewLine>torch_y = torch.from_numpy(y)<NewLine><NewLine># Pytorch train<NewLine>train = torch.utils.data.TensorDataset(torch_X,torch_y)<NewLine><NewLine># data loader<NewLine>train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)<NewLine><NewLine><NewLine><NewLine>def fit(model, train_loader):<NewLine>    optimizer = torch.optim.Adam(model.parameters())<NewLine>    EPOCHS = 50<NewLine>    model.train()<NewLine>    <NewLine>    for epoch in range(EPOCHS):<NewLine>        correct = 0<NewLine>        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):<NewLine>            var_X_batch = X_batch.to(device)<NewLine>            var_y_batch = y_batch.to(device)<NewLine>            optimizer.zero_grad()<NewLine>            output = model(var_X_batch)<NewLine>            loss = custom_loss(output, var_y_batch)<NewLine>            loss.backward()<NewLine>            optimizer.step()<NewLine><NewLine><NewLine>            correct += (output.data == var_y_batch).sum()<NewLine>            if batch_idx % 50 == 0:<NewLine>                print('Epoch : {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}\t Accuracy:{:.3f}%'.format(<NewLine>                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.data.item(), float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))<NewLine>                <NewLine><NewLine><NewLine>class CNN(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(CNN, self).__init__()<NewLine>        <NewLine>        self.conv_block = nn.Sequential(<NewLine>            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),<NewLine>            nn.BatchNorm2d(32),<NewLine>            nn.ReLU(inplace=True),<NewLine>            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),<NewLine>            nn.BatchNorm2d(64),<NewLine>            nn.ReLU(inplace=True),<NewLine>            nn.MaxPool2d(kernel_size=2, stride=2),<NewLine>            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),<NewLine>            nn.BatchNorm2d(128),<NewLine>            nn.ReLU(inplace=True),<NewLine>            nn.MaxPool2d(kernel_size=2, stride=2) <NewLine>        )<NewLine>        <NewLine>        self.linear_block = nn.Sequential(<NewLine>            nn.Dropout(p=0.5),<NewLine>            nn.Linear(128*112*112, 128),<NewLine>            nn.BatchNorm1d(128),<NewLine>            nn.ReLU(inplace=True),<NewLine>            nn.Dropout(0.5),<NewLine>            nn.Linear(128, 64),<NewLine>            nn.BatchNorm1d(64),<NewLine>            nn.ReLU(inplace=True),<NewLine>            nn.Dropout(0.5),<NewLine>            nn.Linear(64, 245)<NewLine>        )<NewLine>        <NewLine>    def forward(self, x):<NewLine>        x = self.conv_block(x)<NewLine>        print(x.shape)<NewLine>        x = x.view(x.size(0), -1)<NewLine>        x = self.linear_block(x)<NewLine>        <NewLine>        return x<NewLine><NewLine>cnn = CNN().to(device)<NewLine>it = iter(train_loader)<NewLine>X_batch, y_batch = next(it)<NewLine>print(cnn.forward(X_batch).shape)<NewLine><NewLine><NewLine>fit(cnn,train_loader)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/talhaanwarch,(Talha Anwar),talhaanwarch,"November 30, 2019,  9:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This line of code will throw the error:</p><NewLine><pre><code class=""lang-python"">print(cnn.forward(X_batch).shape)<NewLine></code></pre><NewLine><p>, since you didn’t transferred <code>X_batch</code> to the GPU yet (as is done in your <code>fit</code> method).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: December 1, 2019,  2:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
62537,Using non-system compiler and undefined references to pthread,2019-11-30T04:16:53.426Z,0,274,"<div class=""post"" itemprop=""articleBody""><NewLine><p>On a Centos which has gcc 4.8.5 by default, I have built gcc-6.1.0 from source and want to tell pytorch to use that.</p><NewLine><p>I have specified CC and CXX as below:</p><NewLine><pre><code class=""lang-auto"">$ export CC=/storage/users/mnaderan/tools/gcc-6.1.0/bin/gcc<NewLine>$ export CXX=/storage/users/mnaderan/tools/gcc-6.1.0/bin/g++<NewLine></code></pre><NewLine><p>However, I get this error:</p><NewLine><pre><code class=""lang-auto"">Performing C SOURCE FILE Test CMAKE_HAVE_LIBC_PTHREAD failed with the following output:<NewLine>Change Dir: /storage/users/mnaderan/pt/pytorch/build/CMakeFiles/CMakeTmp<NewLine><NewLine>Run Build Command(s):/bin/gmake cmTC_bceed/fast &amp;&amp; gmake: Warning: File `Makefile' has modification time 19 s in the future<NewLine>/bin/gmake -f CMakeFiles/cmTC_bceed.dir/build.make CMakeFiles/cmTC_bceed.dir/build<NewLine>gmake[1]: Entering directory `/storage/users/mnaderan/pt/pytorch/build/CMakeFiles/CMakeTmp'<NewLine>gmake[1]: Warning: File `CMakeFiles/cmTC_bceed.dir/flags.make' has modification time 19 s in the future<NewLine>Building C object CMakeFiles/cmTC_bceed.dir/src.c.o<NewLine>/storage/users/mnaderan/tools/gcc-6.1.0/bin/gcc   -DCMAKE_HAVE_LIBC_PTHREAD -fPIE   -o CMakeFiles/cmTC_bceed.dir/src.c.o   -c /storage/users/mnaderan/pt/pytorch/build/CMakeFiles/CMakeTmp/src.c<NewLine>Linking C executable cmTC_bceed<NewLine>/storage/users/mnaderan/tools/cmake-3.15.4/bin/cmake -E cmake_link_script CMakeFiles/cmTC_bceed.dir/link.txt --verbose=1<NewLine>/storage/users/mnaderan/tools/gcc-6.1.0/bin/gcc  -DCMAKE_HAVE_LIBC_PTHREAD   -rdynamic  CMakeFiles/cmTC_bceed.dir/src.c.o  -o cmTC_bceed<NewLine>CMakeFiles/cmTC_bceed.dir/src.c.o: In function `main':<NewLine>src.c:(.text+0x2f): undefined reference to `pthread_create'<NewLine>src.c:(.text+0x3b): undefined reference to `pthread_detach'<NewLine>src.c:(.text+0x4c): undefined reference to `pthread_join'<NewLine>src.c:(.text+0x60): undefined reference to `pthread_atfork'<NewLine>collect2: error: ld returned 1 exit status<NewLine></code></pre><NewLine><p>Any idea?</p><NewLine></div>",https://discuss.pytorch.org/u/mahmoodn,(Mahmood Naderan),mahmoodn,"November 30, 2019,  4:16am",,,,,
62381,Size dismatch when use multi-cuda(torch.nn.DataParallel),2019-11-28T06:36:16.616Z,0,82,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi~ I’m using a custom model like this</p><NewLine><pre><code class=""lang-auto"">class SimpleNN(nn.Module):<NewLine>    def __init__(self, vectors_size, features_size, hidden_size=15, dropout_rate=0.1):<NewLine>        super(SimpleNN, self).__init__()<NewLine><NewLine>        self.vectors_size = vectors_size<NewLine>        self.features_size = features_size<NewLine>        self.hidden_size = hidden_size<NewLine>        self.dropout_rate = dropout_rate<NewLine><NewLine>        self.vectors_hidden = nn.Sequential(<NewLine>            nn.Dropout(self.dropout_rate),<NewLine>            nn.Linear(vectors_size, vectors_size//2),<NewLine>            nn.Tanh(),<NewLine>            nn.Linear(vectors_size//2, features_size),<NewLine>            nn.Tanh()<NewLine>        )<NewLine>        self.hidden = nn.Sequential(<NewLine>            nn.Linear(features_size*2, hidden_size),<NewLine>            nn.ReLU(),<NewLine>        )<NewLine><NewLine>        self.output = nn.Linear(hidden_size, 2)<NewLine><NewLine>    def forward(self, pairs, features):<NewLine>        """"""<NewLine>        features: (n_samples, features_size)<NewLine>        """"""<NewLine>        vectors = pairs2vectors(train_pub, pairs).to(device)<NewLine>        embedding_features = self.vectors_hidden(vectors)<NewLine>        combined_features = torch.cat([features, embedding_features], dim=1)<NewLine>        return self.output(self.hidden(combined_features))<NewLine></code></pre><NewLine><p>This model works well when i use only one cuda, but after ‘DataParallel’ used like below, it always tell me the size of  <code>features</code>  and  <code>embedding_features</code>  are not match, i find that the n_samples shape of features doesn’t follow my expectation just like another batch data, i dont know why and how to solve this problem.</p><NewLine><pre><code class=""lang-auto"">if torch.cuda.device_count() &gt; 1:<NewLine>        print(""Let's use"", torch.cuda.device_count(), ""GPUs!"")<NewLine>        model = nn.DataParallel(model)<NewLine></code></pre><NewLine><p>BTW, here is the pic of error message is <a href=""https://i.stack.imgur.com/ZNb8x.png"" rel=""nofollow noopener"">Error message</a></p><NewLine></div>",https://discuss.pytorch.org/u/t6am3,(Ivan Rufus),t6am3,"November 28, 2019,  6:36am",,,,,
61921,How to use DataParallel correctly?,2019-11-23T03:38:03.709Z,0,102,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I tested the code on <a href=""https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html#sphx-glr-beginner-blitz-data-parallel-tutorial-py"" rel=""nofollow noopener"">https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html#sphx-glr-beginner-blitz-data-parallel-tutorial-py</a>. It works well.<br/><NewLine>But when I changed the code</p><NewLine><p>class Model(nn.Module):<br/><NewLine># Our model</p><NewLine><pre><code>def __init__(self, input_size, output_size):<NewLine>    super(Model, self).__init__()<NewLine>    self.fc = nn.Linear(input_size, output_size)<NewLine><NewLine>def forward(self, input):<NewLine>    output = self.fc(input)<NewLine>    print(""\tIn Model: input size"", input.size(),<NewLine>          ""output size"", output.size())<NewLine><NewLine>    return output<NewLine></code></pre><NewLine><p>model = Model(input_size, output_size)</p><NewLine><p>to</p><NewLine><p>model = nn.Linear(input_size, output_size)</p><NewLine><p>,the result became to <div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/12c9307a47c71ab2db2363f43cb6b6a621c20403"" href=""https://discuss.pytorch.org/uploads/default/original/3X/1/2/12c9307a47c71ab2db2363f43cb6b6a621c20403.png"" title=""image""><img alt=""image"" data-base62-sha1=""2GbDIPU4KO2WhP5bP104SJFdG1l"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/1/2/12c9307a47c71ab2db2363f43cb6b6a621c20403_2_10x10.png"" height=""147"" src=""https://discuss.pytorch.org/uploads/default/original/3X/1/2/12c9307a47c71ab2db2363f43cb6b6a621c20403.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">718×154 3.34 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div> . WHY?</p><NewLine></div>",https://discuss.pytorch.org/u/Hitori940101,(Hitori940101),Hitori940101,"November 23, 2019,  3:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you wrap your linear layer in <code>nn.DataParallel</code> as well?<br/><NewLine>Since you are just using an <code>nn.Linear</code> module, the input sizes won’t be printed, which is defined in the original <code>forward</code> method.</p><NewLine><p>Let me know, if I misunderstood your question or what your confusion is about.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: November 24, 2019,  7:52am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
61854,Reduce GPU memory when loading the model for inference,2019-11-22T11:03:14.652Z,0,394,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>When I load the model for inference. The model always consumes a lot of memory even the model size is small. For example: model size is 70MB (Encoder + Decoder + attention with Resnet 50 as backbone for encoder) but it occupies approximately 1GB GPU memory.<br/><NewLine>Can we have any ways to reduce the GPU memory when loading the model for inference?</p><NewLine><p>Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/netpcvnn,(L H),netpcvnn,"November 22, 2019, 11:03am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m afraid there is not much we can do.<br/><NewLine>You can check this other topic on the same subject: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/moving-a-tiny-model-to-cuda-causes-a-2gb-host-memory-allocation/61282"">Moving a tiny model to cuda causes a 2Gb host memory allocation</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>How about use of float16 instead of float32?, this conversion makes room twice.</p><NewLine><p>Best,<br/><NewLine>S.Takano</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks all for supports.<br/><NewLine>with Float16, I have used Apex (mixed precision) for training, and the model size is reduced to 50 MB, but when I load the model to GPU, it still consumes 950 MB.<br/><NewLine>Look likes I need to see more for CUDA context for this issue.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/111137; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/netpcvnn; <NewLine> ,"REPLY_DATE 1: November 22, 2019,  3:04pm; <NewLine> REPLY_DATE 2: November 22, 2019,  5:52pm; <NewLine> REPLY_DATE 3: November 23, 2019,  2:51am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
61818,Using scheduler lead to out of memory,2019-11-22T03:06:14.411Z,5,198,"<div class=""post"" itemprop=""articleBody""><NewLine><p>my gpu memory is increasing in circulation of define scheduler</p><NewLine><pre><code class=""lang-auto"">from torch.optim import lr_scheduler<NewLine>import torch.nn as nn<NewLine>import torch<NewLine>class network(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        nn.Module.__init__(self)<NewLine>        self.layer=nn.Sequential(<NewLine>            nn.Linear(4096, 2048),<NewLine>            nn.ReLU(),<NewLine>            nn.Linear(2048, 1024),<NewLine>            nn.ReLU(),<NewLine>            nn.Linear(1024, 512),<NewLine>            nn.ReLU(),<NewLine>        )<NewLine>    def forward(self, ftr):<NewLine>        pass<NewLine>device=torch.device('cuda:0')<NewLine><NewLine><NewLine>for i in range(100**100):<NewLine><NewLine>    net = network().to(device)<NewLine>    optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9,<NewLine>                                weight_decay=0.9)<NewLine>    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.9)<NewLine><NewLine>    for epoch in range(2):<NewLine>        optimizer.step()<NewLine>        scheduler.step()<NewLine><NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/clearlovewl,(Clearlovewl),clearlovewl,"November 22, 2019,  5:34am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The code which i use actually is more complex. This simple one is still have the problem. The first time when i use lr_scheduler function ,my gpu memory is low. While As the iteration goes on, this code need more gpu memory. I don’t know why it would be like this, but when a del the lr_scheduler, the increasing of gpu memory disappear. Is there somebody watch this quesion? I really need help.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the executable code snippet!<br/><NewLine>I failed to reproduce the issue with <code>1.4.0.dev20191109</code>, as my memory usage stays constant.<br/><NewLine>Which PyTorch version are you using?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>i use pytorch 1.2.0 ,which was download from the <a href=""https://pytorch.org/"" rel=""nofollow noopener"">https://pytorch.org/</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>This code may be more clear</p><NewLine><pre><code class=""lang-auto"">from torch.optim import lr_scheduler<NewLine>import torch.nn as nn<NewLine>import torch<NewLine>class network(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        nn.Module.__init__(self)<NewLine>        self.layer=nn.Sequential(<NewLine>            nn.Linear(4096, 2048),<NewLine>            nn.ReLU(),<NewLine>            nn.Linear(2048, 1024),<NewLine>            nn.ReLU(),<NewLine>            nn.Linear(1024, 512),<NewLine>            nn.ReLU(),<NewLine>        )<NewLine>    def forward(self, ftr):<NewLine>        pass<NewLine>device=torch.device('cuda:0')<NewLine><NewLine><NewLine>for i in range(100**100):<NewLine>    print(i)<NewLine>    net = network().to(device)<NewLine>    optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9,<NewLine>                                weight_decay=0.9)<NewLine>    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.9)<NewLine><NewLine>    for epoch in range(1000*100):<NewLine><NewLine>        optimizer.step()<NewLine>        scheduler.step()<NewLine><NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you update to the latest stable release (<code>1.3.1</code>) and rerun the code again?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>yes, it is solved. Thank you  very much.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/clearlovewl; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/clearlovewl; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/clearlovewl; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/clearlovewl; <NewLine> ,"REPLY_DATE 1: November 22, 2019,  5:43am; <NewLine> REPLY_DATE 2: November 22, 2019,  5:47am; <NewLine> REPLY_DATE 3: November 22, 2019,  5:52am; <NewLine> REPLY_DATE 4: November 22, 2019,  5:56am; <NewLine> REPLY_DATE 5: November 22, 2019,  5:57am; <NewLine> REPLY_DATE 6: November 22, 2019,  7:29am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
60981,What is the most common way to deply pytorch model?,2019-11-14T14:18:34.310Z,0,294,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I found that there is no third party for serving. Like tf serving server.</p><NewLine><p>So I searched how pytorch users serve model and there are some ways like this</p><NewLine><ol><NewLine><li>pytoch model -&gt; onnx -&gt; tensorrt inference server</li><NewLine><li>pytorch model -&gt; jit -&gt; tensorrt inference server</li><NewLine><li>pytoch model -&gt; load in python -&gt; serve with flask etc.<br/><NewLine>and so on</li><NewLine></ol><NewLine><p>I wonder what is the common or preferred way to serving pytorch model.</p><NewLine></div>",https://discuss.pytorch.org/u/juhyung,(손주형),juhyung,"November 14, 2019,  2:18pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thre’s also <a href=""https://github.com/microsoft/onnxruntime"" rel=""nofollow noopener"">onnx runtime</a> which has a model server in beta.<br/><NewLine>And there’s an <a href=""https://github.com/pytorch/pytorch/issues/27610"" rel=""nofollow noopener"">RFC</a> open for a production serving framework for PyTorch.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/RicCu; <NewLine> ,"REPLY_DATE 1: November 14, 2019, 10:16pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
60437,Infer Torch model via gunicorn (wsgi),2019-11-09T04:22:37.499Z,0,501,"<div class=""post"" itemprop=""articleBody""><NewLine><p>gunicorn is used as <em>gunicorn  app:app  --preload --workers 3</em><br/><NewLine>Preload is used to share the resources among the workers.<br/><NewLine>Set the OMP_NUM_THREADS to 2.</p><NewLine><p>app.py contains the following code</p><NewLine><pre><code class=""lang-auto"">from flask import Flask, jsonify<NewLine>import torch  <NewLine>from create_model import testtype or paste code here<NewLine><NewLine>app = Flask(__name__) <NewLine>model = torch.load('model.pt')<NewLine><NewLine>@app.route('/predict',methods = ['POST', 'GET']) <NewLine>def prediction(): <NewLine>    constant_input = torch.randn(20, 16, 50, 100)<NewLine>    prediction = model(constant_input)<NewLine>    return jsonify(prediction)<NewLine></code></pre><NewLine><p>model.pt is created using create_model.py containing</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine><NewLine>class test(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(test, self).__init__()<NewLine>        self.conv1 = nn.Conv2d(16, 33, 3, stride=2)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = F.relu(self.conv1(x))<NewLine>        return x<NewLine><NewLine>m = test()<NewLine>input = torch.randn(20, 16, 50, 100)<NewLine>print(m(input))<NewLine>torch.save(m, 'model.pt')<NewLine></code></pre><NewLine><p>But I am not able to infer it .<br/><NewLine>Instead of using a torch model if I use some numpy operation and just return its output, it is able to.</p><NewLine><p>Though using <em>gunicorn  app:app  --preload --workers 3 --threads 2</em> I am able to infer. But anyone please tell me why does it differ only when threads are used and that too in case of pytorch.<br/><NewLine>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/gvskalyan,(Gvskalyan),gvskalyan,"November 9, 2019,  4:39am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> Any idea?    .</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Do not use <code>torch.save()</code> on your the class object. Either save the <code>state_dict</code> or use the a traced version of the model.</p><NewLine><p>Saving the model with JIT:</p><NewLine><pre><code class=""lang-python"">traced_model = torch.jit.trace(model, torch.randn(1, 16, 50, 100))<NewLine>torch.jit.save(traced_model, output_path)<NewLine></code></pre><NewLine><p>Loading the model in  the server code:</p><NewLine><pre><code class=""lang-python"">model = torch.jit.load(model_path, map_location='cpu')<NewLine></code></pre><NewLine><p>The rest should be the same.</p><NewLine><p>Just a tip on your code style: do not start class names with lowercase, it’s not pythonic</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/gvskalyan; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/miguelvr; <NewLine> ,"REPLY_DATE 1: November 11, 2019,  3:54am; <NewLine> REPLY_DATE 2: November 13, 2019, 10:37am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
60842,Use non-system cuda path,2019-11-13T10:27:32.260Z,0,109,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi<br/><NewLine>How can I specify non-system cuda path for pytorch installation?<br/><NewLine>I mean a path other than /usr/local/cuda.</p><NewLine></div>",https://discuss.pytorch.org/u/mahmoodn,(Mahmood Naderan),mahmoodn,"November 13, 2019, 10:27am",,,,,
60608,Function to limit memory cached,2019-11-11T09:11:44.535Z,2,110,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>Is there any function available to limit the memory that is cached?</p><NewLine><p>Thanks,<br/><NewLine>Mahendra S</p><NewLine></div>",https://discuss.pytorch.org/u/mahendrasnilagiri,(Mahendra S),mahendrasnilagiri,"November 11, 2019,  9:11am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>You mean GPU memory?<br/><NewLine>If so no there is not unfortunately.<br/><NewLine>Why would you need such function?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m running a resized Unet inference. The max input size it would accept is 1280x1280, for this pytorch caches around 4.5 GB of memory. At some edge cases like 505x960 (not a good aspect ratio)input is given the cache memory used is 7.8 GB. This doesnt happen often.</p><NewLine><p>I’m running 2 workers, GPU has 16 GB of memory. I get OOM for some edge cases as mentioned above.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>We do not have such feature at the moment.</p><NewLine><p>The main limitation I recall for this is that the CUDA driver uses a significant amount of memory that we cannot track. Thus the limit set by the user my not be respected because of this extra memory. This makes the feature much weaker as it has some unexpected behavior for the end user.<br/><NewLine>If you have a proposal to overcome this issues, please feel free to open a feature request on github !</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mahendrasnilagiri; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: November 11, 2019, 10:08pm; <NewLine> REPLY_DATE 2: November 12, 2019,  6:46pm; <NewLine> REPLY_DATE 3: November 12, 2019,  8:17pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
60740,How to properly Register a Buffer,2019-11-12T12:26:20.737Z,0,152,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>have recently worked with detectron2 and found that developers use <code>nn.Module._buffers</code> explicitly (<a href=""https://github.com/facebookresearch/detectron2/blob/ef096f9b2fbedca335f7476b715426594673f463/detectron2/modeling/anchor_generator.py#L18"" rel=""nofollow noopener"">https://github.com/facebookresearch/detectron2/blob/ef096f9b2fbedca335f7476b715426594673f463/detectron2/modeling/anchor_generator.py#L18</a>). So I wanted to check will it work similar way if we use <code>buffers()</code> method instead.</p><NewLine><p>So here’s the experiment:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine>class MyModule(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.batch_norm = nn.BatchNorm1d(num_features=1)<NewLine>        self.register_buffer(str('new_buf'), torch.rand(1))<NewLine><NewLine>    def forward(self, x):<NewLine>        return x + 10<NewLine><NewLine>m = MyModule()<NewLine>print(m._buffers.values(), m._buffers.keys())<NewLine>print([el for el in m.buffers()])<NewLine></code></pre><NewLine><p>This code produces the following:</p><NewLine><pre><code class=""lang-auto"">odict_values([tensor([0.0600])]) odict_keys(['new_buf'])<NewLine>[tensor([0.0600]), tensor([0.]), tensor([1.]), tensor(0)]<NewLine></code></pre><NewLine><p>Why there is no mention of BatchNorm parameters in first case, we see only buffers added by registration? But registered is visible in second print.<br/><NewLine>Correct me if I am wrong: “One should use <code>nn.Module.buffers()</code> to see all buffers, and if it is needed to see just user defined use <code>._buffers.values()</code> instead (user defined == registered by <code>.register_buffer()</code>)”</p><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"November 12, 2019, 12:37pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The <code>nn.BatchNorm1d</code> buffers are registered in this particular module and you can access them via:</p><NewLine><pre><code class=""lang-python"">m.batch_norm._buffers<NewLine></code></pre><NewLine><p>However, note that <code>_buffers</code> is an internal method (exactly for this reason of confusion <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/> ) and you should stick to <code>buffers()</code> to get all buffers recursively.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: November 12, 2019,  6:24pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
53978,Torch cpu as a dependency of package,2019-08-21T22:18:21.232Z,0,686,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I have a trained model and created a Django app to host this model. Everything works great in development but now as I am trying to package the Django app for production I have the problem that setuptools can’t seem to install pytorch correctly. In my setup.py I have install_requires containing <code>""torch==1.2.0+cpu"", ""torchvision==0.4.0+cpu</code> and the dependency_links containing <code>https://download.pytorch.org/whl/torch_stable.html</code>.</p><NewLine><p>This seems to be finding torch on pip instead and deciding it can’t install.</p><NewLine><p>I have also tried with install_requires containing</p><NewLine><pre><code class=""lang-auto"">""torch==1.2.0"",<NewLine>""torchvision=0.4.0"",<NewLine></code></pre><NewLine><p>and dependency_links containing</p><NewLine><pre><code class=""lang-auto"">https://download.pytorch.org/whl/cpu/torch_stable.html<NewLine></code></pre><NewLine><p>Neither of these seem to work and I can’t find anything online about this issue. Has anyone done this before and how did you do it?</p><NewLine></div>",https://discuss.pytorch.org/u/jdb100,(joe bosch),jdb100,"August 21, 2019, 10:18pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve had some success using URL dependency syntax from <a href=""https://www.python.org/dev/peps/pep-0508/"" rel=""nofollow noopener"">PEP 508</a>. I had to commit to a specific python version and architecture, though. It’s fine in my case, I know the environment where the package will be used. I’ve looked for a way to leave out the environment spec, but haven’t found anything.</p><NewLine><p>Example entry in <code>install_requires</code>: <code>""torch @ https://download.pytorch.org/whl/cpu/torch-1.3.1%2Bcpu-cp36-cp36m-linux_x86_64.whl""</code></p><NewLine><p>The links can be found here: <a href=""https://download.pytorch.org/whl/torch_stable.html"" rel=""nofollow noopener"">https://download.pytorch.org/whl/torch_stable.html</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mlazowik; <NewLine> ,"REPLY_DATE 1: November 8, 2019,  7:09pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
52199,Error when exporting model to onnx: `Auto nesting doesn&rsquo;t know how to process an input object of type str`,2019-07-31T15:12:29.042Z,3,440,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m facing a weird error when trying to export my model to onnx: <code>ValueError: Auto nesting doesn't know how to process an input object of type str. Accepted types: Tensors, or lists/tuples of them</code></p><NewLine><p>The error says I’m using <code>str</code> as input, which I’m not. My model is a M2Det detector and the implementation is quite tricky: there are <code>for</code> loops, <code>if</code>s and <code>ModuleList</code> inside it. However, none of it seem to be the culprit for the error at hand.</p><NewLine><p>The code is:</p><NewLine><pre><code class=""lang-auto"">class M2Det(nn.Module):<NewLine>    def __init__(self, phase, size, config = None):<NewLine>        '''<NewLine>        M2Det: Multi-level Multi-scale single-shot object Detector<NewLine>        '''<NewLine>        super(M2Det,self).__init__()<NewLine>        self.phase = phase<NewLine>        self.size = size<NewLine>        self.init_params(config)<NewLine>        print_info('===&gt; Constructing M2Det model', ['yellow','bold'])<NewLine>        self.construct_modules()<NewLine><NewLine>    def init_params(self, config=None): # Directly read the config<NewLine>        assert config is not None, 'Error: no config'<NewLine>        for key,value in config.items():<NewLine>            if check_argu(key,value):<NewLine>                setattr(self, key, value)<NewLine><NewLine>    def construct_modules(self,):<NewLine>        # construct tums<NewLine>        for i in range(self.num_levels):<NewLine>            if i == 0:<NewLine>                setattr(self,<NewLine>                        'unet{}'.format(i+1),<NewLine>                        TUM(first_level=True, <NewLine>                            input_planes=self.planes//2, <NewLine>                            is_smooth=self.smooth,<NewLine>                            scales=self.num_scales,<NewLine>                            side_channel=512)) #side channel isn't fixed.<NewLine>            else:<NewLine>                setattr(self,<NewLine>                        'unet{}'.format(i+1),<NewLine>                        TUM(first_level=False, <NewLine>                            input_planes=self.planes//2, <NewLine>                            is_smooth=self.smooth, <NewLine>                            scales=self.num_scales,<NewLine>                            side_channel=self.planes))<NewLine>        # construct base features<NewLine>        if 'vgg' in self.net_family:<NewLine>            self.base = nn.ModuleList(get_backbone(self.backbone))<NewLine>            shallow_in, shallow_out = 512,256<NewLine>            deep_in, deep_out = 1024,512<NewLine><NewLine>        self.reduce= BasicConv(shallow_in, shallow_out, kernel_size=3, stride=1, padding=1)<NewLine>        self.up_reduce= BasicConv(deep_in, deep_out, kernel_size=1, stride=1)<NewLine>        <NewLine>        # construct others<NewLine>        if self.phase == 'test':<NewLine>            self.softmax = nn.Softmax()<NewLine>        self.Norm = nn.BatchNorm2d(256*8)<NewLine>        self.leach = nn.ModuleList([BasicConv(<NewLine>                    deep_out+shallow_out,<NewLine>                    self.planes//2,<NewLine>                    kernel_size=(1,1),stride=(1,1))]*self.num_levels)<NewLine><NewLine>        # construct localization and recognition layers<NewLine>        loc_ = list()<NewLine>        conf_ = list()<NewLine>        for i in range(self.num_scales):<NewLine>            loc_.append(nn.Conv2d(self.planes*self.num_levels,<NewLine>                                       4 * 6, # 4 is coordinates, 6 is anchors for each pixels,<NewLine>                                       3, 1, 1))<NewLine>            conf_.append(nn.Conv2d(self.planes*self.num_levels,<NewLine>                                       self.num_classes * 6, #6 is anchors for each pixels,<NewLine>                                       3, 1, 1))<NewLine>        self.loc = nn.ModuleList(loc_)<NewLine>        self.conf = nn.ModuleList(conf_)<NewLine>    <NewLine>    def forward(self,x):<NewLine>        loc,conf = list(),list()<NewLine>        base_feats = list()<NewLine>        if 'vgg' in self.net_family:<NewLine>            for k in range(len(self.base)):<NewLine>                x = self.base[k](x)<NewLine>                if k in self.base_out:<NewLine>                    base_feats.append(x)<NewLine><NewLine>        base_feature = torch.cat(<NewLine>                (self.reduce(base_feats[0]), F.interpolate(self.up_reduce(base_feats[1]),scale_factor=2,mode='nearest')),1<NewLine>                )<NewLine><NewLine>        # tum_outs is the multi-level multi-scale feature<NewLine>        tum_outs = [getattr(self, 'unet{}'.format(1))(self.leach[0](base_feature), 'none')]<NewLine>        for i in range(1,self.num_levels,1):<NewLine>            tum_outs.append(<NewLine>                    getattr(self, 'unet{}'.format(i+1))(<NewLine>                        self.leach[i](base_feature), tum_outs[i-1][-1]<NewLine>                        )<NewLine>                    )<NewLine>        # concat with same scales<NewLine>        sources = [torch.cat([_fx[i-1] for _fx in tum_outs],1) for i in range(self.num_scales, 0, -1)]<NewLine>        <NewLine>        sources[0] = self.Norm(sources[0])<NewLine>        <NewLine>        for (x,l,c) in zip(sources, self.loc, self.conf):<NewLine>            loc.append(l(x).permute(0, 2, 3, 1).contiguous())<NewLine>            conf.append(c(x).permute(0, 2, 3, 1).contiguous())<NewLine><NewLine>        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)<NewLine>        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)<NewLine><NewLine>        if self.phase == ""test"":<NewLine>            output = (<NewLine>                loc.view(loc.size(0), -1, 4),                   # loc preds<NewLine>                self.softmax(conf.view(-1, self.num_classes)),  # conf preds<NewLine>            )<NewLine>        else:<NewLine>            output = (<NewLine>                loc.view(loc.size(0), -1, 4),<NewLine>                conf.view(conf.size(0), -1, self.num_classes),<NewLine>            )<NewLine>        return output<NewLine></code></pre><NewLine><p>Thanks a lot</p><NewLine></div>",https://discuss.pytorch.org/u/arc144,(Eduardo Rocha De Andrade),arc144,"August 2, 2019,  6:50pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you simplify the code a bit or post the definitions of the missing modules, e.g. <code>TUM</code>, <code>BasicConv</code> etc. so that we could debug it?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Of course, I edited the post removing some code there were not being used. In addition, here is the missing modules:</p><NewLine><pre><code class=""lang-auto"">class BasicConv(nn.Module):<NewLine><NewLine>    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, <NewLine>            groups=1, relu=True, bn=True, bias=False):<NewLine>        super(BasicConv, self).__init__()<NewLine>        self.out_channels = out_planes<NewLine>        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, <NewLine>                stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)<NewLine>        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None<NewLine>        self.relu = nn.ReLU(inplace=True) if relu else None<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.conv(x)<NewLine>        if self.bn is not None:<NewLine>            x = self.bn(x)<NewLine>        if self.relu is not None:<NewLine>            x = self.relu(x)<NewLine>        return x<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">class TUM(nn.Module):<NewLine>    def __init__(self, first_level=True, input_planes=128, is_smooth=True, side_channel=512, scales=6):<NewLine>        super(TUM, self).__init__()<NewLine>        self.is_smooth = is_smooth<NewLine>        self.side_channel = side_channel<NewLine>        self.input_planes = input_planes<NewLine>        self.planes = 2 * self.input_planes<NewLine>        self.first_level = first_level<NewLine>        self.scales = scales<NewLine>        self.in1 = input_planes + side_channel if not first_level else input_planes<NewLine><NewLine>        self.layers = nn.Sequential()<NewLine>        self.layers.add_module('{}'.format(len(self.layers)), BasicConv(self.in1, self.planes, 3, 2, 1))<NewLine>        for i in range(self.scales-2):<NewLine>            if not i == self.scales - 3:<NewLine>                self.layers.add_module(<NewLine>                        '{}'.format(len(self.layers)),<NewLine>                        BasicConv(self.planes, self.planes, 3, 2, 1)<NewLine>                        )<NewLine>            else:<NewLine>                self.layers.add_module(<NewLine>                        '{}'.format(len(self.layers)),<NewLine>                        BasicConv(self.planes, self.planes, 3, 1, 0)<NewLine>                        )<NewLine>        self.toplayer = nn.Sequential(BasicConv(self.planes, self.planes, 1, 1, 0))<NewLine>        <NewLine>        self.latlayer = nn.Sequential()<NewLine>        for i in range(self.scales-2):<NewLine>            self.latlayer.add_module(<NewLine>                    '{}'.format(len(self.latlayer)),<NewLine>                    BasicConv(self.planes, self.planes, 3, 1, 1)<NewLine>                    )<NewLine>        self.latlayer.add_module('{}'.format(len(self.latlayer)),BasicConv(self.in1, self.planes, 3, 1, 1))<NewLine><NewLine>        if self.is_smooth:<NewLine>            smooth = list()<NewLine>            for i in range(self.scales-1):<NewLine>                smooth.append(<NewLine>                        BasicConv(self.planes, self.planes, 1, 1, 0)<NewLine>                        )<NewLine>            self.smooth = nn.Sequential(*smooth)<NewLine><NewLine>    def _upsample_add(self, x, y, fuse_type='interp'):<NewLine>        _,_,H,W = y.size()<NewLine>        if fuse_type=='interp':<NewLine>            return F.interpolate(x, size=(H,W), mode='nearest') + y<NewLine>        else:<NewLine>            raise NotImplementedError<NewLine>            #return nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)<NewLine><NewLine>    def forward(self, x, y):<NewLine>        if not self.first_level:<NewLine>            x = torch.cat([x,y],1)<NewLine>        conved_feat = [x]<NewLine>        for i in range(len(self.layers)):<NewLine>            x = self.layers[i](x)<NewLine>            conved_feat.append(x)<NewLine>        <NewLine>        deconved_feat = [self.toplayer[0](conved_feat[-1])]<NewLine>        for i in range(len(self.latlayer)):<NewLine>            deconved_feat.append(<NewLine>                    self._upsample_add(<NewLine>                        deconved_feat[i], self.latlayer[i](conved_feat[len(self.layers)-1-i])<NewLine>                        )<NewLine>                    )<NewLine>        if self.is_smooth:<NewLine>            smoothed_feat = [deconved_feat[0]]<NewLine>            for i in range(len(self.smooth)):<NewLine>                smoothed_feat.append(<NewLine>                        self.smooth[i](deconved_feat[i+1])<NewLine>                        )<NewLine>            return smoothed_feat<NewLine>        return deconved_feat<NewLine></code></pre><NewLine><p>I know it is a lengthy code, I’m sorry</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the code!<br/><NewLine>How should I initialize the model, as <code>phase</code>, <code>size</code> and <code>config</code>  are missing, which will raise an exception.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Indeed, I forget the input!<br/><NewLine>The input is as follows:</p><NewLine><pre><code class=""lang-auto"">phase = 'test'<NewLine>size = 512<NewLine>m2det_config = dict(<NewLine>    backbone='vgg16',<NewLine>    net_family='vgg',  <NewLine>    base_out=[22, 34],<NewLine>    planes=256,<NewLine>    num_levels=8,<NewLine>    num_scales=6,<NewLine>    sfam=False,<NewLine>    smooth=True,<NewLine>    num_classes=3,  <NewLine>)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/arc144"">@arc144</a> hi, have you  solved the problem,  i am also facing the error</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/arc144; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/arc144; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/kasen_zhou; <NewLine> ,"REPLY_DATE 1: August 2, 2019,  6:29pm; <NewLine> REPLY_DATE 2: August 2, 2019,  6:53pm; <NewLine> REPLY_DATE 3: August 2, 2019,  7:07pm; <NewLine> REPLY_DATE 4: August 2, 2019,  7:29pm; <NewLine> REPLY_DATE 5: November 8, 2019,  3:48am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
59997,Problems deploying a resnet_6blocks net with weights,2019-11-04T22:52:15.756Z,0,99,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I was doing some research with this pix2pix implmenetation <a href=""https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"" rel=""nofollow noopener"">https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix</a> and was hoping to be able to deploy the trained models for further research and speed tests. To be honest this is my first approach in saving and loading a model. I found two topics on google about the issue below but nothing worked for me. I fear I might do something fundamentally wrong.</p><NewLine><p>I inserted this code (from the pytorch tutorial site):</p><NewLine><blockquote><NewLine><p>checkpoint = {‘model’: net.module.cpu().model,<br/><NewLine>‘state_dict’: net.module.cpu().state_dict()}<br/><NewLine>torch.save(checkpoint, save_path)</p><NewLine></blockquote><NewLine><p>I’m using this code to load it:</p><NewLine><blockquote><NewLine><p>def load_checkpoint(filepath):<br/><NewLine>checkpoint = torch.load(filepath)<br/><NewLine>print(checkpoint[‘model’])<br/><NewLine>print(checkpoint[‘state_dict’])<br/><NewLine>model = checkpoint[‘model’]<br/><NewLine>model.load_state_dict(checkpoint[‘state_dict’])<br/><NewLine>for parameter in model.parameters():<br/><NewLine>parameter.requires_grad = False</p><NewLine><pre><code>model.eval()<NewLine>return model<NewLine></code></pre><NewLine><p>model = load_checkpoint(‘5_net_G.pth’)</p><NewLine></blockquote><NewLine><p>For debugging I printed out the model and dictionary. This is the model:</p><NewLine><blockquote><NewLine><p>Sequential(<br/><NewLine>(0): ReflectionPad2d((3, 3, 3, 3))<br/><NewLine>(1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), bias=False)<br/><NewLine>(2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(3): ReLU(inplace)<br/><NewLine>(4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<br/><NewLine>(5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(6): ReLU(inplace)<br/><NewLine>(7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<br/><NewLine>(8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(9): ReLU(inplace)<br/><NewLine>(10): ResnetBlock(<br/><NewLine>(conv_block): Sequential(<br/><NewLine>(0): ReflectionPad2d((1, 1, 1, 1))<br/><NewLine>(1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)<br/><NewLine>(2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(3): ReLU(inplace)<br/><NewLine>(4): Dropout(p=0.5)<br/><NewLine>(5): ReflectionPad2d((1, 1, 1, 1))<br/><NewLine>(6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)<br/><NewLine>(7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>)<br/><NewLine>)<br/><NewLine>(11): ResnetBlock(<br/><NewLine>(conv_block): Sequential(<br/><NewLine>(0): ReflectionPad2d((1, 1, 1, 1))<br/><NewLine>(1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)<br/><NewLine>(2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(3): ReLU(inplace)<br/><NewLine>(4): Dropout(p=0.5)<br/><NewLine>(5): ReflectionPad2d((1, 1, 1, 1))<br/><NewLine>(6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)<br/><NewLine>(7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>)<br/><NewLine>)<br/><NewLine>(12): ResnetBlock(<br/><NewLine>(conv_block): Sequential(<br/><NewLine>(0): ReflectionPad2d((1, 1, 1, 1))<br/><NewLine>(1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)<br/><NewLine>(2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(3): ReLU(inplace)<br/><NewLine>(4): Dropout(p=0.5)<br/><NewLine>(5): ReflectionPad2d((1, 1, 1, 1))<br/><NewLine>(6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)<br/><NewLine>(7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>)<br/><NewLine>)<br/><NewLine>(13): ResnetBlock(<br/><NewLine>(conv_block): Sequential(<br/><NewLine>(0): ReflectionPad2d((1, 1, 1, 1))<br/><NewLine>(1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)<br/><NewLine>(2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(3): ReLU(inplace)<br/><NewLine>(4): Dropout(p=0.5)<br/><NewLine>(5): ReflectionPad2d((1, 1, 1, 1))<br/><NewLine>(6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)<br/><NewLine>(7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>)<br/><NewLine>)<br/><NewLine>(14): ResnetBlock(<br/><NewLine>(conv_block): Sequential(<br/><NewLine>(0): ReflectionPad2d((1, 1, 1, 1))<br/><NewLine>(1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)<br/><NewLine>(2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(3): ReLU(inplace)<br/><NewLine>(4): Dropout(p=0.5)<br/><NewLine>(5): ReflectionPad2d((1, 1, 1, 1))<br/><NewLine>(6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)<br/><NewLine>(7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>)<br/><NewLine>)<br/><NewLine>(15): ResnetBlock(<br/><NewLine>(conv_block): Sequential(<br/><NewLine>(0): ReflectionPad2d((1, 1, 1, 1))<br/><NewLine>(1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)<br/><NewLine>(2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(3): ReLU(inplace)<br/><NewLine>(4): Dropout(p=0.5)<br/><NewLine>(5): ReflectionPad2d((1, 1, 1, 1))<br/><NewLine>(6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)<br/><NewLine>(7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>)<br/><NewLine>)<br/><NewLine>(16): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)<br/><NewLine>(17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(18): ReLU(inplace)<br/><NewLine>(19): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)<br/><NewLine>(20): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/><NewLine>(21): ReLU(inplace)<br/><NewLine>(22): ReflectionPad2d((3, 3, 3, 3))<br/><NewLine>(23): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))<br/><NewLine>(24): Tanh()<br/><NewLine>)</p><NewLine></blockquote><NewLine><p>But in the code line model = checkpoint[‘model’] I get the following error:</p><NewLine><blockquote><NewLine><p>Error(s) in loading state_dict for Sequential:<br/><NewLine>Missing key(s) in state_dict: “1.weight”, “2.weight”, “2.bias”, “2.running_mean”,…</p><NewLine></blockquote><NewLine><p>I would be glad for any suggestions. Thanks in advance!<br/><NewLine>Cheers!</p><NewLine></div>",https://discuss.pytorch.org/u/andrew_germ,,andrew_germ,"November 4, 2019, 10:52pm",,,,,
59953,How to export variable of functional during torch-&gt;onnx?,2019-11-04T13:04:05.349Z,0,179,"<div class=""post"" itemprop=""articleBody""><NewLine><p>As the title, I’ve trained an embedding function (e.g. inception-v1) with the last layer of an fc (e.g., 2048-dim vector as its output). In deployment, I want to append an L2-normalize operation after the fc layer and export the appended model (inception-v1 + fc2048 + l2norm) to onnx format? Without considering deployment in the form of trt, I know it is just adding a functional in the forward function of the model class. But in exporting to onnx format, it seems that the output name of a module is needed.</p><NewLine></div>",https://discuss.pytorch.org/u/stoneyang,(Stoneyang),stoneyang,"November 4, 2019,  1:04pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve read <a href=""https://discuss.pytorch.org/t/whats-the-difference-between-nn-relu-vs-f-relu/27599/2"">this thread</a>, and the problem seems to become to find a torch module equivalent of <code>nn.functional.normalize()</code>, correct me if I was wrong.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>And <a href=""https://discuss.pytorch.org/t/runtimeerror-onnx-export-failed-couldnt-export-operator-aten-norm/18634"">this thread</a> seems useful. But I still can’t find the so call <code>LpNormalization</code> mentioned by cfer8395.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/stoneyang; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/stoneyang; <NewLine> ,"REPLY_DATE 1: November 4, 2019,  1:09pm; <NewLine> REPLY_DATE 2: November 4, 2019,  2:10pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
59675,How to build caffe2 with ONNX opset version greater than 9?,2019-10-31T12:34:42.050Z,2,212,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I’ve currently worked with freshly merged feature <a href=""https://github.com/pytorch/vision/pull/1401"" rel=""nofollow noopener"">pytorch/vision#1401</a> and won’t able to find a way to make Caffe2 work with ONNX operation set 10?</p><NewLine><p>Is there a way to build a Caffe2 from source with this opset?</p><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"October 31, 2019, 12:34pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t know but I would venture that arguably the go-to library for ONNX model serving is <a href=""https://github.com/microsoft/onnxruntime"" rel=""nofollow noopener"">ONNXRuntime</a> these days.</p><NewLine><p>See also <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/caffe2-vs-libtorch-realistically/50526/4"">Caffe2 vs libtorch, realistically</a> .</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Indeed. We’ve removed tutorials around this as the Caffe2 path really isn’t being maintained. Assuming you are focused on using ONNX (and not deploying via Torchscript), to leverage the latest opset, it makes more sense to export an ONNX model from PyTorch and then deploy in ONN Runtime as <a class=""mention"" href=""/u/tom"">@tom</a> suggests. Here is a tutorial that is maintained by the MSFT team: <a href=""https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot, I’ve got it. Caffe2 right now is totally out maintenance</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you too. Let me clarify why I was eager to find a way to make Caffe2 work with recent opset. I’ve recently worked with your repo with AICamera that manages PyTorch v1.0.0  to work with Android devices using Caffe2 library. So I know that right now it is TorchScript available in mobile and onnx-runtime doesn’t work with mobile. So I wondered how to deploy on mobile ONNX model with recent opset using Caffe2 backend.<br/><NewLine>P.S. The last statement is not a question just a theme to discuss</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jspisak; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/zetyquickly; <NewLine> ,"REPLY_DATE 1: October 31, 2019,  3:22pm; <NewLine> REPLY_DATE 2: November 4, 2019,  5:05am; <NewLine> REPLY_DATE 3: November 4, 2019,  8:28am; <NewLine> REPLY_DATE 4: November 4, 2019,  8:34am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
56402,Deploy PyTorch 1.0 into production: Flask (Python) vs NodeJS (C++ Addons),2019-09-19T23:39:06.449Z,1,1200,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I notice people in the PyTorch-Libtorch community have started using NodeJS C++ Addons to wrap Libtorch code into production. Is this the most appropriate pipeline to deploy a C++ Libtorch API model from a backend standpoint to retrieve an API endpoint? I’m a fan of backend inference performance.</p><NewLine><p>Most people lately have been using flask but when it comes to speed python is slower than C++ including javascript (NodeJS) since it’s running on Chrome web browser (v8-C++ compiler).</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/33fbb9b7d6dd586d991b503fe35d1bc3d2a4dc4a"" href=""https://discuss.pytorch.org/uploads/default/original/3X/3/3/33fbb9b7d6dd586d991b503fe35d1bc3d2a4dc4a.png"" title=""deploy.png""><img alt="""" data-base62-sha1=""7pRFyEQzWF0eaBvOUqjtF9HDJPA"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/3/3/33fbb9b7d6dd586d991b503fe35d1bc3d2a4dc4a_2_10x10.png"" height=""409"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/3/3/33fbb9b7d6dd586d991b503fe35d1bc3d2a4dc4a_2_689x409.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/3/3/33fbb9b7d6dd586d991b503fe35d1bc3d2a4dc4a_2_689x409.png, https://discuss.pytorch.org/uploads/default/optimized/3X/3/3/33fbb9b7d6dd586d991b503fe35d1bc3d2a4dc4a_2_1033x613.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/3/3/33fbb9b7d6dd586d991b503fe35d1bc3d2a4dc4a.png 2x"" width=""689""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">deploy.png</span><span class=""informations"">1279×759 184 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p><a class=""onebox"" href=""http://blog.christianperone.com/2018/10/pytorch-1-0-tracing-jit-and-libtorch-c-api-to-integrate-pytorch-into-nodejs/?fbclid=IwAR0donyNrLN6HiRDjiPRyt4pMoTJveMircbv_YLKG991waPtpj0lUCtHw6k"" rel=""nofollow noopener"" target=""_blank"">http://blog.christianperone.com/2018/10/pytorch-1-0-tracing-jit-and-libtorch-c-api-to-integrate-pytorch-into-nodejs/?fbclid=IwAR0donyNrLN6HiRDjiPRyt4pMoTJveMircbv_YLKG991waPtpj0lUCtHw6k</a></p><NewLine></div>",https://discuss.pytorch.org/u/rchavezj,(Rchavezj),rchavezj,"September 24, 2019,  7:01am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Agree, for folks where perf isn’t absolutely critical (which is a lot!), Flask is a great and easy option.</p><NewLine><p>The Amazon guys are putting forth an RFC for a high performance Cpp PyTorch serving platform. Would be great to get your thoughts on the RFC:</p><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/27610"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/27610"" rel=""nofollow noopener"" target=""_blank"">[FR][RFC] Build a serving framework to host and serve trained PyTorch models</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-10-09"" data-format=""ll"" data-time=""16:29:17"" data-timezone=""UTC"">04:29PM - 09 Oct 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/vdantu"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""vdantu"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars1.githubusercontent.com/u/36211508?v=4"" width=""20""/><NewLine>          vdantu<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">🚀 Feature<NewLine>Request to build a Serving framework to host and serve trained PyTorch models.<NewLine>@alexwong<NewLine>Motivation<NewLine>PyTorch provides an excellent and easy-to-use interface to...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">feature</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Cheers!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think there is plenty of room to improve on the standard “use flask” advice and the solutions they present. Without leaving Python, one can do many things right (like request batching, using a framework with async etc.). Christian also has a great <a href=""https://speakerdeck.com/player/50a597cc8edf42d799bb239d86be3dea"" rel=""nofollow noopener"">slide deck that lists some desiderata</a>.</p><NewLine><p>I think it is hard to completely avoid copying if you want your server to decode images (because you have request -&gt; JPEG format blob -&gt; Tensor -&gt; Rescaled), but many of the other items from Christian’s list can be handled quite nicely using Python + traced models (and gRPC could probably be used to eliminate some of the copying headaches). The JIT liberates you from annoyances with the GIL during the running of your model.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jspisak; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: August 12, 2020,  4:00am; <NewLine> REPLY_DATE 2: October 31, 2019, 11:15pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
59481,OnnxToCaffe2 - Incompatible ConvTranspose operator,2019-10-29T15:21:06.602Z,0,119,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, noob here - apologies in advance if this is a stupid question.</p><NewLine><p>I’m attempting to load an ONNX model in Caffe2 using: caffe2::onnx::Caffe2Backend::Prepare(const std::string &amp; onnx_model_str, const std::string &amp; device, const std::vector&lt;caffe2::onnx::Caffe2Ops,std::allocator<a>caffe2::onnx::Caffe2Ops</a>&gt; &amp; extras)</p><NewLine><p>I’m getting the following error:<br/><NewLine>“Don’t know how to map unexpected argument ‘group’ (from operator ConvTranspose)”.</p><NewLine><p>If I load the same model as Caffe2 prototext (by converting offline to init/predict via c2.onnx_graph_to_caffe2_net), I don’t have any issues.</p><NewLine><p>Investigating the error, it seems that Caffe2’s ConvTranspose expects the following args: legacy_pad, kernels, strides, pads, adjs, order, shared_buffer, no_bias, whereas ONNX’s ConvTranspose assumes the following: auto_pad, dilations, group, kernel_shape, output_padding, output_shape, pads, strides.</p><NewLine><p>Looks like the ‘group’ argument was added on April 4 2019 <strong>to the Caffe2 operator</strong> :<br/><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""15"" src=""https://github.githubassets.com/favicon.ico"" width=""15""/><NewLine><a href=""https://github.com/pytorch/pytorch/commits/master/caffe2/operators/conv_transpose_op_impl.h"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars2.githubusercontent.com/u/21003710?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/pytorch/pytorch/commits/master/caffe2/operators/conv_transpose_op_impl.h"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch</a></h3><NewLine><p>Tensors and Dynamic neural networks in Python with strong GPU acceleration - pytorch/pytorch</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>But the <strong>Caffe2 ONNX backend schema</strong> wasn’t changed and it doesn’t seem to have kept up with ONNX itself.</p><NewLine><p>Questions:</p><NewLine><ol><NewLine><li>Is my interpretation correct?</li><NewLine><li>If yes, is OnnxNodeToCaffe2Ops (caffe2\onnx\backend.cc) maintained/supported? Can I rely on loading ONNX models directly in Caffe2?</li><NewLine></ol><NewLine><p>Thanks so much!</p><NewLine></div>",https://discuss.pytorch.org/u/paolor,(Paolo R),paolor,"October 29, 2019,  3:21pm",,,,,
59214,Torch 1.3.0.post2 published for MacOS but not other platforms in Pypi,2019-10-25T14:19:58.874Z,3,233,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>It looks like WHL files for other platforms than MacOS haven’t ben published to <a href=""https://pypi.org/project/torch/#files"" rel=""nofollow noopener"">https://pypi.org/project/torch/#files</a> and this creates issue for pip.</p><NewLine><p>Thanks<br/><NewLine>P.</p><NewLine></div>",https://discuss.pytorch.org/u/mandubian,(Pascal Voitot),mandubian,"October 25, 2019,  2:19pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>You can find the wheel for the other platforms at the 1.3.0 tag <a href=""https://pypi.org/project/torch/1.3.0/#files"" rel=""nofollow noopener"">https://pypi.org/project/torch/1.3.0/#files</a></p><NewLine><p>We had some issues with the macos build and had to release a new set of binaries for it.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks.<br/><NewLine>I know that because I’ve used 1.3.0 instead.<br/><NewLine>But if you use a package manager such as pip with a version indicator &gt;=1.3, it considers 1.3.0.post2 as latest version and fails to get it so it fails to build.<br/><NewLine>Maybe it’d also better to release builds for all other platforms too in 1.3.0.pos2 tag to avoid it.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Good to know,</p><NewLine><p><a class=""mention"" href=""/u/smth"">@smth</a> do we want to release post2 versions of the other binaries?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>TBH I think you have to deploy it in any case because here is what you get under Linux for now:</p><NewLine><blockquote><NewLine><p>pip search torch<br/><NewLine>torch (1.3.0.post2)                   - Tensors and Dynamic neural networks in Python with strong GPU acceleration<br/><NewLine>INSTALLED: 1.3.0<br/><NewLine>LATEST:    1.3.0.post2</p><NewLine></blockquote><NewLine><p>I’ve lost some time this morning because I thought I had made an error somewhere till I discovered the missing files on PyPi repo.<br/><NewLine>Thks again</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mandubian; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mandubian; <NewLine> ,"REPLY_DATE 1: October 25, 2019,  2:27pm; <NewLine> REPLY_DATE 2: October 25, 2019,  2:33pm; <NewLine> REPLY_DATE 3: October 25, 2019,  2:34pm; <NewLine> REPLY_DATE 4: October 25, 2019,  2:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
59201,   assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors),2019-10-25T11:25:07.703Z,0,193,"<div class=""post"" itemprop=""articleBody""><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/d6c0cb4f08d6a7b093d362333dc8f627c2b4df61"" href=""https://discuss.pytorch.org/uploads/default/original/3X/d/6/d6c0cb4f08d6a7b093d362333dc8f627c2b4df61.png"" title=""Capture.PNG""><img alt=""Capture"" data-base62-sha1=""uDNeLZhpCibeNpsvXhgNbyyORRD"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/6/d6c0cb4f08d6a7b093d362333dc8f627c2b4df61_2_10x10.png"" height=""224"" src=""https://discuss.pytorch.org/uploads/default/original/3X/d/6/d6c0cb4f08d6a7b093d362333dc8f627c2b4df61.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Capture.PNG</span><span class=""informations"">1044×340 9.71 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div> <img alt=""Capture1"" data-base62-sha1=""bBDmohoijL0esEQLhDQ5afEjZ3a"" height=""368"" src=""https://discuss.pytorch.org/uploads/default/original/3X/5/1/51590087741cb0387c80c4bfa8973696d21c9bc0.png"" width=""534""/></p><NewLine></div>",https://discuss.pytorch.org/u/Safak_Altinok,(Şafak Altınok),Safak_Altinok,"October 25, 2019, 11:25am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Your issue is similar to <a href=""https://discuss.pytorch.org/t/how-can-i-create-a-single-dataloader-for-my-two-csv-files-i-have-tried-this/59099/8"">this one</a>.</p><NewLine><p>Basically, the passed tensors have a different length, which raises this error.</p><NewLine><p>Let me know, if you need more information.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: October 25, 2019, 11:26am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
58500,Compile torch model to not need loading from file,2019-10-17T17:40:17.438Z,2,122,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone, just first wanted to say that I really enjoy working with libtorch.</p><NewLine><p>Is there a way to compile my C++ code with the model in such a way that when the executable is run, it doesn’t need to load the model from the file, but rather has the model “hard coded” into the program itself? That way I could run the executable with no external file dependencies.<br/><NewLine>Is this something that could be done with torchscript?</p><NewLine><p>I ask because that would be really awesome for deployment purposes where I try to make my executable as light and as fast as possible. If that’s not the case, I might have to set it up as a daemon communicating on local sockets.</p><NewLine><p>Cheers!</p><NewLine><p>Edit: to be really clear, my end goal is to incorporate this into C code where it doesn’t need to load the file each time the function is called since the file loading is quite the bottleneck.</p><NewLine></div>",https://discuss.pytorch.org/u/rtkaratekid,(rtkaratekid),rtkaratekid,"October 17, 2019,  5:57pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is very likely a bad idea but can’t you bundle the file content into a static string when you compile and load from that string?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/alband"">@albanD</a> Haha! That was my first idea as well and I was hoping there was something a little less hacky. I may try it out and report back.</p><NewLine><p>It would be really cool if I could compile the model itself down into a binary without such shenanigans though. I’m just not sure how it would be done.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/alband"">@albanD</a> it is indeed a bad idea! The reasoning is actually interesting though. I thought (for some reason) that the loading the model from the file was the bottleneck, but the bottleneck is actually libtorch reading the model settings and creating the algorithm itself. Because of that there is no speed boost from just hard coding the string into the file.</p><NewLine><p>That being said, since the libtorch library essentially “compiles” the model, it would be cool if there were a functionality to save the model as an executable. Unfortunately I don’t have the time to hack on something like that currently. But maybe in the future if I’m looking for a project I’ll work on that.</p><NewLine><p>As for right now I’m just setting my model up as a daemon communicating with other processes through local sockets and seems adequate.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/rtkaratekid; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/rtkaratekid; <NewLine> ,"REPLY_DATE 1: October 17, 2019,  6:01pm; <NewLine> REPLY_DATE 2: October 17, 2019,  6:05pm; <NewLine> REPLY_DATE 3: October 18, 2019,  5:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
58361,Deploying on customer local machines,2019-10-16T08:37:18.980Z,0,148,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Let’s assume that i want to deploy and integrate my pytorch model on the customer local machines… How could i guarantee that the end customer won’t get my model weights??</p><NewLine></div>",https://discuss.pytorch.org/u/Hamada_Fathy,(Hamada Fathy),Hamada_Fathy,"October 16, 2019,  8:37am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>We don’t have any specific tool for this.<br/><NewLine>I don’t know what are the regular python tools that exist for this but they most likely also apply to pytorch.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: October 16, 2019,  4:00pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
58244,[Regression problem with PyTorch] Predict coordinate with sensors data,2019-10-15T04:47:31.084Z,0,360,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I tried to predict the mobile device screen coordinate, but the result were not well.</p><NewLine><p>The input data [300*16] are 300 time points with 16 ACTION_DOWN/ACTION_UP/4 sensors coordinates, the output data [1*2] are correct coordinate (x, y).</p><NewLine><p>The <code>Up Distance</code> value is the distance between correct coordinate and user touch point, and the <code>Predict Distance</code> value is the distance between correct coordinate and predict output. The target is to decrease <code>Predict Distance</code> as possible.</p><NewLine><p>I tried to change optimizer, loss function, model construct etc. but I couldn’t find out where the problem is. Can anyone help me? Thanks so much.</p><NewLine><p>The following are parameters, model and results:</p><NewLine><pre><code class=""lang-auto"">BATCH_SIZE = 128<NewLine>LEARNING_RATE = 0.01<NewLine>DROPOUT = 0.3<NewLine><NewLine>optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)<NewLine>criterion = torch.nn.SmoothL1Loss()<NewLine><NewLine>class CustomModel(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super(CustomModel, self).__init__()<NewLine>        self.conv1_1 = torch.nn.Conv1d(<NewLine>            in_channels=FEATURE_CHANNEL, out_channels=FEATURE_CHANNEL, kernel_size=3, stride=2)<NewLine>        self.pool_1 = torch.nn.MaxPool1d(kernel_size=3, stride=2)<NewLine>        <NewLine>        self.conv1_2 = torch.nn.Conv1d(<NewLine>            in_channels=FEATURE_CHANNEL, out_channels=FEATURE_CHANNEL, kernel_size=3, stride=2)<NewLine>        self.pool_2 = torch.nn.MaxPool1d(kernel_size=3, stride=2)<NewLine><NewLine>        self.dropout = torch.nn.Dropout(DROPOUT)<NewLine>        <NewLine>        self.conv1_3 = torch.nn.Conv1d(<NewLine>            in_channels=FEATURE_CHANNEL, out_channels=FEATURE_CHANNEL, kernel_size=3, stride=2)<NewLine>        self.pool_3 = torch.nn.MaxPool1d(kernel_size=3, stride=2)<NewLine>        <NewLine>        self.conv1_4 = torch.nn.Conv1d(<NewLine>            in_channels=FEATURE_CHANNEL, out_channels=FEATURE_CHANNEL, kernel_size=2, stride=2)<NewLine>        <NewLine>        self.fc1 = torch.nn.Linear(16, 2)<NewLine>        <NewLine>    def forward(self, x):<NewLine>        x = self.conv1_1(x)<NewLine>        x = torch.nn.functional.relu(x)<NewLine>        x = self.pool_1(x)<NewLine>        <NewLine>        x = self.conv1_2(x)<NewLine>        x = torch.nn.functional.relu(x)<NewLine>        x = self.pool_2(x)<NewLine>        <NewLine>        x = self.dropout(x)<NewLine>        <NewLine>        x = self.conv1_3(x)<NewLine>        x = torch.nn.functional.relu(x)<NewLine>        x = self.pool_3(x)<NewLine>        <NewLine>        x = self.conv1_4(x)<NewLine>        x = torch.nn.functional.relu(x)<NewLine>        <NewLine>        x = x.view(-1, 1*16)<NewLine><NewLine>        x = self.fc1(x)<NewLine><NewLine>        return x<NewLine></code></pre><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/cd13daf9fb1fde7627f6b63d55b957e734ff3b5b"" href=""https://discuss.pytorch.org/uploads/default/original/3X/c/d/cd13daf9fb1fde7627f6b63d55b957e734ff3b5b.jpeg"" title=""圖片2.jpg""><img alt=""%E5%9C%96%E7%89%872"" data-base62-sha1=""tgcq7PfjYmT99x228g6SlSJFGYz"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/d/cd13daf9fb1fde7627f6b63d55b957e734ff3b5b_2_10x10.png"" height=""245"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/d/cd13daf9fb1fde7627f6b63d55b957e734ff3b5b_2_517x245.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/d/cd13daf9fb1fde7627f6b63d55b957e734ff3b5b_2_517x245.jpeg, https://discuss.pytorch.org/uploads/default/optimized/3X/c/d/cd13daf9fb1fde7627f6b63d55b957e734ff3b5b_2_775x367.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/c/d/cd13daf9fb1fde7627f6b63d55b957e734ff3b5b_2_1034x490.jpeg 2x"" width=""517""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">圖片2.jpg</span><span class=""informations"">1920×910 197 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/Lindosun,,Lindosun,"October 15, 2019,  4:47am",,,,,
58175,Converting torch.nn.Conv1d with group of 1 to Matlab,2019-10-14T12:08:01.896Z,0,117,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,</p><NewLine><p>I wanted to convert Conv1d with group of 1 to Matlab. Have you experienced this transformation?</p><NewLine><p>Any help is highly appreciated.</p><NewLine><p>Thank you.</p><NewLine><p>Br, KV</p><NewLine></div>",https://discuss.pytorch.org/u/KienVu,(Kien Vu),KienVu,"October 14, 2019, 12:08pm",,,,,
55072,Pytorch and SQL,2019-09-03T17:57:08.769Z,0,357,"<div class=""post"" itemprop=""articleBody""><NewLine><p>We sometimes train models using annotations from multiple datasets. Merging multiple datasets into 1 and building dataloaders take a lot of effort and many, many for loops. I (only) recently found that organizing datasets into SQL tables and do merges/queries greatly reduces the amount of code I have to write and that probably saved a lot of my hairs.</p><NewLine><p>In fact something like COCO annotations</p><NewLine><blockquote><NewLine><p>{“license”: 5,“file_name”: “COCO_train2011<br/><NewLine>4_000000057870.jpg”,“coco_url”: “https://images_cocodataset_org/train2014/CC<br/><NewLine>OCO_train2014_000000057870.jpg”,“height”: 480,“width”: 640,“date_captured”""<br/><NewLine>: “2013-11-14 16:28:13”,“flickr_url”: “https://farm4_staticflickr_com/3153//<br/><NewLine>2970773875_164f0c0b83_z.jpg”,“id”: 57870}, …</p><NewLine></blockquote><NewLine><p>can be represented as a SQL table with fields “license”, “file_name”, “coco_url”, “height”, “width”, etc. Merging annotations from two different datasets can be done by <a href=""https://www.w3schools.com/sql/sql_join.asp"" rel=""nofollow noopener"">SQL joins</a> based on image names/indexes. Dataloaders could be written to process and return rows of a data table.</p><NewLine><p>I think PyTorch could implement something like “torch.db” to do this. Traditional databases can’t efficiently handle GPU tensors, so an opportunity for PyTorch is to enable fast joins of tables with tensors and potentially optimally manage GRAM/RAM/disk access.</p><NewLine></div>",https://discuss.pytorch.org/u/grrr,,grrr,"September 3, 2019,  6:00pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://github.com/pytorch/pytorch/issues/20822"" rel=""nofollow noopener"">This issue</a> talks about an internal FB framework that could benefit you. They were wondering if publishing it made sense and they were asking for input from the community, so you might want to check it out and comment on your use case.<br/><NewLine>I don’t really know the extent of that project nor whether it will ever be open source but it might be what you’re looking for.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/RicCu; <NewLine> ,"REPLY_DATE 1: October 13, 2019,  3:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
57214,Build CUDAExtension on non-CUDA environment,2019-10-01T03:59:50.697Z,1,705,"<div class=""post"" itemprop=""articleBody""><NewLine><p><code>CUDAExtension</code> in <code>setup.py</code> may fail with an environment where CUDA isn’t installed. What is the best practice to make <code>CUDAExtension</code> compatible with a non-CUDA environment?</p><NewLine><pre><code class=""lang-python"">Traceback (most recent call last):<NewLine>  File ""setup.py"", line 23, in &lt;module&gt;<NewLine>    'my_extension.cu',<NewLine>  File ""/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/torch/utils/cpp_extension.py"", line 476, in CUDAExtension<NewLine>    library_dirs += library_paths(cuda=True)<NewLine>  File ""/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/torch/utils/cpp_extension.py"", line 555, in library_paths<NewLine>    if (not os.path.exists(_join_cuda_home(lib_dir)) and<NewLine>  File ""/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/torch/utils/cpp_extension.py"", line 1146, in _join_cuda_home<NewLine>    raise EnvironmentError('CUDA_HOME environment variable is not set. '<NewLine>OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/sublee,(Heungsub Lee),sublee,"October 1, 2019,  3:59am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can’t build a CUDA extension without the CUDA headers and libraries. I think your main options would be to either require the CUDA toolkit (I don’t think you should actually need a GPU, just the toolkit), or have separate CUDA and non-CUDA versions of your extension.<br/><NewLine>If going with separate versions you should be able to either have a shared codebase with separate CUDA specific stuff. Or use standard C++ tricks like defines to use the same codebase while excluding CUDA stuff on CPU only builds.<br/><NewLine>From the python setup.py end you’d want a <a href=""https://stackoverflow.com/questions/677577/distutils-how-to-pass-a-user-defined-parameter-to-setup-py"" rel=""nofollow noopener"">custom setuptools command</a> to allow custom command line options to control whether you install a <code>CUDAExtension</code> or a <code>CppExtension</code>. Or just have separate python packages.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for the great answer!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>No problem. Actually, while I think the basic idea of selective compilation is correct, you can do the setup side more easily by just checking for the CUDA toolkit and if detected install a <code>CUDAExtension</code> otherwise install a <code>CppExtension</code>. See torchvision for an example of how to do this nicely. Especially around <a href=""https://github.com/pytorch/vision/blob/49b01e3a7d9d248e2218c2c57d55346743bfb89d/setup.py#L103"" rel=""nofollow noopener"">here</a> where, removing all the actual config, you have this sort of thing:</p><NewLine><pre><code class=""lang-auto"">def get_extensions():<NewLine>    extension=CppExtension<NewLine>    if (torch.cuda.is_available() and CUDA_HOME is not None) or os.getenv('FORCE_CUDA', '0') == '1':<NewLine>        extension = CUDAExtension<NewLine>        define_macros += [('WITH_CUDA', None)]<NewLine>    return extension(define_macros)<NewLine><NewLine>setup(<NewLine>    ext_modules=get_extensions()<NewLine>)<NewLine></code></pre><NewLine><p>Where that define is used to add in CUDA stuff in the C++. Much cleaner than my suggestion of a command.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/TomB; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/sublee; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/TomB; <NewLine> ,"REPLY_DATE 1: October 2, 2019,  3:41am; <NewLine> REPLY_DATE 2: October 2, 2019,  3:42am; <NewLine> REPLY_DATE 3: October 4, 2019,  7:04am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
51191,Terminate called after throwing an instance of &lsquo;c10::Error&rsquo;,2019-07-21T09:22:12.635Z,0,1102,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using pytorch 1.1 (python 3.6, cuda 10, ubuntu 18.04).<br/><NewLine>Test with an official imagenet code <a href=""https://github.com/pytorch/examples/tree/master/imagenet"" rel=""nofollow noopener"">pytorch example imagenet</a>.</p><NewLine><p>Kept running into an unspecified launch failure:</p><NewLine><pre><code class=""lang-auto"">terminate called after throwing an instance of 'c10::Error'                                          <NewLine>  what():  CUDA error: unspecified launch failure (insert_events at /pytorch/c10/cuda/CUDACachingAllo<NewLine>cator.cpp:564)<NewLine>frame #0: std::function&lt;std::string ()&gt;::operator()() const + 0x11 (0x7fe33af7f441 in /home/ana<NewLine>conda3/envs/pt1.1/lib/python3.6/site-packages/torch/lib/libc10.so)                                   <NewLine>frame #1: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x2a (0x7fe33af7ed7a in /home/<NewLine>anaconda3/envs/pt1.1/lib/python3.6/site-packages/torch/lib/libc10.so)                          <NewLine>frame #2: &lt;unknown function&gt; + 0x13652 (0x7fe33a51e652 in /home/anaconda3/envs/pt1.1/lib/python<NewLine>3.6/site-packages/torch/lib/libc10_cuda.so)                                                          <NewLine>frame #3: c10::TensorImpl::release_resources() + 0x50 (0x7fe33af6fce0 in /home/anaconda3/envs/p<NewLine>t1.1/lib/python3.6/site-packages/torch/lib/libc10.so)                                                <NewLine>frame #4: &lt;unknown function&gt; + 0x30facb (0x7fe2d6406acb in /home/anaconda3/envs/pt1.1/lib/pytho<NewLine>n3.6/site-packages/torch/lib/libtorch.so.1)                                                          <NewLine>frame #5: &lt;unknown function&gt; + 0x1420bb (0x7fe33b5080bb in /home/anaconda3/envs/pt1.1/lib/pytho<NewLine>n3.6/site-packages/torch/lib/libtorch_python.so)                                                     <NewLine>frame #6: &lt;unknown function&gt; + 0x6be7c1 (0x7fe33ba847c1 in /home/anaconda3/envs/pt1.1/lib/pytho<NewLine>n3.6/site-packages/torch/lib/libtorch_python.so)                                                     <NewLine>frame #7: std::_Sp_counted_ptr&lt;c10d::Reducer*, (__gnu_cxx::_Lock_policy)2&gt;::_M_dispose() + 0x12 (0x7f<NewLine>e33ba84902 in /home/anaconda3/envs/pt1.1/lib/python3.6/site-packages/torch/lib/libtorch_python.<NewLine>so)                                                                                                  <NewLine>frame #8: std::_Sp_counted_base&lt;(__gnu_cxx::_Lock_policy)2&gt;::_M_release() + 0xa2 (0x7fe33b4e4bb2 in /<NewLine>home/anaconda3/envs/pt1.1/lib/python3.6/site-packages/torch/lib/libtorch_python.so)            <NewLine>frame #9: &lt;unknown function&gt; + 0x6b375b (0x7fe33ba7975b in /home/anaconda3/envs/pt1.1/lib/pytho<NewLine>n3.6/site-packages/torch/lib/libtorch_python.so)                                                     <NewLine>frame #10: &lt;unknown function&gt; + 0x12fbc7 (0x7fe33b4f5bc7 in /home/anaconda3/envs/pt1.1/lib/pyth<NewLine>on3.6/site-packages/torch/lib/libtorch_python.so)                                                    <NewLine>frame #11: &lt;unknown function&gt; + 0x12fe2e (0x7fe33b4f5e2e in /home/anaconda3/envs/pt1.1/lib/pyth<NewLine>on3.6/site-packages/torch/lib/libtorch_python.so)<NewLine>...<NewLine><NewLine>-- Process 0 terminated with the following error:<NewLine>Traceback (most recent call last):<NewLine>  File ""/home/anaconda3/envs/pt1.1/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"",<NewLine> line 19, in _wrap<NewLine>    fn(i, *args)<NewLine>  File ""/share/imagenet/main.py"", line 238, in main_worker<NewLine>    train(train_loader, model, criterion, optimizer, epoch, args)<NewLine>  File ""/share/imagenet/main.py"", line 301, in train<NewLine>    progress.display(i)<NewLine>  File ""/share/imagenet/main.py"", line 386, in display<NewLine>    entries += [str(meter) for meter in self.meters]<NewLine>  File ""/share/imagenet/main.py"", line 386, in &lt;listcomp&gt;<NewLine>    entries += [str(meter) for meter in self.meters]<NewLine>  File ""/share/imagenet/main.py"", line 375, in __str__<NewLine>    return fmtstr.format(**self.__dict__)<NewLine>  File ""/home/anaconda3/envs/pt1.1/lib/python3.6/site-packages/torch/tensor.py"", line 386, in $<NewLine>_format__<NewLine>    return self.item().__format__(format_spec)<NewLine>RuntimeError: CUDA error: unspecified launch failure<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/FreemanG,(Freeman G),FreemanG,"July 21, 2019,  9:22am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you solved the problem? I met the same error recently.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually, I am not sure how I solved this. After I re-installed pytorch, cuda/cudnn, nvidia-driver, the problem had gone away.</p><NewLine><p>Also, this could be a hardware problem, since the problem occured on a flawed workstation (multi-gpu training might make it crash).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/sherylwang; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/FreemanG; <NewLine> ,"REPLY_DATE 1: September 20, 2019,  3:33am; <NewLine> REPLY_DATE 2: September 20, 2019,  8:50am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
56103,"I have 8 GPUs ,buyt why my torch.cuda.device_count() return only 1?",2019-09-17T01:22:56.854Z,0,482,"<div class=""post"" itemprop=""articleBody""><NewLine><p>i use ‘nvidia-smi -L’ to list my avaiable gpus,<br/><NewLine><img alt=""image"" data-base62-sha1=""lQtb6eNquEzYBe39Mf4Cad6k7k8"" height=""190"" src=""https://discuss.pytorch.org/uploads/default/original/2X/9/991c104a79063a9194142085a549d0dd5dc0799c.png"" width=""620""/><br/><NewLine>.<br/><NewLine>i don’t set the CUDA_VISIBLE_DEVICES and run my script ‘print(torch.cuda.device_count())’ and it always return 1, anyone can tell me what’s happen? thx!</p><NewLine></div>",https://discuss.pytorch.org/u/Steve_Hu,(Steve Hu),Steve_Hu,"September 17, 2019,  1:22am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you check, if this environment variable is set somewhere in your code?<br/><NewLine>The line should e.g. look like this:</p><NewLine><pre><code class=""lang-auto"">os.environ[""CUDA_VISIBLE_DEVICES""]=""0""<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: September 20, 2019,  9:50am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
55524,Which function is invoked when do slicing on Tensor?,2019-09-09T09:53:42.134Z,0,100,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When two tensors are added together, for example,</p><NewLine><pre><code class=""lang-auto"">c = a + b<NewLine></code></pre><NewLine><p>the function <code>tensor.__add__</code> is invoked to really “add” two tensors.</p><NewLine><p>When a tensor is sliced on a given axis, which function is invoked?</p><NewLine><pre><code class=""lang-auto""># slice on axis=1<NewLine>b = a[:, 1:2]<NewLine></code></pre><NewLine><p>I searched for <code>*slice</code> on the pytorch reference documentation but found nothing.</p><NewLine></div>",https://discuss.pytorch.org/u/Crazyai,(gg),Crazyai,"September 9, 2019,  9:53am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>The <a href=""https://docs.python.org/3/reference/datamodel.html#object.__getitem__"" rel=""nofollow noopener""><code>__getitem__</code></a> method of that class is called.<br/><NewLine>If you’re interested of how pytorch implements this, it is routed to <a href=""https://github.com/pytorch/pytorch/blob/58a0dee749e1bbac92bdedf0379ac89158ca93d3/torch/csrc/autograd/python_variable_indexing.cpp#L284"" rel=""nofollow noopener"">this</a> function.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: September 9, 2019, 10:47pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
55401,How to save and restore entire model without class definition,2019-09-07T02:27:36.438Z,0,127,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, everyone!<br/><NewLine>I want to deploy my pytorch model and I don’t want other people to know the detail of my model’s parameter setting.<br/><NewLine>I noticed that when using save&amp;load, you should always include your model’s definition code, so I wonder if there exists a way that I can save the model weights and parameters simultaneously, and restore without the model class defined in somewhere.</p><NewLine></div>",https://discuss.pytorch.org/u/kevin_sandy,(kevin sandy),kevin_sandy,"September 7, 2019,  2:27am",,,,,
53922,Analyzing Hook drastically decreases performance,2019-08-21T10:22:25.932Z,0,437,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi. I’m using a PyTorch Hook to analyzing intermediate feature map’s statistics with a large-scale model training.<br/><NewLine>Also I’m using multi-GPU to accelerate the training process.(with torch.nn.DataParallel)</p><NewLine><p>The problem is, GPU utilization is drastically drops down when my Hook is executing(I compared GPU utilization with when I don’t activated my Hook - naive training without analyzing)</p><NewLine><p>I know this is Indispensable situation because my analyzing Hook is quite high-cost function, but all my multi-GPU are freezing in Hook phase, which seems not reasonable.</p><NewLine><p>I think there’s some solution, such as Hook-specify GPU setting, like:<br/><NewLine>GPU 0~3: model training<br/><NewLine>GPU 4: gather intermediate feature map and analyze</p><NewLine><p>Is it possible? Or is there any good solution to solve my problem?<br/><NewLine>Any answers will be welcomed. Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/FruitVinegar,(NHK),FruitVinegar,"August 21, 2019, 10:22am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Hooks are very slow for multiple reasons: They are python code and cannot run concurrently (due to python limitations). They require the value of the Tensor usually and so will for the GPU to sync all computation (wait for everything to run, execute, then start queuing more work). Since multi-gpu splits a batch between the gpus, the whole batch runs as slow as the slowest part of the batch. In your case, that would be the GPU that has the hooks.</p><NewLine><p>A possible solution could be to only collect these statistics every 10 batch?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry for late appreciation.<br/><NewLine>I totally re-wrote my code and changed my algorithm, so I can reduce 11 Days -&gt; 4 Days training time with my hooks.<br/><NewLine>But oscillating GPU Utilization, such as:</p><NewLine><p>almost 100% GPU util(training process: like backpropagation)<br/><NewLine>-&gt; 4~10% GPU util(my hooks; even I wrote all operations to be executed in GPU)<br/><NewLine>-&gt; 100%…<br/><NewLine>(repeating)</p><NewLine><p>This speed-difference between optimized training process and my custom task is still bottleneck.<br/><NewLine>I hope pytorch to have more powerful optimizing semantics on hooks, cause it’s very useful tool for researchers.</p><NewLine><p>Thanks again for your answers.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/FruitVinegar; <NewLine> ,"REPLY_DATE 1: August 21, 2019,  7:17pm; <NewLine> REPLY_DATE 2: September 6, 2019,  4:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
55167,Torch 1.1.0.post2 doesn&rsquo;t contain &ldquo;manylinux&rdquo; wheel files. Who can push them?,2019-09-04T17:23:34.878Z,0,127,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m using snorkel on a centos machine. And it’s trying to download torch 1.1.0.post2 but it’s not installing properly.</p><NewLine><p>According to <a href=""https://pypi.org/project/torch/1.1.0.post2/#files"" rel=""nofollow noopener"">https://pypi.org/project/torch/1.1.0.post2/#files</a><br/><NewLine>The manylinux versions don’t exist for the different python versions. Who can push them?</p><NewLine><p>Compare:<br/><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://pypi.org/static/images/favicon.6a76275d.ico"" width=""16""/><NewLine><a href=""https://pypi.org/project/torch/1.1.0/#files"" rel=""nofollow noopener"" target=""_blank"">PyPI</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://pypi.org/static/images/twitter.c0030826.jpg"" width=""60""/><NewLine><h3><a href=""https://pypi.org/project/torch/1.1.0/#files"" rel=""nofollow noopener"" target=""_blank"">torch</a></h3><NewLine><p>Tensors and Dynamic neural networks in Python with strong GPU acceleration</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://pypi.org/static/images/favicon.6a76275d.ico"" width=""16""/><NewLine><a href=""https://pypi.org/project/torch/1.1.0.post2/#files"" rel=""nofollow noopener"" target=""_blank"">PyPI</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://pypi.org/static/images/twitter.c0030826.jpg"" width=""60""/><NewLine><h3><a href=""https://pypi.org/project/torch/1.1.0.post2/#files"" rel=""nofollow noopener"" target=""_blank"">torch</a></h3><NewLine><p>Tensors and Dynamic neural networks in Python with strong GPU acceleration</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>",https://discuss.pytorch.org/u/lapolonio,(Leonardo Apolonio),lapolonio,"September 4, 2019,  5:23pm",,,,,
54822,Decomposing Conv Layer,2019-08-31T07:17:39.757Z,0,127,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to decompose the first conv layer of pretrained vgg16 using the package tensorly .and trying to fine tune as follows.</p><NewLine><p>model = models.vgg16(pretrained=True)<br/><NewLine>layer = model.features._modules[‘0’]<br/><NewLine>factors=parafac(layer.data, 16, init=‘svd’)<br/><NewLine>layer = tl.kruskal_to_tensor(factors)<br/><NewLine>model.features._modules[‘0’] = layer<br/><NewLine>model = model.to(device)</p><NewLine><p>I am getting the following error</p><NewLine><p>AttributeError: ‘Tensor’ object has no attribute ‘_apply’</p><NewLine><p>Can anyone help?</p><NewLine></div>",https://discuss.pytorch.org/u/remya,(remya),remya,"August 31, 2019,  7:17am",,,,,
53782,Blas version the pytorch uses,2019-08-19T23:18:19.285Z,0,261,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is there any way to check which Blas lib pytorch uses in the runtime? Is it MKL or OpenBlas?</p><NewLine><p>I checked the loaded dll, and find both MKL and openblas lib there.</p><NewLine></div>",https://discuss.pytorch.org/u/amsword,(Jianfeng Wang),amsword,"August 19, 2019, 11:18pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-python"">import torch<NewLine>print(*torch.__config__.show().split(""\n""), sep=""\n"")<NewLine></code></pre><NewLine><p>PS: this works for PyTorch 1.1 or earlier.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/LeviViana; <NewLine> ,"REPLY_DATE 1: August 20, 2019, 12:49pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
53741,Can pytorch 1.2 support cuda 9.0?,2019-08-19T13:48:24.310Z,0,745,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I try to install pytorch 1.2.0 with anaconda, but found that  <code>torch.cuda.is_available()</code>  always returns False. Turns out our server environment can only support CUDA 9.0, but I do not have administrator right to upgrade the nvidia driver.<br/><NewLine>Give that pytorch 1.2.0 introduced Bool type, and I have already updated my code for corresponding change of API, I do not want to undo this efforts.<br/><NewLine>Will pytorch 1.2.0 support CUDA 9.0 backend in the future?</p><NewLine></div>",https://discuss.pytorch.org/u/ano,(ano),ano,"August 19, 2019,  1:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You could build PyTorch from source as described <a href=""https://github.com/pytorch/pytorch#from-source"" rel=""nofollow noopener"">here</a> and use your local CUDA installation to build it.<br/><NewLine>The binaries currently ship with CUDA9.2 and CUDA10.0.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: August 19, 2019,  2:02pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
53186,Memory problems in torch.from_numpy(),2019-08-12T15:25:35.298Z,0,205,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, this is my first post as often I encouter that all my PyTorch questions have been answered here at some point. Please accept my apologies if it happens this question has been already addressed.</p><NewLine><p>In particular, I found a problem regarding converting variables from numpy to torch; I found that using torch.from_numpy() on big variables results in what appears to be a memory leakage. Here’s a very simple code in which I create a dictionary where each key will hold a 66x2 array. If I create the array beforehand and convert it through torch.from_numpy (Option 1), I find a MemoryError message when trying to dump the variable onto the file. If instead I convert each 66x2 array to torch “on-the-fly”, I find no errors when dumping the files. Does anyone have a clue about why this might be happening?</p><NewLine><p>Many thanks <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>import pickle, torch, numpy as np<br/><NewLine>######### ----------- Option 1 ------------ #################<br/><NewLine>keys = [’{:06d}’.format(k) for k in range(10000)]<br/><NewLine>db = {}<br/><NewLine>points = np.random.randn(len(keys),66,2).astype(‘float’)<br/><NewLine>points = torch.from_numpy(points).float()<br/><NewLine>for i,k in enumerate(keys):<br/><NewLine>db[k] = points[i,:,:]</p><NewLine><p>pickle.dump(db,open(‘fromnumpy.pkl’,‘wb’))</p><NewLine><blockquote><NewLine><blockquote><NewLine><blockquote><NewLine><p>MemoryError</p><NewLine></blockquote><NewLine></blockquote><NewLine></blockquote><NewLine><p>######## ----------- Option 2 --------------##################<br/><NewLine>keys = [’{:06d}’.format(k) for k in range(10000)]<br/><NewLine>db = {}<br/><NewLine>points = np.random.randn(len(keys),66,2).astype(‘float’)</p><NewLine><p>for i,k in enumerate(keys):<br/><NewLine>db[k] = torch.from_numpy(points[i,:,:]).float()</p><NewLine><p>pickle.dump(db,open(‘fromnumpy.pkl’,‘wb’))</p><NewLine></div>",https://discuss.pytorch.org/u/Enrique,(Enrique Sanchez Lozano),Enrique,"August 12, 2019,  3:25pm",,,,,
52362,Officially supported CPU architectures?,2019-08-02T00:06:54.225Z,0,120,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Do we have an official list of supported CPU architectures? For example, is the 32-bit x86 supported?</p><NewLine></div>",https://discuss.pytorch.org/u/xuhdev,(Hong Xu),xuhdev,"August 2, 2019, 12:06am",,,,,
52059,Transfer from Tensorflow to Pytorch,2019-07-30T13:56:53.356Z,0,153,"<div class=""post"" itemprop=""articleBody""><NewLine><p>a, b = sess.run([c, d], feed_dict={…}), how to substitute this in pytorch?</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/53bc702f9e456997f4c4ab4bb6e27fb6e54d3cf1"" href=""https://discuss.pytorch.org/uploads/default/original/2X/5/53bc702f9e456997f4c4ab4bb6e27fb6e54d3cf1.png"" title=""Screenshot 2019-07-30 at 9.56.14 PM.png""><img alt=""14%20PM"" data-base62-sha1=""bWLm8DxyhcNOVTirmF67nn9WZk5"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/53bc702f9e456997f4c4ab4bb6e27fb6e54d3cf1_2_10x10.png"" height=""49"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/53bc702f9e456997f4c4ab4bb6e27fb6e54d3cf1_2_690x49.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/53bc702f9e456997f4c4ab4bb6e27fb6e54d3cf1_2_690x49.png, https://discuss.pytorch.org/uploads/default/optimized/2X/5/53bc702f9e456997f4c4ab4bb6e27fb6e54d3cf1_2_1035x73.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/5/53bc702f9e456997f4c4ab4bb6e27fb6e54d3cf1_2_1380x98.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screenshot 2019-07-30 at 9.56.14 PM.png</span><span class=""informations"">1924×138 27.8 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/111121,(ChenXiaoMeng),111121,"July 30, 2019,  1:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>In PyTorch you can directly use all operations without compiling a static graph and use sessions (similar to numpy, but with GPUs and Autograd).<br/><NewLine>Have a look at the <a href=""https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"" rel=""nofollow noopener"">tutorials</a> for a quick start <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 30, 2019, 10:14pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
50942,How to extract torch.nn.conv1d complete python source code,2019-07-18T10:48:51.150Z,0,141,"<div class=""post"" itemprop=""articleBody""><NewLine><p>where do i find torch.nn.conv1d source code as shown below?</p><NewLine><pre><code class=""lang-auto"">fun= torch.nn.Conv1d(in_channels,out_channels,kernel_size,1,groups=in_channels,bias=False,padding=2,dilation=2)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/A.V_Yugesh,(A V Yugesh),A.V_Yugesh,"July 23, 2019,  7:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>In the pytorch documentation, each class will have a source in the right side. Just click that and read about it</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Balamurali_M; <NewLine> ,"REPLY_DATE 1: July 25, 2019,  2:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
51455,Need a Help in executing the Project,2019-07-24T03:09:54.537Z,0,113,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am new to python like absolute new to coding too but for my college project i need to execute this project which is available in Github <a href=""https://github.com/anujdutt9/Disease-Prediction-from-Symptoms"" rel=""nofollow noopener"">https://github.com/anujdutt9/Disease-Prediction-from-Symptoms</a> . I can learn python but it takes time but I need to submit this project within two days and i dont know how to execute it i have installed anaconda and jupyter notebook but cant able to execute…</p><NewLine><p>Please Help very urgent…</p><NewLine></div>",https://discuss.pytorch.org/u/Mark_Steve,(Mark Steve),Mark_Steve,"July 24, 2019,  3:09am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Since this seems to be a homework assignment, I think it would be inappropriate to solve it for you.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 24, 2019,  5:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
51372,Libtorch for Arm Core is not yet?,2019-07-23T10:20:03.366Z,0,372,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, All.</p><NewLine><p>Libtorch for Arm Core is not yet?<br/><NewLine>In Raspberry Pi 3B, I just failed install.</p><NewLine><p>I wonder …</p><NewLine><p>Thanks in advance.</p><NewLine><p><span class=""mention"">@bemoregt</span>.</p><NewLine></div>",https://discuss.pytorch.org/u/bemorept,(Wonwoo Park),bemorept,"July 23, 2019, 10:20am",1 Like,,,,
51352,Can Pytorch serving in c++ like tfserving accept proto?,2019-07-23T07:59:36.208Z,1,299,"<div class=""post"" itemprop=""articleBody""><NewLine><p>TFserving use prediction_service.proto for gRPC request/response ,and I know  ""LOADING A PYTORCH MODEL IN C++ "" from this <a href=""https://pytorch.org/tutorials/advanced/cpp_export.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/cpp_export.html</a> ,like this:</p><NewLine><p>// Create a vector of inputs.<br/><NewLine>std::vector<a>torch::jit::IValue</a> inputs;<br/><NewLine>inputs.push_back(torch::ones({1, 3, 224, 224}));</p><NewLine><p>// Execute the model and turn its output into a tensor.<br/><NewLine>at::Tensor output = module-&gt;forward(inputs).toTensor();</p><NewLine><p>std::cout &lt;&lt; output.slice(/<em>dim=</em>/1, /<em>start=</em>/0, /<em>end=</em>/5) &lt;&lt; ‘\n’;</p><NewLine><p>and create a vector of  <code>torch::jit::IValue</code>  (a type-erased value type  <code>script::Module</code>  methods accept and return) and add a single input.<br/><NewLine><strong>Is it possible torch::jit::IValue can like PredictRequest  or PredictResponse that can be SerializeToString ParseFromString</strong> ,in that way ,so I can deploy it more conveniently</p><NewLine><p><strong>Can libtorch C++ provide PB protocol (or any others protocol  can be Serialize and Deserialize) as  module-&gt;forward input and output parameters,or torch::jit::IValue provide Serialize and Deserialize method?</strong></p><NewLine></div>",https://discuss.pytorch.org/u/jun_yu,(jun yu),jun_yu,"July 23, 2019,  8:19am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please don’t try to tag certain people, as this might discourage others to provide an answer.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 23, 2019, 10:06am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
51333,Does the Transfer learning or fine-tuned Pytorch model could not converted to Caffe model or ONNX?,2019-07-23T05:11:33.620Z,0,110,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, All.</p><NewLine><p>Does the Transfer learning or fine-tuned Pytorch model could not converted to Caffe model or ONNX?</p><NewLine><p>Or does the conversion work well, and does not model the inference code optimization?</p><NewLine><p>Have you ever tried Caffe, or onnx Inference, as a transfer learning model?</p><NewLine></div>",https://discuss.pytorch.org/u/bemorept,(Wonwoo Park),bemorept,"July 23, 2019,  5:11am",,,,,
51285,Throwing this Error &ldquo;Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same&rdquo; when I tried to REMOVE nn.sequential using dictionaries,2019-07-22T16:07:04.880Z,0,146,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">class TemporalConvNet(nn.Module):<NewLine>    def __init__(self, N, B, H, P, X, R, C, norm_type=""gLN"", causal=False,<NewLine>                 mask_nonlinear='relu'): <NewLine><NewLine>        super(TemporalConvNet, self).__init__()<NewLine>        # Hyper-parameter<NewLine>        self.C = C<NewLine>        self.X = X<NewLine>#        self.z = []<NewLine>        self.mask_nonlinear = mask_nonlinear<NewLine>        # Components<NewLine>        # [M, N, K] -&gt; [M, N, K]<NewLine>        self.layer_norm = ChannelwiseLayerNorm(N)<NewLine>        # [M, N, K] -&gt; [M, B, K]<NewLine>        self.bottleneck_conv1x1 = nn.Conv1d(N, B, 1, bias=False)<NewLine>        # [M, B, K] -&gt; [M, N, K]<NewLine>        self.dwsConv={}<NewLine>        self.conv1x1={}<NewLine>        self.prelu={}<NewLine>        self.norm={}<NewLine>        self.pwConv={}<NewLine>        for d in range(X):<NewLine>            self.dwsConv[""depthwise_conv{}"".format(d)] = nn.Conv1d(N, N, P,<NewLine>                                                           stride=1, padding=2**d,<NewLine>                                                           dilation=2**d, groups=N,<NewLine>                                                           bias=False)<NewLine>            self.conv1x1[""conv1d{}"".format(d)] = nn.Conv1d(B, N, 1, bias=False)<NewLine>            self.prelu[""prelu{}"".format(d)] = nn.PReLU()<NewLine>            self.prelu[""prelu{}"".format(d+100)] = nn.PReLU()<NewLine>            self.norm[""norm{}"".format(d)] = chose_norm(norm_type, N)<NewLine>            self.norm[""norm{}"".format(d+100)] = chose_norm(norm_type, N)<NewLine>        # [M, N, K] -&gt; [M, B, K]<NewLine>            self.pwConv[""pointwise_conv{}"".format(d)] = nn.Conv1d(N, B, 1, bias=False)<NewLine>        # [M, B, K] -&gt; [M, C*N, K]  <NewLine>        self.mask_conv1x1 = nn.Conv1d(B, C*N, 1, bias=False)<NewLine><NewLine><NewLine><NewLine>    def forward(self, x):<NewLine>        """"""<NewLine>        Keep this API same with TasNet<NewLine>        Args:<NewLine>            mixture_w: [M, N, K], M is batch size<NewLine>        returns:<NewLine>            est_mask: [M, C, N, K]<NewLine>        """"""<NewLine>        <NewLine>        M, N, K = x.size()<NewLine>        x = self.layer_norm(x)<NewLine>        x = self.bottleneck_conv1x1(x)<NewLine>        <NewLine>        for d in range(self.X):<NewLine>            y = self.conv1x1[""conv1d{}"".format(d)](x)<NewLine>            y = self.prelu[""prelu{}"".format(d)](y)<NewLine>            y = self.norm[""norm{}"".format(d)](y)<NewLine>            y = self.dwsConv[""depthwise_conv{}"".format(d)]<NewLine>            y = self.prelu[""prelu{}"".format(d+100)](y)<NewLine>            y = self.norm[""norm{}"".format(d+100)](y)<NewLine>            y = self.pwConv[""pointwise_conv{}"".format(d)](y)<NewLine>            x = x + y<NewLine>        <NewLine>        score = self.mask_conv1x1(x)<NewLine>#        score = self.network(mixture_w)  # [M, N, K] -&gt; [M, C*N, K]<NewLine>        score = score.view(M, self.C, N, K) # [M, C*N, K] -&gt; [M, C, N, K]<NewLine>        if self.mask_nonlinear == 'softmax':<NewLine>            est_mask = F.softmax(score, dim=1)<NewLine>        elif self.mask_nonlinear == 'relu':<NewLine>            est_mask = F.relu(score)<NewLine>        else:<NewLine>            raise ValueError(""Unsupported mask non-linear function"")<NewLine>        return est_mask<NewLine></code></pre><NewLine><p>In the code above I removed sequential layers and replaced them with dictionary as it was difficult to check output at each layer.<br/><NewLine>After removing nn.sequential,  newly initialized layer weights are not considered as same sub-module, I guess. They are initialized as non-CUDA tensors.</p><NewLine><p>Whats the solution? how to initialize them as torch.cuda.FloatTensor?<br/><NewLine>Is there any other way to remove sequential for the code above?<br/><NewLine>Thanks in Advance.</p><NewLine></div>",https://discuss.pytorch.org/u/Sasank_Kottapalli,(Sasank Kottapalli),Sasank_Kottapalli,"July 22, 2019,  4:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Plain Python <code>dicts</code> won’t be properly registered as submodules inside your model (similar to Python <code>lists</code>).<br/><NewLine>Try to use <code>nn.ModuleDict</code> instead.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 22, 2019,  8:49pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
49589,The problem about Loading a Pytorch Model in C++,2019-07-03T09:52:56.529Z,6,768,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I follow the tutorial and use the same code but it comes up a problem</p><NewLine><h4>*** Error in `./example-app': free(): invalid pointer: 0x00007ffe733866b0 ***</h4><NewLine>I run them in the centos, How can I solve this problem?<NewLine>        </div>",https://discuss.pytorch.org/u/kevinhaoliu,(kevin),kevinhaoliu,"July 3, 2019,  9:52am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you post the complete stack trace, if you have it?<br/><NewLine>You are apparently trying to free something which isn’t pointing to a “freeable” memory address.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>stack trace:<div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/25865c544fdb1287e9e94e172cf88e0aadc67150"" href=""https://discuss.pytorch.org/uploads/default/original/2X/2/25865c544fdb1287e9e94e172cf88e0aadc67150.png"" title=""stack trace.png""><img alt=""stack%20trace"" data-base62-sha1=""5lXx24gotjSuzjtjX8ySd15CADe"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/2/25865c544fdb1287e9e94e172cf88e0aadc67150_2_10x10.png"" height=""158"" src=""https://discuss.pytorch.org/uploads/default/original/2X/2/25865c544fdb1287e9e94e172cf88e0aadc67150.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">stack trace.png</span><span class=""informations"">1694×390 20.7 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>But I only run the same code by following the tutorial.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I update the g++ from 4.8.5 to 4.9.2 and it works.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I update the g++ too, but still have the same problem.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>What’s your g++ version?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>gcc version 4.9.2 20150212 (Red Hat 4.9.2-6) (GCC)</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can go to <a href=""https://github.com/pytorch/pytorch/issues/21627"" rel=""nofollow noopener"">this</a> to explore possible solutions.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry, I re-compile the project and  no more free() problem.<br/><NewLine>It is another one <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>operation failed in interpreter:<br/><NewLine>op_version_set = 0<br/><NewLine>def forward(self,<br/><NewLine>input: Tensor) -&gt; Tensor:<br/><NewLine>if bool(torch.gt(torch.sum(input), 0)):<br/><NewLine>output = torch.mv(self.weight, input)<br/><NewLine>~~~~~~~~ &lt;— HERE<br/><NewLine>else:<br/><NewLine>output_2 = torch.add(self.weight, input, alpha=1)<br/><NewLine>output = output_2<br/><NewLine>return output</p><NewLine><p>Abandon (core dumped)</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you please show me your C++ code?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>I use the tutorial code :</p><NewLine><p><span class=""hashtag"">#include</span> &lt;torch/script.h&gt; // One-stop header.</p><NewLine><p><span class=""hashtag"">#include</span> <br/><NewLine><span class=""hashtag"">#include</span> </p><NewLine><p>int main(int argc, const char* argv[]) {<br/><NewLine>if (argc != 2) {<br/><NewLine>std::cerr &lt;&lt; “usage: example-app \n”;<br/><NewLine>return -1;<br/><NewLine>}</p><NewLine><p>// Deserialize the ScriptModule from a file using torch::jit::load().<br/><NewLine>std::shared_ptr<a>torch::jit::script::Module</a> module = torch::jit::load(argv[1]);</p><NewLine><p>assert(module != nullptr);<br/><NewLine>std::cout &lt;&lt; “ok\n”;<br/><NewLine>/ Create a vector of inputs.<br/><NewLine>std::vector<a>torch::jit::IValue</a> inputs;<br/><NewLine>inputs.push_back(torch::ones({1, 3, 224, 224}));</p><NewLine><p>// Execute the model and turn its output into a tensor.<br/><NewLine>at::Tensor output = module-&gt;forward(inputs).toTensor();</p><NewLine><p>std::cout &lt;&lt; output.slice(/<em>dim=</em>/1, /<em>start=</em>/0, /<em>end=</em>/5) &lt;&lt; ‘\n’;<br/><NewLine>}</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>And I use this code to create the “model.pt”</p><NewLine><p>import torch</p><NewLine><p>import torchvision</p><NewLine><h1>An instance of your model.</h1><NewLine><p>model = torchvision.models.resnet18()</p><NewLine><h1>An example input you would normally provide to your model’s forward() method.</h1><NewLine><p>example = torch.rand(1, 3, 224, 224)</p><NewLine><p>print(example)</p><NewLine><h1>Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.</h1><NewLine><p>traced_script_module = torch.jit.trace(model, example)</p><NewLine><p>traced_script_module.save(“model.pt”)</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://discuss.pytorch.org/t/loading-batchnorm1d-with-jit/37552"">It</a> has the similar problem with you, you can see this for solution.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you. I have the version 1.1.0 but still have the same problem with “torch.mv” function :</p><NewLine><p>terminate called after throwing an instance of ‘std::runtime_error’<br/><NewLine>what():<br/><NewLine>mv: Expected 1-D argument vec, but got 4-D (check_1d at /pytorch/aten/src/ATen/native/LinearAlgebra.cpp:143)<br/><NewLine>frame <span class=""hashtag"">#0:</span> std::function&lt;std::string ()&gt;::operator()() const + 0x11 (0x7f30c5a46441 in /home/pixur/Documents/Zaynab/TestLibtorch/libtorch/lib/libc10.so)</p><NewLine><p>I didn’t mke any chnge in the code, I just follow the tutorial</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>I used the wrong model.pt. It working now. Thank you!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kevinhaoliu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/kevinhaoliu; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/kevinhaoliu; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/zaynabhabibi; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/kevinhaoliu; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/zaynabhabibi; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/kevinhaoliu; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/zaynabhabibi; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/kevinhaoliu; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/zaynabhabibi; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/zaynabhabibi; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/kevinhaoliu; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/zaynabhabibi; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/zaynabhabibi; <NewLine> ,"REPLY_DATE 1: July 3, 2019,  4:44pm; <NewLine> REPLY_DATE 2: July 4, 2019,  2:18am; <NewLine> REPLY_DATE 3: July 4, 2019,  2:22am; <NewLine> REPLY_DATE 4: July 16, 2019,  9:24am; <NewLine> REPLY_DATE 5: July 22, 2019,  9:33am; <NewLine> REPLY_DATE 6: July 22, 2019,  9:34am; <NewLine> REPLY_DATE 7: July 22, 2019,  9:36am; <NewLine> REPLY_DATE 8: July 22, 2019,  9:42am; <NewLine> REPLY_DATE 9: July 22, 2019,  9:43am; <NewLine> REPLY_DATE 10: July 22, 2019,  9:47am; <NewLine> REPLY_DATE 11: July 22, 2019,  9:50am; <NewLine> REPLY_DATE 12: July 22, 2019,  9:56am; <NewLine> REPLY_DATE 13: July 22, 2019, 10:03am; <NewLine> REPLY_DATE 14: July 22, 2019, 12:50pm; <NewLine> REPLY_DATE 15: July 22, 2019,  1:00pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> 
51240,How to use smapler,2019-07-22T06:52:44.415Z,1,186,"<div class=""post"" itemprop=""articleBody""><NewLine><p>i know how to make and use a custom dataset. but i am not sure how to make and sue a custom sampler.<br/><NewLine>can someone pls tell me how a sampler works</p><NewLine><p>for example when i make a custom dataset, i simple write custom_dataset[index] and it outputs the data present at that index. ,and further when i pass this custom_dataset into a batch loader it creates batches of those data. its clear and simple</p><NewLine><p>but in case of a custom sample, how to i get a sample data out of the sampler ?</p><NewLine><p>it would be great if you could me the working of a sampler. Thanks in advance <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/n0obcoder,(n0obcoder),n0obcoder,"July 22, 2019,  6:52am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The sampler returns the indices used to index your <code>Dataset</code>.<br/><NewLine>Have a look at <a href=""https://github.com/pytorch/pytorch/blob/b8c8977be797cb16354aa97c3fcdc910d3e9e5c5/torch/utils/data/sampler.py#L103"" rel=""nofollow noopener"">these implementations</a> or the implemented samplers.<br/><NewLine>As you can see, the <code>__iter__</code> method returns an iterator or a generator.</p><NewLine><p>How should your custom sampler generate the indices?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>i was going through <a href=""https://github.com/adambielski/siamese-triplet"" rel=""nofollow noopener"">https://github.com/adambielski/siamese-triplet</a> and i was not able to figure out whats the use of a batch sampler that has been written in datasets.py</p><NewLine><p>So i wanted to know how and where are these batch samplers are used.</p><NewLine><p>I am still clueless</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/n0obcoder; <NewLine> ,"REPLY_DATE 1: July 22, 2019,  9:09am; <NewLine> REPLY_DATE 2: July 22, 2019, 12:34pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
50919,ConvTranspose3d cuDNN bug non-contiguous input continues to persist,2019-07-18T08:43:06.196Z,0,140,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Continue to have the problem</p><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/1664"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://github.com/edgarriba"" rel=""nofollow noopener""><NewLine><img class=""thumbnail onebox-avatar"" height=""96"" src=""https://avatars1.githubusercontent.com/u/5157099?v=2&amp;s=96"" width=""96""/><NewLine></a><NewLine><h4><a href=""https://github.com/pytorch/pytorch/issues/1664"" rel=""nofollow noopener"" target=""_blank"">Issue: RuntimeError: CUDNN_STATUS_NOT_SUPPORTED</a></h4><NewLine><div class=""date"" style=""margin-top:10px;""><NewLine><div class=""user"" style=""margin-top:10px;""><NewLine>	opened by <a href=""https://github.com/edgarriba"" rel=""nofollow noopener"" target=""_blank"">edgarriba</a><NewLine>	on <a href=""https://github.com/pytorch/pytorch/issues/1664"" rel=""nofollow noopener"" target=""_blank"">2017-05-26</a><NewLine></div><NewLine><div class=""user""><NewLine>	closed by <a href=""https://github.com/apaszke"" rel=""nofollow noopener"" target=""_blank"">apaszke</a><NewLine>	on <a href=""https://github.com/pytorch/pytorch/issues/1664"" rel=""nofollow noopener"" target=""_blank""></a><NewLine></div><NewLine></div><NewLine><pre class=""content"" style=""white-space: pre-wrap;"">I found this runtime error<NewLine>RuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.<NewLine>The code reproducing the error...</pre><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">high priority</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">module: cudnn</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">topic: dependency bug</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>which appears to be a hot topic in GitHub even today:</p><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/4107"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://github.com/nikostr"" rel=""nofollow noopener""><NewLine><img class=""thumbnail onebox-avatar"" height=""420"" src=""https://avatars1.githubusercontent.com/u/11029073?v=2&amp;s=96"" width=""420""/><NewLine></a><NewLine><h4><a href=""https://github.com/pytorch/pytorch/issues/4107"" rel=""nofollow noopener"" target=""_blank"">Issue: CUDNN_STATUS_NOT_SUPPORTED for large matrix input</a></h4><NewLine><div class=""date"" style=""margin-top:10px;""><NewLine><div class=""user"" style=""margin-top:10px;""><NewLine>	opened by <a href=""https://github.com/nikostr"" rel=""nofollow noopener"" target=""_blank"">nikostr</a><NewLine>	on <a href=""https://github.com/pytorch/pytorch/issues/4107"" rel=""nofollow noopener"" target=""_blank"">2017-12-11</a><NewLine></div><NewLine><div class=""user""><NewLine></div><NewLine></div><NewLine><pre class=""content"" style=""white-space: pre-wrap;"">The following code (adapted from https://discuss.pytorch.org/t/cudnn-status-not-supported-error-occurs-when-apply-autograd-grad-to-compute-high-order-differentiation/8256) gives me a CUDNN_STATUS_NOT_SUPPORTED error. I'm running the pytorch 0.3.0 with cuDNN 7.0.4, CUDA 9.0.176,...</pre><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">module: cudnn</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Can anything be done, for example, contact NVIDIA?</p><NewLine></div>",https://discuss.pytorch.org/u/Nicolo_Savioli,(Nicolò Savioli),Nicolo_Savioli,"July 18, 2019,  8:46am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Nicolo,</p><NewLine><p>I couldn’t reproduce the issue with the last code snippet posted <a href=""https://github.com/pytorch/pytorch/issues/4107#issuecomment-512336351"" rel=""nofollow noopener"">here</a>. Is this code raising this exception on your system?<br/><NewLine>If not, could you provide an executable code snippet so that we can reproduce this issue?</p><NewLine><p>We are currently working on a similar fix to <a href=""https://github.com/pytorch/pytorch/issues/22496"" rel=""nofollow noopener"">this issue</a>, which is also related to large inputs/outputs.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I think the problem is due to large matrices. I don’t think it’s a problem of torch but rather of CuDNN, the problem is that working high resolution with volumes here is impossible.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Nicolo_Savioli; <NewLine> ,"REPLY_DATE 1: July 18, 2019, 12:04pm; <NewLine> REPLY_DATE 2: July 20, 2019,  9:58pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
50611,"Values with the same index in a sparse tensor, how to only remain the max value when convert the sparse tensor to an dense one",2019-07-15T03:22:03.033Z,1,157,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Values with the same index in a sparse tensor are accumulated when  use torch.cuda.sparse.FloatTensor(i, v, torch.Size([2,3])).to_dense().</p><NewLine><p>How to only remain the max value when  convert the sparse tensor to an dense one.<br/><NewLine>Any tricks to implement this?</p><NewLine><p>Specifically,<br/><NewLine>indices = torch.cuda.LongTensor([[0, 1, 0], [2, 0, 2]])<br/><NewLine>values = torch.cuda.FloatTensor([3, 4, 5])<br/><NewLine>t = torch.cuda.sparse.FloatTensor(i, v, torch.Size([2,3])).to_dense()<br/><NewLine>then<br/><NewLine>t’s values are [[ 0  0  5],[  4  0  0]] rather than [[ 0  0  8],[  4  0  0]].</p><NewLine></div>",https://discuss.pytorch.org/u/peter_zhu,(peter ),peter_zhu,"July 15, 2019,  3:22am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is <strong>NOT</strong> what sparse tensors want to do! The sparse tensor represents <code>[[ 0 0 8],[ 4 0 0]]</code>.</p><NewLine><p>The operation you want is requested in <a href=""https://github.com/pytorch/pytorch/issues/22378"" rel=""nofollow noopener"">#22378</a>, which also links to a great <a href=""https://pytorch-scatter.readthedocs.io/en/latest/functions/max.html"" rel=""nofollow noopener"">third party implementation of <code>scatter_max</code></a>.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: July 15, 2019,  8:26am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
50185,CPU support for LibTorch,2019-07-10T03:05:13.692Z,1,158,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Could someone provide a minimal code to use LibTorch on a machine that does not have CUDA? I was running into errors trying the same.</p><NewLine></div>",https://discuss.pytorch.org/u/pratyush911,(Pratyush Maini),pratyush911,"July 10, 2019,  3:05am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>it is already supported with CPU in libtorch. You simply needs convert your tensor to device CPU</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Okay I realized that there is a different download option on the website for that. Thanks <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/><br/><NewLine><a class=""onebox"" href=""https://download.pytorch.org/libtorch/cpu/libtorch-win-shared-with-deps-latest.zip"" rel=""nofollow noopener"" target=""_blank"">https://download.pytorch.org/libtorch/cpu/libtorch-win-shared-with-deps-latest.zip</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jinfagang; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pratyush911; <NewLine> ,"REPLY_DATE 1: July 10, 2019,  3:57am; <NewLine> REPLY_DATE 2: July 10, 2019,  5:42am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
50128,How to retrieve weights of individual layers from sequential .network,2019-07-09T13:32:08.832Z,0,101,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">""""""         Sequence of operations           """"""<NewLine><NewLine>        mixture_w , mixture_ws = self.encoder(mixture)<NewLine>        est_mask = self.separator(mixture_w)<NewLine>        est_source = self.decoder(mixture_ws, est_mask)<NewLine><NewLine>class Encoder(nn.Module):<NewLine>    """"""Estimation of the nonnegative mixture weight by a 1-D conv layer.<NewLine>    """"""<NewLine>    def __init__(self, L, N):<NewLine>        super(Encoder, self).__init__()<NewLine>        # Hyper-parameter<NewLine>        self.L, self.N = L, N<NewLine>        # Components<NewLine>        # 50% overlap<NewLine>        self.conv1d_U = nn.Conv1d(1, N, kernel_size=L, stride=L // 2, bias=False)<NewLine><NewLine>    def forward(self, mixture):<NewLine>        """"""<NewLine>        Args:<NewLine>            mixture: [M, T], M is batch size, T is #samples<NewLine>        Returns:<NewLine>            mixture_w: [M, N, K], where K = (T-L)/(L/2)+1 = 2T/L-1<NewLine>        """"""<NewLine>        mixture = torch.unsqueeze(mixture, 1)  # [M, 1, T]<NewLine>        mixture_w = F.relu(self.conv1d_U(mixture))  # [M, N, K]<NewLine>        mixture_ws = F.sigmoid(self.conv1d_U(mixture))<NewLine>        return mixture_w<NewLine><NewLine></code></pre><NewLine><p>From the above code in the model, I am able to retrieve weights of conv1d_U shown below:<br/><NewLine>by using model.encoder.con1d_U.weight</p><NewLine><pre><code class=""lang-auto""> self.conv1d_U = nn.Conv1d(1, N, kernel_size=L, stride=L // 2, bias=False)<NewLine></code></pre><NewLine><p>When I tried to retrieve weights of bottleneck_conv1x1 shown in the code below. I couldn’t find bottleneck_conv1x1 in the model.seperator.network.<br/><NewLine>Can you explain me how to retrieve weights of individual layers? when they are sequentially coded in a .network, like the one in my case?</p><NewLine><pre><code class=""lang-auto"">class TemporalConvNet(nn.Module):<NewLine>    def __init__(self, N, B, H, P, X, R, C, norm_type=""gLN"", causal=False,<NewLine>                 mask_nonlinear='relu'):<NewLine>        """"""<NewLine>        Args:<NewLine>            N: Number of filters in autoencoder<NewLine>            B: Number of channels in bottleneck 1 × 1-conv block<NewLine>            H: Number of channels in convolutional blocks<NewLine>            P: Kernel size in convolutional blocks<NewLine>            X: Number of convolutional blocks in each repeat<NewLine>            R: Number of repeats<NewLine>            C: Number of speakers<NewLine>            norm_type: BN, gLN, cLN<NewLine>            causal: causal or non-causal<NewLine>            mask_nonlinear: use which non-linear function to generate mask<NewLine>        """"""<NewLine>        super(TemporalConvNet, self).__init__()<NewLine>        # Hyper-parameter<NewLine>        self.C = C<NewLine>        self.mask_nonlinear = mask_nonlinear<NewLine>        # Components<NewLine>        # [M, N, K] -&gt; [M, N, K]<NewLine>#        chomp = Chomp1d(padding)<NewLine>        layer_norm = ChannelwiseLayerNorm(N)<NewLine>        # [M, N, K] -&gt; [M, B, K]<NewLine>        bottleneck_conv1x1 = nn.Conv1d(N, B, 1, bias=False)<NewLine>        # [M, B, K] -&gt; [M, B, K]<NewLine>        repeats = []<NewLine>        for self.r in range(R):<NewLine>            blocks = []<NewLine>            for self.x in range(X):<NewLine>                <NewLine>                dilation = 2**self.x<NewLine>                padding = (P - 1) * dilation if causal else (P - 1) * dilation // 2<NewLine>                blocks += [TemporalBlock(B, H, P, stride=1,<NewLine>                                         padding=padding,<NewLine>                                         dilation=dilation,<NewLine>                                         norm_type=norm_type,<NewLine>                                         causal=causal)]<NewLine>                <NewLine>            repeats += [nn.Sequential(*blocks)]<NewLine>        temporal_conv_net = nn.Sequential(*repeats)<NewLine>        # [M, B, K] -&gt; [M, C*N, K]<NewLine>        mask_conv1x1 = nn.Conv1d(B, C*N, 1, bias=False)<NewLine>        # Put together<NewLine>        self.network = nn.Sequential(layer_norm,<NewLine>                                     bottleneck_conv1x1,<NewLine>                                     temporal_conv_net,<NewLine>                                     mask_conv1x1)<NewLine><NewLine>    def forward(self, mixture_w):<NewLine>        """"""<NewLine>        Keep this API same with TasNet<NewLine>        Args:<NewLine>            mixture_w: [M, N, K], M is batch size<NewLine>        returns:<NewLine>            est_mask: [M, C, N, K]<NewLine>        """"""<NewLine>        M, N, K = mixture_w.size()<NewLine>        score = self.network(mixture_w)  # [M, N, K] -&gt; [M, C*N, K]<NewLine>        score = score.view(M, self.C, N, K) # [M, C*N, K] -&gt; [M, C, N, K]<NewLine>        if self.mask_nonlinear == 'softmax':<NewLine>            est_mask = F.softmax(score, dim=1)<NewLine>        elif self.mask_nonlinear == 'relu':<NewLine>            est_mask = F.relu(score)<NewLine>        elif self.mask_nonlinear == 'sigmoid':<NewLine>            est_mask = F.sigmoid(score)<NewLine>        elif self.mask_nonlinear == 'softsign':<NewLine>            est_mask = self.softsign(score)<NewLine>        else:<NewLine>            raise ValueError(""Unsupported mask non-linear function"")<NewLine>        return est_mask<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Sasank_Kottapalli,(Sasank Kottapalli),Sasank_Kottapalli,"July 9, 2019,  1:32pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The <code>bottleneck_conv1x1</code> layer should be accessible using <code>model.network[1]</code>.<br/><NewLine>I’m not sure, what <code>separator</code> refers to, but if you are assigning <code>TemporalConvNet</code> to it, it should be something like <code>model.separator.network[1]</code> instead.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 9, 2019,  9:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
48361,Deploy pytorch model on spark,2019-06-19T07:03:42.711Z,0,1770,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have trained a model on GPU with PyTorch on python. Now I want to deploy the model on spark environment for production, I wonder how to deploy the model on Spark.</p><NewLine></div>",https://discuss.pytorch.org/u/0bff83efac608c536648,(lhj),0bff83efac608c536648,"June 19, 2019,  7:03am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>Maybe I should write a java/scala code to load the parameters and make the prediction</li><NewLine><li>Maybe rewrite the code by keras and  load the model by Deeplearning4j</li><NewLine><li>I wonder if there is a more easy way?</li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/0bff83efac608c536648; <NewLine> ,"REPLY_DATE 1: July 8, 2019,  2:50am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
49279,Binary convolution,2019-06-29T13:56:33.211Z,0,87,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there,</p><NewLine><p>I would like to train a binary convolution, is there any good reference implementation out there?</p><NewLine><p>Thank you</p><NewLine></div>",https://discuss.pytorch.org/u/AtenaNguyen,(Atena Nguyen),AtenaNguyen,"June 29, 2019,  1:56pm",,,,,
48873,Thoughts on Visual Development,2019-06-25T02:42:11.578Z,13,343,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I think machine learning applications, especially deep learning, can be abstracted as data processing. If that’s true, can we do some visualization for the data processing pipeline, like a graphic workflow configuration tool to build the training and validation process? I’ve seen that Microsoft already has it in their <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/service/ui-concept-visual-interface"" rel=""nofollow noopener"">Azure</a> . And the <a href=""https://developer.nvidia.com/digits"" rel=""nofollow noopener"">NVIDIA DIGITS</a> seems also want’s to do this.</p><NewLine><p>Recently I’ve been working on this for a time but I’ve met a lot of troubles. I want to keep the flexibility as well as the simplicity, but it seems like very hard to keep both of them. And I start to doubt is this the correct way, or should we just write plain code?</p><NewLine><p>I want to know what the community sees about this, so if you have any ideas, positive or negative, please let me know : )</p><NewLine><p>Here are some screenshots about these kind of products:</p><NewLine><p>Microsoft Azure:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/c6b4332325a77c000deb32532159f40dbed86995"" href=""https://discuss.pytorch.org/uploads/default/original/2X/c/c6b4332325a77c000deb32532159f40dbed86995.png"" title=""image.png""><img alt=""image"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/c/c6b4332325a77c000deb32532159f40dbed86995_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/c/c6b4332325a77c000deb32532159f40dbed86995_2_438x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/c/c6b4332325a77c000deb32532159f40dbed86995_2_438x500.png, https://discuss.pytorch.org/uploads/default/optimized/2X/c/c6b4332325a77c000deb32532159f40dbed86995_2_657x750.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/c/c6b4332325a77c000deb32532159f40dbed86995_2_876x1000.png 2x"" width=""438""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">954×1088 94.7 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>NVIDIA DIGITS<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/5297ed596679f65b62e17045fe4212251733366c"" href=""https://discuss.pytorch.org/uploads/default/original/2X/5/5297ed596679f65b62e17045fe4212251733366c.jpeg"" title=""image.jpg""><img alt=""image"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/5297ed596679f65b62e17045fe4212251733366c_2_10x10.png"" height=""491"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/5297ed596679f65b62e17045fe4212251733366c_2_690x491.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/5/5297ed596679f65b62e17045fe4212251733366c_2_690x491.jpeg, https://discuss.pytorch.org/uploads/default/optimized/2X/5/5297ed596679f65b62e17045fe4212251733366c_2_1035x736.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/2X/5/5297ed596679f65b62e17045fe4212251733366c_2_1380x982.jpeg 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.jpg</span><span class=""informations"">1732×1234 397 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>Both of them are hard to use, I’d rather write plain code : (</p><NewLine></div>",https://discuss.pytorch.org/u/chenglu,(ChengLu She),chenglu,"June 25, 2019,  7:37am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>r/MachineLearning is the best place for your discussion:</p><NewLine><p><a class=""onebox"" href=""https://www.reddit.com/r/MachineLearning/"" rel=""nofollow noopener"" target=""_blank"">https://www.reddit.com/r/MachineLearning/</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can agree with you anymore.</p><NewLine><p>those graphics tools only help for newbies but for masters it’s not needed at all.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have already posted there <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think you’re right for the 2 productions(Azure and NVIDIA DIGITS), I have tried them and I’m confused about WHO will use these tools:</p><NewLine><ul><NewLine><li>For developers? Not flexible enough to build complicated applications.</li><NewLine><li>For newbies? Too much to learn and also, not flexible enough.</li><NewLine></ul><NewLine><p>So the baseline is, the machine learning applications require coding abilities. But is it possible to visualize the process, or even build the process by semi-graph-semi-code way?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Cloud you give me the link to your post?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Story is, last year I have developed a <a href=""https://github.com/minetorch/minetorch"" rel=""nofollow noopener"">tool</a> to help people use PyTorch more easily. It does bring me benefits in some Kaggle competitions.<br/><NewLine>So I want it to do more, like visualizing pipeline of the dataflow, or even better: configure the dataflow through web interface.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""192"" src=""https://www.redditstatic.com/desktop2x/img/favicon/android-icon-192x192.png"" width=""192""/><NewLine><a href=""https://www.reddit.com/r/MachineLearning/comments/c53eq1/thoughts_on_visual_development/"" rel=""nofollow noopener"" target=""_blank"">reddit</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail"" height="""" src=""https://external-preview.redd.it/YH8gNap4KoGcFgyFYrNZ86fXmYfRn6pa7uuwIlZkjEE.jpg?auto=webp&amp;amp;s=e62264227377a9581e2e2946169864d130fa3217"" width=""""/><NewLine><h3><a href=""https://www.reddit.com/r/MachineLearning/comments/c53eq1/thoughts_on_visual_development/"" rel=""nofollow noopener"" target=""_blank"">r/MachineLearning - Thoughts on Visual Development</a></h3><NewLine><p>1 vote and 0 comments so far on Reddit</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""48873""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/jinfagang/40/9405_2.png"" width=""20""/> jinfagang:</div><NewLine><blockquote><NewLine><p>only help for newbies but for masters it’s not needed at all</p><NewLine></blockquote><NewLine></aside><NewLine><p>Well, I always count myself in the <em>still learning</em> group even though I sometimes claim to have a bit of first hand knowledge around PyTorch. That aside, if the vision is that ML/Deep Learning will be as ubiquitous as, say, Excel is today, there is a dire need for tools that are as easy to use as Excel. Python goes a long way in making non-programmers feel at home fast, but it’s not perfect.</p><NewLine><p>Quite likely it  would be beneficial to have something visual that is extensible.<br/><NewLine>Think about PyTorch itself: You have the ready-made <code>torch.nn.*</code> that you can just compose (with <code>torch.nn.Sequential</code> if you wish) and you can easily roll your own because that’s what we’re there for.</p><NewLine><p>Another learning from this line of thought: A good and general visualization is the first step to a good visual editor.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""9"" data-topic=""48873""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tom/40/3162_2.png"" width=""20""/> tom:</div><NewLine><blockquote><NewLine><p>Quite likely it would be beneficial to have something visual that is extensible.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thanks tom, what you said is inspiring. We are developing this kind of tool now.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>One other thought: I wonder if there is also inspiration to come from Scratch / <a href=""https://developers.google.com/blockly/"" rel=""nofollow noopener"">Blockly</a>.</p><NewLine><p>But then you’re very likely quite ahead of me in thinking this through! I look forward to seeing your results!</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes! Have been thinking about <code>Blockly</code>, but we think it’s more complicated than workflow. The web should be as simple as it could be. So we chose workflow rather than blockly as the frontend interface. <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/></p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is Workflow a framework? (Sorry, I’m rather clueless.)</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Nope, it’s a concept, like <a href=""https://github.com/features/actions"" rel=""nofollow noopener"">Github Actions</a> or  <a href=""https://workflow.is/"" rel=""nofollow noopener"">Apple’s Workflow</a>.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>Also I’ve been thinking about using TorchScript for the model part. Just saw your tweet about that, really looking forward to see it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Tony-Y; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jinfagang; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/chenglu; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/chenglu; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Tony-Y; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/chenglu; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/chenglu; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/chenglu; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/chenglu; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/chenglu; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/chenglu; <NewLine> ,"REPLY_DATE 1: June 25, 2019,  6:56am; <NewLine> REPLY_DATE 2: June 25, 2019,  7:02am; <NewLine> REPLY_DATE 3: June 25, 2019,  7:32am; <NewLine> REPLY_DATE 4: June 25, 2019,  7:48am; <NewLine> REPLY_DATE 5: June 25, 2019,  7:58am; <NewLine> REPLY_DATE 6: June 25, 2019,  8:09am; <NewLine> REPLY_DATE 7: June 25, 2019,  8:09am; <NewLine> REPLY_DATE 8: June 25, 2019,  9:19am; <NewLine> REPLY_DATE 9: June 25, 2019,  9:32am; <NewLine> REPLY_DATE 10: June 25, 2019, 10:33am; <NewLine> REPLY_DATE 11: June 25, 2019, 10:51am; <NewLine> REPLY_DATE 12: June 25, 2019, 11:01am; <NewLine> REPLY_DATE 13: June 25, 2019, 11:04am; <NewLine> REPLY_DATE 14: June 25, 2019, 11:14am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> 
48836,Error when converting to onnx format,2019-06-24T14:32:29.342Z,0,536,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello to everyone!</p><NewLine><p>I want to convert a custom layer to onnx format. But I get an error<br/><NewLine><code>RuntimeError: tuple appears in op that does not forward tuples</code>.</p><NewLine><h2>To Reproduce</h2><NewLine><p>My code to reproduce the error:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine><NewLine><NewLine>class CustomFunction(torch.autograd.Function):<NewLine>    @staticmethod<NewLine>    def symbolic(g, input):<NewLine>        return g.op('Custom', input, outputs=2)<NewLine><NewLine>    @staticmethod<NewLine>    def forward(ctx, input):<NewLine>        return input, input<NewLine><NewLine>    <NewLine>class Custom(torch.nn.Module):<NewLine><NewLine>    def forward(self, input):<NewLine>        return CustomFunction.apply(input)<NewLine>    <NewLine>model = Custom()<NewLine>batch = torch.FloatTensor(1, 3)<NewLine>torch.onnx.export(model, batch, ""test.onnx"", verbose=True)<NewLine></code></pre><NewLine><h2>Expected behavior</h2><NewLine><pre><code class=""lang-python"">---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-7-6ff6d6df1bca&gt; in &lt;module&gt;<NewLine>     19 model = Custom()<NewLine>     20 batch = torch.FloatTensor(1, 3)<NewLine>---&gt; 21 torch.onnx.export(model, batch, ""test.onnx"", verbose=True)<NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/torch/onnx/__init__.py in export(*args, **kwargs)<NewLine>     23 def export(*args, **kwargs):<NewLine>     24     from torch.onnx import utils<NewLine>---&gt; 25     return utils.export(*args, **kwargs)<NewLine>     26 <NewLine>     27 <NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py in export(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, strip_doc_string)<NewLine>    129             operator_export_type=operator_export_type, opset_version=opset_version,<NewLine>    130             _retain_param_name=_retain_param_name, do_constant_folding=do_constant_folding,<NewLine>--&gt; 131             strip_doc_string=strip_doc_string)<NewLine>    132 <NewLine>    133 <NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py in _export(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, propagate, opset_version, _retain_param_name, do_constant_folding, strip_doc_string)<NewLine>    361                                                         output_names, operator_export_type,<NewLine>    362                                                         example_outputs, propagate,<NewLine>--&gt; 363                                                         _retain_param_name, do_constant_folding)<NewLine>    364 <NewLine>    365         # TODO: Don't allocate a in-memory string for the protobuf<NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py in _model_to_graph(model, args, verbose, training, input_names, output_names, operator_export_type, example_outputs, propagate, _retain_param_name, do_constant_folding, _disable_torch_constant_prop)<NewLine>    276 <NewLine>    277     graph = _optimize_graph(graph, operator_export_type,<NewLine>--&gt; 278                             _disable_torch_constant_prop=_disable_torch_constant_prop)<NewLine>    279 <NewLine>    280     # NB: ONNX requires complete information about output types, which might be<NewLine><NewLine>~/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py in _optimize_graph(graph, operator_export_type, _disable_torch_constant_prop)<NewLine>    181     torch._C._jit_pass_erase_number_types(graph)<NewLine>    182     # onnx does not support tuples, so try to remove them<NewLine>--&gt; 183     torch._C._jit_pass_lower_all_tuples(graph)<NewLine>    184     torch._C._jit_pass_peephole(graph, True)<NewLine>    185     torch._C._jit_pass_lint(graph)<NewLine><NewLine>RuntimeError: tuple appears in op that does not forward tuples (VisitNode at /pytorch/torch/csrc/jit/passes/lower_tuples.cpp:117)<NewLine>frame #0: std::function&lt;std::string ()&gt;::operator()() const + 0x11 (0x7f9b0c2b3441 in /home/videoanalytics/anaconda3/lib/python3.7/site-packages/torch/lib/libc10.so)<NewLine>frame #1: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x2a (0x7f9b0c2b2d7a in /home/videoanalytics/anaconda3/lib/python3.7/site-packages/torch/lib/libc10.so)<NewLine>frame #2: &lt;unknown function&gt; + 0xaf61f5 (0x7f9b0b4b41f5 in /home/videoanalytics/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1)<NewLine>frame #3: &lt;unknown function&gt; + 0xaf6464 (0x7f9b0b4b4464 in /home/videoanalytics/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1)<NewLine>frame #4: torch::jit::LowerAllTuples(std::shared_ptr&lt;torch::jit::Graph&gt;&amp;) + 0x13 (0x7f9b0b4b44a3 in /home/videoanalytics/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1)<NewLine>frame #5: &lt;unknown function&gt; + 0x3f9444 (0x7f9b4b87b444 in /home/videoanalytics/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)<NewLine>frame #6: &lt;unknown function&gt; + 0x130fac (0x7f9b4b5b2fac in /home/videoanalytics/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)<NewLine>frame #7: _PyMethodDef_RawFastCallKeywords + 0x264 (0x564a1c09b6e4 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #8: _PyCFunction_FastCallKeywords + 0x21 (0x564a1c09b801 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #9: _PyEval_EvalFrameDefault + 0x4e8c (0x564a1c0f72bc in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #10: _PyEval_EvalCodeWithName + 0x2f9 (0x564a1c0384f9 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #11: _PyFunction_FastCallKeywords + 0x387 (0x564a1c09aa27 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #12: _PyEval_EvalFrameDefault + 0x14ce (0x564a1c0f38fe in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #13: _PyEval_EvalCodeWithName + 0x2f9 (0x564a1c0384f9 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #14: _PyFunction_FastCallKeywords + 0x325 (0x564a1c09a9c5 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #15: _PyEval_EvalFrameDefault + 0x416 (0x564a1c0f2846 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #16: _PyEval_EvalCodeWithName + 0xbb9 (0x564a1c038db9 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #17: _PyFunction_FastCallKeywords + 0x387 (0x564a1c09aa27 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #18: _PyEval_EvalFrameDefault + 0x14ce (0x564a1c0f38fe in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #19: _PyEval_EvalCodeWithName + 0x2f9 (0x564a1c0384f9 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #20: _PyFunction_FastCallDict + 0x400 (0x564a1c039800 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #21: _PyEval_EvalFrameDefault + 0x1e20 (0x564a1c0f4250 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #22: _PyEval_EvalCodeWithName + 0x2f9 (0x564a1c0384f9 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #23: _PyFunction_FastCallKeywords + 0x387 (0x564a1c09aa27 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #24: _PyEval_EvalFrameDefault + 0x14ce (0x564a1c0f38fe in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #25: _PyEval_EvalCodeWithName + 0x2f9 (0x564a1c0384f9 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #26: PyEval_EvalCodeEx + 0x44 (0x564a1c0393c4 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #27: PyEval_EvalCode + 0x1c (0x564a1c0393ec in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #28: &lt;unknown function&gt; + 0x1e004d (0x564a1c10204d in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #29: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x564a1c09b569 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #30: _PyCFunction_FastCallKeywords + 0x21 (0x564a1c09b801 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #31: _PyEval_EvalFrameDefault + 0x4755 (0x564a1c0f6b85 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #32: _PyGen_Send + 0x2a2 (0x564a1c094672 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #33: _PyEval_EvalFrameDefault + 0x1a6d (0x564a1c0f3e9d in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #34: _PyGen_Send + 0x2a2 (0x564a1c094672 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #35: _PyEval_EvalFrameDefault + 0x1a6d (0x564a1c0f3e9d in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #36: _PyGen_Send + 0x2a2 (0x564a1c094672 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #37: _PyMethodDef_RawFastCallKeywords + 0x8c (0x564a1c09b50c in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #38: _PyMethodDescr_FastCallKeywords + 0x4f (0x564a1c09b86f in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #39: _PyEval_EvalFrameDefault + 0x4c4c (0x564a1c0f707c in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #40: _PyFunction_FastCallKeywords + 0xfb (0x564a1c09a79b in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #41: _PyEval_EvalFrameDefault + 0x416 (0x564a1c0f2846 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #42: _PyFunction_FastCallKeywords + 0xfb (0x564a1c09a79b in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #43: _PyEval_EvalFrameDefault + 0x6a0 (0x564a1c0f2ad0 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #44: _PyEval_EvalCodeWithName + 0x2f9 (0x564a1c0384f9 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #45: _PyFunction_FastCallDict + 0x400 (0x564a1c039800 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #46: _PyObject_Call_Prepend + 0x63 (0x564a1c050c43 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #47: PyObject_Call + 0x6e (0x564a1c04595e in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #48: _PyEval_EvalFrameDefault + 0x1e20 (0x564a1c0f4250 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #49: _PyEval_EvalCodeWithName + 0x5da (0x564a1c0387da in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #50: _PyFunction_FastCallKeywords + 0x387 (0x564a1c09aa27 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #51: _PyEval_EvalFrameDefault + 0x14ce (0x564a1c0f38fe in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #52: &lt;unknown function&gt; + 0x171cc6 (0x564a1c093cc6 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #53: &lt;unknown function&gt; + 0x171ecb (0x564a1c093ecb in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #54: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x564a1c09b569 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #55: _PyCFunction_FastCallKeywords + 0x21 (0x564a1c09b801 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #56: _PyEval_EvalFrameDefault + 0x4755 (0x564a1c0f6b85 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #57: _PyEval_EvalCodeWithName + 0x5da (0x564a1c0387da in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #58: _PyFunction_FastCallKeywords + 0x387 (0x564a1c09aa27 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #59: _PyEval_EvalFrameDefault + 0x6a0 (0x564a1c0f2ad0 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #60: &lt;unknown function&gt; + 0x171cc6 (0x564a1c093cc6 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #61: &lt;unknown function&gt; + 0x171ecb (0x564a1c093ecb in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #62: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x564a1c09b569 in /home/videoanalytics/anaconda3/bin/python)<NewLine>frame #63: _PyCFunction_FastCallKeywords + 0x21 (0x564a1c09b801 in /home/videoanalytics/anaconda3/bin/python)<NewLine></code></pre><NewLine><h2>Environment</h2><NewLine><ul><NewLine><li><NewLine><p>PyTorch version: 1.1.0</p><NewLine></li><NewLine><li><NewLine><p>Is debug build: No</p><NewLine></li><NewLine><li><NewLine><p>CUDA used to build PyTorch: 9.0.176</p><NewLine></li><NewLine><li><NewLine><p>OS: Ubuntu 18.04.2 LTS</p><NewLine></li><NewLine><li><NewLine><p>GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0</p><NewLine></li><NewLine><li><NewLine><p>CMake version: Could not collect</p><NewLine></li><NewLine><li><NewLine><p>Python version: 3.7</p><NewLine></li><NewLine><li><NewLine><p>Is CUDA available: Yes</p><NewLine></li><NewLine><li><NewLine><p>CUDA runtime version: Could not collect</p><NewLine></li><NewLine><li><NewLine><p>GPU models and configuration: GPU 0: GeForce RTX 2080 Ti</p><NewLine></li><NewLine><li><NewLine><p>Nvidia driver version: 418.56</p><NewLine></li><NewLine><li><NewLine><p>cuDNN version: Could not collect</p><NewLine></li><NewLine><li><NewLine><p>Versions of relevant libraries:<br/><NewLine>[pip] numpy==1.14.6<br/><NewLine>[pip] numpydoc==0.8.0<br/><NewLine>[pip] torch==1.1.0<br/><NewLine>[pip] torchvision==0.2.2.post3<br/><NewLine>[conda] blas                      1.0                         mkl<br/><NewLine>[conda] mkl                       2019.3                      199<br/><NewLine>[conda] mkl-service               1.1.2            py37he904b0f_5<br/><NewLine>[conda] mkl_fft                   1.0.10           py37ha843d7b_0<br/><NewLine>[conda] mkl_random                1.0.2            py37hd81dba3_0<br/><NewLine>[conda] torch                     1.1.0                    pypi_0    pypi<br/><NewLine>[conda] torchvision               0.2.2.post3              pypi_0    pypi</p><NewLine></li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/3123f2121a27c2cebf0c,(Мирас Амир),3123f2121a27c2cebf0c,"June 24, 2019,  2:55pm",,,,,
46212,"Using PyCharm to debug Pytorch model on GCE, AWS or Azure",2019-05-25T19:33:37.317Z,4,947,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone,<br/><NewLine>I know my question is not directly related to deep learning or Pytorch, but I have found this community so helpful and one time more I need your help.</p><NewLine><p>I have implemented a model using Pytorch, but I do not have access to GPU on my local environment and using Google Colab is so frustrating because of lack of access to debugging utilities.</p><NewLine><p>Now I come across this idea to use Google Preemptible Compute Engine or Amazon EC2 Spot or other cloud services, to test my implementation.<br/><NewLine>The problem is I am not sure about connecting cloud runtime to my local IDE and run debugs by using breakpoints in my local IDE.</p><NewLine><p>I have read about this in the docs on clouds and PyCharm and it seems it is possible, but I am not sure about it.</p><NewLine><p>Thanks for your replies.</p><NewLine></div>",https://discuss.pytorch.org/u/Nikronic,(M. Doosti Lakhani),Nikronic,"June 15, 2019,  4:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It is possible to do remote debugging from PyCharm Professional to the cloud instances through ssh connection. And in addition to GCP and AWS, it works with Colab too. Done that.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply.</p><NewLine><p>But, are you sure about Google colab? Because, based on the documentation, you can control your local environment through colab, not controlling colab through your local environment.</p><NewLine><p>I have googled a lot, but I did not find any procedure to make it happen.<br/><NewLine>Can you please provide any documenation about using Google colab as a remote debugger?</p><NewLine><p>Thank you</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""46212""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/nikronic/40/12962_2.png"" width=""20""/> Nikronic:</div><NewLine><blockquote><NewLine><p>But, are you sure about Google colab?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, I’m sure. Done that several times. Having free K80 GPU right in PyCharm is super handy when you want to make your code GPU proofed.</p><NewLine><p>Configuration is tricky, multistep process, but doable. The main problem is that Colab VM instance is hidden behind a firewall. But inside of the virtual machine lives Linux Ubuntu, co we can do some hocus-pocus.</p><NewLine><p>High level approach would be to:</p><NewLine><ol><NewLine><li>Create “setup ssh forwarding” jupyter notebook in Google Colab. You can use my configuration notebook as a template for yours - <a href=""https://github.com/wojtekcz/ml_seminars/tree/master/demo_colab_ssh_access"" rel=""nofollow noopener"">demo_colab_ssh_access</a>. Make sure you generate and then use your RSA access keys, customize port numbers, etc.</li><NewLine><li>Setup and start ssh server on running Colab VM instance.</li><NewLine><li>Establish ssh tunnel from Colab VM instance to some server on the internet, forward traffic from port 22 to some port there and…</li><NewLine><li>Expose this port for external access. <a href=""https://portmap.io"" rel=""nofollow noopener"">portmap.io</a> service allows one port forwarding for free.</li><NewLine><li>From now on you have ability to ssh from your computer to Colab VM instance and you can proceed with <a href=""https://www.jetbrains.com/help/pycharm/configuring-remote-interpreters-via-ssh.html"" rel=""nofollow noopener"">configuring remote interpreter via ssh in PyCharm</a>.</li><NewLine><li>Every time you start new Colab session you’ll just replay “setup ssh forwarding” jupyter notebook and voila <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/><NewLine></li><NewLine></ol><NewLine><p>Ping me if you get stuck.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi again,<br/><NewLine>Sorry, I got stuck in the final step.</p><NewLine><p>I do not know what I should write in the <code>Allowed IP</code> section. By the way, I get this error Every time I try to make a tunnel from my local machine to google Colab:</p><NewLine><pre><code class=""lang-python"">Warning: remote port forwarding failed to listen port XXXX<NewLine></code></pre><NewLine><p>XXXX is the port <a href=""http://portmap.io"" rel=""nofollow noopener"">portmap.io</a> assigned to my rule.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""46212""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/nikronic/40/12962_2.png"" width=""20""/> Nikronic:</div><NewLine><blockquote><NewLine><p>I do not know what I should write in the <code>Allowed IP</code> section.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Leave it empty for now.</p><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""46212""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/nikronic/40/12962_2.png"" width=""20""/> Nikronic:</div><NewLine><blockquote><NewLine><p>I get this error</p><NewLine></blockquote><NewLine></aside><NewLine><p>I don’t know which step doesn’t work for you. But… it took me best part of a year to master port forwardings, anyway <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine><p>I can help you to debug this interactively, preferably through audio + screen sharing. Would you like to do it? Hangouts or Skype perhaps?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ow, Actually, I am taking this semester final tests, meanwhile I do not have access to any reliable internet connections at the university unfortunately (I know it is weird but it is the truth!).</p><NewLine><p>I will try more next time and if I really have no other options to proceed, I will let you know.</p><NewLine><p>I am so appreciated for your help.<br/><NewLine>Thanks again.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/wojtekcz; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Nikronic; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/wojtekcz; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Nikronic; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/wojtekcz; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Nikronic; <NewLine> ,"REPLY_DATE 1: June 15, 2019,  7:25am; <NewLine> REPLY_DATE 2: June 15, 2019, 11:12am; <NewLine> REPLY_DATE 3: June 17, 2019,  4:39pm; <NewLine> REPLY_DATE 4: June 19, 2019, 12:50pm; <NewLine> REPLY_DATE 5: June 19, 2019,  2:37pm; <NewLine> REPLY_DATE 6: June 20, 2019,  6:10pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
47902,RuntimeError: cuda runtime error : device-side assert triggered while trying loss.backward(),2019-06-13T23:08:05.082Z,0,672,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have the following error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-10-3370ce850bee&gt; in &lt;module&gt;()<NewLine>     87 <NewLine>     88 # train the model<NewLine>---&gt; 89 model_scratch = train(30, loader_scratch, model_scratch, optimizer_scratch, criterion_scratch, use_cuda, '/data/model_scratch.pt')<NewLine>     90 <NewLine>     91 # load the model that got the best validation accuracy<NewLine><NewLine>&lt;ipython-input-10-3370ce850bee&gt; in train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path)<NewLine>     38             loss = criterion(output, target)<NewLine>     39             # backward pass: compute gradient of the loss with respect to model parameters<NewLine>---&gt; 40             loss.backward()<NewLine>     41             # perform a single optimization step (parameter update)<NewLine>     42             optimizer.step()<NewLine><NewLine>/opt/conda/lib/python3.6/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)<NewLine>     91                 products. Defaults to ``False``.<NewLine>     92         """"""<NewLine>---&gt; 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)<NewLine>     94 <NewLine>     95     def register_hook(self, hook):<NewLine><NewLine>/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)<NewLine>     87     Variable._execution_engine.run_backward(<NewLine>     88         tensors, grad_tensors, retain_graph, create_graph,<NewLine>---&gt; 89         allow_unreachable=True)  # allow_unreachable flag<NewLine>     90 <NewLine>     91 <NewLine><NewLine>/opt/conda/lib/python3.6/site-packages/torch/autograd/function.py in apply(self, *args)<NewLine>     74 <NewLine>     75     def apply(self, *args):<NewLine>---&gt; 76         return self._forward_cls.backward(self, *args)<NewLine>     77 <NewLine>     78 <NewLine><NewLine>/opt/conda/lib/python3.6/site-packages/torch/nn/_functions/dropout.py in backward(ctx, grad_output)<NewLine>     47     def backward(ctx, grad_output):<NewLine>     48         if ctx.p &gt; 0 and ctx.train:<NewLine>---&gt; 49             return grad_output * ctx.noise, None, None, None<NewLine>     50         else:<NewLine>     51             return grad_output, None, None, None<NewLine><NewLine>RuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu:331<NewLine></code></pre><NewLine><p>My model is on cuda() and the data and target also.</p><NewLine><pre><code class=""lang-auto"">        for batch_idx, (data, target) in enumerate(loaders['train']):<NewLine>            # move to GPU Y<NewLine>            if use_cuda:<NewLine>                data, target = data.to(device), target.to(device)<NewLine>            # clear the gradients of all optimized variables<NewLine>            optimizer.zero_grad()<NewLine>            # forward pass: compute predicted outputs by passing inputs to the model<NewLine>            output = model(data)<NewLine>            print(""OUTPUT____"", output)<NewLine>            print(""TARGET____"", target)<NewLine>            # calculate the batch loss<NewLine>            criterion = nn.CrossEntropyLoss()<NewLine>            loss = criterion(output, target)<NewLine>            # backward pass: compute gradient of the loss with respect to model parameters<NewLine>            loss.backward()  #### HERE happens the error<NewLine>            # perform a single optimization step (parameter update)<NewLine>            optimizer.step()<NewLine>            # update training loss<NewLine>            train_loss += loss.item()*data.size(0)<NewLine></code></pre><NewLine><p>The error happens by loss.backward().</p><NewLine><p>output and target shape:</p><NewLine><p>OUTPUT____ torch.Size([1, 10])<br/><NewLine>TARGET____ torch.Size([1])</p><NewLine></div>",https://discuss.pytorch.org/u/CHoix,(Choix),CHoix,"June 13, 2019, 11:08pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you run the code on CPU and check the error message, as it might be a bit clearer?<br/><NewLine>If it’s running fine on the CPU, could you post the GPU you are using as well as the CUDA version?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: June 14, 2019, 12:18pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
47799,"RuntimeError: shape &lsquo;[-1, 50176]&rsquo; is invalid for input of size 66304",2019-06-12T22:31:47.440Z,1,3454,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to setup a CNN model. My images are 224x224.</p><NewLine><pre><code class=""lang-auto"">import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine><NewLine># define the CNN architecture<NewLine>class Net(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Net, self).__init__()<NewLine>        # convolutional layer<NewLine>        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)<NewLine>        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)<NewLine>        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)<NewLine>        # max pooling layer<NewLine>        self.pool = nn.MaxPool2d(2, 2)<NewLine>        self.fc1 = nn.Linear(64 * 28*28 , 500)<NewLine>        # linear layer (500 -&gt; 10)<NewLine>        self.fc2 = nn.Linear(500, 10)<NewLine>        # dropout layer (p=0.25)<NewLine>        self.dropout = nn.Dropout(0.25)<NewLine>        <NewLine>    def forward(self, x):<NewLine>        # add sequence of convolutional and max pooling layers<NewLine>        x = self.pool(F.relu(self.conv1(x)))<NewLine>        x = self.pool(F.relu(self.conv2(x)))<NewLine>        x = self.pool(F.relu(self.conv3(x)))<NewLine>        # flatten image input<NewLine>        print(x.shape)<NewLine>        x = x.view(-1, 64 * 28*28 )<NewLine>        print(x.shape)<NewLine>        # add dropout layer<NewLine>        x = self.dropout(x)<NewLine>        # add 1st hidden layer, with relu activation function<NewLine>        x = F.relu(self.fc1(x))<NewLine>        # add dropout layer<NewLine>        x = self.dropout(x)<NewLine>        # add 2nd hidden layer, with relu activation function<NewLine>        x = self.fc2(x)<NewLine>        return x<NewLine>    <NewLine># instantiate the CNN<NewLine>model_scratch = Net()<NewLine>print(model_scratch)<NewLine># move tensors to GPU if CUDA is available<NewLine>if use_cuda:<NewLine>    model_scratch.cuda()<NewLine><NewLine></code></pre><NewLine><p>My Train and Validation loop:</p><NewLine><pre><code class=""lang-auto""># the following import is required for training to be robust to truncated images<NewLine>from PIL import ImageFile<NewLine>ImageFile.LOAD_TRUNCATED_IMAGES = True<NewLine><NewLine>def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):<NewLine>    """"""returns trained model""""""<NewLine>    # initialize tracker for minimum validation loss<NewLine>    valid_loss_min = np.Inf <NewLine>    <NewLine>    for epoch in range(1, n_epochs+1):<NewLine>        # initialize variables to monitor training and validation loss<NewLine>        train_loss = 0.0<NewLine>        valid_loss = 0.0<NewLine>        print(""====================EPOCHE"", epoch)<NewLine>        ###################<NewLine>        # train the model #<NewLine>        ###################<NewLine>        model.train()<NewLine>        print('model trained')<NewLine>        for batch_idx, (data, target) in enumerate(loaders['train']):<NewLine>            print(""Loop over Train"", batch_idx)<NewLine>            print(""USE_CUDA"",use_cuda)<NewLine>            # move to GPU<NewLine>            if use_cuda:<NewLine>                data, target = data.cuda(), target.cuda()<NewLine>            # clear the gradients of all optimized variables<NewLine>            optimizer.zero_grad()<NewLine>            # forward pass: compute predicted outputs by passing inputs to the model<NewLine>            output = model(data)<NewLine>            # calculate the batch loss<NewLine>            loss = criterion(output, target)<NewLine>            # backward pass: compute gradient of the loss with respect to model parameters<NewLine>            loss.backward()<NewLine>            # perform a single optimization step (parameter update)<NewLine>            optimizer.step()<NewLine>            # update training loss<NewLine>            train_loss += loss.item()*data.size(0)<NewLine>            ## find the loss and update the model parameters accordingly<NewLine>            ## record the average training loss, using something like<NewLine>            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))<NewLine>            <NewLine>        ######################    <NewLine>        # validate the model #<NewLine>        ######################<NewLine>        model.eval()<NewLine>        for batch_idx, (data, target) in enumerate(loaders['valid']):<NewLine>            # move to GPU<NewLine>            print(""USE_CUDA2"",use_cuda)<NewLine>            if use_cuda:<NewLine>                data, target = data.cuda(), target.cuda()<NewLine>            ## update the average validation loss<NewLine>            # forward pass: compute predicted outputs by passing inputs to the model<NewLine>            output = model(data)<NewLine>            # calculate the batch loss<NewLine>            loss = criterion(output, target)<NewLine>            # update average validation loss <NewLine>            valid_loss += loss.item()*data.size(0)<NewLine><NewLine>        <NewLine>        # calculate average losses<NewLine>        train_loss = train_loss/len(train_loader.dataset)<NewLine>        valid_loss = valid_loss/len(valid_loader.dataset)<NewLine>        <NewLine>        <NewLine>        # print training/validation statistics <NewLine>        print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format(<NewLine>            epoch, train_loss, valid_loss))<NewLine><NewLine>        # save model if validation loss has decreased<NewLine>        if valid_loss &lt;= valid_loss_min:<NewLine>            print('Validation loss decreased ({:.6f} --&gt; {:.6f}).  Saving model ...'.format(<NewLine>            valid_loss_min,<NewLine>            valid_loss))<NewLine>            torch.save(model.state_dict(), 'model_scratch.pt')<NewLine>            valid_loss_min = valid_loss<NewLine>            <NewLine>    # return trained model<NewLine>    return model<NewLine><NewLine><NewLine># train the model<NewLine>model_scratch = train(100, loader_scratch, model_scratch, optimizer_scratch, <NewLine>                      criterion_scratch, use_cuda, 'model_scratch.pt')<NewLine><NewLine># load the model that got the best validation accuracy<NewLine>model_scratch.load_state_dict(torch.load('model_scratch.pt'))<NewLine></code></pre><NewLine><p>I get the above error, and don’t really understand how to flatten my data right:</p><NewLine><pre><code class=""lang-auto"">torch.Size([1, 64, 37, 28])<NewLine>---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-70-ef616b23c264&gt; in &lt;module&gt;<NewLine>     81 # train the model<NewLine>     82 model_scratch = train(100, loader_scratch, model_scratch, optimizer_scratch, <NewLine>---&gt; 83                       criterion_scratch, use_cuda, 'model_scratch.pt')<NewLine>     84 <NewLine>     85 # load the model that got the best validation accuracy<NewLine><NewLine>&lt;ipython-input-70-ef616b23c264&gt; in train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path)<NewLine>     27             optimizer.zero_grad()<NewLine>     28             # forward pass: compute predicted outputs by passing inputs to the model<NewLine>---&gt; 29             output = model(data)<NewLine>     30             # calculate the batch loss<NewLine>     31             loss = criterion(output, target)<NewLine><NewLine>~\Anaconda3\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)<NewLine>    491             result = self._slow_forward(*input, **kwargs)<NewLine>    492         else:<NewLine>--&gt; 493             result = self.forward(*input, **kwargs)<NewLine>    494         for hook in self._forward_hooks.values():<NewLine>    495             hook_result = hook(self, input, result)<NewLine><NewLine>&lt;ipython-input-67-ebe7a14cbba0&gt; in forward(self, x)<NewLine>     26         # flatten image input<NewLine>     27         print(x.shape)<NewLine>---&gt; 28         x = x.view(-1, 64 * 28*28 )<NewLine>     29         print(x.shape)<NewLine>     30         # add dropout layer<NewLine><NewLine>RuntimeError: shape '[-1, 50176]' is invalid for input of size 66304<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/CHoix,(Choix),CHoix,"June 12, 2019, 10:31pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok, You got the error because you were trying to resize the data into a wrong shape. Try this,</p><NewLine><pre><code class=""lang-auto"">x = x.view(x.size(0), -1) <NewLine></code></pre><NewLine><p>keeping the batch size the same, and flattening out the remaining dimension.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply, when I reshape it like you said, I get this error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-17-ef616b23c264&gt; in &lt;module&gt;<NewLine>     81 # train the model<NewLine>     82 model_scratch = train(100, loader_scratch, model_scratch, optimizer_scratch, <NewLine>---&gt; 83                       criterion_scratch, use_cuda, 'model_scratch.pt')<NewLine>     84 <NewLine>     85 # load the model that got the best validation accuracy<NewLine><NewLine>&lt;ipython-input-17-ef616b23c264&gt; in train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path)<NewLine>     27             optimizer.zero_grad()<NewLine>     28             # forward pass: compute predicted outputs by passing inputs to the model<NewLine>---&gt; 29             output = model(data)<NewLine>     30             # calculate the batch loss<NewLine>     31             loss = criterion(output, target)<NewLine><NewLine>~\Anaconda3\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)<NewLine>    491             result = self._slow_forward(*input, **kwargs)<NewLine>    492         else:<NewLine>--&gt; 493             result = self.forward(*input, **kwargs)<NewLine>    494         for hook in self._forward_hooks.values():<NewLine>    495             hook_result = hook(self, input, result)<NewLine><NewLine>&lt;ipython-input-16-8e80a47d55dc&gt; in forward(self, x)<NewLine>     31         x = self.dropout(x)<NewLine>     32         # add 1st hidden layer, with relu activation function<NewLine>---&gt; 33         x = F.relu(self.fc1(x))<NewLine>     34         # add dropout layer<NewLine>     35         x = self.dropout(x)<NewLine><NewLine>~\Anaconda3\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)<NewLine>    491             result = self._slow_forward(*input, **kwargs)<NewLine>    492         else:<NewLine>--&gt; 493             result = self.forward(*input, **kwargs)<NewLine>    494         for hook in self._forward_hooks.values():<NewLine>    495             hook_result = hook(self, input, result)<NewLine><NewLine>~\Anaconda3\lib\site-packages\torch\nn\modules\linear.py in forward(self, input)<NewLine>     90     @weak_script_method<NewLine>     91     def forward(self, input):<NewLine>---&gt; 92         return F.linear(input, self.weight, self.bias)<NewLine>     93 <NewLine>     94     def extra_repr(self):<NewLine><NewLine>~\Anaconda3\lib\site-packages\torch\nn\functional.py in linear(input, weight, bias)<NewLine>   1404     if input.dim() == 2 and bias is not None:<NewLine>   1405         # fused op is marginally faster<NewLine>-&gt; 1406         ret = torch.addmm(bias, input, weight.t())<NewLine>   1407     else:<NewLine>   1408         output = input.matmul(weight.t())<NewLine><NewLine>RuntimeError: size mismatch, m1: [1 x 53760], m2: [50176 x 500] at C:/w/1/s/tmp_conda_3.7_044431/conda/conda-bld/pytorch_1556686009173/work/aten/src\THC/generic/THCTensorMathBlas.cu:268<NewLine></code></pre><NewLine><p>I then realized that the shape of my images are not proportional 1:1, I changed the transform pipe like this:</p><NewLine><pre><code class=""lang-auto"">min_img_size = 224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.<NewLine>transform_pipeline = transforms.Compose([transforms.Resize((min_img_size, min_img_size)),<NewLine>                                     transforms.ToTensor(),<NewLine>                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],<NewLine>                                                          std=[0.229, 0.224, 0.225])])<NewLine></code></pre><NewLine><p>But now got this error:</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-20-ef616b23c264&gt; in &lt;module&gt;<NewLine>     81 # train the model<NewLine>     82 model_scratch = train(100, loader_scratch, model_scratch, optimizer_scratch, <NewLine>---&gt; 83                       criterion_scratch, use_cuda, 'model_scratch.pt')<NewLine>     84 <NewLine>     85 # load the model that got the best validation accuracy<NewLine><NewLine>&lt;ipython-input-20-ef616b23c264&gt; in train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path)<NewLine>     31             loss = criterion(output, target)<NewLine>     32             # backward pass: compute gradient of the loss with respect to model parameters<NewLine>---&gt; 33             loss.backward()<NewLine>     34             # perform a single optimization step (parameter update)<NewLine>     35             optimizer.step()<NewLine><NewLine>~\Anaconda3\lib\site-packages\torch\tensor.py in backward(self, gradient, retain_graph, create_graph)<NewLine>    105                 products. Defaults to ``False``.<NewLine>    106         """"""<NewLine>--&gt; 107         torch.autograd.backward(self, gradient, retain_graph, create_graph)<NewLine>    108 <NewLine>    109     def register_hook(self, hook):<NewLine><NewLine>~\Anaconda3\lib\site-packages\torch\autograd\__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)<NewLine>     91     Variable._execution_engine.run_backward(<NewLine>     92         tensors, grad_tensors, retain_graph, create_graph,<NewLine>---&gt; 93         allow_unreachable=True)  # allow_unreachable flag<NewLine>     94 <NewLine>     95 <NewLine><NewLine>RuntimeError: cublas runtime error : the GPU program failed to execute at C:/w/1/s/tmp_conda_3.7_044431/conda/conda-bld/pytorch_1556686009173/work/aten/src/THC/THCBlas.cu:25<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/god_sp33d; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/CHoix; <NewLine> ,"REPLY_DATE 1: June 13, 2019, 12:43am; <NewLine> REPLY_DATE 2: June 13, 2019, 10:01am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
47270,How to put a module in different GPUs,2019-06-06T17:17:53.985Z,0,158,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m currently training a word2vec model with a large vocabulary (50 million). When I use a embedding layer with 64 dimension, the volume of the embedding weight (64x50milliton) will exceed the memory of a single GPU, so I wanna put the weight on different GPUs.</p><NewLine><p>I’ve already tried DataParallel but it still doesn’t work.</p><NewLine><pre><code class=""lang-auto"">        self.in_embed = nn.Embedding(self.num_classes, self.embed_size, sparse=True)<NewLine>        self.in_embed.weight = Parameter(t.FloatTensor(self.num_classes, self.embed_size).uniform_(-1, 1))<NewLine>        self.in_embed = DataParallel(self.in_embed,device_ids=[0,1,2])<NewLine>        self.in_embed.cuda()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Shengyu_Feng,(Shengyu Feng),Shengyu_Feng,"June 6, 2019,  5:17pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>DataParallel</code> won’t work in this case, as this will copy the model to each GPU and split the data in dim0 sending each chunk to the corresponding GPU.<br/><NewLine>I haven’t tested this approach, but it might be possible to split the weigth matrix and push the splits onto different GPUs (similar to model sharding, but with a single layer).<br/><NewLine>Here is a dummy code (currently CPU only) to test for equal results:</p><NewLine><pre><code class=""lang-python""># Reference embedding, which is too large on single GPU<NewLine>emb = nn.Embedding(10, 300)<NewLine>emb1 = nn.Embedding(5, 300)<NewLine>emb2 = nn.Embedding(5, 300)<NewLine><NewLine># Copy weigths so that we can compare both approaches<NewLine>with torch.no_grad():<NewLine>    emb1.weight.copy_(emb.weight[:5])<NewLine>    emb2.weight.copy_(emb.weight[5:])<NewLine><NewLine># Initialize some input randomly<NewLine>x = torch.randint(0, 10, (3, 7))<NewLine><NewLine># Split input and call corresponding embedding layer (on different GPUs)<NewLine>emb1_idx = (x &lt; 5).nonzero()<NewLine>emb2_idx = (x &gt;= 5).nonzero()<NewLine><NewLine>out1 = emb1(x[emb1_idx.split(1, 1)])<NewLine>out2 = emb2(x[emb2_idx.split(1, 1)] - 5)<NewLine><NewLine># Concatenate output to check for equal results<NewLine>out_cat = torch.zeros(x.size(0), x.size(1), 300)<NewLine>out_cat[emb1_idx.split(1, 1)] = out1<NewLine>out_cat[emb2_idx.split(1, 1)] = out2<NewLine><NewLine># Get reference output<NewLine>out = emb(x)<NewLine><NewLine># Compare outputs<NewLine>print((out == out_cat).all())<NewLine><NewLine># Call backward and compare gradients<NewLine>out_cat.mean().backward()<NewLine>out.mean().backward()<NewLine><NewLine>print((torch.cat((emb1.weight.grad, emb2.weight.grad), 0) == emb.weight.grad).all())<NewLine></code></pre><NewLine><p>In your code, you would have to push the embedding “sublayers” as well as the corresponding input chunks to different devices.</p><NewLine><p>Let me know, if I’m missing something obvious.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: June 6, 2019,  6:10pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
47126,Use SVD with 2-D input (with batch_size) on GPU,2019-06-05T11:18:27.095Z,0,206,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to use ‘torch.svd()’ on GPU, but the funcation only accept 2-D input. I have implemented the svd by a for loop, but it’s too slow. In <a href=""https://github.com/gchanan/pytorch/wiki/Broadcasting-Notes"" rel=""nofollow noopener"">https://github.com/gchanan/pytorch/wiki/Broadcasting-Notes</a>, it’s stated that “torch.svd” can be used in the same way as <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html"" rel=""nofollow noopener"">numpy.linalg.svd</a>. But it causes an error when I pass a 3-D input to the function. Could you please help me?</p><NewLine></div>",https://discuss.pytorch.org/u/Gyaya,(Gyaya),Gyaya,"June 5, 2019, 11:18am",,,,,
46702,.cuda slow on GTX Titan vs. GTX 1080 Ti,2019-05-31T01:12:17.736Z,0,333,"<div class=""post"" itemprop=""articleBody""><NewLine><p>EDIT: This isn’t a docker issue, the slowdown is still there when I install outside the container.</p><NewLine><p>Not much of a docker (or pytorch) expert but I have a text-to-speech codebase where I am shoe horning a general transformation (Griffin-Lim) into a torch.nn.Module. In one machine, runs on the order of a second (ignoring startup).  This code has been put into a docker image on a host with faster CPU and GPUs and I get an immense slowdown (1s to 18s).</p><NewLine><p>I’ve profiled, and if one compares appropriately (a little bothered by why the number of calls isn’t the exact same), it looks like the culprit is the calls to the .cuda() method (3s on fast machine total vs. 21s on slow machine, pasted at end).</p><NewLine><p>I’m currently using .cuda() as opposed to .to(“cuda”/device).  Is there something obviously idiotic I am doing that would result in such a slowdown in the docker image? The python versions between the machines differ slightly (fast machine = 3.6.3, slow machine = 3.6.7) but profile environment summary matches:</p><NewLine><hr/><NewLine><h2>Environment Summary</h2><NewLine><p>PyTorch 0.4.1 compiled w/ CUDA 9.0.176<br/><NewLine>Running with Python 3.6 and CUDA 9.0.176</p><NewLine><h2>Fast machine:</h2><NewLine><h2>cProfile output</h2><NewLine><pre><code>     1299411 function calls (1275111 primitive calls) in 17.041 seconds<NewLine></code></pre><NewLine><p>Ordered by: internal time<br/><NewLine>List reduced from 7173 to 15 due to restriction &lt;15&gt;</p><NewLine><p>ncalls  tottime  percall  cumtime  percall filename:lineno(function)<br/><NewLine>1    9.531    9.531    9.531    9.531 /lib/python3.6/site-pa<br/><NewLine>ckages/numpy/linalg/linalg.py:1299(svd)<br/><NewLine>324    3.486    0.011    3.486    0.011 {method ‘cuda’ of ‘torch._C._TensorBase’ objects}<br/><NewLine>…</p><NewLine><h2>Slow machine:</h2><NewLine><h2>cProfile output</h2><NewLine><pre><code>     1311902 function calls (1287710 primitive calls) in 28.400 seconds<NewLine></code></pre><NewLine><p>Ordered by: internal time<br/><NewLine>List reduced from 7152 to 15 due to restriction &lt;15&gt;</p><NewLine><p>ncalls  tottime  percall  cumtime  percall filename:lineno(function)<br/><NewLine>385   21.639    0.056   21.639    0.056 {method ‘cuda’ of ‘torch._C._TensorBase’ objects}<br/><NewLine>1    3.791    3.791    3.791    3.791 /usr/local/lib/python3.6/dist-packages/numpy/linalg/linalg.py:1299(svd)</p><NewLine><p>…</p><NewLine></div>",https://discuss.pytorch.org/u/mittimithai,(mittimithai),mittimithai,"June 4, 2019,  9:42pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>So the performance gap persists when I compare outside the docker container, I didn’t realize that the difference in the hardware was the issue. The Titan is much older than the 1080 Ti.</p><NewLine><p>GeForce GTX 1080 Ti<br/><NewLine>ncalls  tottime  percall  cumtime  percall filename:lineno(function)<br/><NewLine>324    8.589    0.027    8.589    0.027 {method ‘cuda’ of ‘torch._C._TensorBase’ objects}</p><NewLine><p>GeForce GTX TITAN<br/><NewLine>ncalls  tottime  percall  cumtime  percall filename:lineno(function)<br/><NewLine>324   21.860    0.067   21.860    0.067 {method ‘cuda’ of ‘torch._C._TensorBase’</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mittimithai; <NewLine> ,"REPLY_DATE 1: June 4, 2019, 11:15pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
46762,CPU utilization freezes after a while (GPU works great),2019-05-31T17:33:01.564Z,0,139,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I have been trying to make a CPU accelerated inference for our model. The only thing I change from GPU to CPU is <code>torch.set_num_threads(threads)</code>. I am running the model on AWS <code>m5.24xlarge</code> with 96 CPUs. I set the num_threads to 86 and num_workers to 8. It starts pretty well, giving an ETA of ~16hours but after 40% completion, most of the pytorch processes start jumping between “S-R” (stall and running) states. It got stuck at 47%, and I had to kill it.</p><NewLine><p>Should I do something else for a CPU-accelerated version?</p><NewLine><p>This is on Ubuntu 18.04 (the deeplearning AMI available from AWS)<br/><NewLine>Python3: 3.6.7<br/><NewLine>PyTorch: 1.1</p><NewLine><p>The model is here: <a href=""https://github.com/kishwarshafin/helen/blob/master/modules/python/models/TransducerModel.py"" rel=""nofollow noopener"">https://github.com/kishwarshafin/helen/blob/master/modules/python/models/TransducerModel.py</a></p><NewLine><p>And the way I am doing is here:<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/kishwarshafin/helen/blob/master/modules/python/models/predict.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/kishwarshafin/helen/blob/master/modules/python/models/predict.py"" rel=""nofollow noopener"" target=""_blank"">kishwarshafin/helen/blob/master/modules/python/models/predict.py</a></h4><NewLine><pre><code class=""lang-py"">import sys<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>from torch.utils.data import DataLoader<NewLine>from modules.python.models.dataloader_predict import SequenceDataset<NewLine>from modules.python.TextColor import TextColor<NewLine>from tqdm import tqdm<NewLine>import numpy as np<NewLine>from modules.python.models.ModelHander import ModelHandler<NewLine>from modules.python.Options import ImageSizeOptions, TrainOptions<NewLine>from modules.python.DataStore import DataStore<NewLine>""""""<NewLine>This script implements the predict method that is used by the call consensus method.<NewLine><NewLine>The algorithm is described here:<NewLine><NewLine>  1) INPUTS:<NewLine>    - directory path to the image files generated by MarginPolish<NewLine>    - model path directing to a trained model<NewLine>    - batch size for minibatch prediction<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/kishwarshafin/helen/blob/master/modules/python/models/predict.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>Any help would be highly appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/kishwarshafin,(Kishwar Shafin),kishwarshafin,"May 31, 2019,  5:39pm",,,,,
46732,RuntimeError: after reduction step 2: device-side assert triggered Pytorch on Jupyter Notebook,2019-05-31T11:01:29.892Z,0,114,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am training a binary classifier using Pytorch on Jupyter Notebook. The following is the architecture:</p><NewLine><pre><code class=""lang-auto"">class AlexNet(nn.Module):<NewLine>    def __init__(self, num_classes=1):<NewLine>        super(AlexNet, self).__init__()<NewLine>        self.conv_base = nn.Sequential(<NewLine>            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2, bias=False),<NewLine>            nn.BatchNorm2d(96),<NewLine>            nn.ReLU(inplace=True),<NewLine>            nn.MaxPool2d(kernel_size=3, stride=2),<NewLine><NewLine>            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2, bias=False),<NewLine>            nn.BatchNorm2d(256),<NewLine>            nn.ReLU(inplace=True),<NewLine>            nn.MaxPool2d(kernel_size=3, stride=2),<NewLine><NewLine>            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),<NewLine>            nn.ReLU(inplace=True),<NewLine><NewLine>            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),<NewLine>            nn.ReLU(inplace=True),<NewLine><NewLine>            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),<NewLine>            nn.ReLU(inplace=True),<NewLine>            nn.MaxPool2d(kernel_size=3, stride=2),<NewLine>        )<NewLine>        self.fc_base = nn.Sequential(<NewLine>            nn.Dropout(),<NewLine>            nn.Linear(256*6*6, 4096),<NewLine>            nn.ReLU(inplace=True),<NewLine><NewLine>            nn.Dropout(),<NewLine>            nn.Linear(4096, 4096),<NewLine>            nn.ReLU(inplace=True),<NewLine><NewLine>            nn.Linear(4096, num_classes),<NewLine>        )<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.conv_base(x)<NewLine>        x = x.view(x.size(0), 256*6*6)<NewLine>        x = self.fc_base(x)<NewLine>        return x<NewLine></code></pre><NewLine><p>These are my parameters for training:</p><NewLine><pre><code class=""lang-auto"">criterion = nn.BCELoss()<NewLine><NewLine># specify optimizer<NewLine>optimizer = torch.optim.SGD(model.parameters(), lr=2e-5, momentum=0.9)<NewLine></code></pre><NewLine><p>When I start the training, I get the following error:</p><NewLine><blockquote><NewLine><p>RuntimeError: after reduction step 2: device-side assert triggered</p><NewLine></blockquote><NewLine><p>Some weird things are also happening:</p><NewLine><ol><NewLine><li><NewLine><p>It was training at one point in time, but the learning rate was small. So when I changed it and started the training process again, I got the error.</p><NewLine></li><NewLine><li><NewLine><p>I’m also getting this error when I run this command:</p><NewLine></li><NewLine></ol><NewLine><pre><code class=""lang-auto"">model = AlexNet()<NewLine>model.cuda()<NewLine></code></pre><NewLine><p>the error:</p><NewLine><blockquote><NewLine><p>RuntimeError: CUDA error: device-side assert triggered</p><NewLine></blockquote><NewLine><p>Some solutions that I found which worked temporarily are:</p><NewLine><ol><NewLine><li>Add  <code>os.environ['CUDA_LAUNCH_BLOCKING'] = '1'</code>  to the notebook. 2) GPU memory being used. So I removed the process that was running the notebook.</li><NewLine></ol><NewLine><p>However, nothing seems to be working permanently. Any suggestions?</p><NewLine></div>",https://discuss.pytorch.org/u/Flock1,(Flock Anizak),Flock1,"May 31, 2019, 11:01am",,,,,
46407,How to cross compile PyTorch for Pine64 platform,2019-05-28T09:01:47.981Z,0,149,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Can anyone help me…what are the steps I need to follow for cross-compiling the PyTorch for the pine64 board</p><NewLine></div>",https://discuss.pytorch.org/u/dinesh_mareedu,(Dinesh Mareedu),dinesh_mareedu,"May 28, 2019,  9:01am",,,,,
46366,About weight in BCEWithLogitsLoss to handle imbalanced samples,2019-05-28T01:56:07.190Z,0,123,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I read document of BCEWithLogitsLoss and I find “weight” parameter is different with   CrossEntropLoss’s “weight”. In BCEWithLogitsLoss, “weight” has to be a Tensor of<br/><NewLine>size nbatch , but in CrossEntropLoss, “weight” is a shape of C (number of classes).   I first not understand “nbatch” meaning, number of batch or batchsize. After trying, I<br/><NewLine>find it should be batchsize. I give a Tensor with N*C (batchsize * class_weights) size<br/><NewLine>as “weight” to BCEWithLogitsLoss and the loss function is running without any<br/><NewLine>problems.</p><NewLine><p>But I am still worried about whether I do this correct or not. So my question is<br/><NewLine>The “weight” parameter in BCEWithLogitsLoss do the same effort (balance imbalance<br/><NewLine>sample) as the one in CrossEntropLoss’s, right? If so, why they are different. I think<br/><NewLine>this may bother some user as me.</p><NewLine><p>I would be quite appreciate if someone could help me. Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/Elvin_Wang,(Elvin Wang),Elvin_Wang,"May 28, 2019,  1:56am",,,,,
46215,What would be typical non Python PyTorch production environments?,2019-05-25T21:21:05.945Z,0,140,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I guess C++, but I would need to get all options.</p><NewLine></div>",https://discuss.pytorch.org/u/Intel_Novel,(Intel Novel),Intel_Novel,"May 25, 2019,  9:21pm",,,,,
45864,2080ti cost more memory than 1080ti?,2019-05-22T02:33:55.524Z,1,286,"<div class=""post"" itemprop=""articleBody""><NewLine><p><img alt=""TIM%E5%9B%BE%E7%89%8720190522102742"" height=""399"" src=""https://discuss.pytorch.org/uploads/default/original/2X/6/667d7e4aafe7cb5714166b355237cb91029ddc96.png"" width=""569""/> <img alt=""TIM%E5%9B%BE%E7%89%8720190522102747"" height=""346"" src=""https://discuss.pytorch.org/uploads/default/original/2X/a/a8f376d8413ae5ce22dc69046f53c20a3bcc9c90.png"" width=""564""/></p><NewLine><p>mycode:</p><NewLine><pre><code class=""lang-auto"">from torchvision.models import vgg16<NewLine>import time<NewLine>model = vgg16.cuda()<NewLine>time.sleep(1000)<NewLine></code></pre><NewLine><p>it costs only 895MB memory on 1080ti ,while 1369MB memory on 2080ti.</p><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> Do you know the reason?</p><NewLine><p>1080ti environments:<br/><NewLine>pytorch version:1.1<br/><NewLine>cuda version:9.0<br/><NewLine>ubuntu version:18.04</p><NewLine><p>2080ti environments:<br/><NewLine>pytorch version:1.1<br/><NewLine>cuda version:10.1<br/><NewLine>ubuntu version:16.04</p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"May 22, 2019,  2:36am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s probably memory reserved by the CUDA driver. That seems to increase with newer cards. NVIDIA doesn’t explain why, but it might have to do with changes to the instruction set on newer architectures.</p><NewLine><p>You can look at how much is reserved by the driver by doing a minimal allocation, which creates a CUDA context:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import time<NewLine>torch.randn(1).cuda()<NewLine>time.sleep(1000)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It costs 357MB memory on 1080TI and 471MB on 2080TI. Thank you .</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/colesbury; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dalalaa; <NewLine> ,"REPLY_DATE 1: May 23, 2019,  1:05am; <NewLine> REPLY_DATE 2: May 23, 2019,  8:29am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
44543,Memory leak/ Random memory Allocation in Pytorch - CPU,2019-05-06T19:02:45.736Z,0,302,"<div class=""post"" itemprop=""articleBody""><NewLine><p>This is pertaining to pytorch version - 1.1.0. I am doing a simple inference on images. On each iteration, it will take one image and forward it. But everytime I run the code, memory utilization is different. In some runs, memory just keeps on increasing with each iteration. While in other runs it becomes stable after few iterations(value it gets settled to is also random).<br/><NewLine>The code is</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>#import torch.backends.cudnn as cudnn<NewLine>#cudnn.enabled = False<NewLine>#torch.backends.cudnn.deterministic = True<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>from torch.autograd import Variable<NewLine>import time<NewLine>import os<NewLine>import math<NewLine>import numpy as np<NewLine>import pickle<NewLine>import psutil<NewLine>process = psutil.Process(os.getpid())<NewLine><NewLine><NewLine>class GaussianBlur(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(GaussianBlur, self).__init__()<NewLine>        self.pad = 4<NewLine>        ##weight is proxy for gaussian kernel<NewLine>        weight = torch.from_numpy(np.random.uniform(0.0,1.0,(1,1,9,9)).astype(np.float32))<NewLine>        self.register_buffer('buf', weight)<NewLine>        return<NewLine>    def forward(self, x):<NewLine>        w = Variable(self.buf)<NewLine>        blurred = F.conv2d(F.pad(x, (self.pad,self.pad,self.pad,self.pad), 'replicate'), w, padding = 0)<NewLine>        return blurred<NewLine><NewLine>class ScalePyramid(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(ScalePyramid,self).__init__()<NewLine>        return<NewLine>    def forward(self,x):<NewLine>        curr = GaussianBlur().eval()(x)<NewLine>        return curr<NewLine><NewLine>with torch.no_grad():<NewLine>    ScalePyrGen = ScalePyramid()<NewLine>    while True:<NewLine>        ### y is proxy for image<NewLine>        y = np.random.uniform(0.0,255.0,(1,1,321, 512)).astype(np.float32)<NewLine>        y = torch.from_numpy(y)<NewLine>        ### Trimmed down the scale pyramid generation for the image<NewLine>        temp = ScalePyrGen(y)<NewLine>        ##Printing Memory Utilization<NewLine>        print(process.memory_info().rss/(1024*1024))<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Koustubh_Sinhal,(Koustubh Sinhal),Koustubh_Sinhal,"May 7, 2019,  6:03am",,,,,
44200,RNNLearner predict causes mem leak in docker,2019-05-02T16:58:33.104Z,0,162,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to run a simple predict, but it keeps eating memory. I am not sure is it a fastai or torch issue and I have no clue why.<br/><NewLine>[ENV]<br/><NewLine>docker: docker Ubuntu18.04, python3.6 container<br/><NewLine>pytorch: <a href=""https://download.pytorch.org/whl/cpu/torch-1.1.0-cp36-cp36m-linux_x86_64.whl"" rel=""nofollow noopener"">https://download.pytorch.org/whl/cpu/torch-1.1.0-cp36-cp36m-linux_x86_64.whl</a><br/><NewLine>fastai: 1.0.46</p><NewLine><pre><code class=""lang-auto"">from fastai.basic_train import load_learner<NewLine>predictor = load_learner(path, ""file.pkl"")<NewLine>pred_class, pred_idx, outputs = predictor.predict(""string"")<NewLine></code></pre><NewLine><p>I cant reproduce the memory leak on my mac or linux machine, so I think it is docker related problem, that maybe some missing lib. Any thought?</p><NewLine></div>",https://discuss.pytorch.org/u/zh0uquan,(Zhouquan),zh0uquan,"May 2, 2019,  4:58pm",,,,,
44075,Units tests for train plots (may be Jenkins?),2019-05-01T05:32:44.187Z,0,157,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!</p><NewLine><p>Just want to know, is there any method for unit testing for matplotlib plots generated by PyTorch. And may be with the Jenkins?</p><NewLine></div>",https://discuss.pytorch.org/u/rajasekhar,(Rajasekhar),rajasekhar,"May 1, 2019,  5:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>looking at this, there is some snippet, from here: <a href=""https://github.com/pytorch/pytorch/blob/master/test/test_tensorboard.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/test/test_tensorboard.py</a></p><NewLine><pre><code class=""lang-auto"">TEST_TENSORBOARD = True<NewLine>try:<NewLine>    import tensorboard.summary.writer.event_file_writer  # noqa F401<NewLine>except ImportError:<NewLine>    TEST_TENSORBOARD = False<NewLine><NewLine>TEST_MATPLOTLIB = True<NewLine>try:<NewLine>    import matplotlib<NewLine>    if os.environ.get('DISPLAY', '') == '':<NewLine>        matplotlib.use('Agg')<NewLine>    import matplotlib.pyplot as plt<NewLine>except ImportError:<NewLine>    TEST_MATPLOTLIB = False<NewLine>skipIfNoMatplotlib = unittest.skipIf(not TEST_MATPLOTLIB, ""no matplotlib"")<NewLine></code></pre><NewLine><p>I don’t know how to use these tests, any help?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/rajasekhar; <NewLine> ,"REPLY_DATE 1: May 1, 2019,  7:57am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
43835,How to compile libtorch with clang on linux,2019-04-28T13:01:37.739Z,0,190,"<div class=""post"" itemprop=""articleBody""><NewLine><p>how to compile libtorch with clang on linux</p><NewLine></div>",https://discuss.pytorch.org/u/shdut,(shdut),shdut,"April 28, 2019,  1:01pm",,,,,
43668,The best way to deploy ensemble model,2019-04-26T07:58:06.349Z,0,330,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everybody,<br/><NewLine>I’m face following problem: I got models from every cross validation fold and I want to put it all together in nice way. I think thats something like ensamble model problem.</p><NewLine><p>I’m familiar with some solutions, but I will be happy with every answer, which is why this question is open.</p><NewLine></div>",https://discuss.pytorch.org/u/Iksi,,Iksi,"April 26, 2019,  7:58am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think you are mistaking what K-fold cross validation is for: remember the difference between the process of training a model which returns a model (trained, hopefully) and the process of evaluating a model, which returns an evaluation of a model (a number usually).</p><NewLine><p>You can dig into K-fold briefly by reading <a href=""https://machinelearningmastery.com/k-fold-cross-validation/"" rel=""nofollow noopener"">this post</a>.</p><NewLine><p>Basically you use K-fold CV to evaluate the performance of a model. The final performance is the mean performance (± standard deviation) over all the folds.<br/><NewLine>If you want to obtain the final trained model you can retrain from scratch your model on all your training set (without K-split) and the evaluation you have done will tell you the expected performance on the separate test set.</p><NewLine><p>Maybe also this link can clarify:</p><NewLine><aside class=""onebox stackexchange""><NewLine><header class=""source""><NewLine><a href=""https://stats.stackexchange.com/questions/52274/how-to-choose-a-predictive-model-after-k-fold-cross-validation"" rel=""nofollow noopener"" target=""_blank"">stats.stackexchange.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><a href=""https://stats.stackexchange.com/users/3572/berk-u"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""Berk U."" class=""thumbnail onebox-avatar"" height=""60"" src=""https://www.gravatar.com/avatar/de1c1bf8393206415463491920067f8b?s=128&amp;d=identicon&amp;r=PG"" width=""60""/><NewLine></a><NewLine><h4><NewLine><a href=""https://stats.stackexchange.com/questions/52274/how-to-choose-a-predictive-model-after-k-fold-cross-validation"" rel=""nofollow noopener"" target=""_blank"">How to choose a predictive model after k-fold cross-validation?</a><NewLine></h4><NewLine><div class=""tags""><NewLine><strong>cross-validation, model-selection</strong><NewLine></div><NewLine><div class=""date""><NewLine>  asked by<NewLine>  <NewLine>  <a href=""https://stats.stackexchange.com/users/3572/berk-u"" rel=""nofollow noopener"" target=""_blank""><NewLine>    Berk U.<NewLine>  </a><NewLine>  on <a href=""https://stats.stackexchange.com/questions/52274/how-to-choose-a-predictive-model-after-k-fold-cross-validation"" rel=""nofollow noopener"" target=""_blank"">02:21AM - 15 Mar 13 UTC</a><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/acobobby; <NewLine> ,"REPLY_DATE 1: April 27, 2019, 11:33am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
42790,Rebuild libtorch on ubuntu 16.04 and slower than official library,2019-04-17T02:52:28.077Z,0,432,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I had rebuild libtorch C++ library(v1.0.1) based on /tools/build_libtorch.py file, unfortunately they are slower than official version.<br/><NewLine>The following is my configuration output. Are there any tricks? Thanks your enthusiasm.<br/><NewLine><img alt="":grinning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/grinning.png?v=9"" title="":grinning:""/><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/02735fb288b969f6ce6be70440e8e38c2d4ed989"" href=""https://discuss.pytorch.org/uploads/default/original/2X/0/02735fb288b969f6ce6be70440e8e38c2d4ed989.png"" title=""image.png""><img alt=""image"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/0/02735fb288b969f6ce6be70440e8e38c2d4ed989_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/0/02735fb288b969f6ce6be70440e8e38c2d4ed989_2_568x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/0/02735fb288b969f6ce6be70440e8e38c2d4ed989_2_568x500.png, https://discuss.pytorch.org/uploads/default/original/2X/0/02735fb288b969f6ce6be70440e8e38c2d4ed989.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/0/02735fb288b969f6ce6be70440e8e38c2d4ed989.png 2x"" width=""568""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">828×728 103 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/bdc49dd76f5b08426b569e2a7f436aeb8a9c5fe4"" href=""https://discuss.pytorch.org/uploads/default/original/2X/b/bdc49dd76f5b08426b569e2a7f436aeb8a9c5fe4.png"" title=""image.png""><img alt=""image"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/2X/b/bdc49dd76f5b08426b569e2a7f436aeb8a9c5fe4_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/2X/b/bdc49dd76f5b08426b569e2a7f436aeb8a9c5fe4_2_394x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/2X/b/bdc49dd76f5b08426b569e2a7f436aeb8a9c5fe4_2_394x500.png, https://discuss.pytorch.org/uploads/default/optimized/2X/b/bdc49dd76f5b08426b569e2a7f436aeb8a9c5fe4_2_591x750.png 1.5x, https://discuss.pytorch.org/uploads/default/original/2X/b/bdc49dd76f5b08426b569e2a7f436aeb8a9c5fe4.png 2x"" width=""394""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image.png</span><span class=""informations"">758×960 107 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/VisionZQ,(Vision Zq),VisionZQ,"April 17, 2019,  3:22am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you found out the reason?<br/><NewLine>I installed pytorch from source on win10, however the speed of loading model and predicting, libtorch is 3 times slower than that of caffe. I don’t know why. I’m much confused!</p><NewLine><p>– TORCH_VERSION : 1.1.0<br/><NewLine>– CAFFE2_VERSION : 1.1.0<br/><NewLine>– BUILD_ATEN_MOBILE : OFF<br/><NewLine>– BUILD_ATEN_ONLY : OFF<br/><NewLine>– BUILD_BINARY : False<br/><NewLine>– BUILD_CUSTOM_PROTOBUF : ON<br/><NewLine>– Link local protobuf : ON<br/><NewLine>– BUILD_DOCS : OFF<br/><NewLine>– BUILD_PYTHON : True<br/><NewLine>– Python version : 3.6.6<br/><NewLine>– Python executable : C:/Users/qjhs/AppData/Local/Programs/Python/Python36/python.exe<br/><NewLine>– Pythonlibs version : 3.6.6<br/><NewLine>– Python library : C:/Users/qjhs/AppData/Local/Programs/Python/Python36/libs/python36.lib<br/><NewLine>– Python includes : C:/Users/qjhs/AppData/Local/Programs/Python/Python36/include<br/><NewLine>– Python site-packages: Lib/site-packages<br/><NewLine>– BUILD_CAFFE2_OPS : True<br/><NewLine>– BUILD_SHARED_LIBS : ON<br/><NewLine>– BUILD_TEST : True<br/><NewLine>– USE_ASAN : OFF<br/><NewLine>– USE_CUDA : True<br/><NewLine>– CUDA static link : False<br/><NewLine>– USE_CUDNN : ON<br/><NewLine>– CUDA version : 10.0<br/><NewLine>– cuDNN version : 7.5.0<br/><NewLine>– CUDA root directory : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0<br/><NewLine>– CUDA library : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64/cuda.lib<br/><NewLine>– cudart library : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64/cudart.lib<br/><NewLine>– cublas library : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64/cublas.lib<br/><NewLine>– cufft library : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64/cufft.lib<br/><NewLine>– curand library : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64/curand.lib<br/><NewLine>– cuDNN library : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64/cudnn.lib<br/><NewLine>– nvrtc : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64/nvrtc.lib<br/><NewLine>– CUDA include path : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include<br/><NewLine>– NVCC executable : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin/nvcc.exe<br/><NewLine>– CUDA host compiler : F:/ProgramFiles/VS2017L/VC/Tools/MSVC/14.11.25503/bin/HostX64/x64/cl.exe<br/><NewLine>– USE_TENSORRT : OFF<br/><NewLine>– USE_ROCM : False<br/><NewLine>– USE_EIGEN_FOR_BLAS :<br/><NewLine>– USE_FBGEMM : OFF<br/><NewLine>– USE_FFMPEG : False<br/><NewLine>– USE_GFLAGS : OFF<br/><NewLine>– USE_GLOG : OFF<br/><NewLine>– USE_LEVELDB : False<br/><NewLine>– USE_LITE_PROTO : OFF<br/><NewLine>– USE_LMDB : False<br/><NewLine>– USE_METAL : OFF<br/><NewLine>– USE_MKL : ON<br/><NewLine>– USE_MKLDNN : OFF<br/><NewLine>– USE_NCCL : False<br/><NewLine>– USE_NNPACK : OFF<br/><NewLine>– USE_NUMPY : ON<br/><NewLine>– USE_OBSERVERS : ON<br/><NewLine>– USE_OPENCL : OFF<br/><NewLine>– USE_OPENCV : False<br/><NewLine>– USE_OPENMP : ON<br/><NewLine>– USE_PROF : OFF<br/><NewLine>– USE_QNNPACK : OFF<br/><NewLine>– USE_REDIS : OFF<br/><NewLine>– USE_ROCKSDB : OFF<br/><NewLine>– USE_ZMQ : OFF<br/><NewLine>– USE_DISTRIBUTED : False</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/asa008; <NewLine> ,"REPLY_DATE 1: April 23, 2019,  7:18am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
43277,How to disregard neurons with zero weight?,2019-04-23T01:09:11.942Z,0,119,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In the paper “Learning both Weights and Connections for Efficient Neural Networks”, the author mentioned that he masks the model and make the model disregards the neurons with zeros weight. I’m trying to implement this pruning techniques, however, I found that the size of the model and the inference time would not change if I only mask the model. I’m wondering is it possible to make the model disregard the neurons with zero weights to reduce the inference time or save the model with sparse tensor to reduce the size of the model?</p><NewLine></div>",https://discuss.pytorch.org/u/wenlee0816,(Wei-Jen Lee),wenlee0816,"April 23, 2019,  1:09am",,,,,
43014,When I should use transform_iddep_net.py?,2019-04-19T06:36:19.322Z,0,106,"<div class=""post"" itemprop=""articleBody""><NewLine><p>when i use ideep with caffe2 to deploy model, if i don’t fuse mul and add，there will be an error. TO fuse mul and add is must?</p><NewLine></div>",https://discuss.pytorch.org/u/ZHAIXINGZHAIYUE,(zhangkun),ZHAIXINGZHAIYUE,"April 19, 2019,  6:36am",,,,,
42678,Variable declared in nn.Parameter is not shown in model.parameters output,2019-04-16T01:59:29.492Z,8,520,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!</p><NewLine><p>I implemented a model where one variable is defined inside the function <code>nn.Parameter()</code> in order to be used during the optimization process. However, when I request the list of variables defined in the model, that variable is not displayed.</p><NewLine><p>Here is the class, in this case, the variable I need to optimize is <strong>W_ij</strong>:</p><NewLine><pre><code>class CapsNet(nn.Module):<NewLine>	def __init__(self, r = 1):<NewLine>		super(CapsNet, self).__init__()<NewLine>		self.ReLUConv1 = nn.Sequential(<NewLine>			nn.Conv2d(in_channels=1, out_channels=256, kernel_size=9, stride=1),<NewLine>			nn.ReLU(inplace=True)<NewLine>		)<NewLine>		self.W_ij = nn.Parameter(torch.rand((32, 10, 6*6, 16, 8), requires_grad = True))<NewLine>		self.PrimaryCaps = nn.ModuleList()<NewLine>		self.r = r<NewLine>		for _ in range(32):<NewLine>			self.PrimaryCaps.append(nn.Conv2d(in_channels=256, out_channels=8, kernel_size=9, stride=2))<NewLine>		self.decoder = nn.Sequential(<NewLine>			nn.Linear(16*10, 512),<NewLine>			nn.ReLU(inplace=True),<NewLine>			nn.Linear(512, 1024),<NewLine>			nn.ReLU(inplace=True),<NewLine>			nn.Linear(1024, 784),<NewLine>			nn.Sigmoid()<NewLine>		)<NewLine><NewLine>	def forward(self)...<NewLine></code></pre><NewLine><p>And here is the list of variables displayed by the model parameters output:</p><NewLine><pre><code>model = CapsNet()<NewLine>model.parameters<NewLine><NewLine>&lt;bound method Module.parameters of CapsNet(<NewLine>  (ReLUConv1): Sequential(<NewLine>	(0): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))<NewLine>	(1): ReLU(inplace)<NewLine>  )<NewLine>  (PrimaryCaps): ModuleList(<NewLine>	(0): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(1): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(2): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(3): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(4): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(5): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(6): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(7): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(8): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(9): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(10): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(11): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(12): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(13): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(14): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(15): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(16): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(17): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(18): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(19): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(20): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(21): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(22): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(23): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(24): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(25): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(26): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(27): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(28): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(29): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(30): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>	(31): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))<NewLine>  )<NewLine>  (decoder): Sequential(<NewLine>	(0): Linear(in_features=160, out_features=512, bias=True)<NewLine>	(1): ReLU(inplace)<NewLine>	(2): Linear(in_features=512, out_features=1024, bias=True)<NewLine>	(3): ReLU(inplace)<NewLine>	(4): Linear(in_features=1024, out_features=784, bias=True)<NewLine>	(5): Sigmoid()<NewLine>  )<NewLine>)&gt;<NewLine></code></pre><NewLine><p>Only the variables defined in the sequential functions are defined as parameters of my model, but not the variable <strong>W_ij</strong>. In this regards, I have two questions:</p><NewLine><ol><NewLine><li>How I can include the variable <strong>W_ij</strong> as a member of the set of parameters to be updated in the optimization step. So far, the loss function does not change during that part and I believe this is the reason.</li><NewLine><li>Do I have to set  <code>requires_grad = True</code> in the definition of the variable despite it is included in the nn.Parameter function?</li><NewLine></ol><NewLine><p>Any suggestions?</p><NewLine><p>Henry</p><NewLine></div>",https://discuss.pytorch.org/u/henrychacon,(Henry Chacon),henrychacon,"April 16, 2019,  1:59am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you try to print the parameters using:</p><NewLine><pre><code class=""lang-python"">print(list(model.parameters()))<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>:</p><NewLine><p>Here is part of the output because is larger than the number of characters allowed:</p><NewLine><pre><code>[Parameter containing:<NewLine>tensor([[[[[7.6739e-01, 5.7694e-01, 1.4690e-01,  ..., 8.4265e-01,<NewLine>			1.0182e-02, 5.0916e-01],<NewLine>		   [5.8372e-01, 7.2278e-01, 8.8761e-01,  ..., 3.6188e-01,<NewLine>			9.8832e-02, 7.6833e-01],<NewLine>		   [1.7700e-01, 4.6641e-01, 8.7544e-01,  ..., 1.2639e-01,<NewLine>			4.7846e-01, 5.5806e-01],<NewLine>		   ...,<NewLine>		   [7.1373e-01, 4.6280e-01, 7.5365e-01,  ..., 4.9755e-01,<NewLine>			8.8003e-01, 9.3670e-01],<NewLine>		   [5.3323e-01, 4.0508e-01, 4.6126e-01,  ..., 3.7920e-01,<NewLine>			7.5806e-01, 4.8837e-01],<NewLine>		   [5.3375e-02, 2.3048e-01, 3.9690e-01,  ..., 1.2138e-01,<NewLine>			4.3386e-01, 3.4856e-01]],<NewLine><NewLine>		  [[4.5299e-01, 3.3679e-01, 4.0874e-01,  ..., 7.7343e-01,<NewLine>			9.1206e-01, 1.8913e-01],<NewLine>		   [3.6609e-01, 3.6512e-01, 5.8318e-01,  ..., 5.7687e-01,<NewLine>			6.6697e-01, 5.4229e-02],<NewLine>		   [8.8892e-01, 4.5645e-02, 4.1808e-01,  ..., 3.7985e-01,<NewLine>			3.3769e-01, 6.7484e-01],<NewLine>		   ...,<NewLine>		   [5.0250e-01, 1.1942e-01, 3.0485e-01,  ..., 4.9193e-01,<NewLine>			6.6392e-01, 1.7286e-01],<NewLine>		   [1.4054e-01, 5.4048e-01, 4.5034e-02,  ..., 5.4134e-01,<NewLine>			1.1507e-01, 4.3528e-01],<NewLine>		   [4.1365e-01, 3.7377e-01, 8.4924e-01,  ..., 5.0650e-01,<NewLine>			6.7584e-01, 8.7766e-01]],<NewLine><NewLine>		  [[3.2468e-01, 1.3789e-01, 1.9836e-01,  ..., 6.3810e-01,<NewLine>			3.7559e-01, 7.9397e-01],<NewLine>		   [3.1314e-01, 9.2096e-01, 7.8847e-01,  ..., 6.8551e-01,<NewLine>			8.1996e-01, 2.2501e-01],<NewLine>		   [4.4605e-01, 4.3197e-01, 3.0152e-01,  ..., 1.6773e-01,<NewLine>			2.6052e-01, 4.0138e-01],<NewLine>		   ...,<NewLine>		   [1.4110e-01, 3.6810e-01, 1.4151e-01,  ..., 4.0450e-01,<NewLine>			8.0524e-01, 1.7194e-01],<NewLine>		   [3.4931e-01, 6.4747e-01, 8.5577e-01,  ..., 7.8586e-01,<NewLine>			1.7757e-01, 9.3789e-01],<NewLine>		   [4.0471e-01, 9.2119e-01, 2.8132e-02,  ..., 9.2249e-01,<NewLine>			8.8225e-01, 3.8468e-01]],<NewLine><NewLine>		  ...,<NewLine><NewLine>		  [[8.8568e-01, 7.2598e-01, 4.1293e-02,  ..., 1.0657e-01,<NewLine>			8.1734e-01, 3.4054e-01],<NewLine>		   [1.3737e-01, 6.8914e-01, 4.7814e-01,  ..., 2.0132e-02,<NewLine>			4.1065e-01, 9.6494e-01],<NewLine>		   [3.2537e-01, 1.7425e-02, 1.9855e-01,  ..., 2.5449e-01,<NewLine>			6.8413e-01, 8.9184e-02],<NewLine>		   ...,<NewLine>		   [4.0885e-01, 8.8580e-01, 4.7947e-01,  ..., 8.4072e-01,<NewLine>			1.7515e-01, 9.2421e-03],<NewLine>		   [2.8516e-01, 3.0517e-01, 6.1949e-01,  ..., 9.6947e-01,<NewLine>			3.4296e-01, 2.5695e-01],<NewLine>		   [4.7322e-01, 2.8092e-02, 8.4044e-01,  ..., 8.8433e-01,<NewLine>			1.0560e-01, 1.9199e-01]],<NewLine><NewLine>		  [[5.4768e-01, 1.4706e-01, 7.2479e-01,  ..., 5.7813e-01,<NewLine>			9.4183e-01, 1.6523e-01],<NewLine>		   [8.5458e-01, 2.9474e-01, 7.6239e-01,  ..., 8.4729e-01,<NewLine>			9.4218e-01, 7.7710e-01],<NewLine>		   [3.9862e-01, 4.3010e-01, 5.2872e-01,  ..., 5.6694e-01,<NewLine>			3.3170e-01, 9.6968e-01],<NewLine>		   ...,<NewLine>		   [6.9340e-02, 6.8474e-01, 9.6966e-01,  ..., 9.2907e-01,<NewLine>			4.6483e-01, 3.6082e-01],<NewLine>		   [8.2517e-02, 9.0183e-01, 8.5331e-01,  ..., 5.2363e-01,<NewLine>			7.5155e-01, 7.1173e-01],<NewLine>		   [1.4189e-01, 9.0067e-01, 7.9025e-01,  ..., 9.7414e-01,<NewLine>			1.3880e-01, 9.1656e-01]],<NewLine><NewLine>		  [[4.4559e-01, 7.4797e-01, 3.7663e-01,  ..., 4.4997e-01,<NewLine>			7.5805e-01, 1.4281e-02],<NewLine>		   [7.1061e-01, 1.3400e-01, 3.4829e-01,  ..., 4.0531e-01,<NewLine>			7.6288e-01, 9.3623e-03],<NewLine>		   [7.3082e-01, 1.6582e-01, 9.1881e-01,  ..., 2.0254e-02,<NewLine>			5.2745e-01, 1.7543e-01],<NewLine>		   ...,<NewLine>		   [1.2445e-01, 6.4993e-01, 7.2403e-01,  ..., 1.0495e-01,<NewLine>			8.2953e-01, 8.4761e-01],<NewLine>		   [2.2911e-01, 1.7264e-01, 7.1898e-01,  ..., 1.3536e-01,<NewLine>			2.9724e-01, 2.1592e-01],<NewLine>		   [7.1262e-03, 3.1743e-01, 3.5177e-02,  ..., 9.6428e-01,<NewLine>			2.6071e-01, 9.1934e-01]]],<NewLine><NewLine><NewLine>		 [[[4.1528e-01, 2.5934e-01, 7.9593e-01,  ..., 8.9152e-01,<NewLine>			7.7328e-01, 5.6743e-01],<NewLine>		   [7.9241e-01, 1.9825e-01, 5.7067e-01,  ..., 7.4108e-01,<NewLine>			3.2185e-01, 4.3047e-01],<NewLine>		   [8.9096e-01, 3.9115e-01, 6.8938e-01,  ..., 7.1494e-01,<NewLine>			1.4202e-01, 7.4137e-01],<NewLine>		   ...,<NewLine>		   [7.9495e-01, 7.3813e-01, 2.4878e-01,  ..., 4.3278e-01,<NewLine>			5.1972e-01, 5.1730e-01],<NewLine>		   [2.9028e-01, 1.6429e-01, 5.8873e-01,  ..., 6.6769e-01,<NewLine>			5.7929e-01, 3.6350e-01],<NewLine>		   [6.8588e-01, 5.2247e-01, 1.9651e-01,  ..., 9.9136e-01,<NewLine>			4.4651e-01, 6.3356e-01]],<NewLine><NewLine>		  [[2.0207e-01, 5.5784e-01, 4.7625e-01,  ..., 4.2619e-02,<NewLine>			9.9390e-01, 8.2427e-01],<NewLine>		   [1.3512e-01, 7.1366e-01, 3.0682e-01,  ..., 5.2905e-01,<NewLine>			7.0548e-01, 3.7589e-01],<NewLine>		   [3.9355e-01, 7.6408e-01, 2.8698e-01,  ..., 3.3699e-01,<NewLine>			1.1217e-02, 7.8939e-01],<NewLine>		   ...,<NewLine>		   [1.7819e-01, 7.4309e-01, 6.5005e-01,  ..., 2.3578e-01,<NewLine>			4.5845e-01, 6.0670e-01],<NewLine>		   [5.8735e-02, 1.4817e-02, 2.4480e-01,  ..., 6.4879e-01,<NewLine>			1.9488e-01, 6.4382e-01],<NewLine>		   [7.9225e-01, 7.8734e-01, 5.3828e-02,  ..., 3.0421e-02,<NewLine>			5.1285e-01, 5.9031e-02]],<NewLine><NewLine>		  [[5.7578e-01, 8.2437e-01, 6.7843e-01,  ..., 6.5092e-01,<NewLine>			5.2705e-01, 7.0242e-02],<NewLine>		   [3.6100e-01, 6.9821e-01, 6.8266e-01,  ..., 9.9500e-01,<NewLine>			1.7203e-03, 3.6993e-01],<NewLine>		   [2.3840e-01, 3.5518e-01, 5.6097e-02,  ..., 4.5848e-01,<NewLine>			5.1774e-01, 5.7378e-01],<NewLine>		   ...,<NewLine>		   [5.9616e-01, 7.2055e-01, 5.1226e-01,  ..., 1.6518e-01,<NewLine>			8.6783e-01, 2.9836e-01],<NewLine>		   [8.3414e-01, 5.8019e-01, 6.2259e-01,  ..., 2.7784e-01,<NewLine>			5.0720e-01, 5.6435e-01],<NewLine>		   [5.9361e-01, 2.3667e-01, 9.4878e-01,  ..., 6.9894e-02,<NewLine>			3.2900e-01, 5.0268e-01]],<NewLine><NewLine>		  ...,<NewLine><NewLine>		  [[5.9668e-01, 1.8945e-02, 7.3882e-01,  ..., 1.6845e-01,<NewLine>			2.7940e-01, 3.1264e-01],<NewLine>		   [8.7321e-02, 5.1667e-01, 1.5205e-02,  ..., 3.9717e-01,<NewLine>			1.6481e-02, 7.3159e-01],<NewLine>		   [8.0506e-01, 8.7750e-01, 5.5920e-01,  ..., 9.0319e-01,<NewLine>			2.0946e-01, 1.1467e-01],<NewLine>		   ...,<NewLine>		   [2.8784e-01, 8.4264e-02, 9.8612e-01,  ..., 7.0642e-01,<NewLine>			7.5766e-01, 5.6043e-01],<NewLine>		   [3.5024e-01, 4.3340e-01, 5.8842e-01,  ..., 5.2658e-01,<NewLine>			3.8618e-01, 7.4563e-01],<NewLine>		   [9.5279e-01, 2.0545e-01, 3.0640e-01,  ..., 8.6450e-01,<NewLine>			2.7790e-01, 3.3090e-01]],<NewLine><NewLine>		  [[3.2479e-01, 7.5468e-01, 5.2824e-01,  ..., 4.0495e-02,<NewLine>			4.0978e-01, 8.5620e-01],<NewLine>		   [4.3639e-01, 5.1542e-01, 4.6560e-01,  ..., 6.2835e-01,<NewLine>			9.0295e-01, 3.5546e-01],<NewLine>		   [4.6104e-01, 3.1599e-01, 3.6135e-02,  ..., 3.3267e-01,<NewLine>			6.4670e-01, 8.5443e-01],<NewLine>		   ...,<NewLine>		   [6.3489e-01, 8.9880e-02, 5.4958e-02,  ..., 4.7443e-01,<NewLine>			8.0634e-01, 8.3051e-01],<NewLine>		   [4.0276e-01, 2.4590e-01, 2.0508e-01,  ..., 9.1851e-01,<NewLine>			5.1149e-01, 4.3412e-01],<NewLine>		   [1.5342e-03, 1.8887e-01, 2.1114e-01,  ..., 9.6045e-02,<NewLine>			2.0662e-01, 4.6233e-01]],<NewLine><NewLine>		  [[7.3246e-01, 4.2454e-01, 6.3876e-01,  ..., 4.6388e-01,<NewLine>			6.6292e-01, 7.6189e-01],<NewLine>		   [9.1187e-01, 5.4363e-01, 5.4122e-01,  ..., 1.5614e-01,<NewLine>			6.9637e-01, 1.4138e-01],<NewLine>		   [1.7490e-01, 2.1351e-01, 8.4375e-01,  ..., 5.0843e-01,<NewLine>			9.5231e-01, 2.9017e-01],<NewLine>		   ...,<NewLine>		   [9.4328e-01, 7.4659e-01, 9.0242e-01,  ..., 1.7258e-01,<NewLine>			2.8283e-01, 8.7365e-01],<NewLine>		   [3.9166e-01, 7.4339e-01, 1.7533e-01,  ..., 3.8704e-01,<NewLine>			1.5566e-01, 6.7763e-02],<NewLine>		   [7.5075e-01, 1.2519e-01, 8.7407e-02,  ..., 2.5569e-01,<NewLine>			7.1487e-01, 5.2214e-01]]],<NewLine><NewLine><NewLine>		 [[[8.6069e-01, 3.0978e-01, 2.6947e-01,  ..., 8.8378e-01,<NewLine>			2.0586e-01, 2.4953e-01],<NewLine>		   [4.9243e-01, 5.7869e-01, 6.8052e-01,  ..., 6.4253e-01,<NewLine>			3.3459e-01, 4.3123e-02],<NewLine>		   [8.4683e-01, 2.0274e-01, 7.2146e-01,  ..., 5.8167e-01,<NewLine>			3.4023e-01, 8.2120e-01],<NewLine>		   ...,<NewLine>		   [7.2076e-01, 5.7922e-01, 5.3045e-01,  ..., 4.4999e-01,<NewLine>			3.1915e-01, 9.4792e-01],<NewLine>		   [8.5810e-02, 9.8088e-01, 2.8822e-01,  ..., 2.4828e-01,<NewLine>			5.9598e-01, 6.7687e-01],<NewLine>		   [7.3960e-01, 1.3854e-01, 1.7701e-01,  ..., 4.7463e-01,<NewLine>			8.7126e-03, 4.5565e-01]],<NewLine><NewLine>		  [[9.6817e-01, 3.5741e-01, 1.5915e-01,  ..., 2.2334e-01,<NewLine>			2.2585e-01, 4.2289e-02],<NewLine>		   [9.7334e-01, 9.7148e-01, 7.7071e-01,  ..., 6.4645e-01,<NewLine>			4.1084e-01, 5.8376e-01],<NewLine>		   [3.0449e-01, 4.0146e-01, 9.2943e-01,  ..., 8.4704e-01,<NewLine>			6.9278e-01, 4.3482e-01],<NewLine>		   ...,<NewLine>		   [7.4013e-01, 6.5953e-01, 8.9109e-01,  ..., 8.8038e-01,<NewLine>			9.1634e-02, 8.0954e-01],<NewLine>		   [7.8204e-01, 3.8120e-01, 1.0622e-01,  ..., 6.0812e-01,<NewLine>			3.3097e-02, 4.0971e-03],<NewLine>		   [5.7029e-03, 2.9170e-01, 1.4529e-02,  ..., 5.3710e-01,<NewLine>			9.9842e-03, 5.4186e-01]],<NewLine><NewLine>		  [[7.6594e-01, 9.5179e-01, 4.7323e-01,  ..., 1.4289e-01,<NewLine>			3.7384e-01, 8.2052e-01],<NewLine>		   [4.9726e-01, 5.7614e-01, 3.8949e-01,  ..., 5.6762e-01,<NewLine>			8.2138e-01, 3.3427e-01],<NewLine>		   [8.1521e-01, 8.3014e-01, 7.4531e-01,  ..., 4.6001e-01,<NewLine>			3.4006e-01, 4.3338e-01],<NewLine>		   ...,<NewLine>		   [5.3341e-01, 4.2412e-01, 4.1900e-01,  ..., 9.3899e-01,<NewLine>			5.7563e-02, 4.1517e-01],<NewLine>		   [2.4708e-01, 3.8884e-01, 3.6465e-01,  ..., 4.6556e-01,<NewLine>			5.2193e-02, 7.1592e-01],<NewLine>		   [9.0153e-01, 5.5619e-02, 7.6606e-01,  ..., 5.6541e-01,<NewLine>			2.7944e-01, 9.8611e-01]],<NewLine><NewLine>		  ...,<NewLine><NewLine>		  [[5.5920e-02, 5.4057e-02, 7.8622e-01,  ..., 7.3954e-01,<NewLine>			5.8098e-01, 8.3185e-01],<NewLine>		   [2.0571e-01, 6.4555e-01, 7.4586e-01,  ..., 8.1000e-01,<NewLine>			8.9614e-01, 7.8818e-02],<NewLine>		   [5.9937e-01, 2.9032e-01, 7.9013e-01,  ..., 8.8225e-01,<NewLine>			5.3767e-01, 2.5079e-02],<NewLine>		   ...,<NewLine>		   [2.4588e-01, 9.7691e-01, 8.1896e-01,  ..., 5.9354e-01,<NewLine>			3.5735e-01, 6.8635e-01],<NewLine>		   [3.0577e-01, 8.1374e-01, 5.8728e-01,  ..., 4.0778e-01,<NewLine>			1.0003e-01, 2.9440e-01],<NewLine>		   [6.7664e-01, 1.3860e-01, 1.4878e-01,  ..., 9.8078e-02,<NewLine>			6.8442e-02, 7.3116e-01]],<NewLine><NewLine>		  [[3.5372e-01, 1.7224e-01, 6.5888e-01,  ..., 6.5219e-02,<NewLine>			9.8796e-01, 7.1238e-01],<NewLine>		   [4.2807e-01, 2.1368e-01, 8.6156e-01,  ..., 9.3650e-01,<NewLine>			8.0852e-01, 7.2590e-01],<NewLine>		   [2.3365e-01, 7.5662e-01, 1.5440e-01,  ..., 3.9711e-02,<NewLine>			7.2361e-01, 6.0657e-01],<NewLine>		   ...,<NewLine>		   [9.2539e-02, 9.3710e-01, 2.0238e-01,  ..., 1.1861e-01,<NewLine>			1.0211e-01, 8.5345e-01],<NewLine>		   [4.7541e-02, 5.3932e-01, 1.7160e-01,  ..., 6.5774e-01,<NewLine>			5.3190e-01, 3.4805e-01],<NewLine>		   [8.0904e-01, 7.6469e-01, 2.4000e-01,  ..., 3.4909e-01,<NewLine>			6.9401e-01, 8.1628e-01]],<NewLine><NewLine>		  [[7.4898e-01, 8.6198e-01, 2.7199e-01,  ..., 1.7970e-01,<NewLine>			4.8535e-01, 9.6696e-02],<NewLine>		   [5.6890e-01, 2.5746e-01, 9.2011e-01,  ..., 8.0284e-01,<NewLine>			9.1778e-01, 6.9542e-01],<NewLine>		   [8.8634e-01, 3.3647e-01, 7.8304e-01,  ..., 9.5858e-01,<NewLine>			8.6181e-01, 7.0420e-01],<NewLine>		   ...,<NewLine>		   [1.1258e-01, 2.9942e-01, 7.0951e-01,  ..., 6.7769e-01,<NewLine>			7.7942e-02, 7.8483e-01],<NewLine>		   [9.6725e-01, 1.1985e-01, 7.3592e-02,  ..., 4.0335e-01,<NewLine>			7.1557e-01, 5.9206e-01],<NewLine>		   [2.7142e-01, 1.0375e-01, 6.8905e-01,  ..., 9.1768e-01,<NewLine>			7.1096e-01, 5.4644e-01]]],<NewLine><NewLine><NewLine>		 ...,<NewLine><NewLine><NewLine>		 [[[2.3049e-01, 2.2262e-01, 6.9831e-01,  ..., 3.4785e-01,<NewLine>			1.9261e-01, 1.6485e-01],<NewLine>		   [8.7370e-02, 4.0153e-01, 5.5550e-01,  ..., 8.7350e-01,<NewLine>			6.5091e-01, 5.7027e-01],<NewLine>		   [8.2581e-01, 8.0229e-01, 5.1905e-02,  ..., 8.4354e-03,<NewLine>			5.5208e-01, 8.0015e-01],<NewLine>		   ...,<NewLine>		   [3.5276e-01, 1.7161e-02, 9.9382e-01,  ..., 5.7967e-01,<NewLine>			6.4500e-01, 3.8972e-01],<NewLine>		   [2.9985e-02, 4.9689e-01, 3.9898e-01,  ..., 9.8758e-01,<NewLine>			4.6939e-01, 8.5264e-01],<NewLine>		   [4.9803e-01, 8.2988e-01, 1.3325e-01,  ..., 3.8771e-02,<NewLine>			5.5842e-01, 6.0082e-01]],<NewLine><NewLine>		  [[4.4922e-01, 1.4637e-01, 3.6825e-01,  ..., 3.8888e-01,<NewLine>			9.9635e-01, 7.6231e-01],<NewLine>		   [6.0588e-01, 5.4020e-02, 7.7364e-01,  ..., 7.0918e-01,<NewLine>			3.1280e-01, 4.1706e-01],<NewLine>		   [5.4832e-01, 5.0517e-01, 3.4220e-01,  ..., 7.1315e-01,<NewLine>			4.6581e-01, 3.6415e-01],<NewLine>		   ...,<NewLine>		   [3.8192e-01, 4.6033e-01, 5.8473e-01,  ..., 6.5528e-01,<NewLine>			2.8630e-01, 7.3145e-01],<NewLine>		   [9.0302e-01, 9.5635e-01, 1.0854e-01,  ..., 1.2172e-01,<NewLine>			8.8041e-01, 4.6316e-01],<NewLine>		   [2.3110e-01, 3.6821e-01, 7.7418e-01,  ..., 6.3039e-01,<NewLine>			9.4273e-01, 2.1212e-01]],<NewLine><NewLine>		  [[7.8398e-01, 7.0921e-01, 9.6487e-02,  ..., 8.9825e-01,<NewLine>			5.1998e-01, 9.8542e-01],<NewLine>		   [9.7818e-01, 5.7448e-01, 2.6835e-02,  ..., 7.5816e-01,<NewLine>			3.2348e-01, 4.5573e-02],<NewLine>		   [9.0839e-01, 1.1082e-01, 1.4642e-01,  ..., 8.3780e-01,<NewLine>			8.9171e-01, 4.4667e-01],<NewLine>		   ...,<NewLine>		   [5.9776e-01, 9.3012e-01, 7.6937e-01,  ..., 3.2172e-01,<NewLine>			3.7485e-01, 2.8772e-01],<NewLine>		   [3.4350e-01, 6.0984e-01, 3.2810e-01,  ..., 4.6941e-01,<NewLine>			3.6490e-01, 8.4864e-02],<NewLine>		   [7.5752e-01, 5.6821e-01, 3.1447e-01,  ..., 8.8520e-01,<NewLine>			9.6437e-01, 3.7760e-01]],<NewLine><NewLine>		  ...,<NewLine><NewLine>		  [[7.1025e-02, 7.6608e-01, 2.3979e-01,  ..., 8.0521e-02,<NewLine>			4.2642e-01, 1.3974e-02],<NewLine>		   [1.8102e-01, 4.2851e-01, 5.1516e-01,  ..., 7.2275e-01,<NewLine>			3.9572e-01, 3.0240e-01],<NewLine>		   [1.6883e-01, 1.5237e-01, 1.8105e-01,  ..., 8.7423e-01,<NewLine>			6.8085e-01, 6.6449e-01],<NewLine>		   ...,<NewLine>		   [4.6055e-01, 9.7129e-01, 5.5012e-01,  ..., 1.2927e-01,<NewLine>			1.9321e-01, 5.7518e-01],<NewLine>		   [2.5797e-04, 3.2652e-01, 4.4737e-01,  ..., 8.0914e-01,<NewLine>			5.6270e-01, 6.4787e-01],<NewLine>		   [1.7741e-01, 5.4729e-01, 9.7928e-03,  ..., 3.7991e-01,<NewLine>			7.8216e-01, 9.6301e-01]],<NewLine><NewLine>		  [[8.4330e-01, 2.1859e-01, 9.1057e-01,  ..., 1.6024e-01,<NewLine>			6.5961e-01, 5.2498e-01],<NewLine>		   [9.8151e-01, 3.6714e-01, 2.8921e-01,  ..., 8.2588e-01,<NewLine>			3.7878e-02, 8.7757e-01],<NewLine>		   [9.8103e-01, 5.2342e-01, 2.5119e-01,  ..., 5.1633e-01,<NewLine>			1.6919e-01, 5.6062e-01],<NewLine>		   ...,<NewLine>		   [9.0493e-01, 8.4267e-01, 3.7056e-01,  ..., 6.0395e-01,<NewLine>			6.1962e-01, 4.1659e-01],<NewLine>		   [7.0052e-01, 8.7154e-01, 1.7824e-01,  ..., 5.4432e-02,<NewLine>			5.0960e-01, 9.5224e-02],<NewLine>		   [5.7251e-01, 8.0133e-01, 5.8870e-01,  ..., 3.0814e-01,<NewLine>			7.8979e-01, 4.4498e-01]],<NewLine><NewLine>		  [[7.7022e-01, 9.3518e-01, 8.9363e-01,  ..., 6.5739e-01,<NewLine>			5.6208e-01, 2.7751e-02],<NewLine>		   [7.1668e-01, 8.3066e-02, 8.0086e-01,  ..., 3.1376e-01,<NewLine>			8.2750e-01, 1.1812e-01],<NewLine>		   [5.1732e-01, 8.6545e-01, 7.4015e-01,  ..., 2.9727e-02,<NewLine>			9.6790e-01, 1.1910e-01],<NewLine>		   ...,<NewLine>		   [6.4426e-01, 6.2727e-01, 3.4956e-01,  ..., 8.2050e-01,<NewLine>			2.2073e-01, 5.4220e-01],<NewLine>		   [1.3593e-01, 1.4675e-01, 7.4674e-01,  ..., 8.3513e-01,<NewLine>			1.7398e-01, 3.7785e-01],<NewLine>		   [3.8345e-01, 4.9171e-01, 9.0600e-01,  ..., 1.4847e-01,<NewLine>			9.7346e-01, 6.1814e-01]]],<NewLine><NewLine><NewLine>		 [[[4.1622e-01, 1.5042e-02, 6.2890e-01,  ..., 5.4780e-01,<NewLine>			6.1426e-01, 9.4636e-01],<NewLine>		   [8.7305e-01, 9.5013e-01, 2.2473e-01,  ..., 9.7041e-01,<NewLine>			9.1911e-01, 3.5189e-01],<NewLine>		   [6.5247e-01, 3.0404e-01, 3.0104e-01,  ..., 8.4302e-01,<NewLine>			4.1354e-01, 3.5731e-01],<NewLine></code></pre><NewLine><p>I uploaded the output of the file in the following link in case you want to review the entire set of records</p><NewLine><p><a href=""https://github.com/henrychacon/CapsNet/blob/master/parameters.txt"" rel=""nofollow noopener"">https://github.com/henrychacon/CapsNet/blob/master/parameters.txt</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>, by the way, the version of Torch I am using it is: 1.0.1.post2</p><NewLine><p>Thanks!<br/><NewLine>Henry</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Probably it would be easier to just check the names, since the actual parameter values are quite big:</p><NewLine><pre><code class=""lang-python"">for name, _ in model.named_parameters():<NewLine>    print(name)<NewLine></code></pre><NewLine><p>Could you run this code and check if your parameter is there?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>:</p><NewLine><p>When I run the command:</p><NewLine><pre><code>for name, _ in model.named_parameters():<NewLine>    print(name)<NewLine></code></pre><NewLine><p>The parameter <strong>W_ij</strong> is displayed, however, it is not optimized during the learning process.</p><NewLine><ul><NewLine><li>Is there any way that I explicitly can include that parameter into the optimization process?</li><NewLine><li>Why the parameter is shown in the command <code>named_parameter</code> in the for loop only (not when I execute <code>print(model.named_parameters)</code>) and not in the <code>model.parameters</code> used as input of <code>optimizer = Adam(model.parameters())</code>?</li><NewLine></ul><NewLine><p>Here is the output:</p><NewLine><pre><code>W_ij<NewLine>ReLUConv1.0.weight<NewLine>ReLUConv1.0.bias<NewLine>PrimaryCaps.0.weight<NewLine>PrimaryCaps.0.bias<NewLine>PrimaryCaps.1.weight<NewLine>PrimaryCaps.1.bias<NewLine>PrimaryCaps.2.weight<NewLine>PrimaryCaps.2.bias<NewLine>PrimaryCaps.3.weight<NewLine>PrimaryCaps.3.bias<NewLine>PrimaryCaps.4.weight<NewLine>PrimaryCaps.4.bias<NewLine>PrimaryCaps.5.weight<NewLine>PrimaryCaps.5.bias<NewLine>PrimaryCaps.6.weight<NewLine>PrimaryCaps.6.bias<NewLine>PrimaryCaps.7.weight<NewLine>PrimaryCaps.7.bias<NewLine>PrimaryCaps.8.weight<NewLine>PrimaryCaps.8.bias<NewLine>PrimaryCaps.9.weight<NewLine>PrimaryCaps.9.bias<NewLine>PrimaryCaps.10.weight<NewLine>PrimaryCaps.10.bias<NewLine>PrimaryCaps.11.weight<NewLine>PrimaryCaps.11.bias<NewLine>PrimaryCaps.12.weight<NewLine>PrimaryCaps.12.bias<NewLine>PrimaryCaps.13.weight<NewLine>PrimaryCaps.13.bias<NewLine>PrimaryCaps.14.weight<NewLine>PrimaryCaps.14.bias<NewLine>PrimaryCaps.15.weight<NewLine>PrimaryCaps.15.bias<NewLine>PrimaryCaps.16.weight<NewLine>PrimaryCaps.16.bias<NewLine>PrimaryCaps.17.weight<NewLine>PrimaryCaps.17.bias<NewLine>PrimaryCaps.18.weight<NewLine>PrimaryCaps.18.bias<NewLine>PrimaryCaps.19.weight<NewLine>PrimaryCaps.19.bias<NewLine>PrimaryCaps.20.weight<NewLine>PrimaryCaps.20.bias<NewLine>PrimaryCaps.21.weight<NewLine>PrimaryCaps.21.bias<NewLine>PrimaryCaps.22.weight<NewLine>PrimaryCaps.22.bias<NewLine>PrimaryCaps.23.weight<NewLine>PrimaryCaps.23.bias<NewLine>PrimaryCaps.24.weight<NewLine>PrimaryCaps.24.bias<NewLine>PrimaryCaps.25.weight<NewLine>PrimaryCaps.25.bias<NewLine>PrimaryCaps.26.weight<NewLine>PrimaryCaps.26.bias<NewLine>PrimaryCaps.27.weight<NewLine>PrimaryCaps.27.bias<NewLine>PrimaryCaps.28.weight<NewLine>PrimaryCaps.28.bias<NewLine>PrimaryCaps.29.weight<NewLine>PrimaryCaps.29.bias<NewLine>PrimaryCaps.30.weight<NewLine>PrimaryCaps.30.bias<NewLine>PrimaryCaps.31.weight<NewLine>PrimaryCaps.31.bias<NewLine>decoder.0.weight<NewLine>decoder.0.bias<NewLine>decoder.2.weight<NewLine>decoder.2.bias<NewLine>decoder.4.weight<NewLine>decoder.4.bias</code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>If the parameter is shown in <code>model.named_parameters()</code>, it will also be in <code>model.parameters()</code>, since internally <code>parameters()</code> calls <code>named_parameters()</code> as shown <a href=""https://github.com/pytorch/pytorch/blob/db611b7caf0369ba628c5ed8bba42cc22aadfb20/torch/nn/modules/module.py#L815"" rel=""nofollow noopener"">here</a>.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>How do you check, if the parameter has been optimized? Do you substract it from itself? If yes, you may want to do an additional <code>clone</code> on the previous version because if you don’t clone it, the variable only holds a reference to this parameter , which means the difference will always be 0.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/justusschock"">@justusschock</a>, in my model, I have two loss functions, one is associated with a matrix parameter defined with nn.parameter() and the other with a sequential CNN. In the first one, the loss value is constant during the optimization process, while in the second one no.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/justusschock"">@justusschock</a> and <a class=""mention"" href=""/u/ptrblck"">@ptrblck</a>: Thanks for your help. I changed the optimizer function from Adam to SGD and the loss function is minimized.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/henrychacon; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/henrychacon; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/henrychacon; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/justusschock; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/henrychacon; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/henrychacon; <NewLine> ,"REPLY_DATE 1: April 16, 2019, 10:36am; <NewLine> REPLY_DATE 2: April 16, 2019,  2:54pm; <NewLine> REPLY_DATE 3: April 16, 2019,  7:06pm; <NewLine> REPLY_DATE 4: April 16, 2019, 10:05pm; <NewLine> REPLY_DATE 5: April 16, 2019, 11:10pm; <NewLine> REPLY_DATE 6: April 17, 2019,  9:37am; <NewLine> REPLY_DATE 7: April 17, 2019,  9:40am; <NewLine> REPLY_DATE 8: April 17, 2019, 11:40pm; <NewLine> REPLY_DATE 9: April 18, 2019,  1:42am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> 
42734,e-GPU for running pre-trained PyTorch models from a macbook,2019-04-16T12:40:58.472Z,0,211,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone,<br/><NewLine>I am seeking for advice before investing in an e-GPU.</p><NewLine><p>Doing PhD experiments in deep learning for sound synthesis and audio effects.<br/><NewLine>I would like to make some demonstrations, for instance when presenting in conference, of my models running real-time.</p><NewLine><p>So the setup is not for training models, but for running them in real-time from my macbook. For this an average GPU and 4GB memory should be enough.</p><NewLine><p>I have seen the Nvidia Jetson Nano and consider it later for doing standalone instruments.</p><NewLine><p>But for now, I would like to ask about e-GPUs that are Mac OS compatible, with cuda and pytorch support.<br/><NewLine>Are there some rather straight-forward solutions nowadays ? That are well compatible and allow for easy deployment from training models on servers to using them in live conditions from a laptop ?<br/><NewLine>Regarding budget, we could go up to 500€ for the enclosure and 500€ for the card.</p><NewLine><p>If needed, few more details about the implementations. I already experimented with exporting models with LibTorch to C++ and embedding them as PureData externals to make realtime instruments and plugins. I want to experiment more with this kind of deployment and would thus need an e-GPU to avoid latency.</p><NewLine><p>Thanks in advance and happy learning everyone !</p><NewLine></div>",https://discuss.pytorch.org/u/catosphere,(catosphere),catosphere,"April 16, 2019, 12:40pm",,,,,
42619,How can I feed the big data in Pytorch when the CPU sources are limited?,2019-04-15T11:41:40.055Z,0,138,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My train dataset is 20GB saved in a .mat file.Before I train the model,I need to transform the array format to tensor, this caused the error that RAM is not enough. Can Pytorch feed the array directly?I think the tensor is occupied so much computational space.How can I transform the big dataset into tensor to train the model successfully?</p><NewLine></div>",https://discuss.pytorch.org/u/PuYuQian,(VeraPu),PuYuQian,"April 15, 2019, 11:41am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think it has default implementation for this. How about saving the .mat file into several parts?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/klory; <NewLine> ,"REPLY_DATE 1: April 15, 2019,  2:02pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
41857,New category for PyTorch deployment,2019-04-07T03:56:40.265Z,2,265,"<div class=""post"" itemprop=""articleBody""><NewLine><p>With PyTorch now supporting production, thoughts on great projects for serving, model mgmt, etc…? We’ve tested a few projects like MLFlow/Kubeflow (with varying success) but would love to hear your thoughts on whether you’ve found them to support your needs or if there are other options to consider.</p><NewLine></div>",https://discuss.pytorch.org/u/jspisak,"(Facebook AI, Product Manager)",jspisak,"April 7, 2019,  3:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’d really like to see something like tf serving for PyTorch. In terms of framework, automatic request batching seems one of the most important features where I would not know how to easily get that.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks Thomas, we have someone looking into implementing Autobatching and will keep you looped as to the progress.</p><NewLine><p>On serving, are you using a cloud platform today or another project? Concretely what features do you most critically need (tf serving has a lot of things that I suspect go unused)?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jspisak; <NewLine> ,"REPLY_DATE 1: April 7, 2019,  8:41am; <NewLine> REPLY_DATE 2: April 7, 2019,  3:19pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> 
