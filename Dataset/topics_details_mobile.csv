id,title,created_at,reply_count,views,description,creator_link,creator_name,creator_alias,post_date,post_likes,replies,repliers_links,reply_dates,reply_likes
57726,About the Mobile category,2019-10-08T23:12:39.747Z,0,209,"<div class=""post"" itemprop=""articleBody""><NewLine><p>This category is dedicated for iOS and Android issues, new features and general discussion of PyTorch Mobile.</p><NewLine></div>",https://discuss.pytorch.org/u/jspisak,"(Facebook AI, Product Manager)",jspisak,"October 10, 2019,  3:49pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We tried to deploy a object detection model in Android. In PyTorch model, ‘nn.functional.interpolate’ was used in forward path to upsample feature maps . Torch script converted the model without any issues. We are not able to load the model in Android app using ‘Module.load( )’ API. This API is throwing an exception.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vgsprasad; <NewLine> ,"REPLY_DATE 1: November 15, 2019,  7:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
75830,1 channel input on Pytorch Mobile - Android,2020-04-08T09:39:42.815Z,0,128,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m trying to do a mnist classifier with pytorch-mobile.<br/><NewLine>I’ve export my model with jit in python.</p><NewLine><pre><code class=""lang-auto"">example = torch.rand(1, 1, 28, 28)<NewLine>trace = torch.jit.trace(net, example)<NewLine>trace.save(""mymodel.pt"")<NewLine></code></pre><NewLine><p>And this is how I use it in my kotlin project.</p><NewLine><pre><code class=""lang-auto"">        fun assetFilePath(context: Context, assetName: String): String {<NewLine>            val file = File(context.filesDir, assetName)<NewLine>            if (file.exists() &amp;&amp; file.length() &gt; 0) {<NewLine>                return file.absolutePath<NewLine>            }<NewLine>            context.assets.open(assetName).use { inputStream -&gt;<NewLine>                FileOutputStream(file).use { outputStream -&gt;<NewLine>                    val buffer = ByteArray(4 * 1024)<NewLine>                    var read: Int<NewLine>                    while (inputStream.read(buffer).also { read = it } != -1) {<NewLine>                        outputStream.write(buffer, 0, read)<NewLine>                    }<NewLine>                    outputStream.flush()<NewLine>                }<NewLine>                return file.absolutePath<NewLine>            }<NewLine>        }<NewLine><NewLine>        val module:Module = Module.load(assetFilePath(this, ""mymodel.pt""))<NewLine><NewLine>        val resizedBitmap:Bitmap = Bitmap.createScaledBitmap(<NewLine>            MyCanvasView.extraBitmap,<NewLine>            28,<NewLine>            28,<NewLine>            true<NewLine>        )<NewLine><NewLine>        val inputTensor: Tensor = TensorImageUtils.bitmapToFloat32Tensor(<NewLine>            resizedBitmap,<NewLine>            TensorImageUtils.TORCHVISION_NORM_MEAN_RGB,<NewLine>            TensorImageUtils.TORCHVISION_NORM_STD_RGB<NewLine>        )<NewLine><NewLine>        val outputTensor = module.forward(IValue.from(inputTensor)).toTensor()<NewLine></code></pre><NewLine><p>the problem is inputTensor’s shape is [1, 3, 28, 28].  How can I create a single channel tensor from a bitmap?</p><NewLine><pre><code class=""lang-auto"">java.lang.RuntimeException: Given groups=1, weight of size 8 1 3 3, expected input[1, 3, 28, 28] to have 1 channels, but got 3 channels instead<NewLine>    The above operation failed in interpreter.<NewLine></code></pre><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/3b180dbc4992c1bacfaf971fb98b82008f08932b"" href=""https://discuss.pytorch.org/uploads/default/original/3X/3/b/3b180dbc4992c1bacfaf971fb98b82008f08932b.png"" title=""image""><img alt=""image"" data-base62-sha1=""8qLHJQ00lJIjytV4wc1b9vIle8b"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/3/b/3b180dbc4992c1bacfaf971fb98b82008f08932b_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/3/b/3b180dbc4992c1bacfaf971fb98b82008f08932b_2_257x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/3/b/3b180dbc4992c1bacfaf971fb98b82008f08932b_2_257x500.png, https://discuss.pytorch.org/uploads/default/optimized/3X/3/b/3b180dbc4992c1bacfaf971fb98b82008f08932b_2_385x750.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/3/b/3b180dbc4992c1bacfaf971fb98b82008f08932b.png 2x"" width=""257""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">423×820 14.1 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/cansozbir,(Can Sözbir),cansozbir,"April 8, 2020, 10:50am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am also facing a similar issue of using 1 channel images in the PyTorch Android. Did you find any way out?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, you can try this way.</p><NewLine><p>Make sure that, x = torch.mean(x,dim=1,keepdim=True), is the first statement in the forward method of pytorch model.</p><NewLine><p>Then,</p><NewLine><p>example = torch.rand(1, 3, 28, 28)<br/><NewLine>trace = torch.jit.trace(net, example)<br/><NewLine>trace.save(“mymodel.pt”)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/NAVEEN_PALURU; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/NAVEEN_PALURU; <NewLine> ,"REPLY_DATE 1: September 27, 2020,  6:44am; <NewLine> REPLY_DATE 2: September 28, 2020,  8:16am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
70937,How can I enable GPU support for torch mobile on android?,2020-02-25T03:22:58.752Z,0,370,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! Is there any way to use GPU delegate (like options.gpuDelegate for tflite models) for torch mobile on android to speed up inference time? I’ve found CPU only options both for QNNPACK and FBGEMM models . Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/ZackPashkin,(Zack Pashkin),ZackPashkin,"February 25, 2020,  3:22am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote"" data-post=""2"" data-topic=""66641""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v4/letter/s/db5fbb/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/network-inference-on-gpu-under-android/66641/2"">Network inference on GPU under Android</a> <a class=""badge-wrapper bullet"" href=""/c/mobile""><span class=""badge-category-bg"" style=""background-color: #92278F;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""This category is dedicated for iOS and Android issues, new features and general discussion of PyTorch Mobile."">Mobile</span></a><NewLine></div><NewLine><blockquote><NewLine>    This is currently not supported, but we are actively looking into supporting inference on GPU on mobile devices. <NewLine>Stay tuned for future releases!<NewLine>  </blockquote><NewLine></aside><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is there any update on this? We would really love to go forward using pytorch mobile, but this is a blocker.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Tassilo; <NewLine> ,"REPLY_DATE 1: September 14, 2020,  3:03pm; <NewLine> REPLY_DATE 2: September 22, 2020, 11:36am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
96197,Mobile deployment best practice?,2020-09-14T07:58:30.211Z,1,57,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, my goal is to deploy CNNs (or Conv RNNs) on Android devices with Snapdragon processors. ex) Pixel 4 (snapdragon 855)<br/><NewLine>I want to make my models run as fast as possible with GPU or DSP.</p><NewLine><p>Currently, I find 2 ways to deploy PyTorch models.</p><NewLine><ol><NewLine><li>PyTorch mobile</li><NewLine></ol><NewLine><p>This is a natively supported method of PyTorch and I like it.<br/><NewLine>But mobile GPU execution is not supported yet.<br/><NewLine>I see several PRs related to vulkan APIs <a href=""https://github.com/pytorch/pytorch/pulls?q=is%3Apr+is%3Aopen+vulkan"" rel=""nofollow noopener"">here</a><br/><NewLine>Will mobile GPU/DSP be supported in near future?</p><NewLine><p>I found this <a href=""https://developer.qualcomm.com/blog/snapdragon-supports-pytorch-10-ai-research-and-production-same-framework"" rel=""nofollow noopener"">article</a> from Qualcomm that they will support PyTorch but I don’t find any follow-up.</p><NewLine><ol start=""2""><NewLine><li>PyTorch -&gt; onnx</li><NewLine></ol><NewLine><p>Pytorch models could be exported to onnx format.<br/><NewLine>From onnx I can convert the models into DLC format with SNPE SDK.<br/><NewLine>But it will be more difficult to create an app with such forms.</p><NewLine><p>Are there any other ways I didn’t find? What would be the best way to deploy PyTorch models on mobile devices?<br/><NewLine>I used TensorFlow lite before but would prefer to use PyTorch if possible as I mainly use PyTorch.</p><NewLine></div>",https://discuss.pytorch.org/u/seungjun,,seungjun,"September 14, 2020,  7:58am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t know when Pytorch Mobile will support GPU execution. I looking forward to it <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>Meanwhile, I run on CPU. I follow <a href=""https://pytorch.org/tutorials/recipes/mobile_perf.html"" rel=""nofollow noopener"">this recipe</a> to squeeze the maximum performance as possible. I found it that Pytorch is faster than TF Lite and as fast as TF  Lite when using XNNPACK experimental backend.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for sharing your experience!<br/><NewLine>I typically work on image regression models that consume quite heavy computation.</p><NewLine><p>While waiting for the official mobile GPU support, I guess your suggestion is the most practical way at the time being.</p><NewLine><p>By the way, I found that GPU &amp; NNAPI acceleration might be possible if we use onnx runtime.<br/><NewLine><aside class=""quote"" data-post=""4"" data-topic=""66641""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v4/letter/s/90ced4/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/network-inference-on-gpu-under-android/66641/4"">Network inference on GPU under Android</a> <a class=""badge-wrapper bullet"" href=""/c/mobile/18""><span class=""badge-category-bg"" style=""background-color: #92278F;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""This category is dedicated for iOS and Android issues, new features and general discussion of PyTorch Mobile."">Mobile</span></a><NewLine></div><NewLine><blockquote><NewLine>    I recently read about ONNX runtime and it might be an alternative. <NewLine>PyTorch could export models to ONNX format and run the model with ONNX runtime. <NewLine><a class=""onebox"" href=""https://pytorch.org/docs/stable/onnx.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/stable/onnx.html</a> <NewLine>According to the <a href=""https://github.com/microsoft/onnxruntime/blob/master/docs/execution_providers/NNAPI-ExecutionProvider.md"" rel=""nofollow noopener"">onnxruntime</a>, NNAPI is supported including CPU and GPU inference. <NewLine>I didn’t try it but looks like it is the only Android gpu-enabled accelerator for now.<NewLine>  </blockquote><NewLine></aside><NewLine></p><NewLine><p>I will try both ways <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vferrer; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/seungjun; <NewLine> ,"REPLY_DATE 1: September 17, 2020, 11:21am; <NewLine> REPLY_DATE 2: September 17, 2020, 11:46am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
66641,Network inference on GPU under Android,2020-01-14T14:05:24.005Z,1,228,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Does PyTorch support such type of inference?</p><NewLine></div>",https://discuss.pytorch.org/u/Chame_call,(chame_call),Chame_call,"January 14, 2020,  5:34pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is currently not supported, but we are actively looking into supporting inference on GPU on mobile devices.<br/><NewLine>Stay tuned for future releases!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Any updates on this?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I recently read about ONNX runtime and it might be an alternative.</p><NewLine><p>PyTorch could export models to ONNX format and run the model with ONNX runtime.<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/docs/stable/onnx.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/stable/onnx.html</a></p><NewLine><p>According to the <a href=""https://github.com/microsoft/onnxruntime/blob/master/docs/execution_providers/NNAPI-ExecutionProvider.md"" rel=""nofollow noopener"">onnxruntime</a>, NNAPI is supported including CPU and GPU inference.</p><NewLine><p>I didn’t try it but looks like it is the only Android gpu-enabled accelerator for now.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Samuel_Menaker; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/seungjun; <NewLine> ,"REPLY_DATE 1: January 24, 2020,  9:55pm; <NewLine> REPLY_DATE 2: September 15, 2020,  6:18pm; <NewLine> REPLY_DATE 3: September 17, 2020,  7:48am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
96579,Android : Pytorch,2020-09-17T05:59:21.634Z,0,36,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Can someone suggest how to use android PyTorch for 1 channel images?</p><NewLine></div>",https://discuss.pytorch.org/u/NAVEEN_PALURU,(Naveen Paluru),NAVEEN_PALURU,"September 17, 2020,  5:59am",,,,,
96324,"How to indexing IValue in java? (my case : tuple of [1,496,640,2])",2020-09-15T07:46:02.417Z,0,21,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I have java.torch.Size([1, 496, 640, 2]) &lt;- outputs</p><NewLine><p><code>IValue[] outputs = model.forward(inputs).toTuple()</code></p><NewLine><p>which is IValue</p><NewLine><p>I want to get [1,:,:,1] and [1,:,:,2] by java command…<br/><NewLine>how can I get this? also…what is the correspondence of the np.array in java?<br/><NewLine>I want to get [1,:,:,1] and [1,:,:,2]  as float.array in java.</p><NewLine><p>why pytorch has very few examples??</p><NewLine></div>",https://discuss.pytorch.org/u/SungmanHong,(Sungman Hong),SungmanHong,"September 15, 2020,  7:50am",,,,,
96322,"Expected IValue type 10, actual type 2",2020-09-15T07:36:05.522Z,0,18,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have error in java.<br/><NewLine><code>Expected IValue type 10, actual type 2</code></p><NewLine><p>actually… what is the type10 and type2?<br/><NewLine>how can i find any doc?</p><NewLine></div>",https://discuss.pytorch.org/u/SungmanHong,(Sungman Hong),SungmanHong,"September 15, 2020,  7:36am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found type2 is tuple and 10 is double list, but don’t know any doc is exists</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/SungmanHong; <NewLine> ,"REPLY_DATE 1: September 15, 2020,  7:39am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
96174,Tuple outputs = model.forward(inputs).toTuple();,2020-09-14T02:59:31.824Z,0,31,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi I have Tuple output in pytorch mobile.</p><NewLine><p>But the type of output has no type of tuple</p><NewLine><p>in the code,</p><NewLine><p>x outputs = model.forward(inputs).toTuple();</p><NewLine><p>what should I type in the x?</p><NewLine></div>",https://discuss.pytorch.org/u/SungmanHong,(Sungman Hong),SungmanHong,"September 14, 2020,  2:59am",,,,,
96092,Perform reshape of tensor in JAVA.pytorch,2020-09-13T07:12:36.788Z,0,29,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I have python code<br/><NewLine>and converting the code to android, and I have question of some commands of pythons.</p><NewLine><p>First, my python code is <code>unsqueeze(x.0)</code> where x is pytorch.tensor.<br/><NewLine>e.g) when unsqueeze(x,0) is performed, shape of x will be [3,2,4] to [1,3,2,4] &lt;-- add just 1 dimension</p><NewLine><ol><NewLine><li><NewLine><p>in JAVA, can I do it how…?? (java command is so unusual for me… please help)</p><NewLine></li><NewLine><li><NewLine><p>Also, I want to reorder the pytorch.tensor in JAVA<br/><NewLine>e.g) [3,2,4] to [2,4,3]<br/><NewLine>in python it is performed by <code>torch.from_numpy(x).permute(2, 0, 1)</code><br/><NewLine>In JAVA, how to can I do it? (JAVA command is fine to me… I found some java function code or package that support reshape in python where the input is array… in pytorch.java… what is the type of tensor? it is array?)</p><NewLine></li><NewLine></ol><NewLine><p>thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/SungmanHong,(Sungman Hong),SungmanHong,"September 13, 2020,  7:13am",,,,,
94098,RuntimeError: Mobile optimized model cannot be inferenced on GPU,2020-08-25T20:37:57.086Z,2,73,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have used <code>mobile_optimizer.optimize_for_mobile()</code> function to optimize my model and I wanted to benchmark it with other optimization techniques for faster inference but it gives me runtime error when I try to run it on GPU</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-15-c705c21babec&gt; in &lt;module&gt;<NewLine>      1 optimized_traced_model = mobile_optimizer.optimize_for_mobile(traced_model)<NewLine>      2 <NewLine>----&gt; 3 get_ipython().run_line_magic('timeit', 'with torch.no_grad(): optimized_traced_model(example1)')<NewLine><NewLine>~/anaconda3/envs/e2r/lib/python3.7/site-packages/IPython/core/interactiveshell.py in run_line_magic(self, magic_name, line, _stack_depth)<NewLine>   2324                 kwargs['local_ns'] = sys._getframe(stack_depth).f_locals<NewLine>   2325             with self.builtin_trap:<NewLine>-&gt; 2326                 result = fn(*args, **kwargs)<NewLine>   2327             return result<NewLine>   2328 <NewLine><NewLine>&lt;decorator-gen-60&gt; in timeit(self, line, cell, local_ns)<NewLine><NewLine>~/anaconda3/envs/e2r/lib/python3.7/site-packages/IPython/core/magic.py in &lt;lambda&gt;(f, *a, **k)<NewLine>    185     # but it's overkill for just that one bit of state.<NewLine>    186     def magic_deco(arg):<NewLine>--&gt; 187         call = lambda f, *a, **k: f(*a, **k)<NewLine>    188 <NewLine>    189         if callable(arg):<NewLine><NewLine>~/anaconda3/envs/e2r/lib/python3.7/site-packages/IPython/core/magics/execution.py in timeit(self, line, cell, local_ns)<NewLine>   1161             for index in range(0, 10):<NewLine>   1162                 number = 10 ** index<NewLine>-&gt; 1163                 time_number = timer.timeit(number)<NewLine>   1164                 if time_number &gt;= 0.2:<NewLine>   1165                     break<NewLine><NewLine>~/anaconda3/envs/e2r/lib/python3.7/site-packages/IPython/core/magics/execution.py in timeit(self, number)<NewLine>    167         gc.disable()<NewLine>    168         try:<NewLine>--&gt; 169             timing = self.inner(it, self.timer)<NewLine>    170         finally:<NewLine>    171             if gcold:<NewLine><NewLine>&lt;magic-timeit&gt; in inner(_it, _timer)<NewLine><NewLine>~/anaconda3/envs/e2r/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)<NewLine>    720             result = self._slow_forward(*input, **kwargs)<NewLine>    721         else:<NewLine>--&gt; 722             result = self.forward(*input, **kwargs)<NewLine>    723         for hook in itertools.chain(<NewLine>    724                 _global_forward_hooks.values(),<NewLine><NewLine>RuntimeError: The following operation failed in the TorchScript interpreter.<NewLine>Traceback of TorchScript (most recent call last):<NewLine>    graph(%input, %weight, %bias, %stride:int[], %padding:int[], %dilation:int[], %groups:int):<NewLine>        %output_min_max : None = prim::Constant()<NewLine>        %packed_weight_bias = prepacked::conv2d_clamp_prepack(<NewLine>                              ~~~~~~~~~ &lt;--- HERE<NewLine>            %weight, %bias, %stride, %padding, %dilation, %groups,<NewLine>            %output_min_max, %output_min_max)<NewLine>RuntimeError: Could not run 'prepacked::conv2d_clamp_prepack' with arguments from the 'CUDA' backend. 'prepacked::conv2d_clamp_prepack' is only available for these backends: [CPU].<NewLine></code></pre><NewLine><p>Also, when I run it on CPU, the runtime of the optimized model is more than an order magnitude <strong>slower</strong> then the un-optimized model.</p><NewLine></div>",https://discuss.pytorch.org/u/zshn25,(Zeeshan Khan Suri),zshn25,"August 25, 2020,  8:38pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t know which optimizations are applied in <code>optimizer_for_mobile</code>, but would assume that you would see a speedup on a mobile platform, i.e. not necessarily for an x86 architecture.<br/><NewLine>Did you deploy the model and profiled it on a mobile device?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, I am specifically looking for mobile devices with a GPU like NVIDIA Jetson.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>For inference use cases on the Jetson platform, you could check out the <a href=""https://github.com/dusty-nv/jetson-inference"">jetson-inference</a> repository, which provides some utilities and examples.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/zshn25; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: August 27, 2020,  7:16am; <NewLine> REPLY_DATE 2: August 28, 2020, 10:10am; <NewLine> REPLY_DATE 3: August 28, 2020, 11:44pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
73841,Pytorch not compatible with React-native library,2020-03-20T05:27:44.705Z,1,481,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve been using react native for one of my module.Recently we started using Pytorch . Pytorch along with react native is crashing.<br/><NewLine>Below is the error</p><NewLine><p>java.lang.UnsatisfiedLinkError: couldn’t find DSO to load: libpytorch_jni.so caused by: dlopen failed: cannot locate symbol “_ZN8facebook3jni10JByteOrder11nativeOrderEv” referenced by “/data/app/*****-4ZGDZJcLItKdhIsz3hj2rQ==/lib/arm64/libpytorch_jni.so”.</p><NewLine><p>Please help me.I am in great urgency.</p><NewLine></div>",https://discuss.pytorch.org/u/Arun_Kumar1,(Arun Kumar),Arun_Kumar1,"March 20, 2020,  5:27am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, <a class=""mention"" href=""/u/arun_kumar1"">@Arun_Kumar1</a></p><NewLine><p>Could you please share your build setup: gradle files, CMakeLists.txt ( if you have it)?</p><NewLine><p>Both react native and pytorch android use fbjni library, while “_ZN8facebook3jni10JByteOrder11nativeOrderEv” was added for pytorch android, it looks like system loads first react native libfbjni.so that does not have JByteOrder::nativeOrder method which is needed for libpytorch_jni.so</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ivankobzarev"">@IvanKobzarev</a>,</p><NewLine><p>Please check the below build.gradle file content.I don’t have CMakeLists.txt.</p><NewLine><p>apply plugin: “com.android.application”</p><NewLine><p>import com.android.build.OutputFile</p><NewLine><p>/**</p><NewLine><ul><NewLine><li>The react.gradle file registers a task for each build variant (e.g. bundleDebugJsAndAssets</li><NewLine><li>and bundleReleaseJsAndAssets).</li><NewLine><li>These basically call <code>react-native bundle</code> with the correct arguments during the Android build</li><NewLine><li>cycle. By default, bundleDebugJsAndAssets is skipped, as in debug/dev mode we prefer to load the</li><NewLine><li>bundle directly from the development server. Below you can see all the possible configurations</li><NewLine><li>and their defaults. If you decide to add a configuration block, make sure to add it before the</li><NewLine><li><NewLine><code>apply from: ""../../node_modules/react-native/react.gradle""</code> line.</li><NewLine><li><NewLine></li><NewLine><li>project.ext.react = [</li><NewLine><li>// the name of the generated asset file containing your JS bundle</li><NewLine><li>bundleAssetName: “index.android.bundle”,</li><NewLine><li><NewLine></li><NewLine><li>// the entry file for bundle generation</li><NewLine><li>entryFile: “index.android.js”,</li><NewLine><li><NewLine></li><NewLine><li>// whether to bundle JS and assets in debug mode</li><NewLine><li>bundleInDebug: false,</li><NewLine><li><NewLine></li><NewLine><li>// whether to bundle JS and assets in release mode</li><NewLine><li>bundleInRelease: true,</li><NewLine><li><NewLine></li><NewLine><li>// whether to bundle JS and assets in another build variant (if configured).</li><NewLine><li>// See <a href=""http://tools.android.com/tech-docs/new-build-system/user-guide#TOC-Build-Variants"" rel=""nofollow noopener"">http://tools.android.com/tech-docs/new-build-system/user-guide#TOC-Build-Variants</a><NewLine></li><NewLine><li>// The configuration property can be in the following formats</li><NewLine><li>//         ‘bundleIn${productFlavor}${buildType}’</li><NewLine><li>//         ‘bundleIn${buildType}’</li><NewLine><li>// bundleInFreeDebug: true,</li><NewLine><li>// bundleInPaidRelease: true,</li><NewLine><li>// bundleInBeta: true,</li><NewLine><li><NewLine></li><NewLine><li>// whether to disable dev mode in custom build variants (by default only disabled in release)</li><NewLine><li>// for example: to disable dev mode in the staging build type (if configured)</li><NewLine><li>devDisabledInStaging: true,</li><NewLine><li>// The configuration property can be in the following formats</li><NewLine><li>//         ‘devDisabledIn${productFlavor}${buildType}’</li><NewLine><li>//         ‘devDisabledIn${buildType}’</li><NewLine><li><NewLine></li><NewLine><li>// the root of your project, i.e. where “package.json” lives</li><NewLine><li>root: “…/…/”,</li><NewLine><li><NewLine></li><NewLine><li>// where to put the JS bundle asset in debug mode</li><NewLine><li>jsBundleDirDebug: “$buildDir/intermediates/assets/debug”,</li><NewLine><li><NewLine></li><NewLine><li>// where to put the JS bundle asset in release mode</li><NewLine><li>jsBundleDirRelease: “$buildDir/intermediates/assets/release”,</li><NewLine><li><NewLine></li><NewLine><li>// where to put drawable resources / React Native assets, e.g. the ones you use via</li><NewLine><li>// require(’./image.png’)), in debug mode</li><NewLine><li>resourcesDirDebug: “$buildDir/intermediates/res/merged/debug”,</li><NewLine><li><NewLine></li><NewLine><li>// where to put drawable resources / React Native assets, e.g. the ones you use via</li><NewLine><li>// require(’./image.png’)), in release mode</li><NewLine><li>resourcesDirRelease: “$buildDir/intermediates/res/merged/release”,</li><NewLine><li><NewLine></li><NewLine><li>// by default the gradle tasks are skipped if none of the JS files or assets change; this means</li><NewLine><li>// that we don’t look at files in android/ or ios/ to determine whether the tasks are up to</li><NewLine><li>// date; if you have any other folders that you want to ignore for performance reasons (gradle</li><NewLine><li>// indexes the entire tree), add them here. Alternatively, if you have JS files in android/</li><NewLine><li>// for example, you might want to remove it from here.</li><NewLine><li>inputExcludes: [“android/<strong>"", ""ios/</strong>”],</li><NewLine><li><NewLine></li><NewLine><li>// override which node gets called and with what additional arguments</li><NewLine><li>nodeExecutableAndArgs: [“node”],</li><NewLine><li><NewLine></li><NewLine><li>// supply additional arguments to the packager</li><NewLine><li>extraPackagerArgs: []</li><NewLine><li>]<br/><NewLine>*/</li><NewLine></ul><NewLine><p>project.ext.react = [<br/><NewLine>entryFile: “index.js”,<br/><NewLine>enableHermes: true,  // HERE!<br/><NewLine>]</p><NewLine><p>apply from: “…/…/node_modules/react-native/react.gradle”</p><NewLine><p>/**</p><NewLine><ul><NewLine><li>Set this to true to create two separate APKs instead of one:</li><NewLine><li><NewLine><ul><NewLine><li>An APK that only works on ARM devices</li><NewLine></ul><NewLine></li><NewLine><li><NewLine><ul><NewLine><li>An APK that only works on x86 devices</li><NewLine></ul><NewLine></li><NewLine><li>The advantage is the size of the APK is reduced by about 4MB.</li><NewLine><li>Upload all the APKs to the Play Store and people will download</li><NewLine><li>the correct one based on the CPU architecture of their device.<br/><NewLine>*/<br/><NewLine>def enableSeparateBuildPerCPUArchitecture = false</li><NewLine></ul><NewLine><p>/**</p><NewLine><ul><NewLine><li>Run Proguard to shrink the Java bytecode in release builds.<br/><NewLine>*/<br/><NewLine>def enableProguardInReleaseBuilds = false</li><NewLine></ul><NewLine><p>android {<br/><NewLine>compileSdkVersion rootProject.ext.compileSdkVersion<br/><NewLine>buildToolsVersion ‘28.0.3’</p><NewLine><pre><code>defaultConfig {<NewLine>    applicationId ""com.progressreport""<NewLine>    minSdkVersion rootProject.ext.minSdkVersion<NewLine>    targetSdkVersion rootProject.ext.targetSdkVersion<NewLine>    versionCode 1<NewLine>    versionName ""1.0""<NewLine>    ndk {<NewLine>        abiFilters 'x86','armeabi-v7a','arm64-v8a','x86_64'<NewLine>    }<NewLine>}<NewLine><NewLine>packagingOptions {<NewLine>    pickFirst ""**/libfbjni.so""<NewLine><NewLine>}<NewLine>splits {<NewLine><NewLine>    // Configures multiple APKs based on screen density.<NewLine>    splits {<NewLine>        abi {<NewLine>            enable true<NewLine>            reset()<NewLine>            include 'armeabi-v7a'<NewLine>            universalApk false<NewLine>        }<NewLine>    }<NewLine>}<NewLine>buildTypes {<NewLine>    release {<NewLine>        minifyEnabled enableProguardInReleaseBuilds<NewLine>        proguardFiles getDefaultProguardFile(""proguard-android.txt""), ""proguard-rules.pro""<NewLine>    }<NewLine>}<NewLine>compileOptions {<NewLine>    sourceCompatibility 1.8<NewLine>    targetCompatibility 1.8<NewLine>}<NewLine>packagingOptions {<NewLine>    pickFirst '/armeabi-v7a/libc++_shared.so'<NewLine>    pickFirst '/x86/libc++_shared.so'<NewLine>    pickFirst '/arm64-v8a/libc++_shared.so'<NewLine>    pickFirst '/x86_64/libc++_shared.so'<NewLine>    pickFirst '/x86/libjsc.so'<NewLine>    pickFirst '/armeabi-v7a/libjsc.so'<NewLine>    pickFirst '/arm64-v8a/libpytorch_jni.so'<NewLine>}<NewLine>// applicationVariants are e.g. debug, release<NewLine>applicationVariants.all { variant -&gt;<NewLine>    variant.outputs.each { output -&gt;<NewLine>        // For each separate APK per architecture, set a unique version code as described here:<NewLine>        // http://tools.android.com/tech-docs/new-build-system/user-guide/apk-splits<NewLine>        def versionCodes = [ ""x86"":1,""x86_64"":2,""armeabi-v7a"":3,""arm64-v8a"":2]<NewLine>        def abi = output.getFilter(OutputFile.ABI)<NewLine>        if (abi != null) {  // null for the universal-debug, universal-release variants<NewLine>            output.versionCodeOverride =<NewLine>                    versionCodes.get(abi) * 1048576 + defaultConfig.versionCode<NewLine>        }<NewLine>    }<NewLine>}<NewLine></code></pre><NewLine><p>}</p><NewLine><p>dependencies {<br/><NewLine>implementation project(’:react-native-svg’)<br/><NewLine>implementation project(’:lottie-react-native’)<br/><NewLine>implementation project(’:react-native-localization’)<br/><NewLine>implementation project(’:react-native-linear-gradient’)<br/><NewLine>implementation project(’:react-native-vector-icons’)<br/><NewLine>implementation project(’:react-native-gesture-handler’){}<br/><NewLine>implementation fileTree(dir: “libs”, include: [""*.jar""])</p><NewLine><pre><code>implementation ""com.android.support:appcompat-v7:${rootProject.ext.supportLibVersion}""<NewLine>implementation (""com.facebook.react:react-native:+"" ){<NewLine>    exclude  module: 'fbjni-java-only'<NewLine>} // From node_modules<NewLine>def jscFlavor = 'org.webkit:android-jsc:+'<NewLine>def enableHermes = project.ext.react.get(""enableHermes"", false);<NewLine>if (enableHermes) {<NewLine>    // for RN 0.61+<NewLine>    def hermesPath = ""$rootDir/../node_modules/hermes-engine/android/"";<NewLine>    debugImplementation files(hermesPath + ""hermes-debug.aar"")<NewLine>    releaseImplementation files(hermesPath + ""hermes-release.aar"")<NewLine>} else {<NewLine>    implementation jscFlavor<NewLine>}<NewLine>def useIntlJsc = false<NewLine>if (useIntlJsc) {<NewLine>    implementation ('org.webkit:android-jsc-intl:+')<NewLine>} else {<NewLine>    implementation ('org.webkit:android-jsc:+')<NewLine>            {<NewLine>                exclude group:'com.facebook.fbjni',module:  ""fbjni-java-only-0.0.3""<NewLine>            } // From node_modules/ From node_modules<NewLine><NewLine>}<NewLine>implementation(name:'pytorch_android-1.5.0-SNAPSHOT', ext:'aar')<NewLine>implementation(name:'pytorch_android_fbjni-1.5.0-SNAPSHOT', ext:'aar')<NewLine>implementation 'com.facebook.soloader:nativeloader:0.8.0'<NewLine><NewLine><NewLine>implementation ""androidx.multidex:multidex:2.0.1""<NewLine></code></pre><NewLine><p>}</p><NewLine><p>// Run this once to be able to run the application with BUCK<br/><NewLine>// puts all compile dependencies into folder libs for BUCK to use<br/><NewLine>task copyDownloadableDepsToLibs(type: Copy) {<br/><NewLine>from configurations.compile<br/><NewLine>into ‘libs’<br/><NewLine>}</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/arun_kumar1"">@Arun_Kumar1</a>,<br/><NewLine>I have reproduced your problem. We have contacted react-native team to align our libfbjni.so’s. It might take some time and updating to react-native master.</p><NewLine><p>If you desperately need to unblock yourself you could try hacky way to use libfbjni.so from pytorch-android in react-native.</p><NewLine><ol><NewLine><li>Downloading nightly build of pytorch android that contains libfbjni.so inside,</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">wget -O pa.aar https://oss.sonatype.org/service/local/repositories/snapshots/content/org/pytorch/pytorch_android/1.5.0-SNAPSHOT/pytorch_android-1.5.0-20200324.090131-79.aar<NewLine><NewLine># We will need absolute path to downloaded file<NewLine>PA_AAR_ABSOLUTE_PATH=$(pwd)/pa.aar<NewLine></code></pre><NewLine><ol start=""2""><NewLine><li>Edit <code>node_modules/react-native/ReactAndroid/build.gradle</code>  (<a href=""https://github.com/facebook/react-native/blob/master/ReactAndroid/build.gradle#L368"" rel=""nofollow noopener"">https://github.com/facebook/react-native/blob/master/ReactAndroid/build.gradle#L368</a>) changing <code>task extractJNIFiles</code> which is at the moment is used only for fbjni</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">copy {<NewLine>    from zipTree(file)<NewLine>    into ""$projectDir/src/main/jni/first-party/$packageName/""<NewLine>    include ""jni/**/*""<NewLine>}<NewLine></code></pre><NewLine><p>to</p><NewLine><pre><code class=""lang-auto"">copy {<NewLine>    from zipTree(new File(""${PA_AAR_ABSOLUTE_PATH}""))<NewLine>    into ""$projectDir/src/main/jni/first-party/$packageName/""<NewLine>    include ""jni/**/libfbjni.so""<NewLine>}<NewLine></code></pre><NewLine><p>where new File(…) argument is absolute path to pa.aar that was downloaded on step 1.<br/><NewLine>It worked for me.<br/><NewLine>(The main idea of this hack - to use libfbjni.so from pytorch-android)</p><NewLine><p>I will reply here when react-native and pytorch-android libfbjni will be aligned.<br/><NewLine>Sorry for inconvenience.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/ivankobzarev"">@IvanKobzarev</a> for quick followup.<br/><NewLine>Will follow the same steps as mentioned to unblock ourselves for now.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, is there an update on the .so alignment between RN &amp; PyTorch? Or an issue to follow?<br/><NewLine>I wanted to use PyTorch from RN but I’m blocked and don’t really want to force a fbjni that may cause issue in production for RN.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>After some more investigation, using the release 1.5.0 android, I was able to get the app to compile but not run the PyTorch. So I got almost the same issue but with a slightly different error message:</p><NewLine><pre><code class=""lang-auto"">E/SoLoader: couldn't find DSO to load: libpytorch_jni.so caused by: dlopen failed: <NewLine>cannot locate symbol ""_ZN8facebook3jni11JByteBuffer5orderENS0_9alias_refINS0_10JByteOrderEEE"" <NewLine>referenced by ""/data/app/com.offline-EWIa3xVhdQYckXATRsmgrw==/lib/arm64/libpytorch_jni.so""... result: 0<NewLine></code></pre><NewLine><p>The symbol of <code>_ZN8facebook3jni11JByteBuffer5orderENS0_9alias_refINS0_10JByteOrderEEE</code> differ from <code>_ZN8facebook3jni10JByteOrder11nativeOrderEv</code> of the original question.</p><NewLine><p>When trying the proposed solution (which is to get the fbjni from PyTorch rather than RN one, it produced the same error</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/hugogresse"">@HugoGresse</a> Please upgrade RN to latest version.If you are using custom build Pytorch library.<br/><NewLine>Please do the following.</p><NewLine><p>implementation(“com.facebook.react:react-native:0.62.2”){<br/><NewLine>exclude  module: ‘fbjni-java-only’<br/><NewLine>}</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/arun_kumar1"">@Arun_Kumar1</a><br/><NewLine>I make an upgrading to RN 0.62.2, but I still have the same error as <a class=""mention"" href=""/u/hugogresse"">@HugoGresse</a> mentioned.<br/><NewLine>Do you have another way to fix this bug ?<br/><NewLine>Sorry for my bad English!</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Guys, importing pytorch like that seems to work for me:</p><NewLine><pre><code class=""lang-auto"">    implementation ('org.pytorch:pytorch_android:1.5.0'){<NewLine>     exclude module: 'fbjni-java-only'<NewLine>}<NewLine>    implementation ('org.pytorch:pytorch_android_torchvision:1.5.0'){<NewLine>     exclude module: 'fbjni-java-only'<NewLine>}<NewLine>    implementation ('org.pytorch:pytorch_android_fbjni:1.5.0'){<NewLine>     exclude module: 'fbjni-java-only'<NewLine>}<NewLine>    implementation('com.facebook.react:react-native:+'){<NewLine>     exclude module: 'fbjni-java-only'<NewLine>}<NewLine><NewLine></code></pre><NewLine><p>I also have</p><NewLine><pre><code class=""lang-auto"">android {<NewLine>        packagingOptions {<NewLine>	pickFirst '**/*.so'<NewLine>}<NewLine>}<NewLine></code></pre><NewLine><p>so that it won’t go crazy over those multiple <code>libfbjni.so</code> I’m getting for some reason. Hope that helps!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Arun_Kumar1; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Arun_Kumar1; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/HugoGresse; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/HugoGresse; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Arun_Kumar1; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/duclh3; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Sxela; <NewLine> ,"REPLY_DATE 1: March 22, 2020,  3:26pm; <NewLine> REPLY_DATE 2: March 23, 2020,  5:08am; <NewLine> REPLY_DATE 3: March 24, 2020, 10:36pm; <NewLine> REPLY_DATE 4: March 25, 2020,  5:55am; <NewLine> REPLY_DATE 5: August 10, 2020,  8:55am; <NewLine> REPLY_DATE 6: August 18, 2020,  2:10pm; <NewLine> REPLY_DATE 7: August 18, 2020,  2:32pm; <NewLine> REPLY_DATE 8: August 23, 2020,  9:08am; <NewLine> REPLY_DATE 9: August 28, 2020, 11:57am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> 
88892,Android build cannot find pytorch_android:addHeadersTobundleReleaseAar,2020-07-12T17:09:14.360Z,0,147,"<div class=""post"" itemprop=""articleBody""><NewLine><p>hi all，</p><NewLine><p>I am trying to build pytorch android from source by following this website: <a href=""https://pytorch.org/mobile/android/#building-pytorch-android-from-source"" rel=""nofollow noopener"">https://pytorch.org/mobile/android/#building-pytorch-android-from-source</a></p><NewLine><p>I runned it on mac with zsh.</p><NewLine><p>gradle: v5.6.4</p><NewLine><p>jdk 8</p><NewLine><p>witht newest ndk and sdk</p><NewLine><pre><code class=""lang-auto"">git clone --recursive https://github.com/pytorch/pytorch<NewLine>cd pytorch<NewLine>git submodule sync<NewLine>git submodule update --init --recursive<NewLine><NewLine>cd pytorch<NewLine>sh ./scripts/build_pytorch_android.sh<NewLine></code></pre><NewLine><p>at the end I recived the error:</p><NewLine><blockquote><NewLine><p>Task :pytorch_android:bundleReleaseAar FAILED</p><NewLine><p>FAILURE: Build failed with an exception.</p><NewLine><ul><NewLine><li>Where:</li><NewLine></ul><NewLine><p>Build file ‘/Users/huanghenglin/pytorch/android/pytorch_android/build.gradle’ line: 115</p><NewLine><ul><NewLine><li>What went wrong:</li><NewLine></ul><NewLine><p>Execution failed for task ‘:pytorch_android:bundleReleaseAar’.</p><NewLine><p>Could not find method execute() for arguments [] on task ‘:pytorch_android:addHeadersTobundleReleaseAar’ of type org.gradle.api.DefaultTask.</p><NewLine></blockquote><NewLine><p>what should I do to finish the compilar?</p><NewLine><p>note:</p><NewLine><p>I did changed the build_android.sh, at line 58 from</p><NewLine><blockquote><NewLine><p>CMAKE_ARGS+=(""-DCMAKE_PREFIX_PATH=$(python -c ‘from distutils.sysconfig import get_python_lib; print(get_python_lib())’)"")<br/><NewLine>CMAKE_ARGS+=(""-DPYTHON_EXECUTABLE=$(python -c ‘import sys; print(sys.executable)’)"")</p><NewLine></blockquote><NewLine><p>to</p><NewLine><blockquote><NewLine><p>CMAKE_ARGS+=(""-DCMAKE_PREFIX_PATH=$(python3 -c ‘from distutils.sysconfig import get_python_lib; print(get_python_lib())’)"")<br/><NewLine>CMAKE_ARGS+=(""-DPYTHON_EXECUTABLE=$(python3 -c ‘import sys; print(sys.executable)’)"")</p><NewLine></blockquote><NewLine><p>since the system cannot find my python3, but I did not see it is a problem.</p><NewLine></div>",https://discuss.pytorch.org/u/henry_huang,(henry huang),henry_huang,"July 12, 2020,  6:29pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I used the pyenv to change the environment, it seems not a python issue</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I resolved it by using gradle version 4.10.2</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/henry_huang; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/111380; <NewLine> ,"REPLY_DATE 1: July 14, 2020,  3:19pm; <NewLine> REPLY_DATE 2: September 17, 2020,  1:03pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
94190,XNNPACK engine not utilized,2020-08-26T13:34:51.407Z,0,57,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>If I understand correctly, XNNPACK has become the default computation engine for Pytorch mobile after the 1.6 release. I’ve built Pytorch mobile from source according to the following tutorial: <a href=""https://pytorch.org/mobile/android/#building-pytorch-android-from-source"" rel=""nofollow noopener"">https://pytorch.org/mobile/android/#building-pytorch-android-from-source</a> with -DUSE_XNNPACK=ON, but XNNPACK engine is still not being used (verified by putting some print statements in aten/src/ATen/native/xnnpack/Convolution.cpp). Any help on how to resolve this issue would be appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/JustasZ,,JustasZ,"August 26, 2020,  1:34pm",,,,,
93947,Is reverse engineered Pytorch model a concern?,2020-08-24T17:05:45.420Z,0,46,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi ,</p><NewLine><p>One of the concern on found Android is that Pytorch model (.pt) file can be reverse engineered.<br/><NewLine>Is the files extracted from .pt file is a concern? How can we secure if it is so?</p><NewLine><p>Please let me know on an urgent note</p><NewLine><p>Thanks,<br/><NewLine>Arun</p><NewLine></div>",https://discuss.pytorch.org/u/Arun_Kumar1,(Arun Kumar),Arun_Kumar1,"August 24, 2020,  5:05pm",,,,,
93883,LibTorch in iOS message filtering extension,2020-08-24T08:56:47.219Z,0,49,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to build an iOS message filtering extension, and I am using LibTorch 1.6, however after integrating with the LibTorch library, I find that my message extension no longer works: it compiles, runs, can be installed and enabled on my device, but the filtering just stops working (i.e. all messages are no longer filtered).</p><NewLine><p>Debugging this is really difficult, because there are no error messages. I wonder if anyone has run into similar issues before, or if anyone has successfully run LibTorch in this setup. I can confirm that my main iOS App runs perfectly, it’s just the extension (.apex) that doesn’t.</p><NewLine></div>",https://discuss.pytorch.org/u/jiayu,,jiayu,"August 24, 2020,  8:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>After digging through the log an device console I found the cause: it is hitting memory limit: on my device it is only 6MB.</p><NewLine><p>Guess that renders LibTorch hardly usable now…</p><NewLine><p>Hope this is helpful to somebody else.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jiayu; <NewLine> ,"REPLY_DATE 1: August 24, 2020, 12:02pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
93179,"Abort message: &lsquo;terminating with uncaught exception of type c10::Error: _ivalue_ INTERNAL ASSERT FAILED at ../torch/csrc/jit/api/object.cpp:19, please report a bug to PyTorch. (_ivalue at ../torch/csrc/jit/api/object.cpp:19)",2020-08-18T07:21:07.882Z,3,90,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When i’m loading a pt file in c++:</p><NewLine><pre><code class=""lang-auto"">Abort message: 'terminating with uncaught exception of type c10::Error: _ivalue_ INTERNAL ASSERT FAILED at ../torch/csrc/jit/api/object.cpp:19, please report a bug to PyTorch.  (_ivalue at ../torch/csrc/jit/api/object.cpp:19)<NewLine></code></pre><NewLine><p>NDK version：20.0.5594570</p><NewLine><p>Someone can help，thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/KevinZ1992,(XiaoJian Zhang),KevinZ1992,"August 18, 2020,  7:23am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you explain your use case and, if possible, create a code snippet to reproduce this issue?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just write a new jni to load my serialized model on android,  a code snippet just like</p><NewLine><pre><code class=""lang-auto"">            std::istringstream pt(s);<NewLine>            module_ = torch::jit::load(pt);<NewLine>            module_.eval();<NewLine></code></pre><NewLine><p>This error doesn’t appear every time，and it just occurs on Pixel 2 so far.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you please create a <a href=""https://github.com/pytorch/pytorch/issues"">GitHub issue</a>, so that it’ll be tracked?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>A new  Github issue <a href=""https://github.com/pytorch/pytorch/issues/43388"" rel=""nofollow noopener"">#43388</a> just be created.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/KevinZ1992; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/KevinZ1992; <NewLine> ,"REPLY_DATE 1: August 20, 2020,  9:05am; <NewLine> REPLY_DATE 2: August 20, 2020, 10:22am; <NewLine> REPLY_DATE 3: August 20, 2020,  6:32pm; <NewLine> REPLY_DATE 4: August 21, 2020,  2:56am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
93450,PyTorch version compatibility with Libtorch iOS,2020-08-20T06:43:46.030Z,0,47,"<div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/xta0"">@xta0</a> Just wanted to ask if there is any guidance for compatibility between torchscript generated from a specific version of PyTorch with the corresponding LibTorch version on iOS.</p><NewLine><p>Should the PyTorch and LibTorch iOS version always match? (ex. PyTorch 1.5 generated torchscript should only be used on LibTorch 1.5)</p><NewLine><p>Or should LibTorch work on any torchscript generated from a base version of PyTorch (say torchscript generated from PyTorch 1.5 should work for LibTorch 1.5 or greater unless otherwise specified)?</p><NewLine><p>Thanks for any help!</p><NewLine></div>",https://discuss.pytorch.org/u/mark_jimenez,,mark_jimenez,"August 20, 2020,  6:43am",,,,,
93193,"Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2",2020-08-18T08:51:02.910Z,1,441,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Following guide at <a href=""https://pytorch.org/mobile/android/"" rel=""nofollow noopener"">https://pytorch.org/mobile/android/</a>.<br/><NewLine>Try to generate model with script provided</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torchvision<NewLine><NewLine>model = torchvision.models.resnet18(pretrained=True)<NewLine>model.eval()<NewLine>example = torch.rand(1, 3, 224, 224)<NewLine>traced_script_module = torch.jit.trace(model, example)<NewLine>traced_script_module.save(""app/src/main/assets/model.pt"")<NewLine></code></pre><NewLine><p>after that<br/><NewLine>build android app from <a href=""https://github.com/pytorch/android-demo-app/tree/master/HelloWorldApp"" rel=""nofollow noopener"">https://github.com/pytorch/android-demo-app/tree/master/HelloWorldApp</a><br/><NewLine>and got error <strong>""Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2""</strong></p><NewLine><p>my python package version</p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; import torch<NewLine>&gt;&gt;&gt; import torchvision<NewLine>&gt;&gt;&gt; print(torch.__version__)<NewLine>1.6.0<NewLine>&gt;&gt;&gt; print(torchvision.__version__)<NewLine>0.7.0<NewLine>&gt;&gt;&gt;<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/billysutomo,(Billy Sutomo),billysutomo,"August 18, 2020,  8:51am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Use torch.save( …, _use_new_zipfile_serialization=False) for 1.6.0 when load pt file lower than 1.6.0</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/kevinz1992"">@KevinZ1992</a>  do you have the example for my code?</p><NewLine><p>I try with</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torchvision<NewLine><NewLine>model = torchvision.models.resnet18(pretrained=True)<NewLine>model.eval()<NewLine>example = torch.rand(1, 3, 224, 224)<NewLine>traced_script_module = torch.jit.trace(model, example)<NewLine># traced_script_module.save(""app/src/main/assets/model.pt"")<NewLine><NewLine>torch.save(model, ""app/src/main/assets/model.pt"", _use_new_zipfile_serialization=False)<NewLine></code></pre><NewLine><p>But still got the same error when try to load the model into android torch</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""93193"" data-username=""billysutomo""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/billysutomo/40/27909_2.png"" width=""20""/> billysutomo:</div><NewLine><blockquote><NewLine><p><code>traced_script_module.save</code></p><NewLine></blockquote><NewLine></aside><NewLine><p>Maybe you can change the version of org.pytorch:pytorch_android from 1.4.0 to 1.6.0 in HelloWorldApp/app/build.gradle file.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I have encountered the same problem and the solution is to downgrade your torch version to 1.5.1 and torchvision to 0.6.0 using below command:<br/><NewLine>conda install pytorch==1.5.1 torchvision==0.6.1 cudatoolkit=10.2 -c pytorch</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/KevinZ1992; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/billysutomo; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/KevinZ1992; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/sachinsharma9780; <NewLine> ,"REPLY_DATE 1: August 18, 2020,  9:47am; <NewLine> REPLY_DATE 2: August 18, 2020, 10:32am; <NewLine> REPLY_DATE 3: August 19, 2020,  8:51am; <NewLine> REPLY_DATE 4: September 27, 2020, 10:55pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
67629,Converting Tensor_float32 to Bitmap,2020-01-25T13:50:49.895Z,0,212,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I can transform Bitmap to Tensor_float32 with method TensorImageUtils.bitmapToFloat32Tensor() but how can I get a bitmap from the Tensor_float32? Found that solution but it is only for grayscale images: <a href=""https://itnext.io/converting-pytorch-float-tensor-to-android-rgba-bitmap-with-kotlin-ffd4602a16b6"" rel=""nofollow noopener"">https://itnext.io/converting-pytorch-float-tensor-to-android-rgba-bitmap-with-kotlin-ffd4602a16b6</a></p><NewLine></div>",https://discuss.pytorch.org/u/2kan,(Сергей Гончаров),2kan,"January 25, 2020,  1:50pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I have the same problem, did you find a solution?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/sepiiii; <NewLine> ,"REPLY_DATE 1: August 17, 2020,  9:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
92485,Pix2pix model tracing,2020-08-11T21:56:10.779Z,0,64,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,<br/><NewLine>I am trying to deploy the pix2pix model in an android app, I need to trace the model with torch.jit.script() but it doesnt seem to work.<br/><NewLine>Here is my code:</p><NewLine><pre><code class=""lang-auto"">    model = create_model(opt)     <NewLine>    model.setup(opt)             <NewLine>    model.eval()<NewLine>    a = torch.jit.script(model)<NewLine>    torch.jit.save(a,'optmodel.pt')<NewLine></code></pre><NewLine><p>and the full stacktrace is as follows:</p><NewLine><pre><code class=""lang-auto"">  File ""toScript.py"", line 58, in &lt;module&gt;<NewLine>    a = torch.jit.script(model)<NewLine>  File ""/local-scratch/Anaconda3/envs/pix2pix/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1263, in script<NewLine>    qualified_name = _qualified_name(obj)<NewLine>  File ""/local-scratch/Anaconda3/envs/pix2pix/lib/python3.7/site-packages/torch/_jit_internal.py"", line 760, in _qualified_name<NewLine>    name = obj.__name__<NewLine>AttributeError: 'Pix2PixModel' object has no attribute '__name__'<NewLine></code></pre><NewLine><p>What’s wrong?<br/><NewLine>Any help would really be really appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/sepiiii,,sepiiii,"August 11, 2020,  9:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found the solution, the generator should only be scripted.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/sepiiii; <NewLine> ,"REPLY_DATE 1: August 17, 2020,  9:44pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
89694,"Examples for object detection,pose estimation,segmentation",2020-07-18T14:04:01.683Z,4,163,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Recently I used image classification in mobile and it works good.Will there be documents to deploy object detection,pose estimation, segmentation in mobile as in tensorflow in future?</p><NewLine></div>",https://discuss.pytorch.org/u/zeus,(pyzeus),zeus,"July 18, 2020,  2:04pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I was able to get a segmentation model work by using the classification tutorial from the pytorch homepage. I used <code>&lt;tensor&gt;.getDataAsFloatArray()</code> to get the output values.</p><NewLine><p>In any case, currently the amount of documentation content for pytorch-mobile is ridiculously low. Like zero tutorials…</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah. But I saw in discussion saying that they are looking into it. Maybe in near future we can except those.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>did you get it out as a single dimensional array and have to put it back into it proper structure?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I only tried image classification. But wanted to do for other tasks also. If torch has these tutorials then I can fully move into torch</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have been using a mobilenet-ssd successfully.<br/><NewLine>I use the below function to restructure the array. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><pre><code class=""lang-auto"">    public float[][] floatArray1dto2d(final float[] array, final int rows, final int cols) {<NewLine>        if (array.length != (rows * cols))<NewLine>            throw new IllegalArgumentException(""Invalid array length"");<NewLine><NewLine>        float[][] outArray = new float[rows][cols];<NewLine>        for (int i = 0; i &lt; rows; i++)<NewLine>            System.arraycopy(array, (i * cols), outArray[i], 0, cols);<NewLine><NewLine>        return outArray;<NewLine>    }<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/redney_quartz"">@Redney_Quartz</a> what’s the size of your model? and on which platform you was able to use the model (ios/android) ?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Ludwig_Friborg; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/zeus; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Redney_Quartz; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/zeus; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Redney_Quartz; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/kshitijpatil09; <NewLine> ,"REPLY_DATE 1: July 30, 2020, 11:30am; <NewLine> REPLY_DATE 2: July 31, 2020,  3:07pm; <NewLine> REPLY_DATE 3: August 10, 2020,  5:42am; <NewLine> REPLY_DATE 4: August 11, 2020,  5:48pm; <NewLine> REPLY_DATE 5: August 15, 2020,  3:34am; <NewLine> REPLY_DATE 6: August 16, 2020, 10:42am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> 
92643,Protecting Pytoch .pt model from decompilation in mobile app,2020-08-13T07:21:47.871Z,0,57,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I wonder if there is a good practice available on how to protect trained Pytorch models compiled to <code>.pt</code> files and deployed to an Android app from being readable to anyone decompiling the app?<br/><NewLine>Seems to be an important IP-protection issue.</p><NewLine><p>I am writing an app that needs to be very performant and thus I move parts of the app logic into a C++ library that I include as an external package to my Android project.<br/><NewLine>Would it be theoretically possible to include my Pytorch models directly as a dependency to the C++ library and include it together with the C++ library as a compiled artifact in the Android project?</p><NewLine><p>Anyone has any experience with something like this?</p><NewLine></div>",https://discuss.pytorch.org/u/mxf,(Max),mxf,"August 13, 2020,  7:21am",,,,,
90105,Linking error when calling Pytorch c++ API from my c++ project for Android,2020-07-22T07:37:49.097Z,0,132,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I’m following this example: <a href=""https://github.com/ljk53/pytorch-android-cpp-demo"" rel=""nofollow noopener"">pytorch-android-cpp-demo</a><br/><NewLine>and I successfully compiled libtorch for Android but I’m having problems linking the static libs to my c++ project.<br/><NewLine>Here are the details:</p><NewLine><h1>Steps</h1><NewLine><ol><NewLine><li>Compiling libtorch for Android on  <strong>Linux</strong>  as explained in the example using:</li><NewLine></ol><NewLine><ul><NewLine><li>Ubuntu 20.04</li><NewLine><li>NDK 21 (for Linux 64b)</li><NewLine><li>Cmake 3.16.3<br/><NewLine>The resulting files are  <strong>static (.a)</strong>  files.</li><NewLine></ul><NewLine><ol start=""2""><NewLine><li>Linking torch .a files to an Android c++ project on  <strong>Windows</strong>  using:</li><NewLine></ol><NewLine><ul><NewLine><li>NDK 21 (for Windows)</li><NewLine><li>Android .mk files (I can’t use Cmake)<NewLine><ul><NewLine><li>Compiled for armeabi-v7a arch. (32b)</li><NewLine><li>Used  <code>APP_STL := c++_static</code><NewLine></li><NewLine><li>Used prebuilt and LOCAL_STATIC_LIBRARIES to link the torch libraries:</li><NewLine></ul><NewLine></li><NewLine></ul><NewLine><pre><code class=""lang-auto"">LOCAL_STATIC_LIBRARIES += c10-prebuilt<NewLine>LOCAL_STATIC_LIBRARIES += torch-prebuilt<NewLine>LOCAL_STATIC_LIBRARIES += torch_cpu-prebuilt<NewLine><NewLine>include $(CLEAR_VARS)<NewLine>LOCAL_MODULE := c10-prebuilt<NewLine>LOCAL_SRC_FILES := C:/REPOSITORIES/pytorch-android-cpp-demo/pytorch/build_android/install/lib/libc10.a<NewLine>include $(PREBUILT_STATIC_LIBRARY)<NewLine><NewLine>include $(CLEAR_VARS)<NewLine>LOCAL_MODULE := torch-prebuilt<NewLine>LOCAL_SRC_FILES := C:/REPOSITORIES/pytorch-android-cpp-demo/pytorch/build_android/install/lib/libtorch.a<NewLine>include $(PREBUILT_STATIC_LIBRARY)<NewLine><NewLine>include $(CLEAR_VARS)<NewLine>LOCAL_MODULE := torch_cpu-prebuilt<NewLine>LOCAL_SRC_FILES := C:/REPOSITORIES/pytorch-android-cpp-demo/pytorch/build_android/install/lib/libtorch_cpu.a<NewLine>include $(PREBUILT_STATIC_LIBRARY)<NewLine></code></pre><NewLine><h1>Error Message</h1><NewLine><p>This is the error message I’m getting when compiling my project:</p><NewLine><p>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/core/ivalue.cpp:430: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::Device const&amp;)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/core/ivalue.cpp:489: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::Device const&amp;)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/build_android/aten/src/ATen/Functions.cpp:145: error: undefined reference to ‘c10::toString(c10::DispatchKey)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/build_android/aten/src/ATen/Functions.cpp:165: error: undefined reference to ‘c10::toString(c10::DispatchKey)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/core/TensorOptions.h:252: error: undefined reference to ‘c10::get_default_dtype()’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/core/TensorOptions.h:252: error: undefined reference to ‘c10::get_default_dtype()’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/build_android/aten/src/ATen/Functions.cpp:289: error: undefined reference to ‘c10::toString(c10::DispatchKey)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/build_android/aten/src/ATen/Functions.cpp:827: error: undefined reference to ‘c10::toString(c10::DispatchKey)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/util/StringUtil.h:37: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::DispatchKey)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/util/StringUtil.h:37: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::DeviceType)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/core/impl/DeviceGuardImplInterface.h:0: error: undefined reference to ‘c10::impl::device_guard_impl_registry’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/util/StringUtil.h:37: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::DeviceType)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/torch/csrc/jit/ir/constants.cpp:110: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::Device const&amp;)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/torch/csrc/jit/ir/constants.cpp:177: error: undefined reference to ‘c10::Device::Device(std::__ndk1::basic_string&lt;char, std::__ndk1::char_traits, std::__ndk1::allocator &gt; const&amp;)’<br/><NewLine>/mnt/c/Android/linux/android-ndk-r21d/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include/c++/v1/memory:0: error: undefined reference to ‘c10::detail::deleteNothing(void*)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/util/intrusive_ptr.h:0: error: undefined reference to ‘c10::detail::deleteNothing(void*)’<br/><NewLine>/mnt/c/Android/linux/android-ndk-r21d/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include/c++/v1/new:0: error: undefined reference to ‘c10::detail::deleteNothing(void*)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/core/TensorOptions.h:252: error: undefined reference to ‘c10::get_default_dtype()’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/torch/csrc/jit/serialization/unpickler.cpp:399: error: undefined reference to ‘c10::Device::Device(std::__ndk1::basic_string&lt;char, std::__ndk1::char_traits, std::__ndk1::allocator &gt; const&amp;)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/torch/csrc/jit/serialization/unpickler.cpp:425: error: undefined reference to ‘c10::DeviceTypeName(c10::DeviceType, bool)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/torch/csrc/jit/serialization/unpickler.cpp:542: error: undefined reference to ‘c10::Device::Device(std::__ndk1::basic_string&lt;char, std::__ndk1::char_traits, std::__ndk1::allocator &gt; const&amp;)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/util/StringUtil.h:37: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::DispatchKey)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/util/StringUtil.h:37: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::DispatchKey)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/torch/csrc/jit/frontend/function_schema_parser.cpp:251: error: undefined reference to ‘c10::Device::Device(std::__ndk1::basic_string&lt;char, std::__ndk1::char_traits, std::__ndk1::allocator &gt; const&amp;)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/core/Generator.h:102: error: undefined reference to ‘c10::GeneratorImpl::device() const’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/core/Generator.h:102: error: undefined reference to ‘c10::GeneratorImpl::device() const’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/util/StringUtil.h:37: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::DeviceType)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/util/StringUtil.h:37: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::DeviceType)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/util/StringUtil.h:37: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::Device const&amp;)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/native/BinaryOps.cpp:173: error: undefined reference to ‘c10::get_default_dtype()’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/core/Generator.h:102: error: undefined reference to ‘c10::GeneratorImpl::device() const’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/core/Generator.h:102: error: undefined reference to ‘c10::GeneratorImpl::device() const’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/util/StringUtil.h:37: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::TensorOptions const&amp;)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/native/TensorShape.cpp:73: error: undefined reference to ‘c10::GetAllocator(c10::DeviceType const&amp;)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/native/TypeProperties.cpp:84: error: undefined reference to ‘c10::get_default_complex_dtype()’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/core/LegacyTypeDispatch.h:0: error: undefined reference to ‘c10::detail::deleteNothing(void*)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/native/xnnpack/Factory.cpp:17: error: undefined reference to ‘c10::GetDefaultMobileCPUAllocator()’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/native/xnnpack/Factory.cpp:41: error: undefined reference to ‘c10::GetDefaultMobileCPUAllocator()’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/TH/THAllocator.cpp:25: error: undefined reference to ‘c10::GetCPUAllocator()’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/TH/THAllocator.cpp:571: error: undefined reference to ‘c10::reportMemoryUsageToProfiler(void*, long long, c10::Device)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/quantized/Quantizer.cpp:75: error: undefined reference to ‘c10::GetDefaultMobileCPUAllocator()’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/build_android/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.DEFAULT.cpp:78: error: undefined reference to ‘c10::Scalar::operator-() const’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/caffe2/serialize/inline_container.cc:222: error: undefined reference to ‘c10::GetCPUAllocator()’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:18: error: undefined reference to ‘c10::detail::getNonDeterministicRandom(bool)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:43: error: undefined reference to ‘c10::GeneratorImpl::GeneratorImpl(c10::Device, c10::DispatchKeySet)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:73: error: undefined reference to ‘c10::detail::getNonDeterministicRandom(bool)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp:43: error: undefined reference to ‘c10::GeneratorImpl::GeneratorImpl(c10::Device, c10::DispatchKeySet)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/PTThreadPool.h:13: error: undefined reference to ‘c10::ThreadPool::ThreadPool(int, int, std::__ndk1::function&lt;void ()&gt;)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/ParallelThreadPoolNative.cpp:26: error: undefined reference to ‘c10::ThreadPoolRegistry()’<br/><NewLine>/mnt/c/Android/linux/android-ndk-r21d/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include/c++/v1/memory:2133: error: undefined reference to ‘c10::ThreadPool::~ThreadPool()’<br/><NewLine>/mnt/c/Android/linux/android-ndk-r21d/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include/c++/v1/memory:2133: error: undefined reference to ‘c10::ThreadPool::~ThreadPool()’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/PTThreadPool.h:8: error: undefined reference to ‘c10::ThreadPool::~ThreadPool()’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/PTThreadPool.h:14: error: undefined reference to ‘c10::setThreadName(std::__ndk1::basic_string&lt;char, std::__ndk1::char_traits, std::__ndk1::allocator &gt;)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/ParallelThreadPoolNative.cpp:48: error: undefined reference to ‘c10::ThreadPoolRegistry()’<br/><NewLine>C:/REPOSITORIES/pytorch-android-cpp-demo/pytorch/build_android/install/lib/libtorch_cpu.a(ParallelThreadPoolNative.cpp.o):ParallelThreadPoolNative.cpp:vtable for at::PTThreadPool: error: undefined reference to ‘c10::ThreadPool::run(std::__ndk1::function&lt;void ()&gt;)’<br/><NewLine>C:/REPOSITORIES/pytorch-android-cpp-demo/pytorch/build_android/install/lib/libtorch_cpu.a(ParallelThreadPoolNative.cpp.o):ParallelThreadPoolNative.cpp:vtable for at::PTThreadPool: error: undefined reference to ‘c10::ThreadPool::size() const’<br/><NewLine>C:/REPOSITORIES/pytorch-android-cpp-demo/pytorch/build_android/install/lib/libtorch_cpu.a(ParallelThreadPoolNative.cpp.o):ParallelThreadPoolNative.cpp:vtable for at::PTThreadPool: error: undefined reference to ‘c10::ThreadPool::numAvailable() const’<br/><NewLine>C:/REPOSITORIES/pytorch-android-cpp-demo/pytorch/build_android/install/lib/libtorch_cpu.a(ParallelThreadPoolNative.cpp.o):ParallelThreadPoolNative.cpp:vtable for at::PTThreadPool: error: undefined reference to ‘c10::ThreadPool::inThreadPool() const’<br/><NewLine>C:/REPOSITORIES/pytorch-android-cpp-demo/pytorch/build_android/install/lib/libtorch_cpu.a(ParallelThreadPoolNative.cpp.o):ParallelThreadPoolNative.cpp:vtable for at::PTThreadPool: error: undefined reference to ‘c10::ThreadPool::~ThreadPool()’<br/><NewLine>C:/REPOSITORIES/pytorch-android-cpp-demo/pytorch/build_android/install/lib/libtorch_cpu.a(ParallelThreadPoolNative.cpp.o):ParallelThreadPoolNative.cpp:typeinfo for at::PTThreadPool: error: undefined reference to ‘typeinfo for c10::ThreadPool’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/util/StringUtil.h:37: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::DispatchKeySet)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/ThreadLocalState.cpp:11: error: undefined reference to ‘c10::ThreadLocalDebugInfo::current()’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/ATen/ThreadLocalState.cpp:32: error: undefined reference to ‘c10::ThreadLocalDebugInfo::_forceCurrentDebugInfo(std::__ndk1::shared_ptrc10::ThreadLocalDebugInfo const&amp;)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/util/StringUtil.h:37: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::DispatchKey)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/TH/THGeneral.cpp:156: error: undefined reference to ‘c10::alloc_cpu(unsigned int)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/aten/src/TH/THGeneral.cpp:188: error: undefined reference to ‘c10::free_cpu(void*)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/torch/csrc/autograd/engine.cpp:0: error: undefined reference to ‘c10::impl::device_guard_impl_registry’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/torch/csrc/autograd/engine.cpp:621: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::TensorOptions const&amp;)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/torch/csrc/autograd/engine.cpp:621: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::TensorOptions const&amp;)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/torch/csrc/autograd/engine.cpp:0: error: undefined reference to ‘c10::impl::device_guard_impl_registry’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/core/impl/InlineEvent.h:60: error: undefined reference to ‘c10::DeviceTypeName(c10::DeviceType, bool)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/core/impl/InlineEvent.h:60: error: undefined reference to ‘c10::DeviceTypeName(c10::DeviceType, bool)’<br/><NewLine>/mnt/c/REPOSITORIES/pytorch-android-cpp-demo/pytorch/c10/core/impl/InlineEvent.h:76: error: undefined reference to ‘c10::DeviceTypeName(c10::DeviceType, bool)’<br/><NewLine>clang++: error: linker command failed with exit code 1 (use -v to see invocation)<br/><NewLine>make: *** [c:/Android/android-ndk-r21d/build//…/build/core/build-binary.mk:738: C:/REPOSITORIES/SVLTemp2/obj/local/armeabi-v7a/dczoom_native] Error 1</p><NewLine><p>Can you please help me figure out what is wrong?<br/><NewLine>Thank you</p><NewLine></div>",https://discuss.pytorch.org/u/Limba,,Limba,"July 22, 2020,  7:37am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi~  I have the same problem.  Have you solved it？Can you post the answer? thankyou</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/walktim; <NewLine> ,"REPLY_DATE 1: August 11, 2020,  1:12am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
61360,[Torchvision] [Android] Torchvision.ops when will they be available on Mobile,2019-11-18T13:34:29.509Z,1,443,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>Let me introduce a problem. Last month two significant features for mobile deployment was added to pytorch=1.4. I am talking about torchvision operations torchvision::nms and torchvision::roi_align that are now convertible from eager python code via torchscript to <code>.pt</code> model.</p><NewLine><p>The most recent builds of pytorch for mobile don’t contain this features. Is there any estimation when wiil tey be available to use on phones?</p><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"November 18, 2019,  1:34pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We’re looking into this.  Probably late this year or early next year.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you so much for the update. Would nms be planned to support on Android anytime soon?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, When nms will be supported in Android?</p><NewLine><p>Thanks</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Any further progress?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/noranart.v; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/AmrAhmed; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Redney_Quartz; <NewLine> ,"REPLY_DATE 1: November 20, 2019, 11:30pm; <NewLine> REPLY_DATE 2: April 16, 2020,  3:36am; <NewLine> REPLY_DATE 3: May 14, 2020,  2:28am; <NewLine> REPLY_DATE 4: August 6, 2020,  4:41am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
91215,Numpy-like tensor processing on Android,2020-07-31T10:36:24.502Z,2,111,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone. I’m deploying a deeplab model onto an android application. The output of the model is a [1,21, 400,400] tensor. In order to get my final result mask, I have to run an argmax operation on the 2nd dimension. In python I figure the operation would be something like this out = np.argmax(out, axis=1). Then I’ll have to create a mask based on the result (out[out==classId] = colorValue).</p><NewLine><p>But there is nothing quite like numpy on android. Looping through the whole thing to find all the maximum values for each pixel, and then set the color is going to be very slow. Is there any way I can do this efficiently. Any help would be greatly appreciated</p><NewLine></div>",https://discuss.pytorch.org/u/ryanaleksander,(Ryan Aleksander),ryanaleksander,"July 31, 2020, 10:36am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The easiest probably is to code this in TorchScript and call that.<br/><NewLine>You can also run libtorch proper on PyTorch and do your own JNI wrapper for whatever you need.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply. I’m not all that familiar with TorchScript and Android. What would the process be like? Would you mind recommending me a starting place?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>You write your numpy-like processing in PyTorch as a plain python function or better yet as a nn.Module,</li><NewLine><li>run it through torch.jit.script similar to the TorchScript tutorials,</li><NewLine><li>now you can use it similar to the Android tutorials.</li><NewLine></ol><NewLine><p>Using a nn.Module in step 1 isn’t better in the sense that it would be preferable per se, but it will reduce the amount of adaptation you need to do in steps 2 and 3.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ryanaleksander; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> ,"REPLY_DATE 1: July 31, 2020,  5:42pm; <NewLine> REPLY_DATE 2: August 1, 2020,  4:33am; <NewLine> REPLY_DATE 3: August 1, 2020,  9:31am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
91090,Are there any best practices for using torchscript with pytorch-mobile?,2020-07-30T11:38:15.651Z,0,87,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Are there any best practices for using torchscript with pytorch-mobile? Im currently trying to implement a conv-net with pytorch-mobile on android. The model is very slow in comparison to an identical tensorflow trained tflite model. Perhaps this might have something to do with the torchscript tracing.</p><NewLine><p>Any thoughts would be appreciated</p><NewLine></div>",https://discuss.pytorch.org/u/Ludwig_Friborg,(Ludwig Friborg),Ludwig_Friborg,"July 30, 2020, 11:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tries freeze the graph using <code>torch.utils.mobile_optimizer.optimize_for_mobile</code> which is new for pytorch 1.6.0. It seems to improve performance a little but not significantly.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Ludwig_Friborg; <NewLine> ,"REPLY_DATE 1: July 31, 2020, 11:56am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
86126,How to print out model weights in iOS Libtorch?,2020-06-19T23:06:44.748Z,12,313,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Seems like the stuff that people are doing in C++ doesn’t quite work for iOS. For example, I tried:</p><NewLine><pre><code class=""lang-auto"">for (const auto&amp; params : module.parameters()) {<NewLine>          std::cout &lt;&lt; params.values() &lt;&lt; std::endl;<NewLine>}<NewLine></code></pre><NewLine><p>which yields:</p><NewLine><p><code>values not implemented for TensorTypeSet(VariableTensorId, CPUTensorId) (values at /Users/distiller/project/build_ios/aten/src/ATen/core/TensorMethods.h:3534) (no backtrace available)</code></p><NewLine></div>",https://discuss.pytorch.org/u/Bryan_Wang,(Bryan Wang),Bryan_Wang,"June 19, 2020, 11:06pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Turns out params is a tensor so the following works:</p><NewLine><pre><code class=""lang-auto"">  for (const auto&amp; params : _impl.parameters()) {<NewLine>          params.print();<NewLine>  }<NewLine></code></pre><NewLine><p>However this only outputs the tensor shapes.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>can you try <code>params[i][j].item&lt;float&gt;()</code> or use the buffer pointer <code>float* buffer = params.data_ptr&lt;float&gt;()</code></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/xta0"">@xta0</a>, that works great. I am using your iOS sample project as a template, and have run into a big problem that I was wondering if you could help with (I can make another thread if needed).</p><NewLine><p>The traced model outputs very different results in Swift than it does in Python, and I don’t know why. I followed your steps in the sample project:</p><NewLine><p>First, trace the model and see the output on a tensor of ones:</p><NewLine><pre><code class=""lang-auto"">model = ExplicitSpeechResModel(36, 19)<NewLine>model.load('../output/model_1h.pt')<NewLine>model.eval()<NewLine>ones = torch.ones([1, 101, 40])<NewLine>traced_script_module = torch.jit.trace(model, ones)<NewLine>traced_script_module.save(""../output/traced_model.pt"")<NewLine>traced_script_module(ones)<NewLine><NewLine>//outputs:<NewLine>//tensor([[-18.7624, -30.3478, -31.0299,  -1.1888,   8.6857,  33.7217, -36.2783,<NewLine>// 51.8277,  55.9391,   9.0642,   8.6428,  10.3509,   0.2688,  43.9576,<NewLine>// -7.1114, -55.3318, -16.7983, -13.5788,  -3.9336,  -1.1792,  14.3855,<NewLine>//-31.8519, 101.3712, -43.9597,  40.5726, -16.2946, -15.8538,  21.1088,<NewLine>//-31.5852, -14.2146, -14.5817,  19.9373, -21.5292,   9.4006, -45.0686,<NewLine>// 21.4724]], grad_fn=&lt;DifferentiableGraphBackward&gt;)<NewLine></code></pre><NewLine><p>On the Swift side, here is my TorchModule.mm. As you will see, the output on the same tensor is completely different:</p><NewLine><pre><code class=""lang-auto"">@implementation TorchModule {<NewLine> @protected<NewLine>  torch::jit::script::Module _impl;<NewLine>}<NewLine><NewLine>- (nullable instancetype)initWithFileAtPath:(NSString*)filePath {<NewLine>  self = [super init];<NewLine>  if (self) {<NewLine>    try {<NewLine>      auto qengines = at::globalContext().supportedQEngines();<NewLine>      if (std::find(qengines.begin(), qengines.end(), at::QEngine::QNNPACK) != qengines.end()) {<NewLine>        at::globalContext().setQEngine(at::QEngine::QNNPACK);<NewLine>      }<NewLine>      _impl = torch::jit::load(filePath.UTF8String);<NewLine>      _impl.eval();<NewLine>    } catch (const std::exception&amp; exception) {<NewLine>      NSLog(@""%s"", exception.what());<NewLine>      return nil;<NewLine>    }<NewLine>  }<NewLine>  return self;<NewLine>}<NewLine><NewLine><NewLine>- (NSArray&lt;NSNumber*&gt;*)predictImage:(void*)imageBuffer {<NewLine>  try {<NewLine><NewLine>    // try out dummy input of all ones<NewLine>    at::Tensor tensor = torch::ones({1, 101, 40});<NewLine>    torch::autograd::AutoGradMode guard(false);<NewLine>    at::AutoNonVariableTypeMode non_var_type_mode(true);<NewLine>    auto outputTensor = _impl.forward({tensor}).toTensor();<NewLine>    float* floatBuffer = outputTensor.data_ptr&lt;float&gt;();<NewLine>    if (!floatBuffer) {<NewLine>      return nil;<NewLine>    }<NewLine>    NSMutableArray* results = [[NSMutableArray alloc] init];<NewLine>    for (int i = 0; i &lt; 36; i++) {<NewLine>      [results addObject:@(floatBuffer[i])];<NewLine>    }<NewLine>    return [results copy];<NewLine>  } catch (const std::exception&amp; exception) {<NewLine>    NSLog(@""%s"", exception.what());<NewLine>  }<NewLine>  return nil;<NewLine>}<NewLine>@end<NewLine></code></pre><NewLine><p>Calling predictImage results in the following vector:</p><NewLine><p><code>[11.918683, -21.391111, -18.756794, -15.70252, -14.593732, 28.798603, -22.37965, 10.117706, 5.1135015, -8.376111, -25.258512, -7.270096, 0.9224758, 3.4262152, 28.566887, 2.90841, 25.247177, 35.124638, 14.7190695, -37.291008, -4.821145, 33.09956, 47.47553, 11.395653, 9.54897, -5.713372, -32.897644, -18.26301, -5.596691, -18.339537, -25.02614, -23.303043, -3.3603168, 31.69397, 3.0528922, 7.3663263]</code></p><NewLine><p>It is totally different from the one in Python. The model weights seem to be the same, I’m calling model.eval() … not sure what else is missing. Here is my model architecture in case it is helpful:</p><NewLine><pre><code class=""lang-auto"">class ExplicitSpeechResModel(SerializableModule):<NewLine>    def __init__(self, n_labels, n_maps):<NewLine>        super().__init__()<NewLine>        self.conv0 = nn.Conv2d(1, n_maps, (3, 3), padding=(1, 1), bias=False)<NewLine>        self.conv1 = nn.Conv2d(n_maps, n_maps, (3, 3), padding=int(2**(0 // 3)), dilation=int(2**(0 // 3)), bias=False)<NewLine>        self.bn1 = nn.BatchNorm2d(n_maps, affine=False)<NewLine>        self.conv2 = nn.Conv2d(n_maps, n_maps, (3, 3), padding=int(2**(1 // 3)), dilation=int(2**(1 // 3)), bias=False)<NewLine>        self.bn2 = nn.BatchNorm2d(n_maps, affine=False)<NewLine>        self.conv3 = nn.Conv2d(n_maps, n_maps, (3, 3), padding=int(2**(2 // 3)), dilation=int(2**(2 // 3)), bias=False)<NewLine>        self.bn3 = nn.BatchNorm2d(n_maps, affine=False)<NewLine>        self.conv4 = nn.Conv2d(n_maps, n_maps, (3, 3), padding=int(2**(3 // 3)), dilation=int(2**(3 // 3)), bias=False)<NewLine>        self.bn4 = nn.BatchNorm2d(n_maps, affine=False)<NewLine>        self.conv5 = nn.Conv2d(n_maps, n_maps, (3, 3), padding=int(2**(4 // 3)), dilation=int(2**(4 // 3)), bias=False)<NewLine>        self.bn5 = nn.BatchNorm2d(n_maps, affine=False)<NewLine>        self.conv6 = nn.Conv2d(n_maps, n_maps, (3, 3), padding=int(2**(5 // 3)), dilation=int(2**(5 // 3)), bias=False)<NewLine>        self.bn6 = nn.BatchNorm2d(n_maps, affine=False)<NewLine>        self.conv7 = nn.Conv2d(n_maps, n_maps, (3, 3), padding=int(2**(6 // 3)), dilation=int(2**(6 // 3)), bias=False)<NewLine>        self.bn7 = nn.BatchNorm2d(n_maps, affine=False)<NewLine>        self.conv8 = nn.Conv2d(n_maps, n_maps, (3, 3), padding=int(2**(7 // 3)), dilation=int(2**(7 // 3)), bias=False)<NewLine>        self.bn8 = nn.BatchNorm2d(n_maps, affine=False)<NewLine>        self.conv9 = nn.Conv2d(n_maps, n_maps, (3, 3), padding=int(2**(8 // 3)), dilation=int(2**(8 // 3)), bias=False)<NewLine>        self.bn9 = nn.BatchNorm2d(n_maps, affine=False)<NewLine>        self.conv10 = nn.Conv2d(n_maps, n_maps, (3, 3), padding=int(2**(9 // 3)), dilation=int(2**(9 // 3)), bias=False)<NewLine>        self.bn10 = nn.BatchNorm2d(n_maps, affine=False)<NewLine>        self.conv11 = nn.Conv2d(n_maps, n_maps, (3, 3), padding=int(2**(10 // 3)), dilation=int(2**(10 // 3)), bias=False)<NewLine>        self.bn11 = nn.BatchNorm2d(n_maps, affine=False)<NewLine>        self.conv12 = nn.Conv2d(n_maps, n_maps, (3, 3), padding=int(2**(11 // 3)), dilation=int(2**(11 // 3)), bias=False)<NewLine>        self.bn12 = nn.BatchNorm2d(n_maps, affine=False)<NewLine>        self.conv13 = nn.Conv2d(n_maps, n_maps, (3, 3), padding=int(2**(12 // 3)), dilation=int(2**(12 // 3)), bias=False)<NewLine>        self.bn13 = nn.BatchNorm2d(n_maps, affine=False)<NewLine>        self.output = nn.Linear(n_maps, n_labels)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = x.unsqueeze(1)<NewLine>        y0 = F.relu(self.conv0(x))<NewLine>        x = self.bn1(F.relu(self.conv1(y0)))<NewLine>        y2 = F.relu(self.conv2(x)) + y0<NewLine>        x = self.bn2(y2)<NewLine>        x =  self.bn3(F.relu(self.conv3(x)))<NewLine>        y4 = F.relu(self.conv4(x)) + y2<NewLine>        x = self.bn4(y4)<NewLine>        x = self.bn5(F.relu(self.conv5(x)))<NewLine>        y6 = F.relu(self.conv6(x)) + y4<NewLine>        x = self.bn6(y6)<NewLine>        x =  self.bn7(F.relu(self.conv7(x)))<NewLine>        y8 = F.relu(self.conv8(x)) + y6<NewLine>        x = self.bn8(y8)<NewLine>        x = self.bn9(F.relu(self.conv9(x)))<NewLine>        y10 = F.relu(self.conv10(x)) + y8<NewLine>        x = self.bn10(y10)<NewLine>        x =  self.bn11(F.relu(self.conv11(x)))<NewLine>        y12 = F.relu(self.conv12(x)) + y10<NewLine>        x = self.bn12(y12)<NewLine>        x =  self.bn13(F.relu(self.conv13(x)))<NewLine>        x = x.view(x.size(0), x.size(1), -1) # shape: (batch, feats, o3)<NewLine>        x = torch.mean(x, 2)<NewLine>        return self.output(x)<NewLine></code></pre><NewLine><p>Sorry for the long post. I really appreciate your help so far, and the sample project you made as well.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>This seems to be related to <a href=""https://github.com/pytorch/pytorch/pull/39591"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/39591</a>. Can you manually apply the PR and recompile from the source? Please refer to the section here - <a href=""https://pytorch.org/mobile/ios/#build-pytorch-ios-libraries-from-source"" rel=""nofollow noopener"">https://pytorch.org/mobile/ios/#build-pytorch-ios-libraries-from-source</a></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks. I’ll let you know how it goes!</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m getting a bunch of errors like:</p><NewLine><pre><code class=""lang-auto"">Undefined symbols for architecture x86_64:<NewLine>  ""c10::IValue::toModule() const"", referenced from:<NewLine>      torch::jit::slot_iterator_impl&lt;torch::jit::detail::ModulePolicy&gt;::next() in TorchModule.o<NewLine>      torch::jit::slot_iterator_impl&lt;torch::jit::detail::ParameterPolicy&gt;::next() in TorchModule.o<NewLine>  ""c10::ivalue::Object::type() const"", referenced from:<NewLine>      torch::jit::slot_iterator_impl&lt;torch::jit::detail::ModulePolicy&gt;::valid() const in TorchModule.o<NewLine>      torch::jit::slot_iterator_impl&lt;torch::jit::detail::ModulePolicy&gt;::next() in TorchModule.o<NewLine>      torch::jit::slot_iterator_impl&lt;torch::jit::detail::ParameterPolicy&gt;::valid() const in TorchModule.o<NewLine>      torch::jit::slot_iterator_impl&lt;torch::jit::detail::ParameterPolicy&gt;::next() in TorchModule.o<NewLine></code></pre><NewLine><p>Seems like something went wrong with the initial build. I followed the posted instructions, and added a few steps.</p><NewLine><ol><NewLine><li>Pod install LibTorch 1.4.0 then replace the install folder in Pods/LibTorch with the install generated by the build.</li><NewLine><li>Change C++ compiler version in XCode to C++14.</li><NewLine></ol><NewLine><p>Do you know what the problem might be?</p><NewLine><p>If it is relevant, my cmake version is 3.12.3, and clang version is 11.0.3. I’m using Mac OS Catalina.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>you’re not supposed to use cocoapods to recompile. Can you try</p><NewLine><ol><NewLine><li>Clone pytorch and check out the master branch</li><NewLine><li>Manually apply the changes in the PR (only one liner)</li><NewLine><li>Follow the section <a href=""https://pytorch.org/mobile/ios/#build-pytorch-ios-libraries-from-source"" rel=""nofollow noopener"">https://pytorch.org/mobile/ios/#build-pytorch-ios-libraries-from-source</a> to recompile from source code and setup XCode</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I did as you suggested, and am still getting these errors. Seems like Xcode can’t find some header file.</p><NewLine><p><code>Undefined symbols for architecture x86_64:   ""torch::jit::Object::find_method(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) const"", referenced from:       torch::jit::Object::get_method(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) const in TorchModule.o   ""at::GradMode::is_enabled()"", referenced from:       at::AutoGradMode::AutoGradMode(bool) in TorchModule.o   ""at::GradMode::set_enabled(bool)"", referenced from:       at::AutoGradMode::AutoGradMode(bool) in TorchModule.o       at::AutoGradMode::~AutoGradMode() in TorchModule.o</code></p><NewLine><p>I tried replacing the <code>force_load</code> flag with <code>all_load</code> and <code>Objc</code> but it has the same result. My header search path is <code>$(PROJECT_DIR)/install/include</code>. If I toggle this to “recursive”, the above errors disappear and are replaced by errors like:</p><NewLine><p><code>No member named 'signbit' in the global namespace; did you mean 'sigwait'?</code></p><NewLine><p>Seems like the recursive setting makes it so that cmath is incorrectly loaded. I have been trying to fix this for a while, and I was wondering if you had any suggestions! Thanks for the help.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you add these two guards?</p><NewLine><pre><code class=""lang-auto"">at::AutoNonVariableTypeMode nonVarTypeModeGuard(true);<NewLine>torch::autograd::AutoGradMode guard(false);<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, I had those guards. The error is in compile time. Here is a minimal reproduction of the bug, if it helps.</p><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""16""/><NewLine><a href=""https://github.com/BryanWBear/PytorchDev"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars1.githubusercontent.com/u/7650109?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/BryanWBear/PytorchDev"" rel=""nofollow noopener"" target=""_blank"">BryanWBear/PytorchDev</a></h3><NewLine><p>repro a compiler bug. Contribute to BryanWBear/PytorchDev development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/bryan_wang"">@Bryan_Wang</a> I’m kinda busy recently, I’ll ask my colleague to followup with you</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/bryan_wang"">@Bryan_Wang</a>, as <a class=""mention"" href=""/u/xta0"">@xta0</a> mentioned, the wrong prediction issue should have been fixed via PR 39591. Now I’m looking into the problem you have now. Thanks for your patience.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please follow the tutorial <a href=""https://pytorch.org/mobile/ios/#build-pytorch-ios-libraries-from-source"" rel=""nofollow noopener"">https://pytorch.org/mobile/ios/#build-pytorch-ios-libraries-from-source</a> <strong>very carefully</strong> as <a class=""mention"" href=""/u/xta0"">@xta0</a> mentioned. Double check you’ve changed the value of <code>Header Search Paths</code>, <code>other linker flags</code>, and <code>bitcode </code> accordingly before you build it. When you drag <code>install</code> folder to the project, please make sure those three check boxes are checked as the screenshot shows, <strong>this is important</strong>.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/d94862bf4f5402db8a8fa650d061793150eae143"" href=""https://discuss.pytorch.org/uploads/default/original/3X/d/9/d94862bf4f5402db8a8fa650d061793150eae143.png"" title=""image""><img alt=""image"" data-base62-sha1=""v0aHcvuuP0sNvd3ca0a0FHp4CXh"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/9/d94862bf4f5402db8a8fa650d061793150eae143_2_10x10.png"" height=""406"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/9/d94862bf4f5402db8a8fa650d061793150eae143_2_690x406.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/9/d94862bf4f5402db8a8fa650d061793150eae143_2_690x406.png, https://discuss.pytorch.org/uploads/default/original/3X/d/9/d94862bf4f5402db8a8fa650d061793150eae143.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/d/9/d94862bf4f5402db8a8fa650d061793150eae143.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">730×430 56.5 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>If you dragged in successfully, the color of the folder icon of <code>install</code> in the side bar should be <strong>yellow</strong> instead of blue.</p><NewLine><p>I also attached a successfully built project which is based on the minimal project you attached (not sure if PR 39591 is included or not, just for showing you the project file structure and settings). Hopefully it will help. <a href=""https://www.dropbox.com/s/sh0plrh2oakbxcj/PytorchDev-Fixed.zip?dl=0"" rel=""nofollow noopener"">https://www.dropbox.com/s/sh0plrh2oakbxcj/PytorchDev-Fixed.zip?dl=0</a> The password is <code> pytorchios</code></p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you so much for your help. I have been traveling for the past two weeks so unable to respond.</p><NewLine><p>Would it be possible for you to reshare the link? I am getting “The owner hasn’t granted you access to this link” with no password prompt.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://www.dropbox.com/s/mqe3pyme2mnh8o6/PytorchDev-Fixed.zip?dl=0"" rel=""nofollow noopener"">https://www.dropbox.com/s/mqe3pyme2mnh8o6/PytorchDev-Fixed.zip?dl=0</a>   BTW, PyTorch 1.6 now has been released, so it is supposed to be working as well if you import PyTorch via Cocoapod. Thanks.</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ah, I’ll do that as well. Thank you.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Bryan_Wang; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Bryan_Wang; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Bryan_Wang; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Bryan_Wang; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Bryan_Wang; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Bryan_Wang; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/husthyc; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/husthyc; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/Bryan_Wang; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/husthyc; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/Bryan_Wang; <NewLine> ,"REPLY_DATE 1: June 19, 2020, 11:59pm; <NewLine> REPLY_DATE 2: June 20, 2020,  1:10am; <NewLine> REPLY_DATE 3: June 20, 2020,  2:51am; <NewLine> REPLY_DATE 4: June 20, 2020,  9:04pm; <NewLine> REPLY_DATE 5: June 20, 2020, 11:22pm; <NewLine> REPLY_DATE 6: June 21, 2020,  6:51pm; <NewLine> REPLY_DATE 7: June 22, 2020, 10:49pm; <NewLine> REPLY_DATE 8: July 1, 2020,  8:55pm; <NewLine> REPLY_DATE 9: July 2, 2020,  5:47am; <NewLine> REPLY_DATE 10: July 2, 2020,  6:27pm; <NewLine> REPLY_DATE 11: July 4, 2020,  7:30pm; <NewLine> REPLY_DATE 12: July 8, 2020,  9:29pm; <NewLine> REPLY_DATE 13: July 13, 2020,  9:14pm; <NewLine> REPLY_DATE 14: July 29, 2020,  4:44am; <NewLine> REPLY_DATE 15: July 29, 2020,  5:36am; <NewLine> REPLY_DATE 16: July 29, 2020,  5:47am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: 1 Like; <NewLine> REPLY 12 LIKES: 1 Like; <NewLine> REPLY 13 LIKES: 2 Likes; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: 1 Like; <NewLine> REPLY 16 LIKES: ; <NewLine> 
89543,Error about custom build. When runing quanted model on android,2020-07-17T06:42:12.089Z,0,60,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My quanted model “quanted.pt” runing well on android without custom build.<br/><NewLine>Use torch.jit.export_opnames() to get  SELECTED_OP_LIST,but there is no Relu op such as<br/><NewLine>“- aten::relu”.Try Relu6 meet the same question.</p><NewLine><p>torch version:1.5.0<br/><NewLine>Here is the RuntimeError:</p><NewLine><pre><code class=""lang-auto"">java.lang.RuntimeException: The following operation failed in the TorchScript interpreter.<NewLine>    Traceback of TorchScript, serialized code (most recent call last):<NewLine>      File ""code/__torch__/torch/nn/quantized/functional.py"", line 9, in forward<NewLine>        pass<NewLine>      if inplace:<NewLine>        _1 = torch.relu_(input)<NewLine>             ~~~~~~~~~~~ &lt;--- HERE<NewLine>      else:<NewLine>        _1 = torch.relu(input)<NewLine>   <NewLine>    Traceback of TorchScript, original code (most recent call last):<NewLine>      File ""###/python3.7/site-packages/torch/nn/quantized/functional.py"", line 322, in forward<NewLine>            raise ValueError(""Input to 'quantized.relu' must be quantized!"")<NewLine>        if inplace:<NewLine>            return torch.relu_(input)<NewLine>                   ~~~~~~~~~~~ &lt;--- HERE<NewLine>        else:<NewLine>            return torch.relu(input)<NewLine>    RuntimeError: Operator has been stripped in the custom build.<NewLine></code></pre><NewLine><p>Someone can help,thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/KevinZ1992,(XiaoJian Zhang),KevinZ1992,"July 17, 2020,  6:42am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>i changed the relu inplace=False，it will work</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/KevinZ1992; <NewLine> ,"REPLY_DATE 1: July 24, 2020,  2:36am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
82011,TorchScript Error Unknown builtin op,2020-05-19T15:45:47.604Z,1,253,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there,</p><NewLine><p>I am trying to use a torch script exported module traced on iOS device but I am getting this error:</p><NewLine><pre><code class=""lang-auto"">ibc++abi.dylib: terminating with uncaught exception of type torch::jit::ErrorReport: <NewLine>Unknown builtin op: aten::_batch_norm_impl_index_backward.<NewLine>Could not find any similar ops to aten::_batch_norm_impl_index_backward. This op may not exist or may not be currently supported in TorchScript.<NewLine>:<NewLine>  File ""&lt;string&gt;"", line 19<NewLine><NewLine>            def backward(grad_output):<NewLine>                dinput, dweight, dbias = torch._batch_norm_impl_index_backward(<NewLine>                                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>                    impl_idx, input, grad_output, weight, running_mean, running_var,<NewLine>                    save1, save2, training, eps, [True, has_weight, has_bias], reserve)<NewLine><NewLine></code></pre><NewLine><p>This only occurs when loading the traced script on mobile. Using a desktop platform runs ok.<br/><NewLine>I have read a related issue <a href=""https://github.com/pytorch/pytorch/pull/30067"" rel=""nofollow noopener"">pytorch repository issue</a> but it seems that it does not fix the problem in this case.<br/><NewLine>Could anyone help me with this?</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/DTSED,,DTSED,"May 19, 2020,  3:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/dtsed"">@DTSED</a> were you able to figure this out? I’m encountering the same issue, not sure what in my torchscript model is triggering it.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/xta0"">@xta0</a>, I can replicate the error on iOS using the code below. Not sure if this is a regression because I don’t see how <code>backward</code> is called anywhere.</p><NewLine><p>Python code:</p><NewLine><pre><code class=""lang-auto"">class Net(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Net, self).__init__()<NewLine>        self.fc1 = nn.Linear(784, 392)<NewLine>        self.fc2 = nn.Linear(392, 10)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.fc1(x)<NewLine>        x = nn.functional.relu(x)<NewLine>        x = self.fc2(x)<NewLine>        return x<NewLine><NewLine>n = Net()<NewLine>X = th.randn(3, 28 * 28)<NewLine><NewLine><NewLine>simple_net = th.jit.trace(n, X)<NewLine></code></pre><NewLine><p>iOS C++ Code</p><NewLine><pre><code class=""lang-auto"">auto qengines = at::globalContext().supportedQEngines();<NewLine>if (std::find(qengines.begin(), qengines.end(), at::QEngine::QNNPACK) != qengines.end()) {<NewLine>    at::globalContext().setQEngine(at::QEngine::QNNPACK);<NewLine>}<NewLine>_impl = torch::jit::load(filePath.UTF8String);<NewLine>_impl.eval();<NewLine><NewLine>std::vector&lt;torch::jit::IValue&gt; modelArgs;<NewLine><NewLine>at::Tensor tensor = torch::from_blob(data, trainingDataVectorShape, at::kFloat);<NewLine>modelArgs.push_back(tensor);<NewLine><NewLine>auto result = _impl.forward(modelArgs);<NewLine><NewLine></code></pre><NewLine><p>Resulting error:</p><NewLine><pre><code class=""lang-auto"">libc++abi.dylib: terminating with uncaught exception of type torch::jit::ErrorReport: <NewLine>Unknown builtin op: aten::_batch_norm_impl_index_backward.<NewLine>Could not find any similar ops to aten::_batch_norm_impl_index_backward. This op may not exist or may not be currently supported in TorchScript.<NewLine>:<NewLine>  File ""&lt;string&gt;"", line 19<NewLine><NewLine>            def backward(grad_output):<NewLine>                dinput, dweight, dbias = torch._batch_norm_impl_index_backward(<NewLine>                                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>                    impl_idx, input, grad_output, weight, running_mean, running_var,<NewLine>                    save1, save2, training, eps, [True, has_weight, has_bias], reserve)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/dtsed"">@DTSED</a> <a class=""mention"" href=""/u/mark_jimenez"">@mark_jimenez</a> can you guys try adding <code>torch::autograd::AutoGradMode guard(false);</code> before calling <code>torch.jit.load</code> and <code>forward</code>?</p><NewLine><pre><code class=""lang-auto"">    at::AutoNonVariableTypeMode nonVarTypeModeGuard(true);<NewLine>    torch::autograd::AutoGradMode guard(false);<NewLine>    auto model = torch::jit::load(path.UTF8String);<NewLine>    auto input = torch::randn({3,784});<NewLine>    auto output = model.forward({input});<NewLine>    std::cout&lt;&lt;output.toTensor().sizes()&lt;&lt;std::endl;<NewLine></code></pre><NewLine><p>Autograd features have been disabled on mobile, the current workaround is to use the RAII guard above. <code>torch::autograd::AutoGradMode guard(false);</code> is similar to <code>with torch.no_grad():</code> in python.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Works great, thank you again <span class=""mention"">@xtao</span> !</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>It works for me too, sorry for the delay responding.<br/><NewLine>Thanks</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mark_jimenez; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mark_jimenez; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mark_jimenez; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/DTSED; <NewLine> ,"REPLY_DATE 1: June 5, 2020, 10:31am; <NewLine> REPLY_DATE 2: June 6, 2020,  8:16am; <NewLine> REPLY_DATE 3: June 9, 2020,  5:46pm; <NewLine> REPLY_DATE 4: June 9, 2020,  7:54am; <NewLine> REPLY_DATE 5: July 13, 2020,  2:48pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> 
88354,How to use (torch.utils.data.DataLoader) in android?,2020-07-08T09:04:51.138Z,4,91,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Its testcode need （torch.utils.data.DataLoader）。 you can look PSENet Project .&gt; test_ic15.py 72 lines<br/><NewLine>I have torch==1.4.0 change PSENet.pth ==&gt; PSENet.pt and model load in Android is OK。But，<br/><NewLine>next I don’t know what to do.<br/><NewLine>I want a little alittle translation the PSENet testcode in Android 。<br/><NewLine>Sorry，my English is very poor, if you can, give me some android advice</p><NewLine></div>",https://discuss.pytorch.org/u/Micla-SHL,(Micla Shl),Micla-SHL,"July 8, 2020,  9:04am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://pytorch.org/mobile/android/"">This Android tutorial</a> should give you a quick overview of the basic functionality.<br/><NewLine>I’m currently unsure where you are stuck, so the tutorial(s) might be helpful.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi, I have looked the android document when I asked。 What I want to say is ： I trained a text detection model and I wanted it to work on android. I’ve successfully loaded it on android, but the follow-up work (read pictures, output test results) I don’t have a clue. The original code used the (torch.utils.data.dataLoader) interface, so how I used it in android (torch.utils.data.data.DataLoader), I wanted to do nothing more than turn the test code step by step to android runable</p><NewLine><p>Above from Bing Translation</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You wouldn’t need to use a <code>DataLoader</code>, but could stick to the <a href=""https://pytorch.org/mobile/android/#6-preparing-input"">example</a> of loading a single sample.<br/><NewLine>I’m not sure, if the <code>DataLoader</code> with the complete functionality is ported to the mobile API (and if not, if it’s planned to do so).</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>OK，thanks.  I will try something</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Micla-SHL; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Micla-SHL; <NewLine> ,"REPLY_DATE 1: July 9, 2020,  9:07am; <NewLine> REPLY_DATE 2: July 10, 2020,  5:56am; <NewLine> REPLY_DATE 3: July 10, 2020,  8:19am; <NewLine> REPLY_DATE 4: July 13, 2020,  7:55am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
88657,AttributeError: &lsquo;builtin_function_or_method&rsquo; object has no attribute &lsquo;dim&rsquo;,2020-07-10T09:18:05.204Z,2,241,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The following error report occurred when I used the LSTM network to train the IMDB dataset for affective dichotomy</p><NewLine><p>AttributeError                            Traceback (most recent call last)<br/><NewLine> in <br/><NewLine>4 for epoch in range(N_EPOCHS):<br/><NewLine>5     start_time = time.time()<br/><NewLine>----&gt; 6     train_loss, train_acc = train(model, train_iterator, optimizer, criterion)<br/><NewLine>7     valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)<br/><NewLine>8</p><NewLine><p> in train(model, iterator, optimizer, criterion)<br/><NewLine>17     # batch.text 就是上面forward函数的参数text<br/><NewLine>18     # 压缩维度，不然跟 batch.label 维度对不上<br/><NewLine>—&gt; 19     predictions = model(batch.text).squeeze(1)<br/><NewLine>20<br/><NewLine>21     loss = criterion(predictions, batch.label)</p><NewLine><p>/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py in <strong>call</strong>(self, *input, **kwargs)<br/><NewLine>487             result = self._slow_forward(*input, **kwargs)<br/><NewLine>488         else:<br/><NewLine>–&gt; 489             result = self.forward(*input, **kwargs)<br/><NewLine>490         for hook in self._forward_hooks.values():<br/><NewLine>491             hook_result = hook(self, input, result)</p><NewLine><p> in forward(self, text)<br/><NewLine>38<br/><NewLine>39<br/><NewLine>—&gt; 40     return self.fc(hidden.squeeze)</p><NewLine><p>/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py in <strong>call</strong>(self, *input, **kwargs)<br/><NewLine>487             result = self._slow_forward(*input, **kwargs)<br/><NewLine>488         else:<br/><NewLine>–&gt; 489             result = self.forward(*input, **kwargs)<br/><NewLine>490         for hook in self._forward_hooks.values():<br/><NewLine>491             hook_result = hook(self, input, result)</p><NewLine><p>/usr/local/lib/python3.6/site-packages/torch/nn/modules/linear.py in forward(self, input)<br/><NewLine>65     <span class=""mention"">@weak_script_method</span><br/><NewLine>66     def forward(self, input):<br/><NewLine>—&gt; 67         return F.linear(input, self.weight, self.bias)<br/><NewLine>68<br/><NewLine>69     def extra_repr(self):</p><NewLine><p>/usr/local/lib/python3.6/site-packages/torch/nn/functional.py in linear(input, weight, bias)<br/><NewLine>1348         - Output: :math:<code>(N, *, out\_features)</code><br/><NewLine>1349     “”""<br/><NewLine>-&gt; 1350     if input.dim() == 2 and bias is not None:<br/><NewLine>1351         # fused op is marginally faster<br/><NewLine>1352         ret = torch.addmm(torch.jit._unwrap_optional(bias), input, weight.t())</p><NewLine><p>AttributeError: ‘builtin_function_or_method’ object has no attribute ‘dim’</p><NewLine><p>This is the associated source code</p><NewLine><p>import torch.nn as nn<br/><NewLine>import torch.nn.functional as F</p><NewLine><p>class RNN(nn.Module):<br/><NewLine>def <strong>init</strong>(self, vocab_size, embedding_dim, hidden_dim, output_dim,<br/><NewLine>n_layers, bidirectional, dropout, pad_idx):</p><NewLine><pre><code>super().__init__()<NewLine>self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)<NewLine><NewLine># embedding_dim: 每个词向量的维度<NewLine># hidden_dim: 隐藏层的维度<NewLine># num_layers: 神经网络深度，纵向深度<NewLine># bidrectional: 是否双向循环RNN<NewLine># dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。<NewLine># 经过交叉验证，隐含节点dropout率等于0.5的时候效果最好，原因是0.5的时候dropout随机生成的网络结构最多。<NewLine>self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,<NewLine>                   bidirectional=bidirectional, dropout=dropout)<NewLine><NewLine>self.fc = nn.Linear(hidden_dim*2, output_dim)  # *2是因为BiLSTM<NewLine>self.dropout = nn.Dropout(dropout)<NewLine></code></pre><NewLine><p>def forward(self, text):<br/><NewLine>embedded = self.dropout(self.embedding(text)) # [sent len, batch size, emb dim]</p><NewLine><pre><code># output = [sent len, batch size, hid dim * num directions]<NewLine># hidden = [num layers * num directions, batch size, hid dim]<NewLine># cell = [num layers * num directions, batch size, hid dim]<NewLine>output, (hidden, cell) = self.rnn(embedded)<NewLine><NewLine># concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers<NewLine># and apply dropout<NewLine># [batch size, hid dim * num directions], 横着拼接的<NewLine># 倒数第一个和倒数第二个是BiLSTM最后要保留的状态<NewLine>hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))<NewLine></code></pre><NewLine><h1>hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :])))</h1><NewLine><pre><code>return self.fc(hidden.squeeze)<NewLine></code></pre><NewLine><p>def train(model, iterator, optimizer, criterion):</p><NewLine><p>epoch_loss = 0<br/><NewLine>epoch_acc = 0<br/><NewLine>total_len = 0</p><NewLine><h1>model.train()代表了训练模式</h1><NewLine><h1>model.train() ：启用 BatchNormalization 和 Dropout</h1><NewLine><h1>model.eval() ：不启用 BatchNormalization 和 Dropout</h1><NewLine><p>model.train()</p><NewLine><h1>iterator为train_iterator</h1><NewLine><p>for batch in iterator:<br/><NewLine># 梯度清零，加这步防止梯度叠加<br/><NewLine>optimizer.zero_grad()</p><NewLine><pre><code># batch.text 就是上面forward函数的参数text<NewLine># 压缩维度，不然跟 batch.label 维度对不上<NewLine>predictions = model(batch.text).squeeze(1)<NewLine><NewLine>loss = criterion(predictions, batch.label)<NewLine>acc = binary_accuracy(predictions, batch.label)<NewLine><NewLine>loss.backward()  # 反向传播<NewLine>optimizer.step() # 梯度下降<NewLine><NewLine># loss.item() 以及本身除以了 len(batch.label)<NewLine># 所以得再乘一次，得到一个batch的损失，累加得到所有样本损失<NewLine>epoch_loss += loss.item() * len(batch.label)<NewLine><NewLine># (acc.item(): 一个batch的正确率) * batch数 = 正确数<NewLine># train_iterator 所有batch的正确数累加<NewLine>epoch_acc += acc.item() * len(batch.label)<NewLine><NewLine># 计算 train_iterator 所有样本的数量，应该是17500<NewLine>total_len += len(batch.label)<NewLine></code></pre><NewLine><h1>epoch_loss / total_len ：train_iterator所有batch的损失</h1><NewLine><h1>epoch_acc / total_len ：train_iterator所有batch的正确率</h1><NewLine><p>return epoch_loss / total_len, epoch_acc / total_len</p><NewLine><p>N_EPOCHS = 10<br/><NewLine>best_valid_loss = float(‘inf’)</p><NewLine><p>for epoch in range(N_EPOCHS):<br/><NewLine>start_time = time.time()<br/><NewLine>train_loss, train_acc = train(model, train_iterator, optimizer, criterion)<br/><NewLine>valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</p><NewLine><pre><code>end_time = time.time()<NewLine><NewLine>epoch_mins, epoch_secs = epoch_time(start_time, end_time)<NewLine><NewLine>if valid_loss &lt; best_valid_loss:<NewLine>    best_valid_loss = valid_loss<NewLine>    torch.save(model.state_dict(), 'lstm-model.pt')<NewLine><NewLine>print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')<NewLine>print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')<NewLine>print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')<NewLine></code></pre><NewLine><p>How can I solve this problem？？</p><NewLine></div>",https://discuss.pytorch.org/u/lianzhang132,(Lianzhang132),lianzhang132,"July 10, 2020,  9:18am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems, you are accidentally passing a method to a <code>forward</code> function in this line of code:</p><NewLine><pre><code class=""lang-python"">return self.fc(hidden.squeeze)<NewLine></code></pre><NewLine><p>Make sure you are calling the <code>squeeze</code> operation and pass its output to <code>self.fc</code> via:</p><NewLine><pre><code class=""lang-python"">return self.fc(hidden.squeeze())<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply. You are right</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/lianzhang132; <NewLine> ,"REPLY_DATE 1: July 12, 2020,  2:19am; <NewLine> REPLY_DATE 2: July 13, 2020,  6:26am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
88763,Mesh R-CNN on android,2020-07-11T06:01:40.573Z,0,77,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to use the pretrained Mesh R-CNN model in the android app using the existing pytorch android deployment pipeline? (for on-device inference)</p><NewLine><p>Mesh R-CNN model from the page <a href=""https://github.com/facebookresearch/meshrcnn/blob/master/INSTRUCTIONS_PIX3D.md"" rel=""nofollow noopener"">https://github.com/facebookresearch/meshrcnn/blob/master/INSTRUCTIONS_PIX3D.md</a></p><NewLine><p><a class=""mention"" href=""/u/zdevito"">@zdevito</a> <a class=""mention"" href=""/u/michael_suo"">@Michael_Suo</a><br/><NewLine><a class=""mention"" href=""/u/jcjohns"">@jcjohns</a></p><NewLine></div>",https://discuss.pytorch.org/u/sujay,(sujay),sujay,"July 11, 2020,  6:07am",,,,,
88388,Check which engine is used for convolution operations,2020-07-08T12:22:55.629Z,0,82,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am trying to figure out which engine is being used for convolution operations when I run my model with PyTorch Mobile. I would like to change it to XNNPACK if that isn’t the default. Any help on how to do any of these two things would be appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/JustasZ,,JustasZ,"July 8, 2020, 12:22pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/justasz"">@JustasZ</a>. If you compile pytorch from the current master branch, the XNNPACK has already been enabled on mobile. But if you want to use cocoapods or  Android maven, please wait for the 1.6.0 release, which will set XNNPACK as the default computation kernel.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/xta0; <NewLine> ,"REPLY_DATE 1: July 13, 2020,  6:20am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
88100,Mobile GPU support on Android,2020-07-06T11:37:51.881Z,0,115,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I know that currently there isn’t any gpu support for android platforms(am I right?) but is there any plan that we can estimate that such support when will be in the release. And if you love pytorch(like me) and use pytorch for training, how you deploy your application on mobile devices with gpu support?(like torch-&gt;tflite or torch-&gt;onnx-&gt;coreML)</p><NewLine></div>",https://discuss.pytorch.org/u/saeed_masoomi,(saeed masoomi),saeed_masoomi,"July 6, 2020, 11:41am",1 Like,,,,
66482,Model with tensor and number operations errors in iOS,2020-01-13T07:14:13.457Z,11,654,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Traced a model and saved it using pytorch 1.3.1 with the following code.</p><NewLine><pre><code class=""lang-auto"">class TestModule(torch.nn.Module):<NewLine>    <NewLine>    def forward(self, W):<NewLine>        g = 2 * W<NewLine>        return g<NewLine><NewLine>W = torch.rand(10)<NewLine><NewLine>test_model = torch.jit.trace(test_model, [W])<NewLine>test_model.save(""test_model.pt"")<NewLine></code></pre><NewLine><p>and loaded the model into iOS (libtorch 1.3.1) using the following code</p><NewLine><pre><code class=""lang-auto"">torch::jit::script::Module testModel = torch::jit::load(filePath.UTF8String);<NewLine><NewLine>torch::IValue w = torch::IValue(torch::rand({10}));<NewLine><NewLine>testModel.forward({w});<NewLine></code></pre><NewLine><p>and it gives the following error on <code>forward</code></p><NewLine><pre><code class=""lang-auto"">libc++abi.dylib: terminating with uncaught exception of type c10::Error: false CHECK FAILED at /Users/distiller/project/c10/core/Backend.h (tensorTypeIdToBackend at /Users/distiller/project/c10/core/Backend.h:106)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/mark_jimenez,,mark_jimenez,"January 13, 2020,  7:14am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mark_jimenez"">@mark_jimenez</a>, can you add these two lines before running <code>forward</code></p><NewLine><pre><code class=""lang-auto"">torch::autograd::AutoGradMode guard(false);<NewLine>at::AutoNonVariableTypeMode non_var_type_mode(true);<NewLine></code></pre><NewLine><p>The first one tells the engine to disable autograd, the second one is sort of a workaround. We can get rid of it in 1.4.0 which will be released soon.</p><NewLine><p>Let me know if you have any questions.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/xta0"">@xta0</a>. It worked when I added those two lines. Thank you!</p><NewLine><p>I discovered a few more issues with the current build (1.3.1). Should I post them in a new thread, or is 1.4 going to be released soon so I can check them in that version?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mark_jimenez"">@mark_jimenez</a>  you can post here in this thread, I’ll follow up. In 1.4.0, you still need this line - <code>torch::autograd::AutoGradMode guard(false);</code>, but <code>at::AutoNonVariableTypeMode non_var_type_mode(true);</code> is not necessary.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <a class=""mention"" href=""/u/xta0"">@xta0</a>. I’ll ask one issue at a time so its not overwhelming.</p><NewLine><p>I couldn’t include the libtorch in a unit test target. I filed an issue in cocoapods and they said it’s because libnnpack.as isn’t built as a universal library.</p><NewLine><p>I’ve also filed the issue here where you can find more details: <a href=""https://github.com/pytorch/pytorch/issues/32040"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/32040</a></p><NewLine><p>Thank you for the help.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Dear <a class=""mention"" href=""/u/xta0"">@xta0</a>, I have a similar error.</p><NewLine><p>I have exported a pre-trained model from python to c++ (model.pt) and it works perfectly in a c++ application.<br/><NewLine>I tried to use the same exported model in my iPad, and it gives the following error on <code>torch::jit::load()</code> method:</p><NewLine><pre><code class=""lang-auto"">libc++abi.dylib: terminating with uncaught exception of type c10::Error: false CHECK FAILED at /Users/distiller/project/c10/core/Backend.h (tensorTypeIdToBackend at /Users/distiller/project/c10/core/Backend.h:106)<NewLine>(no backtrace available)<NewLine></code></pre><NewLine><p>When I add the two lines (to disable autograd and the second one) before running <code>torch::jit::load()</code>, it returns:</p><NewLine><pre><code class=""lang-auto"">libc++abi.dylib: terminating with uncaught exception of type c10::Error: !v.defined() || v.is_variable() INTERNAL ASSERT FAILED at/Users/distiller/project/torch/csrc/jit/ir.h (t_ at /Users/distiller/project/torch/csrc/jit/ir.h:718)<NewLine>(no backtrace available)<NewLine></code></pre><NewLine><p>Do you have any idea how to solve this problem?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mark_jimenez"">@mark_jimenez</a> this is a known issue, because NNPACK doesn’t support the iOS simulator architecture, as is shown here - <a href=""https://github.com/Maratyszcza/NNPACK"" rel=""nofollow noopener"">https://github.com/Maratyszcza/NNPACK</a> . So for the simulator build, operators are not being run via NNPACK. However, <a class=""mention"" href=""/u/ashkanaliabadi"">@AshkanAliabadi</a> in our team has been actively working on XNNPACK, which will replace NNPACK in the future. Sorry for the inconvenience.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/fabricionarcizo"">@fabricionarcizo</a> What version of libtorch were you using? My guess is that your desktop version of PyTorch didn’t match the version of your libtorch. This will affect how your torchscript model is generated. You can verify the version by typing the command below<br/><NewLine><code>torch.version.__version__</code></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/xta0"">@xta0</a> I have used the pytorch 1.3.1 and libtorch 1.3.1. I generated the model in Python, and used <code>torch.jit.trace</code> and <code>torch.jit.save</code> to save the model in a <code>.pt</code> file. I’m able to load the model using <code>torch::jit::load</code> method in the C++ code. However, the same method (<code>torch::jit::load</code>) doesn’t work in the Objective-C version (also 1.3.1).</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/fabricionarcizo"">@fabricionarcizo</a> Is it OK to paste your python code here (or somewhere I can see)?  So that I can debug. Because from your description, I’ve no idea of what could go wrong.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/xta0"">@xta0</a> Thank you for the update on NNPACK. It’s alright, I understand that porting NNPACK is a big project. I’ll be patient for any updates.</p><NewLine><p>BTW, congratulations on the <code>1.4</code> release. It fixed some of the problems I was going to ask from <code>1.3.1</code>.</p><NewLine><p>I wanted to ask if on device training and/or Swift and Objective-C API  will be supported in the next release? Or at least is on the <code>1.5</code> branch. I see that the podspec on the pytorch github repo is at 1.5 (<a href=""https://github.com/pytorch/pytorch/blob/master/ios/LibTorch.podspec"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/ios/LibTorch.podspec</a>). Is there any way we can work on that version?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mark_jimenez"">@mark_jimenez</a> Thanks.  We have teams working on enabling on-deivce training, but I’m not sure if that can be released in 1.5.0.  As for the API wrappers, we have a proposal internally - <a href=""https://github.com/pytorch/pytorch/pull/25541"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/25541</a>. We’ve been proactively collecting feedbacks from communities, but haven;t decided when to release it, so feel free to submit ideas, proposals, etc. The 1.5.0 version in <code>.podspec</code> is just a placeholder, nothing particular has been done on that branch.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/xta0"">@xta0</a> Thank you for the update!</p><NewLine><p>We’re working on a workaround right now for training in mobile by updating the weights of the model by ourself in torchscript. We can’t get it to work on libtorch <code>1.4</code> on iOS (not sure on Android). I think it may have something to do with how autograd is implemented on mobile.</p><NewLine><p>For example here’s the python code compiled in PyTorch <code>1.4</code>.</p><NewLine><pre><code class=""lang-auto"">class TestModule(torch.nn.Module):<NewLine>    def forward(self, x, y):<NewLine>        z = x + y<NewLine>        L = z.sum()<NewLine>        L.backward()<NewLine>        return x.grad, y.grad<NewLine><NewLine>model = torch.jit.script(TestModule(), torch.tensor([1.]), torch.tensor([1.])) <NewLine></code></pre><NewLine><p>And in iOS, the model is loaded like this</p><NewLine><pre><code class=""lang-auto""><NewLine>    torch::jit::script::Module testModel = torch::jit::load(testFilePath.UTF8String);<NewLine><NewLine>    auto result = testModel.forward({torch::rand({1}, torch::TensorOptions().requires_grad(true)), torch::rand({1}, torch::TensorOptions().requires_grad(true))});<NewLine></code></pre><NewLine><p>I get the same error as the one you filed here: <a href=""https://github.com/pytorch/pytorch/pull/30067"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/30067</a></p><NewLine><p>So reading that, I guess autograd isn’t implemented for the mobile builds? Is that still the case for libtorch <code>1.4</code>?</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mark_jimenez"">@mark_jimenez</a> you’re right. The autograd is not available on mobile so far.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/fabricionarcizo"">@fabricionarcizo</a> <a class=""mention"" href=""/u/xta0"">@xta0</a>  I am having the same issue. The program breaks at this line in module.h file:</p><NewLine><pre><code class=""lang-auto"">IValue forward(std::vector&lt;IValue&gt; inputs) {<NewLine>    return get_method(""forward"")(std::move(inputs)); // here it breaks<NewLine>  }<NewLine></code></pre><NewLine><p>PyTorch: 1.4.0<br/><NewLine>LibTorch: 1.4.0<br/><NewLine>iOS 13.5<br/><NewLine>XCode 11.5</p><NewLine><p>Did you manage to resolve this? I happened to notice that PyTorch iOS tutorial project on PyTorch website “HelloWorld” also breaks in <strong>release mode</strong>. The model runs <strong>okay in debug mode</strong>.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/harsh_thaker"">@Harsh_Thaker</a> thanks for reporting. I was able to repro, looks like the the crash was in pthreadpool when invoking convolution via NNPACK</p><NewLine><pre><code class=""lang-auto"">* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x10dcbd3a4)<NewLine>    frame #0: 0x00000001013a83b4 HelloWorld`compute_input_packing + 256<NewLine>    frame #1: 0x000000010108b2cc HelloWorld`std::__1::function&lt;void (int, unsigned long)&gt;::operator()(int, unsigned long) const + 48<NewLine>    frame #2: 0x000000010108b884 HelloWorld`caffe2::ThreadPool::run(std::__1::function&lt;void (int, unsigned long)&gt; const&amp;, unsigned long)::FnTask::Run() + 44<NewLine>    frame #3: 0x000000010108b530 HelloWorld`caffe2::WorkersPool::Execute(std::__1::vector&lt;std::__1::shared_ptr&lt;caffe2::Task&gt;, std::__1::allocator&lt;std::__1::shared_ptr&lt;caffe2::Task&gt; &gt; &gt; const&amp;) + 304<NewLine>    frame #4: 0x000000010108b11c HelloWorld`caffe2::ThreadPool::run(std::__1::function&lt;void (int, unsigned long)&gt; const&amp;, unsigned long) + 740<NewLine>    frame #5: 0x00000001010885e4 HelloWorld`pthreadpool_compute_1d + 84<NewLine>    frame #6: 0x0000000101087f00 HelloWorld`pthreadpool_compute_2d_tiled + 148<NewLine>    frame #7: 0x00000001013a68b0 HelloWorld`nnp_convolution_inference + 3168<NewLine>    frame #8: 0x0000000100aca13c HelloWorld`at::native::_nnpack_spatial_convolution(at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::ArrayRef&lt;long long&gt;, c10::ArrayRef&lt;long long&gt;)::$_0::operator()(unsigned long) const + 680<NewLine>    frame #9: 0x0000000100ac9284 HelloWorld`at::native::_nnpack_spatial_convolution(at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::ArrayRef&lt;long long&gt;, c10::ArrayRef&lt;long long&gt;) + 1700<NewLine>    frame #10: 0x0000000100d075a4 HelloWorld`at::TypeDefault::_nnpack_spatial_convolution(at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::ArrayRef&lt;long long&gt;, c10::ArrayRef&lt;long long&gt;) + 248<NewLine>...<NewLine></code></pre><NewLine><p>Can you confirm whether you were seeing the same stack trace? If so, I belive that issue has been resolved in</p><NewLine><ul><NewLine><li><a href=""https://github.com/pytorch/pytorch/pull/39868"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/39868</a></li><NewLine><li><a href=""https://github.com/pytorch/pytorch/pull/40951"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/40951</a></li><NewLine></ul><NewLine><p>Both of them wiil be merged to our 1.6.0 release.</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p><code>#21	0x00000001030f86d0 in torch::jit::script::Module::forward(std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;) at ios-demo-app-master/HelloWorld/HelloWorld/Pods/LibTorch/install/include/torch/csrc/jit/script/module.h:113</code></p><NewLine><p>That’s the one in my case. I could solve the problem by setting Optimization level flag for release in Swift compiler to <strong>None</strong> from default <strong>Optimize for speed</strong>. What can be the issue with that?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mark_jimenez; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mark_jimenez; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/fabricionarcizo; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/fabricionarcizo; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mark_jimenez; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/mark_jimenez; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/Harsh_Thaker; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/Harsh_Thaker; <NewLine> ,"REPLY_DATE 1: February 11, 2020,  1:23am; <NewLine> REPLY_DATE 2: January 14, 2020, 10:17am; <NewLine> REPLY_DATE 3: January 14, 2020,  6:33pm; <NewLine> REPLY_DATE 4: January 15, 2020,  6:33am; <NewLine> REPLY_DATE 5: January 15, 2020,  1:45pm; <NewLine> REPLY_DATE 6: January 15, 2020,  6:44pm; <NewLine> REPLY_DATE 7: January 15, 2020,  7:24pm; <NewLine> REPLY_DATE 8: January 16, 2020, 10:04am; <NewLine> REPLY_DATE 9: January 17, 2020,  1:04am; <NewLine> REPLY_DATE 10: January 17, 2020,  7:18am; <NewLine> REPLY_DATE 11: January 17, 2020,  8:00pm; <NewLine> REPLY_DATE 12: January 21, 2020,  6:27am; <NewLine> REPLY_DATE 13: January 21, 2020,  6:41pm; <NewLine> REPLY_DATE 14: July 4, 2020, 10:01am; <NewLine> REPLY_DATE 15: July 4, 2020,  7:08pm; <NewLine> REPLY_DATE 16: July 5, 2020, 12:58pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> 
84570,Error when loading quantized Retinanet in android environment,2020-06-07T21:21:30.248Z,0,120,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to deploy a quantized Retinanet in a simple android application, the quantization process is accomplished just fine and the accuracy managed to stay practically the same, after I convert the model to torchscript and try to load it in the android application, an exception occurs, the message states the following</p><NewLine><pre><code class=""lang-auto"">com.facebook.jni.CppException: <NewLine>Arguments for call are not valid.<NewLine>The following variants are available:<NewLine>  <NewLine>  aten::upsample_nearest2d(Tensor self, int[2] output_size) -&gt; (Tensor):<NewLine>  Expected at most 2 arguments but found 4 positional arguments.<NewLine>  <NewLine>  aten::upsample_nearest2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out) -&gt; (Tensor(a!)):<NewLine>  Argument out not provided.<NewLine><NewLine>The original call is:<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\torch\nn\functional.py(2990): interpolate<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\torch\nn\modules\upsampling.py(131): forward<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\torch\nn\modules\module.py(534): _slow_forward<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\torch\nn\modules\module.py(548): __call__<NewLine>d:\github\pytorch-retinanet\retinanet\modelQ.py(64): forward<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\torch\nn\modules\module.py(534): _slow_forward<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\torch\nn\modules\module.py(548): __call__<NewLine>d:\github\pytorch-retinanet\retinanet\modelQ.py(274): forward<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\torch\nn\modules\module.py(534): _slow_forward<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\torch\nn\modules\module.py(548): __call__<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\torch\jit\__init__.py(1027): trace_module<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\torch\jit\__init__.py(873): trace<NewLine>&lt;ipython-input-8-f99b9332c7b3&gt;(3): &lt;module&gt;<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\IPython\core\interactiveshell.py(3331): run_code<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\IPython\core\interactiveshell.py(3254): run_ast_nodes<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\IPython\core\interactiveshell.py(3062): run_cell_async<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\IPython\core\async_helpers.py(68): _pseudo_sync_runner<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\IPython\core\interactiveshell.py(2886): _run_cell<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\IPython\core\interactiveshell.py(2857): run_cell<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\ipykernel\zmqshell.py(536): run_cell<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\ipykernel\ipkernel.py(300): do_execute<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\tornado\gen.py(209): wrapper<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\ipykernel\kernelbase.py(539): execute_request<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\tornado\gen.py(209): wrapper<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\ipykernel\kernelbase.py(268): dispatch_shell<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\tornado\gen.py(209): wrapper<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\ipykernel\kernelbase.py(361): process_one<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\tornado\gen.py(748): run<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\tornado\gen.py(787): inner<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\tornado\ioloop.py(743): _run_callback<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\tornado\ioloop.py(690): &lt;lambda&gt;<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\asyncio\events.py(81): _run<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\asyncio\base_events.py(1859): _run_once<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\asyncio\base_events.py(570): run_forever<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\tornado\platform\asyncio.py(149): start<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\ipykernel\kernelapp.py(583): start<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\traitlets\config\application.py(664): launch_instance<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\site-packages\ipykernel_launcher.py(16): &lt;module&gt;<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\runpy.py(86): _run_code<NewLine>C:\Users\anoua\anaconda3\envs\PFE_env\lib\runpy.py(193): _run_module_as_main<NewLine>Serialized   File ""code/__torch__/torch/nn/modules/upsampling.py"", line 14<NewLine>    _6 = torch.mul(torch.to(_4, 6, False, False, None), torch.detach(_5))<NewLine>    _7 = torch.floor(torch.to(_6, 6, False, False, None))<NewLine>    x = torch.upsample_nearest2d(argument_1, [int(_3), int(_7)], None, None)<NewLine>        ~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>    return x<NewLine><NewLine></code></pre><NewLine><p>at first I suspected quantisation as the culprit, but when I try to deploy the baseline model the same error pop up, did I do something wrong, i tried to follow the pytorch mobile tutorial as best as I can.  Some help with this would be much appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/Anouar_LAOUICHI,(Anouar LAOUICHI),Anouar_LAOUICHI,"June 7, 2020,  9:21pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I got the same issue.<br/><NewLine>Using a newer version of Pytorch mobile has solved the issue.<br/><NewLine>Here is what i included in build.gradle:</p><NewLine><p>implementation ‘org.pytorch:pytorch_android:1.5.0’<br/><NewLine>implementation ‘org.pytorch:pytorch_android_torchvision:1.5.0’</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ntreepoint; <NewLine> ,"REPLY_DATE 1: June 29, 2020,  8:54pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
86907,Linker errors when trying to use C++ API from my own project,2020-06-25T14:06:07.439Z,0,80,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I’m trying to use the C++ API from my own CPP Android project.<br/><NewLine>I used your pytorch-android-cpp-demo and successfully built Android libtorch.a and libc10.a<br/><NewLine>However, While I’m trying to add these static libraries to my project I’m getting the following errors:</p><NewLine><p>/mnt/c/repos/pytorch-android-cpp-demo/pytorch/aten/src/ATen/core/ivalue.cpp:390: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::Device const&amp;)’<br/><NewLine>/mnt/c/repos/pytorch-android-cpp-demo/pytorch/aten/src/ATen/core/ivalue.cpp:449: error: undefined reference to ‘c10::operator&lt;&lt;(std::__ndk1::basic_ostream&lt;char, std::__ndk1::char_traits &gt;&amp;, c10::Device const&amp;)’<br/><NewLine>/mnt/c/repos/pytorch-android-cpp-demo/pytorch/build_android/aten/src/ATen/Functions.cpp:145: error: undefined reference to ‘c10::toString(c10::DispatchKey)’<br/><NewLine>/mnt/c/repos/pytorch-android-cpp-demo/pytorch/build_android/aten/src/ATen/Functions.cpp:165: error: undefined reference to ‘c10::toString(c10::DispatchKey)’<br/><NewLine>/mnt/c/repos/pytorch-android-cpp-demo/pytorch/c10/core/TensorOptions.h:252: error: undefined reference to ‘c10::get_default_dtype()’<br/><NewLine>/mnt/c/repos/pytorch-android-cpp-demo/pytorch/c10/core/TensorOptions.h:252: error: undefined reference to ‘c10::get_default_dtype()’<br/><NewLine>/mnt/c/repos/pytorch-android-cpp-demo/pytorch/build_android/aten/src/ATen/Functions.cpp:289: error: undefined reference to ‘c10::toString(c10::DispatchKey)’<br/><NewLine>/mnt/c/repos/pytorch-android-cpp-demo/pytorch/build_android/aten/src/ATen/Functions.cpp:827: error: undefined reference to ‘c10::toString(c10::DispatchKey)’</p><NewLine><p>Can you help me figure out what am I missing?</p><NewLine></div>",https://discuss.pytorch.org/u/Aviais,(avirzu),Aviais,"June 25, 2020,  2:06pm",,,,,
86901,Load own model on android app,2020-06-25T13:48:16.251Z,0,95,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I want create my Android app by using my convolutional model written in Pytorch 1.4.0.<br/><NewLine>I was following the guidelines written in <a href=""https://pytorch.org/mobile/android/"" rel=""nofollow noopener"">https://pytorch.org/mobile/android/</a>, in detail I was following example code indicating here: <a href=""https://github.com/pytorch/android-demo-app/tree/master/HelloWorldApp"" rel=""nofollow noopener"">https://github.com/pytorch/android-demo-app/tree/master/HelloWorldApp</a>.</p><NewLine><p>I have problem when I import my model. Error is:</p><NewLine><p>Process: org.pytorch.helloworld, PID: 15383<br/><NewLine>java.lang.RuntimeException: Unable to start activity ComponentInfo{org.pytorch.MainActivity}: com.facebook.jni.CppException: [enforce fail at inline_container.cc:143] . PytorchStreamReader failed reading zip archive: failed finding central directory<br/><NewLine>(no backtrace available)</p><NewLine><p>on Android studio (versione 4.0). I read that this can be due to erroneous procedure to save model_traced.pt.</p><NewLine><p>I saved Mymodel.pt  as below:</p><NewLine><p>model.load_state_dict(torch.load(Mymodel.pt)<br/><NewLine>model.eval()<br/><NewLine>example_inputs=torch.rand(1,1,66,100)<br/><NewLine>Mymodel_traced=torch.jit.trace(model,example_inputs=example_inputs)<br/><NewLine>Mymodel_traced.save(“model_traced.pt”)</p><NewLine><p>In my gradle I have:</p><NewLine><p>dependencies {<br/><NewLine>implementation ‘androidx.appcompat:appcompat:1.1.0’<br/><NewLine>implementation ‘org.pytorch:pytorch_android:1.4.0’<br/><NewLine>implementation ‘org.pytorch:pytorch_android_torchvision:1.4.0’<br/><NewLine>}</p><NewLine><p>How can I solve? Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/GreenSun,,GreenSun,"June 25, 2020,  1:48pm",,,,,
80694,Compilation error in &lsquo;jit/ir/ir.h&rsquo;,2020-05-11T12:18:51.046Z,2,220,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I’m trying to include PyTorch library in my CPP project.<br/><NewLine>I’m using ndk21 and C++14, torch version 1.5.0 for CPU.</p><NewLine><p>I get the following compilation errors, all from csrc/jit/ir/ir.h and regarding c10 namespace:</p><NewLine><p>C:\REPOS\CPU\libtorch-shared-with-deps-latest\libtorch/include\torch/csrc/jit/ir/ir.h:67:17: error: no<br/><NewLine>namespace named ‘prim’ in namespace ‘c10’; did you mean simply ‘prim’?<br/><NewLine>using namespace ::c10::prim;<br/><NewLine>^~~~~~~~~~~<br/><NewLine>prim<br/><NewLine>C:\REPOS\CPU\libtorch-shared-with-deps-latest\libtorch/include\torch/csrc/jit/ir/ir.h:66:11: note: namespace<br/><NewLine>‘prim’ defined here<br/><NewLine>namespace prim {<br/><NewLine>^<br/><NewLine>C:\REPOS\CPU\libtorch-shared-with-deps-latest\libtorch/include\torch/csrc/jit/ir/ir.h:70:17: error: no<br/><NewLine>namespace named ‘attr’ in namespace ‘c10’; did you mean simply ‘attr’?<br/><NewLine>using namespace ::c10::attr;<br/><NewLine>^~~~~~~~~~~<br/><NewLine>attr<br/><NewLine>C:\REPOS\CPU\libtorch-shared-with-deps-latest\libtorch/include\torch/csrc/jit/ir/ir.h:69:11: note: namespace<br/><NewLine>‘attr’ defined here<br/><NewLine>namespace attr {<br/><NewLine>^<br/><NewLine>C:\REPOS\CPU\libtorch-shared-with-deps-latest\libtorch/include\torch/csrc/jit/ir/ir.h:73:17: error: no<br/><NewLine>namespace named ‘aten’ in namespace ‘c10’; did you mean simply ‘aten’?<br/><NewLine>using namespace ::c10::aten;<br/><NewLine>^~~~~~~~~~~<br/><NewLine>aten<br/><NewLine>C:\REPOSCPU\libtorch-shared-with-deps-latest\libtorch/include\torch/csrc/jit/ir/ir.h:72:11: note: namespace<br/><NewLine>‘aten’ defined here<br/><NewLine>namespace aten {<br/><NewLine>^<br/><NewLine>C:\REPOS\CPU\libtorch-shared-with-deps-latest\libtorch/include\torch/csrc/jit/ir/ir.h:1309:34: error: no<br/><NewLine>member named ‘prim’ in namespace ‘c10’; did you mean simply ‘prim’?<br/><NewLine>static constexpr Symbol Kind = ::c10::prim::profile;<br/><NewLine>^~~~~~~~~~~<br/><NewLine>prim<br/><NewLine>C:\REPOS\CPU\libtorch-shared-with-deps-latest\libtorch/include\torch/csrc/jit/ir/ir.h:66:11: note: ‘prim’<br/><NewLine>declared here<br/><NewLine>namespace prim {<br/><NewLine>^<br/><NewLine>C:\REPOS\CPU\libtorch-shared-with-deps-latest\libtorch/include\torch/csrc/jit/ir/ir.h:1309:47: error: no<br/><NewLine>member named ‘profile’ in namespace ‘torch::jit::prim’<br/><NewLine>static constexpr Symbol Kind = ::c10::prim::profile;<br/><NewLine><s><s><s>^<br/><NewLine>C:\REPOS\CPU\libtorch-shared-with-deps-latest\libtorch/include\torch/csrc/jit/ir/ir.h:1311:21: error: no<br/><NewLine>member named ‘prim’ in namespace ‘c10’; did you mean simply ‘prim’?<br/><NewLine>: Node(graph, ::c10::prim::profile), callback_(callback) {}<br/><NewLine>^</s></s></s>~~~~<br/><NewLine>prim<br/><NewLine>C:\REPOS\CPU\libtorch-shared-with-deps-latest\libtorch/include\torch/csrc/jit/ir/ir.h:66:11: note: ‘prim’<br/><NewLine>declared here<br/><NewLine>namespace prim {<br/><NewLine>^<br/><NewLine>C:\REPOS\CPU\libtorch-shared-with-deps-latest\libtorch/include\torch/csrc/jit/ir/ir.h:1311:34: error: no<br/><NewLine>member named ‘profile’ in namespace ‘torch::jit::prim’<br/><NewLine>: Node(graph, ::c10::prim::profile), callback_(callback) {}</p><NewLine><p>Can someone please help me to overcome this?<br/><NewLine>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Aviais,(avirzu),Aviais,"May 15, 2020,  6:47pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><strong>More information:</strong></p><NewLine><ol><NewLine><li>I have a CPP android project. I’m compiling it using ndk-build (ndk21 and C++14).</li><NewLine><li>I have a file named “main.cpp” and I’m trying to  <strong><span class=""hashtag"">#include</span> “torch/script.h”</strong> .</li><NewLine><li>I downloaded libtorch using this command:<br/><NewLine><strong>wget <a href=""https://github.com/Kitware/CMake/releases/download/v3.15.2/cmake-3.15.2.tar.gz"" rel=""nofollow noopener"">https://github.com/Kitware/CMake/releases/download/v3.15.2/cmake-3.15.2.tar.gz</a></strong><NewLine></li><NewLine><li>In my Android.mk file, I’m giving the path to:<br/><NewLine>LOCAL_C_INCLUDES += $(TORCH_REPO_PATH)<br/><NewLine>$(TORCH_REPO_PATH)/include<br/><NewLine>$(TORCH_REPO_PATH)/include/torch<br/><NewLine>$(TORCH_REPO_PATH)/include/torch/csrc/api/include</li><NewLine><li>When I’m trying to compile, I get the errors demonstrated above.<br/><NewLine>How can I overcome this issue?</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This question seems to be related to <a href=""https://github.com/pytorch/pytorch/issues/38319"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/38319</a></p><NewLine><p>Copy the options from there:</p><NewLine><ol><NewLine><li>You can use the prebuilt binaries we released for mobile, see details: <a href=""https://pytorch.org/mobile/home/"" rel=""nofollow noopener"">https://pytorch.org/mobile/home/</a><NewLine></li><NewLine><li>You can build it from source on your own for Android using the following script:</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">scripts/build_pytorch_android.sh<NewLine></code></pre><NewLine><ol start=""3""><NewLine><li>If you don’t want to use the JNI wrapper, you can also use the C++ API directly from your Android C++ code. This is not officially supported yet, but here is a simple example: <a href=""https://github.com/ljk53/pytorch-android-cpp-demo"" rel=""nofollow noopener"">https://github.com/ljk53/pytorch-android-cpp-demo</a><NewLine></li><NewLine></ol><NewLine><p>For option 1 &amp; option 2, you will get libtorch built in the form of Android AAR (<a href=""https://stackoverflow.com/questions/23915619/android-archive-library-aar-vs-standard-jar"" rel=""nofollow noopener"">https://stackoverflow.com/questions/23915619/android-archive-library-aar-vs-standard-jar</a>), which provides Java API and can be called from your Java code on Android.</p><NewLine><p>If you want to call libtorch C++ API in your mobile app directly, you can try Option 3 (which is not well supported yet) - please check out the example code and follow README to see how it works.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/ljk53"">@ljk53</a>,<br/><NewLine>Thank you for your help.</p><NewLine><ul><NewLine><li>I was able to run my model using the JAVA example.</li><NewLine><li>I was able to execute the “pytorch-android-cpp-demo” on my Android device. Now I’m trying to use the generated binaries in my cpp android project.</li><NewLine></ul><NewLine><p>Can you please tell me what can be done to compile the binaries for arm64? I would like to get .so files and not static libraries.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Aviais; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ljk53; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Aviais; <NewLine> ,"REPLY_DATE 1: May 13, 2020,  7:11am; <NewLine> REPLY_DATE 2: June 24, 2020,  2:30pm; <NewLine> REPLY_DATE 3: June 24, 2020,  2:27pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
86415,Crash when running my TorchScript model from JAVA,2020-06-22T14:36:45.125Z,1,108,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I’m trying to run my TorchScript model in JAVA, but I get an error and my application crashes. I based on the code in the “HelloWorld” example application, which worked fine on my Android device.</p><NewLine><p><strong>code lines:</strong><br/><NewLine>super.onCreate(savedInstanceState);<br/><NewLine>setContentView(R.layout.activity_main);</p><NewLine><pre><code>Bitmap bitmap = null;<NewLine>Module module = null;<NewLine>try {<NewLine>  // creating bitmap from packaged into app android asset 'image.jpg',<NewLine>  Log.d(""PytorchHelloWorld"", ""Loading input image"");<NewLine>  bitmap = BitmapFactory.decodeStream(getAssets().open(""image2.jpg""));<NewLine><NewLine>  // loading serialized torchscript module from packaged into app android asset model.pt,<NewLine>  // app/src/model/assets/model.pt<NewLine>  Log.d(""PytorchHelloWorld"", ""Loading pytorch model"");<NewLine>  module = Module.load(assetFilePath(this, ""torchScript_model.pt""));<NewLine>} catch (IOException e) {<NewLine>  Log.e(""PytorchHelloWorld"", ""Error reading assets"", e);<NewLine>  finish();<NewLine>}<NewLine><NewLine>// showing image on UI<NewLine>ImageView imageView = findViewById(R.id.image);<NewLine>imageView.setImageBitmap(bitmap);<NewLine><NewLine>// preparing input tensor<NewLine>Log.d(""PytorchHelloWorld"", ""Preparing input tensor"");<NewLine>final Tensor inputTensor = TensorImageUtils.bitmapToFloat32Tensor(bitmap,<NewLine>    TensorImageUtils.TORCHVISION_NORM_MEAN_RGB, TensorImageUtils.TORCHVISION_NORM_STD_RGB);<NewLine><NewLine>// running the model<NewLine>Log.d(""PytorchHelloWorld"", ""Running the model"");<NewLine>final Tensor outputTensor = module.forward(IValue.from(inputTensor)).toTensor();<NewLine>TextView textView = findViewById(R.id.text);<NewLine>textView.setText(""ok"");<NewLine></code></pre><NewLine><p>}</p><NewLine><p><strong>logs</strong><br/><NewLine>PytorchHelloWorld:	 Loading input image	<br/><NewLine>PytorchHelloWorld:	 Loading pytorch model	<br/><NewLine>AndroidRuntime:	 Shutting down VM	<br/><NewLine>AndroidRuntime:	 FATAL EXCEPTION: main	<br/><NewLine>AndroidRuntime:	 Process: org.pytorch.helloworld, PID: 8625	<br/><NewLine>AndroidRuntime:	 java.lang.RuntimeException: Unable to start activity ComponentInfo{org.pytorch.helloworld/org.pytorch.helloworld.MainActivity}: com.facebook.jni.CppException: 	<br/><NewLine>AndroidRuntime:	 Arguments for call are not valid.	<br/><NewLine>AndroidRuntime:	 The following variants are available:	<br/><NewLine>AndroidRuntime:		<br/><NewLine>AndroidRuntime:	   aten::upsample_bilinear2d(Tensor self, int[2] output_size, bool align_corners) -&gt; (Tensor):	<br/><NewLine>AndroidRuntime:	   Expected at most 3 arguments but found 5 positional arguments.	<br/><NewLine>AndroidRuntime:		<br/><NewLine>AndroidRuntime:	   aten::upsample_bilinear2d.out(Tensor self, int[2] output_size, bool align_corners, *, Tensor(a!) out) -&gt; (Tensor(a!)):	<br/><NewLine>AndroidRuntime:	   Argument out not provided.</p><NewLine><p>Can you please help me understand what’s the issue?</p><NewLine></div>",https://discuss.pytorch.org/u/Aviais,(avirzu),Aviais,"June 22, 2020,  2:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, <a class=""mention"" href=""/u/aviais"">@Aviais</a><br/><NewLine>Which version of pytorch did you use to get your model.pt file?<br/><NewLine>(To check the version:  <code>python -c ""import torch; print(torch.version.__version)</code> )</p><NewLine><p>Which version of gradle dependencies do you use?</p><NewLine><p>It could be that the model was traced/scripted with different version from gradle dependencies, while between those versions aten::upsample_bilinear2d.out maybe changed.</p><NewLine><p>If you trace/script with nightly build - its better to use android dependencies from nightly channel (<a href=""https://github.com/pytorch/pytorch/tree/master/android#nightly"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/tree/master/android#nightly</a>)</p><NewLine><p>And matching version of gradle dependencies and installed pytorch.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""86415"" data-username=""IvanKobzarev""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ivankobzarev/40/19031_2.png"" width=""20""/> IvanKobzarev:</div><NewLine><blockquote><NewLine><p>torch.version.__version</p><NewLine></blockquote><NewLine></aside><NewLine><p>Hi <a class=""mention"" href=""/u/ivankobzarev"">@IvanKobzarev</a>,<br/><NewLine>Thank you! You solved my problem!<br/><NewLine>I used PyTorch 1.5.0 to get my ‘model.pt’ and gradle dependencies of 1.4.0. When I changed my dependencies to 1.5.0, the issues was solved!</p><NewLine><p>Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Aviais; <NewLine> ,"REPLY_DATE 1: June 24, 2020,  6:42am; <NewLine> REPLY_DATE 2: June 24, 2020,  6:41am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
85198,"Torch Mobile android app is huge (90mb apk, but 30mb download size)",2020-06-12T11:41:59.811Z,0,103,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve used a torchvision for a mobile image classification app but it is absolutely massive for what it does.<br/><NewLine>The apk itself is 90mb, but the download size is 30mb. What’s causing this massive disparity in size? Is there any way for me to reduce this?</p><NewLine></div>",https://discuss.pytorch.org/u/aditya-hari,(Aditya Hari),aditya-hari,"June 12, 2020, 11:41am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>For pytorch-android 1.5.0 size of the aar is 25mb,<br/><NewLine><a class=""onebox"" href=""https://bintray.com/pytorch/maven/org.pytorch%3Apytorch_android/1.5.0#files/org%2Fpytorch%2Fpytorch_android%2F1.5.0"" rel=""nofollow noopener"" target=""_blank"">https://bintray.com/pytorch/maven/org.pytorch%3Apytorch_android/1.5.0#files/org%2Fpytorch%2Fpytorch_android%2F1.5.0</a></p><NewLine><p>Nightly builds are a bit bigger.</p><NewLine><p>What is the size of your vision model that you pack in application?</p><NewLine><p>(As an idea to have smaller model you may try to use quantized model or models with smaller size like mobileNetV2 or download model on the device at runtime)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> ,"REPLY_DATE 1: June 23, 2020, 12:32am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
81390,Forwarding a list of tensors,2020-05-15T14:20:04.458Z,2,228,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I’m testing pytorch mobile capabilities and when I try to forward a single tensors there’s no problem.<br/><NewLine>But trying to pass a list of tensors is completely different.<br/><NewLine>I declared and defined an array of tensors in which I passed the images I want to forward to the module.<br/><NewLine>Then I convert that to a list of tensors with this: “List i = Arrays.asList(mInputTensor);”.<br/><NewLine>Because I would like to pass the entire list to the forward method, then I do: “final Tensor[] o = module.forward(IValue.listFrom(i)).toTensorList();”.<br/><NewLine>But this expression gives me errors, I can’t understand where is the problem, could anyone help me with this?<br/><NewLine>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/simoloca,(simooo),simoloca,"May 15, 2020,  2:20pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could anyone help me?<br/><NewLine>I just want to know if I can put as input an array of tensors to the forward method, and if yes how.<br/><NewLine>Thanks</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/simoloca"">@simoloca</a> I face the same problems with you. I want to feed a network with List[Tensor] using c++.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/simoloca"">@simoloca</a>, <a class=""mention"" href=""/u/dragen"">@dragen</a><br/><NewLine>Sorry for delay with reply.</p><NewLine><p>List[Tensor] is a separate type than list of IValue that is Tensor inside.<br/><NewLine>Make sure that overloaded  <code>IValue.listFrom</code>  goes to  <code>IValue.listFrom(Tensor... list)</code> , not to  <code>IValue.listFrom(IValue... array)</code> .</p><NewLine><p>I created PR for example where I call scripted method that accepts  <code>List[Tensor]</code> :<br/><NewLine></p><aside class=""onebox githubpullrequest""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/40408/files"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Pull Request""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 12 16"" width=""60""><path d=""M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/pull/40408"" rel=""nofollow noopener"" target=""_blank"">[NOT_FOR_LAND] Test android api with torchscript methos arg List[Tensor]</a><NewLine></h4><NewLine><div class=""branches""><NewLine><code>pytorch:gh/IvanKobzarev/47/base</code> ← <code>pytorch:gh/IvanKobzarev/47/head</code><NewLine></div><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-06-23"" data-format=""ll"" data-time=""00:08:21"" data-timezone=""UTC"">12:08AM - 23 Jun 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/IvanKobzarev"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""IvanKobzarev"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars3.githubusercontent.com/u/6638825?v=4"" width=""20""/><NewLine>          IvanKobzarev<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""2 commits changed 6 files with 60 additions and 9 deletions""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/40408/files"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+60</span><NewLine><span class=""removed"">-9</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine>print(torch.version.__version__)<NewLine><NewLine>def scriptAndSave(module, fileName):<NewLine>    print('-' * 80)<NewLine>    script_module = torch.jit.script(module)<NewLine>    print(script_module.graph)<NewLine>    outputFileName = fileName<NewLine>    script_module.save(outputFileName)<NewLine>    print(""Saved to "" + outputFileName)<NewLine>    print('=' * 80)<NewLine><NewLine>class Test(torch.jit.ScriptModule):<NewLine>    def __init__(self):<NewLine>        super(Test, self).__init__()<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def forward(self, input):<NewLine>        return None<NewLine><NewLine>    @torch.jit.script_method<NewLine>    def sumTensorsList(self, input):<NewLine>        # type: (List[Tensor]) -&gt; Tensor<NewLine>        sum = torch.zeros_like(input[0])<NewLine>        for x in input:<NewLine>            sum += x<NewLine>        return sum<NewLine>scriptAndSave(Test(), ""sum_tensors_list.pt"")<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">  private Tensor makeTensor(final long[] shape, float value) {<NewLine>    long numElements = 1;<NewLine>    for (int i = 0; i &lt; shape.length; i++) {<NewLine>      numElements *= shape[i];<NewLine>    }<NewLine>    FloatBuffer buffer = Tensor.allocateFloatBuffer((int) numElements);<NewLine>    for(int i = 0; i &lt; numElements; i++) {<NewLine>      buffer.put(i, value);<NewLine>    }<NewLine>    return Tensor.fromBlob(buffer, shape);<NewLine>  }<NewLine><NewLine>  final long[] shape = new long[] {2, 2};<NewLine><NewLine>    IValue ivalue = IValue.listFrom(<NewLine>      makeTensor(shape, 1),<NewLine>      makeTensor(shape, 2),<NewLine>      makeTensor(shape, 3));<NewLine><NewLine>    final Tensor outputTensor = mModule.runMethod(""sumTensorsList"", ivalue).toTensor();<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/simoloca; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dragen; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> ,"REPLY_DATE 1: May 16, 2020, 10:33am; <NewLine> REPLY_DATE 2: June 9, 2020,  8:16am; <NewLine> REPLY_DATE 3: June 23, 2020, 12:23am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
78872,[JIT] [Mobile] Op may not exist in TorchScript,2020-04-28T20:33:32.857Z,1,142,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone!</p><NewLine><p>Once again I’ve run into something I don’t understand, need your help.</p><NewLine><p>I’ve compiled <code>speed_benchmark_torch</code> for mobile to test my TorchScript RCNN model.</p><NewLine><p>The model is from detectron2. I’ve slightly varied caffe2 conversion of RCNN to use <code>torch.jit.trace</code> instead <code>torch.onnx.export</code> <a href=""https://github.com/facebookresearch/detectron2/blob/master/tools/deploy/caffe2_converter.py"" rel=""nofollow noopener"">https://github.com/facebookresearch/detectron2/blob/master/tools/deploy/caffe2_converter.py</a><br/><NewLine>That gives a valid model to run on PC but on Mobile there is an issue.</p><NewLine><pre><code class=""lang-auto"">Unknown builtin op: _caffe2::GenerateProposals.<NewLine>Could not find any similar ops to _caffe2::GenerateProposals. This op may not exist or may not be currently supported in TorchScript.<NewLine>...<NewLine>Serialized   File ""code/__torch__/detectron2/export/c10.py"", line 22<NewLine>    scores = torch.detach(_6)<NewLine>    bbox_deltas = torch.detach(_7)<NewLine>    _16, _17 = ops._caffe2.GenerateProposals(scores, bbox_deltas, im_info, _4, 0.25, 1000, 100, 0.69999999999999996, 0., True, -180, 180, 1., False, None)<NewLine>               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>    scores0 = torch.detach(_8)<NewLine>    bbox_deltas0 = torch.detach(_9)<NewLine></code></pre><NewLine><p>The question is possible to un TorchScript model with caffe2 ops or we should use old <code>speed_benchmark</code> for caffe2 models?<br/><NewLine>Why there are caffe2 related options in <code>./speed_benchmark_torch --help</code>?</p><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"April 29, 2020,  3:32pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/zetyquickly"">@zetyquickly</a>, I was curios if you every found a solution to your problem. I am currently running into basically the same error trying to deploy a model to ios.  Any tips would be greatly appreciated thanks!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, <a class=""mention"" href=""/u/kyle_armstrong"">@Kyle_Armstrong</a></p><NewLine><p>I’m afraid that you won’t be able to use PyTorch backend to run Caffe2 operations on mobile. If you are interested in deploy of Caffe2 models you should build a Caffe2 .pb model files and then run it with Caffe2 backend on mobile, e.g. using <code>speed_benchmark</code> (not <code>speed_benchmark_torch</code>)<br/><NewLine>To run models using only PyTorch backend your TorchScript model must contain only PyTorch operators and only supported on with mobile PyTorch (e.g. you’re unable to run <code>torchvision::nms</code> op on mobile, at least that was the case recently)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Kyle_Armstrong; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/zetyquickly; <NewLine> ,"REPLY_DATE 1: June 22, 2020,  5:42pm; <NewLine> REPLY_DATE 2: June 22, 2020,  8:14pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
86135,Does Pytorch support on-device training?,2020-06-20T01:55:32.910Z,0,89,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am wondering is there any way we can perform on-device (e.g., Android) training using Pytorch or libtorch.</p><NewLine></div>",https://discuss.pytorch.org/u/ANG_LI,(ANG LI),ANG_LI,"June 20, 2020,  1:55am",,,,,
85277,PyTorch Android cannot load model using trace_script module,2020-06-12T23:43:04.637Z,0,212,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I was following this example: <a href=""https://pytorch.org/mobile/android/"" rel=""nofollow noopener"">https://pytorch.org/mobile/android/</a> except that I am using FastRCNN net with FASTRCNNPredictor. (source code from trace_model.py below)</p><NewLine><p>The first problem was that the output of FastRCNN is a list of dictionaries of tensors and torch.jit.trace takes output only as a list of tensors or a tuple of tensors. But that’s okay. I’ve tracked the error and looked into <strong>generilized_rcnn.py</strong> and changed the method <strong>eager_outputs</strong> in:</p><NewLine><pre><code class=""lang-auto"">def forward(self, images, targets=None):<NewLine>        # type: (List[Tensor], Optional[List[Dict[str, Tensor]]])<NewLine>        """"""<NewLine>        Arguments:<NewLine>            images (list[Tensor]): images to be processed<NewLine>            targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional)<NewLine><NewLine>        Returns:<NewLine>            result (list[BoxList] or dict[Tensor]): the output from the model.<NewLine>                During training, it returns a dict[Tensor] which contains the losses.<NewLine>                During testing, it returns list[BoxList] contains additional fields<NewLine>                like `scores`, `labels` and `mask` (for Mask R-CNN models).<NewLine><NewLine>        """"""<NewLine>        if self.training and targets is None:<NewLine>            raise ValueError(""In training mode, targets should be passed"")<NewLine>        original_image_sizes = torch.jit.annotate(List[Tuple[int, int]], [])<NewLine>        for img in images:<NewLine>            val = img.shape[-2:]<NewLine>            assert len(val) == 2<NewLine>            original_image_sizes.append((val[0], val[1]))<NewLine><NewLine>        images, targets = self.transform(images, targets)<NewLine>        features = self.backbone(images.tensors)<NewLine>        if isinstance(features, torch.Tensor):<NewLine>            features = OrderedDict([('0', features)])<NewLine>        proposals, proposal_losses = self.rpn(images, features, targets)<NewLine>        detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)<NewLine>        detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)<NewLine><NewLine>        losses = {}<NewLine>        losses.update(detector_losses)<NewLine>        losses.update(proposal_losses)<NewLine><NewLine>        if torch.jit.is_scripting():<NewLine>            if not self._has_warned:<NewLine>                warnings.warn(""RCNN always returns a (Losses, Detections) tuple in scripting"")<NewLine>                self._has_warned = True<NewLine>            return (losses, detections)<NewLine>        else:<NewLine>            return self.eager_outputs(losses, detections)<NewLine></code></pre><NewLine><p>from</p><NewLine><pre><code class=""lang-auto"">def eager_outputs(self, losses, detections):<NewLine>        # type: (Dict[str, Tensor], List[Dict[str, Tensor]]) -&gt; Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]<NewLine>        if self.training:<NewLine>            return losses<NewLine><NewLine>        return detections<NewLine></code></pre><NewLine><p>to</p><NewLine><pre><code class=""lang-auto""> def eager_outputs(self, losses, detections):<NewLine>        # type: (Dict[str, Tensor], List[Dict[str, Tensor]]) -&gt; Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]<NewLine>        if self.training:<NewLine>            return losses<NewLine>        tensor_output = list()<NewLine>        <NewLine>        for dictionary in detections:<NewLine>            for key in dictionary:<NewLine>                tensor_output.append(dictionary[key])<NewLine>        <NewLine>        <NewLine>            return tensor_output<NewLine></code></pre><NewLine><p>and I was able to run trace_model.py to generate .pt file.</p><NewLine><p>Then I used the HelloWorldApp to check out if everything’s compiling and if I can load the model in java. So I just changed the model.pt to my model generated from trace_model.py on line 42 and I’ve runned this application: <a href=""https://github.com/pytorch/android-demo-app/tree/master/HelloWorldApp/app/src/main/java/org/pytorch/helloworld"" rel=""nofollow noopener"">https://github.com/pytorch/android-demo-app/tree/master/HelloWorldApp/app/src/main/java/org/pytorch/helloworld</a>.</p><NewLine><p>Gradle build was successful yet I keep getting errors that I can’t cope with. I am almost sure the problem is with the <strong>module = Module.load(assetFilePath(this, “model_actual.pt”));</strong> part since I’ve commented the rest and it was fine.</p><NewLine><pre><code class=""lang-auto"">2020-06-13 01:16:33.541 30594-30594/org.pytorch.helloworld E/AndroidRuntime: FATAL EXCEPTION: main<NewLine>    Process: org.pytorch.helloworld, PID: 30594<NewLine>    java.lang.RuntimeException: Unable to start activity ComponentInfo{org.pytorch.helloworld/org.pytorch.helloworld.MainActivity}: com.facebook.jni.CppException: <NewLine>    Arguments for call are not valid.<NewLine>    The following variants are available:<NewLine>      <NewLine>      aten::upsample_bilinear2d(Tensor self, int[2] output_size, bool align_corners) -&gt; (Tensor):<NewLine>      Expected at most 3 arguments but found 5 positional arguments.<NewLine>      <NewLine>      aten::upsample_bilinear2d.out(Tensor self, int[2] output_size, bool align_corners, *, Tensor(a!) out) -&gt; (Tensor(a!)):<NewLine>      Argument out not provided.<NewLine>    <NewLine>    The original call is:<NewLine>    C:\Users\patry\AppData\Roaming\Python\Python37\site-packages\torch\nn\functional.py(3013): interpolate<NewLine>    C:\Users\patry\AppData\Roaming\Python\Python37\site-packages\torchvision\models\detection\transform.py(92): resize<NewLine>    C:\Users\patry\AppData\Roaming\Python\Python37\site-packages\torchvision\models\detection\transform.py(45): forward<NewLine>    C:\Users\patry\AppData\Roaming\Python\Python37\site-packages\torch\nn\modules\module.py(534): _slow_forward<NewLine>    C:\Users\patry\AppData\Roaming\Python\Python37\site-packages\torch\nn\modules\module.py(548): __call__<NewLine>    C:\Users\patry\AppData\Roaming\Python\Python37\site-packages\torchvision\models\detection\generalized_rcnn.py(72): forward<NewLine>    C:\Users\patry\AppData\Roaming\Python\Python37\site-packages\torch\nn\modules\module.py(534): _slow_forward<NewLine>    C:\Users\patry\AppData\Roaming\Python\Python37\site-packages\torch\nn\modules\module.py(548): __call__<NewLine>    C:\Users\patry\AppData\Roaming\Python\Python37\site-packages\torch\jit\__init__.py(1027): trace_module<NewLine>    C:\Users\patry\AppData\Roaming\Python\Python37\site-packages\torch\jit\__init__.py(875): trace<NewLine>    d:\AndroidREPO\HelloWorldApp\android-demo-app\HelloWorldApp\trace_model.py(17): &lt;module&gt;<NewLine>    Serialized   File ""code/__torch__/torchvision/models/detection/transform.py"", line 24<NewLine>        _11 = torch.mul(torch.to(_9, 6, False, False, None), torch.detach(_10))<NewLine>        _12 = torch.to(_11, 6, False, False, None)<NewLine>        _13 = torch.upsample_bilinear2d(input, [int(_8), int(torch.floor(_12))], False, None, None)**<NewLine>              ~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE**<NewLine>        img = torch.select(_13, 0, 0)**<NewLine>        height = ops.prim.NumToTensor(torch.size(img, 1))**<NewLine>    <NewLine>        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2946)<NewLine>        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3081)<NewLine>        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:78)<NewLine>        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:108)<NewLine>        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:68)<NewLine>        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1831)<NewLine>        at android.os.Handler.dispatchMessage(Handler.java:106)<NewLine>        at android.os.Looper.loop(Looper.java:201)<NewLine>        at android.app.ActivityThread.main(ActivityThread.java:6810)<NewLine>        at java.lang.reflect.Method.invoke(Native Method)<NewLine>        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:547)<NewLine>        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:873)<NewLine>     Caused by: com.facebook.jni.CppException: <NewLine>    Arguments for call are not valid.<NewLine>    The following variants are available:<NewLine>      <NewLine>      aten::upsample_bilinear2d(Tensor self, int[2] output_size, bool align_corners) -&gt; (Tensor):<NewLine>      Expected at most 3 arguments but found 5 positional arguments.<NewLine>      <NewLine>      aten::upsample_bilinear2d.out(Tensor self, int[2] output_size, bool align_corners, *, Tensor(a!) out) -&gt; (Tensor(a!)):**<NewLine>      Argument out not provided.**<NewLine></code></pre><NewLine><p>And my trace_model.py code:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torchvision<NewLine>from torchvision.models.detection.faster_rcnn import FastRCNNPredictor<NewLine><NewLine>model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)<NewLine><NewLine>example = torch.rand(1, 3, 800, 800)<NewLine>in_features = model.roi_heads.box_predictor.cls_score.in_features<NewLine><NewLine>model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)<NewLine><NewLine>model.eval()<NewLine>PATH = 'model_weights.pth'<NewLine>model.load_state_dict(torch.load(PATH))<NewLine><NewLine>traced_script_module = torch.jit.trace(model, example, check_trace = False)<NewLine><NewLine>traced_script_module.save(""app/src/main/assets/model_actual.pt"")<NewLine><NewLine></code></pre><NewLine><p>I would be very grateful if anyone gave me a hint on what could cause a trouble here. I’ve went through hundreds of PyTorch’s code but I couldn’t find anything. I’m still a beginner at programming. Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/Patric,(Patryk),Patric,"June 13, 2020, 12:01am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!</p><NewLine><p>I see that you have <code>check_trace = False</code>. There is a reason for that? Also, did you check traced module in pytorch? As it returns a different number of bboxes and trace doesn’t support control flow, it may have compiled another <code>upsample_bilinear2d</code> method. You can mix <a href=""https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html"" rel=""nofollow noopener"">trace and scripting</a></p><NewLine><p>Finally, if you face an error in pyotrch mobile, make sure that the exact <strong>same</strong> image runs well on the traced/scripted model in python to pinpoint where the bug reside (in your code, in torchscript, in  mobile, etc).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vferrer; <NewLine> ,"REPLY_DATE 1: June 16, 2020, 10:25am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
84552,&lsquo;Undefined symbols for architecture x86_64&rsquo; for libtorch in swift unit test,2020-06-07T18:23:02.985Z,3,215,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When trying to build my tests, I exclusively run into errors such as the following:</p><NewLine><pre><code class=""lang-auto"">Undefined symbols for architecture x86_64:<NewLine>  ""vtable for c10::AutogradMetaInterface"", referenced from:<NewLine>      c10::AutogradMetaInterface::AutogradMetaInterface() in TorchModule.o<NewLine></code></pre><NewLine><p>None of my fixes have worked so far. I have created an objective-c bridging header for my tests, and here is my Podfile:</p><NewLine><pre><code class=""lang-auto""># Uncomment the next line to define a global platform for your project<NewLine># platform :ios, '9.0'<NewLine><NewLine>target 'FinalDancer' do<NewLine>  # Comment the next line if you don't want to use dynamic frameworks<NewLine>  use_frameworks!<NewLine><NewLine>  # Pods for FinalDancer<NewLine>  pod 'LibTorch', '~&gt; 1.4.0'<NewLine><NewLine>  target 'FinalDancerTests' do<NewLine>    inherit! :search_paths<NewLine>    # Pods for testing<NewLine>    pod 'LibTorch', '~&gt; 1.4.0'<NewLine>  end<NewLine><NewLine>end<NewLine></code></pre><NewLine><p>Hope this helps, thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/Bryan_Wang,(Bryan Wang),Bryan_Wang,"June 7, 2020,  6:23pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>For some reason, my xcconfig for test didn’t have the following line:<br/><NewLine><code>LIBRARY_SEARCH_PATHS = $(inherited) ""${PODS_ROOT}/LibTorch/install/lib""</code><br/><NewLine>It was also missing some OTHER_LDFLAGS like pytorch_qnnpack.</p><NewLine><p>Not really sure why or how this fixed it, but I pasted those lines in. There is some code duplication that my compiler is warning me of but my tests are running.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/bryan_wang"">@Bryan_Wang</a> has your issue been resolved?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/xta0"">@xta0</a>,</p><NewLine><p>The issue hasn’t been resolved, unfortunately. Instead, there is a runtime error</p><NewLine><pre><code class=""lang-auto"">Unsupported TypeMeta in ATen: float (please report this error)<NewLine></code></pre><NewLine><p>I suspect that it’s this issue: <a href=""https://gitmemory.com/issue/pytorch/pytorch/25489/526862641"" rel=""nofollow noopener"">https://gitmemory.com/issue/pytorch/pytorch/25489/526862641</a>, but I’m not sure how to fix it.</p><NewLine><p>I tried looking for examples of unit tests in the Helloworld project, but there don’t seem to be any examples I can pull from. Any help would be appreciated!</p><NewLine><p>Thanks,<br/><NewLine>Bryan</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Seems like it’s duplicated with <a href=""https://github.com/pytorch/pytorch/issues/32040"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/32040</a>. I can come up with a PR to disable NNPACK, but not sure if it’ll fix your issue. Anyway, I’ll give an update here once I have the PR.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/xta0"">@xta0</a> Seems like the exact issue I’m experiencing. Thank you!</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/bryan_wang"">@Bryan_Wang</a> can you try to apply this PR?  <a href=""https://github.com/pytorch/pytorch/pull/39868"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/39868</a>? Let me know if the issue still exist. Thanks</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ll let you know what happens!</p><NewLine><p>Edit: Actually, how do I apply this PR to my XCode project? I am quite new to this. Or if you would like me to test on a sample project, I can do that as well.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Bryan_Wang; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Bryan_Wang; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Bryan_Wang; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Bryan_Wang; <NewLine> ,"REPLY_DATE 1: June 7, 2020,  7:06pm; <NewLine> REPLY_DATE 2: June 7, 2020, 11:53pm; <NewLine> REPLY_DATE 3: June 10, 2020,  3:43am; <NewLine> REPLY_DATE 4: June 10, 2020,  9:19am; <NewLine> REPLY_DATE 5: June 11, 2020,  4:40am; <NewLine> REPLY_DATE 6: June 11, 2020,  6:40pm; <NewLine> REPLY_DATE 7: June 12, 2020,  1:14am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
63098,Pytorch mobile object detection example,2019-12-05T07:46:28.959Z,3,792,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone, Is there some example for object detection model yet that deployed on android?</p><NewLine></div>",https://discuss.pytorch.org/u/Kiki_Rizki_Arpiandi,(Kiki Rizki Arpiandi),Kiki_Rizki_Arpiandi,"December 5, 2019,  7:46am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The demo app has a classification example, but we have not posted any detection demos yet.  Stay tuned!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Glad to hear that. Hope it will arrive soon enough</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Any updates on this?<br/><NewLine>Need to deploy on Android and main option currently is TFLite, which means working in TF…really hoping PyTorch will be a competitive option soon.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I didn’t deploy any object detection model with Pytorch but from docs and app code, Pytorch Mobile should work fine with these type of models. The only limitations that I’m aware are the missing support for nms and roi_align torchivision operations (see <a href=""https://discuss.pytorch.org/t/torchvision-android-torchvision-ops-when-will-they-be-available-on-mobile/61360"">post</a>). I think that TFLite doesn’t support NMS. So, a mobilenet + ssd/ssdlite head or similar  should work fine.</p><NewLine><p>That said,  It would be nice to have some reference implementation. At least, to view the predicted bbox.</p><NewLine><p>If you are worried about pytorch performance, you may want to take a look into this <a href=""https://github.com/pytorch/android-demo-app/issues/71"" rel=""nofollow noopener"">issue</a>. I found that Hello World App performance to be much worse than Pytorch Demo App. Demo App runs nearly as fast as TFLite under the same conditions without any hassle <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/></p><NewLine><p>Finally, last month I was searching for mobile object detection models and I found DiceNet. It’s interesting and researchers have used Pytorch.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/vferrer"">@vferrer</a> for the info!</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""63098"" data-username=""vferrer""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/v/c77e96/40.png"" width=""20""/> vferrer:</div><NewLine><blockquote><NewLine><p>Finally, last month I was searching for mobile object detection models and I found DiceNet. It’s interesting and researchers have used Pytorch.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I tried to quantize the model but fail(details write in this <a href=""https://discuss.pytorch.org/t/the-expanded-size-of-the-tensor-must-match-the-existing-size-at-non-singleton-dimension/82566/2"">post</a>), could you quantize it?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://github.com/qfgaohao/pytorch-ssd"" rel=""nofollow noopener"">https://github.com/qfgaohao/pytorch-ssd</a> this is working for me i could export it with torchscript and got it loading into android using the pytorch demo apps, it fails when i try to interpret the output.<br/><NewLine>Would be good to have an android demo with bboxes.</p><NewLine><p>failing to get the Tensor output which is 2D into a java array without flattening it</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Kiki_Rizki_Arpiandi; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/lessw2020; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/vferrer; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/lessw2020; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ngap_wei_Tham; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/neumann; <NewLine> ,"REPLY_DATE 1: December 9, 2019,  9:50pm; <NewLine> REPLY_DATE 2: December 10, 2019,  3:27am; <NewLine> REPLY_DATE 3: May 5, 2020,  6:58pm; <NewLine> REPLY_DATE 4: May 6, 2020,  7:19am; <NewLine> REPLY_DATE 5: May 7, 2020,  9:32pm; <NewLine> REPLY_DATE 6: May 26, 2020,  5:58am; <NewLine> REPLY_DATE 7: June 11, 2020,  1:46pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 2 Likes; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> 
85006,TorchScript traced model returns inconsistent output tensors on each run,2020-06-11T07:24:51.015Z,0,94,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a traced model that works correctly on Python and C++. However, when i try loading it in Android, I’m getting inconsistent output tensors on every run. I did call <strong>model.eval()</strong> before tracing the model, plus the same .pt file is working as intended on Python and C++.</p><NewLine><p>I tried loading <a href=""https://pytorch.org/docs/stable/torchvision/models.html"" rel=""nofollow noopener"">these pretrained models</a> in Android and they are returning consistent output tensors on each run though. I would appreciate if anyone could provide me some insights on why my traced model is having such inconsistent output on every run. <img alt="":bowing_man:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/bowing_man.png?v=9"" title="":bowing_man:""/></p><NewLine><p>Considering <a class=""mention"" href=""/u/david_reiss"">@David_Reiss</a>’s reply in this <a href=""https://discuss.pytorch.org/t/pytorch-mobile-ios-resnet50-not-computing/61014/11"">issue</a>, could this behavior be a bug?</p><NewLine><p>Here’s the layers to my traced model</p><NewLine><pre><code class=""lang-auto"">Backbone(<NewLine>  original_name=Backbone<NewLine>  (input_layer): Sequential(<NewLine>    original_name=Sequential<NewLine>    (0): Conv2d(original_name=Conv2d)<NewLine>    (1): BatchNorm2d(original_name=BatchNorm2d)<NewLine>    (2): PReLU(original_name=PReLU)<NewLine>  )<NewLine>  (output_layer): Sequential(<NewLine>    original_name=Sequential<NewLine>    (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>    (1): Dropout(original_name=Dropout)<NewLine>    (2): Flatten(original_name=Flatten)<NewLine>    (3): Linear(original_name=Linear)<NewLine>    (4): BatchNorm1d(original_name=BatchNorm1d)<NewLine>  )<NewLine>  (body): Sequential(<NewLine>    original_name=Sequential<NewLine>    (0): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (1): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (2): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (3): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): Conv2d(original_name=Conv2d)<NewLine>        (1): BatchNorm2d(original_name=BatchNorm2d)<NewLine>      )<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (4): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (5): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (6): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (7): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): Conv2d(original_name=Conv2d)<NewLine>        (1): BatchNorm2d(original_name=BatchNorm2d)<NewLine>      )<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (8): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (9): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (10): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (11): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (12): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (13): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (14): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (15): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (16): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (17): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (18): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (19): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (20): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (21): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): Conv2d(original_name=Conv2d)<NewLine>        (1): BatchNorm2d(original_name=BatchNorm2d)<NewLine>      )<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (22): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (23): bottleneck_IR_SE(<NewLine>      original_name=bottleneck_IR_SE<NewLine>      (shortcut_layer): MaxPool2d(original_name=MaxPool2d)<NewLine>      (res_layer): Sequential(<NewLine>        original_name=Sequential<NewLine>        (0): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (1): Conv2d(original_name=Conv2d)<NewLine>        (2): PReLU(original_name=PReLU)<NewLine>        (3): Conv2d(original_name=Conv2d)<NewLine>        (4): BatchNorm2d(original_name=BatchNorm2d)<NewLine>        (5): SEModule(<NewLine>          original_name=SEModule<NewLine>          (avg_pool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)<NewLine>          (fc1): Conv2d(original_name=Conv2d)<NewLine>          (relu): ReLU(original_name=ReLU)<NewLine>          (fc2): Conv2d(original_name=Conv2d)<NewLine>          (sigmoid): Sigmoid(original_name=Sigmoid)<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>  )<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/maxmx911,(Sean Ng),maxmx911,"June 11, 2020,  8:35am",,,,,
84781,Pytorch C++ with Java Native Interface (JNI) linking error,2020-06-09T15:04:26.985Z,0,116,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am running into a linking problem when running Torch with JNI. I am not sure if this is the right forum as it might not be directly related to torch but maybe someone might be able to help me:</p><NewLine><p>I have a java program Main.java that has native functions declared which are implemented in torchJNI.cpp and defined in torchJNI.h. Of course, for this to work I also load torchJNI with <code>System.loadLibrary(""torchJNI"");</code><br/><NewLine>This works when I create tensors in the native functions and print them etc. The problem comes, when I do more complex things (loss.backward() and optim.step()). In particular, functions that depend on the MKL shared library where the .so files are in /opt/intel/…/.<br/><NewLine>I implemented a main function in torchJNI.cpp which does training. When compiling and running the torchJNI.cpp file this all of this works perfectly fine. However, when I make a shared library out of torchJNI.cpp/torchJNI.h which gets loaded with JNI and4 I try to run the native method that “has the exact same code” as the main function I get the following error:</p><NewLine><pre><code class=""lang-auto"">INTEL MKL ERROR: /opt/intel/mkl/lib/intel64/libmkl_vml_avx2.so: undefined symbol: mkl_lapack_dspevd.    <NewLine>Intel MKL FATAL ERROR: cannot load libmkl_vml_avx2.so or libmkl_vml_def.so.<NewLine></code></pre><NewLine><p>now I though okay I have to do the same thing to load this shared library, so I added the following lines to the java file:</p><NewLine><pre><code class=""lang-auto""> System.loadLibrary(""mkl_vml_avx2"");     <NewLine> System.loadLibrary(""mkl_vml_def"");<NewLine></code></pre><NewLine><p>Now, when I try to run the java file I get:</p><NewLine><pre><code class=""lang-auto"">Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /opt/intel/mkl/lib/intel64/libmkl_vml_avx2.so: /opt/intel/mkl/lib/intel64/libmkl_vml_avx2.so: undefined symbol: mkl_lapack_dspevd ....<NewLine></code></pre><NewLine><p>note, that I run Java with -Djava.library.path=:./path/to/torchJNI/:/opt/intel/mkl/lib/intel64<br/><NewLine>I looked, and both libmkl_vml_avx2.so and libmkl_vml_def.so are present in /opt/intel/mkl/lib/intel64.</p><NewLine><p>To summarize:</p><NewLine><ol><NewLine><li><NewLine><p>Everything seems to work fine (compilation and runnning) as long as I stay completely in C++.</p><NewLine></li><NewLine><li><NewLine><p>When I invoke a native function from java that only does simple Torch things that do not require mkl everything also works.</p><NewLine></li><NewLine><li><NewLine><p>When I invoke a native function from java that uses the torch function backward (for example) I get an INTEL MKL ERROR … undefined symbol: mkl_lapack_dspevd</p><NewLine></li><NewLine><li><NewLine><p>Trying to fix 3. I tried loading  libmkl_vml_avx2.so or libmkl_vml_def.so. in Java manually then the error messages changes to a java exception.</p><NewLine></li><NewLine></ol><NewLine><p>Can anyone help me out or take a guess what the problem may be?</p><NewLine></div>",https://discuss.pytorch.org/u/jparsert,(Julian Parsert),jparsert,"June 9, 2020,  3:04pm",,,,,
83857,Slow inference with U-2-Net on iOS,2020-06-02T11:41:00.957Z,2,155,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m using <a href=""https://github.com/NathanUA/U-2-Net"" rel=""nofollow noopener"">U-2-Net</a> on mobile.<br/><NewLine>I converted the model to TorchScript, and used it succesfully on Android. The inference takes about 1 sec.<br/><NewLine>Now, if I use the same model on iOS, the inference is about half a minute. I disable autograd, and run the model in inference mode.<br/><NewLine>Any idea what could cause such a big performance difference using the same model?</p><NewLine><p>(I use 1.6.0 nightly build on Android, and 1.5.0 production build on iOS)</p><NewLine></div>",https://discuss.pytorch.org/u/wpmed92,(Ahmed Harmouche),wpmed92,"June 2, 2020, 11:53am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Android 1.6.0 nightly is using XNNPACK, whereas iOS 1.5.0 is not. We’ll be enabling XNNPACK on iOS in 1.6.0 release.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok, thank you! Meanwhile we managed to speed up the iOS inference. The thing is that on Android we scripted to model by calling jit.script(), and we tried to use this exact same model on iOS, which was slow. I tried another approach for converting to TorchScript by tracing the model’s execution with jit.trace, and for some reason the “traced model” performance on iOS was on par with the Android inference. Can it be also related to XNNPACK somehow?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Cool, it’s nice to hear the issue has been resolved. Short answer, no. The XNNPACK is our new computation kernels that has nothing to do with scripting. Yea, tracing is supposed to be faster.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/wpmed92; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/xta0; <NewLine> ,"REPLY_DATE 1: June 8, 2020,  6:28am; <NewLine> REPLY_DATE 2: June 8, 2020,  6:28am; <NewLine> REPLY_DATE 3: June 8, 2020,  6:40pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
81795,Use to use GPU if available on mobile,2020-05-18T10:54:05.517Z,0,140,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><p>I have neural network that I trained on my PC and then jit compiled and run inferences on my Android phone. I can see from Android Studio debugger it is processing using the mobile’s CPU. How can I check and use GPU if available on android mobile?</p><NewLine><p>Thanks in advance</p><NewLine></div>",https://discuss.pytorch.org/u/Bernicchi,(Dino Bernicchi),Bernicchi,"May 18, 2020, 10:54am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Unfortunately, mobile GPU is a completely different language and architecture to everything else. So we don’t support it yet.<br/><NewLine>It is work in progress though and you can see all the PRs working it here: <a href=""https://github.com/pytorch/pytorch/pulls?q=is%3Apr+is%3Aopen+vulkan"">https://github.com/pytorch/pytorch/pulls?q=is%3Apr+is%3Aopen+vulkan</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> ,"REPLY_DATE 1: May 31, 2020,  9:33pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
83229,Trouble getting started with pytorch mobile,2020-05-28T12:21:31.848Z,1,113,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was trying to make an android app for a model I trained and stumbled upon the hello world demo.<br/><NewLine>I followed the steps and exported my own model (resnet34), and am getting array index out of bounds error.</p><NewLine><pre><code class=""lang-auto"">Process: org.pytorch.helloworld, PID: 26163<NewLine>    java.lang.RuntimeException: Unable to start activity ComponentInfo{org.pytorch.helloworld/org.pytorch.helloworld.MainActivity}: java.lang.ArrayIndexOutOfBoundsException: length=2; index=751<NewLine>        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3121)<NewLine>        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3260)<NewLine>        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:78)<NewLine>        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:108)<NewLine>        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:68)<NewLine>        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1976)<NewLine>        at android.os.Handler.dispatchMessage(Handler.java:106)<NewLine>        at android.os.Looper.loop(Looper.java:193)<NewLine>        at android.app.ActivityThread.main(ActivityThread.java:6912)<NewLine>        at java.lang.reflect.Method.invoke(Native Method)<NewLine>        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493)<NewLine>        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:860)<NewLine>     Caused by: java.lang.ArrayIndexOutOfBoundsException: length=2; index=751<NewLine>        at org.pytorch.helloworld.MainActivity.onCreate(MainActivity.java:69)<NewLine>        at android.app.Activity.performCreate(Activity.java:7148)<NewLine>        at android.app.Activity.performCreate(Activity.java:7139)<NewLine>        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1293)<NewLine>        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3101)<NewLine>        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3260) <NewLine>        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:78) <NewLine>        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:108) <NewLine>        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:68) <NewLine>        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1976) <NewLine>        at android.os.Handler.dispatchMessage(Handler.java:106) <NewLine>        at android.os.Looper.loop(Looper.java:193) <NewLine>        at android.app.ActivityThread.main(ActivityThread.java:6912) <NewLine>        at java.lang.reflect.Method.invoke(Native Method) <NewLine>        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493) <NewLine>        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:860) <NewLine></code></pre><NewLine><p>All I did was change the model to my exported model and replaced the <code>IMAGENET_CLASSES</code> array to my array(It has just to elements in it).<br/><NewLine>If I want to create an android app for my model, what changes do I need to do in the code, and how to begin for a more complex application.</p><NewLine></div>",https://discuss.pytorch.org/u/Rohan_Sharma1,(Rohan Sharma),Rohan_Sharma1,"May 28, 2020, 12:21pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""83229"" data-username=""Rohan_Sharma1""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/rohan_sharma1/40/16483_2.png"" width=""20""/> Rohan_Sharma1:</div><NewLine><blockquote><NewLine><p><code>     Caused by: java.lang.ArrayIndexOutOfBoundsException: length=2; index=751</code></p><NewLine></blockquote><NewLine></aside><NewLine><p>I think you had 2 labels (java array has length 2) but you are trying to retrieve label 752 (<code>index=751</code>). So, I think that you aren’t using your custom model. You need to change the model you load. If I remember correctly, It’s in the main activity.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>For starters I was just changing the model and the IMAGENET_CLASSES array to check how my model performs and then later build my own app as per my requirenments.<br/><NewLine>I got that error, so I built my own app reffering to the demo and am still getting the same error.<br/><NewLine>So what changes do I need to do to resolve this issue?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vferrer; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Rohan_Sharma1; <NewLine> ,"REPLY_DATE 1: May 29, 2020,  6:48am; <NewLine> REPLY_DATE 2: May 30, 2020,  5:54am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
62731,Model Quantization Pytorch and use in android,2019-12-02T10:00:30.935Z,7,852,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have pytorch model saved as .pth file.<br/><NewLine>I wanted to use quantized model in my android project.<br/><NewLine>How can I achieve quantized .pt model ?<br/><NewLine>I tried some of the ways but I am not able to achieve it.<br/><NewLine>However I am done with just converting to .pt model and using it in android.</p><NewLine></div>",https://discuss.pytorch.org/u/mohit7,(Mohit Ranawat),mohit7,"December 2, 2019, 10:00am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/mohit7"">@mohit7</a>,</p><NewLine><p>Did you try to follow our quantization tutorial <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a> ?</p><NewLine><p>If yes, which of the steps does not work or need more details/comments?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ivankobzarev"">@IvanKobzarev</a> yes I followed the steps given but it throws me error.<br/><NewLine>My model is Resnet-34 base model on top of that I have some more functionality.<br/><NewLine>I am on Windows machine.</p><NewLine><p>Error</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Didn't find engine for operation quantized::conv_prepack NoQEngine (operator () at ..\aten\src\ATen\native\quantized\cpu\qconv_prepack.cpp:264)<NewLine>(no backtrace available)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>This looks like the same issue as <a href=""https://github.com/pytorch/pytorch/issues/29327"">https://github.com/pytorch/pytorch/issues/29327</a> .  In short, it looks like quantization is not currently supported on Windows.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/david_reiss"">@David_Reiss</a> so I moved from Windows to Linux for this quantization.<br/><NewLine>As I  know BatchNormalization support is still not there so we have to fuse the BatchNormalization layer.<br/><NewLine>I tried to fuse the Batchnormalization layer with Convolution layer but it is not happening nor throwing any error.<br/><NewLine>After I am doing jit trace it is throwing same error which means BatchNormalization need to be fused</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mohit7"">@mohit7</a>, can you post the error you’re getting in your latest version?</p><NewLine><p><a class=""mention"" href=""/u/supriyar"">@supriyar</a> or <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a>, do you know how to ensure that BN gets folded during quantization?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mohit7"">@mohit7</a>, it might be useful to take a look at the quantized models uploaded in torchvision.<br/><NewLine>Here is a link to the resnet model - <a href=""https://github.com/pytorch/vision/blob/master/torchvision/models/quantization/resnet.py"" rel=""nofollow noopener"">https://github.com/pytorch/vision/blob/master/torchvision/models/quantization/resnet.py</a><br/><NewLine>I think if you follow the same flow for your model by re-implementing <code>def fuse_model(self):</code>, it should work.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/david_reiss"">@David_Reiss</a>  I error I got is</p><NewLine><pre><code class=""lang-auto"">RuntimeError: No function is registered for schema aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -&gt; (Tensor, Tensor, Tensor) on tensor type QuantizedCPUTensorId; available functions are CPUTensorId, MkldnnCPUTensorId, VariableTensorId<NewLine></code></pre><NewLine><p>I have followed all the steps correctly but batch normalization is not fusing</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/supriyar"">@supriyar</a> Can you help?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mohit7"">@mohit7</a> did you modify the fuse_model function to work with your model (<a href=""https://github.com/pytorch/vision/blob/master/torchvision/models/quantization/resnet.py#L45"" rel=""nofollow noopener"">https://github.com/pytorch/vision/blob/master/torchvision/models/quantization/resnet.py#L45</a>)?</p><NewLine><p>Currently fusion is only supported for <code>conv + bn</code> or <code>conv + bn + relu</code>. Does your model have a use case other than that?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mohit7"">@mohit7</a>: Can you share the code for your model?  Batch norm fusion is supported , but you need to call it explicitly prior to calling prepare/convert to quantize your model.</p><NewLine><p>Thanks,</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>I followed the tutorial on a custom model it did make the model much smaller in size, however, the inference time increased heavily when testing it on android. Any clues as to why this is? Also, on top of that <code>FloatFunctional().add(x,y)</code> really slows down inference a lot.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/gigadeplex"">@gigadeplex</a>, the FloatFunctional().Add() is increasing time on inference because this function first convert the ongoing quantized tensor to float tensor, then addition and then converting back to quantized tensor.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mohit7; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mohit7; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mohit7; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mohit7; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/gigadeplex; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/mohit7; <NewLine> ,"REPLY_DATE 1: December 2, 2019,  9:47pm; <NewLine> REPLY_DATE 2: December 3, 2019,  4:02am; <NewLine> REPLY_DATE 3: December 3, 2019,  9:23pm; <NewLine> REPLY_DATE 4: December 4, 2019,  4:09am; <NewLine> REPLY_DATE 5: December 5, 2019,  1:01am; <NewLine> REPLY_DATE 6: December 5, 2019,  1:24am; <NewLine> REPLY_DATE 7: December 5, 2019,  5:25am; <NewLine> REPLY_DATE 8: December 9, 2019,  4:50am; <NewLine> REPLY_DATE 9: December 9, 2019, 10:02pm; <NewLine> REPLY_DATE 10: December 9, 2019, 10:50pm; <NewLine> REPLY_DATE 11: May 29, 2020,  1:46pm; <NewLine> REPLY_DATE 12: May 30, 2020,  2:09am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> 
70829,"(Java) Tensor.fromBlob(data, shape) how to define shape for 2D and more?",2020-02-24T08:15:30.275Z,1,269,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I have a more java related question but I’m stuck. In the examples on Android, we can read:</p><NewLine><blockquote><NewLine><p>Tensor input = Tensor.fromBlob(data, new long[]{1, data.length});</p><NewLine></blockquote><NewLine><p>In my case, I have an array:</p><NewLine><blockquote><NewLine><p>float[][] myArray = new float[150][7];</p><NewLine></blockquote><NewLine><p>And</p><NewLine><blockquote><NewLine><p>Tensor input = Tensor.fromBlob(myArray, new long[]{1, myArray.length});</p><NewLine></blockquote><NewLine><p>Doesn’t work.</p><NewLine></div>",https://discuss.pytorch.org/u/eranor,(eranor),eranor,"February 24, 2020,  8:15am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok I got it. Didn’t read properly the doc. I need to flatten my array and then give the proper tensor dimension.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you give a link to the documentation page that shows how to flatten the array? (I have the same problem)</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/venryx"">@Venryx</a> You have to do it yourself. If you use Kotlin, you have a method <a href=""https://kotlinlang.org/api/latest/jvm/stdlib/kotlin.collections/flatten.html"" rel=""nofollow noopener"">https://kotlinlang.org/api/latest/jvm/stdlib/kotlin.collections/flatten.html</a><br/><NewLine>If you use Java, you can do it recursively (<a href=""https://stackoverflow.com/questions/31851548/flatten-nested-arrays-in-java"" rel=""nofollow noopener"">https://stackoverflow.com/questions/31851548/flatten-nested-arrays-in-java</a> might help, I didn’t check the content).</p><NewLine><p>In my case, my model is expecting an array of float of dimension (n, b, m). So in kotlin:</p><NewLine><pre><code class=""lang-auto"">...<NewLine>val input: Tensor = Tensor.fromBlob(myFloatArray, longArrayOf(n, b, m))<NewLine>val output: Tensor = module.forward(IValue.from(input))!!.toTensor() <NewLine>...<NewLine></code></pre><NewLine><p>You can find the doc there: <a href=""https://pytorch.org/docs/1.3.0/org/pytorch/Tensor.html#id2"" rel=""nofollow noopener"">https://pytorch.org/docs/1.3.0/org/pytorch/Tensor.html#id2</a><br/><NewLine>Hope it helps.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/eranor; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Venryx; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/eranor; <NewLine> ,"REPLY_DATE 1: February 24, 2020,  9:17am; <NewLine> REPLY_DATE 2: May 24, 2020, 12:44pm; <NewLine> REPLY_DATE 3: May 26, 2020,  1:39pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
82156,[Caffe2] Setting XNNPACK engine for convolution operations,2020-05-20T09:54:03.687Z,0,130,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am using Caffe2 for mobile and I am trying to test the XNNPACK engine for convolutions. Until now I have been using NNPACK and it has resulted in some performance boost compared to the default engine, though, when I try to change the engine to XNNPACK, it seems that it switches to the default engine, because the performance becomes worse. Is there a way to use XNNPACK in caffe2 (built from pytorch v1.5) and if yes, how should it be done?</p><NewLine><p>Code:</p><NewLine><pre><code class=""lang-auto"">caffe2::NetDef modelDef;<NewLine>CAFFE_ENFORCE(caffe2::ReadProtoFromFile(""path to model def file"", &amp;modelDef));<NewLine>for (int i = 0; i &lt; modelDef.op_size(); ++i)<NewLine>{<NewLine>	caffe2::OperatorDef *opDef = modelDef.mutable_op(i);<NewLine>	if (opDef-&gt;type() == ""Conv"")<NewLine>	{<NewLine>		opDef-&gt;set_engine(""NNPACK""); // Works<NewLine>		opDef-&gt;set_engine(""XNNPACK""); // Probably not working<NewLine>	}<NewLine>}<NewLine></code></pre><NewLine><p>NOTE: The code was built with XNNPACK support.</p><NewLine></div>",https://discuss.pytorch.org/u/JustasZ,,JustasZ,"May 20, 2020,  9:54am",,,,,
81585,Image Classification completely wrong with PyTorch Mobile iOS example,2020-05-17T01:35:38.133Z,5,246,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Dear all,</p><NewLine><p>My trained/traced model got a good performance on PC. However, when i ship the model.pt to PyTorch Mobile and tested on iOS. the classification of same Image is completely wrong.<br/><NewLine>i have no idea where is the problem and how to solve it.<br/><NewLine>my model.pt is generated using Transfer learning with resnet18…</p><NewLine><p>Please help! thanks</p><NewLine></div>",https://discuss.pytorch.org/u/dormouse,(Daniel),dormouse,"May 17, 2020,  1:35am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Without seeing the code it’s hard to debug, but you could check:</p><NewLine><ul><NewLine><li>your image loading and make sure all images are loaded in RGB format (which is the default in PyTorch, as it’s using <code>PIL</code>)</li><NewLine><li>make sure the same preprocessing is applied (same resizing, normalization etc.)</li><NewLine><li>call <code>model.eval()</code> after creating the instance and loading the <code>state_dict</code>.</li><NewLine></ul><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>here is the code of a Standard Transfer Learning with PyTorch:</p><NewLine><pre><code class=""lang-auto""><NewLine>from __future__ import print_function, division<NewLine><NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.optim as optim<NewLine>from torch.optim import lr_scheduler<NewLine>import numpy as np<NewLine>import torchvision<NewLine>from torchvision import datasets, models, transforms<NewLine>import matplotlib.pyplot as plt<NewLine>import time<NewLine>import os<NewLine>import copy<NewLine><NewLine>plt.ion()   # interactive mode<NewLine><NewLine><NewLine># Data augmentation and normalization for training<NewLine># Just normalization for validation<NewLine>data_transforms = {<NewLine>    'train': transforms.Compose([<NewLine>        transforms.RandomResizedCrop(224),<NewLine>        transforms.ColorJitter(brightness=1, contrast=1, saturation=1, hue=0.5),<NewLine>        transforms.RandomRotation(degrees=15),<NewLine>        transforms.RandomHorizontalFlip(),<NewLine>        transforms.ToTensor(),<NewLine>        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])<NewLine>    ]),<NewLine>    'val': transforms.Compose([<NewLine>        transforms.Resize(256),<NewLine>        transforms.CenterCrop(224),<NewLine>        transforms.ToTensor(),<NewLine>        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])<NewLine>    ]),<NewLine>}<NewLine><NewLine>data_dir = '20200328_data/EA2020Test'<NewLine>image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),<NewLine>                                          data_transforms[x])<NewLine>                  for x in ['train', 'val']}<NewLine>dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,<NewLine>                                             shuffle=True, num_workers=4)<NewLine>              for x in ['train', 'val']}<NewLine>dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}<NewLine>class_names = image_datasets['train'].classes<NewLine><NewLine>device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")<NewLine><NewLine><NewLine>def train_model(model, criterion, optimizer, scheduler, num_epochs=25):<NewLine>    since = time.time()<NewLine><NewLine>    best_model_wts = copy.deepcopy(model.state_dict())<NewLine>    best_acc = 0.0<NewLine><NewLine>    for epoch in range(num_epochs):<NewLine>        print('Epoch {}/{}'.format(epoch, num_epochs - 1))<NewLine>        print('-' * 10)<NewLine><NewLine>        # Each epoch has a training and validation phase<NewLine>        for phase in ['train', 'val']:<NewLine>            if phase == 'train':<NewLine>                model.train()  # Set model to training mode<NewLine>            else:<NewLine>                model.eval()   # Set model to evaluate mode<NewLine><NewLine>            running_loss = 0.0<NewLine>            running_corrects = 0<NewLine><NewLine>            # Iterate over data.<NewLine>            for inputs, labels in dataloaders[phase]:<NewLine>                inputs = inputs.to(device)<NewLine>                labels = labels.to(device)<NewLine><NewLine>                # zero the parameter gradients<NewLine>                optimizer.zero_grad()<NewLine><NewLine>                # forward<NewLine>                # track history if only in train<NewLine>                with torch.set_grad_enabled(phase == 'train'):<NewLine>                    outputs = model(inputs)<NewLine>                    _, preds = torch.max(outputs, 1)<NewLine>                    loss = criterion(outputs, labels)<NewLine><NewLine>                    # backward + optimize only if in training phase<NewLine>                    if phase == 'train':<NewLine>                        loss.backward()<NewLine>                        optimizer.step()<NewLine><NewLine>                # statistics<NewLine>                running_loss += loss.item() * inputs.size(0)<NewLine>                running_corrects += torch.sum(preds == labels.data)<NewLine>            if phase == 'train':<NewLine>                scheduler.step()<NewLine><NewLine>            epoch_loss = running_loss / dataset_sizes[phase]<NewLine>            epoch_acc = running_corrects.double() / dataset_sizes[phase]<NewLine><NewLine>            print('{} Loss: {:.4f} Acc: {:.4f}'.format(<NewLine>                phase, epoch_loss, epoch_acc))<NewLine><NewLine>            # deep copy the model<NewLine>            if phase == 'val' and epoch_acc &gt; best_acc:<NewLine>                best_acc = epoch_acc<NewLine>                best_model_wts = copy.deepcopy(model.state_dict())<NewLine><NewLine>        print()<NewLine><NewLine>    time_elapsed = time.time() - since<NewLine>    print('Training complete in {:.0f}m {:.0f}s'.format(<NewLine>        time_elapsed // 60, time_elapsed % 60))<NewLine>    print('Best val Acc: {:4f}'.format(best_acc))<NewLine><NewLine>    # load best model weights<NewLine>    model.load_state_dict(best_model_wts)<NewLine>    #torch.save(model.state_dict(), 'newtest.pth')<NewLine>    return model<NewLine><NewLine><NewLine><NewLine><NewLine>def visualize_model(model, num_images=6):<NewLine>    was_training = model.training<NewLine>    model.eval()<NewLine>    images_so_far = 0<NewLine>    fig = plt.figure()<NewLine><NewLine>    with torch.no_grad():<NewLine>        for i, (inputs, labels) in enumerate(dataloaders['val']):<NewLine>            inputs = inputs.to(device)<NewLine>            labels = labels.to(device)<NewLine><NewLine>            outputs = model(inputs)<NewLine>            _, preds = torch.max(outputs, 1)<NewLine><NewLine>            for j in range(inputs.size()[0]):<NewLine>                images_so_far += 1<NewLine>                ax = plt.subplot(num_images//2, 2, images_so_far)<NewLine>                ax.axis('off')<NewLine>                ax.set_title('predicted: {}'.format(class_names[preds[j]]))<NewLine>                imshow(inputs.cpu().data[j])<NewLine><NewLine>                if images_so_far == num_images:<NewLine>                    model.train(mode=was_training)<NewLine>                    return<NewLine>        model.train(mode=was_training)<NewLine><NewLine>model_conv = torchvision.models.resnet18(pretrained=True)<NewLine>for param in model_conv.parameters():<NewLine>	param.requires_grad = False<NewLine><NewLine># Parameters of newly constructed modules have requires_grad=True by default<NewLine>num_ftrs = model_conv.fc.in_features<NewLine>model_conv.fc = nn.Linear(num_ftrs, len(class_names))<NewLine><NewLine>model_conv = model_conv.to(device)<NewLine><NewLine>criterion = nn.CrossEntropyLoss()<NewLine><NewLine># Observe that only parameters of final layer are being optimized as<NewLine># opposed to before.<NewLine>optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)<NewLine><NewLine># Decay LR by a factor of 0.1 every 7 epochs<NewLine>exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)<NewLine><NewLine>model_conv = train_model(model_conv, criterion, optimizer_conv,exp_lr_scheduler, num_epochs=25)<NewLine><NewLine>example = torch.rand(1, 3, 224, 224)<NewLine><NewLine>traced_script_module = torch.jit.trace(model_conv.cpu(), example)<NewLine><NewLine>traced_script_module.save(""mymodel-jit-cpu.pt"")<NewLine><NewLine><NewLine>#model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=25)<NewLine><NewLine>#visualize_model(model_conv)<NewLine><NewLine></code></pre><NewLine><p>And here is the code of validation on MacOS:</p><NewLine><pre><code class=""lang-auto""># sample execution (requires torchvision)<NewLine><NewLine>import torch<NewLine>from PIL import Image<NewLine>from torchvision import datasets, models, transforms<NewLine>import os<NewLine><NewLine>data_transforms = {<NewLine>    'train': transforms.Compose([<NewLine>        transforms.RandomResizedCrop(224),<NewLine>        transforms.ColorJitter(brightness=1, contrast=1, saturation=1, hue=0.5),<NewLine>        transforms.RandomRotation(degrees=15),<NewLine>        transforms.RandomHorizontalFlip(),<NewLine>        transforms.ToTensor(),<NewLine>        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])<NewLine>    ]),<NewLine>    'val': transforms.Compose([<NewLine>        transforms.Resize(256),<NewLine>        transforms.CenterCrop(224),<NewLine>        transforms.ToTensor(),<NewLine>        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])<NewLine>    ]),<NewLine>}<NewLine><NewLine>input_image = Image.open('./20200328_data/EA2020Test/val/Class3/frame100.jpg')<NewLine>preprocess = transforms.Compose([<NewLine>    transforms.Resize(256),<NewLine>    transforms.CenterCrop(224),<NewLine>    transforms.ToTensor(),<NewLine>    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),<NewLine>])<NewLine>input_tensor = preprocess(input_image)<NewLine>input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model<NewLine><NewLine>data_dir = '20200328_data/EA2020Test'<NewLine>image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),<NewLine>                                          data_transforms[x])<NewLine>                  for x in ['train', 'val']}<NewLine>dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,<NewLine>                                             shuffle=True, num_workers=4)<NewLine>              for x in ['train', 'val']}<NewLine>dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}<NewLine>class_names = image_datasets['train'].classes<NewLine><NewLine><NewLine><NewLine>model_conv = torch.jit.load('mymodel-jit-cpu.pt')<NewLine>model_conv.eval()<NewLine><NewLine># move the input and model to GPU for speed if available<NewLine>if torch.cuda.is_available():<NewLine>    input_batch = input_batch.to('cuda')<NewLine>    model.to('cuda')<NewLine><NewLine>with torch.no_grad():<NewLine>    output = model_conv(input_batch)<NewLine>    _, preds = torch.max(output, 1)<NewLine>print(class_names[preds[0]])<NewLine><NewLine></code></pre><NewLine><p>With these codes, the single image is correctly classified. (e.g. Class3)</p><NewLine><p>And here is the github of where the HelloWorld PyTorch mobile used:<br/><NewLine></p><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""16""/><NewLine><a href=""https://github.com/pytorch/ios-demo-app/tree/master/HelloWorld"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars2.githubusercontent.com/u/21003710?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/pytorch/ios-demo-app/tree/master/HelloWorld"" rel=""nofollow noopener"" target=""_blank"">pytorch/ios-demo-app</a></h3><NewLine><p>PyTorch iOS examples. Contribute to pytorch/ios-demo-app development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>I just replaced the model-jit-cpu.pt and words.txt in the aboved example code.<br/><NewLine>but the same image (e.g. frame28.jpg) is classified as Class23…</p><NewLine><p>Thanks for helping me to figure out where is the problem.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The workflow seems to be correct. Are you able to check the raw outputs of your model and compare it to your Python validation code?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks a lot for the hints, i will let you know the raw outputs dataset (40 tensor) later. Thanks</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here is the raw output of my mode on iOS xcode environment:<br/><NewLine>where the classification is totally wrong, the 23th tensor 7.87…</p><NewLine><pre><code class=""lang-auto"">override func viewDidLoad() {<NewLine>        super.viewDidLoad()<NewLine>        let image = UIImage(named: ""frame28.jpg"")!<NewLine>        imageView.image = image<NewLine>        let resizedImage = image.resized(to: CGSize(width: 224, height: 224))<NewLine>        guard var pixelBuffer = resizedImage.normalized() else {<NewLine>            return<NewLine>        }<NewLine>        guard let outputs = module.predict(image: UnsafeMutableRawPointer(&amp;pixelBuffer)) else {<NewLine>            return<NewLine>        }<NewLine>        print(outputs)<NewLine>        let zippedResults = zip(labels.indices, outputs)<NewLine>        print(labels.indices)<NewLine>        let sortedResults = zippedResults.sorted { $0.1.floatValue &gt; $1.1.floatValue }.prefix(3)<NewLine>        <NewLine>        var text = """"<NewLine>        for result in sortedResults {<NewLine>            text += ""\u{2022} \(labels[result.0]) \n\n""<NewLine>        }<NewLine>        resultView.text = text<NewLine></code></pre><NewLine><p>result:<br/><NewLine>[0.7592794, 1.170417, 0.4990637, 4.677132, -0.9478517, -2.447758, 1.918674, 0.5767481, 1.738559, -0.2370548, 0.4759183, 2.106035, -0.5786518, -1.511588, -2.636765, -2.717191, -0.9784743, -0.7371585, -2.024575, -2.237826, -2.281566, -0.03008107, <strong>7.870783</strong>, 1.824642, 0.3037256, -0.4274126, 0.4281527, -0.4137115, -2.859639, -2.277968, -1.029291, -0.2246231, 1.358673, 2.990178, 2.051248, -0.05761524, 2.349458, 0.004223851, -1.494618, -2.429391]</p><NewLine><p>Here is the code of Python validation:</p><NewLine><pre><code class=""lang-auto""># sample execution (requires torchvision)<NewLine><NewLine>import torch<NewLine>from PIL import Image<NewLine>from torchvision import datasets, models, transforms<NewLine>import os<NewLine><NewLine>data_transforms = {<NewLine>    'train': transforms.Compose([<NewLine>        transforms.RandomResizedCrop(224),<NewLine>        transforms.ColorJitter(brightness=1, contrast=1, saturation=1, hue=0.5),<NewLine>        transforms.RandomRotation(degrees=15),<NewLine>        transforms.RandomHorizontalFlip(),<NewLine>        transforms.ToTensor(),<NewLine>        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])<NewLine>    ]),<NewLine>    'val': transforms.Compose([<NewLine>        transforms.Resize(256),<NewLine>        transforms.CenterCrop(224),<NewLine>        transforms.ToTensor(),<NewLine>        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])<NewLine>    ]),<NewLine>}<NewLine><NewLine>input_image = Image.open('./20200328_data/EA2020Test/val/Class3/frame28.jpg')<NewLine>preprocess = transforms.Compose([<NewLine>    transforms.Resize(256),<NewLine>    transforms.CenterCrop(224),<NewLine>    transforms.ToTensor(),<NewLine>    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),<NewLine>])<NewLine>input_tensor = preprocess(input_image)<NewLine>input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model<NewLine><NewLine>data_dir = '20200328_data/EA2020Test'<NewLine>image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),<NewLine>                                          data_transforms[x])<NewLine>                  for x in ['train', 'val']}<NewLine>dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,<NewLine>                                             shuffle=True, num_workers=4)<NewLine>              for x in ['train', 'val']}<NewLine>dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}<NewLine>class_names = image_datasets['train'].classes<NewLine><NewLine><NewLine><NewLine>model_conv = torch.jit.load('mymodel-jit-cpu.pt')<NewLine>model_conv.eval()<NewLine><NewLine># move the input and model to GPU for speed if available<NewLine>if torch.cuda.is_available():<NewLine>    input_batch = input_batch.to('cuda')<NewLine>    model.to('cuda')<NewLine><NewLine>with torch.no_grad():<NewLine>    output = model_conv(input_batch)<NewLine>    _, preds = torch.max(output, 1)<NewLine>print(class_names[preds[0]])<NewLine>print(output)<NewLine># The output has unnormalized scores. To get probabilities, you can run a softmax on it.<NewLine># print(torch.nn.functional.softmax(output[0], dim=0))<NewLine></code></pre><NewLine><p>And here is the output tensor:<br/><NewLine>tensor([[ 3.0214e+00,  3.8240e-01, <strong>-9.9551e-01</strong>,  3.9651e+00,  2.6704e+00,<br/><NewLine>-1.5943e+00,  9.1830e-01, -6.0747e-03, -1.3737e+00, -6.4241e-01,<br/><NewLine>5.8611e-01,  2.2428e+00, -1.2174e+00, -1.3305e-01, -2.4817e+00,<br/><NewLine>-1.5725e+00, -1.0427e+00, -2.0795e+00, -2.4415e+00, -2.3984e-01,<br/><NewLine>-2.3185e+00, -9.6899e-02,  <em><strong>9.8113e+00</strong></em>,  1.5801e-01, -8.7105e-01,<br/><NewLine>-1.9445e+00, -5.7967e-01, -2.9598e-01, -1.9398e+00, -1.6080e+00,<br/><NewLine>-7.6736e-01, -9.2793e-01,  1.9376e+00,  1.9328e+00,  1.3377e+00,<br/><NewLine>-1.8019e-01,  2.2197e+00, -7.0646e-02, -2.4584e-01, -1.6802e+00]])<br/><NewLine>And Python classificaiton is correct, where the 3rd class (Class3) is labeled.<br/><NewLine>I got some feeling it might be some difference between torch.max (Python validation) and torch.exp<br/><NewLine>(Torch Mobile)?</p><NewLine><p>here is the code of TorchModule.mm</p><NewLine><pre><code class=""lang-auto"">#import ""TorchModule.h""<NewLine>#import &lt;LibTorch/LibTorch.h&gt;<NewLine><NewLine>@implementation TorchModule {<NewLine> @protected<NewLine>  torch::jit::script::Module _impl;<NewLine>}<NewLine><NewLine>- (nullable instancetype)initWithFileAtPath:(NSString*)filePath {<NewLine>  self = [super init];<NewLine>  if (self) {<NewLine>    try {<NewLine>      auto qengines = at::globalContext().supportedQEngines();<NewLine>      if (std::find(qengines.begin(), qengines.end(), at::QEngine::QNNPACK) != qengines.end()) {<NewLine>        at::globalContext().setQEngine(at::QEngine::QNNPACK);<NewLine>      }<NewLine>      _impl = torch::jit::load(filePath.UTF8String);<NewLine>      _impl.eval();<NewLine>    } catch (const std::exception&amp; exception) {<NewLine>      NSLog(@""%s"", exception.what());<NewLine>      return nil;<NewLine>    }<NewLine>  }<NewLine>  return self;<NewLine>}<NewLine><NewLine>- (NSArray&lt;NSNumber*&gt;*)predictImage:(void*)imageBuffer {<NewLine>  try {<NewLine>    at::Tensor tensor = torch::from_blob(imageBuffer, {1, 3, 224, 224}, at::kFloat);<NewLine>    torch::autograd::AutoGradMode guard(false);<NewLine>    at::AutoNonVariableTypeMode non_var_type_mode(true);<NewLine>    auto outputTensor = _impl.forward({tensor}).toTensor();<NewLine>    float* floatBuffer = outputTensor.data_ptr&lt;float&gt;();<NewLine>    if (!floatBuffer) {<NewLine>      return nil;<NewLine>    }<NewLine>    NSMutableArray* results = [[NSMutableArray alloc] init];<NewLine>    for (int i = 0; i &lt; 40; i++) {<NewLine>      [results addObject:@(floatBuffer[i])];<NewLine>    }<NewLine>    return [results copy];<NewLine>  } catch (const std::exception&amp; exception) {<NewLine>    NSLog(@""%s"", exception.what());<NewLine>  }<NewLine>  return nil;<NewLine>}<NewLine><NewLine>@end<NewLine><NewLine></code></pre><NewLine><p>thansk a lot for your help!</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/dormouse"">@dormouse</a>, could you try using png instead of jpg? - <a href=""https://github.com/pytorch/pytorch/issues/27813"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/27813</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, i have just tried with frame28.png, but the classification is exactly the same (far from ground truth) as jpg. <img alt="":joy:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/joy.png?v=9"" title="":joy:""/> <img alt="":joy:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/joy.png?v=9"" title="":joy:""/> <img alt="":joy:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/joy.png?v=9"" title="":joy:""/> <img alt="":joy:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/joy.png?v=9"" title="":joy:""/> <img alt="":joy:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/joy.png?v=9"" title="":joy:""/> <img alt="":joy:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/joy.png?v=9"" title="":joy:""/> <img alt="":joy:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/joy.png?v=9"" title="":joy:""/> <img alt="":joy:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/joy.png?v=9"" title="":joy:""/></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> <a class=""mention"" href=""/u/xta0"">@xta0</a><br/><NewLine>I have figured out where is the problem.<br/><NewLine>i double checked the predict output of Desktop and iOS, the torch.max gives the same tensor value(more or less same)<br/><NewLine>But in iOS, the label.txt should be generated along with PyTorch learning process.<br/><NewLine>I manually created a label.txt, like:<br/><NewLine>Class1<br/><NewLine>Class2<br/><NewLine>Class3<br/><NewLine>…<br/><NewLine>which is not the case during PyTorch learning.<br/><NewLine>During the PyTorch learning, with the dataloader, the class_name should be shuffled. So, just replace the shuffled label.txt with manually generated label.txt in xcode project should be fine!</p><NewLine><p>Thanks for your attention.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dormouse; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/dormouse; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/dormouse; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/dormouse; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/dormouse; <NewLine> ,"REPLY_DATE 1: May 17, 2020,  7:53am; <NewLine> REPLY_DATE 2: May 17, 2020, 10:08am; <NewLine> REPLY_DATE 3: May 18, 2020,  6:59am; <NewLine> REPLY_DATE 4: May 18, 2020,  8:15am; <NewLine> REPLY_DATE 5: May 18, 2020,  7:28pm; <NewLine> REPLY_DATE 6: May 19, 2020,  6:08am; <NewLine> REPLY_DATE 7: May 19, 2020,  6:48am; <NewLine> REPLY_DATE 8: May 19, 2020,  7:13pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> 
81646,How does method listFrom() works?,2020-05-17T14:17:07.038Z,0,71,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,<br/><NewLine>as a question that I posted before, I would like to know how to use the method listFrom when passing a tensor to the method forward.<br/><NewLine>Could anyone make some example of use? Explaining with an example too, if possible, the type of input necessary, because I cannot really understand from the API.<br/><NewLine>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/simoloca,(simooo),simoloca,"May 17, 2020,  2:17pm",,,,,
81017,Torchvision.ops.nms on Android Mobile,2020-05-13T11:43:06.099Z,0,124,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I try to use torchvision.ops.nms in my model. and i have used the following to save my model using torch.jit.trace.</p><NewLine><pre><code class=""lang-auto"">example = torch.rand(1, 3, 224, 224)<NewLine>traced_script_module = torch.jit.trace(model, example)<NewLine>traced_script_module.save(""model.pt"")<NewLine></code></pre><NewLine><p>Everything works well. But when load model.pt on mobile,came across error.</p><NewLine><pre><code class=""lang-auto"">E/AndroidRuntime: Caused by: com.facebook.jni.CppException: <NewLine>    Unknown builtin op: torchvision::nms.<NewLine>    Could not find any similar ops to torchvision::nms. This op may not exist or may not be currently supported in TorchScript.<NewLine>    :<NewLine>.................<NewLine>/usr/lib/python3.6/runpy.py(85): _run_code<NewLine>    /usr/lib/python3.6/runpy.py(193): _run_module_as_main<NewLine>    Serialized   File ""code/__torch__/resnet.py"", line 68<NewLine>        _39 = torch.slice(torch.select(scores0, 0, 0), 0, 0, 9223372036854775807, 1)<NewLine>        _40 = torch.slice(_38, 1, 0, 9223372036854775807, 1)<NewLine>        anchors_nms_idx = ops.torchvision.nms(_40, torch.select(_39, 1, 0), 0.5)<NewLine>                          ~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        _41 = torch.slice(torch.select(classification0, 0, 0), 1, 0, 9223372036854775807, 1)<NewLine>        anchors_nms_idx0 = torch.to(anchors_nms_idx, dtype=4, layout=0, device=torch.device(""cpu""), pin_memory=False, non_blocking=False, copy=False, memory_format=None)<NewLine>    <NewLine></code></pre><NewLine><p>Can anyone please help me? Thank you</p><NewLine></div>",https://discuss.pytorch.org/u/AmrAhmed,(Amr Ahmed),AmrAhmed,"May 13, 2020, 11:44am",,,,,
65165,How to convert model format from PyTorch to tflite?,2019-12-27T15:21:14.842Z,1,2821,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to convert trained model from PyTorch to tflite.<br/><NewLine>I saved trained model bellow.<br/><NewLine>torch.save(net.to(“cpu”).state_dict(), ‘mobilenet_v2.pth’)</p><NewLine><p>How to convert model format from PyTorch to tflite?</p><NewLine><p>python 3.5.6<br/><NewLine>pytorch 1.3.1<br/><NewLine>torch 1.4.0<br/><NewLine>torchvision 0.4.2<br/><NewLine>tensorflow 2.0.0</p><NewLine></div>",https://discuss.pytorch.org/u/syarudhi,(Syarudhi),syarudhi,"December 27, 2019,  3:21pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We don’t officially support this.  It might be possible by using ONNX.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, first export to ONNX, then onward to the format of your choosing.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried this, but get the problem of pytorch and onnx working witch NCHW Tensor order, while tensorflow / tflite expects NHWC.<br/><NewLine>Has anyone been able to solve this?<br/><NewLine>I was trying to convert a vanilla mobilenet V2 model and ran into this issue as explained <a href=""https://stackoverflow.com/questions/61679908/pytorch-convert-2d-cnn-model-to-tflite"" rel=""nofollow noopener"">here on stackoverflow</a></p><NewLine><p>My hunch is that Mobilenet V2 is too new for onnx…<br/><NewLine>Here is my Configuration:</p><NewLine><pre><code class=""lang-auto"">torch                1.6.0.dev20200508 (needs pytorch-nightly to work with mobilenet V2 from torch.hub)<NewLine>tensorflow-gpu       1.14.0<NewLine>onnx                 1.6.0              <NewLine>onnx-tf              1.5.0 <NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Try <a href=""https://github.com/nerox8664/pytorch2keras"" rel=""nofollow noopener"">https://github.com/nerox8664/pytorch2keras</a> Then, export it to tflite. Six months ago, I managed to export it. However, I recommend you to try pytorch mobile. It’s almost as fast as tflite but completely flawless.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/glenn.jocher; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mcPytorch; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/vferrer; <NewLine> ,"REPLY_DATE 1: January 10, 2020,  8:44pm; <NewLine> REPLY_DATE 2: April 30, 2020,  8:16pm; <NewLine> REPLY_DATE 3: May 11, 2020,  9:00am; <NewLine> REPLY_DATE 4: May 11, 2020,  2:05pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> 
80546,Compiling Pytorch Models to run on Apple ANE,2020-05-10T08:54:13.747Z,0,92,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I benchmarked 2 different Resnet50 Models - the Apple CoreML model, available on the Apple website, and a pretrained Torchvision Resnet50 model which I converted using ONNX (Opset9) and CoreMLTools (iOS Version 13).</p><NewLine><p>I tested both models on a brand new iPhone XR.</p><NewLine><p>Inference Times:</p><NewLine><ol><NewLine><li><NewLine><strong>Apple Resnet50</strong> : CPU Inference 100ms, GPU Inference 60ms, ANE Inference 15ms</li><NewLine><li><NewLine><strong>Torchvision Resnet50</strong> : CPU Inference 100ms, GPU Inference 60ms, ANE Inference 60ms.</li><NewLine></ol><NewLine><p>As you can see, the ANE Inference for the Apple version is 4x faster.</p><NewLine><p>Memory Usage:</p><NewLine><ol><NewLine><li><NewLine><strong>Apple Resnet50</strong> : 46 MB CPU, 92 MB GPU, 35 MB ANE</li><NewLine><li><NewLine><strong>Torchvision Resnet50</strong> : 48 MB CPU, 91 MB GPU, 91 MB ANE</li><NewLine></ol><NewLine><p>The Apple version uses 3x less memory than the Torchvision one.</p><NewLine><p>I am concluding the Torchvision Model doesn’t run on the ANE.</p><NewLine><p>I was wondering if anyone knew how the Apple Repo models were compiled in order to run on the ANE? From the model description, the Apple Models were originally written in Keras, but as far as I know the Keras implementation isn’t different from the Pytorch Torchvision one, so I am not sure where the difference comes from.</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/Jonathan_Fuchs,(Jonathan Fuchs),Jonathan_Fuchs,"May 10, 2020,  8:54am",,,,,
80349,Does model serialization have any restriction?,2020-05-08T16:02:16.807Z,0,78,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi to everybody,<br/><NewLine>I’m new with this mobile framework provided by Pytorch.<br/><NewLine>I’m trying to customize the HelloWorld app using a model trained by me, but as I launch the app on Android Studio, the app on the virtualized device crashes.<br/><NewLine>I put my model into the asset folder and change the value of the variable that contains the proper name of the model.<br/><NewLine>So I cannot understand why it doesn’t work, could anybody help me with this?<br/><NewLine>Thanks a lot.</p><NewLine></div>",https://discuss.pytorch.org/u/simoloca,(simooo),simoloca,"May 8, 2020,  4:02pm",,,,,
78952,Deserialize tensor on iOS,2020-04-29T11:23:42.105Z,0,80,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Our team are trying to deploy our model to iOS platform. We encountered some mysterious problems and we want to deserialize the tensors in iOS to locate the problem more precisely. As for now I cannot find any resource regarding achieving so I want to ask:</p><NewLine><ol><NewLine><li>Is it possible?</li><NewLine><li>If so, how? And on what platforms those deserialized tensors can be read?</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/seeker,(LIU Qingyuan),seeker,"April 29, 2020, 11:23am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Correction: Not deserialize but serialize.<br/><NewLine>We want to check the value of tensors to see if we constructed it correctly, so we want to serialize tensors to files and deserialize to python to check whether the value is correct.</p><NewLine><p>(BTW How can I edit my post?)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/seeker; <NewLine> ,"REPLY_DATE 1: May 5, 2020,  2:07am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
79241,Train Pytorch model on Android device,2020-05-01T10:19:42.751Z,1,162,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!<br/><NewLine>Is it possible to train a NN model on Android device? (and not using a pre-trained model)</p><NewLine></div>",https://discuss.pytorch.org/u/abediee,(Ali Abedi),abediee,"May 1, 2020, 10:19am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>In principle this is possible if you compile a full version of libtorch and use that. It won’t be lightning fast, though.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you so much!</p><NewLine><p>regards</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/abediee; <NewLine> ,"REPLY_DATE 1: May 1, 2020, 10:55am; <NewLine> REPLY_DATE 2: May 2, 2020,  2:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
79256,USE_NNAPI with PyTorch,2020-05-01T12:46:53.532Z,0,108,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello guys,</p><NewLine><p>I’ve noticed, there is a possibility to use NNAPI - is that more information about how to use it (e.g. examples) enable it. Particularly I am interested to run PyTocrch inference for the embedded device via NNAPI.</p><NewLine><p>Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/Peter_Peter,(Peter Peter),Peter_Peter,"May 1, 2020, 10:27pm",,,,,
79034,Run inference on Android for the quantized model,2020-04-29T22:08:50.925Z,0,104,"<div class=""post"" itemprop=""articleBody""><NewLine><p>What is the use case for LiteModelLoader?(<a href=""https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/src/main/java/org/pytorch/LiteModuleLoader.java"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/src/main/java/org/pytorch/LiteModuleLoader.java</a>)<br/><NewLine>torch::jit::load in general supports both of the regular and quantized model, does it already cover the functionality of LiteModelLoader?</p><NewLine></div>",https://discuss.pytorch.org/u/stu1130,(Jake Lee),stu1130,"April 29, 2020, 10:09pm",,,,,
78863,Deployment of PyTorch model in production,2020-04-28T19:16:12.814Z,0,150,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a couple of questions related to using PyTorch models in production</p><NewLine><ol><NewLine><li><NewLine><p>How can we deploy PyTorch models in a stateless server on-cloud?</p><NewLine></li><NewLine><li><NewLine><p>What’s the best practice to deploy a model offline to do inference on phone? I mean I know about the tracing and direct embedding method but from a security perspective is it safe to roll out an APK by tracing? By safety I mean can someone reverse engineer the weights of my model from the APK?</p><NewLine></li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/vishalagarwal,(Vishal Agarwal),vishalagarwal,"April 29, 2020,  5:31am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’d suggest taking a look at <a href=""https://github.com/pytorch/serve/blob/master/README.md#serve-a-model"" rel=""nofollow noopener"">torchserve</a> for model serving. Not sure if it’s possible to do stateless though, I assume you mean something like a Lambda function?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/thvasilo; <NewLine> ,"REPLY_DATE 1: April 28, 2020, 10:55pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
78060,PyTorch Cross-platform Development (Android &amp; iOS),2020-04-23T14:12:22.943Z,2,228,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone !<br/><NewLine>I’m planning to do an app that will be using a PyTorch model. I would like this app to be compatible with both Android and iOS (like a React Native app).<br/><NewLine>What are the best solutions to do that (considering the use of a PyTorch model) ?</p><NewLine></div>",https://discuss.pytorch.org/u/Untitled,,Untitled,"April 23, 2020,  2:12pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Up! Is there any way to do a React Native App with a working PyTorch model inside ?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have not used PyTorch with React Native, but what you can do is to write your model using PyTorch, adapt it to ONNX (for Android) and CoreML (for iOS). You can export to ONNX directly from PyTorch, and you can convert from ONNX to CoreML using <a href=""https://github.com/onnx/onnx-coreml"" rel=""nofollow noopener"">https://github.com/onnx/onnx-coreml</a> .</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>And so you think that if I’m detecting the OS inside the React Native app and then I use the proper model (ONNX for Android, CoreML for iOS) it could potentially work ?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I would presume so, yes. I have managed to get a TensorFlow model working on Android / iOS using a very similar approach. I have to imagine it would have been possible with PyTorch as well. I do need to stress that I am not not up-to-date with PyTorch and mobile devices, so it is entirely possible that there exists an easier approach. I encourage you to do further research on your own just to be sure. However, checking the ONNX repository, this example should get you started: <a href=""https://github.com/onnx/tutorials/blob/master/tutorials/PytorchCaffe2MobileSqueezeNet.ipynb"" rel=""nofollow noopener"">https://github.com/onnx/tutorials/blob/master/tutorials/PytorchCaffe2MobileSqueezeNet.ipynb</a></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Okay, thank you very much!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Untitled; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Joachim; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Untitled; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Joachim; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Untitled; <NewLine> ,"REPLY_DATE 1: April 27, 2020,  1:28pm; <NewLine> REPLY_DATE 2: April 27, 2020,  2:18pm; <NewLine> REPLY_DATE 3: April 28, 2020, 10:57am; <NewLine> REPLY_DATE 4: April 28, 2020, 11:18am; <NewLine> REPLY_DATE 5: April 28, 2020, 12:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
74427,Totally different results when using TorchScript on Android,2020-03-26T10:02:40.056Z,2,291,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Here’s <a href=""https://github.com/minhduc0711/neural-style-transfer/blob/35e7cd82bafc82e18cc7e35491082935834580d9/style_transfer/models.py#L27"" rel=""nofollow noopener"">my model code</a>, which is basically the generative model from the fast neural style PyTorch example.<br/><NewLine>I converted it to TorchScript:</p><NewLine><pre><code class=""lang-auto"">net = TransformerNet(alpha=0.3)<NewLine>net.load_state_dict(torch.load(""trained_models/starry_night_small.pth"",<NewLine>                               map_location='cpu'))<NewLine>net.eval()<NewLine><NewLine># Convert to TorchScript<NewLine>ts_module = torch.jit.script(net, torch.ones(1, 3, 256, 256))  # I also tried jit.trace() but no luck<NewLine>ts_module.save(""trained_models/starry_night.zip"")<NewLine></code></pre><NewLine><p>I tested right in Python to see if eager mode and tracing give the same results, and strangely the results are the <strong>exact</strong> same (I thought there has to be a small error due to conversion):</p><NewLine><pre><code class=""lang-auto"">example_input = torch.randn(1,3,256,256)<NewLine>with torch.no_grad():<NewLine>    orig_out = net(example_input)<NewLine>    ts_out = ts_module(example_input)<NewLine>diff = F.l1_loss(orig_out.flatten(), ts_out.flatten()).item()<NewLine>print(""Error: "", diff)<NewLine># output: 0.0<NewLine></code></pre><NewLine><p>And when I brought the model onto an Android app, the model gave way different results.<br/><NewLine>Here’s the Android code:</p><NewLine><pre><code class=""lang-java"">float[] dummyInput = new float[3 * 5 * 5];<NewLine>long[] shape = new long[]{1, 3, 5, 5};<NewLine>Arrays.fill(dummyInput, 1.0f);<NewLine>final Tensor inputTensor = Tensor.fromBlob(dummyInput, shape);<NewLine>Tensor outputTensor = net.forward(IValue.from(inputTensor)).toTensor();<NewLine>float[] rawPixels = outputTensor.getDataAsFloatArray();<NewLine>// rawPixels = {0.38102132, 0.15379308, 0.21999769, 0.33937144, 0.24030314, 0.15130955, 0.3354013, 0.19232287,...}<NewLine></code></pre><NewLine><p>while the results on Python for <code>torch.ones()</code> is just:</p><NewLine><pre><code class=""lang-python"">ts_module(torch.ones(1,3,5,5)).flatten().unique()<NewLine># output: tensor([0.2949, 0.3554, 0.4184], grad_fn=&lt;NotImplemented&gt;)<NewLine></code></pre><NewLine><p>Sorry for the lengthy post, but I would really appreciate it if someone points me in the right direction to debug this.<br/><NewLine>Thank you in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/minhduc0711,,minhduc0711,"March 26, 2020, 10:06am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you solved this problem？I meet this problem now, the result in android and PC is different.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not yet sadly. However I’ve kinda encountered this problem <a href=""https://discuss.pytorch.org/t/module-output-slightly-off-on-android/62838/10"">before</a>, but the cause due to was jpg files. You can check it out to see if it’s related.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I modified the architecture of the network and loaded the old weights, it work well. But l don’t know why the old net work differently and the new net work well.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Silbernitrat; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/minhduc0711; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Silbernitrat; <NewLine> ,"REPLY_DATE 1: April 23, 2020,  9:18am; <NewLine> REPLY_DATE 2: April 24, 2020,  1:27pm; <NewLine> REPLY_DATE 3: April 24, 2020,  1:35pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
77368,Delete model.pt from asset folder (&ldquo;Hello World Android App&rdquo;),2020-04-18T23:23:15.842Z,3,128,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I tried to run the “Hello World” Android application and everything goes correctly.<br/><NewLine>I deleted the model.pt file from the assets folder and the application continues to run correctly. How is it possible that the model was found if it was deleted?<br/><NewLine>Sorry for the stupid question and for my bad English</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/601f97e0da77b21c95cddcca6b09eed84cd11e5c"" href=""https://discuss.pytorch.org/uploads/default/original/3X/6/0/601f97e0da77b21c95cddcca6b09eed84cd11e5c.png"" title=""Schermata 2020-04-18 alle 23.43.57""><img alt=""Schermata 2020-04-18 alle 23.43.57"" data-base62-sha1=""dIlwzNyvtFEG8rIwu4M3gaYgJWs"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/6/0/601f97e0da77b21c95cddcca6b09eed84cd11e5c_2_10x10.png"" height=""265"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/6/0/601f97e0da77b21c95cddcca6b09eed84cd11e5c_2_690x265.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/6/0/601f97e0da77b21c95cddcca6b09eed84cd11e5c_2_690x265.png, https://discuss.pytorch.org/uploads/default/optimized/3X/6/0/601f97e0da77b21c95cddcca6b09eed84cd11e5c_2_1035x397.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/6/0/601f97e0da77b21c95cddcca6b09eed84cd11e5c_2_1380x530.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Schermata 2020-04-18 alle 23.43.57</span><span class=""informations"">2502×962 334 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/juvegimmy,,juvegimmy,"April 18, 2020, 11:44pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Cause the java app has builded before you deleted the file</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>so if I want to change the model what should I do to delete the old one?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not only re-run the app but also rebuild it</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve already done it and it doesn’t work. I even tried to delete the app and download it again, immediately delete the model.pt and run it for the first time, incredibly it works. I am desperate</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/simaiden; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/juvegimmy; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/simaiden; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/juvegimmy; <NewLine> ,"REPLY_DATE 1: April 19, 2020, 12:03am; <NewLine> REPLY_DATE 2: April 19, 2020, 12:13am; <NewLine> REPLY_DATE 3: April 19, 2020, 12:43am; <NewLine> REPLY_DATE 4: April 19, 2020, 12:48am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
77144,LibTorch on tvOS?,2020-04-17T04:58:52.982Z,0,64,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Is it possible to use LibTorch on tvOS platform? The <code>pod install</code> command gives following error message -</p><NewLine><blockquote><NewLine><p>[!] The platform of the target <code>HelloWorld</code> (tvOS 13.2) is not compatible with <code>LibTorch (1.4.0)</code>, which does not support <code>tvOS</code>.</p><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/ar97,(AR),ar97,"April 17, 2020,  5:06am",,,,,
75544,Can&rsquo;t use iOS libtorch as a dependency for a library using Cocoapods,2020-04-06T12:35:47.527Z,1,236,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m using libtorch as a dependency for our library using Cocoapods. It compiles fine when I use I use libtorch in an app, but fails when I import libtorch headers</p><NewLine><p>Specific Steps:</p><NewLine><ol><NewLine><li><code>pod lib create &lt;pod&gt;</code></li><NewLine><li>Add the C++ library as a dependency on the podspec  <code>s.dependency 'LibTorch', '~&gt; 1.3.0'</code><NewLine></li><NewLine><li>Add Objective-C files  <code>TestClass.h</code>  and  <code>TestClass.m</code><NewLine></li><NewLine><li>Import the library on <code>TestClass.m</code><NewLine></li><NewLine></ol><NewLine><pre><code class=""lang-auto"">  <NewLine>#import ""TestClass.h""<NewLine>#import &lt;LibTorch/LibTorch.h&gt;<NewLine><NewLine>@implementation TestClass<NewLine><NewLine><NewLine><NewLine>@end<NewLine></code></pre><NewLine><ol start=""4""><NewLine><li><code>pod install</code></li><NewLine><li>Build the project, then it fails to build.</li><NewLine></ol><NewLine><p>Error when buidling:  <code>'torch/script.h' not found</code><br/><NewLine>In  <code>Pods/LibTorch/Core/Libtorch.h</code></p><NewLine><pre><code class=""lang-auto"">#ifndef LibTorch_h<NewLine>#define LibTorch_h<NewLine><NewLine>#include &lt;torch/script.h&gt;<NewLine><NewLine>#endif<NewLine></code></pre><NewLine><p>I’ve filed a similar issue in cocoapods here <a href=""https://github.com/CocoaPods/CocoaPods/issues/9678"" rel=""nofollow noopener"">https://github.com/CocoaPods/CocoaPods/issues/9678</a> but I think this may be a libtorch issue with how its header/include paths are set.</p><NewLine><p>Here is a sample project: <a href=""https://github.com/mjjimenez/SamplePod"" rel=""nofollow noopener"">https://github.com/mjjimenez/SamplePod</a></p><NewLine><p><a class=""mention"" href=""/u/xta0"">@xta0</a> can I ask you for some help on this?</p><NewLine></div>",https://discuss.pytorch.org/u/mark_jimenez,,mark_jimenez,"April 6, 2020, 12:35pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><span class=""mention"">@xtao</span> I’ve filed an issue here if you need something you can track: <a href=""https://github.com/pytorch/pytorch/issues/36387"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/36387</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mark_jimenez"">@mark_jimenez</a> if you want to wrap pytorch into your custom podspec, you need to specify the header search path manually. see here for example - <a href=""https://github.com/xta0/PytorchExpObjC/blob/master/PytorchExpObjc.podspec"" rel=""nofollow noopener"">https://github.com/xta0/PytorchExpObjC/blob/master/PytorchExpObjc.podspec</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/xta0"">@xta0</a> it works perfectly. Thank you again for your help!</p><NewLine><p>Not sure why is it in cocoapods you still need to redeclare header paths in your own podspec, if it’s already declared in libtorch’s podspec. But hey, as long as it works.</p><NewLine><p>I’ll be closing the issue in github.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mark_jimenez; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mark_jimenez; <NewLine> ,"REPLY_DATE 1: April 12, 2020,  1:28pm; <NewLine> REPLY_DATE 2: April 14, 2020, 12:46am; <NewLine> REPLY_DATE 3: April 13, 2020, 12:49pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
71783,Android and PyTorch,2020-03-02T22:14:31.722Z,7,346,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have trained a custom model and wanna add it to my android app. I saved the model with jet but when I run it on my android device I get this error:</p><NewLine><p>com.facebook.jni.CppException: empty not implemented for TensorTypeSet(VariableTensorId, CUDATensorId) (empty at aten/src/ATen/Functions.h:3999)</p><NewLine><p>Any fixes ?</p><NewLine></div>",https://discuss.pytorch.org/u/ntinos,(Ntinos),ntinos,"March 3, 2020,  1:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>It seems like you’re trying to use a CUDA Tensor in your mobile app. I’m pretty sure that is not supported. You might want to make sure that your model is on the CPU before doing the export.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Any code advice how to do that ?<br/><NewLine>When I trained my model I moved it to the cpu and then extracted with this code:</p><NewLine><pre><code class=""lang-auto"">model.cpu()<NewLine>example = torch.rand(1, 3, 224, 224)<NewLine>traced_script_module = torch.jit.trace(model, example)<NewLine>traced_script_module.save(""app/src/main/assets/model.pt"")<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>That looks right. And when you use this model you see the error above?<br/><NewLine>How do you create the input to the model on the mobile side?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Im using the sample PyTorchDemo app from pytorch. Just changed the model’s name in the code nothing else.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is weird, maybe someone from the mobile team will have a better idea?</p><NewLine><p>You can also try to double check your model to make sure it does not contain any weight that is on the gpu?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>hmm okay thanks for your help.<br/><NewLine>is there any easy way how to check the location of the weights ?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can print the <code>.device</code> field of any Tensor to check where it lives.<br/><NewLine>Also you can try to install a cpu-only build of pytorch and try to load your model in there. If there is cuda weights in it, it will fail as well, but with a friendlier error message.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! I have the same problem, even with model.cpu().<br/><NewLine>Did you find a solution?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ntinos; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ntinos; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ntinos; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Untitled; <NewLine> ,"REPLY_DATE 1: March 2, 2020, 10:52pm; <NewLine> REPLY_DATE 2: March 2, 2020, 11:05pm; <NewLine> REPLY_DATE 3: March 3, 2020,  1:56pm; <NewLine> REPLY_DATE 4: March 3, 2020,  2:15pm; <NewLine> REPLY_DATE 5: March 3, 2020,  2:21pm; <NewLine> REPLY_DATE 6: March 3, 2020,  2:23pm; <NewLine> REPLY_DATE 7: March 3, 2020,  2:28pm; <NewLine> REPLY_DATE 8: April 10, 2020,  4:54pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
69073,LibTorch on WatchOS?,2020-02-08T19:13:16.807Z,17,646,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>Does anyone have a guide/tutorial on how to get LibTorch (C++) installed/setup for a WatchOS project?<br/><NewLine>I was hoping it would be as easy as installing a Pod like the iOS version… but I get</p><NewLine><p><code>The platform of the target </code>XXX WatchKit Extension<code>(watchOS 6.1) is not compatible with</code>LibTorch (1.4.0)<code>, which does not support </code>watchOS<code>.</code></p><NewLine><p>I’ve also tried pulling the source and building using:</p><NewLine><p><code>IOS_PLATFORM=WATCHOS scripts/build_ios.sh</code></p><NewLine><p>But I keep running into various build issues… and even then I’m not totally sure how to get that built version into my XCode project if I did manage to get it built.</p><NewLine><p>Has anyone out there successfully set up a WatchOS project with LibTorch?</p><NewLine><p>Thanks in advance!!</p><NewLine></div>",https://discuss.pytorch.org/u/mapes911,(Allan Mercado),mapes911,"February 8, 2020,  7:13pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/mapes911"">@mapes911</a>, we haven’t run any tests on WatchOS yet. But you should be able to compile the code for that architecture. Can you double check your build command? The one below works on my machine.</p><NewLine><pre><code class=""lang-auto"">BUILD_PYTORCH_MOBILE=1 IOS_PLATFORM=WATCHOS ./scripts/build_ios.sh<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/xta0"">@xta0</a>,</p><NewLine><p>Thanks! That did it. The build worked this time <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>I’m running up against another issue now though… not sure if it’s because I was messing around with Eigen libraries earlier or not… but I’m getting these math errors after importing pytorch into my XCode library.</p><NewLine><pre><code class=""lang-auto"">.../pytorch/include/caffe2/perfkernels/math.h:3:10: 'cstdint' file not found<NewLine>.../WatchSimulator6.1.sdk/usr/include/dispatch/dispatch.h:25:10: Could not build module 'Darwin'<NewLine>.../WatchSimulator6.1.sdk/System/Library/Frameworks/Foundation.framework/Headers/Foundation.h:6:10: Could not build module 'CoreFoundation'<NewLine></code></pre><NewLine><p>I’ve uninstalled and re-installed XCode as per some Stack Overflow posts I’ve found with similar issues and no luck.</p><NewLine><p>Any ideas?</p><NewLine><p>Thanks again for responding!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Scratch that, I found the problem.  I had my Header Search Paths set to recursive and that caused my issues.</p><NewLine><p>For anyone that finds this… here’s what I did to get this installed on WatchOS.</p><NewLine><ul><NewLine><li>cloned the pytorch repo</li><NewLine><li>cd pytorch</li><NewLine><li>git checkout v1.4.0</li><NewLine><li>git submodule sync</li><NewLine><li>git submodule update --init --recursive</li><NewLine><li>BUILD_PYTORCH_MOBILE=1 IOS_PLATFORM=WATCHOS scripts/build_ios.sh</li><NewLine><li>add files from build_ios/install/include/ and build_ios/install/lib/ to my XCode project</li><NewLine><li>in XCode target build settings, add the include directory to Header Search Paths, make sure it’s non-recursive.</li><NewLine><li>in XCode target build settings, Other Linker Flags,  -force_load “path to lib directory”</li><NewLine></ul><NewLine><p>That should do it!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s awesome news. Thanks for sharing this.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/xta0"">@xta0</a> I’m afraid I spoke too soon.  I thought my build was ok because it was just failing on one of my own bugs, but once I fixed that I’ve run into a couple of issues that I’m not sure how to solve.</p><NewLine><p>I’m getting this error.</p><NewLine><blockquote><NewLine><p>libtorch.a(SparseCPUType.cpp.o)’ does not contain bitcode. You must rebuild it with bitcode enabled (Xcode setting ENABLE_BITCODE), obtain an updated library from the vendor, or disable bitcode for this target. for architecture armv7k</p><NewLine></blockquote><NewLine><p>there is no setting for WatchOS to disable bitcode in the build like there is for iOS. is there a way to build the pytorch libs with bitcode enabled?</p><NewLine><p>another issue I was seeing was…</p><NewLine><blockquote><NewLine><p>building for watchOS-arm64_32 but attempting to link with file built for watchOS-armv7k</p><NewLine></blockquote><NewLine><p>I am attempting to load onto a series 4 watch… any ideas?</p><NewLine><p>Thanks again for your help!!</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mapes911"">@mapes911</a>, Below is my command that work on my machine. Feel free to try it out.</p><NewLine><pre><code class=""lang-auto"">BUILD_PYTORCH_MOBILE=1 ENABLE_BITCODE=1 IOS_PLATFORM=WATCHOS IOS_ARCH=arm64_32 ./scripts/build_ios.sh<NewLine></code></pre><NewLine><p>As for the bitcode, you can use the commands below to verify</p><NewLine><pre><code class=""lang-auto"">#check the architecure<NewLine>&gt; lipo -i libcpuinfo.a<NewLine>#Non-fat file: libcpuinfo.a is architecture: arm64_32<NewLine><NewLine>#verify bitcode<NewLine>&gt; otool -l libtorch_cpu.a | grep bitcode<NewLine># sectname __bitcode<NewLine></code></pre><NewLine><p>Again, I haven’t run any tests on watchOS platforms. So I’m not sure whether they’ll work or not. But I’m curious to see your results. Please let me know if you have any questions. THanks.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/xta0"">@xta0</a></p><NewLine><p>feels like we’re getting closer!<br/><NewLine>re-built using the command you sent.</p><NewLine><p>the lipo command shows all the libraries built properly for arm64_32<br/><NewLine>the otool command you gave is showing “-i functionality obsolete”  but the bitcode errors went away so I’m assuming that worked ok.</p><NewLine><p>after doing that I still got a bunch of errors, but i was able to get rid of most by doing:</p><NewLine><ul><NewLine><li>Build Settings -&gt; Valid Architectures.  Removing armv7k and just leaving it as arm64_32.  Is this ok to do? Or will I run into issues later because I removed armv7k.</li><NewLine></ul><NewLine><p>But… now my (hopefully) last hurdle is this error:</p><NewLine><blockquote><NewLine><p>libpytorch_qnnpack.a(8x8-aarch64-neon.S.o), building for watchOS, but linking in object file ( …pytorch/lib//libpytorch_qnnpack.a(8x8-aarch64-neon.S.o)) built for macOS, for architecture arm64_32</p><NewLine></blockquote><NewLine><p>so… I’m not sure how to resolve this issue.  any ideas?</p><NewLine><p>Thanks Tao!!</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mapes911"">@mapes911</a> that’s because the QNNPACK kernel was written in assembly which has conficts with Apple’s LLVM IR (bitcode). If your model is not quantized, you don’t have to include QNNPACK.</p><NewLine><p>Go to the root <code>CMakeLists.txt</code>, search for <code>set(USE_PYTORCH_QNNPACK ON)</code>, and turn it off</p><NewLine><pre><code class=""lang-auto"">set(USE_PYTORCH_QNNPACK OFF)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/xta0"">@xta0</a></p><NewLine><p>awesome. it’s compiling, linking and loading on my watch now!! thank you.</p><NewLine><p>however, I am now getting this error when I attempt to load up a trained model.</p><NewLine><blockquote><NewLine><p>libc++abi.dylib: terminating with uncaught exception of type torch::jit::script::ErrorReport:<br/><NewLine>Unknown builtin op: aten::mul.<br/><NewLine>Could not find any similar ops to aten::mul. This op may not exist or may not be currently supported in TorchScript</p><NewLine></blockquote><NewLine><p>I’ve tried a couple of models, including one that I was able to load on my phone with LibTorch 1.4.0 using the Podfile install.</p><NewLine><p>How would I look into what this aten::mul method does? Any idea if this has to do with the way I’ve built the libraries?  I’ve found a few other forum posts with similar errors but no immediate solution.</p><NewLine><p>Just in case, here’s the code that loads the model.</p><NewLine><pre><code class=""lang-auto"">torch::jit::script::Module module;<NewLine>module = torch::jit::load(file_path);<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mapes911"">@mapes911</a> That’s good news to hear. <code>aten::mul</code> is a very basic op that should be registered by default. Did you apply the <code>-all_load</code> to the static libraries? (<code>force_load</code> on libtorch_cpu.a should also work).  Also,  does that model work on your phone?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>yes! thank you so much. it is compiled/linked and i can now read in my model.</p><NewLine><p>i appreciate all your help <a class=""mention"" href=""/u/xta0"">@xta0</a></p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mapes911"">@mapes911</a> No problem, were you able to run your model on your watch?</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes and no <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>I was able to load the model and run one of my data points through… but the second time through my event handler crashes.  Assuming this is me not really knowing C++ very well anymore… and I’m probably doing something bad with memory or not setting up my tensor properly.</p><NewLine><p>So… I’m not fully sure that the data is going through the model yet.  Will try more when I get home tonight.</p><NewLine><p>Thanks</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/xta0"">@xta0</a> looks like i’m able to run data through my model just fine on my watch now, thank you very much for your help.</p><NewLine><p>any idea when a WatchOS version will be supported in the LibTorch Podfile?  would love to be able to keep up with new versions without going through this pain.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/mapes911"">@mapes911</a> That’s good news to hear! Thanks for letting me know.  We haven’t discussed about supporting watchos via Cocoapods yet. However, we did simplify the build script  - <a href=""https://github.com/pytorch/pytorch/pull/33318"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/33318</a>.</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, I am facing the same issue. Where is CMakeLists.txt file located?</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/CMakeLists.txt"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/master/CMakeLists.txt"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/master/CMakeLists.txt</a></h4><NewLine><pre><code class=""lang-txt"">cmake_minimum_required(VERSION 3.5 FATAL_ERROR)<NewLine>#cmake_policy(SET CMP0022 NEW)<NewLine>#cmake_policy(SET CMP0023 NEW)<NewLine><NewLine># Use compiler ID ""AppleClang"" instead of ""Clang"" for XCode.<NewLine># Not setting this sometimes makes XCode C compiler gets detected as ""Clang"",<NewLine># even when the C++ one is detected as ""AppleClang"".<NewLine>cmake_policy(SET CMP0010 NEW)<NewLine>cmake_policy(SET CMP0025 NEW)<NewLine><NewLine># Suppress warning flags in default MSVC configuration.  It's not<NewLine># mandatory that we do this (and we don't if cmake is old), but it's<NewLine># nice when it's possible, and it's possible on our Windows configs.<NewLine>if(NOT CMAKE_VERSION VERSION_LESS 3.15.0)<NewLine>  cmake_policy(SET CMP0092 NEW)<NewLine>endif()<NewLine><NewLine>if(NOT CMAKE_VERSION VERSION_LESS 3.10)<NewLine>  set(FIND_CUDA_MODULE_DEPRECATED ON)<NewLine>endif()<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/pytorch/blob/master/CMakeLists.txt"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply. That’s the path of PyTorch repo. How can changes there resolves the error in XCode? Can you please elaborate the process ?</p><NewLine><p>Error:<br/><NewLine>ld: warning: building for iOS, but linking in object file (/path-to-project/Pods/LibTorch/install/lib/libpytorch_qnnpack.a(8x8-aarch64-neon.S.o)) built for macOS<br/><NewLine>ld: warning: building for iOS, but linking in object file (/path-to-project/Pods/LibTorch/install/lib/libpytorch_qnnpack.a(8x8-aarch64-neon.S.o)) built for macOS<br/><NewLine>Undefined symbols for architecture arm64:<br/><NewLine>“<em>OBJC_CLASS</em>$_TorchModule”, referenced from:<br/><NewLine>objc-class-ref in ViewController.o<br/><NewLine>ld: symbol(s) not found for architecture arm64<br/><NewLine>clang: error: linker command failed with exit code 1 (use -v to see invocation)</p><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey there,</p><NewLine><p>The CMakeLists.txt changes I made were to turn off QNNPACK which was causing issues for me when I loaded the model in my XCode project.<br/><NewLine>I’m not sure what your issue is, but if you read through the thread I had to go through several iterations (and issues) to arrive at a LibTorch build that allowed me to get my trained model running in a WatchOS project.</p><NewLine><p>What is the process you are going through that is getting you to this error?<br/><NewLine>(I’m no expert in this, but <a class=""mention"" href=""/u/xta0"">@xta0</a> 's comments really helped )</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mapes911; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mapes911; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mapes911; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mapes911; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/mapes911; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/mapes911; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/mapes911; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/mapes911; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/Harsh_Thaker; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/mapes911; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/Harsh_Thaker; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/mapes911; <NewLine> ,"REPLY_DATE 1: February 11, 2020,  1:00am; <NewLine> REPLY_DATE 2: February 10, 2020, 11:39pm; <NewLine> REPLY_DATE 3: February 11, 2020,  7:12am; <NewLine> REPLY_DATE 4: February 11, 2020, 12:59am; <NewLine> REPLY_DATE 5: February 11, 2020,  7:59am; <NewLine> REPLY_DATE 6: February 12, 2020,  1:32am; <NewLine> REPLY_DATE 7: February 12, 2020,  6:04am; <NewLine> REPLY_DATE 8: February 12, 2020, 10:19pm; <NewLine> REPLY_DATE 9: February 13, 2020,  3:08am; <NewLine> REPLY_DATE 10: February 13, 2020,  5:54am; <NewLine> REPLY_DATE 11: February 13, 2020,  7:22am; <NewLine> REPLY_DATE 12: February 13, 2020,  7:15pm; <NewLine> REPLY_DATE 13: February 13, 2020,  7:30pm; <NewLine> REPLY_DATE 14: February 15, 2020,  8:52pm; <NewLine> REPLY_DATE 15: February 18, 2020,  9:23pm; <NewLine> REPLY_DATE 16: April 3, 2020, 12:27pm; <NewLine> REPLY_DATE 17: April 3, 2020,  2:44pm; <NewLine> REPLY_DATE 18: April 4, 2020,  9:11am; <NewLine> REPLY_DATE 19: April 4, 2020,  5:25pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> REPLY 11 LIKES: 1 Like; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> REPLY 18 LIKES: ; <NewLine> REPLY 19 LIKES: ; <NewLine> 
75309,React Native 0.61.3 + PyTorch incompatible?,2020-04-04T07:02:28.183Z,0,191,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to build my react native app in Android Studio and I’m getting this error:</p><NewLine><pre><code class=""lang-auto"">org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':app:checkDebugDuplicateClasses'.<NewLine>	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:148)<NewLine>	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:145)<NewLine>	at org.gradle.internal.Try$Failure.ifSuccessfulOrElse(Try.java:191)<NewLine>	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:138)<NewLine>	at org.gradle.api.internal.tasks.execution.ResolveBeforeExecutionStateTaskExecuter.execute(ResolveBeforeExecutionStateTaskExecuter.java:75)<NewLine>	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:62)<NewLine>	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:108)<NewLine>	at org.gradle.api.internal.tasks.execution.ResolveBeforeExecutionOutputsTaskExecuter.execute(ResolveBeforeExecutionOutputsTaskExecuter.java:67)<NewLine>	at org.gradle.api.internal.tasks.execution.ResolveAfterPreviousExecutionStateTaskExecuter.execute(ResolveAfterPreviousExecutionStateTaskExecuter.java:46)<NewLine>	at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:94)<NewLine>	at org.gradle.api.internal.tasks.execution.FinalizePropertiesTaskExecuter.execute(FinalizePropertiesTaskExecuter.java:46)<NewLine>	at org.gradle.api.internal.tasks.execution.ResolveTaskExecutionModeExecuter.execute(ResolveTaskExecutionModeExecuter.java:95)<NewLine>	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:57)<NewLine>	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:56)<NewLine>	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:36)<NewLine>	at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.executeTask(EventFiringTaskExecuter.java:73)<NewLine>	at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.call(EventFiringTaskExecuter.java:52)<NewLine>	at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.call(EventFiringTaskExecuter.java:49)<NewLine>	at org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:416)<NewLine>	at org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:406)<NewLine>	at org.gradle.internal.operations.DefaultBuildOperationExecutor$1.execute(DefaultBuildOperationExecutor.java:165)<NewLine>	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:250)<NewLine>	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:158)<NewLine>	at org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:102)<NewLine>	at org.gradle.internal.operations.DelegatingBuildOperationExecutor.call(DelegatingBuildOperationExecutor.java:36)<NewLine>	at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter.execute(EventFiringTaskExecuter.java:49)<NewLine>	at org.gradle.execution.plan.LocalTaskNodeExecutor.execute(LocalTaskNodeExecutor.java:43)<NewLine>	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:355)<NewLine>	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:343)<NewLine>	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336)<NewLine>	at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322)<NewLine>	at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134)<NewLine>	at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129)<NewLine>	at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202)<NewLine>	at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193)<NewLine>	at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129)<NewLine>	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)<NewLine>	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)<NewLine>	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<NewLine>	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<NewLine>	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)<NewLine>	at java.lang.Thread.run(Thread.java:748)<NewLine>Caused by: com.android.ide.common.workers.WorkerExecutorException: 1 exception was raised by workers:<NewLine>java.lang.RuntimeException: Duplicate class com.facebook.jni.CppException found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.CppSystemErrorException found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread$1 found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread$Destructor found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread$DestructorList found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread$DestructorStack found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread$Terminus found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.HybridClassBase found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.HybridData found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.HybridData$Destructor found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.IteratorHelper found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.MapIteratorHelper found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.NativeRunnable found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.ThreadScopeSupport found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.UnknownCppException found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine><NewLine>Go to the documentation to learn how to Fix dependency resolution errors.<NewLine><NewLine>	at com.android.ide.common.workers.ExecutorServiceAdapter.await(ExecutorServiceAdapter.kt:108)<NewLine>	at com.android.build.gradle.internal.tasks.CheckDuplicateClassesDelegate.run(CheckDuplicateClassesDelegate.kt:67)<NewLine>	at com.android.build.gradle.internal.tasks.CheckDuplicateClassesTask.doTaskAction(CheckDuplicateClassesTask.kt:61)<NewLine>	at com.android.build.gradle.internal.tasks.NonIncrementalTask$taskAction$$inlined$recordTaskAction$1.invoke(AndroidVariantTask.kt:51)<NewLine>	at com.android.build.gradle.internal.tasks.NonIncrementalTask$taskAction$$inlined$recordTaskAction$1.invoke(AndroidVariantTask.kt:31)<NewLine>	at com.android.build.gradle.internal.tasks.Blocks.recordSpan(Blocks.java:91)<NewLine>	at com.android.build.gradle.internal.tasks.NonIncrementalTask.taskAction(NonIncrementalTask.kt:34)<NewLine>	at sun.reflect.GeneratedMethodAccessor1224.invoke(Unknown Source)<NewLine>	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<NewLine>	at java.lang.reflect.Method.invoke(Method.java:498)<NewLine>	at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:103)<NewLine>	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.doExecute(StandardTaskAction.java:48)<NewLine>	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:41)<NewLine>	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:28)<NewLine>	at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:702)<NewLine>	at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:669)<NewLine>	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$5.run(ExecuteActionsTaskExecuter.java:401)<NewLine>	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:402)<NewLine>	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:394)<NewLine>	at org.gradle.internal.operations.DefaultBuildOperationExecutor$1.execute(DefaultBuildOperationExecutor.java:165)<NewLine>	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:250)<NewLine>	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:158)<NewLine>	at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:92)<NewLine>	at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)<NewLine>	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:390)<NewLine>	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:373)<NewLine>	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.access$200(ExecuteActionsTaskExecuter.java:79)<NewLine>	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$TaskExecution.execute(ExecuteActionsTaskExecuter.java:210)<NewLine>	at org.gradle.internal.execution.steps.ExecuteStep.lambda$execute$1(ExecuteStep.java:33)<NewLine>	at java.util.Optional.orElseGet(Optional.java:267)<NewLine>	at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:33)<NewLine>	at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:26)<NewLine>	at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:58)<NewLine>	at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:35)<NewLine>	at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48)<NewLine>	at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33)<NewLine>	at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39)<NewLine>	at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73)<NewLine>	at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54)<NewLine>	at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35)<NewLine>	at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51)<NewLine>	at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45)<NewLine>	at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31)<NewLine>	at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:201)<NewLine>	at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70)<NewLine>	at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45)<NewLine>	at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49)<NewLine>	at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43)<NewLine>	at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32)<NewLine>	at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38)<NewLine>	at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24)<NewLine>	at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96)<NewLine>	at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89)<NewLine>	at java.util.Optional.map(Optional.java:215)<NewLine>	at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54)<NewLine>	at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:38)<NewLine>	at org.gradle.internal.execution.steps.ResolveChangesStep.execute(ResolveChangesStep.java:77)<NewLine>	at org.gradle.internal.execution.steps.ResolveChangesStep.execute(ResolveChangesStep.java:37)<NewLine>	at org.gradle.internal.execution.steps.legacy.MarkSnapshottingInputsFinishedStep.execute(MarkSnapshottingInputsFinishedStep.java:36)<NewLine>	at org.gradle.internal.execution.steps.legacy.MarkSnapshottingInputsFinishedStep.execute(MarkSnapshottingInputsFinishedStep.java:26)<NewLine>	at org.gradle.internal.execution.steps.ResolveCachingStateStep.execute(ResolveCachingStateStep.java:90)<NewLine>	at org.gradle.internal.execution.steps.ResolveCachingStateStep.execute(ResolveCachingStateStep.java:48)<NewLine>	at org.gradle.internal.execution.impl.DefaultWorkExecutor.execute(DefaultWorkExecutor.java:33)<NewLine>	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:117)<NewLine>	... 38 more<NewLine>Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Duplicate class com.facebook.jni.CppException found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.CppSystemErrorException found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread$1 found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread$Destructor found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread$DestructorList found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread$DestructorStack found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread$Terminus found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.HybridClassBase found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.HybridData found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.HybridData$Destructor found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.IteratorHelper found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.MapIteratorHelper found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.NativeRunnable found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.ThreadScopeSupport found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.UnknownCppException found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine><NewLine>Go to the documentation to learn how to Fix dependency resolution errors.<NewLine>	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006)<NewLine>	at com.android.ide.common.workers.ExecutorServiceAdapter.await(ExecutorServiceAdapter.kt:102)<NewLine>	... 101 more<NewLine>Caused by: java.lang.RuntimeException: Duplicate class com.facebook.jni.CppException found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.CppSystemErrorException found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread$1 found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread$Destructor found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread$DestructorList found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread$DestructorStack found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.DestructorThread$Terminus found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.HybridClassBase found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.HybridData found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.HybridData$Destructor found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.IteratorHelper found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.MapIteratorHelper found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.NativeRunnable found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.ThreadScopeSupport found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine>Duplicate class com.facebook.jni.UnknownCppException found in modules classes.jar (com.facebook.react:react-native:0.61.3) and classes.jar (org.pytorch:pytorch_android_fbjni:1.4.0)<NewLine><NewLine>Go to the documentation to learn how to Fix dependency resolution errors.<NewLine>	at com.android.build.gradle.internal.tasks.CheckDuplicatesRunnable.run(CheckDuplicateClassesDelegate.kt:128)<NewLine>	at com.android.ide.common.workers.ExecutorServiceAdapter$submit$submission$1.run(ExecutorServiceAdapter.kt:68)<NewLine>	at java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1386)<NewLine>	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)<NewLine>	at java.util.concurrent.ForkJoinTask.externalInterruptibleAwaitDone(ForkJoinTask.java:361)<NewLine>	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1001)<NewLine>	... 102 more<NewLine></code></pre><NewLine><p>In my <code>build.gradle</code> file, I have this:</p><NewLine><pre><code class=""lang-auto"">    implementation 'org.pytorch:pytorch_android:1.4.0'<NewLine>    implementation 'org.pytorch:pytorch_android_torchvision:1.4.0'<NewLine></code></pre><NewLine><p>Haven’t seen much of anything on the internet related to this issue. Any thoughts?</p><NewLine><p>Thanks ahead of time.</p><NewLine></div>",https://discuss.pytorch.org/u/Neeraj_Kapoor,(Neeraj Kapoor),Neeraj_Kapoor,"April 4, 2020,  7:02am",,,,,
74879,Lib size almost does not change when doing custom build on iOS compared to Android,2020-03-31T07:39:12.251Z,0,130,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Folks,</p><NewLine><p>On master branch when doing custom build for Android I have got 40% decrease in Lib size (for libpytorch_jni.so). However when doing custom build for iOS with the same .yaml file I have got about 2.7% decrease (for libtorch_cpu.a).<br/><NewLine>I did custom builds as described here: <a href=""https://pytorch.org/mobile/home/"" rel=""nofollow noopener"">https://pytorch.org/mobile/home/</a></p><NewLine><p>I have also created following GitHub issue:<br/><NewLine><a href=""https://github.com/pytorch/pytorch/issues/35306"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/35306</a></p><NewLine><p>Regards,<br/><NewLine>Hovhannes</p><NewLine></div>",https://discuss.pytorch.org/u/hhov,(Hovhannes Harutyunyan),hhov,"March 31, 2020,  7:39am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/hhov"">@hhov</a>  The size reduction happens when you link your static libraries to output the final binary, as linker can strip most of the unused code ( <code>-dead_strip</code> ). Unlike dynamic libraries(.so in Android), linker can’t strip code when compiling static libraries.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/xta0"">@xta0</a> Thanks for info. Besides comparing .a file sizes I also checked size as described here:<a href=""https://stackoverflow.com/questions/32003262/find-size-contributed-by-each-external-library-on-ios/39593377"" rel=""nofollow noopener"">https://stackoverflow.com/questions/32003262/find-size-contributed-by-each-external-library-on-ios/39593377</a><br/><NewLine>Both ways gave insignificant difference. I tried also with <code>-dead_strip</code> but seams XCode is already doing it.<br/><NewLine>After that I tried simply comparing final .ipa files and result was as expected, about 40% decrease in size.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hhov; <NewLine> ,"REPLY_DATE 1: March 31, 2020,  8:23pm; <NewLine> REPLY_DATE 2: April 1, 2020,  7:23pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
72243,com.facebook.jni.CppException: version_ &lt;= kMaxSupportedFileFormatVersion,2020-03-06T07:02:12.264Z,0,798,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to integrate keypoint_rcnn on android. But after solving errors i succeeded to generate .pt file using following script with torch 1.5 and torchvision 0.6.0</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torchvision<NewLine><NewLine>model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)<NewLine>model.eval()<NewLine>example = torch.rand(1, 3, 800, 800)<NewLine>traced_script_module = torch.jit.script(model, example)<NewLine>traced_script_module.save(""/home/bemrr/Desktop/model4.pt"")<NewLine></code></pre><NewLine><p>But now while installing it on android device, error is like this:</p><NewLine><blockquote><NewLine><p>java.lang.RuntimeException: Unable to start activity ComponentInfo{org.pytorch.helloworld/org.pytorch.helloworld.MainActivity}: com.facebook.jni.CppException: version_ &lt;= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED at …/caffe2/serialize/inline_container.cc:132, please report a bug to PyTorch. Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2. Your PyTorch installation may be too old. (init at …/caffe2/serialize/inline_container.cc:132)</p><NewLine></blockquote><NewLine><p>Thats why I am assuming that as my gradle dependency is like</p><NewLine><pre><code class=""lang-auto"">implementation 'org.pytorch:pytorch_android:1.4.0'<NewLine>implementation 'org.pytorch:pytorch_android_torchvision:1.4.0'<NewLine></code></pre><NewLine><p>is incompatible with .pt file which is generated using torch 1.5, so I want to take a pull from latest pytorch repo and then build torch mobile using source. But I can not find any good instructions anywhere. Can someone please help me?</p><NewLine></div>",https://discuss.pytorch.org/u/hurricane013,(parth),hurricane013,"March 6, 2020,  7:34am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://pytorch.org/favicon.ico?"" width=""32""/><NewLine><a href=""https://pytorch.org/mobile/android/#building-pytorch-android-from-source"" target=""_blank"">pytorch.org</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""500"" src=""https://pytorch.org/assets/images/pytorch-logo.png"" width=""500""/><NewLine><h3><a href=""https://pytorch.org/mobile/android/#building-pytorch-android-from-source"" target=""_blank"">PyTorch</a></h3><NewLine><p>An open source deep learning platform that provides a seamless path from research prototyping to production deployment.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>(See the section “Building PyTorch Android from Source”)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: March 25, 2020,  5:09pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
73111,Problems with LAPACK on Android,2020-03-13T10:41:54.076Z,0,321,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello! I’m trying to build libtorch for android with linear algebra support LAPACK. For building I’ve used this guide: <a href=""https://github.com/pytorch/pytorch/tree/master/android"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/tree/master/android</a> and similar solution for iOS <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/problems-with-lapack-on-ios/69240"">Problems with LAPACK on iOS</a></p><NewLine><p>After adding flag  CMAKE_ARGS+=(""-DUSE_LAPACK=ON"")<br/><NewLine>The compiler results in error linking LAPACK (some external function are not found)</p><NewLine><pre><code class=""lang-auto"">&gt; Task :fbjni:compileReleaseJavaWithJavac<NewLine>Note: Some input files use unchecked or unsafe operations.<NewLine>....<NewLine>&gt; Task :pytorch_android:externalNativeBuildRelease<NewLine>Build multiple targets pytorch_jni_arm64-v8a fbjni_arm64-v8a<NewLine>ninja: Entering directory <NewLine>....<NewLine>[17/17] Linking CXX shared library ../../../../build/intermediates/cmake/release/obj/arm64-v8a/libpytorch_jni.so<NewLine>FAILED: ../../../../build/intermediates/cmake/release/obj/arm64-v8a/libpytorch_jni.so<NewLine>: &amp;&amp; /home/dmitryv/Android/Sdk/ndk/20.0.5594570/toolchains/llvm/prebuilt/linux-x86_64/bin/clang++ --target=aarch64-none-linux-android21 --gcc-toolchain=/home/dmitryv/Android/Sdk/ndk/20.0.5594570/toolchains/llvm/prebuilt/linux-x86_64 --sysroot=/home/dmitryv/Android/Sdk/ndk/20.0.5594570/toolchains/llvm/prebuilt/linux-x86_64/sysroot -fPIC -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -fno-addrsig -Wa,--noexecstack -Wformat -Werror=format-security   -O2 -DNDEBUG  -Wl,--exclude-libs,libgcc.a -Wl,--exclude-libs,libatomic.a -static-libstdc++ -Wl,--build-id -Wl,--warn-shared-textrel -Wl,--fatal-warnings -Wl,--no-undefined -Qunused-arguments -Wl,-z,noexecstack -shared -Wl,-soname,libpytorch_jni.so -o ../../../../build/intermediates/cmake/release/obj/arm64-v8a/libpytorch_jni.so CMakeFiles/pytorch_jni.dir/src/main/cpp/pytorch_jni_jit.cpp.o CMakeFiles/pytorch_jni.dir/src/main/cpp/pytorch_jni_common.cpp.o  ../../../../build/intermediates/cmake/release/obj/arm64-v8a/libfbjni.so -Wl,--gc-sections -Wl,--whole-archive ../../../../src/main/jniLibs/arm64-v8a/libtorch.a ../../../../src/main/jniLibs/arm64-v8a/libtorch_cpu.a -Wl,--no-whole-archive ../../../../src/main/jniLibs/arm64-v8a/libc10.a ../../../../src/main/jniLibs/arm64-v8a/libnnpack.a ../../../../src/main/jniLibs/arm64-v8a/libXNNPACK.a ../../../../src/main/jniLibs/arm64-v8a/libpytorch_qnnpack.a ../../../../src/main/jniLibs/arm64-v8a/libeigen_blas.a ../../../../src/main/jniLibs/arm64-v8a/libcpuinfo.a ../../../../src/main/jniLibs/arm64-v8a/libclog.a -landroid -llog -latomic -lm &amp;&amp; :<NewLine>../../../../src/main/jniLibs/arm64-v8a/libtorch_cpu.a(BatchLinearAlgebra.cpp.o): In function `void at::native::lapackSolve&lt;std::__ndk1::complex&lt;double&gt; &gt;(int, int, std::__ndk1::complex&lt;double&gt;*, int, int*, std::__ndk1::complex&lt;double&gt;*, int, int*)':<NewLine>/storage/pytorch/build_android/../aten/src/ATen/native/BatchLinearAlgebra.cpp:131: undefined reference to `zgesv_'<NewLine></code></pre><NewLine><p>Which library do I have to link?<br/><NewLine>Any thoughts would be appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/kulikovv,(Kulikov Victor),kulikovv,"March 13, 2020, 10:41am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/kulikovv"">@kulikovv</a>,<br/><NewLine>Sorry for delay with reply.</p><NewLine><p>If you are ok with supporting at the moment only armeabi-v7a, arm64-v8a, you can try to use LAPACK implementation from Qualcomm Math Library.</p><NewLine><p>I experimented with linking it to pytorch_android with <code>CMAKE_ARGS+=(""-DUSE_LAPACK=ON"")</code><br/><NewLine>It linked successfully for me.</p><NewLine><p>I prepared PR with small instructions and changes in <code>android/pytorch_android/CMakeLists.txt</code>:<br/><NewLine><aside class=""onebox githubpullrequest""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/35200"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Pull Request""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 12 16"" width=""60""><path d=""M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/pull/35200"" rel=""nofollow noopener"" target=""_blank"">[NOT_FOR_COMMIT][android] Pytorch android build with USE_LAPACK and QML lib</a><NewLine></h4><NewLine><div class=""branches""><NewLine><code>pytorch:master</code> ← <code>pytorch:ik_lapack_qml</code><NewLine></div><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-03-23"" data-format=""ll"" data-time=""04:30:46"" data-timezone=""UTC"">04:30AM - 23 Mar 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/IvanKobzarev"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""IvanKobzarev"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars3.githubusercontent.com/u/6638825?v=4"" width=""20""/><NewLine>          IvanKobzarev<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""1 commits changed 2 files with 20 additions and 0 deletions""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/35200/files"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+20</span><NewLine><span class=""removed"">-0</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>Instructions from PR that I will duplicate here:</p><NewLine><ol><NewLine><li>Download Qualcomm Math Library <a href=""https://developer.qualcomm.com/software/qualcomm-math-library"" rel=""nofollow noopener"">https://developer.qualcomm.com/software/qualcomm-math-library</a> (You might need to register new user to download it)</li><NewLine><li>Unpack it in  <code>${PYTORCH_ROOT}/android/libs</code><NewLine></li><NewLine></ol><NewLine><p>QML can be used for armeabi-v7a, arm64-v8a android abis</p><NewLine><ol start=""3""><NewLine><li>sh ./scripts/build_pytorch_android.sh arm64-v8a,armeabi-v7a</li><NewLine></ol><NewLine><p>I have not checked runtime using of it, if you test it, please reply here if it works or not.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your answer! I’ve tried your approach, you need also to include (in my case manually libQML.so into arr archive) For devices with arm32 architecture (armeabi-v7a) libQML works fine, but for arm64 the loading of libtorch.so hangs.</p><NewLine><p>Currently we try to compile and statically link CLAPACK for linear algebra support. And also with arm32 everything is good, but in the case of arm64 - incompatible function arguments.</p><NewLine><p>About the project: we use a PCA model inside or neural network as an regulariser it works pretty well, and right now we are deploying it on mobiles.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kulikovv; <NewLine> ,"REPLY_DATE 1: March 23, 2020,  4:36am; <NewLine> REPLY_DATE 2: March 24, 2020,  7:48am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
73494,Module.forward() method run slowly on Pixel 3,2020-03-17T09:10:41.283Z,0,246,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to run PyTorch on Android system following this <a href=""https://pytorch.org/mobile/android/"" rel=""nofollow noopener"">document</a>. And the model is working fine, however i noticed that forward() method is consuming too much time.</p><NewLine><p>On my Pixel 3 it need average 300ms to run (MobileNetV2 from torchvision). I also tried SSD-MobileNetV2-Lite model (pretrained) from <a href=""https://github.com/qfgaohao/pytorch-ssd"" rel=""nofollow noopener"">this</a> repo, and it cost about 700ms to run.</p><NewLine><p>Same model on Tensorflow mobile seems much faster.</p><NewLine><p>Can anyone tell me why and how to solve this performance issue?</p><NewLine></div>",https://discuss.pytorch.org/u/royaff0,(ROYA),royaff0,"March 17, 2020,  9:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/royaff0"">@royaff0</a>,</p><NewLine><ol><NewLine><li><NewLine><p>Could you please try using our nightlies  ( <a href=""https://github.com/pytorch/pytorch/tree/master/android#nightly"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/tree/master/android#nightly</a> ) what will be the performance. ( Some significant performance improvements were merged recently )</p><NewLine></li><NewLine><li><NewLine><p>You may try to use quantized pre-trained model (for example mobilenet_v2, specifying quantize=True)<br/><NewLine><a href=""https://github.com/pytorch/vision/blob/master/torchvision/models/quantization/mobilenet.py#L59"" rel=""nofollow noopener"">https://github.com/pytorch/vision/blob/master/torchvision/models/quantization/mobilenet.py#L59</a><br/><NewLine>Are quantize accuracy and performance acceptable for your task?</p><NewLine></li><NewLine></ol><NewLine><p>Tensorflow lite has the support of mobile GPU, while PyTorch android at the moment only uses CPU, but we are working on this support at the moment.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> ,"REPLY_DATE 1: March 23, 2020,  8:23am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
72990,Error using custom face alignment model on IOS,2020-03-12T13:06:47.015Z,3,262,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Im trying to build and run an IOS App for facial recognition using a custom face alignment model generated with pytorch.</p><NewLine><p>I followed the tutorial to setup libtorch with cocoapod on my xcode project, and managed to load and use my custom model in C++ code.<br/><NewLine>Everything worked fine and the app run on an IOS simulator, I’m able detect faces on images.</p><NewLine><p>But running the app on a real Iphone give the following error in xcode when calling for my model forward method:</p><NewLine><blockquote><NewLine><p><strong>libc++abi.dylib: terminating with uncaught exception of type std::runtime_error: NNPACK SpatialConvolution_updateOutput failed</strong><br/><NewLine><strong>The above operation failed in interpreter.</strong><br/><NewLine><strong>Traceback (most recent call last):</strong><br/><NewLine><strong>/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py(342): conv2d_forward</strong><br/><NewLine><strong>/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py(345): forward</strong><br/><NewLine><strong>/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py(525): _slow_forward</strong><br/><NewLine><strong>/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py(539): <strong>call</strong></strong><br/><NewLine><strong>/usr/local/lib/python3.5/dist-packages/face_alignment/detection/sfd/net_s3fd.py(97): forward</strong><br/><NewLine><strong>torch_compilo.py(47): forward</strong><br/><NewLine><strong>/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py(525): _slow_forward</strong><br/><NewLine><strong>/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py(539): <strong>call</strong></strong><br/><NewLine><strong>/usr/local/lib/python3.5/dist-packages/torch/jit/<strong>init</strong>.py(997): trace_module</strong><br/><NewLine><strong>/usr/local/lib/python3.5/dist-packages/torch/jit/<strong>init</strong>.py(865): trace</strong><br/><NewLine><strong>torch_compilo.py(53): </strong><br/><NewLine><strong>Serialized   File “code/<strong>torch</strong>/face_alignment/detection/sfd/net_s3fd.py”, line 206</strong><br/><NewLine><strong>input29 = torch.relu(input28)</strong><br/><NewLine><strong>input30 = torch.max_pool2d(input29, [2, 2], [2, 2], [0, 0], [1, 1], False)</strong><br/><NewLine><strong>input31 = torch._convolution(input30, weight12, _27, [1, 1], [3, 3], [1, 1], False, [0, 0], 1, False, False, True)</strong><br/><NewLine>**~~~~~~~~~~~~~~~~~~ &lt;— HERE**<br/><NewLine><strong>input32 = torch.relu(input31)</strong><br/><NewLine><strong>input33 = torch._convolution(input32, weight13, _29, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True)</strong></p><NewLine></blockquote><NewLine><p>Any help regarding this issue will be appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/Lambda,(Lambda),Lambda,"March 12, 2020,  1:17pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/lambda"">@Lambda</a>. Can you take a look at this one?  <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/cant-run-nn-conv1d-with-libtorch-on-ios/71719"">Can't run nn.Conv1d with libtorch on iOS</a>. I guess the error is the same, just disable NNPACK</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/xta0"">@xta0</a> and thank you for your answer.</p><NewLine><p>I followed your instructions, build from source code for mobile device withe NNPACK disabled</p><NewLine><p>I setup headers and static libraries in my xcode project without using cocoapod, It does build and run on my Iphone but I’m back to <a href=""https://discuss.pytorch.org/t/pytorch-is-not-linked-with-support-for-cpu-devices/64441"">this error</a> while loading my model in C++</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you add <code>all_load</code> to your linker flags?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>That probably the part where I’m having an issue.</p><NewLine><p>In other linker flags I need to add the path of each archive file in /install/lib in order to build on IOS instead of only adding the libtorch.a path as is it said in the IOS Pytorch Tutorial.</p><NewLine><p>It looks like that</p><NewLine><blockquote><NewLine><p>$(PROJECT_DIR)/pytorch/install/lib/libtorch.a<br/><NewLine>$(PROJECT_DIR)/pytorch/install/lib/libc10.a<br/><NewLine>$(PROJECT_DIR)/pytorch/install/lib/libcpuinfo.a<br/><NewLine>$(PROJECT_DIR)/pytorch/install/lib/libpthreadpool.a $(PROJECT_DIR)/pytorch/install/lib/libpytorch_qnnpack.a $(PROJECT_DIR)/pytorch/install/lib/libtorch_cpu.a<br/><NewLine>$(PROJECT_DIR)/pytorch/install/lib/libclog.a<br/><NewLine>$(PROJECT_DIR)/pytorch/install/lib/libeigen_blas.a</p><NewLine></blockquote><NewLine><p>Adding -all_load flag bring me to this error</p><NewLine><pre><code class=""lang-auto"">duplicate symbol '_pthreadpool_compute_4d_tiled' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-legacy.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool.cc.o)<NewLine><NewLine>duplicate symbol '_pthreadpool_compute_3d_tiled' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-legacy.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool.cc.o)<NewLine><NewLine>duplicate symbol '_pthreadpool_compute_2d_tiled' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-legacy.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool.cc.o)<NewLine><NewLine>duplicate symbol '_pthreadpool_compute_1d_tiled' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-legacy.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool.cc.o)<NewLine><NewLine>duplicate symbol '_pthreadpool_compute_2d' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-legacy.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool.cc.o)<NewLine><NewLine>duplicate symbol '_pthreadpool_destroy' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-pthreads.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool_impl.cc.o)<NewLine><NewLine>duplicate symbol '_pthreadpool_get_threads_count' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-pthreads.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool_impl.cc.o)<NewLine><NewLine>duplicate symbol '_pthreadpool_create' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-pthreads.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool_impl.cc.o)<NewLine><NewLine>duplicate symbol '_pthreadpool_compute_1d' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-legacy.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool_impl.cc.o)<NewLine><NewLine>duplicate symbol '_pthreadpool_parallelize_2d' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-pthreads.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool_new_if_impl.c.o)<NewLine><NewLine>duplicate symbol '_pthreadpool_parallelize_6d_tile_2d' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-pthreads.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool_new_if_impl.c.o)<NewLine><NewLine>duplicate symbol '_pthreadpool_parallelize_5d_tile_2d' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-pthreads.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool_new_if_impl.c.o)<NewLine><NewLine>duplicate symbol '_pthreadpool_parallelize_4d_tile_2d' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-pthreads.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool_new_if_impl.c.o)<NewLine><NewLine>duplicate symbol '_pthreadpool_parallelize_3d_tile_2d' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-pthreads.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool_new_if_impl.c.o)<NewLine><NewLine>duplicate symbol '_pthreadpool_parallelize_2d_tile_2d' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-pthreads.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool_new_if_impl.c.o)<NewLine><NewLine>duplicate symbol '_pthreadpool_parallelize_1d' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-pthreads.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool_new_if_impl.c.o)<NewLine><NewLine>duplicate symbol '_pthreadpool_parallelize_2d_tile_1d' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-pthreads.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool_new_if_impl.c.o)<NewLine><NewLine>duplicate symbol '_pthreadpool_parallelize_1d_tile_1d' in:<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libpthreadpool.a(threadpool-pthreads.c.o)<NewLine><NewLine>    /Users/root_anatoscope/Documents/cross-compile/opencvxcode/pytorch/install/lib/libtorch_cpu.a(pthreadpool_new_if_impl.c.o)<NewLine><NewLine>ld: 18 duplicate symbols for architecture arm64<NewLine><NewLine>clang: error: linker command failed with exit code 1 (use -v to see invocation)<NewLine></code></pre><NewLine><p>-force_load flag bring me to the previous issue</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Lambda; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Lambda; <NewLine> ,"REPLY_DATE 1: March 13, 2020,  2:14am; <NewLine> REPLY_DATE 2: March 13, 2020, 10:44am; <NewLine> REPLY_DATE 3: March 19, 2020, 10:28pm; <NewLine> REPLY_DATE 4: March 20, 2020, 10:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
67340,Speed benchmarking on android?,2020-01-22T06:55:24.756Z,4,809,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am interested to know how fast some of my models run on the CPUs of a Pixel 3 phone. I am a moderately experienced pytorch programmer and linux user, but I have zero experience with android. I am not looking to build an app right now; I just want to know how fast my model runs on this particular phone.</p><NewLine><p>The TensorFlow repo has this barebones android test thingy for timing the latency of a neural net of your choice on an android phone in TensorFlow: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/android/README.md"" rel=""nofollow noopener"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/android/README.md</a></p><NewLine><p>Has anyone made anything similar for pytorch?</p><NewLine></div>",https://discuss.pytorch.org/u/solvingPuzzles,,solvingPuzzles,"January 22, 2020,  6:55am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We have a binary to do this that can run on your android phone using adb.</p><NewLine><p>To build,</p><NewLine><pre><code class=""lang-auto"">./scripts/build_android.sh \                                                                                                                       <NewLine>-DBUILD_BINARY=ON \<NewLine>-DBUILD_CAFFE2_MOBILE=OFF \<NewLine>-DCMAKE_PREFIX_PATH=$(python -c 'from distutils.sysconfig import get_python_lib; print(get_python_lib())') \<NewLine>-DPYTHON_EXECUTABLE=$(python -c 'import sys; print(sys.executable)')<NewLine></code></pre><NewLine><p>To run the binary, push it to the device using adb and run the following command<br/><NewLine><code>./speed_benchmark_torch --model=model.pt  --input_dims=""1,3,224,224"" --input_type=float --warmup=10 --iter 10 --report_pep true </code></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/supriyar"">@supriyar</a> do you know if <a href=""https://github.com/facebook/FAI-PEP"" rel=""nofollow noopener"">FAI-PEP</a> will integrate with pytorch mobile?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It should be possible, we already output the total network latency in a format that is acceptable by FAI-PEP.<br/><NewLine>The flow should be similar to existing caffe2 for mobile, but use the <code>speed_benchmark_torch</code> binary instead.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks! Is there a certain NDK version that is preferred? I know in TensorFlow, they like using old NDK versions for some reason.</p><NewLine><p>Also, do we need the Android SDK to be visible to PyTorch anywhere?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>The instructions for <code>speed_benchmark_torch</code> worked for me on the first try!</p><NewLine><p>If anyone else wants try this on a Pixel 3 android phone, here is the setup that worked for me:</p><NewLine><pre><code class=""lang-auto""># in bash shell<NewLine>cd pytorch #where I have my `git clone` of pytorch<NewLine><NewLine>export ANDROID_ABI=arm64-v8a<NewLine>export ANDROID_NDK=/path/to/Android/Sdk/ndk/21.0.6113669/<NewLine><NewLine>./scripts/build_android.sh \<NewLine>-DBUILD_BINARY=ON \<NewLine>-DBUILD_CAFFE2_MOBILE=OFF \<NewLine>-DCMAKE_PREFIX_PATH=$(python -c 'from distutils.sysconfig import get_python_lib; print(get_python_lib())') \<NewLine>-DPYTHON_EXECUTABLE=$(python -c 'import sys; print(sys.executable)') \<NewLine><NewLine># speed_benchmark_torch appears in pytorch/build_android/install/bin/speed_benchmark_torch<NewLine></code></pre><NewLine><p>Next, I followed <a href=""https://github.com/pytorch/android-demo-app"" rel=""nofollow noopener"">these instructions</a> to export a resnet18 torchscript model:</p><NewLine><pre><code class=""lang-auto"">#in python<NewLine>import torch<NewLine>import torchvision<NewLine><NewLine>model = torchvision.models.resnet18(pretrained=True)<NewLine>model.eval()<NewLine>example = torch.rand(1, 3, 224, 224)<NewLine>traced_script_module = torch.jit.trace(model, example)<NewLine>traced_script_module.save(""resnet18.pt"")<NewLine></code></pre><NewLine><p>Then, I put the files onto the android device</p><NewLine><pre><code class=""lang-auto""># in bash shell on linux host computer that's plugged into Pixel 3 phone<NewLine>adb shell mkdir /data/local/tmp/pt<NewLine>adb push build_android/install/bin/speed_benchmark_torch /data/local/tmp/pt<NewLine>adb push resnet18.pt /data/local/tmp/pt<NewLine></code></pre><NewLine><p>And finally I run on the android device</p><NewLine><pre><code class=""lang-auto""># in bash shell on linux host computer that's plugged into Pixel 3 phone<NewLine>adb shell  /data/local/tmp/pt/speed_benchmark_torch \<NewLine>--model  /data/local/tmp/pt/resnet18.pt --input_dims=""1,3,224,224"" \<NewLine>--input_type=float --warmup=5 --iter 20<NewLine><NewLine></code></pre><NewLine><p>It prints:</p><NewLine><pre><code class=""lang-auto"">Starting benchmark.<NewLine>Running warmup runs.<NewLine><NewLine>Main runs.<NewLine>Main run finished. Milliseconds per iter: 188.382. Iters per second: 5.30836<NewLine></code></pre><NewLine><p>Pretty good! I believe resnet18 is about 4 gflop (that is, 2 gmac) per frame, so (4 gmac) / (188 ms) = 21 gflop/s. Not bad for ARM CPUs! (At least I assume it’s executing on the ARM CPUs and not any GPUs or other accelerators.)</p><NewLine><p>Also, this whole process took me about 25 minutes, and everything worked on the first try. I use pytorch day-to-day, but I have very little experience with android, and this was also my first time using torchscript, so I’m surprised and impressed that it was so straightforward.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>This thread is very useful and I’m trying to get this working. I can’t get past the step where build_android.sh is run without a bunch of errors. You can view my CMakeError.log <a href=""https://drive.google.com/file/d/1ZXvdvsA05d7Dwdqb2PgihLK9uppayKMy/view?usp=sharing"" rel=""nofollow noopener"">here</a>. Does anyone know what’s going on here? Alternatively, if someone could link me their speed_benchmark_torch executable that might also work.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/nufsty2"">@nufsty2</a>, Could I see your build command, please?</p><NewLine><p>And, what operating system do you have on the computer where you are compiling the binary?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/solvingpuzzles"">@solvingPuzzles</a> I’m in Ubuntu 18.04. I actually just got it working. I had to run<br/><NewLine><code>git submodule update --init --recursive</code><br/><NewLine>within the pytorch clone as well as run the</p><NewLine><pre><code class=""lang-auto"">./scripts/build_android.sh \<NewLine>-DBUILD_BINARY=ON \<NewLine>-DBUILD_CAFFE2_MOBILE=OFF \<NewLine>-DCMAKE_PREFIX_PATH=$(python -c 'from distutils.sysconfig import get_python_lib; print(get_python_lib())') \<NewLine>-DPYTHON_EXECUTABLE=$(python -c 'import sys; print(sys.executable)')<NewLine></code></pre><NewLine><p>command as <code>sudo -E</code>. I also had to run the python commands as sudo so it could actually write the .pt file.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/nufsty2"">@nufsty2</a> Way to go!</p><NewLine><p>That’s a bit odd <code>sudo</code> was needed to write the .pt file. I haven’t had to use sudo for that. Perhaps you’re saving the .pt in a write-protected directory?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/solvingpuzzles"">@solvingPuzzles</a> hmm… maybe? Thanks for the help though!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/RicCu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/solvingPuzzles; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/solvingPuzzles; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/nufsty2; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/solvingPuzzles; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/nufsty2; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/solvingPuzzles; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/nufsty2; <NewLine> ,"REPLY_DATE 1: January 24, 2020,  7:53am; <NewLine> REPLY_DATE 2: January 23, 2020, 12:20am; <NewLine> REPLY_DATE 3: January 23, 2020,  2:16am; <NewLine> REPLY_DATE 4: January 24, 2020,  2:40am; <NewLine> REPLY_DATE 5: January 24, 2020,  7:51am; <NewLine> REPLY_DATE 6: March 16, 2020,  5:38pm; <NewLine> REPLY_DATE 7: March 16, 2020,  6:09pm; <NewLine> REPLY_DATE 8: March 16, 2020,  6:42pm; <NewLine> REPLY_DATE 9: March 16, 2020,  6:50pm; <NewLine> REPLY_DATE 10: March 16, 2020, 10:32pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 2 Likes; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> 
61009,[Android] Something&rsquo;s went wrong with pytorch_android-1.4.0-SNAPSHOT,2019-11-14T17:18:47.805Z,4,809,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Tried to set up HelloWorldApp (<a href=""https://github.com/pytorch/android-demo-app"" rel=""nofollow noopener"">https://github.com/pytorch/android-demo-app</a>) with the most latest version of PyTorch built from source.<br/><NewLine>After a little bit of struggling with gradle build I’ve ended up with crash on devices.</p><NewLine><p>Here is the <code>build.gradle :app</code></p><NewLine><pre><code class=""lang-auto"">apply plugin: 'com.android.application'<NewLine><NewLine>repositories {<NewLine>    jcenter()<NewLine>    flatDir {<NewLine>        dirs 'libs'<NewLine>    }<NewLine>}<NewLine><NewLine>android {<NewLine>    packagingOptions {<NewLine>        pickFirst ""**/libfbjni.so""<NewLine>    }<NewLine>    compileSdkVersion 28<NewLine>    buildToolsVersion ""29.0.2""<NewLine>    defaultConfig {<NewLine>        applicationId ""org.pytorch.helloworld""<NewLine>        minSdkVersion 21<NewLine>        targetSdkVersion 28<NewLine>        versionCode 1<NewLine>        versionName ""1.0""<NewLine>    }<NewLine>    buildTypes {<NewLine>        release {<NewLine>            minifyEnabled false<NewLine>        }<NewLine>    } <NewLine>}<NewLine><NewLine>dependencies {<NewLine>    implementation 'androidx.appcompat:appcompat:1.1.0'<NewLine>    // implementation 'org.pytorch:pytorch_android:1.3.0'<NewLine>    // implementation 'org.pytorch:pytorch_android_torchvision:1.3.0'<NewLine>    implementation(name:'pytorch_android-1.4.0-SNAPSHOT', ext:'aar')<NewLine>    implementation(name:'pytorch_android_torchvision-1.4.0-SNAPSHOT', ext:'aar')<NewLine>    implementation(name:'pytorch_android_fbjni-1.4.0-SNAPSHOT', ext:'aar')<NewLine>}<NewLine></code></pre><NewLine><p>Note: <code>.aar</code> libs are present</p><NewLine><p>Crash stacktrace</p><NewLine><pre><code class=""lang-auto"">2019-10-04 22:31:16.059 16203-16203/org.pytorch.helloworld E/AndroidRuntime: FATAL EXCEPTION: main<NewLine>    Process: org.pytorch.helloworld, PID: 16203<NewLine>    java.lang.NoClassDefFoundError: Failed resolution of: Lcom/facebook/soloader/nativeloader/NativeLoader;<NewLine>        at org.pytorch.Module.&lt;init&gt;(Module.java:27)<NewLine>        at org.pytorch.Module.load(Module.java:23)<NewLine>        at org.pytorch.helloworld.MainActivity.onCreate(MainActivity.java:39)<NewLine>        at android.app.Activity.performCreate(Activity.java:8068)<NewLine>        at android.app.Activity.performCreate(Activity.java:8056)<NewLine>        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1320)<NewLine>        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3757)<NewLine>        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3955)<NewLine>        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:91)<NewLine>        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:149)<NewLine>        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:103)<NewLine>        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2392)<NewLine>        at android.os.Handler.dispatchMessage(Handler.java:107)<NewLine>        at android.os.Looper.loop(Looper.java:213)<NewLine>        at android.app.ActivityThread.main(ActivityThread.java:8147)<NewLine>        at java.lang.reflect.Method.invoke(Native Method)<NewLine>        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:513)<NewLine>        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1100)<NewLine>     Caused by: java.lang.ClassNotFoundException: Didn't find class ""com.facebook.soloader.nativeloader.NativeLoader"" on path: DexPathList[[zip file ""/data/app/org.pytorch.helloworld-eCefKQPfLWFqNPNZZUphIA==/base.apk""],nativeLibraryDirectories=[/data/app/org.pytorch.helloworld-eCefKQPfLWFqNPNZZUphIA==/lib/arm64, /data/app/org.pytorch.helloworld-eCefKQPfLWFqNPNZZUphIA==/base.apk!/lib/arm64-v8a, /system/lib64, /system/product/lib64, /hw_product/lib64, /system/product/lib64]]<NewLine>        at dalvik.system.BaseDexClassLoader.findClass(BaseDexClassLoader.java:196)<NewLine>        at java.lang.ClassLoader.loadClass(ClassLoader.java:379)<NewLine>        at java.lang.ClassLoader.loadClass(ClassLoader.java:312)<NewLine>        at org.pytorch.Module.&lt;init&gt;(Module.java:27) <NewLine>        at org.pytorch.Module.load(Module.java:23) <NewLine>        at org.pytorch.helloworld.MainActivity.onCreate(MainActivity.java:39) <NewLine>        at android.app.Activity.performCreate(Activity.java:8068) <NewLine>        at android.app.Activity.performCreate(Activity.java:8056) <NewLine>        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1320) <NewLine>        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3757) <NewLine>        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3955) <NewLine>        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:91) <NewLine>        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:149) <NewLine>        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:103) <NewLine>        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2392) <NewLine>        at android.os.Handler.dispatchMessage(Handler.java:107) <NewLine>        at android.os.Looper.loop(Looper.java:213) <NewLine>        at android.app.ActivityThread.main(ActivityThread.java:8147) <NewLine>        at java.lang.reflect.Method.invoke(Native Method) <NewLine>        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:513) <NewLine>        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1100) <NewLine></code></pre><NewLine><p>May be we should not rely on unstable version but <code>module = Module.load()</code> is a core feature <img alt="":slightly_smiling_face:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slightly_smiling_face.png?v=9"" title="":slightly_smiling_face:""/></p><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"November 14, 2019,  5:19pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There is still an issue.<br/><NewLine>Here’s what I’ve noticed except an issue <code>pytorch_android_torchvision-1.4.0-SNAPSHOT.aar</code> works with <code>org.pytorch:pytorch_android:1.3.0</code> from remote repo.<br/><NewLine>The problem occurs only with <code>pytorch_android-1.4.0-SNAPSHOT.aar</code></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello Emil,</p><NewLine><p>If you have not changed the source - you can use our nightly builds that are published every night to nexus sonatype maven repo.</p><NewLine><p>To use it:</p><NewLine><p>repositories {<br/><NewLine>…<br/><NewLine>maven {<br/><NewLine>url “<a href=""https://oss.sonatype.org/content/repositories/snapshots"" rel=""nofollow noopener"">https://oss.sonatype.org/content/repositories/snapshots</a>”<br/><NewLine>}<br/><NewLine>}</p><NewLine><p>dependencies {<br/><NewLine>implementation ‘androidx.appcompat:appcompat:1.1.0’<br/><NewLine>implementation ‘org.pytorch:pytorch_android:1.4.0-SNAPSHOT’<br/><NewLine>implementation ‘org.pytorch:pytorch_android_torchvision:1.4.0-SNAPSHOT’<br/><NewLine>}</p><NewLine><p>I rechecked HelloWorldApp with the latest published 1.4.0-SNAPSHOT - it works fine.</p><NewLine><ol start=""2""><NewLine><li>About your failure:</li><NewLine></ol><NewLine><p>It looks like that in your aar you do not have dependency: ( com.facebook.soloader:nativeloader )<br/><NewLine>which was added after 1.3.0 release:<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/build.gradle#L59"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/build.gradle#L59"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/master/android/pytorch_android/build.gradle#L59</a></h4><NewLine><pre class=""onebox""><code class=""lang-gradle""><ol class=""start lines"" start=""49"" style=""counter-reset: li-counter 48 ;""><NewLine><li><NewLine></li><NewLine><li>    useLibrary 'android.test.runner'</li><NewLine><li>    useLibrary 'android.test.base'</li><NewLine><li>    useLibrary 'android.test.mock'</li><NewLine><li>}</li><NewLine><li><NewLine></li><NewLine><li>dependencies {</li><NewLine><li>    api project(':fbjni')</li><NewLine><li><NewLine></li><NewLine><li>    implementation 'com.android.support:appcompat-v7:28.0.0'</li><NewLine><li class=""selected"">    implementation 'com.facebook.soloader:nativeloader:0.8.0'</li><NewLine><li><NewLine></li><NewLine><li>    testImplementation 'junit:junit:' + rootProject.junitVersion</li><NewLine><li>    testImplementation 'androidx.test:core:' + rootProject.coreVersion</li><NewLine><li><NewLine></li><NewLine><li>    androidTestImplementation 'junit:junit:' + rootProject.junitVersion</li><NewLine><li>    androidTestImplementation 'androidx.test:core:' + rootProject.coreVersion</li><NewLine><li>    androidTestImplementation 'androidx.test.ext:junit:' + rootProject.extJUnitVersion</li><NewLine><li>    androidTestImplementation 'androidx.test:rules:' + rootProject.rulesVersion</li><NewLine><li>    androidTestImplementation 'androidx.test:runner:' + rootProject.runnerVersion</li><NewLine><li>}</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>Please add it manually:<br/><NewLine>implementation ‘com.facebook.soloader:nativeloader:0.8.0’</p><NewLine><p>In case of using maven dependency gradle will parse pom file that has this dependency and add it automatically. For manual aar files we have to add all transitive dependencies manually.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>hello Emil,</p><NewLine><p>I am having trouble while trying to deploy keypoint_rcnn in android. Can you please explain me how to build latest pytorch mobile using source? I read documentation but its not clear enough for me. Can you please help me out?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>The latest source is built daily so I recommend you to try to follow the solution that Ivan has provided.<br/><NewLine>This snapshot version is the most recent. Add the repository and pytorch mobile library will be downloaded automatically</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have tried with adding dependency pytorch 1.4 snapshot to export keypoint_rcnn in android. But now build fails without showing any errors.  what should be exact problem?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>At first I think you need to make sure that your TorchScript model works well in Python and it can be found in the right place on your phone.</p><NewLine><p>At the second point I think there should be an errors if the build crashes</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes my torchscript  model works well on python. and there is no error after failing of build. By the way, I came to know that they haven’t added support for keypoint_rcnn yet. Take a look at this : <a href=""https://github.com/pytorch/vision/issues/1943"" rel=""nofollow noopener"">https://github.com/pytorch/vision/issues/1943</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/hurricane013; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/hurricane013; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/hurricane013; <NewLine> ,"REPLY_DATE 1: November 15, 2019,  9:45am; <NewLine> REPLY_DATE 2: November 18, 2019,  9:37am; <NewLine> REPLY_DATE 3: March 6, 2020, 11:14am; <NewLine> REPLY_DATE 4: March 6, 2020, 11:45am; <NewLine> REPLY_DATE 5: March 11, 2020,  8:40am; <NewLine> REPLY_DATE 6: March 11, 2020, 12:00pm; <NewLine> REPLY_DATE 7: March 12, 2020,  9:35am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
71719,Can&rsquo;t run nn.Conv1d with libtorch on iOS,2020-03-02T12:47:51.693Z,4,455,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I get an error when I want to use nn.Conv1d on iOS using libtorch. I have this simple model:</p><NewLine><pre><code class=""lang-auto"">class Conv1dTestModel(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Conv1dTestModel, self).__init__()<NewLine>        self.conv1d_stack = nn.Sequential(<NewLine>            nn.Conv1d(1, 64, kernel_size=128, padding=8),<NewLine>            nn.MaxPool1d(kernel_size=5, padding=1),<NewLine>        )<NewLine>        <NewLine>    def forward(self, x):<NewLine>        N, C, L = x.shape[0], x.shape[1], x.shape[2]<NewLine>        return self.conv1d_stack(x)<NewLine>    <NewLine>N = 16<NewLine>L = 1024<NewLine>C = 1<NewLine>T = torch.rand(N, C, L)<NewLine><NewLine>model = Conv1dTestModel()<NewLine>model(T).shape<NewLine><NewLine>torch.jit.script(model.eval().to('cpu')).save('Conv1dTestModel_scripted.pt')<NewLine></code></pre><NewLine><p>When trying to run this model on iOS with a random tensor of shape {16, 1, 1024} I get the following error message</p><NewLine><pre><code class=""lang-auto"">NNPACK SpatialConvolution_updateOutput failed<NewLine>The above operation failed in interpreter.<NewLine>Traceback (most recent call last):<NewLine>  File ""/home/dnagy/anaconda3/envs/pt11/lib/python3.6/site-packages/torch/nn/modules/conv.py"", line 201<NewLine>                            self.weight, self.bias, self.stride,<NewLine>                            _single(0), self.dilation, self.groups)<NewLine>        return F.conv1d(input, self.weight, self.bias, self.stride,<NewLine>               ~~~~~~~~ &lt;--- HERE<NewLine>                        self.padding, self.dilation, self.groups)<NewLine>Serialized   File ""code/__torch__/torch/nn/modules/conv.py"", line 23<NewLine>      _8 = _2<NewLine>    else:<NewLine>      _8 = torch.conv1d(input, self.weight, self.bias, [1], [8], [1], 1)<NewLine>           ~~~~~~~~~~~~ &lt;--- HERE<NewLine>    return _8<NewLine></code></pre><NewLine><p>I’m using PyTorch 1.4.0</p><NewLine></div>",https://discuss.pytorch.org/u/dnnagy,(Nagy Dániel),dnnagy,"March 2, 2020, 12:47pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/dnnagy"">@dnnagy</a>, sorry for the late reply.  I need sometime to debug into NNPACK to figure out why this happens. If this blocks you, you can get around with it by disabling NNPACK in the iOS build script and recompile the static libraries from source code. Here is what I did</p><NewLine><ol><NewLine><li>Add <code>CMAKE_ARGS+=(""-DUSE_NNPACK=OFF"") CMAKE_ARGS+=(""-DUSE_XNNPACK=OFF"")</code>to <code>build_ios.sh</code><NewLine></li><NewLine><li>Run <code>BUILD_PYTORCH_MOBILE=1 IOS_ARCH=arm64 ./scripts/build_ios.sh</code><NewLine></li><NewLine><li>Follow the tutorial to link the static libraries in XCode</li><NewLine></ol><NewLine><p>I was able to run your model above without NNPACK, and it worked fine on both of my iphonex and desktop simulator</p><NewLine><pre><code class=""lang-auto"">    torch::autograd::AutoGradMode guard(false);<NewLine>    NSString* path = [[NSBundle mainBundle]pathForResource:@""model.pt"" ofType:@""""];<NewLine>    auto model = torch::jit::load(path.UTF8String);<NewLine>    model.eval();<NewLine>    auto input = torch::ones({16,1,1024});<NewLine>    auto result = model.forward({input}).toTensor(); <NewLine>    result.print(); //[CPUFloatType [16, 64, 183]]<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/xta0"">@xta0</a>, thank you very much for your anwser.<br/><NewLine>I am facing the same problem as <a class=""mention"" href=""/u/dnnagy"">@dnnagy</a> but I can’t seem to find the build_ios.sh file.</p><NewLine><p>When installing Libtorch with POD and running my code on the simulator everything works fine until I want to port it on my iphone. Should I rebuild Libtorch locally without POD with the arguments you have listed above ?<br/><NewLine>If so, do I need to jit.trace my model again ?<br/><NewLine>Thank you very much !</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/matthieu_tpops"">@Matthieu_TPops</a>, The reason why simulator works is that we’ve disabled the NNPACK for the simulator build. You can definitely rebuild your pod via the custom build script, please refer to the tutorial here - <a href=""https://pytorch.org/mobile/ios/#build-pytorch-ios-libraries-from-source"" rel=""nofollow noopener"">https://pytorch.org/mobile/ios/#build-pytorch-ios-libraries-from-source</a>. As for the model, you don’t need to trace it again. but one thing to keep in mind is that if you build your pod from master, you need to also use the pytorch master code to trace the model. Otherwise, you might run into some backward compatibility issues.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank You very much ! It seems linking the static libraries is not enough.</p><NewLine><p>Is it necessary to change the TorchBridge as well ? ( As Libtorch.h is removed from the Podfile)</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you’re using Swift, you need a bridge to call objective-c code which wraps the C++ torch libraries.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Matthieu_TPops; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Matthieu_TPops; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/xta0; <NewLine> ,"REPLY_DATE 1: March 5, 2020,  7:19pm; <NewLine> REPLY_DATE 2: March 9, 2020,  5:04pm; <NewLine> REPLY_DATE 3: March 9, 2020,  7:46pm; <NewLine> REPLY_DATE 4: March 10, 2020,  2:44pm; <NewLine> REPLY_DATE 5: March 10, 2020,  5:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
60873,[Android] Build failed,2019-11-13T15:41:28.296Z,2,2056,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone,</p><NewLine><p>Tried to build android version from current pytorch source and there’s an error.<br/><NewLine>Is it relevant to (<a href=""https://github.com/pytorch/pytorch/issues/29159"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/29159</a>) ?</p><NewLine><pre><code class=""lang-auto"">root@Callisto:~/pytorch/android# /root/Android/gradle-6.0/bin/gradle -p /root/pytorch/android clean assembleRelease --stacktrace               <NewLine><NewLine>FAILURE: Build failed with an exception.<NewLine><NewLine>* What went wrong:<NewLine>A problem occurred configuring project ':pytorch_android'.<NewLine>&gt; Failed to notify project evaluation listener.<NewLine>   &gt; org.gradle.api.file.ProjectLayout.fileProperty(Lorg/gradle/api/provider/Provider;)Lorg/gradle/api/file/RegularFileProperty;<NewLine><NewLine>* Try:<NewLine>Run with --info or --debug option to get more log output. Run with --scan to get full insights.<NewLine><NewLine>* Exception is:<NewLine>org.gradle.api.ProjectConfigurationException: A problem occurred configuring project ':pytorch_android'.<NewLine>        at org.gradle.configuration.project.LifecycleProjectEvaluator.wrapException(LifecycleProjectEvaluator.java:80)<NewLine>        at org.gradle.configuration.project.LifecycleProjectEvaluator.addConfigurationFailure(LifecycleProjectEvaluator.java:73)<NewLine>        at org.gradle.configuration.project.LifecycleProjectEvaluator.access$600(LifecycleProjectEvaluator.java:53)<NewLine>        at org.gradle.configuration.project.LifecycleProjectEvaluator$NotifyAfterEvaluate.run(LifecycleProjectEvaluator.java:199)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:402)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:394)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor$1.execute(DefaultBuildOperationExecutor.java:165)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:250)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:158)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:92)<NewLine>        at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)<NewLine>        at org.gradle.configuration.project.LifecycleProjectEvaluator$EvaluateProject$1.run(LifecycleProjectEvaluator.java:112)<NewLine>        at org.gradle.internal.Factories$1.create(Factories.java:26)<NewLine>        at org.gradle.internal.work.DefaultWorkerLeaseService.withLocks(DefaultWorkerLeaseService.java:189)<NewLine>        at org.gradle.internal.work.StopShieldingWorkerLeaseService.withLocks(StopShieldingWorkerLeaseService.java:40)<NewLine>        at org.gradle.api.internal.project.DefaultProjectStateRegistry$ProjectStateImpl.withProjectLock(DefaultProjectStateRegistry.java:238)<NewLine>        at org.gradle.api.internal.project.DefaultProjectStateRegistry$ProjectStateImpl.withMutableState(DefaultProjectStateRegistry.java:232)<NewLine>        at org.gradle.api.internal.project.DefaultProjectStateRegistry$ProjectStateImpl.withMutableState(DefaultProjectStateRegistry.java:193)<NewLine>        at org.gradle.configuration.project.LifecycleProjectEvaluator$EvaluateProject.run(LifecycleProjectEvaluator.java:96)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:402)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:394)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor$1.execute(DefaultBuildOperationExecutor.java:165)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:250)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:158)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:92)<NewLine>        at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)<NewLine>        at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:68)<NewLine>        at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:699)<NewLine>        at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:142)<NewLine>        at org.gradle.execution.TaskPathProjectEvaluator.configure(TaskPathProjectEvaluator.java:36)<NewLine>        at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:64)<NewLine>        at org.gradle.configuration.DefaultProjectsPreparer.prepareProjects(DefaultProjectsPreparer.java:61)<NewLine>        at org.gradle.configuration.BuildOperatingFiringProjectsPreparer$ConfigureBuild.run(BuildOperatingFiringProjectsPreparer.java:52)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:402)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:394)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor$1.execute(DefaultBuildOperationExecutor.java:165)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:250)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:158)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:92)<NewLine>        at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)<NewLine>        at org.gradle.configuration.BuildOperatingFiringProjectsPreparer.prepareProjects(BuildOperatingFiringProjectsPreparer.java:40)<NewLine>        at org.gradle.initialization.DefaultGradleLauncher.prepareProjects(DefaultGradleLauncher.java:204)<NewLine>        at org.gradle.initialization.DefaultGradleLauncher.doClassicBuildStages(DefaultGradleLauncher.java:142)<NewLine>        at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:130)<NewLine>        at org.gradle.initialization.DefaultGradleLauncher.executeTasks(DefaultGradleLauncher.java:110)<NewLine>        at org.gradle.internal.invocation.GradleBuildController$1.execute(GradleBuildController.java:60)<NewLine>        at org.gradle.internal.invocation.GradleBuildController$1.execute(GradleBuildController.java:57)<NewLine>        at org.gradle.internal.invocation.GradleBuildController$3.create(GradleBuildController.java:85)<NewLine>        at org.gradle.internal.invocation.GradleBuildController$3.create(GradleBuildController.java:78)<NewLine>        at org.gradle.internal.work.DefaultWorkerLeaseService.withLocks(DefaultWorkerLeaseService.java:189)<NewLine>        at org.gradle.internal.work.StopShieldingWorkerLeaseService.withLocks(StopShieldingWorkerLeaseService.java:40)<NewLine>        at org.gradle.internal.invocation.GradleBuildController.doBuild(GradleBuildController.java:78)<NewLine>        at org.gradle.internal.invocation.GradleBuildController.run(GradleBuildController.java:57)<NewLine>        at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:31)<NewLine>        at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)<NewLine>        at org.gradle.launcher.exec.BuildOutcomeReportingBuildActionRunner.run(BuildOutcomeReportingBuildActionRunner.java:63)<NewLine>        at org.gradle.tooling.internal.provider.ValidatingBuildActionRunner.run(ValidatingBuildActionRunner.java:32)<NewLine>        at org.gradle.launcher.exec.BuildCompletionNotifyingBuildActionRunner.run(BuildCompletionNotifyingBuildActionRunner.java:39)<NewLine>        at org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner$3.call(RunAsBuildOperationBuildActionRunner.java:51)<NewLine>        at org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner$3.call(RunAsBuildOperationBuildActionRunner.java:45)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:416)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:406)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor$1.execute(DefaultBuildOperationExecutor.java:165)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:250)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:158)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:102)<NewLine>        at org.gradle.internal.operations.DelegatingBuildOperationExecutor.call(DelegatingBuildOperationExecutor.java:36)<NewLine>        at org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner.run(RunAsBuildOperationBuildActionRunner.java:45)<NewLine>        at org.gradle.launcher.exec.InProcessBuildActionExecuter$1.transform(InProcessBuildActionExecuter.java:50)<NewLine>        at org.gradle.launcher.exec.InProcessBuildActionExecuter$1.transform(InProcessBuildActionExecuter.java:47)<NewLine>        at org.gradle.composite.internal.DefaultRootBuildState.run(DefaultRootBuildState.java:78)<NewLine>        at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:47)<NewLine>        at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:31)<NewLine>        at org.gradle.launcher.exec.BuildTreeScopeBuildActionExecuter.execute(BuildTreeScopeBuildActionExecuter.java:42)<NewLine>        at org.gradle.launcher.exec.BuildTreeScopeBuildActionExecuter.execute(BuildTreeScopeBuildActionExecuter.java:28)<NewLine>        at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:78)<NewLine>        at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:52)<NewLine>        at org.gradle.tooling.internal.provider.SubscribableBuildActionExecuter.execute(SubscribableBuildActionExecuter.java:59)<NewLine>        at org.gradle.tooling.internal.provider.SubscribableBuildActionExecuter.execute(SubscribableBuildActionExecuter.java:36)<NewLine>        at org.gradle.tooling.internal.provider.SessionScopeBuildActionExecuter.execute(SessionScopeBuildActionExecuter.java:68)<NewLine>        at org.gradle.tooling.internal.provider.SessionScopeBuildActionExecuter.execute(SessionScopeBuildActionExecuter.java:38)<NewLine>        at org.gradle.tooling.internal.provider.GradleThreadBuildActionExecuter.execute(GradleThreadBuildActionExecuter.java:37)<NewLine>        at org.gradle.tooling.internal.provider.GradleThreadBuildActionExecuter.execute(GradleThreadBuildActionExecuter.java:26)<NewLine>        at org.gradle.tooling.internal.provider.ParallelismConfigurationBuildActionExecuter.execute(ParallelismConfigurationBuildActionExecuter.java:43)<NewLine>        at org.gradle.tooling.internal.provider.ParallelismConfigurationBuildActionExecuter.execute(ParallelismConfigurationBuildActionExecuter.java:29)<NewLine>        at org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:60)<NewLine>        at org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:32)<NewLine>        at org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:55)<NewLine>        at org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:41)<NewLine>        at org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:48)<NewLine>        at org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:32)<NewLine>        at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:68)<NewLine>        at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:37)<NewLine>        at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)<NewLine>        at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:39)<NewLine>        at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)<NewLine>        at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:27)<NewLine>        at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)<NewLine>        at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:35)<NewLine>        at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)<NewLine>        at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.create(ForwardClientInput.java:78)<NewLine>        at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.create(ForwardClientInput.java:75)<NewLine>        at org.gradle.util.Swapper.swap(Swapper.java:38)<NewLine>        at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:75)<NewLine>        at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)<NewLine>        at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55)<NewLine>        at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)<NewLine>        at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:63)<NewLine>        at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:37)<NewLine>        at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)<NewLine>        at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:82)<NewLine>        at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:37)<NewLine>        at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:104)<NewLine>        at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:52)<NewLine>        at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297)<NewLine>        at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)<NewLine>        at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)<NewLine>        at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)<NewLine>Caused by: org.gradle.internal.event.ListenerNotificationException: Failed to notify project evaluation listener.<NewLine>        at org.gradle.internal.event.AbstractBroadcastDispatch.dispatch(AbstractBroadcastDispatch.java:86)<NewLine>        at org.gradle.internal.event.BroadcastDispatch$CompositeDispatch.dispatch(BroadcastDispatch.java:325)<NewLine>        at org.gradle.internal.event.BroadcastDispatch$CompositeDispatch.dispatch(BroadcastDispatch.java:235)<NewLine>        at org.gradle.internal.event.ListenerBroadcast.dispatch(ListenerBroadcast.java:141)<NewLine>        at org.gradle.internal.event.ListenerBroadcast.dispatch(ListenerBroadcast.java:37)<NewLine>        at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)<NewLine>        at com.sun.proxy.$Proxy28.afterEvaluate(Unknown Source)<NewLine>        at org.gradle.configuration.project.LifecycleProjectEvaluator$NotifyAfterEvaluate$1.execute(LifecycleProjectEvaluator.java:191)<NewLine>        at org.gradle.configuration.project.LifecycleProjectEvaluator$NotifyAfterEvaluate$1.execute(LifecycleProjectEvaluator.java:188)<NewLine>        at org.gradle.api.internal.project.DefaultProject.stepEvaluationListener(DefaultProject.java:1434)<NewLine>        at org.gradle.configuration.project.LifecycleProjectEvaluator$NotifyAfterEvaluate.run(LifecycleProjectEvaluator.java:197)<NewLine>        ... 114 more<NewLine>Caused by: java.lang.NoSuchMethodError: org.gradle.api.file.ProjectLayout.fileProperty(Lorg/gradle/api/provider/Provider;)Lorg/gradle/api/file/RegularFileProperty;<NewLine>        at com.android.build.gradle.internal.scope.BuildArtifactsHolder$createArtifactFile$1.invoke(BuildArtifactsHolder.kt:718)<NewLine>        at com.android.build.gradle.internal.scope.BuildArtifactsHolder$createArtifactFile$1.invoke(BuildArtifactsHolder.kt:60)<NewLine>        at com.android.build.gradle.internal.scope.BuildArtifactsHolder.createFileOrDirectory(BuildArtifactsHolder.kt:471)<NewLine>        at com.android.build.gradle.internal.scope.BuildArtifactsHolder.createArtifactFile(BuildArtifactsHolder.kt:386)<NewLine>        at com.android.build.gradle.internal.scope.BuildArtifactsHolder.createArtifactFile(BuildArtifactsHolder.kt:351)<NewLine>        at com.android.build.gradle.tasks.ProcessLibraryManifest$CreationAction.preConfigure(ProcessLibraryManifest.java:255)<NewLine>        at com.android.build.gradle.internal.tasks.factory.TaskAction.doPreConfig(TaskFactoryUtils.kt:96)<NewLine>        at com.android.build.gradle.internal.tasks.factory.TaskAction.postRegisterHook(TaskFactoryUtils.kt:88)<NewLine>        at com.android.build.gradle.internal.tasks.factory.TaskFactoryUtils.registerTask(TaskFactoryUtils.kt:39)<NewLine>        at com.android.build.gradle.internal.tasks.factory.TaskFactoryImpl.register(TaskFactoryImpl.kt:45)<NewLine>        at com.android.build.gradle.internal.TaskManager.createMergeLibManifestsTask(TaskManager.java:780)<NewLine>        at com.android.build.gradle.internal.LibraryTaskManager.createTasksForVariantScope(LibraryTaskManager.java:120)<NewLine>        at com.android.build.gradle.internal.VariantManager.createTasksForVariantData(VariantManager.java:491)<NewLine>        at com.android.build.gradle.internal.VariantManager.createAndroidTasks(VariantManager.java:365)<NewLine>        at com.android.build.gradle.BasePlugin.createAndroidTasks(BasePlugin.java:767)<NewLine>        at com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:81)<NewLine>        at com.android.build.gradle.BasePlugin.lambda$createTasks$4(BasePlugin.java:651)<NewLine>        at com.android.build.gradle.internal.crash.CrashReporting$afterEvaluate$1.execute(crash_reporting.kt:37)<NewLine>        at com.android.build.gradle.internal.crash.CrashReporting$afterEvaluate$1.execute(crash_reporting.kt)<NewLine>        at org.gradle.configuration.internal.DefaultListenerBuildOperationDecorator$BuildOperationEmittingAction$1.lambda$run$0(DefaultListenerBuildOperationDecorator.java:152)<NewLine>        at org.gradle.configuration.internal.DefaultUserCodeApplicationContext.reapply(DefaultUserCodeApplicationContext.java:60)<NewLine>        at org.gradle.configuration.internal.DefaultListenerBuildOperationDecorator$BuildOperationEmittingAction$1.run(DefaultListenerBuildOperationDecorator.java:152)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:402)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:394)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor$1.execute(DefaultBuildOperationExecutor.java:165)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:250)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:158)<NewLine>        at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:92)<NewLine>        at org.gradle.configuration.internal.DefaultListenerBuildOperationDecorator$BuildOperationEmittingAction.execute(DefaultListenerBuildOperationDecorator.java:149)<NewLine>        at org.gradle.internal.event.BroadcastDispatch$ActionInvocationHandler.dispatch(BroadcastDispatch.java:92)<NewLine>        at org.gradle.internal.event.BroadcastDispatch$ActionInvocationHandler.dispatch(BroadcastDispatch.java:80)<NewLine>        at org.gradle.internal.event.AbstractBroadcastDispatch.dispatch(AbstractBroadcastDispatch.java:42)<NewLine>        at org.gradle.internal.event.BroadcastDispatch$SingletonDispatch.dispatch(BroadcastDispatch.java:231)<NewLine>        at org.gradle.internal.event.BroadcastDispatch$SingletonDispatch.dispatch(BroadcastDispatch.java:150)<NewLine>        at org.gradle.internal.event.AbstractBroadcastDispatch.dispatch(AbstractBroadcastDispatch.java:58)<NewLine>        ... 124 more<NewLine><NewLine><NewLine>* Get more help at https://help.gradle.org<NewLine><NewLine>Deprecated Gradle features were used in this build, making it incompatible with Gradle 7.0.<NewLine>Use '--warning-mode all' to show the individual deprecation warnings.<NewLine>See https://docs.gradle.org/6.0/userguide/command_line_interface.html#sec:command_line_warnings<NewLine></code></pre><NewLine><p>The system I use is: pytorch/pytorch:latest docker image, with cmake=3.15.5, sdkmanager=26.1.1, ndk=20.1.5948944, gradle=6.0</p><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"November 13, 2019,  3:41pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello Emil,<br/><NewLine>Method org.gradle.api.file.ProjectLayout.fileProperty was removed in gradle v6.0.0 in commit:<br/><NewLine><aside class=""onebox githubcommit""><NewLine><header class=""source""><NewLine><a href=""https://github.com/gradle/gradle/commit/c5374cacaede25add4628c21cf43527093054fcc#diff-27a4a85dbdd966edddc1cca046442368"" rel=""nofollow noopener"" target=""_blank"">github.com/gradle/gradle</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Commit""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M10.86 7c-.45-1.72-2-3-3.86-3-1.86 0-3.41 1.28-3.86 3H0v2h3.14c.45 1.72 2 3 3.86 3 1.86 0 3.41-1.28 3.86-3H14V7h-3.14zM7 10.2c-1.22 0-2.2-.98-2.2-2.2 0-1.22.98-2.2 2.2-2.2 1.22 0 2.2.98 2.2 2.2 0 1.22-.98 2.2-2.2 2.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/gradle/gradle/commit/c5374cacaede25add4628c21cf43527093054fcc"" rel=""nofollow noopener"" target=""_blank"">Remove deprecated ProjectLayout and DefaultTask methods</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        committed <span class=""discourse-local-date"" data-date=""2019-08-07"" data-format=""ll"" data-time=""16:21:59"" data-timezone=""UTC"">04:21PM - 07 Aug 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/lptr"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""lptr"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/495366?v=4"" width=""20""/><NewLine>          lptr<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""changed 5 files with 24 additions and 211 deletions""><NewLine><a href=""https://github.com/gradle/gradle/commit/c5374cacaede25add4628c21cf43527093054fcc/files"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+24</span><NewLine><span class=""removed"">-211</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>But it is still used in gradle android plugin:<br/><NewLine><a class=""onebox"" href=""https://android.googlesource.com/platform/tools/base/+/studio-master-dev/build-system/gradle-core/src/main/java/com/android/build/gradle/internal/scope/BuildArtifactsHolder.kt"" rel=""nofollow noopener"" target=""_blank"">https://android.googlesource.com/platform/tools/base/+/studio-master-dev/build-system/gradle-core/src/main/java/com/android/build/gradle/internal/scope/BuildArtifactsHolder.kt</a></p><NewLine><p>At the moment our CI jobs are based on gradle version 4.10.3, but it should also work with gradle 5.*</p><NewLine><p>Please try to build it with gradle 4.10.3+ or  5.*</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I wonder if I build pytorch with gradle v4 would it be possible to build Android project with this AAR libs with gradle v5?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, AAR packaging is the same, no dependency on gradle versions</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> ,"REPLY_DATE 1: November 14, 2019, 10:58am; <NewLine> REPLY_DATE 2: November 14, 2019, 11:09am; <NewLine> REPLY_DATE 3: November 15, 2019,  7:40pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
66935,How to convert pretrained .pth extension pytorch model into torchvision model to load on android,2020-01-17T05:44:15.310Z,6,1033,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello</p><NewLine><p>I have one .pth extension pytorch model this model works and predict correctly on web app but now i want to use and load this model on android app i know i have to convert this .pth extension model into torchvision for load model on android i have found code snippet and guide on pytorch guide to convert pretrained model into torchvision this is following code and its working correctly and convert pretrained mobile net model into torchvision but how to convert my .pth extension pytorch model into torchvision what changes i have to make in code i have tried to put my model path with model variable in following code but its not working.</p><NewLine><p>import torch<br/><NewLine>import torchvision</p><NewLine><p>model = torchvision.models.resnet18(pretrained=True)<br/><NewLine>model.eval()<br/><NewLine>example = torch.rand(1, 3, 224, 224)<br/><NewLine>traced_script_module = torch.jit.trace(model, example)<br/><NewLine>traced_script_module.save(“app/src/main/assets/model.pt”)</p><NewLine></div>",https://discuss.pytorch.org/u/kamlesh.panchal,(Kamlesh panchal),kamlesh.panchal,"January 17, 2020,  5:45am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you already have a pre-trained torchscript model (.pth file), you don’t need to use torchvision to load it. In the tutorial, we use torchvision to get the model so you can skip this step as you already have the model.</p><NewLine><p>You can follow the rest of the tutorial steps to load the model and pass in the inputs based on what it expects. See link below for more information -<br/><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://pytorch.org/favicon.ico?"" width=""32""/><NewLine><a href=""https://pytorch.org/mobile/android/#5-loading-torchscript-module"" rel=""nofollow noopener"" target=""_blank"">pytorch.org</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""500"" src=""https://pytorch.org/assets/images/pytorch-logo.png"" width=""500""/><NewLine><h3><a href=""https://pytorch.org/mobile/android/#5-loading-torchscript-module"" rel=""nofollow noopener"" target=""_blank"">PyTorch</a></h3><NewLine><p>An open source deep learning platform that provides a seamless path from research prototyping to production deployment.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply<br/><NewLine>yes i have pre-trained pytorch model with extension .pth but i want to load this on android so this pytorch model cant load directly on android for loading pytorch model on adnroid i need to convert into torchscript model right?</p><NewLine><p>so can you help me how can i convert my .pth pytorch model into torchscipt model for load on andorid.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The code snippet in your original post should be sufficient.  <code>torch.jit.trace</code> will produce TorchScript, and the file you get from calling <code>save</code> will load on Android.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/david_reiss"">@David_Reiss</a></p><NewLine><p>Thanks for reply i m using this following code for produce torchscript is it all correct? i m using torch version 1.3.0 and torchvision 0.4.1</p><NewLine><p>import torch<br/><NewLine>import torchvision</p><NewLine><p>model = “/home/kamleshpanchal/Downloads/PytorchMobile-master/agestage250.pth”<br/><NewLine>input_tensor = torch.rand(1,3,224,224)</p><NewLine><p>script_model = torch.jit.trace(model,input_tensor)<br/><NewLine>script_model.save(“converted.pt”)</p><NewLine><p>after run this code i m getting following error</p><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “convert.py”, line 10, in <br/><NewLine>script_model = torch.jit.trace(model,input_tensor)<br/><NewLine>File “/usr/local/lib/python3.6/dist-packages/torch/jit/<strong>init</strong>.py”, line 880, in trace<br/><NewLine>name = _qualified_name(func)<br/><NewLine>File “/usr/local/lib/python3.6/dist-packages/torch/_jit_internal.py”, line 636, in _qualified_name<br/><NewLine>name = obj.<strong>name</strong><br/><NewLine>AttributeError: ‘str’ object has no attribute ‘<strong>name</strong>’</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>It looks like you’re just passing the model file name to <code>torch.jit.trace</code>.  You need to load the model first and then pass the model object instead.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/david_reiss"">@David_Reiss</a> I m using following code for load model and passing model object Please look into it. is it correct way?</p><NewLine><p>import torch<br/><NewLine>import torchvision<br/><NewLine>model = torch.load(“agestage250.pth”)<br/><NewLine>model.eval()<br/><NewLine>input_tensor = torch.rand(1,3,224,224)</p><NewLine><p>script_model = torch.jit.trace(model,input_tensor)<br/><NewLine>script_model.save(“converted.pt”)</p><NewLine><p>after run this code i m getting following error</p><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “convert.py”, line 6, in <br/><NewLine>model.eval()<br/><NewLine>AttributeError: ‘collections.OrderedDict’ object has no attribute ‘eval’</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>It looks like you’re not loading the model properly.  See <a href=""https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-state-dict-recommended"">https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-state-dict-recommended</a></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Use this script :<br/><NewLine>requirements : pytorch 1.5 and torchvision 0.6.0 use this command <code>conda install pytorch torchvision cudatoolkit=10.1 -c pytorch-nightly</code></p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torchvision<NewLine><NewLine>model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)<NewLine>model.eval()<NewLine>example = torch.rand(1, 3, 800, 800)<NewLine>traced_script_module = torch.jit.script(model, example)<NewLine>traced_script_module.save(""/home/parth/Desktop/model4.pt"")<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kamlesh.panchal; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/kamlesh.panchal; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/kamlesh.panchal; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/hurricane013; <NewLine> ,"REPLY_DATE 1: January 17, 2020,  6:09pm; <NewLine> REPLY_DATE 2: January 20, 2020,  6:52am; <NewLine> REPLY_DATE 3: January 27, 2020,  9:46pm; <NewLine> REPLY_DATE 4: January 28, 2020,  7:34am; <NewLine> REPLY_DATE 5: January 28, 2020, 10:45pm; <NewLine> REPLY_DATE 6: January 29, 2020,  5:04am; <NewLine> REPLY_DATE 7: February 20, 2020,  7:42pm; <NewLine> REPLY_DATE 8: March 6, 2020,  6:51am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
62490,Threading of Model Pytorch Android,2019-11-29T06:11:07.083Z,5,648,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to deploy my model on android device.Model should be fast in android but it is taking time as model is big.<br/><NewLine>Is model threading option is there while declaring the model interpreter in android  as it is available in tensorflow-lite ?</p><NewLine></div>",https://discuss.pytorch.org/u/mohit7,(Mohit Ranawat),mohit7,"November 29, 2019,  6:11am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/mohit7"">@mohit7</a><br/><NewLine>At the moment we do not have this setting, but we are thinking to add this option to control it.<br/><NewLine>Current threading model is determined by device, the number of threads is about number of BIG cores on the device.<br/><NewLine>More details about number of threads per device you can find in the code of function <code>caffe2::ThreadPool::defaultThreadPool()</code></p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/caffe2/utils/threadpool/ThreadPool.cc#L24"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/master/caffe2/utils/threadpool/ThreadPool.cc#L24"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/master/caffe2/utils/threadpool/ThreadPool.cc#L24</a></h4><NewLine><pre class=""onebox""><code class=""lang-cc""><ol class=""start lines"" start=""14"" style=""counter-reset: li-counter 13 ;""><NewLine><li><NewLine></li><NewLine><li>// Whether or not threadpool caps apply to iOS</li><NewLine><li>C10_DEFINE_int(caffe2_threadpool_ios_cap, true, """");</li><NewLine><li><NewLine></li><NewLine><li>namespace caffe2 {</li><NewLine><li><NewLine></li><NewLine><li>// Default smallest amount of work that will be partitioned between</li><NewLine><li>// multiple threads; the runtime value is configurable</li><NewLine><li>constexpr size_t kDefaultMinWorkSize = 1;</li><NewLine><li><NewLine></li><NewLine><li class=""selected"">std::unique_ptr&lt;ThreadPool&gt; ThreadPool::defaultThreadPool() {</li><NewLine><li>  CAFFE_ENFORCE(cpuinfo_initialize(), ""cpuinfo initialization failed"");</li><NewLine><li>  int numThreads = cpuinfo_get_processors_count();</li><NewLine><li><NewLine></li><NewLine><li>  bool applyCap = false;</li><NewLine><li>#if C10_ANDROID</li><NewLine><li>  applyCap = FLAGS_caffe2_threadpool_android_cap;</li><NewLine><li>#elif C10_IOS</li><NewLine><li>  applyCap = FLAGS_caffe2_threadpool_ios_cap;</li><NewLine><li>#endif</li><NewLine><li><NewLine></li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Please write us if you have threadding issues with some particular device.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/mohit7"">@mohit7</a></p><NewLine><p>We just exposed control on global number of threads used by pytorch android, it was landed in <a href=""https://github.com/pytorch/pytorch/commit/62254430093fef5f2dece3825d95b25d443faf63"" rel=""nofollow noopener"">master</a></p><NewLine><pre><code class=""lang-auto"">method org.pytorch.Module#setNumThreads(int numThreads)<NewLine></code></pre><NewLine><p>(<a href=""https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/src/main/java/org/pytorch/Module.java#L57"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/src/main/java/org/pytorch/Module.java#L57</a>)<br/><NewLine>The latest android nightlies already include them: <a href=""https://github.com/pytorch/pytorch/tree/master/android#nightly"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/tree/master/android#nightly</a> (you might need gradle argument <code>--refresh-dependencies</code> if you already using them)</p><NewLine><pre><code class=""lang-auto"">Module module = Module.load(moduleFileAbsoluteFilePath);<NewLine>module.setNumThreads(1);<NewLine></code></pre><NewLine><p>This is new functionality, please report if you find any issues with it.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/ivankobzarev"">@IvanKobzarev</a> it worked.<br/><NewLine>But you should explicitly put a restriction on no of threads user is setting because after a certain limit instead of decreasing run-time it is increasing.</p><NewLine><p>One more doubt-<br/><NewLine>Just like in pytorch can we pass as batch of images in case of pytorch android. ?</p><NewLine><p>Thanks,<br/><NewLine>Mohit Ranawat</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""62490""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/mohit7/40/17896_2.png"" width=""20""/> mohit7:</div><NewLine><blockquote><NewLine><p>But you should explicitly put a restriction on no of threads user is setting because after a certain limit instead of decreasing run-time it is increasing.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, performance degrades when that thread number is more than cpu cores number as more thread switches and thread contention.<br/><NewLine>But additional capping may introduce some non-transparency for this API, we will think about it.<br/><NewLine>Our plan is to revise our default thread pool number to be optimal for inference time by default for as much as possible devices.</p><NewLine><aside class=""quote no-group""><NewLine><blockquote><NewLine><p>One more doubt-<br/><NewLine>Just like in pytorch can we pass as batch of images in case of pytorch android. ?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Vision models is the same, the input shape is <code>N_images * N_channels (e.g. 3) * IMAGE_HEIGHT * IMAGE_WIDTH</code>.<br/><NewLine>So if you prepare Tensor with <code>N_images &gt; 1</code> that should work as in desktop version.</p><NewLine><p>But <a href=""https://github.com/pytorch/pytorch/blob/master/android/pytorch_android_torchvision/src/main/java/org/pytorch/torchvision/TensorImageUtils.java"" rel=""nofollow noopener""> <code>org.pytorch.torchvisio.TensorImageUtils</code></a> has api how to prepare tensors only with <code>N_images==1</code>,  so it needs some additional code to prepare it with  <code>N_images &gt; 1</code>.</p><NewLine><p>Do you think that will be useful to have in TensorImageUtils api some helper methods to prepare Tensors for image batches?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ivankobzarev"">@IvanKobzarev</a> it will be good for image batches as it might help for application to applicaion.<br/><NewLine>Suppose our case model prediction is critical so we thought of passing multiple images to model to reduce misprediction.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ivankobzarev"">@IvanKobzarev</a> suddenly setNumThreads is not working.<br/><NewLine>Is it removed from the nightly version ?<br/><NewLine>Thanks,<br/><NewLine>Mohit Ranawat</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/mohit7"">@mohit7</a>,</p><NewLine><p>Sorry for inconvenience.<br/><NewLine>We moved <code>setNumThreads</code> method to separate class <code>org.pytorch.PyTorchAndroid</code> as a static method.<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/src/main/java/org/pytorch/PyTorchAndroid.java#L33"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/src/main/java/org/pytorch/PyTorchAndroid.java#L33"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/master/android/pytorch_android/src/main/java/org/pytorch/PyTorchAndroid.java#L33</a></h4><NewLine><pre class=""onebox""><code class=""lang-java""><ol class=""start lines"" start=""23"" style=""counter-reset: li-counter 22 ;""><NewLine><li>  public static Module loadModuleFromAsset(final AssetManager assetManager, final String assetName) {</li><NewLine><li>    return new Module(new NativePeer(assetName, assetManager));</li><NewLine><li>  }</li><NewLine><li><NewLine></li><NewLine><li>  /**</li><NewLine><li>   * Globally sets the number of threads used on native side. Attention: Has global effect, all</li><NewLine><li>   * modules use one thread pool with specified number of threads.</li><NewLine><li>   *</li><NewLine><li>   * @param numThreads number of threads, must be positive number.</li><NewLine><li>   */</li><NewLine><li class=""selected"">  public static void setNumThreads(int numThreads) {</li><NewLine><li>    if (numThreads &lt; 1) {</li><NewLine><li>      throw new IllegalArgumentException(""Number of threads cannot be less than 1"");</li><NewLine><li>    }</li><NewLine><li><NewLine></li><NewLine><li>    nativeSetNumThreads(numThreads);</li><NewLine><li>  }</li><NewLine><li><NewLine></li><NewLine><li>  private static native void nativeSetNumThreads(int numThreads);</li><NewLine><li>}</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ivankobzarev"">@IvanKobzarev</a> now we are in problem.<br/><NewLine>The problem we were facing initially with the model loading with some specific layer is resolved in nightly version not in stable version. So If I am using the nightly version I can’t use threading.<br/><NewLine>Can you also move it to nightly version or give me suggestion for this problem?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/mohit7"">@mohit7</a>,<br/><NewLine>PyTorchAndroid with setNumThreads was published only in nightly builds(they are published from master branch), it will be in stable only with 1.4 release (January)</p><NewLine><p>I rechecked our sonatype nightlies:<br/><NewLine>The latest published artifact is <a href=""https://oss.sonatype.org/service/local/repositories/snapshots/content/org/pytorch/pytorch_android/1.4.0-SNAPSHOT/pytorch_android-1.4.0-20191219.100544-76.aar"" rel=""nofollow noopener"">https://oss.sonatype.org/service/local/repositories/snapshots/content/org/pytorch/pytorch_android/1.4.0-SNAPSHOT/pytorch_android-1.4.0-20191219.100544-76.aar</a></p><NewLine><p>It contains PyTorchAndroid.class in classes.jar with setNumThreads</p><NewLine><p>Just to check your gradle setup to use nightlies:</p><NewLine><pre><code class=""lang-auto"">repositories {<NewLine>    maven {<NewLine>        url ""https://oss.sonatype.org/content/repositories/snapshots""<NewLine>    }<NewLine>}<NewLine><NewLine>dependencies {<NewLine>    ...<NewLine>    implementation 'org.pytorch:pytorch_android:1.4.0-SNAPSHOT'<NewLine>    implementation 'org.pytorch:pytorch_android_torchvision:1.4.0-SNAPSHOT'<NewLine>    ...<NewLine>}<NewLine></code></pre><NewLine><p>You might need key <code>--refresh-dependencies</code> to force update of the dependencies if your gradle cache config is very long.</p><NewLine><p>Let me know if you have any problems with it.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/ivankobzarev"">@IvanKobzarev</a> are you supporting setNumThread in the stable version or is it removed?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/mohit7; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mohit7; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mohit7; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mohit7; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mohit7; <NewLine> ,"REPLY_DATE 1: December 2, 2019,  9:44pm; <NewLine> REPLY_DATE 2: December 16, 2019, 11:46am; <NewLine> REPLY_DATE 3: December 12, 2019,  8:55am; <NewLine> REPLY_DATE 4: December 12, 2019,  6:46pm; <NewLine> REPLY_DATE 5: December 13, 2019,  4:41am; <NewLine> REPLY_DATE 6: December 16, 2019, 11:45am; <NewLine> REPLY_DATE 7: December 16, 2019,  8:20pm; <NewLine> REPLY_DATE 8: December 17, 2019,  4:12am; <NewLine> REPLY_DATE 9: December 19, 2019,  8:17pm; <NewLine> REPLY_DATE 10: February 25, 2020, 12:17pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> 
70910,Model forward fails,2020-02-24T21:22:41.766Z,1,275,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve been facing this problem since several days now.<br/><NewLine>I have converted my model just like this:</p><NewLine><pre><code class=""lang-auto"">model = DenoisingAutoencoderSheetEdges()<NewLine>model.load_state_dict(torch.load('models/model_3.pth').state_dict())<NewLine>model.threshold = 0.75<NewLine>model.hard_mode = True<NewLine>model.eval()<NewLine><NewLine>qmodel = quantization.convert(module=model)<NewLine>qmodel = torch.jit.script(qmodel)<NewLine>qmodel.save('mobile/android_edge_model_2.4.pt')<NewLine></code></pre><NewLine><p>my model:</p><NewLine><pre><code class=""lang-auto"">class DenoisingAutoencoderSheetEdges(nn.Module):<NewLine><NewLine>    def __init__(self):<NewLine>        super(DenoisingAutoencoderSheetEdges, self).__init__()<NewLine>        self.hard_mode = False<NewLine>        self.threshold = 0.75<NewLine><NewLine>        self.encoder = nn.Sequential(<NewLine>            nn.ConvTranspose2d(1, 6, kernel_size=5),<NewLine>            nn.ReLU(True),<NewLine>            nn.ConvTranspose2d(6, 16, kernel_size=5),<NewLine>            nn.ReLU(True),<NewLine>            nn.ConvTranspose2d(16, 25, kernel_size=5),<NewLine>            nn.ReLU(True),<NewLine>            nn.ConvTranspose2d(25, 50, kernel_size=5),<NewLine>            nn.ReLU(True)<NewLine>        )<NewLine><NewLine>        self.decoder = nn.Sequential(<NewLine>            nn.Conv2d(50, 25, kernel_size=5),<NewLine>            nn.ReLU(True),<NewLine>            nn.Conv2d(25, 16, kernel_size=5),<NewLine>            nn.ReLU(True),<NewLine>            nn.Conv2d(16, 6, kernel_size=5),<NewLine>            nn.ReLU(True),<NewLine>            nn.Conv2d(6, 1, kernel_size=5),<NewLine>            nn.Sigmoid(),<NewLine>        )<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.encoder(x)<NewLine>        x = self.decoder(x)<NewLine>       <NewLine>        if self.hard_mode:<NewLine>            x_tensor = torch.ones(x.shape)<NewLine>            y_tensor = torch.zeros(x.shape)<NewLine>            x = torch.where(x &gt; self.threshold, x_tensor, y_tensor)<NewLine><NewLine>        return x<NewLine></code></pre><NewLine><p>The model is loaded correctly, but the following error occurs during forwarding:</p><NewLine><pre><code class=""lang-auto"">     Caused by: java.lang.RuntimeException: [enforce fail at CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 642720000 bytes. Buy new RAM!<NewLine>    <NewLine>    The above operation failed in interpreter.<NewLine>    Traceback (most recent call last):<NewLine>    Serialized   File ""code/__torch__/models.py"", line 11<NewLine>      def forward(self: __torch__.models.DenoisingAutoencoderSheetEdges,<NewLine>        x: Tensor) -&gt; Tensor:<NewLine>        x0 = (self.encoder).forward(x, )<NewLine>              ~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        x1 = (self.decoder).forward(x0, )<NewLine>        if self.hard_mode:<NewLine>      File ""G:\Anaconda3\envs\PyTorch_pip2\lib\site-packages\torch\nn\modules\container.py"", line 100<NewLine>        def forward(self, input):<NewLine>            for module in self:<NewLine>                input = module(input)<NewLine>                        ~~~~~~ &lt;--- HERE<NewLine>            return input<NewLine>    Serialized   File ""code/__torch__/torch/nn/modules/container.py"", line 30, in forward<NewLine>        input5 = (_5).forward(input4, )<NewLine>        input6 = (_6).forward(input5, None, )<NewLine>        return (_7).forward(input6, )<NewLine>                ~~~~~~~~~~~ &lt;--- HERE<NewLine>      File ""G:\Anaconda3\envs\PyTorch_pip2\lib\site-packages\torch\nn\modules\container.py"", line 100<NewLine>        def forward(self, input):<NewLine>            for module in self:<NewLine>                input = module(input)<NewLine>                        ~~~~~~ &lt;--- HERE<NewLine>            return input<NewLine>    Serialized   File ""code/__torch__/torch/nn/modules/container.py"", line 29, in forward<NewLine>        input4 = (_4).forward(input3, None, )<NewLine>        input5 = (_5).forward(input4, )<NewLine>        input6 = (_6).forward(input5, None, )<NewLine>                  ~~~~~~~~~~~ &lt;--- HERE<NewLine>        return (_7).forward(input6, )<NewLine>      File ""G:\Anaconda3\envs\PyTorch_pip2\lib\site-packages\torch\nn\modules\conv.py"", line 776<NewLine>            output_padding = self._output_padding(input, output_size, self.stride, self.padding, self.kernel_size)<NewLine>    <NewLine>            return F.conv_transpose2d(<NewLine>                   ~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>                input, self.weight, self.bias, self.stride, self.padding,<NewLine>                output_padding, self.groups, self.dilation)<NewLine>    Serialized   File ""code/__torch__/torch/nn/modules/conv/___torch_mangle_2.py"", line 16, in forward<NewLine>          pass<NewLine>        output_padding = (self)._output_padding(input, output_size, [1, 1], [0, 0], [5, 5], )<NewLine>        _0 = torch.conv_transpose2d(input, self.weight, self.bias, [1, 1], [0, 0], output_padding, 1, [1, 1])<NewLine>             ~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        return _0<NewLine>      def _output_padding(self: __torch__.torch.nn.modules.conv.___torch_mangle_2.ConvTranspose2d,<NewLine>    <NewLine>        at org.pytorch.NativePeer.forward(Native Method)<NewLine>        at org.pytorch.Module.forward(Module.java:37)<NewLine>        at com.example.scannerapp_0.DenoiseEdge.getEdgeData(DenoiseEdge.java:43)<NewLine>        at com.example.scannerapp_0.MainActivity.net(MainActivity.java:99)<NewLine>        	... 13 more<NewLine></code></pre><NewLine><p>I do not understand what the error is trying to tell me or what I need to improve.<br/><NewLine>I would appreciate any hint or explination.</p><NewLine><p>Greetings,<br/><NewLine>Unity</p><NewLine></div>",https://discuss.pytorch.org/u/Unity05,(Unity05),Unity05,"February 24, 2020,  9:23pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""70910"" data-username=""Unity05""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/unity05/40/20944_2.png"" width=""20""/> Unity05:</div><NewLine><blockquote><NewLine><p>Caused by: java.lang.RuntimeException: [enforce fail at CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 642720000 bytes. Buy new RAM!</p><NewLine></blockquote><NewLine></aside><NewLine><p>From the beginning of the stack trace, it seems like you’re trying to make a Tensor which is too large (600MB). Is that expected? How much memory does it use if you run it on your machine?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ye, that actually caused the error.<br/><NewLine>I was wondering why the error occured on my emulator device with 2gb memory<br/><NewLine>since the device in idle state uses only about 800mb, that’s why I thought the cause would he more complex.<br/><NewLine>However with a 4gb memory device it worked fine.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Unity05; <NewLine> ,"REPLY_DATE 1: February 25, 2020, 11:57am; <NewLine> REPLY_DATE 2: February 25, 2020, 11:56am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
70197,Limitations of Torch Mobile//Torchscript (Object Detection + Segmentation),2020-02-18T16:13:28.331Z,0,609,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I was wondering if its possible to script YOLO, MobileSSD, or FasterRCNN as Object Detection models to run in iOS or android via Libtorch on Pytorch mobile. Are all of these models currently unavailable? Is there a custom approach to running FasterRCNN via Torch/TorchScript on mobile?</p><NewLine><p>Same question but in the case of Semantic Segmentation models such as PSPNet, DeepLabv3, and MaskRCNN.</p><NewLine><p>I believe someone mentioned that TorchVision is not supported but I don’t understand the difference between that and the Resnet and Inception models that are already currently supported/scriptable for mobile. Are resnets/inceptions not considered Torchvision?</p><NewLine><p>Thanks in advance,<br/><NewLine>Haris</p><NewLine></div>",https://discuss.pytorch.org/u/HussainHaris,(Haris),HussainHaris,"February 18, 2020,  4:13pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Basic torchvision models are supported, but specialized ops for detection are currently not included.  We are looking into it.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is there an ETA for obj detection in PyTorch mobile? In the meantime is there a tutorial on how to use Caffe2 mobile for obj detection?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/RicCu; <NewLine> ,"REPLY_DATE 1: February 26, 2020,  4:24pm; <NewLine> REPLY_DATE 2: February 24, 2020, 10:28pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
70691,"Unable to getPixels(), pixel access is not supported on Config#HARDWARE bitmaps",2020-02-22T17:49:38.898Z,0,559,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am looking at the demo android app. <a href=""https://github.com/pytorch/android-demo-app"" rel=""nofollow noopener"">https://github.com/pytorch/android-demo-app</a></p><NewLine><p>When I run the app in android version 10. There’s an error produce when this code below run.</p><NewLine><p><strong>Code</strong><br/><NewLine>inputTensor = TensorImageUtils.bitmapToFloat32Tensor(<br/><NewLine>bitmap,<br/><NewLine>TensorImageUtils.TORCHVISION_NORM_MEAN_RGB,<br/><NewLine>TensorImageUtils.TORCHVISION_NORM_STD_RGB<br/><NewLine>)</p><NewLine><p><strong>Error</strong><br/><NewLine>2020-02-22 11:24:29.690 19276-19594/com.say.dogbreedfinder E/TAG: Error during image analysis<br/><NewLine>java.lang.IllegalStateException: unable to getPixels(), pixel access is not supported on Config#HARDWARE bitmaps<br/><NewLine>at android.graphics.Bitmap.checkHardware(Bitmap.java:423)<br/><NewLine>at android.graphics.Bitmap.getPixels(Bitmap.java:1935)<br/><NewLine>at org.pytorch.torchvision.TensorImageUtils.bitmapToFloatBuffer(TensorImageUtils.java:67)<br/><NewLine>at org.pytorch.torchvision.TensorImageUtils.bitmapToFloat32Tensor(TensorImageUtils.java:108)<br/><NewLine>at org.pytorch.torchvision.TensorImageUtils.bitmapToFloat32Tensor(TensorImageUtils.java:34)</p><NewLine><p>Note: if I run on android 9, it’s working fine.</p><NewLine><p>Anyone know what could be the issue and how we can resolve this ?</p><NewLine></div>",https://discuss.pytorch.org/u/vortana_say,(vortana say),vortana_say,"February 22, 2020,  5:49pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found a work around is by config Bitmap to not use the HARDWARE.</p><NewLine><pre><code>        val mutableBitmap = bitmap.copy(Bitmap.Config.RGBA_F16, true)<NewLine><NewLine>        inputTensor = TensorImageUtils.bitmapToFloat32Tensor(<NewLine>            mutableBitmap,<NewLine>            TensorImageUtils.TORCHVISION_NORM_MEAN_RGB,<NewLine>            TensorImageUtils.TORCHVISION_NORM_STD_RGB<NewLine>        )<NewLine></code></pre><NewLine><p>Even though, this work around work, I would like to know if we can preserve the HARDWARE config since it’s provide more benefits with later Android version.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vortana_say; <NewLine> ,"REPLY_DATE 1: February 22, 2020, 11:27pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
69714,Quantized QNNPACK model is slowing down on android,2020-02-14T09:09:59.591Z,1,424,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi! My quantized segmentation model is slowing down on android , as well as on desktop cpu. Can’t figure out why. I tried different way  to quantize, used different layers. Here I attached a full example with QNNPACK</p><NewLine><p>For android I use. Also I tried different versions here</p><NewLine><pre><code class=""lang-auto"">  implementation 'org.pytorch:pytorch_android:1.5.0-SNAPSHOT'<NewLine>   implementation 'org.pytorch:pytorch_android_torchvision:1.5.0-SNAPSHOT'<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import os<NewLine><NewLine>from torch import nn<NewLine>from torchvision.models.resnet import BasicBlock, ResNet<NewLine>from torch.quantization import fuse_modules<NewLine>from torch.nn import functional as F<NewLine><NewLine><NewLine>def conv1x1(in_planes, out_planes, stride=1):<NewLine>    """"""1x1 convolution""""""<NewLine>    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)<NewLine><NewLine><NewLine>class QuantizableBasicBlock(BasicBlock):<NewLine>    def __init__(self, *args, **kwargs):<NewLine>        super(QuantizableBasicBlock, self).__init__(*args, **kwargs)<NewLine>        self.add_relu = torch.nn.quantized.FloatFunctional()<NewLine><NewLine>    def forward(self, x):<NewLine>        identity = x<NewLine><NewLine>        out = self.conv1(x)<NewLine>        out = self.bn1(out)<NewLine>        out = self.relu(out)<NewLine><NewLine>        out = self.conv2(out)<NewLine>        out = self.bn2(out)<NewLine><NewLine>        if self.downsample is not None:<NewLine>            identity = self.downsample(x)<NewLine><NewLine>        out = self.add_relu.add_relu(out, identity)<NewLine><NewLine>        return out<NewLine><NewLine>    def fuse_model(self):<NewLine>        torch.quantization.fuse_modules(self, [['conv1', 'bn1', 'relu'],<NewLine>                                               ['conv2', 'bn2']], inplace=True)<NewLine>        if self.downsample:<NewLine>            torch.quantization.fuse_modules(self.downsample, ['0', '1'], inplace=True)<NewLine><NewLine><NewLine>class ResNet(nn.Module):<NewLine><NewLine>    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,<NewLine>                 groups=1, width_per_group=64, replace_stride_with_dilation=None,<NewLine>                 norm_layer=None):<NewLine>        super(ResNet, self).__init__()<NewLine>        if norm_layer is None:<NewLine>            norm_layer = nn.BatchNorm2d<NewLine>        self._norm_layer = norm_layer<NewLine>        shapes = (16, 32, 64, 128)<NewLine>        block = QuantizableBasicBlock<NewLine>        self.inplanes = shapes[0]<NewLine>        self.dilation = 1<NewLine>        if replace_stride_with_dilation is None:<NewLine>            replace_stride_with_dilation = [False, False, False]<NewLine>        if len(replace_stride_with_dilation) != 3:<NewLine>            raise ValueError(""replace_stride_with_dilation should be None ""<NewLine>                             ""or a 3-element tuple, got {}"".format(replace_stride_with_dilation))<NewLine>        self.groups = groups<NewLine>        self.base_width = width_per_group<NewLine>        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False, dilation=1)<NewLine>        self.bn1 = norm_layer(self.inplanes)<NewLine>        self.relu = nn.ReLU(inplace=True)<NewLine>        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)<NewLine>        self.layer1 = self._make_layer(block, shapes[0], layers[0])<NewLine>        self.layer2 = self._make_layer(block, shapes[1], layers[1], stride=2,<NewLine>                                       dilate=replace_stride_with_dilation[0])<NewLine>        self.layer3 = self._make_layer(block, shapes[2], layers[2], stride=2,<NewLine>                                       dilate=replace_stride_with_dilation[1])<NewLine>        self.layer4 = self._make_layer(block, shapes[3], layers[3], stride=2,<NewLine>                                       dilate=replace_stride_with_dilation[2])<NewLine><NewLine>        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))<NewLine>        self.fc = nn.Linear(512 * block.expansion, num_classes)<NewLine><NewLine>        for m in self.modules():<NewLine>            if isinstance(m, nn.Conv2d):<NewLine>                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')<NewLine>            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):<NewLine>                nn.init.constant_(m.weight, 1)<NewLine>                nn.init.constant_(m.bias, 0)<NewLine><NewLine>    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):<NewLine>        norm_layer = self._norm_layer<NewLine>        downsample = None<NewLine>        previous_dilation = self.dilation<NewLine>        if dilate:<NewLine>            self.dilation *= stride<NewLine>            stride = 1<NewLine>        if stride != 1 or self.inplanes != planes * block.expansion:<NewLine>            downsample = nn.Sequential(<NewLine>                conv1x1(self.inplanes, planes * block.expansion, stride),<NewLine>                norm_layer(planes * block.expansion),<NewLine>            )<NewLine><NewLine>        layers = []<NewLine>        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,<NewLine>                            self.base_width, previous_dilation, norm_layer))<NewLine>        self.inplanes = planes * block.expansion<NewLine>        for _ in range(1, blocks):<NewLine>            layers.append(block(self.inplanes, planes, groups=self.groups,<NewLine>                                base_width=self.base_width, dilation=self.dilation,<NewLine>                                norm_layer=norm_layer))<NewLine><NewLine>        return nn.Sequential(*layers)<NewLine><NewLine>    def _forward_impl(self, x):<NewLine>        x = self.conv1(x)<NewLine>        x = self.bn1(x)<NewLine>        x = self.relu(x)<NewLine>        x = self.maxpool(x)<NewLine>        x = self.layer1(x)<NewLine>        x = self.layer2(x)<NewLine>        x = self.layer3(x)<NewLine>        x = self.layer4(x)<NewLine>        x = self.avgpool(x)<NewLine>        x = torch.flatten(x, 1)<NewLine>        x = self.fc(x)<NewLine>        return x<NewLine><NewLine>    def forward(self, x):<NewLine>        return self._forward_impl(x)<NewLine><NewLine><NewLine>def _replace_relu(module):<NewLine>    reassign = {}<NewLine>    for name, mod in module.named_children():<NewLine>        _replace_relu(mod)<NewLine>        if type(mod) == nn.ReLU or type(mod) == nn.ReLU6:<NewLine>            reassign[name] = nn.ReLU(inplace=False)<NewLine><NewLine>    for key, value in reassign.items():<NewLine>        module._modules[key] = value<NewLine><NewLine><NewLine>class QuantizableResNet(ResNet):<NewLine><NewLine>    def __init__(self, *args, **kwargs):<NewLine>        super(QuantizableResNet, self).__init__(*args, **kwargs)<NewLine><NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine>        x = self._forward_impl(x)<NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine><NewLine>    def fuse_model(self):<NewLine>        fuse_modules(self, ['conv1', 'bn1', 'relu'], inplace=True)<NewLine>        for m in self.modules():<NewLine>            if isinstance(m, (QuantizableBasicBlock, DecoderBlock)):<NewLine>                m.fuse_model()<NewLine><NewLine><NewLine>class Conv2dReLU(nn.Module):<NewLine>    def __init__(self, in_channels, out_channels, kernel_size, padding=0,<NewLine>                 stride=1, use_batchnorm=True, **batchnorm_params):<NewLine>        super().__init__()<NewLine>        layers = [<NewLine>            nn.Conv2d(in_channels, out_channels, kernel_size,<NewLine>                      stride=stride, padding=padding, bias=not (use_batchnorm), groups=1),<NewLine>            nn.ReLU(),<NewLine>        ]<NewLine><NewLine>        if use_batchnorm:<NewLine>            layers.insert(1, nn.BatchNorm2d(out_channels, **batchnorm_params))<NewLine><NewLine>        self.block = nn.Sequential(*layers)<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.block(x)<NewLine><NewLine>    def fuse_model(self):<NewLine>        fuse_modules(self, ['block.0', 'block.1', 'block.2'], inplace=True)<NewLine>        for m in self.modules():<NewLine>            if type(m) == QuantizableBasicBlock:<NewLine>                m.fuse_model()<NewLine><NewLine><NewLine>class Up(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine><NewLine>    def forward(self, x):<NewLine>        return F.upsample_bilinear(x, scale_factor=2)<NewLine><NewLine><NewLine>class DecoderBlock(nn.Module):<NewLine>    def __init__(self, in_channels, out_channels):<NewLine>        super().__init__()<NewLine>        self.block = nn.Sequential(<NewLine>            Conv2dReLU(in_channels, in_channels // 4, kernel_size=1, use_batchnorm=True),<NewLine>            Up(),<NewLine>            Conv2dReLU(in_channels // 4, out_channels, kernel_size=1, use_batchnorm=True),<NewLine>        )<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.block(x)<NewLine><NewLine>    def fuse_model(self):<NewLine>        for m in self.modules():<NewLine>            if isinstance(m, Conv2dReLU):<NewLine>                m.fuse_model()<NewLine><NewLine><NewLine>class ResNetUnet(QuantizableResNet):<NewLine><NewLine>    def __init__(self, *args, **kwargs):<NewLine>        super().__init__(*args, **kwargs)<NewLine>        self.pretrained = False<NewLine>        in_channels = (128, 64, 32, 16, 16)<NewLine>        prefinal_channels = 16<NewLine>        final_channels = 3<NewLine>        self.block1 = DecoderBlock(in_channels[0], in_channels[1])<NewLine>        self.block2 = DecoderBlock(in_channels[1], in_channels[2])<NewLine>        self.block3 = DecoderBlock(in_channels[2], in_channels[3])<NewLine>        self.block4 = DecoderBlock(in_channels[3], in_channels[4])<NewLine>        self.block5 = DecoderBlock(in_channels[4], prefinal_channels)<NewLine>        self.final_conv = nn.Conv2d(prefinal_channels, final_channels, kernel_size=(1, 1))<NewLine>        self.linear = nn.Linear(128, 10)<NewLine>        self.pool = nn.AdaptiveAvgPool2d(1)<NewLine>        self.add = torch.nn.quantized.FloatFunctional()<NewLine><NewLine>        del self.fc<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine>        x, cls = self._forward_impl(x)<NewLine>        x = self.dequant(x)<NewLine>        cls = self.dequant(cls)<NewLine>        return x, cls<NewLine><NewLine>    def _forward_impl(self, x):<NewLine>        x0 = self.conv1(x)<NewLine>        x0 = self.bn1(x0)<NewLine>        x0 = self.relu(x0)<NewLine>        x1 = self.maxpool(x0)<NewLine>        x1 = self.layer1(x1)<NewLine>        x2 = self.layer2(x1)<NewLine>        x3 = self.layer3(x2)<NewLine>        x4 = self.layer4(x3)<NewLine>        cls_out = self.linear(self.pool(x4).view(x4.size(0), -1))<NewLine>        x4 = self.block1(x4)<NewLine>        x3 = self.add.add(x3, x4)<NewLine>        x3 = self.block2(x3)<NewLine>        x2 = self.add.add(x2, x3)<NewLine>        x2 = self.block3(x2)<NewLine>        x1 = self.add.add(x1, x2)<NewLine>        x1 = self.block4(x1)<NewLine>        x0 = self.add.add(x0, x1)<NewLine>        x0 = self.block5(x0)<NewLine>        x0 = self.final_conv(x0)<NewLine><NewLine>        return x0, cls_out<NewLine><NewLine><NewLine>def print_size_of_model(model):<NewLine>    torch.save(model.state_dict(), ""temp.p"")<NewLine>    print('Size (MB):', os.path.getsize(""temp.p"") / 1e6)<NewLine>    os.remove('temp.p')<NewLine><NewLine><NewLine>def get_model():<NewLine>    model = ResNetUnet(QuantizableBasicBlock, [2, 2, 2, 2])<NewLine>    _replace_relu(model)<NewLine><NewLine>    return model<NewLine><NewLine><NewLine>def quantize_model(model, backend):<NewLine>    _dummy_input_data = torch.rand(1, 3, 224, 224)<NewLine>    if backend not in torch.backends.quantized.supported_engines:<NewLine>        raise RuntimeError(""Quantized backend not supported "")<NewLine>    torch.backends.quantized.engine = backend<NewLine>    model.eval()<NewLine>    if backend == 'fbgemm':<NewLine>        model.qconfig = torch.quantization.QConfig(<NewLine>            activation=torch.quantization.default_observer,<NewLine>            weight=torch.quantization.default_per_channel_weight_observer)<NewLine>    elif backend == 'qnnpack':<NewLine>        model.qconfig = torch.quantization.QConfig(<NewLine>            activation=torch.quantization.default_observer,<NewLine>            weight=torch.quantization.default_weight_observer)<NewLine><NewLine>    model.fuse_model()<NewLine>    torch.quantization.prepare(model, inplace=True)<NewLine>    model(_dummy_input_data)<NewLine>    torch.quantization.convert(model, inplace=True)<NewLine><NewLine><NewLine>def evaltime(model):<NewLine>    from time import time<NewLine>    res = 0<NewLine>    n = 500<NewLine>    for _ in range(n):<NewLine>        t = time()<NewLine>        with torch.no_grad():<NewLine>            model(torch.ones(1, 3, 224, 224))<NewLine>        res += time() - t<NewLine>    return res / n<NewLine><NewLine><NewLine>torch.set_num_threads(1)<NewLine>model = get_model()<NewLine>model.eval()<NewLine>print('time/image, initial model', evaltime(model)) # 0.025<NewLine>print_size_of_model(model) # Size (MB): 2.89155<NewLine>quantize_model(model, 'qnnpack')<NewLine>print('time/image, quantized model', evaltime(model)) # 0.101<NewLine>print_size_of_model(model) # Size (MB): 0.740964<NewLine><NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/ZackPashkin,(Zack Pashkin),ZackPashkin,"February 14, 2020,  9:12am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>For QNNPACK, the time you measure is on a x86 CPU. I am not surer how fast QNNPACK quantized implementation is compared to float on x86.</p><NewLine><p>Can you try with fbgemm on a desktop to see what you get?</p><NewLine><p>Also,  you should not reuse the same module (self.add.add) multiple times in forward. This is because each instance needs to collect its own statistics.  This will lead to poor accuracy with the quantized model.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you!<br/><NewLine>I’ve tried FBGEMM , but it seems it doens’t supported on most android devices (armeabi-v7a. arm64-v8a). Then I found this post <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/fbgemm-with-pytorch-mobile/60752"">FBGEMM with PyTorch Mobile</a> pointing out that FBGEMM is only compatible with x86 achitecture. Do you think I should try FBGEEM</p><NewLine><p>I’ll try out removing  multiple self.add.add in forward</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ZackPashkin; <NewLine> ,"REPLY_DATE 1: February 21, 2020,  3:32am; <NewLine> REPLY_DATE 2: February 21, 2020,  5:12am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
68709,Pytorch/torch/csrc/api/src/serialize/output-archive.cpp:39: error: undefined reference to &lsquo;torch::jit::ExportModule,2020-02-05T07:21:07.183Z,0,128,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>Below is my C++ code</p><NewLine><p><span class=""hashtag"">#include</span> &lt;torch/torch.h&gt;<br/><NewLine><span class=""hashtag"">#include</span> &lt;torch/script.h&gt; // One-stop header.</p><NewLine><p><span class=""hashtag"">#include</span> <br/><NewLine><span class=""hashtag"">#include</span> </p><NewLine><p>struct ID : torch::nn::Module {};<br/><NewLine>using namespace std;</p><NewLine><p>int main()<br/><NewLine>{<br/><NewLine>auto test = std::make_shared();<br/><NewLine>//torch::load(spkr_id,“net.pt”);<br/><NewLine>torch::save(test,“net.pt”);<br/><NewLine>}</p><NewLine><p>I’m getting Linkage error As I’m running on the Android Platform with cross-compiled PyTorch libraries</p><NewLine><p>rm: cannot remove ‘pytorch_app’: No such file or directory<br/><NewLine>– Check for working CXX compiler: /home/suresh/ANdroid_NEW_NDK/android-ndk-r21/toolchains/llvm/prebuilt/linux-x86_64/bin/clang++<br/><NewLine>– Check for working CXX compiler: /home/suresh/ANdroid_NEW_NDK/android-ndk-r21/toolchains/llvm/prebuilt/linux-x86_64/bin/clang++ – works<br/><NewLine>– Detecting CXX compiler ABI info<br/><NewLine>– Detecting CXX compiler ABI info - done<br/><NewLine>– Detecting CXX compile features</p><NewLine><p>– Detecting CXX compile features - done<br/><NewLine>– Configuring done<br/><NewLine>– Generating done<br/><NewLine>– Build files have been written to: /home/suresh/spkrid_ali/spkrid_jio/testsrc/android/build<br/><NewLine>Scanning dependencies of target pytorch_app<br/><NewLine>[ 25%] Building CXX object CMakeFiles/pytorch_app.dir/src/mian.cpp.o<br/><NewLine>[ 50%] Building CXX object CMakeFiles/pytorch_app.dir/src/test.cpp.o<br/><NewLine>[ 75%] Building CXX object CMakeFiles/pytorch_app.dir/src/test_nn.cpp.o<br/><NewLine>[100%] Linking CXX executable pytorch_app<br/><NewLine>/home/suresh/pytorch/torch/csrc/api/src/serialize/output-archive.cpp:39: error: undefined reference to ‘torch::jit::ExportModule(torch::jit::script::Module const&amp;, std::__ndk1::basic_string&lt;char, std::__ndk1::char_traits, std::__ndk1::allocator &gt; const&amp;, std::__ndk1::unordered_map&lt;std::__ndk1::basic_string&lt;char, std::__ndk1::char_traits, std::__ndk1::allocator &gt;, std::__ndk1::basic_string&lt;char, std::__ndk1::char_traits, std::__ndk1::allocator &gt;, std::__ndk1::hash&lt;std::__ndk1::basic_string&lt;char, std::__ndk1::char_traits, std::__ndk1::allocator &gt; &gt;, std::__ndk1::equal_to&lt;std::__ndk1::basic_string&lt;char, std::__ndk1::char_traits, std::__ndk1::allocator &gt; &gt;, std::__ndk1::allocator&lt;std::__ndk1::pair&lt;std::__ndk1::basic_string&lt;char, std::__ndk1::char_traits, std::__ndk1::allocator &gt; const, std::__ndk1::basic_string&lt;char, std::__ndk1::char_traits, std::__ndk1::allocator &gt; &gt; &gt; &gt; const&amp;, bool)’<br/><NewLine>clang++: error: linker command failed with exit code 1 (use -v to see invocation)<br/><NewLine>CMakeFiles/pytorch_app.dir/build.make:157: recipe for target ‘pytorch_app’ failed<br/><NewLine>make[2]: *** [pytorch_app] Error 1<br/><NewLine>CMakeFiles/Makefile2:67: recipe for target ‘CMakeFiles/pytorch_app.dir/all’ failed<br/><NewLine>make[1]: *** [CMakeFiles/pytorch_app.dir/all] Error 2<br/><NewLine>Makefile:83: recipe for target ‘all’ failed<br/><NewLine>make: *** [all] Error 2</p><NewLine></div>",https://discuss.pytorch.org/u/Suresh,(Sutradhar),Suresh,"February 5, 2020, 12:05pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>for “torch:: load(test, “net.pt”);” Compilation &amp; Linking got successful, But I’m stuck in torch:: save() API, is there any wrong in implementation or else definitions in the torch library is not compiled?</p><NewLine><p>Please help me because I’m new to this. Thanks in Advance.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Our Android builds currently exclude the code for saving because they are intended for inference.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Suresh; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: February 10, 2020,  7:17am; <NewLine> REPLY_DATE 2: February 20, 2020,  7:43pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
69794,PyTorch Android Score,2020-02-15T00:50:59.415Z,1,232,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I follow the android demo app from <a href=""https://github.com/pytorch/android-demo-app"" rel=""nofollow noopener"">https://github.com/pytorch/android-demo-app</a>, and I have questions below:</p><NewLine><p>From each prediction from the pytorch android, we get back score of each result. I know that the number that close to 0 is the best predicted result.</p><NewLine><p>My questions are</p><NewLine><ol><NewLine><li>How is the score calculated internally ?</li><NewLine><li>Why the score is negative?</li><NewLine><li>How to get percentage of the result based on the score?</li><NewLine></ol><NewLine><p>I also posted in github as well: <a href=""https://github.com/pytorch/android-demo-app/issues/60"" rel=""nofollow noopener"">https://github.com/pytorch/android-demo-app/issues/60</a></p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/vortana_say,(vortana say),vortana_say,"February 15, 2020, 12:50am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li><NewLine><p>Based on the demo code it looks like one demo is using a pretrained <code>torchvision.models.resnet18</code>.<br/><NewLine>This model outputs logits for each class, such that the highest logit would correspond to the predicted class. You could apply a softmax, which is not necessary to get the prediction, but will give you probabilities, which might be easier to interpret.</p><NewLine></li><NewLine><li><NewLine><p>If I’m right in point 1, you should see positive and negative numbers as the logit output.</p><NewLine></li><NewLine><li><NewLine><p>You could add a softmax layer to your model.</p><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> Thanks for the explanation.</p><NewLine><p>For 3, I already have a trained model and serialized it to be used in the mobile app, so by adding a softmax layer to my model, does it mean I have to re-train it since the original model doesn’t have a softmax layer. Are there anyway, that we can calculate percentage based on the result of the serialized model?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, you don’t need to retrain the model if you just want to get the probabilities with a softmax layer.<br/><NewLine>I think the easiest way could be to pass your current model and the <code>nn.Softmax</code> layer to a new custom model or an <code>nn.Sequential</code> container and use this new combined model in your app.</p><NewLine><p>Alternatively, you could compute the softmax on the mobile, as the formula is quite straightforward.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> thanks.</p><NewLine><p>I added:</p><NewLine><h1>Add softmax layer</h1><NewLine><p>mobilev2_loaded_model = nn.Sequential(<br/><NewLine>mobilev2_loaded_model,<br/><NewLine>nn.Softmax(1)<br/><NewLine>)</p><NewLine><p>and serialized model again and now I can multiple 100 to the score and get the percentage of that class.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vortana_say; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/vortana_say; <NewLine> ,"REPLY_DATE 1: February 15, 2020,  4:50am; <NewLine> REPLY_DATE 2: February 15, 2020, 11:52am; <NewLine> REPLY_DATE 3: February 15, 2020,  7:06pm; <NewLine> REPLY_DATE 4: February 20, 2020,  7:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
69627,Support torchvision.ops.nms on mobile CPU?,2020-02-13T16:53:47.133Z,0,210,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I try to use torchvision.ops.nms in my model, and convert mymodel.pth to mymodel.pt, it all sucess.But when load mymodel.pt  on mobile,came across error.</p><NewLine><pre><code class=""lang-auto"">    Serialized   File ""code/__torch__/torch/nn/modules/module/___torch_mangle_324.py"", line 63<NewLine>        _27 = torch.slice(_26, 1, 1, 9223372036854775807, 2)<NewLine>        _28 = torch.copy_(_27, torch.view(_25, [90, 2]), False)<NewLine>        keep = ops.torchvision.nms(boxes, scores0, 0.5)<NewLine>               ~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        _29 = torch.slice(boxes, 1, 0, 9223372036854775807, 1)<NewLine>        keep0 = torch.to(keep, dtype=4, layout=0, device=torch.device(""cpu""), pin_memory=False, non_blocking=False, copy=False, memory_format=None)<NewLine>    <NewLine>        at org.pytorch.NativePeer.initHybrid(Native Method)<NewLine>        at org.pytorch.NativePeer.&lt;init&gt;(NativePeer.java:18)<NewLine>        at org.pytorch.Module.load(Module.java:23)<NewLine>        at org.pytorch.helloworld.MainActivity.onCreate(MainActivity.java:39)<NewLine>        at android.app.Activity.performCreate(Activity.java:7458)<NewLine>        at android.app.Activity.performCreate(Activity.java:7448)<NewLine>        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1286)<NewLine>        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3409)<NewLine>        	... 11 more<NewLine></code></pre><NewLine><p>Someone can help,thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/KevinZ1992,(XiaoJian Zhang),KevinZ1992,"February 13, 2020,  4:53pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We don’t currently build and ship torchvision for mobile. We’re looking into it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/xta0; <NewLine> ,"REPLY_DATE 1: February 13, 2020,  6:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
69420,Torch::nn in iOS C++ is unavailable,2020-02-11T21:39:25.578Z,0,146,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I started from the HelloWorld example on iOS and everything worked until i tried to add <code>nn</code> functionality like</p><NewLine><pre><code class=""lang-auto"">auto loss = torch::nn::MSELoss(); // Error: No member named 'nn' in namespace 'torch'<NewLine></code></pre><NewLine><p>Is there a way to include <code>torch::nn</code> and <code>torch::nn::functional</code> API in iOS?</p><NewLine></div>",https://discuss.pytorch.org/u/dnnagy,(Nagy Dániel),dnnagy,"February 11, 2020,  9:54pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/dnnagy"">@dnnagy</a> the APIs under <code>nn</code> namespace are being used to support eager mode in Python. Since we’re using torchscript on mobile, we didn’t compile that part of code in our binary. However, I think you can manually enable it by changing the <code>NO_API</code> value to <code>OFF</code> in the root CMakeLists.txt and rebuild the libraries. However, I personally haven’t tried, so not sure if they’ll work or not.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/xta0; <NewLine> ,"REPLY_DATE 1: February 12, 2020, 10:48pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
69240,Problems with LAPACK on iOS,2020-02-10T16:01:53.440Z,3,290,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!<br/><NewLine>I’m trying to run my model on <strong>iOS</strong> and encounter a problem with <strong>LibTorch</strong>. The problem is in the call of <strong>SVD</strong> function.</p><NewLine><p>The following code:</p><NewLine><pre><code class=""lang-auto"">#import &lt;torch/script.h&gt;<NewLine>#import &lt;ATen/Functions.h&gt;<NewLine>at::Tensor tensor = torch::ones({3,3});<NewLine>auto result = at::svd(tensor);<NewLine></code></pre><NewLine><p>crushes with error as execution time:</p><NewLine><pre><code class=""lang-auto"">svd: LAPACK library not found in compilation (apply_svd at /Users/distiller/project/aten/src/ATen/native/BatchLinearAlgebra.cpp:935)<NewLine>(no backtrace available)<NewLine></code></pre><NewLine><p>Any thoughts would be appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/kulikovv,(Kulikov Victor),kulikovv,"February 10, 2020,  4:14pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am not very familiar with the iOS build system. Did you built libtorch from source yourself?</p><NewLine><p>The error comes from the fact that there were no linear algebra library found at compile time and so linear algebra functions (svd, eig, etc) are not available.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""69240"" data-username=""albanD""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/alband/40/215_2.png"" width=""20""/> albanD:</div><NewLine><blockquote><NewLine><p>The error comes from the fact that there were no linear algebra library found at compile time and so linear algebra functions (svd, eig, etc) are not available.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thanks for your reply! No, I’ve followed the tutorial <a href=""https://pytorch.org/mobile/ios/"" rel=""nofollow noopener"">https://pytorch.org/mobile/ios/</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Interesting. I don’t see any good reason why the official binary was not compiled with Lapack…<br/><NewLine>Let’s wait for some mobile people to shed some light.</p><NewLine><p>In the meantime. If you want, you can try to compile from source using the instructions from that link. And double check that during the compilation, your lapack library (the native apple one should be good) is properly detected and used.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/kulikovv"">@kulikovv</a>,</p><NewLine><p>We didn’t compile LAPACK into our binary. However, since LAPCK is supported by the Accelerate.framework on iOS. You can manually enable it by following the steps below</p><NewLine><ol><NewLine><li>Add a compiler flag - <code>CMAKE_ARGS+=(""-DUSE_LAPACK=ON"")</code> in <code>build_ios.sh</code> to tell cmake to compile LAPACK.</li><NewLine><li>Follow the tutorial to run the build script and setup your project</li><NewLine><li>Link Accelerate.framework to your project</li><NewLine></ol><NewLine><p>Let me know if it works for you. Thanks.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks! It solved my problem!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kulikovv; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/kulikovv; <NewLine> ,"REPLY_DATE 1: February 10, 2020,  4:05pm; <NewLine> REPLY_DATE 2: February 10, 2020,  4:07pm; <NewLine> REPLY_DATE 3: February 10, 2020,  4:15pm; <NewLine> REPLY_DATE 4: February 11, 2020,  1:35pm; <NewLine> REPLY_DATE 5: February 11, 2020,  1:37pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 2 Likes; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
61047,Anyway to use c++ api on mobile to do inference task?,2019-11-15T04:41:13.730Z,7,429,"<div class=""post"" itemprop=""articleBody""><NewLine><p>As the title mentioned, anyway to compile the c++ api and use them on android/ios? Or there exist prebuild binary?</p><NewLine></div>",https://discuss.pytorch.org/u/ngap_wei_Tham,(Ngap Wei Tham),ngap_wei_Tham,"November 15, 2019,  4:41am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>We do have a binary that you can build in order to test/run your model on an android device.<br/><NewLine>Here is a link to the file (in C++) - <a href=""https://github.com/pytorch/pytorch/blob/master/binaries/speed_benchmark_torch.cc"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/binaries/speed_benchmark_torch.cc</a></p><NewLine><p>You can build the binary by running</p><NewLine><pre><code class=""lang-auto"">./scripts/build_android.sh \<NewLine>-DBUILD_BINARY=ON \<NewLine>-DBUILD_CAFFE2_MOBILE=OFF \<NewLine>-DCMAKE_PREFIX_PATH=$(python -c 'from distutils.sysconfig import get_python_lib; print(get_python_lib())') \<NewLine>-DPYTHON_EXECUTABLE=$(python -c 'import sys; print(sys.executable)')<NewLine></code></pre><NewLine><p>To execute you can run</p><NewLine><pre><code class=""lang-auto"">./speed_benchmark_torch --model test.pt --input_dims=""1,3,224,224"" --input_type=float --warmup=5 --iter 20<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Any plan to release prebuild lib like linux, mac and windows?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ngap_wei_tham"">@ngap_wei_Tham</a><br/><NewLine>Pytorch mobile is moving towards providing tooling for building custom mobile builds which include  only operators that particular model needs. As all-ops default build maybe overshoot by lib size.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>We have prebuilt libraries for Android and iOS.  The iOS tutorial uses the C++ API: <a href=""https://pytorch.org/mobile/ios/"" rel=""nofollow noopener"">https://pytorch.org/mobile/ios/</a> .  We will probably support the C++ API on Android eventually, but for now the easiest path is to use the Java API.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have an example app here showing how to use a pytorch model in C++:</p><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://github.githubassets.com/favicon.ico"" width=""32""/><NewLine><a href=""https://github.com/RuABraun/android-with-pytorch/"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""420"" src=""https://avatars0.githubusercontent.com/u/12561072?s=400&amp;v=4"" width=""420""/><NewLine><h3><a href=""https://github.com/RuABraun/android-with-pytorch/"" rel=""nofollow noopener"" target=""_blank"">RuABraun/android-with-pytorch</a></h3><NewLine><p>Contribute to RuABraun/android-with-pytorch development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""61047""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/david_reiss/40/16743_2.png"" width=""20""/> David_Reiss:</div><NewLine><blockquote><NewLine><p>We will probably support the C++ API on Android eventually, but for now the easiest path is to use the Java API.</p><NewLine></blockquote><NewLine></aside><NewLine><p>It would be much easier to create cross-platform app if we could use c++ api, I have tried tensorflow-lite, but old of the tutorials of how to build the c++ api are too old, now trying with caffe2, hope it work.</p><NewLine><p>ps : Hope one day there will have a c++ library which could run the inference part of deep learning models efficiently on major platforms(desktop, mobiles and embedded devices), today we have to use x-tools for y platforms for performance(and other) sake.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, could you run object detection model by c++ api?mask-rcnn do not support yet(even it is, maybe too slow for mobile), have you tried with mobileNet quantized?Thanks</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>When I build the pytorch from source, I always get the error message</p><NewLine><p><code>src.cxx:1:10: fatal error: 'glog/stl_logging.h' file not found</code></p><NewLine><p>os : ubuntu18.04.3 LTS 64bits<br/><NewLine>ndk : r18b<br/><NewLine>sdk : 29(minimum 21)<br/><NewLine>gradle: 4.1.6<br/><NewLine>python : 3.6, install pytorch1.3 with conda</p><NewLine><p>Do you know how to solve it? Thanks</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>You probably need to install glog: <code>sudo apt install libgoogle-glog-dev</code></p><NewLine><p>Don’t forget you have to build pytorch for android and set <code>TORCHPATH</code> correctly in the CMakeLists.txt file!</p><NewLine><p>Let me know if it works.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, don’t work for 1.3.0, haven’t tried with 1.4.0 yet</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ngap_wei_Tham; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/divinho; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ngap_wei_Tham; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ngap_wei_Tham; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/ngap_wei_Tham; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/divinho; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/ngap_wei_Tham; <NewLine> ,"REPLY_DATE 1: November 15, 2019,  5:24am; <NewLine> REPLY_DATE 2: November 24, 2019,  8:07pm; <NewLine> REPLY_DATE 3: November 27, 2019, 10:47pm; <NewLine> REPLY_DATE 4: December 3, 2019,  7:23pm; <NewLine> REPLY_DATE 5: December 11, 2019,  6:46pm; <NewLine> REPLY_DATE 6: December 30, 2019,  3:38pm; <NewLine> REPLY_DATE 7: December 30, 2019,  4:10pm; <NewLine> REPLY_DATE 8: December 30, 2019,  5:11pm; <NewLine> REPLY_DATE 9: January 3, 2020, 11:45pm; <NewLine> REPLY_DATE 10: February 6, 2020,  3:57pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: 2 Likes; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> 
67550,Getting build_android/CMakeFiles/CMakeTmp/src.cxx:1:10: fatal error: &lsquo;glog/stl_logging.h&rsquo; file not found,2020-01-24T12:31:46.595Z,0,214,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to build the source code on ubuntu 18;<br/><NewLine>Getting the following error:<br/><NewLine>build_android/CMakeFiles/CMakeTmp/src.cxx:1:10: fatal error: ‘glog/stl_logging.h’ file not found</p><NewLine><p>I am using the following command to build the same.<br/><NewLine>./scripts/build_pytorch_android.sh</p><NewLine><p>Could you please suggest me to get out of this.</p><NewLine><p>Thanks &amp; Regards,<br/><NewLine>Jamili.</p><NewLine></div>",https://discuss.pytorch.org/u/jamilireddy,(JAMILI REDDY OBULAREDDY),jamilireddy,"January 28, 2020,  4:50am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This question looks like it should be routed to the Mobile section of the forum. Cc <a class=""mention"" href=""/u/david_reiss"">@David_Reiss</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think that’s a normal message that happens when the build system detects that glog is not available.  Can you create a paste of the full build output?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Jeff_Smith; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: January 27, 2020,  6:44pm; <NewLine> REPLY_DATE 2: February 5, 2020,  6:36pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
67986,Build PyTorch android binaries with -O0?,2020-01-29T07:46:11.182Z,0,147,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to track down a bug that happens inside a pytorch android binary (speed_benchmark_torch) for one of my models.</p><NewLine><p>When I look at the backtrace on android <code>logcat</code>, it looks like debug symbols are disabled, so I don’t know what line of code is failing. Also, I tried building with <code>-DANDROID_DEBUG_SYMBOLS=1</code>, and I still couldn’t get a stacktrace with debug symbols.</p><NewLine><p>I suspect that I will get a better stacktrace if I can compile with <code>-O0</code> instead of the current <code>-O2</code>. Towards this end, I changed one of the lines in <code>pytorch/CMakeLists.txt</code> from this:<br/><NewLine><code>set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -O2 -fPIC"")</code><br/><NewLine>to this:<br/><NewLine><code>set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -O0 -fPIC"")</code></p><NewLine><p>I also changed the <code>O2</code> to <code>O0</code> in the QNNPACK CMakeLists.</p><NewLine><p>And, after making that change, I recompiled pytorch. When I look at <code>pytorch/build/compile_commands.json</code>, I see we use -O0 in the pytorch build. Great.</p><NewLine><p>Now, I run <code>scripts/build_android.sh</code>, and to my dismay I see that each command in <code>build_android/compile_commands.json</code> has both -O0 and -O2. That’s not good!</p><NewLine><p>Does anyone know…<br/><NewLine>(a) Where might this stray -O2 be coming from, and<br/><NewLine>(b) How do I do an “only O0” build of the android binaries, particularly speed_benchmark_torch?</p><NewLine><p>One idea I had is to just manually compile speed_benchmark_torch, since I have its compile command in <code>build_android/compile_commands.json</code>. However, I don’t know the link command, and I imagine there’s a long list of flags on the link command.</p><NewLine></div>",https://discuss.pytorch.org/u/solvingPuzzles,,solvingPuzzles,"January 30, 2020,  7:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Figured it out. The solution is to use <code>-DCMAKE_BUILD_TYPE=Debug</code> in build_android.sh. Otherwise, it defaults to <code>Release</code>, which seems to add the <code>-O2</code> flag.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/solvingPuzzles; <NewLine> ,"REPLY_DATE 1: February 2, 2020, 11:53pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
68182,Export Torchscript module and call other than forward,2020-01-31T03:12:38.641Z,0,129,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I have an exported torch script module that looks loosely like:</p><NewLine><pre><code class=""lang-auto"">class Model(torch.nn.Module):<NewLine>    def forward(self, tensor):<NewLine>        // do forward pass<NewLine><NewLine>    @torch.jit.export<NewLine>    def reinitialize_hidden_buffers(self, tensor):<NewLine>        // reinitializes model parameters<NewLine></code></pre><NewLine><p>When I load the torchscript module in Python, I can call reinitialize_hidden_buffers, but when the model is exported and loaded into objective c (for deployment on iOS),  I get<br/><NewLine><code>no member named 'reinitialize_hidden_buffers' in 'torch::jit::script::Module</code><br/><NewLine>Is there a way to call this correctly? Or should I just put control flow logic in the forward pass to do it.</p><NewLine></div>",https://discuss.pytorch.org/u/PCerles,(P Cerles),PCerles,"January 31, 2020,  6:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This ended up being really straightforward.</p><NewLine><pre><code class=""lang-auto"">torch::autograd::AutoGradMode guard(false);<NewLine>at::AutoNonVariableTypeMode non_var_type_mode(true);<NewLine>_impl.run_method(""reinitialize"");<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/PCerles; <NewLine> ,"REPLY_DATE 1: February 20, 2020,  7:42pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
66634,Torch.no_grad() for Java,2020-01-14T13:00:19.428Z,1,204,"<div class=""post"" itemprop=""articleBody""><NewLine><p>What is the equivalent for doing <code>torch.no_grad()</code> in Java?<br/><NewLine>Like in C++ it is <code>torch::NoGradGuard</code></p><NewLine></div>",https://discuss.pytorch.org/u/vishalagarwal,(Vishal Agarwal),vishalagarwal,"January 14, 2020,  3:45pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>You mean on mobile? Or do you refer to another Java binding?<br/><NewLine>IIRC it does not support training, so it is running in no_grad all the time.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I mean running it on mobile.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/vishalagarwal"">@vishalagarwal</a><br/><NewLine>Java binding works for forward only and auto grad is disabled all the time.</p><NewLine><p>Details:<br/><NewLine>It has torch::autograd::AutoGradMode g{false} before every forward operation.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp#L28"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp#L28"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/master/android/pytorch_android/src/main/cpp/pytorch_jni_jit.cpp#L28</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""18"" style=""counter-reset: li-counter 17 ;""><NewLine><li>#include &lt;android/asset_manager_jni.h&gt;</li><NewLine><li>#include &lt;android/log.h&gt;</li><NewLine><li>#endif</li><NewLine><li><NewLine></li><NewLine><li>namespace pytorch_jni {</li><NewLine><li><NewLine></li><NewLine><li>namespace {</li><NewLine><li><NewLine></li><NewLine><li>struct JITCallGuard {</li><NewLine><li>  // AutoGrad is disabled for mobile by default.</li><NewLine><li class=""selected"">  torch::autograd::AutoGradMode no_autograd_guard{false};</li><NewLine><li>  // Disable graph optimizer to ensure list of unused ops are not changed for</li><NewLine><li>  // custom mobile build.</li><NewLine><li>  torch::jit::GraphOptimizerEnabledGuard no_optimizer_guard{false};</li><NewLine><li>};</li><NewLine><li><NewLine></li><NewLine><li>} // namespace</li><NewLine><li><NewLine></li><NewLine><li>class MemoryReadAdapter final : public caffe2::serialize::ReadAdapterInterface {</li><NewLine><li> public:</li><NewLine><li>  explicit MemoryReadAdapter(const void* data, off_t size)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vishalagarwal; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> ,"REPLY_DATE 1: January 14, 2020,  3:45pm; <NewLine> REPLY_DATE 2: January 15, 2020,  5:17am; <NewLine> REPLY_DATE 3: January 24, 2020,  9:55pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 2 Likes; <NewLine> 
63179,Model Loading issue in iOS,2019-12-06T00:30:49.676Z,9,527,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I get the following error when trying to initialize a traced model with the iOS framework:<br/><NewLine>PyTorchDemo[25132:6735005] false CHECK FAILED at /Users/distiller/project/c10/core/Backend.h (tensorTypeIdToBackend at /Users/distiller/project/c10/core/Backend.h:106)<br/><NewLine>(no backtrace available)</p><NewLine><p>The error appears similar to these Android issues:<br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/android-demo-app/issues/19"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/android-demo-app</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/android-demo-app/issues/19"" rel=""nofollow noopener"" target=""_blank"">Failed to Load Custom Model in Android</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-10-16"" data-format=""ll"" data-time=""01:37:23"" data-timezone=""UTC"">01:37AM - 16 Oct 19 UTC</span><NewLine></div><NewLine><div class=""date""><NewLine>          closed <span class=""discourse-local-date"" data-date=""2019-10-24"" data-format=""ll"" data-time=""16:40:35"" data-timezone=""UTC"">04:40PM - 24 Oct 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/junjuew"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""junjuew"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars0.githubusercontent.com/u/1829669?v=4"" width=""20""/><NewLine>          junjuew<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">Hello,<NewLine>I'm trying to deploy a custom model in python to Android using pytorch mobile. I used torch.jit.trace to trace and then...</p><NewLine></div><NewLine><div class=""labels""><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine><aside class=""quote quote-modified"" data-post=""1"" data-topic=""60990""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/mohit7/40/17896_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/model-loading-issue-in-android/60990"">Model Loading issue in android</a> <a class=""badge-wrapper bullet"" href=""/c/mobile""><span class=""badge-category-bg"" style=""background-color: #92278F;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""This category is dedicated for iOS and Android issues, new features and general discussion of PyTorch Mobile."">Mobile</span></a><NewLine></div><NewLine><blockquote><NewLine>    My model contains nn.Functional.interpolate layer and it is converted properly to .pt model using jit. <NewLine>But Loading same model in android giving issues. <NewLine>This is happening in case of interpolate layer only. <NewLine>Edit - Error trace <NewLine>java.lang.RuntimeException: Unable to start activity ComponentInfo{org.pytorch.custom_model/org.pytorch.custom_model.MainActivity}: com.facebook.jni.CppException: false CHECK FAILED at …/c10/core/Backend.h (tensorTypeIdToBackend at …/c10/core/Backend.h:106) <NewLine>(no backtrace…<NewLine>  </blockquote><NewLine></aside><NewLine></p><NewLine><p>The second linked issue states that there’s an Android update with a fix --is there a fix for the iOS framework?</p><NewLine><p>The model I’m using was traced with PyTorch 1.3.1, and I’m using LibTorch 1.3.1</p><NewLine></div>",https://discuss.pytorch.org/u/kennywalker,(Kenny Walker),kennywalker,"December 6, 2019, 12:30am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like the fix was landed to master as mentioned on <span class=""hashtag"">#29806</span> so it will be part of 1.4 release. You can test the nightly build of iOS. <a class=""mention"" href=""/u/ivankobzarev"">@IvanKobzarev</a> could you please confirm?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah, I’m pretty sure this is resolved.  If you want to verify, the steps for testing with the nightly build are in the first half of this comment: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/incomprehensible-behaviour/61710/4"">Incomprehensible behaviour</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/kennywalker"">@kennywalker</a>, are you using cocoapods on iOS? if so, the correct version should be 1.3.1. If the error still exist, try the iOS nightly build - <a href=""https://ossci-ios-build.s3.amazonaws.com/libtorch_ios_nightly_build.zip"" rel=""nofollow noopener"">https://ossci-ios-build.s3.amazonaws.com/libtorch_ios_nightly_build.zip</a>. Let me know if you have any questions.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>The model loads with the nightly build.</p><NewLine><p>Thank you for the help <a class=""mention"" href=""/u/ljk53"">@ljk53</a>, <a class=""mention"" href=""/u/david_reiss"">@David_Reiss</a>, <a class=""mention"" href=""/u/xta0"">@xta0</a></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/xta0"">@xta0</a></p><NewLine><p>I was using cocoapods 1.3.1. Can the 1.4 podspec be published?.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/kennywalker"">@kennywalker</a>, we’re going to publish 1.4 in Jan (Hopefully). Could you give a solution mark on this?  If there is any questions, feel free to leave a comment.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/xta0"">@xta0</a>, is there an update about the release date of 1.4 for iOS?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think it’ll be in the middle of Jan. <a class=""mention"" href=""/u/jspisak"">@jspisak</a> do you know the exact date?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, we are targeting mid next week to release 1.4. Look for the blog post on <a href=""http://pytorch.org"">pytorch.org</a>. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Dear <a class=""mention"" href=""/u/xta0"">@xta0</a>, I tried to use the current iOS nightly build, but it doesn’t work. I’ve noticed that the file <code>libtorch.a</code> has only 944 bytes.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Dear <a class=""mention"" href=""/u/kennywalker"">@kennywalker</a>, could you share with us the version you downloaded on December 19th, please?</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/fabricionarcizo"">@fabricionarcizo</a> the nightly build is from master branch. We’ve recently split the <code>libtorch.a</code> into multiple libraries. If you unzip the nightly build, you should see this - <code>libtorch_cpu.a</code> which is the old <code>libtorch.a</code>.</p><NewLine><ul><NewLine><li>PR regarding the spliting - <a href=""https://github.com/pytorch/pytorch/pull/30315"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/30315</a><NewLine></li><NewLine><li>Nightly build - <a href=""https://ossci-ios-build.s3.amazonaws.com/libtorch_ios_nightly_build.zip"" rel=""nofollow noopener"">https://ossci-ios-build.s3.amazonaws.com/libtorch_ios_nightly_build.zip</a><NewLine></li><NewLine></ul><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/xta0"">@xta0</a> I just tried to compile the <code>libtorch.a</code> using the branch <code>v1.4.0</code> with the following command:</p><NewLine><pre><code class=""lang-auto"">SELECTED_OP_LIST=predictor.yaml BUILD_PYTORCH_MOBILE=1 IOS_ARCH=arm64 ./scripts/build_ios.sh<NewLine></code></pre><NewLine><p>It compiles correctly, but I got some additional error when I try to link the <code>libtorch.a</code> on Xcode.</p><NewLine><pre><code class=""lang-auto"">Undefined symbol: c10::NonVariableTypeMode::is_enabled()<NewLine>Undefined symbol: torch::jit::script::Module::module_object() const<NewLine>Undefined symbol: c10::NonVariableTypeMode::set_enabled(bool)<NewLine>Undefined symbol: caffe2::detail::_typeMetaDataInstance_preallocated_6<NewLine>Undefined symbol: at::TypeDefault::to(at::Tensor const&amp;, c10::TensorOptions const&amp;, bool, bool)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/fabricionarcizo"">@fabricionarcizo</a> <code>SELECTED_OP_LIST</code> is used for the custom build. Also, the build is in arm64, were you running in simulator?</p><NewLine><p>Since I don’t have your code, my recommendation is to follow the tutorials</p><NewLine><ul><NewLine><li>1.4.0 non-custom build - <a href=""https://github.com/pytorch/ios-demo-app"" rel=""nofollow noopener"">the HelloWorld example</a><NewLine></li><NewLine><li>1.4.0 custom build - <a href=""https://pytorch.org/mobile/ios/"" rel=""nofollow noopener"">https://pytorch.org/mobile/ios/</a><NewLine></li><NewLine></ul><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ljk53; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/kennywalker; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/kennywalker; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/thoval; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/jspisak; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/fabricionarcizo; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/fabricionarcizo; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/fabricionarcizo; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/xta0; <NewLine> ,"REPLY_DATE 1: December 6, 2019,  4:23am; <NewLine> REPLY_DATE 2: December 9, 2019,  9:53pm; <NewLine> REPLY_DATE 3: December 9, 2019, 10:28pm; <NewLine> REPLY_DATE 4: December 10, 2019,  7:00am; <NewLine> REPLY_DATE 5: December 10, 2019,  7:02am; <NewLine> REPLY_DATE 6: December 10, 2019,  9:42pm; <NewLine> REPLY_DATE 7: January 7, 2020, 11:21am; <NewLine> REPLY_DATE 8: January 10, 2020,  1:10am; <NewLine> REPLY_DATE 9: January 10, 2020,  2:15am; <NewLine> REPLY_DATE 10: January 15, 2020,  3:38pm; <NewLine> REPLY_DATE 11: January 15, 2020,  3:38pm; <NewLine> REPLY_DATE 12: January 15, 2020,  6:23pm; <NewLine> REPLY_DATE 13: January 16, 2020, 10:08am; <NewLine> REPLY_DATE 14: January 16, 2020,  7:42pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> 
66532,JIT android issue,2020-01-13T14:37:51.516Z,2,163,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have created JIT model of the object detection SSD network and it runs fine when I run the traced model on Python or C++. I am following PyTorch mobile’s tutorial to load the network on phone. But when I do forward pass of the same traced model in Java environment of Android Studio, it is waits on that line for hours and the execution is completed. Anyone with any pointers what might be the issue?<br/><NewLine>Help appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/vishalagarwal,(Vishal Agarwal),vishalagarwal,"January 13, 2020,  6:40pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/vishalagarwal"">@vishalagarwal</a> ,<br/><NewLine>Which version of pytorch android api do you use 1.3, 1.3.1 or nightly builds?</p><NewLine><p>We had a deadlock issue that was fixed in  <a href=""https://github.com/pytorch/pytorch/pull/29885"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/29885</a></p><NewLine><p>If you are not using nightly builds, could you please check if it reproduces on nightlies?(<a href=""https://github.com/pytorch/pytorch/tree/master/android#nightly"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/tree/master/android#nightly</a>)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am using pytorch 1.3.1<br/><NewLine>Let me try with nightly build</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It resolved with the nightly build. Thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vishalagarwal; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vishalagarwal; <NewLine> ,"REPLY_DATE 1: January 14, 2020,  8:36am; <NewLine> REPLY_DATE 2: January 14, 2020,  6:19am; <NewLine> REPLY_DATE 3: January 14, 2020,  8:36am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
65671,Using Pytorch Android under Kotlin,2020-01-03T10:51:12.262Z,1,558,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Can I use Kotlin for that purpose or it’s possible only with Java?</p><NewLine></div>",https://discuss.pytorch.org/u/Chame_call,(chame_call),Chame_call,"January 3, 2020, 10:51am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/david_reiss"">@David_Reiss</a> - knows better. We do actually support Kotlin… <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yep.  All Java code is fairly easy to use from Kotlin: <a href=""https://kotlinlang.org/docs/reference/java-interop.html"">https://kotlinlang.org/docs/reference/java-interop.html</a> .  That includes the PyTorch Java bindings.  We’ve tested it out briefly, and they seem to work fine, so let us know if you run into any trouble!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jspisak; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: January 10, 2020,  8:43pm; <NewLine> REPLY_DATE 2: January 10, 2020,  8:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> 
65504,How to load custom TorchScript ops in PyTorch Mobile?,2020-01-01T09:05:44.157Z,0,170,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is there any tutorials or examples about loading custom TorchScript ops in PyTorch Mobile</p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"January 1, 2020,  9:05am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/david_reiss"">@David_Reiss</a> <a class=""mention"" href=""/u/ivankobzarev"">@IvanKobzarev</a>  Could I load custom TorchScript ops in PyTorch Mobile?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>what are the custom ops you are trying to script? If the model is scriptable you shouldnt need to do anything else (unless I am missing something).</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you create a library (.so) that features your custom op registration as a static variable and link that into your app, it’ll work. That’s how I did it back in 2018 and it should still work.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thomas is correct.  Here’s a demo of using the C++ API directly, which should allow custom op registration: <a href=""https://github.com/ljk53/pytorch-android-cpp-demo"">https://github.com/ljk53/pytorch-android-cpp-demo</a></p><NewLine><p>I know the process is a bit cumbersome.  We’re looking at simplifying this over time.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dalalaa; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jspisak; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: January 7, 2020,  1:49am; <NewLine> REPLY_DATE 2: January 7, 2020,  6:24am; <NewLine> REPLY_DATE 3: January 7, 2020,  7:40am; <NewLine> REPLY_DATE 4: January 10, 2020,  1:12am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 2 Likes; <NewLine> 
64833,"Fatal signal 11 (SIGSEGV), code 1 when calling model.load()",2019-12-23T16:13:50.236Z,3,1184,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When starting the application I get the following output and app crashes:</p><NewLine><pre><code class=""lang-auto""> W/linker: libpytorch_jni.so: unused DT entry: type 0x6ffffffe arg 0x2f524<NewLine>    libpytorch_jni.so: unused DT entry: type 0x6fffffff arg 0x3<NewLine>    libfbjni.so: unused DT entry: type 0x6ffffffe arg 0x2e18c<NewLine>W/linker: libfbjni.so: unused DT entry: type 0x6fffffff arg 0x2<NewLine>A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x0 in tid 19006 (rg.pytorch.demo)<NewLine>Process 19006 terminated.<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">My `build.gradle`:<NewLine><NewLine>apply plugin: 'com.android.application'<NewLine>apply plugin: 'com.google.gms.google-services'<NewLine><NewLine>repositories {<NewLine>    jcenter()<NewLine>    maven {<NewLine>        url ""https://oss.sonatype.org/content/repositories/snapshots""<NewLine>    }<NewLine>}<NewLine><NewLine>android {<NewLine>    compileOptions {<NewLine>        sourceCompatibility 1.8<NewLine>        targetCompatibility 1.8<NewLine>    }<NewLine>    compileSdkVersion 28<NewLine>    defaultConfig {<NewLine>        applicationId ""org.pytorch.demo""<NewLine>        minSdkVersion 21<NewLine>        targetSdkVersion 28<NewLine>        versionCode 1<NewLine>        versionName ""1.0""<NewLine>        testInstrumentationRunner ""androidx.test.runner.AndroidJUnitRunner""<NewLine>    }<NewLine>    buildTypes {<NewLine>        release {<NewLine>            minifyEnabled false<NewLine>            proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'<NewLine>        }<NewLine>    }<NewLine>}<NewLine><NewLine>dependencies {<NewLine><NewLine>    implementation 'androidx.appcompat:appcompat:1.0.0-beta01'<NewLine>    implementation 'androidx.constraintlayout:constraintlayout:1.1.3'<NewLine><NewLine>    def camerax_version = '1.0.0-alpha05'<NewLine>    implementation ""androidx.camera:camera-core:$camerax_version""<NewLine>    implementation ""androidx.camera:camera-camera2:$camerax_version""<NewLine>    implementation 'com.google.android.material:material:1.0.0-beta01'<NewLine><NewLine><NewLine>    implementation 'org.pytorch:pytorch_android:1.4.0-SNAPSHOT'<NewLine>    implementation 'org.pytorch:pytorch_android_torchvision:1.4.0-SNAPSHOT'<NewLine>    implementation 'com.google.firebase:firebase-ml-vision:24.0.1'<NewLine>}<NewLine></code></pre><NewLine><p>What causes the crash?</p><NewLine></div>",https://discuss.pytorch.org/u/Chame_call,(chame_call),Chame_call,"December 23, 2019,  4:13pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The strangest thing that I already launched the app early.<br/><NewLine>And as I have the previous built .apk I compared it with my new one.<br/><NewLine>And they differ in sizes of libs (on the screen below), but when I replace my new libs in …/gradle/caches/… by older ones and build app in Android Studio they are replaced again.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/49a2ed8779febe7d911fb7a1ce164518f54a2cae"" href=""https://discuss.pytorch.org/uploads/default/original/3X/4/9/49a2ed8779febe7d911fb7a1ce164518f54a2cae.png"" title=""Selection_035""><img alt=""Selection_035"" data-base62-sha1=""avpVs2dc3uJoczocCMYQ4Vg3T4i"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/9/49a2ed8779febe7d911fb7a1ce164518f54a2cae_2_10x10.png"" height=""144"" src=""https://discuss.pytorch.org/uploads/default/original/3X/4/9/49a2ed8779febe7d911fb7a1ce164518f54a2cae.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Selection_035</span><span class=""informations"">940×197 12.4 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>So why app is built with different libs in different cases?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is that possible that <code>pytorch_android_torchvision:1.4.0-SNAPSHOT</code> was updated and older version worked while current doesn’t…?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is a bug.  Fix is <a href=""https://github.com/pytorch/pytorch/pull/31582"">https://github.com/pytorch/pytorch/pull/31582</a> .  Should be available in the next nightly build.  Sorry for the trouble.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>You mean I have to make correspond corrects like in <a href=""http://here"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/31582/commits/25345223c6c6e5912f2d02894bf4475270ad1a12</a> ?</p><NewLine><p>or vice versa??</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh, no.  Sorry.  I meant it is a bug in PyTorch, and that commit should fix it.  Please try again with the latest nightly release, without changing any of your code, and the problem should be gone.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks, problem has gone)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Chame_call; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Chame_call; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Chame_call; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Chame_call; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Chame_call; <NewLine> ,"REPLY_DATE 1: December 23, 2019,  5:08pm; <NewLine> REPLY_DATE 2: December 23, 2019,  8:29pm; <NewLine> REPLY_DATE 3: December 23, 2019,  8:45pm; <NewLine> REPLY_DATE 4: December 24, 2019,  9:48am; <NewLine> REPLY_DATE 5: December 24, 2019, 10:41pm; <NewLine> REPLY_DATE 6: December 27, 2019,  4:56pm; <NewLine> REPLY_DATE 7: December 27, 2019,  4:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> 
60846,[JIT] [Mobile] Wrong substitution of aten::to,2019-11-13T11:09:48.829Z,0,270,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello again,</p><NewLine><p>As I’ve introduced here (<a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/jit-scripted-model-aten-to-failed-on-mobile/60759"">[JIT] Scripted model aten::to failed on Mobile</a>) there might be a bug inside TorchScript scripting or inside mobile library.<br/><NewLine>I’ve found out that any <code>aten::to</code> scripted from Python code to JIT comes with extra parameter inside function call. Let me show an example, here is python code and and the place where interpreter finds an error</p><NewLine><pre><code class=""lang-auto"">Compiled from code at /root/anaconda2/envs/pytorch-nightly/lib/python3.7/site-packages/torchvision/ops/boxes.py:68:14<NewLine>            in decreasing order of scores<NewLine>        """"""<NewLine>        if boxes.numel() == 0:<NewLine>            return torch.empty((0,), dtype=torch.int64, device=boxes.device)<NewLine>        # strategy: in order to perform NMS independently per class.<NewLine>        # we add an offset to all the boxes. The offset is dependent<NewLine>        # only on the class idx, and is large enough so that boxes<NewLine>        # from different classes do not overlap<NewLine>        max_coordinate = boxes.max()<NewLine>        offsets = idxs.to(boxes) * (max_coordinate + 1)<NewLine>                  ~~~~~~~ &lt;--- HERE<NewLine>        boxes_for_nms = boxes + offsets[:, None]<NewLine>        keep = nms(boxes_for_nms, scores, iou_threshold)<NewLine>        return keep<NewLine></code></pre><NewLine><p>here’s what is wrong</p><NewLine><pre><code class=""lang-auto"">Arguments for call are not valid.<NewLine>    The following operator variants are available:<NewLine>      <NewLine>      aten::to.other(Tensor self, Tensor other, bool non_blocking=False, bool copy=False) -&gt; (Tensor):<NewLine>      Expected at most 4 arguments but found 5 positional arguments.<NewLine>      <NewLine>      aten::to.dtype(Tensor self, int dtype, bool non_blocking=False, bool copy=False) -&gt; (Tensor):<NewLine>      Expected at most 4 arguments but found 5 positional arguments.<NewLine>      <NewLine>      aten::to.device(Tensor self, Device device, int dtype, bool non_blocking=False, bool copy=False) -&gt; (Tensor):<NewLine>      Expected a value of type 'Device' for argument 'device' but instead found type 'Tensor'.<NewLine>      <NewLine>      aten::to.dtype_layout(Tensor self, *, int dtype, int layout, Device device, bool pin_memory=False, bool non_blocking=False, bool copy=False) -&gt; (Tensor):<NewLine>      Argument dtype not provided.<NewLine>      <NewLine>      aten::to(Tensor(a) self, Device? device, int? dtype=None, bool non_blocking=False, bool copy=False) -&gt; (Tensor(b|a)):<NewLine>      Expected a value of type 'Optional[Device]' for argument 'device' but instead found type 'Tensor'.<NewLine>      <NewLine>      aten::to(Tensor(a) self, int? dtype=None, bool non_blocking=False, bool copy=False) -&gt; (Tensor(b|a)):<NewLine>      Expected a value of type 'Optional[int]' for argument 'dtype' but instead found type 'Tensor'.<NewLine>      <NewLine>      aten::to(Tensor(a) self, bool non_blocking=False, bool copy=False) -&gt; (Tensor(b|a)):<NewLine>      Expected a value of type 'bool' for argument 'non_blocking' but instead found type 'Tensor'.<NewLine>    <NewLine>    The original call is:<NewLine>    at code/__torch__/torchvision/ops/boxes.py:16:9<NewLine>      if torch.eq(torch.numel(boxes), 0):<NewLine>        _3 = ops.prim.device(boxes)<NewLine>        _1, _2 = True, torch.empty([0], dtype=4, layout=None, device=_3, pin_memory=None, memory_format=None)<NewLine>      else:<NewLine>        _1, _2 = False, _0<NewLine>      if _1:<NewLine>        _4 = _2<NewLine>      else:<NewLine>        max_coordinate = torch.max(boxes)<NewLine>        _5 = torch.to(idxs, boxes, False, False, None)<NewLine>             ~~~~~~~~ &lt;--- HERE<NewLine>        offsets = torch.mul(_5, torch.add(max_coordinate, 1, 1))<NewLine>        _6 = torch.slice(offsets, 0, 0, 9223372036854775807, 1)<NewLine>        boxes_for_nms = torch.add(boxes, torch.unsqueeze(_6, 1), alpha=1)<NewLine>        _4 = __torch__.torchvision.ops.boxes.nms(boxes_for_nms, scores, iou_threshold, )<NewLine>      return _4<NewLine></code></pre><NewLine><p>For some reason JIT provided code with extraneous parameter <code>None</code>. It will do it even we provide all named parameters to define which overriden function should be used.</p><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"December 26, 2019,  6:33pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Notice that please. There’s high probability of bug. TorchScript produces <code>aten::to</code> with extra <code>None</code> parameter</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am still coming across this issue. TorchScript <code>script</code> produces redundant <code>None</code> parameter</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry for the late reply.  In the future, please post in the “mobile” category so the mobile developers will see them.  Do you have a script or notebook that we can use to reproduce this issue?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: November 15, 2019,  9:47am; <NewLine> REPLY_DATE 2: November 19, 2019,  9:14am; <NewLine> REPLY_DATE 3: December 26, 2019,  6:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
60759,[JIT] Scripted model aten::to failed on Mobile,2019-11-12T14:59:30.555Z,0,246,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, everyone<br/><NewLine>Recently I have been working on transfering complex model written on Python via TorchScript to use on mobile.<br/><NewLine>After successful scripting I am unable to load model on Android device and here is the reason why. <code>torch.jit.script</code> produces inconsistent graph. Here is the error (from android studio):</p><NewLine><pre><code class=""lang-auto"">2019-11-12 17:39:52.357 13915-13923/? I/orch.helloworl: jit_compiled:[OK] java.lang.AbstractStringBuilder java.lang.AbstractStringBuilder.append(java.lang.String) @ /apex/com.android.runtime/javalib/core-oj.jar<NewLine>2019-11-12 17:39:52.357 13915-13915/? E/AndroidRuntime:     at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:91)<NewLine>        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:149)<NewLine>        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:103)<NewLine>        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2368)<NewLine>        at android.os.Handler.dispatchMessage(Handler.java:107)<NewLine>        at android.os.Looper.loop(Looper.java:213)<NewLine>        at android.app.ActivityThread.main(ActivityThread.java:8106)<NewLine>        at java.lang.reflect.Method.invoke(Native Method)<NewLine>        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:513)<NewLine>        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1100)<NewLine>     Caused by: com.facebook.jni.CppException: <NewLine>    Arguments for call are not valid.<NewLine>    The following operator variants are available:<NewLine>      <NewLine>      aten::to.other(Tensor self, Tensor other, bool non_blocking=False, bool copy=False) -&gt; (Tensor):<NewLine>      Expected a value of type 'Tensor' for argument 'other' but instead found type 'int'.<NewLine>      <NewLine>      aten::to.dtype(Tensor self, int dtype, bool non_blocking=False, bool copy=False) -&gt; (Tensor):<NewLine>      Expected at most 4 arguments but found 5 positional arguments.<NewLine>      <NewLine>      aten::to.device(Tensor self, Device device, int dtype, bool non_blocking=False, bool copy=False) -&gt; (Tensor):<NewLine>      Expected a value of type 'Device' for argument 'device' but instead found type 'int'.<NewLine>      <NewLine>      aten::to.dtype_layout(Tensor self, *, int dtype, int layout, Device device, bool pin_memory=False, bool non_blocking=False, bool copy=False) -&gt; (Tensor):<NewLine>      Argument dtype not provided.<NewLine>      <NewLine>      aten::to(Tensor(a) self, Device? device, int? dtype=None, bool non_blocking=False, bool copy=False) -&gt; (Tensor(b|a)):<NewLine>      Expected a value of type 'Optional[Device]' for argument 'device' but instead found type 'int'.<NewLine>      <NewLine>      aten::to(Tensor(a) self, int? dtype=None, bool non_blocking=False, bool copy=False) -&gt; (Tensor(b|a)):<NewLine>      Expected at most 4 arguments but found 5 positional arguments.<NewLine>      <NewLine>      aten::to(Tensor(a) self, bool non_blocking=False, bool copy=False) -&gt; (Tensor(b|a)):<NewLine>      Expected a value of type 'bool' for argument 'non_blocking' but instead found type 'int'.<NewLine>    <NewLine>    The original call is:<NewLine>    at code/__torch__/refactored/box_regression.py:20:13<NewLine>        boxes: Tensor) -&gt; Tensor:<NewLine>        _0 = torch.eq(deltas, deltas)<NewLine>        _1 = torch.bitwise_not(torch.eq(torch.abs(deltas), inf))<NewLine>        bool_tensor = torch.__and__(_0, _1)<NewLine>        _2 = int(torch.item(torch.all(bool_tensor)))<NewLine>        if bool(_2):<NewLine>          pass<NewLine>        else:<NewLine>          ops.prim.RaiseException(""Exception"")<NewLine>        boxes0 = torch.to(boxes, ops.prim.dtype(deltas), False, False, None)<NewLine>                 ~~~~~~~~ &lt;--- HERE<NewLine>        _3 = torch.slice(boxes0, 0, 0, 9223372036854775807, 1)<NewLine>        _4 = torch.slice(boxes0, 0, 0, 9223372036854775807, 1)<NewLine>        widths = torch.sub(torch.select(_3, 1, 2), torch.select(_4, 1, 0), alpha=1)<NewLine>        _5 = torch.slice(boxes0, 0, 0, 9223372036854775807, 1)<NewLine>        _6 = torch.slice(boxes0, 0, 0, 9223372036854775807, 1)<NewLine>        heights = torch.sub(torch.select(_5, 1, 3), torch.select(_6, 1, 1), alpha=1)<NewLine>        _7 = torch.slice(boxes0, 0, 0, 9223372036854775807, 1)<NewLine>        ctr_x = torch.add(torch.select(_7, 1, 0), torch.mul(widths, 0.5), alpha=1)<NewLine>        _8 = torch.slice(boxes0, 0, 0, 9223372036854775807, 1)<NewLine>    Compiled from code at /root/refactored/box_regression.py:86:16<NewLine>            bool_tensor : torch.Tensor = (deltas == deltas) &amp; ~(torch.eq(deltas.abs(), torch._six.inf))<NewLine>            # bool_tensor : torch.Tensor = torch.functional.isfinite(deltas)<NewLine>            assert bool_tensor.all().item(), ""Error!""<NewLine>            boxes = boxes.to(dtype=deltas.dtype)<NewLine>                    ~~~~~~~~ &lt;--- HERE<NewLine>            ...<NewLine></code></pre><NewLine><p>For some reason scripting added <code>None</code> as fifth parameter to function call <code>aten::to</code></p><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"December 26, 2019,  6:33pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry for the late reply.  In the future, please post in the “mobile” category so the mobile developers will see them.  Do you have a script or notebook that we can use to reproduce this issue?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: December 26, 2019,  6:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
64615,Model Loading issue Android,2019-12-20T11:37:34.186Z,1,333,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was loading and using my model in android app till yesterday but today my model is not loading.<br/><NewLine>I am using torch-1.4.0-Snapshot version.<br/><NewLine>Is there anything changed from last day that I should aware?</p><NewLine></div>",https://discuss.pytorch.org/u/mohit7,(Mohit Ranawat),mohit7,"December 20, 2019, 11:37am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/mohit7"">@mohit7</a>,<br/><NewLine>snapshot version is based on the latest master branch, it is possible that serialization etc. was changed in libtorch.</p><NewLine><p>Did you try it with the retraced/rescripted model using the latest pytorch nightly?</p><NewLine><p>Do you see any messages in logcat?</p><NewLine><p>Also we have one recently resolved issue with our nightlies<br/><NewLine>(Crash, that happened if <code>Tensor.fromBlob</code> was called before Module loading):<br/><NewLine>Issue: <a href=""https://github.com/pytorch/pytorch/issues/31419"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/31419</a><br/><NewLine>Fix in master: <a href=""https://github.com/pytorch/pytorch/commit/3a19980b78b6638235edef367784ecc3dc37e364"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/commit/3a19980b78b6638235edef367784ecc3dc37e364</a><br/><NewLine>Nightlies are already republished.</p><NewLine><p>That might be the reason of your Module loading issue.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Ivan,</p><NewLine><p>I am also facing same issue while loading model in android with pytorch nightly version. To reproduce this issue, I simply modified the build.gradle file in android ‘HelloWorldApp’ as follows</p><NewLine><p>repositories {<br/><NewLine>//jcenter()<br/><NewLine>maven {<br/><NewLine>url “<a href=""https://oss.sonatype.org/content/repositories/snapshots"" rel=""nofollow noopener"">https://oss.sonatype.org/content/repositories/snapshots</a>”<br/><NewLine>}<br/><NewLine>}</p><NewLine><p>dependencies {<br/><NewLine>implementation ‘androidx.appcompat:appcompat:1.1.0’</p><NewLine><pre><code>//implementation 'org.pytorch:pytorch_android:1.3.0'<NewLine>//implementation 'org.pytorch:pytorch_android_torchvision:1.3.0'<NewLine><NewLine>implementation 'org.pytorch:pytorch_android:1.4.0-SNAPSHOT'<NewLine>implementation 'org.pytorch:pytorch_android_torchvision:1.4.0-SNAPSHOT'<NewLine></code></pre><NewLine><p>}</p><NewLine><p>With this change, the app is not working. It is trowing an error</p><NewLine><p>A*<code>/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0 in tid 28782 (orch.helloworld), pid 28782 (orch.helloworld)</code>*</p><NewLine><p>Thanks<br/><NewLine>VGS</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is a bug.  Fix is <a href=""https://github.com/pytorch/pytorch/pull/31582"">https://github.com/pytorch/pytorch/pull/31582</a> .  Should be available in the next nightly build.  Sorry for the trouble.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vgsprasad; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: December 20, 2019, 11:13pm; <NewLine> REPLY_DATE 2: December 23, 2019,  5:57am; <NewLine> REPLY_DATE 3: December 24, 2019, 12:07am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
63813,Strange behavior in return list tensor,2019-12-12T03:47:52.483Z,0,130,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi guys,<br/><NewLine>I wanna convert tensor with size (4,3,2) to list of tensor (3,2) and run it on mobile.</p><NewLine><p>Here is code creating torchscript:</p><NewLine><pre><code class=""lang-auto"">class BatchAsList(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>    <NewLine>    def forward(self):<NewLine>        x = torch.arange(4*3*2).view(4,3,2)<NewLine>        a = []<NewLine>        for i in range(x.size(0)):<NewLine>            a.append(x[i])<NewLine>        return a<NewLine>    <NewLine>module = BatchAsList()<NewLine>script = torch.jit.script(module)        <NewLine>script.save('batch_as_list.pth')<NewLine></code></pre><NewLine><p>And code run on android:</p><NewLine><pre><code class=""lang-auto"">    // mTestModule is loaded from file batch_as_list.pth<NewLine>    Tensor [] output = mTestModule.forward().toTensorList();<NewLine>    for (int i = 0 ; i&lt; output.length; i++){<NewLine>      Log.i(TAG, ""Output "" + i + "" shape : "" + output[i]);<NewLine>    }<NewLine><NewLine>    long [] output0 = output[0].getDataAsLongArray();<NewLine>    long [] output1 = output[1].getDataAsLongArray();<NewLine>    long [] output2 = output[2].getDataAsLongArray();<NewLine>    long [] output3 = output[3].getDataAsLongArray();<NewLine><NewLine>    for (int j = 0 ; j&lt; output0.length; j++){<NewLine>      Log.i(TAG, ""Output : "" + output0[j] +"" ,""+ output1[j] +"" ,""+ output2[j] + "", ""+ output3[j] );<NewLine>    }<NewLine></code></pre><NewLine><p>And result:</p><NewLine><pre><code class=""lang-auto"">2019-12-12 10:35:10.352 18859-18859/org.pytorch.demo D/TEST: /data/user/0/org.pytorch.demo/files/batch_as_list.pth<NewLine>2019-12-12 10:35:10.381 18859-18859/org.pytorch.demo I/TEST: Output 0 shape : Tensor([3, 2], dtype=torch.int64)<NewLine>2019-12-12 10:35:10.381 18859-18859/org.pytorch.demo I/TEST: Output 1 shape : Tensor([3, 2], dtype=torch.int64)<NewLine>2019-12-12 10:35:10.382 18859-18859/org.pytorch.demo I/TEST: Output 2 shape : Tensor([3, 2], dtype=torch.int64)<NewLine>2019-12-12 10:35:10.382 18859-18859/org.pytorch.demo I/TEST: Output 3 shape : Tensor([3, 2], dtype=torch.int64)<NewLine>2019-12-12 10:35:10.382 18859-18859/org.pytorch.demo I/TEST: Output : 0 ,0 ,0, 0<NewLine>2019-12-12 10:35:10.382 18859-18859/org.pytorch.demo I/TEST: Output : 1 ,1 ,1, 1<NewLine>2019-12-12 10:35:10.382 18859-18859/org.pytorch.demo I/TEST: Output : 2 ,2 ,2, 2<NewLine>2019-12-12 10:35:10.382 18859-18859/org.pytorch.demo I/TEST: Output : 3 ,3 ,3, 3<NewLine>2019-12-12 10:35:10.382 18859-18859/org.pytorch.demo I/TEST: Output : 4 ,4 ,4, 4<NewLine>2019-12-12 10:35:10.382 18859-18859/org.pytorch.demo I/TEST: Output : 5 ,5 ,5, 5<NewLine></code></pre><NewLine><p>Shape of output is correct, but something wrong with the value of output. Output that i expected:</p><NewLine><pre><code class=""lang-auto"">0, 6,  12, 18<NewLine>1, 7,  13, 19<NewLine>2, 8,  14, 20<NewLine>3, 9,  15, 21<NewLine>4, 10, 16, 22<NewLine>5, 11, 17, 23<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/l4zyf9x,(Trình),l4zyf9x,"December 12, 2019,  3:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This looks like a bug with the handling of tensor strides.  We should be able to get it fixed in PyTorch 1.5.  In the mean time, if you change your line to <code>a.append(x[i].clone())</code>, it works around the bug.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is the PR that fixes the issue: <a href=""https://github.com/pytorch/pytorch/pull/31584"">https://github.com/pytorch/pytorch/pull/31584</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: December 24, 2019, 12:08am; <NewLine> REPLY_DATE 2: December 23, 2019, 11:53pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
63965,Pytorch - ONNX - mobile vs Pytorch#jit - mobile,2019-12-13T11:32:08.997Z,0,267,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I saw at least two ways to export pytorch models to android device namely via transfering a model from <code>Pytorch</code> to <code>Caffe2</code> and exporting a model with <code>jit</code> module.</p><NewLine><p>What’s the difference of the ways?<br/><NewLine>Which of them in which situation is preferable?</p><NewLine></div>",https://discuss.pytorch.org/u/Chame_call,(chame_call),Chame_call,"December 13, 2019,  4:52pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Using <code>torch.jit.script</code> or <code>torch.jit.trace</code> is preferable to using Caffe2.  The Caffe2 mobile engine is not being actively developed.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: December 23, 2019, 11:36pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
63950,[JIT] [Android] Debugging the model,2019-12-13T09:43:54.238Z,0,257,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello there,</p><NewLine><p>Recently I worked a lot with mobile deployment of PyTorch models and figured out that  having debugged TorchScript model on PC sometimes isn’t enough to run it without errors on Android.</p><NewLine><p>The main question is how to debug mobile model? At least print <code>Tensor</code> shapes during runtime. As I noticed Python <code>print</code> statements won’t produce outputs on mobile</p><NewLine><p>Excuse me for direct mention. Would you mind to clarify it <a class=""mention"" href=""/u/ivankobzarev"">@IvanKobzarev</a>?</p><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"December 13, 2019,  9:43am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Helo <a class=""mention"" href=""/u/zetyquickly"">@zetyquickly</a><br/><NewLine>Yes, python “print” does not show up in logcat, (I even tried it with  <code>adb shell setprop log.redirect-stdio true</code> on emulator)<br/><NewLine>By this moment our understanding is that model is debugged using python and we only run debugged model on mobile.<br/><NewLine>But we will think how we can expose print and maybe something else to logcat.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/zetyquickly"">@zetyquickly</a>,</p><NewLine><p>Just update about torchscript print and logcat:<br/><NewLine><code>adb shell setprop log.redirect-stdio</code> redirects only java <code>System.out</code><br/><NewLine>torchscript print by default uses native stdout.</p><NewLine><p>If you desperately need this print in android logcat, you may use this patch:<br/><NewLine><aside class=""onebox githubcommit""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/commit/a038b15a4d89dada18da55f6218058aaff85cc61"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Commit""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M10.86 7c-.45-1.72-2-3-3.86-3-1.86 0-3.41 1.28-3.86 3H0v2h3.14c.45 1.72 2 3 3.86 3 1.86 0 3.41-1.28 3.86-3H14V7h-3.14zM7 10.2c-1.22 0-2.2-.98-2.2-2.2 0-1.22.98-2.2 2.2-2.2 1.22 0 2.2.98 2.2 2.2 0 1.22-.98 2.2-2.2 2.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/commit/a038b15a4d89dada18da55f6218058aaff85cc61"" rel=""nofollow noopener"" target=""_blank"">[NFC][WIP] Android native stdout,stderr to logcat</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        committed <span class=""discourse-local-date"" data-date=""2019-12-17"" data-format=""ll"" data-time=""18:57:02"" data-timezone=""UTC"">06:57PM - 17 Dec 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/IvanKobzarev"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""IvanKobzarev"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars3.githubusercontent.com/u/6638825?v=4"" width=""20""/><NewLine>          IvanKobzarev<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""changed 2 files with 33 additions and 0 deletions""><NewLine><a href=""https://github.com/pytorch/pytorch/commit/a038b15a4d89dada18da55f6218058aaff85cc61"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+33</span><NewLine><span class=""removed"">-0</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>which I copy-pasted from <a href=""https://codelab.wordpress.com/2014/11/03/how-to-use-standard-output-streams-for-logging-in-android-apps/"" rel=""nofollow noopener"">https://codelab.wordpress.com/2014/11/03/how-to-use-standard-output-streams-for-logging-in-android-apps/</a></p><NewLine><p>If you build pytorch-android with this patch (<code>gradle -p android assembleDebug</code> and <a href=""https://github.com/pytorch/pytorch/tree/master/android#building-pytorch-android-from-source"" rel=""nofollow noopener"">use result aar files directly</a> ) after calling <code>PyTorchAndroid.nativeStdOutErrToLogcat()</code> you will see torchscript print in logcat.</p><NewLine><p>But we are not going to merge this :), we think about custom handling of print on mobile and print it to logcat. But doing it might take some time.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ivankobzarev"">@IvanKobzarev</a></p><NewLine><p>That will be very helpful thanks!</p><NewLine><p>BTW to call <code>PyTorchAndroid.nativeStdOutErrToLogcat()</code> should one import this function from some module, or <code>PyTorchAndroid</code> already is an upper level module?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s in <code>org.pytorch</code> package</p><NewLine><pre><code class=""lang-auto"">import org.pytorch.PyTorchAndroid;<NewLine><NewLine>...<NewLine>    PyTorchAndroid.nativeStdOutErrToLogcat()<NewLine>...<NewLine></code></pre><NewLine><p>Importing the whole class should work. It’s packaged in <code>pytorch_android*aar</code></p><NewLine><p>I have added example of its usage to this commit:<br/><NewLine><aside class=""onebox githubcommit""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/commit/62d94d10466991595d316c7e5ceebbb1e2f18210"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Commit""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M10.86 7c-.45-1.72-2-3-3.86-3-1.86 0-3.41 1.28-3.86 3H0v2h3.14c.45 1.72 2 3 3.86 3 1.86 0 3.41-1.28 3.86-3H14V7h-3.14zM7 10.2c-1.22 0-2.2-.98-2.2-2.2 0-1.22.98-2.2 2.2-2.2 1.22 0 2.2.98 2.2 2.2 0 1.22-.98 2.2-2.2 2.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/commit/62d94d10466991595d316c7e5ceebbb1e2f18210"" rel=""nofollow noopener"" target=""_blank"">[NFC][WIP] Android native stdout,stderr to logcat</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        committed <span class=""discourse-local-date"" data-date=""2019-12-17"" data-format=""ll"" data-time=""18:57:02"" data-timezone=""UTC"">06:57PM - 17 Dec 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/IvanKobzarev"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""IvanKobzarev"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars3.githubusercontent.com/u/6638825?v=4"" width=""20""/><NewLine>          IvanKobzarev<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""changed 3 files with 34 additions and 0 deletions""><NewLine><a href=""https://github.com/pytorch/pytorch/commit/62d94d10466991595d316c7e5ceebbb1e2f18210"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+34</span><NewLine><span class=""removed"">-0</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>(the branch is <a href=""https://github.com/pytorch/pytorch/compare/ik_wip_android_stdouterr_to_logcat"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/compare/ik_wip_android_stdouterr_to_logcat</a>)<br/><NewLine>We already use <code>org.pytorch.PyTorchAndroid</code> in our testapp in repo for loading module from asset:<br/><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/android/test_app/app/src/main/java/org/pytorch/testapp/MainActivity.java#L94"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/android/test_app/app/src/main/java/org/pytorch/testapp/MainActivity.java#L94</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> ,"REPLY_DATE 1: December 17, 2019,  7:39pm; <NewLine> REPLY_DATE 2: December 17, 2019,  7:49pm; <NewLine> REPLY_DATE 3: December 17, 2019,  7:49pm; <NewLine> REPLY_DATE 4: December 17, 2019,  9:02pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
63516,Android pytorch forward() method running in a separate thread slow down UI thread,2019-12-09T18:14:27.583Z,0,399,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is that possible to find out the reason which caused lagging of UI like specific thread or method?</p><NewLine><p>Cause I kind of put all the heavy work into another thread but I’ve still getting a lagged UI.</p><NewLine><p>I’m using  <code>CameraX</code>  where  <code>ImageAnalysis</code>  works in separate thread in which I do emotion detection via neural network. So when a neural network process an image in this not UI thread my UI thread also lags.</p><NewLine><p>Is that possible at all? Can the separate thread slow down the UI thread by executing heavy task?</p><NewLine><p>Maybe there’s some plugin for Android Studio or something like that to solve the problem.</p><NewLine><p>I will be glad to any advice…</p><NewLine></div>",https://discuss.pytorch.org/u/Chame_call,(chame_call),Chame_call,"December 9, 2019,  9:00pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/chame_call"">@Chame_call</a>.<br/><NewLine>At the moment pytorch android thread count is fixed by device and equal “number of big cores of cpu”(N) on device.<br/><NewLine>As you also have bg thread for image decoding + UI thread - at the moment of inference you have at least (N + 2) competing threads for N big cores (+other applications threads). I think that is the reason why you see UI thread slow downs.</p><NewLine><p>We are thinking to expose control of number of threads to java api, in that case you can set for example singleThread mode, that should not affect too much UI thread responsiveness.</p><NewLine><p>For more details I would recommend to use android systrace.<br/><NewLine>If you are building pytorch android from the source you can do deeper investigation with systrace if you build it with environment variable <code>TRACE_ENABLED=1</code><br/><NewLine>(<a href=""https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/CMakeLists.txt#L7"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/CMakeLists.txt#L7</a>)<br/><NewLine>It controls logging of additional sections for systrace, in that case you will see operators sections.</p><NewLine><p>If you need all debug symbols for tracing - you may check example of test_app in our repo, that has a script how to build it with all c++ debug symbols:</p><NewLine><pre><code class=""lang-auto"">TRACE_ENABLED=1 sh android/build_test_app.sh<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/chame_call"">@Chame_call</a></p><NewLine><p>We just exposed conrtol on global number of threads used by pytorch android, it was landed in <a href=""https://github.com/pytorch/pytorch/commit/62254430093fef5f2dece3825d95b25d443faf63"" rel=""nofollow noopener"">master</a></p><NewLine><pre><code class=""lang-auto"">method org.pytorch.Module#setNumThreads(int numThreads)<NewLine></code></pre><NewLine><p>(<a href=""https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/src/main/java/org/pytorch/Module.java#L57"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/src/main/java/org/pytorch/Module.java#L57</a>)<br/><NewLine>The latest android nightlies already include them: <a href=""https://github.com/pytorch/pytorch/tree/master/android#nightly"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/tree/master/android#nightly</a> (you might need gradle argument <code>--refresh-dependencies</code> if you already using them)</p><NewLine><p>For your case when inference is slowing down UI thread - you can experiment with setting number of threads to 1 or 2, maybe depending on device num cores, smth like:</p><NewLine><pre><code class=""lang-auto"">Module module = Module.load(moduleFileAbsoluteFilePath);<NewLine>module.setNumThreads(1);<NewLine></code></pre><NewLine><p>I think that should help with UI thread responsiveness, but check if inference time with this setting is still acceptable for your solution.</p><NewLine><p>This is new functionality, please report if you find any issues with it.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Update:</p><NewLine><p>We moved  <code>setNumThreads</code>  method to separate class  <code>org.pytorch.PyTorchAndroid</code>  as a static method.<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/src/main/java/org/pytorch/PyTorchAndroid.java#L33"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/src/main/java/org/pytorch/PyTorchAndroid.java#L33"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/master/android/pytorch_android/src/main/java/org/pytorch/PyTorchAndroid.java#L33</a></h4><NewLine><pre class=""onebox""><code class=""lang-java""><ol class=""start lines"" start=""23"" style=""counter-reset: li-counter 22 ;""><NewLine><li>  public static Module loadModuleFromAsset(final AssetManager assetManager, final String assetName) {</li><NewLine><li>    return new Module(new NativePeer(assetName, assetManager));</li><NewLine><li>  }</li><NewLine><li><NewLine></li><NewLine><li>  /**</li><NewLine><li>   * Globally sets the number of threads used on native side. Attention: Has global effect, all</li><NewLine><li>   * modules use one thread pool with specified number of threads.</li><NewLine><li>   *</li><NewLine><li>   * @param numThreads number of threads, must be positive number.</li><NewLine><li>   */</li><NewLine><li class=""selected"">  public static void setNumThreads(int numThreads) {</li><NewLine><li>    if (numThreads &lt; 1) {</li><NewLine><li>      throw new IllegalArgumentException(""Number of threads cannot be less than 1"");</li><NewLine><li>    }</li><NewLine><li><NewLine></li><NewLine><li>    nativeSetNumThreads(numThreads);</li><NewLine><li>  }</li><NewLine><li><NewLine></li><NewLine><li>  private static native void nativeSetNumThreads(int numThreads);</li><NewLine><li>}</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> ,"REPLY_DATE 1: December 9, 2019,  9:55pm; <NewLine> REPLY_DATE 2: December 11, 2019, 11:20pm; <NewLine> REPLY_DATE 3: December 16, 2019,  8:21pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
63981,RuntimeException: empty not implemented for TensorTypeSet,2019-12-13T14:15:44.819Z,0,451,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve exported Pytorch model for android and trying to load Module I’ve got the following error:</p><NewLine><blockquote><NewLine><pre><code>java.lang.RuntimeException: Unable to start activity ComponentInfo{org.pytorch.demo/org.pytorch.demo.vision.FrameProcessingActivity}: com.facebook.jni.CppException: empty not implemented for TensorTypeSet(VariableTensorId, CUDATensorId) (empty at aten/src/ATen/Functions.h:3679)<NewLine>(no backtrace available)<NewLine></code></pre><NewLine></blockquote><NewLine><p>whole stack trace is:</p><NewLine><blockquote><NewLine><p>E/AndroidRuntime: FATAL EXCEPTION: main<br/><NewLine>Process: org.pytorch.demo, PID: 19189<br/><NewLine>java.lang.RuntimeException: Unable to start activity ComponentInfo{org.pytorch.demo/org.pytorch.demo.vision.FrameProcessingActivity}: com.facebook.jni.CppException: empty not implemented for TensorTypeSet(VariableTensorId, CUDATensorId) (empty at aten/src/ATen/Functions.h:3679)<br/><NewLine>(no backtrace available)<br/><NewLine>at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2379)<br/><NewLine>at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2442)<br/><NewLine>at android.app.ActivityThread.access$800(ActivityThread.java:156)<br/><NewLine>at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1351)<br/><NewLine>at android.os.Handler.dispatchMessage(Handler.java:102)<br/><NewLine>at android.os.Looper.loop(Looper.java:211)<br/><NewLine>at android.app.ActivityThread.main(ActivityThread.java:5389)<br/><NewLine>at java.lang.reflect.Method.invoke(Native Method)<br/><NewLine>at java.lang.reflect.Method.invoke(Method.java:372)<br/><NewLine>at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1020)<br/><NewLine>at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:815)<br/><NewLine>Caused by: com.facebook.jni.CppException: empty not implemented for TensorTypeSet(VariableTensorId, CUDATensorId) (empty at aten/src/ATen/Functions.h:3679)<br/><NewLine>(no backtrace available)<br/><NewLine>at org.pytorch.NativePeer.initHybrid(Native Method)<br/><NewLine>at org.pytorch.NativePeer.(NativePeer.java:18)<br/><NewLine>at org.pytorch.Module.load(Module.java:23)<br/><NewLine>at org.pytorch.demo.models.BeautyDefiner.(BeautyDefiner.java:23)<br/><NewLine>at org.pytorch.demo.vision.FrameProcessingActivity.onCreate(FrameProcessingActivity.java:83)<br/><NewLine>at android.app.Activity.performCreate(Activity.java:5990)<br/><NewLine>at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1106)<br/><NewLine>at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2332)<br/><NewLine>… 10 more</p><NewLine></blockquote><NewLine><p>How to solve that error?</p><NewLine></div>",https://discuss.pytorch.org/u/Chame_call,(chame_call),Chame_call,"December 13, 2019,  2:15pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The problem was that I ran the model on the gpu before exporting with jit.<br/><NewLine>So I move the model to cpu and exported the model again.<br/><NewLine>And it worked.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Chame_call; <NewLine> ,"REPLY_DATE 1: December 13, 2019,  4:52pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
63102,Trace ssd decode function,2019-12-05T08:16:02.684Z,0,136,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone. I need to deploy ssd object detection model on android, but since torch ops does not available on android. I need to trace decode function (<a href=""https://github.com/amdegroot/ssd.pytorch/blob/master/layers/functions/detection.py"" rel=""nofollow noopener"">https://github.com/amdegroot/ssd.pytorch/blob/master/layers/functions/detection.py</a>) to decode detection result. but I do not manage to trace it. Does any one succeed to trace the decode part ?</p><NewLine><p>I tried to trace the function but I got the following error:</p><NewLine><blockquote><NewLine><p>Compiled from code export_decode_model.py(121): decode<br/><NewLine>export_decode_model.py(129): forward<br/><NewLine>/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py(525): _slow_forward<br/><NewLine>/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py(539): <strong>call</strong><br/><NewLine>/usr/local/lib/python3.6/dist-packages/torch/jit/<strong>init</strong>.py(997): trace_module<br/><NewLine>/usr/local/lib/python3.6/dist-packages/torch/jit/<strong>init</strong>.py(858): trace<br/><NewLine>export_decode_model.py(142): </p><NewLine></blockquote><NewLine><p>The above operation failed in interpreter, with the following stack trace:</p><NewLine></div>",https://discuss.pytorch.org/u/Kiki_Rizki_Arpiandi,(Kiki Rizki Arpiandi),Kiki_Rizki_Arpiandi,"December 5, 2019,  8:19am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We don’t have a recipe for custom ops yet.  Stay turned!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: December 10, 2019,  3:26am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
63088,Is there a function that can replace torch.Tensor.max() at mobile android?,2019-12-05T05:40:05.416Z,0,133,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My model need run torch.Tensor.max() meth to get the final index of my word dictionaries.<br/><NewLine>But now I don’t know what to do.</p><NewLine></div>",https://discuss.pytorch.org/u/mawserver,(MawManager),mawserver,"December 5, 2019,  5:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh, I found I could write one myself:sweat_smile:<br/><NewLine>But hopefully the mobile.Tensor will add some math util:laughing:</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>We are not planning to implement the full tensor API in the Java bindings.  If you want some extra result calculated (like the max), your best bet is to compute it as part of your model.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/mawserver; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: December 5, 2019,  5:57am; <NewLine> REPLY_DATE 2: December 9, 2019,  9:49pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
62838,Module output slightly off on Android,2019-12-03T06:44:23.938Z,6,574,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m learning how to port a module onto an Android app, but I’ve found that the output is a bit different on Android compared to the Python script.</p><NewLine><p>Here’s my model:</p><NewLine><pre><code class=""lang-python"">class Dummy(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Dummy, self).__init__()<NewLine>        self.fc = nn.Linear(416 * 416 * 3, 10)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = x.reshape(x.shape[0], -1)<NewLine>        x = self.fc(x)<NewLine>        return x<NewLine></code></pre><NewLine><p>And the model convert script:</p><NewLine><pre><code class=""lang-auto"">model = Dummy()<NewLine>model.load_state_dict(torch.load(""dummy.pt""))<NewLine><NewLine>example = torch.randn(1, 3, 416, 416)<NewLine><NewLine>traced_script_module = torch.jit.trace(model, example)<NewLine>traced_script_module.save(""dummy-android.pt"")<NewLine></code></pre><NewLine><p>In the Python script, the output to my example image was:<br/><NewLine><code>[-0.1566336453,  0.0841832012,  0.0985929519,  0.0115705878, -0.3650150299, 0.3889884949,  0.0307857171, -0.1416997164, 0.2296864390, -0.2360394597]</code><br/><NewLine>And on Android:<br/><NewLine><code>[-0.15890475, 0.07862158, 0.09994077, 0.015047294, -0.3655298, 0.38745546, 0.03088907, -0.13880219, 0.23389207, -0.24074274]</code></p><NewLine><p>Is this behavior normal?</p><NewLine><p>Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/minhduc0711,,minhduc0711,"December 3, 2019,  6:46am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried to reproduce the issue with the code you posted - the results I got on desktop and android device were the same with the same input tensor.<br/><NewLine>Could you please double check if the input tensor is identical? Did you try some fixed tensor like torch.ones(1, 3, 416, 416)? or did you decode it from some image/text file?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The results I got above was from an image.</p><NewLine><p>When I try torch.ones(1, 3, 416, 416) the results are closer:</p><NewLine><ul><NewLine><li><NewLine><strong>Python</strong>:<br/><NewLine><code>[-0.2647885382,  0.1896176785,  0.5414127707, -0.1591370702, -0.4948869348,  0.9562743902,  0.2362334579, -0.3959787488, 0.4024670124, -0.4271552861] </code><NewLine></li><NewLine><li><NewLine><strong>Android</strong>:<br/><NewLine><code>[-0.26478815, 0.18961564, 0.5414131, -0.15913847, -0.49488664, 0.956275, 0.23623104, -0.39597753, 0.40246645, -0.4271558]</code><NewLine></li><NewLine></ul><NewLine><p>The L2 norm for these vectors was 3.8e-06, comparing to 1e-02 for my first results.<br/><NewLine><a class=""mention"" href=""/u/ljk53"">@ljk53</a> What do your results look like?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah, the new results were similar to the difference I saw. 1e-02 seems too large. What’s the format of the image? Do you use the same library to decode the image? Did you convert the input to 0~255 or 0~1? I’d use some random input at the same scale to double check. Can you please check if the raw input tensors decoded from the image are about the same?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I did convert the image to 0~1.<br/><NewLine>My image decoding code on Android:</p><NewLine><pre><code class=""lang-java"">Bitmap bitmap = BitmapFactory.decodeStream(getAssets().open(""dog_processed.jpg""));<NewLine>float[] zeros = new float[]{0f, 0f, 0f};<NewLine>float[] ones = new float[]{1f, 1f, 1f};  // Since the return values is already 0~1<NewLine>Tensor inputTensor = TensorImageUtils.bitmapToFloat32Tensor(bitmap, zeros, ones);<NewLine></code></pre><NewLine><p>I also tried using a smaller input of size 3 x 2 x 2 (and smaller model), and the norm goes to 9.57e-08.<br/><NewLine>So my inputs were correct, and the error increases with more computations.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you please share the python code you used to convert image to tensor as well? I’ll try to repro on my side. Thanks!</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sure here it is:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import imageio<NewLine><NewLine>img = imageio.imread(""data/small.jpg"")<NewLine>dummy_input = torch.tensor(img).permute([2, 0, 1]).unsqueeze(0) / 255.<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/minhduc0711"">@minhduc0711</a> <a class=""mention"" href=""/u/ljk53"">@ljk53</a> I remember somebody posted a very similar issue for iOS couple of months ago, not sure it’s related,  but worth looking into - <a href=""https://github.com/pytorch/pytorch/issues/27813"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/27813</a>. Short answer - Try <code>.png</code> or <code>.bmp</code> instead of <code>.jpg</code>.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, I tried <code>.png</code> and the norm was 2e-6.</p><NewLine><p>Nevertheless, this behavior seems pretty strange. I hope there will be a workaround for <code>.jpg</code> files in the future.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>We discussed this internally, and one other issue that came up was the size of your linear layer.  You’re doing a sum of 50,000 products, which can accumulate a lot of floating point error (which can differ between different platforms).  You should get better results with a more traditional model that uses some convolutional and pooling layers before the linear layer.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ljk53; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/minhduc0711; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ljk53; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/minhduc0711; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ljk53; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/minhduc0711; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/minhduc0711; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: December 5, 2019,  4:10am; <NewLine> REPLY_DATE 2: December 5, 2019, 10:38am; <NewLine> REPLY_DATE 3: December 6, 2019,  3:44am; <NewLine> REPLY_DATE 4: December 6, 2019,  4:41am; <NewLine> REPLY_DATE 5: December 6, 2019,  6:15am; <NewLine> REPLY_DATE 6: December 6, 2019,  6:30am; <NewLine> REPLY_DATE 7: December 13, 2019,  2:09am; <NewLine> REPLY_DATE 8: December 8, 2019,  9:36am; <NewLine> REPLY_DATE 9: December 13, 2019,  2:09am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> 
62862,How to multiply tensor on mobile,2019-12-03T10:13:39.144Z,0,141,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I need to multiply tensor for calculating priors/anchorbox for my object detection model on android does Java has tensor multipication operation yet?</p><NewLine></div>",https://discuss.pytorch.org/u/Kiki_Rizki_Arpiandi,(Kiki Rizki Arpiandi),Kiki_Rizki_Arpiandi,"December 3, 2019, 10:23am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, and we have no plans to add it.  Please update your TorchScript model to do any tensor operations you need so that the Java code can simply provide raw input and consume raw output.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: December 6, 2019,  4:03am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
63196,Can not create a tensor in torchscript loaded on mobile,2019-12-06T03:36:01.191Z,0,268,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I tried to create tensor follow below code:</p><NewLine><pre><code class=""lang-auto"">...<NewLine>punc_character_torch = torch.tensor([191, 193, 194, 195, 196], dtype=token.dtype, device=device)<NewLine></code></pre><NewLine><p>Convert this code to Torchscript and run it on my pc normally. But when i loaded torchscript to mobile, i got below issue:</p><NewLine><pre><code class=""lang-auto"">2019-12-06 10:24:21.652 15260-15260/org.pytorch.demo E/AndroidRuntime: FATAL EXCEPTION: main<NewLine>    Process: org.pytorch.demo, PID: 15260<NewLine>    java.lang.RuntimeException: is_variable() || !defined() CHECK FAILED at ../torch/csrc/autograd/variable.h<NewLine>    The above operation failed in interpreter, with the following stack trace:<NewLine>    at code/__torch__/torchscript/Model.py:1686:27<NewLine>          ops.prim.RaiseException(""Exception"")<NewLine>        if torch.gt(alpha, 0):<NewLine>          pass<NewLine>        else:<NewLine>          ops.prim.RaiseException(""Exception"")<NewLine>        device = ops.prim.device(encoder_outputs)<NewLine>        out_dtype = ops.prim.dtype(encoder_outputs)<NewLine>        encoder_output_hidden_size = torch.size(encoder_outputs, -1)<NewLine>        max_encoder_length = torch.size(encoder_outputs, 1)<NewLine>        punc_character_torch = torch.tensor([191, 193, 194, 195, 196], dtype=ops.prim.dtype(token), device=device, requires_grad=False)<NewLine>                               ~~~~~~~~~~~~ &lt;--- HERE<NewLine></code></pre><NewLine><p>It seem that torchscript still does not support create tensor on mobile device. Any idea to tackle this problem?</p><NewLine></div>",https://discuss.pytorch.org/u/l4zyf9x,(Trình),l4zyf9x,"December 6, 2019,  3:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you try this with the latest nightly (1.4.0-SNAPSHOT)?  Instructions are in the first half of this post: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/android-somethings-went-wrong-with-pytorch-android-1-4-0-snapshot/61009"">[Android] Something's went wrong with pytorch_android-1.4.0-SNAPSHOT</a></p><NewLine><p>This part of the code was refactored and the issue should either be fixed or at least have a clearer error message.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: December 12, 2019,  3:06am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
60990,Model Loading issue in android,2019-11-14T15:13:35.986Z,5,735,"<div class=""post"" itemprop=""articleBody""><NewLine><p>My model contains nn.Functional.interpolate layer and it is converted properly to .pt model using jit.<br/><NewLine>But Loading same model in android giving issues.<br/><NewLine>This is happening in case of interpolate layer only.</p><NewLine><p>Edit - Error trace</p><NewLine><p>java.lang.RuntimeException: Unable to start activity ComponentInfo{org.pytorch.custom_model/org.pytorch.custom_model.MainActivity}: com.facebook.jni.CppException: false CHECK FAILED at …/c10/core/Backend.h (tensorTypeIdToBackend at …/c10/core/Backend.h:106)<br/><NewLine>(no backtrace available)</p><NewLine></div>",https://discuss.pytorch.org/u/mohit7,(Mohit Ranawat),mohit7,"November 15, 2019,  4:19am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Provide a stacktrace please</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/zetyquickly"">@zetyquickly</a> I have edit the trace in the post. Kindly look into it</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The most common reason for this is version discrepancies between the PyTorch version you trace the model with and the PyTorch Mobile one.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/tom"">@tom</a> , I am using pytorch_android 1.3.1 and pytorch android torchvision 1.3.1<br/><NewLine>and pytorch version 1.3.1 cpu. I didn’t find any discrepancies for versions.<br/><NewLine>Can you enlighten more?</p><NewLine><p>Thanks<br/><NewLine>Mohit Ranawat</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi@mohit7,I meet the same problem as you. Now I am trying to retrain the model with pytorch 1.3.If you have solved the problem, I hope you can give me some advice, thank you. You can contact with me. My email address is 1209648713@qq. com.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/janice_w"">@Janice_w</a> I didn’t got any solution yet. Hope I’ll get the solution.<br/><NewLine>If you got the solution tell me</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I meet this problem too(on iOS though). The model I use is from here but I use my own images for SRGAN.<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/leftthomas/SRGAN/blob/master/model.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/leftthomas/SRGAN/blob/master/model.py"" rel=""nofollow noopener"" target=""_blank"">leftthomas/SRGAN/blob/master/model.py</a></h4><NewLine><pre><code class=""lang-py"">import math<NewLine>import torch<NewLine>from torch import nn<NewLine><NewLine><NewLine>class Generator(nn.Module):<NewLine>    def __init__(self, scale_factor):<NewLine>        upsample_block_num = int(math.log(scale_factor, 2))<NewLine><NewLine>        super(Generator, self).__init__()<NewLine>        self.block1 = nn.Sequential(<NewLine>            nn.Conv2d(3, 64, kernel_size=9, padding=4),<NewLine>            nn.PReLU()<NewLine>        )<NewLine>        self.block2 = ResidualBlock(64)<NewLine>        self.block3 = ResidualBlock(64)<NewLine>        self.block4 = ResidualBlock(64)<NewLine>        self.block5 = ResidualBlock(64)<NewLine>        self.block6 = ResidualBlock(64)<NewLine>        self.block7 = nn.Sequential(<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/leftthomas/SRGAN/blob/master/model.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>P.S. I tried onnx-&gt;coreml and onnx-&gt;tensorflow pb -&gt; coreml and both way failed. Very frustrated as a beginner.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>hey <a class=""mention"" href=""/u/kunstlicheseele"">@kunstlicheSeele</a> onnx to tensorflow conversion is not stable.<br/><NewLine>Better to not use it.You can go fro keras to tensorflow and you can transfer your weights from pytorch to keras.<br/><NewLine>Hope this helps.<br/><NewLine>Thanks,<br/><NewLine>Mohit Ranawat</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>For anyone who is following this thread, the issue was resolved at <a href=""https://github.com/pytorch/pytorch/issues/29806"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/29806</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mohit7; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/mohit7; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Janice_w; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/mohit7; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/kunstlicheSeele; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/mohit7; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: November 14, 2019,  3:16pm; <NewLine> REPLY_DATE 2: November 15, 2019,  4:19am; <NewLine> REPLY_DATE 3: November 15, 2019,  7:18am; <NewLine> REPLY_DATE 4: November 15, 2019,  7:31am; <NewLine> REPLY_DATE 5: November 20, 2019,  8:04am; <NewLine> REPLY_DATE 6: November 20, 2019,  8:44am; <NewLine> REPLY_DATE 7: November 27, 2019,  4:09pm; <NewLine> REPLY_DATE 8: November 29, 2019,  6:02am; <NewLine> REPLY_DATE 9: December 5, 2019, 12:59am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> REPLY 9 LIKES: ; <NewLine> 
58814,Tutorial and End to End Example for Pytorch Mobile on Android,2019-10-21T15:00:31.578Z,0,591,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi everyone, i recently published a tutorial on <strong>Image Classification on Android with Pytorch Mobile</strong></p><NewLine><p>Since documentation and tutorials are still few on this, i included an easy to comprehend open source android application for image recognition.</p><NewLine><p>The post is here: <a href=""https://heartbeat.fritz.ai/pytorch-mobile-image-classification-on-android-5c0cfb774c5b"" rel=""nofollow noopener"">https://heartbeat.fritz.ai/pytorch-mobile-image-classification-on-android-5c0cfb774c5b</a></p><NewLine><p>And the repository for the android application is here: <a href=""https://github.com/johnolafenwa/PytorchMobile"" rel=""nofollow noopener"">https://github.com/johnolafenwa/PytorchMobile</a></p><NewLine><p>This is intended as a reference to help pytorch developers easily deploy models with Pytorch Mobile.</p><NewLine></div>",https://discuss.pytorch.org/u/johnolafenwa,(John Olafenwa),johnolafenwa,"October 21, 2019,  3:01pm",5 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Great tutorial, do you manage to deploy object detection on android?. I need to multiply detection output tensor with priorbox tensor somehow</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Kiki_Rizki_Arpiandi; <NewLine> ,"REPLY_DATE 1: December 3, 2019, 10:32am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
62444,Worse performance using PyTorch Mobile than on GPU/CPU,2019-11-28T16:35:24.168Z,0,398,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a custom model that is a variation on YOLOv3, to test the results, I have asserted that the inputTensor on device is the same as that I am loading on the computer. The output (which has detections and classifications) is giving near identical object and class confidences. However, the locations (x,y,w,h) are slightly off. Is this expected behaviour? Do you know if there is anything in particular I should investigate in my model or the trace of my model?</p><NewLine></div>",https://discuss.pytorch.org/u/IsaacBerman,(Isaac Berman),IsaacBerman,"November 28, 2019,  4:35pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/isaacberman"">@IsaacBerman</a>,</p><NewLine><p>Do you use completely the same model on mobile and desktop, without any quantization on mobile?</p><NewLine><p>If you already debugged it, maybe you have some ideas which operator/layer is producing different results?</p><NewLine><p>Could you please dump all operators that your model use, here are steps how to do it:</p><NewLine><p>First we need <code>dump_operator_names</code> executable which can be built with command:</p><NewLine><pre><code class=""lang-auto"">BUILD_BINARIES=1 python setup.py build<NewLine></code></pre><NewLine><p>After succesfull build you can find it in path like <code>./build/lib.linux-x86_64-3.7/torch/bin/dump_operator_names</code>.</p><NewLine><p>It depends on <code>libcaffe2_observers.so</code>, <code>libtorch.so</code>, <code>libc10.so</code>, so they should be either located in the same folder or be installed on the system.</p><NewLine><pre><code class=""lang-auto""><NewLine>mkdir tmp<NewLine>cp ./build/lib.linux-x86_64-3.7/torch/bin/dumpop_operator_names tmp/<NewLine>cp ./build/lib.linux-x86_64-3.7/torch/lib/libcaffe2_observers.so tmp<NewLine>cp ./build/lib.linux-x86_64-3.7/torch/lib/libtorch.so tmp<NewLine>cp ./build/lib.linux-x86_64-3.7/torch/lib/libc10.so tmp<NewLine></code></pre><NewLine><p>After that you can run it, specifing model and output file.</p><NewLine><pre><code class=""lang-auto"">./dump_operator_names --model=model.pt --output=model_ops.yaml<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Ivan,</p><NewLine><p>Thanks for your response. I ended up solving the error. It turned out it was a problem with the tracing – see here: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/torch-jit-trace-only-works-on-example-input/62478"">Torch.jit.trace() only works on example input?</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/IsaacBerman; <NewLine> ,"REPLY_DATE 1: December 2, 2019,  9:33pm; <NewLine> REPLY_DATE 2: December 5, 2019, 12:58am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
61710,Incomprehensible behaviour,2019-11-21T10:05:18.772Z,1,896,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I’ve tried to use custom model on Android but forward fails with error I unable to understand.</p><NewLine><p>I’ve scripted my model using TorchScript annotation method and now I am trying to preform a forward on mobile.</p><NewLine><p>Model looks something like that:</p><NewLine><pre><code class=""lang-auto"">config = ...<NewLine>class WrapRPN(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.rpn = RPN(config).eval().cpu()<NewLine>    def forward(self, features):<NewLine>        # type: (Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]<NewLine>        mock_input : InputClass = InputClass(torch.rand((N, 320, 320)))<NewLine>        instances = self.rpn(mock_input, features)<NewLine>        output : Dict[str, torch.Tensor] = {}<NewLine>        for idx in range(len(instances)):<NewLine>            inst : Instances = instances[idx]<NewLine>            box_tensor : torch.Tensor = inst.proposal_boxes.tensor<NewLine>            output[str(idx)] = box_tensor<NewLine>        return output<NewLine></code></pre><NewLine><p>It has been converted and loaded to mobile, but fails on runtime</p><NewLine><pre><code class=""lang-auto"">E/AndroidRuntime: FATAL EXCEPTION: main<NewLine>    Process: org.pytorch.helloworld, PID: 20157<NewLine>    java.lang.RuntimeException: Unable to start activity ComponentInfo{org.pytorch.helloworld/org.pytorch.helloworld.MainActivity}: com.facebook.jni.CppException: forward() Expected a value of type 'Dict[str, Tensor]' for argument 'features' but instead found type 'Dict[str, Tensor]'.<NewLine>    Position: 1<NewLine>    Declaration: forward(ClassType&lt;WrapRPN&gt; self, Dict(str, Tensor) features) -&gt; (Dict(str, Tensor)) (checkArg at ../aten/src/ATen/core/function_schema_inl.h:194)<NewLine>    (no backtrace available)<NewLine>        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3784)<NewLine>        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3955)<NewLine>        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:91)<NewLine>        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:149)<NewLine>        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:103)<NewLine>        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2392)<NewLine>        at android.os.Handler.dispatchMessage(Handler.java:107)<NewLine>        at android.os.Looper.loop(Looper.java:213)<NewLine>        at android.app.ActivityThread.main(ActivityThread.java:8147)<NewLine>        at java.lang.reflect.Method.invoke(Native Method)<NewLine>        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:513)<NewLine>        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1100)<NewLine>     Caused by: com.facebook.jni.CppException: forward() Expected a value of type 'Dict[str, Tensor]' for argument 'features' but instead found type 'Dict[str, Tensor]'.<NewLine>    Position: 1<NewLine>    Declaration: forward(ClassType&lt;WrapRPN&gt; self, Dict(str, Tensor) features) -&gt; (Dict(str, Tensor)) (checkArg at ../aten/src/ATen/core/function_schema_inl.h:194)<NewLine>    (no backtrace available)<NewLine>        at org.pytorch.NativePeer.forward(Native Method)<NewLine>        at org.pytorch.Module.forward(Module.java:37)<NewLine>        at org.pytorch.helloworld.MainActivity.onCreate(MainActivity.java:66)<NewLine>        at android.app.Activity.performCreate(Activity.java:8068)<NewLine>        at android.app.Activity.performCreate(Activity.java:8056)<NewLine>        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1320)<NewLine>        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3757)<NewLine>        	... 11 more<NewLine></code></pre><NewLine><p>Java code is following:</p><NewLine><pre><code class=""lang-auto"">    final Tensor inputTensor = TensorImageUtils.bitmapToFloat32Tensor(bitmap,<NewLine>        TensorImageUtils.TORCHVISION_NORM_MEAN_RGB, TensorImageUtils.TORCHVISION_NORM_STD_RGB);<NewLine><NewLine>    Map&lt;String, IValue&gt; hm = new HashMap&lt;String, IValue&gt;();<NewLine>    List&lt;String&gt; keys = Arrays.asList(""p2"", ""p3"", ""p4"", ""p5"", ""p6"");;<NewLine>    for (String key : keys) {<NewLine>      hm.put(key, IValue.from(inputTensor));<NewLine>    }<NewLine>    final IValue rpn_input = IValue.dictStringKeyFrom(hm);<NewLine>    module.forward(rpn_input);<NewLine></code></pre><NewLine><p>What does it mean?<br/><NewLine><code>Expected a value of type 'Dict[str, Tensor]' for argument 'features' but instead found type 'Dict[str, Tensor]'</code> <img alt="":exploding_head:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/exploding_head.png?v=9"" title="":exploding_head:""/></p><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"November 21, 2019, 10:08am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>So, I’ve spent a little time to understand what is going on and to find a way to overcome this issue.</p><NewLine><p>What I’ve noticed:</p><NewLine><ol><NewLine><li>If we use constructions such as <code>IValue.dictStringKeyFrom(hm)</code>, where <code>hm</code> is <code>HashMap&lt;String, IValue&gt;</code> or if we use <code>IValue.listFrom(lst)</code>, where <code>lst</code> is <code>List&lt;IValue&gt;</code> we will obtain the behaviour described above.</li><NewLine><li>But if we use <code>IValue.from(arr)</code>, where <code>arr</code> of type <code>Tensor[]</code>, we will not face this issue and JAVA won’t tell that it’s expected <code>List[Tensor]</code> but got <code>List[Tensor]</code><NewLine></li><NewLine></ol><NewLine><p>Concluding this, there’s no way to construct analogue of <code>(2)</code> using dictionaries.<br/><NewLine><code>IValue</code> (<a href=""https://pytorch.org/docs/stable/org/pytorch/IValue.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/org/pytorch/IValue.html</a>) doesn’t have overriden method <code>public static IValue dictStringKeyFrom(Map&lt;String, T&gt; map)</code>, where <code>T</code> is <code>Tensor</code>.<br/><NewLine>I think in this case it might work, but it does not deny the fact there might be a bug</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for this finding.</p><NewLine><p>I reproduced it locally and debugging it. List[Tensor] will be represented as a separate type on libtorch IValue side.</p><NewLine><p>Looks like we have some unexpected behavior on jni with Dict types which is converted to libtorch IValue{GenericDict}}.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/zetyquickly"">@zetyquickly</a>,</p><NewLine><p>Thanks one more time for this finding.</p><NewLine><p>It happened as in jni tensorType was deduced from the first entry value of dictionary, including shape, requires_grad etc.<br/><NewLine>While torchscript function did not have it. Dict is not covariant, so the subtype check required equal KeyType(str) and ValueType(TensorType0).<br/><NewLine>TensorType’s of function argument and provided value were different and typecheck failed.<br/><NewLine>Error message did not include that additional information about TensorType.</p><NewLine><p>This problem was fixed on android-jni level in commit:<br/><NewLine><aside class=""onebox githubcommit""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/commit/5ada5363fc18d36ddf3f6b5f54123b80a42ec819"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Commit""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M10.86 7c-.45-1.72-2-3-3.86-3-1.86 0-3.41 1.28-3.86 3H0v2h3.14c.45 1.72 2 3 3.86 3 1.86 0 3.41-1.28 3.86-3H14V7h-3.14zM7 10.2c-1.22 0-2.2-.98-2.2-2.2 0-1.22.98-2.2 2.2-2.2 1.22 0 2.2.98 2.2 2.2 0 1.22-.98 2.2-2.2 2.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/commit/5ada5363fc18d36ddf3f6b5f54123b80a42ec819"" rel=""nofollow noopener"" target=""_blank"">GenericDict/List type use unshapedType() (#30428)</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        committed <span class=""discourse-local-date"" data-date=""2019-11-26"" data-format=""ll"" data-time=""23:31:26"" data-timezone=""UTC"">11:31PM - 26 Nov 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/IvanKobzarev"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""IvanKobzarev"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars3.githubusercontent.com/u/6638825?v=4"" width=""20""/><NewLine>          IvanKobzarev<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""changed 1 files with 5 additions and 6 deletions""><NewLine><a href=""https://github.com/pytorch/pytorch/commit/5ada5363fc18d36ddf3f6b5f54123b80a42ec819"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+5</span><NewLine><span class=""removed"">-6</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><pre class=""github-content"" style=""white-space: normal;"">Summary:<NewLine>Pull Request resolved: https://github.com/pytorch/pytorch/pull/30428<NewLine>Reported issue https://discuss.pytorch.org/t/incomprehensible-behaviour/61710<NewLine>Steps to reproduce:<NewLine>```<NewLine>class WrapRPN(nn.Module):<NewLine> def __init__(self):<NewLine> super().__init__()<NewLine> def forward(self, features):<NewLine> # type: (Dict[str, Tensor]) -&gt; int<NewLine>...</pre><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>It was merged in master recently.</p><NewLine><p>Separate issue for more detailed error messages in TensorType checks.<br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/30418"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/30418"" rel=""nofollow noopener"" target=""_blank"">More detailed information about TensorType in error messages</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-11-25"" data-format=""ll"" data-time=""21:34:09"" data-timezone=""UTC"">09:34PM - 25 Nov 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/IvanKobzarev"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""IvanKobzarev"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars3.githubusercontent.com/u/6638825?v=4"" width=""20""/><NewLine>          IvanKobzarev<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">🚀 Feature<NewLine>More detailed information about TensorType, including TensorType specifiers as shape, strides, req_grad etc. in error messages<NewLine>Motivation<NewLine>Reported issue https://discuss.pytorch.org/t/incomprehensible-behaviour/61710<NewLine>Error message is...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">enhancement</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>Android nightlies (snapshots) are already republished with this fix, the error should not happen with them.<br/><NewLine>To use nightlies (to force refresh dependencies gradle has argument <code>--refresh-dependencies</code>)</p><NewLine><pre><code class=""lang-auto"">repositories {<NewLine>    maven {<NewLine>        url ""https://oss.sonatype.org/content/repositories/snapshots""<NewLine>    }<NewLine>}<NewLine><NewLine>dependencies {<NewLine>    ...<NewLine>    implementation 'org.pytorch:pytorch_android:1.4.0-SNAPSHOT'<NewLine>    implementation 'org.pytorch:pytorch_android_torchvision:1.4.0-SNAPSHOT'<NewLine>    ...<NewLine>}<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""61710""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/z/3da27b/40.png"" width=""20""/> zetyquickly:</div><NewLine><blockquote><NewLine><p>if we use <code>IValue.listFrom(lst)</code> , where <code>lst</code> is <code>List&lt;IValue&gt;</code> we will obtain the behaviour described above</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thanks a lot for the FIX! Tell me please, does it also affect also behviour of <code>List&lt;IValue&gt;</code>? When I tried it also fails with this “List not a List” error</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, that problem affected both our Generic containers (Dict and List) when the element type was TensorType. After the fix GenericList is also initialized with c10::unshapedType(firstElement) which should fix the problem like ‘List[Tensor] is not List[Tensor]’</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp#L509"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp#L509"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/master/android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp#L509</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""499"" style=""counter-reset: li-counter 498 ;""><NewLine><li>              facebook::jni::JArrayClass&lt;JIValue::javaobject&gt;::javaobject()&gt;(</li><NewLine><li>              ""toList"");</li><NewLine><li>  auto jarray = jMethodGetList(jivalue);</li><NewLine><li>  size_t n = jarray-&gt;size();</li><NewLine><li>  if (n == 0) {</li><NewLine><li>    return at::IValue{c10::impl::GenericList(c10::TensorType::get())};</li><NewLine><li>  }</li><NewLine><li><NewLine></li><NewLine><li>  auto jivalue_first_element = jarray-&gt;getElement(0);</li><NewLine><li>  auto first_element = JIValue::JIValueToAtIValue(jivalue_first_element);</li><NewLine><li class=""selected"">  c10::impl::GenericList list{c10::unshapedType(first_element.type())};</li><NewLine><li>  list.reserve(n);</li><NewLine><li>  list.push_back(first_element);</li><NewLine><li>  for (auto i = 1; i &lt; n; ++i) {</li><NewLine><li>    auto jivalue_element = jarray-&gt;getElement(i);</li><NewLine><li>    auto element = JIValue::JIValueToAtIValue(jivalue_element);</li><NewLine><li>    list.push_back(element);</li><NewLine><li>  }</li><NewLine><li>  return at::IValue{list};</li><NewLine><li>} else if (JIValue::kTypeCodeDictStringKey == typeCode) {</li><NewLine><li>  static const auto jMethodGetDictStringKey =</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/IvanKobzarev; <NewLine> ,"REPLY_DATE 1: November 21, 2019,  6:39pm; <NewLine> REPLY_DATE 2: November 21, 2019, 10:54pm; <NewLine> REPLY_DATE 3: December 5, 2019, 12:58am; <NewLine> REPLY_DATE 4: November 27, 2019, 12:29pm; <NewLine> REPLY_DATE 5: November 27, 2019,  4:44pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
58336,Anyone tried out the PyTorch Mobile demo apps yet?,2019-10-16T03:40:33.794Z,20,1760,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Would love the feedback!</p><NewLine></div>",https://discuss.pytorch.org/u/jspisak,"(Facebook AI, Product Manager)",jspisak,"October 17, 2019,  3:37am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>So I gave it a shot for android but immediately entered a world of pain that probably is entirely unrelated to PyTorch but more with Android and my lack of Android experience.</p><NewLine><p>I should admit that my experience with Android is very limited (I adapted the Caffe2 AI Camera demo app to use PyTorch with the ResNet, MarkRCNN, and Style Transfer about a year ago and ported PyTorch to Android for it, but that is about it).</p><NewLine><p>Some of these comments might eventually lead to useful additions to the tutorial, but I don’t think I’m there yet.</p><NewLine><p>The tutorial says:</p><NewLine><blockquote><NewLine><p>We recommend you to open this project in <a href=""https://developer.android.com/studio"" rel=""nofollow noopener"">Android Studio</a>, in that case you will be able to install Android NDK and Android SDK using Android Studio UI.</p><NewLine></blockquote><NewLine><p>This looks foolproof, but it seems not quite Thomas-poof:</p><NewLine><ul><NewLine><li>It seems that you absolutely need to use the latest Android studio (mine was, I’m guessing, about 10 months old) - if you don’t have it, you get the most undescriptive error messages for an internet search reveals meant “upgrade to latest android studio” three versions ago, too,</li><NewLine><li>Maybe one could say that one shoud <em>Import</em> the (gradle) project rather than opening it. Opening did nothing for me.</li><NewLine></ul><NewLine><p>Then I tried the Hello World app:</p><NewLine><ul><NewLine><li>Didn’t seem to work on the Android Emulator - I just got a white screen.</li><NewLine><li>Works on actual hardware (I get a Wolf or dog classified as such).</li><NewLine></ul><NewLine><p>Happily, the PyTorch demo app worked better - it worked on both.</p><NewLine><p>On my phone  (BQ Aquaris U Plus, so not a high-end phone), I get ~3.5x images / second for the quantized resnet. This left me wondering what I should expect. In particular, I have been wondering whether the PyTorch build I’m using is a debug or a release build - I realize that Android release builds come with signing and stuff, so it would be a debug build from the android studio side, but that doesn’t necessarily mean anything for the PyTorch library itself. I am guessing that it is a non-debug build of PyTorch – with the assumption that most people don’t need that even when they are debugging their app.</p><NewLine><p>It is neat so see this work out and I admire that you furnished a text classification example, too. It would be supercool if you could add the model construction / serialization for the quantized resnet and text app, too (the HelloWorld app has the (trivially) simple tracing of resnet 18, but I didn’t see them for the PyTorch demo app).</p><NewLine><p>All in all, it looks good! Thank you.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the feedback!  We’ll get the docs updated and figure out why HelloWorld isn’t working on the emulator.</p><NewLine><p>The ResNet model is not quantized and expected to be fairly slow.  We might just delete it from the demo app.</p><NewLine><p>The quantized MobileNetV2 model is based on this tutorial: <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a> .  The text model was prepared by <a href=""https://nbviewer.jupyter.org/gist/dreiss/ee4ff1ed2e137326d13e96bb4f953061"" rel=""nofollow noopener"">https://nbviewer.jupyter.org/gist/dreiss/ee4ff1ed2e137326d13e96bb4f953061</a> .  The weights came from a trained PyText model.  We’ll look into getting these links included in the app or docs.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you, David!</p><NewLine><p>Besteht regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I trained a custom resnet18 and mobilenetv2 with 3 classes output. However they all failed when I substituted the “resnet18.pt” from the example with my own models.</p><NewLine><pre><code class=""lang-auto"">    findViewById(R.id.vision_card_resnet_click_area).setOnClickListener(v -&gt; {<NewLine>      final Intent intent = new Intent(VisionListActivity.this, ImageClassificationActivity.class);<NewLine>      intent.putExtra(ImageClassificationActivity.INTENT_MODULE_ASSET_NAME, ""resnet18-custom.pt"");<NewLine>      intent.putExtra(ImageClassificationActivity.INTENT_INFO_VIEW_TYPE,<NewLine>          InfoViewFactory.INFO_VIEW_TYPE_IMAGE_CLASSIFICATION_RESNET);<NewLine></code></pre><NewLine><p>Relevant error logs:</p><NewLine><pre><code class=""lang-auto"">E/PyTorchDemo: Error during image analysis<NewLine>    com.facebook.jni.CppException: false CHECK FAILED at aten/src/ATen/Functions.h (empty at aten/src/ATen/Functions.h:3535)<NewLine>    (no backtrace available)<NewLine>        at org.pytorch.Module$NativePeer.initHybrid(Native Method)<NewLine>        at org.pytorch.Module$NativePeer.&lt;init&gt;(Module.java:70)<NewLine>        at org.pytorch.Module.&lt;init&gt;(Module.java:25)<NewLine>        at org.pytorch.Module.load(Module.java:21)<NewLine>        at org.pytorch.demo.vision.ImageClassificationActivity.analyzeImage(ImageClassificationActivity.java:167)<NewLine>        at org.pytorch.demo.vision.ImageClassificationActivity.analyzeImage(ImageClassificationActivity.java:31)<NewLine></code></pre><NewLine><p>Is there anything else I need to do to change to use my custom model?</p><NewLine><p>Thank you.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah, well.</p><NewLine><ul><NewLine><li>There is no C++ traceback, so it’s hard to tell what went wrong. My guess from the traceback is that it is in the loading of the model rather than executing.</li><NewLine><li>Are you sure you put a <em>TorchScript</em> (most probably traced)  model at the right place?</li><NewLine></ul><NewLine><p>The HelloWorld app has a .py for exporting the model.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your response. I did put in a traced script module though. And it’s indeed during the model loading time.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/lkhphuc"">@lkhphuc</a> <a class=""mention"" href=""/u/tom"">@tom</a>. A common cause for this is converting the model with a version of Pytorch less than 1.3.0. As of present, only models converted with the latest pytorch version would load.</p><NewLine><p>See my latest tutorial on Pytorch Mobile including an easy to reuse open source image recognition example.<br/><NewLine>This provides a useful guide</p><NewLine><p><a class=""onebox"" href=""https://heartbeat.fritz.ai/pytorch-mobile-image-classification-on-android-5c0cfb774c5b"" rel=""nofollow noopener"" target=""_blank"">https://heartbeat.fritz.ai/pytorch-mobile-image-classification-on-android-5c0cfb774c5b</a></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>My model was jit.trace and save yesterday on Google Colab. I checked and the pytorch version is 1.3.0+cu100. I think it was the latest already.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>If the Pytorch version is fine. This error also occurs when the model path is not found. Since the Module class accepts an absolute path to the model file. I suggest you confirm that the path to the model is a valid file.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you able to share this model?  If so, we can debug and improve this error message.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/johnolafenwa"">@johnolafenwa</a> Even if I changed my model name to ‘resnet18.pt’ to replace the current one in the Android repo, it’s still has problem.<br/><NewLine><a class=""mention"" href=""/u/david_reiss"">@David_Reiss</a> You can find the model here: <a href=""https://drive.google.com/file/d/1fAhLpkoqNR32KQtzEkWe0LhBfAYb7Eiu/view?usp=sharing"" rel=""nofollow noopener"">https://drive.google.com/file/d/1fAhLpkoqNR32KQtzEkWe0LhBfAYb7Eiu/view?usp=sharing</a></p><NewLine><p>Thank you.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for sharing the model. We are following up the issue at: <a href=""https://github.com/pytorch/pytorch/issues/28379"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/28379</a></p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>So I have played with it some more:</p><NewLine><ul><NewLine><li>I must say I dislike the hack around reading models (copying from assets to fs). I got burnt by forgetting to overwrite it more than once. My libtorch/JNI adaptation used <code>torch::jit::load</code> with string streams. This is less memory efficient, of course, but for development it is much more convenient. If it isn’t controversial, I could see if I file a PR.</li><NewLine><li>One of the first things I needed was a method to get output pictures, I’ll file a PR for adding something like that to torchvision.</li><NewLine><li>I hit a bug with some arm32 ops. Model works fine on Android/x86, colors/scanline widths are off on Android/arm(32). The net uses <code>aten::_convolution, aten::add, aten::contiguous, aten::instance_norm, aten::reflection_pad2d, aten::relu_, aten::tanh, prim::Constant, prim::GetAttr, prim::ListConstruct</code> with the most fancy convolutions being with stride=2 and a transposed one with stride=2 and output padding=1 (so no non-default dilations,  kernel 1,3,7). I’m still trying to narrow this down. As error looks like it is some “striding”/contiguous problem, I tried inserting lots of <code>contiguous</code> but it didn’t immediately help. I know this isn’t narrowed down enough yet, unfortunately.</li><NewLine></ul><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/david_reiss"">@David_Reiss</a> and <a class=""mention"" href=""/u/ljk53"">@ljk53</a></p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I must say I dislike the hack around reading models (copying from assets to fs).</p><NewLine></blockquote><NewLine><p>We expect most production apps to download models at runtime, so reading from the filesystem should be more natural in that case.  However, you’re right that it feels hacky when you’re first developing with the model as an asset.  We would accept a PR that added reading the model directly from the APK.</p><NewLine><blockquote><NewLine><p>I hit a bug with some arm32 ops.</p><NewLine></blockquote><NewLine><p>Please let us know what you find.  Or, if you are comfortable sharing the model, we can debug as well.</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried out the hello-world app and could not get it to work on an emulator or actual device (Huawei P20 Lite). However, the demo app works perfectly.</p><NewLine><p>I tried setting up a test with my own model (for person re-identification) using a similar setup as on the demo app and I get an error as soon as I load my model:</p><NewLine><pre><code class=""lang-auto"">java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.pytorchmobile/com.example.pytorchmobile.MainActivity}: com.facebook.jni.CppException: false CHECK FAILED at ../c10/core/Backend.h (tensorTypeIdToBackend at ../c10/core/Backend.h:106)<NewLine>    (no backtrace available)<NewLine></code></pre><NewLine><p>This is also the case when I try importing the Super Resolution model from here: <a href=""https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html</a></p><NewLine><p>Also, another question: <strong>is PyTorch mobile stable enough</strong> and suitable for production? Or should we just use ONNX and Caffe 2 until it is more mature?</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>To me the error being around the Backend sounds like you have your model on GPU but need CPU.</p><NewLine><aside class=""quote no-group"" data-post=""18"" data-topic=""58336""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/kareem_belgharbi/40/17340_2.png"" width=""20""/> Kareem_Belgharbi:</div><NewLine><blockquote><NewLine><p>Also, another question: <strong>is PyTorch mobile stable enough</strong> and suitable for production? Or should we just use ONNX and Caffe 2 until it is more mature?</p><NewLine></blockquote><NewLine></aside><NewLine><p>So I’m not in a position to make official pronouncements, but so the bulk of PyTorch mobile is “just libtorch”, and that is reasonably stable. So there might be bugs and I would expect that mobile gets better over time (maybe the API can be made nicer, there certainly seems some room for faster), but I would expect that what runs today will continue run well. Also people here will try to help you when you run into something blocking you.<br/><NewLine>Caffe2 on the other hand <a href=""https://discuss.pytorch.org/t/how-to-build-caffe2-with-onnx-opset-version-greater-than-9/59675/3"">doesn’t have support anymore.</a></p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the response. The model was trained with CPU only PyTorch so I’m not sure if it can be the problem.</p><NewLine><p>My main recommendation for PyTorch mobile is to add more descriptive error messages. Most errors just show up in the logcat as a cpp file (usually backend, import, or function_schema_inl.h) and line number, which is hard to debug. Right now the platform is very new, and there isn’t much information out there on how to fix errors so we have to figure it out based on the error messages.</p><NewLine><p>Overall though, I think the API is actually quite nice, and it’s really great that mobile support has been added.</p><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>We tried both ‘Hello World’ and ‘Demo’ apps. We were able to port on device. ResNet18 model with FC layer as final layer is working fine. But, when we try to replace the FC layer with Conv1x1 layer with same weights and bias, the App is not giving same output as the original one. We used Conv1x1 layer instead of FC layer in our models, because some frameworks like Intel’s OpenVINO, etc are not supporting FC layers.</p><NewLine><p>In PyTorch, both models are giving same output. In App, they are giving different outputs. Is there any issue in ‘torch.jit.trace’ API, when Conv1x1 is used in model?</p><NewLine><p>Conv1x1 layer weights and bias are initialized as follows</p><NewLine><p><em>resnet_model.conv1x1.weight.data = (resnet_model.model.fc.weight.data).view(1000, 512, 1, 1)</em><br/><NewLine><em>resnet_model.conv1x1.bias.data = resnet_model.model.fc.bias.data</em></p><NewLine><p>Your feedback is appreciated.</p><NewLine><p>Thanks and Regards<br/><NewLine>Prasad</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/lkhphuc; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/lkhphuc; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/johnolafenwa; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/lkhphuc; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/johnolafenwa; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/lkhphuc; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/ljk53; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/jspisak; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/David_Reiss; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/Kareem_Belgharbi; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/Kareem_Belgharbi; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/vgsprasad; <NewLine> ,"REPLY_DATE 1: October 16, 2019, 10:56am; <NewLine> REPLY_DATE 2: October 16, 2019,  2:24pm; <NewLine> REPLY_DATE 3: October 16, 2019,  2:29pm; <NewLine> REPLY_DATE 4: October 20, 2019,  4:26pm; <NewLine> REPLY_DATE 5: October 20, 2019,  6:09pm; <NewLine> REPLY_DATE 6: October 20, 2019,  7:02pm; <NewLine> REPLY_DATE 7: October 21, 2019,  3:06pm; <NewLine> REPLY_DATE 8: October 21, 2019,  3:25pm; <NewLine> REPLY_DATE 9: October 21, 2019,  4:32pm; <NewLine> REPLY_DATE 10: October 21, 2019,  4:47pm; <NewLine> REPLY_DATE 11: October 21, 2019,  5:43pm; <NewLine> REPLY_DATE 12: October 21, 2019,  8:01pm; <NewLine> REPLY_DATE 13: October 30, 2019,  9:09am; <NewLine> REPLY_DATE 14: November 4, 2019,  4:57am; <NewLine> REPLY_DATE 15: November 5, 2019,  4:38pm; <NewLine> REPLY_DATE 16: November 5, 2019,  6:22pm; <NewLine> REPLY_DATE 17: November 6, 2019,  6:46am; <NewLine> REPLY_DATE 18: November 6, 2019,  6:18pm; <NewLine> REPLY_DATE 19: November 8, 2019,  4:39am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 3 Likes; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: 1 Like; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> REPLY 18 LIKES: ; <NewLine> REPLY 19 LIKES: ; <NewLine> 
61014,Pytorch Mobile iOS Resnet50 (Not Computing),2019-11-14T17:33:20.535Z,7,532,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using torch.jit to trace a pretrained vanilla resnet50 to import over to iOS and call using Pytorch Mobile // C++ on iOS.</p><NewLine><pre><code class=""lang-auto"">- (NSInteger)predictImage:(void*)imageBuffer forLabels:(NSInteger)labelCount {<NewLine>    int outputLabelIndex = -1;<NewLine>    try {<NewLine>        std::cout &lt;&lt; ""\npredictImage"";<NewLine>    at::Tensor tensor = torch::from_blob(imageBuffer, {1, 3, 224, 224}, at::kFloat);<NewLine>        std::cout &lt;&lt; ""\npredictImageTwo"";<NewLine>    torch::autograd::AutoGradMode guard(false);<NewLine>    at::AutoNonVariableTypeMode non_var_type_mode(true);<NewLine>// Pass in image tensor to C++ scripted torch module<NewLine>    **auto outputTensor = _impl.forward({tensor}).toTensor();**<NewLine>        std::cout &lt;&lt; ""\nReceived outputTensor"";<NewLine></code></pre><NewLine><p>The line before I print received outputTensor never prints because I never receive anything from _impl.forward in the Predict Image function in my “TorchModule.mm” file in my iOS project.</p><NewLine><p>Basically, I’ve gotten resnet18, 34, and mobilenet to work using Pytorch Mobile and the iOS demo but won’t receive the output tensor for resnet50 and above… is resnet50 supported? If someone is able to get a resnet50 working for mobile could they help?</p><NewLine><p>Much appreciated!</p><NewLine></div>",https://discuss.pytorch.org/u/HussainHaris,(Haris),HussainHaris,"November 14, 2019,  5:33pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">#from torchvision.models import inception_v3<NewLine>device = torch.device('cpu')<NewLine><NewLine>model = models.resnet50(pretrained=True)<NewLine>model.load_state_dict(models.resnet50(pretrained=True).state_dict())<NewLine>model = nn.Sequential(<NewLine>    #ImageScale(),<NewLine>    model,<NewLine>    nn.Softmax(1)<NewLine>)<NewLine>model.eval()<NewLine>input_tensor = torch.rand(1,3,224,224)<NewLine>script_model = torch.jit.trace(model, input_tensor)<NewLine>script_model.save(""models/resnet50.pt"")<NewLine></code></pre><NewLine><p>Just for reference this is how I’m saving my model before taking it into iOS. Worked perfectly for resnet34 and resnet18</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Haris, this is a known issue. We’ve been working on fixing it. The problem is that the pthreadpool runs into a deadlock situation when running resnet50.  There are a couple of work around for it, you can try</p><NewLine><ul><NewLine><li>Set the number of thread to one in ThreadPool.cc</li><NewLine><li>Use a different mutex in <code>int ThreadPool::getNumThreads() const</code> function</li><NewLine><li>Use <code>std::unique_lock&lt;std::mutex&gt; guard(executionMutex_, std::defer_lock);</code> instead</li><NewLine></ul><NewLine><p>Then recompile the PyTorch from source code by following the link here - <a href=""https://pytorch.org/mobile/ios/#build-pytorch-ios-libraries-from-source"" rel=""nofollow noopener"">https://pytorch.org/mobile/ios/#build-pytorch-ios-libraries-from-source</a>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Tao!</p><NewLine><p>Thanks so much for the response! For “3.” just to clarify you’re suggesting std:: unique_lock as an alternative mutex to use for the 2nd solution? Will try this tomorrow and update here. Also I don’t have to do both one and two do I? I can do either one xor two?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry for the confusion, I just re-edited the comment. Actually, I have a fix being reviewed here - <a href=""https://github.com/pytorch/pytorch/pull/29885"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/29885</a>. If you’d like to try it out, you can patch that PR.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I cloned the Pytorch source repo and then followed the instructions to recompile/build iOS libraries (with the same file patch/changes as the PR)</p><NewLine><p>I then replaced the install folder under Pods/Libtorch/Install in my iOS project with the newly compiled install folder; I don’t get any errors regarding path changes and Swift builds the project correctly.</p><NewLine><p>However, when calling Predict Image on any model now (including previously working resnet34), I get the following check failederror:</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/a22090b1fb60f9ad74a1d827b0e883c78d301cb8"" href=""https://discuss.pytorch.org/uploads/default/original/3X/a/2/a22090b1fb60f9ad74a1d827b0e883c78d301cb8.png"" title=""48%20AM""><img alt=""48%20AM"" data-base62-sha1=""n8f7uBHBII7BFnOZpRHkHwK1l7q"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/2/a22090b1fb60f9ad74a1d827b0e883c78d301cb8_2_10x10.png"" height=""96"" src=""https://discuss.pytorch.org/uploads/default/original/3X/a/2/a22090b1fb60f9ad74a1d827b0e883c78d301cb8.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">48%20AM</span><span class=""informations"">806×113 31.3 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>Tracing into the Dispatcher.h in the repo, I find it’s breaking here</p><NewLine><pre><code class=""lang-auto"">  const std::string dispatchKeyStr = toString(*dispatchKey);<NewLine>  TORCH_CHECK(false, ""Could not run '"", dispatchTable.operatorName(), ""' with arguments"",<NewLine>          "" from the '"", dispatchKeyStr, ""' backend. '"",<NewLine>          dispatchTable.operatorName(), ""' is only available for these backends: "",<NewLine>          dispatchTable.listAllDispatchKeys(), ""."");<NewLine>}<NewLine></code></pre><NewLine><p>Previously my Libtorch was installed from Cocopods specifically version 1.3.1. Is the version compiled from the repo I got from following the steps an equivalent version?</p><NewLine><p>Using a 1.4 nightly build of pytorch for the Python jit trace, was working perfectly fine before.</p><NewLine><p>I’ve seen false check errors solved by updating Pytorch or using nightly builds?</p><NewLine><p>In this case for the patch how can I prevent this, or could I take a specific file with the pthread change from my newly compiled install folder and replace at Libtorch in my project?</p><NewLine><p>Thanks in advance,<br/><NewLine>Haris</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Hussain, if you use <code>BUILD_PYTORCH_MOBILE=1 IOS_ARCH=arm64 ./scripts/build_ios.sh</code> to build your libraries, you shouldn’t see that error.</p><NewLine><p>If you’re still seeing that, I believe that was come out this morning or later yesterday. Obviously, our mobile CI failed to do its job. I’m working on adding the simulator tests now. Sorry for the frustration. Will have updates here once we’ve fixed it.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yep that’s exactly how I built it and got the issue.</p><NewLine><p>Yeah please let me know here when the Mobile CI is fixed and what versions it should work on/I should be building, I would greatly appreciate it!</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/hussainharis"">@HussainHaris</a></p><NewLine><p>The master is back to normal. You can try recompiling from source code. Let me know if you have any questions.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, I recompiled source code after pulling on Monday and it worked just fine! Thanks for all the help.</p><NewLine><p>Just a quick question before closing thread, what are the limitations on the types of Pytorch models that can currently go mobile with a trace? Ex) Inception, Faster RCNN (for object detection), segmentation models with encoders and decoders etc…</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Most models should be compatible with tracing or scripting.  More details are at <a href=""https://pytorch.org/docs/stable/jit.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/jit.html</a> .  If any ScriptModule works on server but not mobile, we would consider that a bug.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/HussainHaris; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/HussainHaris; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/HussainHaris; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/HussainHaris; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/xta0; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/HussainHaris; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/David_Reiss; <NewLine> ,"REPLY_DATE 1: November 14, 2019,  5:34pm; <NewLine> REPLY_DATE 2: November 15, 2019,  3:48am; <NewLine> REPLY_DATE 3: November 15, 2019,  1:15am; <NewLine> REPLY_DATE 4: November 15, 2019,  4:17am; <NewLine> REPLY_DATE 5: November 15, 2019,  5:12pm; <NewLine> REPLY_DATE 6: November 16, 2019, 12:47am; <NewLine> REPLY_DATE 7: November 18, 2019,  5:53pm; <NewLine> REPLY_DATE 8: November 20, 2019,  8:39pm; <NewLine> REPLY_DATE 9: November 20, 2019,  8:38pm; <NewLine> REPLY_DATE 10: November 21, 2019, 12:49am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> 
60752,FBGEMM with PyTorch Mobile,2019-11-12T14:15:25.786Z,1,1013,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to run a model with fbgemm qconfig on mobile? Or is it x86 only? Simply plugging such model into demo app triggers qnnpack assert here <a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp#L223"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp#L223</a><br/><NewLine>It seems like FBGEMM support was disabled by this commit for some reason <a href=""https://github.com/pytorch/pytorch/commit/6fead9afd4cdc6306fb0e2180ca625160b59ea71"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/commit/6fead9afd4cdc6306fb0e2180ca625160b59ea71</a></p><NewLine></div>",https://discuss.pytorch.org/u/pshashk,,pshashk,"November 12, 2019,  2:41pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I wasn’t able to get good results from QNNPACK compatible per tensor quantization qconfig. Target metric value relative to fp32 model:<br/><NewLine><code>get_default_qconfig('fbgemm')</code> -&gt; 99.8%<br/><NewLine><code>get_default_qconfig('qnnpack')</code> -&gt; 58.5%<br/><NewLine><code>default_qconfig</code> -&gt; 54.4%<br/><NewLine>Is there any way to reduce that gap without changing architecture?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>FBGEMM is supported only for x86. You can get very good accuracies for qnnpack also.<br/><NewLine>Please make sure that when you set:</p><NewLine><pre><code class=""lang-auto"">qconfig = torch.quantization.get_default_qconfig('qnnpack')<NewLine></code></pre><NewLine><p>You also do:</p><NewLine><pre><code class=""lang-auto"">torch.backends.quantized.engine = 'qnnpack'<NewLine></code></pre><NewLine><p>before running the model.<br/><NewLine>The poorer accuracy numbers are likely due to FBGEMM saturating for large weight/activation values, due to this issue:<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/intel/mkl-dnn/blob/f38fecf5b76421fe277cfb15ec1d5090f1d30c07/doc/advanced/int8_computations.md#1-inputs-of-mixed-type-u8-and-s8"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/intel/mkl-dnn/blob/f38fecf5b76421fe277cfb15ec1d5090f1d30c07/doc/advanced/int8_computations.md#1-inputs-of-mixed-type-u8-and-s8"" rel=""nofollow noopener"" target=""_blank"">intel/mkl-dnn/blob/f38fecf5b76421fe277cfb15ec1d5090f1d30c07/doc/advanced/int8_computations.md#1-inputs-of-mixed-type-u8-and-s8</a></h4><NewLine><pre><code class=""lang-md"">Int8 Computation Aspects {#dev_guide_int8_computations}<NewLine>=======================================================<NewLine><NewLine>&gt; This document uses **int8** to denote 8-bit integer no matter whether it is<NewLine>&gt; signed or unsigned. To emphasize the signedness of the data type<NewLine>&gt; **u8** (`uint8_t`) or **s8** (`int8_t`) are used. In particular, if a<NewLine>&gt; primitive has two inputs the types would be written using ""/"". For instance:<NewLine>&gt; - int8 GEMM denotes any integer GEMM with 8-bit integer inputs, while<NewLine>&gt; - u8/s8 GEMM denotes dnnl_gemm_u8s8s32() only.<NewLine><NewLine>The operation primitives that work with the int8 data type<NewLine>(#dnnl::memory::data_type::s8 and #dnnl::memory::data_type::u8)<NewLine>typically use s32 (`int32_t`) as an intermediate data type<NewLine>(#dnnl::memory::data_type::s32) to avoid integer overflows.<NewLine><NewLine>For instance, the int8 average [pooling](@ref dev_guide_pooling) primitive<NewLine>accumulates the int8 input values in a window to an s32 accumulator, then<NewLine>divides the result by the window size, and then stores the result back to the<NewLine>int8 destination:<NewLine><NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/intel/mkl-dnn/blob/f38fecf5b76421fe277cfb15ec1d5090f1d30c07/doc/advanced/int8_computations.md#1-inputs-of-mixed-type-u8-and-s8"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks. With engine set preparation, calibration, and conversion of the model work fine. But evaluation triggers errors like: <code>Error in QNNPACK: failed to create convolution with 0.1966128 input scale, 1.698165 kernel scale, and 0.2075303 output scale: convolution scale 1.608829 is greater or equal to 1.0</code>. The cause seems to be in the SE block implemented via 1x1 convolution that receives 1x1 input. I probably should have used <code>Linear</code> anyway, but maybe it will be useful to someone.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok, I’ve managed to get good result from QNNPACK. Maybe <code>torch.backends.quantized.engine</code> should be mentioned somewhere on <a href=""https://pytorch.org/docs/master/quantization.html"" rel=""nofollow noopener"">quantization page</a>?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Great that this worked! We will make sure to mention this on our quantization page. Thanks for the suggestion! cc <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pshashk; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pshashk; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/pshashk; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/supriyar; <NewLine> ,"REPLY_DATE 1: November 12, 2019,  3:21pm; <NewLine> REPLY_DATE 2: December 5, 2019, 12:57am; <NewLine> REPLY_DATE 3: November 13, 2019,  7:21am; <NewLine> REPLY_DATE 4: November 13, 2019,  3:21pm; <NewLine> REPLY_DATE 5: November 13, 2019,  5:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 2 Likes; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> 
